[
  {
    "id": 39244254,
    "title": "A Brief History of U.S. Government's Backdoor Attempts",
    "originLink": "https://www.atlasobscura.com/articles/a-brief-history-of-the-nsa-attempting-to-insert-backdoors-into-encrypted-data",
    "originBody": "A Brief History of the U.S. Trying to Add Backdoors Into Encrypted Data by Jessie Guy-Ryan February 21, 2016 A Brief History of the U.S. Trying to Add Backdoors Into Encrypted Data A government agent uses an NSA IBM 360/85 console in 1971 (Photo: Wikimedia Commons/NSA). It’s been a weird week for America’s most valuable company—a firm whose tech products have such consumer goodwill they got away with forcing us to listen to U2—who is poised to go to court against its own government over its users’ right to privacy. The government is invoking an obscure law dating back almost to the founding of the country to force the company to comply. It’d be a pretty good movie. ATLAS OBSCURA COURSES Learn with Us! Check out our lineup of courses taught by world-class experts from around the world. See Courses But it’s just the most dramatic flare-up in a lengthy battle between government officials, cybersecurity experts, and the tech industry over how consumer’s technical data is protected, and whether or not the government has a right to access that information. In fact, the government has actually won this fight before—secretly. Throughout 2015, U.S. politicians and law enforcement officials such as FBI director James Comey have publicly lobbied for the insertion of cryptographic “backdoors” into software and hardware to allow law enforcement agencies to bypass authentication and access a suspect’s data surreptitiously. Cybersecurity experts have unanimously condemned the idea, pointing out that such backdoors would fundamentally undermine encryption and could exploited by criminals, among other issues. While a legal mandate or public agreement would be needed to allow evidence obtained via backdoors to be admissible in court, the NSA has long attempted—and occasionally succeeded—in placing backdoors for covert activities. An Enigma machine at Bletchley Park, long-rumored to be one of the first backdoored devices (Photo: Flickr/Adam Foster). One of the most important developments in cryptography was the Enigma machine, famously used to encode Nazi communications during World War II. For years, rumors have persisted that the NSA (then SSA) and their British counterparts in the Government Communications Headquarters collaborated with the Enigma’s manufacturer, Crypto AG, to place backdoors into Enigma machines provided to certain countries after World War II. Crypto AG has repeatedly denied the allegations, and in 2015 the BBC sifted through 52,000 pages of declassified NSA documents to find the truth. The investigation revealed that while no backdoors were placed in the machines, there was a “gentlemen’s agreement” that Crypto AG would keep American and British intelligence appraised of “the technical specifications of different machines and which countries were buying which ones,” allowing analysts to decrypt messages much more quickly. Consider it a security “doggy-door.” Next, in 1993, the NSA promoted “Clipper chips,” which were intended to protect private communications while still allowing law enforcement to access them. In 1994, researcher Matt Blaze uncovered significant vulnerabilities in the “key escrow” system that allowed law enforcement access, essentially making the chips useless. By 1996, Clipper chips were defunct, as the tech industry adopted more secure, open encryption standards such as PGP. In more recent years, the NSA was unequivocally caught inserting a backdoor into the Dual_EC_DRBG algorithm, a cryptographic algorithm that was supposed to generate random bit keys for encrypting data. The algorithm, developed in the early aughts, was championed by the NSA and included in NIST Special Publication 800-90, the official standard for random-number generators released in 2007. Within a matter of months, researchers discovered the backdoor, and awareness that the algorithm was insecure quickly spread, although it continued to be implemented in consumer software Windows Vista. What was really odd, as crypto expert Bruce Schneier explained in a 2007 essay published in Wired, was that Dual_EC_DRBG wasn’t even worth the NSA’s effort: It makes no sense as a trap door: It’s public, and rather obvious. It makes no sense from an engineering perspective: It’s too slow for anyone to willingly use it. And it makes no sense from a backwards-compatibility perspective: Swapping one random-number generator for another is easy. A Chipper clip—one of the NSA’s unsuccessful backdoor attempts (Photo: Wikimedia Commons/Travis Goodspeed). Although the NSA’s effort puzzled crypto experts, documents leaked by Edward Snowden in 2013 proved that the NSA did indeed build a backdoor into Dual_EC_DRBG and paid RSA, a computer security company, to include the compromised algorithm in its software. These are the incidents that have been proven. There are, of course, numerous theories and insinuations that the NSA has made many more efforts along these lines—from backdoors in Lotus Notes to persistent allegations that Microsoft routinely includes backdoors in its software. Additionally, the Snowden leak proved that the NSA is constantly working to decrypt common encryption standards. As our lives become more and more dominated by the digital, security experts have become increasingly vocal in their calls for truly secure encryption, and some governments have begun to listen. Holland’s government has agreed not to use backdoors and support open encryption standards, and despite calls to do so in response to the Paris terrorist attacks, France refuses to implement a backdoor mandate. Even former NSA director Michael Hayden has said that backdoors are a bad idea (and he would know). As Apple vs. FBI wends its way through the courts, we are probably far from the end of this public battle. Whatever the results of this landmark case, the NSA’s classified efforts to subvert cryptography will likely continue. Read next The 21st Century Indiana Jones is a Woman Crowdsourcing Space Archaeology cybersecuritycryptographynews Want to see fewer ads? Become a Member. Most Read Stories 1 history How a Medieval Murder Map Helped Solve a 700-Year-Old London Cold Case It all started as a Cambridge criminologist’s macabre hobby. Sofia Quaglia 2 rodents Why NASA Is Watching Where Idaho’s Parachuting Beavers Landed The mini-paratroopers have a new role to play today. Danielle Hallock 3 performances Vienna's Vegetable Orchestra Is a Feast for Your Ears Inside a group that's spent 25 years turning beets into beats. Kaja Seruga 4 maps The WWII Treasure Map That Caused A Modern Day Hunt The question remains: Where is the loot? Frank Jacobs, Big Think 5 we all talk weird Why Do Nevadans Pronounce Their State's Name So Strangely? How you say it certainly says something about who you are. Dan Nosowitz View More Stories Want to see fewer ads? Become a Member.",
    "commentLink": "https://news.ycombinator.com/item?id=39244254",
    "commentBody": "A brief history of the U.S. trying to add backdoors into encrypted data (2016) (atlasobscura.com)427 points by whatever3 13 hours agohidepastfavorite149 comments mmaunder 10 hours agoHonorable mention for the ITAR regs that prevented Phil Zimmerman from exporting PGP 128 bit encryption until Zimmerman and MIT press printed the source as a book protected by the first amendment, exported it, and this enabled others to OCR it, and recompile it offshore. Also that ITAR enabled Thawte in South Africa (where I’m from) as a business to completely dominate sales for 128 bit SSL certs outside the US. Thawte was eventually acquired by Verizon for $600 million and the founder Mark Shuttleworth used the cash to become an astronaut and then founded Ubuntu. reply CWuestefeld 8 hours agoparentAt the time, I had a t-shirt that said \"this t-shirt is a munition\", because it also had on it the RSA public key algorithm encoded as a barcode. reply dramm 5 hours agorootparentHad the same t-shirt with the barcode readable source code on it. I think prompted by seeing Greg Rose wear one, may have gotten it from him/mutual friends. As an foreign citizen I was never brave enough to wear it through a USA entry airport. reply a-dub 1 hour agorootparentprevi seem to have vague recollection of a variant of this shirt that had a perl script on it? or was the perl script for decoding the barcode? reply mmaunder 8 hours agorootparentprevHaha I remember that. OG fist bump! reply e40 8 hours agorootparentprevCane to say the same thing. It sparked a lot of interesting conversations over the years. reply tjoff 2 hours agoparentprev> [...] and this enabled others to OCR it, and recompile it offshore. Did it? Or did it just give them plausible deniability? I remember playing with OCR as a kid and all the software I could get my hands on gave horrendous results, even if the input was as perfect as one could hope for. And even today I sometimes run tesseract on perfect screenshots and it still makes weird mistakes. Would be interesting to know if the book had any extra OCR-enabling features. I'm sure the recipients would get access to proper tools and software but OCRing source-code still seems like a nightmare back then. reply consp 59 minutes agorootparentBanks used it on checques for ages, why would it be that difficult? You do need a compatible typesetting though. reply bouncycastle 30 minutes agoparentprevRSA = Republic of South Africa reply p-e-w 7 hours agoparentprevI never understood the story about the book-printed PGP source code. Isn't source code protected speech under the first amendment anyway, regardless of the form in which it is transmitted? All kinds of media receive first amendment protection, including things like monetary donations, corporate speech, art, etc. I've never heard of there being a requirement for the printed form. Did the interpretation of the first amendment change recently in this regard? reply femto 4 hours agorootparentThe book was published in 1995 [1,2]. Free speech protection for source code under US law wasn't decided until 1996, in Bernstein v. United States [3]. [1] https://en.wikipedia.org/wiki/Phil_Zimmermann#Arms_Export_Co... [2] https://www.eff.org/deeplinks/2015/04/remembering-case-estab... [3] https://en.wikipedia.org/wiki/Bernstein_v._United_States reply int_19h 7 hours agorootparentprevThe idea was to make it blatantly clear that it's not a \"munition\". reply p-e-w 7 hours agorootparentWhy would that matter, if source code is protected speech anyway? And why is it more \"clear\" with a printed book vs. an emailed text file? reply rfw300 6 hours agorootparentI think if you’re looking for a logical answer from first principles, you won’t find one. It’s more that the legal system runs on precedent, and a book fits far more squarely in the fact patterns of previous First Amendment cases. Likely the source code case would end up with the same outcome, but it doesn’t hurt to make it more obvious. reply int_19h 4 hours agorootparentprevLegally speaking, it didn't really matter. But symbolically, having the feds argue that a book constitutes \"munitions\" would be bad optics for them in a way that is more understandable to the average American, compared to more legal arcane arguments about software having 1A protections. reply __MatrixMan__ 6 hours agorootparentprevThis was a time when we were happy when they managed to get congresspeople to understand that the internet is not like a truck, but more like a series of tubes. I think anchoring it to something old school like a book was a good call. reply inquirerGeneral 8 hours agoparentprevI didn't know any of this. Thanks. \"The U.S. Munitions List changes over time. Until 1996–1997, ITAR classified strong cryptography as arms and prohibited their export from the U.S.[5] Another change occurred as a result of Space Systems/Loral's conduct after the February 1996 failed launch of the Intelsat 708 satellite. The Department of State charged Space Systems/Loral with violating the Arms Export Control Act and the ITAR.[6][7] As a result, technology pertaining to satellites and launch vehicles became more carefully protected.\" https://en.wikipedia.org/wiki/International_Traffic_in_Arms_.... reply lkdfjlkdfjlg 10 hours agoparentprevnext [11 more] [flagged] int_19h 7 hours agorootparentIt really depends on the kind of law that you have. As the Soviet joke goes, \"severity of the laws is mitigated by not having to follow them\". reply rpmisms 8 hours agorootparentprevI know. We definitely aren't a nation founded on outright treason and insurrection, and thumbing our nose at authority and doing the moral thing isn't in our DNA in any way, shape, or form. Frustrating that some people think otherwise. reply Andrex 6 hours agorootparentThere can be moral reasons for treason. The founding fathers certainly considered themselves moral and principled. Your frustration is misguided. Simply do not confuse illegality with immorality. Human beings generally want to feel like they're doing more good than bad. As for \"the DNA of a nation,\" we would probably spend a few hours figuring out the definition of what that even is just for starters. reply makeitdouble 5 hours agorootparentFrom the outside, unironically calling any group of politicians \"fathers\" feels so weird. I know it's a super common turn of phrase, and that's kinda what gets me. > Human beings generally want to feel like they're doing more good than bad We're experts at convincing ourself that what is beneficial to us is also \"good\", whatever this actually means. reply Andrex 3 hours agorootparentWe can either rage against the era we were born into, or we can accept what we can change and what we cannot. One path leads to less frustration than the other. Be careful that you don't find yourself enjoying irritation. reply samstave 6 hours agorootparentprevThey were by facetious and sarcastic. ;-) reply FactKnower69 8 hours agorootparentprevnext [4 more] [flagged] alexdunmow 8 hours agorootparentMaybe you just have a faulty sarcasm detector? reply bongodongobob 7 hours agorootparentIt's no longer possible to tell. Some of the ideas that avg Americans have on the way just about anything works is frightening. My favorite lately is that the US was the real bad guy in WWII because we used nukes. reply int_19h 7 hours agorootparentThat US was a \"real bad guy\", or that nuking cities was an evil and unnecessary thing? The latter, while not universally held, is a fairly common take, and has been that way for a few decades now. reply halJordan 9 hours agorootparentprevITAR is quite sensible. We can make mistakes and we can be sensible but wrong. reply progbits 13 hours agoprevAs this is from 2016 it doesn't include this new fun revelation: > On 11 February 2020, The Washington Post, ZDF and SRF revealed that Crypto AG was secretly owned by the CIA in a highly classified partnership with West German intelligence, and the spy agencies could easily break the codes used to send encrypted messages. https://en.m.wikipedia.org/wiki/Crypto_AG reply EthanHeilman 6 hours agoparentI wrote blog entry on this subject with a very similar name [0] which covers the CryptoAG story in more detail. It doesn't have the 2020 news. [0]: A Brief History of NSA Backdoors (2013), https://www.ethanheilman.com/x/12/index.html reply samstave 6 hours agorootparentThis is an epically cool blog post! - submit it to HN on its own merits. This was of particular interest to me: >>>\"...1986 Reagan tipped off the Libyans that the US could decrypt their communications by talking about information he could only get through Libya decrypts on TV15. In 1991 the Iranians learned that the NSA could break their diplomatic communications when transcripts of Iranian diplomatic communications ended up in a French court case...\" Because, in 1986 - thats effectively when a lot of the phreaking and social engineering was at a peak - Cyberpunk was moving from imagination --> zeitgeist --> reality. Social engineering and line-printer litter recovery were yielding the backdoors into the Telecom Switching system. BBS's were raging [0]. So when you get a gaph-guffaw look into infosec in a slipup like these ones, it reinforces in mind that the 80s were some really wild times all around as technology tsunami'd from people's minds business and reality. [0] BBS Docu - https://www.imdb.com/title/tt0460402/ [1] phreaking - https://en.wikipedia.org/wiki/Phreaking [2] history of phreaking - https://www.youtube.com/watch?v=8PmkUPBhL4U reply EthanHeilman 5 hours agorootparentThanks, just submitted reply hedora 12 hours agoparentprevMore details here: https://web.archive.org/web/20200212014117/https://www.washi... reply pgeorgi 10 hours agoparentprevThe CIA/BND connection wasn't known, but the collusion with certain agencies was known to different degrees for decades: https://en.wikipedia.org/w/index.php?title=Crypto_AG&oldid=7... reply lcnPylGDnU4H9OF 10 hours agorootparentConsidering that I remember reading the CIA’s own historical document on this operation, I would guess its usefulness had run its course. If I’m not mistaken, it was the CIA who released the document to journalists; it seemed like bragging. reply treflop 8 hours agoparentprevThe guy who founded Crypto AG was really good friends with a guy who became a top dog at the NSA. reply 1337biz 6 hours agoparentprevWould be interesting what similar companies are (in parts) most likely agency fronts. My guess would be quite a few in the soft privacy selling business, such as VPN or email providers. reply Goodroo 5 hours agorootparentProton mail is a CIA front email provider reply hairyplanner 1 hour agorootparentI actually wish this was true. I want an email service that would last forever and is secure enough from my threats, namely security breaches of the email host and account takeover from non state actors. Gmail is close enough, but I want an alternative. An email service run by the nsa or the cia would be great. (No sarcasm is intended) reply AB1908 4 hours agorootparentprevIt is impossible to tell if this is satire or not. reply p-e-w 7 hours agoparentprev> The company had about 230 employees, had offices in Abidjan, Abu Dhabi, Buenos Aires, Kuala Lumpur, Muscat, Selsdon and Steinhausen, and did business throughout the world. That's a... really strange list of office locations, especially considering the relatively small number of employees. > The owners of Crypto AG were unknown, supposedly even to the managers of the firm, and they held their ownership through bearer shares. How does this work in practice? If management doesn't know who owns the company, how can the owners exercise influence on company business? reply quasse 5 hours agorootparentVia lawyer / legal representative if I had to hazard a guess. reply p-e-w 2 hours agorootparentHow does that representative prove that they really represent the owners, if the owners aren't known to management? How can they authorize someone without revealing identifying information? reply eviks 55 minutes agorootparentWhere would this need to really prove anything arise from? The intermediaries just hire and pay the managers, that's enough reply keepamovin 7 hours agoprevI was so curious about the origins of the SHA algorithms that I made a FOIA to NSA about SHA-0^0, as I wanted to understand how it was developed and requested all internal communications, diagrams, papers and so on responsive to that. Interestingly I found that after I got a reply (rough summary: you are a corporate requester, this is overly broad, it will be very expensive) I could no longer access the NSA website. Some kind of fingerprint block. The block persisted across IP addresses, browsers, incognito tabs, and devices so it can't be based on cookies / storage. Still in place today: Access Denied You don't have permission to access \"http://nsa.gov/serve-from-netstorage/\" on this server. 0: https://en.wikipedia.org/wiki/SHA-1#Development reply justinclift 3 hours agoparentThat url (http://nsa.gov/serve-from-netstorage/) works via Tor, so maybe try that? ;) reply Anon4Now 3 hours agoparentprevI'm curious as to why the NSA still has http:// urls. reply p-e-w 7 hours agoparentprev> Some kind of fingerprint block. The block persisted across IP addresses, browsers, incognito tabs, and devices so it can't be based on cookies / storage. Then what is it based on, if it happens across different devices and different IP addresses? I find it very surprising that the NSA would go to such technologically advanced lengths to block FOIA requesters from their website (which, needless to say, doesn't contain any sensitive information). reply keepamovin 7 hours agorootparentYeah weird, right? Highly surprising, high entropy, highly informative bit of signal possibly. Obvious way to admit SHA-0 is a pressure point maybe. Idk, maybe you can figure out the block, I think it's beyond me. Here's a picture if that helps haha! :) https://imgur.com/a/rNIjrB2 Highly unlikely to be a coincidence but I took it to mean: Don't make these requests ... OK ... haha! :) reply rvnx 4 hours agorootparentIt’s just Akamai being overzealous against bots. It could simply be you read more pages and it may have triggered anti-scraping rules. I cannot access many .gov websites either, and maybe it was after 5 pages or so. reply MichaelDickens 4 hours agorootparentprevThis seems like a good way to learn what information your system is leaking that it shouldn't be leaking, eg if you use a VPN and they still block you, your VPN is probably not doing what it claims to be doing. (AFAIK a correctly implemented VPN would not send any of your computer or browser information to nsa.gov.) reply themoonisachees 28 minutes agorootparentVPNs do not do what you seem to think they do. A VPN is a privacy tool as much as restarting your router to get a new IP lease is a privacy tool. reply lcnPylGDnU4H9OF 4 hours agorootparentprevThis honestly seems kinda fun. If one was really dedicated: buy new device with cash; purchased and used outside city of residence; don’t drive there, non-electric bike or walk; only use device to connect to the website from public wifi; never connect to own wifi; don’t use same VPN service as usual. Not sure if I missed anything. Probably did. reply numpad0 3 hours agorootparentOr walk into an internet cafe. Cafe membership systems, if any, probably aren't yet connected enough to prevent showing you the raw Internet for first few minutes, for few more years. Everyone who's vocal online should try this once in a while. Even Google search results noticeably change depending on your social classes inferred from location and whatnot. reply ranger_danger 5 hours agorootparentprevthere are MANY different ways to fingerprint something or someone, see e.g. https://abrahamjuliot.github.io/creepjs/ or https://scrapeops.io/web-scraping-playbook/how-to-bypass-clo.... also fun fact, even Tor Browser can't hide the real OS you're running when a site uses javascript-based OS queries. reply AtlasBarfed 2 hours agoparentprevThey probably have someone specifically assigned to crack every device you use. reply hn8305823 13 hours agoprevIn case anyone is wondering about the context for this 2016 article, it was right after the 2015 San Bernardino attack and the FBI was trying to get into one of the attacker's phones. Apple resisted the request primarily because they wanted a certificate that would allow them to install any rogue firmware/app/OS on any iPhone, not just the attacker's. https://en.wikipedia.org/wiki/2015_San_Bernardino_attack reply KennyBlanken 1 hour agoparentThe FBI has used damn near every major incident to push for nerfing encryption, and inbetween they bray about child porn. reply loughnane 13 hours agoprevThis topic comes up a bunch still. Someone please correct me, but as I understand it anyone using new chips that use Intel ME (or AMD's equivalent) have a gaping hole in their security that no OS can patch. I know puri.sm[0] takes some steps to try to plug the hole, but haven't read up to see if it's effective or no. [0] https://puri.sm/learn/intel-me/ reply bri3d 12 hours agoparent> anyone using new chips that use Intel ME (or AMD's equivalent) have a gaping hole in their security that no OS can patch Not really; anyone using chips with Intel ME or AMD PSP have an additional large binary blob running on their system which may or may not contain bugs or backdoors (of course, also realizing a sufficiently bad bug is indistinguishable from a backdoor). There are tens to hundreds of such blobs running on almost any modern system and these are just one example. I would argue that ME and PSP are not the worst blob on many systems; they have both unsupported but almost certainly effective (MEcleaner / ME code removal), supported and almost certainly effective (HAP bit), or supported and likely effective (ME / PSP disable command) mechanisms to disable their functionality, and they are comparatively well-documented versus the firmware that runs on every other peripheral (networking, GPU, etc.) and comparatively hardened versus EFI. reply loughnane 11 hours agorootparentYeah, this lives in the back of my mind too. I run debian on 11th gen intel, but with the non-free blobs included to make life easier. I've been meaning to try it without them, but it's too tempting to just get things 'up' instead of hacking on it. reply matheusmoreira 2 hours agorootparentThere's little we can do about it short of running ancient libreboot computers. We'll never be truly free until we have the technology to manufacture free computer chips at home, just like we can make free software at home. reply voldacar 1 hour agorootparentThere's the talos II, if you can afford it. reply mistrial9 10 hours agorootparentprevDebian has been hacked by Intel's blobs from my point of view reply wtallis 12 hours agoparentprevMost consumer products (as opposed to some of those marketed to businesses) don't have enough of the components in place for the ME to accomplish anything, good or bad. reply loughnane 11 hours agorootparentWhat do you mean? What sort of components? reply wtallis 10 hours agorootparentFor starters, few consumer systems have the ME wired up to a supported Intel NIC to provide the remote access functionality that is usually seen as the scariest feature among those related to the ME. The processors are usually not vPro-enabled models so the firwmare will refuse to enable those features due to Intel's product segmentation strategy. And even if all the right hardware is in place, I think a system still needs to be provisioned by someone with physical access to turn on those features. For most consumers, the main valid complaint about the ME is that it's a huge pile of unnecessary complexity operating at low levels of their system with minimal documentation. Anything fitting that description is a bit of a security risk, but the ME is merely one of many of those closed firmware blobs. reply dylan604 9 hours agoparentprevAre these blob type of attacks accessible after boot? Essentially, are these only accessible if you have physical access? And at that point, isn't it game over anyways? reply bri3d 9 hours agorootparentIntel ME allows intentional remote access through the ME in some enterprise scenarios (vPro). The driver support matrix is quite small and this is a massively overblown concern IMO, but it’s the root of a lot of the hand wringing. However, onboard firmware based attacks are absolutely accessible remotely and after boot in many scenarios. It’s certainly plausible in theory that an exploit in ME firmware could, for example, allow an attacker to escape a VM or bypass various types of memory protection. Unfortunately the actual role of the ME is rather opaque (it’s known, for example, to manage peripherals in s0ix sleep). Ditto for any other blob. Maybe a specially crafted packet can exploit a WiFi firmware. Maybe a video frame can compromise the GPU. These are also good persistence vectors - gain access remotely to the NOR flash containing EFI, and you have a huge attack surface area to install an early boot implant. (or if secure boot isn’t enabled, it’s just game over anyway). On Linux, it’s often just hanging out in /dev as a block device; otherwise, once an attacker has access to the full address space, it’s not too hard to bitbang. These are all fairly esoteric attacks compared to the more likely ways to get owned (supply chain, browser bugs, misconfiguration), but they’re definitely real things. The closed-sourceness is only a tiny part of the problem, too - a lot of the worst attacks so far are actually in open source based EFI firmware, which is riddled with bugs. Which takes me back to my original response to “isn’t everyone backdoored by ME” - sure, maybe, but if you’re looking for practical holes and back doors, ME is hardly your largest problem. reply aspenmayer 8 hours agorootparent> The closed-sourceness is only a tiny part of the problem, too - a lot of the worst attacks so far are actually in open source based EFI firmware, which is riddled with bugs. Can you elaborate and/or provide context/links? reply bri3d 7 hours agorootparenthttps://eclypsium.com/blog/understanding-detecting-pixiefail... https://binarly.io/posts/The_Far_Reaching_Consequences_of_Lo... reply aspenmayer 7 hours agorootparentMakes sense given the context. When you said bugs in open source EFI implementations, I thought you meant bugs in things like rEFI/rEFInd/rEFIt. reply sweetjuly 8 hours agoparentprevPeople always complain about ME/PSP but it misses the point: there is no alternative to trusting your SoC manufacturer. If they wanted to implement a backdoor, they could do so in a much more powerful and secretive way. reply Onavo 10 hours agoparentprevDoes Apple have a warranty canary? How do we know that the M series of chips haven't been compromised? reply adamomada 9 hours agorootparentmarcan of the Asahi Linux project got into a discussion on reddit about this, and says that when it comes to hardware, you just can’t know. > I can't prove the absence of a silicon backdoor on any machine, but I can say that given everything we know about AS systems (and we know quite a bit), there is no known place a significant backdoor could hide that could completely compromise my system. And there are several such places on pretty much every x86 system (Long) thread starts here, show hidden comments for the full discussion https://old.reddit.com/r/AsahiLinux/comments/13voeey/what_is... I highly recommend reading this if you’re interested https://github.com/AsahiLinux/docs/wiki/Introduction-to-Appl... reply charcircuit 11 hours agoparentprev>but as I understand it anyone using new chips that use Intel ME (or AMD's equivalent) have a gaping hole in their security that no OS can patch. The existence of security coprocessors is not a security hole and firmware updates to these processors can be released if a security issue was found. reply nimbius 11 hours agoprevEveryone forgets the speck and simon crypto the NSA wanted in the Linux kernel that were, ultimately, removed from it entirely after a lot of well deserved criticism from heavy hitters like Schneier. https://en.m.wikipedia.org/wiki/Speck_(cipher) reply xyst 12 hours agoprevfor a long time, the US considered cryptography algos as a munition. Needed some arms license to export. Also, US tried to convince the world only 56 bits of encryption was sufficient. As SSL (I don’t think TLS was a thing back then) was becoming more mainstream, US govt only permitted banks and other entities to use DES [1] to “secure” their communications. Using anything more than 56 bits was considered illegal. https://en.m.wikipedia.org/wiki/Data_Encryption_Standard reply londons_explore 11 hours agoparentEven now, if you join a discussion on crypto and say something like \"Why don't we double the key length\" or \"Why not stack two encryption algorithms on top of one another because then if either is broken the data is still secure\", you'll immediately get a bunch of negative replies from anonymous accounts saying it's unnecessary and that current crypto is plenty secure. reply Geisterde 8 hours agorootparentWell, I think that would sevearly inhibit future development. Scaling on bitcoin has been a delicate game of optimizing every bit that gets recorded, but also support future developments that dont even exist yet, there is no undo button either. New signature schemas and clevar cryptography tricks can do quite a bit, but when you slap another layer of cryptography on you will inevitably make things worse in the long run. Histories biggest bug bounty is sitting on the bitcoin blockchain, if it were even theoretically plausible to crack sha-256 like that then we would probably know, and many have tried. reply monero-xmr 4 hours agorootparentThe real security of Bitcoin is the choice of secp256k1. Basically unused before Bitcoin, but chosen specifically because he was more confident it wasn’t backdoored. https://bitcoin.stackexchange.com/a/83623 reply crotchfire 2 hours agorootparentAnd ed25519 was out of the question, since -- being brand new -- its use would have given away the fact that DJB was among the group of people who presented themselves as Satoshi Nakamoto. reply voldacar 1 hour agorootparentEvidence? reply AnotherGoodName 7 hours agorootparentprevThe best is the claim that multiple encryption makes it weaker or that encryption is the weaker of the two. If that were true we'd break encryption just by encrypting once more with a weaker algo. reply piperswe 5 hours agorootparentThe invalidity of that claim is a bit more nuanced. Having an inner, less secure algorithm may expose timing attacks and the like. There are feasible scenarios where layered encryption (with an inner weak algo and outer strong algo) can be less secure than just the outer strong algorithm on its own. reply KennyBlanken 2 hours agorootparentprevI'll do you one better. The head of security for Golang, a google employee, was also part of the TLS 1.3 committee and in Golang, it's impossible by design to disable specific ciphers in TLS 1.3 The prick actually had the nerve to assert that TLS 1.3's security is so good this should never be necessary, and that even if it were, they'll just patch it and everyone can upgrade. So someone releases a 0-day exploit for a specific TLS cipher. Now you have to wait until a patch is released and upgrade your production environment to fix it - all the while your pants are down. That's assuming you're running a current version in production and you don't have to test for bugs or performance issues upgrading to a current release. Heaven fucking forbid you hear a cipher is exploitable and be able to push a config change within minutes of your team hearing about it. I'd place 50/50 odds on it being a bribe by the NSA vs sheer ego. reply ahazred8ta 9 hours agorootparentprevEVERYTHING IS FINE. WOULD YOU LIKE A BRAIN SLUG? reply paulpauper 8 hours agorootparentprevtwo encryption algorithms will mean needing two completely unrelated , unique passwords. this can be impractical and increase odds of being locked out forever reply ranger_danger 5 hours agorootparentno it doesn't mean that at all reply meroes 10 hours agoparentprevDo you have more on the legality aspect? I knew NSA pressured for a weaker key but what aspect could be made illegal? I had to write an undergrad paper on the original DES and I never saw an outright illegality aspect but wouldn’t be surprised. They also put in their own substitution boxes which I surprisingly never found much info on how exactly NSA could use them. So much speculation but why no detailed post mortems in the modern age? reply MattPalmer1086 8 hours agorootparentIt seems that they changed the S boxes to make them more resistant to differential analysis (which they knew about but the public didn't). So this is actually a case of them secretly strengthening the crypto. Presumably this is because they didn't want adversaries being able to decrypt stuff due to a fundamental flaw. I guess it's possible they also weakened it in another way, but if so nobody has managed to find it. reply ahazred8ta 8 hours agorootparentprevIn the US, since the 1950s, you need a permit to export any product which has encryption. There are fines if you don't file the right paperwork. In the 1970s and 80s they would only approve keys of 40 bits or less. https://en.wikipedia.org/wiki/Export_of_cryptography_from_th... reply mannyv 11 hours agoprevThere's talk that the NSA put its own magic numbers into elliptical curve seeds. Does that count? https://www.bleepingcomputer.com/news/security/bounty-offere... reply whatever3 11 hours agoprevI posted this because of the Enigma/Crypto AG mixup in the article, but it doesn't seem that anyone noticed. Seemed relevant considering the post about fabricated Atlas Obscura stories a few days ago. reply schmudde 12 hours agoprevSharing this seems like an appropriate way of commemorating David Kahn's passing (https://news.ycombinator.com/item?id=39233855). Ultimately it will take strict legislation and compliance measurement along with penalties to protect the government from overstepping the bounds they promise not to step over already, let alone new ones. They will find ways to not comply, often blatantly. They have no scruples. reply paulpauper 8 hours agoparentprevA backdoor, which works anywhere and way better than the wrench option. reply jarjar2_ 8 hours agorootparentThey don't need it, which was my point. They have all the tools the need right now to get what they want. Why should anyone grant them more? reply lazide 6 hours agorootparentWhy would they not try to get a magic back door that makes their lives easier, even if they don’t need it? reply mouzogu 2 hours agoprevhow likely is it that whatsapp or telegram are backdoored? i wonder what tools do guerilla armies or drug lords use to communicate.. or maybe its better to hide in plain sight. just use some kind of double speak that gives plausible deniability. reply KennyBlanken 2 hours agoparentTelegram is a poster-child for sketchy security. reply jonathanyc 8 hours agoprevNow the argument coming from civil society for backdoors is based on CSAM: > Heat Initiative is led by Sarah Gardner, former vice president of external affairs for the nonprofit Thorn, which works to use new technologies to combat child exploitation online and sex trafficking. In 2021, Thorn lauded Apple's plan to develop an iCloud CSAM scanning feature. Gardner said in an email to CEO Tim Cook on Wednesday, August 30, which Apple also shared with WIRED, that Heat Initiative found Apple's decision to kill the feature “disappointing.” > “Apple is one of the most successful companies in the world with an army of world-class engineers,” Gardner wrote in a statement to WIRED. “It is their responsibility to design a safe, privacy-forward environment that allows for the detection of known child sexual abuse images and videos. For as long as people can still share and store a known image of a child being raped in iCloud we will demand that they do better.” https://www.wired.com/story/apple-csam-scanning-heat-initiat... reply int_19h 7 hours agoparentThis isn't even a recent thing anymore. \"iPhone will become the phone of choice for the pedophile\" was said by a senior official in 2014, when full device encryption was starting to become common. reply rpmisms 8 hours agoparentprevCSAM is evil, and I personally believe we should execute those who distribute it. I have an even stronger belief in the right to privacy, and those in the government who want to break it should be executed from their positions (fired and publicly shamed). reply matheusmoreira 2 hours agoparentprevThe perfect political weapon. Anyone who opposes is automatically labeled a pedophile and child abuser. Their reputations are destroyed and they will never oppose again. reply quasarj 9 hours agoprevI like that typo in the image label - the Chipper Clip lol reply bemusedthrow75 7 hours agoparentWe've had enough of chipper clips to last a lifetime! It looks like you're writing an article about encryption. Would you like help? (o) Insert a joke about Apple forcing a U2 album on us (o) Let me write the joke myself [x] Don't show me this tip again reply hunglee2 9 hours agoprevthe biggest threat to a citizens privacy is always your own government. reply BLKNSLVR 8 hours agoparentYep. I use Chinese brand phones because if they're snooping all my shit, they're much further away from me than my own government and not likely to have sharing arrangements. reply aspenmayer 8 hours agorootparentWouldn’t Chinese branded phones be a higher priority target by foreign agencies in the first place? reply BLKNSLVR 5 hours agorootparentIt's likely an additional data point in some kind of 'suspicious' rating. I think I hit quite a few of those 'suspicious' check-boxes that law enforcement would consider important, whilst actually technically knowledgeable people wouldn't even blink at them. Refer: https://news.ycombinator.com/item?id=39050898 reply AtlasBarfed 2 hours agoparentprevI kinda disagree, because the government, even now, can be shamed and outraged. Corporations however? They are, by design, utterly amoral. So the modern state is that corporations are hoovering all your data they can for \"ad research and optimization\". I think I read recently that facebook has thousands of companies involved in the customer data supply chain? And if those companies have your data, it's not that YOUR government has it guaranteed. It's that ALL governments have your data. reply Joel_Mckay 8 hours agoprevEncryption is meaningless with cpu-level side-channel memory key dumps active on most modern platforms. The reality is if you have been targeted for financial or technological reasons, than any government will eventually get what they are after. One can't take it personally, as all despotic movements also started with sycophantic idealism. Have a great day, =) https://xkcd.com/538/ reply keepamovin 6 hours agoparentAgree with this. Makes me think that the code-breakers themselves must be using specialized hardware to protect their own side-channels. But for this to be feasible you need to have big chipmakers in on it. Fascinating to consider reply Joel_Mckay 5 hours agorootparentNo need, data collection is a different function than exploitation. People that are turned into assets are often not even aware how they are being used. I once insisted I could be bribed to avoid the escalation of coercion as a joke, that was funny until someone actually offered $80k for my company workstation one day. It is a cultural phenomena, as in some places it is considered standard acceptable practice. My advice is to be as boring as possible, legally proactive, and keep corporate financial profit modes quiet. Good luck =) reply coppsilgold 13 hours agoprev [–] FBI director James Comey have publicly lobbied for the insertion of cryptographic “backdoors” into software and hardware to allow law enforcement agencies to bypass authentication and access a suspect’s data surreptitiously. Cybersecurity experts have unanimously condemned the idea, pointing out that such backdoors would fundamentally undermine encryption and could exploited by criminals, among other issues. \"could exploited by criminals\" is sadly a disingenuous claim. A cryptographic backdoor is presumably a \"Sealed Box\"[1] type construct (KEM + symmetric-cipher-encrypted package). As long as the government can keep a private key secure only they could make use of it. There are plenty of reasons not to tolerate such a backdoor, but using false claims only provides potential ammunition to the opposition. [1]reply bayindirh 11 hours agoparentLet’s see: Mercedes recently forgot a token in a public repository which grants access to everything. Microsoft forgot its “Golden Key” in the open, allowing all kinds of activation and secure boot shenanigans. Microsoft’s JWT private key is also stolen, making the login page a decoration. Somebody stole Realtek’s driver signing keys for Stuxnet attack. HDMI master key is broken. BluRay master key is broken. DVD CSS master key is broken. TSA master keys are in all 3D printing repositories now. Staying on the physical realm, somebody made an automated tool to profile, interpret and print key blanks for locks with \"restricted keyways\" which has no blanks available. These are the ones I remember just top of my head. So yes, any digital or physical secret key is secure until it isn’t. It’s not a question of if, but when. So, no escrows or back doors. Thanks. reply wkat4242 8 hours agorootparentI've been waiting for those wildvine keys to leak which would finally let me choose what to play my stuff on. But it still hasn't happened. They are getting better at secrecy sadly. reply piperswe 5 hours agorootparentprevIt's apparently now trivial to brute force the private key used for Windows XP-era Microsoft Product Activation, as another example. (that's where UMSKT and the like get their private keys from) reply whatshisface 13 hours agoparentprev>As long as the government can keep a private key secure only they could make use of it. Your devices would be secure as long as a private key that happened to be the most valuable intelligence asset in the United States, accessed thousands of times per day, by police spread across the entire nation, was never copied or stolen. reply dylan604 9 hours agorootparentWell, it's a good thing that we don't have to worry about corrupt police /s reply buffet_overflow 12 hours agoparentprev> As long as the government can keep a private key secure only they could make use of it. Well, keep in mind they would have to keep it secure in perpetuity. Any leak over the lifetime of any of that hardware would be devastating to the owners. Blue Team/Defensive security is often described as needing to be lucky every time, where as Red Team/attackers just have to get lucky once. This attack vector is in addition to just exploiting the implementation in some way, which I don't think can be handwaved away. reply mnw21cam 11 hours agoparentprev> As long as the government can keep a private key secure... Which government? Software crosses borders. You can bet that if the US mandated a back door to be inserted into software that was being exported to another country, that country would want to either have the master key for that back door, or a different version of the software with a different back door or without the back door. A software user could choose the version of the software that they wanted to use according to which country (if any) could snoop on them. It's unworkable. reply Rebelgecko 12 hours agoparentprev>As long as the government can keep a private key secure only they could make use of it. That's a big \"if\". Look at how the government has protected physical keys... Ever since the TSA accidentally leaked them, you can buy a set of keys on Amazon for $5 that opens 99% of \"TSA approved\" locks reply eviks 53 minutes agoparentprev> As long as the government can keep a private key secure only they could make use of it. That's a disingenuous claim since it's known they can't reply Hikikomori 10 hours agoparentprevAre they lobbying for this because they can't access stuff today and \"need\" it or is just a psyop so we believe what that they cannot access it today. reply catlifeonmars 13 hours agoparentprev> As long as the government can keep a private key secure only they could make use of it. Not disingenuous. Keys are stolen or leaked all the time. And the blast radius of such a master key would be extremely large. reply devwastaken 13 hours agoparentprevIt's not a false claim, assuming the feds will keep such a key \"secure\" is not backed by evidence. Top secret materials are leaked all the time. Private keys from well secured systems are extracted from hacks. The FBI having such a key would make them a very profitable target for the various corps that specialize in hacking for hire. For example, NSO group. If the power doesn't exist, nobody can exploit it. reply coppsilgold 13 hours agorootparentDo military cryptographic keys leak often? Do nuclear codes leak? The times highly valuable cryptographic keys leaked for various cryptocurrency exchanges it has generally if not always been due to gross negligence. Such a key would be highly sensitive and it would also require very little traffic to use. You would just need to send the secure system a KEM ( Do nuclear codes leak? For many years, the code was 00000000. https://arstechnica.com/tech-policy/2013/12/launch-code-for-... reply jliptzin 12 hours agorootparentprevWhat are you going to do with a nuclear code without access or authority to launch the nukes? reply devwastaken 11 hours agorootparentprevYou're creating so many assumptions that nothing you've stated could be concluded to be an honest reflection of reality. Nobody has to know the rate of leaks, it's irrelevant. Gross negligence is not necessary, how would you even know? Leaks by definition are rarely exposed, we only see some of them. A \"highly sensitive\" key doesn't mean anything. Assigning more words to it doesn't somehow change the nature of it. Humans are bad at securing things, that's why the best security is to not have a system that requires it. Whatever hypothetical solution you have would be crushed under the weight of government committees and office politics until your security measures are bogus. reply 2OEH8eoCRo0 13 hours agoparentprevAnd Apple has a backdoor that only Apple can use. Why don't criminals exploit Apple's backdoor? reply frickinLasers 13 hours agorootparenthttps://arstechnica.com/security/2023/12/exploit-used-in-mas... Looks like criminals were using it for four years undetected. reply catlifeonmars 13 hours agorootparentprevFWIW this is a fair and valid argument. Generally, no one entity should have that much power. Doesn’t really matter if it’s USG or a tech giant. reply quickslowdown 13 hours agorootparentprevWhich backdoor do you mean? I'm not an Apple expert by any means, but I thought they encrypted customer data in a way that even they can't get to it? Wasn't that the crux of this case, that Apple couldn't help the FBI due to security measures, prompting the agency to ask for a backdoor? reply 2OEH8eoCRo0 13 hours agorootparentWhat's an update? They can sign and push any code they want remotely. reply dataangel 13 hours agorootparentIIRC the question is when the phone is totally locked, e.g. if you turn it off then turn it back on and haven't entered the PIN yet. In this state even apple can't get an update to run, the secure hardware won't do it unless you wipe the phone first. And your data is encrypted until you unlock the phone. In practice though most people are screwed b/c it's all already in icloud. reply fragmede 11 hours agorootparentwith advanced data protection, it's encrypted before it hits iCloud, so apple, nor the feds can't get at it. reply catlifeonmars 13 hours agorootparentprevSource/reference? I’m not aware of such a backdoor reply adrian_b 12 hours agorootparentSee the posting above about the Arstechnica article. During the last days of 2023 there was a big discussion, also on HN, after it was revealed that all recent Apple devices had a hardware backdoor that allowed bypassing all memory access protections claimed to exist by Apple. It is likely that the backdoor consisted in some cache memory test registers used during production, but it is absolutely incomprehensible how it has been possible for many years that those test registers were not disabled at the end of the manufacturing process but they remained accessible for the attackers who knew Apple's secrets. For instance any iPhone could be completely controlled remotely after sending to it an invisible iMessage message. reply rightbyte 9 hours agorootparentCan't Apple just push an software update with some: if (user_id == \"adrian_b\") pwn (); ? reply sylware 9 hours agorootparentprev\"Convenient software/hardware bugs\"... but \"they are not backdoors, I swear!\" reply sowbug 12 hours agoparentprevYou assume a perfect implementation of the backdoor. Even if the cryptographic part were well-implemented, someone will accidentally ship a release build with a poorly safeguarded test key, or with a disabled safety that they normally use to test it. It's an unnecessary moving part that can break, except that this particular part breaking defeats the whole purpose of the system. reply salawat 10 hours agoparentprevThe same government that failed to keep all of it's Top Secret clearance paperwork secure? How soon we forget the OPM hack... reply nonrandomstring 13 hours agoparentprev [–] > false claims As Pauli said, \"That's not even wrong\". It cannot even meet the basic criteria for truth or falsehood. It's simply naked hubris. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article provides a historical overview of the U.S. government's attempts to introduce backdoors into encrypted data.",
      "It highlights the ongoing debate between government officials, cybersecurity experts, and the tech industry regarding the balance between consumer data protection and government access to information.",
      "The article mentions examples of previous attempts to insert backdoors, including the Enigma machine and the Dual_EC_DRBG algorithm.",
      "Some governments, like Holland and France, have refused to implement backdoors, while experts advocate for truly secure encryption.",
      "The article concludes by suggesting that despite the outcome of the Apple vs. FBI case, the government will likely continue its efforts to undermine cryptography."
    ],
    "commentSummary": [
      "The discussion thread covers various topics including encryption, government backdoors, espionage, hardware vulnerabilities, and privacy concerns.",
      "It explores the history of attempts to add backdoors to encrypted data and the use of ITAR regulations to restrict the export of encryption software.",
      "The thread also discusses revelations about the CIA's ownership of Crypto AG, security vulnerabilities in Intel ME and AMD chips, debates on multiple encryption layers' impact on Bitcoin's security, and the risks of cryptographic backdoors and government access to encrypted data."
    ],
    "points": 427,
    "commentCount": 149,
    "retryCount": 0,
    "time": 1706991076
  },
  {
    "id": 39239265,
    "title": "Apple Introduces Pkl - A Programmable Configuration Language",
    "originLink": "https://pkl-lang.org/index.html",
    "originBody": "Pkl Language Bindings Java Kotlin Swift Go Frameworks Spring (Boot) Editors IntelliJ Neovim VS Code Resources GitHub Package Docs Style Guide Security Community GitHub Discussions Blog Configuration that is Programmable, Scalable, and Safe bird.pkl name = \"Swallow\" job { title = \"Sr. Nest Maker\" company = \"Nests R Us\" yearsOfExperience = 2 } ↓ bird.jsonbird.yamlbird.plistbird.properties { \"name\": \"Swallow\", \"job\": { \"title\": \"Sr. Nest Maker\", \"company\": \"Nests R Us\", \"yearsOfExperience\": 2 } } name: Swallow job: title: Sr. Nest Maker company: Nests R Us yearsOfExperience: 2 name Swallow jobtitle Sr. Nest Maker company Nests R Us yearsOfExperience 2name = Swallow job.title = Sr. Nest Maker job.company = Nests R Us job.yearsOfExperience = 2 Generate any static configuration format Define all your data in Pkl, and generate output for JSON, YAML, Property Lists, and other configuration formats. Integrated application configuration Embed Pkl into your applications for runtime configuration, and receive code generation for Java, Kotlin, Swift, and Go. Pkl Java Kotlin Swift Go module example.MyAppConfig /// The hostname for the application host: String /// The port to listen on port: UInt16(this > 1000) package example; public final class MyAppConfig { /** * The hostname for the application */ public final @NonNull String host; /** * The port to listen on */ public final int port; public MyAppConfig( @Named(\"host\") @NonNull String host, @Named(\"port\") int port) { this.host = host; this.port = port; } public MyAppConfig withHost(@NonNull String host) { /*...*/ } public MyAppConfig withPort(int port) { /*...*/ } @Override public boolean equals(Object obj) { /*...*/ } @Override public int hashCode() { /*...*/ } @Override public String toString() { /*...*/ } } package example data class MyAppConfig( /** * The hostname for the application */ val host: String, /** * The port to listen on */ val port: Int ) // Code generated from Pkl module `example.myAppConfig`. DO NOT EDIT. enum MyAppConfig {} extension MyAppConfig { struct Module { /// The hostname for the application let host: String /// The port to listen on let port: UInt16 } } // Code generated from Pkl module `example.myAppConfig`. DO NOT EDIT. package myappconfig import (\"context\"\"github.com/apple/pkl-go/pkl\" ) type MyAppConfig struct {// The hostname for the applicationHost string `pkl:\"host\"`// The port to listen onPort uint16 `pkl:\"port\"` } Incredible IDE Integration Great tooling for writing Pkl with the same ease as a static typed language. We offer plugins and extensions for IntelliJ, Visual Studio Code and Neovim, with Language Server Protocol support coming soon. Catch errors before deployment With a rich type and validation system, catch configuration errors before deploying your application. email: String = \"dev-team@company.com\" port: Int(this > 1000) = 80 ↓ –– Pkl Error –– Type constraint `this > 1000` violated. Value: 80 3port: Int(this > 1000) = 80 ^^^^^^^^^^^ at config#port (config.pkl, line 3) 3port: Int(this > 1000) = 80 ^^ at config#port (config.pkl, line 3) 106text = renderer.renderDocument(value) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Copyright © 2024 Apple Inc. All rights reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=39239265",
    "commentBody": "Apple releases Pkl – configuration as code language (pkl-lang.org)422 points by isodev 22 hours agohidepastfavorite5 comments neongreen 22 hours agoDuplicate of https://news.ycombinator.com/item?id=39232976 reply dang 17 hours agoparentComments moved thither. Thanks! reply isodev 22 hours agoparentprevOh yes indeed, I didn't see it. reply talkingtab 15 hours agoprev [–] I feel like the guy in \"Big\": I just don't get it. Using Java and learning a new syntax in order to automatically generate JSON & YAML? The complexity for what seems like a fairly simple task unless you have either massive files or a massive number of files? What's the fun in that? I just don't get it. reply jjirsa 15 hours agoparent [–] Some companies have both massive files and a massive number of files reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Pkl is a programming language that enables developers to define static configuration formats and generate output for different formats like JSON, YAML, and Property Lists.",
      "Developers can use Pkl for integrated application configuration at runtime and code generation in languages like Java, Kotlin, Swift, and Go.",
      "Pkl offers IDE integration, code-writing tools, and a type and validation system to prevent configuration errors before deployment, making it a secure and scalable configuration solution for developers."
    ],
    "commentSummary": [
      "Apple has introduced Pkl, a configuration as code language aimed at generating JSON and YAML.",
      "The release has generated doubts and skepticism among users who questioned the necessity of a new language, considering that Java can accomplish similar tasks.",
      "However, it has been highlighted that Pkl may be beneficial for organizations that handle extensive and numerous files."
    ],
    "points": 422,
    "commentCount": 5,
    "retryCount": 0,
    "time": 1706958727
  },
  {
    "id": 39243303,
    "title": "Exploring Rocket Limitations on High-Gravity Planets",
    "originLink": "https://space.stackexchange.com/questions/14383/how-much-bigger-could-earth-be-before-rockets-wouldnt-work",
    "originBody": "Stack Exchange Network Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers. Visit Stack Exchange Loading… Tour Start here for a quick overview of the site Help Center Detailed answers to any questions you might have Meta Discuss the workings and policies of this site About Us Learn more about Stack Overflow the company, and our products current community Space Exploration help chat Space Exploration Meta your communities Sign up or log in to customize your list. more stack exchange communities company blog Log in Sign up Space Exploration Stack Exchange is a question and answer site for spacecraft operators, scientists, engineers, and enthusiasts. It only takes a minute to sign up. Sign up to join this community Anybody can ask a question Anybody can answer The best answers are voted up and rise to the top Space Exploration Home Questions Tags Users Unanswered Teams Stack Overflow for Teams – Start collaborating and sharing organizational knowledge. Create a free Team Why Teams? Teams Create free Team Teams Q&A for work Connect and share knowledge within a single location that is structured and easy to search. Learn more about Teams How much bigger could Earth be, before rockets wouldn't work? Ask Question Asked 7 years, 11 months ago Modified 2 years, 10 months ago Viewed 50k times This question shows research effort; it is useful and clear 107 This question does not show any research effort; it is unclear or not useful Save this question. Show activity on this post. $\\begingroup$ hint: Apparently the Tsiolkovsky rocket equation does not actually say that you can launch a conventional rocket into orbit around an arbitrarily large and massive body. I'm looking for a number based on scaling the earth radius and maintaining the same average density. Must attain LEO, which also gets faster as the planet grows. Don Pettit's Tyranny mentioned in this nice answer is fun, but does not present enough math. On this Earth, rockets barely work. Payloads can only be a few percent of the total mass for LEO, and less than one percent for deep space. If we define slightly heavier Earths, say Earth1.1, Earth1.2... where the radii were 1.1, 1.2, etc. times that of Earth and the masses were 1.13, 1.23, etc. times the Earth's mass (in other words same average density, same \"iron/rock ratio\") what happens? Is there some point where chemical rockets simply will no longer be able to put things in space, or does the payload mass simply become ridiculously tiny? If there is a cut-off, is it different for LEO and deep space? For our purposes, let's not explore alternative or hybrid launch systems or boost systems (such as balloons, planes, laser beams, space elevators etc.). Just stick to chemical propellant rockets. edit: here is a guide. So for a scaling factor $f$: $$ r = f r_{earth} $$ $$ m = f^3 m_{earth} $$ $$ g = G \\frac{m}{r^2} = \\frac{f^3}{f^2}g_{earth} = f g_{earth} $$ $$ H = \\frac{kT}{gm_{molecule}} = f^{-1}H_{earth} $$ We catch a little break here. Assuming same surface atmosphere composition, temperature and pressure (STP), the scale height H actually decreases with increasing $f$. (If we were \"world builders\" we should probably increase pressure to get more oxygen needed for moving in the higher gravity, but that's a different Stack Exchange.) As far as LEO altitude is concerned (thanks @Lex for catching that) one might define it as the same number of scale heights as would be on Earth. That's not really so useful because the density profiles of the bits of the atmosphere responsible for drag (Thermosphere and Exosphere are affected by many phenomenon, including the solar wind, and don't scale at all like the lower layers. Nonetheless for historical reasons I'll leave the following, as it is not essential to the question: $$ h_{LEO} = h_{LEOearth} \\frac{H}{H_{earth}} = f^{-1} h_{LEOearth} $$ $$ v_{LEO}=fv_{LEOearth} $$ The LEO period is independent of the size of a planet, if the average density is fixed. However, the velocity of LEO does scale with radius! launch rockets payload escape-velocity performance Share Improve this question Follow Follow this question to receive notifications edited May 8, 2020 at 10:10 uhoh asked Mar 9, 2016 at 7:45 uhohuhoh 149k5252 gold badges471471 silver badges14631463 bronze badges $\\endgroup$ 31 6 $\\begingroup$ Related: On a Super-Earth 1.5x the volume and mass of Earth, would our rocket technology allow us to reach orbit?. Look at Russell Borogove's answer at the bottom. $\\endgroup$ – kim holder Mar 9, 2016 at 15:53 4 $\\begingroup$ True, but the main thing was that, in theory, any gravity can be overcome if you put enough stages in your humongous rocket. So then the question turns into a matter of what is practical. I don't know if there is a way to define the question to put a useful limit on that. The limits of structural materials? The GDP of the planet? Not being facetious, btw. $\\endgroup$ – kim holder Mar 9, 2016 at 16:06 2 $\\begingroup$ Let us continue this discussion in chat. $\\endgroup$ – kim holder Mar 9, 2016 at 16:33 2 $\\begingroup$ I still think the issue is what point we will assign as being the boundary of 'ridiculous'. Say, if there was a beautiful garden world that was a moon of this planet, maybe they would expend extraordinary effort to get just one rocket there. $\\endgroup$ – kim holder Jul 23, 2016 at 15:05 2 $\\begingroup$ Yeah, but that is the thing - as i understand it, there is no such number, theoretically. Any number of stages can be placed on a rocket, and each one increases the gravity that can be overcome - by ever smaller amounts, but that amount is never zero. I suppose there is a point where the amount is so small it is less than the variance introduced by wind and temperature and such... maybe that could be a limit... $\\endgroup$ – kim holder Jul 23, 2016 at 15:27Show 26 more comments 6 Answers Sorted by: Reset to default Highest score (default) Date modified (newest first) Date created (oldest first) This answer is useful 145 This answer is not useful Save this answer. +100 This answer has been awarded bounties worth 100 reputation by kim holder Show activity on this post. $\\begingroup$ Because linear increases in delta-v require exponential increases in mass, small changes to the assumptions you make about fuel tank structural mass and engine thrust-to-weight ratio start to make very large changes in the final size of the rocket. For example, if you're getting off a 3.6g planet with a 7-stage rocket, the difference between 88% fuel fraction and 92% fuel fraction yields about a 10:1 difference in the total mass of the rocket. So I don't think it's really reasonable to talk about ultimate theoretical limits; too many engineering factors are involved. Locking down a lot of variables, I can tell you what kind of rocket you'd need for a given surface g, though. Let's make these assumptions: We are placing 1 ton of payload into low planetary orbit. Required delta-v to reach orbit, including atmospheric and gravity losses, is 10,000m/s per surface g. Seems to hold for Earth, Mars, and the \"Earthtoo\" which was discussed in another Q/A. We can build rocket stages of arbitrary size, with a tankage propellant fraction of 90%; the rocket stage mass is the tank mass plus the engine mass -- ullage rockets, interstage, etc. is all handwaved out. We have an infinite supply of Apollo-era rocket engines: RL-10, J-2, M-1, H-1, and F-1. First-stage TWR at ignition must be at least 1.2 (relative to local gravity) Middle-stage TWR at ignition must be at least 0.8 Final-stage TWR at ignition must be at least 0.5 Given those assumptions, here is a table of surface gravity, stage count, first-stage engines, and total rocket mass. Surface First Total Saturn V Gravity Stages Stage Mass, t Equivalent 0.5 2 1x RL-10 4.5 1.0 3 1x H-1 49.4 0.02 1.5 3 1x F-1 249.2 0.1 2.0 4 5x F-1 1329.0 0.5 2.5 5 40x F-1 8500.9 3 3.0 6 274x F-1 50722.2 17 3.5 7 2069x F-1 331430.9 100 4.0 8 20422x F-1 2836598.4 950 4.5 8 392098x F-1 47 million 15000 5.0 9 3.5 million F-1 391 million 130000 6.0 11 400 million F-1 38 billion millions 10.0 18 2.88e19 F-1 1.65e21 quadrillions Up above 10g, something really interesting happens that is kind of a theoretical limit. The mass of the rocket reaches a measurable fraction of the mass of the entire planet it's launching from. At 10.3g, rocket mass is 0.035 of the mass of the planet. 10.4g, rocket mass is one fifth of the mass of the planet. This doesn't actually alter the ∆v requirement -- we're going into orbit around the rocket/planet barycenter! At 10.47g, the rocket is the planet, and we're... just... chewing it up entirely, pulverizing it in a dust cloud expanding at 4km/s. These extreme conclusions appear to be corroborated by this independently derived paper, which explores some other related aspects of super-Earth-based chemical rockets. Another consideration recently brought up by user @uhoh is that as the linear scale of a given rocket stage increases, its mass, and thus the required thrust force to lift it, goes up by the cube of the scale, but the area available at the base of the rocket to mount engines goes up only by the square of the scale; this problem is made even worse here by the increasing surface gravity. The Saturn V was just about at the point where this relation starts to become problematic; the outboard engines on its first stage are mounted at the very edge of the stage in order to make room for their nozzles to gimbal. Solid rockets don't have the same dimensional constraints, and have very good thrust-to-weight and thrust-to-cost ratios, so they're probably more likely to be used in lower stages for these very large rockets. Stages much larger than the Saturn V first stage would need to address this with some combination of being shorter and squatter, or compromising engine gimbal range, or mounting engines in pods surrounding the tankage, and there might be fairly hard engineering limits at some point for those reasons. At the 3g mark, for example, the 274 first-stage engines would require a stage about 90 meters in diameter and 9 meters tall, at which point the engineering inefficiencies associated with the fuel tank proportions will be becoming serious. Share Improve this answer Follow Follow this answer to receive notifications edited Jun 8, 2020 at 16:20 StayOnTarget 1,12111 gold badge77 silver badges1717 bronze badges answered Jul 30, 2016 at 20:53 Russell BorogoveRussell Borogove 168k1313 gold badges591591 silver badges697697 bronze badges $\\endgroup$ 17 9 $\\begingroup$ Awesome. I'm happy now. :) $\\endgroup$ – kim holder Jul 30, 2016 at 21:01 3 $\\begingroup$ @kimholder me too - thank you very much for your support and interest! I'm glad we go to the bottom of this. $\\endgroup$ – uhoh Jul 30, 2016 at 22:18 3 $\\begingroup$ Thanks you very much Russell Borogove for your systematic approach and persistence. We can call this The Tyranny of the Exponential! Very nice. $\\endgroup$ – uhoh Jul 30, 2016 at 22:22 7 $\\begingroup$ From that article: \"On worlds with a surface gravity of >10g0, a sizable fraction of the planet needs to be used up as chemical fuel per launch, limiting the total number of flights\" -- good corroboration! $\\endgroup$ – Russell Borogove Apr 5, 2018 at 17:42 11 $\\begingroup$ This is like an XKCD what-if, the everybody dies kind. Love it. $\\endgroup$ – GdD May 9, 2018 at 15:22Show 12 more comments This answer is useful 31 This answer is not useful Save this answer. Show activity on this post. $\\begingroup$ First, let us look at the rocket equation: $$\\Delta v=\\ln \\left(\\frac{m_0}{m_f}\\right)v_e$$ That tells how much a rocket can change its velocity (the $\\Delta v$). The requirements for reaching a higher velocity for a minimal orbit would increase on your heavier Earth. (For constant density it is proportional to the radius.) How can we increase the $\\Delta v$ of the rocket to keep up? We can increase the exhaust velocity, $v_e$, of the engine, but that cut-off is around 5000 m/s for chemical engines. The other thing we could do is increasing the mass ratio of the rocket $\\left(\\frac{m_0}{m_f}\\right)$. That is problematic too, as we can not really make the fuel tanks out of soap bubbles. Staging is the option left, you could place a big rocket under a small rocket to get a little more change in velocity. Then you are getting a linear benefit for an exponential expense. As an example, the Saturn V rocket got into LEO (~9000 m/s), sent a payload towards the Moon (3120 m/s), the service module slowed the stack into LMO (820 m/s), and finally the LM landed and took off again (2*1720 m/s). There are still some unused fuel left in the service module then, so let us just call the total $\\Delta v$ of the Saturn V/Apollo 17 km/s. That is less than the requirements for a 2x radius Earth. The Apollo program was pretty expensive [citation needed], so it may take a while before a nation of a 2x Earth world attempts to go into orbit. The limit is, as you state, the ridiculously low payload ratio. Another consideration is the increased surface gravity. (That scales linearly with diameter at constant density). That requires the rocket to have a higher thrust to weight ratio, and that will increase the dry mass, reducing the possible $\\Delta v$. (It also increases gravity losses, but that is mostly compensated by the lower scale height of the planet, reducing drag losses). Eventually, the gravity is so high that even the most powerful engine can not lift itself from the ground. That at least is a definitive limit. A more theoretical consideration, is $\\Delta v$ requirements actually a finite limit? Surprisingly, it is not. Remember what I said about staging earlier: \"you are getting a linear benefit for an exponential expense\". But there is not limit to what we can expend! Consider the following scenario: We add more and more stages at the bottom of the rocket, each of them has the same mass as all the stages on top of it. Then burning each of them gives the same mass ratio between before and after, therefore each of them are supplying the same amount of $\\Delta v$. To add 10 times that amount, you need 10 stages each doubling the mass. To add 100 times that amount, you need to double a hundred times. The mass grows ridiculously fast, even doubling 10 times are over a thousand times more. But why should we stop :) But can we really continue to add exponentially larger stages for ever? After a while, other problems show up. For instance: Rockets are long and thin, to minimize drag. That shape can not be kept for very large rockets. The reason not is the square cube law. Conserving the same dimensional proportions, a rocket twice the height has 8 times more mass. But the base area of the rocket has only increased 4 times. That means that each unit of area has to support more mass. Sooner or later, even the strongest materials must give up, and you must give up the traditional rocket shape in favour of a wider base. That adds a lot to the drag! Problems like that are going to continue to show up: \"More mass means more problems, exponentially more mass means exponentially more problems.\" Summarized: A modern design, larger rocket than the Saturn V, with modifications to increase the T/W ratio could probably make it to orbit on a 2x radius, 8x mass Earth. That is a feasibility limit, rockets that are ridiculously much larger may have a few km/s extra $\\Delta v$, but that does not alter the numbers a lot. In theory though, rockets can grow until the drag stops them, or the engines can no longer lift even themselves. Or perhaps you at some point want to use the available resources of the planet to launch a single rocket to orbit. Share Improve this answer Follow Follow this answer to receive notifications edited Apr 6, 2021 at 8:30 WilliamL 322 bronze badges answered Mar 9, 2016 at 8:48 SE - stop firing the good guysSE - stop firing the good guys 43.3k33 gold badges141141 silver badges244244 bronze badges $\\endgroup$ 7 $\\begingroup$ Thanks! Can you summarize as a number in the format described in the question? Are you saying that a Saturn V can give you \"LEO\" on an earth with 2x radius and 8x mass (e.g. earth_2.0)?? I don't think that's going to work! I'm looking for the limits. The discussion of things to consider is great, but let's get to a single number, or possibly two - one for LEO one for \"deep space\". $\\endgroup$ – uhoh Mar 9, 2016 at 15:05 $\\begingroup$ OK, when you say \"That is less than the requirements for a 2x radius Earth.\" can you show that using math, not words? What are the requirements for a 2x radius (8x mass) earth, numerically, and why does a Saturn V satisfy it? If there are design changes (T/W) how much? 10%? 10X? Is there a chance this can be done with a simulator program or even Kerbal? $\\endgroup$ – uhoh Mar 9, 2016 at 16:08 $\\begingroup$ @uhoh Don't be too fast on that, especially when a bounty is involved. I agree this is a good answer, but sometimes even better answers come along. Bounties exist to stimulate that kind of interest. $\\endgroup$ – kim holder Jul 25, 2016 at 16:23 1 $\\begingroup$ I was interested by @RussellBorogove's comment in The Pod Bay about the limitations on engine T/W ratios. That seems to be something that can be given a theoretical limit if only chemical engines are considered. $\\endgroup$ – kim holder Jul 25, 2016 at 16:27 1 $\\begingroup$ @kimholder I can try to write something about that too. $\\endgroup$ – SE - stop firing the good guys Jul 25, 2016 at 16:34Show 2 more comments This answer is useful 12 This answer is not useful Save this answer. Show activity on this post. $\\begingroup$ note: I've accepted an answer 2.5 years ago. This paper was published recently so I thought I would add this supplemental answer since it may be an interesting reference for future readers. The Space.com article No Way Out? Aliens on 'Super-Earth' Planets May Be Trapped by Gravity links to Michael Hippke's ArXiv preprint Spaceflight from Super-Earths is difficult. While the calculation is based on escape velocity rather than LSEO (Low Super-Earth Orbit) the conclusion is similar, the problem is exponential and it gets really difficult quickly. The author uses the example of the planet Keppler-20b (see also here), and although there is some uncertainty, the planet's size is roughly 1.9 that of earth, and it's mass is almost 10 times that of Earth. For a mass ratio of 83, the minimum rocket (1 t to $v_{esc}$) would carry 9,000 t of fuel on Kepler-20b, which is 3× larger than a Saturn V (which lifted 45 t). To lift a more useful payload of 6.2 t as required for the James Webb Space Telescope on Kepler-20 b, the fuel mass would increase to 55,000 t, about the mass of the largest ocean battleships. For a classical Apollo moon mission (45 t), the rocket would need to be considerably larger, ∼ 400,000 t. This is of order the mass of the Pyramid of Cheops, and is probably a realistic limit for chemical rockets regarding cost constraints. (emphasis added) Share Improve this answer Follow Follow this answer to receive notifications edited Feb 22, 2019 at 11:07 answered Apr 24, 2018 at 3:56 uhohuhoh 149k5252 gold badges471471 silver badges14631463 bronze badges $\\endgroup$ 4 1 $\\begingroup$ More stages that's all. $\\endgroup$ – Muze Jan 1, 2019 at 20:25 3 $\\begingroup$ @Muze: I was taken aback to discover that indeed the linked paper makes no attempt to consider additional stages beyond a simplistic two-stage Falcon-9-alike. (It also makes the distressing blunder of conflating the linear mass increase from a payload increase with the exponential mass increase of a velocity increase.) $\\endgroup$ – Nathan Tuggy Jan 2, 2019 at 10:48 8 $\\begingroup$ Actually, the best quote from this paper is \"On worlds with a surface gravity of $\\geq$ 10g, a sizable fraction of the planet would need to be used up as chemical fuel per launch, limiting the total number of flights.\" $\\endgroup$ – Everyday Astronaut Jan 13, 2020 at 6:59 3 $\\begingroup$ \"limiting the total number of flights\" Unless you make your flights with escape velocity, in which case you will soon not be troubled by a high gravity any more. :) $\\endgroup$ – Suma Feb 5, 2021 at 8:47 Add a commentThis answer is useful 9 This answer is not useful Save this answer. Show activity on this post. $\\begingroup$ Not a planetological exposition in sight so, I'll add my two cents to this rather theoretical discussion. Amongst exoplanetologists, the consensus has emerged that 1.6 Earth radii and 5 Earth masses is likely to be the upper limit to rocky planets. Simulations have shown that above these figures, the bodies develop increasingly Mini-Neptune like characteristics. This means very thick Helium Hydrogen atmospheres and crushing surface pressure. Also since Michael Hippke's slightly whimsical paper was referenced in one of the answers it seems appropriate to mention Ocean worlds at Super Earth masses. Ocean worlds present a host of habitability hurdles including a paucity of certain life critical elements like phosphorus, lack of volcanism, no water rock interface due to high pressure ice on the marine floor and others. These conditions will likely limit or even prevent the establishment of the vibrant prebiotic chemical environments that are necessary for biogenesis. If the first assumption holds true, the highest gravity on a potentially habitable world will not exceed approximately 2.5g.(edit: and thus making it not quite so difficult to reach orbit with chemical rockets as would have been the case with a higher g value) Share Improve this answer Follow Follow this answer to receive notifications edited Feb 5, 2021 at 17:43 Suma 23722 silver badges1313 bronze badges answered Feb 16, 2020 at 3:20 pres1dentkangpres1dentkang 9111 silver badge22 bronze badges $\\endgroup$ 7 $\\begingroup$ Thanks for your answer! I can't guess what \"increasingly Mini-Neptune like characteristics\" means or what \"Hippke's paper\" is. Without any linked sources or cited references in your post it's hard to know what this means or judge your answer's veracity. Is it possible to support this with some links and/or citations? Thanks, and Welcome to Space! $\\endgroup$ – uhoh Feb 16, 2020 at 3:40 1 $\\begingroup$ Where in your answer is the question about rockets addressed? $\\endgroup$ – Organic Marble Feb 16, 2020 at 4:20 $\\begingroup$ Hi, uhoh! Thanks for the greeting. I edited for a little clarity and added a couple random expository footnotes. There are thousands of papers on these topics. $\\endgroup$ – pres1dentkang Feb 17, 2020 at 5:36 2 $\\begingroup$ @ Organic Marble, the primary impediment to reaching orbit discussed here is high gravity. Thus, I think a comment on the likely upper limit of habitable worlds gravitational strength is warranted. As a side note greater aerodynamic forces at higher atmospheric pressures than exist on Earth also deserves attention, but as an aerospace engineer you're likely better equipped to write about that than I am. Cheers. $\\endgroup$ – pres1dentkang Feb 17, 2020 at 16:04 1 $\\begingroup$ \"1.6 Earth radii and 5 Earth masses\" It seems en.wikipedia.org/wiki/Kepler-20b mentioned in another anwer is a counterexample to this, assumed to be a rocky world with 9 M⊕ and 1.9 R⊕ (still Wolfram Alpha tells me its surface gravity should be around 2.5g) $\\endgroup$ – Suma Feb 5, 2021 at 8:52Show 2 more comments This answer is useful 2 This answer is not useful Save this answer. Show activity on this post. $\\begingroup$ Great answers have been given, but one of the major themes is they assume a fixed wet to dry mass ratio of 10:1 (ish). The justification is: You need to fix this as: there are no meaningful answers with without a value and, which value is subject to engineering nuances, which are difficult to handle. 10:1 is a good pick. (We can't do much better than this and still have everything work so it seems sensible to stick at this) The problem is that's the limit of what we can make work on earth. A lot of the dry mass of a rocket is either: directly related to the thrust-to-mass ratio (i.e. number/size of engines) indirectly related to TMR (i.e. supports the structural loads) Note, to keep gravity loses equivalent in practice the accelerations needed, hence TMR, is linear with the surface gravity. Hence so is a part of the the wet/dry mass ratio. Once we take that into consideration things look a lot bleaker for the high g super-earths getting something into orbit using chemical rockets. The actual numbers here are a little difficult to know, but if 5g world leads to a rocket with a w/d mass ratio of 5 to 1 (which I think is about right but...), you're staring down the barrel of a $10^{20}$t type figure for launch mass. To put that into perspective, the 'moon rocket' is no longer a good comparison. That's the mass of the moon it got to. Theoretical limit? I'd say so. At that mass things start taking a turn for the 'XKCD'. Forget the practical issues they're clearly long gone at \"moon-sized-anything\". We hit cold hard theoretical limits. You start having to deal with your own gravity. Firstly those practical issues are big ones even if we laugh a little 'engineering' problems (like money, and where we might find $10^{19}$t of aerospace grade materials). For example that's the sort of size that when you're made out something solid and are already floating in space under 0G, you deforming under you own gravity into a ball. Trying to make that out of mostly liquid fuel and subject it to 5-10g..., you're not staying the shape you started. Doesn't matter what mass-ratio 'hit' you are willing to take. But we've got this far, we aren't going to let a lack of unobtainium stop us. No the real hard limit is how being so heavy effects your exhaust velocity. At the risk of getting too meta here if you're heavy enough, its difficult to get things to come apart from you. It applies to planet sized rockets as much as it does planets. If you're a few million kilos, your 'exhaust velocity' is the velocity you can get your propellant to get to. If you have more mass than the moon, your propellant will have lost a lot of momentum by the time it's left your gravitational influence. And this is the fate our rocket meets. LOX/H2 has an exhaust velocity of about $4,400ms^{-1}$, about as good as we can do. Let's just say our moon-sized rocket has the density of the moon too, and so has a similar escape velocity of $2,380ms^{-1}$. Then the useful exhaust velocity of our rocket (initial less escape) is less than half. hence half the delta-v. You won't be going to space to day. \"Ok\", I hear you say, \"that just mean's you can't go to space in that rocket.\", \"How about a bigger one?\". Well \"No\". This is another one of those \"Even if everything sort of worked as before, you want go twice as fast, which is going to be a lot more mass.\" type problems. Except now we really can't just take the \"make in 10 orders of magnitude bigger\" approach. Apart from the fact that our rocket is now lot bigger than the planet which means we couldn't possibly construct it, now we have no chance of using chemical rockets to propel us anywhere. To gain any momentum we need to chuck something out of our gravity well, and the exhaust velocity of chemical rockets don't make when we are this big. We are now truly stuck. But wait: directly the exhaust doesn't make it out, but I wonder if you could try different way of getting mass out of a very deep gravity well. Shouldn't be too hard. Even if it was only a little bit, we could always just scale it up... Share Improve this answer Follow Follow this answer to receive notifications edited Feb 1, 2019 at 17:54 answered Feb 1, 2019 at 17:00 drjpizzledrjpizzle 34911 silver badge55 bronze badges $\\endgroup$ 4 1 $\\begingroup$ Try making your answer less verbose and heavier on facts. As written, it doesn't answer the question either. $\\endgroup$ – Organic Marble Feb 1, 2019 at 18:26 1 $\\begingroup$ @OrganicMarble thank's for your interest (and down-vote). To start with not answering the question: you're right I didn't give a hard answer. The problem is this is really a grey area. How big really just equates to how many issues are you're willing to magic away. My answer really only says unless your willing to ignore conservation of mass, 5 times as big will be a problem. I think that's interesting. $\\endgroup$ – drjpizzle Feb 1, 2019 at 18:33 1 $\\begingroup$ @OrganicMarble To answer your \"being heavy lowers exhaust velocity, not making sense\" comment sorry, perhaps I explained it badly. Maybe this image would help. Imagine throwing a ball tethered to your hand by a strand of elastic. How thick the elastic is will effect how much momentum you can ultimately put into it for a given throw. $\\endgroup$ – drjpizzle Feb 1, 2019 at 18:41 $\\begingroup$ I can't see how this is an answer to the question, or even supplements other answers. This reads like a conversation or a blogpost. Can you add a tl;dr at the beginning summarizing the key points of your answer? The conversational approach doesn't really fit the Stack Exchange Question/Answer format. $\\endgroup$ – uhoh Feb 3, 2019 at 0:48 Add a commentThis answer is useful -2 This answer is not useful Save this answer. Show activity on this post. $\\begingroup$ On a practical engineering side of things. Ultimately you are limited by exhaust velocity. In theory you can always just make a bigger engine, bigger tanks, etc. Ridiculously expensive, but possible. This would seem to set the real limit to material strength. Material strength is likely to give out before the gravity wells pull exceeds the exhaust velocity of even moderately modern fuels. For example, LF+LOX typically has an exhaust velocity of around 4,400 m/s. Which will fight up to 448 G of gravity. Literally more than the sun. Practically however much less than that. So size of the planet itself presents no real deal killers, it just makes the payloads mass fraction very VERY low. At some point though other technologies, like nuclear bomb drives (https://en.wikipedia.org/wiki/Project_Orion_(nuclear_propulsion)), become the only feasible affordable way off the planet. Share Improve this answer Follow Follow this answer to receive notifications answered Jan 1, 2019 at 18:00 Anthony BachlerAnthony Bachler 119 $\\endgroup$ 2 2 $\\begingroup$ I'm not sure how you are connecting exhaust velocity with \"fighting gravity\". It is true that a rocket with an exhaust velocity of 4400 m/s could only lift about 1 seconds worth of fuel supply against 448g, but that's not really directly relevant. You need to use the rocket equation and formulae for the escape or orbital velocities of planets of various sizes and densities to get a sensible answer. $\\endgroup$ – Steve Linton Jan 1, 2019 at 18:43 1 $\\begingroup$ A real rocket will never get close to the 1 second point. Payload for this calculation includes the engine and fuel tanks as well, which will always have nonzero mass. the 448 G's is literally the point where the fuel cant even lift itself off the ground. That is a jet of exhaust moving at that velocity would not even be able to escape the engine if pointed straight up., let alone lift any additional fuel $\\endgroup$ – Anthony Bachler Mar 18, 2019 at 21:08 Add a commentYour Answer Thanks for contributing an answer to Space Exploration Stack Exchange! Please be sure to answer the question. Provide details and share your research! But avoid … Asking for help, clarification, or responding to other answers. Making statements based on opinion; back them up with references or personal experience. Use MathJax to format equations. MathJax reference. To learn more, see our tips on writing great answers. Sign up or log in Sign up using Google Sign up using Facebook Sign up using Email and Password Submit Post as a guest Name Email Required, but never shown Post Your Answer Discard By clicking “Post Your Answer”, you agree to our terms of service and acknowledge you have read our privacy policy. Not the answer you're looking for? Browse other questions tagged launch rockets payload escape-velocity performance or ask your own question. Featured on Meta Updates to the Acceptable Use Policy (AUP) – January 2024 What would you like to change about the moderator election process? Linked 36 How much stronger does gravity have to be for space travel to be impossible? 11 On a Super-Earth 1.5x the volume and mass of Earth, would our rocket technology allow us to reach orbit? 5 Is there a maximum gravity limit for conventional rockets? 3 Is it possible for a space rocket to escape a planet's gravity if the gravity was 10 times that of earth? 2 How much higher could g be before we couldn't launch rockets? 2 How massive can a planet become before it is impossible to escape from using chemical rocket propulsion? 2 At what gravity would the rocket equation mean \"cannot reach orbit from the surface\"? 1 Escape Velocity 27 Why have SpaceX skipped the SpaceX Starship SN12, SN13, and SN14? 15 Is this a correct understanding of Tsiolkovsky's rocket equation? See more linked questions Related 9 Why are there such large differences in launcher payloads to higher orbits (GEO, Lunar) compared to LEO? 56 Can a miniature Saturn V get to the moon and back? 11 How could we make a fair comparison between space rockets regarding their payload mass in low Earth orbit and Earth escaping? 9 How much payload can Falcon Heavy Reusable lift? 13 Largest radius sphere with Earth's surface gravity on which you could jump at escape velocity? Bigger than B612? 4 Is a kilowatt per square centimeter a typical launch engine's thermal energy flux (density)? 6 What's up with the lack of subminiature chemical rockets? 1 How much easier is it, to explore space from a low orbit satellite, than from Earth? Hot Network Questions \"Ghost\", as in I am What do Americans mean by Coffee and Cream? Double-reduce a sequence of integers Daunting papers/books and how to finally read them Sufficient conditions for refactoring Induction in a magnetostatic scenario About Ireland visa Why doesn’t Cypher live out his fantasies in test programs? How to make my privacy balcony glass more transparent? Why in Japanese do they use \"kai\" instead of \"hai\" in these words? Radio telescope equipment Draw a line separating north and south quadrants passing through the centre point What would be easily missed, but undeniable, proof of life from non-advanced detectors? If someone used my open source (MIT license) project's logo and name to create their website/service, is it allowed? What is the point of entering numbers in the two-factor authentication app? Count the frequencies of each word in a file why hardlink doesn't corrupt if we remove the orginal file Probability of overflow in a summation of fixed-size signed integers How can I make a fan game entirely legally? Why does the U.S. seek to establish a demilitarized Palestinian state? Retaking an exam to improve a low, but passing grade. Why do universities not allow this? Which type of attorney should I consult for a complex case involving employment, contract, and IP law? Comparing indexed induction to recursion Making an expression with the numbers 1 to 100 odd (or even) more hot questions Question feed Subscribe to RSS Question feed To subscribe to this RSS feed, copy and paste this URL into your RSS reader. Space Exploration Tour Help Chat Contact Feedback Company Stack Overflow Teams Advertising Collectives Talent About Press Legal Privacy Policy Terms of Service Cookie Settings Cookie Policy Stack Exchange Network Technology Culture & recreation Life & arts Science Professional Business API Data Blog Facebook Twitter LinkedIn Instagram Site design / logo © 2024 Stack Exchange Inc; user contributions licensed under CC BY-SA. rev 2024.2.1.4133",
    "commentLink": "https://news.ycombinator.com/item?id=39243303",
    "commentBody": "How much bigger could Earth be before rockets wouldn't work? (space.stackexchange.com)345 points by trashtensor 15 hours agohidepastfavorite182 comments idlewords 8 hours agoThe converse of this was kind of an open problem in the early days of rocketry. Given the theoretical rocket concept, was there a propellant combination with sufficient exhaust velocity to make an orbital rocket practical? The answer was not immediately obvious, and there's a Goddard paper where he talks about just how big the rocket has to grow as you lower the propellant velocity to get equivalent performance. Eventually you're burning entire mountains of gunpowder just to get a few dozen miles up. It was a nice surprise (and a relief) to the early rocket pioneers to realize that we lived on a planet where gravity and chemistry would make orbital rockets possible. The rest was just engineering. reply thedanbob 8 hours agoparentFor anyone interested in the history of rocket propellants, I highly recommend \"Ignition!\" by John D. Clark[0]. It has plenty of chemistry if you're into that, but even if you're not (like me) it's an enjoyable read. [0] https://library.sciencemadness.org/library/books/ignition.pd... reply borlanco 6 hours agorootparentJust one of dozens of amazing passages in this book (page 48): > \"... its density was a little better than that of the other acid, and it was magnificently hypergolic with many fuels. (I used to take advantage of this property when somebody came into my lab looking for a job. At an inconspicuous signal, one of my henchmen would drop the finger of an old rubber glove into a flask containing about 100 cc of mixed acid -and then stand back. The rubber would swell and squirm for a moment, and then a magnificent rocket-like jet of flame would rise from the flask, with appropriate hissing noises. I could usually tell from the candidate's demeanor whether he had the sort of nervous system desirable in a propellant chemist.)\" reply signalToNose 39 minutes agorootparentprevThere is an excellent YouTube channel that explains the V2 rocket in detail. Almost down to the last screw. Highly recommend. https://youtube.com/@RocketPlanet?si=DdgyQ8HFnswrZgZr reply idlewords 7 hours agorootparentprevSeconded! That is such a wonderful book. reply refurb 5 hours agorootparentprevAs a former chemist, I thought this book was a great example of \"applied chemistry\". The theoretical aspects are challenging enough. But then you realize just how difficult the practical application of the theory can be. Sure, a mixture of fuming nitric acid and hydrazine will produce enough propulsion, but how do you dump tons of it into an engine without it just exploding? reply graphe 1 hour agorootparenthttps://youtu.be/K0FLy2nI13E turbopump https://en.wikipedia.org/wiki/Turbopump reply ethbr1 3 hours agorootparentprevThe section on building high-precision detonation speed timing apparatuses (and occasional explosive deconstruction of same) made me realize how uncomfortably close \"information we require\" and \"catastrophic consequences of collection that information\" are in the field. reply landryraccoon 6 hours agoparentprevWouldn’t it be fascinating if there were an advanced civilization on a planet with gravity that was much higher than earth that couldn’t build chemical rockets and was therefore forced to build nuclear rockets? What if that actually made the exploration of their solar system easier, since once they left the gravity well of their planet getting to other planets with nuclear rockets was comparatively trivial? reply godelski 5 hours agorootparentThese things are fun to imagine, but the real fun gets to be when you start talking about all the downstream effects. For example, if you can't build rockets you can't build GPS. Building a global communication system is much harder, which means things like shipping and flying are much more difficult. Not to mention that the gravity is much higher in the first place so flying is going to require way more fuel so how long does it take for them to get to that stage of civilization and how does their technological path differ? It gets even trickier once you start thinking about how the atmospheric composition will be different as gases follow similar escape velocities (e.g. Earth loses 3kg H/s but only 50g He/s) and it also determines what can even stay aloft. In general much of the technological paths are fairly straight forward, always iterating off of the current state (leaps and bounds are not common as they're more often a lack of domain expertise or not properly contextualized around the historical knowledge). But I think people forget how connected a lot of these things are. Then again, people often question why it is important that we build rockets, while asking those questions on their handheld computer connected to a global communication network. It's quite incredible how complex these interaction chains actually are and I think make you only admire the beautify of it all that much more. reply kolinko 42 minutes agorootparentInteresting thought. I think ground-based GPS wouldn't be too difficult though - we already have most of earth covered by GSM/3G/LTE, and with updated towers you could have something as precise, if not more, as GPS. Of course the coverage wouldn't be 100%, and navigating in ocean would be more difficult. Planes would be replaced by trains and aquaplanes for sure. Our modern fastest trains (TGV, Maglev) are only half as slow as the fastest commercial planes. Also, you might have rocketry on such a planet, just not for orbit, and for things that right now we use jets for. The biggest issue with be probably no detailed aerial maps, and in later stages - no space mining, so such civilisation would be limited to resources on their own planet. Also, I'm imagining that such a civilisation would send out more signals into space to encourage someone to come and visit them, and hopefully dropship resources from orbit :D Imagine two civilisations living like that in symbiosis - one on the orbit, able to drop things to the one that is lower, but being able to extract only information / art / mental labour / energy from below. reply necovek 1 hour agorootparentprevInterestingly, development of rockets has only made a bunch of the things you mention cheaper (to the multiple orders of magnitude), not impossible. Eg. determining location through radio signal triangulation can tell you a location pretty well, but would require placing a lot of signal stations throughout the world. Eg. remember the time-synchronisation mechanisms for watches through AM signals (including in hand watches). Similarly, we did build a global communications network by placing expensive undersea cables across the world, but systems like StarLink are much cheaper (once you get to economies of scale for launching satellites). So, like many things, rockets have accellerated discovery and progress, but are ultimately not the be-all solution: they work in tandem with the rest of science and engineering (including cultural development). reply AtlasBarfed 1 hour agorootparentprevWhich actually leads me to thinking that a space-adapted race really doesn't want to bother with planets and their big ass gravity well. Resource extraction from asteroids or moons is a lot easier than carting it out of a big gravity well. Building stations in zero G rather than having to worry about orbital degradation and the like. Atmospheres get in the way of solar energy collection. Earth is probably only useful as a vacation destination. Unless of course all those UFO reports are actual physics-defying antigrav drives with little green men. reply fy20 4 hours agorootparentprev> once they left the gravity well of their planet getting to other planets with nuclear rockets was comparatively trivial? We are actually on that planet. Spacecraft have what is called delta-v, which is basically a measure of what orbit changes they can perform given the amount of fuel they have onboard. For example getting from the ground to LEO has one measure, and getting from LEO to moon orbit has another. It varies somewhat by the specific rocket to get into space (due to drag and effects of higher gravity), but once you are there it's basically the same for all spaceships. It takes around 9.6km/s (no relation to gravity, just a coincidence) of delta-v to get into LEO, however once you are there it's fairly cheap to get around the solar system. To get from Earth LEO to a captured orbit around Mars needs a delta-v of around 5km/s - yes, less than to get into Earth orbit. To get out further to Neptune would need around 12km/s of delta-v. reply scotty79 6 hours agoparentprev> we lived on a planet where gravity and chemistry would make orbital rockets possible It's kind of insane luck. Bit heavier planet and we wouldn't be able to have a single satellite before building nuclear engines. reply eru 6 hours agorootparentI'm not sure how nuclear engines would help? A nuclear reactor is a bit like an ion drive: great for long distance space travel, but not great for getting off a planet. Unless you mean the kind of nuclear engine that consists of detonating atomic bombs behind you? See https://en.wikipedia.org/wiki/Project_Orion_(nuclear_propuls... reply liamwire 1 hour agorootparentFor non-manned launches and those that can be hardened to withstand extraordinary g-force, something akin to setup that resulted in the missing (900 kg) borehole cap of Operation Plumbbob may do the trick. Acceleration to 66 km/s is probably a little bit overkill, even. https://en.m.wikipedia.org/wiki/Operation_Plumbbob reply jccooper 3 hours agorootparentprevThrust to weight of a nuclear engine is fairly poor, so they are best suited for upper stage or in-space work. A heavy-planet rocket might use chemical propulsion in a lower stage just like we do and a high-energy nuclear upper stage (or two) where the really high Isp would be quite useful. reply adrianN 2 hours agorootparentprevMaybe something like NERVA reply bell-cot 14 hours agoprevIf you have a sufficiently tall* first stage, and use hot staging, then you can make it work on even on an extremely large Earth.** *First stage may need to extend well above the atmosphere. **No, that's for-sure not a Randall Munroe book in my hand. reply defaultcompany 13 hours agoparentThis is the same way you make a train capable of traveling from Los Angeles to New York in 1 second. A sufficiently long train. reply mike_hock 12 hours agorootparentI think the speed of sound in steel (or whatever the train is made out of) is slower than that. But I guess you could cheat by having multiple engines along the way that all accelerate in lockstep. reply contravariant 12 hours agorootparentOr you could just put the engine in the front. There's no rule a train can't arrive before leaving after all. reply defaultcompany 12 hours agorootparentprevRelated theoretical question for those who are of the physics mindset - if I had a long (very long like 1 light minute long) bar of metal and I pushed on one end, I'm assuming the other end would not move instantaneously because that would imply some part somewhere inside the bar was moving faster than the speed of light. So I'm assuming that the bar would just compress slightly and for a period of time in between when I pushed on one end and when the other end moved the bar would be slightly shorter. That's fine if that's the case. But what if the thing I push on is a quantum particle? Does this same thing happen at the smallest scales? If one end of a quark is pushed on does the other end move instantaneously or is there a small(!) delay? Probably the answer is just \"that's not how quarks work\" but I've always been curious. reply AlotOfReading 10 hours agorootparentThe other answers are correct that it's an acoustic wave, but sometimes it helps to see a demo \"proving\" it: https://youtu.be/DqhXsEgLMJ0 reply addaon 12 hours agorootparentprev> If one end of a quark is pushed on does the other end move instantaneously Quarks don't have an \"other end.\" To the best of our knowledge, particles are points. reply eru 5 hours agorootparentWell, at that level, you are talking about clouds of probability or structures in the wave function or something like that. reply amelius 10 hours agorootparentprevSo an object of zero volume will arrive in New York. reply superposeur 3 hours agorootparentprevInteractions between particles such as quarks are mediated by fields filling the space between them (such as electromagnetic field and gluon field). Ripples in these fields propagate at speed less than or equal to c. This is a classical picture, but the quantum picture is similar: evolution is generated by a local Hamiltonian constructed out of field operators attached to every point of space. So, both classically and quantumly, relativity demands the existence of fields filling space to propagate causal influences at finite speed. reply evilduck 12 hours agorootparentprevOn your long bar, the push propagates at the speed of sound in the material. Look up Slinky drops on YouTube. reply jasonwatkinspdx 10 hours agorootparentprevWhen you tap on the bar it creates an acoustic wave that will propagate at the speed of sound in the material. For subatomic particles, the most intuitive way to think about things is to adopt the \"fields are real\" mindset. Here fields are the underlying reality, and particles are just a pattern of waves excited in the fields. Disturbances in all fundamental fields we've discovered propagate at the speed of light, and we have pretty solid reasons for believing no future discovery will contradict that, as it would break causality in a fundamental way. reply meindnoch 10 hours agorootparentprevThere's no such thing as instantenously \"pushing\" on a particle. E.g. electrons can be accelerated by electromagnetic fields. If the field changes, the electron feels a force and is accelerated according to a = F/m (handwaving away relativity). When you macroscopically push against a rigid body, what happens at the particle level is your constituent atoms' electrons (and protons) interact with each other through the electromagnetic field. reply grecy 10 hours agorootparentprevSurely for a train to \"travel\" from LA to NY, it \"starts\" when the front of the vehicle passes a line in LA, and \"finishes\" when the front of the vehicle crosses a line in NY. reply Sharlin 12 hours agoparentprevYou’re going to have a hell of a difficult time trying to construct anything that tall on a high-g planet. The taper ratio between the base and the top would have to be enormous – likely a sizeable fraction of the radius of the planet! Though I guess it would have to be anyway so you have somewhere to attach all those first-stage engines… reply jstanley 12 hours agorootparentIf you're allowed to build it tall enough, just don't even light the first stages. Launch the final stage directly from a high enough altitude that it can escape on its own. reply Sharlin 11 hours agorootparentWell, yeah, but building something tall enough to reach the synchronous orbit is impossible even on Earth, there’s no material with even a thousandth of the compressive strength required. Space elevators are only possible because they’re tensile structures and the “bottom” that supports the weight of the entire structure is up there in a low gee environment. Remember that just getting outside the atmosphere is the almost trivial part of rocketry compared to the problem of having to then accelerate to >= orbital speed fast enough to not fall down! And anyway you’d have to dismantle the planet to build your launch tower, which I guess would solve your problem, in a fashion. Though – whatever you turned your planet into would just have an annoying tendency to rapidly collapse back into a ball. reply eru 5 hours agorootparentOn earth, you could use active support to build your tall structures, then you don't need exotic super-materials. See eg https://en.wikipedia.org/wiki/Space_fountain reply usrusr 9 hours agorootparentprevIf you really have a lot of time for the project (starting early in the star's burn?), you might try using photovoltaics to move a lot of mass across the surface to ahead of where tides would accumulate, slowly speeding up the day/night cycle. The faster you spin, the flatter your geoid and you should probably stop accelerating before your entire equator region goes interplanetary. reply amluto 14 hours agoparentprevI’m trying to decide whether you’re describing a rocket or a space elevator. If you build a tower that extends to somewhere near geostationary orbit, you can pretend it’s a rocket stage delivering a delta V of zero, and you can “hot stage” a tiny little stage off the top, and voila, you’re in orbit. Of course, once you’ve managed to build this, the rockets are basically optional. :) reply ivanjermakov 12 hours agoparentprevYou just need to launch off of the highest point on the planet! (Kerbal Space Program approved). We don't talk about ground logistics though. reply Taek 12 hours agoparentprevYou could just perch the first stage at the top of a sufficiently tall mountain. reply cratermoon 12 hours agorootparentCould, but at least on Earth the difficulties outweigh the gains enough to make it too expensive. https://youtu.be/4m75t4x1V2o?si=6FzbQYrLtl7Zfe0J&t=157 reply chipweinberger 12 hours agoparentprev** And assuming sufficient propellant exists on your planet :) reply patrickwalton 12 hours agoprevFascinating. This may weigh down the Drake equation, particularly in reducing the average time civilizations survive on planets with high gravity because their ability to become multiplanetary and survive great filters is limited. reply anonymouskimmer 12 hours agoparentOne of the biggest hypothetical great filters is massive war. Higher engineering requirements for rockets (or even simple projectiles such as cannons or arrows) would set limits on the rate of increase of warfare technology. It's possible other means of diplomacy would advance at sufficient speed to preempt population annihilation from global war. I'm curious what effect an increase of gravity may have on heavier-than-water displacement craft (canoes and other modern boats). I think probably none, since you're dealing with density, not weight. Except for any increase in density of early building materials and cargo/supercargo. But it's been long enough from physics I'm unsure. I think atmospheric density is more dependent on magnetic field than gravity. reply wolfram74 11 hours agorootparentThe thing I often think about is while the demands for an orbital class vehicle quickly become untenable, ICBM's stay viable for a lot longer. I don't know if MAD is more or less stable without the prospect of space exploration. reply nine_k 9 hours agorootparentI would argue that situation is usually more stable with less secrecy, so a lack of spy satellites would not be beneficial. Absence of comm satellites would also help fragment the world and make the idea of a surprise attacks more enticing. reply Aeolun 9 hours agorootparentNot beneficial, but I imagine you’d just see a lot more spyplanes. reply nine_k 8 hours agorootparentSpy planes can be shot from the ground, or with another plane. This is closer to an act of hot war :( Orbital space (around modern Earth) is ex-territorial, so killing a spy satellite would be seen as an act of aggression, not legitimate defense. This holds back \"kinetic action\" in near-Earth space. reply eru 6 hours agorootparent> Orbital space (around modern Earth) is ex-territorial, [...] That's a historical accident of arrangements on earth (so less useful in the Fermi-paradox / filter debate), and could easily have gone differently. Ie air space could also have been seen as ex-territorial. reply unsupp0rted 2 hours agorootparentprevWar may be the exception, not the rule. Just because we're built for it doesn't mean other species will be. If evolving in a different environment, they might be built for cooperation. That is, in a certain environment the only species that can evolve enough to go interplanetary might be a species that learned to co-exist internally and externally, otherwise the environment would have kept them down. reply Intralexical 2 hours agorootparent> War may be the exception, not the rule. > Just because we're built for it doesn't mean other species will be. You heard of what chimps get up to? Ants? Microbes? They don't just have wars; They have raiding parties, take slaves, serve as battlefield medics, compete in intrafactional and interfactional rivalries that slowly boil over… Hell, even trees actively release toxins to try to kill other nearby plants. On a long enough timescale, war is almost certainly highly (and lethally) maladaptive. But in a non-post-scarcity environment with social contact, creatures whose bodies disagree with entropy tend to learn that violence is an effective tactic for taking others' calories/oil and nutrients/minerals. Maybe there's exceptions. I hope so, anyway. reply unsupp0rted 1 hour agorootparentAll those species evolved on the same planet, with the same constraints. War works well for them, so they evolved to get better and better at it. But war is resource-intensive and costs lives. Lives are easily replaced on Earth. It’s not hard to imagine a planet where going to war would be mutually assured destruction on a species level, even for ants and microbes. reply Jensson 40 minutes agorootparent> It’s not hard to imagine a planet where going to war would be mutually assured destruction on a species level, even for ants and microbes. That is very hard to imagine, how do you reckon that would be possible? Does the planet only support a couple of anthills and then all resources are consumed? How would ants even appear on such a planet? reply kolinko 30 minutes agorootparentprevInteresting thought, but how would that work? reply hermitcrab 11 hours agorootparentprev>I think atmospheric density is more dependent on magnetic field than gravity. Atmospheric density is very much affected by gravity. I'm not sure magnetic field has any appreciable effect at all on the density of the earth's atmosphere. Why would it? The vast majority of the atmosphere isn't charged, so doesn't interact directly with magnetism. reply slavboj 11 hours agorootparentMagnetic field protects from solar wind that strips atmosphere. reply outofpaper 10 hours agorootparentIt's often said but with heavy evidence to yhr contrary e.g. Venus. Venue's negligible magnetic field is almost non-existent yet its atmosphere is many many times thicker than ours. reply idiotsecant 9 hours agorootparentIn fact, earth actively loses material to space because is has a magnetosphere, polar outflow of oxygen for example. reply kijin 1 hour agorootparentVenus loses a lot of material, too. One of the reasons Venus still has a dense atmosphere is because its atmosphere is mostly composed of a relatively heavy compound, CO2, which is harder to lose than lighter gases like H2, N2, and O2. reply hermitcrab 10 hours agorootparentprevIf you turned off the earth's magnetic field today, then presumably the atmosphere would be gradually stripped, away over millions of years. Similar to what happened to Mars. But it would not make any immediate difference to the density. reply AnimalMuppet 9 hours agorootparentNot immediate, no. But magnetic fields don't usually just switch off. If the planet didn't have one to begin with, then it probably doesn't have much of an atmosphere for long enough for advanced life. reply datameta 11 hours agorootparentprevBuoyancy is indeed not affected by gravity, you're correct. reply antod 9 hours agorootparentDepends how compressible the fluid you're floating in is right? Note that's 'fluid' rather than just liquid. As you increase gravity, with fully compressible fluids the buoyancy scales the same as weight, you wouldn't sink lower. But with any incompressibilty you'd need to displace proportionally more (ie sink down more) to counter the increasing weight. (I think) reply Intralexical 2 hours agorootparent> But with any incompressibilty you'd need to displace proportionally more (ie sink down more) to counter the increasing weight. The water would also weigh more. Buoyancy is the force of the water around the volume you displaced being pulled down into that space, exerting pressure that pushes you up. So you'd float just as well. Actually, the compressible fluids would become denser­, and make it easier for you to float (assuming you're relatively incompressible). At the extreme end, you could swim in pressure-liquified air (assuming you survive being crushed, of course). reply bugbuddy 11 hours agorootparentprevThis is technically wrong. Increase in gravity does affect buoyancy because air density changes with gravity. The reason is that the column of air above you is compressed by gravity. With very large gravity, all the atmosphere could be compressed down to possibly a few km. reply BuyMyBitcoins 10 hours agoparentprevYou may find the concept of Superhabitabilty interesting. The hypothesis suggests that larger planets with more mass and gravity than Earth would be more favorable to life. It’s certainly possible that there is a lot more life out there on planets where getting into space is nearly impossible with conventional chemical rockets. We may be living on a comparatively barren rock, but the tradeoff of that we are actually able to get into orbit. https://en.wikipedia.org/wiki/Superhabitable_planet reply scotty79 6 hours agorootparentImagine visiting such planet. They'd think you are gods because, how did you get up there? But you can't really land because it would be one way trip for your tech. So you just hang around and talk with radiowaves, sending them pictures of their world from above they could never see otherwise. reply Intralexical 2 hours agorootparentIf you have the power to cross the stars, surely taking off from a steep gravity well wouldn't be a problem. But I do like the idea that you wouldn't be able to. So instead you slingshot your orbital craft past the planet, using its gravity well itself to build up speed— And you release a cable ahead of you, that swings down through the atmosphere to zero surface velocity at the point of your perigee, so your away team and their new friends can attach it to a glass elevator and be smoothly hoisted into space. reply Maxion 1 hour agorootparent> If you have the power to cross the stars, surely taking off from a steep gravity well wouldn't be a problem. Different problems entirely. You don't need a lot of thrust to get to high velocities. But you need a lot of thrust to leave a planets gravity well. On a planet, your thrust needs to win not only gravity, but also any atmospheric losses. E.g. on a 3g planet, you'd need thurst in excess of 3g's to leave. But to reach say 0.25c, a tiny ion engine over a long enough time would suffice. an engine that wouldn't even get you off of earth. reply eru 6 hours agorootparentprevWell, you could send remotely controlled probes. And there might be some volunteers for a one-way trip, too. There are certainly volunteers for one-way trips to Mars right now, and by the time humanity would be an interstellar species, our population size would have gone up by several orders of magnitude; so even if volunteers are rarer as a proportion, they would be much more numerous in absolute terms. reply unsupp0rted 2 hours agorootparentprevRockets are out, but you could drop a space-elevator line down reply p-e-w 3 hours agorootparentprevNo advanced civilization would conclude they are dealing with \"gods\" just because they see someone with presumably better technology than themselves. In fact, if they are anything like humans, they have probably already realized that species that live on a lower-gravity planet could escape that planet using the same chemical reactions that are available to them also. reply Intralexical 2 hours agoparentprev> […] ability to become multiplanetary and survive great filters is limited. So, the known quantities that term refers to tend to be steps more like planetary habitability and abiogenesis, which might prevent complex life from getting established in the first place. But it sounds like you mean some kind of cataclysmic event which wipes out an already existing industrialized civilization. What, specifically, are the \"Great Filter\" scenarios which being multiplanetary is actually supposed to help with? Supernovas? GRBs? Simple asteroid impacts? You can usually see those coming from millions of years in advance. And surely building a couple layers of solar sail material to shield the planet, stockpiling ozone generators to repair the damage quickly, gently nudging the asteroid, or simply digging some holes/eating a gas giant and weathering the storm, would be easier and save vastly more people than establishing a sizable population in another star system. The other \"Great Filter\" idea which seems to be memetically adapted for proliferating in modern discourse is the idea of a locust-like swarm of technologically advanced aliens that kill any industrial civilizations which do emerge. But in that case, presumably settling multiple star systems is the opposite of what you'd want to do; You'd be better off quieting your emissions to shrink your footprint than spreading even more biomarkers around at high blueshift. Frankly, I think this entire idea of needing to \"become multiplanetary and survive great filters\" is more mainstreamed now largely due to one specific individual fancying himself a savior of humanity. SpaceX builds interesting machines, but I liked it better when it was people like Sagan, Aldrin, and Zubrin getting excited about Mars. But even then, I'm not sure if the idea of colonizing more planets in order to survive planet-scale catastrophes really jives with how people think— Plenty of us already live within splash radius of the Pacific Ring of Fire, Yellowstone Caldera, tornadoes, tropical cyclones, land below sea level… and yet there's no billion-dollar emergency backup cities in Antarctica to \"make San Francisco into a multicontinental city and survive great quakings\". reply ikari_pl 2 hours agorootparent> surely building a couple layers of solar sail material to shield the planet, stockpiling ozone generators to repair the damage quickly, gently nudging the asteroid, or simply digging some holes/eating a gas giant and weathering the storm, would be easier The assumption is that we have a problem getting anything off the planet. All these would require some good rocket engineering. I agree with everything else here a lot. reply GuB-42 7 hours agoparentprevThe article limits itself to chemical rockets. They work well enough on Earth so that's what we are using, but we can do better. Replace chemistry with nuclear, and use the air in the atmosphere as a reaction mass. On Earth, that would cause more problems than it would solve, that's why we don't do that despite having the tech to do it. But on a higher gravity planet it may be what we would do. Harder, but not impossible. It is interesting how we got nuclear technology that would allow for way more capable rockets at the same time we perfected chemical rockets enough to get to orbit. So much that we could have been able to escape a 10g planet almost as soon as we have escaped Earth. reply eru 6 hours agorootparentIf you want to use nuclear technology to get to orbit on a planet with an atmosphere, you pretty much have to use bombs. See https://en.wikipedia.org/wiki/Project_Orion_(nuclear_propuls... More conventional nuclear propulsion has similar trade-offs to an ion drive: great for long distance travel when you are already in space, but useless to get off a planet. reply brucethemoose2 10 hours agoparentprevIs this really a big factor? Not being multiplanetary seems like the least of our existential problems here on Earth, and will continue to be that way for awhile. At the same time, chemical rocket efficiency becomes totally irrelevant for a slightly more advanced civilization than us. reply eru 5 hours agorootparent> Not being multiplanetary seems like the least of our existential problems here on Earth, and will continue to be that way for awhile. That might be true, especially if your 'for awhile' talks about millennia at most. But it's an extremely relevant concern in the context of the Fermi paradox. reply nine_k 8 hours agorootparentprevYou can't build a space elevator before getting to the orbit first. A jet engine capable of leaving a deep gravitational well must have a big ratio of thrust to weight. If a chemical rocket is too weak, a nuclear jet engine is the only remaining option. Would you be comfortable running it in the thick atmosphere of a densely inhabited planet? reply brucethemoose2 5 hours agorootparentA civilization just a few decades ahead of us is (theoretically) almost unimaginable. Bio augmentation, true AIs, who knows what advances in fundamental physics knowledge... Just to start. What I'm saying is that whatever engineering and environmental limitations we currently perceive are probably irrelevant. reply awwaiid 7 hours agoparentprevYeah, but those evolved on high-gravity planets are smarter (and maybe stronger) since they too must calculate thrown-object trajectories but have to do so faster. Our brains use just enough energy, but no more, to keep us alive. Being smarter than we are would be a waste .... but if we HAD to think faster, we would evolve to match. So maybe they'd figure it out. reply eru 6 hours agorootparentHumans are (nearly?) the only animals that got really into throwing stuff, especially throwing stuff with heft and precision. (As a corollary: you can train seals to balance balls, and apes might through excrement; but only (some) humans can juggle.) If throwing things well had been much harder, perhaps no animal would have ever bothered? reply bloopernova 14 hours agoprevIs there an equivalent to the Drake equation that includes a factor that describes planets small enough to escape? Very depressing to me to think about how vanishingly rare smart, spacefaring life might be. But on the flipside of that, there may be a little corner of the universe where multiple spacefarers contemporaneously live within a few light years of each other. That might be cool from a space opera point of view but it'd probably end up being dominated by a space fascist enslaving everyone. reply HeatrayEnjoyer 13 hours agoparentRockets are not the only way off a planet. If humans had spent space program amounts of resources on railguns or another method of locomotion there's real possibility it would have been successful too. Rockets are most convenient for Earth's variables so engineers optimized for them. reply choilive 13 hours agorootparentRailguns also become much harder in a larger gravity well. Bigger planets generally have thicker atmospheres as well. Your payload will end up disintegrating at the velocities required even on Earth. reply avar 13 hours agorootparentThe Earth is larger than Venus, but its atmosphere is 90 times denser than ours. There's a lot more variables that just gravity. reply contravariant 12 hours agorootparentprevBalloon stage followed by a rail gun might work. reply the__alchemist 8 hours agorootparentprevVacuum inside the tube? reply eru 5 hours agoparentprev> That might be cool from a space opera point of view but it'd probably end up being dominated by a space fascist enslaving everyone. Fascism already barely works on earth, and gets out-competed. See https://tvtropes.org/pmwiki/pmwiki.php/Main/FascistButIneffi... Similarly with slavery. (See https://www.econlib.org/library/Columns/LevyPeartdismal.html to go off an slight tangent.) In space, slavery is even less useful. That's mostly because humans are even less useful: we are already doing pretty much all of our useful space exploration with robots, and sending humans is just for bragging rights. Keeping space slaves alive costs you more than they ever could conceivably do for you. Of course, aliens might have biologies that are much better adapted to surviving in space, maybe? reply idlewords 7 hours agoparentprevI think mostly the Drake equation shows a lack of imagination about the forms life might take. Every time you add a term to it, you're baking in additional assumptions. reply mcmoor 1 minute agorootparentBoth lack of imagination of what life can be, and supporting factors we take for granted. I dismiss drake equation as fully useless as it doesn't provide useful upper nor lower bound. reply eru 5 hours agorootparentprevMaybe. But you still have to explain the observation that the night sky is empty of signs of life. Keep in mind that human technology is pretty close to being good enough to detect not just foreign civilisations (via eg radio waves), but signs of life itself: studying the spectra of light reflected by exoplanets can tell you what chemical elements are in their atmosphere, so you can detect atmospheres that are far from chemical equilibrium, like earth's oxygen rich one. We emitted radio waves for only a few decades. But earth had oxygen for billions of years. So that widens the window of time of development that we could detect. reply szundi 13 hours agoparentprevFew lightyears is something like a hundred stars. Probably zero chance to nurture more than one unprobable life. reply Retric 12 hours agorootparentThe upper limit on the size of the universe is infinite. So we can’t exactly rules stuff out for simply being improbable. reply kuchenbecker 7 hours agorootparentYou can make a fairly strong statistical argument there are no spacefaring species in our galaxy. Even at 1% the speed of light a species could fully colonize the galaxy in 10 million years, 0.1% the age of the universe. That we see nothing implies intellgent life is rare, short lived, or we're early in the age of the universe. For example, red dwarfs will last trillions of years compared to the sun's 5B lifespan. reply Intralexical 2 hours agorootparent> That we see nothing implies intellgent life is rare, short lived, or we're early in the age of the universe. Or it implies that not everybody's first instinct on seeing a vast galaxy is to try to take it over ASAP. If you want resources for quality of life— Gas giants are a thing. If you're an explorer driven by curiosity, then take only samples, leave only memories, right. If you want money— Century-long shipping times with civilization-scaled fuel costs tend to eat into profit margins. If you're worried about survival— genuinely worried about survival, on a level personal enough to motivate action, not just academically or for fun— then the focus is on people you know and care about; all of those are here. In fact, I suspect the ones that see other stars and immediately think \"Mine mine all mine!\" probably have a higher chance of nuking themselves before they even get out of their star system. reply Retric 5 hours agorootparentprev1% the speed of light is stupidly fast to colonize a galaxy. It only sounds reasonable in the same way startup claim if we can just capture 1% of the X market, while forgetting numbers much smaller exist. Let’s assume super tech they can build that somehow allows vastly faster speeds than we can today ~120 km/s worth of DeltaV. Half that is spent slowing down so we’re talking 0.02% c. Now let’s assume half the time is spent in flight and half the time is spent colonizing stars before launching ships. So now we’re down to 0.01% C. Suddenly 1 Billion years is a more reasonable estimate and even that takes super tech we don’t have any idea how to build and assumes nothing fails. Several more advanced civilizations could be colonizing the galaxy today that are still 10 billion years from finishing. reply eru 5 hours agorootparentprevThat's why we typically only talk about the observable universe, which is very much finite. reply nathanaldensr 13 hours agoparentprevnext [7 more] [flagged] HeatrayEnjoyer 13 hours agorootparentThey didn't say space authoritarians, they said space fascists. reply jurynulifcation 13 hours agorootparentprevnext [6 more] [flagged] Terr_ 13 hours agorootparentIf anything, it should be the opposite: The people who don't know history--such as the difference between an absolute monarchy or a tyrannical theocracy or a modern fascist state--are more likely to blithely want something terrible in the near future. reply dTal 11 hours agorootparentThere was nothing to imply that the original comment held any misconceptions about the difference between fascism and authoritarianism (nor is it especially relevant in context) so the rush to \"correct\" it signals a somewhat disquieting obsession, hinting perhaps at an unusually personal investment in the fine distinction between fascism and authoritarianism. reply Terr_ 13 hours agorootparentprevP.S.: I realize this thread already wandered quite far from the rocket equation, but I'm going to lean on the principle that quoting Deus Ex (2000) is always nice: > [Be Safe: Be Suspicious] How can you tell who might be a terrorist? Look for the following characteristics: > * A stranger or foreigner. > * Argumentative, especially about politics or philosophy. > * Probing questions about your work, particularly high-tech. > * Spends a greater than average amount of time on the Net. > * Interests in chemistry, electronics, or computers. > * Large numbers of mail-order deliveries. > * Taking photographs of major landmarks. > And those are just a few. If you're suspicious, then turn them in to your local law enforcement for a thorough background check. Better safe than sorry. You and your neighbors will sleep more securely knowing that you're watching each other's back. reply CapitalistCartr 10 hours agorootparentHey, I qualify for every point on that list. I better keep an eye on myself, maybe have to turn myself in. reply black_puppydog 13 hours agorootparentprevAs unexpected this was to read for me as well, it's actually good to keep in mind. Because someone saying \"but I'm not fascist\" in a political debate may well be technically right, and still be authoritarian. reply boringuser2 11 hours agoparentprev>fascist This word is wild. Very interesting. It basically means \"evil bad guy\". reply wongarsu 7 hours agorootparentIf I had to describe fascism in one sentence it would rather be \"take social darwinism 'the strong rule the weak' and add a pragmatic leader who helps your group crush those who hold you back and rule everyone else\" You know, exactly the kind of ideology you don't want a neighboring nation to have, no matter how you judge their actions morally reply boringuser2 5 hours agorootparentThat's definitely not even close to reality. National Socialism and Italian Fascism were all about the strong taking responsibility for the nation, in a philosophical sense. These were social welfare states that provided for the \"people\" far more than modern American liberalism (the standard operating produre for free economies of scale in the modern world), for example. Now, obviously I'm not saying this was a good system, but you're so far off base that it's ridiculous. The discourse around this complex historical movement is profoundly anti-intellectual. I know this because I grew up in the same society you did, the one where I also learned and used fascist as a pejorative without any further information. Now, as someone interested in having sophisticated ideas about systems, I'm not really in a mental state where I'm going to take appeals or emotion or shortcuts that shut down thought seriously. Regardless of the context, I still want to try and reflect reality as closely as possible in my minds eye. How many liberal, or even communist thinkers have you read? Personally, countless. As for fascists, almost none. It's a taboo, the works aren't translated, etc -- but it's there, and has intellectual underpinning that is more complex than mindlessly calling people you don't like \"fascists\". This can't be conscionable to any earnest intellectual. Imagine sitting here and tolerating people pejoratively calling people \"communists\". It's so stupid. Edit: I get in trouble here because there's something really interesting going on with controversial topics: all you need to do is make a choice and your model of reality is much more accurate than the presented model of reality. It's the easiest way to take Ws out of discourse, nobody has good ideas when the id has the reins. reply thriftwy 1 hour agorootparent> Imagine sitting here and tolerating people pejoratively calling people \"communists\". Also tolerated - by calling them \"Russians\". The logic goes, if there's nothing innately wrong with Communism then Russians (and, to extent, Chinese) must be up to blame. reply defrost 57 minutes agorootparentThat's pretty much it - if you check your history it's clear that neither Russia nor China adopted Communism in practice - they both went for authoritarian committe rule with power struggles as some kind of \"neccessary\" middle state while they work their way towards Twue Communism. reply thriftwy 36 minutes agorootparentIt was still way more socialist than what California or South European left wants. Soviet Union had socialized assigned housing, affirmative action policy and equalized wages right from the start. Still, it is largely ignored by socialist LARPers of today. reply defrost 30 minutes agorootparentAustralia was and likely still is more socialist than any US centralists wants. The 1900's Harvester agreement indexed the minimum wage to am eight hour work day with a week sufficient to feed, house, and clothe a worker and their family. The Whitlam years saw free university education for anyone that merited by high school (and equivilency) exams, health care has been universal - now with a split of both public and private, pharmacy companies are capped on their generics so that costs are reasonable, differences are picked up for those that can't afford medication (for almost all prescriptions), etc. reply somewhereoutth 7 hours agorootparentprevFor the record (and the GP is using the term somewhat out of bounds), there are definitions of fascism that are rather tighter than 'evil bad guy': https://en.wikipedia.org/wiki/Ur-Fascism reply boringuser2 5 hours agorootparentThat's a stronger definition, but it's still weak because it isn't primary source material. This guy isn't a disinterested political historian. Imagine taking as definition the views of Giovanni Gentile on Marxism. I wouldn't call any of these groups \"fascist\" except in the loose pejorative sense The closest governments to actual Fascism that I can think of are governments like modern China or even Singapore, and I don't mean that in a pejorative sense. They're just very fascist in character, i.e. national interest, social welfare, strong-arming of capital, etc. reply bell-cot 12 hours agoparentprevThe Galactic Emperor is a monarchist, thankyouverymuch! And It treats all of Its loyal subjects quite well, with no discrimination against the water-based ones. Vs. if you have the misfortune to visit the Andromeda Galaxy... reply zuminator 9 hours agoprevOn a something like a gas giant with a hydrogen atmosphere surrounding a rocky core, would it be possible for the vessel to be hydrogen breathing until it reaches the edge of space and then ignite a stage to carry it out of the gravity well? Or if a nitrogen or CO2 atmosphere is thick enough, to fly aerodynamically or even float until it reaches a point where the gravity is appreciably lower than at surface level? reply wongarsu 7 hours agoparentOn rocky planets gravity doesn't get much lower in the orbits we're concerned about. For example the ISS still experiences 90% of the gravity we experience at the surface. Reaching orbit is mostly about reaching a speed where the arc in which you are falling never intersects the surface. But you can absolutely use an aircraft to gain height and speed, and then launch a much smaller rocket from that aircraft (where the speed is the primary advantage, and is what rockets use most of their fuel for). This setup is used by Virgin Galactic's SpaceShipTwo. There is also Virgin Orbit's LauncherOne, which is a small rocket that launches from a modified Boeing 747. On Earth it's just about not worth the additional complexity, but on planets with stronger gravity but comparable access to powered flight this might be the preferred method of reaching space. One important factor might be the speed of sound. Subsonic flight is much easier for aircraft than supersonic flight. In an atmosphere with a much higher speed of sound, like say hydrogen, aircraft could reach much higher speeds and thus would be a much more advantageous launch platform for rockets. Assuming you already solved the issue of powering those planes of course. reply foota 3 hours agorootparentI've never really understood this, why is it easier for a plane to reach that speed than a rocket? Is it sort of just another rocket stage? reply orost 3 hours agorootparentAn air-breathing jet engine doesn't need to carry oxidizer, which in a rocket is most of the propellant weight. It also has access to unlimited reaction mass, so it can be much more energy-efficient in producing thrust (it is more efficient to produce thrust by accelerating a lot of mass by a little, than by accelerating a little mass by a lot, but a rocket can't take advantage of this because it would need to carry all that extra mass. A plane can use ambient air for this purpose) This all adds up to a plane needing to carry many times less mass to gain the same altitude and speed as a rocket, at least within relatively dense atmosphere. reply mr_toad 7 hours agoparentprevOn a smaller gas giant you could build a floating platform (like a giant zeppelin) and launch from there. Because gas giants are so large the “surface” gravity at the altitude such a platform would be floating at is not as high as you might expect. On Uranus and Neptune it’s actually lower than 1G. However, past Jupiter size the mass keeps increasing while the radius doesn’t, so even from a floating platform you’re contending with multiple G’s. reply jasonwatkinspdx 8 hours agoparentprevThe basic physics of https://en.wikipedia.org/wiki/SABRE_(rocket_engine) have been vetted, which is an air breathing rocket engine. I don't know how much difference trying to liquify H2 vs O2 is though. reply jaggederest 8 hours agorootparentThe oxygen is the majority of the mass (but not volume!) in a stoichiometric hydrogen engine, so the mass savings would be less I think. The RS-25 (space shuttle main engine) runs at a higher fuel ratio. Should work very similar to SABRE in general - the concept of a high speed atmosphere collector and precooler is pretty universal to any gas, and hydrogen has an extremely high heat transmission rate. reply sandworm101 8 hours agoparentprevNo. To float you would need a gas lighter than hydrogen, which isnt a thing. And powered flight (wings) without oxygen would be trickey, requiring more of a rocket motor than an aeroplane engine. Yes, you could use a balloon filled with vacuum, but lifting something the size of an orbital rocket in a hydrogren atmosphere would require a vacuum chamber at least the size of a city, possibly the size of a small state. It would probably be easier to build a tower. reply mr_toad 7 hours agorootparent> No. To float you would need a gas lighter than hydrogen, which isnt a thing. The atmosphere gets denser further down. You just need a negative pressure vessel, or to heat the hydrogen, like a hot air balloon. At 1 (Earth) atmospheric pressure the gravity of most Gas giants is quite low. reply zuminator 8 hours agorootparentprevSorry if I wasn't clear, but I brought up the question of floating with respect to nitrogen/CO2 atmospheres (thinking Titanlike or Venuslike) not hydrogen. reply lisper 8 hours agorootparentIt's a moot point, but I still want to point out that \"a gas lighter than hydrogen\" is a thing: it is simply hydrogen at a higher temperature, i.e. a hot-hydrogen balloon, analogous to a hot-air balloon. reply simcop2387 8 hours agorootparentI do not want to ride in a hot hydrogen balloon :) reply eru 6 hours agorootparentWhy not? Hydrogen inside a hydrogen atmosphere is perfectly safe. It's hydrogen inside an oxygen atmosphere that's the problem. (So on Jupiter, you wouldn't want to ride in an oxygen balloon, ie you wouldn't want to ride in a hot air balloon there.) reply thfuran 8 hours agorootparentprevWell, maybe if there's no oxygen nearby. reply lisper 7 hours agorootparentIf you have a planet with a hydrogen atmosphere, it's a pretty good bet there is no (free) oxygen nearby. reply thfuran 6 hours agorootparentFair point reply pottspotts 7 hours agorootparentprevHow do you \"fill something with vacuum\"? reply xyzzy123 7 hours agorootparentWith a vacuum pump ;) reply crazygringo 13 hours agoprevSince it's barely mentioned in the answers, and was my first thought -- nuclear thermal rockets are something to think about too, at least in theory: https://en.wikipedia.org/wiki/Nuclear_thermal_rocket reply ravi-delia 13 hours agoparentNot so much for takeoff! Most rocket designs better than chemical rockets trade off thrust for specific impulse. That's an improvement in orbit, since delta-v is delta-v. But imagine a 10kg rocket- it's receiving ~100N of gravity. If your engine doesn't put out 100N of thrust you'll just sit there on the launch pad. As you pick up speed you no longer have to deal with that (after all, LEO has basically the same gravity and doesn't have to burn against gravity at all) but when you're launching off something other than a point mass, some of your thrust has to go towards ensuring you don't hit the planet, or you will not into space today. The practical designs we have for NTRs are solid core, which after long effort got up to a thrust to weight ratio of 7:1, meaning they could in principle carry up to 6 times their weight and accelerate up in Earth's gravity rather than down. Chemical rockets can get 70:1. No one ever had plans to use NTRs in lift platforms- instead they could serve as more efficient upper stage engines, for orbit-orbit transfer burns and the like. In principle there are engines which are technically NTR and offer much better performance, but no one's ever gotten a working prototype. Also you probably wouldn't want to launch with an open cycle rocket, since the open part describes how the radioactive fuel is ejected out the rear. Unfortunately, with the technology we have, we have to make tradeoffs between efficiency and thrust. For the lift stages chemical rockets are, for now, unrivaled. (Unless of course your nuclear propulsion is of the more, shall we say, entertaining variety. Project Orion has its proponents...) reply lumost 11 hours agorootparentWhen discussing potential alien civilizations, one can’t discount the existence of civilizations which exist on substantially more radioactive planets. If the background radiation of earth was 100x higher, would we care about an Orion launch? Or a small nuclear exchange… reply Dylan16807 11 hours agorootparentprevThe more fuel you have to pile onto the rocket, the less the weight of the engine matters. Using the chart in the accepted answer, launching with chemical engines takes 50 thousand tons at 3x gravity and 3 million tons at 4x gravity. Now consider a theoretical engine that has a 7:1 thrust to weight ratio at 1G but sips fuel. Take a 25 ton engine, strap 10 tons of fuel to it and 1 ton of payload. Watch it go to orbit on a single stage. A real NTR doesn't save nearly as much fuel, but it can still be useful in certain ranges. reply jowea 10 hours agorootparentprevI can't help but think that any species insane enough to use Orion drives in the first stage probably already found a way to blow itself up before it gets to that point. And maybe I'm taking Terra Invicta too seriously but maybe they would wait until they figure out nuclear fusion and have more options. reply hermitcrab 10 hours agorootparentprevI once got to briefly discuss project Orion with Freeman Dyson at a book signing. IIRC (it was a long time ago) he said that : - he thought it could be made to work - all big engineering projects (dams, skyscrapers etc) kill people - putting all that radiation into the earth's atmosphere couldn't be justified reply jaywee 5 hours agoparentprevNTR is a very inefficient use of nuclear fuel. What you want is a NSWR: https://en.wikipedia.org/wiki/Nuclear_salt-water_rocket A true nuclear rocket. Just like a chemical rocket is a controlled explosion, NSWR is a controlled (cough) nuclear explosion. reply SigmundA 9 hours agoparentprevNTR have high specific impulse but relatively low power to weight, this makes them good in space and poor for getting out of the gravity well as discussed here. They are efficient at using reaction mass but not for power to weight. From the article: Early publications were doubtful of space applications for nuclear engines. In 1947, a complete nuclear reactor was so heavy that solid core nuclear thermal engines would be entirely unable[23] to achieve a thrust-to-weight ratio of 1:1, which is needed to overcome the gravity of the Earth at launch. Over the next twenty-five years, U.S. nuclear thermal rocket designs eventually reached thrust-to-weight ratios of approximately 7:1. This is still a much lower thrust-to-weight ratio than what is achievable with chemical rockets, which have thrust-to-weight ratios on the order of 70:1. reply trashtensor 15 hours agoprevThe post about the 1.55R⊕ planet made me curious and I thought this was an interesting discussion reply exe34 13 hours agoparentnext [7 more] [flagged] trashtensor 12 hours agorootparentHonestly the whole thing - this is a topic I don't know very much about and the question and answers were fascinating to read. reply starttoaster 11 hours agorootparentPlease don't take this as rude, but speaking more plain and directly than the previous person you replied to: I think they were politely telling you to come up with more substance in your comment. The culture on Hacker News has historically been along the lines of, \"if your comment is not more insightful than providing an upvote on the post, withhold it and leave an upvote instead.\" This was on the \"welcome\" page when signing up for a Hacker News account, and can be found here: https://news.ycombinator.com/newswelcome.html See: > The most important principle on HN, though, is to make thoughtful comments. Thoughtful in both senses: civil and substantial. ... The test for substance is a lot like it is for links. Does your comment teach us anything? I think Hacker News comments tend to miss the mark on this test more often than not, to be fair. But it's perhaps worth at least politely pointing out when the substance test falls as far as, \"this was essentially my approval of the more substantial parts of the post.\" The irony of my comment is that I'm adding nothing to the discussion, myself. But hopefully there's some small value in reinforcing Hacker News' social ideals. reply noizejoy 10 hours agorootparent> The irony of my comment is that I'm adding nothing to the discussion, myself. Arguably by now the percentage of genuinely new information or arguments on HN is pretty low and falling. Which in a place where discussion > new knowledge is only natural. Pretty much like any place I can think of, HN is increasingly a virtual water-cooler social hangout where the participants cycle through. Just reading the thread about encryption at the top of HN as I’m writing this comment was a bit depressing, because I didn’t find any remotely novel questions, answers or lines of reasoning before giving up on that thread. The intellectual and emotional downer for me wasn’t any better than reading the shallow/empty comment that you responded to. So while I wholeheartedly concur with the sentiment you expressed, I’m also pretty much resigned to the fact, that I should expect less from HN and just move on to other parts of the net. Thank goodness, that RSS seems to be making a quiet comeback. reply trashtensor 11 hours agorootparentprevI mean I made the comment because the field is there when submitting the post, I didn't realize it was going to end up as an independent comment. Maybe the HN devs need to make it clearer what will happen if you fill out that field when a link is provided. reply Dylan16807 11 hours agorootparentOh definitely. I didn't even realize you were the person that submitted it. reply AnimalMuppet 9 hours agorootparentprevWait, what? I keep seeing these posts on the \"New\" page, with only one comment, by the submitter, with some summary comment that doesn't add much, and I always assumed that it was a sketchy way to try to attract attention to the post. So I downvoted such comments when I saw them, because such games should be discouraged. Now I feel like a real jerk. I've been downvoting innocent people who were led astray by the forum software. I don't remember any specific names, but I owe a number of you an apology... reply blackoil 6 hours agoprevWhat technological advancements would be impossible for a civilization that can't go to space? reply tester457 5 hours agoparentThose aliens have no gps and worse internet. Flight travel and shipping is more expensive too. Weather forecasting is more difficult. They never create Starlink. reply Balgair 4 hours agorootparent> and shipping I want to mention that this would only be for heavier than air based airborne shipping. Liquid based shipping is unaffected by gravity. Archimedes' principle has the buoyancy force as the weight of the displaced liquid. The gravitational effects cancel out. Also, dirigibles would be possibly more useful here as, again, gravity cancels out. Something neat I remembered, great comment all the same, thank you. reply scotty79 6 hours agoparentprevSatellites mostly. reply jokoon 11 hours agoprevmore depressing is that a space elevator might never see the day, since the material required for it is difficult to make and even if it did exist, I have no idea how that thing would be put in place if I remember, in the mars trilogy, it's assembled in high altitude, low gravity, and then put in place? but gravity is lower on mars so rockets work better? anyway, for earth, assembling a space elevator in space, meaning putting tough cable in orbit, would require so many launches and would emit a lot of CO2 in the process. also the cable might be progressively thicker starting maybe at 1/3 of the distance, to bear the entire weight of the lower cable that is the most affected by gravity, while the rest of the cable would have a progressively centrifugal force away from earth to compensate, so maybe the cable would not need to be thick everywhere. maybe that question was already asked reply dtaht 11 hours agoparentI have been pointing out for years that space elevators are feasible from a class of asteroid called a \"fast rotator\". They do not need to be very big either. reply eru 5 hours agoparentprevYou can use active support to make a space elevator without super-materials. See https://en.wikipedia.org/wiki/Space_fountain reply mxkopy 21 minutes agoprevI wonder if this was part of the inspiration for Outer Wilds, where the system’s planets are so small that they could be explored with wooden spaceships. reply simne 12 hours agoprevInterest thoughts, but forgot one very practical calculation, unfortunately not easy to calculate. I say about shock-wave, which is known from practice on Earth, and for Earth limit rocket starting mass about 10k metric tonnes at sea level. What it mean, shockwave from supersonic engine exhaust creates literally powerful pressure on construction, so on mentioned scale, nothing will withstand it long enough. If it is possible to create much stronger materials, as I know at the moment, is unknown and we cannot forecast. Sea level is important, because, at the moment I only remember TWO space rockets, which started from much different position, and high altitude (air) launch have very different atmosphere properties, which could be solution to shockwave problem (but have other limitations). https://en.wikipedia.org/wiki/Northrop_Grumman_Pegasus https://en.wikipedia.org/wiki/LauncherOne reply hermitcrab 10 hours agoparentVacuum-dwelling spherical cows are immune to shockwaves. reply hinkley 14 hours agoprevIt’s easier to build a space elevator for a low gravity planet as well. So if some day we find a species living on a heavy earth, even throwing them a rope may be difficult. Though I don’t suppose we’ll be visiting any aliens with chemical rockets regardless. We don’t have that kind of patience. reply dmix 8 hours agoprevThis thread made me curious, what's the most amount of stages a rocket has been launched with? reply idlewords 7 hours agoparentI'm guessing you mean stages to LEO, but technically Apollo was a six-stage rocket. 1. Saturn V first stage 2. Saturn V second stage 3. Saturn V third stage 4. Lunar module descent stage 5. Lunar module ascent stage 6. Service module for Earth return. reply wongarsu 6 hours agoparentprevThere are a couple of four-stage rockets. For example the Proton has a couple of four-stage variants, and India's primary workhorse, the PSLV, has four stages in all configurations. Five stage rockets are a lot more exotic. There is the Minotaur V, which was launched exactly once, and India's ASLV, which they abandoned after a couple launches due to budget issues. reply Nevermark 3 hours agoprevNow try being from a water planet, and getting to escape velocity! reply le-mark 11 hours agoprevBy the same token, a space faring civilization based from the Moon or Mars is much more feasible, and a large argument for colonizing either imo, also rarely discussed nowadays. reply jowea 11 hours agoparentSpacefaring would be much easier if we were Martians but going up, then down to colonize Mars just to start launching rockets back up from there seems mostly pointless? Isn't it much easier to colonize some asteroids, or Mars' moons? reply tooltower 4 hours agorootparentThe idea is that you can pick up cargo, fuel, and rocket-building minerals directly from Mars. reply adolph 12 hours agoprevIn Project Hail Mary one of the exoplanets is 8.45 Earth masses and the residents are able to attain space flight. https://www.reddit.com/r/ProjectHailMary/comments/s5n7j4/eri... reply namrog84 11 hours agoparentWonderful book! One of my all time favorites. Though just cause they did in a book doesn't validate the science of doing it. reply PaulDavisThe1st 10 hours agorootparentYou're suggesting that Andy Weir did not science the shit out of that? reply eru 5 hours agorootparentWell, he gave the aliens some hand-wave-y super-material called Xenonite. reply seiferteric 11 hours agoprevI wonder if air breathing rockets would change this much. reply eek2121 8 hours agoprevOoh I absolutely did not click on the link yet, but I love this question! reply eek2121 8 hours agoparent(and the answers were just as amazing as I expected!) reply kebman 12 hours agoprevThis was a delightfully weird question! I'm sure it makes sense to calculate this before landing on another planet, though. reply nntwozz 11 hours agoprevYou can escape any gravity with teleportation, but it's easier said than done. Or maybe we're just a dumb civilization/species? Maybe it's also dumb to assume our intelligence is \"normal\". reply PaulDavisThe1st 10 hours agoprevIf Randall Munroe's name is not on the answer, it's not the answer. reply SilasX 10 hours agoprevUm doesn’t balloon assistance become increasingly effective in that case? Use your plentiful surface energy blow up a balloon and float it up past the upper atmosphere. But they explicitly exclude that from this question: >For our purposes, let's not explore alternative or hybrid launch systems or boost systems (such as balloons, planes, laser beams, space elevators etc.). Just stick to chemical propellant rockets. reply PeterisP 6 hours agoparentFloating a balloon to the upper atmosphere doesn't make a meaningful difference in escaping the planet, it saves you a few percent of the energy but you still have to do most of the work to bring it up to escape velocity. Going to space isn't about getting high, it's about getting fast. reply eru 5 hours agorootparentThough you might be able to get past a substantial portion of the atmosphere, and that would help you get past a lot of sources of friction. Getting off a planet, even a heavy one, that doesn't have an atmosphere would be relatively easier, because you could 'just' build very long, flat rails to accelerate along. reply m3kw9 10 hours agoprevUnless you are in a black hole you can get off any planet theoraatally reply corn13read2 5 hours agoprev [–] And now let’s break all the numbers by mentioning it’s likely the aliens are not dumb enough to make rockets so inefficient for their task. Nuclear at minimum would be used. reply dalyons 5 hours agoparent [–] Do you have an explanation for how nuclear would actually be better? As far as I understand it’s terrible for orbital insertion levels of thrust. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author examines the constraints of rockets and space travel in high-gravity environments, considering factors like rocket size, mass, and engine thrust-to-weight ratios.",
      "There is a suggested limit to the gravitational strength of habitable worlds, beyond which it becomes more challenging to achieve the necessary escape velocity using conventional chemical rockets.",
      "The passage also highlights the potential for alternative launch systems and emphasizes the importance of funding and materials in considering space exploration options."
    ],
    "commentSummary": [
      "The space-focused forum discussion covers various topics related to space travel, including the size limit of Earth for effective rocket launches and challenges associated with rocket propellants and orbital rockets.",
      "It explores the importance of technological advancements, limitations faced by civilizations without access to space mining or detailed aerial maps, and the potential benefits of space travel in low Earth orbit.",
      "The conversation also delves into topics such as quantum particle movement, building tall structures on high-gravity planets, launching rockets from high points, atmospheric composition and magnetic fields, rarity of intelligent life in the universe, challenges of interstellar travel, and nuclear propulsion and alternative space travel methods."
    ],
    "points": 345,
    "commentCount": 182,
    "retryCount": 0,
    "time": 1706985380
  },
  {
    "id": 39241448,
    "title": "Quora's Decline: AI-generated Content and Lack of Moderation Trouble Users",
    "originLink": "https://slate.com/technology/2024/02/quora-what-happened-ai-decline.html",
    "originBody": "USERS How Quora Died The site used to be a thriving community that worked to answer our most specific questions. But users are fleeing. BY NITISH PAHWA FEB 02, 20245:45 AM Illustration by Slate TWEET SHARE COMMENT “Why Do So Many Music Venues Use Ticketmaster?” “What’s It Like to Train to Be a Sushi Chef?” “How Do Martial Artists Break Concrete Blocks?” If you were looking for answers to such questions 10 years ago, your best resource for finding a thorough, expert-informed response likely would have been one of the most interesting and longest-lasting corners of the internet: Quora. Most people have encountered Quora in some form, whether they know it or not: in Google search results, in writing samples from famous authors, or perhaps in reprints of certain Q&As in digital publications, like Slate. One of those Slate-via-Quora contributors, author and researcher Erica Friedman, joined the site back in 2011, when it was “starting to get a little bit of new traffic” thanks in part to Yahoo Answers’ decline in reliability and activity. This, she said, allowed Quora to stand out as an accuracy-focused, knowledge-centric text platform. That was a unique offering in an age when Facebook and Twitter were coming to dominate the social internet, and YouTube was doing its own thing. Friedman was so enamored of the quirky Q&A monolith that she—and many, many others—contributed answers for free. “There was a period of time, for a number of years in the mid-2010s, that a lot of us were really dedicated to a particular mission,” she said. “That was: ‘Let’s make Quora the place on the internet that says you can’t be a jerk here. Let’s put those policies into action, and let’s make it impossible for people to come here in bad faith and act in bad faith.’ ” A smart and passionate community dedicated to maintaining a positive and affirmative space where the most curious netizens could gather—what sounded more ideal than that? No wonder Quora had such a growth spurt in the 2010s. ADVERTISEMENT Today’s Quora, however, hardly meshes with those utopian aims. The once-beloved forum is now home to a never-ending avalanche of meaningless, repetitive sludge, filled with bizarre, nonsensical, straight-up hateful, and A.I.–generated entries along with a slurry of all-caps non-questions like “OMG! KING CHARLES SHOCK the WORLD with ROYAL BAN ON PRINCE HARRY AND MEGHAN MARKLE. SAD?” (The answer to this “question,” which garnered about 7 million views, links to a bizarre, barely functional royals-watching website called red-carpett.com.) Whereas once you could Google a question about current events and find links to thoughtful Quora answers near the top of the results, you’re now more likely to come upon, say, a bunch of folks asking in the year of our Lord 2024 whether the consistently racist Donald Trump is, in fact, racist. Or, maybe, the featured Google snippet will tell you that eggs can melt, thanks to a nonsense Quora answer caught in the search crawler. this is actually hilarious. Quora SEO'd themselves to the top of every search result, and is now serving chatGPT answers on their page, so that's propagating to the answers google gives the internet is dying pic.twitter.com/gcV9b36vEA — Tyler Glaiel (@TylerGlaiel) September 25, 2023 ADVERTISEMENT Quora’s still-strong SEO has only brought more attention to the issue. Commenters across a variety of forums have bemoaned Quora’s downgrade in quality, and the Atlantic recently asked, “If There Are No Stupid Questions, Then How Do You Explain Quora?” Just scroll through the “Insane People Quora” subreddit if you’d like more examples of this pronounced decline in quality. Quora’s shrinking utility isn’t due entirely to A.I.: Longtime writers cite issues with moderation and functionality that started well before the ChatGPT era. But its decline has been accelerating—much to the chagrin of the uniquely attached and now-fraying community—with the rise of this new knowledge broker. Earlier this month, the A.I.–accelerationist venture capital hub Andreessen Horowitz blessed Quora with a much-needed $75 million investment—but only for the sake of developing its on-site generative-text chatbot, Poe. The early advantage Quora had over the (many) other Q&A sites in the late 2000s was that it was designed with social networking in mind. Co-founders Adam D’Angelo and Charlie Cheever were both early Facebook employees who quit in 2009 in order to build a website where, as they told TechCrunch at the time, “we’re trying to get information out of people’s heads, so it’s not on sources that are hard to access on the internet, and get it into a really useful format to make a valuable database.” Their plan was to persuade experts in specialized fields to share their insights with knowledge-seekers and, from there, build a vibrant community around this free exchange of authentic information. “There were a lot of high-quality answers from people who wanted to just share their experiences,” said Friedman—a feature that stood in stark contrast with Yahoo Answers, which “never built up that community.” ADVERTISEMENT Ariel Williams, one of Quora’s first 500,000 members, agreed. “You had Yahoo Answers, and the quality was horrible: People would write a question and somebody would just say something disgusting,” she told me. “Quora had a focus on quality, and they were looking for quality answers, they were looking for quality questions, there was active moderation, and the whole site was set up around the people, around the users.” It didn’t take long for experts like Stan Hanks, best known as a pioneering network engineer who built the first IP virtual private networks, to show up. In late 2012, he told me, he would log on to Quora, and “there would be something where I had personal experience where I knew the people involved, where I had backstories, and it would just light me up, and I would just write.” To keep the volunteer experts happy, Quora built out its perks. The company established a Top Writer program for Quora’s best and most fervent answer-writers, built a system that incentivized thoughtful discussion, and even invited these happy Quorans the opportunity to hang out at summits held at company headquarters. “The Top Writer program spanned from 2012 to 2018,” said Williams, herself a Top Writer in an elite crew of just a few hundred Quorans. “There was a physicist that had worked with Freeman Dyson. There were people that worked at NASA. There were people with doctorates.” ADVERTISEMENT There was also robust human backup for all these writers. “There was a moderation team, a review team, and a support team,” said Hanks. “Full-time paid moderators were employees of Quora, and part-time moderators had other jobs like writer engagement.” There were paid community managers who enforced a basic standard of BNBR (“Be Nice, Be Respectful”), customer service support staff, and a department for cross-publishing particular Q&As to websites like Forbes and HuffPost. The social network racked up hundreds of millions of page views, raised millions of dollars from cash-happy investors, and carved out a corner of the internet distinct from Wikipedia, Reddit, or Facebook. It may not have had the ubiquity and fame of those sites, but that was fine—everyone touching Quora knew what it stood for. But even then, there were issues plaguing Quora that would continue to fester. First, an anonymous former Quoran told me, the site started “shortening the length of questions.” The professed reason was to increase Quora’s visibility on Google, but that brevity came with a cost: It also made it difficult for users to ask the types of complex questions that could be addressed by specialists, including extremely specific business-related queries of the type Hanks would answer. (For example: “How much equity should I get as a co-founder to build a startup from scratch? I’ve been offered 10%, subject to dilution, with a typical CTO salary. The company hasn’t started yet, they don’t have a prototype. 10% for a CTO is very low. What is fair?”) ADVERTISEMENT Screenshot from Quora Then there was what former Top Writer J. Starr (who has since deleted her account) characterized to me as “optimizing the feed.” At first, when a user signed in to Quora, they saw what they had come there to see: questions to be answered. But soon, the site started “putting ‘content’ articles into everyone’s feed,” flooding the site with articles that were, according to Starr, “just dreck, gossipy-rag articles about Hollywood figures.” A grim precursor, it seems, to the current-day all-caps PSAs around King Charles. There were early problems with ads and bots too. When Quora started putting ads on the site in 2016, Williams and other Top Writers suggested that there be some sort of creator revenue-sharing program, she told me. As a result, higher-ups created “the Quora partner program, which I joined myself,” Williams said. But that “was all about trying to come up with questions that would draw in more views and more people,” she said—not about incentivizing high-quality answers. It was all about adding webpages of individual questions, for SEO purposes. ADVERTISEMENT And Quora “partners” weren’t the only ones being recruited to this task—the site also attracted bots that would pull questions from Reddit threads onto Quora pages. They weren’t the type of questions the Quora community was looking for. “You would have people creating bots with templates of ‘What are the best places to eat at,’ and then it would put in city names, state, country names,” Williams said. (Redditors on r/Quora began to notice and complain about this practice.) “The quality of the writing had taken a back seat to a pure volume of traffic,” said Bethann Siviter, another former Top Writer. “With the partner program, it became clear that quantity meant more than quality. You could report people over and over, and nothing happened.” Even though the partner program didn’t earn participants all that much money (maybe a few thousand dollars at most), the people who made bots soon learned that this was the easiest way to cash in quickly. This was all downstream of the fact that Quora was also slow to monetize, and it didn’t help that the site, popular as it was, was aiming for pieces of the same digital-advertising pie that was rapidly overtaken by Facebook and Amazon (as well as its No. 1 traffic referrer, Google). Investors hesitated to continue pouring cash into Quora in light of its steep expenses. So, the company slashed budgets, shrinking the moderation teams, the customer support apparatus, and the Top Writer initiative. Other originating features—the requirement that Quorans use their real names, “Suggested Edits” that readers could propose for answers that had mistakes or typos—were also cut. All these choices led to more unchecked spam and a deluge of trolls, which volunteer users could not hold off on their own. ADVERTISEMENT Related From Slate ANNIE RAUWERDA On Wikipedia, Anyone Can Be a Model READ MORE Nelson McKeeby, an author who joined Quora in 2013, said that things got worse in a post-Gamergate internet, with alt-right, Trump-loving trolls invading online forums, aided by anonymity and weakened moderation. “When real users tried to take down demonstrably false answers, troll farms with multiple servers were able to overpower users,” he wrote to me in an email. Further, as Quora launched “Spaces”—basically, private user–run community blogs—problems with moderation continued to spiral, with ugly ideologies running rampant. Loyal Quora users attempted to report bigoted, transphobic, and obscene content without adequate support from the company. And then came A.I. As the spamming bots got even worse, Quora changed the terms of service and did away with BNBR, then automated the moderation process. Needless to say, it did not improve the situation on the site. Nor did the integrated A.I. chatbots offer good questions or answers. Instead, they “made up some really generic, and oftentimes first-grade-level questions,” wrote user Steven P. Robinson in an email. “It is a good example of A.I. not being ready for prime time.” Now Quora is even offering A.I.–generated images to accompany users’ answers, even though the spanwed illustrations make little sense. Screenshot from Quora Screenshot from Quora To top it all off, after Quora began using A.I. to “generate machine answers on a number of selected question pages,” the site made clear the possibility that human-crafted answers could be used for training A.I. This meant that the detailed writing Quorans provided mostly for free would be ingested into a custom large language model. Updated terms of service and privacy policies went into effect at the site last summer. As angel investor and Quoran David S. Rose paraphrased them: “You grant all other Quora users the unlimited right to reuse and adapt your answers,” “You grant Quora the right to use your answers to train an LLM unless you specifically opt out,” and “You completely give up your right to be any part of any class action suit brought against Quora,” among others. (Quora’s Help Center claims that “as of now, we do not use answers, posts, or comments added to Quora to train LLMs used for generating content on Quora. However, this may change in the future.” The site offers an opt-out setting, although it admits that “opting out does not cover everything.”) POPULAR IN TECHNOLOGY One of Our Best Websites Died While No One Was Looking What Flight Safety Experts Say About Flying Right Now It’s Surprisingly Easy to Live Without an Amazon Prime Subscription I Regret to Inform You That Marjorie Taylor Greene and Tucker Carlson Are Right About Zyn This raised the issue of consent and ownership, as Quorans had to decide whether to consent to the new terms or take their work and flee. High-profile users, like fantasy author Mercedes R. Lackey, are removing their work from their profiles and writing notes explaining why. “The A.I. thing, the terms of service issue, has been a massive drain of top talent on Quora, just based on how many people have said, Downloaded my stuff and I’m out of there,” Lackey told me. It’s not that all Quorans want to leave, but it’s hard for them to choose to remain on a website where they now have to constantly fight off errors, spam, trolls, and even account impersonators. Quora is far from the only digital community to face an existential battle for its identity in the age of A.I.—Reddit and Google are facing related, if slightly distinct, concerns. The tragedy of Quora is not just that it crushed the flourishing communities it once built up. It’s that it took all of that goodwill, community, expertise, and curiosity and assumed that it could automate a system that equated it, apparently without much thought to how pale the comparison is. McKeeby has a grim prediction for the future: “Eventually Quora will be robot questions, robot answers, and nothing else.” I wonder how the site will answer the question of why Quora died, if anyone even bothers to ask. TWEET SHARE COMMENT Artificial Intelligence Facebook Google Internet Internet Culture",
    "commentLink": "https://news.ycombinator.com/item?id=39241448",
    "commentBody": "How Quora died (slate.com)323 points by CharlesW 18 hours agohidepastfavorite358 comments ethbr1 17 hours agoIf your core business product is content contributed by users... I'm turning to the opinion that you need a C-level ombudsman- / dean of contributors-type role. Whose sole job it is to (1) figure out what contributors are thinking and want & (2) advocate for them inside the company. Every contributor-driven platform has eventually jumped the shark, and all in exactly the same way. Management begins to take contributions for granted. Stops caring about attracting contributors. Then focuses on revenue. Then makes changes to the platform that kill contributions, in pursuit of revenue. And they miss so many obvious ways to placate and delight their contributors. \"It'd be nice to have a mod tool that does X\" shouldn't be a 3-year back burner ask. Maybe make it harder for your company to footgun your golden goose? reply jfengel 17 hours agoparentQuora had one, William Gunn. He was laid off, along with the entire human moderation staff and practically all of the user-facing developers. Quora had been going downhill for quite some time before that, as they realized they had no idea how to monetize the content and were grasping at straws. But that was the point where they appear to have pivoted entirely towards AI and continued the human generated content side on minimum life support. reply RajT88 17 hours agorootparent> they realized they had no idea how to monetize the content This sort of thing should be a nonprofit. When it was good, it was literally making the world a better place. Possibly the Library of Congress would be a good steward for such a platform. Something to make it resistant to enshittification. reply allenrb 16 hours agorootparentAgreed re: being a non-profit. My great hope was that Elon might do that with Twitter. Needless to say, I was… not right about that. reply ethbr1 16 hours agorootparentArguably, prelon Twitter was not-for-profit. reply marcus0x62 12 hours agorootparentPost Elon, they are not-turning-a-profit. reply miohtama 10 hours agorootparentThey will, but it would likely need restructuring the debts and the cap table, and would not be Elon's Twitter anymore. reply marcus0x62 9 hours agorootparentYes, if the business was different, it would be different. reply prepend 11 hours agorootparentprevAre their losses greater or less than pre purchase. I know revenue is down, but I wonder how the cost cuts affected margin. reply simonw 11 hours agorootparentThey made a profit in 2018 and 2019 ($1.2bn and $1.45bn according to SEC filings) but not in any other year. reply Aeolun 8 hours agorootparentPresumably they could have done Elon’s massive firing spree at any point in time and become profitable overnight? reply StackRanker3000 10 hours agorootparentprevDon’t forget the massive amount of new debt that needs servicing. reply slily 8 hours agorootparentprevGood to know a publicly-traded company known to censor content to be more advertiser-friendly and with notoriously predatory content discovery algorithms designed to elicit emotions to keep users coming is apparently not-for-profit if it happens to be unprofitable during the ZIRP period. Or is Facebook also not-for-profit? If anything, Musk is less profit-oriented. Someone looking to profit off the platform wouldn't be actively driving off advertisers. But I suppose that because we must all believe that Elon Bad, he must also be the evilest capitalistest person in the whole world, and everything before him was sunshine and roses. reply latexr 7 hours agorootparent> known to censor content to be more advertiser-friendly And now it’s know to censor content to be friendly to autocratic governments. https://www.vice.com/en/article/88xqnv/elon-musk-censors-twi... reply slily 6 hours agorootparentObviously your post has nothing to do with Twitter being for-profit before the acquisition and Musk prioritizing profit less than his predecessors, but I'll bite. By complying with government regulations when displaying content in their respective regions to avoid getting the entire network banned there, right. While this hit piece from a notoriously biased outlet would like to equate this with ye olde Twitter's regular practice of of suppressing or deleting content worldwide at the whim of the US government, it's obviously more transparent and fair to comply with censorship locally and provide a reason for the missing content. Reeks of \"it's okay when we do it\". reply jacquesm 15 hours agorootparentprevThanks for the laugh. reply Luctct 16 hours agorootparentprevnext [3 more] [flagged] hollerith 16 hours agorootparentWikipedia is non-profit and so is https://theguardian.com. Do you LOL at them, too? In fact till mid-1993, with a few unimportant exceptions (Clari-Net, early \"retail\" ISPs like Netcom) the entire Internet, which was already the largest network of computers by a comfortable margin, was non-profit. reply Luctct 12 hours agorootparentWikipedia only pretends to be non-profit and Wikipedia didn't buy anything for $4 billion. reply edgyquant 3 hours agorootparentprevHow exactly was quora ever making the world a better place? reply fullshark 17 hours agorootparentprevHow about Wikimedia? reply ivanmontillam 17 hours agorootparentWikimedia is a non-profit that operates like a for-profit when the yearly round of donations comes around. They literally hoard money[0]. I will get downvoted for this, but it's my perception. -- [0]: https://www.makeuseof.com/tag/wikipedia-millions-bank-beg/ reply _Algernon_ 16 hours agorootparent>I will get downvoted for this, but it's my perception. I will always downvote a comment when someone writes something like this. It is irrelevant to the conversation and screams insecurity. reply gn4d 7 hours agorootparentBased. It is r*ddit-tier passive aggression. This place is not much better, but we should still resist this type of crap when we can. reply SantalBlush 12 hours agorootparentprevIt's also tinged with a \"People can't handle the based truth I'm laying down\" attitude, which is annoying to read. reply njharman 13 hours agorootparentprev100% reply Karellen 11 hours agorootparentprevOne person's hoard is another person's endowment. To-mah-to, to-may-to. https://en.wikipedia.org/wiki/Financial_endowment reply seadan83 16 hours agorootparentprevInteresting reference. Non profits cannot hoard money. I believe it becomes taxable and they can lose their non profit status (i am not an expert) Though, hoarding seems like a mischaracterization. Per the article linked, the cash burn rate is on the order of $100M/yr, having $150M in the bank is 18 months worth of funding. The biggest gripe I read in the article is the \"high\" expenditure rate and how necessary it is. It seems like reasonable people may disagree on whether that spend rate is excessive. If the expenditure rate were lower, I'd agree it would be hoarding. reply lupire 14 hours agorootparentCitation needed for your claim about hoarding money, which is refuted by simple observation of the many, many endowments that exist. reply HillRat 12 hours agorootparentprevThere are no limits on a nonprofit's ability to raise and maintain cash reserves; there are limits on how and to whom funds can be disbursed and (to a lesser extent) the kinds of activities that can be used to generate funds. But a nonprofit can sit on an endlessly-growing hoard of cash if that's what they (and their donors) want to do. reply e_y_ 6 hours agorootparentprevI think the distribution requirement is specifically for private foundations, not public charities. reply fallingknife 16 hours agorootparentprevI guess I can't see the reasonable argument that it is necessary when you look at the growth rate of their spending. https://en.m.wikipedia.org/wiki/User:Guy_Macon/Wikipedia_has... If they kept expenditures low and hoarded, I would actually be fine with that and happy to contribute. I see nothing wrong with forming a large endowment for a project like Wikipedia. reply rospaya 16 hours agorootparentprevI'm fine with that. reply FireBeyond 14 hours agorootparentprev> the yearly round of donations comes around The what? I cannot visit Wikipedia without seeing a donate bar, even when I close it every time. \"Year-round\" maybe. reply BlueTemplar 16 hours agorootparentprevWikimedia is not a platform. Their sister Wikicities / Wikia / Fandom (also relying on MediaWiki), and now owned by Texas Pacific Group, is however, another example of a platform getting enshittified. reply boringuser2 10 hours agorootparentprevYou're going to entrust making the world a better place to... the US government? reply stevage 10 hours agorootparentSure. Much better than to VC backed startups anyway. reply Solvency 8 hours agorootparentLike when the sugar industry lobbied and the US government told everyone fat is bad for 50 years? reply ruszki 1 hour agorootparentDid the government say that sugar is good? reply boringuser2 9 hours agorootparentprevWe're going to have to agree to disagree. The US has done terrible things in the too-recent future in the explicit name of making the world a better place. At least, that rationale triggers my gag reflex. reply Aeolun 8 hours agorootparentIsn’t a lot of what the US government does decided by capital anyway? reply boringuser2 8 hours agorootparentKind of. There is a lot of money in lobbying for certain things. Companies generally don't want political outcomes per se via lobbying outside of their own niche interests. Much more impactful are dedicated lobby groups. For exmaple, there's a particular country that I'm not allowed to mention here under duress of being banned that has a lot of moneyed lobbying. reply choppaface 11 hours agorootparentprevMarc Bodnick was the original “community manager” and frequent contributor. He’s now working on his own social net, the first-ish of which (Telepath) already flopped. reply throw__away7391 13 hours agoparentprevI have no experience running a site centered on user generated content, but it seems to me that the biggest problem/threat to any of them is the people who start showing up once it gains traction and becomes more popular. Early users are a different band of the population, on every successful platform from StackOverflow to Uber they create a particular culture which is impossible to maintain as the general public arrives in large numbers. reply jonathanyc 9 hours agorootparentA StackOverflow question I asked 13 years ago (and which someone else answered 13 years ago!) was deleted just last month for being off-topic. All the mods’ accounts were years younger than both the question and answer. Even weirder, the mod who initiated the deletion first tried to answer the question. He only voted to delete it after I commented that the question already had had an accepted answer for 13 years. reply musicale 2 hours agorootparentStackOverflow seems to have perverse incentives for bad moderation. Fortunately it's so terrible I was easily dissuaded from ever answering anything. reply Aeolun 8 hours agorootparentprevThere should be something you can do about such blatantly misaligned incentives… reply meowtimemania 12 hours agorootparentprevIn what way were early users of Uber different from the current users? Are you saying the drivers used to be better? reply throw__away7391 11 hours agorootparentTo me personally, no, not better, but at one time the phrase “ride share” had a lot more meaning than it does today. People would creatively decorate their cars for example. reply tomjakubowski 6 hours agorootparentprevNot sure about what OP was saying but, for a brief time when Uber was still new, before UberX launched in mid-summer 2012, their only service was black cars. Drivers had limousine licenses, and were merely filling time in between other gigs they had booked traditionally. reply anigbrowl 9 hours agorootparentprevA car ride isn't 'content'. reply johnnyanmac 5 hours agorootparentIt can be. Have you been in cars that have small snacks, karaoke, or some other sort of entertainment setup? reply kiba 17 hours agoparentprevPerhaps VC backed businesses aren't really ideal for these type of sites. reply manquer 9 hours agorootparentYouTube does very well ? . reply asveikau 17 hours agoparentprev> Every contributor-driven platform has eventually jumped the shark, and all in exactly the same way. Do you think Wikipedia has jumped the shark? Obviously not perfect, but it's certainly a lot better than Quora in terms of accuracy. It's been around for a very long time and it doesn't seem to me like quality has decreased. Edit: to be clear and re-iterate, I never said Wikipedia was perfect, just that it has a certain baseline, generally above where Quora is, and hasn't seemed to decline over the years. reply throw310822 16 hours agorootparentA lot of Wikipedia entries have become extremely partisan, and kept that way by moderators who are deeply invested in certain narratives. I find it very disappointing that no countermeasures have been taken- afaik Wikipedia still works exactly like 20 years ago, a barebone wiki governed by a lot of obscure, complex and unstructured politics. Granted, it might be the best possible way for an open-source encyclopedia to work. It is after all an incredible success. It's just pretty bad in some parts. reply apexalpha 16 hours agorootparentCould you point to some examples of partisan entries? reply throw310822 16 hours agorootparentAs one example, I wrote this comment a few weeks ago: https://news.ycombinator.com/item?id=38974614 But to expand a bit, for example I'm annoyed at many of the entries calling their subject \"conspiracy theory/ theorist\". Not that conspiracy theories and their believers don't exist, but at this point it has become a highly pejorative and judgemental term to frame ideas and narratives that need to be stigmatized rather than explained. Passing judgements should not, in my view, be the primary focus of an encyclopedia entry. reply emj 14 hours agorootparentSo climate change, is it surprising the thread went bonkers? https://en.wikipedia.org/wiki/Susan_J._Crockford What ever I answer in this case the easiest reaction will be; for you will be to judge me and lump me together with what you cal activists, and for those activists it will me easiest to put me in the same conspiracy theorist group as you. This is not a problem with Wikipedia, this is a human problem. While I also think that article is abysmal, it does give context what to expect from people posting stuff by her. The biggest problem with your complaint is that you are not linking to an alternative, just do a fast draft on Wikipedia remove everything that you feel is not relevant and link that. Sure it will be reverted but that is probably because you first draft will not be a good article, those are evidently very hard to write when people think so differently about something. reply mistermann 2 hours agorootparent> This is not a problem with Wikipedia, this is a human problem. Wikipedia could be an excellent attack vector for sorting out human's heads at scale. reply wonderfulcloud 14 hours agorootparentprevEssentially any issue that relates to politics or geopolitics tends to have a strong left wing bias, as if it is written from the perspective of the left wing against the right wing. reply Karellen 11 hours agorootparentThen again, reality has a well-known liberal bias. ;-) reply edgyquant 3 hours agorootparentNot saying you do, but that certain folks believe this shows just how deep the capture of mainstream media and education by the left goes. reply johnnyanmac 5 hours agorootparentprevAt least, the internet does. Providing labor el gratis apparently goes against their purview, after all. But they sure will complain about free labor reply ffgjgf1 7 hours agorootparentprevSo it would be centrist/moderate then, certainly not leftwing? reply function_seven 6 hours agorootparentNot if the Overton Window is out of whack. Reality could conceivably be to the left or to the right of what we consider the extremes. What we label as “centrist” isn’t guaranteed to be the “actual” center of reality’s spectrum of ideology. reply kikokikokiko 10 hours agorootparentprevnext [2 more] [flagged] Apocryphon 9 hours agorootparentActually, numbers run on a libertarian-communitarian spectrum reply Angostura 11 hours agorootparentprevOr, your Overton window has shifted. reply Aeolun 8 hours agorootparentprevPerhaps unsurprisingly left wing people feel more like being unpaid editors on Wikipedia. Right wing people are too busy earning them sweet, sweet dollars. reply oxfordmale 16 hours agorootparentprevWikipedia is great for non political, non-controversial topics, such as sciences. I wouldn't trust it on active politicians or other figures of public interest. One major problem is that you need to submit references to back up any claim. PR companies can easily buy an article in a fringe news site and them modify Wikipedia with a reference to that article. It is almost impossible to roll back. I once tried to update a company page to state they were going through redundancies. I was personally affected by this, and had internal emails to back this up. However, lacking a public reference it was rolled back. reply multjoy 16 hours agorootparentWhich is how it works. It is an encyclopaedia, not a breaking news site. reply oxfordmale 16 hours agorootparentThe problem is that PR companies can push out whatever they want, where as an ordinary individual with personal knowledge of a topic, can't. It means many Wiki articles on person or companies, are often glorified advertisements, rather than encyclopedic entries. reply multjoy 13 hours agorootparentA blog post can be sufficient reference. The point is that Wikipedia isn't original research. You updating the page on the basis of personal knowledge isn't the way it is supposed to work. reply dlahoda 10 hours agorootparentI added Dimon as short name for Dmitry in RU, referenced YT video with 40M views were Dimon was short name for Dmitry, video was named \"He is not Dimon\" for person with official name Dmitry, who was prime minister of Russia. I am Dmitry too btw, I know Dimon is also in Belarus. Was reverted as that was not reference to some book or sciene article. Never contributed after that. May be one or 2 graph theory improvements. Wiki denies realitity. And for math i use only 10% of my math requests. reply ethbr1 9 hours agorootparentThat seems to usually root cause to asshole editors. - Thing done - Editor makes a judgement call - Turns out to be a bad call - Editor refuses to admit mistake - Things escalate reply oxfordmale 12 hours agorootparentprevYes, I am not necessarily disagreeing with this approach. The problem is the imbalance. A PR company can buy a news article, link it and update a Wikipedia page. It means the balance is in favour of big corporations and wealthy individuals. reply stevage 10 hours agorootparentElsewhere in this thread people complain that the balance is all left wing. reply johnnyanmac 4 hours agorootparentcorporations are bipartisan pretending to be partisan. I'm liberal and don't doubt liberal-facing companies can and do do this. reply astrange 9 hours agorootparentprevNobody writes blogs anymore, so that doesn't seem like it'd help. reply ethbr1 16 hours agorootparentprevI've thought a two/three-lens Wikipedia split was needed to handle this, because each brings its benefits. I remember when nascent Wikipedia had user-contributed content, and the niche articles were way more interesting and detailed. Though at the cost of inaccuracy. Now, the submarine PR being backfed into it makes cited content pretty beige. Something like (1) a non-reader-visible, upstream Wikifacts + (2) a community-driven, laxer Wikiprototype + (3) an authoritative, cited Wikipedia. reply Theodores 12 hours agorootparentprevActually it is really hard if you work for a company and you need some amends on Wikipedia. Imagine your quite large company has a small guitar shop on the other side of the globe that has the same name. You just want a disambiguation so nobody thinks your company is this little guitar shop. You can't just edit it yourself, you need to find a Wikipedia expert and bribe them to somehow help with your plight. It is easy to imagine that a vast PR company will wave a magic wand, but it is not always like that. reply oxfordmale 10 hours agorootparentJust a few links to companies that advertise to clean up your company profile on Wikipedia. https://www.wikiexpertsinc.com/get-free-wikipedia-consultanc... https://www.legalmorning.com/cleaning-up-a-company-wikipedia... reply piperswe 9 hours agorootparentAnd a writeup on how those companies are usually scams: https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2... reply kranke155 12 hours agorootparentprevWikipedia is extremely hard to edit these days, and some of the other language Wikipedias are dominated by ill intended accounts. The Portuguese language Wikipedia is censored by mods that work for local politicians in Portugal. It’s virtually impossible to add corruption cases to their page. So now you have this bizarro world where English language pages on Portuguese politicians have corruption scandals but Portuguese language do not. reply stevage 10 hours agorootparentEnglish Wikipedia is just as easy to edit these days as ever. reply araes 14 hours agorootparentprevI like Wikipedia and think it is still one of the better user contribution sites on the web. However, having been a long time contributor to [Current Events](https://en.wikipedia.org/wiki/Portal:Current_events) (fought over because of news relevancy), there have been waves of sock puppeting and attempts to constantly revoke all edits from certain users, that often make you not want to contribute. There is also this editorial talking about spending that makes some arguments about current issues with Wikipedia. Namely, that there's a lot of money going in, and not a lot of requested feature development. https://en.wikipedia.org/wiki/User:Guy_Macon/Wikipedia_has_C... reply TulliusCicero 13 hours agorootparentprevYeah, it's probably more accurate to say > Every for-profit contributor-driven platform has eventually jumped the shark reply esafak 8 hours agorootparentprevWikipedia is doing fine because it is not advertising supported. reply thriftwy 16 hours agorootparentprevhttps://news.ycombinator.com/item?id=39019573 An amusing discussion where the OP says Wikipedia is here to stay, and every answer is downvoted, most of them flagged. All of them voicing concerns of some validity. This is super representative of that happens on Wikipedia. reply asveikau 15 hours agorootparentI have show dead enabled here. There is a shocking amount of irrational negativity in that thread, even for HN. reply thriftwy 13 hours agorootparentIt is well-known that people radicalize when they are excluded from political process. Wikipedia is notorious for excluding ordinary contributors from the process. Anybody can revert your change and that takes precedence over what you did, and there is no obvious appeal process for you, but de facto there is for the well-connected long-time editors. They can always gather some cavalry and run you over. So they should probably do what Stack Overflow tried to do, and explicitly say that new users / infrequent contributors have to receive more care, that needs to be provided by veterans / frequent contributors, as well as provide mechanisms to do so. Otherwise it is not a wikipedia that everyone can edit, rather a wikipedia written by a cabal who is also contains a large number of biased people, often getting direct or indirect funding from maintaining that bias. reply stevage 9 hours agorootparentStackOverflow also raises the bar for becoming a contributor. You can't start commenting until you have some upvotes on answers or questions for instance. I think these things turn out to be a bit important. reply musicale 2 hours agorootparentStackOverflow simply discourages contribution. Which saves me time that I might otherwise spend submitting thoughtful answers on StackOverflow. reply bbarnett 17 hours agorootparentprevIt's only accurate if the view espoused in the page, matches what the gatekeeping moderator believes. reply eastbound 16 hours agorootparentprevWikipedia is in the business of paying secret writers for content to sway opinions, politically. Of course they have jumped the shark, they’re basically working for agencies. reply sp332 16 hours agorootparenthttps://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2... reply edgyquant 3 hours agorootparentSurely we can trust Wikipedia about this topic /s reply binkHN 12 hours agoparentprevSounds a lot like Reddit. reply scoofy 11 hours agoparentprev>Maybe make it harder for your company to footgun your golden goose? As someone who has started a \"content contributed by users\" website recently (golfcourse.wiki), I've thought long and hard about why the enshitification creep happens. I really think it happens because most of the people who start these businesses either need to take venture capital to survive, or are looking for a way to sell the business to retire rich af. I thought long and hard about a monetization strategy, and I've got a few in my mind that don't suck. However, to achieve those goals, the project has to stay a side project and it has to run on a shoe string budget. Call me an optimist, but in the long run, I think we will replace our enshitified websites with more open ones, the slow but steady growth fediverse shows this is happening, it's just that it won't happen fast enough for most of us to be satisfied consumers. I just think we are looking at this through the lens of consumers, not generous creators. I occasionally help edit Wikipedia, and it looks like about 10,000-to-1, at best, people who contribute to my dumb little site. If we're all willing to waste a bit of time we can build some pretty cool sites, but there always needs to be a way for users to capture that good if the company turns evil instead of the company trying to capture the users for profit. I see it as, well, a sort of mexican-standoff relationship that Wikipedia has built for itself (for lack of a better term). Basically, we won't enshitify the website because you'll just copy-paste it, but you won't copy-paste it because you know it's a waste of time right now. If you don't need to squeeze revenues, that's a very good relationship for long-term success. reply physicles 6 hours agorootparentEveryone is talking about enshittification these days, but hardly anyone is doing anything about it (including myself). Thanks for doing something about it. Do you have a blog post with your thoughts on monetization? reply BlueTemplar 16 hours agoparentprevWhat would be a platform that is NOT contributor-driven ?? reply ethbr1 16 hours agorootparentFacebook: social over random content. Streaming services: commissioned content. Arguably non-monetized YouTube: where its used simply for its utility of serving video. reply BlueTemplar 13 hours agorootparentYes, but all of this \"content\"* comes from contributors, who else ?? Streaming services (you mean 5he likes of Netflix ?) are arguably not platforms, because they are much closer to distributors or even publishers (rather than just editors, sometimes), with all the extra legal issues and contract-signing that come with that. *\"content\" is really corporate-speak, I would hate my works to be called that : https://craphound.com/content/Cory_Doctorow_-_Content.html#1 reply samstave 16 hours agoparentprevIts almost as if user-created-content sites are un-MBA-able... in the way that MBAs are cloned to measure only certain metrics in a soulless gaze toward a self-exit from the growth hocky stick launching pad they are attempting to build on bad faith and anti-patterns... Reddit is a cluster that manages to ONLY keep going because of how agressive mods can be about their positions, but as we have seen time in, time out, MODs positions get molested by ADMINs and ADMINs posing as MODs. MODs on the take. The point being that when Reddit IPOs - there are undoubtedly mods who have/are/will-be compensated/profit when IPO hits. reply raydev 8 hours agoparentprev> Every contributor-driven platform has eventually jumped the shark, and all in exactly the same way. Reddit seems to be the exception to the rule. I and millions of others still spend a ton of time there. People have loudly griped about ads and API pricing, meanwhile I'm still finding new interesting subreddits. They seem to have monetized without destroying the UX. I hope the forthcoming IPO doesn't ruin this comment! reply edgyquant 3 hours agorootparentlol, lmao even. Reddit is literally the worst website on the internet and has slid so far downhill as to be nothing but a place to see the latest astroturfed memes. Any interesting discussion is buried underneath regurgitation and repetitive comments and all subreddits converge to be differing shades of r/politics. The users are actually the worst, they are the most argumentative people I’ve seen on any platform and I’ve been on Internet forums since the late 90s. reply bastawhiz 9 hours agoprevI was a top writer for two years (and got a pretty sweet jacket out of it). I was a multiple knowledge prize winner for my answers (which paid out enough for me to report it on my taxes!). Here's the nugget from the article that's the core reason: > that “was all about trying to come up with questions that would draw in more views and more people,” she said—not about incentivizing high-quality answers. It was all about adding webpages of individual questions, for SEO purposes. I remember the day they announced to the top writer forum that they were killing incentives for good answers and starting to incentivize engaged question askers. Overnight, you got bots posting questions about trivial math problems (literally just multiplication in most cases). The top writers disappeared. What hurts the most: I deleted my account. At some point it became undeleted. I don't have the ability, as far as I'm aware, to kill it. My profile lives on to enrich the people who betrayed the writers. Kind of sad, but it was all about engagement. Ad dollars. reply dchftcs 7 hours agoparent>What hurts the most: I deleted my account. At some point it became undeleted. I don't have the ability, as far as I'm aware, to kill it. My profile lives on to enrich the people who betrayed the writers. Kind of sad, but it was all about engagement. Ad dollars. This sounds horrific and ironic when Quora's founder is on the OpenAI board with a role to supposedly oversee ethics. But, after all he was the CTO of a company that pioneered putting tracking pixels everywhere, so this is just typical silicon valley stuff. reply bastawhiz 6 hours agorootparentI wouldn't call it horrific (though I guess others might). I'm inclined to think it wasn't intentional. It certainly doesn't keep me up at night, and it's not like I regret anything that I posted on the account. I just find it disrespectful that I'm prevented from expressing my dissent around the company's reversal of its original premise. reply joshxyz 5 hours agorootparentprevexactly lol. quora is shit, poe is shit. it's like everything he builds and touches eventually evolves into a cancer to the society. if you'll put drug cartels on one end of a spectrum, he'll be on the opposite end where everything he did sure is legal but definitely monetized and ruined the same amount of lives. reply mgh2 8 hours agoparentprevAds ruin everything. No respect for power users, plain greed. Is there any parallel to Medium? reply bastawhiz 8 hours agorootparentI think medium had the right idea but I don't think the business model was really solid. Substack got it right: people want to pay creators to write, not a company to broker access to creators. Instead of pivoting, they seemed to double down. reply Solvency 8 hours agorootparentWhy aren't science journals the same? Why do I still have to pay some middle man? reply bastawhiz 7 hours agorootparentI don't know. I'm not in academia or doing research, but from the outside looking it, it seems like a racket. reply johnnyanmac 6 hours agorootparentprevMy high level speculation is curation. There actually is (or should be?) some sort of peer review system by people for that journal. That labor is valuable. Also, simple grandfathering. I imagine once upon a time Science journals was a literal term. Easier to keep the same hook if your users were used to it in another medium. reply elashri 2 hours agorootparentIt is actually very rare to pay the reviewers. The standard is that peer review process is done by senior scientists as volunteers. reply oneoff123 5 hours agorootparentprevThere was a time when it made sense to have both done by the same entity. Time for a devorce. One does the work the other collects the reward?? reply j7ake 6 hours agorootparentprevBecause academics care more about which journal a paper is published in, rather than the content of the paper. reply throwaway167 8 hours agoparentprevGiven you now live in the EU you could try exercising your right to account deletion. reply CoastalCoder 7 hours agorootparentHow long does one need to be in the EU to have/exercise that right? E.g., could an American issue a deletion order during a layover in a European airport? reply Engineering-MD 2 hours agorootparentI believe you just need to be a European resident, although how you define that is the crux. Realistically if you have a way to prove an address you reside at in Europe, that should be sufficient. reply bastawhiz 7 hours agorootparentprevI do not live in the EU. reply krkhan 16 hours agoprevI was an active contributor to Quora in the mid-10's -- my answers gained some 3m views. It was really something back then, as many people have fondly reminisced about the era. There were so many great writers who brought so much understanding to whatever crazy stuff they were passionate about by writing for free on Quora. You had diplomats, chefs, former propaganda writers, people living through violent conflict, doctors, teachers, scientists, physicists, astronauts and the list goes on. Quora from that time truly helped me understand the world better. AI might have driven the final nail in the coffin but the one decision which was truly an inflection point was starting a program that would pay for asking X amount of questions. The platform was cool because of people who wrote ANSWERS -- to the point where the original question sometimes became even tangential to the actual answer but you'd learn so much nevertheless. Quora not only ended the top-writer program for those people it started offering monetary incentives for asking the most inane bullshit questions as long as they got the views. It is truly baffling to me how something that holds so much genuine value can be driven into the ground while making decisions left, right and center (and the whole community trying to tell you exactly what you're doing) that destroyed everything of value in the Quora process. The older answers are still there, but the community magic has utterly evaporated. I guess it's pretty normal for folks who have seen this happening in BBS/Usenet era but regardless of all of Quora's faults (the tone-moderation of the language was always a bit overboard in IMHO but I don't think it led to the downfall -- it was always there and writers worked around it) it is genuinely disappointing to see every successive platform try to build something of value only to falter and disappear because we just can't seem to keep anything good around. reply throwaway277432 11 hours agoparentStackOverflow has the same problem, prioritizing question askers instead of answerers. As a result, the site is now \"welcoming\" to new users asking low-quality questions, but actively hostile to what the answerers and mods would like. While they're the ones left to clean up the spam. And when the low-effort questions are closed that then drives away the question askers too, because all the \"nice\" onboarding didn't tell them their question should actually be well-researched. Their expectations of getting help now clash with reality and they end up hating the experience. But all this drives views and \"questions\" in the short-term and management is so clueless it's hopeless. See e.g. the recent mod strike due to the AI policy issues, where they initially wanted to prevent/ban mods from deleting low-quality AI content from serial ChatGPT spammers. reply ChrisMarshallNY 7 hours agorootparentAs someone that has asked a lot more (quite good) questions on SO, than has given answers, and almost all my SO rep. is based on those questions, I can report that my experience has been that I've been treated somewhat barbarously, by high-score mods. Just the mere fact that I was asking a question, made me \"lesser,\" in their eyes. This could be rather infuriating, because I actually sorta know my stuff. One of the things that I know, is that I don't know a lot of stuff, and appreciate finding out, from folks that do. For that reason, I learned to bite my tongue, when they sneered, and appreciated the answer, which was often quite good. I haven't really done much there, in years. I still get upvotes for my questions, years after I wrote them. At some point, people just stopped answering my questions, and I usually ended up posting my solution, when I figured it out on my own. I feel that Quora has turned into the same thing that LinkedIn Groups have become: A haven for ad-scammers and \"SEO\" people. One of the things that happens on both platforms, is that someone asks a specific question, and ignores sincere, legitimate answers, but rewards the \"shill\" \"answers,\" where a particular product is recommended as the \"solution.\" I dumped every LI group, after encountering these folks, and I never really warmed to Quora, because, I guess, I came late to the party, after these people had started their games. I realized the platforms were worthless to me. I won't go, where I'm not wanted. I can't imagine getting paid for either answers or questions. They both take so much time and brainpower, that I can't believe that the income would be at all worthy of the effort. I think that SO actually did a good job on \"gamifying\" the Q&A process. I may have issue with the inevitable status games, but I learned to just keep my peace, and appreciate the help. reply gn4d 7 hours agorootparentCheck my recent submissions, I agree. The mods were just as bad as, if not worse than, the administrators on SO. It was actually the mods that drove more people away imho (from conversations with other high rep contributors). reply gn4d 7 hours agorootparentprevlol I just typed an answer to the top level question, pressed Submit, then saw yours haha. Cheers. It felt great to leave that hellhole a few years ago. I was a (admittedly lowly chump) top 1% contributor with an account for over a decade. reply city41 13 hours agoparentprevQuora really captured lightning in a bottle around that time. I used to get a weekly (daily? Can't remember) email from Quora and I would read just about every question in it. I came to look forward to the emails. I've never had that experience with any other other newsletter. reply Agraillo 12 hours agorootparentAs a daily destination, Quora is no longer usable for me, no need to repeat what others already said. But the digest still surprises me and the posts are often good, even if it's a rewritten Wikipedia entry. It's like a user-generated \"wonders of the past/world\" newspaper for me. Daily was too much, but weekly is ok. I'm not sure whether every e-mail send it personalized, but at least in some way it looks like dependent on what I clicked before. reply JSavageOne 12 hours agoparentprevFascinating and confirms my suspicions. The point when the questions popping up in my feed were obviously not genuine is when I lost interest in Quora. The worst was how it'd always show me variations of questions fetishizing working at Google. Seriously every single Quora digest email had some variation of a question like \"What is the best thing about working at Google?\" Unsubscribed and don't miss it. reply astrange 9 hours agorootparentMy newsletters always had one-two of those and then another that was always like \"what's the hardest part of how nobody in society respects you for having a 160 IQ?\" reply civilized 12 hours agoparentprev> It is truly baffling to me how something that holds so much genuine value can be driven into the ground while making decisions left, right and center (and the whole community trying to tell you exactly what you're doing) that destroyed everything of value in the Quora process. It's crazy that the operators of a website like this could understand so little about what made it special, but I guess all they understood was engagement metrics. reply gn4d 7 hours agoparentprev> the most inane bullshit questions as long as they got the views StackOverflow fell into a similar trap with reputation on controversial or highly upvoted question farms. The problem was further compounded by a ridiculous Triage queue system that could issue account suspensions and bans for honest opinions and feedback if your answers did not align with the prevailing groupthink of a few elite arbiters there. Not that different from what happened somewhat to HN. Read my name backwards. reply samstave 12 hours agoparentprevI rage quit Quora some years ago publicly here on HN, I dont recall now the exact reason (something to do with unwanted profile/content/privacy change or some such... It was quite a while back, but I have never really gone to quora since, especially since its basically a paywall (I also never go to NYT unfortunately because their paywall is so BFY;TW I cant stand it. reply StressedDev 4 hours agorootparentHow do you expect the New York Times to pay its reporters if they do not charge? Internet advertising is not a large money maker for a lot of sites. Also, if people will not pay for news, unethical corporations and unethical governments will, and our \"free\" news will be propaganda. My main point is people on the Internet cannot have it both ways. We either pay for high quality content or we accept we are going to get clickbait, astroturf, propaganda, fake news, etc. reply stevage 9 hours agorootparentprevWow what is that acronym? reply lifeisstillgood 9 hours agorootparentBecause Fuck You; That’s Why https://www.urbandictionary.com/define.php?term=BFYTW Somehow having an acronym with a semicolon actually makes a vulgar rage explosion seem controlled and even educated. Where did we all go so Wrong? reply jjeaff 4 hours agorootparentVenture capital is where we rent wrong. they are to startups what PE is to developed companies. reply samstave 9 hours agorootparentprevBFY;TW - \"Because Fuck You; Thats Why\" reply matsemann 17 hours agoprevWhen I visit Quora, mainly through it being a result for something I've searched for on Google, I can never quite understand what's the question, the answer, and what's an answer to something completely different. So I've never felt compelled to stay or join myself. Also, for a while I just hit a login wall, which of course makes you not click a link there the next time. reply CM30 17 hours agoparentYeah, this is the main problem I have with Quora. You go to a thread to look for answers to a question, then find the platform has merged in a bunch of answers to related questions alongside the ones I'm actually there to read. For example, I found this random question just now: https://www.quora.com/How-does-blockchain-technology-work-11 About half the answers there are actually to that question. The rest are to questions that are about the same thing but with different wording: * 'How does block chain technology work?' * 'How blockchain technology works?' * etc or to related but not quite identical ones: * 'What is blockchain technology and how does it work?' * 'What is blockchain technology, could you explain it in an easy way?' * 'What is a blockchain?' So you'll see a lot of answers... which may or may not actually be for the question you were interested in. Or which may explain it in a weird way, since the actual question they answered was worded very differently. At least with StackOverflow you know that the answers are all related to the same question. reply Arainach 16 hours agorootparent>At least with StackOverflow you know that the answers are all related to the same question. I don't know, most StackOverflow answers I run into are \"closed as duplicate of \" or \"closed as duplicate of \" reply ahoka 15 hours agorootparentOr gods forbid, someone had a slightly open ended question. reply _a_a_a_ 14 hours agorootparentSlightly open-ended doesn't seem to be a problem for me on SO. Where I've really had a problem is the admins not reading the question properly then closing it as already answered elsewhere. I find that screeching like a banshee gets it apologetically reopened. Bit frustrating but a flawed SO is better than no SO at all. reply frabjoused 15 hours agorootparentprevI really fail to understand how a company can consciously guide itself to degrade so badly. Like these were actual decisions of meetings and the results of a ton of work of teams to implement. And it destroyed the site. Product of too much VC, probably. reply AlbertCory 15 hours agorootparentProduct of middle managers, the cause of nearly all corporate problems. reply lupire 14 hours agorootparentMiddle managers pursue the CEO's vision. They aren't steering the ship. reply AlbertCory 14 hours agorootparentThat's your theory, anyway. reply motoxpro 12 hours agorootparentEither the CEO sees what’s going on and is fine with it mean it is their vision, they are putting their feet up and don’t care, or they are not fine with it and have no control over the company. Seems bad any way you look at it. reply AlbertCory 12 hours agorootparent> they are not fine with it and have no control over the company something something \"all unhappy families are unhappy in their own way\" In some places, the \"vision\" is sufficiently vague that all sorts of dumb decisions can be justified by it. Or the CEO is so stupid that he only listens to his direct reports, who all tell him everything is under control. And when they tell him not to \"micromanage\" he listens and backs off. You can't make any universal statements about this. reply motoxpro 10 hours agorootparenti.e. \"bad\" CEO. I don't really think the reason matter. reply frabjoused 13 hours agorootparentprevCan't see a something like \"show answers to related questions directly under a single answer to the intended question\" being a middle-manager decision. reply AlbertCory 12 hours agorootparentYou can't? How do you think it happens in a fairly large company, then? reply StressedDev 3 hours agorootparentprevMore likely, it's a product of not making money and of trying to find a way to make money and not go out of business. reply baxtr 16 hours agoparentprevI’m pretty sure this is the result of a heavy A/B testing culture. Every single idea was probably increasing an important KPI. The page in it’s entirety got destroyed over time. reply bertil 15 hours agorootparentA/B tests are likely involved because no human would suggest those changes in earnest, but what metric could those tests have improved!? Either it’s a series of unchallenged false positives, or there’s a lot about web interface that neither I nor they understand, but I would challenge the idea that is the result of a well-informed A/B testing culture. It’s the output of a thousand monkeys running a thousand tests a day for three years, and releasing anything that hits the green line. reply lupire 14 hours agorootparentad views, and paid subdcriptions, and retention of subscribers. reply foota 14 hours agorootparentprevAnswers per question? reply hn_throwaway_99 16 hours agorootparentprevCouldn't agree more. It's easy to see this user hostility increase slightly, over and over across a period of years, each time having some idea that there is some short-term metric that is positively affected. Considering I rarely understand what I'm even looking at with the mish mosh of answers, and I'm not willing to create an account, I've outright blocked Quora in my browser. As an aside, I think the extreme enshittification of Quora basically highlights every single problem with VC-driven, growth-at-all costs, \"metrics and data driven\" tech platforms (as an aside, the reason I put square quotes there is not that I think metrics and data are bad, but I think most companies are not honest that they're optimizing for metrics and data that are easy to gather, while the fuzzier but actually more critical concepts of user trust and sentiment get ignored over time). I mean honestly, is there any single employee at Quora that thinks that what the site became is anything but a total shit show? reply esafak 8 hours agorootparentprevIn general there are two causes: * Shortsighted metrics that don't contribute to the holistic quality of the product. Ad-supported products are especially prone to this. * Treating a mature product just like a new one, and adding too many features instead of maintaining it and doing less. reply nine_zeros 16 hours agorootparentprev> I’m pretty sure this is the result of a heavy A/B testing culture. Every single idea was probably increasing an important KPI. The page in it’s entirety got destroyed over time. This is the reality. Not just in Quora but all tech companies. A/B testing is used to determine how to squeeze more and more engagement, in small parts of the app. And over time the app becomes an ad factory with unrelated posts because apparently people are more engaged when they are lost and unable to find any information. reply chiefalchemist 15 hours agorootparentprevI feel the same way about Amazon.com. It feels cluttered and messy. Most the opt-ins / opt-outs have to be read closely to make sure you're not being suckered by some dark pattern. If you visit Amazon often enough you kinda get used to it. But a new vistor imho would be shellshocked. reply recursivecaveat 13 hours agorootparentI remember fondly this paragraph from the famous google platform memo: > Jeff Bezos is an infamous micro-manager. He micro-manages every single pixel of Amazon's retail site. He hired Larry Tesler, Apple's Chief Scientist and probably the very most famous and respected human-computer interaction expert in the entire world, and then ignored every goddamn thing Larry said for three years until Larry finally -- wisely -- left the company. Larry would do these big usability studies and demonstrate beyond any shred of doubt that nobody can understand that frigging website, but Bezos just couldn't let go of those pixels, all those millions of semantics-packed pixels on the landing page. They were like millions of his own precious children. So they're all still there, and Larry is not. reply k__ 16 hours agorootparentprevSeems like jobs after LLMs still have a chance sob reply karaterobot 14 hours agoparentprevReddit is the same way if you don't go to old.reddit.com. You see that there are N comments, but you only get to read the first few, and suddenly you're looking at an entirely different post. Every time I accidentally go there I get confused by it. There is no chance anybody asked for this, it's so stupid it had to come out of perversely chasing some metric. reply temporarara 11 hours agorootparentThat behaviour is complete madness. And old.reddit has pretty much the perfect interface for computer usage anyway. Why they won't just label it \"desktop style\" and call it a day is beyond me. reply dpkirchner 11 hours agorootparentprevAnd often when you expand comment replies the comment you were looking at disappears and you have to scroll around to find it again. It must be deliberate, there's no way you accidentally make something work that poorly. reply dbbk 16 hours agoparentprevThe single change they made that killed the platform was changing the dropdown default from \"answers to the question\" to \"answers to related questions\". Absolutely insane. reply p1esk 16 hours agorootparentI bet whoever suggested this insanity got promoted because “user engagement” went up. Users becoming more confused. reply lencastre 17 hours agoparentprevThat’s exactly my experience so I just avoided quora, for some reason the question I was looking at had many answers to different questions below, after scrolling a bit downwards something resembling a connection with the question in the form of an answer would be found but… it’s too much trouble. reply temporallobe 15 hours agoparentprevYep. As a UI/UX specialist, it’s nothing short of a nightmare. I can’t decide if it’s purposeful (they’re trying to kill the site), negligent (they can’t hire or retain talent, or who they have is making bad decisions), or just full of corporate/organizational bloat (basically all decisions made by non-engineers such as stakeholders or investors designed to maximize profit and ad space).I am leaning towards bloat. reply mattmaroon 15 hours agorootparentI think it’s just a really logical way for them to monetize. If the UI was good you’d never click on (or likely even see) an ad. I’d bet they just tested it and found that’s how they make money. reply lupire 14 hours agorootparentPeople complaining about the site wouldn't pay for the site they say they want. reply input_sh 17 hours agoparentprevThere's a toggle on top left that's by default set to \"all related\", as in answers to the question asked + a bunch of answers to things you're definitely not looking for, but are tangentially related. If you switch the toggle to \"answers\", you only get answers to the question. reply zdragnar 16 hours agorootparentI occasionally visit quora off of search results because I kinda got used to parsing out what I wanted, but never knew about this toggle. I literally cannot understand how anyone thought intermixing related questions and answers together is in any way useful or beneficial to their users, especially when the default is set to ON. reply darksim905 15 hours agorootparentCan people not really understand this? Some of those related questions are dupes, other times they are paths to rabbit holes. ycombinator does the same thing. When you guys see posts that come up that have been here before, you post links to previous discussions. I saw this recently specifically with the links to articles on Martin Couney (I had a hard time finding that because the article is so poorly worded). Even Reddit has linkability to detect the URL and show you an 'other discussions' lens (for Old Reddit). Granted, this is a slight taxonomy difference of 'other discussions' of the same exact topic, vs other discussions related to your topic. But I think it's a useful and helpful feature. reply zdragnar 15 hours agorootparentThe problem is that there's no continuity between what is related and what is an answer to the question asked. Related answers are intermixed with real answers at the same level. You can only tell something is a real answer by the absence of bold text and presence of light grey text. The link upthread isn't a good example, because the question is so basic it's easy for them to have a lot of related (duplicate) questions and answers. When the topic requires more nuance or is more specific, then the related questions and answers really aren't related at all, and they just add a bunch of useless noise. hackere news isn't a question / answer site. I don't come here with the same expectations that I do when I go to something like stack overflow or quora. Ironically, if I do a google search for something and ycombinator comes up in the results, the link is directly to the comment that is relevant- not to the top level of the article with all of the related sibling comments. If anything, from that perspective hacker news is actually a better question and answer site than quora. reply bbarnett 15 hours agorootparentThat, and I have seen AI answers in the mix too, often wildly wrong or completely off topic. So you look for a question, and all the answers are not even relevant, and often completely off topic or wrong. reply motoxpro 12 hours agorootparentprevSeems wild to put a bunch of “related” answers that are relationship advice, movie questions and other things when you look up things like programming questions. I can understand the rabbit hole thing. Never thought about that. That they think of themselves as TikTok and not stack overflow or Reddit. reply moralestapia 17 hours agorootparentprevSure ... but 99 out of 100 users just say \"weird\" and leave. And that's \"How Quora died\". reply input_sh 17 hours agorootparentI'm not here to defend Quora, but my speculation is that Google's preference for long-form content is equally to blame. Basically the same reason as recipes that start with a long life story: Google prefers pages with lots of content, even if 10% of it is actually useful to anyone. So a page with 50+ \"answers\" is preferable to a page with two actual answers. reply voisin 16 hours agorootparent> Google prefers pages with lots of content And that’s how Google search will be overtaken by GPT with ease. People want answers, not an ad-driven algorithm’s desire for content that is long enough to support more ads. reply klabb3 11 hours agorootparent> And that’s how Google search will be overtaken by GPT with ease. Yes if the LLMs are designed to give you the best results. Right now, it’s a honey moon phase for consumers, when there’s a fierce competitive race. But just like any other tech its deployment and realization is governed by incentives. If we get an entrenched LLM monopolist I don’t think it’ll be any different from a search monopolist. Perhaps worse. reply esafak 8 hours agorootparentIt's going to be hard to become entrenched with open source LLMs in the wild. reply klabb3 7 hours agorootparentYou could make the same argument for search or many other products - but it doesn’t hold in practice. If the commercial closed one is more powerful and/or convenient people will not choose freedom. There’s a reason these companies are getting drenched in investor money. I can think of a thousand ways that they could get that convenience edge against open models. For instance, when LLMs are personalized based on your own history, chats etc people won’t give those conveniences up. And that data would not be accessible to the open models, at least not seamlessly. reply moralestapia 16 hours agorootparentprevExactly this. I was recently involved in a decision of whether or not to make a nascent community SEO-friendly, which meant significant changes to the UI and reduced UX. In the end the conclusion was \"f... SEO, let's build something good instead and have users come back because of that\". reply voisin 12 hours agorootparentKeep fighting the good fight. This is the hill to die on. reply 6510 15 hours agorootparentprevThey most likely also prefer titles that get clicked. But the best results are those where the user uses the back button and clicks something else. They must not click back within n seconds of course. The user has to spend a minimum amount of time on the almost useless page. I got this feeling looking for many things. I know there are pages but get many results very very close to what I was looking for. Dubious reversal of key words where \"how to turn a car into an airplane\" becomes \"how to turn an airplane into a car\" Then people start making perfect SEO pages like \"how to style css with html\" reply mkl 11 hours agorootparent> \"how to style css with html\" I was intrigued by the idea of such a page, but unfortunately your comment is apparently the only mention of that phrase on the internet. reply moralestapia 10 hours agorootparent@6510 should sell that precious internet real estate reply freediver 15 hours agoparentprevGoogle sending traffic to Quora, one ad-based business to another, with user being secondary and content on both being incentivized by ad clickthrough rates, is a dying paradigm. It had a good run though but I do not imagine it will be missed. reply russellbeattie 14 hours agorootparentGoogle needs to nerf their rankings. It's essentially a near-useless click bait website for ads. Any new site with such horrible signal to noise ratio would never get such a priority in results. I can't imagine any other website linking to it, so I can't imagine how Quora remains so high anyways, even if page rank is long gone. reply pram 14 hours agoparentprevI’ve never used Quora because of this, and also it has always looked like a scam website. You know those pages you find on google that are just SO/Wikipedia articles copied? That’s what Quora looks like lol reply joegahona 12 hours agoparentprevQuora also comingles ads with content, and the ads look almost identical to content. Quora has become unpleasant and unusable in the past decade, and I only end up there on accident. I just logged in out of curiosity, and I have an avalanche of “notifications” that are just pathetic — “Dark Psychology Facts posted in a space you might like: ‘Can a human win in a battle with a hyena?’” reply thayne 16 hours agoparentprevThat used to not be the case, and the site design was more reasonable. But now, I completely agree with you. reply pokstad 17 hours agoparentprevIt’s a UX trainwreck reply flurdy 8 hours agoparentprevYeah, the login wall made me always close the tab. I am just assuming there is still a login requirement with Quara so I still automatically close the tab. reply conductr 16 hours agoparentprevI’m not sure what their business model is, I assume ads, so I always wonder what good the login wall does and making the UX so horrible. I too only ever get there from a Google search too, much like Wikipedia, but with Wikipedia I’m usually going to make a few clicks once I’m there as the related link exploration is natural. I assume quora could get more clicks out of me if it wasn’t so awful. They’ve instead taught me to never click past the initial Google link, my bounce rate is 100%. It seems like such a missed opportunity. Even when I compare to my stack exchange usage, I’m not a registered user, I’m only ever consuming information, but I click around and they benefit through ads. reply frfl 16 hours agorootparentBiggest difference is Wikipedia isn't ad driven and the information on there is actually interesting, naturally drives you to explore. My bounce rate is 100% and I rarely click a Quora link if I can help it. 1% useful 99% noise. I agree with StackExchange, sometimes the random things on the sidebar grab my attention, whether it's some random travel question, workplace, about writing/sci-fi, etc. Pinterest is another spammy site. Same with Quora. Like what is the model here? Spam SEO, drive ad revenue? In my experience Yahoo Answers was 10x more interesting than anything Quora offers, but maybe I and others like me are the exception and companies like Quora are actually getting eyeballs with their UX and business model and that's why Yahoo Answers isn't a thing anymore.? reply telesilla 16 hours agorootparentprev>I’m not sure what their business model is Probably also email harvesting and any associated segmentation they get with your reading history, since it's required to sign up to read anything. reply hot_gril 16 hours agoparentprevI think it's an attempt at question deduplication where they had more confidence in that matching algo than they should have. It usually conflates two questions with similar words but very different meanings. reply rjh29 9 hours agoparentprevInstalled uBlocklist, and both quora and pinterest are on it, never have to see them in results again. reply scarface_74 17 hours agoparentprevQuora is the only site that I actively block to make sure that I don’t go there by mistake. It has to be the worse site on the internet. reply croisillon 16 hours agorootparentnah i think pinterest is even a notch worse reply frfl 16 hours agorootparentQuora and Pinterest... one has cornered the text-based SEO-spam segment, other has cornered the SEO-spam image segment. reply whimsicalism 16 hours agorootparentprevpinterest can be better if you are actually there to use it (ie not directed from search traffic) reply babyshake 16 hours agoparentprevThis is the result of stickiness being more important than utility. reply whimsicalism 16 hours agoparentprevfeel like this was somewhat recent. i didn’t use quora much already, but this is the change that took me from using ever to never using reply CM30 17 hours agoprevWell, the revenue sharing probably didn't help much for sure. I've seen dozens of sites and platforms add that as a feature, and as unfortunate as it is, this almost always backfires. People see 'make money contributing to this platform' as 'free cash for spamming!' and the quality often falls off a cliff. You need really strict rules and a good moderation team to keep a platform under control in this situation, and that's rarely the case for services like Quora. You can see the same issues cropping up on Medium with its partner program, on Twitter thanks to Twitter Blue and revenue sharing, and even to some degree on the likes of YouTube and Twitch (though they're so massive that the scammers don't really stand out/do as well). Put money on the table, and the scammers will come out of the woodwork to try and get as much of it as they can, quality content be damned. Edit: I also suspect part of the issue with revenue sharing programs is that they don't offer enough to incentive genuine experts, but offer enough that if you're utterly desperate and local costs of living are low, you're incentivised to spam for it. reply matsemann 17 hours agoparentYeah, the comments to any viral post on Xitter now is just blue checkmarks spamming memes in order to get likes and eyeballs on their junk content, and thus make money. reply aniforprez 5 hours agorootparentAny large film/music/video games twitter account post gets spammed by AI bots and I mean literal AI bots that occasionally give the game away with \"you have run out of credits\" or \"I don't understand this prompt\" messages. Also a bunch of them just retweeting their own posts that have nothing to do with the original in an attempt to farm engagement for their older stuff reply username332211 17 hours agoprevSo, the fourth paragraph of the article is supposed to demonstrate the decline of Quora. It has a number of links. One of those links shows a UI that's fairly obviously Reddit! [1] Another is to genius.com's lyrics to some song[2] which don't seem to have anything to do with anything the author's complaining about. Of the links actually relevant to Quora, one is about an obvious joke[3], which the author either doesn't understand or pretends not to. What's left is a case of spam[4], that the article links twice. I'd suppose because it's the only evidence the author has of low quality questions on Quora. While Quora is a horrible website, I'd say the evidence shows Slate is worse. Their reporters can't even find genuine evidence of Quora's low quality. They show us Reddit instead. [1] https://twitter.com/3DrakaiNa/status/1743993622737863056 [2] https://genius.com/23826656 [3] https://twitter.com/Zeronelite/status/1743449094360596802 [4] https://hollywoodactress02.quora.com/OMG-KING-CHARLES-SHOCK-... reply plorkyeran 15 hours agoparent[3] is, as far as anyone can tell, an insane person and not a joke. They've posted literally hundreds of questions involving atheists, christian babies, and nonsensical situations. reply yannis 17 hours agoprevNot only Quora, but many other sites including the various \"stackexchange\" sites started declining I would say from about 2018, so this is not a phenomenon attributed to llms. IMHO I attribute this to the following: a) The overuse of social media conditioning users to provide short answers, rather than long thoughtful write-ups, b) Most topics have been saturated to the limit c) lack of interest from the new generation d) Bad management and moderation. reply ojosilva 1 hour agoparentThe internet is a wild place, but there was a time it seemed tamed, Google fixed search and got rid of mail spam with Gmail, FB and Twitter got the feed right, ads became text-only with Adwords instead of animated gifs, ExpertsExchange, SO and various interesting personal websites and blogs had answers and were fun to read. But just like with antibiotics, bugs were trained on it and they got stronger finding the exploit until spam, scam and greed destroyed it once again. Sigh! reply klabb3 13 hours agoparentprevYou are missing the elephant in the room, the incentives that drive shit content: SEO. Both quora and stack exchange – as businesses – are downstream of Google search in particular. Their main goal is to appease PageRank. All else follows from that. reply dclowd9901 16 hours agoparentprevI think stack exchange’s moderators killed that site. Common complaints were how antagonistic they were toward new users. Hard to grow or sustain a user base if you turn out all the new ones. reply IshKebab 15 hours agorootparentYeah it's a really big problem with StackOverflow. The mods are all crazy. I can't find it now but the StackOverflow Devs proposed actually fixing (or at least improving) question closing by allowing the asker to reopen their question once. I can't remember the exact details, but it sounded like a good first step. Downvoted to hell by the existing mods of course. They're a bit screwed because they rely so much on volunteer mods but the volunteer mods are crazy... reply matsemann 12 hours agorootparentDoesn't sound like either of you know how SO works. Very little is done by mods, mainly it's votes by normal users. Like I could be voting to close something as a duplicate, off topic etc with my privileges. It's the community doing this, not moderators. And the community is tired of people asking for help with their homework or doing very little effort themselves before asking. Why do these people deserve others spending their time helping them if they can't even bother to search or formulate something coherent? reply IshKebab 11 hours agorootparent> And the community is tired of people asking for help with their homework I don't think anyone objects to closing those questions. That's not really what this is about. My questions are very clearly not asking for help with homework and trivial \"how do I write a for loop\" or \"it doesn't work\" questions. Still get downvoted/closed frequently. Most often: * It gets closed as duplicate because there's a vaguely similar - but different - question. Or sometimes there's a completely different question with an answer that incidentally also answers my question. * It gets closed as too vague or not clear because it's simply outside the domain of expertise of the voter. It's clear to people that know what I'm talking about. To be clear when I say \"mods\" I don't exclusively mean people with official mod power. It's also people that moderate for fun - those that trawl the new questions. Let me know if you have a better name for those people. You can tell it's them because you very often get a couple of downvotes immediately and then if you check back a month or two later it will have been upvoted by many more people that actually had the same question and arrived there via Google. reply matsemann 11 hours agorootparentFeel free to drop links to your said questions. Often it's just vague allegations with little evidence. Getting something closed as a duplicate shouldn't be taken as a slight. It's often good that various formulations of a question then can point to the same answer. Or if the answer to the other question answers yours, isn't it just good that you and other people will be directed there? It's a Q&A site, not a forum. Which question stands doesn't matter except for feelings. Same with your vague or clarity. It's Q&A, not a forum with back and forth discussions. It's just not a right fit. You might think that's not how you'd like the site to be, but it is how it is. It's not overzealous moderators closing it. reply int_19h 4 hours agorootparentI've been on StackOverflow for almost 15 years now, with 100k reputation (mostly from old answers). I can tell you that I routinely run into examples of what GP described while searching for answers - it happens quite often that search engine produces link to an SO question that is about the same exact thing I'm investigating, and it's a well-written question covering the details and various explored avenues... and it's closed as \"duplicate\" of something that isn't actually relevant. reply user_7832 9 hours agorootparentprevNot the person you replied to but StackOverflow is ridiculously hostile to new users including formerly me. I have over 2k points on HN and much more on reddit. I've spent much more time on SE than on HN but I don't even bother contributing because the times I tried it, it went nowhere. reply dclowd9901 9 hours agorootparentprevAnd the site is dying as a result. That fact alone makes your point moot. reply dclowd9901 9 hours agorootparentprevThat’s great. You haven’t addressed my point: who do you think is asking questions about their homework? A community needs a steady influx of new users to remain vital. You’re not going to get that with _often incorrect_ references to duplicates. Fact is, nobody wants to go to Stack overflow anymore for answers because the experience sucks. And they don’t mind using ChatGPT. Thanks for all the training data, Stack! reply spamizbad 9 hours agoprevQuora never lived as far as I'm concerned. It was always touted as some better, savvier alternative to the StackExchange network but it was just clogged up with crummy answers to bad questions. They made it extremely difficult to see the actual content and would make it near impossible to follow threads/discussions without making an account (which would in turn spam you) The fact that it was SEO-optimized up the wazoo and always clogged up search results didn't make it any better. For that reason I have \"-site:quora.com\" seared into my muscle memory when searching. reply keiferski 17 hours agoprevI think the upvote/downvote model for evaluating content is fundamentally not a good one, and this deficiency is the root problem for a lot of sites (Reddit, Quora, etc.) that were founded around the same time, circa 2005-2012. It lends itself too much to groupthink, popularity contests, and self-promotion. X/Twitter has a million other problems, but I do like their new Community Notes of fact-checking claims. It's essentially anonymous and somewhat immune to self-promotion. Going forward, a better path for FAQ-type sites is probably something similar to that model. The hard part is how you get users to answer questions while removing the self-promotion and gamification/scoreboard incentives. It's possible that AI might just destroy this space entirely, but I still think you need some form of human fact-checking in order to avoid hallucinations. reply gn4d 7 hours agoparent>I think the upvote/downvote model for evaluating content is fundamentally not a good one This is why the bump model that 4chan uses is superior. You can sort threads by activity, but all posters display with equal weight, and you must utilize critical thinking to evaluate each and every post (unless it is from a trip user whose authority you trust in a particular domain), not some little number that pushes them to the top or sinks them to the bottom. There are (You)'s, but that can indicate consensus, disagreement, controversy, or something else, and you need to sus that our for yourself. Consensus-based truth models are frankly shit for most types of topics. Heck, even for scientific topics, the peer review model has been shown to be completely bankrupt. reply passion__desire 17 hours agoparentprevAnyone who shares an article or news which has been community noted gets an immediate downvote in recommendations for me. reply wonderfulcloud 14 hours agorootparentCommunity Notes could just be an expansion of a good tweet to clear up some ambiguity. The up/down feature tends to create echo chambers that eventually exclude half the population. Reddit is worse at this because of excessive moderation combined with doomscrollong incentives. reply walterbell 17 hours agoprevNo mention of Quora founder at OpenAI? https://archive.is/xsK8S > His position on the [OpenAI] board has also raised eyebrows because Quora has been in increasingly direct competition with OpenAI’s best-known service: ChatGPT ... Shortly after OpenAI launched ChatGPT a year ago, Quora introduced Poe, a platform that allows people to ask questions from various AI chatbots, including ChatGPT. reply robg 17 hours agoparentI’m still surprised that D’Angelo didn’t get as much scrutiny in the OpenAI mess, especially as the only board member still in place. Seems obvious in hindsight that Quora continuing to fail can only be saved by leveraging as training data for the next thing. And OpenAIs ambitions threaten that. reply seahawks78 15 hours agoprevI have some very good opinion of this. I work in big tech and I might be a canonical example of some users who no longer find Quora interesting. From 2012 to about 2014 I was absolutely hooked to Quora spending anywhere between 2 to 4 hours every day. At that time I was a young professional in my early 30s, newly married and just at the beginning of my tech career after grad school. I used to love the questions posted on the forum which seemed very relevant to me e.g. how to build a career in tech, dating/relationship advice, tourism advice etc. I was hooked. Fast forward 10 years from then. I am a 40 year old middle aged guy. Still working in tech albeit at a Senior level; with one school going kid and have a mortgage. I still visit Quora occasionally but am hard pressed to find any content that is interesting and appropriate for my age. It seems that they are still showing me questions which were mostly relevant to my younger self 10 years back and not now. There is practically no content or discussion in that forum that can attract and keep a 40 year old middle aged guy. Its just that I evolved but they didn't. Not to mention the fact that they also became more annoying with all these \"promoted\", \"relevant\" questions intermixed within the same page. I think their lack of ability to evolve over time absolutely finished the product. My two cents. reply indigoabstract 17 hours agoprevQuora seems pure clickbait at this point. Unfortunately, because it didn't start that way, now anything still relevant on that site is burried under the weight of the nonsense. reply hot_gril 16 hours agoparentI get random emails from them about \"popular\" questions like \"why has Javascript replaced C++?\" Seems like ragebait. reply spiderice 15 hours agorootparentOne thing I noticed back when I browsed Quora a lot is that most the answers were in direct opposition to the question being asked. It’s like clockwork. Q: why is running so good for you A: it actually isn’t good for you.Q: why do so many people run when it’s so bad for you A: it actually is really good for you.Every single question seemed to be this way. It probably reveals a lot about human nature, but it really turned me off to know I was just reading knee jerk reactions to questions rather than anything accurate. All this to say, yes, ragebait is the lifeblood of Quora. reply hot_gril 15 hours agorootparentFor programming questions, the answer is always \"it depends\" even if it doesn't really depend. reply schleck8 13 hours agoprevQuora is the spammiest large website, for sure. Depending on your query like 80 % of responses are scams from India and/or SEO fluff bullshit and self promotion. reply Yusefmosiah 14 hours agoprevQuora is, I think, part of the reason that OpenAI has a big lead over their rivals. Quora has, specifically for training language models, very high-signal data, when compared to Reddit, twitter, gmail, and Meta’s platforms. And OpenAI is afaik, the only AI lab with a license to Quora’s data. Quora has long been a world leader of SEO and dark UX patterns — it was never as valuable to users as its Google ranking would indicate — so it’s hard to mourn its demise. But yes, it does stink to see the quality of information going down while the quantity grows at an ever-increasing exponential rate. reply nottorp 17 hours agoprevDon't blame LLMs, blame the registration requirement. This is one of the kinds of product that people will skip if they can't evaluate it frictionless. reply Wistar 17 hours agoparentThis is what always stops me at quora’s door. reply nonrandomstring 17 hours agorootparentMe too. I've a simple but strict set of security rules for whether I'll use a website. - it does not want to run javascript - that I can access it over Tor without being blocked - that it will work in a text based browser like elinks Despite the protestations by idealogues that no such sites exist, HN meets all those requirements, as do dozens of useful sites I regularly use. If and only if I get to evaluate the quality of a site based on those requirements, I may eventually register, with nothing more than an email, in order to post replies. Quora fell off that list long ago. More recently so did StackExchange. Often I find that things break as soon as Cloudflare proxies are involved. reply nottorp 16 hours agorootparentHmm I was going to say something about Stallman-like fanaticism, but then I remembered Stallman proved to be mostly right... reply nonrandomstring 13 hours agorootparentRichard is most concerned about injustices toward people that take away their freedoms. I am most concerned about the security of people and how software makes them insecure in order to profit from, or abuse them. These are proximate but different. It is a bit worrying that both freedom and security are thought 'fanatical' by some. reply jstarfish 12 hours agorootparent> It is a bit worrying that both freedom and security are thought 'fanatical' by some. Because those are your grandpa's and your dad's causes, respectively. The operative word for this generation is \"consent.\" Manipulate someone into giving Consent and you can do whatever the hell you want to them. They asked for and agreed to the abuse! It all starts with clicking \"I accept.\" My skin crawls anytime I see someone bring it up; it's a red flag that someone is trying to apply BDSM protocol negotiation to an otherwise-simple interaction. Attorneys do this shit too. People only ever do it at all when they're trying to rewrite the rules in their favor. reply nottorp 11 hours agorootparent> It all starts with clicking \"I accept.\" You don't have to click 'I accept'. If the dialog is too complex to figure out what you're accepting, click reject or close the page. If there is no obvious reject (even if there is a hidden one) just close the page and never come back. reply nonrandomstring 9 hours agorootparentprevIn all honesty I think grandpa's and dad's causes were getting home with all their arms legs still attached, but I hear you. Once the sound of trumpets fades and the flags are folded, it's hard to get out of your head what has been internalised so deeply. Yeah they're just words, usually uttered by men who've sacrificed nothing. But hey, this thing about \"consent\". Jolly interesting. Because not a single person I've met born after 2000 really has the capacity I do not mean that in a disparaging way, but I do mean it in a serious legal sense. I don't think that in the 21st century the vast majority of adults have the capacity to consent to digital contracts. It started with EULAs, and has since plunged the entire legal profession into degeneracy. It's total failure to protect the lives of common people from technology predators is shameful. reply obernard 17 hours agorootparentprevYou must agree that for almost every web-based product, designers and product managers can safely decide to completely ignore all people who have requirements like yours without it ever affecting their success. reply nonrandomstring 16 hours agorootparentYes I absolutely do agree with that. Designers and product managers may completely ignore people like me, trample all over our needs and ignore us. Why would they care about the one percent. All good luck power to them and their values. But I am also not the least bit concerned with whether their products are a success or not. Why would I care if they do not? Yet I cannot concur that \"almost every\" site does. No, there are some that do. Moreover, those sites seem to self curate as being of very good quality. So I am happy that there are thoughtful, intelligent people out there who \"get it\". When they stop, so do I, and just move on. It's not personal and I'm not invested in them. reply thexumaker 11 hours agorootparentYou're not even 1%, maybe .000001% reply foobiekr 15 hours agorootparentprevIt’s not 1%, more like 0.005%. reply astrange 9 hours agorootparentprevMy experience running websites tells me that ignoring people like you is actually mandatory because they'll take up all your time and have opposite tastes of contributing users. reply nonrandomstring 8 hours agorootparent> people like you eeeeeeeewww! That's not a great look is it!? ^^^ delete button maybe? > have opposite tastes of contributing users. The people you exclude are unlike the contributing users.... yep no tautology there then :) reply CamperBob2 16 hours agorootparentprevNot only can they ignore such users, they have to. reply nonrandomstring 16 hours agorootparentAnd yet here I am, the annoying fly in the ointment of your hypothesis, no? reply CamperBob2 12 hours agorootparentOne thing I like about HN is that they make the same amount of money off of me whether I run JavaScript or not. I imagine we can agree there... reply nonrandomstring 10 hours agorootparentOh indeed. What a wonderful training set this will make one day when it's sold. We'll be too sore to sit down in the morning, but right now at least we're getting a reach-around. reply int_19h 3 hours agorootparentGiven that HN has a public API to export all the conversations nobody needs to wait for that day to make it into a training set. In fact, I would be extremely surprised if it hasn't already been trained on. reply scarface_74 16 hours agorootparentprevI can 100% guarantees you that Quora didn’t die because of people who didn’t want to run JavaScript or be able to access it over Tor. reply nonrandomstring 16 hours agorootparentSurely my absence played no pivotal role in their downfall. I didn't even know Quora had \"died\". How sad. RIP Quora. But a tjpnz said: \"They're also driving away contributors with more altruistic motivations\" Now, I can agree with that, because I see a very strong alignment between the set of people who strongly uphold their own values and have self-respect, and those who stand up for the rights of others and have something to give. The \"walled garden\" internet basically drives away people who give a fuck. reply zht 15 hours agorootparentOne other way of saying “drives away people who give a fuck” could be saying those people are self-aggrandizing, self important, always yelling at the top of their lungs about their zealotry telling everyone at first opportunity about how they turn off JavaScript and don’t contribute the much to the discussion I liken people like this to the Amish. They balk at modernity as being incompatible with their beliefs. So they withdraw from the world and live life as they see fit. What they don’t do is go around all the time yelling at people telling them how their lifestyle is wrong or constantly telling other people how they don’t use certain technologies. reply nonrandomstring 15 hours agorootparentThat really flipped your wig didn't it? I'm intellectually curious, what you're so deeply invested in that was just threatened? reply scarface_74 15 hours agorootparentHe’s right, people who always talk about they don’t use sites that require Javacript adds about as much to the conversation as people who show up in television conversations and say “I haven’t owned a television in 20 years, do people still watch TV?” reply nonrandomstring 14 hours agorootparentIf we stick with that analogy, then since HN requires no Javascript, it's more like we're having a discussion on a radio show, and I am saying \"I don't really watch much TV\", no? What I hear is that Javascript is a means of speed and power to you. And for you, forgoing that power would serve no useful purpose. Do I understand that correctly? What I am wondering is, do you think that for other people, they should, maybe even must, feel the same way? Even if they are very pleased just listening to the radio? reply scarface_74 11 hours agorootparentWell, how many people do you really think disable JavaScript and want to browse the web over Tor? reply nonrandomstring 14 hours agorootparentprevForget about me for a moment. Does Javascript really mean something important to you? reply scarface_74 14 hours agorootparentNot running JavaScript in 2024 is like putting a speed governor on your own car that limits you to going 35 miles an hour. It serves no useful purpose reply nonrandomstring 14 hours agorootparent> It serves no useful purpose No useful purpose to who? reply scarface_74 11 hours agorootparentWhat purpose do you have for removing functionality that has been part of the web since the 90s? reply nonrandomstring 10 hours agorootparentOf course. What I do not want or need is code that's been part of the web since about 2016-2020, which is arbitrary code execution by complete strangers. Not even in a sandboxed disposable web browser. Now I suspect you'll want to tell me how actually Javascript is perfectly safe... and that's a conversation we can save for another day for the sake of both our dignity and friendship. I'm afraid it's my job to know otherwise. reply scarface_74 9 hours agorootparentWell despite your appeal to authority… If you knew about that many exploits in the wild, surely you could (white hat) tell the company who created the browser or (black hat) make a lot of money by selling it to three letter agencies or private companies that sell it to three letter agencies. However, in the real world, a fully patched Android or iOS operating system - especially one that is in “lock down mode” is not any more susceptible to Zero day exploits than the number of other exploitable parts of the OS that you also don’t have any control over. And even if you only run open source software, how long was the OpenSSL bug in operating systems before it was discovered? Do you use your mobile’s messaging system? That’s been one of the primary targets of exploits that will target you directly. reply nonrandomstring 8 hours agorootparentI appreciate your sincere and sweet overtures scarface, but I'm just not that kinda javascript girl. You're right though, everything out there is riddled with holes. Let's not give up though. reply BlueTemplar 15 hours agorootparentprevIt's not completely unrelated I think. It's hard to me to see how a site with a vision like Quora (had) can continue existing for long after they start ignoring accessibility issues. Sounds like I should be looking for a Stack Overflow alternative too. (I'm suspecting that this might be related to the recent issue of better accessibility also making it easier to abuse for neural network based abusers, and it certainly looks like a hard problem to solve for the most popular websites.) reply nottorp 11 hours agorootparent> Sounds like I should be looking for a Stack Overflow alternative too. SO went to hell when they started demanding ready to copy paste answers instead of 'teach the man how to fish' answers. reply scarface_74 14 hours agorootparentprevAccessibility is important. But it doesn’t relate to success of a company. Besides, modern screen readers and even accessibility affordances on modern cell phones - at least iPhones - don’t struggle or care whether the site uses JavaScript reply nonrandomstring 14 hours agorootparent> But it doesn’t relate to success of a company. I am wondering what you mean by \"success\" of a company here? Is that purely financial success? Or rate of growth or something like that? reply scarface_74 11 hours agorootparentFinancial success or rate of growth reply nonrandomstring 10 hours agorootparentRight and those are good things. Noble things. There's nothing wrong with making a few pennies and jobs for others. But they are not the only things that define the success of a person, or a company. WOuld you agree? reply BlueTemplar 14 hours agorootparentprevThis is not what users seem to say ? https://news.ycombinator.com/item?id=39211037 reply tjpnz 17 hours agoparentprevThey're also driving away contributors with more altruistic motivations. There are a few niche subjects for which I can offer a semi-useful perspective. But I'm not going to do it if Quora build a big fucking wall around it. When they started introducing that I deleted my account. Fuck them. reply hot_gril 16 hours agoparentprevI blame what the other comment says, the UI mixing in answers to other questions. reply 78 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Quora is experiencing a decrease in users due to the presence of low-quality, repetitive, and AI-generated content",
      "Users are dissatisfied with the platform's lack of moderation and functionality",
      "Quora has also struggled with issues such as flooding the site with irrelevant articles, prioritizing views over quality, and dealing with trolls and account impersonators"
    ],
    "commentSummary": [
      "Users are discussing the decline and challenges of platforms like Quora, StackOverflow, and Reddit, as well as flaws in websites like Wikipedia.",
      "Concerns include lack of attention to contributors, prioritization of revenue over user satisfaction, biased content, aggressive moderators, and negative user experiences.",
      "The conversation also covers topics like monetization strategies, user experience, privacy concerns, and the impact of AI and machine learning on these platforms. Overall, there is a sentiment of dissatisfaction with the direction and management of these websites."
    ],
    "points": 323,
    "commentCount": 358,
    "retryCount": 0,
    "time": 1706975455
  },
  {
    "id": 39239449,
    "title": "The Decline of Physical Cash: Implications of a Cashless Society",
    "originLink": "https://www.prospectmagazine.co.uk/ideas/economics/64492/the-end-of-money-cashless-society",
    "originBody": "Log in Subscribe Menu Politics World Ideas Views Culture Latest issue Podcasts Newsletters Log in Subscribe Hide this label Latest issue Newsletters Podcasts Sections Politics England Scotland Northern Ireland Wales Constitution Brexit Analysis Policy World Environment United States China Europe Middle East The Americas Asia Africa Ideas Philosophy Law Technology Media Religion Economics Business Identity Views Columnists People Lives Letters Brief Encounter Culture Books Film TV Stage Music Art Gaming More from Prospect Past issues Events Insight Reports Illustration by Miguel Porlan Economics From the March 2024 issue The end of money Physical cash is dying—and you don’t need to be a conspiracist to worry about the consequences By Stuart Jeffries January 20, 2024 One day over the summer, Piers Corbyn picked up a punnet of strawberries in the Aldi supermarket in Greenwich. He then went to the help desk and placed £1.19 in change on the counter. The controversial climate change denier and anti-vaxxer (and brother of former Labour leader Jeremy)—who, during the pandemic, released an unspeakable music video in which he sang “wearing a mask is like trying to keep a fart in your trousers” (the research is awaiting peer review)—was making a political point. This particular branch does not accept cash payments but tracks customers as they shop, before charging them via an app as they exit. Welcome to the cashless utopia—or, as Corbyn calls it, dystopia. Corbyn wasn’t having this Orwellian development. “So you people take that money—£1.19—and I will take my strawberries outside,” he tells staff, who, on the video he released on social media, can be heard remonstrating with him. “You can’t take that,” shouts one worker off camera, who advises him the police have been called. “If you want to call the police, call the police. I’ve paid my legal tender,” Corbyn retorts as he pushes his way through security gates designed only to be activated by the app. “Don’t break the doors,” shouts the same Aldi worker. Once outside, Corbyn victoriously hands out strawberries to supporters, telling them: “I paid by legal tender in this dystopian place.” “You should be able to spend whatever you want. It’s an absolute joke,” claims one of his supporters in the video. There are several problems with what Corbyn did, not least pompously calling supermarket workers “you people”. First, there is a Sainsbury’s opposite that almost certainly does take cash and whose strawberries, I imagine, are no less tasty than Aldi’s. More importantly, as critics pointed out on social media, Corbyn doesn’t grasp the concept of legal tender. “Most people think it means the shop has to accept the payment form. But that’s not the case,” explains the Bank of England website. “Legal tender has a narrow technical meaning which has no use in everyday life. It means that if you offer to fully pay off a debt to someone in legal tender, they can’t sue you for failing to repay.” In everyday life, shop owners can choose what payment they accept. If, say, you produce a £50 note to pay for a bunch of bananas, it’s perfectly legal for the shopkeeper to make sarcastic remarks and refuse your offer. Physical cash, to the chagrin of Piers Corbyn, is in decline. It’s in such short supply that, in August, the Treasury moved to protect public demand for physical money by announcing that banks would be fined for failing to provide free access to cash withdrawals for consumers and businesses. “Whilst the growing choice and convenience of digital payments is great, cash has an important and continuing role to play,” said Andrew Griffith, economic secretary to the Treasury at the time. “People shouldn’t have to trek for hours to withdraw a tenner to put in someone’s birthday card—nor should businesses have to travel large distances to deposit cash takings.” But if the supply of cash is being safeguarded, is there really demand for it? When did you last put folding money inside a birthday card? According to research published in September by UK Finance, a trade association, nearly 22m Britons used cash only once a month or not at all in 2022. Just under one million mainly used cash. Since 2017, payments made by debit cards (including with phones and watches) have overtaken those made in physical cash. The trend may have been hastened by Covid: during lockdown, fewer retailers than ever wanted the loose change from Piers Corbyn’s or anybody else’s trousers, for fear that it could spread infection. But the decline is structural: the volume of UK payments that do not involve cash rose from 45 to 85 per cent in the decade to 2021. That said, something striking happened once the pandemic eased. For the first time in a decade, cash payments rose—admittedly only by 7 per cent, but bucking the longer-term trend. Why? UK Finance cites anecdotal evidence that, in a cost-of-living crisis and with inflation for a while into double figures, people were finding it easier to manage their money by paying in cash. The long-term decline in the use of cash may be disappointing to one man in particular: King Charles III. After an unconscionable wait, the septuagenarian is finally getting his mug on the coins of the realm—only for fewer Britons to be using them than did during mummy’s reign. That, perhaps, is a shame, because these rather beautiful new coins that will enter circulation by the end of next year have been devised to nudge those who hold them in their hot little hands to behave in two socially interesting ways. Each coin has the denomination marked out in large numerals next to images of Britain’s flora and fauna, including threatened species. The 1p coin, for instance, depicts a hazel dormouse, which has seen its population halve since 2007. The 10p features a capercaillie, the world’s largest grouse, which can be found in a small part of Scotland and is threatened with extinction. Rebecca Morgan, director at the Royal Mint, explained the value of this new coinage: “The large numbers will be very appealing to children who are learning to count and about the use of money. Also, the animals and everything you see on these coins will appeal to children. They are great conversation starters.” One of the great joys of paying in cash is that it doesn’t leave a digital trail. Physical money can do things—encourage numeracy, make us realise that we need to act to save the capercaillie before it’s too late—that swiping a debit card on a soulless digital reader never will. The decline in cash is unwelcome, not just to Piers Corbyn, kings and the Mint’s design team, but to those who, voluntarily or otherwise, don’t have bank accounts. It may also create understandable worries regarding the drift of our digitised societies to a place where our every keystroke, phone call and eyeball roll is digitally harvested—by shady tech bros with shareholders to satisfy or authoritarian rulers with dubious agendas—in ways that would make Big Brother envious. Cash, by contrast, is an “anonymous bearer instrument”, meaning it is traded without any records and payable to anyone (the opposite is an “order instrument” payable to a specific person). The Piers Corbynites apparently believe that cash liberates us from totalitarian oversight by the Man. Unless, of course, you decide, as Corbyn did when he took his stand in Aldi, to film the exchange and post it online. If the Orwellian ramifications of the rise of cashlessness seem far-fetched, consider what happened in Kenya last August. As Corbyn was paying for his strawberries in Greenwich, thousands were queuing in Nairobi to have their eyeballs scanned as part of the “Worldcoin” project, founded by OpenAI chief Sam Altman. In exchange for handing over their biometric data, some 350,000 Kenyans each received 25 free crypto tokens, worth approximately $50. Those tokens were transferred directly via Kenya’s main mobile payments app, M-Pesa. The stuff of science fiction: a man has his eye scanned by an orb for biometric registration to the cryptocurrency Worldcoin © AP/Emilio Morenatti/Sipa Asia/Shutterstock But Worldcoin faced what’s being called a techlash over this project, not least because it seems like dystopian ­science fiction. A hardware unit known as an orb scans irises to identify individuals and prove they are human. Being able to verify a person in that way will be a key part of the new digital economy when artificial intelligence is integrated into society, according to the company. When the Kenyan government stepped in a week after Worldcoin’s launch in the east African republic to establish “the authenticity and legality of the aforesaid activities”, it was possibly also motivated by revulsion for what looks like a neo-colonialist enterprise predicated on plundering human data in ways redolent of how British imperialists violently exploited Kenya’s natural resources and human beings. Rachel O’Dwyer, lecturer in digital cultures at the National College of Art and Design in Dublin, argues that Altman’s Worldcoin project is symptomatic of a wider development of major digital platforms now trading your digital identity for “tokens”. “By tokens, I mean money-like things ranging from airtime credit and loyalty points to game tokens… and gift balances.” She also means NFTs—non-fungible tokens—unique digital assets that have turned the art world on its head, inducing it to value not boring old paintings and sculptures but works that exist only digitally. One of the great joys of paying in cash is that it doesn’t leave a digital trail “Tokens have always ghosted the economy of publicly mandated currency, but now they are coming to the fore, threatening to replace money itself. It raises the question: was money only ever a blip?” In her riveting new book Tokens: The Future of Money in the Age of the Platform, O’Dwyer argues that online platforms are becoming the new banks. Handing over one’s biometric data can be the condition of admission to this new monetary regime—something O’Dwyer describes as a Faustian pact. But what does it mean when online platforms become the new banks? The radical dream that began with bitcoin inventor Satoshi Nakamoto’s breakthrough 2008 paper, “Bitcoin: A Peer-to-Peer Electronic Cash System”, was to cut out untrustworthy middlemen—especially central bankers—and realise a radical future of decentralised authority. This would allow, for instance, Argentine citizens to escape hyperinflation by holding bitcoin, not pesos. This article is from the March 2024 issue Click here to explore more from this issue Nakamoto’s innovation was creating a distributed ledger, a global registry of transactions that was not held by a single bank—as with a credit card—but by thousands of independent bitcoin “miners”. Because this “blockchain” is held not by a single entity but—at least in theory—by thousands of independent users, the need for a central, third-party authority is obviated and banks might become obsolete. Fifteen years on, the dreams behind bitcoin’s radical reinvention of money have not been realised, just as the vision of a decentralised, uncensored, free internet foundered with its commercialisation. The cryptocurrency space has been taken over by charlatans and swindlers. The consequence of all this is that bank-controlled cash appears—at least to those of a dystopian temper—to be at risk of steady replacement by payment in tokens. And examples of that development are not limited to Worldcoin. Consider Amazon’s Mechanical Turk, a (bizarrely named) website owned by the vast retail multinational where humans are employed online to do tasks which, for the time being at least, are not suitable for artificial intelligence. This might include identifying specific content in an image or video, writing product descriptions or answering survey questions. The online ad reads: “Make money in your spare time. Get paid for completing simple tasks.” But, as O’Dwyer points out, thousands of Mechanical Turks without US bank accounts are not paid in cash—they are paid in Amazon gift cards. The world’s first large-scale crypto art exhibition in Beijing back in 2021 © AP/Emilio Morenatti/Sipa Asia/Shutterstock O’Dwyer argues such closed loops of token-based remuneration constrain freedom, and they are becoming more commonplace. In the United States, for instance, food stamps—known as SNAP benefits—are pre-loaded onto Electronic Benefits Transfer cards. These tokens can be used for fresh produce, but not alcohol. When I suggest to O’Dwyer that this sounds like fairly benign social nudge theory in action, she counters that the tokens cannot be used to pay for hygiene products such as tampons, nor for vitamins or medicines. What’s more, tokens can be used for cold deli food, but not for hot meals. “Why cold, pre-cooked meat but not half a rotisserie chicken? It seems there is something too ‘easy’—too idle, maybe—about buying a hot meal for your family using benefits.” But such nudging through state payments to change behaviour is not new. In 2003, for instance, the Bolsa Família card was released in Brazil, giving households below the poverty line a monthly stipend of around $20 per vaccinated child attending school. In 2016, the UN World Food Programme trialled a system that issued relief payments through the use of biometrics. Refugees were able to buy items from the supermarket using their biometric data as a credential: all they needed to do was scan their iris to pay for an item. The idea was to improve convenience and prevent fraud. Since that trial, the World Food Programme’s Building Blocks initiative has become the world’s largest humanitarian use of blockchain technology. The WFP estimates it is responsible for financially supporting four million people each month in countries including Bangladesh, Jordan and, since mid-2022, Ukraine. The WFP reckons this technology helps tailor its support to meet humanitarian needs. O’Dwyer, though, has worries about this trend. Tokens can also be programmed to specify not only an exchange value, but a whole range of other terms and conditions that must be met before an exchange will take place. “Whose values are these? We’re not at liberty to know,” O’Dwyer says. This, no doubt, is what worries Piers Corbyn and his followers. Unlike him, though, O’Dwyer takes heart from human subversiveness in undermining the values of state-­proffered tokens. In Ireland in the 1980s, she tells me, there was a surfeit of butter resulting from the EU’s Common Agricultural Policy. There was so much butter—piled into great butter mountains, or stacked up like what O’Dwyer drolly calls “Kerrygold bullion”—that butter vouchers were issued to the poor and needy, effectively padding out dole money with food stamps. “MUST NOT BE EXCHANGED FOR ITEMS OTHER THAN BUTTER,” said the booklet that held the vouchers. But people did, with the assistance of sympathetic shopkeepers. The vouchers were accepted as payment for tobacco, alcohol and groceries. “They were passed around. People even put them to non-transactional uses—as a decoy for insurance discs displayed on a car’s windshield, one official paper masquerading as another.” You couldn’t have done that with physical money. I mention this phenomenon to Jen Parker, operations manager of the Oasis Partnership, a charity for supporting vulnerable people in Buckinghamshire. Six months ago, the partnership became involved in a project in High Wycombe to cut down on anti-social begging through the creation of three “diverted giving points”. These are card terminals allowing locals to make donations to groups offering assistance with shelter, food, and drugs and alcohol support. The hope for Thames Valley Police and local councillors backing the scheme was that scammers posing as beggars, deprived of cash handouts, would cease operations, thus reducing street unpleasantness in the town. “We thought it was a good way to ensure funds were given to the homeless community rather than going to services that wouldn’t help that community,” Parker tells me. The scheme also had the added benefit of encouraging locals to donate by swiping their cards—crucial when fewer people than ever have cash in their pockets. “I’m as guilty of this as anyone,” Parker says. “I just don’t carry cash around anymore. In the past I would have donated that way, but for a lot of people that’s not happening anymore, so initiatives like this are important.” Beggars in China with QR codes which collect personal data rather than money © Sipa US “We weren’t sure how much money we would make from these diverted giving points, but we planned to consult with homeless people through our… outreach workers and supply them with what they want—sleeping bags, food, whatever.” Sadly, Parker tells me, after six months, the scheme has only raised £200. “Whether that’s because people are not aware of them [the card terminals] I don’t know, but we haven’t much to spend on marketing. It might also be because High Wycombe is not that prosperous. There’s a similar scheme in Windsor which is probably more successful.” When I mention how Chinese beggars are equipped with QR codes, Parker surprises me by being enthusiastic. “It’s quite enterprising and it’s not as though anybody’s being coerced. If people want to give away their private data to help homeless beggars, that’s their choice. My concern would be, is the homeless person being paid a fair wage for effectively doing this work for these companies? If they are, then it seems quite a good system. “We have to think of new ways of financially supporting homeless people because their days of shaking a bucket for cash are over.” Some have rebelled against the money fetish by burning vast wodges of cash If so, that is a big change. Physical money has been part of human society for thousands of years. In early 2021, the world’s earliest known coin-minting site was uncovered in Henan Province, China—it dates back to 650BC. Karl Marx detected a downside to the monetary phenomenon, writing that “Money is the alienated essence of man’s labour and life; and this alien essence dominates him as he worships it.” In our godless age, the money fetish has replaced earlier deities, which has led some to rebel by burning vast wodges of cash. For instance, on 23rd August 1994, the chaotic music duo and performance artists Bill Drummond and Jimmy Cauty burned £1m of notes behind a boathouse on the Scottish island of Jura. They are not alone in attempting to reduce money’s power by sacrificing it to the flames. Jonathan Harris, also known as MBG (Money Burning Guy), has been burning money since 2007. “I see it as a ritual of disenchantment,” he has explained, “a de-fetishisation of money, a rite that paves the way for a new narrative.” Harris, author of The Money Burner’s Manual: A Guide to Ritual Sacrifice, claims on burnyourmoney.org that he and like-minded cash destroyers have burned £7,266,157 since 1969. Piers Corbyn during his stunt buying strawberries at a cashless Aldi © Alamy Stock Photo/via YouTube Harris describes himself as high priest of the churchofburn.org, whose homepage is emblazoned with this quote from Charlie Waterhouse of Extinction Rebellion: “Church of Burn is a portal into a glorious world we’re not supposed to see. Where money’s power is revoked, reclaimed and reassigned. A sacrificial rite that rewrites our failing modern world. For once one has burnt money, what else is possible?” Such thinking suggests that Piers Corbyn doesn’t go far enough—he’s still championing physical cash. For those who despise money itself, we should worry not just about the rise of cashless Aldis but such bizarre new phenomena as Axie Infinity. This is an online video game based around NFTs—themselves a mind-bending new development in the way society (or the very online parts of it) understands value—and developed by Vietnamese studio Sky Mavis. The game involves breeding and trading cartoon axolotls (a type of salamander) which can be swapped for money and which earn tokens that can also be swapped for money. During the pandemic, players in the Philippines and elsewhere made this game their work, collecting digital cartoon characters and using them to cash out. They converted the airy nothings of cyberspace into pesos. O’Dwyer sounds an optimistic note. Irish butter mountain subversives from the 1980s have their spiritual descendants today. Tokens are “lively and subversive,” she writes. “Users online and on the ground remake them to be paid in informal economies, to communicate, to protest, and to reimagine money… Gig workers, streamers, gamers and sex workers still find ways of transforming online gift cards, wish lists… and gaming tokens into liquid cash.” The future of money, even if it doesn’t include the physical cash that Piers Corbyn romanticises, may not be entirely dystopian. Society Currency Economics Popular in Economics Daron Acemoglu: the opportunity economist Gillian Tett’s diary: How Keynes saw the crisis coming With a fat cat pay gulf this wide, it’s no wonder we’re facing a winter of discontent Related articles How Labour is getting business on board Can’t the generations just get along? Who will win the culture wars? Listen to our podcast Is the right-wing media ‘out of control’? Argylle: Taylor Swift's first novel? Or not... How the government captured the BBC About us Subscribe Contact us Submission guidelines Jobs and vacancies Institutional subscriptions © 2024 Prospect Publishing Limited Acceptable Use Terms of Use Contributor Terms & Conditions Privacy Complaints Advertising Guidelines",
    "commentLink": "https://news.ycombinator.com/item?id=39239449",
    "commentBody": "Physical cash is dying–and you don't need to be a conspiracist to worry (prospectmagazine.co.uk)231 points by rwmj 22 hours agohidepastfavorite457 comments happytiger 20 hours agoNot using cash means someone gets a cut of a processing fee. Every time, with almost no exceptions. I’m not down to have rent seeking companies taking a slice of my hard earned money just so I don’t have to manage a few pieces of paper. You’re generally paying 3-4 percent of every transaction now, as that’s pretty standard for debit cards on businesses, and it doesn’t matter if it’s charged to the card holder or the business, the consumer ultimately pays for it in higher prices. Heck, people pay transaction fees on payments they make the to government because the government won’t take cash. Do we really need to make not paying invisible fees in everything illegal? Seems crazy. reply noodlesUK 20 hours agoparentThere were already costs. There were always costs and risks associated with cash handling: armored trucks doing cash collection, risks of money in tills being lost. If it really were a lot cheaper you wouldn’t see businesses clamouring to go cashless. That’s not to say I think cash should go the way of the dodo, but I don’t think there’s that much difference paying g4s to collect your money vs Mastercard. IMO the benefits of cash are on the consumer, not the business. reply SoftTalker 18 hours agorootparentYes there are costs to handling cash. And they are vastly higher than the cost of handling bits. A 3% fee to write some bits to a database is insane. A 3% fee to pay all the people who need to handle the currency in a cash transaction is probably not covering costs. reply gizmo 18 hours agorootparentThat’s such a misrepresentation. The credit card fees exist because customers get benefits like delivery insurance, return insurance, holiday insurance, cashback, free credit, etc. If you just want to transfer bits you can do that for next to no fees (in Europe, where digital payments without consumer protections are common). reply atmartins 16 hours agorootparentCash back? Attention is finite and I don't want a payment processor forcing me to pay a fee so I can turn around and try and earn it back. How about lower the fee and let me keep that 1%?? Then I can earn interest on it for the month, not them. And then I don't have to read additional terms and conditions, use it or lose it, book travel through their marked up garbage store, etc. It's insane to me that you'd defend cash back as some kind of advantage to consumers when it's anything but. It's a crappy product that you can choose to avoid but I want the up front transaction fee reduced by the same amount. reply multjoy 14 hours agorootparentThey're talking about being able to take cash out at the point of sale, not the shit discount model. reply santoshalper 9 hours agorootparentprevMine is just literal cash I can apply to the balance on my card. Obviously the fact that cash back exists means that the card companies are making even more money on the fees. However, since most stores charge an identical amount whether you use CC or cash, everyone is collectively paying it and you might as well get the cash/miles/rewards back. The FCBA coverage, on the other hand, has some real value. That said, even though I have used it a few times, I doubt it has saved me as much as I've spent in CC fees. reply panarky 16 hours agorootparentprev> such a misrepresentation Exactly. All the talk about fees misses the point because accepting and securing paper money is also very costly. And the provocateur's stunt in the article also misses the point, because electronic payments for groceries aren't a dystopian tyranny no matter how loud he screams about it. And the crypto skeptics are missing the point when they say you can't buy a coffee with bitcoin, because buying coffee is a solved problem, tap to pay with a bank card works just fine. And the crypto advocates are missing the point when they say that lightning or solana or something else will solve the coffee-buying conundrum because that's not a problem that needs solving. The real point about cash payments and crypto is whether people should be able to make large person-to-person payments that are not trivially surveilled or controlled. In a free society, should people have this capability? Or are the dangers of illegal transactions so great that this freedom must be eliminated? Europe has mostly free and mostly instant SEPA transfers which are one answer. Large payments are easy and cheap, though they are not secret or untraceable. Perhaps that's a reasonable trade-off. The US payments system is a mess by comparison, where we have no privacy paired with extreme inconvenience. Try to buy a $20,000 car from a private party on a holiday weekend to see how well that works. reply trogdor 6 hours agorootparent>Try to buy a $20,000 car from a private party on a holiday weekend to see how well that works. I’m not convinced that spur-of-the-moment, trustless private-party $20,000 transactions are a significant problem. Every system has edge cases. reply StopTheTechies 16 hours agorootparentprevDebit cards have the same processing fees, the credit cards are a complete red herring. If anything credit cards are much more evil than just charging the processing fee. reply pxx 18 hours agorootparentprevYeah so you get consumer surplus if you pay 2.5% to a credit card processor instead of paying more than 3% to physical cash logistics. Everybody's happy besides your weird sense of \"insane.\" Furthermore you get a lot more than just writing a few bits with a credit card transaction. You can bypass the cc companies and cash logistics costs by taking checks. But certainly the revealed preference is to not do that. reply Ekaros 15 hours agorootparentprev3% is insane. As why many sane places in world have set much much lower limits... reply tyfon 19 hours agorootparentprevI work in a bank and cash handling is a major headache in terms of internal control, logistics, reconciliation and AML. It's a large extra cost. It is much cheaper for the bank and customers to use cards, but where I live the banks own a payment network that does not involve visa and mastercard so the cost is much lower and there is no rent seeking so card use have no fees. We also have an app for sending \"cash\" for free to friends, family and businesses that under the hood are just instant bank payments between the accounts. reply noodlesUK 19 hours agorootparentThe issue is, whilst I’m sure that cards are cheaper overall, they present a serious burden for people who can’t use them: unbanked people for various reasons, older people, children etc. reply tyfon 19 hours agorootparentChildren and old people here all use cards or apps. And derisking is illegal, so you have to provide a bank account and cards to anyone who wants. The high risk customers just need more follow-up on terms of AML. reply noodlesUK 18 hours agorootparentWhere are you based where derisking is illegal? Here in the UK it makes the news fairly frequently when someone notable becomes unbanked because of CIFAS or SARs or similar. Also, I wasn’t able to get a debit card until I was 12, but perhaps that has changed over the past few decades. reply tyfon 18 hours agorootparentI live in Norway. My kids got a card at around 5, but they are older now. Not with visa/mastercard of course :) reply qubex 18 hours agorootparentSounded like Italy. Same exact situation. The interbank payment consortium even created a cashless payment system called “Satispay” that lots of consumers and small retail businesses use on a daily basis. reply Ekaros 14 hours agorootparentprevHmm in Finland it seems there is \"Basic\" card. Which I think is technically just more limited VISA debit card. Preventing such things as overdrafts. reply multjoy 14 hours agorootparentprevIt rarely makes the news and it happens all the time, but usually because people don't see fraud as criminal until it catches up with them. reply asystole 17 hours agorootparentprevDiscussions like this on reddit/HN always get stuck in this loop of people projecting how things are in their part of the world onto the rest of the world and assuming everyone has the same problems/challenges/tradeoffs. reply lelandfe 18 hours agorootparentprevIf you’re homeless (and esp. if lacking ID), getting a bank account is a pretty tall order. reply tyfon 18 hours agorootparentCash is not illegal here, in fact shops are required by law to also accept cash. However the homeless problem is virtually non existent in this country, same with the lack of id. It's just so much more convenient to use digital payments that almost nobody use cash anymore. reply vixen99 18 hours agorootparentprevSome quote! \"I work in a bank and cash handling is a major headache\". Doubtless but perhaps we should reconsider the larger purposes of the banks in relation to their customers. reply tyfon 18 hours agorootparentThe biggest reason for banks to exist is too provide \"grease\" for the economy, i.e. to connect people with money to people who needs to borrow :) Most banks here don't handle the actual payments themselves except the biggest ones. The small \"savings banks\" like the one I work in doesn't have shareholders either, they are owned by the depositors. Almost every small town and community here has one. reply tomatocracy 16 hours agorootparentThe most valuable service banks provide is not just \"matchmaking\" lenders with borrowers (an exchange of some sort can do that much more cheaply) or even spreading credit risk across a pool of borrowers (again there are other ways to do that which are cheaper), but maturity transformation: depositors can lend money to the bank which is repayable on demand, but the banks' borrowers are given set repayment schedules. reply graemep 19 hours agorootparentprevI do not think so. Otherwise the British government would not have needed to outlaw charging customers more for card transactions to encourage the move away from cash. reply noodlesUK 19 hours agorootparentI strongly suspect that the reason cash is seen to be cheaper for these businesses is that they don’t report all of their cash income to their tax authority, rather than because cash is actually that much cheaper. Alternatively, they were counting the time they spent cash handling as a business owner as zero cost, which isn’t really true. reply graemep 17 hours agorootparentThose are reasons some people prefer cash, but it does not explain the need to ban passing on card fees. Someone willing to take the risk of the severe penalties for dodging taxes would not be deterred from offering discounts for cash payments because they broke another law by doing so. They can also avoid the law by not taking card payments at all. I have found that many people in businesses traditionally took cash payments do not take cards but are happy to be paid by bank transfer (which is usually free on both sides in the UK if the sender is not a business). The value of the time spent on cash handling may be very low. What is the opportunity cost of the time if it can be done in otherwise dead time? It is worth what value they attach to it. reply nikau 9 hours agorootparentThey can't just stop taking cards as they would lose a lot of customers. Most small places not declaring cash want to stay off the radar, they use tricks like using old cash resisters that don't have trackable invoice ids. Food places are rife with it as they can claim excess food was thrown out as spoiled. reply mab122 16 hours agorootparentprevI would argue that cash maybe a little better than cashless because fees are tied to physical problem of moving cash around where as cashless fees are just one board meeting from changing ten fold. reply poisonborz 20 hours agorootparentprevThese are even higher for digital, all the associated software development, audits, etc. That processing fee is on top of this, and is easily variable. reply pjc50 19 hours agorootparentYou need exactly the same level of software durability for cash handling; see the Post Office scandal, where the computer system was misrecording cash transactions. reply Am4TIfIsER0ppos 12 hours agorootparentprevIf it was cheaper the government wouldn't force you to take part in the scam under pain of death. reply dr_dshiv 19 hours agoparentprevHaha, Americans with their 2-4% transaction tax, it’s so funny. No seriously, when I first moved to the Netherlands I was wondering what kind of backwards country wouldn’t let me use a credit card at the grocery store. And that’s when I learned about how crazy it is that credit cards and even American debit cards have such needlessly high transaction fees. reply hhh 19 hours agorootparentI use an Visa tied to an American bank every day for groceries in NL. I only have ever had issues using a card at a very small record store, I just make sure to carry 100eur in cash just in case. I do have more issues online when something only takes iDEAL though. reply somenameforme 18 hours agorootparentI'm sure you're aware of this - though others may not be: another trick US banks use is gouging you on exchange rates. My bank was charging something like a 0.5% fee visible fee and then an invisible fee several times that in exchange rate gouging. I found trying to use US banks abroad an absolutely painful experience, to the point I swapped my banking abroad which is itself a huge PITA thanks to FATCA and other stuff making Americans borderline persona non grata at most foreign banks. reply xadhominemx 15 hours agorootparentBanks all over the world make healthy margins on cross-border transactions involving a currency cross. It is not unique to American banks at all. reply newaccount74 13 hours agorootparentMy EU bank charges less than 1% currency exchange fee. US banks have charged me 5-8%. It's not the same. reply xadhominemx 13 hours agorootparentNo way a US bank charged you that amount unless you made the purchase in an esoteric fashion. reply xadhominemx 16 hours agorootparentprevYou get 95% of it back in the form of rewards, so it's not really a tax at all. reply dr_dshiv 16 hours agorootparentIt seems kind of like a scam, though, credit card rewards. I don’t know enough about it, I generally don’t like participating in those sorts of things. Do you have a ref on the 95%? reply hackstack 14 hours agorootparentLet’s just take 95% as a true figure … first of all the 5% you don’t get back is a straight up loss. The rewards you get at the end of the month are fine (eg cash back) but you’ve missed out on earning interest on the money you “get back” reply xadhominemx 14 hours agorootparentCard network fees (the part credit card users don't get back) are about the same for the European payment systems. For card users on net there is essentially no difference. The interest argument of course is nonsense because the opportunity cost ia outweighed ~50x by the interest benefit you receive from the entire purchase going on credit vs just the 2% interchange. reply hackstack 5 hours agorootparent“Of course”. Thanks for responding — can you clarify this interest benefit you are talking about? reply Remnant44 2 hours agorootparentYou are able to buy now, but not pay until 30 days later, paying no interest unless paid in full. Which means YOU earn interest on your money during that period... reply xadhominemx 15 hours agorootparentprevIf you don't know anything about it, why would you call it a tax? reply DANmode 13 hours agorootparentprevIn exchange for signing away usage of your personal data/legally agreeing to participate in ad targeting. reply namaria 40 minutes agoparentprevUsing debased money (does the paper in the note have any value?) is already submitting to paying seignorage to the issuing authority. There's not escaping it unless you sit on the top of that food chain. reply moooo99 20 hours agoparentprev> I’m not down to have rent seeking companies taking a slice of my hard earned money just so I don’t have to manage a few pieces of paper. You’re pretending as if there weren’t any costs associated with handling cash. There’s almost always a handling fee if you want to put the cash into an account, which you will need to do if you want to pay people that are somewhere else than you are. If you have larger quantities of cash, you’ll also have to pay for safe transportation of the funds. If you make an honest comparison reply happytiger 20 hours agorootparentI’m not. Visa and Mastercard collected $138 billion in service fees in 2021. They are among the most profitable companies in the world. You’re pretending that cash handling is a binary. You need a blend. For example, after extensive testing we are not going to send wire transfers exclusively in gold bricks. Though the loss weighs heavy on me. But you should be very careful in eliminating cash. I assume cash handling fees are made up by banks who want more transaction fees. That’s not a real thing in my experience. I many times have gotten a discount for cash. I have not shopped a few places because they don’t accept cash. But I have yet to be charged a cash fee by any of the banks I use. And they have insane surveillance data on their customers from doing so. Data that is used for all kinds of unintended purposes. Cash is also one of the last strongholds left to maintain anonymity and keep purchasing information truly private without handing heaps of data to the corporations that make a buck off your receipts—or worse, to hackers, in the case of a data breach. To say nothing of the unbanked… which is a massive issue. If that’s the future you want, we don’t see eye-to-eye. You are giving up a LOT more than you realize. reply gruez 20 hours agorootparent>Cash handling fees are made up by banks who want more transaction fees. That’s not a real thing. I have yet to be charged a cash fee by any of the banks I use. That's because you presumably have a consumer account. If you had a business account and were depositing thousands in 20 bills they're probably going to charge you fees. reply DANmode 13 hours agorootparentRemember when banks made their money by taking in as many deposits as possible, and then lending that money out? reply fzeroracer 19 hours agorootparentprevThe fact that you immediately assume that something doesn't exist because you don't encounter it is insanely presumptuous of you. The entire reason why small businesses moved away from cash is because its cheaper even taking into account fees. Food trucks, popups, small merchant kiosks. Not needing to take cash means far less overhead. You're more than welcome to argue the benefits of cash transactions and in most cases I would agree. But you should not miss the truth of the situation. reply newaccount74 13 hours agorootparentAnd danger to employees! My partner used to work as a waitress, and was always very nervous when she had to go home with a days revenue when she was the last to leave. Electronic payments are much safer, no physical cash someone can steal. reply pasttense01 6 hours agorootparentprevNo, it's not the entire reason. If a store doesn't accept customers' preferred payment methods some of them will switch to competing retailers who do. reply caeril 20 hours agorootparentprev> There’s almost always a handling fee Citation needed. I've been depositing cash for years, with multiple banks, and nobody has ever charged a \"handling fee\". > safe transportation of the funds Yes, a lot of people have this irrational reaction to carrying large amounts of cash, as if your unremarkable Toyota miraculously sports a giant \"BRINKS\" logo every time you carry cash, attracting the attention of would-be criminals everywhere. I assume you believe that thieves and pickpockets are regularly carrying millimeter-wave scanners to see what's in your pockets, briefcases, backpacks, etc? reply gruez 20 hours agorootparent>Citation needed. I've been depositing cash for years, with multiple banks, and nobody has ever charged a \"handling fee\". It's a fee associated with business accounts. >https://www.bankofamerica.com/smallbusiness/deposits/resourc... >Cash Deposit Processing No fee for first $7,500 in cash deposited per statement cycle at an ATM or Financial Center; then $0.30 per $100 deposited thereafter reply Espressosaurus 19 hours agorootparent0.3% vs. 3-4% reply jasode 19 hours agorootparent>0.3% vs. 3-4% No, the total costs of handling cash currency & coins for large businesses will still add up to 3% or more which is in the range of the fees of credit-cards processing. Costs of accepting cash also includes: - paying for armored trucks and security guards to transport bags of cash to the bank - percentage of cash lost to crime such as --- employees/cashiers theft/skimming by handling cash, --- robberies, --- counterfeit bills https://www.google.com/search?q=retailer+total+costs+of+hand... For a business, a true arbitrage of not paying 3% credit-card fees would be convincing customers to pay by check or electronic ACH bank account withdrawal. Those payment methods avoid the costs of managing loose cash and also avoid the fees from the credit-card processors. But they also have the same privacy concerns as CCs if customers care about that aspect. reply Ekaros 14 hours agorootparentTime spend managing the tills that you do not either have too much money at one time or have enough money always to give change... Oh and at the end of shift counting the money... Like today I returned some deposit bottles and they had run out of 20€ bills... Which should be the most common ones. Going on trip so I preferred 10€ bills, but still... reply canucker2016 12 hours agorootparentprevIn Canada, from a 2009 article, https://www.cbc.ca/news/card-costs-who-pays-what-to-whom-1.8...: ==== How we pay - Credit cards account for 31 per cent of transactions. - Cash accounts for 29 per cent of transactions. - Debit cards account for 26 per cent of transactions. So what does it cost to process a transaction? The Bank of Canada survey looked at the estimated cost of processing a $36.50 transaction, which was the median cash transaction in its survey. Costs broke down like this: Debit card: 19 cents. Cash: 25 cents. Credit card: 82 cents. The study notes that the overall cost of electronic transactions falls as the volume increases, which the authors suggest, may encourage greater use of electronic payments. ==== reply multjoy 14 hours agorootparentprevSquare will do you 1.75%, no contract, no fees and now you don't even need a physical reader as you can do contactless payment on your phone. You can get it lower with other providers if you negotiate right. Nobody is paying 4% for any cardholder-present transactions. reply gruez 14 hours agorootparent>Square will do you 1.75%, no contract, no fees Where? A quick search on their website shows 2.6%: https://squareup.com/us/en/pricing#rates reply multjoy 13 hours agorootparentEverywhere, apparently, other than the US. You should fix your banking system. reply emeril 13 hours agorootparentsomeone has to pay for our big rewards program!!! :) often as high as 2-2.5% reply solatic 19 hours agoparentprevBanks sometimes charge fees to make cash withdrawals from ATMs, even ATMs belonging to the same bank that holds your account (true at least in my personal case, outside the US). If you are paid via direct deposit, you're financially incentivized to stay cashless. The direct cost of spending via debit/credit card (if one doesn't carry a balance) is zero, compared to the cost of withdrawing cash. If you want to inventivize salaried professionals to pay with cash, try outlawing fees on ATM withdrawals first. reply tgsovlerkhgsel 18 hours agoparentprevThe EU has indeed banned most of the rent-seeking on payment cards by setting upper bounds on various fees that can be charged. reply marcosdumay 17 hours agoparentprevI pay a fixed monthly fee for my bank that comes with full access to the interbank payment system. Every business on my country does the same for the other side, and we get unlimited transactions with no marginal cost. Anyway, as somebody else said, a 3% fee on transactions is just ridiculous. If you get any bit of competition, transaction-value based fees tend to be on the order of 0.5%. reply aembleton 15 hours agoparentprevMaybe the government should limit the cut that can be taken to something like 1%. All it would require is some legislation, reply caldarons 17 hours agoparentprevmaybe we should start thinking of payment infrastructure as a core public service if we move away from cash? Just like there is an official entity that can \"print\" cash, we should start doing the same with digital payments (I believe the EU is doing somethig similar). That way you don't have a duopoly taking a cut of every transaction. reply Gare 17 hours agorootparentThis is called CBDC (Central Bank Digital Currency) and it is being worked on. But there are concerns about state surveillance and misuse. reply lobocinza 6 hours agorootparentCBDCs are a pandora box that once opened will not be closed. At best they will use for massive surveillance which is already scary. reply Aerroon 17 hours agorootparentprevGovernments should be running their own digital counterparts to cash, but they need to add the anonymity of this cash into the constitution. Otherwise the security apparatus is going to ruin it for everyone. reply krapp 17 hours agorootparent... because the security apparatus is definitely held in check by the Constitution? reply scotty79 19 hours agoparentprevExchanging value between two economical entities using pieces of paper seems crazy when you have better ways. Try to explain buying with cash in 200 years to someone who never seen it. What if I need to pay just a little, do I tear a bit of the paper? No, you can't tear. If your piece of paper is worth too much then the seller gives you other pieces of paper worth less. Can I throw out the rest of paper after I'm done buying? Not unless you want to lose a piece of your value. What if I don't have enough pieces of paper on me? Then you can't buy. So I should just carry all my worth in pieces of paper on me at all times? That's a bad idea because when you loose this paper or it gets stolen or destroyed you no longer own their value. Where do I get the paper from? Your employer can five it to you every month or week and you hold it in your home. Alternatively employer transfers the value to the bank normal way and when you need paper the bank can dispense it to you through various devices or service points. Are any of those devices handheld? Unfortunately no. What if my house get burned down or robbed? Then you lose all the value associated with all the papers kept there. And so on. reply nayuki 18 hours agorootparentExchanging value between two economic entities using primitive card numbers seems crazy when you have better ways. Try to explain buying with a credit card online 200 years ago to someone who has never seen it. You type in about 23 digits (16-digit card number, 4-digit expiry month, 3-digit verification code) to make a payment. But these digits only change every few years as you renew your credit card. So many parties will learn of your digits and you won't know who leaked them. The merchant on the other end can take out any amount of money from your account; it's pull-based, not push-based. The merchant on the other end can trap you in a recurring subscription without your knowledge or make it very hard to cancel. People have credit limits in the thousands of dollars, and in any single transaction you are liable to lose that much money. Meanwhile, you can't lose more than the hundreds of dollars of cash in your wallet. You can call your card issuer to dispute charges, but now you're at the mercy of an intermediary. The intermediary skims about 2~4% of every transaction, for the luxury of being able to spend your own money. You can't process certain types of transactions in an offline environment, such as debit cards during a power outage or on an airplane (though Wi-Fi is slowly becoming more common). And you think cash is crazy? reply neuralRiot 12 hours agorootparentJust yesterday, i went with my friend to some place, we park but the spot is metered, the park meters have been replaced by a “more convenient” digital system, there used to be machines where you could pay and print a ticket, of course this was still inconvenient so the machines were removed altogether and now it’s an app-only system, wonderful but there are several apps for different zones so if you go to an unusual place chances are you need to install a new app then enter the location, the time you want to park, the plate number, a picture of the tag, the payment info, the 2FA code and receive the confirmation. All this replaced the inconvenience of popping a quarter in the meter. reply scotty79 12 hours agorootparentprevI have almost never done that. I just touch my personal computing device to a payment terminal and it's done. I agree that giving anyone your card number is insanity. I only did it few times in hotels and it required tremendous leap of faith from me. To be honest I should have gotten plastic prepaid for this purpose. reply dudefeliciano 18 hours agorootparentprevI don't think OP was arguing to completely get rid of all wired transactions. And for all the points you made you still missed OPs major gripe, paying a third party for every single transaction you make. Wanna give your children pocket money? bank gets a cut...doesn't seem right tbh reply scotty79 12 hours agorootparentBut you pay for facilitating the service of value transfer. If you do it by hand yourself the effort burdens you and the seller. You might argue that the cost is too much and maybe it should be state run and paid from taxes since it's the state that provides the service of money. reply cubefox 19 hours agoparentprevThere are now systems that are far cheaper than credit cards or debit cards. In the US it's called: FedNow. The system enables direct bank transfers in real-time, almost for free. https://en.wikipedia.org/wiki/FedNow The support is still very limited, but in India a similar system (UPI) managed to become ubiquitous. reply pasttense01 6 hours agorootparentWith Visa/Mastercard... debit and credit cards you get some purchase protection. With those direct bank transfer systems you don't. Have you never done a chargeback on a credit card? With Fednow, UPI etc you can't. reply santoshalper 9 hours agoparentprevIt definitely matters who it is charged to. In almost all cases, it's just rolled into the overall price of the item, which means you pay it whether you use cash or not. It would be very interesting to see if prices for card were 3% higher across the board, if people would still use them as much. I honestly have no idea. reply phpisthebest 20 hours agoparentprevThe reality is most business prefer the 3% fee because the cost of Cash processing is higher when you factor in Bank Fees, Security, Accounting, Theft, and other factors. [1] https://www.businesswire.com/news/home/20180130005244/en/New... reply beAbU 20 hours agoparentprevAnd yet, when last did you get a 3-4% discount when paying cash? I remember that from the 90s and 00s. In fact, I have seen some small places ask a 3-4% surcharge for cash payments, because banks don't process cash for free any more. reply Workaccount2 19 hours agorootparentI have never seen a place with extra fees for cash. However I know many small shops that give discounts for cash or only take cash for small purchases. reply fullspectrumdev 20 hours agorootparentprevA lot of places around here charge a bit less for cash payments. reply j7ake 19 hours agorootparentprevCash only restaurants are often 10-20 percent cheaper than comparable restaurants. reply jusssi 19 hours agorootparentThat would be because at least some of the cash bypasses the cash register and the tax report. Edit: alternatively they might not have same profitability requirement as other restaurants, because a cash-only restaurant is perfect front for money laundering. reply ghaff 19 hours agorootparentAnd they're probably just not comparable restaurants on average. reply SoftTalker 18 hours agorootparentprevMy barber only takes cash. $18 for a standard haircut. reply progbits 16 hours agorootparentThat's not because of card fees but because of taxes. They can underreport their cash income, but can't do that if the bank and card processor are involved. reply thuuuomas 18 hours agorootparentprevEvery gas station in my area offers a lower cash price. reply ImJamal 5 hours agorootparentI'm guessing it could be because when you pay cash you go into the store which causes people to buy stuff and they more than make up the cost. reply nonrandomstring 21 hours agoprevThere are two fallacies I frequently feel the need to debunk in this discussion. - Cash is anonymous - Cash is an \"old\" or \"low\" technology. When a serious crime is committed you might be surprised how much the police can do to trace cash. Serial numbers in and out of ATMs and shops are tracked, and frequently your face is captured by cameras in stores, ATMs and vending machines. Modern cash is high technology. Look at a UK or European note already. Holograms, nano-technology, advanced materials science... It's well past the point of being worth forging, and it is likely that future banknotes will use einks, embedded printable passive electronics to do all of the things people hope or fear of purely digital cash, such as time-out, store variable value, and be more traceable. The function cash really performs is a resilience buffer for person to person transactions that works independently of electrical power and Internet. Smart people at the top are never going to let that slide, because it would be national security suicide and practically inviting civil unrest. Cash, in evolving forms is here to stay, but it may not have all the properties we think it has and defend it for. reply dahart 17 hours agoparentWhat are you responding to? Cash is anonymous, by and large. That’s a fact, not a fallacy. I’ve never in my life seen any shops track serial numbers of cash transactions, nor associate them with me. I’m skeptical that anyone does it, but I welcome any evidence of this dubious claim that you’d like to provide. Having your face capture by a camera does not track the bills you used, nor the items you purchased. You’re spinning a mostly false tale. E-money on the other hand tracks everything you buy, what you buy, when you buy, what you paid, where you bought it, and that information is used to construct a valuable profile of your purchasing habits. That profile is used for advertising. But it could also be used to deny insurance claims. Should be illegal, but I knew someone who worked for a credit card company that informed me they were thinking about how to do this legally. E-money is tracked on everyone for everything by default, and cash does non of this. It’s insanely expensive for the police to try to trace cash, and they can only do it for one criminal at a time. They do not, and cannot, track everyone’s cash. Spending profiles captured via e-money tracking are captured by private corporations and they are offered for sale. Any criminal cash tracking by police is used solely for capturing criminal and is otherwise not available to anyone else. > Modern cash is high technology Again, who’s even arguing? This is a straw man, the context is digital money. There was no debate about how much technology is used for printing bills. > Cash, in evolving forms is here to stay So you say? I’m not going to argue or pretend to make predictions, but one thing we know for absolute sure is that there is a downward trend of cash transactions globally. reply coldtea 21 hours agoparentprev>When a serious crime is committed you might be surprised how much the police can do to trace cash. Serial numbers in and out of ATMs and shops are tracked, and frequently your face is captured by cameras in stores, ATMs and vending machines. Which is neither here, not there. Cash is effectively anonymous for 99% of uses that don't involve \"major crime\". Your buying something is not logged anywhere, for example. reply nonrandomstring 20 hours agorootparent> Which is neither here, not there. Cash is effectively anonymous for 99% of uses that don't involve \"major crime\". Exactly. Almost all cash transactions remain anonymous because there's no criminal investigation involved. Most people are honest and just going about their business with friends and family. That's how it should be. A cost bar for tracing is a desirable feature for non-authoritarian societies. Digital currency mediated by bank apps, active cards, smartphones or whatever, makes for \"convenience\" but also zero cost to spy on people who are not criminals. And that's a problem. So maybe it is here. Or there. I'm not sure where you're standing :) reply grobbyy 20 hours agorootparentprevYou fell for a simple fallecy. The line isn't that cash is used in a major crime. It's that someone in power wants someone in prison. 1. You might not be involved in a major crime but you might be a suspect. Lots of innocent suspects get convicted. 2. You might be a whistleblower. Lots of whistleblowers get framed. Etc. \"If I've done nothing wrong I have nothing to hide\" didn't work out well for Soviet Russia or Nazi Germany. A lot of the reason many of us want privacy preserved is to prevent a similar path here. Totalitarian governments don't always pop up overnight on a revolution; they often creep up on you. The creeping process is also often: 1. We'll track X with major checks on privacy; it will only be used for terrorism and child abuse investigations and held in a secure vault 2. We're just doing a better job of sharing / using data we already collect Neither off those is objectionable, but together, you end up in a distopia. reply solatic 19 hours agoparentprevYou're making an argument which is a variant of \"if I'm not doing anything wrong, then I have nothing to hide.\" Privacy is a worthwhile value in and of itself, not least of which because different people have different notions of what \"wrong\" is and their conception of \"wrong\" changes over the course of their life. An example list of completely legal transactions which you might not want someone in your life knowing about: a) You don't want your spouse to know that you bought some beers / fast food / pornographic magazines / cigarettes / other vice b) You don't want your employer to know that you saw a therapist / oncologist / family planning services c) You don't want your parents to know that you bought contraception d) You don't want your car insurance provider to know that you purchased a repair from a mechanic (i.e. you don't want your rates increased). Making payments in cash is making a decision for them not to be entered in some database which may be hacked and sold for profit to someone who values their interests more than your privacy. reply nonrandomstring 19 hours agorootparentWith respect, you're reading too much intent into my words. I am vehemently pro-privacy. In fact I seriously consider over-intrusive authoritarianism to be a mental health disorder and have read somewhat into the works of Adorno etc [0,1] (cue a dozen responses bemoaning how everything done at Berkeley in the 1960s is now-debunked leftist rubbish) Part of that is helping people to have a realistic but not paranoid view of technological society. My work also means I help people who have serious crimes or security issues to solve, and have legitimate investigatory needs. I think privacy violations are not rooted in economic greed, but in unboundaried, controlling impulses that get encoded into technology. Hope I am responding respectfully and appropriately, sorry if I got you wrong. BTW, in a safe way (ie not online) take one of those \"authoritarian personality tests\". It will shock you how much we have hidden in ourselves just ready to be provoked by demagogues or disinformation. [0] https://en.wikipedia.org/wiki/The_Authoritarian_Personality [1] https://www.psychologytoday.com/us/blog/rethinking-mental-he... reply dazc 20 hours agoparentprevWhat the police and other authorities 'can do' is a world away from what they 'actually do', in 99% of cases. You could be unlucky if your crime passes a certain threshold or affects someone with power but, most of the time, it will not be investigated with the resources of the state. reply nonrandomstring 19 hours agorootparentExactly the feature desirable to everyone in society. Including the police. I had a funny conversation with some LE types the other day (in the context of something we call \"fly-tipping\") about how, in theory, everyone who litters could be brought to book because they leave their fingerprints all over stuff. Nobody in forensics is ever going to do that unless it's a very serious case. Nobody wants to be compelled to investigate some bs crime just because its possible. Make things too convenient and you've got China. reply poisonborz 19 hours agoparentprevYou mix things up. Most of the tech you describe is about making sure that bank note is not fake. The tracking part is ad-hoc and shoddy, works only in dense urban settings, closed systems like banking or a big supermarket. Cash usage itself is anonymous. reply yyyk 20 hours agoparentprev>how much the police can do to trace cash It doable but not at all trivial. >It's well past the point of being worth forging The news seem to indicate plenty of forgers still, e.g.: https://www.europol.europa.eu/media-press/newsroom/news/poss... >it is likely that future banknotes will use einks einks are too expensive right now? reply nonrandomstring 18 hours agorootparent> einks are too expensive right now? Maybe yes. But track their falling cost and you'll see where the price point intersects with near disposable cost. European transport tickets made of cheap plasticised card only took a couple of years to incorporate RFID - that's already a long way toward low cost \"intelligent cash\". For a brief few weeks in the 1980s we had \"phone cards\" with stored, decrementable value that you could use a few times. Purely passive and using an optical technique to see where pressure had been applied to plastic (which turns milky). In Hungary in the communist era I saw tram tickets using ingenious mathematics (an ephemeral mechanical key set by the driver) and punched paper holes to provide multi-use. It's amazing what can be done to make cheap or even disposable portable artifacts to transfer value. reply yyyk 17 hours agorootparent>track their falling cost I do recall that patents/trademarks/tech are owned by E-Ink inc., and one of the biggest reason for the cost is them gouging the market? That mechanism can keep high prices for a long time. reply braunboffel 7 hours agorootparent> I do recall that patents/trademarks/tech are owned by E-Ink inc., and one of the biggest reason for the cost is them gouging the market? That mechanism can keep high prices for a long time. recall? gouging? You mean you have data showing is not just simple volume issues? reply Fredkin 20 hours agoparentprev> Smart people at the top... Who exactly? Where are these people in power defending physical cash or recommending an alternative monetary medium resilient to grid failure? reply nonrandomstring 20 hours agorootparentYou know that's the best question anyone has asked me on HN. I simply said \"smart people at the top\". I didn't say there were any. reply mdp2021 20 hours agoparentprev> anonymous ... you might be surprised how much the police can do to trace cash That is not the point. The point is in the contract - e.g., that you are not accepting to untick a \"do not track\" flag. That circumvention is always possible is not what matters there. reply gn4d 6 hours agoparentprev> Serial numbers in and out of ATMs and shops are tracked lmfao what. Unless someone is committing large scale crimes involving bands, no one is tracking some small bills between private citizens, some stores, more citizens, et al. For the vast majority of people, cash is anonymous. No advertising company or government is obtaining tracking data on your purchase of an apple and a cup of coffee with cash. reply hasty_pudding 21 hours agoparentprev> Serial numbers in and out of ATMs and shops are tracked Is this true? When I buy something from my local gas station is there a system in place tracking every serial number exchanged on the cash? reply 34679 20 hours agorootparentNot in the US. There are no laws requiring businesses to do so. Why would employers spend the resources required for such a thing? reply nonrandomstring 20 hours agorootparentSome scenarios I hope you see as obvious: - you take out cash from an ATM. The machine is loaded with sequential or logged order of SN that can be matched to withdrawals and times. - you take that cash immediately to a vending machine or supermarket auto-checkout that takes notes. The machine scans all notes it ingests and can match them to timings and purchase details. - Even a small shop might have a camera at the till, along with a UV light frequency used to check for forgeries. Obviously as the time between withdrawal and re-appearing in a system grows the strength of the association gets lower, as notes change hands. But in a serious crime investigation that doesn't matter, it's a good-enough capability to warrant chasing. reply 34679 18 hours agorootparentI haven't commented on the ATMs, I was only replying to the question, not the statement that preceded it. I have owned a retail shop. There are plenty of rules and regulations to worry about, and labor is a massive expense. At no point would I have been inclined to purchase equipment or pay for labor to track serial numbers on bills without a specific mandate to do so. No such mandate exists. reply jijijijij 15 hours agorootparentprevYou are describing the possible collection of circumstantial evidence, not systemic surveillance inherent to cash. Completely different thing than the transaction logs of electronic money transfers. There is a reason why governments don't want large cash transactions. It enables illegal sales, tax evasion and money laundering. Dude/ette, if you think the police can track a single 20€ bill that precisely, they wouldn't have a problem seeing through 500k€ cash deals when drug money goes legit through real estate. Basically every restaurant and cafe on earth uses cash to do creative accounting. However, of course, there are particular crimes where money tracking does matter. If you rob a bank or break into an ATM your cash prize comes with strings attached, as those known bills will get flagged. I think the most important implication would be these bills becoming actual evidence instead of an happenstance mysterious cash finding. If a bank scan finds them in circulation again, they may be able to narrow down your location, do statistics with mobile network data and so on, but it's not like every note is tracked by default at every endpoint. Crime runs on cash. How often have you read about anyone getting caught on cash serials?? Why on earth would any commercial store invest in cash tracking? They only got one point of the cash flow, are you implying they are all connected in some grand conspiracy?? Such stores got an abundance of way more accessible and relevant customer data they could (super illegally) collect, like biometrics, RF device IDs, loyalty programs, ... I also never seen a self checkout store which accepted cash at all. You are telling a fatalist narrative which only helps the abandonment of cash, especially for small, daily transactions, which matter most for privacy. Crypto won't happen. Lol. reply nonrandomstring 9 hours agorootparentSorry I missed this reply and hope you still see mine. > You are telling a fatalist narrative which only helps the abandonment of cash, especially for small, daily transactions, which matter most for privacy. I hope that's not true and this is the only point I dispute. Yes cash can be tracked, down to a single note, but it's extremely hard and expensive, as most of us here with a little more knowledge understand. No big conspiracies, just a sobering appraisal of the state of the art. What that knowledge does is debunk the political lie that digital cash is the only way to fight crime and money laundering etc. Now, we were going to do a Cybershow episode on drug dealers who accept Google and Apple pay, but shelved it because frankly I'm out of my depth journalistically and can't protect sources or navigate the risks of side effects. But it happens, and those companies collude (because it must be bloody obvious from the data). Meanwhile I think the front line is dispelling the \"convenience myth\". Convenience is such a dirty and dishonest word that most-times stands in for \"something that makes me feel good\". In reality I can pay cash for a round of drinks and tip the barmaid before my mates have got their phones out their pockets. reply seszett 19 hours agorootparentprev> - you take that cash immediately to a vending machine or supermarket auto-checkout that takes notes. The machine scans all notes it ingests and can match them to timings and purchase details. I can tell you they don't do that, at least for the systems I know in Europe. And for manual systems obviously the cashier doesn't record serials and match them to transactions. reply nonrandomstring 19 hours agorootparentI'd love to hear more about what you know about the systems you've examined or designed. PM me at cybershow.uk and we might do an episode and bring in some other engineers too. Specifically can you prove (or give compelling anecdotes why) the linear CCD array used to determine the legitimacy of the note cannot, on internet connected machines, retain and store that scan? I know that's a tall order, because I'm asking a negative. reply seszett 18 hours agorootparentI haven't designed such a system and as you say I can't really prove anything, but the embedded software I worked on just didn't record anything like that. Of course one can always imagine a secret government firmware or whatever, it's just highly unlikely IMO. The most obvious proof I'd say is that there is no law mandating this, so nobody is going to bother doing it just for fun. reply pzo 17 hours agorootparentprevHow about money counting machines? Medium and bigger shops probably have those and if not they probably at least every few days bring cash to bank to deposit on their account. Remember supposedly most printers were printing some semi invisible serial codes - for tracking whistleblowers and leaks. Probably cannot easily prove that ATM or money counter machines manufactures are not doing something similar. reply multjoy 14 hours agorootparentMoney counting machines do it by weight. They're certainly not scanning and recording the serial numbers. reply grobbyy 20 hours agorootparentprevBecause it's basically no resources. If they're not doing it now, they'll be doing it in a few years, and if not banned, selling the data. It's literally adding a 50 cent camera to a cash register. reply hasty_pudding 20 hours agorootparentwhere do you set up the camera to track the serial numbers and the person's identity that they're being given to? reply hasty_pudding 20 hours agorootparentprevwhy would the person I'm responding to say such a thing? reply 23B1 18 hours agoparentprevThe only fallacy here is that you expect anonymity to be 100% provided by cash. reply huytersd 18 hours agoparentprevCash is very easily tumbled or made anonymous. It’s as simple as going to a casino, buying a large number of chips and then cashing those chips out again. reply phpisthebest 20 hours agoparentprev>>When a serious crime is committed you might be surprised how much the police can do to trace cash. Serial numbers in and out of ATMs and shops are tracked, and frequently your face is captured by cameras in stores, ATMs and vending machines. Go to bank, take out 100's, to small mom and pop get change... problem solved. Further it is highly unlikely that if some bill I got out of ATM finds it way to a crime scene or something that they will be proof of anything a defense lawyer would shoot that down in about 2 secs >> It's well past the point of being worth forging Not really. >>future banknotes will use einks, embedded printable passive electronics to do all of the things people hope or fear of purely digital cash and those should be opposed as well reply mypastself 21 hours agoprev> [On] 23rd August 1994, the chaotic music duo and performance artists Bill Drummond and Jimmy Cauty burned £1m of notes behind a boathouse on the Scottish island of Jura. They are not alone in attempting to reduce money’s power… They literally increased the purchasing power of each remaining pound by doing so. reply huppeldepup 21 hours agoparentThere’s a documentary called “ watch the K foundation burn a million quid” well worth watching. reply furyofantares 20 hours agoparentprevYou might argue they distributed the money amongst all existing money-havers, with each getting a microscopic share in proportion to how much money they already have. In that case it's a donation primarily to the rich. But it's also no different than if they stuck it under a mattress for 10 years, at least until the 10 years is up. So that's also sort of a donation primarily to the rich. Of course when the 10 years is up and they spend money, more of it will wind up in rich people's accounts than in anyone else's, if not immediately then eventually (otherwise they wouldn't stay rich.) reply ProllyInfamous 18 hours agorootparent>In that case it's a donation primarily to the rich. [laughs, in disappearing bitcoin] reply CraigJPerry 20 hours agoparentprevThe Bank of England destroys between 10-30 billion pounds worth of notes per year (no longer fit for circulation). Sterling has other issuing banks too beyond BoE. I don’t think it’s possible to measure 1m notes going absent by any observable metric. Even if it were possible, the purchasing power wouldn’t change - you coffee shop wouldn’t reduce their prices but they might adjust staffing levels in an extreme case (not remotely likely to happen with £1mm diff in circulation) reply hilbert42 20 hours agorootparent\"The Bank of England destroys between 10-30 billion pounds worth of notes per year (no longer fit for circulation).\" The difference is that when BoE destroys notes they are accounted for by either the issuing of new notes and or crediting their withdrawal with a bank-entry credit thus the money in circulation does not change. Burning notes willy nilly effectively reduces the amount of currency in circulation because the central bank thinks the money still in circulation and doesn't replace it. This is why in many countries defacing or destroying the currency is unlawful. reply yard2010 19 hours agorootparentI believe he meant it's such a low number it wouldn't be relevant anyway - same as merging facebook accounts with wifey to not take that space on facebook reply hilbert42 18 hours agorootparentRight, clearly burning a million or perhaps even quite some millions would have a negligible effect on the currency, similarly millions in counterfeit money would have to be circulated to actually destabilize the currency (that's assuming the dud notes aren't noticed first). Clearly, in modern times in modern democracies it's very unlikely that in any practical scenario that some entity would be capable of unofficially adding or withdrawing enough money from a currency to actually destabilize it, moreover in such an event the central bank/government would be onto it in seconds. It's however an altogether different scenario if the citizenry somehow has the perception that the currency isn't exactly what the central bank purports it to be. Just the notion that something may be wrong with the currency could destabilize it, hence hellishly tight laws to ensure this never happens. reply mypastself 18 hours agorootparentprevYes, but e.g. even though the gravitational force exerted by my body affects you only in miniscule amounts, it’s still there. (Miniscule until I manage to stuff my face with enough donuts.) reply ttoinou 20 hours agorootparentprevCorrect, it's the rarety of all others bank notes that is increased, increasing purchasing power is only one consequence possible but not the only one. By the way, being hard to measure doesn't mean it doesn't exist. And there are plenty of things easy to measure that are meaningless reply CraigJPerry 18 hours agorootparentBank notes are not re-issued 1:1 with destroyed notes. >> being hard to measure doesn't mean it doesn't exist No in this context it does. Either you can measure the increase purchasing power or you can’t. And if you can’t measure it, it doesn’t exist in this specific case. reply ttoinou 12 hours agorootparentMost economics consequences will never be measured ever reply CraigJPerry 10 hours agorootparentI think we’d be getting into something akin to “homeopathic” monetary theory with that line of reasoning…! reply rich_sasha 20 hours agorootparentprevAs a nitpick, presumably the banknotes destroyed by BoE are then replaced by other banknotes. But I agree this is likely unmeasurable. reply keiferski 21 hours agoparentprevBy what, 0.00001%? I think the quoted sentence is obviously talking about the symbolic power of money – and burning a million pounds certainly has a lot of that. reply croes 21 hours agoparentprev>They literally increased the purchasing power of each remaining pound by doing so. Did they. Notes aren't a limited resource and they can still be considered in the cashflow reply thaumasiotes 21 hours agorootparentAssuming they (or someone else) owned the notes, yes. By reducing their own holdings, they raised the value of everyone else's. If the notes were counterfeit or had otherwise managed to come into existence without being owned by anyone, then no. reply venusenvy47 21 hours agorootparentWould it raise the value of the currency if the central bank didn't know this stash was destroyed? It seems like this pile of cash would still be considered to be \"in circulation\" until the central bank declared it to not be. reply Exuma 20 hours agorootparentA better question is do I make everyone else’s dollar worth more for the duration that I hold and don’t spend my dollars reply ImHereToVote 20 hours agorootparentYes. That is precisely why we don't get runaway inflation when people own a ton of property or have tons of money in index funds. That money supply isn't doing anything in particular. reply thaumasiotes 20 hours agorootparentprevDo you make everyone else's stock worth more for the duration that you hold your shares without selling them? Rephrasing that question, if you were holding a bunch of shares of stock, and you sell them, will that lower the price of that stock? reply thaumasiotes 20 hours agorootparentprevThe central bank's knowledge isn't relevant in any way. The value of the currency goes up because the willingness of people to spend it went down. You can't spend money that doesn't exist, so this effect will occur regardless of anyone's knowledge of anything. reply satellite2 20 hours agorootparentI don't really understand your comment. Generally it's accepted that the value went up because it became more rare as the number of unit went down. Or similarly because if all the money represents the price of everything in the economy, as there is less money and assuming that everything in the economy didn't change, then everything has now a slightly lower price. reply retwertett 18 hours agorootparentReally interesting discussion guys. I recommend this short essay by F.A.Hayek on how prices are discovered. https://www.econlib.org/library/Essays/hykKnw.html reply thaumasiotes 20 hours agorootparentprev> Generally it's accepted that the value went up because it became more rare as the number of unit went down. That's true. > Or similarly because if all the money represents the price of everything in the economy, as there is less money and assuming that everything in the economy didn't change, then everything has now a slightly lower price. That can't be true in any meaningful way, because all the money doesn't represent the price of everything in the economy. \"Everything in the economy\" is always worth many multiples of \"all the money\". The definition you'll see in economics is MV = PY. It's defined with reference to the concept of GDP, and the variables are: M is the amount of money (that year). V is the \"velocity of money\", the average number of times that a particular dollar gets spent (that year). P is the price level. The value of money (that year) is 1/P. Y is the amount of non-monetary stuff that is produced (that year). (I would be happier if Y was the amount of non-monetary stuff that existed, but that does not appear to be how economists prefer to talk about it. This set of definitions lets you say that MV is equal to NGDP.) You can characterize the effect of destruction of money as a reduction in M or as a reduction in V. If you take the view that the money still exists, because it's listed on a balance sheet somewhere, then M has not gone down, but under that view V still has; it's impossible to spend that money. Assuming everyone's urge to spend as a function of their ability to spend stays unchanged, you'll see that the party who locked up $1 million (by destroying it) will, as a result, spend less. Everyone else in the relevant universe is unaffected and spends just as much as they would have anyway. Then, by our special analysis, the amount of total money spent has gone down, and the amount of money in existence hasn't, so the velocity of money has gone down. Y is unaffected by the destruction of money. For PY to go down while Y stays constant, P must go down, which means that 1/P, the value of money, goes up. ------ Note that the implication of this velocity-based analysis is that, if a miser with tons of money in bank accounts that he'll never use happens to one day withdraw some of that money and destroy it, there will be no effect on the value of money. This is correct; that money had already been effectively destroyed when the miser committed to not spending it. (Over a much longer term, the effect of destruction would be visible when the miser's heirs inherited a smaller amount of money than otherwise and consequently spent less.) reply croes 20 hours agorootparentprevBy your logic the currency goes up when the poor get poorer reply thaumasiotes 20 hours agorootparentYes, if a million poor people lose $1 each, that has the same effect as one guy losing $1 million. reply croes 19 hours agorootparentIf million people lose $1, nobody cares. If one guy loses 1 million he might cut costs that will affect others. If people like Musk lose wealth because of sinking stock prices, his workers don't have suddenly more buying power. reply thaumasiotes 17 hours agorootparentSomething seems to have gone badly wrong in your choice of example - that didn't even involve any money. reply mouse_ 21 hours agorootparentprevYes reply yard2010 19 hours agorootparentprevIt's confusing to think about money as a relative not absolute resource reply amelius 20 hours agoparentprevAnd the purchasing power of any pound still to be printed by the government. reply api 21 hours agoparentprevGood podcast about the KLF: https://thenonsensebazaar.com/listen/119-the-klf/ reply hasty_pudding 20 hours agoparentprevThis is just rich people flexing under the BS guise of sticking it to the man. reply ascorbic 20 hours agorootparentIt was their entire remaining royalty earnings. They also deleted their back catalogue so they couldn't earn any more. I remember having to buy Japanese imports when I wanted CD copies of the White Room and Chill Out as a teenager. It was only a couple of years ago when they finally reissued any of their work. reply timruffles 20 hours agorootparentprevNot so - it was the majority of their music industry earnings, and they are apparently quite haunted by it to this day! reply hasty_pudding 17 hours agorootparentThey sound to me like astronomical idiots the likes of which will rarely be approached in the past, present, and future of the human race. reply CommanderData 19 hours agoprevThe ability to shutdown any persons the government doesn't like using any growing list of $Excuses is worrying. Currency is a freedom to perform transaction and arguably a given right, it's existed since humans learnt trade. Cash allows anyone to transact without a prerequisite or control. Two parties. Centralised digital currency will change it. That worry doesn't even include the privacy concerns and as systems become centralised either through standards or legislation. It will be easy to track someones habits or location globally. reply fzeroracer 19 hours agoparentThe government has always been able to shut you down. If they wanted to, they would pull you aside with no recourse and the police would execute civil forfeiture (see also: steal) against anything on your person, car or home. Cash makes this worse. Nor does it save you from 90% of transactions you do. Banks will know when you deposit/withdraw money. Stores know the moment you walk in. Unless you deal purely in cash (and I mean PURELY) someone somewhere will know when and how you're spending your money. If you want to deal with privacy concerns then it needs to be codified in law. And that involves dealing with the entire chain, not just point of sale and not just thinking cash will save you. reply HomeDeLaPot 18 hours agorootparentThere's a qualitative difference between the government being able to freeze the accounts of every person whose phone was detected at a protest, and the government having to send agents out to physically find each of those people, search them and their property, and seize their cash. The first is cheap and easy for the government to do on a large scale, and difficult for the protesters to block. The second is far more expensive for the government, and the protesters have more options to fight back—run, hide the cash, etc. Not to mention it will probably drum up sympathy for the oppressed. And in the end, physically seizing cash & goods only takes away what the protesters have at the time—they can still go earn more cash to transact with, unless they are arrested. Someone whose accounts are frozen, and who is prevented from opening new accounts, and who can't use cash, is running out of options. reply marcosdumay 17 hours agorootparentWhat kind of dystopian dictatorship are we talking about really? But well, no, there isn't much of a difference. reply fzeroracer 18 hours agorootparentprevLet's be real here: You, I, and most of the people in this thread talking about privacy concerns when it comes to cash and what not are playing pretend. I say this because the odds are we have a significant amount of our money invested in some form of digital investment. Whether it's stocks, certificates, money sitting in a bank account. The nature of cash means it scales up poorly; and the nature of holding cash means you are literally losing money year over year thanks to inflation. The system is built to encourage this. If the government wanted to freeze your accounts and take most of your net worth, they can. And while cash transactions can be secure, in our day to day life where our transactions are done cash is not meaningfully adding anything to our privacy. To actually achieve any sort of privacy with cash involves operating purely with cash: That means no bank accounts, no investments, you are paid directly by your workplace with cash etc. That's why again, cash will not protect you. It will not protect most of the people here acting like it will. reply nayuki 18 hours agorootparentprevIt sounds like you're still upset about the truckers protest in Ottawa in February 2022. reply jonkho 18 hours agorootparentprevThere are things that are justified to exist whose sole purpose is keeping things in-check. If any of your aforementioned tools gets abused by the authorities we can still fall back to using cash. If you take away the fallback alternative, then abuse is much more difficult to keep in check. This is called Game Theory. reply EasyMark 18 hours agorootparentprevjust like security it's a spectrum and not a 0 or 1. I could live on cash right now if I wanted to, so it isn't 90%. If the government stops issue it and issues debit cards only or allows banks to do it then that becomes impossible. The government could always come to your house and disappear you or trump up some charges and throw you in prison indefinitely, even in the USA, so that's not really an argument reply realcertify 17 hours agoprevThe current trend is to move from cash to electronic money controlled by the banks. Previously the money deposited to a bank account was the clients money that the bank just kept. Now it's the banks money that can be frozen by any reason and it's the client's responsibility to prove his innocence. Next step is a blockchain-based crypto, controlled only by a single structure, such as Fed. Total control directly by the government, no banks involved. Full access to the history of all purchases and means to immediately lock out anyone who does anything \"wrong\". Personally I'm going to move out of the country as soon as it forbids the cash transactions, as I don't want my kids to be slaves to the government. reply yakireev 17 hours agoparent> Personally I'm going to move out of the country as soon as it forbids the cash transactions Which country is it? In many jurisdictions cash transactions are already illegal above certain threshold. It might be that you won't have too many places to go. reply yankoff 17 hours agoparentprevTheoretically, they can lock you out by suspending your passport and preventing you from traveling even if you leave the country. Also they can have full access to your bank records given the warrant and you can't make any serious purchases with cash. Even in an unlikely apocalyptic scenario you have described they won't gain much more power than they already have. I don't see how this makes you a slave by itself. Only if government begins using this as a tool to control you but then, if we hit such a point, they have much more other tools people have to worry about. reply xadhominemx 16 hours agoparentprevWhat are you talking about? Deposits that came in via paper money are treated in exactly the same manner as those deposited electronically. reply yankoff 8 hours agorootparentCash transactions over $10k have to be reported to the IRS. You'd have to provide ID and TIN. If that weren't the case, criminals wouldn't have had problems with money laundering. reply yaky 20 hours agoprevThe other day, I paid cash at the local Chinese-American place, and the owner said \"thank you for using cash, it helps small businesses\", which took me by surprise. So I asked how much card processors charge, and she went on a bit of a rant, listing processing fees for everything from regular debit cards* charging 3-4% to rewards credit cards being around 7% to special international or world credit cards charging something like 17%. And most of them have a very narrow window to correct or void the transaction. With such rent-seeking behavior by banks, I very much understand wanting to keep cash around. * Debit cards charged as debit, by entering a PIN. Some terminals allow you to \"bypass PIN\" and/or automatically charge debit cards as credit. I don't know, it's shady. reply HomeDeLaPot 18 hours agoparentI wish more businesses would (could?) give cash discounts. It's the answer to the convenience and rewards offered by credit cards. If I can save as much or more by paying in cash as I can by paying by card, then I have a great motivation to start carrying cash around! reply yaky 8 hours agorootparentSeveral small businesses in my city do charge 3% for using a card. So that is like a cash discount. Many gas stations (usually ones serving trucks) have cash discounts for fuel, too. (US, midwest) reply dazc 20 hours agoparentprevWe have some small traders in the UK who insist on a minimum spend for accepting cards because of the fees levied. It doesn't really make any business sense although I appreciate the sentiment behind it. reply infinitecost 19 hours agoparentprevWhat part of the world? In the US at least this experience would only apply to a business owner that was getting absolutely fleeced by their processor. reply dpkirchner 14 hours agoparentprevThis is the first I'm hearing of double digit rates -- what cards are those? I'd definitely like to avoid them. reply yaky 7 hours agorootparentI don't recall exactly, I believe it was some type of business cards, or cards for travel and international use. reply imadj 21 hours agoprevI think the ongoing banking crisis in Lebanon (similarly in Egypt and maybe elsewhere too) where there are restrictions on people's access to their deposits or making international transactions should be on everyone's mind when talking about eliminating cash. The downside of going cashless aren't hypothetical or as far-fetched as people think. The whole society can be put on hold overnight. We can see this happening in front of us. Using whatever payment method for convenience in itself is fine. But, eliminating cash as medium, without providing a better alternative, one that can still work in disasters where power or network is down, is wild. reply xadhominemx 16 hours agoparentWhat difference does the elimination of paper money at POS make to ability to access electronic deposits? Unless you are saying people should not bank at all and keep cash under the mattress? reply imadj 15 hours agorootparent> What difference does the elimination of paper money at POS make to ability to access electronic deposits? The difference it makes is that now you only have one way to access your savings. Any failures, technical issues, outage, etc. and you're out of luck to say the least. The principle is that you shouldn't rely on a single system ever. If you're eager to go that way be my guest. I was using a real life scenario of how that turned out bad for people. I don't care about paper vs electronic. I care about relying on a single point of failure. reply xadhominemx 13 hours agorootparentHow is any of this different with cash withdrawals? You're still withdrawing from a digital account. reply david2ndaccount 17 hours agoprevThe issues with physical currency are almost entirely self-inflicted. Governments should retire the smallest denominations and introduce larger coins/bills. In the US, all of the coins are worth so little that the only practical coin is the rare half-dollar. Retire all of those worthless coins, require transactions be rounded to the nearest half-dollar or dollar, and issues with physical currency for the user will go away. Now of course, the government won’t do this as they want physical currency to be inconvenient so they can track you and deny you access to your own money when they find it convenient. We’re in an era of growing authoritarianism. Hopefully we survive this one. reply CSMastermind 17 hours agoparentI would argue the root cause of this phenomenon is inflation and the first step we should take is to make the dollar a stable unit of measure not to consciously allow its value to fall. reply kuchenbecker 17 hours agorootparentDeflation is worse than inflation, and it's hard to hit a single number, so central banks aim for low, stable inflation. 1-2% inflation isn't very perceptible vs 0 but avoids deflation. reply tastyfreeze 13 hours agorootparentPlease explain why deflation is worse. reply kuchenbecker 12 hours agorootparentCash becomes an appreciating asset. It leads people to hoard cash rather than invest in stocks and bonds, and delay purchases stagnating the economy. Japan mid 1990s to mid 2010s is a real-world example: https://en.m.wikipedia.org/wiki/Lost_Decades In the worst cases, deflation causes demand to drop causing a recession, which further depresses demand. https://en.m.wikipedia.org/wiki/Deflation reply tastyfreeze 8 hours agorootparentSo people would save more?Savings is how the wealth of a nation is built. Capital breeds investment. The more people with capital the better. The only reason that people invest in the stock market is to maintain wealth. What would you do if you could count on your dollars buying 2% more next year? Would you shove all your dollars in a mattress and forgo buying food so you could save a few more? Or would you keep buying everything you do now and worry a little less about investing to maintain value? Less dollars in the stockmarket enriching others and more dollars in savings building personal wealth is what deflation brings. The only people this is a problem for are stockbrokers. reply kuchenbecker 7 hours agorootparentEconomies are built on the movement of money not the sum of money. It's truly positive sum where the act of spending money allows someone else to spend money. Highly suggest picking up some econ primers, as its almost universally agreed even mild deflation is destructive. reply tastyfreeze 5 hours agorootparentEconomy is the movement of money. That is not a measure of wealth. There can be a lot of money moving around with nearly everybody living pay check to paycheck. People are better off if they can build wealth. It is more stable for society to make it easy to build wealth. It would take a lot of evidence to convince me that deflation is worse in the long run for human flourishing than inflation. I don't care about the economy as conceived by bankers and economists. If ever more people are flourishing the economy is working. If a few are getting exorbitantly wealthy, at the expense of everybody else, by being close to the flow of new money, the economy is not working. We have been at inflation so long there is no painless way out. reply tastyfreeze 17 hours agoparentprevYou know what? That sounds like a great idea. A single dollar doesn't buy much these days. Maybe we should add a zero to all the bills too. reply alexwhb 17 hours agoparentprevThis is a really good point. And one of the reasons I hardly ever use cash is having to deal with so much annoying change. If you eliminate most change… cash becomes not a big deal reply jason-phillips 21 hours agoprevI live an hour or so outside of Austin and people still primarily write checks as that's the only acceptable payment method, along with cash. My new wife said, \"Wait, why not just use Stripe? I'll tell them.\" I had to explain to her that one does not simply pull forward a society thirty years into the future. There is an incredible amount of cultural inertia to overcome. So I carry a lot of cash out here. reply gambiting 21 hours agoparentAren't cheques even worse for privacy though? Not only your bank knows exactly who you paid and how much, the seller also now has your actual name since it's just written on the cheque - at least when paying by card they get nothing. Maybe I'm getting it wrong - cheques are something I've only seen in films, in my country no bank accepts them since 2020 anymore, they've been completely phased out. reply ta1243 21 hours agorootparentI was given a chequebook when I opened my current bank account in 2004, I've written 8, all of them in the first 10 years. reply jason-phillips 20 hours agorootparentSame, until I moved to a rural area. I did a lot of research about this place but didn't discover the pervasiveness of the cash society until I moved here. reply grobbyy 20 hours agorootparentprevLast I looked, checks had privacy regulations. Most other payment methods don't really. I'm also my bank's customer. For most payment companies, I'm the product. reply astura 20 hours agorootparentWhat sort of \"privacy regulations?\" reply jason-phillips 20 hours agorootparentprevHere - that being outside of Austin in Texas - I think the root causes for this behavior lie with tax cheats and ineptitude. reply PrimeMcFly 13 hours agorootparentprevChecks certainly are not completely phased out in your country, just no longer common for consumers. And shopkeepers can still take a name off a card. reply glimshe 21 hours agoparentprevYou'd be surprised. I visited South America and all I saw was phone payments... They basically skipped the credit card age, as cards were never popular among the poor. reply ajmurmann 21 hours agorootparentAren't the phone payments backed by credit cards or do they by preloaded vouchers? reply baq 21 hours agorootparentBank accounts directly I’d assume. Why add another layer of indirection? reply xadhominemx 16 hours agorootparentIt varies by country but a lot of them are just virtual debit and credit cards because the payments run over the card network rails. reply fundad 18 hours agorootparentprevIt’s not surprising that a lot Murica is decades behind most places in the world. Especially places where foreigners visit. reply EasyMark 17 hours agorootparentHow is it behind? I've not been at a store in a while that I couldn't use apple pay or a card? I know it's popular to bag on America, but I figure most of HN is better than that. reply canucker2016 13 hours agorootparentUntil Apple Pay and other smartphone-related payment methods became common, the USA credit card was technologically behind much of Europe, Canada, and other developed countries. Chip-and-PIN (EMV, https://en.wikipedia.org/wiki/EMV) is the standard in Europe and Canada for relaying credit card info & authentication. Many USA retailers were stuck using the credit card's magnetic stripe for entering card info. Evidently the USA still hasn't moved to Chip-and-PIN, with Chip and Signature being more common. Updating the point-of-sale terminals has been the chokepoint supposedly, https://lauraclery.com/chip-and-pin-united-states/. reply ghaff 21 hours agoparentprevI use very little cash in MA. But checks are still pretty much the default for a lot of people doing various services for me. I can just leave a check for someone like my housekeeper. I could presumably make arrangements to pay in a different form (including cash) but why bother? And, in many cases, it's my bank actually \"writing\" the check and delivering it. I'd change if it became an aberration or weird to do but it's just not a problem in general. (And I wouldn't try to pay by check or cash at Walmart for example.) reply avianlyric 21 hours agorootparentThe lack of fast free digital money transfer infrastructure in the U.S. is something that seems crazy to anyone in the EU. Every EU country (and the UK) has had fast free digital bank transfers for decades now. Here in the UK we pay all “casual” services like cleaners, dog walkers etc via a simple, quick bank transfer. It’s great, I don’t need to keep cash, and person providing the services has the money in their account, ready to spend, about 500ms after I tap the “send” button on my phone. I can see how cheques are convenient for purchasers, but for suppliers they’re a bit of nightmare, as they need physically deliver the cheque to cash it. Then wait for it to clear, and also take on all the risk of it bouncing. All of that just goes away with proper digital money transfer technologies. reply petejansson 20 hours agorootparentFedNow (https://www.federalreserve.gov/paymentsystems/fednow_about.h...) just went live in July, 2023. It will take some time for adoption. reply xadhominemx 16 hours agorootparentprevWe have free digital money transfer in the United States. Venmo, Zelle, PayPal, Cash App, etc. I don't know why you'd believe otherwise. reply rwmj 21 hours agorootparentprevThat's wild. Here in the UK, tradespeople usually have a card reader, or some accept bank transfers. reply ghaff 21 hours agorootparentIt's becoming more common but for someone like my housekeeper, I may not even be home and can just leave a check on the counter. As I say, I could doubtless make alternative arrangements but a check is the default and it's easy. reply rwmj 21 hours agorootparentFor someone I need to pay regularly like a cleaner I would set up a bank transfer recipient in my banking app (or website) and then payments - either one offs or regularly scheduled - are simple, instant and free. reply ghaff 21 hours agorootparentAnd so is writing a check and leaving it on the counter--especially for a service that is somewhat predictable but actually isn't on a set schedule. She texts me that she's coming next week, I write a check and leave it on the counter, done. reply GeoAtreides 20 hours agorootparentWell, it's not really done, is it? She has to go to the bank and cash the check, right? It's not the end of the world, sure, but it's an extra trip compared with a bank transfer -- with a bank transfer she has the money ready to be used immediately. reply ghaff 20 hours agorootparent>She has to go to the bank and cash the check, right? I'm guessing she has a banking app like I do that lets her deposit the check in about a minute? Between the bank writing/mailing checks for you to pay bills and an app you can use to deposit checks, checks just aren't the friction that they used to be. That's probably a big reason why there just isn't a big outcry in the US to get away from checks. For most of us, it's just way, way down the list of day to day pain points. reply rwmj 20 hours agorootparentprevI believe (even in the US!) you can pay in a check by taking a photo in your bank app. reply dazc 20 hours agorootparentThe money isn't instant though. People paying for such services with a cheque would do well living a few weeks in someone else's shoes. reply ghaff 19 hours agorootparent90% of the people I'm paying are businesses. If an individual wants cash they can ask and I'll pay cash. I wouldn't just assume someone prefers everyone to be paying them in cash or screwing around with various electronic payment options that aren't super-common where I live. reply dazc 19 hours agorootparentOK, good to hear. reply fundad 18 hours agorootparentprevMy house cost $240 to clean. Handing around fat envelopes like a wise guy is a macho dream but not practical. reply 15155 12 hours agorootparentWow, a whole 4 bills - that envelope must be bursting at the seams. reply dazc 18 hours agorootparentprevThe argument was for electronic payment against cheques. reply PrimeMcFly 13 hours agorootparentprevIt typically is these days. reply EasyMark 17 hours agorootparentprevpiling up 20 checks for the week and going to the bank for 10 minutes isn't a big deal to most people. reply EasyMark 17 hours agorootparentprevmost here won't take it. My maid won't. They will take venmo, check, or cash though. reply chasd00 17 hours agorootparentprevThe vast vast majority of tradespeople in Texas are what’s called “cash under the table”. They don’t report their income and so never pay taxes. All take cash only or something like a check made out to “cash” instead of a name. It’s how many “poor” families qualify for assistance yet drive $100k pickups and all their kids have the latest iPhones and gaming systems. reply EasyMark 17 hours agorootparentprevThey do here in the US as well. It's not high-tech. Unless you get a sole proprietorship handyman or something, they'll likely take a card. Most will charge you the 3.5% or so extra for card payment though because that's what they have to pay. reply alexwasserman 14 hours agorootparentprevIn CT I use Venmo and Zelle to pay just about everyone. Cleaner, yard guy, electrician, plumber, handyman, etc Everyone takes Zelle or Venmo. reply chasd00 17 hours agoparentprev> I live an hour or so outside of Austin and people still primarily write checks I can’t think of an area within 65 miles of downtown Austin where checks would be common. Which direction from Austin do you live? East maybe? reply madsbuch 21 hours agoprevI Denmark we are removing the 1000kr bill as of May 2025. from then on the highest bill. is 500kr which is roughly equal to 80 usd. Denmark Is the most cashless society I have yet experiences (and I have been traveling a fair bit). This comes with pros and cons. and I think the reason why it works is because of an incredibly high social trust. reply ACS_Solver 20 hours agoparentI'm in Sweden, a strong contender for the most cashless country, and I don't know what the banknotes look like. They were changed some 5 or 6 years ago and I haven't paid with or even withdrawn cash since then. For illustration, cash is generally not accepted outside of grocery stores and some chains. For a typical restaurant, coffee shop or takeaway there's very little chance they take cash. Street vendors may take cash but will definitely have a proper cashless payment method. I really don't like cash and am happy to see it basically dead in the country. I'm also very concerned about privacy, I'm that sort of HN person - never had a Facebook account, run self-hosted email for 20 years, use Signal for messaging as much as I can, etc. But I don't think cash is a good solution to privacy. I'd rather see laws and (better) cryptocurrencies as the privacy toolkit. One common concern I see from US-centric articles is about your transaction history being sold for ad targeting, etc. That at least is illegal here. A store is not allowed to track your history of purchases or similar unless you opt in, such as via a store loyalty card. Banks are not allowed to disclose anything about your accounts except in some investigatory contexts, but definitely not for commercial purposes. So neither \"the store will track my purchases because I pay by card\" nor \"the bank will sell my history to advertisers\", which would be major issues for me, are actual concerns. reply belorn 18 hours agorootparentI share to a large degree this viewpoint that I'd rather see laws and better technology as a solution to the ever connected and tracked future that we live in. I try to use cash in Sweden since we don't have those laws yet, but replacing paper, metal and cashiers would be good if the drawbacks could be resolved. The laws we already have do not work. A large part of Swedish intelligence policy is to trade information with partners like the US, and I would consider it extremely unlikely that highly valuable data like bank transactions was not part of that. The US intelligence policy in turn is known to use such information occasionally for commercial purposes, when it has a national impact. The Swedish courts are also not trained to deal with low-value evidence data. There is an assumption that if data exists, it must do so because there was a reason someone deemed it worth gathering. This assumption breaks apart when data is always collected as a default, which means courts would need to be retrained when this kind of data is presented as evidence (we are slowly getting there but still very far from where I would feel safe). It also makes investigatory contexts dangerous, especially if records are part of fishing operations where single bit errors in storing, retrieving, copying and relaying records can have extreme effects. This include swish which records are very common in court hearings. In some cases it seems that only a single swish record is enough for someone to get charged with a crime, with the accused having to provide evidence to prove that they are innocent. reply imadj 19 hours agorootparentprev> I really don't like cash and am happy to see it basically dead in the country. What would you do in case of a emergency or natural distaster? Basically a situation where any of the following is not available for any period of time: power, network, financial infrastructure, access to digital devices. 1. On personal level: Maybe your wallet and phone was stolen 2. Region: Power/Network is down 3. National level: the whole infrastructure is down or under attack Have you been in such situation or thought about it? reply ACS_Solver 19 hours agorootparentSuch robustness is currently a subject of government studies and planning. It's definitely an area of concern. A personal-level outage is a problem but less so than with cash. I have three separate items on me I can use to pay - my card holder with bank cards, my phone and my watch. That's more redundancy than with cash, which is one item (wallet). Power or network outages aren't a big issue. I've experienced those and it's smooth. Most payment machines work fine without power (batteries) or without network because they then store the transactions locally and send them later. I've even had lunch when the lunch restaurant was having IT problems and couldn't accept any payments - they noted how many people came on paper and I came back the next day to pay, as I'm sure the vast majority of patrons did. Bank or card issuer outages are handled by using another card or payment method. If some bank's cards aren't working (uncommon but it happens), mobile payments generally work still, and most people have more than one card. Major national level outages are a concern. It would be impossible to buy e.g. a train ticket as those require immediate payment, so if somehow Internet was down nationwide, you couldn't get a train ticket online or at the ticket machines. This is one major goal that the e-krona (state-issued digital currency) project is meant to address as e-krona is intended to also work offline. reply boricj 18 hours agorootparent> I have three separate items on me I can use to pay - my card holder with bank cards, my phone and my watch. That's more redundancy than with cash, which is one item (wallet). All of which are electronic and depend on centralized services. Bank notes and coins work offline and without electricity. Not too long ago, a restaurant I went to with a coworker had an Internet outage and no card or mobile payment would go through. I paid for the whole bill since I was the only one carrying cash, then we went to the nearest ATM to reimburse me. I always carry cash. I don't use it often but it always works, even if everything else fails. reply ACS_Solver 17 hours agorootparentThe hardware/software infrastructure for cards and mobile payments is separate. We get card outages or Swish (mobile) outages occasionally, I don't remember any that affected both. Card payments are expected to work offline. We're so dependent on IT systems that even cash doesn't always work. One of the most significant outages we've ever experienced was when Coop, a grocery store chain, had to close all its stores for a couple days because of an IT outage. It didn't help that the stores accept cash - the payments still couldn't be registered, receipts couldn't be printed, so the stores were completely unable to function. We clearly need a robust fallback solution. Some kind of payment system that can work despite centralized outages (a bank's systems failing) or local outages at the merchant. But that shouldn't be cash. The practice of exchanging fragile, filthy pieces of paper whose ownership you cannot prove once outside your possession belongs to the past, we can do much better now. reply madsbuch 19 hours agorootparentprevFor you last point regarding national level outages: I am quite sure that trains companies would instantiate emergency systems in that case – maybe just by letting people travel for free. Such an event is also not expected to last for long. reply ACS_Solver 18 hours agorootparentQuite possibly, yes. There's also the consideration that such an outage would likely be part of an even bigger problem. Over here, like most of Europe, we're pretty safe from natural disasters aside from storms causing local power outages. There are no earthquakes, tsunamis or similarly destructive events. So a hypothetical situation where payments are down nationwide for a significant period of time would almost certainly be part of an even larger disaster where a lot of infrastructure is non-functional due to military attack or something akin to a complete crash of all computers. reply el-salvador 15 hours agorootparentprevNot a natural disaster, but there's something similar happening for about 10 days in Central America. The largest Telco was hit by ransomware and this has affected most of their systems. They are unable to receive electronic and card payments, or sell digital packages, it seems their billing, CRM and Helpdesk systems are down. So all prepaid users are able to use the inrernet and call for free. reply alkonaut 18 hours agorootparentprevThis is a real concern, but even if I keep cash for use in an emergency, stores and banks would also need to be prepared to handle cash in an emergency and I doubt they are today (since no one uses cash). Telling people they need to use a less convenient payment method only to keep it alive for an emergency also seems doomed to fail. The correct solution is probably to force banks and critical stores to always accept cash + encourage people to keep some cash for an emergency. But changing the banknotes (and making the previous ones invalid) every 10 years doesn’t help with encouraging people to save cash at home. That’s another one of those US-isms that blow peoples minds: that dollar bills of any age are legal tender. That and the use of paper checks. I think it was the mid 80’s the last time I saw a check. reply ACS_Solver 18 hours agorootparent> That’s another one of those US-isms that blow peoples minds: that dollar bills of any age are legal tender. That's not so impressive when the National Bank of Sweden for instance is legally obliged to exchange any banknotes it issued for current ones, regardless of their age. You can theoretically get current money in exchange for a 17th century obligation or certificate issued by the bank, though even early 20th century notes are worth more as collection items than their nominal value. reply alkonaut 14 hours agorootparentThey do, but it's a hassle and means people are reluctant to keep money in their mattresses (which might be a good thing). It's unfortunate when it comes to resilience however. Realizing after the big two-week-blackout that the 10x SEK500 bills you stuffed away for this emergency are invalid and would need to be posted to the central bank before you can eat, is not a good experience. reply imadj 18 hours agorootparentprev> Telling people they need to use a less convenient payment method only to keep it alive for an emergency also seems doomed to fail. Who said that? Certainly not me Personally, I'm not advocating for cash. I just think it's stupid to brag about eliminating cash while putting all your savings and trust in a fragile system and infrastructure . There's nothing revolutionary about that. People should have options and backups. reply alkonaut 17 hours agorootparentI meant that (Telling people to use cash more to keep the system afloat) would be one potential solution to the problem, but not one I think would work. I think people should not have to use cash, and I certainly won’t. But I do think society must have a resilient payment system - even if only as backup. reply imadj 17 hours agorootparent> I do think society must have a resilient payment system - even if only as backup On that note we can agree. The goal is having a resilient system where you can access your funds when you need it most. See my other comment for backstory: https://news.ycombinator.com/item?id=39239919 reply madsbuch 19 hours agorootparentprevThis is usually the go-to arguments for cash, though I think it is not the right ones. The right argument is privacy. These things can be handled easily: In case of an emergency you can just revert to writing credit notes to each other, which is is spirit the same as cash (contracts denoting that someone owes you something). > On personal level:",
    "originSummary": [
      "The article examines the decline of physical cash and the implications of a cashless society, including privacy loss and increased surveillance.",
      "It raises concerns about the exploitation of human data and the emergence of digital identity tokens.",
      "The article discusses the use of tokens and cryptocurrencies for payment, questioning their limitations and control, and mentions specific examples and projects related to cashless systems."
    ],
    "commentSummary": [
      "The article delves into the ongoing debate surrounding physical cash versus digital payments, evaluating the pros and cons of each form.",
      "It discusses the convenience and privacy risks associated with digital payments, as well as the advantages of anonymity and personal control that come with using cash.",
      "The conversation covers a wide range of topics, including transaction fees, tax avoidance, tracking of transactions, the role of government and banks in payment systems, and the impact on businesses and individuals, providing a comprehensive perspective on the issue."
    ],
    "points": 231,
    "commentCount": 457,
    "retryCount": 0,
    "time": 1706960766
  },
  {
    "id": 39241113,
    "title": "Discovery of Potentially Habitable Exoplanet TOI-715 b around M4 Star",
    "originLink": "https://academic.oup.com/mnras/article/527/1/35/7172075",
    "originBody": "Skip to Main Content Advertisement Journals Books Search Menu Menu Navbar Search Filter Monthly Notices of the Royal Astronomical Society This issue RAS Journals Astronomy and Astrophysics Books Journals Oxford Academic Mobile Enter search term Search Issues Volume 528, Issue 2, February 2024 (In Progress) Volume 528, Issue 1, February 2024 Browse all More content NAM 2023 Plenary Speakers Virtual Issue MillenniumTNG Project Special Issue Advance Articles Submit Why Publish Author Guidelines Submission Site Open Access Alerts About About Monthly Notices of the Royal Astronomical Society About the Royal Astronomical Society MNRAS journal to publish to Open Access Editorial Board Rights and Permissions Self-Archiving Policy Advertising and Corporate Services Journals Career Network Contact Us Journals on Oxford Academic Books on Oxford Academic RAS Journals Issues Volume 528, Issue 2, February 2024 (In Progress) Volume 528, Issue 1, February 2024 Browse all More content NAM 2023 Plenary Speakers Virtual Issue MillenniumTNG Project Special Issue Advance Articles Submit Why Publish Author Guidelines Submission Site Open Access Alerts About About Monthly Notices of the Royal Astronomical Society About the Royal Astronomical Society MNRAS journal to publish to Open Access Editorial Board Rights and Permissions Self-Archiving Policy Advertising and Corporate Services Journals Career Network Contact Us Close Navbar Search Filter Monthly Notices of the Royal Astronomical Society This issue RAS Journals Astronomy and Astrophysics Books Journals Oxford Academic Enter search term Search Advanced Search Search Menu Article Navigation Close mobile search navigation Article Navigation Volume 527 Issue 1 January 2024 Article Contents ABSTRACT 1 INTRODUCTION 2 STELLAR CHARACTERIZATION 3 IDENTIFICATION OF PLANETARY CANDIDATES 4 VETTING AND VALIDATION 5 GLOBAL PHOTOMETRIC ANALYSIS 6 DISCUSSION 7 CONCLUSIONS ACKNOWLEDGEMENTS DATA AVAILABILITY Footnotes ReferencesArticle Navigation Article Navigation Journal Article A 1.55 R⊕ habitable-zone planet hosted by TOI-715, an M4 star near the ecliptic South Pole Georgina Dransfield, Georgina Dransfield School of Physics & Astronomy, University of Birmingham , Edgbaston, Birmingham B15 2TT , UK E-mail: gxg831@student.bham.ac.uk https://orcid.org/0000-0002-3937-630X Search for other works by this author on: Oxford Academic Google Scholar ADS Mathilde Timmermans, Mathilde Timmermans Astrobiology Research Unit, University of Liège , Allée du 6 août, 19, Sart-Tilman, B-4000 Liège , Belgium E-mail: Mathilde.timmermans@uliege.be Search for other works by this author on: Oxford Academic Google Scholar ADS Amaury H M J Triaud, Amaury H M J Triaud School of Physics & Astronomy, University of Birmingham , Edgbaston, Birmingham B15 2TT , UK E-mail: a.triaud@bham.ac.uk https://orcid.org/0000-0002-5510-8751 Search for other works by this author on: Oxford Academic Google Scholar ADS Martín Dévora-Pajares, Martín Dévora-Pajares Departamento de Física Teórica y del Cosmos, Universidad de Granada , E-18071 Granada , Spain Search for other works by this author on: Oxford Academic Google Scholar ADS Christian Aganze, Christian Aganze Center for Astrophysics and Space Sciences , UC San Diego, UCSD Mail Code 0424, 9500 Gilman Drive, La Jolla, CA 92093-0424 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS Khalid Barkaoui, Khalid Barkaoui Astrobiology Research Unit, University of Liège , Allée du 6 août, 19, Sart-Tilman, B-4000 Liège , Belgium Department of Earth, Atmospheric and Planetary Sciences, MIT , 77 Massachusetts Avenue, Cambridge, MA 02139 , USA Instituto de Astrofísica de Canarias (IAC) , Calle Vía Láctea s/n, E-38200 La Laguna, Tenerife , Spain https://orcid.org/0000-0003-1464-9276 Search for other works by this author on: Oxford Academic Google Scholar ADS Adam J Burgasser, Adam J Burgasser Center for Astrophysics and Space Sciences , UC San Diego, UCSD Mail Code 0424, 9500 Gilman Drive, La Jolla, CA 92093-0424 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS Karen A Collins, Karen A Collins Center for AstrophysicsHarvard & Smithsonian , 60 Garden St, Cambridge, MA 02138 , USA https://orcid.org/0000-0001-6588-9574 Search for other works by this author on: Oxford Academic Google Scholar ADS Marion Cointepas, Marion Cointepas CNRS, Université Grenoble Alpes , IPAG, F-38000 Grenoble , France Observatoire de Genève, Département d’Astronomie, Université de Genève , Chemin Pegasi 51b, CH-1290 Versoix , Switzerland Search for other works by this author on: Oxford Academic Google Scholar ADS Elsa Ducrot, Elsa Ducrot AIM, CEA, CNRS, Université Paris-Saclay, Université de Paris , F-91191 Gif-sur-Yvette , France Search for other works by this author on: Oxford Academic Google Scholar ADS ... Show more Maximilian N Günther, Maximilian N Günther European Space Agency (ESA), European Space Research and Technology Centre (ESTEC) , Keplerlaan 1, NL-2201 AZ Noordwijk , the Netherlands Search for other works by this author on: Oxford Academic Google Scholar ADS Steve B Howell, Steve B Howell NASA Ames Research Center , Moffett Field, CA 94035 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS Catriona A Murray, Catriona A Murray Department of Astrophysical and Planetary Sciences, University of Colorado Boulder , Boulder, CO 80309 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS Prajwal Niraula, Prajwal Niraula Department of Earth, Atmospheric and Planetary Sciences, MIT , 77 Massachusetts Avenue, Cambridge, MA 02139 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS Benjamin V Rackham, Benjamin V Rackham Department of Earth, Atmospheric and Planetary Sciences, MIT , 77 Massachusetts Avenue, Cambridge, MA 02139 , USA https://orcid.org/0000-0002-3627-1676 Search for other works by this author on: Oxford Academic Google Scholar ADS Daniel Sebastian, Daniel Sebastian School of Physics & Astronomy, University of Birmingham , Edgbaston, Birmingham B15 2TT , UK https://orcid.org/0000-0002-2214-9258 Search for other works by this author on: Oxford Academic Google Scholar ADS Keivan G Stassun, Keivan G Stassun Department of Physics & Astronomy, Vanderbilt University , 6301 Stevenson Center Ln., Nashville, TN 37235 , USA https://orcid.org/0000-0002-3481-9052 Search for other works by this author on: Oxford Academic Google Scholar ADS Sebastián Zúñiga-Fernández, Sebastián Zúñiga-Fernández Astrobiology Research Unit, University of Liège , Allée du 6 août, 19, Sart-Tilman, B-4000 Liège , Belgium Search for other works by this author on: Oxford Academic Google Scholar ADS José Manuel Almenara, José Manuel Almenara CNRS, Université Grenoble Alpes , IPAG, F-38000 Grenoble , France https://orcid.org/0000-0003-3208-9815 Search for other works by this author on: Oxford Academic Google Scholar ADS Xavier Bonfils, Xavier Bonfils CNRS, Université Grenoble Alpes , IPAG, F-38000 Grenoble , France Search for other works by this author on: Oxford Academic Google Scholar ADS François Bouchy, François Bouchy Observatoire de Genève, Département d’Astronomie, Université de Genève , Chemin Pegasi 51b, CH-1290 Versoix , Switzerland Search for other works by this author on: Oxford Academic Google Scholar ADS Christopher J Burke, Christopher J Burke Department of Physics and Kavli Institute for Astrophysics and Space Research, Massachusetts Institute of Technology , Cambridge, MA 02139 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS David Charbonneau, David Charbonneau Center for AstrophysicsHarvard & Smithsonian , 60 Garden St, Cambridge, MA 02138 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS Jessie L Christiansen, Jessie L Christiansen NASA Exoplanet Science Institute , Caltech/IPAC, Mail Code 100-22, 1200 E. California Blvd., Pasadena, CA 91125 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS Laetitia Delrez, Laetitia Delrez Astrobiology Research Unit, University of Liège , Allée du 6 août, 19, Sart-Tilman, B-4000 Liège , Belgium https://orcid.org/0000-0001-6108-4808 Search for other works by this author on: Oxford Academic Google Scholar ADS Tianjun Gan, Tianjun Gan Department of Astronomy and Tsinghua Centre for Astrophysics, Tsinghua University , Beijing 100084 , China Search for other works by this author on: Oxford Academic Google Scholar ADS Lionel J García, Lionel J García Astrobiology Research Unit, University of Liège , Allée du 6 août, 19, Sart-Tilman, B-4000 Liège , Belgium Search for other works by this author on: Oxford Academic Google Scholar ADS Michaël Gillon, Michaël Gillon Astrobiology Research Unit, University of Liège , Allée du 6 août, 19, Sart-Tilman, B-4000 Liège , Belgium https://orcid.org/0000-0003-1462-7739 Search for other works by this author on: Oxford Academic Google Scholar ADS Yilen Gómez Maqueo Chew, Yilen Gómez Maqueo Chew Instituto de Astronomía, Universidad Nacional Autónoma de México, Ciudad Universitaria , Ciudad de México 04510 , México Search for other works by this author on: Oxford Academic Google Scholar ADS Katharine M Hesse, Katharine M Hesse Department of Physics and Kavli Institute for Astrophysics and Space Research, Massachusetts Institute of Technology , Cambridge, MA 02139 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS Matthew J Hooton, Matthew J Hooton Cavendish Laboratory , JJ Thomson Avenue, Cambridge CB3 0HE , UK https://orcid.org/0000-0003-0030-332X Search for other works by this author on: Oxford Academic Google Scholar ADS Giovanni Isopi, Giovanni Isopi Campo Catino Astronomical Observatory , Regione Lazio, I-03010 Guarcino (FR) , Italy Search for other works by this author on: Oxford Academic Google Scholar ADS Emmanuël Jehin, Emmanuël Jehin Space Sciences, Technologies and Astrophysics Research (STAR) Institute, Université de Liège , Allée du 6 Août 19C, B-4000 Liège , Belgium Search for other works by this author on: Oxford Academic Google Scholar ADS Jon M Jenkins, Jon M Jenkins NASA Ames Research Center , Moffett Field, CA 94035 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS David W Latham, David W Latham Center for AstrophysicsHarvard & Smithsonian , 60 Garden St, Cambridge, MA 02138 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS Franco Mallia, Franco Mallia Campo Catino Astronomical Observatory , Regione Lazio, I-03010 Guarcino (FR) , Italy Search for other works by this author on: Oxford Academic Google Scholar ADS Felipe Murgas, Felipe Murgas Instituto de Astrofísica de Canarias (IAC) , Calle Vía Láctea s/n, E-38200 La Laguna, Tenerife , Spain CNRS, Université Grenoble Alpes , IPAG, F-38000 Grenoble , France Departamento de Astrofísica, Universidad de La Laguna (ULL) , E-38206 La Laguna, Tenerife , Spain Search for other works by this author on: Oxford Academic Google Scholar ADS Peter P Pedersen, Peter P Pedersen Cavendish Laboratory , JJ Thomson Avenue, Cambridge CB3 0HE , UK Search for other works by this author on: Oxford Academic Google Scholar ADS Francisco J Pozuelos, Francisco J Pozuelos Astrobiology Research Unit, University of Liège , Allée du 6 août, 19, Sart-Tilman, B-4000 Liège , Belgium Space Sciences, Technologies and Astrophysics Research (STAR) Institute, Université de Liège , Allée du 6 Août 19C, B-4000 Liège , Belgium Instituto de Astrofísica de Andalucía (IAA-CSIC) , Glorieta de la Astronomía s/n, E-18008 Granada , Spain Search for other works by this author on: Oxford Academic Google Scholar ADS Didier Queloz, Didier Queloz Cavendish Laboratory , JJ Thomson Avenue, Cambridge CB3 0HE , UK Search for other works by this author on: Oxford Academic Google Scholar ADS David R Rodriguez, David R Rodriguez Space Telescope Science Institute , 3700 San Martin Drive, Baltimore, MD 21218 , USA https://orcid.org/0000-0003-1286-5231 Search for other works by this author on: Oxford Academic Google Scholar ADS Nicole Schanche, Nicole Schanche Department of Astronomy and Tsinghua Centre for Astrophysics, Tsinghua University , Beijing 100084 , China Center for Space and Habitability, University of Bern , Gesellschaftsstrasse 6, CH-3012 Bern , Switzerland Department of Astronomy, University of Maryland , College Park, MD 20742 , USA https://orcid.org/0000-0002-9526-3780 Search for other works by this author on: Oxford Academic Google Scholar ADS Sara Seager, Sara Seager Center for Astrophysics and Space Sciences , UC San Diego, UCSD Mail Code 0424, 9500 Gilman Drive, La Jolla, CA 92093-0424 , USA Departamento de Astrofísica, Universidad de La Laguna (ULL) , E-38206 La Laguna, Tenerife , Spain Department of Aeronautics and Astronautics, Massachusetts Institute of Technology , Cambridge, MA 02139 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS Gregor Srdoc, Gregor Srdoc NASA Goddard Space Flight Center , 8800 Greenbelt Road, Greenbelt, MD 20771 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS Chris Stockdale, Chris Stockdale Kotizarovci Observatory , Sarsoni 90, 51216 Viskovo , Croatia Search for other works by this author on: Oxford Academic Google Scholar ADS Joseph D Twicken, Joseph D Twicken NASA Ames Research Center , Moffett Field, CA 94035 , USA Hazelwood Observatory , Australia SETI Institute , Mountain View, CA 94043 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS Roland Vanderspek, Roland Vanderspek Department of Physics and Kavli Institute for Astrophysics and Space Research, Massachusetts Institute of Technology , Cambridge, MA 02139 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS Robert Wells, Robert Wells Center for Space and Habitability, University of Bern , Gesellschaftsstrasse 6, CH-3012 Bern , Switzerland Search for other works by this author on: Oxford Academic Google Scholar ADS Joshua N Winn, Joshua N Winn Department of Astrophysical Sciences, Princeton University , Princeton, NJ 08544 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS Julien de Wit, Julien de Wit Department of Earth, Atmospheric and Planetary Sciences, MIT , 77 Massachusetts Avenue, Cambridge, MA 02139 , USA Search for other works by this author on: Oxford Academic Google Scholar ADS Aldo Zapparata Aldo Zapparata Campo Catino Astronomical Observatory , Regione Lazio, I-03010 Guarcino (FR) , Italy https://orcid.org/0000-0002-9428-1573 Search for other works by this author on: Oxford Academic Google Scholar ADS Paris Region Fellow, Marie Sklodowska-Curie Action. ESA Research Fellow. 51 Pegasi b Fellow. Author Notes Monthly Notices of the Royal Astronomical Society, Volume 527, Issue 1, January 2024, Pages 35–52, https://doi.org/10.1093/mnras/stad1439 Published: 18 May 2023 Article history Received: 03 November 2022 Revision received: 09 May 2023 Accepted: 09 May 2023 Published: 18 May 2023 Corrected and typeset: 28 October 2023 PDF Split View Views Article contents Figures & tables Video Audio Supplementary Data Cite Cite Georgina Dransfield, Mathilde Timmermans, Amaury H M J Triaud, Martín Dévora-Pajares, Christian Aganze, Khalid Barkaoui, Adam J Burgasser, Karen A Collins, Marion Cointepas, Elsa Ducrot, Maximilian N Günther, Steve B Howell, Catriona A Murray, Prajwal Niraula, Benjamin V Rackham, Daniel Sebastian, Keivan G Stassun, Sebastián Zúñiga-Fernández, José Manuel Almenara, Xavier Bonfils, François Bouchy, Christopher J Burke, David Charbonneau, Jessie L Christiansen, Laetitia Delrez, Tianjun Gan, Lionel J García, Michaël Gillon, Yilen Gómez Maqueo Chew, Katharine M Hesse, Matthew J Hooton, Giovanni Isopi, Emmanuël Jehin, Jon M Jenkins, David W Latham, Franco Mallia, Felipe Murgas, Peter P Pedersen, Francisco J Pozuelos, Didier Queloz, David R Rodriguez, Nicole Schanche, Sara Seager, Gregor Srdoc, Chris Stockdale, Joseph D Twicken, Roland Vanderspek, Robert Wells, Joshua N Winn, Julien de Wit, Aldo Zapparata, A 1.55 R⊕ habitable-zone planet hosted by TOI-715, an M4 star near the ecliptic South Pole, Monthly Notices of the Royal Astronomical Society, Volume 527, Issue 1, January 2024, Pages 35–52, https://doi.org/10.1093/mnras/stad1439 Select Format Select format .ris (Mendeley, Papers, Zotero) .enw (EndNote) .bibtex (BibTex) .txt (Medlars, RefWorks) Download citation Close Permissions Icon Permissions Share Icon Share Facebook Twitter LinkedIn Email Navbar Search Filter Monthly Notices of the Royal Astronomical Society This issue RAS Journals Astronomy and Astrophysics Books Journals Oxford Academic Mobile Enter search term Search Close Navbar Search Filter Monthly Notices of the Royal Astronomical Society This issue RAS Journals Astronomy and Astrophysics Books Journals Oxford Academic Enter search term Search Advanced Search Search Menu ABSTRACT A new generation of observatories is enabling detailed study of exoplanetary atmospheres and the diversity of alien climates, allowing us to seek evidence for extraterrestrial biological and geological processes. Now is therefore the time to identify the most unique planets to be characterized with these instruments. In this context, we report on the discovery and validation of TOI-715 b, a 𝑅 b = 1.55 ± 0.06 R ⊕ planet orbiting its nearby (42 pc) M4 host (TOI-715/TIC 271971130) with a period 𝑃 b = 19.288004 − 0.000024 + 0.000027 d. TOI-715 b was first identified by TESS and validated using ground-based photometry, high-resolution imaging and statistical validation. The planet’s orbital period combined with the stellar effective temperature 𝑇 e f f = 3075 ± 75 K give this planet an installation 𝑆 b = 0.67 − 0.20 + 0.15 S ⊕ ⁠, placing it within the most conservative definitions of the habitable zone for rocky planets. TOI-715 b’s radius falls exactly between two measured locations of the M-dwarf radius valley; characterizing its mass and composition will help understand the true nature of the radius valley for low-mass stars. We demonstrate TOI-715 b is amenable for characterization using precise radial velocities and transmission spectroscopy. Additionally, we reveal a second candidate planet in the system, TIC 271971130.02, with a potential orbital period of 𝑃 02 = 25.60712 − 0.00036 + 0.00031 d and a radius of 𝑅 02 = 1.066 ± 0.092 R ⊕ ⁠, just inside the outer boundary of the habitable zone, and near a 4:3 orbital period commensurability. Should this second planet be confirmed, it would represent the smallest habitable zone planet discovered by TESS to date. planets and satellites: detection, planets and satellites: fundamental parameters, planets and satellites: terrestrial planets 1 INTRODUCTION At long last the era of JWST has arrived, and with it the age of detailed exoplanetary atmospheric characterization (The JWST Transiting Exoplanet Community Early Release Science Team 2023). This achievement was unlocked just as the community hit another significant milestone: the discovery of the 5000th planet beyond the solar system.1 This ever-growing sample combined with the might of JWST is now set to deepen our understanding of planets, including those in our solar system, on a sub-population level. One sub-population of particularly enduring interest consists of small, potentially habitable planets orbiting cool M-type stars. With current instrumentation, M dwarfs represent our best hope of finding temperate terrestrial planets, the best example being the seven Earth-sized planets of the TRAPPIST-1 system orbiting an M8V type star (Gillon et al. 2017). The reduced radii of M dwarfs enable small planets to produce large transit depths; this combined with the shorter orbital periods of these more compact systems makes photometric monitoring from ground and space-based facilities considerably more feasible (e.g. Nutzman & Charbonneau 2008; Reiners et al. 2018; Sebastian et al. 2021; Triaud 2021). In the context of atmospheric characterization by transmission spectroscopy, bright, nearby M dwarfs are ideal planetary hosts as small temperate planets will transit frequently, enabling high signal-to-noise detections of atmospheric features with fewer hours of telescope time (Charbonneau & Deming 2007; Dressing & Charbonneau 2015; Morley et al. 2017). Additionally, these low mass stars appear more likely to host small planets from a theoretical and observational point of view (Montgomery & Laughlin 2009; Bonfils et al. 2013; Dressing & Charbonneau 2013; Mulders, Pascucci & Apai 2015; Alibert & Benz 2017; He, Triaud & Gillon 2017; Sabotta et al. 2021). The so-called ‘habitable zone’ is a circumstellar region where an Earth-like planet could sustain liquid water on its surface (Kasting, Whitmire & Reynolds 1993). With sometimes contradictory definitions of its boundaries, which depend on stellar spectral type, planetary albedo, mass and even cloud cover (Underwood, Jones & Sleep 2003; Kopparapu et al. 2014; Ramirez & Kaltenegger 2016), it can be challenging to categorise planets using this metric. However, the most widely applicable and conservative definition comes from Kopparapu et al. (2013), stating that rocky planets receiving between 0.42 and 0.842 𝑆 ⊕ are in their star’s HZ, irrespective of all other factors. The activity of the M-dwarf host stars themselves is an additional factor to consider in the habitability of planets orbiting M dwarfs, as the effect of their frequent flaring is not fully known (O’Malley-James & Kaltenegger 2017). Stellar flares could destroy a nascent atmosphere entirely (Davenport et al. 2016; Dong et al. 2017), or they might provide the energy needed to catalyse biological processes (Buccino, Lemarchand & Mauas 2007; Patel et al. 2015; Lingam & Loeb 2017; Rimmer et al. 2018). Additionally, we must look beyond our own terrestrial version of habitability, by exploring the possibility that biology might arise on water-worlds (Madhusudhan, Piette & Constantinou 2021) or temperate sub-Neptunes (Seager et al. 2021), as both of these may have different HZ boundaries. We owe much of our current understanding of exoplanetary demographics to the Kepler mission (Borucki et al. 2010), and one of the most influential results to come out of statistical studies of the Kepler sample was the bimodal radius distribution of sub-Neptune sized planets, with a gap between 1.5 and 2 𝑅 ⊕ (Fulton et al. 2017; Van Eylen et al. 2018). The current best explanations for this bimodality are core-powered mass loss (Lopez & Fortney 2013; Ginzburg, Schlichting & Sari 2018; Gupta & Schlichting 2019), photo-evaporation (Owen & Wu 2013, 2017) and volatile-poor formation (Lee, Chiang & Ormel 2014; Venturini & Helled 2017). While the exact location of this so-called ‘radius valley’ depends on stellar mass among other things (Wu 2019; Gupta & Schlichting 2020), it is still unclear whether it is present around M dwarfs or not (Cloutier & Menou 2020). Therefore, planets with sizes within the radius valley (e.g. Cloutier et al. 2020, 2021; Luque et al. 2022) help us to understand the shape and depth of this gap around M dwarfs. A recent study by Luque & Pallé (2022), however, indicates that M-dwarf planets may have a density gap rather than a radius gap separating two populations of small planets (rocky and water worlds), and that these observations are also well explained by formation pathways that include disc-driven migration (Venturini et al. 2020). At present TESS (Ricker et al. 2015) is providing the community with ample new planets to improve our understanding of exoplanetary demographics, including by populating the habitable zone. Notably, while several ‘habitable zone’ planets discovered by TESS have been confirmed (e.g. Gilbert et al. 2020; Vach et al. 2022), none yet have fallen within the conservative habitable zone as described by Kopparapu et al. (2013) – until now. In this context, we report on the discovery of TOI-715 b, a small planet orbiting a nearby M4 star (TOI-715/TIC 271971130). The planet’s relatively long orbital period and cool host provide it with a mild installation, placing TOI-715 b comfortably within its star’s conservative habitable zone. We additionally find a second candidate in the system that, if confirmed, could be TESS’s smallest habitable zone planet discovered to date. Our paper is organised as follows: we begin by characterizing the host star using its spectral energy distribution and reconnaissance spectroscopy in Section 2. We then describe the identification of planetary candidates in the data, first by TESS followed by our own search, in Section 3. In Section 4, we describe our ground-based follow-up campaign and our procedures to validate the system, and in Section 5 we detail our global analysis of all available data. Finally, we contextualize our results in Section 6 and conclude in Section 7. 2 STELLAR CHARACTERIZATION TOI-715 (TIC 271971130) is a nearby (⁠ 42 p c ⁠; Bailer-Jones et al. 2021) M dwarf of spectral type M4. It is a high-proper-motion target with right ascension 07:35:24.56 hms and declination −73:34:38.67 dms (J2000, epoch 2015.5), placing it within TESS’s continuous viewing zone (CVZ). As all our planetary information will be derived using the host star’s parameters, we begin in the section that follows by characterizing TOI-715. All photometry and stellar parameters adopted for this work can be found in Table 1. Table 1. Stellar parameters adopted for this work. Designations TOI-715, TIC 271971130, 2MASS J07352425−7334388, APASS 33649915, Gaia DR2 5262666416118954368, UCAC4 083–012601, WISE J073524.46−733438.7 Parameter Value Source α 07:35:24.56 Gaia Collaboration (2022) δ −73:34:38.67 Gaia Collaboration (2022) Distance 42.46 ± 0.03 pc Bailer-Jones et al. (2021) μα82.67 ± 0.02 m a s y r − 1Gaia Collaboration (2022) μδ9.92 ± 0.02 m a s y r − 1Gaia Collaboration (2022) RV+ 55.8 ± 2.7 k m s − 1Gaia Collaboration (2022) U+ 29.7 ± 0.6 k m s − 1This work V− 54.8 ± 2.1 k m s − 1This work W+ 0.7 ± 0.9 k m s − 1This work SpT M4 This work R⋆ 0.240 ± 0.012R⊙ This work M⋆ 0.225 ± 0.012M⊙ This work Teff 3075 ± 75 K This work log g⋆ 5.0 ± 0.2 This work [ F e / H ] + 0.09 ± 0.20 dex This work (spectroscopy)−0.25 ± 0.25 dex This work (SED) Age 6.2 − 2.2 + 3.2 Gyr This work T mag 13.5308 ± 0.0073 Stassun et al. (2019) B mag 18.14 ± 0.16 Zacharias et al. (2013) V mag 16.24 ± 0.01 Zacharias et al. (2013) G mag 14.8940 ± 0.0007 Gaia Collaboration (2022) J mag 11.808 ± 0.024 Cutri et al. (2003) H mag 11.264 ± 0.026 Cutri et al. (2003) K mag 10.917 ± 0.019 Cutri et al. (2003) W1 mag 10.753 ± 0.023 Cutri et al. (2021) W2 mag 10.571 ± 0.020 Cutri et al. (2021) W3 mag 10.387 ± 0.049 Cutri et al. (2021) W4 mag > 8.92 Cutri et al. (2021) Designations TOI-715, TIC 271971130, 2MASS J07352425−7334388, APASS 33649915, Gaia DR2 5262666416118954368, UCAC4 083–012601, WISE J073524.46−733438.7 Parameter Value Source α 07:35:24.56 Gaia Collaboration (2022) δ −73:34:38.67 Gaia Collaboration (2022) Distance 42.46 ± 0.03 pc Bailer-Jones et al. (2021) μα82.67 ± 0.02 m a s y r − 1Gaia Collaboration (2022) μδ9.92 ± 0.02 m a s y r − 1Gaia Collaboration (2022) RV+ 55.8 ± 2.7 k m s − 1Gaia Collaboration (2022) U+ 29.7 ± 0.6 k m s − 1This work V− 54.8 ± 2.1 k m s − 1This work W+ 0.7 ± 0.9 k m s − 1This work SpT M4 This work R⋆ 0.240 ± 0.012R⊙ This work M⋆ 0.225 ± 0.012M⊙ This work Teff 3075 ± 75 K This work log g⋆ 5.0 ± 0.2 This work [ F e / H ] + 0.09 ± 0.20 dex This work (spectroscopy)−0.25 ± 0.25 dex This work (SED) Age 6.2 − 2.2 + 3.2 Gyr This work T mag 13.5308 ± 0.0073 Stassun et al. (2019) B mag 18.14 ± 0.16 Zacharias et al. (2013) V mag 16.24 ± 0.01 Zacharias et al. (2013) G mag 14.8940 ± 0.0007 Gaia Collaboration (2022) J mag 11.808 ± 0.024 Cutri et al. (2003) H mag 11.264 ± 0.026 Cutri et al. (2003) K mag 10.917 ± 0.019 Cutri et al. (2003) W1 mag 10.753 ± 0.023 Cutri et al. (2021) W2 mag 10.571 ± 0.020 Cutri et al. (2021) W3 mag 10.387 ± 0.049 Cutri et al. (2021) W4 mag > 8.92 Cutri et al. (2021) Open in new tab Table 1. Stellar parameters adopted for this work. Designations TOI-715, TIC 271971130, 2MASS J07352425−7334388, APASS 33649915, Gaia DR2 5262666416118954368, UCAC4 083–012601, WISE J073524.46−733438.7 Parameter Value Source α 07:35:24.56 Gaia Collaboration (2022) δ −73:34:38.67 Gaia Collaboration (2022) Distance 42.46 ± 0.03 pc Bailer-Jones et al. (2021) μα82.67 ± 0.02 m a s y r − 1Gaia Collaboration (2022) μδ9.92 ± 0.02 m a s y r − 1Gaia Collaboration (2022) RV+ 55.8 ± 2.7 k m s − 1Gaia Collaboration (2022) U+ 29.7 ± 0.6 k m s − 1This work V− 54.8 ± 2.1 k m s − 1This work W+ 0.7 ± 0.9 k m s − 1This work SpT M4 This work R⋆ 0.240 ± 0.012R⊙ This work M⋆ 0.225 ± 0.012M⊙ This work Teff 3075 ± 75 K This work log g⋆ 5.0 ± 0.2 This work [ F e / H ] + 0.09 ± 0.20 dex This work (spectroscopy)−0.25 ± 0.25 dex This work (SED) Age 6.2 − 2.2 + 3.2 Gyr This work T mag 13.5308 ± 0.0073 Stassun et al. (2019) B mag 18.14 ± 0.16 Zacharias et al. (2013) V mag 16.24 ± 0.01 Zacharias et al. (2013) G mag 14.8940 ± 0.0007 Gaia Collaboration (2022) J mag 11.808 ± 0.024 Cutri et al. (2003) H mag 11.264 ± 0.026 Cutri et al. (2003) K mag 10.917 ± 0.019 Cutri et al. (2003) W1 mag 10.753 ± 0.023 Cutri et al. (2021) W2 mag 10.571 ± 0.020 Cutri et al. (2021) W3 mag 10.387 ± 0.049 Cutri et al. (2021) W4 mag > 8.92 Cutri et al. (2021) Designations TOI-715, TIC 271971130, 2MASS J07352425−7334388, APASS 33649915, Gaia DR2 5262666416118954368, UCAC4 083–012601, WISE J073524.46−733438.7 Parameter Value Source α 07:35:24.56 Gaia Collaboration (2022) δ −73:34:38.67 Gaia Collaboration (2022) Distance 42.46 ± 0.03 pc Bailer-Jones et al. (2021) μα82.67 ± 0.02 m a s y r − 1Gaia Collaboration (2022) μδ9.92 ± 0.02 m a s y r − 1Gaia Collaboration (2022) RV+ 55.8 ± 2.7 k m s − 1Gaia Collaboration (2022) U+ 29.7 ± 0.6 k m s − 1This work V− 54.8 ± 2.1 k m s − 1This work W+ 0.7 ± 0.9 k m s − 1This work SpT M4 This work R⋆ 0.240 ± 0.012R⊙ This work M⋆ 0.225 ± 0.012M⊙ This work Teff 3075 ± 75 K This work log g⋆ 5.0 ± 0.2 This work [ F e / H ] + 0.09 ± 0.20 dex This work (spectroscopy)−0.25 ± 0.25 dex This work (SED) Age 6.2 − 2.2 + 3.2 Gyr This work T mag 13.5308 ± 0.0073 Stassun et al. (2019) B mag 18.14 ± 0.16 Zacharias et al. (2013) V mag 16.24 ± 0.01 Zacharias et al. (2013) G mag 14.8940 ± 0.0007 Gaia Collaboration (2022) J mag 11.808 ± 0.024 Cutri et al. (2003) H mag 11.264 ± 0.026 Cutri et al. (2003) K mag 10.917 ± 0.019 Cutri et al. (2003) W1 mag 10.753 ± 0.023 Cutri et al. (2021) W2 mag 10.571 ± 0.020 Cutri et al. (2021) W3 mag 10.387 ± 0.049 Cutri et al. (2021) W4 mag > 8.92 Cutri et al. (2021) Open in new tab 2.1 Reconnaissance spectroscopy We collected an optical spectrum of TOI-715 on 2022 January 7 (ut) using the Low Dispersion Survey Spectrograph on the 6.5-m Magellanii (Clay) Telescope at Las Campanas Observatory in Chile. With its upgraded red-sensitive CCD (Stevenson et al. 2016), this instrument is now known as ‘LDSS-3C’. We used LDSS-3C in long-slit mode with the standard setup (fast readout speed, low gain, and 1 × 1 binning) and the VPH-Red grism, OG-590 blocking filter, and the 0 . ′ ′ 75 × 0 . ′ 4 center slit. This setup provides spectra covering 6000–10 000 Å with a resolution of R ∼ 1810, which is insufficient to produce a notable constraint on vsin i⋆. We observed TOI-715 during clear conditions with seeing of 0 . ′ ′ 5. We collected six exposures of 300 s each, totaling 30 min on-source, at an average airmass of 1.445. Afterwards, we collected three 1-s exposures of the nearby F8 V standard star HR 2283 (Maiolino, Rieke & Rieke 1996) at an average airmass of 1.360. At each pointing, we collected a 1-s HeNeAr arc lamp exposure and three, 10-s flat fields with the ‘quartz high’ lamp. We reduced the data with a custom, python-based pipeline, which includes bias removal, flat field correction, and spectral extraction. For wavelength calibration, the HeNeAr arc exposure was used, which was extracted similarly to the science spectrum. For flux calibration, we used the ratio of the spectrum of the flux standard HR 2283 with an F8 V template from Pickles (1998) to compute a relative flux correction; no correction was made to address telluric absorption. The final science spectrum has a maximum SNR per resolution element of 193 at 9202 Å and a mean SNR per resolution element of 124 in the 6000–10 000 Å range. The reduced spectrum (Fig. 1) was analysed using kastredux.2 We compared the spectrum to Sloan Digital Sky Survey templates from Kesseli et al. (2017), finding a best match to an M4 dwarf. This classification was verified through spectral classification indices from Reid, Hawley & Gizis (1995), Lépine, Rich & Shara (2003), and Riddick, Roche & Lucas (2007), all of which measure consistent spectral classifications of M4. We evaluated the ζ metallicity index (Lépine, Rich & Shara 2007; Lépine et al. 2013), determining a value of 1.066 ± 0.002 which corresponds to a metallicity [Fe/H] = +0.09 ± 0.20 dex based on the empirical calibration of Mann et al. (2013). There is no significant evidence of Hα emission, with an equivalent width limit |EW|7.1 are reported as threshold crossing events (TCEs). TOI-715.01 was first reported as a candidate on 2019 May 24 (Guerrero et al. 2021) following a multisector transit search (Jenkins 2002; Jenkins et al. 2010, 2020) conducted on 2019 May 5 for sectors 1–9. The candidate was already identified in the multi-sector search of Sectors 1–6 conducted on 2019 April 18, but did not at this time pass the necessary tests to be reported as a TOI. The transit signal was fitted with an initial limb-darkened transit model (Li et al. 2019) and subjected to a suite of diagnostic tests (Twicken et al. 2018) to help determine whether the signature is from an exoplanet. The transit signature passed all of the tests reported in the Data Validation report for Sectors 1–94, and the difference image centroiding test located the source of the transit signal to within 3 . ′ ′ 7 ± 3 . ′ ′ 5 in the subsequent data validation reports for the multisector searches of Sectors 1–13 and 27–39. At the time of the first reported TCE5, the TESS Input Catalog (TIC) in use was version 7, which had not yet incorporated the Gaia-DR2 data; as such, TOI-715 did not have a stellar radius and the planet candidate was reported as a 7.1 R ⊕ object with a period of ∼ 19.2 d ⁠. Following the update to the TICv8 (Stassun et al. 2019) the planet candidate’s radius estimate was revised, demonstrating that TOI-715.01 was likely a super-Earth and thus a high priority target. spoc TCEs are subject to the TESS-ExoClass6 automated classifier that reduces the number of TCEs that undergo the manual TOI vetting procedure. TESS-ExoClass applies a series of tests that are similar to the KeplerRobovetter (Coughlin et al. 2016; Thompson et al. 2018). TOI 715.01 passes all the tests of TESS-ExoClass and has been placed in the Tier 1 (highest quality candidate) category, and has been classified as a Tier 1 candidate in the subsequent spoc multi-sector 1–13, 1–36, and 1–39 searches. Further details for the TOI assignment process are available in Guerrero et al. (2021). We present the PDCSAP TESS light curves for all 24 sectors of 2-min cadence data in Fig. 4; the timings of the 29 transits are indicated with dark pink arrows. Figure 4. Open in new tabDownload slide PDCSAP flux extracted from the short (2 min) cadence data of the 24 sectors (1–13, 27, 29–37, 39) in which TOI-715 was observed. Light grey point show the 120 s exposures and the purple line shows the flux in 3 m i n bins. The transit events of TOI-715.01 are shown with the dark pink arrows and the locations of possible transits of TIC 271971130.02 are indicated with green arrows; we note that individual transits are not readily apparent by eye. 3.2 Search for additional candidates We make use of the custom pipeline sherlock7, presented in Pozuelos et al. (2020) and Demory et al. (2020), to search the TESS data for additional transiting candidates, as was done in Dransfield et al. (2022a). sherlock downloads all light curves from MAST and, using Wotan (Hippke et al. 2019), applies a bi-weight function with varying window sizes to detrend the data. Each detrended light curve and the original PDCSAP light curve are then searched for transit-like signals using Transit Least Squares (Hippke & Heller 2019). We use only the 2-min cadence in our candidate search, applying a Savitzky–Golay (SG) digital filter (Savitzky & Golay 1964) previously following the strategy described in Delrez et al. (2022)8 and we test 11 window sizes between 0.19 and 1.9 d when detrending with the bi-weight filter. We thus carry out our transit search on the PDCSAP light curve and the 11 detrended light curves and only consider signals with SNR > 7 for further investigation. We recover TOI-715.01 at a period of 19.29 d in the first instance in all 12 light curves, and we find that the highest SNR and SDE are achieved in the PDCSAP flux without any detrending with Wotan. We additionally find two other periodic signals in the data. Of these, the signal with highest SNR is at a period of 25.61 d ⁠, putting it within 0.4 per cent of the first order 4:3 commensurability. The signal is detected in nine of the searched light curves with a maximum SNR of 13.81 and SDE of 11.6; it has a depth of ∼ 1 p p t ⁠, making this a ∼ 1.16 R ⊕ candidate. In order to ensure this signal is not an artefact produced by the SG filter, we additionally search the untreated PDCSAP light curve and recover the signal with an SNR of 6.81. We adopt this candidate as TIC 271971130.029 and highlight the positions of transit events on Fig. 4 with dark green arrows. In Fig. 5, we present the TESS light curve folded on this signal, along with the Lomb–Scargle periodogram. Figure 5. Open in new tabDownload slide Results of our search for additional candidates in the data, using Transit Least Squares as implemented by Sherlock. Upper panel:tls periodogram showing the detected 25.61-d period of TIC 271971130.02 and its harmonics. Lower panel:TESS PDCSAP light curve phase-folded on this period, with a transit model overplotted. We note that the errorbars on the binned points are smaller than the markers, and that the light curve shown here is the one treated with the SG Filter. Given the large amount of data in Fig. 4, it is not apparent by eye how much pre- and post-transit baseline each transit has. We therefore note that the total number of in-transit points for TIC 271971130.02 is 1371, while the total number of points before and after the transits are 1349 and 1369 respectively (counted up to 1 transit duration). Thus on average the pre-transit baseline is 98.4 per cent of a transit duration, while the post-transit baseline is 99.9 per cent. The SPOC ran the Data Validation module at the ephemeris for the second signal and obtained an SNR of 5.5σ, and additionally identified other sub-threshold transit-like signals at higher SNR. The second signal we recover has a period of 7.17 d and a depth of < 1 p p t ⁠; it has a maximum SNR of 9.63 and SDE of 8.17. The very low SNR of the signal makes it challenging to discern by eye whether or not the shape is consistent with that of a planetary transit. We also note that this signal is only found in four of the detrended light curves (with window sizes between 0.38–0.65 d, but not the PDCSAP light curve, indicating that the signal could be dependent on detrending. The duration of a transit at this period on a circular orbit is should be of order 0.065 d, and all tested windows are at least 3 × this duration. We therefore do not believe the detrending will have suppressed the transit in other light curves if it is real. When TESS returns to the southern skies in its second extended mission, the additional photometry will provide further insight into the nature of this signal. It is important to note that the SNR and SDE yielded by sherlock are inherited from the transit least squares algorithm, which uses a simple estimation that cannot be compared with values provided by other pipelines, such as SPOC. Instead, these values are used internally in sherlock to compare the signals found and select the most prominent among them. 4 VETTING AND VALIDATION In this section we describe the results of the multifacility follow-up campaign conducted between May 2020 and April 2022. We begin with the high-resolution imaging observations, and then outline the photometric observations collected from five southern observatories. Finally, we describe how all our follow-up observations were used to validate the planetary nature of TOI-715.01 and TIC 271971130.02. All follow-up observations are summarized in Table 2. Table 2. Summary of ground-based follow-up observations carried out for the validation of TOI-715.01 and TIC 271971130.02. Follow-up Observations High resolution imaging Observatory Filter Date Sensitivity limit Result Gemini South 562 nm 2020 December 26 Δm = 4.69 at 0.5″ No sources detected Gemini South 832 nm 2020 December 26 Δm = 5.07 at 0.5″ No sources detected Photometric Follow-up LCO-SAAO Sloan-i′ 2020 May 13 Ingress Transit ruled out on or off target during the time covered LCO-SAAO Sloan-i′ 2020 October 15 (.01) Ingress Transit detected on target ExTrA1.21 𝜇 m2021 February 26 (.01) Full Detection ExTrA1.21 𝜇 m2021 April 25 (.01) Full Detection TRAPPIST-South I + z′ 2021 April 25 (.01) Full Detection LCO-CTIO Sloan-i′ 2021 April 25 (.01) Full Detection SSO-Callisto Sloan-r′ 2021 September 26 (.01) Full Detection SSO-Io Sloan-r′ 2021 September 26 (.01) Full Detection SSO-Europa Sloan-r′ 2021 September 26 (.01) Full Detection SSO-Ganymede Sloan-r′ 2021 September 26 (.01) Gapped Interruption due to weather – ingress detected TRAPPIST-South I + z′ 2021 September 26 (.01) Full Detection OACC-CAO Sloan-i’2 2021 November 24 (.01) Full Detection SSO-Callisto I + z′ 2021 November 24 (.01) Full Detection SSO-Io I + z′ 2021 November 24 (.01) Full Detection SSO-Ganymede I + z′ 2021 November 24 (.01) Full Detection TRAPPIST-South I + z′ 2021 November 24 (.01) Full Detection ExTrA1.21 𝜇 m2022 February 08 (.01) Full Detection TRAPPIST-South I + z′ 2022 April 07 (.01) Full Detection ExTrA1.21 𝜇 m2022 April 07 (.01) Full Detection TRAPPIST-South I + z′ 2022 October 25 (.02) Egress Inconclusive (high airmass) Spectroscopic Observations Instrument Wavelength range Date Number of spectra Use Magellan/LDSS3380 − 1000 n m2022 January 06 1 Stellar characterization Follow-up Observations High resolution imaging Observatory Filter Date Sensitivity limit Result Gemini South 562 nm 2020 December 26 Δm = 4.69 at 0.5″ No sources detected Gemini South 832 nm 2020 December 26 Δm = 5.07 at 0.5″ No sources detected Photometric Follow-up LCO-SAAO Sloan-i′ 2020 May 13 Ingress Transit ruled out on or off target during the time covered LCO-SAAO Sloan-i′ 2020 October 15 (.01) Ingress Transit detected on target ExTrA1.21 𝜇 m2021 February 26 (.01) Full Detection ExTrA1.21 𝜇 m2021 April 25 (.01) Full Detection TRAPPIST-South I + z′ 2021 April 25 (.01) Full Detection LCO-CTIO Sloan-i′ 2021 April 25 (.01) Full Detection SSO-Callisto Sloan-r′ 2021 September 26 (.01) Full Detection SSO-Io Sloan-r′ 2021 September 26 (.01) Full Detection SSO-Europa Sloan-r′ 2021 September 26 (.01) Full Detection SSO-Ganymede Sloan-r′ 2021 September 26 (.01) Gapped Interruption due to weather – ingress detected TRAPPIST-South I + z′ 2021 September 26 (.01) Full Detection OACC-CAO Sloan-i’2 2021 November 24 (.01) Full Detection SSO-Callisto I + z′ 2021 November 24 (.01) Full Detection SSO-Io I + z′ 2021 November 24 (.01) Full Detection SSO-Ganymede I + z′ 2021 November 24 (.01) Full Detection TRAPPIST-South I + z′ 2021 November 24 (.01) Full Detection ExTrA1.21 𝜇 m2022 February 08 (.01) Full Detection TRAPPIST-South I + z′ 2022 April 07 (.01) Full Detection ExTrA1.21 𝜇 m2022 April 07 (.01) Full Detection TRAPPIST-South I + z′ 2022 October 25 (.02) Egress Inconclusive (high airmass) Spectroscopic Observations Instrument Wavelength range Date Number of spectra Use Magellan/LDSS3380 − 1000 n m2022 January 06 1 Stellar characterization Open in new tab Table 2. Summary of ground-based follow-up observations carried out for the validation of TOI-715.01 and TIC 271971130.02. Follow-up Observations High resolution imaging Observatory Filter Date Sensitivity limit Result Gemini South 562 nm 2020 December 26 Δm = 4.69 at 0.5″ No sources detected Gemini South 832 nm 2020 December 26 Δm = 5.07 at 0.5″ No sources detected Photometric Follow-up LCO-SAAO Sloan-i′ 2020 May 13 Ingress Transit ruled out on or off target during the time covered LCO-SAAO Sloan-i′ 2020 October 15 (.01) Ingress Transit detected on target ExTrA1.21 𝜇 m2021 February 26 (.01) Full Detection ExTrA1.21 𝜇 m2021 April 25 (.01) Full Detection TRAPPIST-South I + z′ 2021 April 25 (.01) Full Detection LCO-CTIO Sloan-i′ 2021 April 25 (.01) Full Detection SSO-Callisto Sloan-r′ 2021 September 26 (.01) Full Detection SSO-Io Sloan-r′ 2021 September 26 (.01) Full Detection SSO-Europa Sloan-r′ 2021 September 26 (.01) Full Detection SSO-Ganymede Sloan-r′ 2021 September 26 (.01) Gapped Interruption due to weather – ingress detected TRAPPIST-South I + z′ 2021 September 26 (.01) Full Detection OACC-CAO Sloan-i’2 2021 November 24 (.01) Full Detection SSO-Callisto I + z′ 2021 November 24 (.01) Full Detection SSO-Io I + z′ 2021 November 24 (.01) Full Detection SSO-Ganymede I + z′ 2021 November 24 (.01) Full Detection TRAPPIST-South I + z′ 2021 November 24 (.01) Full Detection ExTrA1.21 𝜇 m2022 February 08 (.01) Full Detection TRAPPIST-South I + z′ 2022 April 07 (.01) Full Detection ExTrA1.21 𝜇 m2022 April 07 (.01) Full Detection TRAPPIST-South I + z′ 2022 October 25 (.02) Egress Inconclusive (high airmass) Spectroscopic Observations Instrument Wavelength range Date Number of spectra Use Magellan/LDSS3380 − 1000 n m2022 January 06 1 Stellar characterization Follow-up Observations High resolution imaging Observatory Filter Date Sensitivity limit Result Gemini South 562 nm 2020 December 26 Δm = 4.69 at 0.5″ No sources detected Gemini South 832 nm 2020 December 26 Δm = 5.07 at 0.5″ No sources detected Photometric Follow-up LCO-SAAO Sloan-i′ 2020 May 13 Ingress Transit ruled out on or off target during the time covered LCO-SAAO Sloan-i′ 2020 October 15 (.01) Ingress Transit detected on target ExTrA1.21 𝜇 m2021 February 26 (.01) Full Detection ExTrA1.21 𝜇 m2021 April 25 (.01) Full Detection TRAPPIST-South I + z′ 2021 April 25 (.01) Full Detection LCO-CTIO Sloan-i′ 2021 April 25 (.01) Full Detection SSO-Callisto Sloan-r′ 2021 September 26 (.01) Full Detection SSO-Io Sloan-r′ 2021 September 26 (.01) Full Detection SSO-Europa Sloan-r′ 2021 September 26 (.01) Full Detection SSO-Ganymede Sloan-r′ 2021 September 26 (.01) Gapped Interruption due to weather – ingress detected TRAPPIST-South I + z′ 2021 September 26 (.01) Full Detection OACC-CAO Sloan-i’2 2021 November 24 (.01) Full Detection SSO-Callisto I + z′ 2021 November 24 (.01) Full Detection SSO-Io I + z′ 2021 November 24 (.01) Full Detection SSO-Ganymede I + z′ 2021 November 24 (.01) Full Detection TRAPPIST-South I + z′ 2021 November 24 (.01) Full Detection ExTrA1.21 𝜇 m2022 February 08 (.01) Full Detection TRAPPIST-South I + z′ 2022 April 07 (.01) Full Detection ExTrA1.21 𝜇 m2022 April 07 (.01) Full Detection TRAPPIST-South I + z′ 2022 October 25 (.02) Egress Inconclusive (high airmass) Spectroscopic Observations Instrument Wavelength range Date Number of spectra Use Magellan/LDSS3380 − 1000 n m2022 January 06 1 Stellar characterization Open in new tab 4.1 High resolution imaging Close stellar companions (bound or in the line of sight) can confound derived exoplanet properties in a number of ways. The detected transit signal might be a false positive due to a background eclipsing binary and even real planet discoveries will yield incorrect stellar and exoplanet parameters if a close companion exists and is unaccounted for (e.g. Ciardi et al. 2015; Furlan & Howell 2017, 2020). Additionally, the presence of a close companion star leads to the non-detection of small planets residing within the same exoplanetary system (Lester et al. 2021). Approximately 25 per cent of M dwarfs are part of binary or multiple star systems (e.g. Cortes Contreras et al. 2015; Winters et al. 2019), though fewer than 5 per cent of known spectroscopic binaries include an M-dwarf primary star (Pourbaix et al. 2004).10 None the less, high resolution imaging provides crucial information towards our understanding of exoplanetary formation, dynamics, and evolution (Howell et al. 2021). TOI-715 was observed on 2020 December 26 ut using the Zorro speckle instrument on the Gemini South 8-m telescope (Scott et al. 2021). Zorro provides simultaneous speckle imaging in two bands (⁠ 562 n m and 832 n m ⁠) with output data products including a reconstructed image with robust contrast limits on companion detection (see Howell & Furlan 2022). TOI-715 was found to be a single star to within the angular and brightness contrast levels achieved. Eight sets of 1000 × 0.06 s images were obtained and processed by our standard reduction pipeline (Howell et al. 2011). Fig. 6 shows our final contrast curves and the 832 n m reconstructed speckle image. These high-resolution observations revealed no companion star brighter than 5 magnitudes below that of the target star from the 8-m telescope diffraction limit (⁠ 20 m a s ⁠) out to 1.2 arcsec. At the distance of TOI-715 (⁠ 42.4 p c ⁠) these angular limits correspond to spatial limits of 0.84 to 50.9 a u ⁠. Figure 6. Open in new tabDownload slide Plot showing the 5σ speckle imaging contrast curves in both filters as a function of the angular separation out to 1.2 arcsec, the end of speckle coherence. The inset shows the reconstructed 832 n m image with a 1 arcsec scale bar. The star, TOI-715, was found to have no close companions to within the angular and brightness contrast levels achieved. 4.2 Photometric follow-up 4.2.1 Las Cumbres Observatory We used the Las Cumbres Observatory Global Telescopes (LCOGT; Brown et al. 2013) 1.0-m network nodes at South Africa Astronomical Observatory (SAAO) and Cerro Tololo Inter-American Observatory to observe three transits of TOI-715.01. First and second transits were observed with LCO-SAAO on 2020 May 13 and 2020 October 15, and third was observed with LCO-CTIO on 2021 April 25. All observations were carried out with the Sloan-i′ and an exposure time of 180 s. Photometric brightness was measured using an uncontaminated target aperture of 4.3 arcsec (11 pixels). We used the TESS transit Finder, which is a customized version of the Tapir software package (Jensen 2013), to schedule our photometric time series. The 1.0-m telescopes are equipped with 4096 × 4096 SINISTRO cameras with an image scale of 0 . ′ ′ 389 per pixel, resulting in a 26 arcmin × 26 arcmin field of view. The raw images were calibrated with the standard LCO banzai pipeline (McCully et al. 2018), and photometric data were extracted with astroimageJ (Collins et al. 2017). The observation of 2020 May 13 only provided ∼ 20 p e r c e n t in-transit coverage as well as 1.2 h of pre-transit baseline. The transit was ruled out on or off target during the window covered by this observation.11 The subsequent observation of 2020 October 15 confirmed the transit event on target with the detection of an ingress that was 29 min late relative to the ephemeris. The observation of 2021 April 25 resulted in the detection of a ful transit. 4.2.2 SPECULOOS-Southern Observatory The SPECULOOS Southern Observatory (SSO) is comprised of four Ritchey-Chrétien 1.0 m-class telescopes installed at ESO Paranal in the Atacama desert (Delrez et al. 2018). Designed to hunt for small, habitable-zone planets orbiting ultra-cool stars (Sebastian et al. 2021), all four telescopes are equipped with a deep-depletion Andor CCD camera with 2048 × 2048 13- 𝜇 m pixels. Each telescope therefore has a field of view of 12 arcmin × 12 arcmin and a pixel scale of 0 . ′ ′ 35 (Burdanov et al. 2018). All SPECULOOS observations are scheduled using the python package spock.12 (Sebastian et al. 2021), and processed in the first instance by an automatic data reduction pipeline, described in detail in Murray et al. (2020). Successful observations of TESS targets are then reprocessed using prose, a publicly available python framework for processing astronomical images13 as described in Garcia et al. (2021, 2022). Images are calibrated and aligned before performing aperture photometry on the 500 brightest sources detected; prose then performs differential photometry (Broeg, Fernández & Neuhäuser 2005) on the target star to extract the light curve. Individual light curves are detrended for airmass, sky background and FWHM (full width at half maximum) using second order polynomials in time, and they are then modelled using exoplanet (Foreman-Mackey et al. 2021b). We observed TOI-715.01 for the first time on the night of 2021 September 26 with all four telescopes simultaneously (Io, Europa, Ganymede, Callisto) using the Sloan-r′ filter and 120 -s exposures. Cloudy skies during the transit caused some large systematics in three out of four telescopes, although this did not prevent the transit from being detected. The fourth telescope, Ganymede, closed during the weather alert, causing only the ingress to be detected. We re-observed TOI-715.01 on the night of 2021 November 24 with three of our telescopes. We collected 13 -s exposures using the custom filter I + z′ (Sebastian et al. 2021). Observations were once again hampered by the untimely appearance of clouds, which affected the in-transit precision of our observations. Fortunately, the transit was still detected with all three instruments despite the overcast conditions. 4.2.3 TRAPPIST-South We observed four transits of TOI-715.01 with TRAPPIST-South (TS; Gillon et al. 2011; Jehin et al. 2011), located in ESO La Silla Observatory in Chile. This 0.6-m telescope is equipped with a FLI ProLine PL3041-BB camera and a back-illuminated CCD with a pixel size of 0 . ′ ′ 64, providing a total field of view of 22 arcmin × 22 arcmin for an array of 2048 × 2048 pixels. TS is a Ritchey–Chrétien telescope with F/8 and is equipped with a German equatorial mount. All four transits were observed with the custom I + z′ filter to maximize the photometric precision. The observations took place on 2021 April 25, 2021 September 26, 2021 November 24, and 2022 April 7, with exposure times of 50s, 70s, 120s, and 90s, respectively. We reduced the images using prose pipeline (Garcia et al. 2021, 2022) to extract optimal light curves. Individual light curves were then modeled using the exoplanet (Foreman-Mackey et al. 2021b) package and detrended using a second order polynomial of airmass, FWHM and sky background. None of the observations suffered weather losses. We additionally observed TOI-715 on 2022 October 25 during a partial transit of the second candidate (.02) but the target was too low during the event. The observation was therefore inconclusive. 4.2.4 ExTrA ExTrA (Bonfils et al. 2015) is a near-infrared (0.85–1.55 𝜇 m ⁠) multi-object spectrograph fed by three 0.6 m telescopes located at La Silla observatory. We observed four full transits of TOI 715.01 on 2021 February 26, 2021 April 25, 2022 February 8, and 2022 April 7. We observed the first three transits using two telescopes, and the fourth using three; all observations were carried out with 8 arcsec aperture fibers. For both nights, we used the spectrograph’s low resolution mode (R ∼ 20) and 60s exposures. Five fibre positioners are used at the focal plane of each telescope to collect light from the target and four comparison stars. As comparison stars, we also observed 2MASS J07381369−7347351, 2MASS J07410628−7341297, 2MASS J07393394−7330535, and 2MASS J07372328−7321411 with J-magnitude (Skrutskie et al. 2006) and Teff (Gaia Collaboration 2021) similar to TOI-715. The resulting ExTrA data were analyzed with custom data reduction software, detailed in Cointepas et al. (2021). 4.2.5 OACC-CAO We observed a transit of TOI-715.01 with Campo Catino Austral Observatory (OACC-CAO), located in El Sauce Observatory in the Atacama desert in Chile, on the night of 2021 November 24. The telescope is a 0.6 m Planewave CDK 24 arcsec with a Planewave L600 mount, equipped with an FLI filter wheel with Sloan filters and an FLI PL16803 camera with 4096 × 4096 9 𝜇 m pixels. It has a field of view of 32 arcmin and a pixel scale of 0 . ′ ′ 48. TOI-715.01 was observed with the Sloan-i’2 filter using 180s exposures. The images were reduced using AstroImageJ (Collins et al. 2017) and we detect the full transit in an uncontaminated aperture. 4.3 Statistical validation We make use of the statistical validation package triceratops14 (Giacalone & Dressing 2020; Giacalone et al. 2021) to assess the probability of the planet hypothesis for TOI-715.01 and TIC 271971130.02. Triceratops calculates the flux contribution from nearby stars to check if any could be responsible for the transit signal. It then calculates the relative probabilities of a range of transiting planet (TP) and eclipsing binary (EB) scenarios using light curve models fitted to the phase-folded photometry. In the first instance we find that the false positive probabilities (FPP) are 0.403 and 0.592 for candidates .01 and .02, respectively when examining the TESS data, while the threshold for validation is 0.015. For candidate .01, we make use of our follow-up photometry by folding in the four TRAPPIST-South light curves. We choose these as they were observed with the custom I + z’ filter (which is very similar to the TESS bandpass) and all four are at different epochs. With this phase-folded subset of our follow-up photometry, we find that the false positive probability reduces to 0.0107, placing it below the threshold for statistical validation. To similarly reduce the FPP for TIC 271971130.02, we will need to collect ground-based photometry to confirm the event on-target or rule out events on nearby stars (such an observation was scheduled from Hazelwood Observatory but cancelled due to bad weather). 4.3.1 Statistical validation conclusions With a FPP of 0.0107 TOI-715.01 is now below the customary threshold for statistical validation (0.015; Giacalone et al. 2021); we therefore consider the planet validated and refer to it as TOI-715 b. TIC 271971130.02 does not yet meet the criterion for statistical validation, and is therefore classified as a ‘likely transiting planet’ until more evidence such as ground-based photometry is collected. 5 GLOBAL PHOTOMETRIC ANALYSIS We carried out a global photometric analysis of the TESS photometry and the data sets described in Section 4.3 using allesfitter (Günther & Daylan 2019, 2021), a flexible and publicly available python-based inference package. We exclude all partial transits, as well as the transit obtained from OACC (due to large scatter in the data) from our analysis, leaving 15 full transits from our ground-based facilities. Allesfitter generates light curve models using Ellc (Maxted 2016), and Gaussian Process (GP) models using Celerite (Foreman-Mackey et al. 2017). The best fitting models are then chosen using either a nested sampling algorithm via Dynesty (Speagle 2020), or MCMC sampling with Emcee (Foreman-Mackey et al. 2013). For all modeling in this paper, we make use of the nested sampling algorithm as it calculates the Bayesian evidence at each step in the sampling; this allows us to compare the Bayesian evidence for different models by calculating the Bayes Factor (Kass & Raftery 1995). All prior distributions and their bounds can be found in Table 3. Table 3. Priors used in our fit, along with fitted and derived parameters. Uniform priors are indicated as 𝑈 ( l o w e r b o u n d , u p p e r b o u n d ) and normal priors are indicated as 𝑁 ( m e a n , s t a n d a r d d e v i a t i o n ) ⁠. ⋆Equillibrium temperature is calculated assuming an albedo of 0.3 and emissivity of 1.TOI-715 b TIC 271971130.02Fit parametrization and priors Transit depth; Rp/R⋆𝑈 ( 0.01 , 0.1 ) 𝑈 ( 0.01 , 0.1 ) Inverse scaled semimajor axis; (R⋆ + Rp)/a𝑈 ( 0.01 , 0.05 ) 𝑈 ( 0.008 , 0.04 ) Orbital inclination; cos i𝑈 ( 0.000 , 0.04 ) 𝑈 ( 0.000 , 0.04 ) Transit epoch; T0 (BJD)𝑈 ( 2 458 327.3 , 2 458 327.7 ) 𝑈 ( 2 458 342.0 , 2 458 342.4 ) Period; P (d)𝑈 ( 19.0 , 19.4 ) 𝑈 ( 25.4 , 25.8 ) Limb-darkening coefficients TESS u 10.3322 ± 0.0013 TESS q 1 𝑁 ( 0.413 , 0.050 ) TESS u 20.3103 ± 0.0047 TESS q 2 𝑁 ( 0.259 , 0.050 ) I + z u 10.2942 ± 0.0014 I + z q 1 𝑁 ( 0.453 , 0.050 ) I + z u 20.3791 ± 0.0052 I + z q 2 𝑁 ( 0.218 , 0.050 ) Sloan-i’ u 10.3863 ± 0.0018 Sloan-i’ q 1 𝑁 ( 0.532 , 0.050 ) Sloan-i’ u 20.3431 ± 0.0060 Sloan-i’ q 2 𝑁 ( 0.265 , 0.050 ) Sloan-r’ u 10.6408 ± 0.0032 Sloan-r’ q 1 𝑁 ( 0.768 , 0.050 ) Sloan-r’ u 20.2273 ± 0.0079 Sloan-r’ q 2 𝑁 ( 0.218 , 0.050 ) ExTra (1.2 μm) u 10.2058 ± 0.0007 ExTra (1.2μm) q 1 𝑁 ( 0.142 , 0.050 ) ExTra (1.2 μm) u 20.1708 ± 0.0028 ExTra (1.2μm) q 2 𝑁 ( 0.273 , 0.050 ) External priors GP priors Stellar Mass; M⋆ (⁠ M ⊙ ⁠)𝑁 ( 0.225 , 0.012 )Amplitude scale GP ln ⁡ 𝜎 ( flux ) 𝑈 ( − 7 , − 7 ) Stellar Radius; R⋆ (R⊙)𝑁 ( 0.240 , 0.012 )Length scale GP ln ⁡ 𝜌 ( flux ) 𝑈 ( − 7 , 2 ) Stellar Effective Temperature; Teff (K)𝑁 ( 3075 , 75 )Fitted parameters Source Rp/R⋆ 0.0618 ± 0.0017 0.0425 ± 0.0034 2-planet linear fit (R⋆ + Rp)/a 0.01367 ± 0.000170.01129 − 0.00047 + 0.000552-planet linear fit cos i0.00252 − 0.00032 + 0.00030 0.0048 − 0.0024 + 0.00212-planet linear fit T0 (BJD)2 459 002.63051 − 0.00074 + 0.00070 2 459 007.9879 − 0.0041 + 0.00452-planet linear fit P (d)19.288004 − 0.000024 + 0.000027 25.60712 − 0.00036 + 0.000312-planet linear fit Fitted GP hyperparameters Source ExTrA 2gp ln ⁡ 𝜎 − 3.77 − 0.19 + 0.24 gp ln ⁡ 𝜌 − 2.73 − 0.17 + 0.262-planet linear fit ExTrA 3gp ln ⁡ 𝜎 − 4.60 − 0.19 + 0.22 gp ln ⁡ 𝜌 − 2.77 − 0.15 + 0.242-planet linear fit Derived parameters Source Companion radius; Rp (R⊕) 1.550 ± 0.064 1.066 ± 0.092 2-planet linear fit Instellation; Sp (S⊕)0.67 − 0.20 + 0.15 0.48 − 0.17 + 0.122-planet linear fit Semi-major axis; a (au) 0.0830 ± 0.0027 0.0986 ± 0.0054 2-planet linear fit Inclination; i (deg)89.856 − 0.017 + 0.018 89.72 − 0.12 + 0.142-planet linear fit Impact parameter; b0.195 − 0.024 + 0.023 0.44 − 0.22 + 0.182-planet linear fit Total transit duration; Ttot (h) 1.980 ± 0.0251.99 − 0.21 + 0.142-planet linear fit Full-transit duration; Tfull (h) 1.741 ± 0.0221.79 − 0.25 + 0.152-planet linear fit Equilibrium temperature⋆; Teq (K) 234 ± 12 215 ± 12 2-planet linear fit Transit depth TESS; δtr; TESS (ppt)4.48 − 0.23 + 0.26 2.03 − 0.27 + 0.312-planet linear fit Transit depth i + z; δtr; i + z (ppt)4.48 − 0.48 + 0.54- 2-planet linear fit Transit depth Sloan-i’; 𝛿 tr ; Sloan − i ′ (ppt)4.60 − 1.20 + 1.50- 2-planet linear fit Transit depth Sloan-r’; 𝛿 tr ; Sloan − r ′ (ppt)4.81 − 0.90 + 0.78- 2-planet linear fit Transit depth ExTra (⁠ 1.2 𝜇 m ⁠); δtr; ExTra(1.2μm) (ppt)4.20 − 0.96 + 1.10- 2-planet linear fitTOI-715 b TIC 271971130.02Fit parametrization and priors Transit depth; Rp/R⋆𝑈 ( 0.01 , 0.1 ) 𝑈 ( 0.01 , 0.1 ) Inverse scaled semimajor axis; (R⋆ + Rp)/a𝑈 ( 0.01 , 0.05 ) 𝑈 ( 0.008 , 0.04 ) Orbital inclination; cos i𝑈 ( 0.000 , 0.04 ) 𝑈 ( 0.000 , 0.04 ) Transit epoch; T0 (BJD)𝑈 ( 2 458 327.3 , 2 458 327.7 ) 𝑈 ( 2 458 342.0 , 2 458 342.4 ) Period; P (d)𝑈 ( 19.0 , 19.4 ) 𝑈 ( 25.4 , 25.8 ) Limb-darkening coefficients TESS u 10.3322 ± 0.0013 TESS q 1 𝑁 ( 0.413 , 0.050 ) TESS u 20.3103 ± 0.0047 TESS q 2 𝑁 ( 0.259 , 0.050 ) I + z u 10.2942 ± 0.0014 I + z q 1 𝑁 ( 0.453 , 0.050 ) I + z u 20.3791 ± 0.0052 I + z q 2 𝑁 ( 0.218 , 0.050 ) Sloan-i’ u 10.3863 ± 0.0018 Sloan-i’ q 1 𝑁 ( 0.532 , 0.050 ) Sloan-i’ u 20.3431 ± 0.0060 Sloan-i’ q 2 𝑁 ( 0.265 , 0.050 ) Sloan-r’ u 10.6408 ± 0.0032 Sloan-r’ q 1 𝑁 ( 0.768 , 0.050 ) Sloan-r’ u 20.2273 ± 0.0079 Sloan-r’ q 2 𝑁 ( 0.218 , 0.050 ) ExTra (1.2 μm) u 10.2058 ± 0.0007 ExTra (1.2μm) q 1 𝑁 ( 0.142 , 0.050 ) ExTra (1.2 μm) u 20.1708 ± 0.0028 ExTra (1.2μm) q 2 𝑁 ( 0.273 , 0.050 ) External priors GP priors Stellar Mass; M⋆ (⁠ M ⊙ ⁠)𝑁 ( 0.225 , 0.012 )Amplitude scale GP ln ⁡ 𝜎 ( flux ) 𝑈 ( − 7 , − 7 ) Stellar Radius; R⋆ (R⊙)𝑁 ( 0.240 , 0.012 )Length scale GP ln ⁡ 𝜌 ( flux ) 𝑈 ( − 7 , 2 ) Stellar Effective Temperature; Teff (K)𝑁 ( 3075 , 75 )Fitted parameters Source Rp/R⋆ 0.0618 ± 0.0017 0.0425 ± 0.0034 2-planet linear fit (R⋆ + Rp)/a 0.01367 ± 0.000170.01129 − 0.00047 + 0.000552-planet linear fit cos i0.00252 − 0.00032 + 0.00030 0.0048 − 0.0024 + 0.00212-planet linear fit T0 (BJD)2 459 002.63051 − 0.00074 + 0.00070 2 459 007.9879 − 0.0041 + 0.00452-planet linear fit P (d)19.288004 − 0.000024 + 0.000027 25.60712 − 0.00036 + 0.000312-planet linear fit Fitted GP hyperparameters Source ExTrA 2gp ln ⁡ 𝜎 − 3.77 − 0.19 + 0.24 gp ln ⁡ 𝜌 − 2.73 − 0.17 + 0.262-planet linear fit ExTrA 3gp ln ⁡ 𝜎 − 4.60 − 0.19 + 0.22 gp ln ⁡ 𝜌 − 2.77 − 0.15 + 0.242-planet linear fit Derived parameters Source Companion radius; Rp (R⊕) 1.550 ± 0.064 1.066 ± 0.092 2-planet linear fit Instellation; Sp (S⊕)0.67 − 0.20 + 0.15 0.48 − 0.17 + 0.122-planet linear fit Semi-major axis; a (au) 0.0830 ± 0.0027 0.0986 ± 0.0054 2-planet linear fit Inclination; i (deg)89.856 − 0.017 + 0.018 89.72 − 0.12 + 0.142-planet linear fit Impact parameter; b0.195 − 0.024 + 0.023 0.44 − 0.22 + 0.182-planet linear fit Total transit duration; Ttot (h) 1.980 ± 0.0251.99 − 0.21 + 0.142-planet linear fit Full-transit duration; Tfull (h) 1.741 ± 0.0221.79 − 0.25 + 0.152-planet linear fit Equilibrium temperature⋆; Teq (K) 234 ± 12 215 ± 12 2-planet linear fit Transit depth TESS; δtr; TESS (ppt)4.48 − 0.23 + 0.26 2.03 − 0.27 + 0.312-planet linear fit Transit depth i + z; δtr; i + z (ppt)4.48 − 0.48 + 0.54- 2-planet linear fit Transit depth Sloan-i’; 𝛿 tr ; Sloan − i ′ (ppt)4.60 − 1.20 + 1.50- 2-planet linear fit Transit depth Sloan-r’; 𝛿 tr ; Sloan − r ′ (ppt)4.81 − 0.90 + 0.78- 2-planet linear fit Transit depth ExTra (⁠ 1.2 𝜇 m ⁠); δtr; ExTra(1.2μm) (ppt)4.20 − 0.96 + 1.10- 2-planet linear fit Open in new tab Table 3. Priors used in our fit, along with fitted and derived parameters. Uniform priors are indicated as 𝑈 ( l o w e r b o u n d , u p p e r b o u n d ) and normal priors are indicated as 𝑁 ( m e a n , s t a n d a r d d e v i a t i o n ) ⁠. ⋆Equillibrium temperature is calculated assuming an albedo of 0.3 and emissivity of 1.TOI-715 b TIC 271971130.02Fit parametrization and priors Transit depth; Rp/R⋆𝑈 ( 0.01 , 0.1 ) 𝑈 ( 0.01 , 0.1 ) Inverse scaled semimajor axis; (R⋆ + Rp)/a𝑈 ( 0.01 , 0.05 ) 𝑈 ( 0.008 , 0.04 ) Orbital inclination; cos i𝑈 ( 0.000 , 0.04 ) 𝑈 ( 0.000 , 0.04 ) Transit epoch; T0 (BJD)𝑈 ( 2 458 327.3 , 2 458 327.7 ) 𝑈 ( 2 458 342.0 , 2 458 342.4 ) Period; P (d)𝑈 ( 19.0 , 19.4 ) 𝑈 ( 25.4 , 25.8 ) Limb-darkening coefficients TESS u 10.3322 ± 0.0013 TESS q 1 𝑁 ( 0.413 , 0.050 ) TESS u 20.3103 ± 0.0047 TESS q 2 𝑁 ( 0.259 , 0.050 ) I + z u 10.2942 ± 0.0014 I + z q 1 𝑁 ( 0.453 , 0.050 ) I + z u 20.3791 ± 0.0052 I + z q 2 𝑁 ( 0.218 , 0.050 ) Sloan-i’ u 10.3863 ± 0.0018 Sloan-i’ q 1 𝑁 ( 0.532 , 0.050 ) Sloan-i’ u 20.3431 ± 0.0060 Sloan-i’ q 2 𝑁 ( 0.265 , 0.050 ) Sloan-r’ u 10.6408 ± 0.0032 Sloan-r’ q 1 𝑁 ( 0.768 , 0.050 ) Sloan-r’ u 20.2273 ± 0.0079 Sloan-r’ q 2 𝑁 ( 0.218 , 0.050 ) ExTra (1.2 μm) u 10.2058 ± 0.0007 ExTra (1.2μm) q 1 𝑁 ( 0.142 , 0.050 ) ExTra (1.2 μm) u 20.1708 ± 0.0028 ExTra (1.2μm) q 2 𝑁 ( 0.273 , 0.050 ) External priors GP priors Stellar Mass; M⋆ (⁠ M ⊙ ⁠)𝑁 ( 0.225 , 0.012 )Amplitude scale GP ln ⁡ 𝜎 ( flux ) 𝑈 ( − 7 , − 7 ) Stellar Radius; R⋆ (R⊙)𝑁 ( 0.240 , 0.012 )Length scale GP ln ⁡ 𝜌 ( flux ) 𝑈 ( − 7 , 2 ) Stellar Effective Temperature; Teff (K)𝑁 ( 3075 , 75 )Fitted parameters Source Rp/R⋆ 0.0618 ± 0.0017 0.0425 ± 0.0034 2-planet linear fit (R⋆ + Rp)/a 0.01367 ± 0.000170.01129 − 0.00047 + 0.000552-planet linear fit cos i0.00252 − 0.00032 + 0.00030 0.0048 − 0.0024 + 0.00212-planet linear fit T0 (BJD)2 459 002.63051 − 0.00074 + 0.00070 2 459 007.9879 − 0.0041 + 0.00452-planet linear fit P (d)19.288004 − 0.000024 + 0.000027 25.60712 − 0.00036 + 0.000312-planet linear fit Fitted GP hyperparameters Source ExTrA 2gp ln ⁡ 𝜎 − 3.77 − 0.19 + 0.24 gp ln ⁡ 𝜌 − 2.73 − 0.17 + 0.262-planet linear fit ExTrA 3gp ln ⁡ 𝜎 − 4.60 − 0.19 + 0.22 gp ln ⁡ 𝜌 − 2.77 − 0.15 + 0.242-planet linear fit Derived parameters Source Companion radius; Rp (R⊕) 1.550 ± 0.064 1.066 ± 0.092 2-planet linear fit Instellation; Sp (S⊕)0.67 − 0.20 + 0.15 0.48 − 0.17 + 0.122-planet linear fit Semi-major axis; a (au) 0.0830 ± 0.0027 0.0986 ± 0.0054 2-planet linear fit Inclination; i (deg)89.856 − 0.017 + 0.018 89.72 − 0.12 + 0.142-planet linear fit Impact parameter; b0.195 − 0.024 + 0.023 0.44 − 0.22 + 0.182-planet linear fit Total transit duration; Ttot (h) 1.980 ± 0.0251.99 − 0.21 + 0.142-planet linear fit Full-transit duration; Tfull (h) 1.741 ± 0.0221.79 − 0.25 + 0.152-planet linear fit Equilibrium temperature⋆; Teq (K) 234 ± 12 215 ± 12 2-planet linear fit Transit depth TESS; δtr; TESS (ppt)4.48 − 0.23 + 0.26 2.03 − 0.27 + 0.312-planet linear fit Transit depth i + z; δtr; i + z (ppt)4.48 − 0.48 + 0.54- 2-planet linear fit Transit depth Sloan-i’; 𝛿 tr ; Sloan − i ′ (ppt)4.60 − 1.20 + 1.50- 2-planet linear fit Transit depth Sloan-r’; 𝛿 tr ; Sloan − r ′ (ppt)4.81 − 0.90 + 0.78- 2-planet linear fit Transit depth ExTra (⁠ 1.2 𝜇 m ⁠); δtr; ExTra(1.2μm) (ppt)4.20 − 0.96 + 1.10- 2-planet linear fitTOI-715 b TIC 271971130.02Fit parametrization and priors Transit depth; Rp/R⋆𝑈 ( 0.01 , 0.1 ) 𝑈 ( 0.01 , 0.1 ) Inverse scaled semimajor axis; (R⋆ + Rp)/a𝑈 ( 0.01 , 0.05 ) 𝑈 ( 0.008 , 0.04 ) Orbital inclination; cos i𝑈 ( 0.000 , 0.04 ) 𝑈 ( 0.000 , 0.04 ) Transit epoch; T0 (BJD)𝑈 ( 2 458 327.3 , 2 458 327.7 ) 𝑈 ( 2 458 342.0 , 2 458 342.4 ) Period; P (d)𝑈 ( 19.0 , 19.4 ) 𝑈 ( 25.4 , 25.8 ) Limb-darkening coefficients TESS u 10.3322 ± 0.0013 TESS q 1 𝑁 ( 0.413 , 0.050 ) TESS u 20.3103 ± 0.0047 TESS q 2 𝑁 ( 0.259 , 0.050 ) I + z u 10.2942 ± 0.0014 I + z q 1 𝑁 ( 0.453 , 0.050 ) I + z u 20.3791 ± 0.0052 I + z q 2 𝑁 ( 0.218 , 0.050 ) Sloan-i’ u 10.3863 ± 0.0018 Sloan-i’ q 1 𝑁 ( 0.532 , 0.050 ) Sloan-i’ u 20.3431 ± 0.0060 Sloan-i’ q 2 𝑁 ( 0.265 , 0.050 ) Sloan-r’ u 10.6408 ± 0.0032 Sloan-r’ q 1 𝑁 ( 0.768 , 0.050 ) Sloan-r’ u 20.2273 ± 0.0079 Sloan-r’ q 2 𝑁 ( 0.218 , 0.050 ) ExTra (1.2 μm) u 10.2058 ± 0.0007 ExTra (1.2μm) q 1 𝑁 ( 0.142 , 0.050 ) ExTra (1.2 μm) u 20.1708 ± 0.0028 ExTra (1.2μm) q 2 𝑁 ( 0.273 , 0.050 ) External priors GP priors Stellar Mass; M⋆ (⁠ M ⊙ ⁠)𝑁 ( 0.225 , 0.012 )Amplitude scale GP ln ⁡ 𝜎 ( flux ) 𝑈 ( − 7 , − 7 ) Stellar Radius; R⋆ (R⊙)𝑁 ( 0.240 , 0.012 )Length scale GP ln ⁡ 𝜌 ( flux ) 𝑈 ( − 7 , 2 ) Stellar Effective Temperature; Teff (K)𝑁 ( 3075 , 75 )Fitted parameters Source Rp/R⋆ 0.0618 ± 0.0017 0.0425 ± 0.0034 2-planet linear fit (R⋆ + Rp)/a 0.01367 ± 0.000170.01129 − 0.00047 + 0.000552-planet linear fit cos i0.00252 − 0.00032 + 0.00030 0.0048 − 0.0024 + 0.00212-planet linear fit T0 (BJD)2 459 002.63051 − 0.00074 + 0.00070 2 459 007.9879 − 0.0041 + 0.00452-planet linear fit P (d)19.288004 − 0.000024 + 0.000027 25.60712 − 0.00036 + 0.000312-planet linear fit Fitted GP hyperparameters Source ExTrA 2gp ln ⁡ 𝜎 − 3.77 − 0.19 + 0.24 gp ln ⁡ 𝜌 − 2.73 − 0.17 + 0.262-planet linear fit ExTrA 3gp ln ⁡ 𝜎 − 4.60 − 0.19 + 0.22 gp ln ⁡ 𝜌 − 2.77 − 0.15 + 0.242-planet linear fit Derived parameters Source Companion radius; Rp (R⊕) 1.550 ± 0.064 1.066 ± 0.092 2-planet linear fit Instellation; Sp (S⊕)0.67 − 0.20 + 0.15 0.48 − 0.17 + 0.122-planet linear fit Semi-major axis; a (au) 0.0830 ± 0.0027 0.0986 ± 0.0054 2-planet linear fit Inclination; i (deg)89.856 − 0.017 + 0.018 89.72 − 0.12 + 0.142-planet linear fit Impact parameter; b0.195 − 0.024 + 0.023 0.44 − 0.22 + 0.182-planet linear fit Total transit duration; Ttot (h) 1.980 ± 0.0251.99 − 0.21 + 0.142-planet linear fit Full-transit duration; Tfull (h) 1.741 ± 0.0221.79 − 0.25 + 0.152-planet linear fit Equilibrium temperature⋆; Teq (K) 234 ± 12 215 ± 12 2-planet linear fit Transit depth TESS; δtr; TESS (ppt)4.48 − 0.23 + 0.26 2.03 − 0.27 + 0.312-planet linear fit Transit depth i + z; δtr; i + z (ppt)4.48 − 0.48 + 0.54- 2-planet linear fit Transit depth Sloan-i’; 𝛿 tr ; Sloan − i ′ (ppt)4.60 − 1.20 + 1.50- 2-planet linear fit Transit depth Sloan-r’; 𝛿 tr ; Sloan − r ′ (ppt)4.81 − 0.90 + 0.78- 2-planet linear fit Transit depth ExTra (⁠ 1.2 𝜇 m ⁠); δtr; ExTra(1.2μm) (ppt)4.20 − 0.96 + 1.10- 2-planet linear fit Open in new tab We adopt the signal parameters from Section 3 as uniform priors, and the stellar parameters from Section 2 as normal priors and fit for all planetary parameters (Rp/R⋆, (R⋆ + Rp)/a, cos 𝑖 ⁠, T0, and P). We fit two models in the first instance: one where eccentricity is constrained to zero and another where eccentricity is allowed to vary, parametrized as 𝑒 b cos ⁡ 𝜔 b and 𝑒 b sin ⁡ 𝜔 b (as in Triaud et al. 2011). The observations described in Section 4.3 span several photometric filters from the blue to the near-infrared ends of the spectrum. To allow the colour-dependent transit depth to be a free parameter in our models, we also fit each light curve for a dilution parameter (D) which we give a uniform prior between −1 and 1. We fix the dilution of the TESS observations at 0 as the PDCSAP light curve is already corrected for crowding (Stumpe et al. 2012), and use the transit depth as the reference depth; we then use Allesfitter’s ‘ c o u p l e d _ w i t h ’ functionality to ensure that the dilution and quadratic limb-darkening coefficients are fitted together for all observations taken in the same photometric band. We use the python package PyLDTK (Parviainen & Aigrain 2015) and the PHOENIX stellar atmosphere library (Husser et al. 2013) to calculate quadratic limb-darkening coefficients for each photometric band. These are also adopted as normal priors in our models after we reparameterise them in the Kipping (2013) parametrization by converting from u1, u2 to q1, q2. Finally, due to the complex instrument systematics present in the data obtained with the ExTrA telescopes, we model the correlated ‘red’ noise in these observations using a GP. We select the Matérn 3/2 kernel as implemented by Celerite due to its versatility and its ability to model both long and short term trends. We fit for two GP hyperparameters for each ExTrA telescope: the amplitude scale, σ, and the length scale, ρ. We place wide uniform priors on these terms. For all other telescopes, we make use of a hybrid spline to model the baseline. Allesfitter also fits an ‘error scaling’ term to account for white noise in the data. The initial fits (1-planet circular, and 1-planet eccentric) both confirm that TOI-715 b is a 1.550 ± 0.061 R ⊕ planet with an equilibrium temperature of 234 ± 12 K ⁠. We find that the eccentric model has slightly higher evidence, with a logged Bayes Factor of ∼6.5 ± 1.1, although the eccentricity derived from this more complex model, 𝑒 = 0.31 − 0.20 + 0.38 ⁠, is consistent with zero at the 2σ level, and we do not consider this a detection of orbital eccentricity. All modelled ground-based transits of TOI-715 b, as well as the phase-folded TESS photometry, can be found in Figs 7 and 8. In Fig. 9 we present the averaged depths, in each photometric band, demonstrating the achromaticity of the observed transits. Figure 7. Open in new tabDownload slide Photometry of TOI-715 b along with best fitting models. Grey points are raw flux and dark pink circles are 15-min binned points. The dark pink lines are 20 fair draws from the posterior transit model. The flux and the transit models have been corrected by subtracting the baseline models. The TESS, ExTrA 2 and ExTrA 3 photometry are phase-folded, while the others are single transits. Figure 8. Open in new tabDownload slide Photometry of TOI-715 b along with best-fitting models. Grey points are raw flux and dark pink circles are 15-min binned points. The dark pink lines are 20 fair draws from the posterior transit model. The flux and the transit models have been corrected by subtracting the baseline models. All transits shown here are single transits. For the TRAPPIST light curves, the number in the figure titles denotes the visit number. These are treated as single transits due to the different exposure times of each visit. Increased scatter during the transit observed by SSO-Io (bottom right panel) cause the middle binned point to have a large errorbar. Figure 9. Open in new tabDownload slide Measured transit depths versus wavelength. The dark grey horizontal line indicates the depth of the TESS transits, while other depths are highlighted with circles for comparison. We remind the reader that in Section 3, we identified a second transiting planet candidate in the TESS data, which we now incorporate into the model, and test this two-planet hypothesis. We test two further models to seek evidence of a second planet in the system, starting by fixing the linear ephemerides of planet b and allowing the mid-time of each individual transit to vary. This model is motivated by the second candidate’s proximity to a 1st order 4:3 mean-motion resonance with the first planet. If the second candidate is real, we might expect for these planets to experience mutual gravitational interactions exciting transit timing variations (TTVs). We place a wide uniform prior on each transit mid-time, corresponding to the linear predicted mid-time ± 60 min. We find that due to the low SNR of individual transits in the TESS data, the transit mid-times are not well constrained for events observed only by TESS. However, transits observed simultaneously by several ground-based observatories have significantly less scatter and smaller error bars. In these events, we see evidence suggesting TTVs of up to 5 min, although the Bayesian evidence for this model is lower than for the two linear models tested. Models with linear ephemerides are preferred, with Bayes Factors of ∼9.5 and ∼16 for the circular and eccentric models respectively. We note that we did not test a fixed linear ephemerides fit with a 2-planet model as the individual transits in TESS for the second candidate have even lower SNR and we have no ground-based transits at the time of writing. The final model we test is a 2-planet circular Keplerian model. We fit for all the same parameters as before, but now additionally include the transit parameters for the second candidate. The phase-folded TESS photometry for the second candidate is presented in Fig. 10; the fitted and derived parameters for planet b resulting from the 2-planet model are consistent with those emerging from all previous models. Also noteworthy is that the host density as calculated by allesfitter following Seager & Mallén-Ornelas (2003) with the transit parameters of the second candidate is 22.7 ± 3.0 g c m − 3 ⁠, which is within 1σ of the stellar density prior of 23.1 ± 3.2 g c m − 3 ⁠, suggesting that both signals are produced over the same star. The results of this fit indicate that if TIC 271971130.02 is confirmed, it is likely a 1.066 ± 0.092 R ⊕ planet with an orbital period of 25.60712 − 0.00036 + 0.00031 and an equilibrium temperature of 215 ± 12 K ⁠. Figure 10. Open in new tabDownload slide Phase folded TESS photometry of TIC 271971130.02. Grey points are raw flux and green circles are 15-min binned points. The green lines are 20 fair draws from the posterior transit model. The flux and the transit models have been corrected by subtracting the baseline models. All fitted and derived parameters from the linear 2-planet circular model are presented in Table 3. 6 DISCUSSION TOI-715 is host to at least one planet (TOI-715 b), with 𝑅 b = 1.550 ± 0.064 R ⊕ ⁠, receiving an installation 𝑆 b = 0.67 − 0.20 + 0.15 S ⊕ ⁠, which places it within the ‘conservative habitable zone’ defined by Kopparapu et al. (2013) as the circumstellar region where a rocky planet receives and installation flux of 0.42 − 0.842 𝑆 ⊕ ⁠. Our model suggests that there is also a possibly second, smaller planet in the system, with size 𝑅 02 = 1.066 ± 0.092 R ⊕ ⁠, just within the outer edge of the host’s habitable zone, at an installation 𝑆 02 = 0.48 − 0.17 + 0.12 S ⊕ ⁠. Expectation of planet yield by TESS prior to launch showed TESS would detect of order 70 ± 9 Earth-sized planets (⁠ < 1.25 R ⊕ ⁠) and 14 ± 4 conservative ‘habitable zone’ planets < 2 R ⊕ (as in Kopparapu et al. 2013), but that TESS would identify of order 0 Earth-sized (⁠ < 1.25 R ⊕ ⁠) ‘conservative habitable zone planets’ (Sullivan et al. 2015). More recent yield estimates (post-launch) by Kunimoto et al. (2022) appear more pessimistic, however, finding only 5 ± 2 ‘conservative habitable zone planets’ < 2 R ⊕ would be detected within the primary mission and first extended mission of TESS, a factor nearly three times lower than Sullivan et al. (2015). Should the second planet candidate, TIC 271971130.02 be fully confirmed, its existence would thus represent an unexpectedly important discovery made possible by the TESS mission, with the contributions of many ground-based facilities. In this section, we contextualize the system by examining other factors required for habitability, and assessing its suitability for further in-depth characterization. We begin by revisiting the radius valley to understand what can be learned about the mass of TOI-715 b from current mass–radius relations. We then explore the prospects for a precise mass measurement of this planet with current spectroscopic instrumentation, followed by a discussion of the potential for atmospheric characterization with HST and JWST. 6.1 TOI-715 b and the radius valley The radius valley, as identified in the Kepler sample by Fulton et al. (2017), shows a lack of close-in (⁠ 𝑃 < 100 d ⁠) planets with sizes between 1.5 − 2 R ⊕ ⁠; TOI-715 b’s 1.55 R ⊕ appears just inside this underpopulated region of parameter space. The importance of the radius valley lies in its potential to teach us about planetary formation and post-formation evolution (e.g. Owen & Wu 2017; Ginzburg et al. 2018; Venturini et al. 2020), and hence planets inside this gap are crucial in furthering our understanding of the factors that sculpt it. The location and slope of the radius valley has been shown to depend on the properties of the host star (e.g. Fulton & Petigura 2018; Gupta & Schlichting 2019; Berger et al. 2020), and Cloutier & Menou (2020) and Van Eylen et al. (2021) recently re-examined this for low-mass stars. Cloutier & Menou (2020) looked at the small, close-in planet distribution around stars cooler than 4700 K and found that the slope is opposite in sign compared with FGK stars, and also that the peaks of the planet radius distributions are shifted toward smaller planets. According to this, the center of the radius valley also shifts towards smaller radii. The sample examined in this study contained planets discovered by Kepler and K2, and was corrected for completeness. This work states that planets falling between their measured radius valley and the one measured by Martinez et al. (2019) for Sun-like stars are so-called ‘keystone planets’, crucial for further understanding the radius valley around low-mass stars. By this metric TOI-715 b’s radius of 1.55 R ⊕ is just below the boundary of 1.66 R ⊕ for a period of 19.288 d, falling within the super-Earth category. Van Eylen et al. (2021), however, used a significantly smaller sample, restricted to confirmed and well-characterized planets, with masses and radii measured to at least 20 per cent precision, orbiting stars cooler than 4000 K. They found a slope in radius-period space opposite in sign compared with Cloutier & Menou (2020), and therefore consistent in sign with FGK stars as measured by, for instance, Martinez et al. (2019). They do, however, find that the valley shifts towards smaller radii for later spectral types. Contrary to the work of Cloutier & Menou (2020), the functional form of the radius valley derived in this study places TOI-715 b above the radius valley and with the population of sub-Neptunes. In Fig. 11 we present the current sample of exoplanets orbiting stars cooler than 𝑇 eff = 4000 K with planetary radii measured to better than 10 per cent precision15 in radius-period space. We highlight the positions of the radius valleys as measured by Martinez et al. (2019); Cloutier & Menou (2020) and Van Eylen et al. (2021) and the positions of TOI-715 b and TIC 271971130.02. In this sample, a clear radius bimodality for planets orbiting M dwarfs is not visible. What is evident is that precise characterization of planets such as TOI-715 b that fall between the various definitions of the low-mass star radius valley is essential to understand whether or not this bimodality will eventually be borne out by the data. Figure 11. Open in new tabDownload slide Plot of the current sample of planets orbiting hosts cooler than 4000 K in radius-period space. The grey circles are confirmed planets with radii measured to better than 10 per cent precision, and the dark pink and green circles are TOI-715 b and TIC 271971130.02, respectively. We note that some errorbars are smaller than the markers. The blue line indicates the measured radius valley according to Van Eylen et al. (2021) for hosts cooler than 4000 K. The black dashed line is the low-mass star radius valley as measured by Cloutier & Menou (2020) for hosts cooler than 4700 K, while the dotted black line indicated the radius valley for Sun-like stars as measured by Martinez et al. (2019). The yellow shaded area indicates where planets referred to as ‘keystone planets’ can be found. The histogram in the right-hand panel does not include TOI-715 b or TIC 271971130.02. 6.2 TOI-715 b and the density gap Luque & Pallé (2022) recently demonstrated using a sample of planets with precisely measured radii (to at least <8 per cent) and masses (to at least <25 per cent) that small planets orbiting low-mass stars in fact likely have a density gap rather than a radius valley. The results of that study indicate that small planets come in three flavours: rocky, water-worlds and gassy. For planets smaller than 2 R ⊕ they find a clear bi-modality in density which allow us to estimate the mass of TOI-715 b in both the rocky (⁠ ∼ 7 M ⊕ ⁠) and water-world (⁠ ∼ 2 M ⊕ ⁠) scenarios; both different from the Chen & Kipping (2017) estimate of ∼ 3.5 M ⊕ and the rocky and volatile-rich predictions from Otegi, Bouchy & Helled (2020) (⁠ ∼ 4.1 M ⊕ and ∼ 3.5 M ⊕ respectively). While this result is very promising for our understanding of the composition of small planets around low-mass stars, their sample contained only 34 planets; therefore continued systematic characterization of these systems with consistent sample selection criteria remains crucial to understand this distribution and ultimately the composition of planets around M dwarfs. 6.3 Prospects for a mass measurement of TOI-715 b Given the faintness of TOI-715, the only southern hemisphere instrument currently capable of the precision required to measure the mass of planet b is ESPRESSO at the VLT (Very Large Telescope Pepe et al. 2010). If the planet is rocky and has a mass of ∼ 7 M ⊕ ⁠, then we calculate a predicted RV semi-amplitude of 4.5 m s − 1 ⁠. From ESPRESSO’s exposure time calculator (ETC) we find that we could achieve a precision of 7.4 m s − 1 per measurement, meaning that for a 25 per cent precision on the mass of the planet, 44 spectra need to be collected, with 1800 s exposures and SNR ∼ 14. For this, we assume that TOI-715 is a relatively slow rotator (⁠ v s i n i ≲ 2 k m s − 1 ⁠) which is not unusual for old M dwarfs that do not show significant stellar activity (Moutou et al. 2017; Reiners et al. 2022). However the ETC is usually pessimistic. Based on observed radial-velocity precision of mid-M type planet-host stars obtained with ESPRESSO we expect a slightly better performance for this instrument. Observations of Proxima Cen (M5V) resulted in a mean precision of 0.6 m s − 1 (Suárez Mascareño et al. 2020), which is about 40 per cent better than the ETC predicts. A similar case is LHS 1140 (M3V), for which a mean precision of 0.8 m s − 1 has been achieved (Lillo-Box et al. 2020), which is a precision twice better than expected. Empirical results16 for M dwarfs using the MAROON-X spectrograph at Gemini North (Seifahrt et al. 2020) found a more than 40 per cent increase in precision between M0 and M6 dwarf stars. The ESPRESSO ETC assumes an M2 spectral type to predict the RV precision. Thus, this apparent increase in RV precision for mid-M dwarfs, compared to early M dwarfs can explain the observed difference in performance. Since TOI-715 is an M4 dwarf, we can conservatively expect a 20 per cent better RV precision compared to the ETC (thus a precision of ∼ 5.9 m s − 1 ⁠). This means a 25 per cent precision on the mass of the planet can be reached with only ∼ 30 spectra (⁠ 1800 s exposures; SNR ∼ 14). This amounts to 15 h of telescope time, which is not unrealistic for a planet of this importance. If however, TOI-715 b is in fact a ‘water world’, then a mass of ∼ 2 M ⊕ would produce an RV semi-amplitude of just 1.3 m s − 1 ⁠. In this case 150 h of telescope time are needed to constrain the mass with 25 per cent precision. Should the density bi-modality for small planets around M dwarfs suggested by Luque & Pallé (2022) be confirmed, a non-detection might enable us to infer the planet’s mass. In Section 5, we found that there were small hints of transit timing variations (TTVs) in the ground-based transits of TOI-715 b. If the second planet is confirmed and the orbits are commensurate, the masses of both planets could be suitable for characterization using dynamical modelling. Given the low SNR of individual transits and the long orbital periods, such a campaign would be well suited to ASTEP (Antarctic Search for Transiting Exoplanets, Daban et al. 2010; Schmider et al. 2022), given its convenient Antarctic location (Dransfield et al. 2022b). 6.4 Prospects for detailed atmospheric characterization of TOI-715 b The Transmission Spectroscopy Metric (TSM) as established by Kempton et al. (2018) is often used to quantify the suitability of planets for atmospheric characterization via transmission spectroscopy. We calculate the TSM for TOI-715 b in both mass limits17 and for TIC 271971130.02, as well as the published sample of planets with 𝑅 𝑝 < 2 R ⊕ and 𝑆 𝑝 < 1.6 S ⊕ 18 The comparison sample contained 34 planets initially; we then removed the seven planets discovered via radial velocity as these are not known to transit. In Fig. 12, we present this sample of small habitable zone and temperate exoplanets, highlighting the location of the conservative habitable zone (Kopparapu et al. 2013). We also highlight the positions of the five TRAPPIST-1 planets that fall in this parameter space (d-h) (Gillon et al. 2017), the recently discovered LP 890–9 c (SPECULOOS-2 c/TOI-4306 c; Delrez et al. 2022), K2-3 d (Crossfield et al. 2015), and the well characterized mini-Neptune, LHS 1140 b (Dittmann et al. 2017). Figure 12. Open in new tabDownload slide Transmission spectroscopy metric verus installation for transiting planets orbiting hosts cooler than 4000 K. The blue shaded area shows the conservative habitable zone as defined by Kopparapu et al. (2013). The two dark pink circles highlight the positions of TOI-715 b in the rocky and water world scenarios, while the green circle indicates the position of TIC 271971130.02. We highlight in yellow the positions of the seven planets of TRAPPIST-1. The sizes of the circles are scaled with planetary radius. In the case of a ‘water world’ composition, TOI-715 b could have a transmission signal comparable to LHS-1140 b if it does indeed have detectable atmospheric features. In the higher mass limit we find that TOI-715 b has a TSM of approximately 1.9, below the suggested cutoff of 12 for follow-up of terrestrial planets. We use Tierra (Niraula et al. 2022) to calculate simulated transmission spectra for TOI-715 b in the ‘rocky’ (⁠ ∼ 7 M ⊕ ⁠) and ‘water-world’ (⁠ ∼ 2 M ⊕ ⁠) mass scenarios in the case of a primordial hydrogen/helium-dominated atmosphere. We then use Pandexo (Batalha et al. 2017) to simulate JWST observations in both cases to assess the detectability of atmospheric features. We find that in the low-mass case atmospheric features could be detectable with a single JWST transit if the atmosphere is cloudless. In the higher mass case, we find that extracting meaningful features would require at least five transits, also in the cloud-free case. A secondary atmosphere with higher mean molecular mass could suppress the scale height by more than an order of magnitude, in turn making it challenging to detect atmospheric features of these planets (de Wit et al. 2016). In both mass cases, the carbon dioxide feature centered on 4.5 𝜇 m ⁠, the methane feature at 3.3 𝜇 m and multiple water bands are some of the most accessible atmospheric features with the NIRSpec PRISM, the most suitable instrument for JWST follow-up given the similar brightness of TOI-715 compared to TRAPPIST-1. Detection of these features will constrain the metallicity of the putative planetary atmospheres, providing first hints of the carbon-chemistry of the atmosphere itself (Madhusudhan 2012), and insights into their formation history (Öberg, Murray-Clay & Bergin 2011). 6.5 Flares and habitability Both TOI-715 b and the candidate TIC 271971130.02 lie in an interesting location of the parameter space with regard to insolation, radius, and potential rocky composition, naturally posing questions of their habitability. Many recent prebiotic chemistry and astrobiology studies investigated how life may have originated on Earth and other planets (see e.g. Patel et al. 2015; Airapetian et al. 2016; Xu et al. 2018 and reviews by Sutherland 2017; Kitadai & Maruyama 2018). The first processes leading from inorganic compounds towards RNA precursors likely require a liquid solution (e.g. liquid surface water) and an energy source. For the latter, the host star’s flaring may provide the necessary UV radiation (in the 200–280 nm range) to trigger these early steps (e.g. Rimmer et al. 2018; Todd et al. 2018; Rimmer et al. 2021). On the other hand, stellar activity can also pose a significant danger to exoplanets. Strong stellar winds and XUV outbursts can contribute to atmospheric loss (e.g. Atri & Mogan 2020). Even if the atmosphere prevails, coronal mass ejections (charged particle streams) may interact with and dissociate atmospheric ozone (e.g. Tilley et al. 2019). Without the major atmospheric absorber of harmful UV radiation, the next flares might sterilize existing surface biology. We investigate TOI-715’s 24 sectors of TESS data for signs of stellar flaring. We find an average rate of 7 flares per 100 d of observations, with a maximum flare amplitude of 5 per cent in relative flux over nearly 2 yr of data. This analysis was performed using the stella neural network (Feinstein et al. 2020) and as part of the TESS flare catalog (Günther et al., in preparation), building on (Günther et al. 2020). We also compare this flare frequency distribution with the theoretical potential for ozone sterilization (Tilley et al. 2019) and laboratory thresholds for prebiotic chemistry (Rimmer et al. 2018). We find that the flaring is likely too rare and not energetic enough to influence either of these effects. As TOI-715 is an older star (6.6 − 2.2 + 3.2 Gyr), its flaring is likely not as prominent as in younger years, when its planets were forming and prebiotic chemistry steps might have been triggered. Demographic studies of young (<100 Myr) and older M dwarfs show that flaring activity decreases with stellar age, likely due to spin-down and a decreasing stellar dynamo (e.g. Feinstein et al. 2020; Günther et al. 2020). Thus, one could carefully speculate: if stronger flaring or other processes (volcanoes, impacts, or lightning) had led to astrobiology on TOI-715 b or TIC 271971130.02 several Gyr ago, there might be a chance it could still exist (pending, of course, all other criteria such as atmospheric composition, liquid water, etc. are fulfilled). 7 CONCLUSIONS In this work, we have presented the discovery, validation and characterization of TOI-715 b, a 𝑅 b = 1.550 ± 0.064 R ⊕ habitable zone planet orbiting an M4 star with a period of 19.288004 − 0.000024 + 0.000027 d. We also demonstrated that there is possibly a second, smaller planet with radius 𝑅 02 = 1.066 ± 0.092 R ⊕ at a period of 25.60712 − 0.00036 + 0.00031 d, placing it just inside the outer edge of the circumstellar habitable zone. This system represents the first TESS discovery to fall within this most conservative and widely applicable ‘habitable zone’. We have also demonstrated that TOI-715 b is amenable to further characterization with precise radial velocities and transmission spectroscopy; detailed follow-up of this planet is crucial to further our understanding of the formation and evolution of small, close-in planets. Given its location in radius-installation space, TOI-715 b could also help us understand the characteristics of the M-dwarf radius valley. Confirmation of the existence of the TIC 271971130.02 in the coming months will also be crucial in planning further characterization of planet b, as disentangling the two planets in a putative radial velocity campaign could prove challenging if the orbit of a second planet is not well known. ACKNOWLEDGEMENTS We thank the reviewer for their comments and feedback as these helped clarify and streamline the manuscript significantly. Funding for the TESS mission is provided by NASA’s Science Mission Directorate. We acknowledge the use of public TESS data from pipelines at the TESS Science Office and at the TESS Science Processing Operations Center. This research has made use of the Exoplanet Follow-up Observation Program website, which is operated by the California Institute of Technology, under contract with the National Aeronautics and Space Administration under the Exoplanet Exploration Program. This paper includes data collected by the TESS mission that are publicly available from the Mikulski Archive for Space Telescopes (MAST). Based on data collected by the SPECULOOS-South Observatory at the ESO Paranal Observatory in Chile.The ULiege’s contribution to SPECULOOS has received funding from the European Research Council under the European Union’s Seventh Framework Programme (FP/2007-2013) (grant agreement no. 336480/SPECULOOS), from the Balzan Prize and Francqui Foundations, from the Belgian Scientific Research Foundation (F.R.S.-FNRS; grant no. T.0109.20), from the University of Liege, and from the ARC grant for Concerted Research Actions financed by the Wallonia–Brussels Federation. This work is supported by a grant from the Simons Foundation (PI Queloz, grant number 327127). This research is in part funded by the European Union’s Horizon 2020 research and innovation programme (grants agreements no. 803193/BEBOP), and from the Science and Technology Facilities Council (STFC; grant no. ST/S00193X/1 and ST/W000385/1). The material is based upon work supported by NASA under award number 80GSFC21M0002 Based on data collected by the TRAPPIST-South telescope at the ESO La Silla Observatory. TRAPPIST is funded by the Belgian Fund for Scientific Research (Fond National de la Recherche Scientifique, FNRS) under the grant FRFC 2.5.594.09.F, with the participation of the Swiss National Science Fundation (SNF). Based on data coll",
    "commentLink": "https://news.ycombinator.com/item?id=39241113",
    "commentBody": "A 1.55 R⊕ habitable-zone planet hosted by TOI-715 (oup.com)215 points by taubek 18 hours agohidepastfavorite106 comments somenameforme 17 hours agoNo idea when NASA put this up [1] but it's awesome. It's the basic data on the system as well as some pretty nicely done visualizations including the ability overlay our solar system for scale. Quite interesting actually seeing what a dramatic difference the star type makes on habitable distances. This planet is way closer to its star than Mercury is the Sun, for instance. They could do with a lot more advertising for stuff like this, and not quite so much for dronecopters. [1] - https://exoplanets.nasa.gov/exoplanet-catalog/8921/toi-715-b... reply dr_kiszonka 16 hours agoparentThe retro graphics here are gorgeous: https://exoplanets.nasa.gov/alien-worlds/exoplanet-travel-bu... (Stellar, one could say.) reply dylan604 14 hours agorootparentI think they are out of this world myself. The guided tours are interesting. I've mentioned this before, but I am very jealous of this kind of content available for school kids. We had the slide shows with a tape recording with the unforgettable tone to signal to switch to the next slide. We had crappy VHS copies instead of the 8mm film projector, but I'm not sure it was an improvement. All of this interactive material is just so much more immersive. reply jonhohle 14 hours agorootparentprevWhen Costco still did printing we printed serval of those out as posters and hung them up in the kid’s bathroom. The designs are pretty timeless and are fun to daydream about. reply dylan604 15 hours agoparentprevNASA has provided some very compelling visualizations of data over the years. One of my favorites is the meteor showers showing where the debris fields for each one are located within the solar system. You can see the original orbit of the comet that left the debris, where it currently is, when the Earth's orbit crosses the debris field, and just the overall size of the debris so that you can see why some meteor showers are \"better\" than others. There's plenty of others where you can zoom in/out of the solar system to see the modeled view of the Oort cloud and get a sense of scale interactively of Sol's influence. There's also the interactive image viewers of Hubble and JWST data so we can see the differences between visible/IR spectrum. There are plenty of others as well. Oh, and the fact you can access so much of the raw data to make your own visualizations as you like. Overall, I think my tax dollars are performing nicely for my interests. reply echelon 9 hours agorootparent> Overall, I think my tax dollars are performing nicely for my interests. And for the future scientists and engineers who get exposed to this at an early age! reply s1artibartfast 15 hours agoparentprevI dont know the intricacies of stellar physics, but my understand that smaller cooler stars are less stable. Despite an average temperature that is habitable, the variability and EM environment is not suitable for an atmosphere. Edit: 1) proximity also leads these planets to be tidally locked, which some argue is not conducive to habitability, though I don't find that especially compelling. 2) The local EM environment and solar flares would not preclude sub-surface habitability, but that is true for planets outside of what we consider the habitable zone as well. e.g. life on Europa has been hypothesized, but it is FAR outside the suns habitable zone. reply throwup238 16 hours agoparentprevIt looks quite a bit bigger than Earth. I don't think anyone will be leaving that atmosphere with chemical propulsion any time soon... reply mmoskal 12 hours agorootparentI learned something today :) Gravity on surface of this planet will be 3(times heavier than earth)/(1.55(radius)^2), so 1.25g, however escape velocity also depends on planet radius so we get sqrt(1.55 * 1.25) = 1.4 earths escape velocity (so 15.7km/s). Assuming Isp specific impulse of engine of around 300s, let's compute m0/mf (initial mass of rocket / payload) = exp(vEsc/(Isp*g0)) (g0=9.8m/s^2 (on both planets)): For earth, it's 45. For this planet, it's 208. So, the rocket (using our current chemical engine technology) needs 4.6 times more fuel than on earth. Much more difficult, but maybe possible... edit: formatting reply BuyMyBitcoins 10 hours agorootparentWe also have to consider the thickness of the atmosphere. I presume a planet with 1.5x the radius of Earth and 3x the gravity would have an atmosphere that is far more thick and any rocket would have to contend with a significant amount of drag. Plus, if it would take something like a Saturn V just to put the mass of a potato into low orbit I could easily envision an extraterrestrial civilization thinking it wasn’t worth the cost to try. reply deepsun 3 hours agorootparentActually, the atmospheric drag is not a big challenge. Yes, it is a consideration of course, hence the payload fairing etc, but much larger problem going to orbit is getting speed. That's why it doesn't really help much to launch rockets from a balloon -- getting there is not a problem. reply zardo 10 hours agorootparentprev> I presume a planet with 1.5x the radius of Earth and 3x the gravity would have an atmosphere that is far more thick and any rocket would have to contend with a significant amount of drag. I'm sure that's true holding all else the same, but all else isn't the same. Maybe it's airless due to stronger stellar winds. reply callamdelaney 16 hours agorootparentprevJust because it's bigger doesn't mean that the gravity is higher. reply efitz 16 hours agorootparentCorrect, but it’s mass (weight) is 3x higher than earth’s, and since gravity is proportional to mass, you’d likely be much heavier there. If the radius of the planet was the same as earth’s, the gravity would be 3x higher, but if it’s bigger than earth, it might not be as bad. The formula is g= m/r^2 , where m is the mass of the planet and r is the radius of the planet. reply wkat4242 15 hours agorootparentDamn I'm gonna have to get serious about that diet before visiting :'( reply ls612 14 hours agorootparentprevr^2 is around 2.3 for the alleged size so it won't be too big a difference. reply hleszek 15 hours agorootparentprevThe gravity on the surface should be 3,02÷(1,55^(2)) times the gravity on earth. It is equal to ~1.257 so it's about 25% higher. reply samstave 16 hours agorootparentprevYeah, but look at its period: >>>ts mass is 3.02 Earths, it takes 19.3 days to complete one orbit of its star, and is 0.083 AU from its star. Its discovery was announced in 2023 https://i.imgur.com/ghLnFrH.png -- What would gravity be on that thing, it doesnt say what its circadian rhythm is.. so how long is a day, if a \"year\" is 19 earth days (or is the period in 'day' local-solar-system-days?) reply cyberax 4 hours agorootparentIt's probably tidally locked, so the day will be the same as the year. reply zamadatix 15 hours agorootparentprevOrbital period is in Earth days. The same as when we say the orbital period of Venus is 225 days even though a Venetian day is longer than that. It'd be pretty messy to instead define things in the database in object relative terms, particularly when you usually know things like the orbital period with significantly more accuracy than rotational velocity. reply samstave 13 hours agorootparentWell yeah, Venus is in our solar system. Thanks, though - I wasn’t sure if we always use a local reference but then au is as local a unit as possible at these scales. reply dr_kiszonka 16 hours agorootparentprev”Its mass is 3.02 Earths.” reply joombaga 16 hours agorootparentRight, but you also have to account for density when calculating surface gravity. TOI-715 b has a log g value of 5.0 +/- 0.2. Earth is 2.992. reply hypertexthero 13 hours agoparentprevSee also the Jet Propulsion Lab’s gorgeous Visions of the Future posters: https://www.jpl.nasa.gov/galleries/visions-of-the-future reply zamadatix 15 hours agoparentprevThe tool has been around in some form since 2007ish, though the latest update was probably within the last decade. The age probably being why it's not so heavily advertised anymore compared to new activities. I've always been more partial to exoplanet.eu though: https://exoplanet.eu/catalog/toi_715_b--8668/. It's not quite as prettied up but it's usually more practical in presentation. reply delta_p_delta_x 12 hours agorootparentI personally prefer the Caltech Exoplanet Archive, which has downloadable datasets, APIs, and also presents everything in one shot: https://exoplanetarchive.ipac.caltech.edu/ reply sgt101 16 hours agoparentprevI like dronecopters! reply msk-lywenn 17 hours agoparentprev0.083 AU means that its sun would be huge in its sky? But it's also smaller so maybe not... reply p1mrx 16 hours agorootparentIn general, a lower temperature star (3075 K versus the sun at 5778 K) will appear larger in the sky on a habitable planet. reply zamadatix 15 hours agorootparentprevI'm getting about 3x the angular diameter of the Sun from Earth, so not particularly huge. reply mmastrac 17 hours agoparentprevThis was quite a bit more helpful than the paper itself, thanks. reply api 15 hours agoparentprevAt 138 light years any intelligence in this system would start potentially receiving our radio signals in the next two decades. They’d need a massive radio telescope to hear them though since at that distance even the strongest signals would be extremely weak. reply MichaelZuo 16 hours agoparentprevThanks, there's so many resources tucked away in obscure places that there's probably a dozen more cool things like this that never get noticed. reply dvh 16 hours agoprevOver and over and over again planets are reported in habitable zones around flare stars just because they are easy to detect, not because they are actually habitable. reply davedx 14 hours agoparentThe entire notion of a 'habitable zone' is also very over-simplistic. Europa is quite probably 'habitable'. reply 7373737373 14 hours agorootparentI wish habitability metrics would be more holistic, taking into account the star and orbit characteristics, how sun+earthlike they really are reply throwawayqqq11 8 hours agorootparent> orbit characteristics Also, our earth did propably produce complex life so well because of the moon stabilizing earths rotation and allowing more or less stable climate. The mass ratio of moon/earth is comparably larger then other satellite systems, which, encorporated into the model of habitable zones, would make them much less common. Also, jupiter has a significant role in shielding earth from asteriods. Another aspect of stability. I agree, habitable zone is way more than just distance to star or enough gravity for an atmosphere and liquid water. reply moralestapia 12 hours agoparentprev>just because they are easy to detect Well, for some threshold of \"easy\" I guess. reply qayxc 16 hours agoparentprevTrue. It's tiresome that a technical definition is so often misinterpreted as actual habitability. The authors are also overly optimistic when they point out that the M4 class host star is relatively calm due to its age. That fact wouldn't matter much, though, if the first billion years of the star's existence have sterilized the planet and stripped off its atmosphere... reply samstave 15 hours agorootparentMaybe just come up with a better/different label for planets who share a similar ratio of planet-to-star characteristics with Earth, which is a sample size of known_to_support_life=1. \"Orbital Cousins to Earth\" OCEs. Find the # of OCEs, and maybe we have finer granulation, Second Cousins have an atmosphere, Step-sisters have water and an OnlyPlanets account... (which seems to be making the rounds at Nasa) reply weard_beard 12 hours agorootparenthttps://youtu.be/ErpU_tMoV0E reply samstave 9 hours agorootparentHoly crap! I've never watched that show, but thats funny as heck that this was a thing. (and they even say \"I don't have HydroStatic Equillibrium yet...) too funny is the Akashic Zeitgeist... reply blackhaz 11 hours agoprevThe mention of planet's \"installation,\" in the abstract and repeatedly throughout the article, I have found very confusing. The reference to Kopparapu et al. (2013) has no mention of the term whatsoever. I have found a mention of \"instellation\" from one of the authors working with Kopparapu [1]. Instellation, indeed, is, according to Wiki, \"a generalization of insolation to stars other than the sun,\" which makes sense in this context. I wonder if I am missing anything - given it's in MNRAS already - or this is likely an artifact of auto-correction. [1] https://ui.adsabs.harvard.edu/abs/2011AGUFM.P21C1675D/abstra... reply rdslw 27 minutes agoparentI don't have necessary knowledge to explain this, yet while using gpt4 to help me parse the abstract, gpt4 without missing a heartbeat, changed it to \"Insolation refers to the amount of stellar energy a planet receives on its surface. The value \"0.67s\" indicates that TOI-715 b receives 67% of the solar energy Earth receives from the Sun. \" Wiki ref check: https://en.wikipedia.org/wiki/Solar_irradiance reply k__ 17 hours agoprevI always thought it to be funny if we would get faster than light travel in the future and other civilizations are just seeing dinosaurs roaming the earth the moment we visit them. reply 1024core 17 hours agoparentThat would be if they were 70M LY away from us. In order to see dinosaurs from that distance, they would need a telescope wider than our solar system. reply Projectiboga 16 hours agorootparentThe Milkyway is 'only' 100 to 150,000 light years across, in total. reply kilroy123 7 hours agorootparentprevCouldn't you just use a large solar gravitational lensing telescope? reply malfist 17 hours agorootparentprevIf you can make a dyson sphere, a solar system sized telescope doesn't seem out of the question. reply bemmu 16 hours agorootparentYes, once we're commanding autonomous little bots in space, capable of also autonomously creating their own factories, we can build anything on a whim as long as we can design it and have the energy and materials. (I may have played too much Factorio) reply Baeocystin 12 hours agorootparentAll I want is logibots IRL, is that too much to ask reply awb 14 hours agorootparentprevHarvesting that much material might be a challenge. I guess you’d also have to build it in interstellar space to avoid planets and asteroids. reply redcobra762 15 hours agorootparentprevAccording to our current understanding of telescopes, but if FTL is possible, then perhaps we missed something. reply poisonborz 14 hours agorootparentIf FTL is possible, we didn't just missed something, but can throw all of our physics books on fire. reply calamari4065 14 hours agorootparentprevI think at that point, you'd go find a really big black hole to use as a lens instead reply lgkk 16 hours agoparentprevWhat would even be the point of existence anymore once FTL is figured out? Just curious. Seems like it would just remove any grandeur about the universe entirely. reply calamari4065 14 hours agorootparentThe universe is infinite. FTL just makes it slightly less infinite. FTL means we actually get to see the grandeur of the universe instead of hypothesizing mathematical models of it. Without FTL, we'll never leave this star system. The grandeur of the universe will be nothing but our imagination, instead of a real thing you can see with your own eyes. reply jonhohle 14 hours agorootparentI was explaining to my kids yesterday that it took 33 years for Voyager to leave the solar system, the next closes star is 2,000x further than it’s already travelled. That would require infrastructure to support 100 generations of humans, 99 of which would be indentured by their ancestors to a life stuck inside a space-ark. And it would only require one of those generations to fail for the whole endeavor to fail. And there’s nothing there, it would take another 50 generations to get to what is hypothesized to be a habitable planet. Human existence doesn’t scale to inter-star system travel. reply calamari4065 11 hours agorootparentThat's why I choose to believe that FTL will be possible at some point. Otherwise we may as well be the only life in the universe, the rest is dead and pointless. If we can't leave our solar system, our entire universe may as well just be our local system and everything else is just neat wallpaper. My money is on a quantum theory of gravity unlocking the ability to cheaply warp space. reply Jach 4 hours agorootparentThe Voyager is nowhere near max speed, nor is current biological age the final limit on human life span. If you survive another 500 million years or so, you might meet some aliens then. https://grabbyaliens.com/ If FTL were possible, causality breaks too. Plus if there's other life out there who have discovered FTL, we shouldn't even be here. Don't get your hopes up for more than a tiny portion of the universe ever being explorable. reply nrb 12 hours agorootparentprevHow big of a lunatic would our great-great-great grandparents have considered us for telling them that soon the several-month trip across the USA will pretty soon take an afternoon, and going to the moon will take 3 days? Sure, traveling at Voyager’s (impressive, but essentially wagon) speed won’t get it done. But betting against technological advancement has made fools of a vast many. reply calamari4065 11 hours agorootparentGenerally it's taken for granted that the technological problems with generation ships can be solved with sufficient time and resources. We can probably build a metal box that lasts for a thousand years. We can probably design a sustainable closed ecosystem. We could probably build fusion reactors that run on interstellar hydrogen collected with ramscoops. But the real problem with generation ships is not technological. Technology can't solve the fundamental social and psychological problems of locking some humans in a box for a hundred generations. That's the most important problem, and the one that's usually waved away with \"oh you just can't imagine future technology\" reply SoftTalker 6 hours agorootparentYou'd probably need to create a religion for them about some gods/ancients that they are serving in their mission. That seems to be how humans stay focused on long-term social organization across generations. reply gomoboo 3 hours agorootparentSee the Mormon generation ship in The Expanse: https://expanse.fandom.com/wiki/Nauvoo_(Books) They were intending to use the ship to get to the Tau Ceti system: https://exoplanets.nasa.gov/exoplanet-catalog/7179/tau-ceti-... Mormonism seems well-suited for the religion role you mention since they have the concept of a particular planet being close to the residence of their god: https://en.m.wikipedia.org/wiki/Kolob reply konart 1 hour agorootparentprevOr modify them genetically making them biological robots. Or create artificial humanoids similar to Abh: https://seikai.fandom.com/wiki/Abh reply seattle_spring 12 hours agorootparentprevI see someone's read Project Hail Mary reply throwaway11460 10 hours agorootparentprevOn the contrary, nothing would make me excited about existence more than knowing our limits have vastly expanded. Less of unknown, sure. But unimaginably more of grand human civilization, knowledge, diversity... reply AlexAndScripts 14 hours agorootparentprevRemove any grandeur? On the contrary, it would be the beginning of a new era of appreciating its grandeur. reply MattGaiser 15 hours agorootparentprevIt wouldn’t go away. It would just mean we could go see it. reply api 15 hours agorootparentprevDoes the fact that you can drive to a national park make it stop being beautiful? reply FrustratedMonky 15 hours agorootparentprev\"point of existence\" To go see it. Why would the ability to visit other planets, and see them, suddenly negate the point of existence, since we can then go see it. Like there is only a point if things can only be imagined, and not actually seen. That would be like. \"now that cars exists, and we can visit the Grand Canyon, suddenly there is no point of existence\". reply credit_guy 14 hours agoprevIt is 42.46 parsecs away, or about 140 light years. If the Founding Fathers had beamed to those guys the Declaration of Independence, we'd get their congratulations in about 30 years from now. Hopefully. Maybe they decide to side with the King, and they'll send an invasion force to help general Cornwallis. reply chrsw 16 hours agoprevIt doesn't seem like it would take much to make Earth inhabitable for us humans. Like if we wanted to we can do it pretty quickly using primitive methods. It's a very delicate system and we're completely reliant on it. So it is a stretch for me to think there are even a few planets in the galaxy that could host human life as we know it. reply gomoboo 3 hours agoparentDid you mean to say “uninhabitable”? If so, then I agree. Nuclear weapons are the first thing that comes to mind. reply Denvercoder9 16 hours agoparentprev> Like if we wanted to we can do it pretty quickly using primitive methods. It seems that even if we don't want it, we can't help but do it anyway. reply mmastrac 17 hours agoprevThe domain-specific notation in this paper is inscrutable to me. I assume that this means a possible \"Minshara class\" planet (as Gene would say) around a star much different than our own, in that the distance from its sun and insolation received don't rule it out? EDIT: R⊕ appears to be something related to Earth. Perhaps a planetary radius? reply nullhole 17 hours agoparent⊕ is the symbol for earth. R⊕ is earth-radius, as you guessed. It is domain-specific, so it should probably be changed in the title. I will say that it is a very nice symbol - easy to write out by hand, makes for quick note-taking. Same for the symbol used for the sun (which isn't rendering for me, but is a circle with a dot in the middle). https://en.wikipedia.org/wiki/Planet_symbols reply somenameforme 17 hours agoparentprevIt's a planet that has 1.55 Earth's radius orbiting a much cooler star. Planets in the habitable zone are those that are at a distance from their star, such that they have the reasonable potential for liquid water to exist on the planet. reply dwaltrip 17 hours agoparentprev\"Earth radius (denoted as R🜨 or R_E) ...\" https://en.wikipedia.org/wiki/Earth_radius reply chrsw 16 hours agoparentprev\"The domain-specific notation in this paper is inscrutable to me.\" That's almost every paper to me. reply rurban 16 hours agoprev46 parsec is nearby to them? Well, it's relative obviously. That means many, many generations away. reply qayxc 16 hours agoparentWhat's 1500 years (at 0.1c) between friends? reply at_a_remove 16 hours agoprevSo, surface gravity about twenty-five percent higher than that of Earth. We're looking at Antares-level of redness for color. Occupies a large portion of the sky. Good candidate for tidal lock, at which point a lot of the habitability issues become debatable. reply lukan 15 hours agoparent\"Good candidate for tidal lock, at which point a lot of the habitability issues become debatable.\" A tidal locked planet might still allow life in the twilight zones. But since this one is orbiting close to a flare star, life might have other problems there reply qayxc 16 hours agoparentprevThey don't know the mass of the planet yet, so surface gravity is an unknown as of yet. The paper suggests two scenarios: a rocky world (~7 earth masses) or a water world (~2 earth masses). Surface gravity would be 38m/s² for a rocky planet (~390% earth's surface gravity) and 8.2m/s² for a water world (~84% earth's surface gravity). I'm not an astronomer so I don't know if anything in between those values is a realistic scenario, but I wonder how you got your 125% of earth's surface gravity. reply at_a_remove 14 hours agorootparentAnother poster had something else from NASA, suggesting 1.55 Earth radii and 3.02 Earth masses. reply Projectiboga 16 hours agoparentprevI read somewhere, slightly higher than Earth gravity would actually be better for life. So on that metric this is interesting. Magnetic field is likely another need and our single large moon helps a lot here too. reply at_a_remove 16 hours agorootparentI kind of despise these press releases because they almost never attempt to show anything other than what other astronomers might care about. Surface gravity? I just computed that. Color temp? Had to check the ole memory banks. They should be doing stuff like computing how much of the sky this sun occupies, so you could get a sense of it, with the color. Is it tidally locked? Or at least does it fit the criteria? The distance, eccentricity (which here leaves the distance a constant), and temperature of the associated star could be used to give some kind of insolation number, and from there a very casual steady-state blackbody temperature of the planet, which is a start and is better than nothing. At that point you can start looking up what gases would stay and which would go as kind of a maximum before you started thinking about stellar winds and the like. reply anticensor 16 hours agoparentprevIn such a planet, THE LINE becomes a necessity rather than far-fetched idea. reply xyst 14 hours agoprevThis is an interesting read but I can’t help to wonder… We can hardly take care of this planet. Why are we bothering with finding another planet that is habitable if we are only going to destroy it as well? reply konart 1 hour agoparent>Why are we bothering with finding This is like asking why should one try getting a better job while some people live in hunger. Civilization has to work in many directions in order to progress. While some people are trying to find a way to deal with local issues (climate change, production, goverenance etc.) other people search a new potential home or a colony. reply EasyMark 9 hours agoparentprevBecause some people's glass is half full and aren't waiting for the death of the human race? We can study exoplanets and work on saving our own at the same time; which we have the capacity for right now with nuclear fission power. the powers that be haven't been forced to be responsible yet. they eventually will have to deal with it long before humans go extinct. reply wellthisisgreat 4 hours agoprevApologies for an extremely layman question, but what relatively accessible book could one read to understand all the formulas and acronyms in this paragraph? I mean to take them in as tools, if that makes sense? In this context, we report on the discovery and validation of TOI-715 b, a planet orbiting its nearby (42 pc) M4 host (TOI-715/TIC 271971130) with a period d. TOI-715 b was first identified by TESS and validated using ground-based photometry, high-resolution imaging and statistical validation. The planet’s orbital period combined with the stellar effective temperature give this planet an installation , placing it within the most conservative definitions of the habitable zone for rocky planets. reply rglullis 17 hours agoprev0.0 eccentricity means that it has no seasonal variation? reply mitthrowaway2 17 hours agoparentIt means its orbit is a circle. Seasonal variation is due to the tilt of the planet's axis of rotation relative to its orbit, not orbital eccentricity. reply wkat4242 15 hours agorootparentWell on our planet that's the case. Another planet might well have seasons due to orbital eccentricity. Though it would have to be pretty serious eccentricity I think. If they would have no tilt then the seasons would be the same all over the planet unlike ours where they're opposite in each hemisphere. reply rglullis 16 hours agorootparentprevI know what causes seasonal variations, but for some reason I thought 0.0 eccentricity meant no tilt. Thanks reply MadnessASAP 17 hours agoparentprevIt means the orbit is perfectly circular. Earth's seasons are driven almost exclusively by our axial tilt*, not our orbital eccentricity. Our eccentricity also being 0.0(167). * Astrometeorologists feel free to correct me on this. reply fbdab103 16 hours agorootparentWikipedia says this The relative increase in solar irradiation at closest approach to the Sun (perihelion) compared to the irradiation at the furthest distance (aphelion) is slightly larger than four times the eccentricity. For Earth's current orbital eccentricity, incoming solar radiation varies by about 6.8%, while the distance from the Sun currently varies by only 3.4% (5.1 million km or 3.2 million mi or 0.034 au).[9] Perihelion presently occurs around 3 January, while aphelion is around 4 July. When the orbit is at its most eccentric, the amount of solar radiation at perihelion will be about 23% more than at aphelion. However, the Earth's eccentricity is so small (at least at present) that the variation in solar irradiation is a minor factor in seasonal climate variation, compared to axial tilt and even compared to the relative ease of heating the larger land masses of the northern hemisphere.[10] https://en.wikipedia.org/wiki/Milankovitch_cycles reply makeworld 16 hours agoprevWhat is notable about this specific exoplanet? reply yellow_lead 15 hours agoparentMaybe this > Should this second planet be confirmed, it would represent the smallest habitable zone planet discovered by TESS to date. reply aendruk 15 hours agoparentprevNovelty reply FrustratedMonky 17 hours agoprev [–] Please, Please, Please, be it. reply swarnie 17 hours agoparent [–] Be what? reply 1024core 17 hours agorootparentBe have, as my daughter thinks I'm saying... reply optimalsolver 14 hours agorootparentprev [–] Magrathea reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A study has identified a potentially habitable exoplanet named TOI-715 b orbiting an M4 star, analyzing its properties and those of its host star.",
      "The research examines the presence of a second planet in the system and discusses the importance of studying small planets around low-mass stars.",
      "Follow-up observations and data analysis were conducted using different instruments and observatories, with the study suggesting the use of future instruments like JWST to further study the exoplanets and their atmospheres."
    ],
    "commentSummary": [
      "The discussion thread centers around the discovery of TOI-715 b, a potentially habitable planet in a star's habitable zone.",
      "Participants explore the advantages and disadvantages of smaller, cooler stars and discuss the gravity and features of the planet.",
      "The search for Earth-like planets, faster-than-light travel, and the effect of interplanetary exploration on our understanding of Earth are also topics of discussion."
    ],
    "points": 215,
    "commentCount": 106,
    "retryCount": 0,
    "time": 1706973669
  },
  {
    "id": 39246664,
    "title": "Apple's Vision Pro VR Headset: Ambitious Design with Some Shortcomings",
    "originLink": "https://www.ifixit.com/News/90137/vision-pro-teardown-why-those-fake-eyes-look-so-weird",
    "originBody": "Teardowns Vision Pro Teardown—Why Those Fake Eyes Look So Weird Article by: Charlie Sorrel @mistercharlie February 3, 2024 Filed under: Teardowns, Tech News 9 Comments Share The strangest thing about the Vision Pro is also the thing that makes it most uniquely Apple: it’s got a big shiny bubble glass front, which makes it stand out from the aluminum- and plastic-shrouded competition, even when it’s off. And when it’s on, it’s even stranger—instead of being fully transparent, behind the glass, an odd lenticular screen displays a 3D-ish video of the user’s eyes, emulating their gaze. Apple calls it the EyeSight display, and when the user is looking at you, it kind of, sort of, almost looks like you can see through smokey glass. Tech journalists have called EyeSight “bizarre,” “uncanny,” and “of highly dubious utility.” But from a repair perspective, it seems like an achilles heel. Why introduce another screen, more connectors, and so many more points of failure—all for the sake of a slightly creepy feature? Of course, we had to dig in and figure out how it works. We knew it would be tough to get inside (it was). We hoped we wouldn’t break anything (we did). But we knew it would be worth it to see all the new technology Apple squeezed into this thing, from the EyeSight display to the sensor array, the external battery back to the R1 chip. We brought in the heavy hitters for this teardown, including x-ray views of the frame and high-resolution microscope shots of the displays. We’ve got a lot of observations, some opinions, and a couple educated guesses about why we got the Vision Pro we have today on the teardown table. There is a lot in this device, so we’re splitting our analysis into two, with more detail on the lens system and silicon coming in a few days. Let’s go spelunking into a never-before-explored cave of glass. The glass panel is glued on, of course, and it took a lot of heat and time, but we removed it without breakage. Granted it didn’t come out unscathed—the glass has a protective plastic film that got a little peeled up and maybe a bit melted. Apple’s retail fixers might have faster hands than us—but they’ll charge you $799 to replace broken front glass. Heavy Metal At 34 grams, the glass may not be heavy on its own, but fully kitted out with the battery the Vision Pro weighs over a kilogram. Here’s where Apple has a performed a bit of a sleight of hand. Carefully hidden in most publicity shots is the external battery, which rides along in your pocket rather than on your headset. As in the early days of VR, integrating the battery as it is now would make the device crazy heavy. And hey, we’re big fans of modular batteries, when the battery inevitably stops holding a charge in a year or three, you can replace it painlessly. Apple’s hardware team may also be anticipating the upcoming EU battery regulation, which will require all electronics to have user-replaceable batteries by 2027. The battery pack alone weighs 353 grams and is made of three iPhone-sized batteries, delivering a grand total of 35.9 Wh, more than double an iPhone 15 Pro’s 17.3 Wh. The cells themselves are 184 g apiece, surprisingly only about half the weight of the full battery pack. To get inside, we had to soften some perimeter adhesive and release a set of single-use metal clips—then twist open Torx screws galore. Three batteries in the aluminum pack, at ~3.8V each in series, 3166 mAh each, supplying 11.34 Volts in total. Add the weight of the battery pack and the headset together and you get, as mentioned above, over a kilogram—which would be a really heavy pair of glasses. For comparison, the Quest Pro weighs 722 g and the Quest 3 clocks in at 515 g. But weight isn’t just about how it tips the scales. It’s about balance. The weight of the Vision Pro largely rests on your face, all the tech is at the front and even the Pro Dual Loop Band can’t overcome it all without a counterbalance. Apple patented a design for a rear-mounted battery pack, which might’ve helped balance out the heavy front—though it’s hard to imagine wanting to wear something 150% as heavy. So if we’re just counting the weight on your face—the display module, sans battery, in the Meta Quest Pro is 522 grams. The same assembly in the Vision Pro is 532 grams, effectively the same. The key difference in these units is in the weight distribution, and a much heavier pocket battery in the Vision Pro. First impressions, though, are pretty good. “The weight isn’t as bad as expected, although it’s definitely on my forehead/cheeks as opposed to my head which feels weird, like someone is pushing on my head to tilt it down,” says iFixit teardown veteran Sam Goldheart from the teardown lab. Headbands The Vision Pro comes with both a 3D-knitted Solo Knit Band and a Dual Loop Band. These attach to the ends of the stems, just behind the speakers. The now-iconic Solo Knit Band is the one that seen in all the publicity shots, and it does look cool. It wraps around the back of your head, and you adjust the fit with a dial on the side, similar to how you might tighten a bike helmet. So how does it feel? “The fabrics are sooo nice,” says Sam. There’s a very fine, cushy weave on the Solo Knit Band, and it is stretchy enough to accommodate a ponytail and still support the face unit. The speakers are fixed onto the two rigid bands that join to the main headset. To release these, you use our old friend, the SIM-card removal tool. The holes are inside the temples of the main headset, and the removable bands have a row of electrical contacts, just like Lighting connectors, again. Easily removable parts? Only demands tools you’ve probably already got? We love to see it. This makes us hope that opening the headset might not be as daunting as we first assumed. This modular design is similar to the AirPods Max, which we quite liked. Wearables are so easy to damage that it makes good sense to have easily swappable speaker modules. We tried to go further and pry the speaker out of the silicon frame, and instantly broke the molded cables inside. That’s all right, you’re not going to need to pry the speaker modules open. The speakers—not quite as hard to get into as a pair of AirPods Pro, but almost. The speakers themselves point back towards your ears. This is a pretty clear indication that you’re not meant to wear this anywhere noisy. You can wear your AirPods Pro if you prefer—and if you want lossless, low-latency audio, they’ll have to be the latest USB-C version. On the left side is the proprietary battery cable connection, which snaps into place with a magnet and then twists to lock. We understand why Apple used a non-standard connector here, even if we don’t love it—at least it can’t be yanked out by a passing child, or when the cord inevitably catches on your chair. But the plug at the other end of the cable is unforgivable. Instead of terminating with a USB-C plug, it connects to the battery pack with what looks like a proprietary oversized Lightning connector, which you release using a paperclip or SIM-removal tool. The locking design is great, but why couldn’t it be USB-C Apple? WHY? This connector means that you can’t just swap in the USB-C battery pack you already own. Lame. Aww, such a cute family snap! Light Seals and Face Cushions Every face is different, and Apple is selling 28 different light-seal parts to cover all the different face sizes and shapes. Your seal size also changes if you need Zeiss lens inserts. That’s because the seals and cushions are also used to make sure you have the correct eye position relative to the stereo screens and eye sensors. This is why Apple is hand-packing every Vision Pro order—there’s just no “standard” setup. Guess who was wearing makeup? The seals attach to the main headset using magnets, which is Apple through-and-through—it’s either glued in place, or extremely easy to swap. This modularity is a brute force attempt to get an ideal fit on your face. It will be interesting to see if this is required long-term, or if future devices find a simpler way to accomplish this. For the time being, magnets are better than velcro because they can snap the seals into exact alignment. Think how MagSafe snatches the charger and lines it up perfectly over the iPhone’s inductive charging coil. As for cleaning the seals, Apple recommends water and unscented dish soap, which will help stop these sweat-soaking parts from getting too gross, and will be especially good for anyone wearing makeup. In her Wall Street Journal video where she selflessly wore the headset for 24 hours, Joanna Stern said her makeup caked the inside of the seals. And our own Sam Goldheart had the exact same problem this morning. Under the magnetic seals is a permanent seal, also wrapped in a knit fabric, but less likely to get smudged. It also happens to be the way into the interior of the headset. Removing it reveals another surprise: a thin stretchy sheet of plastic. Whether it’s to compensate for gaps in the knit, or to keep particulates out of the innner workings, we’re not to sure. But we are certain this bit looks very masked superhero. EyeSight Display The front-facing gogglebox is the defining feature of the Vision Pro, and, now that reviews are pouring in, one of its most controversial. The patent for the EyeSight describes three display modes: “internal focus,” “external engagement,” and “do not disturb.” The patent has pages and pages of images that might be displayed on the screen—all kinds of cartoon animal eyes, biometric analysis captured by other sensors, hearts when the user is talking to a loved one. The internal camera might read emotional states and project images based on those emotional states. Cool thought. In practice, the EyeSight display is so dim and low-resolution that reviewers say it’s hard to see much on it. The WSJ’s Joanna Stern called it “hard to see,” and Marques Brownlee (aka MKBHD) said, “You can barely see my eyes when I’m wearing the headset.” It turns out that when the EyeSight displays your eyes, it isn’t just displaying a single video feed of your eyes; it’s showing a bunch of videos of your eyes. Exploring inside the glass shell, we found three layers for the front-facing display: a widening layer, a lenticular layer, and the OLED display itself. Why Does EyeSight Look So Wonky? Apple wanted to achieve something very specific: an animated, 3D-looking face with eyes. They had to make very strategic design choices and compromises to accomplish this. Human brains are very sensitive to faces and expressions, its why the uncanny valley is a thing, and part of that is depth sensing. Apple needed to create a believable 3D effect. One reason why 3D renderings don’t look truly 3D is because they lack a stereoscopic effect. For something to look 3D, we need to see subtly different images with each eye. The Vision Pro tackles this problem with lenticular lenses. A lenticular lens displays different images when viewed from different angles. You can use this effect to simulate movement with two frames of an action. Or, you can create a stereoscopic 3D effect with images of the same subject from different angles. The Vision Pro has a lenticular layer on top of the exterior OLED panel. VisionOS renders multiple face images—call them A and B—slices them up, and displays A from one angle serving your left eye, and B from another serving your right eye. This creates a 3D face via the stereoscopic effect. And those angles are tiny, and they are legion, it takes a fancy Evident Scientific microscope to really see what we mean. The curved ridges of the lenticular lens layer. Pixels bending and shining through the lenticular layer There are compromises to this approach. The horizontal resolution is dramatically reduced, being divided between each of the multiple images. For example, if two images are displayed on a 2000 pixel wide display, each image only has 1000 horizontal pixels to work with. Even through we don’t know the resolution of the display, nor do we know the number of images being interwoven, the resolution is necessarily reduced. And that is a major reason why EyeSight eyes seem blurry. In front of the lenticular layer is another plastic lens layer, with similarly lenticular ridges. This layer appears to stretch the projected face wide enough to fit the width of the Vision Pro. Removing this layer and booting the Pro showcases some very oddly pinched eyes. Additionally the lens likely limits the effective viewing angle. Limiting the effect to directly in front of the Vision Pro limits artifacting you might see at extreme angles, sort of like a privacy filter. The downside is that you’re passing an already complex, blurry image through yet another layer of lens. This makes it even blurrier and darker. Lens Inserts, Stereo Displays You can see the outline of the ovoid lens inserts in this x-rays from our illuminous friends at Creative Electron, who spent $3,500 so you could see this photo. The Vision Pro itself performs an automatic interpupillary distance adjustment when you first put it on, with motors adjusting the positioning of the lenses. For everything else there’s prescription lenses. Apple Stores have a machine to determine approximate prescription glasses strength when you come in for a demo. For users with eye conditions (like strabismus) that might interfere with eye tracking, the Vision Pro offers alternative interaction controls in the accessibility features. However, we have heard that lenses are not available for people who have astigmatism, which is 40% of the population. If you know anything more about that, leave it in the comments. The prescription insert lenses themselves require “pairing” with the headset. The decision has already borne poor UI, John Gruber received an incorrect calibration code with his review unit that made eye tracking perform poorly. We hate parts pairing on principle, and there’s got to be a way to enable calibration while still allowing third party lenses. Oh, and Creative Electron was bored after one photo so they shot us a 360 spin. Sweet! R1 and M2 Chips The headset runs on an M2 Mac chip, in tandem with the new R1 chip—which is specifically responsible for processing the input from 12 cameras, the LiDAR sensor, and the TrueDepth camera, all with a minimum of latency. With AR, you need to project the camera view of the real world into the user’s eyes as fast as possible, otherwise their perceived motions won’t match up with what they see, which is a fast ticket to Vomitsville. To keep up, the R1 uses a real-time operating system. That means that tasks are always executed in a fixed amount of time. Most of our computers run on a time-sharing operating system, which schedules tasks on the fly, and can result in slowdowns. Think about jittery mouse cursors, or spinning beach balls, and you’ve got the idea. That won’t fly with something as critical as pass-through video and object rendering. Any glitch there would be like a glitch in the Matrix, and would be jarring at best, and utterly nauseating at worst. It might even cause you to stumble and fall. An Incredible Feat, With One Really Weird Design Decision The original iPhone did something similar. When its underpowered chips couldn’t keep up with rendering a fast-scrolling page, it would switch to a gray-and-white checkerboard, which kept up with all your flicks and swipes. Apple prioritized responsiveness over graphical fidelity. This time around, they have prioritized graphics fidelity and responsiveness, and taken the hit on battery life, weight, and heat. Given how important the experience is to Apple’s AR experience, this is probably the right choice for a first generation device. The Vision Pro is insanely ambitious. Yes, it’s heavy, and the glass is fragile, and that tethered battery might get annoying. But Apple has managed to pack the power of a Mac, plus the performance of a new dedicated AR chip, into a computer that you can wear on your face. Repairability-wise, it’s not great, but on the plus side, some of the connections are quite delightful. You should have seen our teardown team jump up when they realized that the side arms could be popped out using the SIM-removal tool, for example, and the magnetic cushions are yet more user-friendly. So why, when this thing clearly took years and years to create—and is Apple’s latest bet on the future of computing—did Apple fail to live up to their own standards with the EyeSight screen? It’s dim, it’s low-resolution, and it adds a lot of bulk, weight, complexity, and expense to the most weight-sensitive part of the headset. Did they finally hit the drop dead date and miss their targeted performance? Could it be a late-stage manufacturing error? Regardless, we’re sure bringing it to market was a difficult decision. We’ve been disassembling VR headsets since the original Oculus, and they continue to surprise and delight. There is so much fascinating mechanical and optical design packed in here. Apple’s seamless integration of sensors for rock-solid location tracking is just phenomenal, and we’re eager to dive into how they did it. We’re not done with our analysis: there’s lots more to investigate inside this device. Next time, we’ll dive into the internal displays, sensor arrays and we’ll award a repairability score. What else are you excited to see? IPD calibration motors, cooling, specific chips or circuitry? Follow along on social media, or check back here in a few shakes, we’ve got plenty more coming. Related Stories Teardowns If the Meta Quest Pro Is the Future of Computing We’ll Never Be Able to Fix It Right to Repair Meta Quest 3 Teardown and the Future of VR Repairability Teardowns The HTC Vive XR Elite Made Me Like VR Hardware",
    "commentLink": "https://news.ycombinator.com/item?id=39246664",
    "commentBody": "Vision Pro Teardown – Why those fake eyes look so weird (ifixit.com)205 points by zdw 8 hours agohidepastfavorite204 comments barnabyjones 3 hours agoI think the fake eyes and avatars would be a lot more palatable if they ditched the realism attempt and just used cartoon eyes and faces. You could choose different styles like Western or Anime, and people already buy things like eye masks with cartoon eyes on them. It would at least give the device and avatars some personality, not just \"oh man he's using that creepy new Apple thing for the call.\" I guess if they're trying to sell this toy as a Serious Business Machine for Important People, that might be seen as too juvenile, but IMO silly beats creepy. reply nirui 2 hours agoparentApple had a good show on stage, but I think the fake eyes is something you just can't optimize enough to make people like it. No matter how you trying to make it look \"natural\", it just can't, there are countless edge cases out there, it will never be \"natural looking\". For example, what if it shows others that you're staring something straight ahead but when \"in fact\" you're looking at a giant window playing a movie? Isn't that a bit awkward? Not to mention, the devices makes you constantly look like that you about to nick something. It's just look so funny. reply fikama 1 hour agorootparentFrom what I understand it shows eyes only when you are interacting in real world soI guess it shows those colorfull waves and gradients when someones is looking at mentioned window reply xuancanh 2 hours agoparentprevDidn't you forget that Facebook did that and everybody hated it? reply rpunkfu 2 hours agorootparentAlso apple did that with memoji and everybody seems to be using it. reply anonzzzies 1 hour agorootparentHere another data point from someone who should be in the everybody collection; I know 0 people who use that. reply thih9 1 hour agorootparentprevI don’t know anyone using it. Are there any stats? If anything, I’m associating cartoon avatars with Meta, I see more of them on fb and Instagram. reply wingworks 31 minutes agorootparentI always assumed the younger users use it. But I have no stats on it. reply KptMarchewa 1 hour agorootparentprevEverybody? reply itronitron 33 minutes agoprev>> Apple has managed to pack the power of a Mac, plus the performance of a new dedicated AR chip, into a computer that you can wear on your face. Regarding the external battery, I recall watching some movie where an AI discovered that the human body could be used as a battery. Seems like Apple could solve a lot of their battery issues by working on that. reply xuancanh 24 minutes agoparentHuman needs 2000 kcal on average for daily activities, which is equivalent to 470 mAh. So... no. reply ot 0 minutes agorootparentI don't know how you did that math, but you cannot convert kcal to mAh, you need a unit of energy like watt hour 2000 kcal * 4.184 (kJ/kcal) * 0.27777 (Wh/kJ) ~= 2324 Wh which is... a lot of batteries (a large laptop battery is around 100 Wh) reply ant6n 9 minutes agorootparentprev2000 kcal is equivalent to 470 mAh at like 4946V. reply blackoil 7 hours agoprevIf it has to be tethered to external battery always, why not move processing and fans to that also? reply jsheard 7 hours agoparentI would imagine they didn't want to have an external pack and that's just a compromise they had to make for this initial version, with the intention of getting rid of it in a future iteration. It wouldn't be worth overcomplicating the external pack if they don't see that as the way forward. A \"Vision Air\" that exchanges the glass and most of the metal structure for plastic, and maybe ditches the underwhelming front display, but integrates the battery like the Quests would be a pretty good trade IMO. reply karlgkk 6 hours agorootparentThe front display is underwhelming, but it also lets you have a conversation with someone else in the room. When you're wearing it, it's easy to forget it's there while talking with people. That was something that was hard to do with previous headsets, such as Valve's Index. I honestly think that the front display is going to become something that every other vendor replicates in some way. reply al_borland 6 hours agorootparentI get the point of it, but I’ve had countless conversations with people wearing sunglasses, or ski goggles, where I couldn’t see their eyes and it wasn’t a major problem. If I’m having a serious conversation with someone, I’d expect them to take the headset off, just like I’d expect them to put their phone down, or look away from their laptop. With all the negative press they’ve gotten for it, I assume they will put a significant amount of energy into improving the quality of the eye sight for version 2. reply imglorp 4 hours agorootparentBack when this all started, there was a word for people who insist on interacting with others while wearing tech on their face: glasshole. The message it sends is, \"You're not worth me putting my screen down for a minute to have a conversation.\" reply bradgessler 2 hours agorootparentBefore that was “Bluetooth Johnson” that you’d see running around at airports with their Bluetooth ear piece and a BlackBerry. reply mulmen 3 hours agorootparentprevBeing a glass hole wasn’t just about wearing tech while talking to people. It was more about how smug the users were and how overtly “look at me” the interaction was. reply makeitdouble 3 hours agorootparentprevThat was also a time when it was rude generally rude to have earbuds in when interacting with people, meetings needed to be done face to face, and having anything covering you face was just viscerally rejected by a chunk of the population. The consensus has changed a lot in the meantime, or at least you'll find your group that doesn't care about any of that. reply j4yav 2 hours agorootparentprevI think the goal is that people live their lives with their cell phone strapped permanently to their face, so they need to incentivize you to take it off as little as possible. reply blackoil 6 hours agorootparentprevIf you are having serious conversation where expression matters, just take it off or all person may wear it in the Metaverse. In current form it is gimmicky. reply karlgkk 6 hours agorootparentI would say that for quick,Kuo's prediction that the headset will be a standalone device goes against a report by The Information from September claiming that it will need to wirelessly communicate with an an iPhone, an iPad or a computer to handle most of its computing. https://www.engadget.com/apples-mixed-reality-headset-standa... reply naravara 6 hours agorootparentI assume there would be concerns about latency and throughput. reply GeekyBear 6 hours agorootparentThat seems reasonable. Also, wireless bandwidth would be an issue with so many sensors running at the same time. You would need to stream all the sensor data in one direction while also streaming two 4k display streams in the other direction. reply ethbr1 5 hours agorootparentThat seems like a really tight latency budget to run through even an optimized wireless stack. Working backward: frame needs to be received by headset in time to refresh, which means it needs to be sent by host device in time to make its way through the headset wireless phy and be decoded. And working forward: headset needs to encode sensor values, send across wireless, be decoded on host, before frame can start to be generated. Which leaves the time in-between those for actually doing all the processing and frame generation. Ditching the wireless phy and low-level steps seems like an easy trade-off for more processing time. reply sleepybrett 6 hours agorootparentprevAlso, judging from the cable on the valve index ... a much heavier and harder to manage cable... reply bpye 3 hours agorootparentYou could do something like an active optical cable if you wanted - adds expense but given the price tag of this, probably not insurmountable. reply fancy_pantser 6 hours agorootparentprevPlus nobody wants hot pockets. reply dag11 5 hours agorootparentWell, I do now. I'm hungry. reply polyomino 7 hours agoparentprevIt could be done, but there are more tradeoffs than at first glance. They probably didn’t do this because when they took the battery off the head it was already too late to redesign the thing again. You would need to propagate signals from all the sensors back and forth which adds some overall weight in copper, and perhaps latency for more encoding and decoding. Could be worth it, we’ll see in v2. reply iandanforth 6 hours agorootparentTried it today. The weight is a real problem. I'd gladly have a larger side pack to lose weight off the headset. If they ditched the useless eye screen, and moved the m2 to a pack they could probably drop 100g from the headset. reply threeseed 7 hours agoparentprevThere are reports that they tried it. Not just wired but also wireless. But the increase in latency in transmitting 10k+ of video data from the displays to the puck and back was enough to make the user more nauseous. I suspect because they wouldn't be able to process the data in time for the next frame. reply polyomino 7 hours agorootparentMeta can do wireless VR streamed from a PC with airlink. That’s at lower resolution, but it’s at least theoretically possible. reply threeseed 6 hours agorootparent1080p feed (2m pixels) compared to each eye (23m pixels). So you need 10x the network bandwidth at the same power levels. reply endisneigh 6 hours agorootparentprevThe latency is terrible though. I have an oculus quest 2. Maybe it’s an offer of magnitude better on the 3 but I doubt it. reply gruturo 7 minutes agorootparentI'm having an absolute blast with a Quest 3 and Skyrim VR running off my PC, via Steam Link, wireless. Not experiencing any latency (obviously this is all local - copper to my PC, Wifi to the Quest). reply polyomino 6 hours agorootparentprevUsually the WiFi setup is the biggest culprit. You’ll want to have wifi 6 and wire your desktop to the router. Apple could theoretically directly connect between their compute puck and their headset. reply bastawhiz 6 hours agorootparentI think this would theoretically work in the optimistic case. But it doesn't make a lot of practical sense: if the puck has the battery, you need to physically connect it to the headset anyway. Even if you used wifi for the data transfer, you're subject to any interference (retransmission of packets for a real-time OS sounds like a nightmare) and people leaving the puck on their desk and getting too far away. In a way, the cable is a feature and not a limitation. reply astrange 5 hours agorootparentSince it doesn't have a headphone jack (as far as I know) this is actually still a problem since you have to wear wireless earbuds with it, if you don't want other people to hear it. reply pshc 6 hours agorootparentprevWireless streaming VR seems like a non-starter, just an optional bullet point for marketing material. Shorter latency is always better in VR. reply tgsovlerkhgsel 6 hours agorootparentprevThe solutions I've seen adjust for head rotation on the headset (they render and transmit slightly more than the FOV), so latency only affects translation (which happens less frequently and less quickly) and actual scene updates (which are less latency sensitive than head rotation). reply polio 6 hours agoparentprevThe external component is meant to fit in one's pocket, so the added size, heat, and moving parts would make for an awkward fit. reply richardw 4 hours agoparentprevI think it’s a smart option. Fair chance there was a LOT of thinking that went into it. For example, wondering about the characterisation of it with processing in the battery appliance. It would be “an expensive portable device with a dumb headset screen”. Could someone else sell a different headset for the device? Could they sell a Linux machine that is good enough to power the headset? Might people just want one or the other, and bitch about the cost of an ARM based mobile PC that burns up your bag because it has no free-flow air? Might be Apple releases pricy larger batteries. Or some cable that allows you to hot-swap them. I see there is a cable where you can plug the headset into your Mac for development. They hopefully considered the multi-year evolution of it, not just this year’s version. reply cultofmetatron 2 hours agoparentprev> why not move processing and fans to that also? my guess is to minimize latency. reply wtallis 2 hours agorootparentLatency from 2-3 feet of wire is not problematic. But depending on how much data you want to move that distance and how thick a bundle of wires you are willing to use, power to move all those bits could be an issue. reply airstrike 7 hours agoparentprevI suppose it's easier to cool something off your forehead than off your pocket, even if that means extra weight -- it's lighter than other VR devices (although maybe that's not a particularly high (low?) bar reply jayd16 6 hours agorootparentWhat other devices? It's heavier than the Quest 1/2/3 and the PSVR2. The Quest Pro is heavier but more balanced with the battery on the back of the rigid strap. You're right about cooling. It would go from something you can pocket to something that has to be clipped on a belt like the Magic Leap. reply pants2 7 hours agoparentprevIt would be neat to have a version that plugs straight in to a Macbook pro and uses that for all the processing + battery. Then it's basically just a portable external display. reply karlgkk 7 hours agorootparentPosting from Vision, right now. I can say with some serious confidence that this wouldn't work well in that way for multiple reasons. Moving around, not being tethered, being able to take it on the plane, etc. Latency would be an issue. Not to mention, the cost of on board-computing is negligible compared to the necessary cost of the displays, R1, etc. Although, I have been using it with my MacBook Pro as a remote client, to great effect. reply makeitdouble 3 hours agorootparent> being able to take it on the plane I see the point of portability. I also wonder how much people actually care about being entertained in airplanes. You put it as your 3rd bullet point after \"not being tethered\", does it mean you're spending a crazy amount of time in your life inside planes ? Or is such a soul crushing experience that it easily warrants 3500$ of investment to improve ? And as a follow up question, did you buy a Quest2/3 under the same premise, and will replace your current setup with the AVP as an upgrade ? reply comex 2 hours agorootparent(Not the parent.) As someone who flies a few times a year, I do find it kind of soul-crushing, because of the physical discomfort of being crammed into a small space for 6 hours straight. But I suspect a headset would just make this worse; the otherwise-mild physical discomfort of wearing it would compound with the plane’s discomfort. Also, I’d be worried about disturbing adjacent passengers. And my backpack has little enough space already without a giant headset inside. …And yet I did buy a Vision Pro, for mostly unrelated reasons. (Hasn’t arrived yet.) So I’m looking forward to at least giving the airplane AR experience a try, for the novelty if nothing else. reply matt-attack 5 hours agorootparentprevAgree. I don’t think people realize just how many incoming video streams this thing has to process. I’ve heard it’s like 14 or so independent streams of video that needs processing in extremely low latency ways. Apple had to build an entire dedicated chip just to handle the realtime processing of this much incoming data. reply pests 6 hours agorootparentprevYall really going to be using these on the plane? Can't wait to watch zombies pinching at the air. reply karlgkk 6 hours agorootparentFirst of all, you don't have to move your hand or pinch \"on\" something for most tasks. The ones where you do move your hand, the range of motion needed is actually pretty small. You can have your hand in your lap or neutral on an arm rest, and pinch there. Hand position doesn't matter. Anyways, this is going to be indispensable on the plane and for frequent fliers. Having a full 20 foot cinema experience? Being able to quickly pause and interact with an attendant or the person next to you without taking it off? Killer. reply pests 5 hours agorootparent> and pinch there Pinchers! I'm only teasing honestly. reply dymk 4 hours agorootparentprevKids these days with their walkmans and their headphones, no respect at all I tell ya reply dylan604 4 hours agorootparentprevThis was in the marketing for the previous VR headsets. They looked just as goofy as Palmer on the cover. reply wudangmonk 6 hours agorootparentprevAre people looking down and flicking their fingers the hip crowd and the air pinchers the lamers?. reply grecy 4 hours agorootparentprevWhen I want to go to sleep I put on an eye mask on the plane, or I stare like a zombie at the screen on the back of the chair in front of me. Either way I have noise cancelling headphones. Honestly, I just see something like a headset just making my experience a bit more personal, which isn't a bad thing. reply antoniuschan99 5 hours agoparentprevI think eventually when they have an m2 on an iphone then it can work but they’ll probably have to remove the fans somehow. Also the front display I wonder if it will go away one day. To reduce the weight further, cost, and with sunglasses you can’t see the persons eyes. However, it does differentiate the look from oculus reply mort96 5 hours agoparentprevYou keep the battery pack in a pocket. If they did what you're suggesting, size would be a problem, and fans are ... usually less effective in pockets. reply dylan604 4 hours agorootparentnah, just make it a trendy belt pack like those early iPhone cases. reply zharknado 2 hours agorootparent3D-knitted battery bandolier, with slots for your AirPods and Apple Watch. Also comes in a silicone sport option for those times when you need to do spatial computing while trail-running. reply phonon 6 hours agoparentprevIt would add too much latency (even if it would be only a few milliseconds.) reply lolc 52 minutes agorootparentFew milliseconds is way off. Over fiber I get sub-milli ping to the datacenters in the same city. reply iancmceachern 3 hours agoparentprevLatency reply jackyard86 4 hours agoprevHuh. Apple seemed to have finally completed their conversion to USB-C. And then they came up with yet another proprietary connector for the battery pack? That's frustrating. reply GeekyBear 2 hours agoparentThe battery pack does charge though a USB-C port. However, given that this is a device you use while standing and walking about, it would be dangerous to have it suddenly turn into a high tech blindfold just because you snagged the cable on something. reply bredren 56 minutes agorootparentThe locking connector on the battery pack cable is really nice. It not only holds the cord safely, it rotates into an orientation guiding it the right direction for the cord. It would be better to have no cord, but the design is worth a close look. reply Y-bar 1 hour agorootparentprevThere are lockable USB C connectors designed to prevent that from happening: https://www.neutrik.com/en/product/nmc-c-hr reply GeekyBear 1 hour agorootparentI doubt that using a different proprietary locking connector fixes the \"why didn't they use standard USB-C\" complaint. reply wtallis 1 hour agorootparentprevIt's hard to tell given the lack of appropriate pictures, but it seems like that \"solution\" is basically recessing the USB-C port so deeply that only their cable can reach it. reply riscy 2 hours agoparentprevIt’s a power cable that locks on both ends so it doesn’t come loose and cause the device to shut off. Why does a USB-C plug matter, when no one has a locking USB-C cable anyway? reply AnarchismIsCool 2 hours agorootparentThey could have trivially made that connector USBC with the locking tabs such that you could insert a normal USBC cable, just without being able to lock it. Instead they decided to force a new connector on everyone. reply slipheen 2 hours agorootparentIt’s not intended to be a consumer replaceable connector. People opened up the device and figured out that you can get to that cable, but it’s not mentioned anywhere and not designed to be used directly. They’re not trying to force a new connector on anyone - they don’t even sell anything that uses it that part. It’s the way one proprietary part talks to another proprietary part. If you want to charge the system through USB-C, it has a port for that on the battery pack. reply naraic0o 2 hours agorootparentIt designed to be consumer replaceable https://support.apple.com/en-us/HT214013 reply slipheen 1 hour agorootparentI think we’re using different definitions of consumer replaceable. That link is for sending it back to Apple for warranty service. My point was that as far as I can tell, they aren’t expecting you to swap battery packs or change anything that uses that cable. They don’t sell extra batteries so it’s not like they’re trying to upsell on a proprietary connector. Anyway, I do agree that it would be better if it used a regular USB connection — and better still if it didn’t need an external battery pack at all. reply aembleton 2 hours agoparentprevAt least you can charge the battery pack with USB c reply lakpan 3 hours agoparentprevLook at the bright sight: they could have brought back the 30-pin plug. reply ksec 5 hours agoprevNot Seen this is being discussed anywhere. The Lighting in [1], actually shows 3 different pins count. The common / original with 8 pins. the much larger one on the right has 10 pins, while the middle one has 12 pins, the same pin count as USB-C. I would assume their dimensions would be similar as well. I actually wish it was the middle 12-Pins Lightning to be opened spec and used rather than USB-C. Along with that Sim-Lock or a push button-lock design. [1] https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_T... reply serf 6 hours agoprevliterally the only selling feature that has worked towards making me appreciate the vision pro is the automatic IPD adjustment; but that adjustment is so trivial (and never touched again) on other headsets that it seems like needless weight, especially given the front-heavy nature of a battery-free headset. also I think it's pretty crummy to throw in a proprietary battery plug when there isn't some power profile requirement to do so; it's just strictly anti-consumer. reply tgsovlerkhgsel 6 hours agoparentNot surprised about the proprietary battery plug. This isn't a charger, it's the devices only power supply. If the power sags or glitches, you get a crash, and the other side has to deal with a lot of variation in the current pulled. Letting people use random power banks wouldn't work well, and constraining yourself to USB-C for what is essentially the connection between your device's battery and the device itself sounds painful. reply bradgessler 2 hours agorootparentYeah it seems like the batter is more of an uninterruptible power supply. It has a USB-C power input on it to plug into the wall. reply CooCooCaCha 4 hours agoparentprevReally? IPD is the only feature you appreciate? None of the other cutting edge tech is worthy of your appreciation? reply AnarchismIsCool 2 hours agorootparentIt doesn't actually bring anything new to the table. The manufacturing is very advanced but that nets you....literally nothing. Yes it has the computer in the face part but it still has an umbilical so practically speaking it's no different than a normal headset and a laptop. reply dwaite 6 hours agoparentprevIt doesn't have _any_ internal battery. That USB cord has a short or comes unplugged, you are suddenly wearing a blindfold. reply al_borland 5 hours agorootparentHaving a device suddenly turn off can be an issue even with an internal battery. This should be something anyone using Vision Pro keeps in mind. And by that I mean don’t go driving in it, or model Casey Neistat and wear it while riding an electric skateboard in the middle of NYC streets. In normal situations, sitting in your home or on a plane, having it go black isn’t a big deal. Just take it off. This is where AR will really shine, if/when we ever see true AR with similar image quality. Then if it turns off, it comes glasses and not a blindfold. reply matt-attack 5 hours agorootparentprevYep. Same for the other end of the cable. I’ve heard folks ask why it doesn’t use a mag connector like on the Apple Watch since it appears to mimic the shape. reply krasin 6 hours agorootparentprevThe same, especially the short concern, applies to the proprietary cable, no? In general, it seems that the lack of a built-in energy storage (even if short-term, for 2-3 minutes) is an issue; for instance, it makes hot swapping of batteries very hard. reply bastawhiz 6 hours agoparentprev> I think it's pretty crummy to throw in a proprietary battery plug when there isn't some power profile requirement to do so I'm not one to defend Apple, but what else would they use? USB? Folks would just use whatever cord they have lying around, and it probably wouldn't work. USB, for how incredibly amazing it is, has so many incompatible cables that aren't labeled in any way that it's almost a certainty that folks would get it wrong. And with the number of noncompliant cables and chargers out there, you'd already be reading about people frying their brand new Vision Pro devices by plugging them into the wrong thing. At the very least, if you use a knock off Vision Pro battery cable and it fries your device, it's because the cable is manufactured in a grossly negligent way and not because the wrong combination of items were innocently chosen. reply riffraff 4 hours agorootparentI agree that it made sense from a pragmatic point of view, but airpods and ipads come with USB-C and I've never heard a story of anyone frying them with a random cable. I mean, surely it happens but it seems a pretty low frequency event. I feel the intersection of the set of people buying really expensive apple stuff, those who get really cheap USB-C cables, and also not being aware of the issue, is somewhat small. reply CooCooCaCha 4 hours agorootparentI mean, there could be many reasons. Perhaps USB-C was too easy to knock out of the socket. Perhaps they had special power delivery requirements. The headset has no internal battery, whereas other apple devices do have internal batteries which means they can still work if the power is inconsistent or low. reply makeitdouble 3 hours agorootparentprevThe macbook pro comes with a USB cable and charger, so that's something Apple is OK to do on high power devices. reply ehsankia 5 hours agoparentprevNow if only it actually supported multiple users where the automatic IPD adjustment would come in useful. Instead, every non-main user is a guest that has to go through he setup flow from scratch on every use... reply bredren 51 minutes agorootparentIt notices and handles IPD adjustments immediately when the device moves between users. Even in the current rev. reply joemi 2 hours agorootparentprevAt least that's likely just a software thing, so it could theoretically be fixed via a software update. reply matt-attack 5 hours agoparentprevPeople will presumable share these devices with various people in their family. I could see the auto adjustment being pretty critical. reply yellow_postit 6 hours agoparentprev“strictly anti-consumer” is kinda Apple’s jam. They can’t wait to license and collect tolls on everything. reply moribvndvs 1 hour agoprevSolo knit headband seems concerning. Keeping it firmly on your head is critical, and if it’s the same/similar solo knit stuff they’ve used on the Watch, that stuff starts to notably lose its elasticity in under a year, and that’s in a use case where it isn’t supporting half a kilo of weight. reply jelled 1 hour agoparentThe elastic isn't used for support. There's a BOA style dial that tightens things down. reply airstrike 7 hours agoprevFor the YT inclined, here's a video review by MBKHD which I found super helpful earlier today: https://www.youtube.com/watch?v=dtp6b76pMak reply michaelbuckbee 6 hours agoparentI liked that and would recommend pairing it with the review from Casey Neistat - which is much less technical and more \"vibes\" https://youtu.be/UvkgmyfMPks reply gandalfgreybeer 4 hours agorootparentThanks for sharing. Now I really want one despite knowing all the limitations and problems. It always looked so dystopian to me but for some reason I just want to play with it. reply GuestHNUser 3 hours agorootparentprevThis is a great review. I love the cuts between what he is seeing in the vision pro vs reality. I can't get over how delusional the wearer looks using it in public. That moment on the staircase when he is sending a text, for instance, looks uncanny in a way that texting doesn't. It'll be interesting to see how the vision pro will be used by the general public. reply mouzogu 1 hour agoparentprev> MBKHD most of the big YT \"review\" accounts are just giving you a list of reasons to buy {product}. reply Nodebuck 1 hour agorootparentCommented in reply to a video where he tells you not to buy it... I know it seems compelling to just brush off any YouTuber that amassed millions of subscribers – since it's usually rightfully so – but MKBHD has kept to his roots and still puts out reasonable reviews apart from the occasional \"detached\" statement imho. Edit to add to the discussion: If you want to hear what a VR enthusiast has to say about the AVP, I've been watching SadlyItsBradley's unboxing & testing VOD and it's pretty insightful, though over 2 hours and not neatly packed into a short review: https://youtube.com/live/ZZ2aM8kw1ww reply TillE 7 hours agoprevI was a little surprised to see not one but two huge fans. Reviewers have mentioned hearing no fan noise, but maybe they crank up a little if you really push the GPU + CPU with a 3D game. reply MBCook 7 hours agoparentSomeone managed to get a mention of them out of someone at Apple a while ago. Not only are they quiet, but the EarPods (built in headphones) do active noise cancellation against them for the person wearing the Vision Pro. reply perryizgr8 6 hours agorootparentI don't think it is possible to do open air noise cancelation like that. Do you remember where you saw that mentioned? reply ralph84 6 hours agorootparentWhy not? There are active noise cancellation systems for airplanes and cars where the distance between the speaker and the ear is much greater than in this case. reply perryizgr8 2 hours agorootparentAs I understand it, niose cancelation requires an enclosed volume of space. It's why you never see it on open back headphones. Cars and aeroplanes are completely enclosed too. reply wtallis 1 hour agorootparentActive noise cancellation is just destructive interference. It doesn't fundamentally require any sounds to be bouncing off surfaces, and mechanical isolation would be passive noise cancellation. The main caveat to active noise cancellation is that you cannot construct a destructive interference pattern that cancels the noise everywhere. Creating destructive interference to cancel out the noise at your ears will unavoidably mean creating constructive interference somewhere else. reply MBCook 5 hours agorootparentprevI think they have a lot of advantages here. They have tons of microphones, they know exactly what the noise sounds like, exactly where it is positioned and exactly where the speakers are relative to your ears. I suspect that makes the problem constrained enough that it becomes possible. reply coffeebeqn 7 hours agoparentprevAny PC builder can tell you that the bigger and more fans you get the quieter and slower rpm they can run reply polyomino 6 hours agorootparentBut also more weight reply rowanG077 7 hours agoparentprevProbably precisely why they use two fans. Have them really silently instead of one audibly. reply easeout 5 hours agorootparentThat and there are two major chips, the R1 and M2, acting as distinct heat sources. I don't know if they would spin the fans up and down independently, but the two chips' heat output could differ over time. reply georgemcbay 7 hours agorootparentprevAlso probably why they are relatively big for a device of this size. If you're designing for a specific airflow target, making the fans larger lets you make them quieter because they can spin slower than smaller fans while displacing the same amount of air. reply throwing_away 7 hours agoprevThat x-ray shot made me thinking I was looking at the head of a futuristic robot. In some ways, I guess I am. reply whatever1 3 hours agoprevI wonder whether they looked into fiber optics for direct image pass through. Like a system of lenses that move light from the front of the headset to the glass lensw reply lwansbrough 1 hour agoparentThat wouldn't work, but waveguide displays are real and may be the future of AR, as passthrough will likely always have some latency (even the stated 12ms latency on Vision Pro is higher than you want in some contexts.) reply iancmceachern 3 hours agoparentprevFiber optics don't really work that way. They transmit light, not images, over the fibers. reply teruakohatu 3 hours agorootparent> Fiber optics don't really work that way. They transmit light, not images, over the fibers. Images over optical fiber was originally developed in the 1960s, although the concept goes back to the 1840s : https://en.wikipedia.org/wiki/Fiberscope It is possible the concept was observed much earlier than that if someone had polished an ulexite rock: https://en.wikipedia.org/wiki/Ulexite reply iancmceachern 3 hours agorootparentDo we use it for any practical applications in practice? The light losses alone would make this only able to be used in very specific applications. reply whatever1 3 hours agorootparentprevI remember a lab working on it like a decade ago (using a batch of fibers, not just one) Edit: Found it https://phys.org/news/2014-02-optical-fibers-transmit-high-q... reply iancmceachern 3 hours agorootparentYeah, but this isn't practical at all for this application reply lawgimenez 6 hours agoprevI think this is the first review that mentions there might be some issues with people who have astigmatism? Like me. reply zaptrem 6 hours agoparentThe Zeis Lens inserts support cylinder prescriptions just fine (and I can verify this as I tried them in store), so idk what the author was talking about. reply wtallis 1 hour agorootparentA lot of people seem to think astigmatism is the term for needing a prism prescription, or that astigmatism is somewhat rare. reply astrange 5 hours agoparentprevIf you can wear toric contacts or get the prescription inserts then it's fine. reply isodev 3 hours agoprevI like the fake eyes - both on the outside screen as well as FaceTime calls. I think it’s quite striking how well they let one see if you’re paying attention to them. Also, Vision Pro seems to manage to relate whatever emotion/facial expression one is currently making despite wearing 600g of chips and glass on one’s face. reply ranger207 3 hours agoprevThe tech in that outside screen looks a lot like how I remember the Nintendo 3DS's screen works. I'm surprised it's so bad given how much time has passed since then, although I guess patents could be one reason why reply tommoor 5 hours agoprevSo, do we think there is a path to using some future version of the transparent OLED's that were being shown off at CES here? If the same density can be achieved you can imagine how much simpler the entire device would get with just a transparent screen – you'd have to move the rest of the electronics into the edges but that seems very doable. reply threeseed 4 hours agoparentTwo deal breaking issues: 1) The reason that the resolution is so high on the AVP is because it uses Micro-OLED which is also known as OLED on Silicon i.e. there is a layer of silicon behind the diodes which would block the light. 2) Transparent OLED TVs required a motorised screen to put a black panel behind the display. Only options are that or electrochromatic glass. Both of which are not black enough for watching movies etc. reply mort96 5 hours agoparentprevThat feels like it'd cause light bleed issues when you want it to act as a proper VR headset? Can you make transparent OLEDs which can go fully 100% opaque when desired? reply wrsh07 2 hours agorootparentAt the very least there's the low tech solution of an opaque cover you put over it AR and VR have always felt like duals to me - with AR, you just put something behind it and you have VR. With VR you do passthrough video reply daemonologist 4 hours agorootparentprevXreal (Nreal) has a product using electrochromic glass which apparently works pretty well although doesn't quite become completely opaque; that might be an option. I think Apple isn't really interested in VR though, so they might accept some light bleed in a \"just glasses\" form factor Vision product. reply mort96 4 hours agorootparentProper VR is an important aspect of the device. The main focus is on the \"spatial\" AR-style experience, but there's also a lot of focus placed on being able to be in a completely virtual environment in both the marketing materials and the OS. I don't think something which \"works pretty well but doesn't quite become completely opaque\" sounds good enough, especially for something whose only real advantage is possibly slightly improving how the wearer looks to others. For a different product that's glasses instead of a VR headset, sure, it might work (although I don't see why you'd want to go semi-opaque at all with glasses really, it won't be close to an immersive VR-style experience anyway?) reply mtlmtlmtlmtl 7 hours agoprev$800 to repair cracked glass and $2400 to repair \"other damage\". Make it make sense, Apple. reply arrrg 1 hour agoparentHow this can make sense: They have a process for exchanging the cover glass, they don’t have a process for exchanging anything else, meaning you will always get a replacement device. $2400 is just the price for the device minus the profit margin (would be a 31% margin, which fits with Apple‘s typical margins). reply frabjoused 7 hours agoprevHer third eye on the left of the lenticular lens was interesting. reply sleepybrett 6 hours agoprevdo you have to use the battery? Does it have a passthrough for a power adapter? reply judge2020 6 hours agoparentYou must use the battery but the battery has USB-C. Although it will slowly lose power while using if you only supply 5v1a via a USB-A to USB-C cable, using a more powerful brick is required to gain battery during usage. reply scotty79 6 hours agoprevI wonder how light could be vr glasses if they moved everything except the displays to separate pack connected to the glasses with short thin flexible cable. I'd prefer to carry the weight on the back or as a necklace or a collar than all of it on my head. How thin display port cable could be if it was optimised for sinhle purpose and lenght of up to half a meter? reply dpig_ 6 hours agoparentI regularly switch between the Quest and the PSVR2. The Quest has a whole system running on your forehead, while the PSVR2 is always-wired, and leverages the PS5 for compute. The difference in weight is phenomenal. I would absolutely love to have all of that extra stuff offloaded into a pocket pack. Or, at the very least, parked behind the head as a counterbalance. reply dwaite 6 hours agorootparentin addition to the latency of serializing things over the cable, that would essentially be a Mac mini. They surely could make the size work, but it would need active cooling to keep the temperature in a usable range and that wouldn't work inside a pocket. reply dpig_ 5 hours agorootparentPeople are very happy to play competitive VR shooters over a cable, so I can't imagine latency would be an issue broadly. Fair point w/r/t cooling. Again, I think distributing components around the whole head, rather than 100% on the face, is the way to go. reply dns_snek 2 hours agorootparentAs an extreme example, lots of people play PCVR games on Quest headsets by streaming them over WiFi. It seems to work fine for most, even with compression artifacts and added latency. reply redox99 4 hours agorootparentprev> in addition to the latency of serializing things over the cable That doesn't add meaningful latency if bandwidth is sufficient. A 80Gbps bidirectional link (which is around what latest DP/Thunderbolt/USB handles) would work fine. You can always add more lanes if you need anyways, at the expense of a thicker cable. reply wrsh07 2 hours agoparentprevI would love to have a compute backpack that can charge my phone or serve as the computer for my VR headset I would still want it to be relatively lightweight, but you could go much heavier than on the head and have it still be comfortable reply polyomino 6 hours agoparentprevhttps://www.visor.com/ reply scotty79 6 hours agorootparentPretty scarce on the tech details but interesting. We'll see what actually comes out. reply lkj34ifj3 7 hours agoprevnext [15 more] [flagged] nucleardog 7 hours agoparent> while everyone else manages with a single USB-C cable for everything. Which brand and model of cable is that so I can go buy myself a couple dozen? I’ve got a whole bin of seemingly identical cables here and usually have to try several before I can get devices to connect properly. reply fiddlerwoaroof 6 hours agorootparentYeah, standardizing on USB-C would be a great idea, but only in a world where there weren’t fifty different “standards” using visually indistinguishable connectors. Thunderbolt, in particular, should have its own connector: it’s so annoying when I accidentally plug my Thunderbolt 3 dock into a normal USB-C port and then can’t figure out why my external monitor and other peripherals aren’t working. reply pests 6 hours agorootparentprevThe expensive Apple 1m USB-C I believe is a full-featured cable and you pay for it. A lot of the cables that get included with devices only supports the subset of features that device needs - sometimes down to just power delivery only. Cables are much more complicated now with embedded chips and so on so you need to select and use them as if they were any other electronic you would spend time reviewing the specs or comparisons on. reply al_borland 5 hours agorootparentFor those interested, this video shows scans of what is inside Apple’s cable vs cheap cables off Amazon. https://m.youtube.com/watch?v=AD5aAd8Oy84 For those who prefer to read and want to look at the scans in more detail, that can be found here: https://www.lumafield.com/article/usb-c-cable-charger-head-t... reply moondev 34 minutes agorootparentThey compared a thunderbolt 4 cable to several non thunderbolt cables. It's not surprising the non thunderbolt cables do not have thunderbolt chips This cable also has no thunderbolt chip and is only usb 2.0 https://www.apple.com/shop/product/MU2G3AM/A/240w-usb-c-char... reply turquoisevar 5 hours agorootparentprevI got a few, also the longer one. Works 99% for both days and PD. But then I have this one Seagate external SSD that simply won’t mount with it, for which I have to use a TB4 cable (even though it’s not rated for TB). Also, some cheap Chinese devices only work with their supplied USB-A to C cable. Other than those edge cases, those Apple cables are great. It goes to show there is no one cable to rule them all, even though they look identical. reply threeseed 7 hours agoparentprevThe cable is not supposed to be user-replaceable. And the reason it's not USB-C is because it needs to have a locking mechanism and be ultra-robust at the ends. I've never seen a USB-C cable with either. reply al_borland 5 hours agorootparentFrom what I saw, a SIM ejector and allow the user to disconnect the cable. This is more trivia than anything, since it’s a unique connector. I think the bigger reason behind locking the cable to the battery pack, is the same reason the cable locks to the headset. They don’t want it to come unplugged while you’re using it. If the pack falls out of your pocket, it would probably better for it to catch itself and hang, than come unplugged. reply superb_dev 6 hours agoparentprevApple recently started the switch to USB-C, so I don't understand this comment. All of my Apple devices charge with USB-C now reply GeekyBear 6 hours agorootparentThe iPad Pro had already moved to USB C back in 2021 with the iPad Air and entry level iPad following suit the next year. The notion that the EU forced Apple's hand with the iPhones a year after that (and not that Apple continued a transition that was already in progress) is some people's favorite conspiracy theory. reply NdMAND 6 hours agorootparentprevTo be fair - their hand was forced by the EU to do that. They were not forced for VR headsets and they came out with 3-4 new type of connectors for this. So I think the parent comment was fair reply al_borland 5 hours agorootparentApple was well on their way moving everything to USB-C. It was going to happen with or without the EU. I suspect the iPhone was last because it is the most popular device. They don’t want to do anything risky, and it would be good if people already had a bunch of USB-C around the house before the iPhone switch to make it less impactful. People were very upset when Apple moved away from the 30-pin connector, because all their accessories became obsolete overnight. They promised Lighting was designed to last for the next decade, which is about how long it lasted. reply yellow_postit 6 hours agorootparentprevIt’s good to remember Apple was forced to switch by the EU for the phone. It was not choice they made on their own. reply wilsonnb3 6 hours agorootparentApple would have absolutely switched to USB C anyways, at most the EU got them to do it a year or two earlier than planned. reply mouzogu 1 hour agoprevi'm interested in this product from a cultural theory view 80% struggling to pay their groceries while 20% playing fruit ninja in a dystopian walled garden i feel there is some underlying narrative which i'm trying to construct reply 3825 38 minutes agoparentI don’t have spare money to spend on a Vision Pro that I couldn’t spend somewhere better but I have this thought when I was taking a hot shower. I’m here just living normally while there are millions running for their lives from war zones like literally. I’m more worried about the multibillionaires than anyone else, glass hole or not. reply jsheard 7 hours agoprevHas anyone figured out what the main lenses are made of yet? VR lenses are usually plastic to keep the cost and weight down, so you have to be careful when cleaning them, but Apple wasn't overly concerned with cost or weight when designing this. reply bemusedthrow75 6 hours agoparentI would be a little surprised if they weren't PMMA or something like it. It's such a useful material for complex applications like this. reply mtlmtlmtlmtl 4 hours agorootparentPara-methoxy-methamphetamine? Seems like a dangerous choice. reply yieldcrv 7 hours agoparentprevThe counterpoint to the weight criticism is that other leading headsets are heavier just balanced in the back more. Apple will probably do something to distribute weight to make it less noticeable too reply kfarr 7 hours agorootparentOr at least through third party head straps. reply carl_sandland 7 hours agorootparentpreva bit of velcro on the battery attached to the rear of the strap? that should balance it out a bit. reply _kb 7 hours agorootparentGenius. Such a ground breaking, original, and industry defining idea is even patent worthy: https://patentscope.wipo.int/search/en/detail.jsf?docId=WO20.... reply mtlmtlmtlmtl 3 hours agorootparentI'll never understand why Apple feel the need to act as a patent troll. Their brand is so powerful they could take any electronic device, slap their logo on it and sell it to a huge amount of people at much more profit margin than most companies that make functional things can get away with. Maybe this is just to Apple's patent lawyers what pointless UI redesigns is to frontend devs. Gotta do something. reply _kb 2 hours agorootparentI'd hazard a guess this may be primarily for protection from patent trolls and as a mitigation for accidental patent violation. It's likely cheaper to gratuitously patent than risk ceasing production or sale of products. reply wyager 7 hours agorootparentprevThat's what we do for night vision goggles. https://tnvc.com/shop/ab-night-vision-lpbp-go/ reply barkerja 7 hours agorootparentNVGs are also typically designed to be attached to a kevlar (combat helmet). reply Ancapistani 6 hours agorootparentMine are usually mounted to a Crye Nightcap: https://www.cryeprecision.com/NightCap Basically, you need either balance or more points of contact. As long as it’s solidly attached you’re good. reply AceJohnny2 5 hours agoprev> But Apple has managed to pack the power of a Mac, plus the performance of a new dedicated AR chip, into a computer that you can wear on your face. The M2 chip, like any modern SoC, can easily heat up enough to burn skin under powerful workloads. \"The power of a Mac\" is misleading; does this mean a Macbook Air (constrained cooling means a tighter thermal, and thus performance, envelope), a Macbook Pro (better cooling = better performance), or a Mac Mini (best cooling = best performance)? Given the facial proximity and battery, I expect we'll never see that M2 at full power. reply rowanG077 4 hours agoparentThe macbook air has no active cooling. I would expect this machine to have better thermals. reply bemusedthrow75 6 hours agoprev [–] I know this won't be iFixit's answer but maybe we should converge on the idea that it's because faking someone's eyes on a screen when their real eyes are just behind it is a genuinely sick, actively dystopian idea that is intended to offset the sense that they've developed a piece of technology that allows people to selectively pay attention to a human in the same room, even though they are marketing it to people who spent all that energy amplifying the idea of the \"glasshole\"? reply graypegg 6 hours agoparentThis is easily the thing that makes me most uncomfortable, no matter how much it could be improved. The ick-factor of it all is difficult to describe, and if it doesn’t bother others, I’m not even really capable of arguing why it matters to me. There’s functionally very little difference from an “immersion dial” and a person who puts on noise cancelling headphones and uses their phone with intent to disconnect for a bit. I know. As much as it makes me feel like some angry senior who dislikes the kids and their phones… I don’t like it. reply bemusedthrow75 6 hours agorootparentIt is just unavoidably creepy, and yet it was excitably launched with a video that was the most dystopian, unsettling portrayal of the target consumer's life. It so profoundly undermines what Apple is; no other Apple product has ever been remotely \"hell is other people\", especially not \"hell is other people who are in the room with you\". I think there's a lot more than just the airpods comparison that i've seen elsewhere, too. Because the people with the Apple eyes are using fake eyes; it is not filtered attention, it is actual misdirection. Though I personally find having a conversation with people who won't take their airpods out is a sign that I am having a conversation with someone who devalues interacting with humans. reply threeseed 4 hours agorootparent> Though I personally find having a conversation with people who won't take their airpods out is a sign that I am having a conversation with someone who devalues interacting with humans. And that's a generational difference. You and me may feel like that but a lot of kids these days don't. reply j4yav 2 hours agorootparentI’m sure if they can get AR glasses on kids from an earlier age then future generations won’t find living day to day with a cell phone resting over your eyes strange at all. Something like one laptop per child, but with Vision Pro. reply maxglute 4 hours agorootparentprevDifference is video is captured at all time since camera is on at all times for tracking. It's icky not knowing if you're being broadcasted while making eye contact as more than background character. I suppose that's true with audio being recorded as well. reply GeekyBear 6 hours agoparentprev> allows people to selectively pay attention to a human in the same room, even though they are marketing it to people who spent all that energy amplifying the idea of the \"glasshole\"? Perhaps we could learn the lesson from Google's Glass that people didn't like the notion of being recorded without knowing it was happening. > The main critique of Google Glass wasn’t really that they looked stupid (although, to be clear, they did). People were kicked out of bars for wearing Glass because the device represented a form of ubiquitous recording. Glass was outfitted with a camera that the user could activate at any time https://www.wired.com/story/google-glass-reasonable-expectat... Apple decided to give people an indication of whether the people using this device could see them or not, in addition to adding a big button on the top of the device you have to press to take a picture or video. reply bemusedthrow75 6 hours agorootparentI personally think these things are much worse, at this point, because ubiquitous recording is now an old concept, and these things are a literal eye mask with faked attention. There is something so broken about it. But I do take your point, and don't think you deserved a downvote for it. So much downvoting going on. reply GeekyBear 6 hours agorootparentThere is the ubiquitous presence of devices that have the ability to take a recording today. However, unlike Google Glass, modern smartphones play a \"camera shutter\" sound to at least give some indication that they are capturing an image to the people around you. Likewise, Apple is giving people an indication that the person wearing the headset can currently see you. The lesson from Google Glass is that people didn't like not knowing what was happening. reply bemusedthrow75 5 hours agorootparent> The lesson from Google Glass is that people didn't like not knowing what was happening. I think that is at least in part a highbrow post-justification, though I accept it's going to be more common among Apple pundits who need the Vision Pro to be a success but spent time deriding Google Glass. The instinctive reason is quite different; it was about inattention and rudeness. Google \"google glass rude\" and you will see _plenty_ of coverage about it at the time. People don't like this stuff on other people. I certainly reserve the right to dislike these things on a purely instinctive level. Whether or not they have to choose to take a photograph of me or not. reply GeekyBear 5 hours agorootparent> I think that is at least in part a highbrow post-justification I think that the ability to record people without them knowing it is quite literally what people said was the main point of contention with Google Glass. > If you see someone wearing Google Glass wink at you, you might want to get out of the way because they're probably not flirting with you. A new app that's just been developed and released for the futuristic piece of technology lets users take a photo by simply winking an eye. https://phys.org/news/2013-05-google-glass-photo.html reply mtlmtlmtlmtl 2 hours agorootparentprevBeing filmed with other people bothers me a little, sure. It's the rudeness that bothers me the most. I wouldn't let someone be a guest at my place if they're gonna wear this thing. Leave it at the door. reply astrange 5 hours agorootparentprev> However, unlike Google Glass, modern smartphones play a \"camera shutter\" sound to at least give some indication that they are capturing an image to the people around you. I mean, they mostly don't. They do in Japan (maybe Korea too?) but otherwise you can mute it. reply hollerith 3 hours agoparentprev [–] Because people would never ignore someone in the same room as them were it not for tech like this. reply AvesMerit 1 hour agorootparent [–] Right! Most people ignore each other in coffee shops and airport lobbies. Our attention is already stretched so thin, parent should not feel entitled to mine reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Apple's upcoming VR headset, the Vision Pro, stands out with its unique design featuring a bubble glass front and lenticular screen, but some tech journalists criticize its functionality.",
      "Weighing over a kilogram with an external replaceable battery pack, the headset complies with EU battery regulations and offers a modular design for easy repair.",
      "However, the headset has some design flaws, including a low-resolution display and concerns about makeup caking on the seals due to sweat. It runs on an M2 Mac chip and R1 chip for camera and sensor input. Apple faces challenges in addressing these shortcomings before bringing it to market."
    ],
    "commentSummary": [
      "The article explores multiple aspects of Apple's Vision Pro device and virtual reality headsets, including the appearance of fake eyes, the possibility of moving processing components and fans to an external device, and the potential use of wireless streaming for VR.",
      "The article also discusses concerns regarding proprietary connectors and battery plugs, the use of USB-C cables, and the limitations and issues with the Vision Pro.",
      "Additionally, it touches on topics such as noise cancellation technology, ulexite rock for optical fibers, transparent OLEDs for VR, and a comparison between different VR headsets. The article also raises concerns about privacy and inattention due to technology, providing a comprehensive range of perspectives and opinions on these subjects."
    ],
    "points": 205,
    "commentCount": 204,
    "retryCount": 0,
    "time": 1707010663
  },
  {
    "id": 39240471,
    "title": "Optimizing Docker Image Size: Lessons from ugit Tool",
    "originLink": "https://bhupesh.me/publishing-my-first-ever-dockerfile-optimization-ugit/",
    "originBody": "How I reduced the size of my very first published docker image by 40% - A lesson in dockerizing shell scripts 📅 Published: February 3, 2024 • 🕣 17 min read 📌 shell 📌 linux 📌 git I interact with Dockerfiles every day at work, have written a few myself, built containers, and all that. But never published one on the docker hub registry. I wanted to make ugit - a tool to undo git commands (written as a shell script) available to folks who don’t like installing random shell scripts from the internet. Yeah, I know, I know. REWRITE IT IN GO/RUST/MAGICLANG. The script is now more than 500+ lines of bash. I am not rewriting it in any other language unless someone holds a gun to my head (or maybe ends up sponsoring??). Moreover, ugit is close to being feature complete (only a few commands left to undo, which are not that commonly used). Anyway, the rest of the article is about how I went about writing the official Dockerfile for ugit (a shell script) and reduced the image size by almost 40% (going from 31.4 MB to 17.6 MB) by performing step-by-step guided optimization attempts. I hope this motivates other shell enthusiasts to also publish their scripts as docker images! PS: I am not a DevOps or Docker expert, so if you are and see something wrong or something that could be done better, please let me know in the comments below or reachout somewhere. The final docker image is available on docker hub The very first Dockerfile ~attempt~ Path to optimization - reducing image size by 40% 2nd attempt - alpine on alpine Looks like xargs and awk came free? 3rd attempt - Using scratch at 2nd stage Identifying transitive dependencies Primer on shared libraries Shebangs #! are useless A look at terminfo db Everything needed to run ugit Could we reduce the size further? Reason 1: Pin minimum version of fzf? Reason 2: Use the latest bash features? You didn’t try docker-slim? You didn’t try docker-squash? Learnings Acknowledgements Resources The very first Dockerfile ~attempt~ # Use an official Alpine runtime as a parent image FROM alpine:3.18 # Set the working directory in the container to /app WORKDIR /app # Copy the current directory contents into the container at /app COPY . /app # Install dependencies RUN apk add --no-cache \\ bash \\ gawk \\ findutils \\ coreutils \\ git \\ ncurses \\ fzf # Set permissions and move the script to path RUN chmod +x ugit && mv ugit /usr/local/bin/ # Run ugit when the container launches CMD [\"ugit\"] Looks pretty simple, right? It is. You could copy-paste this Dockerfile and build the image yourself assuming you have ugit cloned in the current directory, docker build -t ugit . docker run --rm -it -v $(pwd):/app ugit This should successfully run ugit inside the container. ugit requires binaries like bash (>=4.0), awk, xargs, git, fzf, tput, cut, tr, nl. We had to install findutils because it ships with xargs. We had to install coreutils because it ships with tr, cut and nl. ncurses is required by tput (which is used to get the terminal info). That’s all we need to run ugit on a UNIX-based machine or well, a container. The image size sits at 31.4 MB at this point. Not bad for a first attempt. Let’s see how we can reduce it further. Path to optimization - reducing image size by 40% During our (desperate) micro-optimization attempts in the upcoming sections, we will be covering the following high-level goals: Use multi-stage builds to reduce image size. Get rid of binaries like sleep, watch, du etc. Anything that is not required by ugit to run. Get rid of unnecessary dependencies that these binaries bring in. Get a minimum version of all dependencies that are required by ugit to run. Load up only the required binaries and their dependencies in the final image. 2nd attempt - alpine on alpine I now choose to create a multi-stage build. The 2nd stage will be used to copy only the required binaries and their dependencies. I again choose to use alpine as the base image for this stage. # First stage: Install packages FROM alpine:3.18 as builder RUN apk add --no-cache \\ bash \\ gawk \\ findutils \\ coreutils \\ git \\ ncurses \\ fzf # Copy only the ugit script into the container at /app WORKDIR /app COPY ugit . # Set permissions and move the script to path RUN chmod +x ugit && mv ugit /usr/local/bin/ # Second stage: Copy only necessary binaries and their dependencies FROM alpine COPY --from=builder /usr/local/bin/ugit /usr/bin/ COPY --from=builder /usr/bin/git /usr/bin/ COPY --from=builder /usr/bin/fzf /usr/bin/ COPY --from=builder /usr/bin/tput /usr/bin/ COPY --from=builder /usr/bin/cut /usr/bin/ COPY --from=builder /usr/bin/tr /usr/bin/ COPY --from=builder /usr/bin/nl /usr/bin/ COPY --from=builder /usr/bin/gawk /usr/bin/ COPY --from=builder /usr/bin/xargs /usr/bin COPY --from=builder /usr/bin/env /bin/ COPY --from=builder /bin/bash /bin/ WORKDIR /app # Run ugit when the container launches CMD [\"ugit\"] Just by a straightforward multi-stage build, we were able to reduce the image size to an impressive 20.6 MB. Now the image builds successfully, but it won’t run ugit yet. Error loading shared library libreadline.so.8: No such file or directory (needed by /bin/bash) Turns out, we are missing transitive dependencies. More about this on our 3rd attempt. Looks like xargs and awk came free? Turns out, that both xargs and awk are present by default on the Alpine image. You can verify this by running the following commands: docker run -it alpine /bin/sh -c \"awk --help\" docker run -it alpine /bin/sh -c \"xargs --help\" Rookie mistake. Let’s scratch gawk and findutils from our Dockerfile. # First stage: Install packages FROM alpine:3.18 as builder RUN apk add --no-cache \\ bash \\ coreutils \\ git \\ ncurses \\ fzf # Copy only the ugit script into the container at /app WORKDIR /app COPY ugit . # Set permissions and move the script to path RUN chmod +x ugit && mv ugit /usr/local/bin/ # Second stage: Copy only necessary binaries and their dependencies FROM alpine:3.18 COPY --from=builder /usr/local/bin/ugit /usr/bin/ COPY --from=builder /usr/bin/git /usr/bin/ COPY --from=builder /usr/bin/fzf /usr/bin/ COPY --from=builder /usr/bin/tput /usr/bin/ COPY --from=builder /usr/bin/cut /usr/bin/ COPY --from=builder /usr/bin/tr /usr/bin/ COPY --from=builder /usr/bin/nl /usr/bin/ COPY --from=builder /usr/bin/env /bin/ COPY --from=builder /bin/bash /bin/ WORKDIR /app # Run ugit when the container launches CMD [\"ugit\"] The image size is now down to 20 MB. We are getting there. ugit still won’t run, though. 3rd attempt - Using scratch at 2nd stage This is the part, that most folks don’t attempt. It’s a bit scary and requires a huge commitment. I was a bit scared to go this route as well because I knew everything could be SCRATCHED 🙃. A SCRATCH docker image is just an empty file system. It doesn’t have anything at all. To try out a SCRATCH image, you can refer to docker hub’s README on it. The only thing you need to know is that everything has to be put together by us. Let’s keep our hands on our hearts and replace alpine with scratch. # First stage: Install packages FROM alpine:3.18 as builder RUN apk add --no-cache \\ bash \\ coreutils \\ git \\ ncurses \\ fzf # Copy only the ugit script into the container at /app COPY ugit . # Set permissions and move the script to path RUN chmod +x ugit && mv ugit /usr/local/bin/ # Second stage: Copy only necessary binaries and their dependencies FROM scratch COPY --from=builder /usr/local/bin/ugit /usr/bin/ COPY --from=builder /usr/bin/git /usr/bin/ COPY --from=builder /usr/bin/fzf /usr/bin/ COPY --from=builder /usr/bin/tput /usr/bin/ COPY --from=builder /usr/bin/cut /usr/bin/ COPY --from=builder /usr/bin/tr /usr/bin/ COPY --from=builder /usr/bin/nl /usr/bin/ COPY --from=builder /usr/bin/env /bin/ COPY --from=builder /bin/bash /bin/ WORKDIR /app # Run ugit when the container launches CMD [\"ugit\"] Doing this reduced the size of our image to 12.4MB, a 60% reduction?? Did we just rickroll ourselves? Let’s try to run ugit. $ docker run --rm -it -v $(pwd):/app ugit-a3 exec /usr/bin/ugit: no such file or directory $ docker run --rm -it --entrypoint /bin/bash ugit-a4 exec /bin/bash: no such file or directory Turns out, we broke the bash binary by not shipping its dependencies. Let’s see what we can do about it. Identifying transitive dependencies Okay, time to talk about transitive dependencies. Our script relies on binaries like git, tput, bash; now some of these utils may have their dependencies. We technically call these dependencies, shared libraries. Shared libraries are .so (or in Windows .dll, or in OS X .dylib) files. ldd is a great tool to identify these dependencies. It lists all the libraries needed by a binary to execute. For example, if we run ldd /bin/bash on a fresh Alpine container, we get the following output: / # ldd /bin/ls/lib/ld-musl-aarch64.so.1 (0xffff8c9c0000)libc.musl-aarch64.so.1 => /lib/ld-musl-aarch64.so.1 (0xffff8c9c0000) / # ldd /bin/bash/lib/ld-musl-aarch64.so.1 (0xffffb905a000)libreadline.so.8 => /usr/lib/libreadline.so.8 (0xffffb8f06000)libc.musl-aarch64.so.1 => /lib/ld-musl-aarch64.so.1 (0xffffb905a000)libncursesw.so.6 => /usr/lib/libncursesw.so.6 (0xffffb8e95000) Primer on shared libraries Each lib has a soname, which is the name of the library file without the version number. They start with the prefix lib and end with .so. For example, libpcre2-8.so.0 has a soname libpcre2-8.so. A fully qualified lib includes the directory where it is located. For example, /usr/lib/libpcre2-8.so.0. Each lib has a real name, which is the name of the library file with the version number. For example, libpcre2-8.so.0.3.6 could be a real name. The hexadecimal is the base memory address where this library is loaded in memory. Not useful for us. We are only interested in the library names. Let’s do it for all the binaries we need to run ugit. The path to the right-hand side of => symbol indicates the real path to that shared library. The lib name starting with libcis the C library for that architecture. Remember our “exec /bin/bash: no such file or directory” error? This is the reason we got it. We didn’t ship the C library for our architecture. Below is an excerpt from Douglas Creager’s article on Shared library versions which sums up shared libraries, please read the full article if you want to learn more about shared libraries. With a shared library, you compile the library once and install it into a shared location in the filesystem (typically /usr/lib on Linux systems). Any project that depends on that shared library can use that shared, already compiled representation as-is. Most Linux distributions further reduce compile times by distributing binary packages of popular libraries, where the distribution’s packaging system has compiled the code for you. By installing the package, you download a (hopefully signed) copy of the compiled library, and place it into the shared location, all without ever having to invoke the compiler (or any other part of the build chain that produced the library). We use a similar approach to identify all the unique dependencies for all the binaries we need to run ugit. ℹ We did this by running a command like for cmd in /bin/*; do echo $cmd; ldd $cmd; done # copy lib files COPY --from=ugit-ops /usr/lib/libncursesw.so.6 /usr/lib/ COPY --from=ugit-ops /usr/lib/libncursesw.so.6.4 /usr/lib/ COPY --from=ugit-ops /usr/lib/libpcre* /usr/lib/ COPY --from=ugit-ops /usr/lib/libreadline* /usr/lib/ COPY --from=ugit-ops /lib/libacl.so.1 /lib/ COPY --from=ugit-ops /lib/libattr.so.1 /lib/ COPY --from=ugit-ops /lib/libc.musl-* /lib/ COPY --from=ugit-ops /lib/ld-musl-* /lib/ COPY --from=ugit-ops /lib/libutmps.so.0.1 /lib/ COPY --from=ugit-ops /lib/libskarnet.so.2.13 /lib/ COPY --from=ugit-ops /lib/libz.so.1 /lib/ Notice, that we are copying libc.musl-* and ld-musl-* from the builder image. This is because the build for these libs depends on the architecture of the host machine. Shebangs #! are useless If you look at the very first line of ugit, you’ll see a shebang #!/usr/bin/env bash. The use of env is considered a good practice when writing shell scripts, used to tell the OS which shell interpreter to use to run the script, this is ideal in an everyday dev machine. Linux (and older versions of macOS) get shipped with sh, bash, and on top of it, folks install zsh etc. But since using shebangs is optional, and we already copy the bash binary, we just need to invoke our script using it. This saves us a couple of bytes in the image size as well. Close to 1.9 MB to be precise. # Run ugit when the container launches CMD [\"/bin/bash\", \"/bin/ugit\"] # or # ENTRYPOINT [\"/bin/bash\", \"/bin/ugit\"] A look at terminfo db ugit has colors, thanks to tput. We load up a bare-bones Alpine image with bash and head over to the /etc/terminfo directory. This directory contains the terminal info database. 37a1a77f70ed:/app# cd /etc/terminfo/ 37a1a77f70ed:/etc/terminfo# ls a d g k l p r s t v x Each of these letter-based “directories” represents different $TERM types. For example, xterm is a terminal type. If you run tput -T xterm colors on your local machine, you’ll get the number of colors your terminal supports. For xterm it should be 8, and in the case of xterm-256color it should be 256. Now here’s our chance, to only support 1 terminal type amongst the 40+ that are present in the terminfo db. We can get rid of the rest of the terminal types. This saves us another 97Kb, very little but needed to clear up the clutter. # copy terminfo database for only xterm-256color COPY --from=ugit-ops /etc/terminfo/x/xterm-256color /usr/share/terminfo/x/ # Gib me all the colors ENV TERM=xterm-256color Everything needed to run ugit The final Docker image sits at 17.6 MB with no security vulnerabilities (as reported by docker scout, at the time of writing this article). We have successfully reduced the image size by 40% compare to our first attempt. Here’s the final Dockerfile: FROM alpine:3.18 as ugit-ops RUN apk add --no-cache \\ bash \\ coreutils \\ git \\ ncurses \\ curl # Download fzf binary from GitHub, pin to 0.46.0, ugit requires minimum 0.21.0 RUN curl -L -o fzf.tar.gz https://github.com/junegunn/fzf/releases/download/0.46.0/fzf-0.46.0-linux_amd64.tar.gz && \\ tar -xzf fzf.tar.gz && \\ mv fzf /usr/bin/ # Copy only the ugit script into the container at /app COPY ugit . # Set permissions and move the script to path RUN chmod +x ugit && mv ugit /usr/bin/ # Second stage: Copy only necessary binaries and their dependencies FROM scratch COPY --from=ugit-ops /usr/bin/ugit /bin/ COPY --from=ugit-ops /usr/bin/git /usr/bin/ COPY --from=ugit-ops /usr/bin/fzf /usr/bin/ COPY --from=ugit-ops /usr/bin/tput /usr/bin/ COPY --from=ugit-ops /usr/bin/nl /usr/bin/ COPY --from=ugit-ops /usr/bin/awk /usr/bin/ COPY --from=ugit-ops /usr/bin/xargs /usr/bin/ COPY --from=ugit-ops /usr/bin/cut /usr/bin/cut COPY --from=ugit-ops /usr/bin/tr /usr/bin/tr COPY --from=ugit-ops /bin/bash /bin/ COPY --from=ugit-ops /bin/sh /bin/ # copy lib files COPY --from=ugit-ops /usr/lib/libncursesw.so.6 /usr/lib/ COPY --from=ugit-ops /usr/lib/libncursesw.so.6.4 /usr/lib/ COPY --from=ugit-ops /usr/lib/libpcre* /usr/lib/ COPY --from=ugit-ops /usr/lib/libreadline* /usr/lib/ COPY --from=ugit-ops /lib/libacl.so.1 /lib/ COPY --from=ugit-ops /lib/libattr.so.1 /lib/ COPY --from=ugit-ops /lib/libc.musl-* /lib/ COPY --from=ugit-ops /lib/ld-musl-* /lib/ COPY --from=ugit-ops /lib/libutmps.so.0.1 /lib/ COPY --from=ugit-ops /lib/libskarnet.so.2.13 /lib/ COPY --from=ugit-ops /lib/libz.so.1 /lib/ # copy terminfo database COPY --from=ugit-ops /etc/terminfo/x/xterm-256color /usr/share/terminfo/x/ # Gib me all the colors ENV TERM=xterm-256color WORKDIR /app # Run ugit when the container launches CMD [\"/bin/bash\", \"/bin/ugit\"] I decided to pin the version of fzf to 0.46.0 (the latest at the time of writing this article) because ugit requires a minimum 0.21.0 to run, and I figured what the heck, let’s pin it to the latest version. ℹ docker run --rm -it -v $(pwd):/app ugit will run the ugit container. Make sure your current directory is a git repo. This is how the final file system tree looks like:Permission UID:GID Size Filetreedrwxr-xr-x 0:0 0 B ├── appdrwxr-xr-x 0:0 886 kB ├── bin-rwxr-xr-x 0:0 866 kB │ ├── bash-rwxr-xr-x 0:0 21 kB │ └── ugitdrwxr-xr-x 0:0 1.9 MB ├── lib-rwxr-xr-x 0:0 658 kB │ ├── ld-musl-aarch64.so.1-rwxr-xr-x 0:0 67 kB │ ├── libacl.so.1-rwxr-xr-x 0:0 67 kB │ ├── libattr.so.1-rwxr-xr-x 0:0 658 kB │ ├── libc.musl-aarch64.so.1-rwxr-xr-x 0:0 265 kB │ ├── libskarnet.so.2.13-rwxr-xr-x 0:0 67 kB │ ├── libutmps.so.0.1-rwxr-xr-x 0:0 133 kB │ └── libz.so.1drwxr-xr-x 0:0 15 MB └── usrdrwxr-xr-x 0:0 12 MB ├── bin-rwxr-xr-x 0:0 919 kB │ ├── awk-rwxr-xr-x 0:0 1.2 MB │ ├── cut-rwxr-xr-x 502:20 3.6 MB │ ├── fzf-rwxr-xr-x 0:0 3.0 MB │ ├── git-rwxr-xr-x 0:0 1.2 MB │ ├── nl-rwxr-xr-x 0:0 67 kB │ ├── tput-rwxr-xr-x 0:0 1.2 MB │ ├── tr-rwxr-xr-x 0:0 919 kB │ └── xargsdrwxr-xr-x 0:0 2.8 MB ├── lib-rwxr-xr-x 0:0 396 kB │ ├── libncursesw.so.6-rwxr-xr-x 0:0 396 kB │ ├── libncursesw.so.6.4-rwxr-xr-x 0:0 592 kB │ ├── libpcre2-8.so.0-rwxr-xr-x 0:0 592 kB │ ├── libpcre2-8.so.0.11.2-rwxr-xr-x 0:0 67 kB │ ├── libpcre2-posix.so.3-rwxr-xr-x 0:0 67 kB │ ├── libpcre2-posix.so.3.0.4-rwxr-xr-x 0:0 351 kB │ ├── libreadline.so.8-rwxr-xr-x 0:0 351 kB │ └── libreadline.so.8.2drwxr-xr-x 0:0 4.0 kB └── sharedrwxr-xr-x 0:0 4.0 kB └── terminfodrwxr-xr-x 0:0 4.0 kB └── x-rw-r--r-- 0:0 4.0 kB └── xterm-256color view raw ugit-file-system-tree.txt hosted with ❤ by GitHub This is everything to make our shell app work. No more, no less. Time for a beer? 🍺 PS: The final docker image came down to 16.2 MB. You can find the updated Dockerfile here. For the sake of this article, I kept the image size at 17.6 MB. Could we reduce the size further? Yes, but there are 2 reasons why I didn’t go any further: Reason 1: Pin minimum version of fzf? At the time of writing ugit, the script relied on fzf minimum version 0.20.0, it’s granted that the latest version is going to be larger than the minimum required version. So we should pin the old version, right? No. because then it introduces security vulnerabilities with fzf’s dependencies, i.e., Golang. As reported by docker scout quickview, the older version of Golang has a total of 66 security issues. Maybe they affect the image, maybe they don’t. But I am not taking that risk, I want to keep the image as clean as possible. ℹ In the Alpine ecosystem, it is generally not advised to pin minimum versions of packages. Reason 2: Use the latest bash features? At the time of writing ugit, I relied on bash version 4.0. Both tr and cut could be replaced, if I shifted to a newer version of bash. i.e, 5.0. And that my friends is a breaking change. Getting rid of and would have saved me a couple of bytes, but I didn’t want to break the script for folks who are still on bash 4.0. It doesn’t matter if I am the only one left, my machine still has bash. You didn’t try docker-slim? I did, it did slim down my image, but it also broke the script with missing dependencies. Slim is great, the reader should check it out. Unfortunately, I couldn’t get it to work for ugit in my limited time. Moreover, I wanted to learn how to do it with my own hands, rather than rely on a tool to do it for me. You didn’t try docker-squash? I did, and the size optimization was nearly negligible. Here’s a log of the squash process which I ran on a Linux (AMD) machine (ignore the size of the image, since we are on different architecture, the image size is different):2024-01-26 19:53:30,292 root INFO docker-squash version 1.1.0, Docker 20.10.22, API 1.41...2024-01-26 19:53:30,293 root INFO Using v2 image format2024-01-26 19:53:30,310 root INFO Old image has 30 layers2024-01-26 19:53:30,310 root INFO Checking if squashing is necessary...2024-01-26 19:53:30,310 root INFO Attempting to squash last 30 layers...2024-01-26 19:53:30,310 root INFO Saving image sha256:1b9107b09dec50bd8134a935ad58ef07deb7dc2a639804abc2cb3d47c1e74206 to /tmp/docker-squash-20kl1kbx/old directory...2024-01-26 19:53:30,536 root INFO Image saved!2024-01-26 19:53:30,537 root INFO Squashing image 'ugit:latest'...2024-01-26 19:53:30,538 root INFO Starting squashing...2024-01-26 19:53:30,538 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/3b03ba6aad8ab3b95e5ae7c173b5fe8980a4bb2b57e6e3c78d4c851e62befacd/layer.tar'...2024-01-26 19:53:30,539 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/af8614dc670d37bc2b4ee235fba1fba32f79cc665e1685d17bc09103206add88/layer.tar'...2024-01-26 19:53:30,541 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/fdfcf6e294b868ac5c9bb79f479da7365f074a24932d35cf88e6ee87313be39f/layer.tar'...2024-01-26 19:53:30,542 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/fea7bea22fc0e61f8b3bb33795946844efca8e6e2ece2d4cfb5d9d5dc50fa29c/layer.tar'...2024-01-26 19:53:30,543 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/c6b22f335e663d959cf9ecb676d297de3b818c608c10811f857fbd0950d39126/layer.tar'...2024-01-26 19:53:30,543 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/482d23995758cbffbedc338a3be0325106fc6172a7c54d23a22e8a1d585d2b9e/layer.tar'...2024-01-26 19:53:30,545 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/3b9a86d49326da225c5b1c85e01764d210c15bec579c5af34f0fc171127f5b5f/layer.tar'...2024-01-26 19:53:30,546 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/0ca82f27b278a4c9bb0bfe1ff3f38716051fb4849374eec8925f48efe436009b/layer.tar'...2024-01-26 19:53:30,546 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/3cca8b35bff33c3f212c956b4c71dd419bd86c146ec38afdcdd5a0288d1659b6/layer.tar'...2024-01-26 19:53:30,547 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/81dfb23402548ba22dff2162d2c4291d16e3553a3fc47b761d21013316571a82/layer.tar'...2024-01-26 19:53:30,548 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/273d7b9c894ca049315a77ce845423f819ee684544f171eaf6b8689c77f6ad05/layer.tar'...2024-01-26 19:53:30,550 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/27be6a1ed5a8507dc43bd30cb56ddddc9c0c9d52b6806eec8037f1fa09395552/layer.tar'...2024-01-26 19:53:30,551 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/d5f577b8ef6ec1875b1d986ca79da0339c1d228cc31bce4d42988556135c5909/layer.tar'...2024-01-26 19:53:30,551 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/f292c627dcee37256b008f33c3bc91c65463e7b8a6758306c130f53d80b045e5/layer.tar'...2024-01-26 19:53:30,553 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/41a4f2afaf2abd9098d62d9b4d5c6ee88c2d9478f48be9c8f407a81c7b207d6e/layer.tar'...2024-01-26 19:53:30,555 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/5dc1b8c25faa63c58a7905850043c14c59dced4746d7d34453edad7b86956849/layer.tar'...2024-01-26 19:53:30,557 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/f91a2a9df95e299850a3c512d292167e12f7df86dfc6a598279d35db4d804b7d/layer.tar'...2024-01-26 19:53:30,560 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/72447920860e9e88f3680623d2ac1d10f78111f16c3563b43009852d704d7515/layer.tar'...2024-01-26 19:53:30,563 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/cc9b36b470edfc560a35e67fa7d3adbcf3fe91acfaf82cd484080418a1168c7f/layer.tar'...2024-01-26 19:53:30,566 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/05a8e77bfb6dd759bacb2ba9c1505bc66979885591d623d83ce58cbe0047a539/layer.tar'...2024-01-26 19:53:30,568 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/a3452a5b414ce5be1c02a8d88374419f8de637552486ca47d6d810adc351386e/layer.tar'...2024-01-26 19:53:30,570 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/38af27767b34ce2676958ed305d78508fe745597b548d65b4a5532481ffd3296/layer.tar'...2024-01-26 19:53:30,570 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/c2e188390cb6b51470e879b62e2686add024114bcb76eb163053382cb424928a/layer.tar'...2024-01-26 19:53:30,574 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/d03cc62007f0fa8d1c09d0f34f5e74e0e6f65f48c9a53177b189c57f47be8074/layer.tar'...2024-01-26 19:53:30,578 root INFO Squashing file '/tmp/docker-squash-20kl1kbx/old/fa90033ac9d3c9085ba57fc406aed549a0dc14823c8f63f6358c9557b8f0a9ea/layer.tar'...2024-01-26 19:53:30,578 root INFO Squashing finished!2024-01-26 19:53:30,608 root INFO Original image size: 17.34 MB2024-01-26 19:53:30,608 root INFO Squashed image size: 17.29 MB2024-01-26 19:53:30,608 root INFO Image size decreased by 0.32 %2024-01-26 19:53:30,608 root INFO New squashed image ID is 2c697df9cb2d9fd5f4534e6358d68f1e1a04bb5b6d27087f894dbf492964b7912024-01-26 19:53:30,741 root INFO Image registered in Docker daemon as ugit-squashed:squashed2024-01-26 19:53:30,746 root INFO Done view raw docker-squash-ugit.txt hosted with ❤ by GitHub Learnings Linux is awesome. Everything, every design decision, every tool inspiration that came out of it inherits the same awesomeness. Never shy away from going into deep 🐇 rabbit holes of micro-optimizations. You learn a lot. There’s always something new to learn. Acknowledgements Big thanks to the authors of ldd, and everyone in the docker & alpine linux community. Thanks to folks on the developersIndia discord for helping out with advice & suggestions. dive for helping me visualize the image layers. Resources TLDP.org - Shared Libraries Dockerfile best practices About docker storage drivers I hope you guys got something interesting to read today. Until next time, happy hacking & before you go please give ugit a star & a docker pull? 🧡 Read what the community is saying on hackernews & reddit.",
    "commentLink": "https://news.ycombinator.com/item?id=39240471",
    "commentBody": "A lesson in dockerizing shell scripts (bhupesh.me)194 points by bhupesh 19 hours agohidepastfavorite103 comments zdw 18 hours agoAre \"Random shell scripts from the internet\" categorically worse than \"random docker images from the internet\"? With the shell script, you can literally read it in an editor to make sure it isn't doing anything that weird. A single pass through shellcheck would likely tell you if it's doing anything that is too weird/wrong in terms of structure. Auditing a docker container is way more difficult/complex. \"Dockerize all the things\", especially in cases when the prereqs aren't too weird, seems like it wastes space, and also is harder to maintain - if any of the included components has a security patch, it's rebuild the container time... reply swozey 17 hours agoparentIf you want an example of how little importance vetting oci images is to most ops/infra teams I have a great example- I used to work on low level k8s multitenant networking stuff, think cdns. Most of them use something like multus to split up vfio paths between tenants. Think chopping your NIC into 24 private channels and each channel is one customer. The ENTIRE path has to be private, the container starts and claims that network path on the physical NIC. No network packet can ever be accessed by another channel, server or container. I was alpha-testing multus which controls this network pathing that every customer would take ingress and egress out of a cluster and put up some test containers on dockerhub. Multus sits at the demarc line between the container and the NIC channel. I'm not saying it's possible or ever been done but if I were going to set up a traffic mirror somewhere it'd logically have to be there or after the NIC.. I wrote it 5 years ago. I have no idea what version of multus it's running but even today it's getting pulls, last pull 19 days ago. Overall pulls over 5 years is over 10k. These containers would spin up every time a container starts on k8s that attaches an ovf interface. So, it's pretty much guaranteed that this is in use somewhere in someones scaling infra. I don't know if I SHOULD delete the image and potentially take down someones infra or just let them keep chugging at it. I'm not paying for dockerhub. https://hub.docker.com/repository/docker/swozey/multus/gener... edit: Looks like it's installing the latest multus package so not AS terrible but .. multus is not something to play loose with versioning.. Also I really wish Dockerhub gave you more stats/analytics. It really means nothing in the end but I'm curious. They don't even tell you the number beyond 10k, it just says 10k+ downloads. https://github.com/k8snetworkplumbingwg/multus-cni reply buffet_overflow 13 hours agorootparentSomething like this would show up in perimeter network/firewall logs correct? But if someone was mirroring traffic to the same cloud provider you deploy in, it would be less obvious to find out _which_ set of cloud IPs aren't actually your own. reply tryauuum 12 hours agorootparentassuming you have both perimeter logs and a system which notifies a human if something is weird in logs. Do big clouds have a solution for this? I don't usually use GCP / AWS so I don't know what they have reply beeboobaa 15 hours agoparentprev> Auditing a docker container is way more difficult/complex. I assume you mean auditing docker images. In which case, sure. That's why you grab their dockerfile and build it yourself. Though using dive[1] it's pretty easy to inspect docker images too, as long as they extend a base image you trust. [1] https://github.com/wagoodman/dive reply iforgotpassword 14 hours agorootparent> That's why you grab their dockerfile and build it yourself. Then you still didn't audit anything. What you need to do is inspect the docker file, follow everything it pulls in and audit that, finally audit the script itself that the whole container gets built for in the first place. Whereas when you just download the script and run that directly, you only need to do the last step. reply mook 9 hours agorootparentYeah, people don't seem to actually care. The Bitnami images were quite popular, but looking inside it they all just pull random tarballs from their server, and nothing seemed to indicate where those things came from. reply beeboobaa 11 hours agorootparentprevAll of that is the same as a shell script, yes. A dockerfile is essentially just a glorified shell script installing dependencies, which you'd otherwise just be doing yourself. reply coldtea 10 hours agorootparentprev>Then you still didn't audit anything. What you need to do is inspect the docker file, follow everything it pulls in and audit that You don't need to audit anything it pulls in INSIDE the container. Who cares? Just what kind of access it gives the container to the host. reply stirfish 9 hours agorootparentThis sounds like fine a way to mine Bitcoin for someone else reply coldtea 7 hours agorootparentThe whole point is that you checked that the container gets no access to the network. Not to mention why wouldn't you let a shell script container keep running? reply tomjakubowski 9 hours agorootparentprevYou can use quotas to mitigate that risk, and monitoring to discover it. You'd be monitoring CPU usage anyway, whether or not you build your own images or write your own Dockerfiles. reply agumonkey 13 hours agorootparentprevoh dang, dive is really a nice tool, per layer diff and/or accumulated changes .. really nice reply coldtea 10 hours agoparentprev>Auditing a docker container is way more difficult/complex. As long as it doesn't have access to outside of the container, who cares? You check the dockerfile, see what access it allows, and build the container. Besides a shell script can be 100s of lines, not very fun auditing it. reply rileymat2 9 hours agorootparentWe could probably create a java applet or flash application that runs in the browser safely too! That was more snark than HN likes, but it feels like forgetting promises of the past in a dangerous way. reply coldtea 7 hours agorootparentWe do it everyday with Javascript in the browser, on, like, 10 orders of magnitude bigger frequency than we ever run Java Apples and Flash. The whole web commerce, banking, b2b, etc. depend on it. Imagine that, huh? Is that enough snark? Not to mention, if your problem is container breaking out, you have way way bigger problems that shell-script containers. reply soraminazuki 5 minutes agorootparentI think you nicely summed up why we have a huge problem with the current state of things, both in the NPM and Docker ecosystem. reply e12e 6 hours agorootparentprev> As long as it doesn't have access to outside of the container, who cares? https://snyk.io/blog/cve-2024-21626-runc-process-cwd-contain... reply flaminHotSpeedo 4 hours agorootparentprevEveryone running containers, particularly untrusted ones, should care because containers aren't a security tool and don't provide secure isolation. reply sigotirandolas 17 hours agoparentprevA script running in a container is mostly isolated from the host by default, so it can't just upload whatever SSH keys / Bitcoin wallets / other stuff you have lying around or add some payload on your ~/.bashrc unless you explicit share those files with the container. reply ReleaseCandidat 16 hours agorootparentThis is true, but we are talking about running this script on some codebase (or whatever you want to \"git undo\"). I mean \"I don't trust this script, but let's run it on our source code\" sounds a bit weird. reply sigotirandolas 16 hours agorootparentI agree, in this case it's hard to defend against a rogue script or container image, as you need to give it read-write access to your source code, so it could add a malicious payload to your source code or install a Git hook to break out of the container into your host or get some malicious source code onto your company's Git server. There are measures that could defend against this (run all your development tools inside containers, and mandatory PRs with reviews) but they are probably beyond many/most developers are willing to do security-wise. There are a lot of scenarios where I think security through isolation/containerization makes a lot of sense (e.g. for code analysis tools, end-user applications like video games, browsers, etc.) but not too much for this particular one. reply charrondev 10 hours agorootparentI’d be quite surprised to see a company not using code reviews. Nowadays I work with a pretty large CI/CD pipeline, but even when we were a small company we enforced code reviews on all changes. I’ve seen people be a lot looser with code execution though. reply cassianoleal 10 hours agorootparentprevRun a diff after running the script and it should bring up anything funny. Hopefully people won't be just running it and automatically committing and pushing without inspecting the results, right? reply remram 10 hours agorootparentIt could easily upload your source code, add a git hook that will run (out of the container) next time you commit, create .env or similar files that are git-ignored but automatically run by common tools, etc. reply zdw 17 hours agorootparentprevYes, I understand https://xkcd.com/1200/ as well. Running anything without understanding what it does it is more dangerous than trying to understand it before running it. I'm arguing for less complexity and easier auditing, instead of a series of complex layers that each add to a security story, but make the overall result much harder to audit. reply eropple 17 hours agorootparentTo move directionally in the way you describe, you probably have to make the user experience of running scripts of any kind much weirder. macOS does this to some extent by prompting via GUI if something tries to access data directories on your system (though it confuses iTerm2 for \"anything iTerm2 runs\" and that sucks), but I think people would have a lot more problems with trying to do that in a server shell. To that end, Linux namespacing is probably a better way to constrain the blast radius for most people. That's not to say it should be an either-or, but in the absence of a both-and because the userland is not set up for sufficient policing, I think Docker containers are a pretty clearly better solution. reply inetknght 5 hours agoparentprev> Are \"Random shell scripts from the internet\" categorically worse than \"random docker images from the internet\"? > With the shell script, you can literally read it in an ... ... https://shellcheck.net. Can't do that if all of the work is hidden in a Dockerfile's RUN statement. I commit shell scripts in shell script files, and the Dockerfile just runs that shell script. Then the shell script can be version controlled for purpose, static analyzed, and parsed in an IDE with a plugin supporting a shell script language server. reply amcpu 17 hours agoparentprevThe dive utility helps tremendously for exploring the filesystem contents of a container image. Combine that with the output of `docker inspect` to look at the metadata and you should be able to have a good understanding of what it will do when running as a container. reply zdw 17 hours agorootparentEvaluating the whole contents of a filesystem is significantly more complex than evaluating one shell script. reply photonthug 17 hours agoparentprev> Are \"Random shell scripts from the internet\" categorically worse than \"random docker images from the internet\"? Yes, because inspection aside, at least with a docker invocation you can specify the volumes reply nopurpose 17 hours agorootparenthttps://github.com/containers/bubblewrap allows specifying volumes for scripts too reply bayindirh 11 hours agorootparentprevDocker is just a glorified cgroup plus wrappers. You can isolate any process like that, even a shell script. chrooting the unknown script is being 90% there. reply zdw 17 hours agorootparentprevDoes anyone in practical invocation specify the volumes? Or would they wrap it in yet another shell script that calls docker with a set of options, or a compose file, etc? This quickly turns into complexity stacked on complexity... reply msm_ 14 hours agorootparentYes I run: sudo docker run -it -v (pwd):(pwd) my_dev_image many times every day, to create a development enviromnent in CWD. My_dev_image is a debian-based image with common developer utilities (pip, npm, common packages installed). I don't feel comfortable installing random packages from the internet on my host machine, so I use docker for everything. reply yjftsjthsd-h 17 hours agorootparentprev> Does anyone in practical invocation specify the volumes? First: yes, I have run docker with -v recently. Second: > Or would they wrap it in yet another shell script that calls docker with a set of options, or a compose file, etc? > This quickly turns into complexity stacked on complexity... I agree that it can get out of hand, but a Dockerfile, a compose file, and whatever is going inside the container can be an entirely reasonable set of files to have so long as you stick with that and are reasonable about what goes in each. Where to put it differently, I think it's okay because they actually are separation of concerns. reply galleywest200 18 hours agoparentprevReading the Dockerfile should tell you what was done to create the image. If you have trust issues around the \"base\" images such as Debian or Fedora that is a different set of inquiries. As for patching, you can tell your Dockerfile to always pull the latest versions of the items you are most concerned about. At that point rebuilding the container is as simple as deleting it with \"docker container stop&& docker container rm \" and then run your docker-compose command again. reply zdw 17 hours agorootparentDoes anyone read/diff the build commands every time they get a new `latest` docker image? There would already be implicit trust in whatever the local OS's package manager laid down, and trying to add another set of hard to audit binaries on top is not really an improvement. reply 2OEH8eoCRo0 13 hours agoparentprevI never use containers from the web unless they're created be the company or developer themselves. If they don't produce one then I build my own. reply gbN025tt2Z1E2E4 19 hours agoprevI can appreciate the work to shrink the image, but copying the various standardized CLI tools and related library files into the image versus installing them with APK can introduce _many_ compatibility challenges down the road as new base Alpine versions are released which can be difficult to detect if they don't immediately generate total build errors. Using static binary versions of the various CLI tools would be a better approach here, which inevitably means larger base binaries to begin with, again ballooning the docker image size... all for a minimal gain of 14MB overall is not worth it for a production build unless you're working in the most minimal of minimal embedded OS environments, which the inclusion of FZF -and- findutils would already seem to negate since there is so much duplication in functionality between the two tools already. Overall this approach results in an image so fragile I would never use the resulting product in a high-priority production environment or even just my local dev environment as I want to code in it, not have to fix numerous compatibility issues in my tools all over 14MB of space. reply bhupesh 18 hours agoparentAuthor here > copying the various standardized CLI tools and related library files into the image versus installing them with APK can introduce _many_ compatibility challenges down the road as new base Alpine versions are released which can be difficult to detect if they don't immediately generate total build errors I'm maybe missing some context here, so you are saying that the default location of these binaries can change (the one's that get copied directly)? Or is it about the shared libraries getting updated and the tools depending on these libraries will eventually break? reply mrweasel 18 hours agorootparent> so you are saying that the default location of these binaries can change They could, Debian is in the process of unifying the bin directories, see: https://wiki.debian.org/UsrMerge Realistically it's not much of an issue. Given that you start out with a 31.4 MB image, I don't honestly think the introduced complexity in your build is worth the it. It's a good lesson, for people would doesn't know about build images and ships an entire build pipeline in their Docker image, for a bash script and aThat is _insanely_ shortsighted. Relax. While I wouldn't recommend OPs approach either. But you're not particularly right either. Exodus clearly states: > Exodus is a tool that makes it easy to successfully relocate Linux ELF binaries from one system to another... Server-oriented distributions tend to have more limited and outdated packages than desktop distributions, so it's fairly common that one might have a piece of software installed on their laptop that they can't easily install on a remote machine. Exodus is specifically designed for moving between different systems. He is largely moving from the same base image. In the article base layer is `alpine:3.18` and the target image is `alpine:3.18` and in the latter part of the article `scratch` (less to zero conflict surface). One would assume those two would be coupled. There are other technical merits to not doing what he's doing but you haven't listed any and dismissed his work. I'd venture if you actually knew what you're talking about you'd have better things to add to this conversation than \"OS package managers handle dependencies for a reason.\" Perhaps next time give some feedback that would help the writer get closer to a well-working exodus like solution. It's hackernews, \"dont roll your own\" discouragement should be frowned upon. reply gbN025tt2Z1E2E4 12 hours agorootparentWe see it differently. Exodus is useful in this capacity as much as any other, similar base os image or not for preventing overwriting. reply TJSomething 12 hours agorootparentOverwriting what? The destination's a completely empty root. reply c0l0 18 hours agoprev[...] COPY --from=ugit-ops /usr/bin/tr /usr/bin/tr COPY --from=ugit-ops /bin/bash /bin/ COPY --from=ugit-ops /bin/sh /bin/ # copy lib files COPY --from=ugit-ops /usr/lib/libncursesw.so.6 /usr/lib/ COPY --from=ugit-ops /usr/lib/libncursesw.so.6.4 /usr/lib/ COPY --from=ugit-ops /usr/lib/libpcre* /usr/lib/ COPY --from=ugit-ops /usr/lib/libreadline* /usr/lib/ [...] For me, insane sh*t like this proves that those who do not learn from distribution and package management infrastructure engineering history are condemned to reinvent it, poorly. reply bhupesh 18 hours agoparentHey author here. I understand that you might have some context about package managers that I am missing. Would genuinely like some resources about your comment or maybe a bit of explanation. Thanks reply c0l0 18 hours agorootparentHey there Bhupesh - apologies for the snark! I was just venting some of the frustration I feel every day with modern \"devops\" tooling ;) I am in a bit of a rush right now (which is why I try my absolute best to keep procrastinating on HN at the the absolute minimum, I swear! ;)), but I will try to share some insight later (potentially as a comment on your blog). reply codethief 17 hours agorootparentI'd be interested in this, too, so I'd be grateful if you could notify us here, wherever you end up posting your comment! reply bhupesh 18 hours agorootparentprevThanks, appreciate the help! reply gbN025tt2Z1E2E4 14 hours agorootparentprevI explained a bit here in my reply to your other comment: https://news.ycombinator.com/item?id=39243450 reply tiziano88 17 hours agorootparentprevIt may be worth looking at Nix if you haven't already reply crabique 8 hours agoparentprevAt this point, why not just build statically linked binaries of those utils in the build stage and just copy them over? Or even better, have a static build of busybox with all of the non-dependencies disabled. Those filename bytes add up fast! On unrelated note, --chmod parameter of the COPY instruction provides a way to avoid additional layers just to set the executable bits: # instead of COPY ugit . RUN chmod +x ugit && mv ugit /usr/local/bin/ COPY --from=builder /usr/local/bin/ugit /usr/bin/ # could just be this COPY ugit --chmod=755 /usr/bin/ In all seriousness though, that Dockerfile is basically one big uglified red flag, please don't do this, people. reply SOLAR_FIELDS 18 hours agoprevDive is a great tool for debugging this. I like image reduction work just because it gives me a chance to play with Dive: https://github.com/wagoodman/dive One easy low hanging fruit I see a LOT for ballooning image sizes is people including the kitchen sink SDK/CLI for their cloud provider (like AWS or GCP), when they really only need 1/100 of that. The full versions of both of these tools are several hundred mb each reply gkfasdfasdf 9 hours agoparentdive is amazing. One thing I realized by using the tool is that chmoding a file from an earlier layer results in that file (in it's entirety) getting added to the current layer. E.g. the below Dockerfile creates an alpine image that is around double the size of the original alpine image: FROM alpine RUN chmod -R a+rwx bin dev home lib media mnt opt root run sbin srv tmp usr var reply bhupesh 18 hours agoparentprevCan vouch for dive, the final system tree was generated by dive (should have acknowledged it, my bad) reply bloopernova 18 hours agoparentprevDo you have a link to a recommended guide to slimming down the cloud provider tools? reply codethief 17 hours agoprev> In the Alpine ecosystem, it is generally not advised to pin minimum versions of packages. I think it would be more accurate to say, in the Alpine ecosystem, it is generally not advised to pin versions of packages at all. Actually, this is not so much a recommendation as it is a statement of impossibility: You can't pin package versions (without your Docker builds starting to fail in a week or two), period. In other words: Don't use Alpine if you want reproducible (easily cacheable) Docker builds. I had to learn this the hard way: - There is no way to pin the apk package sources (\"cache\"), like you can on Debian (snapshot.debian.org) and Ubuntu (snapshot.ubuntu.com). The package cache tarball that apk downloads will disappear from pkgs.alpinelinux.org again in a few weeks. - Even if you managed to pin the sources (e.g. by committing the tarball to git as opposed to pinning its URL), or if you decided to pin the package versions individually, package versions that are up-to-date today will likely disappear from pkgs.alpinelinux.org in a few weeks. - Many images that build upon Alpine (e.g. nginx) don't pin the base image's patch version, so you get another source of entropy in your builds from that alone. Personally, I'm very excited about snapshot images like https://hub.docker.com/r/debian/snapshot where all package versions and the package sources are pinned. All I, as the downstream consumer, will have to do in order to stay up-to-date (and patch upstream vulnerabilities) is bump the snapshot date string on a regular basis. Unfortunately, the images don't seem quite ready for consumption yet (they are only published once a month) but see the discussion on https://github.com/docker-library/official-images/issues/160... for a promising step in this direction. reply bhupesh 16 hours agoparent> I think it would be more accurate to say, in the Alpine ecosystem, it is generally not advised to pin versions of packages at all. Actually, this is not so much a recommendation as it is a statement of impossibility: You can't pin package versions (without your Docker builds starting to fail in a week or two), period. In other words: Don't use Alpine if you want reproducible (easily cacheable) Docker builds. Agreed, should have been clear with my sentiment there. Thanks for stating this :) > Personally, I'm very excited about snapshot images like https://hub.docker.com/r/debian/snapshot where all package versions and the package sources are pinned. All I, as the downstream consumer, will have to do in order to stay up-to-date (and patch upstream vulnerabilities) is bump the snapshot date string on a regular basis. This is really helpful, thanks for sharing. Looks like it will be a good change, fingers crossed. reply nunez 12 hours agoprevI love reducing Docker images to their smallest forms. It's great for security (minimizes the bill of materials and makes it easier to update at-risk libraries and such), makes developers really think about what their application absolutely needs to do what it needs to do (again, great for security), and greatly improves startup performance (because they are smaller). We can definitely go smaller than 20MB and six layers. Here's a solution that compresses everything into a single 8.7MB layer using tar and an intermediate staging stage: https://gist.github.com/carlosonunez/b6af15062661bf9dfcb8688... Remember, every layer needs to be pulled individually and Docker will only pull a handful of layers at a time. Having everything in a single layer takes advantage of TCP scaling windows to receive the file as quickly as the pipe can send it (and you can receive it) and requires only one TCP session handshakes instead of _n_ of them. This is important when working within low-bandwidth or flappy networks. That said, in a real-world scenario where I care about readability and maintainability, I'd either write this in Go with gzip-tar compression in the middle (single statically-compiled binaries for the win!) or I'd just use Busybox (~5MB base image) and copy what's missing into it since that base image ships with libc. reply bhupesh 3 hours agoparent> Here's a solution that compresses everything into a single 8.7MB layer using tar and an intermediate staging stage: https://gist.github.com/carlosonunez/b6af15062661bf9dfcb8688... Hey this looks interesting, will try it out. Thanks for writing it! > That said, in a real-world scenario where I care about readability and maintainability, I'd either write this in Go with gzip-tar compression in the middle (single statically-compiled binaries for the win!) or I'd just use Busybox (~5MB base image) and copy what's missing into it since that base image ships with libc. Agreed, rewriting was not the option (as mentioned in the beginning). Moreover, It would have taken longer to build a nice TUI interface then it took to dockerize it. reply Too 2 hours agoprevOne more factor in this is that most developers already have the alpine:latest layer downloaded through some other image. In the best case, that first layer will be reused. Meaning that creating a different base layer may actually increase the size in the end, even if the image in isolation may appear smaller! reply ilaksh 17 hours agoprevHow would I use this? Say I just made a bad commit in my terminal. How would I run this container to fix it? The container doesn't have my working directory does it? Or is that the idea, to mount a volume with the working for or something? In that case, maybe it could be helpful, but to make it convenient, don't I need a script that stays in my main system and invokes the docker run command for me? So if you do that and just give me a one liner install command to copy paste then I guess this actually makes sense. A small docker container could eliminate a lot of potential gotchas with trying to install dependencies in arbitrary environments. Except it's a bash script. I guess it would make more sense to get rid of the dependency on fzf or something nonstandard. Then they can just install your bash script. For cases where you have more dependencies that really can't be eliminated then this would make more sense to me. Why does it need fzf? Is it intended to run the container interactively? reply bhupesh 17 hours agoparent> How would I use this? Say I just made a bad commit in my terminal. How would I run this container to fix it? The container doesn't have my working directory does it? Or is that the idea, to mount a volume with the working for or something? You can refer to usage guidelines on dockerhub https://hub.docker.com/r/bhupeshimself/ugit > So if you do that and just give me a one liner install command to copy paste then I guess this actually makes sense. A small docker container could eliminate a lot of potential gotchas with trying to install dependencies in arbitrary environments. Yes, that was also an internal motivation behind doing this. > Why does it need fzf? Is it intended to run the container interactively? Hey fzf is required by ugit (the script) itself. I didnt want to rely on cli arguments to give ability to users undo command per a matching git command. Adding a fuzzy search utility makes it easier for people to search what they can undo about \"git tag\" for example. reply otteromkram 14 hours agoparentprevIt's not that hard to undo a git commit. I don't see what value the author's side project is bringing other than adding complexity to a simple task (or, more likely, bolstering their resume). reply politelemon 18 hours agoprevThanks for sharing this. I like what the author did, they pursued a goal and kept working at it, until they found a balancing point. I think my experience in similar pursuits would have led me to stop very early on - 31.4 MB is already pretty good, to be fair. Looking at the amount of potential maintenance required in the future, for example if the original ugit tool starts to need more dependencies which then have to be wrangled and inspected, makes me think that the size I didn't reduce is worth the tradeoff. Since the dependencies can be managed with package managers, without having to think too much, and as the author says, Linux is pretty awesome about these things already. reply bhupesh 18 hours agoparentHey author here True, 31.4 MB is definitely a stopping point. But my the nerd inside me kicked in and wanted to know what \"exactly\" is required to run ugit. It was a fun experience. reply SOLAR_FIELDS 18 hours agoparentprevIt always depends on your use case but yeah, in the world of docker images, 30 MB often feels like nothing, because gigabyte plus sizes are not at all out of the norm. To some extent it’s a design flaw of the way images and layers work but also the tool doesn’t seem to discourage the ballooning either reply citruscomputing 13 hours agoprevThis is neat :) I love going and making containers smaller and faster to build. I don't know if it's useful for alpine, but adding a --mount=type=cache argument to the RUN command that `apk add`s might shave a few seconds off rebuilds. Probably not worth it, in your case, unless you're invalidating the cached layer often (adding or removing deps, intentionally building without layer caching to ensure you have the latest packages). Hadolint is another tool worth checking out if you like spending time messing with Dockerfiles: https://github.com/hadolint/hadolint reply Cu3PO42 17 hours agoprevWhile I likely would not have made the same tradeoffs, I do relate to the desire to get the image as small as reasonably possible and commend the efforts. Going to \"FROM scratch\" is likely going to get you one of the best results possible before you start patching the application and switching out components. I find it mildly ironic, however, that bundling the dependencies of a shell script is - in some ways - the exact opposite of saving space, even if it is likely to make running your script more convenient. Unfortunately, I don't have a great alternative to offer. The obvious approach is to either let the users handle dependencies (which you can also do with ugit) or write package definitions for every major distribution. And if I were the author, I wouldn't want to do that for a small side project either. reply yjftsjthsd-h 17 hours agoparent> Unfortunately, I don't have a great alternative to offer. The obvious approach is to either let the users handle dependencies (which you can also do with ugit) or write package definitions for every major distribution. And if I were the author, I wouldn't want to do that for a small side project either. Well... There's nix. Complete packaging system, fully deterministic results, lots of features, huge number of existing packages to draw from, works on your choice of Linux distro as well as Darwin and WSL. All at the tiny cost of a little bit of your sanity and being its own very deep rabbit hole. reply Cu3PO42 15 hours agorootparentI do love Nix, and I think much more people should use it, but I don't really consider that a good alternative in the context of my original comment. I'd argue writing a Nix derivation isn't that different from writing a package definition for any one Linux distribution. It solves the distribution problem for people who use that particular distribution/tool, not everyone. Now, Nix can be installed on any distribution, but if I was going for widespread adoption, I might point to Nix being a solution, but I probably wouldn't advertise it as the main one. reply tuananh 18 hours agoprevugh, i would hate to maintain this dockerfile. i actually dont mind a 34MB docker image vs a 17MB image like this reply osigurdson 18 hours agoprev> or maybe ends up sponsoring... Sponsorship for a 500 line shell script. Wow! reply Cu3PO42 17 hours agoparentThey aren't asking for sponsorship on the tool they created. They expressed that they do not have interest in investing even more work to rewrite it in Rust, Go, or what have you; unless someone paid them to do it. And I think that is completely fair! If someone has no inherent interest in doing something, is not othewise obligated to do it, it is not done as a favor to friends or something, paying that person to do the job anyway is a very accepted practice in our society. Almost all of our employers pay us to do things we might otherwise not do. reply osigurdson 16 hours agorootparentThere is nothing wrong with extracting as much value as possible from a small effort. It just seems highly unlikely anyone would sponsor it so the request seems somewhat ridiculous. alias lsa=ls -a Sponsor me! reply raziel2p 16 hours agorootparentIs someone currently, for free, doing the work the author is suggesting sponsorship for? If not, it's not ridiculous. reply osigurdson 13 hours agorootparentWill you sponsor the maintenance of my alias command above then? I don't think anyone else is maintaining such a command. Or, is it ridiculous? reply kjkjadksj 17 hours agoprevWhats wrong with make or dare I even suggest a package manager like conda? I get having a half dozen dependencies can be specified in tools like docker but its just another way to do the same old task thats been solved a dozen ways for decades. We are sharing a shell script here. Seems crazy to me to run an entire redundant file system to share a couple hundred line bash script. Plus now users need docker skills as well as command line skills to install and run this tooling. There are corners of the command line user/programmer world that have thankfully not been polluted by docker yet so its not nearly as widespread a tool as setting up environments for bash scripts using some older ways. reply swozey 16 hours agoparentI think you're seeing this from the perspective of someone who runs a container for development and not someone who has to run a development container at hyperscale. We can't pass around bash scripts anymore. Every system has to be fungible, reproducible en masse and as agnostic to the underlying technology its on as possible. reply kjkjadksj 16 hours agorootparentYou aren’t writing machine code that can run on anything though, you have this docker dependency in order to run the container. Its just trading one dependency for another because docker is in style these days. I don’t think deploying bash scripts at scale was some insurmountable challenge before docker showed up. reply swozey 16 hours agorootparentWe don't have a \"docker dependency\" - we run OCI containers. You're equating Docker which is a tooling eco system with containers. Containers have been around for a LONG time, Solaris, jails, cgroups, etc are all built-in to the kernels we use today. You don't need to use docker. The idea is fungible services, whether it's literally just a container that starts with a go binary I can quickly scale 1000s of COMPLETELY independent processes and ORCHESTRATE THEM over thousands of clusters from one centralized system. If I need to shift 1000s of that one go binary to US-WEST-1 because US-EAST-1 is down I can run automate it or run one command based on a kubernetes tag label and shift traffic. These are just a few of the massive benefits we get with containers. I can deploy an ENTIRE datacenter with a yaml file. My ENTIRE companies infrasture MTTR (mean time to recovery) from a total outage, starting from a github repo is less than 35 minutes and we're a billion dollar company and 80% of that time is starting load balancers and clusters. The only NOT agnostic hardware stuff in any of this are the load balancers and network related things as each provider has its own apis, IAM/Policies, etc that are completely unique between providers/datacenters. Nothing cares about what ram, distro, cpu or anything else is being used, we can deploy anywhere ARM or x86. Without containers I would need a $150k F5 load balancer to distribute load between a ton of $30k dell poweredeges (and I'd need this x1000's). I've been in Infrastructure for 15+ years at massive scale, webhosts, cdns, I do NOT want to go back to not using containers ever. None of my team writes any non container code or infra. The FIRST thing we do in every single repo is make a dockerfile and docker-compose.yml to easily work on things and every single server any company has in the last decade of my SRE career we've migrated to containers and never once regretted it. reply hitpointdrew 14 hours agoprevDockerizing a shell script???? Unless your tool is converted to a service how would anyone ever use this? Do you expect them to run their project inside of your container? This is very bizarre. reply oftenwrong 13 hours agoparentIt's quite typical. You `docker run`, and specify the options to mount the work tree of the project into the container. reply mhitza 18 hours agoprevI didn't see it in the final tree listing, but I would expect the fzf.tar.gz to linger around after extraction as it was never removed. If that is so, should help squeeze a few more bytes out of the final image. reply tuananh 18 hours agoparentit's multi-stage build. they only copy fzf bin to the final image (scratch) reply avgcorrection 13 hours agoprev> Yeah, I know, I know. REWRITE IT IN GO/RUST/MAGICLANG. The script is now more than 500+ lines of bash. These screeds get more and more random. The standard advice was always to just not let a program in Bash get beyond X lines. Then move to a real programming language. Like Python (est. 1991). reply swozey 16 hours agoprevI've been writing containers for 10+ years and this last few years I've started using supervisord as pid 1 that manages multiple processes inside the container for various things that CAN'T function as disparate microservices in the event that one fails/updated/etc a lot more. And man I love it. It's totally against the 12 microservice laws and shoudl NOT be done in most cases, but when it comes to troubleshooting- I can exec into a container anywhere restart services because supervisord sits there monitoring for the service (say mysql) to exit and will immediately restart it. And because supervisor is pid1 as long as that never dies your container doesn't die. You get the benefit of the containerization and servers without the pain of both, like having to re-image/snapshot a server once you've thoroughly broken it enough vs restarting a container. I can sit there for hours editing .conf files trying to get something to work without ever touching my dockerfile/scripts or restarting a container. I don't have to make some changes, update the entrypoint/dockerfile, push build out, get new image, deploy image, exec in.. I can sit there and restart mysql, postgres, redis, zookeeper, as much as I want until I figure out what I need done in one go and then update my scripts/dockerfiles THEN prepare the actual production infra where it is split into microservices for reliability and scaling, etc. I've written a ton of these for our QA teams so they can hop into one container and test/break/qa/upgrade/downgrade everything super quick. Doesn't give you FULL e2e but it's not we'd stop doing what tests we already do now. I mention this because it was something I did once a long long time ago but completely forgot something that you could do until I recently went that route and it really does have some useful scenarios. https://gdevillele.github.io/engine/admin/using_supervisord/ I'm also really tired of super tiny containers that are absolute nightmares to troubleshoot when you need to. I work on prod infra so I need to get something online immediately when a fire is happening and having to call debug containers or manually install packages to troubleshoot things is such a show stopper. I know they're \"attack vectors\" but I have a vetted list of aliases, bash profiles and troubleshooting tools like jq mtr etc that are installed in every non-scratch container. My containers are all standardized and have the exact same tools, logs, paths, etc. so that everyone hopping into one knows what they can do. If you're migrating your architecture to ARM64 those containers spin up SO fast that the extra 150-200mb of packages to have a sane system to work on when you have a fire burning under you is worth it. For some scale the cross datacenter/cluster/region image replication would be problematic but you SHOULD have a container caching proxy in front of EVERY cluster anyway. Or at least at the datacenter/rack. It could be a container ON your clusters with it's storage volume a singular CEPH cluster, etc. reply codethief 17 hours agoprevDoes anyone here have experience using Nix to build minimal Docker images? How well does it work, and how does it compare to the author's approach of manually copying shared libraries into a scratch image? reply SirensOfTitan 17 hours agoparentIt works quite well and you can get very minimal docker images using nix with very few tricks compared to this. …with that, building those nix images on Mac is still a bit rough—there’s some official docs and work on getting a builder VM set-up, but it’s still a bit rough around the edges. reply codethief 17 hours agoparentprevResponding to myself: I see that someone else here in this thread commented on Nix: https://news.ycombinator.com/item?id=39241768 reply chasil 18 hours agoprevI have been able to run ksh93 in an nspawn container under systemd in a tiny fraction of what is presented here. I did this by tracking the output of the ldd command and moving only needed libraries into the container. Why is docker so big? reply adrianmonk 14 hours agoprev> The use of env is considered a good practice when writing shell scripts, used to tell the OS which shell interpreter to use to run the script When using a shebang line, the reason for 'env' is actually something different. You can just leave out 'env' and do a shebang with 'bash' directly like this: #! /usr/bin/bash But the problem with that is portability. On different systems, the correct path may be /bin/bash or /usr/bin/bash. Or more unusual places like /usr/local/bin/bash. On old Solaris systems that came with ksh, bash might be somewhere under /opt with all the other optional software. But 'env' is at /usr/bin/env on most systems, and it will search $PATH to find bash for you, wherever it is. If you're defining a Docker container, presumably you know exactly where bash is going to be, so you can just put that path on the shebang line. TLDR: You don't have to have a shebang, but you can have a shebang at no cost because your shebang doesn't need an env. reply benreesman 18 hours agoprevOr just write a clean specification and get a docker image close to optimal, and if it’s not, you can prove cryptographically if by some chance you beat the defaults: https://xeiaso.net/blog/i-was-wrong-about-nix-2020-02-10/ I’ve got plenty of gripes with nixlang, but being worse than Dockerfile-lang isn’t one of them. reply Cu3PO42 17 hours agoparentYes, you can use Nix to get extremely small Docker images. I have personally used it to that effect, but it's not a magic bullet. In this specific case, it gives pretty bad results even. I have written the simplest possible Nix derivation for ugit and the resulting Docker image is 158MB gzipped. I haven't explored fully why that is, but that's much worse than even the first effort from the OP. reply jbboehr 1 minute agorootparentI got it down to about 33M, gzipped with musl (completely untested, of course)[0]. Most of it seems to be from git: $ nix path-info --closure-size --human-readable nixpkgs#legacyPackages.x86_64-linux.gitMinimal /nix/store/f7b2yl226nbikiv6sbdhmaxg2452c8h5-git-minimal-2.42.0 112.9M $ nix path-info --closure-size --human-readable nixpkgs#legacyPackages.x86_64-linux.pkgsMusl.gitMinimal /nix/store/25807yw3143a94dpr3a3rffya7vg5r24-git-minimal-2.42.0 73.5M Apparently \"gitMinimal\" it not all that minimal: $ ls /nix/store/25807yw3143a94dpr3a3rffya7vg5r24-git-minimal-2.42.0/libexec/git-core/wc -l 172 [0]: https://gist.github.com/jbboehr/3a5d0dd52a0c1139ce88b76ab82a... reply k__ 17 hours agoprevWhen FirecrackerOS?! Fly.io, deliver us. reply zilti 18 hours agoprevWhenever I think there can't be any worse of a \"use case\" to dockerize something, someone comes along and proves me wrong... For the last goddamn time: Docker is not a package manager! reply renewiltord 18 hours agoprev [–] How does removing the shebang save two megabytes? Seems like a lot. Is it the env binary? reply bhupesh 17 hours agoparent [–] Yes, the size of env closes to 2mb. I maybe wrong here, though. Seems something is wrong. I wasn't able to dig deep enough on why that was the case, considering the \"env\" utility was coming from busybox which on copy averages close to 900Kb. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author recounts their journey of optimizing the Docker image size for the ugit tool.",
      "They employ various techniques such as multi-stage builds, removing unnecessary binaries and dependencies, and experimenting with different base images.",
      "The author highlights the challenges faced with missing dependencies and vulnerability concerns, while expressing appreciation for the development community and urging readers to explore ugit."
    ],
    "commentSummary": [
      "The article and discussions focus on different aspects of using shell scripts and Docker containers.",
      "Key topics covered include auditing and securing Docker containers, the significance of code reviews for both shells scripts and containers, and the potential risks and benefits of using containers.",
      "Other areas discussed include tools for inspecting Docker images, managing package versions in Linux distributions, reducing Docker image size, advantages and trade-offs of Dockerfiles, and the use of containers for scaling and managing processes. Additionally, miscellaneous topics like container management, task sponsorship, and alternative approaches to optimizing image sizes are touched upon."
    ],
    "points": 194,
    "commentCount": 103,
    "retryCount": 0,
    "time": 1706969414
  },
  {
    "id": 39240205,
    "title": "Microsoft Seeks Rust Developers to Rewrite Core C# Code",
    "originLink": "https://www.theregister.com/2024/01/31/microsoft_seeks_rust_developers/",
    "originBody": "Software 79 Microsoft seeks Rust developers to rewrite core C# code 79 Embrace, extend, and ... port? Richard Speed Wed 31 Jan 2024 // 16:30 UTC Microsoft's adoption of Rust continues apace if a posting on the IT titan's careers website is anything to go by. Although headcount at Microsoft might currently be down – by two percent compared to the previous year – recruitment persists at the Windows giant. In this case, the company is forming a team of Rustaceans to tackle a platform move away from C#. The job, a principal software architect for Microsoft 365, has responsibilities that include \"guiding technical direction, design and implementation of Rust component libraries, SDKs, and re-implementation of existing global scale C# based services to Rust.\" According to the post, spotted by MSPowerUser, the job lurks within the Substrate App Platform group, part of the Microsoft 365 Core Platform organization. The Substrate does the heavy lifting behind the scenes for Microsoft's cloud services, making a rewrite into Rust quite a statement of intent. Microsoft said: \"We are forming a new team focused on enabling the adoption of the Rust programming language as the foundation to modernizing global scale platform services, and beyond.\" Burnout epidemic proves there's too much Rust on the gears of open source Biggest Linux kernel release ever welcomes bcachefs file system, jettisons Itanium Dump C++ and in Rust you should trust, Five Eyes agencies urge Rusty revenant Servo returns to render once more The company has had an interest in Rust for a while. In 2023, the director of OS security for Windows announced the arrival of Rust in the Windows kernel and the language has been making its way into multiple components over the years. Microsoft, alongside companies such as Meta and AWS, is a platinum member of the Rust Foundation. Considering the growing enthusiasm for memory-safe programming, something Rust delivers with far less effort than the likes of C++, Microsoft's move is unsurprising. However, the company's desire to shift existing C#-based services to Rust will raise a few eyebrows among the developer community. Memorably, a Microsoft engineer had to rapidly backpedal issue a clarification after proudly proclaiming that Office 365 was being ported to JavaScript. In this instance, while Microsoft remains committed to C#, at least in public, its actions over the last few years and the job posting are indications that the company is keeping its options open. It is also an indicator of the direction of travel for engineers mulling what new skills to pick up in an increasingly competitive world. ® Whitepaper: Top 5 Tips For Navigating Your SASE Journey Share More about Developer Microsoft Microsoft 365 More like these × More about Developer Microsoft Microsoft 365 Rust Narrower topics Active Directory API Azure Bing BSoD Excel Exchange Server Git HoloLens Internet Explorer LinkedIn Microsoft Build Microsoft Edge Microsoft Ignite Microsoft Office Microsoft Surface Microsoft Teams .NET OS/2 Outlook Patch Tuesday Pluton SharePoint Skype Software bug SQL Server Visual Studio Visual Studio Code Windows Windows 10 Windows 11 Windows 7 Windows 8 Windows Server Windows Server 2003 Windows Server 2008 Windows Server 2012 Windows Server 2013 Windows Server 2016 Windows Subsystem for Linux Windows XP Xbox Xbox 360 Broader topics Bill Gates Office 365 Programming Language Software More about Share 79 COMMENTS More about Developer Microsoft Microsoft 365 More like these × More about Developer Microsoft Microsoft 365 Rust Narrower topics Active Directory API Azure Bing BSoD Excel Exchange Server Git HoloLens Internet Explorer LinkedIn Microsoft Build Microsoft Edge Microsoft Ignite Microsoft Office Microsoft Surface Microsoft Teams .NET OS/2 Outlook Patch Tuesday Pluton SharePoint Skype Software bug SQL Server Visual Studio Visual Studio Code Windows Windows 10 Windows 11 Windows 7 Windows 8 Windows Server Windows Server 2003 Windows Server 2008 Windows Server 2012 Windows Server 2013 Windows Server 2016 Windows Subsystem for Linux Windows XP Xbox Xbox 360 Broader topics Bill Gates Office 365 Programming Language Software TIP US OFF Send us news",
    "commentLink": "https://news.ycombinator.com/item?id=39240205",
    "commentBody": "Microsoft seeks Rust developers to rewrite core C# code (theregister.com)190 points by yau8edq12i 20 hours agohidepastfavorite190 comments mmastrac 17 hours agoI love writing Rust, but I was really surprised by how difficult it was to find a job _actually_ writing Rust. I'm happy to see the increased activity in the space, but searching for a job in Rust is probably still 10x harder than C or C++. It worked out in the end, and I'm happy to be getting paid to be writing Rust every day, but I hope that the market for Rust jobs continues to grow -- ideally even faster than it has been. reply diego_moita 16 hours agoparentAgree, I am still looking for a Rust position, although I have more than 3 years of experience with it. And yes, it is very hard. It seems Rust is in a chicken/egg position: employers avoid it because \"there are few developers\", developers avoid it because \"there are few jobs\". Until recently the majority of Rust jobs were blockchain stuff. Now I see a rise in network infrastructure and security. reply godzillabrennus 16 hours agorootparentSeems like RUST needs work to become easier to learn. Hard for a business to take a risk on using it (outside of a use case absolutely needing a memory safe language) if the talent pool is shallow. reply unclad5968 14 hours agorootparentI don't care that it's hard to learn. What turns me off rust is that if I ever make a change that requires a lifetime annotation, I now have to go back through all my code potentially adding lifetime annotations to everything it touches. I was making a little toy compiler in rust and basically gave up when I wanted to make a change that would have amount to string -> string_view in c++ because in rust I now need to tell the compiler that the String to my &String is going to outlive the struct. Now I understand why it's good but Id rather just let c++ blow my feet off. Maybe once I nail down all the data structures I'll rewrite it in rust. reply pornel 12 hours agorootparentThis effect ofspreading everywhere is very frequently a novice mistake of putting temporary scope-bound loans (AKA references) in structs that aren't temporary views themselves. People used to C or C++ perspective tend to reflexively (over)use references to avoid copying, but references in Rust are to avoid owning, and not-copying is handled in different ways. BTW, apart from edgiest of edge cases, &String is a useless type in Rust, because String already stores data \"by reference\" and is never implicitly copied. For loans it's generally better to deref to &str. reply frfl 7 hours agorootparentI feel like this could be a great in depth blog post...unless such a blog post already exists? It would greatly benefit novices and those who haven't gotten to that stage where this mentality kicks in. reply tracker1 4 hours agorootparentThere's a pretty good YouTube video I saw pulling all the types of strings in rust. I find it confusing still... Been wanting to do a few things that would mean dealing with it streams that could be cp437 or utt8, generally and haven't been quite sure how I want to deal with it. reply imetatroll 8 hours agorootparentprevI am a novice when it comes to rust but as I recall the instant you put anything that isn't a scalar type you need lifetimes. Is that what the person you are replying to frustrated with? How would not copying be handled? Sorry for the basic questions. Your comment tickles something in my brain that suggests I need to know. reply pdimitar 8 hours agorootparentprev> and not-copying is handled in different ways Can you give a few examples? reply zozbot234 14 hours agorootparentprevIsn't that why Rc and Arc exist in Rust? So that you can deal with situations where you don't know the object lifecycle at compile time? reply miniupuchaty 12 hours agorootparentprevIf you're fine with dropping safety you can just make a wrapper around a raw pointer. I wouldn't do it in public library but if it's fine for your cpp code then it's fine for your rust code. You can always drop to the \"blow my feet off\" level. reply merb 12 hours agorootparentprevThe last few versions made a lot of the lifetimes elided so you can drop them. reply winrid 43 minutes agorootparentI feel like this has been this way for at least two years, too? reply pdimitar 8 hours agorootparentprevDo you have a direct link? I'd love to read more. reply tracker1 4 hours agorootparentprevIt's not that hard for relatively easy work... I've written a few basic web services, as an example, using a few of the more popular web frameworks for Rust, it hasn't been much harder than say C# or Node. Now some of the more complex things, have been far more complex to do by hand.. Shared data, channels, Arc, etc. Those have been a bit more cumbersome and still not sure I've done the right thing at times. reply wredue 15 hours agorootparentprevHonestly. Rust is not all that “difficult”. There’s just SO MUCH that you MUST know to be effective. You need to know all of the rustisms. Plus you need to have a general knowledge of memory. You need to understand how memory is being abstracted and how to work within those constraints so as not to be slow. You need to know all of the monads, what they mean, why each exists, what the nuance is, and why you might use one over another in a specific situation. It’s not difficult so much as it just takes so much time and challenging-yourself practice to try to memorize everything. A “being effective at rust” book would probably dwarf a “being effective at C++” book in pages assuming similar prose. And we all know how much of a beast C++ is. reply spoiler 15 hours agorootparent> A “being effective at rust” book would probably dwarf a “being effective at C++” book in pages assuming similar prose. And we all know how much of a beast C++ is. I've written C++ for close to a decade before switching to Rust, and this is categorically untrue. > Plus you need to have a general knowledge of memory. You need to understand how memory is being abstracted and how to work within those constraints so as not to be slow. If anything, this is more true so of C++. > You need to know all of the monads, what they mean, why each exists, what the nuance is, and why you might use one over another in a specific situation You can use a lot of stuff before needing to know their full history. You don't need to know the history behind 0, null, pointer, numbers and addresses before, how hardware works, etc to use Option, for example. You also don't need to know the (inconsistent and arcane) history of errno and magic numbers to use Result. Etc, etc. This vastly over-exaggerates Rust's complexity. Yes, it borrows concepts from functional programming, but a lot of these concepts are simpler than stuff you deal with in imperative programming. I think the issues here is that people don't tend to like to change their way of thinking, even if it's for the better. Also, Rust has ample in common with imperative/traditional language, so it's not a huge leap into some alien mental model, as you make it sound. Further, C++ makes you think way more than Rust; in Rust the compiler does a lot of the thinking for you. Not to mention, the type system is easiest to reason about than C++'s old (and mentally inefficient) template system. reply withoutboats3 15 hours agorootparentprev\"Dwarf\"? \"Dwarf\"?! I may be biased (because I am very effective at Rust and terrible at C++) but I can't believe this seems probable. std::optional, move semantics, boost, template templates, SFINAE(!!) C++ has all the expressivity of Rust and more, with far less regulation to assist the programmer in using it effectively. reply PH95VuimJjqBqy 14 hours agorootparentyou don't have to know _any_ of what you just listed in order to be effective with C++. the closest thing to required would be move semantics. reply tracker1 3 hours agorootparentDepends on what you're trying to do. The same can be said of Rust. You can write massive web services serializing days from RDBMS databases quite a bit easier in Rust than with C++ as an example. reply thfuran 11 hours agorootparentprev>It’s not difficult so much as it just takes so much time and challenging-yourself What's the difference? reply PartiallyTyped 9 hours agorootparentprevRust is very easy to learn. Just read what the compiler says. The compiler is there to help you, it has a holistic view of what's going on, and it knows what it needs for your code to be fixed. reply adamnemecek 15 hours agorootparentprevLearning Rust to the point of being productive is 100x easier than C or C++. It's not even funny. reply josephg 15 hours agorootparentThat hasn’t been my experience with it. I found rust significantly harder to learn than C; and this is after I already knew C. (At least - learn rust to the point of feeling productive, and like I wasn’t fighting the borrow checker constantly). Arguably becoming an expert in C means you need to understand all the nuances of undefined behaviour. And that’s a much harder process. But it can happen slowly. Rust preloads all the pain. You essentially can’t write rust at all until you understand rust references, lifetimes (implicit and explicit) and the borrowchecker. The payoff is huge, but I found climbing that mountain to be no joke. reply zozbot234 15 hours agorootparent> You essentially can’t write rust at all until you understand rust references, lifetimes (implicit and explicit) and the borrowchecker. You can if you're willing to use stuff like .clone() and the interior mutability types. In Rust, you can tell when code has been written to be a bit sloppy because it has that kind of boilerplate. And the compiler checks are a huge help when it comes to refactoring the code and making it cleaner and better-performing. reply josephg 12 hours agorootparentEh. If you don’t know how references work in rust, you’ll struggle to use most of the standard library or any 3rd party crates. And if you can’t pass mutable references to functions or iterators there’s a lot of programs that will be hard to write at all. Performance will be pretty rubbish too. You might be able to write some simple programs, but I wouldn’t say you know rust yet, or could really be productive with the language. reply riku_iki 15 hours agorootparentprev> I found rust significantly harder to learn than C language itself maybe, but you also need to learn ecosystem, libs, build systems, testing. Benchmark could be: how fast you can learn and bootstrap some type of app of your choice: high performance DB or torrent server, etc. > You essentially can’t write rust at all until you understand rust references, lifetimes (implicit and explicit) and the borrowchecker why is that? You can write rust the same way you write C but with nicer syntax, standard lib and build system and more potential to future expansion. reply josephg 14 hours agorootparent> but you also need to learn ecosystem, libs, build systems, testing. That’s a really good point that I hadn’t thought of. If we include header files, compiling and linking, makefiles and CMake and the mess of dealing with 3rd party libraries in C - well, yeah. All that stuff is probably worse than learning the rust borrow checker. I think I’ve forgotten how horrific that mountain is for beginners because I learned most of it decades ago, while the pain of learning rust is still fresh. > why is that? You can write rust the same way you write C Because I use pointers everywhere in my C code. Rust’s borrow checker simply won’t compile my code if I transliterate it directly from C. There are software patterns which work well in rust with the borrow checker in mind - but they take time to learn and get used to. Until you do, you simply aren’t productive. reply adamnemecek 14 hours agorootparentYeah, that's what I meant by productive. Not like learn the language but like build stuff. reply riku_iki 14 hours agorootparentprev> Because I use pointers everywhere in my C code. Rust’s borrow checker simply won’t compile my code if I transliterate it directly from C. borrow checker is more relevant to C references I think. For pointers you can use Arc or Rc and don't worry about borrow checker. Disclaimer, I am not a rust or c coder, so my opinion has a high chance to be wrong.. reply josephg 10 hours agorootparentIf you want to learn rust, there really isn’t a way to avoid learning lifetimes and the borrow checker. It’s a core part of how the language works. You can probably get partway there with Rc and by cloning stuff everywhere but you’ll still run into borrowck problems. Rc doesn’t turn off the borrow checker. (Nor does unsafe). But then you won’t have the tools to solve your problems. And you also won’t know how to use most of the standard library or any crates. That’s no way to write rust. > Disclaimer, I am not a rust or c coder, so my opinion has a high chance to be wrong.. I’ll take a step back. I hope this is helpful to someone and not patronising. Go and similar languages are memory safe because no matter what go code you write, go runs your program with a GC that makes sure your variables are freed correctly only when it’s safe to do so. Rust’s safety is very different. It comes from checking at compile time that your program is written in a way that obeys a bunch of complex rules. For that to work, you have to write your code very carefully. If you mess up, the compiler doesn’t fix it for you. It just refuses to compile. So you have to write your code with those invariants in mind, or your program won’t build at all. C code generally doesn’t obey any of rust’s rules - for obvious reasons. If you were to just blithely translate C to rust code, the rust compiler will refuse to compile your program because the rules aren’t being followed. To write rust code that the compiler accepts, you need to first learn the borrow checker’s way of seeing the world. Then you need to restructure your code to work in accordance to rust’s rules. And sometimes that’s a super obscure and tricky problem. Weirdly, I think it’d be much easier to go the other way. Any compiling rust program could be translated into a memory safe C program if we had a compiler that did that. (I think). And lots of C programmers say learning rust made them better at C - because rust’s rules genuinely teach you to be more disciplined with memory and show you some clear rules for making C code that’s much less error prone. Not learning the borrow checker is like not learning how objects work in Python. You could write some simple programs. But you’re going to have a bad time, especially if you try to do anything nontrivial, use library code or read anyone else’s programs. reply adrianN 15 hours agorootparentprevGiven the number of CVEs that can be directly attributed to C and C++ you only need to be half-joking to argue that approximately nobody has managed to learn the languages sufficiently to write production code in them. reply josephg 15 hours agorootparentAs uncomfortable as this is to say, the existence of latent security vulnerabilities doesn’t seem to stop people from shipping a lot of apparently working code in C and C++. And I don’t think a lot of those CVEs come from people misunderstanding the languages. As I understand it they mostly come from honest software bugs. C++ makes it easy for honest mistakes by experts to become security nightmares. This is a controversial opinion - but I think the fault is in C and C++ themselves. Not the programmers who need to work even harder such that they stop ever making mistakes. reply vacuity 14 hours agorootparentWhich is why some people push for no longer using C and C++ as much as possible. It's just not feasible to expect the average programmer to avoid all the security pitfalls. Of course, whether Rust is the best option isn't a given. reply josephg 10 hours agorootparentThere aren’t a lot of memory safe options if you want the kind of performance rust offers. Rust is fine, but it’s far from perfect. It’s hard to learn, complex, and the macro and async systems are honestly a bit of a mess. I’m really looking forward to whatever comes after rust. I’m hoping some bright sparks out there manage to make a language with rust’s memory safety but that cleans up some of rust’s rough edges. reply tracker1 3 hours agorootparentTo be fair, how many applications really need Rust's level of performance? I mean operating system code and drivers... But, many apps are getting the job done in electron with a browser engine. I like rust, I like the sensibilities. Maybe my experience is too limited to higher level problems though. There's plenty of room for the likes of Go, Java and C# is all I'm getting at. reply LtWorf 12 hours agorootparentprevSo you're saying there is no CVE in php, java, go, js, and python? reply josephg 10 hours agorootparentIn their defence, there are entire classes of security bugs that memory safe languages protect you from. A large percentage of the security bugs in Chrome, OpenSSL, iMessage and lots of other apps wouldn’t have happened in any of the languages you listed. Or, almost certainly, in Rust. reply Xeamek 8 hours agorootparentAnd how many of such bugs could be prevented with compiler warnings and flags? reply josephg 1 hour agorootparentI suspect fewer than you think. At least in FAANG code. Google chrome has had a fair few CVEs over the years despite having a lot of the worlds best security researchers working full time looking for security vulnerabilities. I think someone has tried reading the compiler warnings. There’s an old story about John carmack running a new static analysis tool on the quake3 source code. The code worked well. The tool apparently found a huge list of issues in the code - including a massive pile of real bugs. Then he tried another static analysis tool and it found more real bugs. And so on. The story is well worth a read. This is a great takeaway: > This seems to imply that if you have a large enough codebase, any class of error that is syntactically legal probably exists there. C++ is really hard to do “right”, at any scale in a real team. http://www.sevangelatos.com/john-carmack-on-static-code-anal... reply tracker1 3 hours agorootparentprevIf you're going to start writing it testing or C++ like Rust or follow certain guidelines, a lot of them. But if you're going to constrain how you do things, why not use a language that also gives you higher level functionality along the way? reply edgyquant 13 hours agorootparentprevObjectively false reply FlyingSnake 11 hours agorootparentprevI know some teams that are hiring Rust devs. Please feel free to ping me if you would like to know more. reply diego_moita 5 hours agorootparentI sent you an email from diego.moita69 at gmail. Thanks for reaching out! reply TrackerFF 16 hours agoparentprevWe acquired a product last year, where the entire back-end was written in Rust. Unfortunately, Rust developers were hard to get by, and we didn't have any internally that could maintain the Rust code at such scale. The entire back-end ended up being re-written. reply beeboobaa 16 hours agorootparentWhat happened to the developers who wrote the back-end in the first place? reply cerved 11 hours agorootparentmaybe the borrow checker determined their lifetime was up reply ducktective 15 hours agorootparentprev>The entire back-end ended up being re-written. In Go, I presume? reply imetatroll 8 hours agorootparentprevI honestly wish I could have been in that boat because I crave the opportunity to use rust on the job. reply tracker1 3 hours agorootparentSame here... Job hunting right now and would love to bridge the gap to primarily using Rust reply rowanG077 15 hours agorootparentprevWhy not just learn rust? It really is not that exotic to learn. I doubt it is faster to rewrite it. reply jonnytran 15 hours agorootparentYeah, this is just baffling. A team can be so averse to learning new tools, good ones too, that they would rather dump their time into rewriting. Instead of getting paid to level up their skills, they'd rather block forward movement of the company's goals to maintain the status quo. reply kasajian 15 hours agorootparentI'm a bit surprised at this, too. reply codetrotter 16 hours agorootparentprevNooo :'( reply rockwotj 16 hours agoparentprevOn the reverse side of things, trying to find a C++ dev is usually harder than finding people who want to write Rust. reply mmastrac 16 hours agorootparentI suppose C++ devs are on the correct side of the demand curve, unlike myself. :D I was _really_ tempted to take a C++ job at some point but I held out. reply ClimaxGravely 10 hours agorootparentprevIt's been pretty great having one of my primary languages c++ lately wrt job opportunities. reply secondcoming 16 hours agorootparentprevYup, we were hiring C++ devs recently and it was quite common for candidates to mention Rust. Did they not read the job description??? Immediate red flag. reply StewardMcOy 16 hours agorootparentI've been a C++ dev for most of my career. Not looking to change jobs at the moment, and I like C++ a lot. We still occasionally start new projects in C++ where I work. I've tinkered with rust in my spare time, but never introduced it at work. I think the borrow checker is a fantastic tool, but Rust has just never been the right tool for the domain we're in. But I'm a bit confused by your statement. There's a lot of overlap in the domains that C++ and Rust serve. Isn't it a good thing for job candidates to have an interest in learning about other approaches to their work? To understand what makes C++ better or worse than Rust in difference circumstances? Why would a mere mention of Rust be a red flag? reply wredue 15 hours agorootparentIt’s a red flag because you’re going to get people bring in random languages just because, then they fuck off to another job and you’re left supporting code that nobody knows. reply fl0ki 14 hours agorootparent> bring in random languages Rust is hardly a random choice. You can debate its merits for some domains, but replacing C++ in most of the places that C++ is used is exactly what Rust was built for and is widely recognized as doing well at. > just because If your engineers are pitching a technology with no more basis than \"just because\", I can understand dismissing them out of hand. But ask yourself if that's actually what happened, or if they gave specific reasons justifying their proposal and you dismissed them out of hand anyway. Even if you disagree with the reasons, or agree with them but conclude they are outweighed by other arguments, either of those is totally fine, but dismissing them as \"just because\" is not an effective way to make technical decisions as a team. reply wredue 14 hours agorootparentThey’re not “your engineers”. This is an interview for a position listing the job requirements. You go ahead an interview however you like, but generally, giving your interviewer a feeling that you plan to shake up their tech stack because you feel like it isn’t going to go well. reply Capricorn2481 10 hours agorootparentWhat is this job where the position is so high up that it can make unilateral decisions on the tech stack but also needs to be punished if they know about Rust? Would you hire a team leader that didn't know what Rust was? reply namaria 15 hours agorootparentprevPeople _mentioning_ Rust means they will bring a random language into your stack if they are hired? This red flag cultural trend is blinding people to nuanced thought. reply 01HNNWZ0MV43FF 15 hours agorootparentn=1 but I did bring Rust to my C++ job. I needed a web server, and I didn't want to put in an order with the web server team, and writing a web server in C++ looked like a recipe for pain. reply quickthrower2 19 minutes agorootparentdocker run --name docker-nginx -p 80:80 nginx reply EVa5I7bHFq9mnYK 11 hours agorootparentprevWhat pain? Just include mongoose.h and .c in your project. It's in C, but C is C++. reply zaphirplane 13 hours agorootparentprevReally people write a web server from scratch? What industry? Don’t tell me it’s some CRUD reply wredue 14 hours agorootparentprevI feel like you’re giving rust a pass here. It would be a red flag if you were interviewing for react and decided to bring up vue or svelte or angular or whatever else as well. It’s not like it’s only this C++/Rust type deal that is being picked on. Although I would suggest that rust fans tend to be particularly ardent and loud at the current moment, so interviewers may be far more turned off of you as a person just for bringing it up. Interviewers probably felt this way about people bringing up Python 20 years ago (Python devs of yore were WAY WAY worse about their “HAVE YOU HEARD THE WORD?!” Than rust devs are today), blockchain 7 years ago, etc. Anyway. As an interviewee, I’d probably try to avoid being the one to bring up alternative technologies. reply theon144 10 hours agorootparent>It would be a red flag if you were interviewing for react and decided to bring up vue or svelte or angular or whatever else as well. ...why? Seriously, why on earth? I don't follow this train of thought at all; if they demonstrate proficiency within the scope of the position, why does it matter if they also happen to know other technologies? \"Oh, Alice? Yeah, she was a great candidate, unfortunately she also had experience in Vue, so there's nothing we could do. We decided to hire Bob, who has 3 years less experience with React, but fortunately that's the only stack he's ever heard of.\" If anything, it's a sign the person is interested in learning, most great devs I've met were not proficient only with a single technology. This sounds completely alien to me. reply namaria 12 hours agorootparentprevI'm sorry but I'm having a hard time understanding why a person bringing up a tool in conversation during an interview is seen as such a clear and strong signal containing so much information about their professional performance. But nowadays it seems like one has to 'turn on' an interviewer and avoid a minefield of forbidden words to get a job. If a workplace is to become a toxic cult-like environment being policed against such thoughtcrimes as being curious and interested in technology for its own sake I think one would be fortunate to be passed on after an interview to enter such a place. reply UK-Al05 9 hours agorootparentprevYou should value wide knowledge. Not be scared of it. reply jimbob45 16 hours agorootparentprevAre you not a developer? Being proficient in one means that it will take very little time to transfer to the other. C++ and Rust have a great deal of overlap in that way. reply risyachka 16 hours agorootparentIt only works one way though (at least in this case). If you know C++ you can quickly become proficient in Rust. But not the other way around. reply rcxdude 18 minutes agorootparentAre you sure? Many C++ programmers who have learned rust report that it has also made them better C++ programmers. reply timeon 15 hours agorootparentprevYeah C++ have steep learning curve compared to Rust. reply PartiallyTyped 9 hours agorootparentprevIf you know rust, you can carry the same ideas to C++. My C and C++ skills greatly improved as I got better with rust. The compiler forces you to learn proper memory management and that carries over. Smart pointers? Just Box, Rc, Arc and Refcel. Move semantics? It's just another name for ownership. Sure, the OOP stuff are different, but okay, that in and of itself shouldn't hinder you. reply nambit 1 hour agorootparentIf it's an experienced C++ job and you don't know the difference between rvalue/lvalue/etc, you're gonna have a tough time. reply secondcoming 14 hours agorootparentprevIt's not about proficiency, it's about not getting bogged down in programming language bikeshedding. reply fl0ki 14 hours agorootparentSigh. People don't even know what bikeshed means any more. https://en.wiktionary.org/wiki/bikeshedding If you think the differences between C++ and Rust are \"unimportant but easy-to-grasp issues\" then you may not know enough about either/both to contribute to the discussion. We're talking about differences so impactful and well-recognized that everyone from Google[1] to the NSA[2] is advocating for using Rust to reduce the unsafety compared to C++. Are you saying all of them are bogged down in bikeshedding? [1] https://security.googleblog.com/2022/12/memory-safe-language... [2] https://www.nsa.gov/Press-Room/News-Highlights/Article/Artic... reply xedrac 16 hours agoparentprevThe trick is to introduce Rust wherever you're at, assuming it's a good fit for the task. This may be harder to do if you don't have enough influence. reply jonnytran 15 hours agorootparentYes, I did this at my startup. Fast forward a few years, and now the company has more Rust code than Python, and the majority of the company's IP is in Rust. I suggest beginning with small, one-off things that don't have much impact. People, even developers, tend to shy away from things that aren't familiar. By introducing Rust in a small, low-risk way, it helps people get familiar with it. They get to build familiarity with building Rust projects, navigating the project structure, and reading docs. I submit pull requests that get people to read Rust code, even if it's just to say \"looks good\". Their familiarity builds slowly over time, meaning they'll be less triggered by seeing Rust in a larger, more impactful project down the road. How do you boil a software developer? Slowly. If they give Rust a chance and your team has a champion to guide them, they'll see its merits. I think a lot of people come to Rust for the performance, but that's not why they stay. reply Capricorn2481 9 hours agorootparentWhat domain are you working in where Rust is the replacement for Python? reply tracker1 3 hours agorootparentPerformance, portability, reduced memory use.. even containerization which can benefit from all of the above. reply PartiallyTyped 9 hours agorootparentprevI am in the process of oxidizing some stuff at work with Rust. I too am starting from small pieces, things that I can incorporate and call off python directly. Also relying a bit on codegen to blend the two languages and slowly remove all python code. reply tayo42 16 hours agorootparentprevQuick way to be annoying and unlikeable at work. The times I've seen rust introduced, and tried it my self, it introduces a whole bunch of related tech debt. Like hacks to make builds work reply xedrac 10 hours agorootparentWhen I see people on my team (myself included) shoot themselves in the foot over and over with C++, sometimes causing very costly production bugs, and yet I know of a tool that could have prevented nearly all of that pain at compile time, I am going to risk being disliked in order to help lift everyone up. Most people that I mentor on Rust have become completely sold on it after some initial resistance. I don't like change for change's sake, or rewriting things in a new language unless there is some really big payoff. reply tayo42 6 hours agorootparentI think that argument makes sense in a vacuum. Like considering nothing else, yes, 100%, rusts compiler prevents more bugs then c++ would. I don't think bugs are the only way to waste time or break things. In a large company or complex project your never talking about one small code base. There are inter-dependencies within code and outside code. Rewriting all of anything significant is a huge undertaking. In reality your introducing a new language in addition to cpp/java/go. Its another support task. Its another thing to context switch between when your working on \"the legacy\" repo and the new stuff. You need to be skilled in both languages. Its like any team that starts writing automation and support projects in python, when their main language is something else. You get trash python scripts and programs. Its another language where technically the domain is right, but the skill is lacking. reply beeboobaa 15 hours agorootparentprevIf it's a good fit then it's a good fit. Of course you'll need to ensure the CI is set up properly, just like everything else. If it's not a good fit, then you're just forcing it, and yeah... reply summerlight 9 hours agoparentprevOne thing is that a large fraction of C/C++ devs are hired by big companies. And those tend to have a large amount of legacy code and in-house tooling. Adding one more language to such big ecosystem can be a significant investment than kicking off a new project from scratch, so they need a good transition story to convince stakeholders and get actual funding(=more headcount/jobs). I think Rust has reached to the level where many of us agree on that it's probably a good idea to move on, but probably not many of us have a good idea on how to do that. Hopefully some teams are eager to do that, so we will see some success story soon then more and more teams will explore such possibility. reply surajrmal 14 hours agoparentprevIt's equally difficult to try and hire a team of folks who know rust. There is also a lot of legacy code which doesn't cleanly interop with rust (cxx and the like are not sufficiently easy to bridge the gap). It would be much easier to convince your existing team to start using rust where it makes sense than it would be to switch jobs in search of one which primarily uses that. Over time both of these problems will diminish, but that'll likely take another decade. reply davidmurdoch 15 hours agoparentprevThere are tons of Rust jobs in blockchain. reply kaycey2022 1 hour agorootparentI’m interested. Can you tell me what do rust developers build or work on in this space? Where do you start networking and build the demonstrable skills to get into blockchain? reply timeon 18 hours agoprevI upvote almost any post \"in Rust\" here (yeah that's me) but this story went bit too far. It was just one job posting and so many sites acts like MS is going to ditch C#. reply oaiey 16 hours agoparentTotally agree. The .NET team itself is probably costing above 100 million every year, probably more. Microsoft is not ditching .NET, they are heavily investing it. The office division is implementing some code in Rust while the same division just posted a success story of their usage of migrating some low latency server code to .NET Core. Microsoft is a big company and like any other they use JS, TS, Rust, C++, Python, Go and so on. The only thing they do not use is Java ... And I am not 100% confident there. So let us congratulate Rust for entering the office domain and stop writing swan songs about .NET. reply rileymichael 12 hours agorootparentThey use a ton of java, they even have their own distro. In fact, they answer this question on its landing page (https://www.microsoft.com/openjdk): > Java at Microsoft spans from Azure to Minecraft, across SQL Server to Visual Studio Code, LinkedIn and beyond! We use more Java than one can imagine. reply bmicraft 10 hours agorootparentCounting Minecraft as a somehow important use of java in Microsoft is pretty funny when they bought the company that introduced it and then rewrote it from scratch in c++ for w10 reply Alphaeus 9 hours agorootparentThe C++ ports to mobile and consoles predate Microsoft's acquisition by a number of years. reply fortran77 17 hours agoparentprevThat’s funny because I downvote almost any “in Rust” post here! C# — and garbage collected languages in general — aren’t going anywhere. With the shift to Arm, languages compiled for a runtime interpreter will get even more important for Microsoft. reply zahllos 15 hours agorootparentI'm a Rust convert, but that needs to be put into context: I do stuff in in cryptography, often on embedded. Prior to Rust I did C and C++ for most of my work. There's also Ada... and it has SPARK, but Ada has never had the level of exposure that Rust has achieved. But there's all manner of \"business systems\", e.g. Java/RHEL shops, \".net shops\", where GC pauses don't matter, the code is something other than CPU/memory bound anyway,the GC/JIT are mature, and a human writing C++ likely can't do better and will probably do worse. I've seen this with a write-heavy log-ingesting system where the developers used smart pointers everywhere - which internally use atomic reference counting, which implies sync barriers for otherwise entirely unrelated data. I agree, these kinds of places are unlikely to move to Rust - the code writes faster in anything else and runs well. There's also Go, which has a GC but compiles natively, GraalVM, which is AOT compiled Java etc. I definitely agree GC'd languages are not going anywhere. I'm not sure I agree with C# becoming more important because of ARM, though. I don't think it'll change much - I think shops already invested in certain stacks will mostly stick to them and for all Rust's popularity, it doesn't make sense to implement your CRM in it really. It does, however, make sense to reimplement the C++ parts of the .net runtime and miscellaneous other C and C++ parts of Windows that can benefit from the borrow checker, because it vastly reduces spatial memory safety bugs. There's a massive cost to this, but it's no longer just about about on-prem patching but Azure. reply fl0ki 13 hours agorootparentprev> With the shift to Arm, languages compiled for a runtime interpreter will get even more important for Microsoft. How's that? Apple and its app ecosystem just ship universal binaries, so native, heavily optimized x86_64 and aarch64 code is always ready to go with no added fuss for the user. You could certainly argue this makes already-large binaries even larger, but users care much more about CPU & RAM efficiency than on-disk binary size. Especially on ARM machines that many people buy for battery efficiency in the first place. On Linux you don't even bother with universal binaries because you just ship the entire package repository already built for the target CPU. It's only more fuss for the user if they need to pick specific packages from outside their distro. Has Microsoft still not managed to make something similar work on Windows? reply xedrac 16 hours agorootparentprevGiven your username I suspect you're quite a bit older. I've mentored several grey beards on Rust and they weren't at all happy about it, at first. After a few months though, they were some of its strongest advocates. It's hard to not love Rust when you realize just how much pain it saves you from. reply tracker1 3 hours agorootparentAs a gray beard... I'd rather work in rust moving forward for a lot of things. reply candiddevmike 16 hours agorootparentprevWhat pain does it save most folks from that are using GC languages (as GP alluded to)? Genuinely curious. reply josephg 15 hours agorootparentWeirdly enough, some of the worst memory leaks I’ve debugged in my career were in GCed languages. Because any object can reference anything, retaining references can hide anywhere in the code. (Including via closures and other exotic things you forget about). And because in GC languages we don’t often have a destructor (or anything like it), it’s very common for people to forget to clean up resources. (Eg filesystem handles, sockets, wasm object references, and so on). One weird thing about rust is that network sockets and file handles are automatically closed when the handle drops out of scope. The borrow checker takes care of that. If you do the same thing in JavaScript (open a socket then do nothing), the program won’t even quit on its own and you’ll have no idea why. reply tracker1 3 hours agorootparentIn IE6 it was really easy to lay memory with long lived apps. Any HTML object could have a JS event attached and give versa for object references in JS. Done it was across the COM boundary, if you removed an element from the DOM it would stick around if referenced in JS and give versa for inline JS on* attributes... I worked on an application with extjs that the workers had to close and reopen at least once a day. Or unofficial instructions were portable Firefox. reply xigoi 12 hours agorootparentprev> And because in GC languages we don’t often have a destructor (or anything like it), it’s very common for people to forget to clean up resources. (Eg filesystem handles, sockets, wasm object references, and so on). > One weird thing about rust is that network sockets and file handles are automatically closed when the handle drops out of scope. The borrow checker takes care of that. If you do the same thing in JavaScript (open a socket then do nothing), the program won’t even quit on its own and you’ll have no idea why. That’s not an inherent problem of GC. You can have linear types or destructors in a GC language. reply thfuran 11 hours agorootparentThe first sentence you quoted acknowledged that it's a tendency rather than a fact of GC. reply ClimaxGravely 10 hours agorootparentprevIf I understand correctly, what you are describing would often be done with a using scope block in C#. EX using (SomeKindOfHandle handle) { // handle releases when the scope ends } reply tracker1 3 hours agorootparentprevDepends on the language and platform/Runtime... For C# a single executable over a directory of files. For that and Java, a massive runtime, generally slow cold starts, high initial memory use, reduced relative effectiveness for smaller platforms (RPi and similar). For JavaScript and Python, slow introp, in addition to the negatives above. It's also possible to leak memory like a sieve in any language. reply neonsunset 3 hours agorootparent- Native interop in Java is surprisingly slow and very clunky (C# interop with C/C++ can be as cheap as direct calls nowadays) - .NET had been able to produce single-file (and self-contained when needed) executables way before NativeAOT was introduced - RPi is a fairly large platform compared to e.g. Arduino (for which Rust should be awesome) and can be well-served by NativeAOT (today). I might still use Rust for that one in the long run but using C# wouldn't be an issue both in terms of resource usage and OS since it's just an arm64 musl flavour of Linux usually with 1GB of RAM or more which is a lot for .NET (for context ~256MiB containers with ASP.NET Core image is a popular target for back-ends) reply yazaddaruvala 15 hours agorootparentprevOh man, GC is amazingly nice but when it fails sure does it fail. It also typically fails when your biggest task is reducing overall resource spend, or getting that extra 9 after 3+ 9s of latency reduced. On multiple teams of FAANG engineers, Ive seen the GC be a big part of some meltdown and none of us knew a good way around it. Tuning only goes so far. Otherwise, you just have to give it more memory and CPUs (horizontally or vertically). Almost always the answer was, this might need to be reworked in C++ entirely or JNI. In same cases it even got funded and was a tremendous success (like 10x fewer resources). However, then you’re dealing with C++ and all of its issues. I’m looking forward to more value types in Java but managing the heap will always be a bottleneck eventually. reply tracker1 3 hours agorootparentNothing like a game server engine in C# that would freeze for 15-30 seconds now and then for garbage collection... reply williamdclt 15 hours agorootparentprevGC _is_ the pain. Lots of use-cases where precise memory management is critical, or real-time guarantees are needed that are difficult with GC. reply nycdotnet 13 hours agorootparent“The garbage comes from somewhere” -David Fowler reply xedrac 10 hours agorootparentprevNot all GC languages are created equal. The best of the GC languages in my opinion is Haskell. So if a project is well suited to the limitations of a GC language, like Haskell, then the benefits of Rust may be limited to things like its nicer tooling and more polished ecosystem, etc..., while also having a few downsides. But if we make the same comparison to Go or Python, the list of advantages gets a lot bigger. reply secondcoming 16 hours agorootparentprevJust like Jesus! reply xedrac 10 hours agorootparentHaha, that's some high praise! reply josephg 15 hours agorootparentprev> C# — and garbage collected languages in general — aren’t going anywhere. So what? What does that have to do with rust? There’s no fight between C# and rust where only one language will survive. Both languages are great, and both languages probably have a bright future. C# depends on a lot of low level C/C++ code to function and run fast. I can imagine a future where rust enables c# to get better. I love rust, but there’s also lots of programs and teams for which c# is a better language choice. So your comment is really weird to me - there’s no reason loving c# should impact your relationship with rust at all. reply neonsunset 6 hours agorootparentThat’s not technically accurate. While both JIT/ILC and GC are implemented in C++, the former might have as well been written in any other language which would impact the time to JIT/AOT compile the code (startup time) but not the final product. Pretty much all performance-sensitive code is written in C#. reply jauntywundrkind 7 hours agorootparentprevMay every one of us attain sufficient popularity as to attract people who blankety hate you without even bothering to supply reason or argument. May you be so popular & doing such good things people feel the need to downvote everything they see about it on sight. Actually personally I wish we could identify such people & ignore their votes. Actually personally I wish we could take persistent downvoters & fire them into the sun. I don't see or understand why anyone would let contempt so deeply into their soul. The world is no zero sum and I have little patience for those would seem to go out of their way to deny & reject others. reply 01HNNWZ0MV43FF 14 hours agorootparentprevInterpreters such as wasm :P reply rahen 18 hours agoprevWhat's the point of rewriting from C# to Rust, as C# is performant enough and already has memory safety? What will offset the huge rewrite cost? reply dragonwriter 18 hours agoparentC# doesn't have ownership/lifetimes and the kind of safety that comes with them. Also, at the scale of Microsoft’s core global services, when you are paying for the processing, fast enough (or “CPU efficient enough”) isn't the same as with common apps. Even small efficiency gains are going to yield sufficient savings to be worth a fair amount of developer time. reply jmull 17 hours agorootparent> C# doesn't have ownership/lifetimes and the kind of safety that comes with them. That's not really true. Like rust, C# is memory safe, although it comes at it in a very different way. \"safe\" rust does inherently prevent a certain kind of race condition in multi-threaded code that C# does not, thought that's more of a nice incremental improvement, not a fundamental one -- i.e. it doesn't make your multi-threaded code thread-safe, but it does prevent a one type of thread safety violation. reply k_bx 16 hours agorootparentRust gives you clear semantics on when it drops a variable, so it's being used not only for memory safety but to close files, kill threads, synchronization primitives, and whatever else you want (can be implemented via Trait). Does C# have any kind of GC behaviour guarantees that are similar? reply magicalhippo 16 hours agorootparentNot really. There's the IDisposable interface you can implement, and some language features around making it easy to use[2], however there's nothing preventing you from not using those language features and forgetting to call Dispose. In case you forget the GC will call Dispose but it's entirely up to the GC when that happens, so personally I wouldn't say it's similar. [1]: https://learn.microsoft.com/en-us/dotnet/fundamentals/runtim... [2]: https://learn.microsoft.com/en-us/dotnet/csharp/language-ref... reply cerved 16 hours agorootparentDoesn't the compiler complain if you don't use using or try? I forget reply vips7L 15 hours agorootparentIt does. So does every IDE or linter. reply helix278 15 hours agorootparentprevThere are warnings given by analyzers in some cases, but it's not exhaustive reply SigmundA 16 hours agorootparentprev>In case you forget the GC will call Dispose but it's entirely up to the GC when that happens, so personally I wouldn't say it's similar. This actually has to be implement in the classes finalizer its not done automatically. The finalizer is called by the GC which if the designer chooses can call Dispose. This is usually done in well designed classes that use unmanaged resources. Dispose and using give you the determinism if you want it and the finalizer gives you the backstop to prevent leaks if it makes sense. reply magicalhippo 15 hours agorootparentGood point, I forgot about that little, but crucial, detail. So it's flexible, but there are footguns about, especially if you're wrapping a handle to something external. reply jmull 16 hours agorootparentprevThat's great, but not relevant to the topic of memory safety in C#, which is all I was addressing. I think you are making a point in the broader discussion of C# vs. rust, but that doesn't fit here. (And, personally, I don't care about that so have nothing to say on the topic.) reply SigmundA 16 hours agorootparentprevC# is IDisposable [1] for that kind of stuff and has the \"using\" block [2] to make it hard to get wrong. 1. https://learn.microsoft.com/en-us/dotnet/api/system.idisposa... 2. https://learn.microsoft.com/en-us/dotnet/csharp/language-ref... reply krona 17 hours agorootparentprev> C# doesn't have ownership/lifetimes and the kind of safety that comes with them. I believe .net Core has supported several forms of escape analysis for quite a while. reply hgs3 17 hours agorootparentprev> C# doesn't have ownership/lifetimes and the kind of safety that comes with them. Could you elaborate? C# has a garbage collector for tracking resources. reply lucasyvas 17 hours agorootparentI think the parent post is wrong. This rewrite probably has more to do with performance, \"fearless concurrency\", and/or developer happiness. C# is safe. reply whimsicalism 17 hours agorootparentthread safe? reply jmull 16 hours agorootparentIt isn't, but neither is rust. Safe rust is safe from data races, but that's just one kind of potential thread safety issue -- one of the simpler kinds. It's not nothing, but not anywhere close to thread-safe. reply CHY872 16 hours agorootparentprevC# has a garbage collector for specifically tracking memory, but lifetimes are more broadly useful. For example, Rust lifetimes (this is also the case in C++ afaik) can be used to suitably scope the lifetimes of mutexes, to have temporary folders which are deleted when they go out of scope, to require that a connection pool is destroyed _after_ the last connection inside it is returned, etc, etc. Mostly, garbage collected language do a bad job of cleaning up objects which refer to resources held elsewhere. Java had persistent issues with direct ByteBuffers (which were wrappers around malloc (but not free!)). Locks are easily held too long. File handles are easily left open. And depending on your GC settings, that file descriptor that's holding a 10GB file around may not get cleaned up for hours. Refcounted languages can be somewhat better, but they don't avoid the bug, they just mitigate the effects. reply Kinrany 17 hours agorootparentprevI think GP is talking about concurrency issues that arise even in single-threaded code when simultaneous mutation from multiple sources is permitted. reply charles_f 17 hours agorootparentprevAlmost all of the services I've seen at Microsoft were built in C# reply packetlost 17 hours agoparentprev> C# is performant enough until it isn't. At Microsoft's scale, I imagine performance is actually a major concern for certain pieces of applications. reply LeFantome 15 hours agoparentprevHow have you established that there is a “huge rewrite cost”? Despite other comments here, I see nothing in the post that says “we are moving away from C# and rewriting everything in Rust!” Microsoft is using Rust in Windows as well. They have given examples of how. In that case, they are not “rewriting Windows” but rather targeting specific components that benefit from the characteristics of Rust. Those components are just part of the larger application that is still primarily written in C. C# easily with C and therefore with Rust. They interoperate well. So it seems likely that MS is following the same plan and rewriting specific components of the larger system that would benefit from Rust. reply tracker1 3 hours agoparentprevWho says they are? Probably depends and varies by the application. One example I could think of is if it's currently a SaaS on Azure, but it needs to be portable enough for a version to run in a low end Arm IoT device. Or if it's deployed to thousands of servers and taking excessive memory or CPU resources for a monitoring agent. Or GC freezes affecting other adjustments systems. There are plenty of reasons to retire C# to Rust. reply chiph 18 hours agoparentprevI suspect this is the .NET runtime + libraries, not applications written in C#. reply Merad 17 hours agorootparentNo, they're looking at migrating some core Office 365 services to Rust. This is the job posting: https://jobs.careers.microsoft.com/global/en/job/1633482/Pri... At the scale of O365 it totally makes sense to look at a high performance non-GC language for your core systems that see the most traffic (I say this as a .Net dev). reply adrr 15 hours agorootparentNot sure microsoft’s problem is the language. Their frontend JavaScript code is bloated and slow. It makes office365 feel slow and there are so many bugs in the azure management tools. I assume the backend code has the same low quality. reply Merad 15 hours agorootparentI imagine the goal here is to optimize MS' compute and hosting costs. The core pieces of O365 probably handle many billions of requests per day. At that scale even small optimizations can be worth millions a year in cost savings. reply struant 9 hours agorootparentprevSounds like a skill issue to me. Their code sucks and they want to blame the language. reply cebert 16 hours agorootparentprevToo bad you have to be in the office 50% of time for that role. reply solarkraft 16 hours agorootparentprevThat surprises me since I'd expect that you could get really far with hot path optimisation. reply fbdab103 16 hours agoparentprevConsidering the Microsoft consumer software that burns cycles to do trivial things (Teams, OneDrive, pick your favorite example). I do not think language performance should be a compelling argument. reply tracker1 2 hours agorootparentThere's a difference in resources on client machines and resources on thousands of servers in server farms. reply bobajeff 16 hours agoprevI'm not totally sold on rust as a language but I have to admit the tooling and ecosystem is really nice. I'm noticing that I'm increasingly using more stuff built with it. I'm also not sold on zig either for many of the same reasons. I prefer my low level languages to be smaller like c. I think that might be true for languages higher level languages too. I just don't like to have to down a lot of documentation on hundreds of different features and concepts behind them. reply kiririn 12 hours agoparentThe tooling/ecosystem looks probably great for developers actively creating with it, but for developers ‘consuming’ it (downloading some foss from github and changing a couple of lines) it leaves a lot to be desired. Not being able to ‘apt install’ a working toolchain on Debian for example - it’s already too outdated after just a year reply tracker1 2 hours agorootparentThat's pretty true of a lot of development tools on Debian. reply atesti 2 hours agoprevDoes anybody know more about the \"Substrate App Platform group\"? As far as I understood it, actually Microsoft Exchange and ESENT powers a lot of Office 365, e.g. the compliance tools, the search and e.g. teams chats. Next to it is another pillar: Sharepoint, which is also exposed as OneDrive and is based on SQL server. Is substrate or has it been part of Exchange? reply rubyfan 18 hours agoprevCan they start by not needing a multiple gigabyte download and admin rights to get the rust compiler to work on Windows? reply itishappy 17 hours agoparentDoes C# not require a multiple gigabyte download and admin rights? FWIW I've gotten rust working with the GCC build tools on windows without admin rights, thought it's admittedly somewhat janky. reply metaltyphoon 5 hours agorootparentNot C# doesn’t need multiple gigabytes and admin rights. Dotnet tooling != Visual Studio reply vijaybritto 18 hours agoparentprevIs that needed because the compiler accesses some higher privileged APIs? reply Measter 18 hours agorootparentThe MSVC-based compiler doesn't come with a linker, so you need the MSVC build tools to get it. One of the ways to get the build tools is to install Visual Studio, which is fairly large. reply filiphorvat 17 hours agorootparentYou don't need the whole Visual Studio installation, only MSVC build tools and Windows SDK. See here: https://rust-lang.github.io/rustup/installation/windows-msvc... reply Thaxll 17 hours agorootparentprevWhy would a compiler needs any privileges, it's like a text editor reply cerved 16 hours agorootparentIn widows, you get bonus points if your program needs administrative rights reply TideAd 15 hours agoprevI find this very strange, Microsoft has a lot of big internal high-performance services written in C#. You have to be intentional about some things - largely making sure objects are very short-lived or very long-lived, to avoid long GX pauses. But .NET performs much better than it did 10-15 years ago and I can't think of a fundamental reason why you'd rewrite in Rust. reply zozbot234 14 hours agoparentThe Rust folks are working on experimental codegen for the CLR https://fractalfir.github.io/generated_html/rustc_codegen_cl... leveraging the existing CIL/CLR support for \"unsafe\" languages. Once that work is complete, you should be able to rewrite C# code to Rust on a very fine-grained basis (a single function at a time or thereabouts), just like you can when porting C to Rust. Of course, you'll also be able to remove the CIL/CLR dependency altogether if you're left with 100% Rust code, and compile to a binary just like in ordinary C/C++. reply dtx1 15 hours agoparentprevMight not be all about performance but about security aswell. reply jasonjmcghee 14 hours agoprevAre there any rust books that have the focus of “ok you are productive in rust, this is what you should know / do to write proper rust”? Edit: ok found https://github.com/sger/RustBooks Any specific recommendations? reply sn9 9 hours agoparentIt sounds like you want Rust for Rustaceans: https://nostarch.com/rust-rustaceans reply jasonjmcghee 5 hours agorootparentThank you reply Sparkyte 16 hours agoprevI imagine it comes down to the handling of threads vs anything else. Technology is rapidly adopting the more cores strategy of technology since we are hitting some IPC limitations and in the server space more cores is better. reply munchler 10 hours agoparent.NET supports asynchronous code very well, so that hardly seems like a likely reason for rewriting in Rust. reply neonsunset 6 hours agoparentprevMulti-core scaling (in particular within GC) is one of the strongest points of .NET (for example, Go used to have poor scaling on many-core systems (has this changed in 2024?) while in .NET the throughput would continue scaling linearly)). reply bigtex 18 hours agoprevThere is a reddit post where a dotnet developer on the MSFT team details why they are moving certain processes to Rust. TLDR, it is all about performance that scale. reply atesti 2 hours agoparentDoes anybody have a link? reply psuresh 6 hours agoprevExpect MS own clone of rust like C#(for java) or typescript (extended js) in future reply tracker1 2 hours agoparentI think they already have this internally. That said, I believe they already have board seats and are large contributors to the rust foundation. reply lucasRW 17 hours agoprevI can understand the move from unmanaged C to Rust for security reasons, but C Sharp to Rust ? Am I missing something here ? reply tracker1 2 hours agoparentReduced memory and CPU utilization at scale most likely. reply jonhohle 17 hours agoparentprevPerformance reasons. reply thrownaway561 18 hours agoprevJust hire Yehuda Katz reply gigatexal 17 hours agoprevWhy is MS so hyped on Rust? It’s good but doesn’t it have its caveats? It seems they’ve become Rust zealots. reply marcosdumay 17 hours agoparent> It’s good but doesn’t it have its caveats? Compared to C#, it's a low level language, where you have to be concerned with a lot of details. But then, I'd bet this is for replacing bad C# that does low level tasks, that nobody sane would write but MS pushed for anyway because they were hyping C#. reply tialaramex 16 hours agorootparentOffice 365 is was introduced in 2010. So when it was created Microsoft had two reasonable options: 1. Accept the performance penalty from using a safe garbage collected language (such as their own C#) which is slower which may make the service less attractive and more resource hungry which means more expensive to run at scale. 2. Accept the correctness penalty from C++ which delivers good performance but will cause an endless stream of bugs, some very hard to diagnose. Rust makes this much easier, because you aren't paying the correctness penalty, your Rust will probably have the same or fewer correctness bugs as your C#, and yet you get the good performance you'd have wanted from C++. For a 2010 product Rust was not an option, but presumably at some point in the last say five years, Microsoft did the maths and worked out that unless they're killing Office 365 soon or somehow C++ magically becomes safe with no work, a transition to Rust is cost effective. reply jimberlage 16 hours agoparentprevMS has lots of battle scars from the security headaches they went through in the 2000s/2010s. reply 01HNNWZ0MV43FF 14 hours agoparentprevIn 2007 they were .NET zealots and now .NET is old guard reply tracker1 2 hours agorootparentThere plenty of ongoing development and support for .Net, it's much less likely a matter of age, as it is handling more requests per server. reply wslh 16 hours agoprev [–] Hi @dang, regarding HN karma, what happened to the karma points if a user posted the same article just twenty hours earlier? (◔_◔) [1] [1] https://news.ycombinator.com/item?id=39232275 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Microsoft is looking for Rust developers to rewrite core C# code as part of its adoption of the Rust programming language.",
      "The company is forming a team to transition away from C# and re-implement existing C# based services in Rust.",
      "This move demonstrates Microsoft's dedication to memory-safe programming and keeping their future options open."
    ],
    "commentSummary": [
      "Microsoft is actively seeking Rust developers to rewrite core C# code, highlighting the growing demand for Rust jobs in the market.",
      "Rust is considered challenging to learn and work with due to its complex concepts, but its use is increasing in network infrastructure and security.",
      "The adoption of Rust in large companies with existing codebases can be a significant investment, requiring skilled developers and interoperability with legacy code.",
      "Rust offers improved performance, reduced memory usage, and portability, making it a potential solution for preventing security bugs.",
      "Microsoft's interest in Rust is driven by the need for high performance and cost optimization in handling billions of daily requests. This involves considerations of performance, security, scalability, and reducing correctness bugs."
    ],
    "points": 190,
    "commentCount": 190,
    "retryCount": 0,
    "time": 1706967371
  },
  {
    "id": 39241825,
    "title": "The Future of Graphic Design: Figma's Vector Networks Revolutionize Path Creation",
    "originLink": "https://alexharri.com/blog/vector-networks",
    "originBody": "The Engineering behind Figma's Vector Networks November 24, 2019 Adobe Illustrator introduced the pen tool back in 1987 as a tool for creating and modifying paths. Since then the pen tool has become incredibly widespread, so much so that is has become the de facto icon of the graphic design industry. The pen tool's functionality hasn't changed significantly in the 30 years since its introduction. Just click and drag to create smooth curves. Designers have learned to work with it, and around its idiosyncrasies. The pen tool But Figma felt like they could improve some aspects of how the pen tool worked, so they had a go at redesigning it. Instead of it being used to work with traditional paths, they improved the pen tool by creating what they call Vector Networks. In this post we will go through what Vector Networks are and what problems they try to solve. After we've defined what Vector Networks are, we will take a look at some of the engineering challenges you would face if you were to take a stab at implementing them. This post can be thought of as an introduction to a really interesting problem space, and as a resource for people interesting in making use of some aspects of Vector Networks for future applications. I hope it succeeds in providing value to both developers being introduced to new concepts and ideas, and to designers interesting in learning more about the tool they know and love. I will start off by laying out the core concepts behind the pen tool, and from there we will move onto Figma's Vector Networks. Paths The pen tool is used to create and manipulate paths. If you've every worked with graphics software like Illustrator before, you've worked with paths. Paths are a series of lines and curves that may or may not form a loop. Some paths The path to the left loops, while the path to the right doesn't. Both of these are valid paths. The main characteristic of paths is that they form a single continuous unbroken chain. This means that each node can only be connected to one or two other nodes. Not valid paths However, you could construct these shapes from multiple paths if you position them correctly together. Multiple paths are used to create more complex shapes From a combination of paths, you can create any shape imaginable. This beer glass, for example, is just a combination of five different paths positioned and scaled a certain way. The building blocks of paths A path is made up of two things, points and lines. Points and lines The points are known as nodes (or vertices) and the lines are called edges. Together, they make a path Any path can be described as a list of nodes and edges. This path can be described as the series of nodes (0, 1, 2, 3, 4). It could also be thought of as the series of the edges that composed it. That list of edges would be (0, 1), (1, 2), (2, 3), (3, 4), (4, 0). You can think of this like the dot to dot puzzles that you used to do as a kid: Draw the edges of the path in the order that the points lay out. But instead of a kid drawing lines between numbered points on a paper, a cold calculating machine does it along the cartesian coordinate system. Edges An edge is a connection between a pair of nodes. Visually, edges are a line from node a to node b. But that line can be drawn in a lot of different ways. How do you describe those different types of lines? Edges fall into two categories, straight and curvy. Straight edges are as simple as they seem, just a line from a to b. But how are those curvy edges defined? Bezier curves Curvy edges are bezier curves. Bezier curves are a special type of curve defined by four points. The positions of the two nodes in our edge make up the start and end points of the curve. Each of the two nodes has a control point. In most applications, these control points are shown as handles that extend from their respective node. These handles are used to control the shape of the curve. Bezier curves can be chained to make more complex shapes that a single curve can't draw on its own. They can also be combined with straight lines to make some cool designs. But what exactly are the handles doing? How do the handles tell the computer to draw the curve like it does? Computers draw curves by splitting them into straight lines and drawing the individual lines. The more lines you split a curve into, the smoother the curve becomes. So to draw the curve we need to know how to get the different points that make up the curve. If we compute enough of them, we get a smooth curve. Computing a point on a bezier curve Let's compute the point at 25% point of the curve. We can start by connecting the control points with a third blue line. Then for each blue line, we draw a blue dot at the 25% point of the line. Next, draw two green lines between the three blue dots. And we repeat the same step as we did with the blue dots. Draw green points at the 25% points of the green lines. And then one more red line between the newly created green points. Then we add a red point at the 25% point of the red line. And just like that, we've computed the point at the 25% point of the curve. From now on we'll refer to points on curves through a t value, where t is a number from 0 to 1. In the above example, the point would be at t=0.25 (25%). t=0.25, t=0.5 and t=0.75 This way of computing the point at t is called De Casteljau's algorithm and can also be used to subdivide a bezier curve. Using the points we created along the way, we can also subdivide the bezier curve into two smaller bezier curves. Bezier curves are pretty amazing things. Shaping the curve by adjusting the handles feels surprisingly natural, and chaining them together allows you to create detailed and complex shapes. And for computers, they're stable and inexpensive to compute. For this reason they're used for everything from vector graphics to animation curves and automobile bodies. You can see an interactive demo of bezier curves at Jason Davies' site. It's fascinating to watch a series of straight lines trace out a smooth curve. From https://jasondavies.com/animated-bezier The creative constraints of paths Earlier in this post, paths were defined as a continuous series of lines and curves that may or may not form a loop. The fact that paths are a single continuous chain is a pretty big limitation. It means three way intersections are not possible using a single path. To create a three way intersection, two or more paths will have to be used. This means dealing with positioning and grouping different paths together. It also means that changes to a single path can lead to changes to multiple other paths. But that's simply the routine. Seasoned designers know how paths behave, they can plan around it without really thinking about it. For a static design it doesn't really matter how many paths and layers you have to create if the piece is planned properly upfront. But for some situations the constraints that paths impose cause a lot of friction. Vector Networks In 2016, Figma introduced Vector Networks. They lift the “single continuous” limitation by allowing any two nodes to be joined together without restrictions. “A vector network improves on the path model by allowing lines and curves between any two points instead of requiring that they all join up to form a single chain.” Source: https://www.figma.com/blog/introducing-vector-networks/ The cube is the quintessential example for demonstrating Vector Networks. Via traditional paths, you would have to create at minimum 3 different paths to describe this shape. This creates a lot of friction for a seemingly simple and common shape. To modify the cube, you would have to modify two or three different paths. But with Vector Networks you can simply grab an edge and move it around, and the shape behaves like you would expect. So if you would want to increase the extrusion of the cube, you could just grab the two appropriate edges and move them together. This is the big selling point for Figma's Vector Networks. Ease of use. Vector Networks don't enable you to create something that you couldn't create with other tools, but it does remove a lot of the friction in the process of creating things. And you can take this even further. Say you want to add a hole to the side of the cube. Just start off by selecting and copying the sides of the cube. You can then duplicate those edges and scale them to the size you want them to be. And just like that, you have a cube with a hole. And to make this hole believable, you just need the inner edge. Again, Vector Networks may not allow you to create something you couldn't otherwise. Instead, they enable workflows that weren't previously possible. Creating Vector Networks With an understanding of what Vector Networks are, we can now take a look at how we would go about implementing them. Graph The main data structure behind Vector Networks is a graph. A graph can be thought of as a collection of nodes and edges. A graph Nodes A graph may have any number of nodes. For our purposes nodes have two properties, a unique id and a position. Edges Edges are the connection between two nodes. Each edge is a composed of two edge parts. An edge part contains a node's id and an optional control point. The labels n0 and n1 will be used to refer to the nodes at the start and end of an edge, respectively. The control points will be labeled cp0 and cp1. If the control points of the edge are omitted, the edge becomes a straight line. Filling the holes Source: https://www.figma.com/blog/introducing-vector-networks/ When working with vector networks, the Fill tool allows you to “toggle” the fill of different “areas” of the graph. These areas can be defined as a sequence of node ids that go in a circle, a loop, if you will. This loopy sequence is referred to as a cycle. In the above example, the cycle would consist of the nodes n0, n1, n3, n4, n5, n6 and n7. These cycles will be written like (0, 1, 3, 4, 5, 6, 7). If you were to count the different visually distinct “areas” of the cycle your answer would probably be three, but you could easily find more than three cycles. What makes this correct or incorrect? The sequence (0, 1, 2, 3, 4, 5, 6, 7) is a cycle and it loops, but it is not what we're looking for. The problem can be illustrated with the “how many triangles” puzzle you might have seen on Facebook. How many triangles are in this image? You should be able to count 24 different triangles depending on which areas you choose to include. But that's not what we want. What we need to find are the 16 small areas. We need a way to find the “small cycles” in the graph. Minimal cycle basis This paper on Minimal Cycle Basis is a bit less dense than most others academic papers (and it has pictures!). Its goal is: …to compute a minimal number of cycles that form a cycle basis for the graph. What does “Minimal Cycle Basis” mean? It's just a fancy way to refer to all of the “small areas” of a graph. You can think of these as the “visually distinct” areas of a graph. Enclosed areas. Left or right? The main tool for finding the “minimal cycle basis” will be determining which edge to choose based on left- or rightness. Should we go to a or b? We'll think of this in terms of clockwise and counter clockwise. curr for current, prev for previous When traveling left, we choose the counter clockwise most edge (CCW) relative to the previous one. CCW When traveling right, we choose the clockwise most edge (CW) relative to the previous one. CW The algorithm We will be finding the minimal cycle basis for the graph we saw earlier. The first step is choosing the leftmost node in the graph. When traveling from the first node, we want to go clockwise (CW). But relative to which edge? For the first node, we imagine that the previous edge is “below” the current one. We then pick the CW edge relative to that. In this case a is more CW relative to prev than b, so we'll walk to a. After the first walk, we start picking the CCW edge. In this case, that's b. We repeat the previous step and keep selecting the CCW edge until we reach the original node. When we reach the original node again, a cycle is found. We now have the first small cycle in the graph. When a cycle has been found, the first edge of the cycle is then removed from the graph. The first edge, (n0 n1), is removed Then, the filaments of the first two nodes in the cycle are removed. In this case, we only have a single filament Filaments are nodes that only have one adjacent edge. Think of these as dead ends. When a filament is found, we also check whether or not the single adjacent node is a filament. This ensures that the first node of the next cycle has two adjacent nodes. We'll see an example of this later. Now we pick the first node in the next cycle. In our graph, there are two equally left most nodes. When this happens, we pick the bottom node, n1 in this case. We then repeat the process from before. CW from the bottom for the first node, then CCW from the previous node until we find the first node. We find the cycle (1, 2, 3). Now we have the cycles (0, 1, 3, 4, 5, 6, 7) and (1, 2, 3). Then we remove the first edge of the cycle and filaments like before. We start by removing the filaments connected to the first two nodes of the cycle. We keep going until there aren't any filaments left. Finding the next cycle is pretty obvious. CW then CCW We now have all the cycles of the graph. This is the minimal cycle basis of our graph! Now we can toggle the fills of these cycles as we please. The math I want to dig into how the clockwise-ness of an edge relative to another edge is determined. The only prerequisite for understanding this section are vectors; arrows pointing from one point of a 2D coordinate system to another. i = (1, 0), j = (0, 1) With two vectors sitting at the origin, i and j, we can create a square like so. For the unit vectors (1, 0) and (0, 1) the square has an area of 1. We can do this with any two vectors. This shape is called a parallelogram. Parallelograms have one property that we care about, which is that their area is equal to the absolute value of their determinant. That may sound like jargon, but the determinant happens to be really useful for us. Take a look at what happens when we move the vectors of the one-by-one square closer together. When the vectors get closer, their area gets smaller. And when the vectors are parallel, the determinant and area become 0. At this point the natural question to ask is what happens when we keep going and the blue arrow is to the right of the green arrow? The determinant becomes negative! When the blue vector j is to the left of the green vector i the determinant of the parallelogram becomes negative. When the opposite is true it becomes positive. The implication for our use case is that we can check whether the determinant of two vectors is positive or negative to determine whether or not a vector is to the left or right of another vector. And we can do this no matter the direction because the area of a parallelogram does not change depending on the orientation of the vectors that create it. The determinant changes when the orientation of the vectors change relative to each other. With this knowledge as our weapon, we can create a function, det(i, j), that takes in two vectors and returns the determinant. The function will return a positive value when j is to the left (CCW) of i. Applying the math Say we're in the middle of the finding a cycle and we're deciding whether or not to move to n0 or n1. Let's move this into the coordinate system. We'll start off with a. We want to get the vector from curr to a, which we do by subtracting curr from a. We'll call this new vector da. We can do the same for curr using prev. Now we can determine whether a is left of curr by computing the determinant of the parallelogram that da and dcurr form. Note that the order is important. If we use da as i the area is negative. If we use it as j it becomes positive. We can do the same with b. With this information as our weapon, we know whether or not a, b and curr are left or right of each other. What do we do with this information? The green zone We will be focusing on determining whether da is more CCW than db relative to curr. Simply put, is da left of db? If da is more CCW than db relative to dcurr, da can be said to be better than db. The first step is to determine whether the angle between dcurr and db is convex. This “convexity” can more easily visualized by shifting dcurr back and imagining an arc like so: The angle is convex If the angle is convex, we we use the following expression to check whether da is better than db. The ∨ symbol represents the logical OR operator in math. Let's take a look at the individual parts of this expression. Is da CCW of dcurr? Is da CCW of db? I find that it's pretty hard to visualize this mentally, so I think of the two different expressions creating a “green zone” where da is better than db. For the first part of the expression (is da left of dcurr), the green area looks like so. The second part of the expression asks if da is left of db. The green area look looks like so. And since it's an OR expression, either of these sub-expressions being true would result in a being better than b. Thus, the green area looks like this: Is a better than b? We use this to determine the better-ness of a when the angle is convex. But what if the angle from dcurr to db is concave? Then the expression looks like so: The only thing that changed here is that the logical OR operator (∨) changed to the logical AND operator (∧). Let's take a look at what happens with the green zones using this expression. Is da left of dcurr? Is da left of db? And since these sub-expressions are joined by logical AND, the green zone looks like so: Using this method, we can always get the CCW or CW most node. And the great thing is that this method is independent of rotation and really cheap to compute. Computing the determinant Given two vectors, the determinant can be computed with this formula: Intersections in the graph Let's go back to our graph for a bit. This graph is the simplest, most optimistic case. This graph only has straight lines, and no two lines cross each other. This box shape has an intersection. The edge (0, 2) crosses the edge (1, 3). With the intersection, the above area looks fillable. But defining the “filled area” is pretty difficult. What makes defining this area so difficult? Consider this rectangle and line. There are two intersections with the edge (4, 5) intersecting (0, 1) and (2, 3). Say the area left of the line is filled. What happens if we move the line left? Obviously the area shrinks, but what if we keep going and move the line outside of the rectangle? Which of these outcomes below should be the result, and why? In this case kinda feels like the rectangle should be empty. But what if we move the line right? Should it then be filled? Sure, but what if we move the line up or down instead? Should the rectangle fill or empty when the line is no longer separating the two sections? Expanding the graph This is how I believe Figma solves this problem. I call it “expanding the graph”, but the engineers at Figma probably use a different vocabulary to describe it. Expanding the graph means taking each intersection, creating a node at the point of the intersections, and then splitting the edges that intersected each other at that point. This is the original graph: The edge (0, 2) intersects the edge (1, 3). When expanded, the graph would look like so: A new node, 5 would be added at the point of the intersection. The edges (0, 2) and (1, 3) have been removed and replaced by the edges (0, 5), (5, 2), (1, 5), and (5, 3). The structure of the graph has been changed Here's a graphic that should illustrate this a bit more clearly. Expanding an intersection Multiple intersections These steps are pretty simple for line edges with a single intersections. But each edge can have multiple other edges intersecting it, and two cubic bezier curves can create 9 intersections. This complicates things a bit. Let's take a look at a bezier-line intersection. The best way to go about this is to treat the intersections for an edge as separate from the edge that intersected it. The t values go from 0 at the start of the curve to 1 at the end of it The line has two intersections at t = 0.3 and t = 0.7. The bezier has two intersections, but at t = 0.25 and t = 0.75. Before we move on with this example, I want to introduce a different way of thinking about edges since I believe it will help with the overall understanding of the problem. Duplicate edges Two nodes may be connected multiple times by different edges. In this graph, an edge represented by the node pair (2, 3) could represent either of the two edges that connect n2 and n3. To get around this problem, we will give each edge a unique id. For most future examples, I will still refer to edges by the nodes they connect since I feel it's easier to think about. But for the next example it's better to separate nodes and edges. Intersection map We can structure the data for the intersections of the edges like so: Creating nodes at the intersection points of edges When we encounter an intersection, we create a node whose position is at the intersection. We then add intersections to an intersection map that will contain the intersections for each edge with a corresponding t value and a nodeId. These intersections are sorted by the t value. For the intersection with the lowest t value, we create an edge with the first edge part having the nodeId of the first edge part of the original edge. The second edge part should contain the nodeId of the intersection. This creates the edge (2, 4). Subsequent edges will have the first edge part's nodeId be the nodeId of the previous intersection and the second nodeId be the nodeId of the current intersection. In this example, that edge is (4, 5). One additional edge will be created for each edge with any intersections. The first edge part's nodeId will be the nodeId of the last intersection and the second edge part's nodeId will be the nodeId of the second edge part of the original edge. This was a bit of a mouthful, so hopefully this graphic helps a bit with understanding that alphabet soup. Separating the intersections of an edge from the edges that created those intersections makes it easier to think about. It alleviates some of the complexity that might arise from multiple edges intersecting with each other. Self-intersection Cubic beziers can self-intersect. This, unfortunately, means that every single cubic bezier edge has to be checked for self-intersection. It's an interesting problem that involves finding the two different t values that the bezier intersects itself at, but I won't be covering how to find those values here. Once you have the t values, a self-intersecting bezier can be expanded like so: The blue node should be invisible to the user We insert n3 since having a node with an edge that has itself on both ends of the edge is problematic, but it should be hidden from the user. Intersecting the loop of a self-intersecting bezier Removing n3 at the first opportunity Curvy edges Earlier we covered the CW - CCW graph traversal algorithm to find the minimal cycle basis (small areas). Finding the better (counter clockwise most) point adjacent to curr But the algorithm described in the paper was designed to work with nodes connected by straight lines that don't intersect. Introducing edges defined by cubic beziers introduces significant complexity. Which edge to choose, blue or green? In the example above, we can find out that the blue edge is better than the green one by using the determinant. We are stilling defining better to mean the CCW most edge. When working with cubic bezier curves, the naive solution would be to just convert the bezier to a line defined by the points at the start and end of the curve. But that idea breaks down as soon as one edge curves over the other. Oops Let's take a fresh look at a bezier curves and try to work from there. Looking at this, we notice that the tangent at the start of the curve, n0, is parallel to the line from n0 to cp0. So to get the direction at the start of the edge we can use the line (n0, cp0). For clarity, the start of our edge, n0, is the same node as curr. So by converting edges defined by cubic beziers into a line defined by (n0, cp0), we get the initial angle of the curve. This seems like a good solution when looking at the “curve around” case. Looks like we've solved the problem. Right? No intersections Before we move on to further edge cases, it helps to understand that any solutions assume that no two edges may intersect when deciding which edge to travel. This is not allowed The edges of the graph we're traversing must not have any intersections when we compute the cycles (minimal cycle basis) of the graph. We can only operate on an expanded graph. Like we covered earlier, an expanded graph is a graph that has replaced all intersections with new nodes and edges. So if the original, user-defined graph has any intersections, they would have to be expanded before we can find the graph's cycles. The same edges as above, but expanded Parallel edges The next edge case is two edges being parallel (pointing in the same direction). If the lines go in the same direction, determining which is better is impossible without more information. Here are a few possible solutions for the cases where the control points of the curves are parallel. Point at t What if we just take the point on the curve at, for example, t = 0.1? This produces the correct result for curves of a similar length, but we can easily break this with one curve being significantly bigger than the other. This is effectively the same problem as the “curve around” case we saw earlier. Point at length Instead of taking a point at a fixed t value, we could take a point at some length along the curve. The length would be determined by some point on the smaller curve, e.g. at t=0.1. I have not tried implementing this since I have another working solution, but this could possibly be a viable and performant solution if it works for all edge cases. Lasers! The next solution is a bit esoteric but produces the correct result. This is the solution I'm currently using. We begin by splitting each bezier at t = 0.05 (image above is exaggerated). We then tesselate each part into n points. Then, for each point of the tesselated bezier, we check whether a line from n0 to that point intersects the other edge. It's pretty hard to see what's going on at this scale, so let's zoom in a bit. When a point intersects the other edge, we use the point before it. Found an intersection Let's zoom in a bit. The intersection close up For the other edge, we have no intersection. In that case, we just use the end of the edge as the direction line. With this method we've produced lines that seem to represent their respective curves. And this also works for the “curve around” case. But it fails for a “curve behind” case. This would produce the green edge as the more CCW edge, which is wrong. My solution to this problem is to shoot an infinite laser in the direction of the previous edge. We then check whether the points of the tesselated bezier intersect this laser. But a line from n0 to the points would never intersect the laser. Passes right through Instead, we can create a line from the current point to the previous point and use that for the intersection test. When we intersect the laser, we use the previous point. The previous point will always be on the correct side of the laser. The point we use And like that, we have a solution. Parallel, but in reverse! It could also be the case that the blue or green edges, a and b respectively, could be parallel to the edge from curr to prev. a is parallel to prev The process for finding the better edge follows a process similar to the one described above so we will cover this very quickly. There are two cases: A or B are parallel to Prev , but not both If either a or b, but not both, are parallel to prev, we can simply compare the parallel edge to prev. If the parallel edge is CW of prev, the parallel edge is better. If the parallel edge is CCW of prev, the other edge is better. Think a bit about why this is true. If one edge is parallel to prev and curves CW, and the other is not parallel to prev, then the parallel edge is as CCW as can be. This means that the green zone for the other edge is completely empty. The reverse is true if the parallel edge curves CCW, since it would be as CW as possible. This means that the green zone for the other edge is the whole circle. Both A and B are parallel to Prev Using the same laser solution as before, this case is covered. Cycles inside of cycles Now we're going to look at fills for a bit. Let's take a look at a basic example of a graph with a cycle inside of another cycle. You would expect the graph's areas to be defined like so: But as it stands, if you hover over the outer area you get a different, unsatisfactory result. But this makes sense. Let's take a look at the nodes of the graph. The cycle (0, 1, 2, 3) describes the outer boundary of the area we want, but we aren't describing the “inner boundary” of the area yet. Let's take a look at how we can do that. Even-odd rule Telling a computer to draw the outline of a 2D shape is simple enough. But if you want to fill that shape, how do you tell the computer what is “inside” and what is “outside”? One way of finding out whether a point is inside a shape or not is by shooting an infinite laser in any direction from that point and counting how many “walls” it passes through. If the laser intersects an odd number of walls, it's inside of the shape. Otherwise it is outside of the shape. Intersects 1 wall, we're inside of the shape Intersects 4 walls, we're outside of the shape This works for any 2D shape, no matter which point you choose and which direction you shoot the laser in. This also helps in the case of nested paths. This gives us an idea for how we can define the “inner boundary” of a shape. Reducing closed walks Let's look at a graph with a cycle nested inside of another cycle, but with an edge connecting two nodes of the cycles. This will lead back to how we can think about nested cycles and give us a deeper understanding on how to think about them. Let's find the cycles. We use the same CW-CCW method as usual. With this method, we go on what looks like a small detour around the inner cycle. When we reach the node we started at, this is what the cycle looks like. This is the first cycle we've seen where we cross a node twice (both n3 and n4). Something interesting appears when we take a look at the direction that the cycle takes throughout the graph. We start off traveling CCW, but when we cross the edge from the outer cycle to the inner cycle the orientation we travel seems to flip. I will state for now that we want to separate the outer cycle from the inner cycle and treat the edge between them as if it didn't exist. I will go into the why later and explain the how here. We take all repeated nodes, in this case n3, and remove them from the cycle. We also remove any nodes that are between the two repeated nodes. You might notice that n4 is also repeated, but since it's “inside” of the part of the cycle that n3 removes, we can ignore it. We leave one instance of the repeated node, and then we have the cycle that would have been found if the crossing didn't exist. We then mark the edge that connected the outer cycle from the inner cycle. I call these marked edges crossings. It could also be the case that an outer-inner cycle combo has multiple crossings. In that case, we mark all edges adjacent to the node connected to the outer cycle as a crossing. And after all this is done, our cycles look like so: Subcycles Instead of referring to “inner” and “outer” cycles, I will refer to subcycles and parent cycles. This will make it easier to think about multiple cycles relative to each other. Having said that, let's introduce a third cycle. When we hover over the outermost cycle, what do you expect to happen? Because of the even-odd rule, the innermost cycle is filled too! To fix this, we can introduce the concept of direct subcycles. Parent cycles (blue) and their direct subcycles (green) A parent cycle may have multiple direct subcycles. But due to the non-intersection rule, a subcycle may only have a single parent cycle. Let's take a look at how this works. This graph has a a rectangle, our outermost cycle, which has two direct subcycles: a diamond and an hourglass. The diamond has two direct subcycles of its own, and the hourglass has three direct subcycles. We will begin with the rectangle and its direct subcycles. We will name them, c0, c1 and c2. The user has decided to fill some of these cycles, and leave some of them empty. c0 and c1 are filled, and c2 is empty Let's draw the graph without a stroke and with a gray fill. When drawing this graph we start with the outermost cycle, c0. The graph to the left with the render to the right Since c0 is filled, we draw it. If it were not filled we could skip drawing it. We can shoot a laser out of the rectangle and see that it intersects the walls of the rectangle once, so we can expect it to be filled considering the even-odd rule. This may seem really obvious, but it's good to have the rules of the game laid out clearly before we move on. Next we want to draw c1, the diamond in our graph. It was filled, just like the rectangle so we should draw it as well. But if we try to draw the diamond as well, we get the wrong result. Our laser is intersecting two walls as a result of drawing both of the shapes when the have the same fill setting. We intersect an even number of walls, so we're “outside” of the shape So to draw the image the user wanted we can simply skip drawing the diamond since the parent cycle implicitly draws direct subcycles with the same fill setting. The hourglass, c2, is supposed to be empty. With that being the case, just not drawing it seems like a reasonable conclusion. But since the parent cycle (rectangle) has already drawn the hourglass as if it were filled we need to “flip” the fill by drawing the hourglass. And again, if we try to use the laser intersection method we see that the number of intersections is 2, an even number. And with the even-odd rule, an even number of walls means you're “outside” of the shape. Now that we've drawn the rectangle and its direct subcycles, we can move onto the direct subcycles of those. When working with c3 and c4, the direct subcycles of c1, we can treat them as if they're direct subcycles of c0 since c1 had the same fill setting. For c3, we want to “flip” the fill setting so we draw it. But c4 has the same fill setting as its parent cycle so we don't draw it. Even number of intersections so we're outside of the shape And we can think of c5, c6 and c7 in the same way. We don't care whether they're filled or empty when rendering them. We care whether or not they have the same fill as their parent cycle. We only need to draw cycles if their parent cycle has the opposite “fill setting” as themselves. If they have the same fill setting, we don't have to draw them. This means that when drawing cycles, start by drawing the outermost “filled” cycle and then look at that cycle's subcycles. If a subcycle has the same fill setting as its parent cycle, it should not be drawn. Contiguous cycles A graph may have multiple “clusters” of cycles. I use the phrase contiguous cycles to describe the “togetherness” of the cycles, if you will. I often think of these contiguous groups of cycles as being in different colors. Finding these contiguous cycles can be done with a depth-first traversal: Start at the first node of the cycle Color each node you find But remember the crossings? In the search, you may not crawl to adjacent nodes by edges marked as crossings. So in the end, our colors actually look like this: Take this group of contiguous cycles nested inside another group of contiguous cycles: Because of the non-intersection rule we know that if one of the nodes in a group of contiguous cycles is inside of a cycle not in the group, all of them are. This “contiguous cycles” idea is maybe not the most interesting part of this post on the surface, but I've found it to be useful when working on Vector Networks. Partial expansion When hovering an area defined by intersections, we are showing a cycle of the expanded graph. Take this triangle as an example. If we hover over one of its areas, we see an area defined by nodes that don't exist yet. What the blue striped area represents is the area whose fill state would be “toggled” if the user clicks the left mouse button. This area does not exist on the graph as the user defined it. It exists as a cycle on the expanded version of the original graph. The expanded graph When the user clicks to toggle the fill state of the area, we would first have to expand the graph for the nodes and edges that make up that area to exist. The expanded graph But by doing that we've expanded two intersections that we didn't need to expand to be able to describe the area. These expansions are destructive in nature and should be avoided when possible. Instead, we can partially expand the graph by only expanding the intersections that define the selected cycle. Partially expanded graph This allows us to maintain as much of the original graph as possible while still being able to define the fill. Implementing partial expansions The basic implementation is reasonably simple. When you create the expanded graph, just add a little metadata to each expanded node that tells you which two edges of the original graph were used to create it and at what t values those intersections occurred. Then when the cycle is clicked, iterate over each node. If the node exists in the expanded graph but not the original graph, add it to a new partially expanded graph. There are edge cases, but I will not be covering them here. Omitted topics Here are some of the topics that I decided to omit for this post. Go have a stab at them yourself! Joins Figma offers three types of joins. Round, pointy and square. How could these different types of joins be implemented? Stroke align Figma also offers three ways to align the stroke of a graph: center, inside and outside. How do you determine inside- or outside-ness and what happens when the graph has no cycles? Boolean operations Figma, like most vector graphics tools, offers boolean operations. How could those be implemented? Paper.js is open source and has boolean operations for paths, maybe you can start there? Future topics These are some of the more open-ended features and ideas I want to explore in the future. A different way of working with fills There are alternatives to how Figma allows the user to work with fills. One possible solution I'm interested in exploring is multiple different “fill layers” that use one vector object as a reference. This would solve the “one graph, multiple colors” problem without having to duplicate the layer and keep multiple vector objects in sync if you want to make changes later on. Animating the graph Given an expression and reference based system similar to After Effects, what could you achieve when you combine it with Vector Networks? Or maybe we could make use of a node editor similar to Blender's shader editor or Fusion's node based workflow? There's a lot of exploration to be done here and I'm really excited to dive into this topic. In closing Thanks for reading this post! I hope it served as a good introduction to what I think is a really interesting problem space. I've been working on this problem alongside school and work for a good while. It's part of an animation editor plus runtime for the web I'm working on. I intend for a modified version of Vector Networks to be the core of a few features. I've been working on implementing Vector Networks for a bit over half a year now. The vector editor is pretty robust when it comes to creating, modifying and expanding the graph. But the edge cases when modifying the fill state have been stumping me for quite a while now. I wanted to have a fully working demo before publishing this post, but it's going to be a few months until it's stable enough for it to be usable for people that are not me. The big idea behind the project is to be a piece of animation software that's tailor-made for creating and running dynamic animations on the web. I'll share more about this project at a later date. I also just think that Figma's Vector Networks are super cool and it's really hard to find material about it online. I hope this post helps fix the lack of information that I encountered when attempting to find information about Vector Networks.",
    "commentLink": "https://news.ycombinator.com/item?id=39241825",
    "commentBody": "The engineering behind Figma's vector networks (2019) (alexharri.com)189 points by alexharri 17 hours agohidepastfavorite28 comments matroid 15 hours agoIt looks like Vector Networks is based on Boris Dalstein's work (https://www.borisdalstein.com/research/phd). He even has a startup (VGC) for Vector Graphic Editing tool based on these concepts. It is pretty cool! P.S. I have no affiliation with his work, although I did contribute 10$ to his Kickstarter Campaign back in the day. reply rjkaplan 14 hours agoparentFigma supported vector networks a few years before that thesis was written (I worked there at the time). reply goodmachine 12 hours agorootparentAre you sure? Boris Dalstein first published his work on Vector Graphic Complexes (VGC) at SIGGRAPH in 2014 [1] Figma introduced Vector Networks in 2016 [2] [1] https://www.borisdalstein.com/research/vgc/ [2] https://www.figma.com/blog/introducing-vector-networks/ reply dfield 3 hours agorootparentCEO of Figma here. Most of the original insights around vector networks were in 2013, though we continued to polish the implementation over time. We didn't exit stealth and ship the closed beta of Figma until December 2015 which is why there isn't blog content before then. At first glance, this thesis looks super neat! I'm excited to check it out! I don't believe I've seen it before which is surprising given the overlap. reply wslh 11 hours agorootparentprevLearning too much from this thread. I tried to look at Wikipedia and it seems there is an opportunity for a page about vector networks! reply nikkwong 8 hours agoprevVector networks are interesting, but I've always found their addition into Figma to be an solution that was implemented by engineers to scratch their own fancy rather than to be used practically in the real world. Vector networks would conceivably allow you to draw with vectors in a way that is laborious in other tools like Illustrator/Affinity because of the two-connected node limitation; allowing you to create a drawing of something like a wheel with 8 spokes that is just a single path rather than 8 paths that are layered on top of each other in other tools. Vector networks feel like the correct way to draw an object rather than the hack that designers have been relying on since the inception of vector-based art because of this limitation. The problem is that well, vector-networks, in conjunction with the standard pen tool, are not enough to actually be useful for drawing. Figma doesn't have any newer path-drawing functionality like a curvature or 'smart-mode' pen tool which draws curves that are fully continuous and pleasant looking. These newer tools are a cornerstone of creating elegant looking shapes in modern vector design programs, but Figma just has the 25-year old pen tool. Figma also doesn't have other tools that are necessary for drawing—reflect, an isometric mode, path smoothing, blending, repeating, etc. It's a strange positioning because vector networks are basically only useful for drawing (like drawing an object like a car, or a scene, or something). I have never used them once for UI design, which is generally based on rectangles where a vector-network isn't needed. And they would make drawing so much better, as the linked post describes. But without any additional helper tools, you just can't create drawings of any complexity on their own; so they end up not being used. Figma could be the best vector drawing tool because of everything else that makes Figma so great—it's wonderful UI, snappiness, great color handling, etc. I want Figma to be able to replace illustrator for me so damn bad. And they are so close. But they have chosen to not focus on drawing at all; so they have this amazing vector-network thing that they're always bragging about—but I doubt anyone's really using it for anything, since it is useless for drawing just by itself. The real goldmine for artists who create vector based art would be if a tool that is actually built for drawing—like illustrator—decided to add vector-networks as a core part of their offering. I mean, at the end of the day, a vector-network is basically a bunch of paths that have a node with a shared coordinate. Doesn't seem like it would be that hard to implement, but Adobe innovates on Illustrator at snail-like speed. reply doubloon 15 hours agoprevpretty awesome. i really feel like Geometric Algebra, specifically the concepts of wedge product and bivectors, could save everyone a lot of time if it was put into the curriculum. because people keep \"rediscovering\" it all the time and calling it different names. (here \"determinant\") reply nyrikki 12 hours agoparentThis! Gibbs/Heavysides style vectors are convenient because it works with our 3D intuition, but Grassman/Clifford style works better for the differential calculus of gradient decent, hyper planes of DBScan, computer graphics etc... Here is a programer friendly site that may be of help for those who are interested. The geometric product being the sum of the exterior and dot products also solves a lot of things you run into with vector and tensor analysis. https://bivector.net/ reply jiggawatts 46 minutes agorootparentI like to think of GA as strongly-typed matrix algebra. Instead of putting everything into one type (a matrix of numbers), it recognises and makes explicit physical concepts such as surfaces and volumes. reply karmakaze 16 hours agoprevI can't tell who this article is for. It describes points, lines, nodes, edges, and Bezier curves, so I thought it might be not so in depth. But it's long, and skimming down I see talk of determinants and think there might be something there. Even thinking that a \"Vector Network\" is some great discovery when it's allowing more than a single cycle in a graph needs earlier substantiation--I don't have time to read all the background intro throughout the piece to the end. I'll guess that the 'engineering' here is a review of some graph and/or 2D graphics algorithms. reply alexharri 16 hours agoparentThis post was intended as a source of information for developers who might want to implement vector networks in their own projects. Back in 2018 I wanted to support vector networks in my animation editor project, but had a really hard time finding information. I didn't have a math or CS background, so just knowing what concepts to Google was somewhat of a brick wall. Googling \"vector networks\" didn't yield any useful results, because there weren't any resources discussing them (aside from marketing content from Figma). So in writing this post, I wanted to create the resource I needed back then. reply riperoni 1 hour agorootparentThis is a nice writeup, but the definition and usage of determinant makes my nails curl. A determinant is defined for matrices, not for vectors and from the purpse of your article I think you skipped over that part for simplicty/brevity. But really, introducing it once properly, then going to the more pragmatic definition of yours aknowledging that it is skipping some parts would be so much better for someone not knowing this field in depth. Or just not calling it determinant. And mentioning that determinants show a sort of scale factor of transformation, also for 3D, would be nice to connect to 3D rendering and such. Since it is meant as intro/tutorial on implementation I would really point into all the right math-relates directions as a reference. Whoever wants, can read or skip those parts. reply epistasis 16 hours agoparentprevIt's for the interested person who doesn't have much background but wants to go super in depth on the feature. I've taken some computational geometry classes, and loved the interesting theorems and which seemingly trivial problems were actually extremely difficult to prove, and which seemingly complex things were easy to prove, it's a very fun field. Still, I had never actually written the code or seen the algorithms for drawing Bézier curves, so this was great for me. I love drawing with vector graphics, but haven't used Figma much, so seeing their vector graphs was very enlightening for me. I'll be using Figma in the future. It feels like this article was tailor made for me, I loved it. reply jsejcksn 16 hours agoparentprevParagraph five: > This post can be thought of as an introduction to a really interesting problem space, and as a resource for people interesting in making use of some aspects of Vector Networks for future applications. I hope it succeeds in providing value to both developers being introduced to new concepts and ideas, and to designers interesting in learning more about the tool they know and love. reply ricardobeat 10 hours agorootparentSeems like that was too far into the article for them to reach. But there’s always enough time for a lengthy rant about their own inability to focus for a few minutes (and how the article itself is somehow at fault!). reply syspec 1 hour agoparentprevId rather it exist in its current form than not exist. Sometimes it's just hard to write blog posts. How much is too little info, how much is too detailed. It's actually really hard stuff that we all wrestle with when writing. So to the authors, thanks! reply tsuru 13 hours agoprevI haven't had time to deep read all the way through yet, but this feels so similar to the half-edge structure used in many 3d mesh editing programs but in 2d... reply muglug 9 hours agoprev> The big idea behind the project is to be a piece of animation software that's tailor-made for creating and running dynamic animations on the web. I'll share more about this project at a later date. Macromedia/Adobe Flash was exactly this, and supported vector networks. The only big difference relevant to the article was that Flash supported quadratic beziers natively, but not cubic ones. That meant no self-overlapping curves were possible. reply rgovostes 16 hours agoprevComprehensive! > In this case, that's b. We repeat the previous step and keep selecting the CCW edge until we reach the original node. Isn’t it a? reply spankalee 11 hours agoprevThis is a cool feature of Figma, but in the frontend world it's the type of thing that makes Figma not a great tool for UI design because it's hard to translate to UI frameworks. reply ricardobeat 10 hours agoparentWhat does that even mean? Designers can create hard-to-implement UI using plain shapes and lines, vector networks don’t change that. And, we have had full SVG support for ages now. reply CharlesW 10 hours agoparentprevIs it not as simple as using a plug-in to export to SVG, Lottie, Swift UI code, or whatever? reply coldtea 10 hours agoparentprevYou're not supposed to (or to expect to) translate a vector drawing to \"UI frameworks\". reply spiderice 9 hours agorootparentI mean, you could expect to be able to translate from your UI design software to your UI framework. Not sure why you're creating that limitation. reply coldtea 7 hours agorootparentBecause the \"vector networks\" and vector graphics editing are for the graphic design part of Figma, not the UI design one. You don't just draw whatever random vector shape and turn it into UI. You create UI elements and arrange them in UI related containers (grids etc) for that. The \"vector drawing\" is for the decoration of the UI, not for its definition. reply jansan 12 hours agoprevWhere in Figma can I find those vector networks? I see only see very basic path editing features.. reply ricardobeat 10 hours agoparentThis is about the basic path editing and how it differs from traditional software. Reading the first 30% of the post will be enough to see what you’re missing. reply tuukkah 15 hours agoprev [–] TLDR: How to extend Bezier curves to a graph in vector graphics editors. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Figma's Vector Networks is a redesigned pen tool in graphic design that enhances the functionality of paths by using nodes and edges.",
      "The tool allows for the creation and modification of complex shapes with precision, using concepts such as Bezier curves and graph structures.",
      "The passage explores algorithms for finding cycles and determining the best edge, as well as concepts like fill states, subcycles, and color maintenance in a graph."
    ],
    "commentSummary": [
      "Figma's vector networks are a feature in the design software that enables users to create more intricate and adaptable vector graphics.",
      "Boris Dalstein, the founder of VGC, contributed to the development of vector networks, but Figma had already implemented this feature before his work.",
      "Opinions about the practicality of vector networks vary among participants, with some suggesting that Figma should provide additional tools for drawing complex shapes."
    ],
    "points": 189,
    "commentCount": 28,
    "retryCount": 0,
    "time": 1706977600
  },
  {
    "id": 39243044,
    "title": "DIY MBA: Essential Reading List for 2019",
    "originLink": "https://chrisstoneman.medium.com/diy-mba-my-reading-list-f7699bd7d0c6",
    "originBody": "DIY MBA: My Reading List Chris Stoneman · Follow 2 min read · Jul 12, 2019 -- 4 I’m doing a self-taught DIY MBA (read more). Here’s my work-in-progress reading list, add anything you think I’m missing in the comments. Accounting & Finance : Completed. Read the module overview The 10 Day MBA by Steven Silbiger The Portable MBA in Finance and Accounting by Grossman & Livingstone An online course is probably better than a book, I did this one by Acumen Learning. Product, Design & Marketing Inspired: How to Create Tech Products Customers Love by Marty Cagan (notes) Lean Startup by Eric Ries (notes) Hooked: How to Make Habit Forming Products by Nir Eyal (notes) The Design of Everyday Things by Don Norman (notes) Perennial Seller: The Art of Making and Marketing Work that Lasts by Ryan Holiday Marketing: A Love Story by Bernadette Jiwa (notes) Building a Story Brand by Donald Miller (notes) Crossing the Chasm by Geoffrey Moore (notes) Naked Statistics by Charles Wheelan (notes) Organisational Behaviour, Management & Communication High Output Management by Andy Grove (notes) The Coaching Habit by Michael Bungay Stanier Act Like a Leader, Think Like a Leader by Herminia Ibarra Talking from 9 to 5 by Deborah Tannen Influence: The Psychology of Persuasion by Robert Cialdini Hard Thing About Hard Things and What You Do Is Who You Are by Ben Horowitz. Resilient Management by Lara Hogan Never Split The Difference by Chris Voss Global Economics & Investing Capitalism and Freedom by Milton Friedman Debt: The First 5,000 Years by David Graeber Economics: A User’s Guide by Ha-Joon Chang Winner Take All: China’s Race for Resources and What It Means for the World by Dambisa Moyo Economics in One Lesson by Henry Hazlitt The Most Important Thing: Uncommon Sense for the Thoughtful Investor by Howard Marks The Intelligent Investor by Benjamin Graham Strategy & Systems Thinking Good Strategy/Bad Strategy by Richard Rumelt Platform Scale by Sangeet Paul Choudary (notes) 7 Powers: Foundations of Business by Hamilton Helmer (notes) Blue Ocean Strategy by W. Chan Kim & Renée Mauborgne Certain to Win by Chet Richards Getting the Right Things Done by Pascal Dennis Business Model Generation by Alexander Osterwalder & Yves Pigneur Theory in Practice by Chris Argyris & Donald Schön Thinking in Systems by Donella Meadows Antifragile: Things that Gain from Disorder by Nassim Nicholas Taleb The Fifth Discipline: The Art & Practice of The Learning Organization by Peter Senge The Innovator’s Dilemma by Clayton M. Christensen Creativity & Getting Stuff Done The Seven Habits of Highly Effective People by Stephen Covey Creativity, Inc: Overcoming the Unseen Forces That Stand in the Way of True Inspiration by Ed Catmull A Beautiful Constraint by Adam Morgan & Mark Barden Steal Like An Artist by Austin Kleon Finite and Infinite Games by James P Carse The War of Art by Steven Pressfield The Art of Possibility by Rosamund Stone Zander Deep Work by Cal Newport Have I missed anything? Are there better alternatives to these? Let me know with a comment!",
    "commentLink": "https://news.ycombinator.com/item?id=39243044",
    "commentBody": "DIY MBA: My Reading List (2019) (chrisstoneman.medium.com)177 points by yamrzou 15 hours agohidepastfavorite83 comments amb23 13 hours agoThis isn't a great list--it's a lot of fluff, and self-help books masquerading as business content. There are basically two skills you really need to hone in an MBA. The first are the hard skills/knowledge on finance, accounting, (basic) stats, and (basic) economics. The second is being able to assess and analyze business models, from both strategic and operational angles. The former can be learned in MOOCs, while the latter is best approached with case studies, ideally discussed as a group. I'd honestly suggest trying to start a business/entrepreneurs book club with ~12-20 people to try to replicate that experience over reading books on your own. reply minimalist 9 hours agoparentWhat are some good publicly-accessible collections of case studies if someone wanted to start a discussion club? Someone once told me that part of the value proposition of the best business schools is the library of case studies that they amass (and sell). Is that true? reply mdorazio 6 hours agorootparentIf you want good, you're unlikely to get free. We are talking about business, after all. The standard is HBR, which you can buy for what I believe is a reasonable cost: https://store.hbr.org/case-studies/ reply in0v8r 8 hours agorootparentprevYou are not alone in this line of thinking. A case study repository would be an invaluable find. reply hnthrowaway0328 8 hours agoparentprevFrom what I observe in many companies, you only need one skill: fail up. reply klipt 8 hours agorootparentThe real value of an MBA is the network you create with other MBA students. That's why a Harvard MBA is worth more than (unknown school) MBA - the top companies hire Harvard MBAs who then have a business network with other Harvard MBAs - who of course got hired at other top companies. It's self reinforcing. reply hnthrowaway0328 6 hours agorootparentYep, and very important to fail up and fail horizontally. reply aik 12 hours agoparentprevI agree some of these are fluff. However many here are great to have basic knowledge of for specific scenarios one may run into and then know where to dig deeper when needed. Agree though to marry being a ferocious reader / learner with being a do’er is the way to go. Personally I did not grow up with an opportunity to develop an intuition about business and self-motivated action. Has taken a lot of work to move that direction over the years. reply gavinray 8 hours agoparentprevAren't business models complete bullshit? I've spent a decade in startups, and pitch decks along with projections are more or less riding-the-edge-of-grifting fluff. If it were possible to accurately forecast cost and revenue, then everyone would start a profitable business, right? reply mdorazio 6 hours agorootparentYou're conflating startups with the broader business world. Startup models are shit because there are so many unknowns and it's in the interests of the people making them to fluff them up as much as possible. In the other 99% of businesses, business models can be quite accurate and are valuable tools for decision making and evaluation. For example, public companies often issue revenue guidance and analysts do their own earnings projections based on models. reply xyzelement 8 hours agorootparentprevIdeally your business model is evidence (including to yourself) that you've done some hard thinking about who your customers, suppliers, and competitors are - and about some basic dynamics of your space. It doesn't mean anyone should expect it to go that way exactly and the deviation from projection could be quite large, but it's still miles ahead of someone who's not bothered to think through it at all in most cases. reply fsckboy 3 hours agorootparentprev>If it were possible to accurately forecast cost and revenue, then everyone would start a profitable business, right? that doesn't follow: it's just as likely if you could accurately forecast cost and revenue, then you wouldn't start many businesses because you'd know they wouldn't be profitable reply refurb 7 hours agorootparentprevEveryone knows the forecasts don’t accurately predict the future. That’s not the point of forecasts. The value of models is the assumptions you put in. How big is the potential market? What price do you think you can charge? How much of the market will go to competitors. The value of forecasts is in pressure testing assumptions and opportunity. reply cqqxo4zV46cp 6 hours agorootparentprevThere is a world outside of the Bay Area, I assure you. reply mi_lk 11 hours agoparentprevWhat specific MOOCs are recommended for the first one? reply cheriot 9 hours agorootparentI took some of the Wharton first year MBA classes for free on Coursera. Left me incredulous that people take out huge loans for that stuff. It's Wharton, though, so the signaling alone may be worth the price. reply csa 7 hours agorootparent> It's Wharton, though, so the signaling alone may be worth the price. You’re (mostly) not paying for the content of the courses. Reasons to go/pay (in no particular order): 1. It’s a finishing school, with all the good and bad that implies. Some of the details below fall into that category. 2. Developing your peer network. Includes classmates, professors, and alumni. 3. Access to incredible research opportunities and research resources (yes, even for MBAs). 4. Access to jobs that (mostly) only go to grads of Wharton and similar. Think consulting and IB. 5. Access to job networks of niche and often super interesting jobs via peers and professors. 6. Signaling, although this does work both ways. While it opens doors for some jobs, it makes you overqualified for others. 7. Being in an environment of driven and (mostly) competent people. The scope of “what is possible” will probably be redefined for most/many Wharton students. (This might be worded somewhat poorly… not exactly sure how to say it). 8. Related to 7, being graded on a curve against motivated and competent peers can… build character. I’m not sure what it’s like these days. 9. In the past, there were some niche concentration courses that were super interesting. I’m not sure what they are now, and I’m not sure they make it to MOOCs. 10. Wharton Follies. To close, if someone goes to Wharton primarily for access to the classes, then I would say that they have pretty much failed Wharton, regardless of what their grades end up being. Most of the knowledge from the classes can be gained more easily and cheaply in other ways. reply thruhiker 8 hours agorootparentprevI always thought the sums people were paying for MBAs was absolutely outrageous. In tech I feel like MBAs can actually work against you and can be perceived as a red flag. I knew someone a decade ago who paid I believe $120k to Duke Fuqua for a mostly online MBA that also required travel to multiple international destinations. This was a person in their late 20s. reply refurb 1 hour agorootparentAn MBA is a prerequisite for many high level jobs. Management consulting partner, investment banking managing director are just a few. There are paths to those roles that don’t include an MBA, but they are rare. The reason people pay is the math works out. Spending $120k to unlock career tracks that include roles that pay $500,000 or more is a pretty straightforward cost-benefit analysis. My spouse did an MBA at a top school and 15 years out a very large chunk of the class are CEOs of large corporations, founders of private equity firms, etc. The value is mostly in the signaling and network. Being able to get time with half a dozen CEOs simply because you went to the same school is invaluable. reply cqqxo4zV46cp 6 hours agorootparentprevI’ve done part of an MBA program. I’ve only hit pause because I couldn’t make the workload work with how demanding my job is at the moment. That does mean that beyond experiencing the program itself, I spent a fair bit of time looking at MBA program curricula. What I found was a pretty significant amount of study into areas that to me seem integral to actually running a business in reality. The sort of stuff that techies love to pretend doesn’t exist, or that they can just intuit with a combination of reading a few Wikipedia articles + their largely incorrectly self-identified transferable expertise. I don’t know if my MBA program is particularly good. Whilst it’s run by a legitimate institution it’s certainly not a particularly prestigious one. I’d expect that it is in a lot of ways very middle-of-the-road. One thing it has done is reaffirm my pre-existing vague suspicion that, whilst there are obviously no shortage of formulaic, uninspired, and largely street-dumb MBAs out there, a lot of the hate for those with an MBA that I see in communities like Hacker News is probably misdirected hatred toward the realities of business. Especially given how many people here have grown up in a zero-interest environment where they could bounce from place to place being paid very well to sit around in beanbag chairs thinking about engineering problems whilst the money fairies without fail backed truckload after truckload of cash up to their San Francisco office loading docks. reply j45 9 hours agoparentprevSome fluff isn’t common knowledge. MBA programs are also often predicated on predicting the next y years based on the past x years. This is increasingly hard for mbas to do or stay ahead of because how things are done in business isn’t the same, and changing faster than mbas are updated and taught. Like anything it not what your education makes of you, it’s what you make of your education. reply ghaff 14 hours agoprevAs someone who got a long ago MBA (and has also worked as an engineer), a few comments. That's a lot of books. I'm not sure it's more total reading than I did in an MBA program but it's certainly more books than I read. I'm not sure the best way to replicate case study discussions--which is a big part of most actual MBA programs and is probably at least somewhat important. (Also projects as opposed to just reading.) Seems very light on finance and accounting. Honestly accounting doesn't need a lot--the main lesson is things are this way because the financial accounting standards board says it is--but the equivalent of core Finance 101 is probably necessary even if you're not going to be primarily a finance person. What I didn't have in B-school but would want today is a deeper dive into VC/startup/etc. term sheets and the like. There's no real operations on that list. Probably don't really need real operations research but some basic manufacturing lingo and concepts would be useful. I'd probably look for a basic Marketing 101 sort of book/course in addition to UX and things along those lines. I'll stop there but I'd probably encourage more reading along the lines of what's actually used in MBA programs and looking at ways/materials that are closer to case studies and learning by doing. reply dkjaudyeqooe 14 hours agoparent> That's a lot of books. When teaching yourself you need multiple sources as a stand in for things being explained different ways by a teacher or through discussions. A fair metric is about 3 books where you'd otherwise use 1 in class. reply ghaff 14 hours agorootparentI think if I were actually putting together a curriculum (and not worrying about copyright/access), I'd probably have a lot more chapters of books and articles/papers/webpages/videos than I would complete books. And I think you need to bring in discussions/projects in some manner in any case. reply youngtaff 48 minutes agoparentprev> I'm not sure the best way to replicate case study discussions--which is a big part of most actual MBA programs and is probably at least somewhat important. (Also projects as opposed to just reading.) As someone who did an MBA twenty years ago, I think this is a really important point The value for me wasn’t the reading, it’s the discussion the exploration of different (sometime opposed) viewpoints and understanding the differences between them Talking to friends who’ve done other Masters degrees one of the key things we all agree on is they taught us how to “think better” and not sure you’ll get that from reading a bunch of books reply giantg2 13 hours agoprevThe author does acknowledge this in thier linked post, but I think it's worth pointing out. They are in the UK, where MBAs are not as credentialized. DIY MBAs are low value. You can get a BS in Business Administration, or even a minor in it, and have more knowledge and experience than from reading these books. Guess what? Even with those credentials, they will not help you. \"I have an MBA from [prestigious school]\" are the magic words. The hiring managers don't care about lesser credentials or actual knowledge. reply aik 12 hours agoparentIt’s very likely this person doesn’t care about the credentials but rather just wants to gain the knowledge to do something themselves. I more or less did this for years by reading a ton of books + many years of HN / articles / interviews / videos + startup weekend events + tried starting things + coding hobby for years + enterprise consulting work, and now I’m fairly well equipped to co-lead the company that I do. The first 5+ years of doing this I debated attempting to go to a prestigious school for an MBA, primary reason (95%) was for the networking opportunity (meeting great likeminded or driven people), (0% because I would’ve been able to tell people I had an MBA from somewhere for job purposes, the snobbery from that would have been a detriment often instead I believe), and 5% for the education which I think has value but when it’s free and I had the motivation there was no reason to pay $100ks for that. At this point this company is doing fairly well so although I would love to meet more driven and likeminded people, I’m not sure it the best path to do so anymore for me. reply satellite2 10 hours agoparentprevA classic case of Goodhart's law. If your metric is convincing a (rather bad) hiring manager, why even bother? And once you've convinced them, what do you do? reply dylan604 10 hours agoparentprev> DIY MBAs are low value. MBAs are low value...depending on perspective. Around these parts, they are deemed very value, or at least in low regard. Look at Boeing as exhibit A. If you're an engineering company that allows MBAs to make the decisions, you won't be a good engineering company for long. At that point, you become the dead carcass that the MBAs point to as to what their purpose is. reply wslh 12 hours agoprevThe author missed the Fred Wilson's MBA Mondays Archive [1] which personally was a great resource in the quest for my \"business success\" (whatever it means). There is a more organized and illustrated edition here [2]. BTW, I always return to some articles such as \"Commission Plans\" for Sales [3]. Regarding books: - Only the Paranoid Survive by Andrew Grove (Intel) [4]. It is always in my \"pocket\". It is real experience with pain points from a top CEO, not an academic exercise. - Other books that are not focused on business but are more \"epistemological\". For example, \"How Life Imitates Chess\" by Garry Kasparov [5]. I don't know who created this title for the book though. Many autobiographies, in general, or good business biographies such as \"Hard Drive: Bill Gates and the Making of the Microsoft Empire\" [6]. [1] https://avc.com/category/mba-mondays/ [2] https://mba-mondays-illustrated.com/ [3] https://avc.com/2010/08/commission-plans/ [4] https://www.amazon.com/Only-Paranoid-Survive-Exploit-Challen... [5] https://www.amazon.com/How-Life-Imitates-Chess-Boardroom/dp/... [6] https://www.amazon.com/Hard-Drive-Making-Microsoft-Empire/dp... reply gumby 14 hours agoprevHis list begins with The 10 Day MBA which is an excellent book I recommend highly. It gives a brief overview constituting 90+% of what you need to know. The other books on his list, many of which I have read and enjoyed myself, “merely” give greater detail on the material covered by T10DMBA (except macroeconomics which is rarely MBA-relevant anyway). The book is humorous, but quite serious about the material. reply ghaff 14 hours agoparentIt's probably useful to know something about the various macroeconomic theories so you can at least nod knowledgeably :-) But it's not that useful practically. Traditional micro is worth knowing. I was also lucky enough to take a behavioral economics course (before it was called that) and a somewhat related pricing strategy course. reply dotancohen 13 hours agoparentprev> 10 Day MBA Googling I see at least three different books with similar covers but attributed to different authors. Which one should I persue? reply deskamess 13 hours agorootparentThe list specified the author: The 10 Day MBA by Steven Silbiger reply dotancohen 12 hours agorootparentTerrific, thank you. reply toomuchtodo 14 hours agoparentprevWhat would you recommend for the last 10%? reply greggsy 14 hours agorootparentSounds cliched, but the remaining 10% is real world experience applying and reflecting on a myriad of (sometimes pseudo-) psychology concepts developed over the years in the areas of leadership, self awareness, and workplace behaviour. reply ghaff 14 hours agorootparentProbably more like 50%. I got an MBA and pretty much realized I had very little interest in actually being a manager. reply greggsy 8 hours agorootparentI think there’s a lot of ‘teachings’ in those books, but the applying and reflecting it is the ‘10%’. I also did an MBA, but my incentive was mainly for the technical skills. Ive been lucky in that I’ve always worked in high performing and highly experienced teams within my industry (th cybers), so the need to ‘manage’ people is somewhat moot, but it did teach me a lot about managing and influencing and coaching other less experienced (or unprofessional) teams and stakeholders. reply Quinzel 10 hours agoprevI was going to give the list of readings I had for my MBA which I finished last year, but my comment was too long to be posted here. I read a lot. Probably at least 10x more than what is listed in this DIY MBA list. However, the reading was probably only 10% of the actual effort that went into my MBA. There were a lot of projects, group projects and case studies to do, as well as real-life consultancy work, and meetings... with other students, and with real companies. And then... there was also lectures, about 8 hours a week worth of lectures, and then annoying interactive activities I had to complete each week, which were designed to prove that I was indeed actually doing the readings. Some courses wouldn't let you progress to the next week of readings etc, unless you completed the interactive activities, which would then unlock the next week's coursework. Other courses would just take a percentage off your final grade for every activity that was missed. A list of readings is just kind of not really the same experience, even if it contains a bit of the same knowledge. It's also about how you take the knowledge and implement it in real-life situations. reply shay_ker 14 hours agoprevCurrently going through Andrew Lo's MIT Sloan course, Finance Theory 1, for free: https://ocw.mit.edu/courses/15-401-finance-theory-i-fall-200... The lectures are well structured. Class questions are good. And best of all... these lectures were done during the 2008 financial crisis, so you see how people react in real time. Fascinating. reply ghaff 14 hours agoparentYou make a great suggestion. Not that B-schools get everything right or that they're on the cutting edge but, to the degree curricula and lectures are online, I'd probably lean that way more than bestselling business titles. (Which, truth be told, often have a chapter or two that distills down a lot of the key content. In some cases, they're worth the lengthier read to reinforce the key content but often they're not.) reply shay_ker 14 hours agorootparentI find lectures much better than books. A lot more engaging and good lecturers are also good storytellers. Especially if you watch lectures at 2x the speed, you can save A LOT of time. And I'd bet your understanding is better, since lecturers can think through problems out loud. Indispensable, IMO. reply ghaff 14 hours agorootparentI sort of hate doing the 2x thing but I agree with your broader point. My good lecturers were, as you say, good storytellers. Including the guy who won the (I know not really before some pedant corrects me) Nobel Prize for behavioral economics. reply Frogeman 13 hours agoprevAs a previous M7 FTMBA student who dropped out halfway through to do a startup, the real MBA is starting a company. Venture, cash flow/lifestyle or otherwise. reply techwizrd 13 hours agoparentThere have been a few companies advertising something like this (e.g., oneday.io) where you build a startup and earn an accredited professional MBA along the way. reply jimmar 13 hours agoprevYou can't get the MBA experience reading books alone. People expect MBAs to be able to communicate effectively, present themselves confidently, and to be able to manage projects. Every MBA program I know requires students to complete projects to apply concepts, often working with other students and real businesses. A DIY MBA experience would involve a lot of consulting with businesses to help solve their challenges. reply ghaff 12 hours agoparentThere's some element of basic accounting/finance/operations/marketing/etc. that you can get pretty quickly. But, as you say, it's not the equivalent of going through a longer interactive program than reading a few books. reply rimeice 12 hours agoprevBeen running my startup for 7 years, not a unicorn but fairly successful. Can honestly say I’ve not made it through a single one of these books. Certainly have tried and have got a few nuggets, but nothing has compared to learning this stuff first hand, on the job. Sure having passable knowledge of this list is useful, but you’re going to hire in people/consultants/external providers to do almost every module on this list for you at some point, you’ll learn from them firsthand. You don’t need to read this list to get a business off the ground and by the time you do you’ll need to surround yourself with these specialists in some form pretty quickly. reply ipnon 1 hour agoprevMunger recommended 6 intro textbooks that every businessman should know: one each in psychology, accounting, microeconomics, evolution, math and physics. reply kwar13 6 hours agoprevThis list is long on fluff and extremely short on any hard technical skills such as corporate finance and valuation. If you're interested in seeing if finance is for you, I highly suggest reading Investment Valuation by Aswath Damodaran. Highly respected professor and most of his stuff is free on his website as well. reply justajot 14 hours agoprevJosh Kaufman did a lot of work in this area for years, maybe still does -- Personal MBA. https://personalmba.com reply afryer 9 hours agoparentPersonalMBA really is a my go-to reference because it is very comprehensive and extremely accessible online. First 5 chapters tackle the strategy behind the '5 parts of every business' 1) Value Creation 2) Marketing 3) Sales 4) Value Delivery 5) Finance The next 6 Chapters deal with the tactics involved in executing that strategy. 6) The human mind 7) Working With Yourself 8) Working With Others 9) Understanding Systems 10) Analyzing Systems 11) Improving Systems Then If you REALLY wanted to dive deeper, you can read the 99 business books he built the personalmba from: https://personalmba.com/best-business-books/ Josh Kaufman created something great here. reply richardboegli 9 hours agoparentprevA friend bought this as a gift for me 10+ years ago: The Personal MBA: Master the Art of Business by Josh Kaufman [0]. I've given it as a gift to other friends. [0] https://www.amazon.com/Personal-MBA-Master-Art-Business/dp/B... reply realusername 14 hours agoparentprevI'm currently following the marketing section of this one and it's been really helpful to compensate for my huge gaps in knowledge reply WoodenChair 5 hours agoprevListening to our podcast episodes on Business Books & Co. won't quite get you as much as reading the actual books, but it will follow a sort of 80/20 rule in that I think our episodes cover 80% of the major points in each book. Here are the episodes we've done on books in that list based on the categories the author ascribed: Product, Design & Marketing - S4E2 The Lean Startup by Eric Ries https://pnc.st/s/business-books/5d9f9531/the-lean-startup Organisational Behaviour, Management & Communication - S1E1 High Output Management by Andy Grove https://pnc.st/s/business-books/095f226633d34496/high-output... - S3E4 The Hard Thing About Hard Things by Ben Horowitz https://pnc.st/s/business-books/1067de4d/the-hard-thing-abou... - S3E7 Never Split The Difference by Chris Voss (author interview) https://pnc.st/s/business-books/920fe358/never-split-the-dif... Creativity & Getting Stuff Done - S2E2 Creativity, Inc: Overcoming the Unseen Forces That Stand in the Way of True Inspiration by Ed Catmull https://pnc.st/s/business-books/6e3f927715094617/creativity-... We also have 30+ more episodes... many are book summaries/discussions but about 1/3 of recent episodes are author interviews. You can find all of our content for free on your podcast player of choice here: https://pnc.st/s/business-books reply loughnane 13 hours agoprevThis list is long. The problem with long reading lists is that it forces you closer to a box-checking mentality. That's dangerous for learning because people aren't computers---we don't read a book, ingest all the information, go onto another one, and then end with the sum total of the information as useable knowledge. We need context. That's why the order of books, selections from books, and the discussion around books is so much more important than the books themselves, at least if your goal is true understanding. This list is a good start, but if it is to help someone learn, it should be more than a list. reply giantg2 13 hours agoparent\"The problem with long reading lists is that it forces you closer to a box-checking mentality.\" Most MBAs are checkboxes so someone can get a higher paying job. Most people do not care about the theory, nor are they really concerned with being a good leader outside of their ego. reply werrett 13 hours agorootparentYou’re being purposefully reductionist. But even if that’s that’s the sole purpose any one would do seek an MBA the GPs point stands — reading this list doesn’t even tick the “MBA checkbox” reply giantg2 12 hours agorootparentThe people seeking these kinds of lists are seeking checkboxes to tick. Of course this isn't the same as earning an MBA, but I guarantee some of the people searching for this kind of content will find it and follow it, believing it will get them somewhere because they checked the boxes on the list. reply loughnane 13 hours agorootparentprevRight, but with a DIY MBA you don't have a credential to point to. At that point isn't all you have what you learn? reply giantg2 12 hours agorootparent\"At that point isn't all you have what you learn?\" Yes, but without the credential will you be given the opportunity to use it? Is it worth anything without being used? These are not the kinds of questions readers are asking themselves when looking for MBA reading lists. Many see the MBA as a checkbox (which it is) and seek checkbox style lists of books. So the length/style of the list isn't leading to the checkbox mentality, rather that mentality is common in the context of credentialization. reply loughnane 11 hours agorootparentMaybe I midunderstood. I read this as \"get your own MBA through reading\", not \"read these books to help get your official MBA\". In any case, I think we agree that credentialization, flawed though it may be, is a reality that we've got to deal with, whether I like it or not. reply Ozzie_osman 10 hours agoprevAh, the book Hooked + the aspiring MBA... A match made in heaven. And probably more responsible for regrettable product decisions than any other combination in modern history. Signed, An MBA who read Hooked reply primax 10 hours agoprevI was surprised when I did my MBA how they didn't cover anything to do with entrepreneurship... but then again it is a masters of business administration. This is an alright list of books, but as other posters have said anyone following it will sorely miss out on practice experience. reply voisin 10 hours agoparentEntrepreneurship is the application of everything else they teach - marketing, communication, finance, etc etc. I have a hard time believing it can be taught. You either have the drive to apply other topics in a risky way or you don’t. In the entrepreneurship courses I took in my MBA, the approach was nowhere near what I did in practice starting a business nor what any other entrepreneur I know did. But the rest of the classes were of value (mostly…) reply w10-1 13 hours agoprevSchooling should ground one in fundamentals, to some degree. Maybe target fundamental principles instead of recent opinion if you have the time to learn and want to tranform your thinking for a lifetime? Both MBA and medical schools are instead practice programs, where case studies exercise knowledge and principles you learned deeply or crammed. If you crammed, you get good at diagnosing and become primary care or first-line specialist. If you learned deeply, you can do M&A or be a researcher or nth-line specialist - likely some combination - or you flame out for lack of political skills or interest in the small niche available. Because they are practical, they produce a large stream of knowledge, for every business or medical case, that outsiders can enjoy and get familiar with. But the knowledge wouldn't qualify them to take on responsibility. That requires a series of validation steps where you handle increasing responsibility. So DIY MD/MBA won't get you in any door. For business fundamentals, I would recommend reading transaction cost economics to understand the value of e.g., \"disruption\" or operations research in business. Marketing or product management is just hunting or bearing fruit; they make you a useful tool. TCE answers the real questions: what makes some companies huge or other markets scattered? It's grounding for strategic decision-making. reply Dowwie 13 hours agoprevThat's really not what business school is about, but studying these and not just reading them could help reply __loam 2 hours agoprevList of books read by obnoxious people. reply xixixao 13 hours agoprevFor some reason my brain went the opposite way and I was wondering what would an MBA in DIY skills look like. reply sklargh 13 hours agoprevAlternate reading / requirement list. Basically covers meaningful coursework and offers some useful books about how large organizations function (or do not), plus some mind-stimulation on power dynamics. In no particular order. - MOOC, Fin, Acct, Stat, Calc, Ops 101, 102 - Excel, SQL - Launch Decision - The Prince - Starboard Olive Garden Slide Deck - Ogilvy’s tips on writing (it’s what you don’t send that counts folks) - Barbarians at the Gate - Pearl Harbor Warning and Decision - On Bullshit - This is Water - Invisible Asymptotes - Toyota Production System - The meeting scene in Margin Call remains the most accurate portrayal of a high-stakes meeting as a junior staffer - Deploy a simple marketing and e-commerce stack and make some profit - Use slides to teach someone about something intensely uninteresting - Become effective at using CHAMPS KNOW forecasting heuristics - Read and understand a few mid-cap and large-cap 10-Ks and 10-Qs. reply deskamess 13 hours agoprevWasn't there an Udemy course that was really popular? Has anyone had any experience with that one? reply erichmond 12 hours agoprevYou want a real DIY MBA? Start a company. That's it. :D reply gargs 14 hours agoprevWould this help in getting jobs as much as a real MBA would? reply ghaff 14 hours agoparentTo the extent that a real MBA credential is important for a job as it was when I was initially hired out of B-school, no? To the degree that an MBA from certain elite institutions can create connections, no? To the degree that you are familiar with various concepts that may be relevant for your job, possibly? Especially later in career. Absolutely no one cares what degrees I have at that point. reply itsthecourier 13 hours agoprevI would like a books describing how to build roles and systems, to grow from 50 to 200 employees, any ideas? reply nprateem 11 hours agoprevShout out to Four Steps to the Epiphany, since the act of actually researching and validating business ideas, then developing a sales pipelines touches on the majority of things that are important for early stage startups. Add in some books on leadership, HR and accounting if your product takes off. reply silexia 13 hours agoprev [–] Autobiographies are the best way to learn firsthand from those who have \"won\" at business: Sam Walton - this is easily the best. Solid gold nuggets of wisdom every page. Henry Ford - strong business, science, and tech ideas. Plus his biography of his good friend Thomas Edison is great. Larry Ellison - pretty solid. Richard Branson - okay autobiographies, probably too many though. reply graemep 13 hours agoparent [–] The problem with reading \"winners\" is the huge survivorship bias, and autobiographies are inevitably biased towards showing the author in a good light. reply max_ 4 hours agorootparentWhat exactly isn't survivorship bias? Everything can be put in that bucket. Being born without autism, becoming a doctor, engineer, start-up founder, successful athlete et al. They can all be abstracted as lottery tickets. I think the survivorship bias arguments sweeps under the rug the fact that we might be able to learn something from successful people. reply refurb 56 minutes agorootparentI think the point is that the “lessons” from these books suffer from bias. Sam Walton might say “this was the reason Walmart succeeded”, but plenty of people do the same thing and fail. So what’s the lesson? reply max_ 32 minutes agorootparentWell, things like law school & medical school have high rates of failure. So if one of the successful that passed the bar & became doctors people say \"They studied hard\" — yet many failed, even when the also studied. Does that nullify the value of \"studying hard\"? reply refurb 6 hours agorootparentprev [–] Business books are terrible in general for this reason. Even basic statistics is throw out the door in these analyses. The best book I read is “Halo Effect” which calls all this out. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author is creating their own MBA program through self-study and shares their recommended reading list for various subjects.",
      "The reading list covers topics such as accounting & finance, product design & marketing, organizational behavior & management, global economics & investing, strategy & systems thinking, and creativity & getting things done.",
      "The author invites readers to provide their suggestions for additional books or better alternatives in the comment section."
    ],
    "commentSummary": [
      "The discussion thread focuses on the pros and cons of an MBA degree and different resources for business education.",
      "Participants debate the effectiveness of recommended books, the significance of practical experience, and the relevance of credentials versus knowledge in the job market.",
      "The conversation also covers the value of business models, case studies, and lectures compared to reading materials, as well as recommendations for specific books and online resources."
    ],
    "points": 177,
    "commentCount": 83,
    "retryCount": 0,
    "time": 1706983834
  },
  {
    "id": 39241472,
    "title": "Creating a Folk Computer: Button Creation, GPU Programs, Serial Bluetooth, and More",
    "originLink": "https://folk.computer/",
    "originBody": "start hello – Newsletter Directory Guides: Let's make a button! Example programs GPU 'FFI' / shaders Serial Bluetooth OpenCV Long-form notes: Towards a folk computer (“tableshots”) (December 2023) supporters and partners: Ink & Switch Gradient Retreat One Fact Foundation Joël Franusic Val Town Interact and more ... Folk in the media: Emily Dickinson's Apple Computer House (Spike Art Magazine, April 2023) want to try this yourself, or to partner with us? contact@folk.computer start.txt · Last modified: 2024/01/30 02:05 by discord",
    "commentLink": "https://news.ycombinator.com/item?id=39241472",
    "commentBody": "Folk Computer (folk.computer)158 points by schmudde 18 hours agohidepastfavorite43 comments lsy 15 hours agoThis is almost the opposite of what I envisioned when reading the title \"folk computer\", which makes me think of a computing experience that's fundamentally simple, convivial, and transparent to users, something like what's captured by the concept of \"permacomputing\". While the ideas explored here are no doubt interesting and probably important to consider, they seem like they require an enormous amount of abstraction and complexity to implement, while at the same time remaining somewhat impenetrable to the user. For example, using QR codes guarantees that the semantics of any given symbol are impossible to determine at a glance. Is it really an improvement over the mouse-and-screen paradigm to paste QR codes to your hands (the printed hands in the demo are actually two left hands), and set up a rig that requires camera mounts, projectors, and a large flat surface dedicated to object manipulation with extremely specific, non-intuitive semantics? I can't help but wonder how this would ever generalize to the number of uses or amount of convenience that people were able to wring out of even simple text-based terminals. reply rzzzt 10 hours agoparentThe Commodore VIC-20 was marketed as VC-20 in Germany and got backronym-d as \"Volkscomputer\": https://www.c64-wiki.com/wiki/VIC-20 reply amatecha 14 hours agoparentprevYeah, I was pretty excited at first, hoping someone is showing their forward-looking human-friendly computer, encompassing all of the best principles of permacomputing and frugal computing... Was not expecting some kind of \"real-world computing\" platform. It's definitely interesting, but not at all what I expected based on the name. reply ryanwhitney 16 hours agoprevBoth seem to have worked at/with Dynamicland previously, though not mentioned on this site. https://omar.website/posts/notes-from-dynamicland-geokit/ https://cwervo.com/projects/dynamicland-experiments/ reply mortenjorck 13 hours agoparentThe moment I saw papercraft, computer vision, and a projector, I thought this had to have some relation. I’ve never been able to resolve a clear position for myself on Dynamicland. I’ve long admired Brett Victor’s work, and I have only the fondest appreciation for the project's philosophy and the enthusiasm with which Victor writes about it. The only problem is that I’ve never been able to figure out even the first thing about how it works. It’s completely incomprehensible to me, and I just don’t know how to square the project's ideals of human-centered, community-based computing with its seemingly-impenetrable alternate universe of dot stickers and projected images. reply Towaway69 12 hours agorootparentCame here to say the same - Bret Victor is doing similar things[1]. I think the point of these projects is to find an alternative approach to interfacing with technology. Why not combine paper and computers? We make the assumption that interfacing to technology is limited to keyboards, mice and fingers but there is no reason for us to limit ourselves to these approaches. Anyone using punchcards would be amazed by keyboards and so we will be amazed by interfaces that are beyond our imagination. [1] https://dynamicland.org/ reply anonymouskimmer 12 hours agorootparent> Anyone using punchcards would be amazed by keyboards and so we will be amazed by interfaces that are beyond our imagination. Typewriters, proper, pre-existed alongside punchcards for many decades before being incorporated into computer interfaces as keyboards. The fact that they did pre-exist computer keyboards may have led to them becoming the default so fast. While keyboards are amazing, they certainly weren't beyond imagination. I guess you can say that QR codes, projectors, and cameras predate this Folk computer idea as well. But they are also far less intuitive. Using a typewriter well requires knowing basic literacy and a few new functions (carriage return, line feed, shift, etcetera). Graduating from a typewriter to a keyboard requires learning some additional functionality. What current devices are teaching the basic functionality needed to jumpstart adaptation to this Folk computer interface? reply kens 8 hours agorootparentprev> Anyone using punchcards would be amazed by keyboards As someone who uses punchcards regularly, I don't understand what you mean. People used a keyboard to punch cards since the 1930s or earlier. You type on a keyboard and the keypunch puts the holes in the card. reply jdougan 5 hours agorootparentWhich make and model of keypunch do people use these days? I thought that IBM had discontinued the Model 029 ages ago (along with all the other unit record equipment). reply kens 5 hours agorootparentI use the IBM 026 keypunch at the Computer History Museum. This is for historical things, not production use :-) reply Towaway69 2 hours agorootparentOK, is it then safe to say no one uses punch cards in production ;) I just assumed that punch cards have gone the way of dinosaurs but always pleased to change my assumptions. reply joshmarinacci 16 hours agoparentprevI was gonna say, this sounds a lot like continuing the research of Dynamicland, which itself contained research from CDA, VPRI, and other groups. Why is it so hard to find a consistent funder of basic research like PARC in the 70s? reply shon 15 hours agorootparentPARC, though awesome for society, was a massive commercial failure for Xerox. As tech nerd, I love the lore of The PARC and went to visit it as soon as I moved to the Bay Area. However, I assume it would be taught as a lesson of what not to do in a business context: Investing in pure research often yields innovations that are opposed to existing business lines or simply too far out to see as useful by management. That said, many companies have research arms. Microsoft, Walmart, IBM, Meta, Google… reply vajrabum 14 hours agorootparentUh no. Xerox made billions from the laser printer. And yes they missed capturing all the profit but it was crazy profitable. https://www.forbes.com/sites/gregsatell/2015/03/21/how-parc-... reply shon 6 hours agorootparentHmm, thanks for the interesting article. Probably knew that at one time but forgot lol. Still, it shows my point, that research that reaches too far from the company’s core biz is difficult to recognize as a success. The laser printer, was a better printer and it was an improvement to Xerox existing scan+print business. Bell Labs also had a lot of commercial success. reply jrowen 14 hours agorootparentprevLesson: Don't develop innovations that are too advanced for management to understand, because they will pass and let the rest of the world eat their lunch. reply anonymouskimmer 14 hours agorootparentI think the actual lesson learned was to develop these research departments as incubators where the inventors are expected, and trained, to become entrepreneurs who spin out companies that the parent company owns part of. It would have been easier to just make some of the inventors upper-level managers and executives of the parent company. Given the cost of the original Xerox computer, Apple still may have eaten their lunch. Apple itself spent a lot of effort getting the Macintosh, expensive as it was, as cheap as it was. reply schmudde 15 hours agorootparentprevIt's astonishing (and sad) how few opportunities exist for basic research. reply skadamat 15 hours agorootparentDonald Braben's book touches on what's changed pretty well I felt (published by Stripe press too): https://www.amazon.com/Scientific-Freedom-Civilization-Donal... reply schmudde 12 hours agorootparentAh nice tip. Will put it on the reading list! reply refulgentis 14 hours agorootparentprevHonestly and respectfully, would you be clambering to fund this? It's disturbing how little actually came out of dynamic land, and now there's a schism where the founders are pasting QR codes to hands and have as their 3rd bullet point \"GPU FFI shaders\"? Does any of that scream fundamental UX research, or the future of computing? To me, it's just another tired retread of half-baked messianic thinking that tails off into nowhere as the hero complex devolves into a series of half-baked ideas designed to scratch an individuals daily itch, rather than a central purpose. Google had stuff like this internally for quite some time, through much later than most people would guess. The thing is there just isn't some vastly superior paradigm sitting out there to fix computing with. The industry is mature enough that if something truly good and helpful exists, even in parts, it's quickly implemented. One ray of light might be that the rate of change is fast enough that there are likely to be gaps emerging over the next decade. But they're not going to be found in this sort of fashion. reply csmeyer 16 hours agoprevJust saw a demo of this last week. The most exciting use case was when multiple tags “programs” composed. For example, one program measures a value, passes that to one that accumulates values, and another that plots. I think there is a very cool version of this where the primitives are simpler and easier to compose (the composition demo I saw was a bit difficult to pull off). Then, rather than program the purpose of individual tags, you can create programs physically on the table reply anonymouskimmer 14 hours agoparentSo does the primary use case of this seem to be for teaching computational ideas in a simple manner? reply skadamat 15 hours agoparentprev+1 to this, I was also there and it was lots of fun reply aeontech 16 hours agoprevThis was also mentioned recently in a good discussion on another post on the author's blog: https://news.ycombinator.com/item?id=38979412 Tbh it's really cool to see a practical walkthrough of what applying dynamicland ideas can look like, and I love the fact that there is a bunch of open source coming out of it. reply AlphaWeaver 9 hours agoprevThis is so refreshing! As sibling commenter [0] mentions, this is clearly inspired by Dynamicland, and even uses the \"Claim/When\" syntax that Dynamicland uses to implement their \"Realtalk\" operating system. When Omar's post about Dynamicland [1] hit the front page last month, I was frustrated by how little information was available about the project. It seems that the folks behind Dynamicland were very against sharing anything online [2] due to concerns about \"including everybody\". On the one hand, I admire their vision. But on the other hand, it doesn't feel obvious to me that holding back something like Dynamicland would help it ultimately become \"available everywhere.\" Even their answer of \"come visit us in person\" isn't relevant anymore since their physical space is closed. I'm very excited about this approach - even if it isn't open source, they've shared far more information about the way the system works. I'm sure this will inspire others and ultimately blossom into something much greater. Edit: actually, it's open source here: https://git.folk.computer/folk [0]: https://news.ycombinator.com/item?id=39242467 [1]: https://news.ycombinator.com/item?id=38979412 [2]: https://gist.github.com/shaunlebron/4b9a9a986fca1e7abd0bfcad... reply orangesite 14 hours agoprevYeah, another universe where you have wizards who know how to fold the paper correctly to make the ambient computational environment do things. Hard pass. Folk computing for me is a hardware/software layer stripped off the commercial abstractions that hide the inherent simplicity of the medium. reply ripley12 4 hours agoparent> Please don't post shallow dismissals, especially of other people's work. A good critical comment teaches us something. https://news.ycombinator.com/newsguidelines.html reply netghost 16 hours agoprevThe button is pretty amusing, but The rest of the site does a better job of motivating it. I've taught a few kids a little bit of programming with things like scratch, but I think this would be infinitely more fun. I could imagine a group of kids who don't think programming is for them having a blast with something like this. reply jstanley 17 hours agoprevWhat is this? reply pbhjpbhj 6 minutes agoparentExactly, websites like this need straplines and a clear \"about\" page. >\"This is basically the idea behind Folk Computer, a small human-computer-interaction (HCI) research project run out of a converted auto shop in East Williamsburg, Brooklyn, that I visited one afternoon in March. Its founders, Omar Rizwan and Andrés Cuervo, want to make an interface that is haptic and three-dimensional: no longer an image of a desktop, but a real one. Folk’s hardware consists of a tiny, monitor-less Intel computer, a ceiling-mounted camera, a projector, a printer, and lots of paper spread out on an IKEA table. Each sheet is printed with a QR-code-like marker (called an AprilTag) that corresponds to a program written on a laptop and stored on the Github cloud. The camera reads the tag and executes its instructions, which, for now, are fairly simple – “draw a green box,” for example, causes the projector to cast four green lines onto the table. Folk is part of a larger “screenless” trend in HCI, ...\" (https://spikeartmagazine.com/articles/essay-emily-dickinson-...) I guessed that following the \"in the media\" link would lead to a story with a brief synopsis, but the link just goes to a blogroll that's no longer got the \"Folk computer\" sorry on, searching for it gets one there: reply reanimus 17 hours agoparentprevhttps://folk.computer/notes/tableshots This page gives more explanation than the landing page does I think reply stereolambda 17 hours agoparentprevI think a better overview is under the \"Towards a folk computer\" link on the main page (perhaps the submission link should be changed?). Essentially (if I understand correctly) they want to let you control computer by moving things in the physical world: mainly items on a real tabletop. Sort of anti-virtual reality, in a way! reply atrus 16 hours agorootparentNot to sound dismissive, because this is cool, but this pretty solidly AR, just instead of a headset you're using a projector. I don't think it'd be too difficult to port this to vr headset of choice reply staplers 15 hours agorootparentHeadsets are in many ways a less cool version of our own eyes. Also, this would be helpful for the sightless. reply codetrotter 16 hours agorootparentprevTangible Computing :D reply __loam 16 hours agorootparentSomeone get Tim on the line to look into this. reply Joeboy 15 hours agoprevI started on the \"Let's make a button\" page and got very confused. IIUC the missing part is \"you point a camera at stuff and a program on your computer does things in response, including sending output back to the stuff via a projector\". Printing things onto bits of paper and folding them up doesn't cause anything much to happen on its own. reply pyinstallwoes 12 hours agoprevThis would be cool with a \"swarm\" of \"cold-tech\" displays. Like, a bunch of small e-ink displays. Feels like something like elixir/erlang vm would do great with. reply FergusArgyll 14 hours agoprevThis is pretty cool once I figured out what it is (~10 minutes) reply lloydatkinson 16 hours agoprevIs this a paper computer? If so, the first page of the site does a poor job of explaining what it is, and even then going to “make a button” page it’s still not that clear. Where is the about page or some kind of introduction? reply Liftyee 11 hours agoparentAgree that the first page is not very helpful. I found this \"longform\" to be better: https://folk.computer/notes/tableshots#towards-a-folk-comput... reply KomoD 15 hours agoprev [–] Site seems dead. edit 1h later: loaded now, dont know what im looking at reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The summary covers multiple topics and programs related to creating a folk computer, including creating a button, GPU and shaders example programs, Serial Bluetooth, and OpenCV usage.",
      "The document mentions supporters and partners involved in the project.",
      "It also refers to a media article about Emily Dickinson's Apple Computer House and provides a contact email for those interested in trying or partnering with the folk computer project."
    ],
    "commentSummary": [
      "\"Folk computing\" is explored as an alternative approach to technology interfaces through the use of QR codes, papercraft, computer vision, and projectors.",
      "The article emphasizes the limitations of current interfaces and the need for innovative solutions.",
      "Examples from Xerox and Bell Labs are used to highlight the value and direction of research in business contexts.",
      "The Dynamicland project is discussed as a source of inspiration for further developments in folk computing.",
      "The Folk Computer project aims to create a haptic and three-dimensional interface for computers.",
      "User comments mention the potential benefits and improvements of an augmented reality project using a projector."
    ],
    "points": 158,
    "commentCount": 43,
    "retryCount": 0,
    "time": 1706975560
  }
]
