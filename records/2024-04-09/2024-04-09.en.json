[
  {
    "id": 39973467,
    "title": "Llm.c – Train Large Language Models in Pure C/CUDA",
    "originLink": "https://github.com/karpathy/llm.c",
    "originBody": "llm.c LLM training in simple, pure C/CUDA. There is no need for 245MB of PyTorch or 107MB of cPython. For example, training GPT-2 (CPU, fp32) is ~1,000 lines of clean code in a single file. It compiles and runs instantly, and exactly matches the PyTorch reference implementation. I chose GPT-2 as the first working example because it is the grand-daddy of LLMs, the first time the modern stack was put together. Currently, I am working on: direct CUDA implementation, which will be significantly faster and probably come close to PyTorch. speed up the CPU version with SIMD instructions, AVX2 on x86 / NEON on ARM (e.g. Apple Silicon). more modern architectures, e.g. Llama2, Gemma, etc. For the repo, I'd like to maintain both clean, simple reference implementations alongside a also lot more optimized versions that can come close to PyTorch, but in a tiny fraction of the code and dependencies. quick start Download and tokenize a dataset. The tinyshakespeare dataset is the fastest to download and tokenize: python prepro_tinyshakespeare.py This prints: Saved 32768 tokens to data/tiny_shakespeare_val.bin Saved 305260 tokens to data/tiny_shakespeare_train.bin The .bin files are raw byte streams of int32 numbers indicating the token ids with the GPT-2 tokenizer. Alternatively you could also tokenize the TinyStories dataset with prepro_tinystories.py. In principle we'd be ready to the train the model right here. However the baseline CPU/fp32 reference code is so inefficient that it's not practical to train these models from scratch yet. Instead, we initialize with the GPT-2 weights released by OpenAI and just do finetuning. For that, we have to download the GPT-2 weights and save them as a checkpoint we can load in C: python train_gpt2.py You'll recognize this code from nanoGPT as a simple GPT-2 reference implementation in PyTorch. This script will download the GPT-2 (124M) model, overfit a single batch of data for 10 iterations, run a few steps of generation, and most importantly it will save two files: 1) the gpt2_124M.bin file that contains the raw model weights for loading in C, and gpt2_124M_debug_state.bin, which also contains more debug state: the inputs, targets, logits and loss. This is very useful for debugging C code, for unit testing, and making sure we're exactly matching the PyTorch reference implementation. For now all we care about are the model weights in gpt2_124M.bin. We can now initialize with them and train in raw C. First compile the code: make train_gpt2 You can have a look inside the Makefile and its comments. It will try to autodetect if OpenMP is available on your system, which is very helpful for speeding up the code at very low cost of code complexity. Once train_gpt2 is compiled, you can run it: OMP_NUM_THREADS=8 ./train_gpt2 You should tune the number of threads depending on how many cores your CPU has. The program will load the model weights, the tokens, it will run a finetuning loop for a few iterations with Adam lr 1e-4, and then generate a sample from the model. The file is (I think) very readable and you should have a look. Simply, there are implementations for the forward and backward pass of all the layers, and they get strung together into a large, manual, forward/backward/update loop. The output looks like this on my MacBook Pro (Apple Silicon M3 Max): [GPT-2] max_seq_len: 1024 vocab_size: 50257 num_layers: 12 num_heads: 12 channels: 768 num_parameters: 124439808 train dataset num_batches: 1192 val dataset num_batches: 128 num_activations: 73323776 val loss 5.252026 step 0: train loss 5.356189 (took 1452.121000 ms) step 1: train loss 4.301069 (took 1288.673000 ms) step 2: train loss 4.623322 (took 1369.394000 ms) step 3: train loss 4.600470 (took 1290.761000 ms) ... (trunctated) ... step 39: train loss 3.970751 (took 1323.779000 ms) val loss 4.107781 generated: 50256 16773 18162 21986 11 198 13681 263 23875 198 3152 262 11773 2910 198 1169 6002 6386 2583 286 262 11858 198 20424 428 3135 7596 995 3675 13 198 40 481 407 736 17903 11 329 703 6029 706 4082 198 42826 1028 1128 633 263 11 198 10594 407 198 2704 454 680 1028 262 1027 28860 286 198 3237 323 step 40: train loss 4.377757 (took 1366.368000 ms) The generation just gives you the token ids for now, which we have to decode back to text. We can implement this in C quite easily also, because decoding is very straight-forward, it's just string chunk lookups and prints. For now we can use tiktoken: import tiktoken enc = tiktoken.get_encoding(\"gpt2\") print(enc.decode(list(map(int, \"50256 16773 18162 21986 11 198 13681 263 23875 198 3152 262 11773 2910 198 1169 6002 6386 2583 286 262 11858 198 20424 428 3135 7596 995 3675 13 198 40 481 407 736 17903 11 329 703 6029 706 4082 198 42826 1028 1128 633 263 11 198 10594 407 198 2704 454 680 1028 262 1027 28860 286 198 3237 323\".split())))) which prints: Come Running Away, Greater conquer With the Imperial blood the heaviest host of the gods into this wondrous world beyond. I will not back thee, for how sweet after birth Netflix against repounder, will not flourish against the earlocks of Allay I like how Netflix comes up, it's clear that the shadow of the training past is still lurking in the model. I did not attempt to tune the finetuning hyperparameters so it's quite likely this can be improved quite a bit, most likely especially if one was to train a bit longer. test I am also attaching a simple unit test for making sure our C code agrees with the PyTorch code. Compile and run with: make test_gpt2 ./test_gpt2 This now loads the gpt2_124M_debug_state.bin file, runs a forward pass, compares the logits and loss with the PyTorch reference implementation, then it does 10 iterations of training with Adam and makes sure the losses match PyTorch. tutorial I attached a very small tutorial here, in doc/layernorm/layernorm.md. It's a simple, step-by-step guide to implementing a single layer of the GPT-2 model, the layernorm layer. This is a good starting point to understand how the layers are implemented in C. license MIT",
    "commentLink": "https://news.ycombinator.com/item?id=39973467",
    "commentBody": "Llm.c – LLM training in simple, pure C/CUDA (github.com/karpathy)805 points by tosh 13 hours agohidepastfavorite133 comments patrick-fitz 12 hours agohttps://twitter.com/karpathy/status/1777427944971083809 > And once this is a in a bit more stable state: videos on building this in more detail and from scratch. Looking forward to watching the videos. reply 0cf8612b2e1e 12 hours agoparentI love his videos. They are dense, but I get a lot out of them. reply sghiassy 12 hours agorootparent+100 thank you karpathy! reply convexstrictly 7 hours agoprevCandle is a minimalist ML framework for Rust with a focus on performance (including GPU support) and ease of use https://github.com/huggingface/candle reply jeroenvlek 3 hours agoparentLove Candle! I actually ported Karpathy's previous GPT tutorial to candle, including training [0] [0] https://www.perceptivebits.com/building-gpt-from-scratch-in-... reply basbuller 4 hours agoparentprevNot barely as minimal as Karpathy his implementation reply imjonse 5 hours agoparentprevCandle focuses on inference though. reply l-m-z 4 hours agorootparentCandle dev here, we also support training/backdrop! We certainly focus on optimizing inference performance but hopefully that should improve the training efficiency too. reply 0xfedbee 2 hours agoparentprevI wouldn't call in \"minimalist\" after seeing Karpathy's code. reply yinser 12 hours agoprevI've seen his nano GPT implemented using JAX, now we have C/CUDA. I'd love to see if nano GPT could be doable in Mojo. I took a stab at a Mojo conversion of his Wavenet project (Andrej's zero to hero course) and I gotta say... python has so many nice features lol. Stating the obvious I know but what you see done in 6 lines of python takes so much more work in other languages. reply cb321 11 hours agoparentFor a prior generation of karpathy-splaining this is this Nim port: https://github.com/Vindaar/llama2nim - maybe of interest if you are interested in Mojo. reply yinser 9 hours agorootparentThank you! reply auraham 6 hours agoparentprevWhere is the GPT implementation in JAX? I only found this [1] in PyTorch and NumPy. [1] https://github.com/karpathy/nanoGPT reply pavelstoev 7 hours agoparentprevHow in Mojo do you support GPU data parallelism and all the benefits it brings ? reply KeplerBoy 4 hours agorootparentYou don't. Mojo doesn't support GPUs at the moment, which says a lot about a language which claims to be AI first. reply pjmlp 4 hours agorootparentThey only made Mojo available outside the preview circle about a couple of months ago, and it is yet to run on Windows laptops of researchers. I love the attitude of considering 0.x languages production ready for all imaginable kinds of workloads. reply sirsinsalot 28 minutes agoprevKarpathy's code, teaching and contribution to the body of knowledge in this area really is admirable. Sadly I am a generalist, but if I were a specialist, I would hope to contribute as openly and widely as Karpathy. Not clout chasing, click-bait, \"top 5 javascript frameworks of 2023!\" ... just high quality output that marks a specialist. Sorry to gush. reply zzbn00 1 hour agoprevVery nice. In my experience much of the complexity of numerical software is to enable the search for the algorithm that works well with the problem/data you have. Once you know the exact algorithm you want, it is possible to make a nice clean minimalistic implementation, but that does not mean such an implementation would have been easy at the beginning. reply qwertox 12 hours agoprev> direct CUDA implementation, which will be significantly faster and probably come close to PyTorch. It almost hurts, to read that PyTorch is faster. But then again, with these GPU-RAM-prices, let's see how it speeds up the CPU. We really need SO-DIMM slots on the RTX series (or AMD/Intel equivalent) so that we can expand the RAM as we need it to. Is there a technical problem to it? reply jsheard 12 hours agoparentMemory speed is more or less directly proportional to how close the memory is to the processor, with the fastest memory being literally inside the processor (SRAM cache), followed by memory on the same package as the processor (HBM GPUs, Apple M-series), followed by soldered down discrete memory chips (regular GPUs, games consoles), followed by socketed DIMMs in distant last place. There's not really any getting around it, the bandwidth that GPUs crave just isn't compatible with modularity. Even CPUs are starting to move their memory closer to the core in the name of performance, as mentioned Apple is already doing it, Intel is making Xeons with on-chip memory now, and they have a version aimed at consumers on their roadmap. reply tverbeure 11 hours agorootparentFor data rates, as in bandwidth per IO pin, distance is really only a secondary factor. HBM memory, for example, runs at substantially lower data rates than GDDR, yet it sits right next to the GPU die compared to centimeters for the GDDR. And high-speed serial links run at speeds that are an order of magnitude higher than even the internal register files of a CPU. reply wtallis 11 hours agorootparentprevFYI, most discrete GPUs with discrete memory packages soldered to the board near the GPU are running at substantially higher memory frequencies than the on-package DRAM in Apple's chips. But running GDDR at those speeds costs a lot of power. reply osigurdson 8 hours agorootparentI watched a presentation on this today. The presenter focused on the soldering and proximity as well. Is this really the only difference or is this transistor based memory (like L1, L2, etc.)? I get the proximity factor of course (1ft / ns EE rule of thumb). In any case, soldering and proximity don't seem like breakthrough innovations (but maybe I am wrong). reply MobiusHorizons 8 hours agorootparentGpu ram is typically gddr6 or gddr6x which is a different standard to the chips used in ddr5 for example. GPUs have terrible latency to ram, but enormous throughput, and I assume the chips are internally optimized for that. Many aspects of a design change when you choose different latency or clockspeed targets translating into different power / area calculations. reply viraptor 9 hours agorootparentprevThat's true it's got an impact, but I think there's still space available for \"slightly slower with 2x memory\" models. For many local uses, new cards are way past the \"fast enough\" line, but having 64gb on them would be really beneficial. It's love to see some experiments / different SKUs in this area, given people are already diy-ing extra memory on NVIDIA. (https://hackaday.com/2021/01/29/add-an-extra-8gb-of-vram-to-... there were stable experiments later on, but I don't have a link now) reply schneehertz 9 hours agorootparentGraphics card manufacturers believe that selling high-memory consumer graphics cards will affect the market for commercial computing cards, so they will not do so, that's all. reply airspresso 3 hours agorootparentNice room for a new player to disrupt then reply tverbeure 12 hours agoparentprevCheck out PCB back drilling. It's a process where you remove a few hundred microns from the vias that are used to connect GDDR RAMs to the GPUs, to avoid reflections due to the impedance mismatch that's caused by the stub. When you have a pulse coded signal traveling at close to 10GHz, everything becomes an antenna. The technical problem is that you can't do this with a flimsy connector like the ones used for DIMMs. The reason GDDR can have a bandwidth per pin that is 4 times higher than regular DDR is because they are soldered down on the PCB. reply LatticeAnimal 12 hours agoparentprev> We really need SO-DIMM slots on the RTX series (or AMD/Intel equivalent) so that we can expand the RAM as we need it to. Is there a technical problem to it? I imagine it would incur a non trivial latency and cost penalty. The memory modules are placed pretty close to the compute die right now. Cooling would also have to change (the memory modules produce a lot of heat). But there is also no reason for any of the GPU manufacturers to do this. A skew with twice as much memory can go for a lot more than the difference in memory cost alone reply ItsBob 1 hour agorootparentI don't disagree but (I know nothing about this btw...) would it not benefit in terms of, say, a L3 cache kind of thing? Imagine you could stick 2 x 64GB DDR5 DIMMS on the GPU in sockets, would that not be faster to access than the motherboard DIMMS? It won't be as fast as on-die memory of course but could it not act like a sort of halfway house? reply SunlitCat 11 hours agorootparentprevAnd especially doing \"interesting\" combinations of gpu and memory. Like lower end gpu with 16 GB of VRAM, but offering just 8 / 12 GB of VRAM in the middle class and then again 16 GB in the upper class of gpu selection. reply hahnchen 7 hours agoparentprev> It almost hurts, to read that PyTorch is faster. Why? reply theGeatZhopa 4 hours agoparentprevNVIDIA hates that trick. reply osigurdson 7 hours agoprevKind of amazing that something that can be expressed in ~1000 lines of code has completely turned the world on its head. reply KeplerBoy 4 hours agoparentWhich important concept or algorithm can't be expressed in ≤1000 lines? Seems like a pretty common theme among groundbreaking ideas. reply Y_Y 3 hours agorootparentThat's a good question. Unfortunately I think you're asking to compute the Kolmogorov complexity of every interesting concept we have that doesn't yet have an implementation less than n=1000 lines, which is equivalent to the halting problem (modulo unbounded memory). If you could exhaustively list all the interesting algorithms (hard but feasible) you could potentially prove a lower bound for each one's complexity by writing a shorter than n implementation (hard, probably infeasiblel and show positively that GP's prop isn't true. On the other hand showing that it was true would require either some very clever proof which can't apply to all programs, but somehow only these interesting ones (very likely impossible) or enumerate all C^n programs where C is the number of possible lines (something like 64^80) and show that none of them implements at least one of the interesting algorithms (absurdly impossible). reply datascienced 2 hours agoparentprevErr… and the exobytes of training data reply rnewme 1 hour agorootparentAh, not really reply datascienced 17 minutes agorootparent“Here’s one I trained earlier”? reply holoduke 21 minutes agoparentprevSpeed of hardware did. Back in 80a they already knew the principles of llm training. It only took one week to train 10.000 tokens. reply daniel_reetz 4 hours agoparentprevEchoes of DeCSS ;) reply milansuk 1 hour agoprevThis is an implementation of a transformer and in README it's presented as text->text. Tokens are just integers going in and out. Is it possible to use it to train other types of LLMs(text->image, image->text, speech->text, etc.)? reply bootsmann 59 minutes agoparentThe transformer itself just takes arrays of numbers and turns them into arrays of numbers. What you are interested in is the process that happens before and after the transformer. reply flockonus 11 hours agoprevQuestion, apologize if slightly off-topic, it's something I'd like to use this project for: Is there an example of how to train GPT-2 on time series, in particular with covariates? As my understanding of LLM goes at a basic level it's predicting the next token from previous tokens, which sounds directionally similar to time series (perhaps letting aside periodicity). reply teruakohatu 11 hours agoparentYes general LLM models can be used for time series forecasting: https://github.com/KimMeen/Time-LLM reply tehsauce 9 hours agoprevAnother awesome project! Note that as of this moment the CUDA part is aspirational. There is no gpu code in the repo yet. reply mrbonner 7 hours agoprevWow, and this is done after a recent trip to Bhutan to clear his head! I follow karpathy on twitter and he posted that 2 weeks without constantly looking and checking his phone kind of turns off the constantly on radio in his head. reply antirez 1 hour agoprevIs this able to replace PyTorch, ... in normal practice? No. Does this show that in general the most used ML frameworks are a mess? Yes. reply bootsmann 21 minutes agoparentThis is a bit an apples and oranges comparison. Pytorch is a research framework not a transformer inference library. reply antirez 10 minutes agorootparentThis post is about training not inference. And llama.cpp has similarly simple LoRa training code. There is nothing in neural networks themselves so complex to justify the amount of complexity the Python-ML community piled up. MLX, for instance, is a similarly general purpose research framework that is a fraction of the size. reply triyambakam 12 hours agoprevWhen Lex recently talked to Andre, Andre said that he gets positively obsessed with a problem and says \"this must exist\". I imagine this must be one of those outputs. reply rurban 5 hours agoprevhttps://github.com/robjinman/Richard uses Vulkan, thus is portable across GPU's and much faster. It also has more kernels. In simple C++ reply andy99 11 hours agoprevI'd like to think he took the name from my llm.f90 project https://github.com/rbitr/llm.f90 It was originally based off of Karpathy's llama2.c but I renamed it when I added support for other architectures. Probable a coincidence :) reply matteogrella 1 hour agoparentI'm the creator behind https://github.com/nlpodyssey/rwkv.f90. How about joining forces? reply davedx 38 minutes agoprevOn one hand, really nice to see the whole thing in 1000 lines of C code. On the other hand, that malloc function low key terrifies me. :) reply classiebit2025 2 hours agoprevhttps://classiebit.com/eventmie-pro If you looking to host your events but you don't have any platform to host, then once visit Eventmie Pro Platform, Which is the best event management platform in 2024. reply blackeyeblitzar 12 hours agoprevIt would be great if someone created a tutorial around this explaining exactly how it works and how to do a test training run. I’m aware it’s not feasible to train a “real” model on personal hardware but it would be nice to have a practical learning experience. I’m not sure if there are good alternatives for that. reply karpathy 10 hours agoparentI wrote this, which might be a bit helpful: https://github.com/karpathy/llm.c/blob/master/doc/layernorm/... But if you don't have the background, I'd recommend my YouTube videos, see the Zero To Hero playlist: https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxb... reply MAMAMassakali 1 hour agorootparentThank you so much for the Zero To Hero playlist! reply blackeyeblitzar 9 hours agorootparentprevThank you so much for responding. I will definitely check these out and also pass it on to others who might be interested. reply vineyardmike 10 hours agoparentprevThe author has a whole series where he does exactly that. YouTube videos, code examples, documentation, everything. Explains the math, explains how to code it, explains the architecture. Everything. reply idkwhatimdoin 12 hours agoprevIf I was starting from scratch, what resources should I start with to build up an understanding of what this code does and how to read it? It's quite dense and my knowledge of LLMs is quite minimal. Are these terse variable names standard in LLM-land? reply vineyardmike 10 hours agoparentTerse variables are a C thing. “What resources would I need” -> you’re literally commenting on a teachers content. Karpathy (the author) has a very informative YouTube channel where he goes step by step through everything. He has a ton of repos and tutorials. Dig a little. If all else fails… Google it. reply idkwhatimdoin 5 hours agorootparent> you’re literally commenting on a teachers content. How am I supposed to know that? > Karpathy (the author) has a very informative YouTube channel where he goes step by step through everything. Or that, without knowing that he's a teacher? > Terse variables are a C thing. I didn't realize variables had to be so short in C. Glad I write C++ professionally where they've added support for longer variable names. > If all else fails… Google it. There's a lot of LLM garbage out there. I got an answer here in a few minutes pointing to Karpathy's course which seems very high quality. Be kinder. reply vineyardmike 2 hours agorootparent> How am I supposed to know that? You’re not supposed to know that. You asked a question, and this is you being told the answer. It’s very convenient that the author of the post is quite literally the world’s most prolific teacher on this topic. Makes it easy to find Karpathy. You shouldn’t be expected to otherwise know that (or else why ask if you knew). > I didn't realize variables had to be so short in C. Glad I write C++ professionally where they've added support for longer variable names. This feels like a joke but old C compilers did have variable length limits. This is part of why C historically had shorter variables than other more modern languages. Sorry if it came off rude, the internet is hard to communicate over. https://publications.gbdirect.co.uk/c_book/chapter2/keywords... reply viraptor 9 hours agorootparentprev> Terse variables are a C thing. They're a math / toy code thing. Large C projects have long descriptive names just like other languages. reply satokema 9 hours agoparentprevAs siblings have said, his video series are quite good. But if you're just looking at this repo only, you probably want to look at the python reference implementation. (The C is designed to exactly replicate its functionality.) reply tayo42 11 hours agoparentprevCheck out his zero to hero series. Which builds this with python and later pytorch, then probably his other mini C based projects. reply robot 5 hours agoprevvery cool, also the coding style looks good. reply richrichie 9 hours agoprevSee, C does it very well. Great stuff. Karpathy has a gift for teaching. reply andrewstuart 12 hours agoprevOT but question from someone curious..... is Cuda still entrenched as the only option for doing AI or is there growing support for AMD/Intel/Other ways of doing AI? reply adam_arthur 12 hours agoparentYou can run inference today on pretty much any card. Download Ollama on a modern MacBook and can run 13B and even higher (if your RAM allows) at fast speeds. People run smaller models locally on their phones Google has trained their latest models on their own TPUs... not using Nvidia to my knowledge. So, no, there are alternatives. CUDA has the largest mindshare on the training side though. reply BraverHeart 12 hours agoparentprevGeorge Hotz is attempting to solve this: https://github.com/tinygrad/tinygrad reply ZoomerCretin 12 hours agorootparentHe loudly gave up on AMD after they did not fix a blocker he had for 5+ months and gave him the runaround the entire time when he asked for the code to fix it himself. He is still shipping the AMD tinybox with huge warning labels. reply magicalhippo 6 hours agorootparentRandomly stumbled over this[1] post with another fed up open source contributor, due to several serious issues with AMDs GPU drivers and firmware that remain unresolved for years. It also references the geohot decision you mention. Some quotes: I find it incredible that these companies that have large support contracts with you and have invested hundreds of thousands of dollars into your products, have been forced to turn to me, a mostly unknown self-employed hacker with very limited resources to try to work around these bugs (design faults?) in your hardware. In the VFIO space we no longer recommend AMD GPUs at all, in every instance where people ask for which GPU to use for their new build, the advise is to use NVidia. [1]: https://www.reddit.com/r/Amd/comments/1bsjm5a/letter_to_amd_... reply Art9681 8 hours agorootparentprevDidn't they recently announce that everything was open sourced? Would be cool if he took another look at it once all of the souce code is available (if not already). reply xjay 7 hours agorootparent> They haven't open sourced anything. They posted a tweet. [1] [1@2024-04-06] https://www.youtube.com/watch?v=j7MRj4N2Cyk&t=429s [Twitch] https://twitch.tv/georgehotz reply taminka 11 hours agoparentprevthere are obv alternatives from both intel and amd, performant blas/dnn packages, but small teams don’t use them bc cuda is easier to use and has more support, and larger teams don’t use them bc they have deals w/ nvidia or not enough GPUs are available or they’re after the absolute best performance (which is still nvidia) or bc of other stuff like unstable drivers or smth reply towelpluswater 9 hours agoparentprevModular Mojo is the most well funded and full of respectable players for making an alternative possible reply pavelstoev 7 hours agorootparentCheck out Hidet [1]. Not as well funded, but delivers Python based ML acceleration with GPU support (unlike Mojo). [1] https://github.com/hidet-org/hidet reply WithinReason 12 hours agoparentprevThere are some stirrings but don't hold your breath reply sigmoid10 12 hours agoparentprevThere are a few attempts here and there in various stages of progression. But right now, nothing matches Nvidia+CUDA in speed and usability. reply blackeyeblitzar 12 hours agoparentprevSee my comment on this here: https://news.ycombinator.com/item?id=39973816 reply tosh 13 hours agoprev> LLM training in simple, pure C/CUDA. There is no need for 245MB of PyTorch or 107MB of cPython reply QuadmasterXLII 12 hours agoparent107MB of cPython defeated Go to try for self Step 1 download 2.4GB of CUDA reply simonw 12 hours agorootparentThe size of CUDA really is astonishing. Any chance someone might figure out how to slim that down? reply jsheard 12 hours agorootparentTaking a peek inside the package it seems to mostly be the libraries - CuFFT alone is about 350MB for example, twice over for the debug and release versions. I'm guessing those are probably fat binaries pre-compiled for every generation of Nvidia hardware rather than just the PTX bytecode, which would help to speed up fresh builds, at the expense of being huge. reply phdelightful 12 hours agorootparentprevHere’s a blog that breaks down how large different pieces of CUDA are: https://carlpearson.net/post/20231023-cuda-releases/ reply xiphias2 12 hours agorootparentprevTalking directly to the kernel / driver / firmware. As others have said, George Hotz is doing his best in reverse-engineering and skipping layers. reply maille 12 hours agorootparentprevRaise your voice on their forum: https://forums.developer.nvidia.com/t/how-to-overcome-the-hu... Tried my luck 2 years ago but it keeps increasing. reply dartos 12 hours agorootparentprevNvidia is the only one who could, since they own it. reply fsloth 12 hours agorootparentprevI don't think it's about the byte size, but the inherent complexity of the implementation. 1000 lines of C code is extremely simple by any standard. Whereas a sundry collection of Python and PyTorch libraries is anything but. reply dwroberts 12 hours agorootparentprevA bunch of install methods for torch via pip include ~1.5GB of lib/ because of CUDA. libtorch_cuda.so is like 800MB on its own reply gcr 12 hours agorootparentprevI mean, being fair, the 2.4GB CUDA SDK is absolutely required for the cPython implementation as well reply api 13 hours agoparentprevPython has been popular for this because it’s convenient to quickly hack on and experiment with, not because it’s the most efficient thing. reply im3w1l 12 hours agorootparentThe overhead really isn't that bad is it? Since the the python code is mostly about saying multiply matrix A with matrix B, and then that actual computation is done by optimized low level code. reply llm_nerd 11 hours agorootparentIt depends on how you define overhead. Runtime overhead and memory usage is absolutely marginal, and the tightest, most perfect implementation will have trouble beating it. Instead people are trying to optimize install size of dependencies, which while maybe a fun hacking project...who really cares? reply bongodongobob 12 hours agorootparentprevFor that stuff, yeah you're correct. What I've seen is issues with the implementation of those libraries in a project. I don't remember exactly, but I was playing with someone's wrapper for some kind of machine learning snake game and it was taking way longer than it should have on back of the napkin math. The issue was using either a dict or a list in a hot loop and changing it to the other sped it up like 1000x. So it's easy to think \"yeah this library is optimized\" but then you build something on top of it that is not obviously going to slow it down. But, that's the Python tradeoff. reply hcarvalhoalves 12 hours agorootparent> The issue was using either a dict or a list in a hot loop and changing it to the other sped it up like 1000x. The programmer using the wrong data structure is not a problem with the language. reply bongodongobob 10 hours agorootparentKinda. I guess my native tongue is C/C++ and I wouldn't expect such a huge performance difference when using an array vs a linked list or something. It's not like I had millions of items in that structure either, it was like 100. I think it contained the batch training data from each round. I tried to find the project but couldn't. I was just shocked that there was such a huge difference between primitive data structures. In that situation, I wouldn't have guessed it would make a difference. reply CamperBob2 11 hours agorootparentprevIt really is with Python. There are simply too many containers and container-like concepts. Lists, arrays, sets, dicts... reply 0cf8612b2e1e 11 hours agorootparentWhat modern language doesn’t have those? Go kind of cheats and has maps play double duty as sets. reply 0cf8612b2e1e 12 hours agorootparentprevThat sounds irrelevant to Python and just a matter of slow code cropping up in libraries until someone runs a profiler. reply littlestymaar 11 hours agorootparentBut then again if your program have places where choosing the right Python primitive is important for performance, then using python is affecting performance here since even the best algorithm in Python would be slower than the equivalent C. Most of the time it doesn't matter because there's nothing hoy on the Python side, but if there is, then Python is going to be slowing your stuff down. reply jiggawatts 12 hours agorootparentprevI suspect that this has a high chance of running afoul of Ahmdal’s Law. Even if you can parallelise the bulk of the computation, the serial parts remain single-threaded and start to dominate the total runtime. reply taneq 10 hours agorootparentI don’t think the serial parts of ML training are Python’s fault, are they? It’s all “operation B depends on the output of operation A”. reply maxdoop 13 hours agoprevnext [4 more] [flagged] jessenaser 12 hours agoparentThe true hacker mentality is technically this. reply ipsum2 12 hours agoparentprevNothing wrong with critiques. reply sa-code 12 hours agoparentprevI see it as an excuse to talk about it more, and I'm happy to read it all reply fori1to10 12 hours agoprevIt should be rewritten in Rust. (Just joking) reply ddggdd 9 hours agoparentI just pasted the code into claude and reading the converted rust now, definitely need extra work reply naruhodo 7 hours agoparentprevI think you just cracked AI safety. reply eclectic29 12 hours agoparentprevSshh! I asked why it was written in C and got flagged. reply eclectic29 13 hours agoprevnext [12 more] [flagged] flohofwoe 12 hours agoparentLooking at the code it's just loops with simple floating point math. There's absolutely nothing in the code that would benefit from C++ features. reply eclectic29 12 hours agorootparentNot yet. reply jeffbee 12 hours agorootparentprevRaising the question of why it isn't in FORTRAN. reply kleiba 13 hours agoparentprevBetter in which way? reply eclectic29 12 hours agorootparentAutomatic resource management thanks to RAII. To me this is the most important thing. reply kleiba 11 hours agorootparentBut in this particular application, if you look e.g. through the training code, there's very little going on in terms of resource management. A handful of mallocs and some file handling. That doesn't mean that you can have bugs even in a small number of instances, but if automatic resource management is your main argument, perhaps llm.c isn't the most prominent case that could benefit from it. reply QuadrupleA 12 hours agoparentprevRAII sucks the way C++ does it. Magical background BS with massive unintended complexity consequences requiring obtuse intricate crap like the copy-and-swap idiom, a mudball of pointer and reference types, etc. They should have added a defer/scope-exit statement and been done with it. reply dartos 12 hours agoparentprevNext level troll? reply homarp 13 hours agoparentprevRewrite in (safe) Rust, I am sure many will enjoy to compare how it runs. reply dvt 12 hours agorootparentCandle already exists[1], and it runs pretty well. Can use both CUDA and Metal backends (or just plain-old CPU). [1] https://github.com/huggingface/candle reply infamouscow 10 hours agorootparentprevThe programming language colonialists can't help themselves. reply brcmthrowaway 13 hours agoprev [–] Very sad, shouldve used an agnostic framework instead of CUDA reply jsheard 12 hours agoparentIt's only ~1000 LoC, seems like a pretty good case study to port over to other runtimes and show they can stand up to CUDA. reply geph2021 12 hours agoparentprevAs far as I can tell, its optional dependency is Open MP, not CUDA. Doesn't seem directly dependent on CUDA. reply dlazaro 12 hours agorootparentThe plan is to eventually implement with CUDA: \"Currently, I am working on [...] direct CUDA implementation, which will be significantly faster and probably come close to PyTorch.\" reply gpderetta 12 hours agorootparentprevYes, a quick skim of the code only shows openmp dependency. The C/CUDA reference might have meant to be C/OMP . Although I wonder if it would work well with GCC PTX OMP offloading. reply robrenaud 12 hours agoparentprevAre there any strong LLMs trained without CUDA? reply blackeyeblitzar 12 hours agorootparentYes, there are several. See this blog post from Databricks describing the landscape of LLMs trained on AMD hardware for example: https://www.databricks.com/blog/training-llms-scale-amd-mi25... The most interesting one IMO is OLMo from AI2, which is truly open. You can read their blog post about it (https://blog.allenai.org/hello-olmo-a-truly-open-llm-43f7e73...) but basically it is open everything - they released everything you need to reproduce their weights (training data, training code, evaluation code, and weights) with a friendly (Apache) license. reply ZoomerCretin 12 hours agorootparentprevGemini and Gemma were trained on Google's TPUs. reply exe34 11 hours agoparentprev [–] Looking forward to your patches! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The document details a straightforward C/CUDA implementation for training massive language models, such as GPT-2, without relying on extensive frameworks like PyTorch.",
      "The author focuses on enhancing the implementation's speed and efficiency, offering guidelines for dataset acquisition, weight initialization, and model training in C, alongside unit tests and tutorials for accuracy assurance.",
      "The project is open-source under the MIT license, facilitating accessibility and collaboration in the tech community."
    ],
    "commentSummary": [
      "The Github discussion delves into diverse topics like machine learning, GPU memory design, GPT-2 for forecasting, PyTorch limitations, and alternative GPUs.",
      "Enthusiastic users admire Karpathy's input while exchanging insights on technical challenges, improvements, and language model training with Google's TPUs.",
      "The conversation explores memory capacity, access methods, optimizing ML libraries, varying data structures, programming languages, and automated resource management in coding."
    ],
    "points": 805,
    "commentCount": 133,
    "retryCount": 0,
    "time": 1712608729
  },
  {
    "id": 39970915,
    "title": "Lore Harp McGovern: Rise, Fall & Legacy in Tech",
    "originLink": "https://every.to/the-crazy-ones/the-woman-that-tech-history-forgot",
    "originBody": "Subscribe ≡ About Collections Contact Us Sponsor Us Login The Crazy Ones She Built a Microcomputer Empire From Her Suburban Home The untold story of Lore Harp McGovern by Gareth Edwards April 8, 2024 104 1 Listen Every illustration/Inc. magazine. If history is written by the victors, Lore Harp McGovern should have volumes devoted to her contributions to the personal computing industry. In the mid-1970s, from her suburban California home, Harp McGovern—a housewife and mother of two—began assembling memory boards and other computer expansions to sell to the growing hobbyist and business markets. With her friend Carole Ely, she grew their company, Vector Graphic, into a major manufacturer of microcomputers, eventually taking it public before before Big Blue—IBM—muscled into the market. In the latest installment of his column The Crazy Ones, Gareth Edwards tells the untold story of one of the last remaining original founders of Silicon Valley.—Kate Lee London. 1981. A young, smartly dressed woman watches from the sidelines as a stage is prepared. Everyone is there to hear about a hotly anticipated IPO out of Silicon Valley. An investor approaches the woman and asks for a coffee refill from the table behind her. Her train of thought broken, she looks up at him. For a second, she holds his gaze. Then she turns and pours him a coffee. A few minutes later, the host announces that the CEO is ready to speak. Tucking her short brown hair behind her ears, the same young woman straightens her suit and walks confidently up onto the stage. A murmur of surprise spreads across the room, which soon gives way to polite applause. She nods in acknowledgement, her eyes scanning the audience, searching. “My name is Lore Harp, CEO and founder of Vector Graphic,” she says, her accent a mix of German and Californian. She locks eyes with the investor who asked her to refill his cup. “Sir, do you need me to get you any more coffee?” The crowd looks at the embarrassed investor. He shakes his head. “Good,” Lore Harp says with a thin smile. “Let’s continue, then, shall we?” This is the story of Lore Harp McGovern, founder of Vector Graphic. With her friend Carole Ely, she launched a multi-million-dollar computer company from her suburban home and became one of the most important founders of the microcomputer age. It is based on contemporary accounts in publications such as the Harvard Business Review, Interface Age, Kilobaud, Time, and the Los Angeles Times, as well as books such as Women, Technology and Power by Marguerite Zientara; Future Rich by Jacqueline Thompson; and The Untold Story of the Computer Revolution by G.H. Stine. It is also based on the words of Lore Harp McGovern herself. ‘You can do anything you want’ Like many of Silicon Valley’s pioneers, Harp was an immigrant. Born Lore Lange-Hegermann, she grew up in Bottrop, Germany amid World War II, where she shared a bombed-out building with her parents and grandfather, Hermann Lange-Hegermann. Lange-Hegermann had been a businessman and politician during the pre-war Weimar Republic, but the rise of Hitler and the war had cost him his political career and much of his business empire. He believed in the importance of a proper education for all of his children and grandchildren, regardless of gender. As a result, Lore grew up watching her aunt become a successful lawyer and her father take over what remained of her grandfather’s businesses. Lore’s grandfather was her biggest influence. “I remember, as a seven-year-old, getting a little book from him,” she recalled later in an interview with the Computer History Museum. “In my child’s mind’s eye, it was gigantic. And it was a photographic book of people from all over the world in their native costumes, their traditions and all this. And I was so enthused about the book that I said, ‘I will see all these people!’” “You can do anything you want,” her grandfather responded. What her family hadn’t entirely grasped was just how determined Lore was to achieve this goal. Smart and observant, it hadn’t taken long for her to spot that reality didn’t always match the ideal. Her mother was a talented photographer and had studied the subject in college, but society expected her to become a housewife, which she did. So while Lore studied hard, she also watched for an opportunity to do something different. In February 1964, one of her friends pulled out of a planned six-month exchange visit with an American family. It was the opportunity Lore had been waiting for. She went instead. A land of opportunity On February 8, 1964, Lore Lange-Hegermann arrived in Santa Cruz, California. She was 19 years old and had promised her family that she would only stay for six months. Six months later, to her parents’ surprise and disappointment, Lore sold her return ticket to West Germany and hitchhiked to Mexico. After that, she headed to San Francisco. When her tourist visa expired, she became undocumented, unable to officially work. She crashed in a shared house with four other women and took off-the-books jobs to pay rent and survive, while immersing herself in the growing sixties counterculture. During this time, she met Bob Harp. Bob was a young computing researcher and engineer whose talents had secured him a well-paid role at a prestigious research institute in Germany. On his return to the U.S., his path crossed with Lore’s. She was attending a friend’s party when a British sports car roared up outside. It was Bob. Bob was as outgoing as he was brilliant, and the two soon fell into conversation. The fact that Bob had broader horizons than most of his peers and spoke fluent German, thanks to his time in Europe, also helped. He made an instant impression on Lore, and she on him. It wasn’t long before they began dating and fell in love. When he was offered a research job at the California Institute of Technology, she agreed to go with him. For the next year, the pair enjoyed living a bohemian life in Los Angeles, until the inevitable happened—immigration services caught up with Lore. Threatened with deportation and summoned to an interview, the couple hatched a plan. “I put on my little black shift dress,” Lore later said. “My hair was down. I put it in a big bun. I put on my pearl necklace, my father’s gold bracelet, my grandmother’s ring. I sat there like Miss Prim and Proper. And we said, ‘Oh, we just got engaged, and we’re getting married.’” And they did. Lore Lange-Hegermann became Lore Harp. “We found some minister in our neighborhood, and then we went to McDonald’s and had a hamburger.” The gilded cage While Bob had worked at Caltech, Lore was able to immerse herself in the flourishing local art community. But over time the gears of his career began to turn. In the early seventies, Bob was offered a better-paid aeronautical research role at Hughes Research Laboratories, which he accepted. For Harp, this meant leaving vibrant Los Angeles for a more traditional suburban life in Westlake Village, California. “We moved from there to way out in the boonies,” Harp recalled. “Westlake Village which is…oh my God.” By 1975, this lifestyle had begun to take a toll on her mental health. Harp had stayed in America to escape a future where her only option was to be a housewife.Yet in Westlake Village, this was the only role it seemed possible for her to play. While Bob worked, she was required to manage the home and join the various women’s groups and their activities like charity fundraisers and dinner parties—obligations expected by her community and Bob’s employer. To try and prevent this from taking over her whole life, she started studying anthropology. But then she became pregnant with their first child and was forced to abandon her studies. A second—another daughter—followed two years later. When the children were three and five, Harp tried to pursue school again, this time hoping to follow in her aunt’s footsteps and become a lawyer. But law school was not friendly to young mothers. “My husband was not the kind of person who would take over half the chores while I studied, even though he was otherwise supportive,” she said. Forced to quit, Harp felt frustrated and isolated. The social activities that seemed to provide a sense of value and purpose for many of her neighbors just didn’t feel satisfying for her. “I was 32 at the time, and I felt…‘My God, suddenly I’ll be 40, the children will be gone, and where am I going to be?'” The product Harp first met Carole Ely while they were waiting to pick up their children from school. Ely had started a promising career as a bond trader when marriage and children intervened. Now, like Harp, she was stuck living the life of a Westlake Village housewife. She, too, felt caged. The duo talked about starting a business together. Their first idea was a travel agency. However, the travel industry in California was one of entrenched interests—when it came to securing partnerships and permits, who you knew mattered just as much as how good your product was. Networking required membership of the kind of business and social clubs that would never allow two women like Harp and Ely to join. Then, in January 1975, Popular Electronics published a picture of the Altair 8800 on its cover. Bob Harp, like many who worked with large computers in their workplace, had never imagined that he might one day own a machine of his own. He placed an order for the Altair, which was billed as the world’s first mass-market “microcomputer.” When the machine arrived in kit form, Bob quickly assembled it and it became one of his obsessions. The Harp house was soon filled with pamphlets, newsletters, and how-to guides about microcomputers, and Bob discovered, to his delight, that the Altair’s creator Ed Roberts had built it around a principle of “open architecture” via its S-100 expansion socket. Anyone could create memory boards or peripherals that could expand the functionality of the machine . A few third-party expansions were already on the market and more were being released regularly, but Bob decided it would be more interesting (and a fun test of his skills) to design his own expansion board. Over the course of a few months, he created a board for his own use, one that would double the amount of RAM in his Altair from 4K to a mighty 8K. While Bob was at work, Harp would read the computing materials that littered the house. Dinner table conversations would routinely include discussion of the latest trends in technology. Through a form of osmosis, she became versed in the growing computer industry herself. “[Bob] was reading tech publications all the time, and I kind of read them as well, and looked at them, and saw what was going on,” she said. Harp decided that Bob’s 8K RAM board was the opportunity that she and Ely had been waiting for. They would make and sell them from home. In return, Bob would receive a royalty payment on every sale. She called her prospective business partner and asked her if she wanted to team up and sell memory boards. “Sure,” Ely replied. “What’s a memory board?” Two days after that phone call, Lore Harp and Ely attended the Southern California Computer Society’s monthly show. They were there to scout out their potential market. They watched vendors try to sell bags of chips and parts for memory boards and other expansions, often with poorly composed hand-written instructions on how to build them. The whole operation seemed distinctly amateur, yet it seemed to work. Attendees would hand over bundles of money—sometimes as much as $1,000 (about $5,000 today)—in exchange for these products. Two days later, on August 23, 1976, Vector Graphic was incorporated. Its head of marketing was Carole Ely. Lore Harp was the CEO. Carole Ely (left) and Lore Harp. Source: Vector Graphic. The deal “It’s Lore Harp from Vector Graphic. We have a new memory board and need about 50,000 chips.” Harp was seated in the family room of her Westlake home on the phone with a sales representative at chip manufacturer AMD. The company was one of a small number of independent U.S. chip manufacturers that were prepared to supply third parties. Harp and Ely needed these chips so they could package them into kits in Harp’s house and sell them as memory board kits. “Oh!” The sales representative on the other end of the phone sounded keen. “I’d be happy to come to your office to discuss it.” Harp looked around the room and across to the kitchen beyond, where a large pot of stew was cooking on the stove. “We’d love to meet you in our office, but we’re in the middle of moving,” she replied, thinking on her feet. Harp told the representative they were in Westlake and suggested they meet at the Westlake Inn. The sales representative agreed. After calling Carole and telling her the news, Harp called Bob at work. “You’ve got to get home from Hughes,” she told him. “You’ve got to be here at a quarter to five to care for the kids. We have to go meet a guy from AMD.” The memory of that first meeting was seared into Harp’s brain. “He had these little monogrammed cufflinks and that aftershave everybody had. Some Old Spice thing,” she remembered about the salesman later. “And he gave us this up-and-down look. I never forget this.” The three of them sat down in the hotel’s restaurant area and tried to negotiate a deal. Harp told the man from AMD that they planned to manufacture memory boards and sell them to the growing hobbyist market. The salesman nodded along, then gave them a price. It was far too high. “He didn’t even know what an Altair was,” Harp said. As a result, he didn’t believe their assertions that there was a large enough market for their product to make a discount worthwhile. They refused AMD’s terms. Not to be deterred, Harp called Fairchild Semiconductor, another major chip manufacturer. Once again, it led to a meeting at the Westlake Inn. “You know what,” the Fairchild representative replied after Harp had explained their plan. “I’ll help you.” Now Harp took her biggest gamble. They didn’t have enough money to pre-pay for the chips, so she decided to be honest. “We will pay within 30 days because we cannot afford to prepay,” she told him. Harp knew that they’d need to sell enough boards first to cover the cost. “We promise we’ll pay in 30 days or before.” The Fairchild man was silent. Then he laughed. “You know what? I trust you.” They shook hands and went their separate ways. A few weeks later, there was a knock on the Harp family door. When Lore opened it, a man named Dick Kirkpatrick introduced himself. He explained that he was their Fairchild distributor and pointed to the boxes of chips in the back of his rental car. Harp helped him carry the boxes into the back bedroom that they had begun converting into an assembly space. The sell Stan Veit had just opened up his computer store one autumn morning in New York when the phone rang. It was a woman calling from California. She wanted to sell him some memory boards. “I listened because a woman selling computer equipment was indeed a novelty,” he wrote later. “After speaking with her, I found out that her name was Lore Harp.” Veit didn’t know it, but he was about to be subject to a sales pitch created by Carole Ely to make Vector stand out. Harp told Veit that the Vector boards were not only superior to anything else on the market—they were cheaper as well. She made Veit a simple offer. “She would ship me two boards, and if I liked them, I would call her and she would ship me six more and I would pay cash on delivery for the six,” Veit recalled. “If I didn’t want them I just had to send her boards back. Well, that seemed like a fair offer, so I agreed.” When the boards arrived, they were as good as Harp had claimed. Veit made the call. “Thus, I became a dealer for Vector Graphic, a company run by two women,” he wrote later. Veit wasn’t the only one. Harp and Ely systematically went through computer hobbyist magazines like Byte and Interface Age, which were full of advertisements from dealers like Veit. It was from these computer dealers that most hobbyists purchased their microcomputers and parts, so the Vector Graphic team targeted them aggressively. They would call them and pitch the same arrangement that they had offered Veit. By October 1976, orders were flooding in. Harp and Ely’s product wasn’t revolutionary, nor were their boards significantly cheaper than their rivals, but their approach to the process of sales and ordering was more professional. This also extended to the board kits themselves. The pair hadn’t forgotten the poor-quality instructions and packaging they’d seen at computer fairs when starting out. They put extra effort into making sure that the packaging for their own boards was of a high quality, providing clear instructions for assembly, and even color-coded their chips. Overall, Vector’s commitment to high standards made them stand out. By the end of the year, they were almost overwhelmed with orders for their memory boards, with demand starting to exceed the number of kits the two women could assemble themselves at Harp’s home. They were also attracting neighborhood attention. It was hard not to. Every weekday morning, Bob Harp would depart for work and Lore would take the children to school. Shortly after, Lore would return home and Ely would knock on her door. The rest of the day would see a steady stream of men in suits visiting the house. A short while later, they would leave. It would stop when Lore left to pick up the children and Bob got home. The men were computer parts dealers, but the neighbors didn’t know that. “I think our neighbors thought we had a brothel going,” Harp said. Eventually Harp’s next-door neighbor, Jean, grew too curious to remain silent. She knocked on the door and asked what they were up to. Harp invited her in. Jean was stunned by the operation. The rear bedroom and family room of Harp’s house had been converted into a production line for memory boards and board kits. Boxes of chips and parts were stacked on the floor, while the table was covered in fruit bowls into which the correct chips for each board had been carefully sorted, ready to be packaged. In the kitchen, salesmen helped themselves to coffee while discussing orders with the women. Toward the end of the afternoon, local high school students would arrive, paid by Harp and Ely to help with packaging and distribution. Watching, Jean admitted she had never had a job, although she had always wanted one. After a brief discussion, Harp and Ely told Jean that they were expanding and offered her a job as their receptionist. She became one of their first full-time employees. Forty years later, Harp would still vividly recall the joy of giving Jean her first ever paycheck. “It…it was just great,” Harp said, a smile on her face. By December 1976, Harp and Ely had built Vector Graphic into a million-dollar company. In 1977, Harp decided they should go one step further: They should release a desktop microcomputer themselves. This would go from a company oriented around building add-ons for other company’s machines to making machines themselves. The Vector 1 Harp persuaded Bob to leave his job at Hughes and join Vector Graphic full time. Harp and Ely had already employed a number of engineers to design parts, but building a computer would require someone to manage them and create an overarching design. Bob possessed the right skills and vision to do this. The type of computer the company would build was determined by a growing demand for business machines that had been identified by their dealer network. The pair had a reputation for treating the dealers that sold Vector products fairly, so they were happy to be regularly canvassed for their opinion on what products would sell. Vector had an advantage over its competitors when it came to market intelligence. Vector’s dealers reported that their clientele was expanding. The hobbyist market was still the primary driver for sales of first-generation computers like the Altair, but small-to-medium-sized businesses were becoming interested in owning a microcomputer to use for word processing, accounting, and other business-related tasks. Unlike the hobbyists, they were often leaving stores empty-handed. This wasn’t due to fears over price—quite the opposite. These buyers were prepared to pay more than $3,000 (roughly $16,000 today) for the right machine coupled with the right hardware add-ons or business software. It was that they wanted a microcomputer that was reliable, powerful, expandable, and pre-built—unlike hobbyists, most businesses had no desire to assemble their own machines from a kit. It also wouldn’t hurt if it was something that could serve as an office centerpiece around important clients. A machine that ticked all these boxes didn’t yet exist. The market was machines like the Altair or Apple at the low end or expensive, room-sized computers by IBM or others at the high end. There was nothing in between, so Vector decided to fill that gap. At the 1977 West Coast Computer Faire, Steve Jobs and Steve Wozniak set the home computer world abuzz with the announcement of the Apple II, a pre-built machine for home users. Just a few booths down, Lore Harp and Carole Ely did the same thing for businesses with the announcement of the Vector 1, designed by Bob Harp. In terms of CPU and RAM, it wasn’t anything special, launching with the same specifications as the Altair 8800. What differentiated it was that it could be easily expanded—well beyond the limits of the Altair. It also came pre-built. Carole Ely (left) and Lore Harp holding a Vector 1. Source: Vector Graphic. “Maybe people are really interested in having a computer that doesn't have all the switches,” Harp told an excited Personal Computing World journalist. “Maybe it’s a little beyond the hobbyist who likes to fiddle with all that and likes to see everything work.” The Vector 1 was a no-nonsense, ready-to-use computer designed for businesses that wanted a machine that would just work. It also had style, featuring a sleek metal case with rounded corners. It was even available in four colors—burnt orange, dark green, black, and beige. “It was not Apple that did colors first,” Harp remembered later. “The folks at Vector have managed to come up with a rather slick-looking entry into the market,” Kilobaud Microcomputing wrote. “And, it’s all theirs.” Other reviews of the Vector 1 were equally impressive. So, too, were those for the Vector 2—a machine that kept to the same principles but was faster and had more RAM—that followed soon after. Together, the machines pushed Vector Graphic to the top of the second tier of microcomputer manufacturers that almost exclusively focused on business customers. Companies like Apple and Commodore could boast larger unit sales if figures for both the business and home markets were combined, but when it came to more expensive machines for medium-sized businesses, Vector was the dominant player. By 1980, it was achieving $25 million (about $95 million today) in annual sales. The company that had been started in Harp’s converted back bedroom was now a well-known brand within the computing industry. Wall Street also began to notice. From the ground up It wasn’t just the functionality of Vector’s machines that impressed the business market. It was also that they reliably worked. An advertisement for the 1979 Vector MZ, championing its reliability. Source: Author’s collection. In customer surveys of the time, Vector Graphic consistently outperformed Apple, Commodore, Tandy, and others in terms of customer satisfaction among business users, largely due to reliability. Lore Harp had mastered running microcomputer production lines, something most other computer startups struggled with. Under her leadership, Vector rarely suffered any workforce issues, and the quality of output from the production lines themselves was high. Her success wasn’t just due to her eye for detail. It was also because she recognized that production is, fundamentally, about people. In an era in which production-line staff was largely treated as unskilled labor and corporate benefits were reserved for the C-suite, Vector Graphic bucked the trend. When the company hit its first $1 million sales month, Harp closed the factory and took all 500 employees out to play baseball and drink beer. The company also paid well. It even had a daycare and offered home cleaning services to employees who were struggling to balance household responsibilities with their work. These benefits did not go unnoticed in the trade and business press, nor that, as a result, Vector had a higher-than-average number of female employees. In interviews, magazines would ask if Harp’s approach was intended as some kind of feminist experiment. Alternatively, they would ask if it was evidence that female CEOs managed more emotionally than strategically—often with the implicit accusation that the company benefits were a sign of weakness. Harp’s answer was always the same: These benefits and policies were in place because they made economic sense. Harp’s philosophy was made clear when Vector went public in 1981. In the first meeting with the IPO’s prospective underwriters, she dropped a bombshell—she wanted every Vector employee to receive stock options. “That guy in assembly who puts in the last screw? If he is really mad, and he doesn’t do it properly, and Quality Assurance misses it, it’s not just the guy back there who hears about it. It’s the VP of sales,” Harp explained. “Because soon it is sales going down because suddenly you’re no longer delivering quality product. Everybody is deserving, not just those of us sitting at the front office.” She was told it wasn’t normal. Share options for senior staff was fine, but offering them to the entire staff would harm the value of the offering. Harp wouldn’t budge. “Well, you’ve got five minutes,” she told them. “We must have a deal at the end of that time, or we walk.” The underwriters protested again. “Okay,” Harp said calmly, looking at her watch. “Well, that’s 30 seconds gone.” After a frantic discussion, the underwriters agreed to find a way to make it work. In the end, Vector’s staff received stock options based on their time at the company, priced at $1 per share. When the IPO was successful, these shares increased in value by a factor of 13. “People couldn’t believe it,” Harp said later. “They were thrilled.” The IPO also made Harp the second-ever female founder to take a company public on the Nasdaq stock exchange. Fellow Silicon Valley tech entrepreneur Sandra Kurtzig beat her by just a few days. Harp didn’t mind. Kurtzig was the founder of ASK, a software company focused on business productivity applications. As the two most prominent female founders in the industry, they had become good friends and business allies. Although the IPO would represent a major achievement for both Harp and Ely, running Vector was taking a toll on their marriages. Ely’s husband had tolerated her role at Vector as an outlet for her boredom, but as she continued to invest more of her time in the company, their relationship began to deteriorate. For the Harps, cracks were also appearing. Bob had also initially seen Vector as a side project for Lore Harp, and his direct involvement created further problems when the two began to hold different opinions on how the company should be run. He was resentful of the level of attention she received in the press, feeling it diminished his own role at Vector. The Ice Maiden In the beginning, both Harp and Ely were treated as a curious novelty by the rest of Silicon Valley. Their impressive range of products allowed them access to the right circles and markets, but that often came at a price. At best, they were treated with a sort of benign paternalism by competitors that Harp was often all too happy to exploit. Often, the first time a competitor realized how ruthless she could be was when they discovered their customers were now Vector’s customers instead. Deals and product demos often took place in hotel rooms during industry shows or on sales tours. Unfortunately, Harp and Ely were often left having to deal with attempts at abuse or exploitation. “They either just loved you, or they were trying to be after you,” Harp said later. “One guy actually got slapped. I said, ‘I’m here for one purpose only.’” As Harp climbed higher within American industry, the paradigm started to shift. Senior figures within other companies began to resent her presence at the top of the tech pyramid. What had begun as an unthreatening story of two housewives running an odd little business became one instead of two women successfully building and expanding their empire. To many, this didn’t seem possible, and articles of the time suggested that Harp’s husband Bob played a larger role than he actually did. By diminishing the role of the two women in the success of their own company, readers were reassured—whether purposely or not—that behind every successful woman, there must be a man. This attitude wasn’t universal. There were those who saw Harp’s skills clearly for what they were. In late 1980, it was Harp—and Vector—who Adam Osborne approached while looking for a manufacturer for his revolutionary portable computer. Osborne was making the jump from writing about computers to designing them. He knew that manufacturing his new machine—the Osborne 1—would be difficult and complex, so he wanted it to be made by the best. In his mind, the best was Lore Harp. Harp was skeptical and wanted to wait until Osborne had a production prototype. Unwilling to wait, Adam Osborne pushed ahead on his own. After Vector went public in August 1981, the negative perception of Harp held by some executives only grew stronger. Female business icons were allowed, as long as they followed the unwritten rules and remained quiet and humble. Harp was proud of her company and her success, and was happy to call out individuals who treated her poorly, as she had done at that London meeting for Vector’s IPO. Soon, she was referred to as the “Ice Maiden” within the industry. Early in 1982, a female employee wrote to Harp, congratulating her on her success. However, the employee expressed her dismay at some of the things she would hear from male industry figures when she mentioned that she worked at Vector. She told Harp that one man had complained to her about “the awful bitch who was running the company.” “I sent a nice note back,” Harp remembered later, “and said I especially enjoyed the comment about this bitch running the company because that poor guy is either so jealous or he’s so stupid that he doesn’t have anything else to talk about, and I must be terribly important in his eyes.” Long before the phrase was coined, Lore Harp was busy living rent-free in many male executives’ heads. The breakup Harp and Ely’s success came at a cost. By the end of 1981, Carole Ely was divorced; her husband was unhappy with how much of her time was occupied by her career. Lore and Bob Harp had already effectively separated by the end of 1980, and the possibility of a divorce was even declared as a business risk in the company’s IPO documents. Any faint chance of reconciliation ended with the positive press Vector received after the company went public. Much of this focused on Harp’s leadership role and impact on the company’s success, including an interview in Time magazine. Bob, annoyed at the attention his then-wife was receiving, burned a number of copies of Inc. magazine—which had featured Harp on its cover—in front of her. Official divorce proceedings began not long after, eventually concluding in the summer of 1982. In the end, Harp would be divorced for only a single day. She had already begun a relationship with the man who would become her second husband—publishing mogul Pat McGovern. She became Lore Harp McGovern. They proved to be a good match, enjoying their time together but respecting each other’s boundaries. The marriage would last until McGovern’s death in 2014. ‘Big Blue’ awakens Adam Osborne was not the only person in Silicon Valley impressed with what Lore Harp McGovern had built at Vector. Just before she took the company public, a man named Don Estridge led a delegation from IBM to pay her a visit. Estridge indicated that IBM was thinking about making a small move into the microcomputer market, and suggested that Vector could supply computers for IBM to badge and sell under an original equipment manufacturer (OEM) arrangement. Rather than designing and selling its own microcomputer, IBM would be happy to confine itself to purchasing stock from Vector that it would sell—with a mark-up—as IBM products. “Let me get this straight,” Harp McGovern asked him. “You are a $25 billion business. We’re a $25 million business. And you are interested in potentially buying OEM products from us? That seems like a highly unlikely proposition.” Estridge insisted it was true, and Harp McGovern played along. Estridge left with a number of Vector machines as samples. Harp McGovern was smart enough though to know what this really meant: IBM was building a microcomputer. She called an emergency meeting of her senior executives. “We have one year, if that,” she said. “The world is going to change.” Vector Graphic knew that if IBM (“Big Blue” to its friends and enemies) entered the market, it would target business clients—Vector’s bread and butter. This couldn’t have come at a worse time. Bob Harp, who despite the divorce was still an intrinsic part of the company’s design team, had recently overseen two major technical missteps. Vector was late in moving from machines with 8K processing to 16K, which had become the new industry standard. This left Vector—and its dealer network—selling a range of machines based on dated technology longer than other manufacturers. As a result, its customer base was slow to upgrade to new machines, choosing to wait until Vector released one based on 16K technology. This problem was compounded by the release of the Vector 3, its final 8K machine. The computer itself was as reliable as ever, but its keyboard wasn’t detachable and instead was built into the computer’s case. Users found this design uncomfortable to use and the Vector 3 became the first of the company’s machines to garner negative reviews. Because of these mistakes, Vector failed to sufficiently expand its business before Big Blue arrived—even though Harp McGovern had correctly guessed that IBM was about to enter the game. Vector had little financial room to maneuver once the first IBM PC was released in August 1981. Vector made another bad decision later that year, even after the impact of the IBM PC on the business microcomputer market became apparent: Vector continued to use the CP/M operating system rather than switch to Microsoft DOS. CP/M had been the operating system of choice within the business sector for some time. Most business software was written to work with it, which made it the obvious choice for any microcomputer manufacturer around which to build their machine. Despite this, Vector had always enjoyed a close relationship with Microsoft, which had its own aspirations to be a power player in the operating system market. Indeed, Bill Gates occasionally worked out of Harp McGovern’s office when he was in town. As a result, Harp McGovern had the opportunity to see, sooner than most other companies, what Microsoft was adding to its own operating system in an effort to capture the market. Once the IBM PC debuted with Microsoft DOS—not CP/M—installed, building future machines around this upstart operating system began to look more attractive. It could offer more functionality, while IBM’s adoption of DOS all but guaranteed that software companies would rewrite their business software to support it. It was a switch that Harp McGovern herself was inclined to make, so she contacted Gates and negotiated a provisional contract for Vector to pivot to using DOS instead of CP/M on far sweeter terms—and at a much faster pace—than were being offered to other manufacturers. “We had an amazing relationship with Microsoft. I’d signed a contract where every update and every new system in perpetuity we would get at no increased royalty,“ she explained. The deal was taken to the board, but the collective decision was made that it was better to stick with the known quantity that was CP/M for the in-development Vector 4. Switching would potentially mean redesigning the next line of machines. It meant re-educating their dealers and clients in the new operating system, and there was no guarantee that every piece of software they needed would be ported to it. The board, which included a number of ex-IBM employees, was also convinced that IBM would soon lose interest in the microcomputer market, leaving it entirely. If that happened, it would leave Vector alone on DOS—a precarious position to be in. In the end, the board agreed that moving to DOS was the bigger risk. This decision robbed Vector of the chance to be one of the first manufacturers to offer “IBM compatibility.” The window for survival in the post-IBM world would be narrow, and Vector had just narrowed it even further. “I would say not having forced this through to make it happen was probably a flaw on my part,” Harp McGovern reflected. “By kind of going against my instinct that this needed to happen in order to be competitive. Because IBM was lusting after our dealer network.” 1982 was a tough year for Vector and the microcomputer industry in general. IBM aggressively pushed to gain market share with its PC, squeezing out many of the smaller players in this space. Harp McGovern worked hard, and successfully, to defend Vector’s share of the market in this hostile environment. Margins were squeezed, but the company’s commitment to quality and support gaveVector a fiercely loyal customer base and dealer network. Sales remained strong, and the company remained profitable. Vector seemed to have secured its niche. So at the end of 1982, Harp McGovern stepped back, relinquishing the role of CEO that she had held since founding the company in 1976. She oversaw the appointment of Fred Snow, an experienced industry hand from Honeywell, as Vector’s new CEO. Since the divorce from Bob, Harp McGovern had wanted to find more time to spend with her own children. She also wanted to fulfill the promise she had made herself as a child and travel the world in her spare time. Both Bob and Carole Ely had left the company by this point, and she decided it was her turn, too. She thought Vector no longer needed Lore Harp McGovern. She was wrong. The final roll of the dice In 1983, the Vector board petitioned Lore Harp McGovern to retake the position of CEO. In her absence, the company had lost more ground to IBM. Countering its moves required quick, decisive action, which the new CEO was unable to provide. The company’s market share had declined precipitously, and it was losing money at an alarming rate. Harp McGovern was reluctant to step back into the fray. “I had moved up to northern California at the time,” she said later in an interview. “I’d just moved up here a few months earlier with small children, nine and eleven, right after school was out, no friendship, no circle for them.” Harp McGovern had promised to spend more time with her children. It was not a promise she was prepared to break. The board pleaded with her to return anyway, stressing that layoffs were already necessary. Without her back at the helm, they likely faced bankruptcy. Harp McGovern relented. For almost nine months, Harp McGovern worked to save what was left of the business she and Ely had built. She oversaw a round of layoffs, which was painful for someone who believed the employees were the heart of the business. It was clear to everyone now that the future lay in IBM-compatible machines running DOS—the boat that Vector missed in 1982. Harp negotiated new investment to enable the development of IBM compatibility for the Vector 5 and beyond. . With gargantuan effort, Harp McGovern managed to drag Vector back close to profitability. The personal toll, however, was enormous. Keeping the company alive took all of her talent, but it also took her away from her children. When she had agreed to become CEO again, it was conditioned on her not having to move back to LA. She wanted her now school-age children to have a stable home life. To achieve that, she endured multi-hour, long commutes to Vector’s headquarters in southern California. “I commuted every day to Los Angeles between here and Burbank,” she said. “I would take a 6 a.m. flight. I’d get to Burbank, have my car in the parking lot, got to the office at 8:30. And at night, I’d try to get the 6 o’clock flight back so I could have dinner with my kids a little late but still have dinner, and the next day, do the same thing again.” Soon, Harp McGovern wasn’t just fighting external battles. Friction grew with the board over where savings should be found. Where they wanted to find more cost efficiencies in staffing or production, she argued instead that they should seek new investment and expand into markets yet to be targeted by IBM. They rejected her plan to develop a new machine that would focus on networking and telecommunications, which she saw as the future of computing. In the end, Harp McGovern was worn down. In 1984 she walked away (again) from her own company: “I finally said, ‘Guys, I’ve had it. I’m out of here.’ I dumped all my stock, had a good cry at my lawyer’s office because it was just…Oh, it was just…” Life after Vector Harp McGovern’s departure sealed the fate of Vector Graphic, although the company would limp on for a few more years. In October 1987 Vector Graphic finally declared bankruptcy. By that point, Harp McGovern herself had been out of the picture for three years. That didn’t stop a number of journalists from treating its failure as unspoken vindication that housewives—and Ice Maidens—lacked the mettle to succeed. The departure of Bob was often pushed as the turning point. Bob himself was often on hand to provide a useful quote in support of this idea. “They didn’t develop any successful products after I left there,” he told the Los Angeles Times in 1985 after Vector filed for Chapter 11 bankruptcy protection. “If the proper decisions had been made, it would be quite successful.” By contrast, Harp McGovern refused to talk much about Vector for a long time after her departure. To her, Vector wasn’t just a company. It was more than that. It was something she and Ely had built from nothing into a profitable community of people. Harp McGovern had only intended her absence from the business world to be temporary. By 1987, she was back. With Vector gone, she founded (and funded) one of the first companies attempting to develop disposable urinal funnels that would allow women to urinate standing up. Then, in partnership with her husband Pat, she launched a venture capital fund. In 1994, she became one of the original “Band of Angels”—one of the first angel investor groups in California focused on technology and life sciences. “What I really enjoy is growing the company,” Harp McGovern said when asked what drove her. “Growing people within the company, accepting the challenge of being out there, competing against other companies and making an impact.” At first, venture capital and angel investment seemed to offer those opportunities for growth, but over time she became disillusioned with this world. Many investors were happy to take her money, but they refused to accept that she might have useful advice to offer. “After I’d done Vector—building a company from totally nothing to fairly good size with an international distribution network, having gone through raising venture capital, having gone through taking the company public and that sort of thing—I felt I could make a great contribution to other young companies that wanted to start in business,” she said later. The reality was quite different. Most entrepreneurs don’t want all the help I thought I could bring.” Providing funding without hands-on advice held little interest for Harp. She eventually moved away from this industry as well. In the end, Harp McGovern’s legacy would end up being something different. In 2000, alongside her husband Pat, she gave $350 million to endow the McGovern Institute at MIT. She played an active role in establishing it as one of the foremost research institutes into the brain in the world. Of all the figures we have explored so far in this series, and of all those we are yet to explore, Lore Harp McGovern is likely the one who has been overlooked the most. Perhaps only Harp McGovern’s friend Sandy Kurtzig runs close. While Steve Jobs and Steve Wozniak were building a computer empire in the suburbs of California, Lore Harp McGovern and Carole Ely were doing the same. As with Adam Osborne, however, the history of Silicon Valley likes to focus on its winners. But the voices that are the most outspoken about the need for women to step up and beat men at their own game are often those that are the quietest when that actually happens. Harp McGovern didn’t just shatter Silicon Valley’s glass ceiling—the Ice Maiden (or “the awful bitch”) used its broken shards to carve out a place for herself, her company, and her employees along the way. All too often, her successes have been allocated to other people, while her failures have been attributed to her alone. For all these reasons, one of Silicon Valley’s true pioneers has been granted only a single inaccurate paragraph on Wikipedia. In 2016, Harp McGovern recorded an interview with the Computer History Museum. “I'm delighted to do this, mainly because there were so few women in the industry at the time,” she said, when thanked for her contribution to the museum’s archive. “And also for my grandchildren to see that their grandmother actually did things that not a lot of women did in those days.” It’s not just Harp McGovern’s grandchildren who should be aware of her achievements. Because unlike many of those who kickstarted the golden age of computing, Lore Harp McGovern, in her late seventies, is still with us today. If you are a young founder today who finds themselves in her presence, you would do well to ask one of Silicon Valley’s last remaining original founders for her advice. If she’s prepared to offer it, then listen. Gareth Edwards is a digital strategist, writer, and historian. He has worked for startups and corporations in both the UK and U.S. He is an avid collector of old computers, rare books and interviews, and abandoned cats. Follow him on X, Mastodon, and BlueSky. To read more essays like this, subscribe to Every, and follow us on X at @every and on LinkedIn. What did you think of this post? Amazing Good Meh Bad Like this? Become a subscriber. Subscribe → Or, learn more. Read this next: The Crazy Ones The Secret Father of Modern Computing How Ed Roberts created the personal computer industry—and then walked away 263 8 Dec 15, 2023 by Gareth Edwards The Crazy Ones The Rise and Fall of Steve Jobs’s Greatest Rival Adam Osborne was the master of momentum—until it all came crashing down 157 4 Mar 4, 2024 by Gareth Edwards The Crazy Ones She Built a Microcomputer Empire From Her Suburban Home The untold story of Lore Harp McGovern 104 1 Apr 8, 2024 by Gareth Edwards Thanks for rating this post—join the conversation by commenting below. Comments Post @mike_5351 about 12 hours ago wonderful story telling Gareth! And what a story. What an incredible woman. ♡ 0 · Reply ✕ Every smart person you know is reading this newsletter Get one actionable essay a day on AI, tech, and personal development Subscribe Already a subscriber? Login Contact Us · Sponsor Us · Search · Terms ©2024 Every Media, Inc",
    "commentLink": "https://news.ycombinator.com/item?id=39970915",
    "commentBody": "Lore Harp McGovern built a microcomputer empire from her suburban home (every.to/the-crazy-ones)455 points by adrianhon 18 hours agohidepastfavorite103 comments laurex 14 hours agoIt’s not just that she overcame odds as a woman in the tech business that amazes, but that she was so clearly someone who cared about people, and chose to risk her business and reputation more than once to stay true to her values. That’s perhaps even more rare in this industry than being a successful female CEO. reply garius 14 hours agoparentSomething I didn't have space to mention in the piece was that during the recession of the early eighties, Vector went out of their way to support their dealer network. They offered loans and let dealers delay payments on deliveries to get them through the tough times. It arguably cost them ground against IBM because it squeezed them further financially. But it was also another reason the Dealer network remained fiercely loyal to Vector - especially under Harp. reply TMWNN 11 hours agorootparentThanks for the article. Benji Edwards's earlier article was the first time I really became aware of Vector's existence. There are noticeably fewer mentions of the company in Freiberger and Swaine's Fire in the Valley (1984) than, say, Cromemco, and far fewer than IMSAI. reply toolz 14 hours agoparentprevI do believe there are unique challenges to being a woman in tech, but the odds seem in favor of women doing well both back in the 70's and today with todays stats having roughly 20% of CS grads being female while some 23% of SWEs are female. That suggests there are more women in software jobs than women who have been pursing that career academically. What stats do you see that suggest the odds are against women in tech? I frequently recommend tech as a good field for young girls, but I'll probably not do that anymore if the odds are truly against them. reply leononame 14 hours agorootparentHow ist 20%/23% good? Am I reading the numbers wrong? 40%, that I could agree on. But 23% is very low. Another thing is culture. The in the company's where I've worked at, how the men talked about women was pretty off-putting to be honest. They didn't do it in front of women (obviously), but even your nerdy developers would drop comments that had me wondering whether I was really in the ckrrect field. I'm sure the women in those places notice that even if it's behind their backs. reply ekms 13 hours agorootparent23% > 20% which means if someone goes into the field of computer programming they're more likely to remain in the field if they are a woman than if they are a man. \"remain in the field\" is used as a proxy for success. You could argue about whether or not it's a good proxy for success, but your response sounds like you think women would be more likely to drop out of the field alltogether than men, which doesnt appear to be true reply leononame 12 hours agorootparentDoes it really say that or are women just slightly more probable to enter the field without a degree? And I'd argue it's a pretty bad proxy. Because the field might be growing (or shrinking) and percentages don't mean anything. 23% of 10k is less than 20% of 5k, for example. The percentage numbers don't really indicate whether someone will stay in the field, it's just a number that's highly dependent on a lot of variables and a very bad indicator for \"people are staying in the field\". I'm happy to be corrected, it's just how I read this. Additionally, if your assumption is that 23%>20%, that would kind of mean that it's capped at 23%, right? Once more the CS degree quota is higher than 23%, following your logic, that would be an indicator that women are more likely to leave the field because it naturally gravitates towards 23%. But that's not based on anything, you could argue just as well that it's an indicator that more women are starting to take interest in CS as a career. reply bobthepanda 11 hours agorootparentprevconsider that a lot of the culture in tech is also there for the first four years of undergrad, and so 23% often represents the people who basically made it through four years. are people who have experienced it for four years likelier to put up with more of the same? reply toolz 13 hours agorootparentprevwell I'm not making a value judgement, but we're talking about odds, not \"good\" or \"bad\"...if 20% of women go after a software job and the field is made up of an even higher %, that suggests the odds are amazing for women. Odds don't tell the whole story, but the odds seem in women's favor at the moment. reply leononame 12 hours agorootparentIf you define odds being good as \"the odds are good for the ones that choose to study CS\", sure. But if you define the odds as \"women overall\", 20% is a relatively poor number in my opinion. Yes, we're getting better and yes, it takes time. But I don't think we can pat ourselves on the back here. That the women who decide to work on tech do well is (in my very unscientific and unproven) opinion just an indicator that the women who do join tech are on average more skilled than the men who decide to join tech. That's for a myriad of reasons, but the main one being that men gravitate to tech more, so even if they're not a huge talent they still might choose a career in tech, whereas women might prefer a different career unless they have a very strong calling. reply trimethylpurine 3 hours agorootparentThese assumptions and stats are virtually meaningless. Every human being, man or woman, has unique challenges. Classifying these challenges by sex ignores the vast and more important majority of an individual's fitness for one career or another, or lack there of. More than just encouraging your daughter to study tech or any other career (tech might be saturated), encourage them to learn how to interview aggressively, and how to ask for raises. Encourage them to be fearless. And do the same for your sons. reply AlecSchueler 1 hour agorootparent> Every human being, man or woman, has unique challenges. Have you faced sex based discrimination, intimidation or othering in your workplace? > ignores the vast and more important majority of an individual's fitness The issue is that the capacity of women is backgrounded to the point that they have to do more to be seen as talented as their male counterparts. I'm sure every woman in tech would love to focus on skills instead of sex but that's just not the world they're presented with. > More than just encouraging your daughter to study tech More than this teach your sons about bias against women, how to have empathy for historically marginalised groups, how to give space for quieter voices, the broader cultural norms that lead to inequality etc You can teach generations of daughters whatever you like but the weight of solving these issues is far from resting only on women, and the idea that it is is ironically hostile in itself. reply rootusrootus 13 hours agorootparentprevWould be nice if it were higher, for sure. And it will become that way, because more women go to college now than men. Will we care about young men being under represented in college before they get down to 20%? I'd like to think so, but I won't take that bet. reply AlecSchueler 1 hour agorootparent> go to college now than men. Will we care about young men This is called whataboutery. The fact that we are still de-railing conversations about women's representation to centre men's issues shows exactly why there's still so much work to be done. reply randomdata 7 hours agorootparentprev> Will we care about young men being under represented in college before they get down to 20%? We won't care about men being under represented, but colleges may worry that they are losing out on customers if the male population of college buyers swings that low. That may prompt marketing campaigns to try and attract men into college. I mean, it is not like we care about women being under represented either. Nobody is ever bothered by just 5% of firefighters being female. Tech was only ever concerned about women in tech because the industry was desperate for a larger pool of workers and women looked like an untapped source of people. reply AlecSchueler 1 hour agorootparentprevCan you see that you've completely dismissed the lived experiences of many many women, brushing them aside with whatever statistics you could find? And what do those statistics show, only that women are vastly under-represented in work and education. There's very heavy cultural reasons for that and your comment actually feels reflective of them. reply dosinga 12 hours agorootparentprevThat's one explanation. The other is women just have to be better to survive the CS education so if they do, they are going to be better than average. Certainly true for a bunch of female SWEs I have worked with reply IncreasePosts 9 hours agorootparentprevYou need to look at dropout rate...what if women are 50% of freshmen CS majors and only 20% of graduates? reply petesergeant 4 hours agorootparentI'd like to see that rate adjusted for people who were hobby programmers before they started. I suspect more boys than girls do programming before college, and that having done programming before college helps people not to drop out. I believe that the key to increasing diversity in tech is to increase the diversity in kids who are programming for fun. I have previously supported Black Girls Code for this reason. reply laurex 13 hours agorootparentprevPerhaps rather than simply looking at numbers for SWEs, we might also look at numbers for CEOs of successful companies? reply ThomPete 14 hours agoparentprevthere are no odds as a woman in the tech business. The tech industry is on of the most inclusive industries because it measures talent and value creation not features no one can do anything about reply 0xEF 52 minutes agorootparentITT: likely male HN users weighing in on how difficult or not difficult it is to be a woman in the tech industry. I'd love to hear thoughts on this take about just how inclusive the tech industry from women, or LGBTA or BiPoC individuals. reply AlecSchueler 1 hour agorootparentprev> The tech industry is on of the most inclusive industries The irony being that by saying this you're literally dismissing the voices and lived experiences of many many women in tech who would say otherwise. reply worik 13 hours agorootparentprev> tech industry is on of the most inclusive industries because it... Where have you been? That ignoring history reply DontchaKnowit 12 hours agorootparentnext [6 more] [flagged] worik 10 hours agorootparent> Every tech company on the planet is tripping over themselves to hire/promote more minorities That is because of the problems than have been mounting in tech because people only hired people that looked like them. I do not understand what the problem is with DEI. (I am not in the USA, perhaps I am missing out). Making sure that no one is left behind, because they were over looked, seems like a good thing to me. reply hackerlight 9 hours agorootparentThat's a real problem. People want to hire people that are just like them. Same school, same degree, same personality, same age, same political affiliation. I have seen the bias for the same race and gender, and not just white on white but with all races selecting their own, although that's on the down low. I don't know what the current MO of DEI programs is, but I hope they're expansive by also targeting ageism and other present day discriminations. reply px43 6 hours agorootparentTons of companies out there still do \"culture fit\" interviews because they are very intentionally trying to hire a mono-culture. reply userbinator 8 hours agorootparentprevWhen you start hiring people because of who they are, and not what they can do, it won't be long before that shows in the results you get. DEI has effectively replaced competence with identity politics. reply px43 6 hours agorootparentDEI is a counterweight to unconscious bias, which, as far as I've seen, is still way more of a factor in hiring than DEI. reply barrenko 13 hours agorootparentprevNo industry is inclusive, nor will ever be, that is almost by definition. reply cgh 8 hours agorootparentprevNot in the 1970s, which was when these events took place. reply PlunderBunny 6 hours agoprevIt's like a story from an alternate version of reality - I think of all the articles I've read touting 'the two Steves', and this is the first time I've read about Lore Harp McGovern. reply shortformblog 13 hours agoprevI just wanted to say that the framing of this intro is really, really good. Kudos to the author, who is knocking this series out of the park—hell of a writer. reply valley_guy_12 14 hours agoprevI remember seeing a Vector Graphics computer at a computer store around 1978, when I was shopping for my first computer. I was excited by the name Vector Graphics, only to be disappointed to learn that it was a meaningless name, and their computers had nothing to do with vectors or graphics. I vaguely remember that it was a generic business machine (maybe with a 16 bit version?) with nothing to recommend it to a hobbyist over the competition. In that era Apple had an enormous lead in graphics, software, and peripheral cards. reply zitterbewegung 15 hours agoprevInterestingly enough the empire fell when the Vector 4 suffered the same fate of Commodore (albeit later) when the Vector 4 specs were leaked. Although, there were a few blunders on the wikipedia page but this was also indicative of the era during the IBM PC / DOS dominance. [https://en.wikipedia.org/wiki/Vector_Graphic reply garius 15 hours agoparentYeah - one thing that didn't make the edit unfortunately was a few paragraphs on this. They'll make the book chapter though, when I write that. It was one reason I wanted to tackle Osborne first in the series - because Vector did, quite legitimately, Osborne Effect themselves with the 4. Which absolutely didn't help. reply nimfan 13 hours agoprev\"Vector was late in moving from machines with 8K processing to 16K, which had become the new industry standard.\" I was interested in S100 bus machines, but couldn't afford one! If I'd only known, I'd have borrowed to buy a Vector Graphic S100 back then, just for the novelty of having an 8192-bit CPU! ;-) reply mkesper 30 minutes agoparentThe CPU was a Z80, that must have been RAM. http://www.s100computers.com/Hardware%20Folder/Vector%20Grap... reply gary_0 17 hours agoprevThis story of Carole Ely and Lore Harp reminded me a little of the (fictional) women in Halt and Catch Fire. Fantastic show. I wonder if Vector Graphic was an inspiration for the writers. reply delichon 16 hours agoparentThis is further afield, but it reminded me of the novel A Town Like Alice by Nevil Shute. The protagonist is an Englishwoman who inherits a legacy and uses it to open businesses that employ the women of an Australian outpost. reply flockonus 16 hours agoparentprevI was trying to remember this exact show to comment on... fantastic show, exciting and fairly accurate (to fiction terms) depiction of rise of PC & internet. reply josephd79 16 hours agoparentprevIt’s a great show. reply nashashmi 14 hours agoparentprevI thought Lore looked like the actress in the show reply benjedwards 15 hours agoprevIf you enjoyed this, back in 2015 I wrote a feature about Lore Harp McGovern and her business partners for Fast Company that goes into the creation of Vector Graphic in detail: https://www.fastcompany.com/3047428/how-two-bored-1970s-hous... reply garius 15 hours agoparentReading that back in the day was one of the reasons she was on my list for this series to write about. Was the first time I'd heard of her! (Love your ongoing blog and output) reply le-mark 9 hours agoparentprevThat was great and really piqued my interest. I remember reading Byte magazine (late 70s and early 80s) and the pages and pages of adverts for machines similar to the Vector; peripherals, compilers, and softwarein the same work satation category. I’d love to read more about these products and their history. It was all very opaque. Even back then. reply HeyLaughingBoy 15 hours agoparentprevI knew this was familiar but I couldn't remember where I had read it before :-) reply kragen 7 hours agoprevarticle says > With her friend Carole Ely, she grew their company, Vector Graphic, into a major manufacturer of microcomputers wikipedia says > Vector Graphic sales peaked in 1982, by which time the company was publicly traded, at $36 million. It faltered soon after... https://en.wikipedia.org/wiki/Vector_Graphic taking a microcomputer company from nothing to a near-billion-dollar market cap on the public markets is nothing to sneeze at. on the other hand, tens of thousands of microcomputers per year doesn't qualify as 'a major manufacturer of microcomputers'. commodore sold three hundred thousand c64s in 01982. apple broke a billion dollars in sales that year. lore harp's company had almost 4% of that. you could reasonably describe mits, imsai, commodore, apple, atari, and tandy/radio shack as 'major manufacturers of microcomputers' in that time period, but not vector. they were small fry, like heath/zenith or cromemco this unforgivable level of puffery suggests that much of the article may be false (as valley_guy_12 points out in https://news.ycombinator.com/item?id=39972703, this puffery is something it has in common with the company's name, even if it doesn't quite rise to the level of 'intergalactic digital research') reply mycologos 6 hours agoparent\"unforgivable level of puffery\" seems like an overreaction. Apple's total sales in 1978 [1] were only 30% higher than Vector's in 1979 [2]. Yeah, the industry growth at the time means comparing even consecutive years gets dicey, but I don't think the gap between the two was enormous at that point. Comparable to Apple in the late 70s sounds pretty major to me. Also, it is reasonable to say \"major\" is an absolute description that just means \"pretty big\" not \"one of the biggest\". As you mention, their sales peak (IMO, past the company's relevance peak) is pretty big in absolute terms. Extrapolating this disagreement into \"much of the article may be false\" is ... confusing. [1] https://guides.loc.gov/this-month-in-business-history/april/... [2] http://www.s100computers.com/Hardware%20Folder/Vector%20Grap... reply kragen 6 hours agorootparenti'm not sure it would be accurate to describe apple as a major microcomputer manufacturer in 01978 reply KingOfCoders 17 hours agoprevAlso see “Steve\" Shirley, she build a company of coders, women only [0], from the '60s on with remote first :-) https://en.wikipedia.org/wiki/Steve_Shirley [0] She hired all the female IBM coders who couldn't make a career at IBM reply dang 15 hours agoparentRelated: A woman named \"Steve\" – IT pioneer, entrepreneur, philanthropist (2019) - https://news.ycombinator.com/item?id=39585527 - March 2024 (123 comments) All-female distributed-team software startup goes big in 1962 - https://news.ycombinator.com/item?id=6861666 - Dec 2013 (0 comments, but worth reading the article) A Woman's Place - https://news.ycombinator.com/item?id=5692271 - May 2013 (1 comment) reply rsynnott 17 hours agoparentprevWait, how did remote first work in the 60s? Did they post in punchcards? TTYs weren't really much of a thing at that stage, were they? reply Stratoscope 16 hours agorootparentI don't know what methodology Shirley's company used. But yes, Teletype machines were very common in the mid-1960s. For example, Tymshare, where I worked for several years, was founded in 1964. Their customers used Teletype machines at their own locations, dialing into a Tymshare mainframe and paying by the hour. There were a number of similar timesharing companies in that era. Call Computer and Dial Data come to mind, along with Transdata where I worked in Phoenix before moving to the Bay Area. https://en.wikipedia.org/wiki/Time-sharing https://en.wikipedia.org/wiki/Tymshare https://en.wikipedia.org/wiki/Teleprinter I had an office at Tymshare's Cupertino headquarters, and a Teletype at home to work remotely. This proved handy one year when the company was doing some final acceptance tests on the Xerox Data Systems (XDS) Sigma 7. The problem was that all of us preferred the competing DEC PDP-10. So the company really wanted those tests to fail. My manager called me into his office one day and said, \"This conversation is strictly between you and me. You are our best Sigma 7 expert [I'd worked on the similar Sigma 5 at Transdata] and even you like the PDP-10 more. But at this point the only way we can get out of the Xerox deal is if the acceptance tests fail.\" I took the hint, and the acceptance tests mysteriously started going haywire! Eventually I failed to cover my tracks well enough, and Xerox spotted my username in a core dump. Back to my manager's office. \"Xerox figured out what you were doing, and we had to tell them we would fire you. So, you're fired. But you still have your Teletype at home? And you have plenty of other work to do on the PDP-10, right? Can you work from home unofficially and keep track of your hours? Just stay away from the Sigma 7. After this all blows over, we will re-hire you and pay you that back pay.\" So I did, and they did! reply xyst 16 hours agorootparentBack when the word of your boss or manager actually meant something. Today, have to get that in writing otherwise risk getting hung out to dry in court or worse. reply zer00eyz 16 hours agorootparentNo, you just need a boss you can trust. You need to not be a clock puncher of an employee. You need to be present (in an office) for these conversations to happen. This sort of thing still goes on all the time. If your not part of it your either in \"Giant Corp\" or the wrong company, or you have the wrong boss, or you are the wrong person. reply flkiwi 12 hours agorootparentI’ve had (and honored) plenty of those types of conversations with subordinates I’ve never met in person in any of a number of global offices. Physical presence isn’t a prerequisite for being a good boss or worker. reply zer00eyz 7 hours agorootparentThis is true. It is much harder to get to that level of trust when you cant break bread, when you cant read all the body language. In person does make some things easier... One week a month of hot desking can do a lot for teams. reply tmpz22 15 hours agorootparentprevIm calling BS. Plenty of managers are in over their head. Plenty of managers are focused on their next career move. Plenty of managers will only play lip service to \"culture\" or worse \"family\" and after one slack DM from management completely fold over. Many managers see a slightly more difficult hiring environment (for themselves) and completely fold to secure their own position. EDIT: I've met many great managers, or at least individuals who seem great from the outside when the chips aren't on the table. But from the trenches I feel a real lack of leadership in Tech management in the current era. reply tim333 15 hours agorootparentprevI just looked up Wikipedia on teleprinters and had no idea they went back as far as 1887. My school had an ASR 33 Teletype linked to a PDP-10 in the 1970s which seemed kind of antique even then, although it worked ok. There's a youtube interview with Shirley showing someone remote working with some sort of computer like device. A terminal maybe? https://youtu.be/d5nzJ1rQBew?t=228 reply anotheruser13 9 hours agorootparentEven further than that. In 1978, we used a TI Silent 700 terminal connected to a PDP-11 so we could learn BASIC. reply xenospn 15 hours agorootparentprevWhat a great story! Hats off to your manager. reply dws 16 hours agorootparentprevCoding forms, accumulated until someone had access to a keypunch. Turnaround time could be days, which encouraged being very scrupulous when coding. reply EVa5I7bHFq9mnYK 15 hours agorootparentI remember our group of students would chip in on flowers and chocolates for the girl who was punching the cards. Every mistake meant manually cutting new holes and mask taping the extra holes to arrive at the correct character. reply sriram_sun 16 hours agorootparentprevI guess everything was \"remote\". My dad had to mail his code (punch cards) from (IIT) Madras to (IISc) Bangalore. He did say it was a pain though. * IISc - Indian Institute of Science * Madras, now Chennai was probably an overnight drive in the late 60s. reply 5555624 16 hours agorootparentprevWrite the program down on paper, then type it in or punch cards. Since I'm old, I remember writing FORTRAN -- it was all caps back then -- programs in my dorm room and then going down to the computer \"room\" and accessing the Dartmouth Time Sharing System to type it in and run it. reply abraae 15 hours agorootparentWe had a single apple II at our school. It was responsible for me failing most of my classes and getting into programming. Since there was only a single machine, mostly we had to submit our code on cards. Not quite punch cards with chads but an optical equivalent when you marked the hole with a sharpie. reply xyst 16 hours agorootparentprevI tried finding a source on the work environment at the time. But nothing describing the work setup. Might be hidden in some biography though. Speculation: maybe they mailed in their punch cards to main office. Or called it in over the phone. reply KingOfCoders 16 hours agorootparentI found something in German which has a little bit in it, but sadly interviewers where not tech managers and didn't ask the right questions in several interviews I've read. https://www.manager-magazin.de/hbm/eine-firma-ohne-bueros-a-... reply buovjaga 17 hours agoparentprevRecent discussion on HN: https://news.ycombinator.com/item?id=39585527 reply blacklion 16 hours agoparentprevUntil Act which (supposedly) was approved to help fight sexism stops this \"women only\". reply garius 15 hours agoparentprevRest assured Steve is very much on my long-list! If Every commission 'season two' of this series, then I'll likely focus on figures from the software side of Silicon Valley. Steve makes that list in a heartbeat. reply flockonus 16 hours agoprevHoping for an upcoming movie in the vein of BlackBerry (2023) reply lotsofpulp 15 hours agoparentIn case you didn't know, that movie was a lot of fiction. https://www.historyvshollywood.com/reelfaces/blackberry/ I assume all \"documentary\" or \"based on real events\" type media is completely fiction, unless specific events in the media are otherwise noted to be true. reply ugur2nd 14 hours agoprevIt's a fascinating story. I feel that most things are possible when I see stories like this. reply syngrog66 10 hours agoprevI've never heard of that microcomputer \"empire\" before and yet I know of lots of other computer businesses from its supposed era. reply worik 10 hours agoprevBackwards in high heels, indeed Behind every successful woman is a man, who tried to stop her: Not quite, but her husband could not cope with a wife better than him. Perhaps she was \"out of his league\"? reply thimkerbell 14 hours agoprevWhere is Lore Harp McGovern on Twitter? reply pxeger1 14 hours agoparentShe’s 80, so probably nowhere. reply tdeck 7 hours agoprevIn case anyone wondered, here you can see the manual for one of their machines' video cards (I am not sure where it fits in their range). http://www.s100computers.com/Hardware%20Manuals/Vector%20Gra... Sadly both the display technology and the graphics memory are raster. I was hoping it would be something like the Vectrex or the Imlac. reply Solvency 10 hours agoprevIsn't it kind of depressing that it's virtually and effectively impossible for anyone to replicate a comparable success story like this in 2024, short of maybe being a billionaire nepo baby? reply Atreiden 15 hours agoprevI think this paragraph says it all, really: > Although the IPO would represent a major achievement for both Harp and Ely, running Vector was taking a toll on their marriages. Ely’s husband had tolerated her role at Vector as an outlet for her boredom, but as she continued to invest more of her time in the company, their relationship began to deteriorate. For the Harps, cracks were also appearing. Bob had also initially seen Vector as a side project for Lore Harp, and his direct involvement created further problems when the two began to hold different opinions on how the company should be run. He was resentful of the level of attention she received in the press, feeling it diminished his own role at Vector. Their own husbands were resentful of their success. Despite being direct beneficiaries of it. Their egos literally could not take their wives overshadowing their own accomplishments. One can easily imagine how other men in power felt about their presence in the industry. When it comes to Steve Jobs, people sing his praises as a visionary and leader - someone who entirely changed the game, even though he was non-technical. His wikipedia page is 23,000 words, and is available in 161 languages. But a woman comes along in the same time period, building a company from nothing at the frontier of the very same industry, while offering unheard-of benefits and compensation for her employees, and she is cast as a villain (\"Ice Queen\") and relegated to a footnote in history. Her wikipedia page is a single paragraph that mostly mentions her relationship to her second husband. Her company, and it's legacy, ultimately destroyed by the men who overrode her decisions and opted to take the 'safer' route. A portrait of the insidious nature of sexism. reply masswerk 14 hours agoparentTo be fair, the fame of Steve Jobs is some of a different story: after his departure from Apple, he had been kind of a persona non grata, and later, with the demise of NeXT (BTW also known for its flat hierarchy and salary structure), not much talked about, either. It was really with the resurgence of Apple and Jobs' 3rd or 4th comeback that he became idolized, especially after the iPhone. This is quite a biography, and it took 30 years and rebuilding the then most valuable company from what seemed to be its sure ruin to achieve this popularity. reply yardie 13 hours agorootparentI was just a teen when Jobs becaome iCEO at Apple. It felt like a big deal and the only thing that could have made it bigger was if Woz stepped on stage as well. It really did have the air of the band getting back together. At least that was the case for the Apple faithful. reply masswerk 13 hours agorootparentOn the other hand, Jean-Louis Gassée was well remembered and there had been high expectations regarding an integration of BeOS, and even rumours of Sun maybe acquiring Apple. Compared to this, Jobs' return felt much like a \"small (village) solution\" with vague prospects to some. (Notably, this notion of \"small\" is somewhat ironical, given that Apple had been once one of the most successful startups in US corporate history, second only to Xerox.) reply devsda 14 hours agoparentprev> she is cast as a villain (\"Ice Queen\") and relegated to a footnote in history. The company failed for multiple reasons and some of those were a result of sexism. So, I wouldn't say it was the sole cause for their decline. > She told Harp that one man had complained to her about “the awful bitch who was running the company.\" While Jobs and to some extent Gates were called eccentric geniuses for all their misdeeds in their early years, I have no doubts on what Harp & Ely would be called if they attempted to do even half of the bad things Jobs and Gates did. reply jbellis 14 hours agoparentprevI couldn't tell you the names of the founders of contemporary PC builders like North Star or Cromemco, either. But that's just because none of them lasted more than a decade or so. Even the founders of Commodore, which was 10x more successful, are not household names. reply blashyrk 14 hours agoparentprev> Her company, and it's legacy, ultimately destroyed by men > A portrait of the insidious nature of sexism. A tad ironic to make these two statements in succession don't you think? I'm not saying that sexism doesn't or didn't exist (especially in that time period), but trying to dismiss the discrepancy on Wikipedia as sexism, when Jobs helped build a literal worldwide business empire that is Apple of today, doesn't help your case at all. In fact it's the opposite, it sounds like you're fighting windmills. reply Atreiden 14 hours agorootparent>A tad ironic to make these two statements in succession don't you think? Where's the irony? You'll have to point it out to me. > As a result, Harp McGovern had the opportunity to see, sooner than most other companies, what Microsoft was adding to its own operating system in an effort to capture the market. > It was a switch that Harp McGovern herself was inclined to make, so she contacted Gates and negotiated a provisional contract for Vector to pivot to using DOS instead of CP/M on far sweeter terms—and at a much faster pace—than were being offered to other manufacturers. “We had an amazing relationship with Microsoft. I’d signed a contract where every update and every new system in perpetuity we would get at no increased royalty,“ she explained. > The deal was taken to the board, but the collective decision was made that it was better to stick with the known quantity that was CP/M for the in-development Vector 4. She negotiated a sweetheart deal with Microsoft before their big break. She had a personal relationship with Bill Gates. This decision killed the company. > but trying to dismiss the discrepancy on Wikipedia as sexism, when Jobs helped build a literal worldwide business empire that is Apple of today, doesn't help your case at all. In fact it's the opposite, The final line was a summary of the article as a whole, not specifically the difference between Jobs' legacy and hers. I recognize the difference. reply blashyrk 14 hours agorootparent> Where's the irony? You'll have to point it out to me. > The deal was taken to the board, but the collective decision was made that it was better to stick with the known quantity that was CP/M for the in-development Vector 4. Because, if you find it relevant what the sex of the board members that made that mistake was, how is that any better than the alleged sexism that McGovern had endured? If you think that, you must also think that a board consisting mainly (or fully) of women that makes some mistake has to do with them being women, right? reply Atreiden 13 hours agorootparent> Because, if you find it relevant what the sex of the board members that made that mistake was, how is that any better than the alleged sexism that McGovern had endured? You've done some subtle editorializing here to try and make your point stronger, allow me to correct it: > ultimately destroyed by men is not what I wrote, what I wrote is > ultimately destroyed by the men who overrode her decisions and opted to take the 'safer' route. They convey two very different ideas. The strawman that you wrote implies that I believe men, by virtue of their sex, are responsible for the companies failure. This is not the case. What I wrote implies that the board rejected her proposal because they thought they know better. Is it conceivable to you that this belief might have had something to do with the fact that she was a female CEO, formerly a housewife, in an exclusively male industry? Surely you can concede that identifying sexist behavior and committing sexist behavior are not equivalent. reply 1234letshaveatw 13 hours agoparentprevIt's kind of gross and sophomoric to try and portray eg the Harp's relationship in terms of good and evil. I can't imagine that you would characterize their relationship in the same way if the roles were reversed, and what does direct beneficiary even mean in this context? Money was rolling in so suck it up? reply chasd00 13 hours agoparentprevbeing married to a person and your job is one marriage too many. Divorce and unhappy relationships are very common when one person becomes obsessed with work and the other is left in the cold. reply randomdata 6 hours agoparentprev> Their own husbands were resentful of their success. Despite being direct beneficiaries of it. What partner, male or female, isn't resentful of that kind of success even when benefitting from it? To get there requires a complete domination of time and, it turns out, partners tend to dislike being ignored. Since you mentioned Steve Jobs, there is much the same story about the mother of his first child. Their relationship is told to have come to an end because Jobs was putting all of his attention on Apple instead, and she also expressed feeling unacknowledged for her contributions to the Apple story. If you dig into the lives of any successful founder on that kind of level, it is likely you will find the same story over and over, regardless of gender. Nothing unusual here. reply shrubble 5 hours agoparentprevShe married a multi-billionaire, who she was in a relationship of some kind with, before she was ever divorced. Did you notice that in the article? BTW Bob Harp went on to found Corona Data Systems also. reply barrysteve 3 hours agoparentprevWomen are also resentful of their partner's success and their egos can not take it. The paragraph you quoted, Ely's husband sounds jealous of the attention she got and her control over the company. That can happen in any kind of partnership. You're dragging up sexism, where it is not the primary problem. Why? reply nashashmi 14 hours agoparentprevYou can go into a tirade of how women when achieving the same role as men are treated differently. But then you would be ignoring the underlying roles both were MEANT to play. The man is meant to play the role of breadwinner. If he can’t do that, he is not seen as worthy. The women is meant to play the role of housemaker. If she can’t do that, she is seen as incompetent. But if the other outshines the one at their meaningful role, it creates tension. It creates lack of confidence. It creates environments where the person feels small. “I need to take care of the kids and therefore can’t go to conference” for men is equivalent to “I need to go to the conference and therefore can’t stay home” for women. Both are negatives based on the role they play. If you want to create a new world where the roles are switched, or where both put equal time in doing both domestic and professional tasks, you would be ignoring their biological, physical, and mental strengths. The last bit that makes the whole issue worrisome, male social circles are competitive on achievements rather than perceptions, While female social circles are vice versa. There is a world where women can be successful and pioneering. It exists. But it doesn’t exist if there needs to be a tectonic shift. Like in the case here. And in the case of most normative systems where men and women play designated roles. reply nashashmi 12 hours agorootparentYou can go into a tirade of how women when achieving the same role as men are treated differently. But then you would be ignoring the underlying roles both were MEANT to play. The man is meant to play the role of breadwinner. If he can’t do that, he is not seen as worthy. The women is meant to play the role of housemaker. If she can’t do that, she is seen as incompetent. But if the other outshines the one at their meaningful role, it creates tension. It creates lack of confidence. It creates environments where the person feels small. “I need to take care of the kids and therefore can’t go to conference” for men is equivalent to “I need to go to the conference and therefore can’t stay home” for women. Both are negatives based on the role they play. If you want to create a new world where the roles are switched, or where both put equal time in doing both domestic and professional tasks, you would be ignoring their biological, physical, and mental strengths. The last bit that makes the whole issue worrisome, male social circles are competitive on achievements rather than perceptions, While female social circles are vice versa. There is a world where women can be successful and pioneering. It exists. But it doesn’t exist if there needs to be a tectonic shift. Like in the case here. And in the case of most normative systems where men and women play designated roles. reply cgh 8 hours agorootparentConsider spending less time online. reply nashashmi 7 hours agorootparentI don’t think that’s relevant here. reply flkiwi 12 hours agorootparentprevWhat? reply nashashmi 9 hours agorootparentNothing reply caycep 14 hours agoprev [–] Carol Ely eventually took over the CEO job at Sun from Scott McNally, I think? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Lore Harp McGovern founded Vector Graphic, a prosperous computer company, but later faced challenges leading to bankruptcy, overshadowed by male pioneers despite her tech industry and philanthropic contributions.",
      "The newsletter features uplifting tales like Gareth's and daily essays covering diverse subjects."
    ],
    "commentSummary": [
      "Women in the tech industry encounter challenges and discrimination, especially in male-dominated fields, such as technology.",
      "Success stories like Lore Harp McGovern's microcomputer empire are overshadowed by gender bias, evident in comparisons to male figures like Steve Jobs.",
      "Emphasizing the significance of diversity programs and combating bias in hiring and workplace culture is crucial for fostering more inclusive tech environments."
    ],
    "points": 455,
    "commentCount": 103,
    "retryCount": 0,
    "time": 1712591328
  },
  {
    "id": 39972990,
    "title": "After AI Triumph, Go Players Elevate Decision-making and Creativity",
    "originLink": "https://www.henrikkarlsson.xyz/p/go",
    "originBody": "Share this post After AI beat them, professional Go players got better and more creative www.henrikkarlsson.xyz Copy link Facebook Email Note Other Discover more from Escaping Flatland When my daughters aren’t hiding my notebooks, I write essays about stuff that makes me feel human Over 9,000 subscribers Subscribe Continue reading Sign in After AI beat them, professional Go players got better and more creative Henrik Karlsson Jan 23, 2024 110 Share this post After AI beat them, professional Go players got better and more creative www.henrikkarlsson.xyz Copy link Facebook Email Note Other 29 Share A game of the board game Go in Japan 1876 For many decades, it seemed professional Go players had reached a hard limit on how well it is possible to play. They were not getting better. Decision quality was largely plateaued from 1950 to the mid-2010s: Then, in May 2016, DeepMind demonstrated AlphaGo, an AI that could beat the best human Go players. This is how the humans reacted: Source. After a few years, the weakest professional players were better than the strongest players before AI. The strongest players pushed beyond what had been thought possible. Or were they cheating by using the AI? No.1 They really were getting better. And it wasn’t simply that they imitated the AI, in a mechanical way. They got more creative, too. There was an uptick in historically novel moves and sequences. Shin et al calculate about 40 percent of the improvement came from moves that could have been memorized by studying the AI. But moves that deviated from what the AI would do also improved, and these “human moves” accounted for 60 percent of the improvement. My guess is that AlphaGo’s success forced the humans to reevaluate certain moves and abandon weak heuristics. This let them see possibilities that had been missed before. Something is considered impossible. Then somebody does it. Soon it is standard. This is a common pattern. Until Roger Bannister ran the 4-minute mile, the best runners clustered just above 4 minutes for decades. A few months later Bannister was no longer the only runner to do a 4-minute mile. These days, high schoolers do it. The same story can be told about the French composer Pierre Boulez. His music was considered unplayable until recordings started circulating on YouTube and elsewhere. Now it is standard repertoire at concert houses. The recent development in Go suggests that superhuman AI systems can have this effect, too. They can prove something is possible and lift people up. This doesn’t mean that AI systems will not displace humans at many tasks, and it doesn’t mean that humans can always adapt to keep up with the systems—in fact, the human Go players are not keeping up. But the flourishing of creativity and skills tells us something about what might happen at the tail end of the human skill distribution when more AI systems come online. As humans learn from AIs, they might push through blockages that have kept them stalled and reach higher. Another interesting detail about the flourishing in Go, which is teased out in this paper by Shin, Kim, and Kim, is that the trend shift actually happened 18 months after AlphaGo. This coincides with the release of Leela Zero, an open source Go engine. Being open source Leela Zero allowed Go players to build tools, like Lizzie, that show the AI’s reasoning when picking moves. Also, by giving people direct access, it made it possible to do massive input learning2. This is likely what caused the machine-mediated unleash of human creativity. This is not the first time this kind of machine-mediated flourishing has happened. When DeepBlue beat the chess world champion Kasparov in 1997, it was assumed this would be a blow to human chess players. It wasn’t. Chess became more popular than ever. And the games did not become machine-like and predictable. Instead, top players like Magnus Carlsen became more inventive than ever. Our potential is greater than we realize. Even in highly competitive domains, like chess and GO, performance can be operating far below the limit of what is possible. Perhaps AI will give us a way to push through these limits in more domains. Warmly Henrik Subscribe Acknowledgements Several of the points here build on comments made on a Twitter thread I made about this yesterday. Nabeel S. Qureshi (Twitter, blog) read a draft and gave useful pointers. Notes on energy and intelligence becoming cheaper Henrik Karlsson · November 15, 2023 In 2015, I amused myself by training a neural network to generate poems in the style of various poets I knew and submitted the results to a fanzine. The thing I built was a primitive language model and—though I thought it was fascinating, seeing a computer talk—it did not occur to me that it could be useful for much beyond pranks. I would never have gue… Read full story 1 The data in the graph which shows the improvement is from Games of Go on Disk, a project that transcribes games at professional Go tournaments. These games happen in person and have precautions against cheating. There was a recent incident in Chinese Chess when Yan Chenglong, last year's winner of the Chinese tournaments, was accused of cheating by using an anal bead that let him send information to a computer by squeezing, and receiving moves sent back as a code of vibrations—so who knows. But cheating doesn’t seem common enough to explain the trend. 2 It is Shin, Kim and Kim who claim Leela Zero helped because, unlike AlphaGo, it showed the reasoning behind the move, not just the move. This is interesting in light of cognitive apprenticeship theory which posits that the reason people have a hard time learning cognitive skills, like literacy or Go, is that our learning is adapted for imitation and apprenticeship-like situations, and this works poorly for cognitive skills which happen hidden in the head. By opening up the box, so that the thought process can be observed, like Lizzie does, you allow people to apprentice themselves to the cognition, not just the actions. I am not sure I believe this explanation! When I look at subreddits for Go players who use Lizzie, my impression is that they don’t look at the reasoning all that much. They use it mainly to pinpoint moves where the winrate suddenly drops, so they can focus their learning on their biggest mistakes. The true explanation why open source helped might actually be the inverse of what Shin, Kim and Kim propose. It might that the reason open source helped was that it let people do massive input learning—simply flooding themselves with data on how the AI plays—and bypassing reasoning all together. It could be that reasoning was holding people back before. Human moves tend to follow heuristics that are explainable and simplify things so people can do the computations in their heads. The AIs don’t care about these heuristics and explanations and so can play cleaner. In chess parlance, the AI is more “concrete”—reasoning based on specific variations rather than on general principles. Doing massive input training on this kind of concrete play, bypassing heuristics and explanations, might be the why of the improved decision quality. In chess, the new batch of young grandmasters in chess got there largely by playing 10+ hours a day of online speed chess instead of the older strategies that emphasized targeting learning, deliberate practice and slower exercises. This is another example of a shift toward massive input, pushing beyond heuristics to pure pattern matching, and it was, like the shift in Go, facilitated by AI engines. Subscribe to Escaping Flatland By Henrik Karlsson · Hundreds of paid subscribers When my daughters aren’t hiding my notebooks, I write essays about stuff that makes me feel human Subscribe Error 110 Share this post After AI beat them, professional Go players got better and more creative www.henrikkarlsson.xyz Copy link Facebook Email Note Other 29 Share",
    "commentLink": "https://news.ycombinator.com/item?id=39972990",
    "commentBody": "After AI beat them, professional Go players got better and more creative (henrikkarlsson.xyz)360 points by iNic 14 hours agohidepastfavorite183 comments dtnewman 13 hours agoJust look to Chess. The top players today are way better than any of the greats before, because they can train against computers and know exactly where they failed. That said, because they've gotten so good, chess at the top levels is pretty boring... it's hard to come up with a unique strategy so players tend to be defensive. Lots of ties. On the other hand, chess is more popular than ever. It's huge in high schools. I see people playing it everywhere. I know that for me, I love being able to play a game and then view the computer analysis afterwards and see exactly what I did wrong (granted, sometimes a move can be good for a computer who will know how to follow through on the next 10 moves, but not necessarily good for me... but most of the time I can see where I made a mistake when the computer points it out). Side note: I play on LIChess and it's great. Is there an equivalent app for Go? reply hibikir 12 hours agoparentThe defensiveness has absolutely nothing to do with better computers and the improvements in play that came with it, but with tournaments where risk taking is an economic disaster. As others have said, there aren't massive numbers of ties in the candidates tournament, because the difference in value between being first and second is so massive that if you aren't first, you are last. Compare this to regular high level chess in the Grand Chess Tour: It's where most of your money is going to come from if you are a top player. Invitation to the tour as a regular is by rating, and there's enough money at the bottom of the tour than the difference between qualifying or not is massive. Therefore, the most important thing is to stay in the tour train. Lose 20 points of rating, and barring Rex Sinquefield deciding to sponsor your life out of the goodness of his heart, you might as well spend time coaching, because there are so few tournaments where there's a lot of money. This also shows in the big difficulties for youngsters that reach 2650 or so: They are only going to find good enough opponents to move up quickly in a handful of events a year where people with higher rating end up risking their rating against them. See how something like the US championship is a big risk for the top US professionals, because all the young players that show are at least 50 points underrated, if not more. This is what causes draws, not computer prep. Anand was better at just drawing every game in every tournament back when he was still on the tour, and yet computers were far worse than today, especially with opening theory. reply jimbokun 5 hours agorootparentSo they need to mandate a promotion and relegation system for the top levels. Force players in the top flight to beat at least some of their opponents, or get replaced by top players in the next lower tiers. I think that would increase spectator interest even more. In football, relegation battles can be almost as compelling as the title race.. reply doetoe 5 hours agorootparentAnother thing that was done in football, and could be done in chess as well to reduce the number of draws, was to grant 1 point for a draw, but 3 points for a win, up from 2 points in the 90s (earlier in England) reply ummonk 12 hours agorootparentprevAnd it simply doesn’t have to be this way. The top tournaments could just use a prior qualification tournament with an open Swiss. Then invite the top finishers from the open Swiss to participate in the round robin. Can reserve an invitational wildcard or two but the rest should have to earn their place. reply santoshalper 11 hours agorootparentprevVery insightful! reply fryz 13 hours agoparentprevFWIW, I find the classical chess tournaments with the super GMs to be fairly interesting, if only because the focus of the games is more about the metagame than about the game itself. The article linked at the bottom of the source is a WSJ piece about how Magnus beats the best players because of the \"human element\". A lot about the games today are about opening preparation, where the goal is to out-prepare and surprise your opponent by studying opening lines and esoteric responses (somewhere computer play has drastically opened up new fields). Similarly, during the middle/end-games, the best players will try to force uncomfortable decisions on their opponents, knowing what positions their opponents tend to not prefer. For example, in the candidates game round 1, Fabiano took Hikari into a position that had very little in the way of aggressive counter-play, effectively taking away a big advantage that Hikaru would otherwise have had. Watching these games feels somewhat akin to watching generals develop strategies trying to out maneuver their counterparts on the other side, taking into consideration their strengths and weaknesses as much as the tactics/deployment of troops/etc. reply akkartik 13 hours agorootparentOn the other hand, a game like Praggnanandhaa vs Vidit 2 days ago feels like Russian roulette. https://www.chess.com/news/view/2024-fide-candidates-tournam... Mistakes on both sides, including the side that presumably prepared this line with help from computers. reply non-chalad 7 hours agorootparentprevHave they considered metagaming like pirates fight in The Secret of Monkey Island (1)? 1. https://www.youtube.com/watch?v=7a4HF3dIcuo reply Taek 13 hours agoparentprevI think you would see fewer ties if players got 0.2 points each for draws instead of 0.5 points each for draws. It makes the risk of going for a risky strategy lower (you only drop 0.2 pts instead of 0.5 vs getting an easy draw) and it makes the rewards much greater... a single win and 4 losses scores the same as 5 draws. you wont see players doing intentional draws anymore either reply dfan 12 hours agorootparentOne issue with this is that it encourages collusion. If you're a top GM playing someone of equal skill, it's +EV to agree to flip a coin beforehand to determine who will win (and then play a fake game) rather than playing it for real. Some chess tournaments have experimented with giving 1/3 point for draws instead of 1/2 and it didn't really change much. Mostly it acted as a tiebreaker, which you could have done by just using \"most wins\" as a tiebreaker anyway. My favorite idea (not mine) for creating decisive results in chess is that when a draw is agreed, you switch sides and start a new game, but don't reset the clocks. reply paulddraper 5 hours agorootparentBut most tournaments don't have players playing each other an even number of times. Any sport can have a thrown match. A la Rocky. reply BurningFrog 12 hours agorootparentprevFootball (Soccer) did something similar. Before that is was 2 points for a win, 1 point each for a draw. In 1981 they made it 3 points for a win, and the sport has had substantially more offensive play since. reply neysofu 11 hours agorootparentprevAnother possible solution would be to simply... remove draws from the game. Instead of checkmating the goal becomes to capture the opponent's king. Needless to say, no one likes this idea because it throws out of the window centuries of game theory. Endgames would be completely different. I'm not convinced it would be a less interesting game, though. reply wh0knows 11 hours agorootparentIf you have insufficient material how can you capture the king? Checkmate is by definition one move before forced capture of the king, the game doesn’t change by making it end one move later. reply btilly 11 hours agorootparentprevAnd what happens if you wind up with king and rook vs king and rook? Some positions simply do not allow for a win. Yes, you could say do it on time. But then it becomes about mechanical dexterity as people try to be faster than their opponent in a pointless piece shuffle. reply neysofu 11 hours agorootparentYeah, I didn't think it through. I'd imagine such a rule change would still make draws significantly less likely though, right? reply BurningFrog 11 hours agorootparentprevThere are many situations when this is for all practical purposes impossible. For example a King vs King endgame. Even really weak players will never accidentally put their king next to the opponent. reply kgabis 1 hour agorootparentWell... https://www.youtube.com/watch?v=bTS9XaoQ6mg reply debugnik 9 hours agorootparentprevEven worse, it's an illegal move to leave your own king in check, if I recall correctly, so that simply can't happen, not even by accident. The only possible outcome for king vs king is a draw. Unless we were to modify even more rules, of course. reply acchow 11 hours agorootparentprev> Instead of checkmating the goal becomes to capture the opponent's king. These are the same. reply neysofu 11 hours agorootparentThey are not – if the goal becomes to capture the king, and check-related rules are removed from the game, stalemates become impossible. reply dudinax 10 hours agorootparentIm not sure stalemate accounts for most draws at the highest level. Getting rid of check does make for a better game at beginner levels. It's both easier to teach and leads to exciting finishes ss noobs hang their king. reply doctor_phil 3 hours agorootparentThere are a lot of endgames that are drawn because of stalemate though. Many pawn endgames ( e.g. pawn and king against lone king) are drawn because of stalemate, but would be a win in most cases if stalemate didn't exist. reply eterm 2 hours agorootparentYou likely didn't mean to imply all king+pawn vs king endgames are drawn, but to clarify for the layman reading, many are winnable. It depends on the locations of the kings relative to the pawn ( you generally want it in-front of your own pawn ), and the concept of opposition. reply charlysl 3 hours agorootparentprevTrying to force or avoid stalemate is a huge motive in top level endgames though regardless of whether they actually end in stalemate or not. reply whimsicalism 11 hours agorootparentprevat least as described, would not be sufficient to remove draws from the game - but would remove stalemates reply paulddraper 5 hours agorootparentprevCapturing the king changes nothing, except stalemate. (Which affects some draws but not most.) reply whimsicalism 12 hours agorootparentprevthink it is somewhat intrinsic to chess that it makes sense to go for ties as black in top tier play reply zer0-c00l 12 hours agoparentprevhttps://online-go.com/ is the easiest place to get started as a western beginner. The far more active go servers are Asian and have a higher barrier to entry in terms of registration, downloading the client, and dealing with poor localization. (Fox Weiqi, Tygem, etc.) reply dunefox 1 hour agorootparentOGS is great. There's an android app as well. reply Buttons840 3 hours agoparentprevOne nice thing about Go is there are no ties. This is offset by how boring the end games are though and having to count. Chess has explosive and exciting endings, Go just kind of fizzles out at some point. reply ricefield 3 hours agoparentprevOGS is the closest thing I’ve found to lichess but it’s quite good! https://online-go.com/ reply pushedx 12 hours agoparentprevKGS is where I used to play, this is the homepage: https://www.gokgs.com/ and this is the web client: https://shin.gokgs.com/ The homepage hasn't had a redesign since at the latest 2007, but the community is great an there are top players on there. reply thatswrong0 13 hours agoparentprevI wish Chess960 was more popular for this exact reason. It’s super fun to watch and play compared to normal Chess… basically all I do with my friends reply Bootvis 13 hours agoparentprevSecond round of the Candidates tournament played Friday had 4/4 decisive games[1]. In general, a tie might be the most common result but even at the highest level there tend to be chances for both sides. [1]: https://lichess.org/broadcast/fide-candidates-2024--open/rou... reply mtlmtlmtlmtl 12 hours agorootparentIt's really up to the players. SuperGMs these days are somewhat addicted to draws because it's a very safe result in a tournament setting and in terms of rating. Therefore these players tend to favour less risky and more calculable openings. They care more about avoiding a loss than they do about winning. The idea that the large amount of draws is because players are so strong now, is mostly a myth. It's really just psychology and game theory at work. For a perfect illustration of all my points, look at Aronian vs Grischuk from the 2018 candidates tournament. Here both players chose to play into complications, and the resulting game was wildly complex, with both players making several suboptimal moves simply because the position was just too complex even for two of the strongest calculators in the game at the time. And in the end, they still ended up constructing a draw by repetition when all 3 results were still possible. Both players had good winning chances, yet the fear of losing finally overtook them and they collectively bailed out of the game. It's not that players are now so strong it's almost impossible to win, the players just aren't as willing to seek out the necessary positions. reply timetraveller26 13 hours agoparentprevDon't know about go, but Lishogi is Lichess for shogi (Japanese chess) reply jsheard 13 hours agoparentprev> Just look to Chess. The top players today are way better than any of the greats before, because they can train against computers and know exactly where they failed. AlphaGo isn't available for anyone to train against like Stockfish is though, what are Go players using? Has another powerful Go engine been developed since then? reply espadrine 13 hours agorootparentKataGo is an open-source algorithm derived from AlphaGo, but with a number of tricks so that it trained faster: https://katagotraining.org/ It likely surpasses AlphaGo, and just like Stockfish, it delivers a protocol that can hook into many user interface apps: https://github.com/lightvector/KataGo?tab=readme-ov-file#gui... From those technologies, also came an interesting visualisation of how human players changed their habits following AlphaGo: https://drive.google.com/file/d/16-ntvk3D1_pgjJ7u64t4jMYMh0z... reply mafuy 1 hour agorootparent> It likely surpasses AlphaGo If I remember correctly, a recent estimation is around +1000 Elo compared to AlphaGo. reply SonOfLilit 13 hours agorootparentprevWe use KataGo and sometimes LeelaZero (which is a replication of the AlphZero paper). KataGo was trained with more knowledge of the game (feature engineering and loss engineering), so it trained faster. It was also trained on different board sizes and to play to get a good result when it's already behind or ahead. KaTrain is a good frontend. reply kadoban 12 hours agorootparent> KataGo was trained with more knowledge of the game (feature engineering and loss engineering), so it trained faster. Not really important to your point, but it's not really just that it uses more game knowledge. Mostly it's that a small but dedicated community (especially lightvector) worked hard to build on what AlphaGo and LeelaZero did. Lightvector is a genius and put a lot of effort into KataGo. It wasn't just add some game knowledge and that's it. https://github.com/lightvector/KataGo?tab=readme-ov-file#tra... has a bunch of info if you're interested. reply SonOfLilit 12 hours agorootparentI wasn't at all trying to say his work was simple. I was trying to say \"deepmind were trying to build an AI that gets good at games without anything in their structures being specialized for the game, lightvector asked what if we did specialize the model on Go\". And he did some wonderfully clever things. reply baby 7 hours agoparentprevhttps://www.gokgs.com/ reply Angostura 13 hours agoparentprev> The Queens Gambit turned quite a few of my daughter's friends on to chess reply thangalin 10 hours agoparentprevhttps://katagui.baduk.club/ reply yeellow 12 hours agoparentprevI recommend goQuest (mobile app), and playing 9x9 go. I used to play on KGS, but it is less crowded now (the problem is that there are too many servers: OGS, IGS, Tygem, Wbadul, etc and no one dominates, therefore you wait for the game, you need a rating, etc. Most are not very modern, mobile unfriendly, etc.). Also 19x19 takes too much time for me when comparing to chess, 9x9 is perfect, and goQuest has many active players, after a few seconds you get a match (they offer 13x13 and 19x19, but those are less active I suppose). reply tptacek 13 hours agoparentprevonline-go.com reply anononaut 13 hours agorootparentOGS is definitely the best server in the West. Deserves all patronage it gets and more. I wish the AGA was more supportive of it rather than KGS. reply naet 12 hours agorootparentKGS is still pretty great. OGS might be more accessible to new players with one click sign in and a better web app, but I think KGS has a higher population of true dan+ strength players, and has a stronger \"culture\" around community reviews and studying. It used to be even better, but there are less total people playing on KGS than the previous peak. reply tptacek 12 hours agorootparentI found the culture on OGS, particularly wrt moderation, to be pretty great (as a newcomer, and I exclusively play 9x9). I've read about KGS but I've never figured out how to engage with it. (I know it to be the OG, srs bzns venue, though). reply slowmovintarget 10 hours agorootparentFor KGS you had to bring your own client, like cgoban, if memory serves. They have a web client now, but the Java client was what I played on for while. reply Alex3917 12 hours agorootparentprev> I wish the AGA was more supportive of it rather than KGS. Out of curiosity, why do you like OGS more? I find the UX of KGS to be way more intuitive. reply Pet_Ant 12 hours agoparentprev> That said, because they've gotten so good, chess at the top levels is pretty boring Yeah, I feel the same thing about Magic formats when the pros play. When a format is new and people are discovering, and they have to rely on their gut, and make educated guesses. That's when it's fun to play and watch. reply bobogei81123 10 hours agoprevBack when I was a kid learning go, I was taught that the kick joseki (a standard sequence of moves, similar to chess opening) [1] is a bad move, and you were considered trolling (and the teacher would not be pleased) if you played a 3-3 invasion [2] during the opening phase. These are all vindicated thanks to the AI and played pretty commonly nowadays. AI definitely helped eliminate many dogma and myths in go. [1] https://senseis.xmp.net/?44PointLowApproach#toc6 [2] https://senseis.xmp.net/?33PointInvasion#toc2 reply falserum 4 hours agoparent3-3 invasion takes teritory at the expense of influence (future potential). I think I improved a lot when I stopped 3-3ing (it opened up different style of game for me) Noobies love 3-3 (I definitely did), because it’s kind of simple and familiar move. (Especially at the start of the game, when board is empty, there is gadzilion of possibilities and most of them unknown and possibly risky) If not discouraging 3-3, I would still recommend starting without it, to learn that way of play (if for nothing else, to deal with 3-3 invasions) reply chewxy 5 hours agoparentprevyou have to admit that 3-3 invasion is pretty annoying to handle. AI is way too aggro and people are learning to be as aggressive. reply akira2501 13 hours agoprevThe other possibility is that it destroyed the incidental dogma that tends to build up in these types of games and human activities. This is why I like the \"hacker ethos\" as much as I do, it tends to eschew things like \"accepted\" dogma in order to find additional performance that other people were just leaving on the table out of polite comfort. reply JustLurking2022 13 hours agoparentThe dogma generally becomes accepted because it outperforms other known strategies. In a game like Go, that could previously take a while because there are so many possible follow-ups that it takes time to accumulate enough data on whether a new strategy is actually decisively better, or just worse but over-performing because it's less known. There's a big difference between those two and \"the hacker ethos\" will lead to a lot of the latter. However, now computers can simulate enough games to give a relatively high degree of confidence that a variation in strategy is truly better. reply Izkata 12 hours agorootparentI don't know how it's developed since, but from what I remember that was how it started - the AIs weren't following the standard moves (joseki) that we'd built up over centuries and human players were thrown off by the nonstandard responses that were working better than expected. reply foota 9 hours agorootparentI wonder if AI could be built to continually adapt, so that instead of playing an optimal strategy, it instead chooses between various suboptimal strategies. If humans train to play against the optimal strategy, then maybe the AI could do better by playing in suboptimally but less expected ways. reply mafuy 1 hour agorootparentThis is already happening. The point differences for sometimes huge deviations are minuscule, so it's worthwhile to have in one's repertoire. The same is mostly true for purely human games, too: These are trick moves. reply coef2 12 hours agoparentprevSo the progress of human proficiency in Go and our collective advancement over time is hindered by dogmatic rules introduced over time. These rules predispose players toward specific strategies and consequently limit the scope of our creative potential within the game. In contrast, AI algorithms operate without such biases offer a unique advantage in overcoming these limitations. They essentially inspire us to get out of established patterns (or local minima) of play and broaden the range of our strategic moves. reply lordnacho 13 hours agoprevThis is the tip of the iceberg, right? It's foreshadowing AI helping experts become better. I can see it happening in a lot of creative fields, including software. Perhaps this is where it really pulls the experts from the juniors, because only experts will be able to judge whether the AI has helped him create something actually good. reply asolove 13 hours agoparentGo is a constructed game with a precise definition of the rules and victory. The real challenge with AI helping experts is whether it can correctly help them balance their own value function for what \"better\" means. And whether we can still train human experts who can think about that independently with good judgement, if we've automated away the things that beginners would normally do to train their judgement with black boxes that they can't interrogate. Will be interesting for sure. reply LouisSayers 7 hours agoparentprevI'd say GPT4 has definitely helped me become a better programmer - I'm able to ask it questions, learn how I can refactor my code better, or approach a problem in a way I might not have considered. It does hit its limits, but it's been so useful - it's a funny cycle of training AI and having it train us, a great symbiotic relationship. reply jasonfarnon 10 hours agoparentprevI can't think of any reason we should be so lucky that AI will have a ceiling somewhere between human \"juniors\" and human \"experts\" reply Arrath 9 hours agoparentprevMaybe I'm too small minded but I would love to see AI like this enhance...well, the AI in games in general. I long for the day where I no longer play Civilization or an RTS against AI that has perfect knowledge, or is given handicaps to allow it to be competitive. reply sandspar 5 hours agorootparentFlight simulators are set to benefit from AI. Imagine talking to an AI Air Traffic Control that understands natural language. Imagine walking down the aisle of your plane and overhearing AI people's conversations. reply gwern 7 hours agoparentprev> It's foreshadowing AI helping experts become better. The humans are still way worse than the Go programs. People are still willing to pay them to play a game as entertainment. Are lots of people willing to pay you to do whatever it is you do even when AIs do it much better, out of sheer entertainment value & sentimentality? If they are willing to pay you in particular, how many other people like you are they also willing to pay for, and is that number much greater than or much less than the current number paid to do it? reply nottorp 4 hours agorootparentPeople are also cheering for Usain Bolt or whoever is the speediest runner this year, in spite of being able to outrun him by simply getting into a car... reply sandspar 3 hours agorootparentI think about this at the gym. I feel very proud of myself when I can go from being 1% as strong as a front-end loader to being 1.1% as strong. The king of my gym, the most high status guy there, is up to maybe 3%. reply gcanko 13 hours agoparentprevIt's exactly like the invention of agriculture. Not having to hunt for food gave more opportunities for intellectual pursuits because of having more free time. reply rwbt 13 hours agorootparentI'm skeptical of this argument. It gave free time to some people i.e. the landed gentry but also created the toiling peasants and a hierarchical civilization. reply choilive 13 hours agorootparentMany other types of governance was enabled by the agricultural revolution, not just feudalism. reply ohyes 12 hours agorootparentprevToiling peasants had more free time than we do today. reply TulliusCicero 10 hours agorootparentAs I understand it, this is largely inaccurate. People just read \"days off\" as \"more time\", even though peasant farmers would need to engage in a lot of labor around the farm or household even on \"days off\" (your cows and chickens don't care that you're on vacation). Of course people still do some chores today even on days off, but it's a lot less than you need to do on a farm, ask basically any farmer. reply ummonk 11 hours agorootparentprevThe amount of toiling they could do without dying was calorically limited. Having lethargy induced by a shortage of food doesn’t necessarily mean a preferable lifestyle. reply rwbt 12 hours agorootparentprevHunter gatherers had more leisure than farming peasants. Surely, one can spot the trend. reply TulliusCicero 12 hours agorootparentprev[citation needed] reply nicklecompte 10 hours agorootparentprev\"Who's going to teach those idle medieval peasants to read, Ben? Augustine-man?!\" reply brailsafe 11 hours agorootparentprevPfft, disagree. Got laid off a year ago and have had nearly 100% free time since reply rcxdude 1 hour agorootparentprevNo, it just meant you could support more people on the same amount of land. Non-mechanised agriculture is very labour-intensive compared to hunting. reply tptacek 13 hours agoprevWhen you read Go strategy resources, you see a lot of things divided into what best practices were before AlphaGo and what they are now. It's a whole big thing. It is still the case, though, that AI dominates humans at Go; humans didn't get so creative about the game that they put AI back on its toes (though some did discover exploitable AI \"strategy bugs\"). reply nicklecompte 10 hours agoparentThe \"strategy bugs\" are a symptom of a more general shortcoming and why 2024 AI is still basically dumber than a mouse. Keel in mind that if you had a variation of Go where there was a \"hole\" in the middle of the board, both Lee Sedol and a competent amateur would be able to play competent \"Doughnut Go\" without any prior experience. But AlphaGo and its successors would certainly make a ton of dumb unforced errors unless it practiced at least a few hundred games. (I am basing this observation on similar experiments with a similar Breakout AI, not sure if these experiments have been done with Go.) Mammals, including humans, have advanced brains because we evolved to solve weird and unexpected problems with moderate reliability, not to optimize well-known benchmarks with high reliability. (This is also why plants are green instead of black.) By contrast, AlphaGo is a machine designed to solve a highly specific problem. The whole point of machines is that they dominate humans at specific tasks, otherwise we would just use a human. But we don't describe bulldozers as \"superhuman\" unless we're being intentionally obscure; the same should apply to AI. Otherwise we risk assuming the AI is capable of things it probably can't do without retraining. reply sandspar 2 hours agorootparent\"General-purpose\" computer seems like a misnomer. reply pa7ch 13 hours agoparentprevAgreed, but I still think humans should get a little more credit for winning against AI no matter how. Its a competitive game with very simple and clear rules. A hole in AI strategy is a hole, even if quickly patched! I am still so impressed that Lee Sedol beat Alpha Go 1 game out of 5 way back when AI made its breakout. I was sad he felt so sheepish afterward for losing. In hindsight, I think it was an amazing accomplishment even if today an AI could beat Shin Jin-Seo (#1 player) 100 out of 100 times! reply Alex3917 11 hours agoparentprev> When you read Go strategy resources, you see a lot of things divided into what best practices were before AlphaGo and what they are now. It's a whole big thing. Yes and no. The biggest takeaway from AI is that learning all the joseki doesn't actually matter that much, which has freed up players (except for the pros) to spend more of their time focusing on the more fun and interesting parts of the game. There are a lot of videos showing what josekis and strategies the AI recommends, but as a human you're likely not going to be any better off following them. This is for the same reason why AI analysis of fights is largely useless. That is, the reason why you lost the big fight (and the game) isn't that you didn't find that one obscure 9P move that could have saved you, but rather than you let yourself get cut 50 moves earlier. But the AI will never show you the move where you got cut as being the reason why you lost the game, it will only show you the one random move that you'd never in a million years actually be able to find. This video from Shygost sums up the most important strategy stuff that you actually need to know in order to get strong: https://www.youtube.com/watch?v=ig8cWuDSHTg reply SonOfLilit 13 hours agoparentprevYou also see a similar division to 19th century and 20th century when a player called Go Seigen changed the way the game is player even more, I feel, than AI did (but don't take my word, at 7kyu I'm far from qualified to understand how professionals play) reply paulcole 13 hours agoparentprevThis is true in Scrabble as well. When I was playing seriously there were strong players who played a ton over a board and had deep intuition about what made plays good and what made plays bad. In the late 1990s/ early 2000s there started to be a lot more in the way of computer simulation and analysis and some very strong computer players. One (general) example was that older players liked the idea of making longer plays using more tiles to \"win\" a race to the S and blank tiles (the best tiles in the bag). Computer simulations generally show that turnover (as this is called) isn't optimal and you're better off holding strong combinations of letters rather than playing them off hoping to draw something better. Now younger players are better than ever because all of their training came with the help of computer analysis and simulation. Of course in Scrabble a huge part of it comes down to just memorizing the words in the dictionary. reply cdelsolar 12 hours agorootparentAI doesn't dominate people in Scrabble though. The best humans are better than the best AI. reply samatman 12 hours agorootparentI wouldn't have expected that. Is it just a relative lack of interest in building an AI which can dominate Scrabble? It's a partial-information game, but the search space can't be as big as Go, and an AI has an advantage over human players in that the entire valid string family can be encoded into a trie or some other efficient data structure, it's never going to forget a word or think it can play one that isn't valid. My intuition is that AI should be able to crush the best human players at this point in time, but I'm open to being corrected on that if there's some aspect of the game which I'm not modeling correctly. reply ultrasaurus 12 hours agorootparentThat strikes me as odd too -- but it might be because searching a dictionary is such an obvious computer advantage that it's not interesting to optimize. There are only 10 articles on arxiv.org that mention Scrabble vs 100s on Chess https://arxiv.org/search/?query=scrabble&searchtype=all&sour... reply paulcole 11 hours agorootparentJust finding the highest scoring word won’t make you all that good. If you played the highest scoring play available to you each turn you wouldn’t be that strong of a player. Maybe around top 200-500 or so in the US I would guess? And it’d be a super exploitable strategy by a decent player. The reason is that you need to apply some rules, like when to trade vs. making a play, balancing consonants and vowels for future plays, what parts of the board are too dangerous to make certain plays in, etc. It’s because of the distribution of unplayed tiles, the high-scoring spots on the board, and the 50-point bonus for using all of your tiles. Because of that, generally, you’ll do better by building towards a 50-point bonus play every 3 or 4 turns than by maxing your score on each turn. I’d be curious about letting a human player play with the assistance of the best bot available and seeing how much better that would make them. I guess part of the issue though is that in a 13 play game maybe 3 plays are meaningfully difficult. So it’d take awhile to see if the human is improving on the bot or not. reply paulcole 12 hours agorootparentprevI’ll admit I’m out of the loop but how many people in the world today do you think can beat BestBot over a significantly long series? Do you think there’s going to be a bot that dominates people in series like that? I’ve been following Mack Meller’s YT channel and he’s getting beaten pretty handily in his series. I’d put the over/under on people playing today who would beat BestBot in a 100-game series at say 3.5. What side would you take? reply f_allwein 12 hours agoprev„Winning does not tempt that man. This is how he grows: by being defeated, decisively, by constantly greater beings“ Rilke, „The Beholder“ https://martyrion.blogspot.com/2009/11/man-watching-der-scha... reply ummonk 12 hours agoprevThe article is misleading regarding the history of chess. Magnus excepted, most top players did adopt a more cold and calculating material-focused chess style that mimicked Deep Blue and subsequent chess computers. It was only with the success of AlphaGo and LC0 that top chess players have started playing a more creative playstyle again, playing various wing pawn advances, as well as being more willing to give up material for nebulous initiative or positional advantages. reply kccqzy 13 hours agoprev> Shin et al calculate about 40 percent of the improvement came from moves that could have been memorized by studying the AI. But moves that deviated from what the AI would do also improved, and these “human moves” accounted for 60 percent of the improvement. I don't often play Go myself but a number of my friends do. Among non-professional players, it is really common to see game play being not as exciting as before because there's now an easy way: just memorize and copy what the AI does. I don't doubt that professional players still have a ton of creativity, but a lot of non-pros don't really have too much creativity and the whole game becomes memorizing and replicating AI moves. reply csa 12 hours agoparent> Among non-professional players, it is really common to see game play being not as exciting as before because there's now an easy way: just memorize and copy what the AI does This is just… not true. Unless one is playing at high dan ranks, it’s trivially easy to induce a “memorized sequence” that your opponent either will not have memorized or will leave them with a situation that they don’t understand well enough to capitalize on. The “slack moves” in the openings that pros talk about are often worth 1.5 points or less (often a fraction of a point), and that assumes pro-level follow up. This pro-level follow up is laughably rare outside of strong amateur dan levels and pro levels (and even within those ranks there are substantial differences). reply anononaut 13 hours agoparentprevBefore that, weak amateurs were just replicating human joseki. That's nothing new. They definitely give a player a good start, but knowing which to use and when, and of course how to follow up until the game is over is no simple task. It also happens to be the case that AlphaGo, KataGo etc. prefer simplifying the board state. Remove complexity and win only by a thin margin, because that's all that's needed. Memorizing AI preferences is much easier than some of these highly complicated joseki. reply thomasahle 13 hours agoparentprev> a lot of non-pros don't really have too much creativity and the whole game becomes memorizing and replicating AI moves. That makes no sense. After 10-20 moves you are surely in a position that has never been played before. How do you memorize moves after that? reply hibikir 12 hours agorootparentYou'd be surprised. Joseki are corner shapes, which might interact with other corners in the medium to long run, but whose interactions are way too difficult for any human to understand well. Therefore, you have 4 corners, and it's quite likely that you'll see 4 joseki getting played in any game. Joseki sequences have been studied for a long time, so they can be relatively long: Say, 15+ moves of an avalanche joseki, memorized by both players, and that's just one corner. So even before computers were any good, you could still see pretty iffy players using memorized patterns in every corner for a total way past 20 moves. reply roenxi 2 hours agorootparentIf they are iffy players they'll use 4 memorised sequences then enter the mid-game with a losing position. Playing out memorised sequences without considering the interactions the corners have on each other is one of the weights keeping amateurs from moving up to higher ranks. If you are playing someone who is worse at fighting then playing good-enough joseki and making up any theoretical difference in the middle game is a fine strategy. But even choosing good-enough joseki requires thought (or instinct) that does beyond what can reasonably be called memorising. It is critical to recognise when a framework is getting too big and invade before the opportune moment passes. As thomasahle notes, pretty much every game is unique and a memorised sequence unbacked by an algorithm cannot hope to be optimal. reply tasuki 11 hours agorootparentprevThe accepted approach used to be that the direction of play mattered. Now the AI has told us that no, just get locally-even results in all corners and you're fine. I never would've guessed! reply datameta 13 hours agorootparentprevPerhaps sub-positions still repeat with some regularity? Meaning subsets of the board. I have never played Go however, I've only seen the board and read the rules. reply mafuy 1 hour agorootparentAbsolutely so. There are corner patterns but also side patterns and other patterns; all of them are 'joseki', known and often played sequences. These can be memorized. But that's almost useless without understanding why these patterns are good. You still need to pick a pattern that works well with the rest of the board, in particular which conditions elsewhere on the board influence it (ladders, ko) and you need to know all the punishments for deviations. That's really difficult and often subtle, and it is where strong players as well as AI easily outplay someone who just mindlessly memorized patterns. reply matthest 13 hours agoprevEntertainment is one industry that will survive post-AI. We're still going to want to watch humans play sports, music and video games. We're going to want to watch humans act, cook food, and make vlogs. The chess industry is growing rapidly, even though it has already been conquered by AI: https://www.einpresswire.com/article/649379223/chess-market-... reply Ekaros 4 hours agoparentI am not sure why would I care about cooking for one. If AI can recreate a video with same recipe, but I could set constraints on ingredients, time, techniques and such it likely would be preferable option for many myself included. Human acting? Eeh, good enough AI and video could do it for me. Same goes for vlogs. Content tailored for me would trump any existing vlogger. reply mike_hearn 2 hours agoparentprevReally? I'd think it's the opposite, I'm expecting non-sports entertainment to be largely AI dominated within 20 years. High budget movies already often have fully CGI characters in which the only human element is the voice, and now AI voice warping is nearly perfect it's an obvious move to eliminate the continuity risks by making voice actors fully interchangeable, or even fully synthetic. And then even in non-Pixar style movies, many scenes use CGI body doubles for stunts, fully CGI scenes and so on. The human actors often don't appear in all their own scenes, or they're even brought back from death to keep working. So that leaves things like music, sports, etc. Some music genres totally defocus the humans, like electronic music. They go via frequently changed pseudonyms and you never really see them perform in person. Sports I can see remaining fully human. reply ultra_nick 13 hours agoparentprevThere are content farms in Facebook churning out fake grandkids for old folks to fawn over. reply chasd00 12 hours agoparentprevhumans will always have contests to see which human or group of humans is the best at something and it will always be entertaining to watch the contests. op is right. reply suyash 13 hours agoparentprevThat's the next one on the chopping block, wait till Sora and related services comes out, it's all going to be digitally generated and will look just as real so yes it will be content about humans but without human doing/creating much of it. reply tylerchilds 12 hours agorootparentas both an engineer and an entertainer, perception is reality. on the one hand, a computer system could 1:1 recreate a stunt a human did and elevate it to a stunt a human couldn’t do. people witness entertainment to trick their minds, and the disbelief and astonishment in the human condition hinges on “there’s no way _i_ could do that” and once they know they literally could not have done it, they’re not impressed. they may pay to be fooled once, but never twice. there’s a market for what you’re talking about, but that space is b2b and not c2c, which is where entertainment money flows. tl;dr dollar for dollar, ai vs taylor swift, taylor swift wins every time, no contest. reply hackable_sand 9 hours agorootparent> they’re not impressed This is subjective. Impressive things impress. It can be the hand-drawn impossible stunts from the 30's, or a CGI stunt from 2019. reply tylerchilds 7 hours agorootparenttotally agree, but what i’m getting at is the creative core of an expression. there’s an aspect of a piece that impresses and i’m claiming that exists as something the viewer appreciates and imitates in their mind’s eye. as a programmer, i create many things that impress people, but when i show them the methods of that creation, i can palpably feel their excitement wane as they lose interest in the nuance of my execution. beyond the surface, there’s an aspect of being impressed that is also the desire to take part in the recreation of it all. my claim in the first person, “i’m visually impressed by many image generators, but i’m not interested in fiddling with knobs, buttons, and strings to recreate the image in my mind’s eye” reply hackable_sand 5 hours agorootparentI just think that art economy should not be beholden to critical consensus. Art is a continuum of multimedia. At some abstraction you will be able to submit your piece into the collective and draw on increasingly precise inspo. reply bongodongobob 12 hours agoparentprevAs a lifelong musician, AI music has reached a level where it's able to write absolute bangers AND soulful music with feeling. Last weekend I listened to AI music all weekend long in absolute shock. People will still want to go to concerts, but a lot of that music will either be written by, or inspired by AI. This is one that I generated that has me convinced anyway. I made about 100 versions in different styles, but this makes my hair stand up. https://app.suno.ai/song/77d97c83-8633-47d2-80b2-fe47952a6bc... And a stoner rock banger: https://app.suno.ai/song/2071317f-a5ba-4f1f-b77b-048d6ff03a9... I mean, even if you don't think it's perfect, you re-record those and no one's going to know it's AI. reply tyg13 11 hours agorootparentAm I stupid, or is this just AI covering Blackbird by The Beatles in a couple different styles? Is this supposed to be an example of AI writing \"absolute bangers\" or \"soulful music with feeling\" because I just don't see it. It seems like the exact \"paint-by-numbers\" stuff that we always see with AI: it's capable of taking existing art and mashing it together, but there's nothing interesting or novel being created here. It's the pinnacle of carbon-copy, technically impressive but still soulless art. If you played me either of these songs without telling me they were AI, I would think: OK, weird cover of Blackbird? Not particularly moving, but perhaps a stepping off point to something more. What would interest me more than the music itself is the story behind why the song was covered, who was performing it, what were their intended emotions in creating it? And of course, since it's a cover, if they had any original material of their own. Knowing that it's just an audio file being synthesized by an AI takes all of the enjoyment out of it for me (not that it was particularly good to begin with). Music is not simply good because of the sound entering your ears. The story, the artist behind it, the intended emotion and artistry is part of the experience. AI does not deliver on this, and I doubt it ever will, because human connection is what underlies those extra-musical qualities. I care about the people who made the music; I will never care about the machine. reply bongodongobob 7 hours agorootparentThose are two examples I thought I'd give rather than linkbombing. Yes, I chose the lyrics to Blackbird because they are short, it generates in 2:00 chunks, and they don't have obvious meter. It's not like a limerick or something. I wanted to test the musical capabilities, I wasn't interested in generating lyrics, though that can obviously be done. As someone who has studied and played music professionally my whole life, this isn't paint by number, at least in the way you think it is. Most music is very similar. You can write 90% of most popular music using the 6 diatonic trichords, and 80% probably with only 4. There are also only so many ways to rearrange those. This is not using a book of chord changes or melodies. I can promise you that after listening to 100s of versions of Blackbird, Yesterday, and the ABCs, I do not see the sort of pull the changes from a database that you're implying. In fact, I'd wager I could probably find a dozen songs that use the exact same chord changes and you wouldn't even have realized prior. I know because it's a great wedding gig trick, and we'd do it all the time. I don't know what changes you think were carbon copied in the first link. It shows really good songwriting skills. Use of tension and surprise. \"It sounds like everything else in that genre.\" Yes, that's what a genre is. It also shows skill at vocal phrasing. This is really difficult and makes or breaks any song you try to write. Explaining how difficult it is to write good melodies while putting a hard syllabic constraint on it is beyond the scope here. It's not easy or obvious and usually sounds like shit. Like, try to sing Sweet Home Alabama along with the chord changes at the end of \"Layla\". After doing essentially that for years (that's exactly what songwriting is) I doubt I could do it and not have it sound idiotic. This thing can do that. Here's an example of it mimicking Russian to English phrasing: https://app.suno.ai/song/0d2d817f-4bf3-4837-85ab-9ff13abe9b4... David Bowi-ish phrasing: https://app.suno.ai/song/a6b9e419-2e0f-4208-b66e-929b2076d96... West Coast Hip-Hop banger alert if you're over 35 https://app.suno.ai/song/82e05d5b-7e0f-4715-8ed4-5f3d22fb81d... Acid Funk https://app.suno.ai/song/de174b94-1758-4fae-b497-93b79f384a2... Like I said, I have about 100 of these. This shit is nuts. Here's some ABC's Bossa Nova https://app.suno.ai/song/0fe61c85-62be-4327-9f05-5b0865353a6... 90's Grunge https://app.suno.ai/song/321a48e1-611b-4ec6-af0e-6e88815621c... Baroque https://app.suno.ai/song/68257dae-031f-4910-a013-9bc0281cee2... If you're thinking \"Oh that just sounds like the Brandenburg Concertos\" YEAH. THAT'S MY POINT. This level of mimicry is brand new. I've never seen anything close to doing anything like this. If you have, I'm all ears. Now since you think originality is important, here is a poem I wrote for a girl I dated that worked at a coffee shop. Never intended it to be a song, but I love it. https://app.suno.ai/song/9346c871-5e7f-439a-9134-d876aca7086... https://app.suno.ai/song/2be696db-3b3b-48df-9075-66d3388cc11... When I showed all this stuff to my musician friends this weekend (some who contract with Disney, Netflix, write scores, actively touring, etc), the reactions were actual tears, complete disbelief, shock, and existential dread. $1000 says there's no way you could tell me whether this is a Chopin original or not played amongst others, and that's my point. And if you say \"While sure, but that's not really that impressive because computers\", frankly, you don't know what you're talking about and have already made your mind up. https://app.suno.ai/song/bab48740-b977-4ee3-bdb8-0bc85995047... reply sandspar 2 hours agorootparentThanks for the write up. Where do you see this going? For example, what will Spotify's front page look like in two years? reply bongodongobob 1 hour agorootparentI think Spotify will start to fade away to personalized music, or adapt to it and become a search engine of generated music. My mom likes the Kansas, Chicago, early Genesis, ELO, and Rush. She's babysitting her granddaughter. I made this for her tonight in the last two hours, splicing and nudging the prompt. 70s Progressive ABCs: https://app.suno.ai/song/d8211841-2c75-4c8d-ba01-4cff9d35dcb... reply csa 4 hours agorootparentprevThank you for all of this. reply baobabKoodaa 8 hours agoprevI really enjoyed the upbeat positive outlook of the article. Unfortunately, as an ex poker pro, I find it hard to imagine that AI \"lifts people up\" in domains like games. Sandholm's bots pretty much destroyed poker. reply sinuhe69 9 minutes agoparentPoker is all about “taking the emotion out of the game”, isn’t? In such cases, what can beat a machine? Doesn't a machine naturally have the best “poker face”? reply intuitionist 13 hours agoprevThe blog doesn’t say anything about how this “decision quality” metric is calculated… but presumably it’s using very similar Go evaluation functions to the ones used in the superhuman AI players, right? I think it’s highly unsurprising that humans would improve by that metric — they’re learning from the machine, so of course the machine likes it. Also, most things in life are not two-player zero-sum games where you can construct an evaluation function and build a “decision quality” metric out of it. So I’m not sure what the takeaway should be in those cases. reply omoikane 10 hours agoparent\"Decision quality\" appears to be average difference in winning probability between player's move versus a move by Alpha Go (Leela Zero). It's on page 16 of: https://doi.org/10.2139/ssrn.3893835 Where it says \"Measuring the quality of moves\". I found this via citation #31 of: https://arxiv.org/abs/2311.11388 Which was referenced by the second graph in the blog post (they link to Nature, but the paper at Arxiv appears to be the same). reply SonOfLilit 13 hours agoparentprevComputers are so much better than humans at Go that the metric for board evaluation applies better to human games than it does to computer games. Just as I'm better at evaluating code written by GPT than code written by senior developers. Otherwise you'd see players who don't train with computers winning in tournaments against those who do. reply intuitionist 13 hours agorootparentYeah, I don’t think the metric is wrong or bad, just that it’s not telling us anything special. Or maybe it’s telling us something about Go AIs (that the “insights” they have are human-comprehensible) but it’s not at all clear that this fully generalizes. reply visarga 12 hours agorootparentOf course it doesn't work that easy in other fields. Basically the go board is an environment, the the AI model learns by creating its experiences in the environment. This can be applied to other kinds of environment, such as code execution, simulation, video games, human-AI chat rooms or robots. But each environment has its own complexity and searching for good strategies can take a long time. It's the same with scientific research, got to validate the theory in the world. reply SonOfLilit 12 hours agorootparentprevIt sounds from your analysis that \"better as evaluated by AI\" would be true even if it wasn't really objectively better. All I'm saying is that yes, it does mean objectively better in this case. reply ordu 6 hours agoprevMichael Abrash in his Graphics Programming Black Book described something similar with regard to optimization. People become stuck at some point, when they confuse \"it is good enough\" with \"it is the best possible result\". But if some event made them seriously doubt that it is the best possible result they could do wonders, like going from \"this is the fastest code possible\" to making it 10x faster. Just knowing that you could do better is a big deal, but if you have an AI showing you how to do better, then further perfection will become inevitable. reply sandspar 2 hours agoparentHappens in sports. Roger Bannister broke the 4-minute mile in 1954. Before this it was thought to be impossible. Within 3 years, 16 other runners also broke the 4-minute barrier. Their equipment was the same as before; it was a mental thing. Arnold Schwarzenegger tells about a similar thing in weightlifting. Pat Casey broke the 500 lbs bench press record in 1956. Bench press records have since doubled, now exceeding 1,000 lbs in some categories. reply stvltvs 2 minutes agorootparentTo compare apples to apples, the raw world record (unaided by special equipment) is only 782 lbs, not over 1000. Your point is still a good one, just unnecessarily overstated because benching 782 lbs is damn impressive! reply tutfbhuf 2 hours agoprev> A few months later Bannister was no longer the only runner to do a 4-minute mile. These days, high schoolers do it. Wait. I know where you are coming from, but this is simply not true. reply bravura 12 hours agoprev1) I would really be interested in broad brush strokes to understand how go theory has expanded. 2) I really wish we could shake the ant-farm with chess and go Fischer random chess. There's something nice about not having to memorize openings. reply bongodongobob 12 hours agoparentAt the same time, the familiarity of openings is nice. Imagine completely random WoW battlegrounds. Part of the fun is knowing the territory and strategies rather than having to make them up from scratch each game. reply timetraveller26 13 hours agoprevI watched the alpha go doc and it was really shocking to me when one of the top go players decided to retire because the game was meaningless now that computers could beat anybody. it's good seeing that that wasn't the case for all players. reply fjfaase 12 hours agoparentLee Sedol continued games until the summer of 2019, more than three years after the match. He quickly dropped in strength while other players who already were stronger than him during the challenge were further rising in strength, surpassing all previous players according to the graph shown at the Go Ratings website. https://www.goratings.org/en/history/ Just like in mathematics many professional Go player peak before 40, after which the slowly become weaker and weaker. reply Solvency 13 hours agoparentprevi don't get it, this applies to every single game. you can't beat aimbots in fps, you can even rig any game bot to play perfectly. that's why you play against HUMANS. reply krisoft 12 hours agorootparent> this applies to every single game But it did not used to apply for every single game. In fact in Go it was famously true just a few years ago that the best humans were waay waay better than even the best computers. At uni I hung out with Go players and they had all kind of theories why Go is particularly hard for computers. Some of them well quite well reasoned, and some were just a bunch of magical thinking. What was not in doubt is that even at that admitedly medium level the players were better than the computers they had access to. Just a few years ago it went from “we are competing to be the best Go players period” to “we are competing to be the best human Go players”. That is a change in mindset, and it seems at least for that one particular player they couldn’t make the change. Imagine that you grew up in a world where aimbots are just worse than mediocre players, always. And you build up your personal motivations with this fact. And then suddenly the aimbots get better than even the best players. Some systems of motivation will crumble when this happens. Some will manage to adapt. This is just how it is. reply hinkley 9 hours agorootparentA lot of people thought we’d get to 2030 at least before the humans lost. More people thought 2035 or even 2040. reply anononaut 12 hours agorootparentprevPros saw the writing on the wall, but remember that no bot was even particularly close to pro strength until 2016. Go was dogmatically described as still being decades away from bots being able to play competitively. For some professionals like Lee Sedol, the burning desire was to play the best games and best moves possible. To such an abstract game about intuition, seeing it finally be dominated by machines could understandably be crushing. reply gensym 13 hours agorootparentprevI enjoy rock climbing even though I'll never be as good as a monkey or a mountain goat. reply krisoft 12 hours agorootparentYeah but the difference is that you were always worse than a monkey or a mountain goat. And those are all worse than someone riding a helicopter to the top. So you didn’t had to change your mindset in this regard ever. Imagine if you grow up where there are no monkeys, mountain goats or helicopter rides to the top. You never heard of them, they are not a thing in your world. And you put in hard work to become a very very good rock climber. You kinda fancy yourself an apex climber. Maybe your mate George is a bit faster than you sometimes, but sometimes you are faster than him. Sometimes Sarah beaths the both of you, but sometimes you beat her times. You are kinda up there with the bests as far as you know. And then suddenly someone brings a monkey to your rock climbing gym and the monkey smokes all of you. It climbs walls much better than you ever could. Now you have to adapt. Will you change your viewpoint and start seeing yourself as “best among humans” and keep competing like that? Or will you see yourself as “clearly outcompeted so badly I might even give up”? Some people will go one way, some the other way. And then the new generation will grow up with the knowledge of monkeys, and they all naturally will be only the first kind of people, who understand that they can’t be the best only best among their class. Go players had their “the first monkey shows up in the climbing gym” moment during our lifetime. That is why you see some of the players react like that. That is a very different world from rock climbing where everyone already knows about monkeys, and mountain goats and helicopter rides since forever. Every person currently climbing rocks started climbing with the existence of monkeys, mountain goats and helicopters already incorporated into their thinking way before they climbed their first wall. reply hackable_sand 9 hours agorootparentWhat is your goal when competing? At the highest level of play, exploration of the game space eclipses point capture. Points don't matter because it's play, so your emotional attachment to the outcome is a measure of maturity. Play with infinite boundaries is pointless, so they let us explore freely without having emotional stake in arbitrary accumulation. To play with an asymmetrical opponent should be some parts learning with sparse competition to test your knowledge of finite play space. reply 1-6 7 hours agoprevI bet modern day go players have become more stereotypical in their moves. The only parallel I can draw is from professional Starcraft players who stopped doing very exotic moves because it’s usually blocked by players who’ve seen them all. reply caligarn 2 hours agoprevI wonder if the same thing is happening in chess. reply mark_l_watson 11 hours agoprevSeven years ago I took remote Go playing lessons from a South Korean professional player. I stopped after about 5 months and started using CS Pro Go on my iPad Pro and it has a nice teaching feature of rating every one of my moves so after a game I can see where my biggest mistakes were. This is different than pro players learning new surprising strategies, for me it is nice to use. reply Mtinie 13 hours agoprevThis supports with my hypothesis about human-created art, post-AI. People are deeply concerned about how their livelihoods and identities will survive the next few years. I get it, and while there’s certainly a level of existential dread that feels reasonable, I don’t see many people yet discussing what the visual arts industries will look like on the other side. If Go play is in any way a creative exercise—which I’ve heard it is— then I’m super interested to see the state of humans in the arts 24 months out from now. reply jsheard 13 hours agoparentThere is a key difference in the way these models are trained - Chess and Go have clearly defined win conditions, so a model can be taught to explore the possibility space and try to reach victory by any means necessary, potentially with strategies which have never been seen before. With art on the other hand there is no objective measure of quality, so the models are instead taught to treat already existing art as the benchmark to strive towards, making them trite by nature. As I see it AI can absolutely find innovative solutions, but only if you can clearly and explicitly define the problem it needs to solve. reply Mtinie 12 hours agorootparentI use diffusion models and other generative tools to give me inspiration for works. While these aren’t solutions, per say, the tools do help me define (and refine) my approaches and offer visual options to consider. reply CamperBob2 11 hours agorootparentprevWith art on the other hand there is no objective measure of quality, so the models are instead taught to treat already existing art as the benchmark to strive towards, making them trite by nature. Isn't this reminiscent of the arguments that were made at the dawn of photography as an art form? Some were afraid that portraiture was finished as an art form, but we got Impressionism, Cubism, and a host of other innovative forms to take its place. Never mind that portraiture was not in fact killed by photography, nor was any other visual form. Others swore that cameras and film would never be valid implements of art, but they got awfully quiet when Adams and Weston and others showed up on the scene, and you don't hear much from them at all these days. If nobody was afraid of AI -- if nobody was screaming bloody murder about how urgent it was to stop it -- only then could we safely say that it will have no role or relevance in art. reply smokel 13 hours agoparentprevMost of contemporary art is unaffected by the current AI craze. On one hand, the art world has been steadily pushing boundaries since the 19th century, and computer technology is just one blip on the vast radar of interesting subjects (other fashionable ones being gender, colonialist history, social practices, and physical properties of paint). On the other hand, art is mostly created by artists who were professionally trained as artists, i.e. not as scientists. Knowledge about computer technology is typically rather limited with both artists and collectors, leading to fairly bland stuff, or properly misguided hypes such as NFTs. reply Mtinie 9 hours agorootparent> On the other hand, art is mostly created by artists who were professionally trained as artists, i.e. not as scientists. Knowledge about computer technology is typically rather limited with both artists and collectors, leading to fairly bland stuff, or properly misguided hypes such as NFTs. Having reread this section, and considered it, I’m going to hard disagree. You are severely underrating the technical capabilities of artists. Historical and contemporary. reply smokel 3 hours agorootparentNote the \"mostly\" and \"typically\". There are some amazing artists who really know what they're doing. The chances of an artist having a PhD in machine learning and a masters at Goldsmiths are statistically speaking very low. reply Mtinie 12 hours agorootparentprev> Most of contemporary art is unaffected by the current AI craze. The illustrators and digital artists I know would generally disagree. As an abstract painter, I agree with you. Significant genre specificity. reply smokel 3 hours agorootparentI totally agree. There are some confusing differences in definitions of art :) I feel sorry for the illustrators, and wonder how they'll be able to sustain their creative passions. reply cageface 8 hours agorootparentprevMost of the AI generated \"art\" I've seen I'd classify as more craft than actual art. Art is supposed to express something and communicate and so far I haven't seen any AI art that really moves me or says anything insightful about experience or existence. When I scroll through the latest highly rated work on Midjourney, for example, I'm reminded mostly of tacky poster shop stuff. reply Mtinie 8 hours agorootparentThat’s fair but I don’t understand the relevance to my comment. Illustrators and graphic artists are in a hard spot given the commercial work they do. I’ve worked in this industry in the past and can attest that many of the contracts I executed on cared less about the originality of the output and more about the specifics of the prompt. Generative AI is enough for a lot of clients, if the price is right, even when the output is subjectively bad. reply cageface 8 hours agorootparentYes sure the kind of art that pays the rent for commercial artists is definitely seriously threatened by AI. I'm inclined to say that AI companies should have to pay for the training data they use although that does seem to mean only companies with billion dollar warchests can train AIs. reply zerocrates 13 hours agoprevJust finally getting around to reading/finishing my copy of Seven Games by Oliver Roeder, which covers checkers, chess, go, backgammon, poker, Scrabble and bridge, and the efforts for computers winning/solving each. A common theme is the effects of the computers on the human players in elevating (but maybe also homogenizing) play. reply WalterBright 8 hours agoprevI played chess poorly because I'm lazy, so instead of thinking about my next move I'd think about writing a program to pick the move for me. reply usgroup 13 hours agoprevCan I rephrase this? \"Professional Go players finally have a software good enough to beat them, as a result of which they got better by using the software\". reply ajkjk 13 hours agoparentNo? The article makes the point explicitly that they did not only get better by using the software; they're also better at playing moves the computer does not play. reply SonOfLilit 13 hours agorootparentIt makes the point that they learned not only by memorization. They still learned it by using the software. reply suyash 13 hours agoprevIt's just a little boost, AI will keep getting better and better at faster pace, humans will have to figure out a different strategy all together. reply Lacerda69 13 hours agoprevAI will force artists to learn how to draw perfect hands as every artwork with bad hands will be instantly flagged as generated. reply WalterBright 8 hours agoprevA new species arrives on the chess plain. Humans learn and adapt. reply sandspar 2 hours agoparentI could see AI being similar to the domestication of wolves. Humans encounter superior species, we co-opt the species to both species' benefit. Maybe AI would benefit from keeping us around. For example, we could test paths for it. Keeping a suboptimal pet like us for testing eliminates weak strategies, like a QA team finding bugs. It strengthens approaches by exposing flaws early. reply yinser 13 hours agoprevComputers were always going to be better at searching large trees, now they can help steer new heuristics for human players. reply SonOfLilit 13 hours agoparentI'm not sure a computer without search defeats the best human playing without any search, but I know it defeats 1dan players (very smart people who put in 5-10 years of deliberate practice) when they are allowed to use as much search as they are able. reply mafuy 1 hour agorootparentIn I believe it was the AlphaGo Zero paper, pure policy without tree search was estimated to be near pro level. KataGo is likely much better. I'm 3 dan (yet not smart) and I will definitely lose to it even with some handicap >:-( reply Art9681 13 hours agoprevWhen AI beats Go players, they roll up their sleeves and practice their passion and try to get better. When AI beats Hollywood... reply Tenoke 13 hours agoparentThe reward in Hollywood and most professional fields is about who makes the best/most cost efficient product no matter what. If the same was true for chess or go then players would be using computer assistance in every top level game. reply anononaut 13 hours agoparentprevI see the point you're making, and well made, but I think it also highlights a distinction in problems between the two. In the case of go, people who want to play go are the main motivating force. In the case of movies, I don't give a damn about Hollywood, the money, the studios, the IP, the actors. I only care about the quality of the film. Maybe the autuer, if there is one. AI changing conceptions about chess or go is very different than generative Ai which can radically change the means of how something is produced. I'm still going to play go because I love it. Meanwhile, I would happily cut out film studios (as we know them) if it meant I got to watch quality cinema. reply lvl102 13 hours agoprevPro players train with AI and you can often see “blue dot” moves in tournament settings. reply anononaut 13 hours agoparentIt's become go broadcasting standard to show some AI bot's win/loss confidence percentage for a given board state. It was fascinating for a few years, but now I feel like it takes away from some of the magic of watching pro level play. reply idkdotcom 13 hours agoprevGo is a finite search game. So each chess. Equating intelligence to being good as these games is as silly as equating intelligence to being good at solving differential equations. Computers have bested humans at solving differential equations for many decades now. Nobody said \"gee humans are now stupid\". AI, as a knowledge field, is biased in this notion that all it matters when it comes to intelligence is that computers beat humans at Go or Chess. reply seoulmetro 11 hours agoprevIs this a surprise? The best people at any craft learn from the people that beat them. reply yieldcrv 9 hours agoprev> After a few years, the weakest professional players were better than the strongest players before AI. The strongest players pushed beyond what had been thought possible. Human Instrumentality Project reply andrewstuart 12 hours agoprev [–] If I was a professional player of any sort of game that AI can play then I would never play against AI. Just be a human, play against other humans. Who cares what AI can do? reply bongodongobob 12 hours agoparent [–] People who want to get better care. Everyone who plays chess uses AI to improve. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Professional Go players faced a skill plateau until AI, notably AlphaGo, showcased superior abilities, leading to improved decision-making and creativity among players.",
      "The trend shift in Go occurred 18 months post-AlphaGo, aligning with the launch of Leela Zero, an open-source Go engine, which, along with tools like Lizzie, provided access to AI reasoning, fostering input learning and unleashing human creativity.",
      "The relationship between humans and AIs in competitive fields, witnessed in chess and Go, demonstrates the potential for AI to elevate human skills and drive progress beyond existing boundaries."
    ],
    "commentSummary": [
      "The impact of AI on games like Go and chess is discussed, focusing on how computer analysis aids players in enhancing their gameplay.",
      "Recommendations are made for introducing a promotion and relegation system in chess to promote more aggressive gameplay.",
      "The debate also addresses the potential of AI in entertainment, art, and music creation, stressing the significance of human creativity and judgment across different domains."
    ],
    "points": 360,
    "commentCount": 183,
    "retryCount": 0,
    "time": 1712605341
  },
  {
    "id": 39968761,
    "title": "Combatting Malicious Notepad++ Impersonator",
    "originLink": "https://notepad-plus-plus.org/news/help-to-take-down-parasite-site/",
    "originBody": "Help us to take down the parasite website 2024-04-03 I’ve received numerous complaints via email, social media, and forums regarding a website that poses a significant threat to our community. The site in question is https://notepad.plus/ which appears prominently when users google for “download Notepad++”. Some users have mistakenly believed that https://notepad.plus/ is the official Notepad++ website. This confusion has led to frustration and potential security risks. Despite declaring itself an “unofficial fan website created for general information/educational purposes only”, this site harbors a hidden agenda. It is riddled with malicious advertisements on every page. These advertisements aim to deceive unsuspecting Notepad++ users into clicking on them, generating profits for the site owners. The true purpose of https://notepad.plus/ becomes evident when we recognize that it seeks to divert traffic away from the legitimate Notepad++ website, notepad-plus-plus.org. By doing so, it compromises user safety and undermines the integrity of our community. Action Needed: Report the Parasite Site We urgently request your assistance in reporting this malicious website. By doing so, you contribute not only to the protection of the Notepad++ community but also to the preservation of a safe internet environment for all users. Please use the following link to report https://notepad.plus/ as a harmful site: Report malicious software Your vigilance matters. Let’s stand together to safeguard our community and maintain a secure online ecosystem. Thank you for your prompt action.",
    "commentLink": "https://news.ycombinator.com/item?id=39968761",
    "commentBody": "Notepad++: Help us to take down the parasite website (notepad-plus-plus.org)337 points by croes 22 hours agohidepastfavorite98 comments skilled 21 hours agoAt least in its current state, notepad.plus does redirect to the official website when clicking the download button. The site itself is mostly an AI generated mess, on top of that - its rankings are manipulated by articles like this, https://mycours.es/gamedesign2016/2023/03/21/quick-and-easy-... https://www.bacsitannhang.com/how-to-install-notepad-on-ubun... There are a few hundred of them but nothing overly crazy. Most of these articles look like a traditional \"link pyramid\" network. But I am surprised that the site hasn't been penalized by Google yet. All the signs are there that it is a bullshit site. Looking at the current rankings[0], it appears that the site is surging also. That's around 120,000~ Google clicks monthly based on my own estimates. Don probably has the numbers himself also as far as referrals go. [0]: https://i.imgur.com/SvOjalu.jpeg reply nebulous1 21 hours agoparentSo the site is clearly parasitic, doesn't add anything and detracts from security. The term \"malicious\" does imply a little more to me though, like it's actually serving me altered software. I guess the question is whether doing this (injecting yourself into the download flow for an open source piece of software and profiting via ads while doing so) is \"malicious\". I can see an argument that it is, as at the very least the site very much looks like it's the official site unless you read the small print. I don't get the ads on that site at the moment. I assume they are the fake download button type of ad? In any case, we can fight this particular site, but as you point out if this is generated content then I don't see how we're going to manually fight the coming onslaught of similar endeavours, so if the search engines can't keep generated content off their results (and so far they haven't been able to), it's going to be an interesting few years. reply cnity 21 hours agorootparentYour definition of maliciousness is basically user-centric, rather than provider-centric. It is malicious from the perspective of the maintainers of N++ because it robs them of the ability to control their image for the users who find the product through the parasitic page (which is obviously a worse UX). reply benlivengood 21 hours agorootparentIn light of the long-term effort to subvert xz and get a backdoor into sshd it's feasible that these kinds of malicious sites have a second agenda; become popular enough as search results for a Free product to serve backdoored download links selectively or wholesale once they get enough traffic. Regardless of intent, the low-quality ad networks sites like that serve routinely serve fractional malware ads anyway (focus-stealing alert()-style \"you are the 100,000th visitor\" or \"malware detected on your device\" garbage). reply cnity 20 hours agorootparentYes, though I was trying to expand the definition of malicious to include GPs own terms. Even if that never happened, the current situation is already malicious because the site erodes at the trust factor of the victim site in exchange for ad revenue. reply throwaway598 8 hours agorootparentprev> (injecting yourself into the download flow for an open source piece of software and profiting In high frequency trading, the equivalent's called providing liquidity. reply afavour 20 hours agorootparentprev> The term \"malicious\" does imply a little more to me though, like it's actually serving me altered software. Just because it doesn’t today doesn’t mean it won’t tomorrow. Be “legit”, get links to your site, rise in the SEO ranks. Then maliciously alter the software. reply mirekrusin 21 hours agorootparentprevIt may redirect for 99.99% of users for example. Or only for ip ranges that are not relevant targets. reply Flammy 21 hours agorootparentYeah my thought was similar: One day in the future its current behavior changes once it has built up enough traffic... reply cbozeman 21 hours agorootparentprev> I guess the question is whether doing this (injecting yourself into the download flow for an open source piece of software and profiting via ads while doing so) is \"malicious\". Yes. It's malicious. You're trying to earn money - in this case, ad revenue - on a piece of software with which you have nothing to do whatsoever. It's not only malicious, it's disgusting. I would state how I really feel, but dang would ban me. reply baobabKoodaa 21 hours agorootparent> Yes. It's malicious. You're trying to earn money ... No, please stop trying to redefine words. That's not what \"malicious\" means. I agree with you that this website is disgusting and needs to be taken down (as in \"down from google\") because there's a very real risk that the unscrupulous owners of the website _will eventually serve malicious software_ to juice their profits. But simply serving ads is not what the word \"malicious\" means. reply birdman3131 21 hours agorootparentGiven the number of full screen ad's claiming my computer is infected I have ran across just serving ad's is often enough to count as malicious. (Not saying all sites do that but I have seen it often enough.) reply baobabKoodaa 13 hours agorootparentOk, so Google (the website) is malicious, Youtube (the website) is malicious, Facebook (the website) is malicious. Every single ad infested website in the world is malicious, according to you. Stop trying to change language. That's not what \"malicious\" means. reply krisoft 21 hours agoparentprev> At least in its current state, notepad.plus does redirect to the official website when clicking the download button. I’m sure you know this, so i will just state the obvious: the concern is that it might change in the future, or alternatively it might serve a different link if and when some finger printing indicates it. reply rpigab 20 hours agoparentprevI love that you can find other adjacent s[cp]ams by looking at the other articles on mycours.es by the same author. Learn \"Why you should play at a mobile casino\", read reviews on cialis, viagra, ivermectin, in italian, also there's some code that looks like actual Brainfuck (but that might be obfuscated js) at the end of one of these articles, because why not. https://mycours.es/gamedesign2016/2023/04/20/miglior-prezzo-... This ivermectin article from april 2023 looks legit, it seems to be part of a section labeled \"gamedesign2016\". Might be a small mistake. They say the internet's dead, I say it's flourishing, look at how easy it is, you can upload anything, make mistakes, it still \"works\". Some LEDs are blinking, everything looks alright to me. reply Lockal 16 hours agoparentprevThe earliest Web Archive snapshot of this website is from 2020, and back then it looked almost the same, so it is not a \"new era\" AI generated garbo. However in 2020 it the same `/download` link returned some executable. Probably it still does the same (because there is no point to make such links, when you can make a direct link), but it returns different content based on geography/cookies/etc. reply 2-3-7-43-1807 21 hours agoparentprev> But I am surprised that the site hasn't been penalized by Google yet. I'm not really surprised at all. reply DaiPlusPlus 21 hours agorootparentA site like that surely can’t be doing more than $50/day in Adsense revenue - methinks - probably less given how the same audience will be using adblock reply callalex 21 hours agoparentprevThe site is down now so I can’t confirm, but I assume the site serves ads, which means Google is happy to have people visit it and profit. reply larschdk 21 hours agoparentprevYou don't know that they do. They did redirect when you tested, but they may not for everyone. They could easily selectively download-snipe anyone they have identified by IP address or even regional. Big security threat. reply dylan604 20 hours agoparentprevDoes Google actually punish sites that are generating clicks? From a layman's point of view, Google only punishes sites that do not play the SEO game and tries to live organically. reply throw_m239339 21 hours agoparentprevHow did you find the backlinks? reply skilled 21 hours agorootparentUsing this, https://ahrefs.com/backlink-checker/ I use it a lot. It's great to find dev resources, hidden gems, sites that talk about a specific topic, etc. It does have a limit (100 links), but for purposes such as this one - it is an absolute must-have bookmark for me. reply nolok 21 hours agorootparentThere used to be a link:url thing in google itself but it doesn't seem to be working that well anymore, like many thing on google search beside classic casual user search. reply paledot 18 hours agorootparentCasual user search works well on Google these days? That's news to me. reply candiddevmike 21 hours agorootparentprevVery neat tool, and I'd love to pay for the actual product but $100/month is too much unfortunately. reply forgotpwd16 21 hours agoparentprev>But I am surprised that the site hasn't been penalized by Google yet. I am not. Google searches nowadays suck. reply danpalmer 21 hours agoprevThis sucks and I hope the website can be taken down. However I'm not sure reporting it as malicious is the right option here, it depends. This post doesn't indicate whether the site is distributing malware, and the point of the Safe Browsing report is specifically to identify malware, not just sites doing shitty things. I suspect a trademark based appeal to the hosting provider would go further towards getting the site taken down. That said, the offending site appears to currently a) return an invalid certificte, and b) return no content even if certificate warnings are bypassed. If these continue I doubt the site will be listed on search engines for much longer. reply unstatusthequo 21 hours agoparentA lookalike site carefully curated to rise in search result ranking over time can trivially start delivering malware at any point. It happens enough that it’s warranted here. No reason not to report it for many things. It’s illegitimate and has no right to protection. reply danpalmer 21 hours agorootparentIt can also... not deliver malware. The point at which it would be of interest to Safe Browsing would be the point at which it starts delivering malware, and reporting it before then is only going to create false positives that could make it harder to report if/when it did start distributing malware. The Safe Browsing project is not about judgement calls on whether a website is a good citizen or not. The whole point is to have a global database of obviously bad stuff that is not about any sort of editorial control, curation, etc. That way it's a clear win for browsers to implement, and there's little criticism that can be levelled against it for preferential treatment etc. Cloudflare's Radar seems to have a different scope, and may be a more appropriate place to report this site. reply Chabsff 20 hours agorootparentFraudulently pretending to be a trusted authority (trusted enough to click a download link clears the bar) is sufficient to get flagged as Social Engineering, under the deceptive content category, by Safe Browsing. See https://developers.google.com/search/docs/monitor-debug/secu... reply danpalmer 20 hours agorootparentAh, interesting, this is still under Safe Browsing, but it's distinct from the malware reporting that the blog post directs people to. Reports for this would get the site classed as deceptive (which it sounds like it is), but not for malware distribution. That sounds like a good option, perhaps the best option for the Notepad++ blog post to direct people to. Fortunately/unfortunately I still can't load the site on any device or browser so I'm not actually sure what it looks like or how obvious it is. reply EasyMark 18 hours agorootparentprevreporting it to google as a shitty link is one thing, reporting it some place that is used for reporting malicious (aka dangerous) software sites seems wrong to me. reply Zambyte 21 hours agorootparentprevnotepad-plus-plus can also trivially start delivering malware at any point btw. reply baobabKoodaa 21 hours agorootparentSure, but we both know the odds of the scam website delivering malware is about 10000x higher than the odds of the real website delivering malware. reply gunapologist99 21 hours agorootparentprevAny website can deliver malware at any point. Should we report them all? If we start predicting that they will, we've turned the internet into minority report. Just because people are jerks and making money off deceptive ads doesn't specifically mean they're going to start serving malware.. it just means they're jerks. reply earthnail 21 hours agorootparentNo, but in this instance it’s so clearly targeted at a particular program that it really starts becoming dangerous for the makers of said program. Not all third party download sites are evil. But this one is in the veru dark gray area. It is correct to report it now as a threat before something bad actually happens. reply danpalmer 20 hours agorootparentReport it as what, and to who? If you report it as distributing malware when it doesn't it's just crying wolf, and will only take up time from actual malware reporting, or make this less likely to be classed as malware in the future if/when it does distribute it. Reporting it as trademark infringement to the hosting provider under the DMCA is most likely the best course of action, the one most likely to lead to a take down. That's assuming there's a trademark on Notepad++. reply PurpleRamen 20 hours agorootparentprevIt's a matter of probability. It's unlikely that an original project will start becoming abusive on its own, especially if it has a long history of trust. Someone unrelated, who is just high jacking other's fame and has built no trust on its own? More likely. And if it's just a throwaway ad-farm that was set up once, it's likely not receive much care. So there is a higher chance that a hack of the site will go under for a long time, or that the owner will just sell it at some point to a malicious actor. Anyone being long on the internet has seen such things happen more than enough, so many people have a legitimated lack of trust against those sites. reply kreetx 21 hours agoprevTo the person commenting on whether notepad-plus-plus.org was a legitimate domain: this is the strongest proof I've found: https://github.com/notepad-plus-plus/notepad-plus-plus/blob/... reply DRAGONERO 21 hours agoprevMight also be worth reporting directly through Cloudflare https://radar.cloudflare.com/domains/feedback/notepad.plus reply emXdem 21 hours agoparentCloudflare typically doesn't do anything unless you get lawyers involved in my experience. reply baobabKoodaa 21 hours agorootparent...unless the website happens to become a pet peeve of the higher ups at Cloudflare reply jacamera 21 hours agoprevDid the site owner remove the malicious ads since this was posted? I wanted my report to count so I didn't use the link in the blog post and instead googled for \"notepad++ download\" and clicked on the offending website which was ranked third for me. I didn't see any ads on any pages. I don't doubt the complaint but some screenshots and timestamps would be helpful since it's so easy for the site owner to cover their tracks. reply la_fayette 21 hours agoparentLooking into dev tools it seems the code for ad display is there, but a 403 is currently blocking loading it from google... reply fanf2 21 hours agoprevPuTTY has had problems with a parasite website for many years https://www.chiark.greenend.org.uk/~sgtatham/putty/faq.html#... reply forgotpwd16 20 hours agoparentThat explains why putty.org has links to Bitvise. At least they aren't serving ads (though the links are essentially ads) and Bitvise kinda good. reply max-m 21 hours agoprevThe bsnes emulator has a similar problem. The official \"website\" is the Github repository at https://github.com/bsnes-emu/bsnes/ but some unknown entity has snagged bsnes.org and is now also publicly linking to SNES ROMs they host on Github (Github doesn't care, you can report those repositories as much as you want. If you're not a rights holder they won't do anything). reply cbozeman 21 hours agoparentAnother part of this is the rise of search, period. Back in the Ancient Times, when \"search\" sites were in fact, directories of sites, not unlike the Yellow Pages, you had a categories and listings. I've been using The Emulator Zone since 1997. Long before Google, and found them under Yahoo's \"Games\" category. Since they've been around for over 25 years, I trust them. The often do grab the software from repositories and make it easily available. The site does have ads, but I haven't encountered a malicious one. It's all for stuff I actually use (Microsoft Azure, fragrances from House of Creed, and Hertz Car Rental right now), so I have a little leeway with these people, but TEZ has never actively attempted to obfuscate or confuse the reader unlike this site and others like it. reply max-m 20 hours agorootparentPages like TEZ or the \"Awesome XYZ\" list repositories that have become rather popular on Github are perfectly fine. Those are great hubs to get a grasp of what's available. Sometimes they are a little out of date, but if you're interested enough you'll find the up-to-date information by yourself :) But these parasitic pages that pretend to be official (project) pages should be purged. :/ For now they might link to original releases, but they could very well switch to malicious downloads from one second to the next after having gained enough \"trust\" and traffic. And that *unofficial* bsnes site reflects poorly on the emulation community because they actively promote downloads of game ROMs hosted (practically) on their website because they control the Github repository the games are uploaded to. reply hermitcrab 21 hours agoprevReported. Down with parasites. BTW I've used Notepad++ for years. Great piece of software. reply jmkni 21 hours agoparentDefinitely one of the main things I miss from Windows That and Paint.Net reply joshstrange 21 hours agorootparentNot sure if you saw this posted a week or so ago but you might be interested: > NotepadNext - A cross-platform, reimplementation of Notepad++ https://news.ycombinator.com/item?id=39854182 reply hermitcrab 18 hours agorootparentThanks. Currently I use Notepad++ as my general text editor on Windows and Brackets on Mac. Brackets is fine for my use, but a Mac version of Notepad++ is definiately worth a look. reply jmkni 21 hours agorootparentprevNice I'll check that out reply cm2187 21 hours agoprevAvoiding murky download websites is the biggest benefit of using a package manager like chocolatey (along the ease of setting up a new machine or updating all packages). reply throwaway290 20 hours agoparentYep. For all people say there are benefits to centralization & trust reply smcdiggles 21 hours agoprevSo, I just googled notepad++, then scrolled down to just the link. I didn't click it. However, I enable regional blocking for my home router which blocks traffic for certain countries and I just got a notification that it blocked traffic from notepad.plus. Without even clicking on it.. I am guessing this is Google's fault with fetching some sort of data while loading the search page, but thought it odd. reply avianlyric 20 hours agoparentMore likely your browser performing a pre-fetch in anticipation of you clicking. https://developer.mozilla.org/en-US/docs/Glossary/Prefetch reply gordonfish 20 hours agoparentprevIt could make sense if it's checking for robots.txt or so, though I'm not sure if it does that each time search results and I would have thought that that would be conducted from the server side rather than client side. Are you sure you don't have some browser extension that does some checks/inspection/etc of links? reply master-lincoln 20 hours agoprevIs there any moral difference to this practice compared to distributing affiliate links for online shops? reply navane 20 hours agoparentA moral difference? What even is that? Amazon wants you to put out affiliate links, npp wants you not to. So one is with concent, the other without. Consent is the moral difference? reply master-lincoln 20 hours agorootparentApparently you understood what I meant with moral difference, so I don't think it needs to be explained. Consent from the vendors is a good moral difference from the perspective of the right owner, thanks. reply robjan 19 hours agoparentprevThe analogy would be creating a site called amazon.org, providing an e-commerce like experience then redirecting people to the real site once you go to the shopping cart. reply EasyMark 18 hours agoprevI can see why this site is a net negative and a distraction from the real site, but how is it malicious? Is it taking users to places to download spyware/viruses? We've had ad farm pages for easily 20 years. I can't get to it now, so I guess the DNS guys took it down, but malicious to me implies that it intends to do harm to one's computer or \"person\" ? Do I have an antiquated take on the word? Has Gen Z revised the definition? Also, it absolutely shouldn't be on the first 10 pages of a Google search, lol reply orev 17 hours agoparentStep 1: Build an imposter site that delivers safe files Step 2: SEO optimize to get good Google rankings Step 3: Build a good reputation that many people start to think is the real site. Step 4: Wait Step 5: Swap the legit download links to malicious ones. A tool like Noptepad++ is used by IT admins and developers, so just like xz it’s a juicy target for malicious actors. reply deron_cs 21 hours agoprevOn that Domain. If you Google-search for OpenOffice in Germany you get a Website distributing Badware. It used to be the first result, now its the second. (openoffice.de) I always wonder why that site is still up. reply RNAlfons 21 hours agoparentIrfanView has the same problem. reply Tblue468 3 hours agorootparentVLC, too: vlc[.]de serves adware piggybacking on the regular VLC releases. reply vbezhenar 21 hours agoprevI don't understand what's wrong with that site and why should it be taken down. It does not claim being an official website. Notepad++ is open source so anyone can distribute it. reply duxup 21 hours agoparentI think the concern is that it climbs the search rankings and then decides to redirect elsewhere. Unfortunately this is not an usual pattern / been seen before. It already is pushing malicious ads on it. THAT is a problem already. reply HDThoreaun 21 hours agorootparentHow are the ads malicious? reply baobabKoodaa 21 hours agoparentprevA mysterious man ringing your doorbell at night while holding a knife has not done anything illegal, but the odds that they do something bad is high enough to warrant action. reply datavirtue 20 hours agorootparentThat's provocation. reply DEADMINCE 12 hours agorootparentChange the example to a gun in a holster on a hip then. reply bilekas 21 hours agoparentprevIt's deceptive and there is no reason to believe that after some more backlinks are generated by being deceived, that the owners of the fake site wouldn't change the download to their own modified version. It's a security nightmare and overall just scummy behavior. reply pixelpoet 20 hours agoprevFunny thing, after you report the ad-riddled site, you get taken to a spartan confirmation page which reads: ~~~ Thanks for sending a report to Google. Now that you've done your good deed for the day, feel free to: 1. Take a second to rejoice merrily for doing your part in making the web a safer place. 2. Make sure you have upgraded your web browser to the latest version, and that you have applied the latest patches for your operating system. ©2007 Google ~~~ Yeah, that really does sound like old-Google, before they (and advertisers, and mobile phones) killed the internet. RIP :( reply doublerabbit 20 hours agoparentOn the chances that your report has probably been sent to the year 2007 to.. Caused by the timeless time holes Google creates by not archiving creation dates of an article. reply Springtime 21 hours agoprevReminds me of the malicious site for MultiPar, which the (since deprecated) official forums said linked to altered binaries, yet due to the convincing domain name fooled even various regular users linking to it on forums like Reddit. reply BuildTheRobots 21 hours agoprevIt might be worth contacting NameCheap who the domain is registered though. I'm not actually sure if they'd be in a position to (legally) act at this point though. reply ameyv 21 hours agoprevDone. Thank you for wonderful software!! reply ClassyJacket 21 hours agoprevPart of the problem is that the spam website has a less spammy sounding domain than the legitimate website reply djur 21 hours agoparentWhat makes you say that? reply bool3max 21 hours agorootparentDashes (\"-\") have a bad reputation when it comes to domain names. How many official domain names for big established products can you think of that have dashes in the domain? reply SapporoChris 21 hours agorootparentI would guess that some see dashes as a negative. reply robin_reala 20 hours agorootparentprevwww.experts-exchange.com have been around for ages, although originally without the dash to be fair. Also, www.harley-davidson.com is redirected to from www.harleydavidson.com reply batch12 21 hours agoprevSome larger companies have good luck taking control of domains that use their branding or impersonate their services via a brand protection offering. I wonder if this can be done without a service if they contact the registrar or ICANN. reply iJohnDoe 19 hours agoprevGoogle loves situations like this. They want you to spend advertising dollars to make sure your brand is up top. They do this to all major brands. It’s extortion. Article on HN before. reply zzzeek 21 hours agoprevIronic that an app whose name itself was co-opted from the Microsoft product name now has to deal with someone else re-co-opting that same name. reply earthnail 20 hours agoparentThat’s not a fair comparison. The notepad++ owners wrote a piece of software (and an amazing one too). This owner did not write a new notepad fork. reply zzzeek 14 hours agorootparenta name is a name, if someone wrote an all new ORM for Python and called it SQLAlchemy++, as the creator of SQLAlchemy I'd be pretty pissed reply boo-ga-ga 21 hours agoprevDone! reply pipeline_peak 20 hours agoprevIn a world of bloated Electron apps, Notepad++ is truly something to be admired. Yeah it’s not “sexy” like emacs or vim. But every company I’ve worked for, every school I’ve learned to code, people have praised it. It definitely speaks to the millennial Windows programmer. reply Maxious 21 hours agoprev [–] Asked chatgpt > To determine the real websites for downloading Notepad++, you should primarily rely on the official website and reputable sources. The official website for Notepad++ is \"notepad-plus-plus.org.\" Be cautious of websites that may mimic the official site or offer downloads from third-party sources, as they could potentially be scam websites or distribute malware. It's always safest to download software from trusted sources to avoid any security risks. Google could do this. They don't. How the mighty have fallen. reply mkoryak 21 hours agoparentGpts hallucinate, imagine the uproar if Google did this and gave fake results.. reply joshstrange 21 hours agorootparentAnd how would that be different from the shovel blogs or sites that scrape and repost stuff from SO/Github issues already? I mean I understand how it’s different but what I’m trying to say is Google often returns trash as-is. Even their quick answers/answer box thing is wrong from time to time. reply lukan 21 hours agoparentprev [–] \"Google could do this\" Do what exactly? Offer a llm answer to every prompt that might be correct or not? No worries, soon they will, but I do not see how it will solve the problem. reply LeonB 21 hours agorootparent [–] They already use LLMs to generate the questions other “people” are asking — and blatantly lie that the source of these is from real people. They (google) also claim to be the source of the MC Hammer song “U Can’t touch this” (amongst countless other falsehoods) Google has slipped from “slightly worse than it used to be” into a nosedive toward being AOL. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The website https://notepad.plus/ is masquerading as the official Notepad++ site, housing malicious ads that threaten user safety.",
      "It intends to steer users from the authentic site, notepad-plus-plus.org, to profit from deceptive tactics.",
      "Users should report the site as harmful to safeguard the Notepad++ community and ensure a secure online space."
    ],
    "commentSummary": [
      "The website notepad-plus-plus.org is under fire for deceitful tactics, like redirecting users to the official Notepad++ site, raising fears of sketchy download links.",
      "Users worry about the site's security implications and ad-driven profit motives, sparking discussions on its potential fraudulent behavior.",
      "Debates on malicious intent arise, emphasizing the significance of sourcing software downloads only from official channels to mitigate risks associated with deceptive websites and emphasizing domain control and brand protection."
    ],
    "points": 337,
    "commentCount": 98,
    "retryCount": 0,
    "time": 1712577120
  },
  {
    "id": 39974374,
    "title": "OLMo 7B: Empowering AI Research with Open Language Models",
    "originLink": "https://blog.allenai.org/hello-olmo-a-truly-open-llm-43f7e7359222?gi=760105621962",
    "originBody": "Hello OLMo: A truly open LLM AI2 · Follow Published in AI2 Blog · 5 min read · Feb 1, 2024 -- 1 As the world races to deploy AI models that are effective and safe, the demand for Open Large Language Models (LLMs) has exploded. The massive adoption of both open and closed AI models means that AI capabilities have leapfrogged our ability to understand how they are created. Releasing the OLMo framework will provide the industry with an opportunity to understand what is going on inside AI models. Today, The Allen Institute for AI (AI2) has released OLMo 7B, a truly open, state-of-the-art large language model released alongside the pre-training data and training code. This empowers researchers and developers to use the best and open models to advance the science of language models collectively. “Open foundation models have been critical in driving a burst of innovation and development around generative AI,” said Yann LeCun, Chief AI Scientist at Meta. “The vibrant community that comes from open source is the fastest and most effective way to build the future of AI.” OLMo and the framework is designed to aid researchers in training and experimenting with large language models. They are available for direct download on Hugging Face and in GitHub. This work was made possible, in part, via a collaboration with the Kempner Institute for the Study of Natural and Artificial Intelligence at Harvard University and partners including AMD, CSC (Lumi Supercomputer), the Paul G. Allen School of Computer Science & Engineering at the University of Washington and Databricks. The framework features a suite of completely open AI development tools, including: Full pretraining data: The model is built on AI2’s Dolma set which features three trillion token open corpus for language model pretraining, including code that produces the training data. Training code and model weights: The OLMo framework includes full model weights for four model variants at the 7B scale, each trained to at least 2T tokens. Inference code, training metrics and training logs are all provided. Evaluation: We’ve released the evaluation suite used in development, complete with 500+ checkpoints per model, from every 1000 steps during the training process and evaluation code under the umbrella of the Catwalk project. “I’m enthusiastic about getting OLMo into the hands of AI researchers,” said Eric Horvitz, Microsoft’s Chief Scientific Officer and a founding member of the AI2 Scientific Advisory Board. “The new offering continues Allen AI’s tradition of providing valuable open models, tools, and data, which have spurred numerous advancements in AI across the global community.” A truly open model By making OLMo and its training data fully available to the public, AI2 has taken a big step towards collaboratively building the best open language model in the world. In the coming months, AI2 will continue to iterate on OLMo and will bring different model sizes, modalities, datasets, and capabilities into the OLMo family. “Many language models today are published with limited transparency. Without having access to training data, researchers cannot scientifically understand how a model is working. It’s the equivalent of drug discovery without clinical trials or studying the solar system without a telescope,” said Hanna Hajishirzi, OLMo project lead, a senior director of NLP Research at AI2, and a professor in the UW’s Allen School. “With our new framework, researchers will finally be able to study the science of LLMs, which is critical to building the next generation of safe and trustworthy AI.” With OLMo, AI researchers and developers will experience: More Precision: With full insight into the training data behind the model, researchers will be able to work faster and no longer need to depend on qualitative assumptions of how we feel the model is performing but can test it scientifically. Less Carbon: Currently one training run is equivalent to the emissions of nine US homes for one year. By opening the full training and evaluation ecosystem, it radically reduces developmental redundancies, which is critical in the decarbonization of AI Lasting results: Keeping models and their datasets in the open and not behind APIs enables researchers to learn and build from previous models and work. “With OLMo, open actually means ‘open’ and everyone in the AI research community will have access to all aspects of model creation, including training code, evaluation methods, data, and so on” said Noah Smith, OLMo project lead, a senior director of NLP Research at AI2, and a professor in the UW’s Allen School. “AI was once an open field centered on an active research community, but as models grew, became more expensive, and started turning into commercial products, AI work started to happen behind closed doors. With OLMo we hope to work against this trend and empower the research community to come together to better understand and engage with language models in a scientific way, leading to more responsible AI technology that benefits everyone.” “With AI2’s deep expertise in natural language processing combined with AMD high-performance computing engines, the OLMo models developed on the LUMI Supercomputer powered by AMD EPYC™ CPUs and AMD Instinct™ accelerators offer a unique opportunity to truly expand AI experimentation and innovation and advance the industry like never before. This new open framework will provide the AI research community across the world with trusted resources and a platform to contribute to and work directly on language models.” — Ian Ferreria, Senior Director, AI Solutions, AMD “We are happy that we can contribute to this important initiative by providing the computing capacity from the LUMI supercomputer along with our expertise. Public supercomputers like LUMI play a vital role in the infrastructure for open and transparent AI.” Dr. Pekka Manninen, Director of Science and Technology, CSC LUMI supercomputer in Finland is hosted by CSC, and owned by EuroHPC Joint Undertaking and 10 European countries. LUMI is the fastest supercomputer in Europe, and is known for its entirely carbon-free operations and was critical in supporting the pre-training work necessary to develop OLMo. “Databricks is excited to be collaborating with the Allen Institute for AI on the release of their OLMo open source model and framework. OLMo sets the standard for what it means to be open. Everyone in academia, industry, and the broader community will benefit enormously from access to not only the model but all of the training details, including the data, code, and intermediate checkpoints. I am especially proud that this model was developed on the Mosaic AI model training platform from Databricks. As with all great open source releases, the best is yet to come now that these artifacts and tools are in the hands of the community.” — Jonathan Frankle, Chief Scientist (Neural Networks), Databricks. Learn more Getting started with OLMo technical blog OLMo 7B technical report Get OLMo 7B For more information on the OLMo framework and The Allen Institute for AI visit here.",
    "commentLink": "https://news.ycombinator.com/item?id=39974374",
    "commentBody": "Hello OLMo: A truly open LLM (allenai.org)262 points by tosh 11 hours agohidepastfavorite37 comments vjeux 7 hours agoIf I read the license correctly, it seems that if you want to use the LLM, you need to tell the authors what you are doing with it. Am I reading this correctly? https://allenai.org/licenses/impact-mr “Derivative Impact Reports. AI2 seeks to encourage transparency around Derivatives through the use of Derivative Impact Reports, available here. Before releasing a Model Derivative or Data Derivative, You will share with AI2 the intended use(s) of Your Derivative by completing a Derivative Impact Report or otherwise providing AI2 with substantially similar information in writing. You agree that AI2 may publish, post, or make available such information about Your Derivative for review by the general public. You will use good faith efforts to be transparent about the intended use(s) of Your Derivatives by making the information freely available to others who may access or use Your Derivatives. You acknowledge that Derivative Impact Reports are not intended to penalize any good faith disclosures about Derivatives. Accordingly, if You initiate or participate in any lawsuit or other legal action against a Third Party based on information in such Third Party’s Derivative Impact Report, then this MR Agreement will terminate immediately as of the date such lawsuit or legal action is filed or commenced.” reply blackeyeblitzar 7 hours agoparentInteresting. I recall seeing Apache licenses in their official repositories. I wonder how these additional restrictions get pulled in. reply whimsicalism 7 hours agoparentprevno, this is apache license-d. yes it is confusing that AI2 has custom licenses but they aren't using them here reply lolinder 5 hours agorootparentIt looks like the weights [0] and code [1] are Apache licensed, but the training data [2] is using the license that OP is quoting from. [0] https://huggingface.co/allenai/OLMo-7B [1] https://github.com/allenai/OLMo [2] https://huggingface.co/datasets/allenai/dolma reply 6gvONxR4sf7o 5 hours agorootparentIs the license not transitive? Like could your impact report be “i want to remove this part of the license?” reply mkl 7 hours agoparentprevDoes that apply to this model? On huggingface it says \"License: The code and model are released under Apache 2.0.\" reply timsuchanek 8 hours agoprevGreat to see e2e openness. One of the only true OSS models out there, vs most of the models releasing the binaries (weights). Surprised that they didn’t mention Mistral 7b in the comparisons. reply sanxiyn 8 hours agoparentFalcon also released open dataset. reply Havoc 9 hours agoprevNotably “The Pile” doesn’t seem to be part of the training data. So this might be more sound legally than many other “open” LLMs reply sgu999 9 hours agoparentFor those also wondering: https://pile.eleuther.ai > The Pile is a 825 GiB diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together. By what's the legal complication with it? reply simonw 9 hours agorootparentIt is absolutely absolutely packed with unlicensed, copyrighted data. Books3 is the most notable example - nearly 200,000 pirated ebooks - but a lot of the rest of it is (unlicensed) scraped web data. The legal questions over whether this is a problem are currently still unresolved. Many people are also bothered by the ethical implications, which is a separate issue from the legal questions. reply 23B1 7 hours agorootparentIronic that even our everyday governance has little 'Alignment' between ethics and law. reply jacobn 3 hours agorootparentEthics are a lot more nuanced and change a lot faster than laws. Heck, a large fraction of ethics seem to be so fickle that they’re subject to potential revision by every generation. In fact, I’d argue that those revisions are a significant portion of how one generation distinguishes itself from their parents. Yet strangely every generation feels like they have arrived at a set of “universal laws” in their ethics. reply KarlKemp 3 hours agorootparentprevIn this case, both ethics and the law are murky. Pretty excellent alignment, for once? reply codazoda 7 hours agorootparentprevI took a quick peak at this last time it was mentioned and it had dozens of my own repos of unlicensed source code in it. All of that was published on GitHub and made public, but much of it has no license specified. reply blackeyeblitzar 9 hours agorootparentprevIt received DMCA takedowns: https://en.wikipedia.org/wiki/The_Pile_(dataset) > The Books3 component of the dataset contains copyrighted material compiled from Bibliotik, a pirate website. In July 2023, the Rights Alliance took copies of The Pile down through DMCA notices. Users responded by creating copies of The Pile with the offending content removed. reply kikoreis 5 hours agoprevWhat does the risk classification applied to the dataset actually mean? The licensing page [1] AI2 provides for their datasets is really nice but it doesn't really explain [2] what risk means in the context. Does it mean \"risk that the items contained in this set are licensed in a manner incompatible with its use in a training dataset\"? [1] https://allenai.org/impact-license [2] \"the AI2 ImpACT Licenses are artifact-agnostic and are instead structured according to the risk level we’ve assigned a given artifact\" reply ein0p 1 hour agoprevSeems to be surprisingly fast at smaller sizes, too. reply mysteria 9 hours agoprevIs this one of the first LLMs of note that was successfully trained on AMD GPUs? I wonder how seamless the process was and if they faced any issues there. reply sanxiyn 8 hours agoparentDatabricks (who also participated in OLMo, it's probably the same codebase) trained on AMD before, see 2023 post https://www.databricks.com/blog/amd-mi250. It was probably seamless, as any issues were fixed by Databricks in 2023. reply pksebben 3 hours agoprevIt's odd. Running inference on this (and other models in its class) and I keep running into a \"repeating token\" situation with moderate-to-long context windows. It feels almost as if, during inference, the model hits some format of local minimum that it careens around, and while temperature seems to affect this - it doesn't really fix it. at temp 0.2: > [{'generated_text': 'What follows is a transcript of a talk between a mysterious man and an agent of a bureau dedicated to investigating things which is typically referred to by some assortment of letters in the alphabet. The identity, origins, and motivations of the man were not known then and remain so. This transcript is not meant to scare, but provided simply to enlighten the concerned citizen of all the various and sundry things that may or may not go bump in the night. AGENT: Please state your name for the record. MYSTERIOUS STRANGER: I am the man. AGENT: Thank you. I am an agent of the Bureau of Investigation. I am here to investigate the following: 1. The following: 2. The following: 3. The following: 4. The following: 5. The following: 6. The following: 7. The following: 8. The following: 9. The following: 10. The following: 11. The following: 12. The following: 13. The following: 14. The following: 15. The following: 16. The following: 17. The following: 18. The following: 19. The following: 20. The following: 21. The following: 22. The following: 23. The following: 24. The following'}] ...and at temp 0.4: > [{'generated_text': 'What follows is a transcript of a talk between a mysterious man and an agent of a bureau dedicated to investigating things which is typically referred to by some assortment of letters in the alphabet. The identity, origins, and motivations of the man were not known then and remain so. This transcript is not meant to scare, but provided simply to enlighten the concerned citizen of all the various and sundry things that may or may not go bump in the night. AGENT: Please state your name for the record. MYSTERIOUS STRANGER: My name is not important. AGENT: My name is Agent Cyanide. MYSTERIOUS STRANGER: Agent Cyanide. AGENT: I am an agent of the Bureau of Investigations. MYSTERIOUS STRANGER: The Bureau of Investigations. AGENT: The Bureau of Investigations. MYSTERIOUS STRANGER: The Bureau of Investigations. AGENT: The Bureau of Investigations. MYSTERIOUS STRANGER: The Bureau of Investigations. AGENT: The Bureau of Investigations. MYSTERIOUS STRANGER: The Bureau of Investigations. AGENT: The Bureau of Investigations. MYSTERIOUS STRANGER: The Bureau of Investigations. AGENT: The Bureau of Investigations. MYSTERIOUS STRANGER: The Bureau of Investigations'}] reply pksebben 3 hours agoparent... this can get a little goofy even with do_sample=False and no temp:[{'generated_text': \"DAUGHTER: tell me a story FATHER: but it's late DAUGHTER: please? FATHER: okay, once upon a time there was a little girl who lived in a little house with her mother and father and her brother and sister and her dog and her cat and her hamster and her fish and her bird and her rabbit and her horse and her cow and her sheep and her goat and her pig and her chicken and her duck and her turkey and her goose and her llama and her alpaca and her camel and her zebra and her giraffe and her elephant and her hippopotamus and her rhinoceros and her kangaroo and her koala and her panda and her bear and her wolf and her fox and her cat and her dog and her bird and her fish and her hamster and her cat and her dog and her bird and her fish and her hamster and her cat and her dog and her bird and her fish and her hamster and her cat and her dog and her bird and her fish and her hamster and\"}] reply lostmsu 9 hours agoprevToo bad they did not put any comparison tables into the blog post. reply mysteria 9 hours agoparentThey're on Hugging Face. Interestingly enough they don't compare it against Mistral 7B. https://huggingface.co/allenai/OLMo-7B reply blackeyeblitzar 9 hours agoprevThis is the only LLM that is exciting to me. Clearly, LLMs are powerful tools that may end up replacing search and even go much further than simple searches by performing the research for you and producing final answers. Closed models like those from Open AI (ironically) or Anthropic cannot be audited. When most users will end up blindly hitting Microsoft’s Copilot button, which they are forcing OEMs to adopt, who’s to say how the information a user gets is being curated or manipulated by OpenAI or Microsoft or whoever? We’ve already seen real world examples of severe bias injected into LLMs. For example, Google’s Gemini had secret meta prompts that biased it towards certain types of answers and also caused it to produce hallucinated images that were funny but also dystopian (https://arstechnica.com/information-technology/2024/02/googl...). I don’t think we can just let closed AI systems take over society when they can easily be manipulated by the model owners without transparency. What I like about AI2’s approach with OLMo is that they are actually open, not just trading on the marketing benefits of the word “open”. Most “open” models are just open weights not open source. That’s like sharing an executable and not the source code. In my view, being open means that others have to be able to reproduce the final product (the model) if they wanted to and had the means (in terms of training hardware). It also means that they should be able to use whatever is provided freely for any purpose, rather than being subject to proprietary licensing. AI2 shares the training source code, training data, evaluation suite, and the model weights that they’ve produced by running the training process. It all uses the Apache license. And it’s also interesting that they used AMD hardware to train this LLM rather than Nvidia/CUDA. Open weight models like Llama keep repeatedly catching up to the best closed models from OpenAI or Anthropic or others. My hope is that truly open models like OLMa keep developing quickly enough to also keep up. Lastly, I hope that regulation does not block open source private development of AI systems. These systems will be the vehicle for speech for much of society in the future, so blocking private AI systems is a lot like restricting speech. But leaving that aside, open development will also drive innovation and reducing competitive pressure will hurt innovation. reply simonw 9 hours agoparentPet peeve: Google's Gemini LLM model was not to blame for the image generation weirdness. That would be like blaming DALL-E weirdness on GPT-4. Unfortunately, Google marketing decided to slap the \"Gemini\" brand on both the end-user interface used to interact with the model AND the actual model itself, hence people constantly calling out Gemini-the-model for weird decisions made as part of Gemini-the-user-interface. reply yk 6 hours agorootparentDid anybody manage to get the entire prompt out of gemini, or what are you basing your claim on? reply blackeyeblitzar 9 hours agoparentprevOne thing I wanted to add and call attention to is the importance of licensing in open models. This is often overlooked when we blindly accept the vague branding of models as “open”, but I am noticing that many open weight models are actually using encumbered proprietary licenses rather than standard open source licenses that are OSI approved (https://opensource.org/licenses). As an example, Databricks’s DBRX model has a proprietary license that forces adherence to their highly restrictive Acceptable Use Policy by referencing a live website hosting their AUP (https://github.com/databricks/dbrx/blob/main/LICENSE), which means as they change their AUP, you may be further restricted in the future. Meta’s Llama is similar (https://github.com/meta-llama/llama/blob/main/LICENSE ). I’m not sure who can depend on these models given this flaw. reply idle_zealot 8 hours agorootparentDo we even know if these licenses are binding? AFAIK we have no ruling on whether model weights are even eligible for copyright. They're machine-produced derivatives of other work, so it's not a guarantee that copyright protects them. reply blackeyeblitzar 7 hours agorootparentThat’s a great point and I hope more people speak up to treat models as just numerical derivative works so they aren’t automatically granted these protections. It’s better if society meaningfully debates this and chooses the right approach. reply gremlinunderway 9 hours agoparentprev> For example, Google’s Gemini had secret meta prompts that biased it towards certain types of answers and also caused it to produce hallucinated images that were funny but also dystopian (https://arstechnica.com/information-technology/2024/02/googl...). Such a bizarre take to call this \"dystopian\". The model happened to create some out-there pictures. I mean, it's no more outlandish then giant dragons and snakes and such being created yet the thought of a person of color being something historically inaccurate is this massive outcry against revisionism? Who cares? Besides, the article identifies the probable goal which was to eliminate very known biases in existing models (i.e. when generating \"angry person\" you mainly got black people). Clearly this one wasnt tuned well for that goal, but the objective is not only noble but absolutely should be required for anyone producing LLM models. reply blackeyeblitzar 9 hours agorootparentIf I may explain: the dystopian part to me is the lack of transparency around training code, training data sources, tuning, meta prompting, and so forth. In Google’s case, they’re a large corporation that controls how much of society accesses information. If they’re secretly curating what that information is, rather than presenting it as neutrally as they can, it does feel dystopian to me. I’d like transparency as a consumer of information, so I know to the extent possible, what the sources of information were or how I am being manipulated by choices the humans building these systems made. I appreciate the issue you’re drawing attention to in the example you shared about images of an angry person. I think I agree that focused tuning for situations like that might be noble and I would be okay with a model correcting for that specific example you shared. But I also struggle with how to clearly draw that line where such tuning may go too far, which is why I favor less manual biasing. But I disagree that such tuning should be required, if you meant required by the law. Like with speech or art in general, I think anyone should be able to produce software systems that generate controversial or offensive speech or art. Individual consumers can choose what they want to interact with, and reject LLMs that don’t meet their personal standards. reply lynx23 1 hour agorootparentprevRight, \"who cares\" about the truth in our dystopian world? 1984 is apparently too long ago for people to remember the ministry of truth... reply theshackleford 2 hours agoparentprev> Open weight models like Llama keep repeatedly catching up to the best closed models from OpenAI or Anthropic or others. Since when? I’ve had the complete opposite experience. reply refulgentis 8 hours agoprevThis is 2 months old. reply btbuildem 7 hours agoparentAnd yet it's topical and relevant. reply timmg 9 hours agoprev [–] Has their site been hugged-to-death or is it my hotel wifi? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Allen Institute for AI has launched OLMo 7B, an open large language model framework, facilitating researchers and developers in advancing language model science collectively.",
      "OLMo includes pretraining data, training code, and evaluation suites for precise and sustainable AI research, aiming to promote transparency and collaboration.",
      "Available on Hugging Face and GitHub, OLMo encourages responsible AI technology development for the benefit of the AI research community."
    ],
    "commentSummary": [
      "The debate centers on the licensing and openness of the OLMo language model from allenai.org, emphasizing the need for a Derivative Impact Report for derivative works and highlighting the Pile dataset as an alternative.",
      "Legal and ethical concerns regarding the Pile dataset, training models on AMD with Databricks, potential risks of closed AI systems, and the call for more open and transparent AI systems are explored.",
      "The conversation delves into bias, transparency, copyright protection, and finding a balance between artistic freedom and ethical considerations in model development, referencing competitors like Llama and comparing to closed models by companies like OpenAI and Anthropic."
    ],
    "points": 262,
    "commentCount": 37,
    "retryCount": 0,
    "time": 1712615217
  },
  {
    "id": 39969131,
    "title": "Vala: Object-Oriented Programming for GNOME Technologies",
    "originLink": "https://vala.dev/",
    "originBody": "// ExampleApp.vala public class ExampleApp : Gtk.Application { public ExampleApp () { Object (application_id: \"com.example.App\"); } public override void activate () { var win = new Gtk.ApplicationWindow (this); var btn = new Gtk.Button.with_label (\"Hello World\"); btn.clicked.connect (win.close); win.child = btn; win.present (); } public static int main (string[] args) { var app = new ExampleApp (); return app.run (args); } } // Compile command (requires gtk4 package to be installed): // valac --pkg gtk4 ExampleApp.vala Get Started View Source Code Why Vala? Productivity Comfortably write your code in an object-oriented way with high level abstractions, while having access to deep integrations with GNOME technologies such as: GObject, GTK and more! Performance Vala compiles code down to blazingly fast, fully native binaries. You're also able to reuse existing C Code in your Vala programs as well as generate C Code from Vala. Open-Source Vala is free and open-source software. It has a great community that contributes to the compiler and develops lots of tooling. Tooling Documentation Learn more about Vala What can you build? GUI Applications Perfect integration of GTK, Libadwaita and Granite makes development super simple. Easily publish your apps on Flathub without bloated packages. GNOME Getting Started Tutorial elementary OS App Tutorial Command Line Programs Use a large standard library and many third-party libraries available for use in Vala. Libraries Provide bindings for many other languages and API documentation without manual work. Also, dynamic and static linking are both supported. What's new? print(\"Hello Planet GNOME\"); Lorenz Wildberg 2023-10-11 Vala 0.56 Lorenz Wildberg 2022-03-18 View Blog Current Versions Vala 0.56.13 (Stable Long-term Support) Vala main (Development) How to install Showcase Tuba Browse the Fediverse Workbench Learn and prototype with Vala and other GNOME technologies Dino A modern open-source chat client for the desktop. Focuses on providing a clean and reliable Jabber/XMPP experience while having your privacy in mind. Monitor Manage processes and monitor system resources TextSnatcher Copy Text from Images with ease, Perform OCR operations in seconds. Timeshift System restore tool for Linux. Creates filesystem snapshots using rsync+hardlinks, or BTRFS snapshots. View More Vala Projects Community Forums Ask questions related to Vala in the GNOME Discourse forums under the 'vala' tag. Discourse Forums Internals Chat Talk to the developers or ask questions about how to contribute to Vala, use the Vala Matrix channel. It also has a IRC bridge. Matrix Room Community Server Connect with the community on the Discord server. Discord Server Social Media Follow the Vala project across these platforms Mastodon Twitter Telegram Reddit Get Started View Source Code",
    "commentLink": "https://news.ycombinator.com/item?id=39969131",
    "commentBody": "Vala Programming Language (vala.dev)248 points by gtirloni 21 hours agohidepastfavorite175 comments ndiddy 18 hours agoHas Vala seen much activity recently? The last time I remember hearing about it was reading a blog post from one of the main GNOME developers (https://www.bassi.io/articles/2017/02/13/on-vala/) saying that Vala was basically being maintained by a single developer and he didn't recommend using it for new development. reply asabil 18 hours agoparentVala is very much used among other things by Frida. I have been involved with Vala in it’s early day and some hardcore GObject/C users never quite liked the C code that Vala generated although it was perfectly valid and exposed a very clean C header to interface with it. I think that Vala is a really great tool for building applications that make use of Glib/GObject. For example Gtk applications, but also GStreamer and DBus applications. reply nicoco 5 hours agoparentprevAFAIK I use one app written in Vala: dino, a modern XMPP desktop client: https://dino.im/ reply pipeline_peak 18 hours agoparentprevTo be fair, how many developers are even writing GTK applications anymore? How many GUI apps are even using these native GUI frameworks? If you aren't developing a DAW, a browser, or some other performance critical GUI app, you're moving to Electron and other layout engine solutions. And who should blame you, people don't want to fiddle with GTK_Window_Context_Handler* gtk_wdw_ctx_hdlr_ptr = Init_GTK_Window_Context_Handler(Window_Context* w_ctx); just to make a damn app. Even if Vala drops in to save us all from noodling with Window Context Rocket Ship Handlers, layout engines are more portable solutions. The decline of WPF and Winforms is proof of this. reply deepsun 20 minutes agorootparentJavaFX was there since the version 9, portable even between OSes. And Java Swing was there from the beginning, still perfectly usable, and is widely used in industrial applications. reply sitzkrieg 18 hours agorootparentprevthe answer could possibly anyone who cares about the footprint of their software (disk, egress, image running size...), not wanting to deal with an entire browser engine (in every aspect) wpf is still heavily used in industry so that isnt very convincing to me reply creshal 17 hours agorootparentAlso heavily used in the industry are toolkits like sciter, which give you HTML/CSS UIs without an entire browser engine underneath. I suspect those will be eating the remaining market share for 'traditional' toolkits, simply because it's the dominant UI paradigm. reply The_Colonel 16 hours agorootparentThe motivation behind using electron is: a) being able to deploy same codebase on web and desktop b) reusing the vast selection of libraries for web c) tapping the large amount of trained developers. Sciter fails on a) and b), because it doesn't support even such a basic and widely used feature as CSS flex (they have something similar, but incompatible). Even c) is very debatable. reply nicoburns 10 hours agorootparentI'm currently working on a Sciter-like solution that does support widely used CSS features such as Flexbox, CSS Grid, Media Queries, CSS Variables, etc. Although we're not planning to ship a JS engine: scripting will be with Rust that compiles to WASM when running on web. Binary sizes are currently around ~12mb which isn't as good as Sciter but is quite a bit better than Electron. reply c-smile 8 hours agorootparentprev> being able to deploy same codebase on web and desktop Desktop UI and Web UI models are so different that in reality you cannot use the same codebase on 100% in both targets. Web UI uses traditional Web's \"endless tape\" model where only width is known. This does not really work on desktop where width and height are finite. See this Sciter based app https://notes.sciter.com/ as an example. > reusing the vast selection of libraries for web If you do need this then you can use Sciter's WebView that is abacked by system browser: https://sciter.com/wp-content/uploads/2022/04/sciter-webview... reply The_Colonel 2 hours agorootparent> Desktop UI and Web UI models are so different that in reality you cannot use the same codebase on 100% in both targets. I develop an Electron app and the CSS is 100% same on both web and desktop. It's not a \"web page\", it's an app, which doesn't use the \"endless tape\" model. > If you do need this then you can use Sciter's WebView that is abacked by system browser: At that point it seems easier to just use something like Tauri which uses the system browser for all rendering. reply presentation 5 hours agorootparentprevIf you set the height of your body element to 100vh (100dvh in the case of iPhones), then the web acts like the desktop in this regard. They’re different since the tooling is different but not because they’re essentially different. > If you do need this then you can use Sciter's WebView that is abacked by system browser: iframes are very constrained in what you can do, if you’re targeting the web from the start you get the full power of using those libraries beyond the bounds of a single frame. reply jodleif 15 hours agorootparentprevThe path to hell is paved with good intentions reply jayde2767 15 hours agorootparentprevRegarding CSS and layout in general, there is a concept of sizers and spacers in wxWidgets that makes layout far easier to grok for the average dev. That being said, it does (sadly) add to code bloat which is where CSS excels, IMO. reply temporarely 17 hours agorootparentprevThanks for that! That sounds very sane. I agree that dragging an entire browser engine just to have a desktop app is excessive -- less code is always better. https://sciter.com/developers/engine-architecture/ reply mrec 16 hours agorootparentprevWhat's the \"egress\" footprint you mention? I hadn't come across the word in this context before. reply sitzkrieg 15 hours agorootparentoutbound bandwidth on say, \"cloud\" providers like aws. it can be very expensive. we have been actively reducing our installer sizes, for our costs (edge, cdn, some internal egresses), customer bandwidth, and basically giving a shit and doing a decent job. rare stuff these days reply pipeline_peak 18 hours agorootparentprev> anyone who cares about the footprint of their software That’s why I mention performance critical > wpf is still heavily used in industry But is anything new being made with wpf? Because that’s why I left that area. reply sitzkrieg 17 hours agorootparenti actually am working on greenfield wpf currently but ill be first to admit thats an exception. windows shop still reach for it heavily in \"boring\" industry in my experience. there is a difference between performance critical and sloppy of course. most people default to heavy crap cuz its good enough, or a good fit sometimes, i get that and this is coming from someone who had to use wpf for software where it ended up too slow in all rendering paths despite MS \"trust us bro\" and had to render using directx directly. not super uncommon but we were paying the price early (.net fw 3, fuzy wpf text, vibrating layouts, the good days) reply Tomte 16 hours agorootparent> but ill be first to admit thats an exception No, that‘s very common. I‘m peripherally involved in such a greenfield project, too. And my employer is (very) slowly switching existing WinForms applications to WPF. It‘s the new thing. (I know that there are a handful newer MS GUI frameworks, but for us it‘s new) reply sitzkrieg 15 hours agorootparentinteresting, i was wpf old hat and had to justify it vs winui3 (directly) and maui they are both too new reply neonsunset 15 hours agorootparentprevYes, but that's against common advice to use AvaloniaUI instead. reply JaggerJo 15 hours agorootparentWe use AvaloniaUI in production for a handfull of apps. Couldn’t be happier. reply sitzkrieg 11 hours agorootparentprevthis is not cross platform work, but i checked that out and will def keep in mind! esp considering all the AOT limitations in the real world reply mardifoufs 15 hours agorootparentprevAnd even if you want to keep it non-web, I don't see why you'd use gtk instead of qt (especially since QT has pyside6). And I'm not saying that as someone who likes QT either. I wish I started my PoC with web tech from the get go actually. But still, it's so much more convenient and even if you stick with c++ QT still provides a lot of nice to have tools (for threading, signals, concurrency etc). I haven't used qml a lot but I wonder if gtk has an equivalent to that too. What am I missing about gtk here? Like, where does GTK beat qt? reply brabel 18 hours agorootparentprev> how many developers are even writing GTK applications anymore? I would like to know the honest answer to that question, actually. reply mirpa 17 hours agorootparentSee Gnome on Linux. reply maxloh 1 hour agorootparentMost GNOME apps are written in C/C++ actually. reply Spivak 15 hours agorootparentprevYeah, but outside the GNOME devs themselves the adoption has been pretty meh. reply flexagoon 57 minutes agorootparentThere are a lot of third-party Linux apps built with GTK4/Libadwaita. If you just to to https://flathub.org and click on random apps a lot of them will use GTK. reply luciusdomitius 15 hours agorootparentprevThis is a very shallow opinion. I, personally, avoid electron-js apps like the plague. Having 500 MB of RAM wasted by something as trivial as MS Teams or Insomnia is horrendous. ElectronJS apps tend to be not only inefficient but also much less reliable and poorly designed. reply Sammi 12 hours agorootparentConsumers and bosses don't care. They want stuff and they want it now reply pipeline_peak 13 hours agorootparentprevNo, your opinion is out of touch. The companies that release these programs don’t care about 500 mb being wasted. What is your mom using? Very likely bloated software she unwittingly installed. Look around, it’s a bloated, buggy world. Calendar apps aren’t being carefully engineered by European computer scientists. >Electron apps tend to be poorly designed How so, are you saying because they use Electron they’re inherently poorly designed? reply deergomoo 12 hours agorootparent> How so, are you saying because they use Electron they’re inherently poorly designed? I’m not the original commenter, but Electron apps are regularly poorly designed and lacking quality. I won’t say it’s inherent to Electron, but it’s largely due to the fact that the standard controls offered by browser engines are incredibly rudimentary. They are still catered primarily to forms and simple interactivity in a document. Rich interactivity requires you build your own or find some third-party package, whereas at this point most desktop UI toolkits have 20-30 years of development and refinement behind them. Panels and panes are resizable and collapsible; you can have multiple free-floating, smoothly resizing windows; collections/tables will efficiently reuse cells to avoid tanking performance on long or infinitely scrolling lists. All of this stuff is possible with Electron I’m sure, but the platform doesn’t offer it for the taking, so most don’t bother. And when they do, it’s less refined than what we had in desktop apps in 2005. Do people care? Evidently not. I do though. reply kriops 9 hours agorootparentprevI care about my time on battery being cut by 90%. I am in no way above writing GUIs with web tech, but it's very noticably suboptimal for the UX. reply Modified3019 2 hours agorootparentprev“Some of you may OOM, but it’s a sacrifice I am willing to make.” reply sprash 13 hours agorootparentprevThe real reason people stopped developing Gtk applications is that GNOME tends to declare widely used APIs arbitrarily as \"internal interface\" and makes incompatible changes even on minor version updates. It's a constant chase not worth any developers time. reply sirwhinesalot 13 hours agorootparentPrecisely this, the moment they announced they wanted to break backwards compatibility with new GTK versions every 6 months or whatever it was they made it very clear it is exclusively a Gnome API for OSS projects and nothing else. They backtracked a bit on that but they'll still replace GTK4 with GTK5 at some point, probably deprecating context menus or whatever else this time. Clowns. reply TingPing 10 hours agorootparent9 years, thats how long between last GTK API break. Never for GLib. reply sirwhinesalot 14 minutes agorootparentThe first release of GTK3 that could be considered stable was 3.20. Either way, I'm talking about GTK4, where the original plan was 6 months of breaking releases, stabilization in version 4.6, followed by starting work on the incompatible GTK5, to be released 2 years after the first GTK4 release. The backlash was so big they at least changed the versioning scheme, but the \"break shit every 2 years\" is still in full force. reply diego_sandoval 17 hours agorootparentprevI tried to learn GTK a few years ago, because I wanted to create an app that was fast, snappy and looked \"native\" on Linux. But I was unable to find information about how to set up my repo and build process with GTK vendored into the project. All tutorials said that I should just install GTK libraries with my system package manager, but that was not a satisfactory answer for me. After some hours fiddling around stackoverflow, adding flags to my GCC invocation and being unable to solve the problem, I just gave up. Flutter Desktop seems like a compelling alternative to GTK these days. reply audidude 16 hours agorootparent> Flutter Desktop seems like a compelling alternative to GTK these days. Sad news for ya, flutter just wraps GDK (from GTK) on Linux. It also, last time I checked, was limited to a single application window. reply fngjdflmdflg 14 hours agorootparentIt just uses GTK to open a window. Firefox and webkit also do this IIRC. reply TingPing 10 hours agorootparentWebKitGTK has a lot of integration with the toolkit. reply mirpa 17 hours agorootparentprevAdding flags to gcc should be just a matter of pkg-config. In bindings to high level languages that should be handled automatically. reply asddubs 16 hours agorootparentprevqt is a good fit for writing native linux apps. I've tried both and qt is much nicer to work with (in my opinion) reply foresto 15 hours agorootparentI find that Qt apps also feel native on a wider variety of linux desktops than Gtk apps do. For example, Gtk (since v3 I think) encourages apps to override the window manager, stuffing widgets into the title bar and breaking actions like window shading/collapsing. In general, Qt acknowledges the differences between desktop styles and tries to match them, while Gtk doesn't seem to care about any style or interaction model other than its own. reply cies 17 hours agorootparentprevFlutter is cool, and new. Qt is old, and cool. reply rob74 18 hours agorootparentprevYeah... why fiddle with GTK_Window_Context_Handler_Ptr if you can fiddle with the intricacies of CSS layouts instead? reply pipeline_peak 18 hours agorootparentWell, if you want to make things look pretty, which product owners and users expect, you’ll have to fiddle with cosmetic intricacies regardless. reply mhd 17 hours agorootparentGreat thing is that as opposed to the web, the platform/library designers already took care of a lot of that, so the biggest overlap between GUI libraries and CSS is usually the layout (these days, where CSS isn't utterly abominable here). A lot of the color fiddling etc. that web apps due is due to 1.) Graphic designers having no other way to go, now that print is dead 2.) Corporate Identity (which is of lower priority than platform identity) 3.) Selling stuff (not needed as much with desktop apps) reply Spivak 15 hours agorootparentIt's great when you want to outsource your or target a single platform but infeasible when you want to bring your own design or be the same across platforms. And since most folks end up supporting the web as a first class platform you have to BYOD there anyway. Nobody is out there making a JS framework to emulate WinForms. reply mhd 15 hours agorootparentI thought this was about making things enticing to the scrum overlord, but sure… Not everything has to or can be be cross-platform, not everything has to be web-visible. And I'd argue that serving the same view to everyone is quite often a bad idea. (Heck, in the web-case, it's not even just the view) reply megous 17 hours agorootparentprevGTK uses CSS underneath. reply maxloh 1 hour agorootparentYeah, and it doesn't look native except on Linux. reply bandrami 16 hours agorootparentprev> If you aren't developing a DAW Now that Ardour has switched are there still any DAWs using GTK? reply jabl 14 hours agorootparentAFAIU Ardour hasn't switched anything. What they have done is import Gtk+2 into their own source tree, anticipating that Linux distros will get rid of the long deprecated and unmaintained Gtk+2. From https://ardour.org/whatsnew.html# : > From a project-level perspective, perhaps the most important change is that we have moved the source code of our GUI toolkit (GTK v2) into the Ardour source tree. reply nextaccountic 7 hours agoparentprevLast time I heard, some people wanted to replace Vala with Rust. Reason being that the Rust bindings are fairly ergonomic regarding the GTK OO system, plus Rust has a great ecosystem. Relm4 in special is pretty great https://relm4.org/book/stable/ edit: here is some earlier blog post from 2016 https://blogs.gnome.org/despinosa/2016/11/01/rust-and-vala/ but I think there were others since then reply l72 17 hours agoprevI recently wrote an app with vala. It was a pleasant experience. I wanted my app to work well on desktop Linux and my pinephone running postmarketos and phosh. Vala fit the bill nicely. I enjoyed it much more than python and the performance is great. I wanted something that was performant and minimized battery usage since I’d be using it on my pine phone. reply zem 16 hours agoparentI used vala back when I had a Nokia n900 - it really was the perfect thing to write small apps for my own use, and there was a nice cross-compilation toolchain. in all the years of owning an android I have never done that. reply vaylian 16 hours agoprevVala has a sibling language called Genie[1]. Both languages are basically the same, but Vala uses a C#-like syntax while Genie uses significant whitespace like Python. They are both compiled by the same compiler (valac). [1] https://wiki.gnome.org/Projects/Genie reply nu11ptr 15 hours agoparentYep, unfortunately it is a dead language and I believe has been for a while now. Genie = 24 repos: https://github.com/search?q=language%3AGenie%20&type=reposit... Vala = 3.1K repos: https://github.com/search?q=language%3AVala+&type=repositori... reply remon 19 hours agoprevWhat problem/niche is this language a solution/fit for? I've been clicking around for 30 minutes now and have yet to find a defining feature that makes it (significantly) more productive and/or robust than Java or C#. reply sampo 19 hours agoparent> What problem/niche is this language a solution/fit for? Writing software for GNOME. https://en.wikipedia.org/wiki/Vala_(programming_language) https://en.wikipedia.org/wiki/GObject KDE is based on Qt GUI library, and Qt is C++. GNOME is based on GTK GUI library, and they wrote their own object system (GObject) on top of C. Then they wrote Vala, which updates plain C to be more like C# or Java, so it's easier to work with GObject than by using plain C. reply jll29 12 hours agorootparentThanks for formulating that context so well - you should ask the Vala people to start their Web page with your 4 sentences, as most who visit the page will ask themselves. I also liked the itemwise comparison with Java: https://wiki.gnome.org/Projects/Vala/ValaForJavaProgrammers Defining a new language \"just\" for the GTK library will likely be responsible for it not taking off that much, even if - as is the case here - the language looks neat. The fact that it compiles to C, which is universally available, and the fact that it is not associated with corporate interests (unlike Microsoft C#/.net and Oracle Java/JVM) could be interesting, but perhaps decoupling things from GTK could have been wiser to get more reach? I mostly develop software libraries nowadays, and the only GUIs are Webguis to interact with REST services, but for those folks that make the Linux desktop a better place, this seems a good way to be more productive than in C. Speaking of which, the Linux desktop needs a re-think: someone should start and re-develop all apps in a much more clean and homogeneous way so that it is obvious what is done how. The existing bag of tools from different decades, inclusive apps from xfig, xlock, xv over LibreOffice/KWrite to gedit/kedit are just an inconsistent mess; even MS-DOS apps managed (at some point) to stick to standard text menus, from EDIT to TurboPascal. https://www.theregister.com/2024/01/24/rise_and_fall_of_cua/ reply remon 19 hours agorootparentprevAh! That's the bit of context I was lacking. Thanks reply xyst 18 hours agorootparentprevProbably some caveats not shown by using GObject over C++ libs for qt reply icameron 18 hours agoparentprevAbout 10 years ago my team was using Vala for a production embedded system because it would transpile to C and cross compile to target the ARM platform. It was a lot easier to write a couple hundred lines of Vala than implement a bunch of high level language features in C. The Vala became a few thousand lines of hard to grok C as I recall. reply dagelf 19 hours agoparentprevI guess it solves for tiny community centered around a clean language, which it has, as well as Linux Distributions and Desktop Environments mostly written in Vala, like ElementaryOS and Pantheon. Related HN post: https://news.ycombinator.com/item?id=32787251 reply agumonkey 13 hours agorootparentI always assumed that vala was a main reason why elementaryos was so slick. As in freeing people from accidental complexity to spend their neurons on new ideas and optimizations. reply Maken 19 hours agoparentprevVala compiles to native code, no VM or runtime required other than GTK+. reply iso8859-1 18 hours agorootparentJava and C# also support AOT now, and the linker should only link code that is actually used. reply PhilipRoman 18 hours agorootparentThere is a big difference between what Java calls AOT and this. Statically linked hello world in Vala which uses some basic standard library functions is less than a megabyte (most of it taken up by glibc as I couldn't get it to compile with musl). If you assume dynamic linking for core libraries then it is just 15KB. I have no doubt that the Java world has excellent tech behind the AOT support but the footprint is simply not comparable when you can't even construct a java.lang.String without bringing in the entire JapaneseChronology. reply wk_end 18 hours agorootparentprev\"Now\" being the operative word. Vala's quite old; when it came out C# barely existed on Linux. I think these days C# is almost definitely the better choice for whatever you might have used Vala for in the past. reply creshal 17 hours agorootparentC# on Linux still has some warts, the runtime is massive and most distro-packaged software pulls that in its entirety. (And for extra fun, sometimes multiple versions.) All that overhead sits between your code and GTK, and something like Vala completely sidesteps that hassle. That said, it only really shines for the \"gtk based linux desktop\" usecase, for everything else the situation flips around, as you now need to bring your own GTK. Occasionally still worth it embedded, but not much else. reply krylon 14 hours agorootparentThere were a couple of Linux/Un_x desktop applications written in C# back in the day - Tomboy, F-Spot, Banshee, Beagle come to mind. They were pretty good at what they did. But the GNU/Linux camp was rather hesitant to adopt .Net for fear of Microsoft suing people over patent issues. At least that is my memory of it. People wrote replacements for those apps Tomboy -> gNote, F-Spot -> Shotwell, Banshee -> ???, using Vala. Now that Microsoft has made .Net open source, the lawsuit threat seems to be gone (at least for the foreseeable future), but I think the ship has sailed. reply creshal 4 hours agorootparentThere's still/again some C# apps for Linux desktops, like Pinta, but the dotnet sdk experience with it so far has been a lot rockier than with mono back in the days. Performance is great, ergonomics for end users not so much. reply jabl 14 hours agorootparentprevA bit of a shame really. Gnome development could be nicer with a language with some basic memory safety features and GC, but still generates fast code. Like C#. reply Maken 12 hours agorootparentprev> Banshee -> ??? Pretty much everyone reverted back to Rhythmbox, which to this day still feels less polished somehow. reply krylon 12 hours agorootparentI switched to Amarok, then Clementine (Amarok 1.x fork), lately Strawberry (Clementine fork). It's really nice (IMHO). reply krylon 12 hours agorootparentprevEDIT: gNote is written in C++, not Vala. My bad. reply neonsunset 18 hours agorootparentprevThere are quite a few issues that make any JVM language difficult to use in scenarios like this one, in particular, lack of high quality and low overhead interop and certain low level features like C structs. There is also now an option to statically link native libraries into NativeAOT binaries in .NET, and to statically link NAOT-compiled .NET libraries into existing C/C++/Rust binaries. reply aredox 18 hours agoparentprevBeing to gnome what swift is to macOS reply jakjak123 16 hours agoparentprevAre there any good gtk bindings for C# or Java? reply aidenn0 16 hours agorootparentI've had good experiences with GTKSharp. reply novagameco 15 hours agorootparentIt's only available with Mono? reply neonsunset 15 hours agorootparentprevhttps://github.com/gircore/gir.core reply saagarjha 19 hours agoparentprevGTK+ reply flafla2 18 hours agoprevSuperficially this language appears to be very similar to Swift. Beyond the syntax, it also has first class refcounting, C language binding, and no external runtime (compiles straight to binary). I wonder, does Vala have a stable ABI, or native compatibility with other higher-level languages like C++ or ObjC? These are other difficult challenges which Swift attempts to tackle (and depending on who you ask, with varying levels of success). In any case this is an interesting language. Thanks for sharing reply sirwhinesalot 18 hours agoparentVala predates swift by many years. It's meant as a high level implementation of the C GObject system, to which it compiles. reply gwbas1c 18 hours agoparentprev> Superficially this language appears to be very similar to Swift It's almost a clone of C#; it's tightly coupled with the GObject system instead of .Net. reply flafla2 18 hours agorootparentOh got it. So like c# but compiles to native a la Swift? reply bru 18 hours agorootparentThere's more languages than Swift... reply zamalek 17 hours agorootparentDidn't you watch the latest WWDC? Apple invented programming languages. reply neonsunset 18 hours agorootparentprevWhat does .NET JIT emit? What does NativeAOT compile .NET applications to? :) reply pizlonator 15 hours agorootparent(I’ve worked in an AOT for .NET and I’m a JIT expert.) Even if you compile MSIL to native, you’re compiling to the GC’s ABI, which is definitely very different from what Vala and Swift do. reply neonsunset 14 hours agorootparent.NET follows platform calling convention. GC reference assignment to a memory location does involve going through a write barrier (main user of which is concurrent mark phase of GC) but otherwise it's just plain Windows or System V ABI for the respective ISA. Practically speaking, you cannot call .NET methods directly unless they are annotated with [UnmanagedCallersOnly] which is necessary to ensure GC is in consistent state, module initializers have ran, etc. This is a concern for NativeAOT libraries as you don't have to explicitly call their entrypoint before calling them AFAIK. This, however, is true for most languages that are not C. This is also a constraint for both Swift, which has its own reference counting and ABI (Library Evolution ABI) and likely Vala assuming it is reference counted. The runtime vs runtime-less arguments are not exactly helpful given the context - there are \"\"\"runtime\"\"\"-heavy interpreted languages like Python, Elixir or JS, there is Java which assumes JVM, but is already lower level, and then there's .NET, which under the hood looks a lot like a strange flavour of C++ when you e.g. inspect the AOT binaries with Ghidra. Fun fact, native profilers work with NAOT applications transparently on all platforms. You can hit \"sample\" in activity monitor on macOS and it will show you fully symbolicated trace if symbols are available. Just recently, I used Samply to perform multi-threaded profiling and it worked just as well as it would for something written in Rust, if not better. reply pizlonator 12 hours agorootparentSwift and Vala and any other eagerly reference counted language don’t have to worry about native C code squirreling away a reference to a GC’s object and not responding to a GC marking callback (either because the mechanism doesn’t exist or because it isn’t used correctly). That’s an enormous difference in ABI. reply neonsunset 12 hours agorootparentHow would C track references for heap-allocated data originating from (A)RC-based language? I don't think what you say on .NET correlates with reality - you are not supposed to pass managed objects across FFI (you just can't, it won't let you unless you do unsafe cast shenanigans) - the correct way is to either marshal the data or use blittable representation (plain structs can be passed as is, either by reference or value, same goes for arrays/pointers of primitives). On the rare occasion where unmanaged code needs to keep a GC object alive, it is passed via special GCHandle but that's an exception not the rule. Swift has its own high level representation for such objects which are very much not FFI compatible. ARC in Swift is a feature that requires compiler involvement to work correctly, and its structs are not FFI compatible unless they are \"trivial\". Swift also has its own more advanced Swift Library Evolution ABI, which is strictly not C ABI you expect out of your regular library and has its own calling convention and semantics. Overall, there seem to be strange misconceptions about how these languages work so it would be best to check with the documentation first: .NET P/Invoke (FFI): - https://learn.microsoft.com/en-us/dotnet/standard/native-int... - https://learn.microsoft.com/en-us/dotnet/core/deploying/nati... Swift non-C ABI: - https://github.com/apple/swift/blob/main/docs/LibraryEvoluti... - https://www.swift.org/blog/library-evolution/ Swift ARC: - https://docs.swift.org/swift-book/documentation/the-swift-pr... - https://github.com/apple/swift/blob/main/docs/ARCOptimizatio... (I don't actually know if any other other platform, besides Swift itself, implements Swift ABI support, but .NET is going to have it in .NET 9 - likely the first non-Swift platform to do so) reply zamalek 10 hours agorootparent.Net was built for interop (mostly with COM+, but that's close enough to C ABI once you get a function pointer). It's pretty good at making it easy. GCHandle and pinning in general aren't incredibly rare, heck, merely passing a byte array or string to a native function involves pinning. There's also all the heavy lifting it does for you: it doesn't look like you need to pin byte arrays because the JIT does that for you. .Net's safety invariants aren't that hard to uphold either (any more than C). I have lost all love for the platform, but I still have to hand it to Microsoft: no FFI has come anywhere close to .Net in the 25-odd years of its existence. > How would C track references for heap-allocated data originating from (A)RC-based language? Intentionally designed FF interfaces do not at all. C either delegates allocation to the host language, or the host language needs to let C know when it's done with things. I think Lua is an example of the former (it has been a while), the Vulkano crate is a living example of the latter. reply neonsunset 9 hours agorootparentPinning is always done for heap-allocated GC memory, yes. Otherwise fixed statement does nothing for pointers/byrefs that originate from stack and it should be also a no-op for e.g. NativeMemory.Alloc-returned pointers. On the other hand, GCHandles[0] are rare and only ever needed when you have complex object lifetime where, for example, the object reference needs to be passed back from unmanaged and object needs to survive in the meantime. The unmanaged code cannot interact with an object itself because it is not representable and would have arbitrary layout. Today, there is support for multiple calling conventions and ways to interact with FFI. [UnmanagedCallersOnly] exports with NativeAOT-compiled libraries are C exports in the same way they are for the dynamic (or static) libraries compiled with GCC. Various flavours of function pointers have existed for a long time, yes. The most recent one allows to cast pointers directly to the desired unmanaged function signature within unsafe code, or create one given mentioned UnmanagedCallersOnly annotation on a C# method. [0] https://learn.microsoft.com/en-us/dotnet/api/system.runtime.... note specific use case, it's not the bread and butter regular marshaling and pinning are reply pizlonator 11 hours agorootparentprevSwift’s reference counting is exactly what C code on Apple’s platforms were already doing. And Vala’s is exactly what GObject code was already doing. reply neonsunset 11 hours agorootparentThat's not C but Objective-C. From quick skim of the spec[0], it relies on macros and same autorelease pool. There is no free lunch in software engineering, only difficulties with accepting reality. [0] https://clang.llvm.org/docs/AutomaticReferenceCounting.html reply pizlonator 8 hours agorootparentIt's both C and Objective-C. Objective-C's refcounting is exposed to C via CFRetain/CFRelease. Long before there was ARC (the thing you cite), the bulk of the NexTSTEP and then Apple frameworks were written with Objective-C code manually doing [obj retain]/[obj release] and C code manually doing CFRetain(obj)/CFRelease(obj). There was some doc somewhere telling you the protocol. Later, ARC came around to automate it for Objective-C code that opted in. ARC carefully mimicked the semantics of the protocol in that doc. And then later, Swift came along and mimicked the semantics of ARC. There is a free lunch for languages that use reference counting. That free lunch doesn't exist for GC'd languages, which introduce a semantics that is different from the ones that C code already uses. Sorry to burst your bubble. reply neonsunset 8 hours agorootparentIf reading documentation and looking into implementation details doesn't help, I welcome you to measure ARC overhead and observe the numbers with your own eyes at least once, especially in multi-threaded scenario. Might also read one of the previously posted links still, surely something will work eventually. reply pizlonator 6 hours agorootparentI’m not saying ARC is fast. Reference counting is slower than GC if what you want is throughput, unless you somehow manage to do very coarse grained RC, which is what you get in most C++ code (but then it’s far from safe). The GClegacy-C interoperability problem is unavoidable. I’ve been around the block on that problem as someone who writes GCs for a living. It always devolves to some sort of ugliness. Anyway, the fact that Vala uses GObject RC semantics is a point in favor of Vala. Folks who go that route, as Swift also did, should just be honest with themselves about the tradeoff: simpler interop, worse throughput. reply flafla2 18 hours agorootparentprevOh neat I’ve never heard of NativeAOT. I’ve only used Unity’s AOT compiler “Burst” for C# (I assume that is something different?). Cool stuff. edit: Actually I meant to say Unity’s IL2CPP, which transpiles IR to C++. Burst is a different tool with similar goals—it compiles IR straight down to native via LLVM. reply neonsunset 18 hours agorootparentTo be precise, runtime is an umbrella term. Swift, C++ and Rust usually have \"\"\"runtime\"\"\" just as much. Which includes but not limited to: - Automatic or semi-automatic memory management - Threadpool and, optionally, async abstraction implementation - APIs to introspect type system properties / reflection Swift very much has all these. And so does .NET. As for Unity, it has diverged and lags behind \"vanilla\" .NET in features, language versions and performance significantly, so the experience of using it won't translate to what is expected to be \"normal\" C#/.NET of today. reply neonsunset 18 hours agorootparentprevSpeaking of, there is https://github.com/gircore/gir.core reply the_mitsuhiko 18 hours agoparentprevVala is pretty old these days. My understanding is that they built it because Mono became popular but there were patent and licensing concerns with it. reply krylon 14 hours agorootparentThat's how I remember it, too. I still miss F-Spot. reply kmarc 18 hours agoparentprev> compiles straight to binary I checked it ten years ago-ish, I think it transpiles into C (with GObject). Still no runtime though. reply buzzert 17 hours agoprevMy favorite thing about Vala personally is the wide range of native libraries you have access to because of its compatibility with GObjects. There's a great documentation website that has everything located in one place also, which makes development a breeze: https://valadoc.org/ reply pizlonator 15 hours agoparentIs it only pleasant when the library already exposes itself via gobject, or also pleasant if the library is just a C API? reply buzzert 13 hours agorootparentThat's true, but as it turns out there's a lot of examples out there! reply amaccuish 19 hours agoprevIIRC most \"native\" apps in elementaryOS are written in Vala. reply ainar-g 18 hours agoparentWhy the quotation marks? If I recall correctly, Vala compiles (by way of C) into native code. reply iso8859-1 18 hours agorootparentGNOME uses a lot of JavaScript nowadays, so large parts of an application can be interpreted. Here are the GJS docs, it gives you an idea of how much JavaScript there is: https://gjs-docs.gnome.org/ reply nozzlegear 18 hours agorootparentDoes Elementary use Gnome? I thought it was something else. It's been years since I've been on Elementary, but it didn't feel like it was just a Gnome skin iirc. reply tcmart14 16 hours agorootparentTo my understanding, Pantheon is built from scratch, but it is still GTK. So it has a lot of similar fundamentals as GNOME. reply pacifika 16 hours agorootparentprevit uses pantheon reply treyd 18 hours agorootparentprevHaven't they gradually been moving away from gjs though? reply sn41 18 hours agoprevI was very fond of Geary, and looked into Vala because of it. Found it to be an elegant language, and good to get practical programs working in GNOME. I haven't looked at it in the past 7 years. Is Vala still actively adopted? reply ip_addr 17 hours agoprevUsing Rust for GNOME development seems to be becoming more popular. https://gtk-rs.org/ reply jll29 12 hours agoparentRust, as wonderful as it is, may be less suitable for GUI programming since it has no classes/inheritance (which is why Andreas Kling is creating the Jakt language for his SerenityOS project, after coming to the same conclusion after exploring whether to replace C++ by Rust). reply pharrington 18 hours agoprevFor everyone (including myself) asking \"why?\", the Gnome wiki has the programmer friendly answer https://wiki.gnome.org/Projects/Vala/ \"Vala - Compiler Using the GObject Type System\" reply dang 15 hours agoprevRelated: Vala Programming Language - https://news.ycombinator.com/item?id=32113825 - July 2022 (93 comments) Vala Programming Language - https://news.ycombinator.com/item?id=28905761 - Oct 2021 (1 comment) Vala Programming Langauge - https://news.ycombinator.com/item?id=26019680 - Feb 2021 (1 comment) reply Koshkin 13 hours agoparentThe Vala Programming Language - https://news.ycombinator.com/item?id=39528496 - Feb 2024 (no comments :) reply rpncreator 18 hours agoprevLast time I heard about Vala was the Yorba Foundation's (RIP) work with Geary and Shotwell, many many years ago. reply lucraft 11 hours agoprevAhh takes me back! In 2009 I wrote a lib to auto-generate Ruby bindings for Vala code. It gave a very pleasant API to create native extensions for Ruby, if you were happy using glib. https://github.com/danlucraft/valar reply qalmakka 19 hours agoprevVala is a lovely yet criminally underused language. It does a lot of things right IMHO, and it massively simplifies writing GLib/GObject and GTK applications. It's nice to see more awareness around it, honestly. reply freedomben 18 hours agoparentI agree. I have wondered for many years why this is so. I think because Vala was developed so specifically for use with gnome, that its maturation was mainly limited to problems in that domain. I don't think it offered anything that wasn't already mostly available between Java and C#, so it never took off for web. It also wouldn't have fit well as a GUI building language for other native platforms, so it didn't take off there. It's not a good scripting language, so it didn't take off there. In the end, I think it lost due to the network effect. reply skeuomorphism 17 hours agoprevI did a project presentation at university back in 2014/2015 on vala. I remember liking it, but i wasnt yet competent enough to make a verdict. Cool to hear it's still kicking! reply EasyMark 17 hours agoprevFor some app examples: https://github.com/vala-lang/awesome-vala reply account-5 18 hours agoprevI'm sure I read somewhere gnome was moving to flutter, I definitely might be wrong. EDIT: might be getting confused with ubuntu reply popey 18 hours agoparentNo, the Ubuntu community is pushing Flutter, not upstream GNOME. Like the desktop app store, the Ubuntu installer is now written in Flutter. reply skywal_l 19 hours agoprevNot to be confused with Vale[0]. [0] https://vale.dev/ reply khaledh 18 hours agoparentOr Val[0], now called Hylo (for a good reason), or V[1]. [0] https://www.hylo-lang.org [1] https://vlang.io reply Verdex 18 hours agorootparentYeah. I clicked the link thinking, isn't it called hylo now? And then became momentarily confused. At this point I feel something is definitely up. How many V* languages are there floating around out there. Although on the other hand, we have C, C++, C#, and objective C (etc). So maybe this is just something that happened every once and a while through otherwise unsuspicious causes. reply xyst 18 hours agoprevI have tried many boutique programming languages. I think I’ll pass on this one, especially higher level (object oriented) programming languages. I usually end up going back to Java/Scala/C#/python/rust/go. reply zetalemur 13 hours agoprevInteresting default rendering choice for `()` in their demo pretty printer. It renders a space before each opening parenthesis. reply maxloh 19 hours agoprevHeard the language a few years ago, but haven't seen any apps built with it yet. How popular is it within the GNOME community? What are its advantages over more popular garbage-collected programming languages like GoLang? reply rogerbinns 19 hours agoparent> but haven't seen any apps built with it yet. The gitg git gui is written in vala. I've used it for several years because it allows easy staging of individual lines, as well as reverting them. https://github.com/GNOME/gitg/tree/master/gitg reply flexagoon 44 minutes agorootparentDon't you just have a git client in your editor? All popular \"out of the box\" editors include one, and DIY editors like Emacs and Neovim have Magit and Neogit respectively. I find editor git integrations much more comfortable to use than some separate tool. reply all2 17 hours agorootparentprev> because it allows easy staging of individual lines I have been looking for this functionality for literally years at this point. Thank you! I will be trying this. reply funny_falcon 13 hours agorootparent`git gui` and `gitk` my Swiss-knife tools for every day git-ting. They are maintained by git core team. reply rogerbinns 10 hours agorootparentgit gui says unknown command for me. gitk doesn't let you click on individual lines, and revert or add them as far as I could find. reply grumpyprole 3 hours agorootparentGit gui is often in a separate package that you will need to install reply calvinmorrison 14 hours agorootparentprevgit add -p reply rogerbinns 10 hours agorootparentNot a gui, and rather painstaking. With gitg you single click on lines in changes (green added, red removed) which selects them. You can separately click on as many disjoint ones across as many hunks as you want. There is a 'Discard Selection' button which removes the highlighted lines from the code, and a 'Stage Selection' to add those lines to the index. While git add -p is in theory doing something similar, it is a lot more painful to use, and you are still working a hunk at a time instead of just scrolling through changes. I often use this functionality when I've made a number of changes during development and everything is now coherent, especially on a multi-language project. It is then easy to go in and discard lines that were added to debug, and stage lines across multiple commits so they are logically grouped. reply jdiff 18 hours agoparentprevGo is pretty obnoxious to write GTK programs in. Vala is basically a GTK-native language. reply neonsunset 18 hours agoparentprevGo is notoriously difficult to use in a scenario like this, especially when bindings to C# in the form of https://github.com/gircore/gir.core exist (overhead of calling which is very low by nature of interop being a first-class feature since .NET's inception). reply wiseowise 15 hours agoprevVery sleek website, like it. reply colinkiama 14 hours agoparentThanks! reply novagameco 15 hours agoprevI want a flexible native UI framework without it requiring me to use a custom programming language reply jcmontx 17 hours agoprevLooks like java reply swayvil 13 hours agoprevI'm always looking for a simple minimal oop language. Thanks. reply rco8786 19 hours agoprevA \"Vala vs Java\" and/or \"Vala vs C#\" would be helpful reply sandyarmstrong 19 hours agoparentHistorically, it was designed to resemble C#, yet transpile to GNOME-like GObject-based C code, to provide an alternative way for GNOME developers to get higher level language syntax without having to depend on Mono/C# (which was very controversial at the time). So the biggest difference from C# or Java is that there is no VM. It is mostly syntactic sugar over C and GObject. At least, this was the case last I paid attention 15 years ago. reply nlawalker 18 hours agorootparent> it was designed to resemble C# I actually first heard of Vala just a few days ago when I was looking at a C#-related PR[1] for highlight.js: > This fails the tests as the Vala default.txt is recognized now as C#. However, Vala is very close in syntax to C#, and the default.txt also seems to be valid C# so not sure what to do about this. [1] https://github.com/highlightjs/highlight.js/pull/3906 reply lnxg33k1 19 hours agorootparentprevI think I got in a rabbit-hole with it around 2010 for the same reason of liking C# but not liking who backs C#, but I already had enough not marketable languages reply freedomben 18 hours agorootparent> but I already had enough not marketable languages Ha! Ain't that the truth :-) It touches on a big problem for new language development in that there are barriers to entry at getting workable in a new language, but employers don't want to use them unless they can hire for them, and they can't hire for them unless people use them, but people don't use them because they can't get hire'd to use them, rinse and repeat. reply throwaway894345 19 hours agorootparentprevWhen I tried it many years ago it was a really leaky abstraction over GObject and C. If you didn't understand exactly how it worked under the hood, stuff would segfault on you pretty regularly. But I suppose that's par for the course in the GObject universe. reply lnxg33k1 19 hours agoparentprevhttps://wiki.gnome.org/Projects/Vala/ValaForCSharpProgrammer... https://wiki.gnome.org/Projects/Vala/ValaForJavaProgrammers Imagine going to a link called docs and finding the first 2 links are about what you ask for :D reply rco8786 17 hours agorootparentYea I guess I should have been more specific - I was looking for this directly on the main landing site. Maybe I'm alone, but my immediate thought when seeing a new language, with fairly familiar syntax, was \"why is this different than Java\". And if that's my first thought, it probably makes sense to answer that question immediately rather than talk about vague things like Productivity and Open-Source. reply pacifika 16 hours agoprevEvery now and then there’s the “what to use for developing cross platform desktop gui apps”. Well here it is. reply DaSHacka 13 hours agoparentNot qt? In my experience most native linux applications opt for qt over GTK, let alone cross-platform ones. Any reason in particular to use vala over qt? reply fuzztester 17 hours agoprevI saw another one called Vale just today. There are a few HN threads about it. reply fuzztester 10 hours agoparentSome HN monkeyboy jerking off and messing his chaddi again, in a knee-jerk reaction to a comment he can barely parse logically. Downvoting coz doesn't have enuf grey matter to realize dat one can read about Vale outside of HN, and den search hn.algolia.com for vale language and find some hits, and share that info in HN. reply cpersona 19 hours agoprev [–] \"Why choose Vala\" would be helpful. reply lnxg33k1 19 hours agoparenthttps://imgur.com/a/8aEmtDy ? reply beanjuiceII 18 hours agoparentprevvala been around quite a long time too, would be nice if their app showcase showed some more mature apps reply hydral 16 hours agorootparenthttps://vala.dev/ an apps list is here else you have : Pamac-manager is on vala Gnome-Boxes gnome-calculator reply lnxg33k1 18 hours agorootparentprevYeah it would be nice if there was a list with screenshots of applications written in Vala, on the vala homepage, with links to the applications websites :look: reply orthoxerox 18 hours agoparentprev [–] There are very few reasons to. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Vala is an object-oriented programming language seamlessly compatible with GNOME tools like GObject and GTK, offering high-level abstractions and rapid native binary compilation.",
      "It facilitates the utilization of pre-existing C code, making it optimal for crafting GUI apps, command-line tools, and libraries, boasting vibrant support from the open-source community.",
      "Users can delve into Vala projects, engage in community discussions on platforms like Discord and Matrix, with the current version standing at 0.56.13 and continuous development."
    ],
    "commentSummary": [
      "The conversation delves into the Vala Programming Language, its role in creating GTK and other framework applications, notably in projects like Frida and Dino, amid debates on moving towards more portable solutions like Electron and challenges with frameworks like Sciter.",
      "Discussions also touch on alternatives such as Qt and AvaloniaUI, focusing on reducing code size, integrating various GUI toolkits, and comparing memory management and ABI compatibility with languages like Swift, C#, and Java.",
      "Users express both nostalgia for Vala's simplicity in GTK app development and interest in potential upgrades, like GNOME possibly embracing modern technologies like Flutter, highlighting Vala's unique aspects, challenges, and areas for enhancement in desktop app development within the GNOME community."
    ],
    "points": 248,
    "commentCount": 175,
    "retryCount": 0,
    "time": 1712580297
  },
  {
    "id": 39968103,
    "title": "Blocky: DNS Proxy & Ad-Blocker for Local Networks",
    "originLink": "https://0xerr0r.github.io/blocky/",
    "originBody": "Blocky Blocky is a DNS proxy and ad-blocker for the local network written in Go with following features: Features Blocking - Blocking of DNS queries with external lists (Ad-block, malware) and whitelisting Definition of black and white lists per client group (Kids, Smart home devices, etc.) Periodical reload of external black and white lists Regex support Blocking of request domain, response CNAME (deep CNAME inspection) and response IP addresses (against IP lists) Advanced DNS configuration - not just an ad-blocker Custom DNS resolution for certain domain names Conditional forwarding to external DNS server Upstream resolvers can be defined per client group Performance - Improves speed and performance in your network Customizable caching of DNS answers for queries -> improves DNS resolution speed and reduces amount of external DNS queries Prefetching and caching of often used queries Using multiple external resolver simultaneously Low memory footprint Various Protocols - Supports modern DNS protocols DNS over UDP and TCP DNS over HTTPS (aka DoH) DNS over TLS (aka DoT) Security and Privacy - Secure communication Supports modern DNS extensions: DNSSEC, eDNS, ... Free configurable blocking lists - no hidden filtering etc. Provides DoH Endpoint Uses random upstream resolvers from the configuration - increases your privacy through the distribution of your DNS traffic over multiple provider Open source development Blocky does NOT collect any user data, telemetry, statistics etc. Integration - various integration Prometheus metrics Prepared Grafana dashboards (Prometheus and database) Logging of DNS queries per day / per client in CSV format or MySQL/MariaDB/PostgreSQL database - easy to analyze Various REST API endpoints CLI tool Simple configuration - single configuration file in YAML format Simple to maintain Simple to backup Simple installation/configuration - blocky was designed for simple installation Stateless (no database, no temporary files) Docker image with Multi-arch support Single binary Supports x86-64 and ARM architectures -> runs fine on Raspberry PI Community supported Helm chart for k8s deployment Contribution Issues, feature suggestions and pull requests are welcome! Blocky lives on GitHub.",
    "commentLink": "https://news.ycombinator.com/item?id=39968103",
    "commentBody": "Blocky – a DNS proxy and ad-blocker for the local network (0xerr0r.github.io)224 points by kaathewise 23 hours agohidepastfavorite94 comments cj 22 hours ago> Uses random upstream resolvers from the configuration - increases your privacy through the distribution of your DNS traffic over multiple provider The whole project is really interesting but this line caught my eye. For spreading DNS providers, would randomly routing to different ones be more or less private than rotating providers every X minutes? It feels like so many sites request so many different resources that if you make DNS resolution distributed across providers, you might be exposing your \"trail\" to multiple companies at the same time, compared to an alternative approach of switching every X minutes so that any individual company only sees a snapshot of your queries in time rather than your whole journey. reply ignoramous 21 hours agoparentrethinkdns dev here > For spreading DNS providers, would randomly routing to different ones be more or less private than rotating providers every X minutes. Less private for the simple fact that now you'd have to rely on multiple upstream resolvers to respect your privacy. Stick to one; ideally the one with better privacy guarantees like the Mozilla endpoints to Cloudflare DNS. Or, use anonymizing protocols like Oblivious DNS over HTTP and DNSCrypt v3. reply dngray 20 hours agorootparent100% agree. Not sure why you'd randomly play round robin with a bunch of DNS servers. What purpose is this feature even for? If privacy is what you're intending however, DNS is only one part of that and there are other ways in which things can leak https://www.privacyguides.org/en/advanced/dns-overview/ Generally I just recommend to people to use their internal VPN provider's DNS servers and call it a day, or of course Tor. reply nobody9999 13 hours agorootparentI use (and recommend) local recursive resolvers. reply madspindel 2 hours agorootparentIt's unencrypted and exposing your IP. From a privacy perspective it's better to use something like Mullvad DNS over DoT or DoH. reply aesh2Xa1 8 hours agorootparentprev> the Mozilla endpoints to Cloudflare DNS Oh, I did not know that Mozilla had an endpoint on Cloudflare's 1.1.1.1 service, but there it is! https://github.com/mozilla/gecko-dev/blob/c09764753ea40725eb... https://mozilla.cloudflare-dns.com/dns-query Does Mozilla publish their terms somewhere? I'm curious how their endpoint is any different. reply DyslexicAtheist 13 hours agoparentprevnot novel but a standard feature in dnscrypt-proxy reply efitz 16 hours agoprevWhy would I use Blocky vs. Pi-Hole? It seems like a nice project but pi-hole is really mature; I'd love to see a feature comparison or a brag sheet showing what it's better at. reply leipert 2 hours agoparentFor me single binary and single config file is a big one. Much easier to deploy and share configs for example. I have a PiHole running but I kind of dread setting up a second one for redundancy. reply kornhole 5 hours agoparentprevIf you have a PFSense or OPNSense firewall/router, PFblockerNG also does a great job https://docs.netgate.com/pfsense/en/latest/packages/pfblocke.... PFSense comes with a DNS Resolver with option to forward to these other resolvers. It has also too many other features to list especially useful to a homelab. reply adr1an 16 hours agoparentprevIt's a bit faster, yet it lacks a web UI. I prefer it, because I use it directly on the PC/ Laptop via Docker and I like text files more than binding web UIs to ports.. reply password4321 9 hours agoparentprevOr AdGuard Home https://news.ycombinator.com/item?id=39276687 Or NextDNS to let someone else handle it. reply Havoc 22 hours agoprev> Uses random upstream resolvers from the configuration - increases your privacy through the distribution of your DNS traffic over multiple provider Is that the consensus? I thought this would just increase the amount of parties that have insight. eg if today it sends my CNN news reading to cloudflare and tomorrow it sends it to 9999 resolver then that seems worse than sending both to cloudflare. reply girishso 20 hours agoprevCan't block Youtube shorts with these DNS adblockers, I know I can use browser extensions on a computer (and I do), but I really want to block shorts on iOS/Android apps. Tried the squid https proxy rabbit hole, but could not get it to work with mobile devices. Any hints? reply MehdiHK 17 hours agoparentI use revanced for this on Android. Pretty cool, adblocker, sponsor block, customization like removing shorts. It patches YouTube APK, so you retain same UX, no need to use a new app. Edit: forgot to add link - https://revanced.app/ Be aware of the fake ones. reply vwkd 1 hour agoparentprevSideload a tweaked YouTube IPA, like YTLitePlus. reply userbinator 9 hours agoparentprevTried the squid https proxy rabbit hole, but could not get it to work with mobile devices. You'll need to generate your own CA and root certs to install if you're setting up a MITM proxy. reply girishso 7 hours agorootparentI did create a self signed root cert, it works as expected when accessing net from my Mac. But mobile devices refuse to work, zero network requests succeed. I did trust the root cert on both mac and mobile. ¯\\_(ツ)_/¯ reply kiicia 14 hours agoparentprevI stopped using yt app on iOS and instead use browser (safari) with addons reply tremarley 20 hours agoparentprevThere are YouTube clones for iOS & Android that have Adblock, Shorts Block & Sponsor Blocks reply shadowpho 16 hours agorootparentWhich ones for iOS? reply ecliptik 10 hours agorootparentYattee https://github.com/yattee/yattee it requires either public or private Piped or Invidious instaces [1][2]. 1. https://github.com/TeamPiped/Piped/wiki/Instances 2. https://docs.invidious.io/instances/ reply imhoguy 20 hours agoparentprevNewPipe for Android has no shorts reply UberFly 14 hours agorootparentCame to say this. NewPipe is great. Superior to the default android YouTube app in many ways. reply a_subsystem 18 hours agoparentprevI use Orion on ios. reply that_guy_iain 20 hours agoparentprevJust don't go to the shorts section? Or are you talking about the ads in the shorts section? reply animuchan 2 hours agorootparentShorts plague the search. Short clips are always completely irrelevant to anything I can possibly be looking for, while not entirely insane, anyway. reply moe_sc 18 hours agorootparentprevThe shorts section on mobile is kinda inbetween all other videos. If you don't pay attention you can end up on one quite easily... And then it's back to doom scrolling reply poisonborz 22 hours agoprevMy problem with network-level adblockers, also PiHole is that they break a lot of services (yeah it's mostly sloppy or malicious intent from service provider but still). On a browser you can quickly disable uBlock for that site, it is much more tedious for these services - also because it's not even clear that the filtering is causing the problem, also because it maybe someone else on the network experiencing the problem. reply theideaofcoffee 17 hours agoparentI have a handful of different wifi SSIDs set up on my network at home to help with this, some route their DNS queries through a pihole instance and others (say, without the “AB” for ‘adblock’ suffix on their ssid names), don’t. Each ssid is their own vlan and each has their own dhcp listening that doles out the pihole instance address on the Adblock-enabled nets. It’s easy enough to just connect to a different ssid if I see anything breaking, but it’s rare enough that I keep it connected to the pihole/adblock network at all times. Works super well. reply jakjak123 14 hours agorootparentThat is a great idea. What do you use to run multiple wifis with separate vlan? reply JackGreyhat 12 hours agorootparentAn AP or wlan router that supports vlan to ssid mapping. reply theideaofcoffee 10 hours agorootparentprevI use some of the lower-end wifi 6 APs, coupled with an AC (controller), from fs.com, they work solidly, and with seamless roaming around the house, after figuring out some of the rather odd translation in the AC web interface. After some annoying small steps, the ssh consoles on both the AC and the APs are all that I use. They’re mated to a Juniper EX switch running stock JunOS over PoE, primarily one AP for each floor and another one or two to fill in dead spots. I haven’t had to touch it since I’ve set it up, just keeps chugging along. Slight over-kill for the environment, but I’m so damn tired of using bad wifi I just wanted to do it right! reply ai_what 21 hours agoparentprevI have a silly \"trick\" for this. In Firefox you can add a SOCKS5 proxy and click \"Proxy DNS with SOCKS5 proxy\". This bypasses the system DNS. So for example, if you make a container with this, then you can just quickly open the URL that's blocked in the other container and it will bypass the network-level DNS adblock. There are other ways to do it without a container, I'm sure, maybe with an add-on/toggle or something. My VPN provider gives free SOCKS5 access to a few servers, so it didn't cost me anything more. reply belthesar 18 hours agorootparentThat's a clever trick. It's also pretty trivial to set up a SOCKS5 proxy with Shadowsocks if it's desirable to keep that DNS resolution local. reply ranger_danger 17 hours agorootparentprevHow are you using a different proxy per container tab? I've tried FoxyProxy but it leaked DNS requests through the local network resolver even with SOCKS5 proxies. reply AwaAwa 15 hours agorootparentGo into the main extension settings of 'Firefox Multi-Account Containers' that lists your containers and click on 'Manage containers'. Select any container and the last option on the new page is 'Advanced Proxy Settings'. This setting is per container. reply cyberpunk 22 hours agoparentprevI’ve been using https://oisd.nl/ on my network for some time (just cronned into an unbound config file) and nothing breaks. If you report a website breaking to the maintainer, he removes the offending block. Works well, I can recommend it. And it reminds my to send them a little money in thanks. reply UberFly 14 hours agorootparentThis looks really good. Thanks for suggesting it. reply illnewsthat 20 hours agoparentprevI created a bit.ly link that points to http://yourpiholehost/admin/api.php?disable=300&auth=api_key Bookmark it for myself and other people in the house and then turn it off for five mins whenever there is an issue. reply ceejayoz 17 hours agorootparentWould be fun to hook this up to one of those IoT Amazon buttons. reply xp84 17 hours agorootparentHome Assistant can definitely do this. I just now successfully got my HA connected to an Aqara Zigbee button (which, hilariously, only briefly functioned when I was using it with Aqara's garbage hub). reply efitz 16 hours agoparentprevPiHole has an API that can be used to enable and disable the ad blocker. I have a bunch of home automation set up, and through the use of HomeBridge and a plug-in I have a button in my Apple Home app on my iPhone to enable or disable the ad blocker. Since it's exposed as a smart home thing, you could hook it up to a voice assistant like Siri or Alexa. I built an integration once for an Elgato StreamDeck. You can also download apps that do the same thing; I have one called \"Pi-Hole Remote\" that works great. Yes, ad blocker blockers are annoying, but they are trivially worked around. reply qwertimus 19 hours agoparentprevI've had the most luck with OISD as the blocklist; others have the exact problems you describe (in fact all other blocklists I've tested have had frequent issues). Since changing, I've had maybe 2 sites that didn't play nice with the filtering. These issues are not inherent to network-level blockers, but the configuration of those blockers. reply thangngoc89 22 hours agoparentprevWhat annoyed me the most is actually clicking on ads from Google Search. Sometime I searched a product and I couldn't clicked on the first result because they are advertisements. reply Spare_account 22 hours agorootparentEvery time threads like this come up I am reminded that Google has ads. I don't see them, presumably because of uBlock Origin. I'm typing this response not to smugly boast, but because it's a lead in to the question that your comment raised within me: Are you using Pihole to block ads at a network level, but not also using a browser extension to block them at the client? reply quickslowdown 22 hours agorootparentI'm in the same boat as you, but taking it a step further, I'm completely blind to the first 1/3rd of google search results. The first 3 are already ads, and then there's the \"quick info\" card on most searches. I've subconsciously trained myself to just flat out skip over those results. So in my case, I don't know that an ad blocker really even helps me on google, because I'm ignoring those first results anyway. reply maicro 20 hours agorootparentYup, skip all sponsored and ad content. I do the same on Amazon and ebay - even when the advertised product is _exactly_ what I want, and the best price available, I refuse to purchase from a sponsored listing. reply quickslowdown 15 hours agorootparent100%. reply nkozyra 16 hours agorootparentprev> Are you using Pihole to block ads at a network level, but not also using a browser extension to block them at the client? A lot of Pihole users don't bother with browser extensions. In extension-only use cases, these results would just not show up. With Pihole, you have to copy and paste the URL and just enter directly in the browser. It's not like a huge roadblock. reply muppetman 15 hours agorootparentprevAds exist on mobile devices. In iOS games. All sorts of places/devices that can't run uBlock origin. That's why you use a DNS level blocker as well. reply scopeh 21 hours agorootparentprevpersonally i use both. PiHole and uBlock. But PiHole is network wide so smart TVs iPads, etc in the house also get the benefit. reply ceejayoz 20 hours agorootparentprevHonestly, I love this aspect of my Pihole. Stops Google stealing those pennies from the businesses who were already gonna be the #1 result anyways. reply xp84 17 hours agorootparentThe \"sponsored ads on your own name\" extortion racket really is the worst. reply martin_a 22 hours agoparentprevIs that your experience? Honest question, because only yesterday, after several years with PiHole and uBlock Origin, I found that a Shopify shop wasn't working for me. But that's probably the only issue I encountered after all those years... reply dorianh 19 hours agorootparentUnfortunately yes, there are features on big websites which just don't work. Now every time, I have an issue with any website, my first instinct is to turn off pi-hole. Most of the time, pi-hole is not the even the issue, but sometimes it is. It's annoying to browse the internet while constantly thinking \"Maybe there is an issue on my side\". reply dspillett 16 hours agorootparent> Unfortunately yes, there are features on big websites which just don't work. It is very rare that I find something stalker-blocking (pihole on my local network & VPN) causes to break that I care enough to turn blocking off for. It might have happened as little as twice, one of those occurrences predating PiHole. Information is usually in many other places or I really don't care that much, and shopping sites that break are waving a red flag by being broken so I'll go look elsewhere. I do have a bookmarklet on my devices to turn it off for a few minutes, but that was used more when testing it then it has been since. I mostly live alone so don't have the problem of other users, such as a spouse or kids, having trouble. Guests always have the option of using their normal mobile access instead of the local wireless if they experience insurmountable problems. reply dorianh 15 hours agorootparentThat's good to hear. I'm going to try oisd, hoping for fewer false positive :). reply davitocan 21 hours agorootparentprevHome Depot search breaks for me when using Blocky, and is the only consistent issue I observe. reply Marsymars 19 hours agorootparentHome Depot is the most fragile website I'm forced to deal with. It regularly breaks in novel ways for me when it can't load some random dependency that it doesn't actually need the functionality from. reply radicality 18 hours agorootparentAnd it doesn’t load at all from outside the US! I once wanted to purchase before coming back, to learn I need to vpn back in to the US to order something. Crazy. reply userbinator 9 hours agorootparentThank the GDPR for that, probably. reply jokethrowaway 22 hours agorootparentprevNot parent but yes, that's exactly why I stopped using PiHole reply Dalewyn 22 hours agorootparentprevI've come across a few borking issues, including online banking because card rewards are counted as advertising in the list(s) I use. Such problems are few and far between, though, and it wasn't that hard to figure out what to whitelist (granted I'm a computer nerd, not everyone is). reply nirav72 20 hours agoparentprevI use both Pihole and ublock. While ublock is fine for desktop browser , pihole is useful for mobile devices, as well as blocking access for devices and apps that tend to be chatty with data it sends out. For example - I unfortunately bought a few Eufy security cams long before it was found that Eufy was sending user data out to its AWS instance. It was easy to block that access via AWS in Pihole. Of course, it doesn't always work. I've found some devices absolutely need to call home before they'll function. reply squaresmile 20 hours agoparentprevI use a quick setting to quickly switch between ad blocking dns and normal dns for bad websites or captive portal. https://f-droid.org/en/packages/com.jpwolfso.privdnsqt/ reply avel 18 hours agorootparentPretty cool. If you have adguard home and google assistant, you have access to a switch that can enable/disable adguard home protection. So with the homeassistant app you can add that as a quick tile as well. The only caveat is that this will disable it across all devices. reply avel 18 hours agoparentprevIt's not that tedious. In adguard home you have a switch on/off in the web UI. You can also expose that switch to homeassistant. And there are some other good ideas in this thread as well for android and ios. reply zzyzxd 17 hours agorootparentAfter disabling it on the UI, your device may still cache the DNS records for a few minutes. There's no immediate feedback on whether disabling adblocking changed anything. It is extremely tedious especially for non-technical users, and adds \"tech-support\" burden to their technical friend/family who set it up in the first place. reply pnw 15 hours agoparentprevPi-hole Remote for iOS has a disable for X minutes feature which is invaluable when you do run into one of these sites. I use it maybe once a week. https://apps.apple.com/nl/app/pi-hole-remote/id1515445551?l=... reply apexalpha 16 hours agoparentprevYes, I ran into these issues when I installed PiHole in my family home. Most issues were with Google Ads inside Google Search. Often these are relevant and actually what you're looking for. But they don't work. I tried rewriting the \"this is blocked\" page that PiHole would serve so it included a button to temporarily disable blocking for said url but it turned out to be harder than I thought. reply lencastre 22 hours agoparentprevPi-Hole Remote (iOS app) is your friend reply nirav72 20 hours agorootparentYep. There are also browser extensions that will allow pausing pihole blocking temporarily. or simply these URLs: Disable URL : http:///admin/api.php?disable&auth=[your pihole password] Enable URL : http:///admin/api.php?enable&auth=[your pihole password] Disable for [X] Seconds: http:///admin/api.php?disable=[X]&auth=[your pihole password] reply derpymcderpface 22 hours agoparentprevYeah, this is always my hurdle implementing house wide. I can toggle it on and off when I have the stubborn link but my girlfriend just gets utterly annoyed. I just manually point some of my devices at my local server and leave it at that. Guess I should look into aws free tier and set it up there as well or just say screw it and use dns.adguard.com again. reply _ache_ 22 hours agoprevIt's a DNS proxy, like pi-hole, but it seems a lot more powerful than Pi-hole that is basically just a configured dnsmasq server to be user-friendly. https://docs.pi-hole.net/ reply thangngoc89 22 hours agoparentUsually pi-hole is installed together with Unbound [1] for the function of Blocky. [1] https://docs.pi-hole.net/guides/dns/unbound reply mrbluecoat 19 hours agoprev> Blocking of ... response IP addresses (against IP lists) So blocky can block IP addresses? If so it's more powerful than traditional DNS blockers like Pi-Hole and AdGuard Home. > Logging of DNS queries per day / per client in CSV format or MySQL/MariaDB/PostgreSQL database May want to include a time series database, like InfluxDB For those needing layer 7 control, https://github.com/andybalholm/redwood is a nice Go option. reply belthesar 18 hours agoparent> May want to include a time series database, like InfluxDB Seems to have native support for Prometheus, so that seems to be the TSDB to use for the project. That said, if you're at the point where your record density takes advantage of the benefits of a time series DB vs a well indexed RDBMS, I'd also imagine that you're beyond the scope of this little service. reply intuxikated 22 hours agoprevwhy would I use this over something like Adguard Home? reply figmert 22 hours agoparentOne thing I like about AdGuard Home is that it supports normal AdGuard's block list, similar to the ones used in browsers. Of course, it ignores the items that it is unable to block (e.g. cosmetic, or third-party etc), but it is nice being able to take the lists I use in uBlock Origin, and just feeding it into AdGuard Home. reply zikduruqe 21 hours agorootparentAlso AdGuardhome will use optimistic caching, which is great. https://github.com/AdguardTeam/AdGuardHome/discussions/4002#... reply _ache_ 22 hours agoparentprevVery interesting project too ! https://adguard.com/adguard-home.html reply jedisct1 22 hours agoparentprevOr dnscrypt-proxy, especially if you care about not disclosing your IP address to resolvers. reply different_base 21 hours agorootparentI run dnscrypt-proxy on my OpenWrt Router. It's like Syncthing. Install and forget, like how softwares should work in general. reply beaugunderson 17 hours agoprevsuper useful for being able to use cloudflare dns but still resolve the archive.* domains using a different resolver (because archive.* blocks cloudflare for ideological reasons): conditional: mapping: archive.is: 8.8.8.8 archive.today: 8.8.8.8 archive.md: 8.8.8.8 archive.ph: 8.8.8.8 reply silverwind 14 hours agoparenthere's the same in dnsmasq: server=/archive.today/8.8.8.8 server=/archive.ph/8.8.8.8 server=/archive.is/8.8.8.8 server=/archive.li/8.8.8.8 server=/archive.vn/8.8.8.8 server=/archive.fo/8.8.8.8 server=/archive.md/8.8.8.8 server=/archive.to/8.8.8.8 reply breiti 9 hours agorootparentWhich is what pi-hole uses if anyone is curious. reply teamspirit 16 hours agoparentprevIs that why? I’ve been having this problem intermittently for years now and never understood it. Wow. reply Cody-99 16 hours agorootparentThe CEO/cofounder of cloudflare has written about this issue here on HN https://news.ycombinator.com/item?id=19828702 reply mikl 21 hours agoprevThe amount of effort that goes into blocking ads and tracking really says a lot about how messed up the web is these days. reply MrksHfmn 22 hours agoprevi have stopped using local dns resolvers. too many pitfalls, dns leaks or the dns resolver is down and the devices can't resolve the addresses. I have simply set https://dnsforge.de in my router. reply ahmetozer 20 hours agoprevWhat is benefits when it is compared to dnsmasq and hostblock list ? reply epstein 19 hours agoprev [–] No for windows? reply himurae 18 hours agoparent [–] Blocky is best adblocker its lightweight unlike adguard just a simple yml file its dns queries are faster than adguard imo i run it as a container on VYOS (best router software imo) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Blocky Blocky is a DNS proxy and ad-blocker designed for local networks, developed in Go, offering features like filtering DNS queries with external lists, individualized black and white lists for each client group, advanced DNS setup choices, support for multiple protocols, robust security and privacy measures, integration capabilities, and a user-friendly interface.",
      "The platform prioritizes simplicity, performance, and community assistance, ensuring no user data collection and providing open-source code.",
      "Blocky Blocky operates with a strong emphasis on user privacy, making it a reliable choice for managing DNS queries on local networks."
    ],
    "commentSummary": [
      "Discussion focuses on DNS blocking tools like Blocky, Pi-hole, AdGuard Home, and dnscrypt-proxy for boosting internet security and privacy.",
      "Users talk about ad-blocking techniques such as network-level blockers, browser extensions, and adblock-enabled YouTube alternatives.",
      "Challenges arise with some websites malfunctioning due to ad-blocking, but solutions like Pi-hole Remote and browser extensions offer temporary disablement options, showcasing the pros and cons of using various DNS blocking tools for enhanced internet browsing and network security."
    ],
    "points": 224,
    "commentCount": 94,
    "retryCount": 0,
    "time": 1712570868
  },
  {
    "id": 39971673,
    "title": "Android's Find My Device: Locate Devices and Items with Ease",
    "originLink": "https://blog.google/products/android/android-find-my-device/",
    "originBody": "Android 5 ways to use the new Find My Device on Android Apr 08, 2024 2 min read Share Twitter Facebook LinkedIn Mail Copy link The new Find My Device experience helps you easily find your misplaced Android devices and other belongings. Erik Kay VP of Engineering Share Twitter Facebook LinkedIn Mail Copy link Today, the all-new Find My Device is rolling out to Android devices around the world, starting in the U.S. and Canada. With a new, crowdsourced network of over a billion Android devices, Find My Device can help you find your misplaced Android devices and everyday items quickly and securely. Here are five ways you can try it out. 1. Locate offline devices Locate your compatible Android phone and tablet by ringing them or viewing their location on a map in the app — even when they’re offline. And thanks to specialized Pixel hardware, Pixel 8 and 8 Pro owners will also be able to find their devices if they’re powered off or the battery is dead. 2. Keep track of everyday items with compatible Bluetooth tags Starting in May, you’ll be able to locate everyday items like your keys, wallet or luggage with Bluetooth tracker tags from Chipolo and Pebblebee in the Find My Device app. These tags, built specifically for the Find My Device network, will be compatible with unknown tracker alerts across Android and iOS to help protect you from unwanted tracking. Keep an eye out later this year for additional Bluetooth tags from eufy, Jio, Motorola and more. 3. Find nearby items Sometimes what we’re looking for is right under our noses. If you're close to your lost device but need a little extra help tracking it down, a “Find nearby” button will appear to help you figure out exactly where it’s hiding. You’ll also be able to use this to find everyday items, like your wallet or keys, when Bluetooth tags launch in May. 4. Pinpoint devices at home with Nest More often than not, we lose everyday items like our keys or phone right at home. So the Find My Device app now shows a lost device’s proximity to your home Nest devices, giving you an easy reference point. 5. Share accessories with friends and family Share an accessory so everyone can keep an eye on it in the app. For instance, share your house key with your roommate, the TV remote with your friend or luggage with a travel buddy so you can easily divide and conquer if something goes missing. Find My Device is secure by default and private by design. Multi-layered protections built into the Find My Device network help keep you safe and your personal information private, while keeping you in control of the devices connected to the Find My Device network. This includes end-to-end encryption of location data as well as aggregated device location reporting, a first-of-its-kind safety feature that provides additional protection against unwanted tracking back to a home or private location. Read more about how our multi-layered protections for the Find My Device network work. The new Find My Device works with devices running Android 9+. Learn more about Find My Device-compatible devices and all the ways the Find My Device network can help you find a lost device or item. And look out for software updates coming to headphones from JBL, Sony and others, which will join the Find My Device network soon. POSTED IN: Android",
    "commentLink": "https://news.ycombinator.com/item?id=39971673",
    "commentBody": "Find My Device on Android (blog.google)207 points by el_duderino 16 hours agohidepastfavorite276 comments justinmarsan 1 minute agoDoes anyone know of a good way to have an Android phone ring from another Android phone, without sharing all account access ? My wife keeps her phone on silent but loses it multiple times a day. We use the computer to make it ring using Find my Device, but it'd be a lot easier if I could just do it from my phone, which I can if I install the app and then add her Google account to the accounts on my phone, but then I get access (and notified of) all her emails, google photos and stuff like that. Is there an alternative ? Some way to just have both phones agree to sync somehow and then one can make the other ring, regardless of sound settings. reply el_duderino 16 hours agoprevGoogle posted another blog on how they built the new Find My Device network: https://security.googleblog.com/2024/04/find-my-device-netwo... reply ryukoposting 8 hours agoparentAs a chronic loser of keys and wallets, I recognize the user need here. However, this is a prime example of a piece of software that can only be trusted when an open-source implementation is provided. Google and their customers don't have much to gain through a proprietary implementation. If anything, open-sourcing Find My Device Network would allow third party devices to join the network, enhancing its value to Google's customers. Google emphasizes ad nauseum that this system is extremely private and secure, so third party implementations shouldn't be cause for worry. Governments and various potentially dangerous organizations stand to benefit greatly from a closed implementation for reasons that hardly need explaining. Google makes a lot of lofty claims about E2EE and privacy considerations in that blog post, I'd just like to see them prove it. reply rickdeckard 2 hours agorootparentBeside of open-source, this whole use-case should be built as an industry standard, not as a ammunition for an adjacent platform war. With these few huge tech-companies, the industry has lost its natural \"feature\" that required competing players to work together and form an Alliance/SIG to make really big things happen. Instead, the really big companies are happy to take the fruits from that time (Wi-Fi, Bluetooth, NFC,...) and build something proprietary on top instead of contributing back to it... reply sirsinsalot 33 minutes agorootparentprevGovernments already have the means to position your phone, all the time, whenever they wish. They don't need google for that. It's built right into the phone network. reply mikae1 5 hours agorootparentprev> this is a prime example of a piece of software that can only be trusted when an open-source implementation is provided. Try FMD. https://f-droid.org/packages/de.nulide.findmydevice/ reply md_ 1 hour agorootparentBut that doesn't use the \"find my device\" netowrk. I think the parent wasn't saying, \"I want an app that continually reports its location to my server so I can monitor my phone's location.\" Indeed, that's fairly trivial to build, but is useless if, say, your phone doesn't have internet access (like, someone turns it off or it runs out of battery). The thing Google is announcing here is like the Apple \"find my\" network--it seems to allow you to use other people's devices to find your lost device simply based on a BLE ping. That is something that is hard to build by yourself, and would benefit greatly from an industry-wide standard (more peer devices reporting locations!). reply ranguna 1 hour agorootparent> your phone doesn't have internet access > simply based on a BLE ping What if they disable Bluetooth? The linked open source find my device app uses cell network to both receive commands and for location. reply md_ 1 hour agorootparentOn Apple's devices, you cannot disable that BLE broadcast. The same is true on the Pixel 8: https://www.theverge.com/2024/4/8/24123909/google-pixel-8-pr.... That's the whole point of this design, and fundamentally different than just having some app ping a server once in a while so long as it has network access. reply extraduder_ire 1 hour agorootparentEven if you turn on airplane mode on the device? reply md_ 1 hour agorootparentBeats me. I can't find anything definitive on this. Since Apple devices continue to broadcast Find My signals even when powered off (as long as they have a little bit of battery left), I assume they continue to do so in airplane mode. It wouldn't do a lot of good if thieves could just turn off Bluetooth, right? reply extraduder_ire 1 hour agorootparentprevThis only works on the device itself, and while it can contact the internet or SMS. It's more akin to a self-hosted version of device locating features pre-airtag. I think the real utility of tile/airtag/google-find-my-device is the network of other colluding devices listening for bluetooth pings and reporting on the location. Not least because apple (maybe google, don't know) forces your device to send reports if you want to use the feature. Heck, on modern phone OSs, you can't use BLE in apps without turning on the location service. reply KennyBlanken 3 hours agorootparentprevApple extensively describes how \"Find My\" works: https://support.apple.com/guide/security/find-my-security-se... reply phkahler 5 hours agorootparentprevWould an open source version allow anyone to track anyone? The ultimate stalker app... reply cookiengineer 5 hours agorootparent> Would an open source version allow anyone to track anyone? The ultimate stalker app... An open source version would make absolute no difference on what is trackable. That's the point: transparency is necessary with critical data like this. reply ssl-3 4 hours agorootparentOpen-source software would allow us to see how that published software handles our data, but it would not allow us to see how the cumulative sum of that data is handled after it is passed over to a central body. Unless, of course: That centralized data store were also open -- perhaps even by using something like DNS -- but then, anyone of sufficient skill would be able to craft an application to see where others' things are. (Unless... E2E encryption for location data, so it can only be understood by those who generate it? Hmm.) reply cookiengineer 1 hour agorootparentAn open source backend that requires end to end encryption on a per-user-storage basis, while not sharing those keys in the open source client (reference implementation) could effectively prevent third parties from seeing the data. Something like shamir's secret sharing with split up keys effectively making the encrypted data useless, or based on time frames so you can only track the last \"epoche\" (like the last 24 hours, maybe?) similar to how HOTP/TOTP works could work nicely I'd imagine. At Tholian (my company) we're using team based keys where at least 2 of x keys (the elected lead and co lead) in the team must co-sign a data changing action. It's peer to peer, meaning those peers can find each other and co-sign this without our servers having to store any transaction queue that could be compromised. This way we prevent abuse, and if the feds come knocking on our doors, our users stay protected because without those keys we cannot see what's going on in any of the registered teams. That's how I think it should be like, for any web service that supposedly keeps their privacy promises. If they don't do this, their promises were likely lies and once the root keys or the databases are compromised there's no turning back. Looking at you, cross-tenant keys at Azure. Everyone knows you were and are still lying about the security aspects. reply ddalex 16 hours agoprevI love my Tile(s), but since the AirTag and now Android networks now feature the same capability, I feel their model is dead in the water since they don't own the platform and can't compete on the installed base. They should have sold out (edit: to a mobile platform company, either Google or Apple) while they had the chance. reply jsheard 16 hours agoparentThey did sell out, they were acquired by Life360 a while ago. edit: I don't think Google would have had much to gain by acquiring Tile, the actual technology is very simple, their moat was just the fact that they had the largest install base of tracking apps on Android. Google could, and did, just rug-pull them by pushing their own network out to every Android device with Play Services (so nearly all of them). The only thing Tile really has going for it now is that their trackers can be used with both Android and iOS simultaneously, if you have devices in both ecosystems. reply accrual 16 hours agoparentprevAgreed. I bought a couple Tiles before AirTags were available. They worked fine, but AirTags are just better in every way. reply mattmaroon 6 hours agorootparentI have both and I prefer the Tiles. 99.9% of the time the network doesn’t matter because the item isn’t lost, I just don’t remember where I left it, and it’s either in range or wherever Tile shows the last known location to be. And even if it is lost, Tile app remembers it as being at the last place where I had it and my phone, which is practically always where I left it. Airtag surely has better coverage, since like 50% of phones are helping, but the one time I actually lost something Tile found it. Someone had moved it so the location must have been uploaded by another Tile user. I use my Tiles daily to find my AirPods or a key set, I’ve used it once in years to find that (my Airpods case) when it fell out of my pocket in a park. Tile is louder and easier to hear. Tile has several form factors, Airtag only has one, and honestly, any tile form factor is better than Airtags. I swear they picked the worst possible shape for it, it just has to make whatever you put it in bulge. You have to buy some sort of case to even use it most of the time, whereas a Tile can pop directly onto your key ring. Airtag can’t ring my phone. I do that frequently. I’ve used both extensively, I’ll keep buying Tiles. Or maybe one of these newer ones. I’d go so far as to say Airtag is only better at finding something that’s been stolen, and that’s just a tiny percent of the use cases. Tile is better in every other way. reply sunnybeetroot 9 hours agorootparentprevSame for me, and AirTags don’t push a subscription service down my throat with spammy notifications. reply gghffguhvc 9 hours agorootparentprevI find Tiles easier to locate in a room. I think the lower quality sound or frequencies used are easier to locate - especially if under something. reply sleepybrett 14 hours agorootparentprevone way they aren't better. You have the tile but can't find your damn phone. Sure there is 'find my' but the little button on the tile that causes the phone to chime was actually pretty handy. I imagine apple removed it because tags are not paired to phones (like tiles) but instead to the whole account. If you were to push the theoretical button on the airtag your entire house would probably set to chiming (apple tv, homepod, all the phones and ipads on the account all the macs...) reply filoleg 9 hours agorootparent> If you were to push the theoretical button on the airtag your entire house would probably set to chiming (apple tv, homepod, all the phones and ipads on the account all the macs...) There is a simple solution to this: if you press the hypothetical AirTag button, trigger the closest iPhone attached to your account, and then you can manually pick any specific device in Find My as usual from the iPhone. reply saurik 16 hours agorootparentprevAirTags only come in a single size and shape that only makes sense for larger container objects. Tile makes a flat credit-card sized variant that you can slide in your wallet, and the adapters required to put an AirTag on a keychain is pretty ridiculous in comparison to the tiny variants you can get from Tile with a built-in hole for this purpose. (Also: AFAIK the \"large\"--not really, in comparison to the AirTag + adapter, lol--Tile Pro variants have a noticeably longer range than AirTags.) Honestly, the only thing, to me, missing from Tile is the direction arrow finder, which is certainly useful, and yet... the speakers on the Tile are much much better so it just doesn't matter so much? (BTW: it is extremely stupid that that arrow only works in two directions. I waste so much time following the arrow only to realize I need to go upstairs -- I live in a two story apartment with a bedroom and an office above my kitchen and living room: it isn't larger than my friends's apartments, but it is really nice having two floors -- that I am only doing because the sound isn't loud enough on AirTags.) Frankly, the whole argument for AirTags being inherently better is merely that they have a \"bigger network\", but for most of my objects I am not scavenging around town looking for then, nor am I ever going to be chasing some thief around the city in my car looking to steal it back, hoping to catch up to them before they disable or throw out the tracker... that vigilante justice idea associated with this product category is just a crazy pipe dream. Instead, I am pretty much always just trying to figure out which pair of pants I left my wallet in today, or on which shelf I left my keys: I would actually be much happier with a purely local product that only responded to encrypted pings from my personal phone and only even did that when I actively ask it to (as despite using both Tile and AirTags for years now the \"you left your item behind\" notifications have only happened before I myself noticed once or twice in a way which was helpful... it just doesn't ping often enough, with either product, to be maximally useful). And looking at the replies to this, I have learned AirTags are actually missing what is to me a glorious feature of Tile: that I can push a button on my keys and find my phone in reverse. I hadn't noticed this lack before as I am only using AirTags in large objects I don't keep on my person (which makes their notifications really annoying as the UI fails to eat old ones even when I walk back to my object... this seems very fixable), but I don't want to have to buy an Apple Watch just to have a way to find my phone :/. reply thebruce87m 15 hours agorootparentWhile technically true, there are non-AirTag third-party devices you can buy that still work with Find My: https://www.macrumors.com/guide/find-my-network-accessory-pr... Both variants you list are supported by Chipolo: https://chipolo.net/en/products/category/chipolo-spot reply saurik 15 hours agorootparentThat's cool! I am looking at this and seeing another issue with AirTag: I actively use the feature on Tile where I can use the Tile on my keychain to find my phone by pushing the button, and I guess AirTags is entirely missing that feature. reply lotsofpulp 15 hours agorootparentYou're supposed to use your Apple Watch to find your iPhone. reply xp84 10 hours agorootparentAnd if you misplace your Apple Watch®, grab your Apple Vision Pro® to look for the Apple Watch® in Augmented Reality! reply shuckles 7 hours agorootparentYou make fun of it, yet the ecosystem is so popular and well integrated that the US Department of Justice is suing them over it. reply idle_zealot 3 hours agorootparentThe issue is not that they have a well-integrated ecosystem, it's that they build features into their products that only their other products are allowed to use. There would be no need for action if Apple designed a popular ecosystem that all of your devices could participate in. reply worthless-trash 4 hours agorootparentprevThat's apple DOJ soon. reply sleepybrett 14 hours agorootparentprevsomething i initially missed as well but like .. if you pushed the button on your airtag it would probably start all your apple devices to beeping. The tag is associated with your account, not your phone like with tile. reply Dylan16807 2 hours agorootparentDifferent behavior would be very easy to program. reply KennyBlanken 3 hours agorootparentprevMore expensive and the batteries are non-replaceable seems like a double fail. reply thebruce87m 3 hours agorootparentThe circular one has a replaceable battery https://support.chipolo.net/hc/en-us/articles/360020095718-H... reply madeofpalk 16 hours agorootparentprevKind of. Apple's Find My network is open to third party accessory makers. Here's a card-shaped one https://chipolo.net/en/products/chipolo-card-spot I don't believe third party accessories support the compass-style precision finding though. reply saurik 15 hours agorootparentThanks! For the card-shaped one I might actually try that one out and switch. I actually was just adding a thing to my comment about how the location finding thing is certainly useful but not as good as a loud speaker as it doesn't support a Z axis for some silly reason, so I don't really care. But, honestly, the flat versions of the Tile have an issue where the speaker wires can get broken slightly by flexing in the wallet (I took one apart to diagnose this once) and so I could see trying a competitor, as this is the same thickness!! reply dpkirchner 15 hours agorootparentprevFWIW I've had success with this AirTag card thing: https://www.thingiverse.com/thing:4849497 . It's flat enough to fit in my wallet without distorting it too much. reply ianburrell 15 hours agorootparentprevAirTags really should have a hole on the edge. Then could attach a lanyard for keychain or strap without needing an adapter. reply jsheard 15 hours agorootparentprevUnfortunately the wallet trackers are compromised by the fact that most nice wallets come with RFID (and BLE) blocking linings nowadays... Hang onto your old RF-transparent wallet, because RFID blocking has become a standard bullet-point feature they all have to have now. reply ssl-3 4 hours agorootparentThe only time I've used RF-blocking gear for privacy or security was when I had a little Faraday pouch for my old flip phone a million years ago[0]. It was nice-enough case for my phone and it primarily kept the phone from getting beat up by other pocket-stuff that would happen in a workday. But it also had two compartments: One was shielded all around by conductive fabric and this made a fairly good Faraday cage, and for the other one layer of that conductive fabric would be pushed aside so that the Faraday cage was incomplete (and thus largely non-functional). It gave me a choice, and that choice worked well. I think I paid $3 for it back in the day from DealExtreme (which was a lot like Temu is today, except shipping normally took months instead of days or weeks). Is choice not a thing in modern RFID-blocking wallets? [0: There was a time when my company felt it was useful to track people with their company-provided cell phones. I didn't mind that during the workday, so much, but we were also encouraged to use these as our personal phones if we chose to, and strongly encouraged to keep them with us 24/7 so we would be reachable. And I didn't mind being reachable most or all of the time and I certainly enjoyed not having my own cell phone bill back then, but I did not like being tracked 24/7: Sometimes, I wanted to do my own private shit on my own time without being snooped on. I could tell the tracking app to exit, and that did work, but doing so sent a notification to my completely stupid, abusive manager -- who would variously either call to ask why I did that, or just silently turn it back on remotely (which the system permitted). And I did not (and do not) think that, as a professional, I needed to excuse my choice to have privacy at any given time. (Given my own role in the company I had access to this tracking data as well, but I'm not the abusive type.) So, the solution was simple: Normally, the phone was in the non-shielded part of the case. And when I dropped it into the completely-surrounded Faraday portion instead, the phone just simply dropped off and silently stopped providing updates. This worked 100%, and while I refer to them as separate compartments to aid in functional clarity, the design of this case didn't really take up any more space than any other phone-case would have.] reply saurik 12 hours agorootparentprevI had found what I consider to be the perfect wallet and while they did add an RFID lining it was one I could remove. I have bought multiple of them as backups and likely will purchase a few more ;P. reply msthomaa 6 hours agorootparentWhich wallet? Thanks! reply xp84 10 hours agorootparentprevInsane that this seems to be getting downvoted (I assume? It's faded to light gray) just because you are daring to question the superiority of the Apple Solution. reply kiwijamo 8 hours agorootparentOne of the most informative post, downvoted. I guess the traditional HN crowd has moved on and the Apple(TM) crowd is its replacement. reply foooorsyth 16 hours agorootparentprevAirTags are not better in one very important way: they don't prevent theft. They will notify thieves that they are hidden in a valuable item that they are stealing if the thief carries an iPhone. Tile has a feature where you can submit your info and sign some sort of extra contract stating that you won't use your Tile for stalking/illegal activity, and you can actually hide a Tile in your bike frame and it can be used to find your bike if it goes missing. AirTags are useless for this because Apple doesn't want them used for stalking. reply joe5150 15 hours agorootparentHow do hidden trackers \"prevent\" theft? If a thief steals your bike and is notified that it's being tracked, they're probably much more likely to abandon it somewhere, at which point you can recover it. This seems like a safer bet than trying to hunt them down yourself. (It should go without saying the police are not going to go get your bicycle back for you even if you do hand them the GPS coordinates.) reply foooorsyth 12 hours agorootparentIt’s a massive deterrence. LoJack has been in business since 1986 for a reason. Also, the attitude of “don’t go and get your stuff back yourself, it’s not safe” is just castrated cowardice and only emboldens thieves. Thieves should be afraid of both law enforcement and vigilante response. Better society when both are on the table. reply ssl-3 4 hours agorootparentA hidden device can't prevent theft. It takes visibility to be a deterrent. For instance: One or more well-hidden surveillance cameras cannot ever deter anyone from doing anything. However: One or more cameras placed out in the open can do so. (They don't even have to be functional cameras to be a deterrence.) LoJack isn't a deterrence. It is instead a way to react after a theft has already occured. The AirTag, meanwhile: It also can't prevent or deter the theft -- if a thief is going to steal a thing, they're going to steal it whether it has an AirTag hidden somewhere or not. And when an iPhone-using thief is alerted to the presence of that AirTag tracker, they have options beyond just ditching the bike: They can make the AirTag make noise, and also use Precision Finding to draw an arrow on the screen of their phone that helps them to locate the AirTag so it can be destroyed. (This is a deliberate anti-stalking feature on the part of Apple. Tracking stolen goods is fundamentally incompatible with anti-stalking measures: There cannot be both things in the same ecosystem, since if a person can silently track their [stolen?] bicycle, then they can also silently track a living human being.) reply joe5150 11 hours agorootparentprevLoJack stays in business mainly by selling their product to dealers who then demand auto buyers pay for it whether they want it or not. reply lupire 12 hours agorootparentprevHow does an anti-theft tech that know one knows is there deter anything? Cars are frequently stolen, as you may have noticed. reply foooorsyth 12 hours agorootparentThieves will begin to expect it to be there some of the time, after sufficient police/vigilante response. And perhaps it will be hard to find. Hence, deterrence. Cars are stolen far less often than they would be otherwise partially due to technology like LoJack. Did you grow up in the US? Steering wheel locks used to be on every other parked car. Immobilizers and LoJack made them a thing you buy for your weekend classic car only. reply kergonath 2 hours agorootparentprev> castrated cowardice You lose credibility right there. No need for such bullshit it you actually have a point. Performative machismo is completely useless. > Thieves should be afraid of both law enforcement and vigilante response. Better society when both are on the table. lol no. Actually, what the hell are you thinking? Mob justice is not justice at all. What happens when someone decides you are a thief (or rapist, or murderer) just because they don’t like the look of you or would like to have your car very much? This does not frighten thieves; this frightens people other people don’t like. I mean, we’ve had plenty of examples in recent history. I would take the risk of losing my bike every now and then instead of the risk of being lynched any day. reply aabhay 16 hours agorootparentprevIf you hide the tag in a way that it can’t be removed (locked to the bike etc) then what option does the thief have? They might actually decide not to steal the item! reply spogbiper 14 hours agorootparenthulk smash. hulk defeat security reply worthless-trash 4 hours agorootparentNot sure hulk know which part to smash! Hulk not smart because smart hulk not steal. reply sleepybrett 14 hours agorootparentprevcertainly removing the battery from the tag or just smashing it with the bolt cutters you are probably already using to steal the bike... reply linsomniac 6 hours agoparentprevI had a number of Tiles over the years and really wanted to like them, but really didn't. I lose stuff infrequently enough that the battery in the tiles would die 2-10 times before I was likely to lose my keys/wallet. The TV remote, the tile was something my son (the primary loser) would fiddle with constantly and destroy. I carried a dead Tile Pro on my keys probably 95% of the time I carried it. The one place I'd really like a tile is on my Pixel Buds, but a Tile is a significant %age of the size of the case, and again not something I need super frequently. reply avree 5 hours agorootparentDo the Pixel Buds not have a Find feature? I have the same experience as you with Tile, but I use the \"Find\" feature on my iPhone to locate my AirPods all the time. reply mattsmart 4 hours agorootparentYes, Find My Device was activated on my pixel last night and had an entry for my pixel buds. reply causi 16 hours agoparentprevAs someone who owns about eight of them, I don't see how you could love Tile. They have a ridiculous profit margin and the company likes to engage in asshole practices like making it so getting a notification when you lose connection with a tile is a fucking monthly subscription. Companies that charge for things which cost them literally zero dollars deserve to burn to the ground. reply prmoustache 1 hour agorootparentHmmm if all that is true why do you have so many of them? reply Rebelgecko 10 hours agorootparentprevLast time I checked their parent company's financials, they made almost as much revenue from selling to data brokers as they did from hardware sales reply bigiain 9 hours agorootparentAnd these are all under the control of the worlds largest surveillance capitalism advertising company... reply hn_throwaway_99 10 hours agoparentprev> I feel their model is dead in the water since they don't own the platform and can't compete on the installed base. Yeah, this functionality is the ultimate \"network effect\" model. While my most common use case for Tile is walking around my house with my phone to see where I've misplaced my keys, the other (and more advertised) case is that if I lose my keys while out and about, I can get notified of their location if anyone else with the Tile app installed is near my keys. The iOS and Android platforms have a huge advantage because they don't depend on having any app installed - that previous sentence just becomes \"I can get notified of their location if anyone else with an iOS device (for AirTags) or Android device (for this new functionality) is near my keys.\" That's huge, because it means there are fewer places that won't have anyone with those phones. And now that Android has this functionality, I see more and more people switching to it from Tile, which means the Tile network will get smaller and be even less useful for the Tile users who remain. reply bigiain 9 hours agorootparent> if anyone else with an iOS device (for AirTags) or Android device (for this new functionality) is near my keys. I wonder just how quickly this'll roll out over the Android devices in the wild? It needs Android 9+ it seems, which is about 80-85% of the devices from memory. I wonder if it also needs vendor updates? How quickly will Samsung/Huawei et al provide Android updates with this feature? (And will vendors be able to choose to block it?) reply jsheard 9 hours agorootparentIt's rolling out as part of Google Play Services, which is updated out of band from Android itself so just about every device should get it without any action from the manufacturer, aside from the small minority of devices running de-Googled versions of Android. reply tjbiddle 10 hours agoparentprevThought the same; finally bought a Tile a few months ago. Glad I decided to buy only one to test it out rather than go all-in. reply JansjoFromIkea 16 hours agoparentprevYeah I can't really see where they go from here, which is a big shame personally because I got a bunch of very cheap battery replaceable ones years back that are still going strong. Also got a four pack of long dead gen 2s that were pretty easy to cut apart, rejig for replaceable batteries and embed into a few items. reply robotnikman 16 hours agoparentprevSo Airtags work with android now? reply jsheard 16 hours agorootparentNo, the AirTag ecosystem and Google Find My ecosystem are still separate things. Google and Apple did cooperate to standardize stalker detection though, so your iPhone will alert you if you have an unknown Google tag following you, or vice versa if your Android phone notices an unknown AirTag. reply lupire 12 hours agorootparentLow hanging fruit for EU antitrust enforcement. reply fy20 8 hours agoprevIs there a technical reason why this can't work with Apple's Find My network, or is it just that they don't want to work together? Using the \"Unknown Trackers\" feature my Samsung phone can scan for nearby AirTags and tell them to play sounds. reply fkkffdddd 1 hour agoparentIt’s actually two separate aspects, there’s the network for crowdsourced finding, and there’s the localisation of unwanted trackers. The feature that lets you ring nearby trackers is actually standardised between Apple, Google, Samsung and the others. It goes by DULT - Detection of Unwanted Location Trackers. The crowdsource networks are not compatible. reply bevekspldnw 8 hours agoparentprevTypical Apple hiding behind the walled garden to avoid actual competition via users not having any path to switching platforms. The Apple value prop used to be “it all works together”, now it’s “half the shit you spent thousands on won’t work if you leave the island”. The Tim Cook era is great for stock holders, but he’s one of the prime enshittifiers in Silicon Valley. reply pzo 2 hours agorootparent> The Tim Cook era is great for stock holders Not anymore, since 2022 apple stock was mostly flat and today is even below January 2022. Vision Pro is a flop as today (maybe future iteration will change that) - just look at google trends. Hopefully Apple change the course or Tim Cook get replaced - especially if they don't show anything meaningful related to AI/ML at this WWDC. reply HumblyTossed 7 hours agorootparentprevAs much as I hate defending them, Apple does license out \"Find My\" tech, so is this really their fault? reply bevekspldnw 7 hours agorootparentYes, but the entire MFi program is just a consumer tax paid to Apple. It’s an extortion racket. reply GaryNumanVevo 2 hours agorootparentIs it a \"walled garden\" or is it an \"extortion racket\" then? reply cbeach 2 hours agorootparentprevIf we were to force all new technologies to be interoperable with all technologies from all possible competitors we’d create a Kafkaesque nightmare for innovators and particularly early stage startups. reply prmoustache 1 hour agorootparentShouldn't they just all publish the RFCs and actually implement the techs the way they are written in the RFCs? reply fsflover 1 hour agorootparentprevLike, you know, the Internet? reply cbeach 1 hour agorootparentIndeed. Good example. If Cerf, Kahn and others had been forced to make the internet (as we know it today) compatible with with nascent competitors such as X.25, Cyclades, OGAS and EIN, it would have compromised the design process with a “too many chefs” problem. Also would have forced the Internet to a lowest common denominator of capability. reply InsomniacL 32 minutes agorootparentIn the IOT/Home Automation device world, Zigbee and Z-Wave were competing standards. Z-Wave was a proprietary standard and have a device certification process. Zigbee was a open-source IEEE standard. Both had their pros and cons but weren't interoperable. Manufactures have agreed to create and move to a new standard \"Matter\" > https://en.wikipedia.org/wiki/Matter_(standard) I don't see why this success storey couldn't happen for the 'lost device world' or most other proprietary non-interoperable systems. (why can't i send a message from Whatsapp to Messenger) reply ReptileMan 6 minutes agoprev>Locate your compatible Android phone and tablet by ringing them or viewing their location on a map in the app — even when they’re offline. And thanks to specialized Pixel hardware, Pixel 8 and 8 Pro owners will also be able to find their devices if they’re powered off or the battery is dead. Then they are not really offline or off are they? Bring back removable batteries in the phones. reply AlexanderTheGr8 11 hours agoprevDoes anyone have a comparison of Airtag vs \"Bluetooth tracker tags from Chipolo and Pebblebee\"? I imagine Airtag is superior due to non-reliance on Bluetooth? Can Chipolo/Pebblebee tap into the Android network without Bluetooth? And is their tech comparable to Apple's work? reply WinstonSmith84 4 hours agoparentI can't really see how Airtag can be superior in most places around the world: it's simple, there are many more Android users than iPhone users so the Google network can only be better (if not right now, then over time, give it 12 months...?) This difference is even stronger when you leave EU + US and go to Asia, Central / South America, Africa where iPhone users are not less, but very rare. My use case is clearly to track my hand luggage, wallet, backpack etc. when I travel and I'm a nomad. Gonna wait on reviews but I'm very excited! reply NoahKAndrews 8 hours agoparentprevAirTag relies on Bluetooth as well, right? It certainly doesn't have a cellular connection. reply bdcravens 8 hours agorootparentIn addition to BLE, AirTags use UWB. I assume this is how AirTags can provide fairly high-fidelity directional guidance, something that wasn't clear the new set of Android tags provide. reply WinstonSmith84 47 minutes agorootparentMight not be an Android tag limitation rather than a smartphone limitation. Few android smartphones offer UWB, but maybe this will change reply nikeee 15 hours agoprevThis is not compatible with AirTags in any way, no? Wouldn't this be a great opportunity for a common standard? Or is there one? reply lxgr 10 hours agoparentOf course it isn't. The time of interoperable standards in consumer devices is long past. Bluetooth used to support file transfers from device to device on feature- and early smartphones no matter the brand; now there's AirDrop (and maybe sometimes Google Nearby, but only if the humidity and lunar phase is just right). In home audio streaming, there was once DLNA; now there's AirPlay and Google Cast. We have two mutually incompatible walled gardens, and while life within them is admittedly nicer than in a world of poorly-implemented standards, the walls are getting higher by the day. reply sunnybeetroot 9 hours agorootparentGood call out, it’s incredible we went from local file transfers with Bluetooth to needing an internet connection to transfer to a person sitting next to you just because they have a different OS. If only the EU could pressure the big folks to adopt one wireless transfer protocol like it did USB C recently with Apple. reply lxgr 8 hours agorootparentOh, my (optimistic, but I think not unrealistic) read of the DMA is that this is the case already! There are clauses in there that mandate gatekeepers to give access to all kinds of interfaces only accessible to the native OS or first-party apps. I'd expect AWDL/AirDrop to match the type of thing the EU wants Apple to open up quite nicely. If Google had any interest in shaking the duopoly, they'd of course be first to request Apple to open up AirDrop access to Android. Realistically we'll hopefully see at least some third-party app vendor that bridges the gap with something that doesn't need an install on both iOS and Android, since in many AirDrop scenarios people don't have time, knowledge, or a network connection to install some unknown third-party app. reply Shawnj2 9 hours agorootparentprevBluetooth file transfer was always nearly useless in my experience, airdrop was somewhat revolutionary in that it actually worked reliably most of the time reply prmoustache 1 hour agorootparentI would say bluetooth file transfer has always been reliably slow. So good enough for small files, not for larges. But much more reliable than pairing and connecting/reconnecting to audio devices for example. reply kiwijamo 8 hours agorootparentprevAirDrop has its issues too. Used to work in educaiton at a school with 100+ iPads within reception range. Quite common to have A-to-B drops work but not the reverse direction i.e. B-to-A. Solution was often to ask another student C and use their iPad as a proxy i.e. B-to-C-to-A to get something from B to A. Quite fustrating given it would be an issue one day and not the next and what device it appeared on was totally random too. The same device B would send/receive fine to other devices but just not to A, and A itself would send/receive file to others but just not to B! No rhyme or reason to it, and of course being Apple no way to properly debug it. reply worthless-trash 4 hours agorootparentApples Bluetooth stack Has had problems for quite a while. reply sambazi 2 hours agorootparentmaybe that's why the volume controls on my 1st-gen se no longer apply when using a bt-speaker. granted it's dated hardware on both ends but it used to work fine for 5y+ reply KennyBlanken 3 hours agorootparentprevNo, it doesn't - and clearly you've never tried to work on Android bluetooth. reply i5-2520M 6 hours agorootparentprevBluetooth was always insanely reliable for me for file transfers even on phones and PCs that were a decade apart in manufacturing date. It was slow, but for a lot of things that was fine, and it was the obly way to get data from some really old phones. reply lxgr 8 hours agorootparentprevFor me, reliability has been similar. I will say that it is much faster, though (no wonder – hundreds of Mbit/s over 802.11 vs. 2 Mbit/s over Bluetooth EDR). UX is much better too. But would it kill them to just support both? I’d take sometimes janky, always slow transfers to Android over having to WhatsApp photos to somebody literally next to me any day. reply kelnos 13 hours agoparentprevOh sure, it would be a great opportunity, but the idea of Apple voluntarily interoperating with anyone else on most things is just funny to think about. Google isn't significantly better on that metric, of course, but I feel like it's often in Google's business interests to interoperate more often than it is in Apple's. reply shuckles 7 hours agorootparentApple's Find My spec has been available for 3rd party adopters for years. reply kuschku 36 minutes agorootparentOnly for third party tags, not phone apps, though? reply TulliusCicero 4 hours agorootparentprevWhat? Google is generally a lot better on interoperability compared to Apple. Just look at which company has their apps on which platforms. reply MBCook 13 hours agorootparentprevYep. They never adopted or pushed USB-C (it was on laptops for years), Thunderbolt, SCSI, USB, Ethernet, HDMI, WiFi, Bluetooth, FireWire… reply lightbritefight 12 hours agorootparentIt's unclear if you are being sarcastic or not. Apple adopted USB C on their phones because the EU compelled them to. They tend to oppose standards they aren't already using. reply MBCook 11 hours agorootparentBull. That’s why I put the comment in parentheses. They had already switched their laptops to all USB-C many years ago. And iPad Pros. I think the iPad Air and iPad (no name) switched as well. There were also strong rumblings Apple was going to release the USB-C iPhones when the rule was passed. > They tend to oppose standards they aren't already using. You’re kidding right? Then explain USB. Or Thunderbolt. Of every other thing I listed above. Apple didn’t invent any of them and open them. They were all existing standards. reply lxgr 10 hours agorootparentSo you're saying that's what they wanted to do all along, it just took them almost ten years, and it just happened to neatly coincide with the EU regulation as well? Sure, and they're now allowing game streaming apps and retro emulators on iOS because that's what they always knew was best for the world anyway. What a coincidence (with the DMA, in that case)! > Apple didn’t invent any of them and open them. They were all existing standards. Counterpoint: Magsafe. They had USB-C and went back to something proprietary. Another counterpoint: Thunderbolt wasn't an open standard until very recently, and I can only imagine that Intel gave Apple some heavy discounts on the controller chips used (or even their main CPUs) to push the standard. Apple doesn't always hate standards and interoperability, but they will absolutely try to push their proprietary protocols and interfaces whenever it's in their business interest. reply MBCook 9 hours agorootparentUSB-C didn’t exist when they went to Lightning, it wasn’t an option. The fact all their other products were moving seems to indicate they’d move to USB-C on the iPhone as well. Rumors had them working on it for years. Now maybe it would have come out this year and not last. MagSafe: they added something back. You can still charge with USB-C. Works fine, I do it. Thunderbolt: that wasn’t Apple. Intel invented it and Apple put it to use. I have no idea if it’s open or closed, that’s my fault. What I meant was it wasn’t an Apple invention. Besides, what else had that kind of bandwidth at the time in a cable? It’s not like there was some common better thing they shunned. reply sunnybeetroot 8 hours agorootparentprevThey have both MagSafe and USB C now, and MagSafe is much better as a charging adapter anyway. Maybe a USB C MagSafe could be invented? reply scarface_74 9 hours agorootparentprev> Counterpoint: Magsafe. They had USB-C and went back to something proprietary. They added MagSafe and still kept USB C charging. > So you're saying that's what they wanted to do all along, it just took them almost ten years, and it just happened to neatly coincide with the EU regulation as well? They had already starting to move iPads to USB C reply lxgr 8 hours agorootparent> They had already starting to move iPads to USB C Which they're explicitly selling as laptop alternatives, at least the Pro line. People were already connecting all kinds of things (audio interfaces, mice, ethernet adapters etc.) to iPads using the hilariously named lignting-to-USB-host \"camera adapter\", and all of that is just better over USB-C. On the iPhone, the vast majority of people only use the port for charging and maybe listening to music; the few additional iPhone sold to people that actually use them with external storage for ProRes cinematography probably pales in comparison to the lost revenue from MFI license fees. reply scarface_74 7 hours agorootparentWhen the camera adapter was first released, usb-c didn’t exist. And how much do you think Apple really made on MFI licenses as a percentage of revenue? It was a rounding error and many of the knocks off people bought from Amazon weren’t even licensed. reply kiwijamo 8 hours agorootparentprevExcatly right. They switched to USB-C only on certain classes of devices and even then only over a incredibly long period of time when they could have moved all of their devices over to USB-C fairly quickly given their vertical integration advantages. It's interesting how my non-Apple ecosystem has me using USB-C across all devices across multiple manufacturrs but my Apple friends have multiple cables/chargers/etc to accomodate Apple's approach of using different standards across their own product lines. Thankfully EU has made this a thing of the past but if it wasn't for EU there'd be no change or it would have happened years from now. reply accrual 15 hours agoparentprevI don't know the answer but I agree this tech should be standardized. Over time there will be a massive network of phones and tags capable of finding each other, but it's silly to bifurcate it such that you might not find your device because nobody on the \"correct\" network is nearby. reply lxgr 10 hours agorootparentCouldn't agree more – just like it's quite silly to not be able to \"AirDrop\" a photo to an Android device, even though Bluetooth could do it just fine 20 years ago from a Nokia to an Ericsson or Motorola. reply FateOfNations 13 hours agoparentprevApple and Google have agreed on a standard for the anti-stalking functionality. Everything else is still separate. reply MBCook 14 hours agoprevI’m really surprised this took 3 years after AirTags. Especially since AirTags have been rumored to launch for a very long time before that, and it was a surprise to people that they seemed to be done and just not available (IIRC). reply neurostimulant 14 hours agoparentGoogle decided to delay the release until apple updated their unknown tracker alert to support google's bluetooth tracking devices. Not sure why, but it took one year for apple to implement the support. reply MBCook 14 hours agorootparentSeems odd. Unless that was going to require hardware changes I don’t see why you’d need to delay. That still leaves 2 years past Apple’s long rumored debut. reply kelnos 13 hours agorootparentAnti-competitive behavior is why. Apple had no need to update their unknown tracker alert software on any particular timescale. And if Google was willing to wait until Apple made that update, Apple could certainly drag their feet and delay Google's launch for a while. Presumably Google would have, eventually, said \"ok, enough is enough, we're releasing on X date whether or not you've updated yet\". And maybe that's indeed what happened! At that point Apple really would want to finish that update, since not doing so could be a privacy hole for their customers. (The same privacy hole Android users had to deal with in the early days of AirTags when we couldn't detect them.) reply MBCook 13 hours agorootparentApple not updating Apple software is why Google couldn’t release a Google product for Google phones? reply lxgr 11 hours agorootparentYeah, that's the narrative I'm seeing in many news outlets, but it doesn't make sense to me. Did Apple wait for Google to release an anti-stalking feature detecting AirTags before releasing them? reply i5-2520M 6 hours agorootparentNo, and there was a whole lot of negative press/safety concerns due to letting these easy to use trackers out in the wild. reply wolverine876 15 hours agoprev> thanks to specialized Pixel hardware, Pixel 8 and 8 Pro owners will also be able to find their devices if they’re powered off or the battery is dead. This suggests there is separate hardware with its own power and network capabilities, designed for location tracking, designed to be always on. Is there a way to disable it? reply flotzam 15 hours agoparentFWIW, \"It won't be supported by GrapheneOS. (...) Bluetooth still has to be configured and enabled by the OS to make it work that way.\" https://discuss.grapheneos.org/d/11520-android-find-my-devic... reply tivert 14 hours agorootparentThe article linked there has a few more details about the implementation: https://www.androidpolice.com/android-15-powered-off-finding...: > To solve this problem, Google is working on a “Powered Off Finding” feature that allows a device to store precomputed Bluetooth beacons in the memory of the Bluetooth controller. This means that even when a device is powered off, it can continue broadcasting Bluetooth beacons to nearby devices. > Unfortunately, Powered Off Finding isn’t the kind of feature that can just be enabled on any device. This is because the device needs to have hardware support for powering the Bluetooth controller when the rest of the components are shut down. Plus, device makers need to put in some extra engineering work to support this feature. For example, they need to support the Bluetooth Finder hardware abstraction layer (HAL) so that the Android OS can enable Powered Off Finding mode and send those precomputed Bluetooth beacons that I mentioned. I don't know how that could work if the battery is dead, unless they're letting the Bluetooth controller draw on some reserve battery capacity after the phone has powered itself down. reply nradov 14 hours agorootparentThere is mostly dead and then there is all dead. A battery which is unable to supply enough power to keep the entire phone operating can likely supply enough for just the Bluetooth chip. Of course, the resulting deep discharge may damage the battery chemistry. reply nolist_policy 14 hours agorootparentI'm pretty sure phones shut down long before they reach 0%, keeping plenty of charge for the Bluetooth beacon. The battery protection circuit will for sure prevent deep discharging by cutting the power. reply lupire 12 hours agorootparentprevWe've discovered the mythical \"unlimited\". Unlimited Bluetooth power :-) reply alephxyz 15 hours agoparentprevPutting my tinfoil hat on, this explains why Google was okay adding a toggle to location access for apps on Android. They don't need your device to send them their location anymore, they'd only need you to be in range of an UWB or BLE device. reply jiberius 15 hours agorootparentiOS-style runtime permissions (for things like location access) were introduced in Android 6.0, which was released back in 2015. That's quite the long play. reply kelnos 13 hours agorootparentprevI'm all for assuming bad intent when it comes to Google and, but this doesn't really make sense. Google owns the OS; if they want your location, they don't need to care about per-app location permissions. reply Aerbil313 12 hours agorootparent> Google owns the OS Google is still beholden to consumer privacy laws. I’m sure if you turn off location data in your Google account they won’t track you. The guys in the Room 641A will. reply jasonfarnon 10 hours agorootparent\"Google has agreed to pay nearly $392 million in a settlement with 40 states over allegations that the company tracked people through their devices after location tracking had been turned off, a coalition of state prosecutors announced on Monday.\" reply fsflover 13 hours agorootparentprev> Google owns the OS Unless you install something else. reply DaSHacka 13 hours agorootparentAt which point how does what GP said matter? If you install something else, it doesn't matter what toggles exist in stock android.... reply fsflover 13 hours agorootparentDid I misunderstood that the Bluetooth device could exchange some data on its own when the phone is off? reply arcastroe 7 hours agoparentprevDoes this have any implications for \"Airplane mode\". Or the requirement to turn off your cell phone during takeoff. reply explore 13 hours agoparentprevDoes anyone know if the baseband chip communication with the tower is disabled in airplane mode? reply non415556 7 hours agorootparentI've measured with an external, unconnected RF monitor. The cell phone shows clear activity when not in airplane mode, and then zero activity when in airplane mode. There's my anecdata... reply mazsa 40 minutes agorootparentI was told: \"I suspect that in airplane mode it would only communicate at the specific request of the system (and perhaps in spread spectrum mode), so [before baseband is activated from outside] it would not be worth measuring.\" What do you think? reply encom 15 hours agoparentprevI was considering a Pixel 8a when it's released, now I'm not so sure. Not really keen on a mobile botnet. reply nolist_policy 14 hours agorootparentIMHO if you cary around anything that contains a cellular modem you're fair game anyways. reply TheSkyHasEyes 14 hours agorootparentThe if here is some phones can be turned off and not transmit anything, at all. reply bigiain 9 hours agorootparentI have a Pinephone, which has hardware switches to turn off potential privacy destroying features. But it's even more secure than that - them made it so it's such a poor phone that I never managed to get it into use as a daily driver phone for more than a day or two at a time! The most private device is one you never bother carrying! reply nolist_policy 14 hours agorootparentprevModems are usually directly connected to the battery rail, because (so the saying goes) they have high peak power consumption. reply Aerbil313 12 hours agorootparentI once built a cell phone from scratch. They do have high peak power consumption, and I did connect it to the battery rail directly, along with some capacitors to handle the surges. reply fsflover 13 hours agorootparentprevOn my phone, the modem can be killed with a hardware kill switch. reply nolist_policy 13 hours agorootparentWhich one? I have a Pinephone and it is a great device. It's just not a good phone. reply fsflover 13 hours agorootparentLibrem 5. reply Scene_Cast2 16 hours agoprevWould anyone have details on how Pixels can be found even when out of battery? reply kelnos 13 hours agoparent\"Out of battery\" isn't really what we think it means. The phone shuts down (and refuses to start back up) when it still has quite a bit of battery left. If it truly allowed the battery to go flat, that would damage the battery, and you'd have a really hard time charging it again. The amount of juice your entire phone needs to run for, say, a half hour can probably power just the bluetooth LE chipset for occasional wakeups for beacon-sending for... weeks? Months? (Completely made up time spans, but the BTLE chipset really does need a tiny amount of power for infrequent wakesups compared to the entire phone.) Certainly this can't last forever: if your phone is missing for long enough, the battery will eventually run down to a point where the battery controller won't let even the BTLE chipset draw any more power. This definitely requires hardware, firmware, and software support in order to set up, which is why only Pixel 8 is supported so far. Clearly when they were designing the Pixel 8 hardware, they already knew this feature was in the pipeline, and wanted to be able to launch with a supported phone. reply lxgr 11 hours agorootparentThat's how the iPhone does it as well, btw! As far as I remember there's a separate microcontroller that controls \"Find my phone\" beacons without having to boot up iOS on the main application processor. reply bigiain 9 hours agorootparentprevThe CR2032 that AirTags run on for over a year has about 200mAhr of capacity. That's down under 5% of a typical 5000mAhr phone battery. So 1% of the contingency low battery capacity could probably run the BLE stuff for a few months if needed. reply nirava 7 hours agorootparentBut also keep in mind that lipo batteries in phones self-discharge tens of times faster than CR2032s. reply bnc319 15 hours agoparentprevI'm guessing this works exactly the same way as Apple's \"Findable After Power Off\" feature [1]. If the phone \"dies\" the battery still has some reserve to display the \"charging needed\" screen when the power button is pushed, along with sending location. Additionally, it can use the Find My network [2] where other devices that are powered on can relay the proximity device's location using their own location. 1. https://www.theverge.com/22697218/iphone-apple-ios-15-find-m... 2. https://developer.apple.com/find-my/ reply CobrastanJorji 16 hours agoparentprevSame question here. I have to assume this would either work by the device sending out a last gasp \"I'm about to die\" GPS location call home (for finding it when it's far away), or else there's a mostly passive RFID-like thing in the phone that makes it findable by nearby devices (for finding it when it's somewhere in the house/car/office with you). reply wolverine876 15 hours agorootparentHow much energy would be needed to power a minimal wireless network node, of some sufficient capability, for let's say 1 day? Any IoT devs out there? reply ianburrell 15 hours agorootparentIf it is using a similar technology to AirTags, then it could broadcast signal for a long time on remaining battery. Even Bluetooth can do months on coin cell battery. reply bigiain 9 hours agorootparentAirTags get about 12 months on a CR2032 - which is about 200mAhr reply justsomehnguy 15 hours agorootparentprevMoto g30 without cell modem but with enabled WiFi easily gets two weeks of the runtime without much of display time. My RAZR MAXX could sit for months on the one charge (with already busted battery) without any cell activity. So if you don't need the display and a hungry wireless tech to keep on constantly you can have enough juice for days, even when the phone is no longer in the power profile to have a full run. reply sleepybrett 14 hours agorootparentprevI read that they use bluetooth beaconing for this. A simple bluetooth beacon like a tile can run for months with minimal power. The phone isn't actually 'dead dead' it's still powering the bluetooth beacon. reply saganus 16 hours agoparentprevIt says they have specialized hardware so I imagine they use some ultra-low power chips and perhaps even a small extra battery? reply ggm 44 minutes agoprevWant but also worry digital stalking cannot be avoided. reply shp0ngle 14 hours agoprevAirTags/Find My is THE reason why me and my whole family is on iOS. This is great. I wanted my next phone to be Z Flip since Apple stopped making small iPhones (and no foldables in sight), but reliance on Find My stopped me. This is great reply bityard 13 hours agoparentYou may want to watch some videos on the longevity of foldable screens, consensus is that they still don't hold up over time and become especially brittle after being exposed to cooler temperatures. reply MBCook 13 hours agorootparentI can’t imagine anyone, even Apple, solving the issues of the screen being perfectly flat at the hinge. I don’t think it can be made to feel the same as the rest of the screen or have no visual difference either. And I like my iPhone. If it was twice as thick and half as tall when folded, I just don’t see that as “better”. If it was the same thickness as today when folded and half as thick when unfolded I’m not sure that’s better either (and would be harder to make anyway). Folding phones seem like one of those neat things that can be made but don’t solve any problem most people have. It seems like it would be most useful to make something the size of a lipstick case that unfolds into something a bit wider. That would’ve been cool 20 years ago. But I don’t wanna phone that tiny these days. reply shp0ngle 5 hours agorootparentWell they do solve problem for me - I want a small phone, flip is. I like my iPhone Mini but apparently most people didn't. reply Scene_Cast2 16 hours agoprevAfter a car break-in, I got myself airtags and an old iPhone. I was really hoping that the rumors of airtag support coming to Android were true. reply imp0cat 15 hours agoparentSamsung Smart Tags work quite well for this. The devices are detected by most Samsung devices (so not just phones, also TV sets). reply wolverine876 16 hours agoprevCan anyone recommend a similar device small enough for eyeglasses? An elderly person I know loses theirs (and it's a significant problem; they are very limited without them, on top of other difficulties of aging). I can find their phone for them remotely, but not their eyeglasses. reply prmoustache 1 hour agoparenthttps://duckduckgo.com/?t=ffab&q=glasses+cord&iax=images&ia=... reply ruune 14 hours agoparentprevQuick search gave these. I've never used any of those so I can't recommend one, but I have heard some good stuff about Orbit. Maybe there is someone here who has actual first hand experiences with them? https://findorbit.com/en-eu/products/orbit-x-glasses-eu https://home.tag8.co/products/dolphin-smart-eye-wear-tracker reply jeffbee 15 hours agoparentprevThis seems like something achievable with the optics that a phone already has. It should be possible to wave your phone around and have it identify your glasses, if they are somewhere in the room. reply thimkerbell 15 hours agoprevThere should be a config seeting. The 2nd time within 2 minutes that your phone is called from [other phone number] it will ring with increasing loudness, regardless of other settings. (Enable/disable) reply kelnos 13 hours agoparentThat's fine for the phone, but not for other devices and trackers. reply hoseja 2 hours agoprev>And thanks to specialized Pixel hardware, Pixel 8 and 8 Pro owners will also be able to find their devices if they’re powered off or the battery is dead. Mask off moment. reply ganarajpr 11 hours agoprevThe one thing I want to find is my earpods inside their case. I mean how hard is it to implement this for a company? Take my money please ? Just allow me to find my earpod and its case ? reply vineyardmike 10 hours agoparentApple supports this for their (newer?) AirPods. You can find either ear piece, and/or the case. reply brunoqc 3 hours agoparentprevI think the pixel pods have it too. reply ClassyJacket 10 hours agoparentprevYeah, Airpods Pro have this, I'm sure Samsung will follow suit soon reply MBCook 14 hours agoprevCan the Pixel phones do the UWB finding like iPhones can where they tell you it’s 10.7 feet to your left? Or is this more like the older iPhones where it simply tells you if you’re getting closer or further away? reply lxgr 11 hours agoparentUWB (at least as used by Apple, i.e. without using angle-of-arrival techniques) only provides precise distance as well. The rest is done on the iPhone using ARKit and trilateration. Since Android supports UWB [1] (depending on the device of course), I don't see why they couldn't support the same feature. [1] https://source.android.com/docs/core/connect/uwb reply kelnos 13 hours agoparentprevBased on the mockups and description in the article, it seems like it's the latter, unfortunately. If it did the UWB thing, I'm sure Google would have prominently pointed that out. reply MBCook 13 hours agorootparentThat’s what I was afraid of. It’s nice that AirTags can tell if they’re at home or whatever. But they got so much better with UWB on the phone for pinpoint locating. Hopefully they’ll add that in the future. reply XorNot 2 hours agoprevAnd here I am still wanting a damn widget I can put my Android which will \"find my phone\" my wife's phone, and vice versa without her needing to log into her google account. reply ranguna 1 hour agoparentTake a look, maybe this'll help https://f-droid.org/packages/de.nulide.findmydevice/ reply jsheard 16 hours agoprevFinally, this was announced about a year ago but kept getting held up by the effort to standardize stalker detection across Android and iOS. Seems like they've finally got it figured out. reply graiz 15 hours agoprevSeems like the type of thing that would benefit from a joint iOS/Android W3C protocol between Apple/Google/Samsung. Perhaps a find my Matter device? reply agilob 15 hours agoprevWhen I read the title I thought \"NO!\" thinking that Google is discontinuing the app. reply m4lvin 15 hours agoprevFor anyone looking for a free alternative that works without google play services: https://f-droid.org/packages/de.nulide.findmydevice/ reply kelnos 13 hours agoparentThis isn't really what the article is about. This app only does one small piece of what Google is announcing. reply Onavo 15 hours agoprevWill Google give up your location to the feds if they issue a broad area geolocation warrant? reply someplaceguy 15 hours agoparentEven Google has to comply with warrants. Even secret ones. You should assume the location of your devices will be disclosed whenever the US gov has a plausible reason for requesting it, regardless of which part of the globe you live in. reply kelnos 13 hours agoparentprevSure, and this announcement doesn't change that. reply fsflover 13 hours agorootparentIt does, since now your location is available even if you switch off your phone. reply add-sub-mul-div 15 hours agoprevI've never needed this feature before but if a chance of losing my phone once per decade is the price to pay for not having my phone tracked at all times, then I'm cool with it. reply lotsofpulp 15 hours agoparentBeing connected to a mobile network tracks your phone at all times. reply fsflover 13 hours agorootparentUnless you disconnect from it. reply lotsofpulp 12 hours agorootparentThat defeats some of the purpose of the phone, which is to be reachable by others. reply fsflover 11 hours agorootparentYou can disconnect whenever you need privacy. reply renewiltord 16 hours agoprevGoogle is like store-brand Apple. But their home devices are so much better (but still somehow not ‘good’) that more people have coverage with them rather than HomePod so they’ll get the search assist from there. I’m surprised they can only do proximity, though. AirTags do direction quite well and can sometimes even tell altitude difference. reply FredPret 16 hours agoparentCurious to know about the home devices. I'd never allow any google hardware a permanent place in my home even if it's amazing, but it's hard to see another major tech co coming up with something better than Apple TV. The Homepod is amazing too. It's stuck with Siri (is this what you mean?) but has fantastic sound that integrates with the TV for movies and Mac when playing music. reply kelnos 13 hours agorootparent> I'd never allow any google hardware a permanent place in my home The irrational part of me agrees with you, but I already have an Android phone, so the Google Home doesn't really give Google any extra insight into my private life that they don't already have. This is why I don't get it when people are weirded out by a stationary always-on microphone in their homes, when they carry around a mobile, always-on microphone (and tracking device!) in their pockets. (If you also refuse to have an Android phone on these same grounds, then your position seems pretty reasonable, of course.) reply FredPret 12 hours agorootparentI’m all-Apple, but I think my next always-in-my-pocket phone will be a flip phone just for the peace of mind of not having an internet-powered distraction machine on my person reply jsheard 16 hours agoparentprevDirection requires UWB, which is unfortunately still scarcely supported by Android devices. I believe the only ones which have it are the Pixel Pro models (but not the base Pixels or A-series) and Samsungs S+/SU/Z models (but not the base S models or anything lower end) while the entire iPhone stack has had it since the iPhone 11. reply rickdeckard 2 hours agoprevThis is a prime example of the \"big-corp issue\" of today's tech. In the landscape of 20 years ago this \"Find-My\" use-case would have already been made an industry-standard, with an Alliance or SIG of the industry controlling a global spec and roadmap. Tech-companies would have proposed a spec, lobbied competitors and other players to join and contribute to an industry standard to make this happen, because no single company was large enough to launch such a multi-disciplinary product all by itself. You would get a wide range of companies joining, reviewing and vetting the specification to create a strong ecosystem for service providers, component makers, device-manufacturers, etc. Now, a few established big corporations happily use those legacy industry standards (WiFi, BT,...), and instead of contributing back to it to make it stronger, they create proprietary products/services on top, ensuring that everyone else is just a petitioner/supplicant. reply newaccount74 2 hours agoparentWhat makes me sad about this is how it effectively stops innovation. The barrier to entry is so high, that only a handful companies can launch an item tracker service. And everyone who wants to offer a product in that space needs to play by their rules, they prohibit a lot of use cases. I use a few Chipolo trackers with my iPhone, and I find it infuriating how limited they are. There doesn't seem to be a way to query the data, so I can't eg. make a dashboard that shows location of my items, or make my own app that makes it easier to find my stuff. Every time I look for my keys, it takes me 30 seconds to navigate through Apple's stupid interface, wait for it to update, scroll to the right button, and press the \"make sound\" button. Without competition we're forever stuck with mediocre software and a fraction of what would be technically possible. reply rickdeckard 2 hours agorootparentAbsolutely agree. Even worse, I doubt that ANY company can actually compete with such a service. Apple and Google create the tracking-network by baking the capability to scan for trackers into the Smartphone-OS. A competitor first needs to make users install an app to \"manually\" build this network. There's no company capable to build a global network of equal scale. reply amluto 16 hours agoprevGoogle is quite good at these little screw-you features: > At home protection. If a user has chosen to save their home address in their Google Account, their Android device will also ensure that it does not contribute crowdsourced location reports to the Find My Device network when it is near the user’s home. This is a feature that is fundamentally on-device, and it’s even a privacy feature, but for some reason it requires a Google account. reply rickdeckard 2 hours agoparentYeah, I was laughing when I saw this. \"Come on...tell us where you live. We already know anyway, but it's better if you tell us. It's for your own protection!\" reply e44858 12 hours agoparentprevDoes any part of Find My work without a Google account? reply amluto 12 hours agorootparentThat’s a fair question — if your phone doesn’t report beacons without a Google account, then requiring a Google account to mask off areas isn’t so bad. But you should still be able to mask off areas without telling Google your home address. reply modeless 16 hours agoprevWhen will we get precise positioning using UWB? UWB has been in three generations of Pixel phones now and I haven't been able to use it a single time. Not even once. I want a direction finding arrow that points to my lost headphones case or my wife's phone. I want a smart door lock that unlocks automatically when I approach from the outside but not the inside. I want to stop relying on flaky Bluetooth for Tesla's car key. Why are there literally zero products on the market that support UWB on Android as far as I can tell? reply kelnos 13 hours agoparent> UWB has been in three generations of Pixel phones now Only sorta -- the Pro models have it, but the regular ones do not. I wouldn't be surprised if Google eventually rolls this out. But it's probably bad optics to say it's only supported with a very limited set of models. reply sudosysgen 16 hours agoparentprevI don't see why UWB should have anything to do with the Find My network. It's something the tag manufacturer would add separately. Since UWB range is lower than BLE range, the Find My network would get you close enough and then the manufacturer's app would enable you to use UWB. So the answer is, whenever a manufacturer thinks it's worth including in the tag. reply modeless 16 hours agorootparentAccording to this article you use Google's \"Find My Device\" app to find tags, not the manufacturer's app. Google's app needs to support UWB tags and display appropriate direction finding UI. Like Apple's has for years. reply sudosysgen 15 hours agorootparentIt doesn't have to, and you don't have to. There is zero reason why you can't just have the UWB direction finding UI in a different app, even should they not add the hook. The UI/UX change between map oriented passive direction finding from anywhere in the world and camera oriented UWB direction finding only when you're within range is very sharp. If it could eventually do it within the app that could be pretty neat, but even if it doesn't and just has a button to open the manufacturer's app - just like it currently does to open the maps app - it would be basically as good. Alternatively, the manufacturer's app can just show a notification to get you into the UWB UI when active proximity mode is enabled and when you are within UWB range. Apple isn't the only manufacturer to do UWB, and Samsung's UX is pretty similar to this and works just as well. reply modeless 15 hours agorootparentIt is ridiculous for Google to release a new tag finding app today without UWB support and without even mentioning UWB at all as a future feature. Sure, Google could link to third party apps, but they're not. Sure, manufacturers could do it themselves, they could have done it three years ago, but they didn't and they still aren't. I'm asking why none of this happened. It's irrelevant whether someone theoretically could have done it or might do it later. reply sudosysgen 15 hours agorootparentBut manufacturers have? Samsung tags have UWB and they work well. There are also Chinese UWB enabled trackers you can buy. Other manufacturers like Pebblebee, Chipolo and Tile decided not to include it - neither on Android nor iOS. They could have added it for years now, they just decided not to. Chipolo says they think it's just not worth the tradeoff. I don't necessarily disagree - I rarely use it with my samsung tags or airtags, it's just more convenient to use sound 95% of the time. Only time I'll use it is if I'm afraid of waking someone up. I could definitely see it being useful for people with hearing limitations, though. reply modeless 15 hours agorootparent> There are also Chinese UWB enabled trackers you can buy. Where can I buy a UWB tracker that supports direction finding on Pixel phones? I have looked and didn't find a single one. Or literally any other product of any kind with UWB that works on Pixel at all for any purpose, for that matter. reply sudosysgen 15 hours agorootparentYou can buy plenty of Chinese UWB trackers, but they are for industrial applications. Without an open network (this being the first that comes with an UWB API) there's no way to make a compelling product so no one's going to try. The third party UWB API is available (though it's in alpha) and it supports AoA and direction finding : https://developer.android.com/develop/connectivity/uwb I'm guessing the reason you can't find anything compatible is because there was no way to make anything worthwhile before the Find My network economically. It made no sense for Tile to add UWB only for Pixels (since the Smarttags are better), and no one else could make their own network. Once this rolls out, there will probably be trackers with UWB trackers. Otherwise, you can make your own - the chips are pretty cheap and it's a relatively accessible DIY project that has been done before. reply spants 16 hours agorootparentprevsamsung have this. reply imp0cat 15 hours agorootparentThis. Samsung Smart Tags work great, but not all Samsung phones support UWB do do check before you commit to a purchase. reply MarkusWandel 16 hours agoprevIronically, yesterday my wife couldn't find her phone while we were at a restaurant. Is it at home? Well, let's try this. Google it, go to http://android.com/find Ah, but it only shows devices logged into my account. Well, let's log into hers. Which required 2FA from... her device. Sooo useful. reply bnc319 15 hours agoparentWith Apple's Find My solution, when logging in via iCloud.com there is a 2FA bypass link to the Find My web app. If you want to access other iCloud web apps, 2FA is required, but not for Find My. A nice quality of life feature that has probably gotten thousands of people out of a bind. reply godelski 15 hours agoparentprevJust don't log out. Let us collect your data forever and at all times. No need to restart your computer, ever. You're dumb because you didn't create hard copy two factor backup codes! User error. Some potential explanations by people who don't want to resolve this admittedly difficult problem to solve. I find these issues difficult when also considering security keys like Yubikey. Like you can't clone a yubikey and that's considered a feature not a bug. So what, you want me to have everything on a usbc device that barely pokes out of my laptop? That's very easy to lose. Fall off my keychain? I've had many usbs do that (__especially__ the small ones). I am really happy that there are people doing security and making things more secure, but the truth is that usability is necessary too. The reason Signal is so great is not just because encryption, it is because my Grandma can use it. Not aware of any other encrypted text message system besides iMessage and WhatsApp that has that usability and those come with strings. I really think there needs to be something similar for MFA. Consider the user. Edit: A possible suggestion for FindMyDevice is to have trusted users. That you can give your wife, friend, whatever permission to find your device without needing to sign into your account or any other permissions. This seems like a relatively obvious solution, is there something wrong with it? Google devs, can't you patch this in in a few weeks (or less? But we all know, bureaucracy exists) reply doublepg23 13 hours agorootparentThe suggested solution for loosing a Yubikey is to already have n>=2 set up for your account. Apple won't even let you enable it without having two. reply godelski 11 hours agorootparent> The suggested solution for loosing a Yubikey is to already have n>=2 set up for your account >> I find these issues difficult when also considering security keys like Yubikey. Like you can't clone a yubikey and that's considered a feature not a bug. Yes, I am quite aware of this. My complaint is that I cannot expect my grandma to be able to perform this action. It would even be difficult if there was a cloning program, but without it, this is certainly an insurmountable task. Sure, you can make the argument that my grandma is dumb and tech-illiterate, but the truth of the matter is that this is the bar for the average person. You, me, and other Hacker News users would have no issues with these tasks, but this is not representative of the general FIRST WORLD population. Not to mention those in developing countries. The technical aspects are great! But they also need be made available for the average person. Be that by bringing the average person up to sufficient level or through careful design to bridge this gap. But what is clear is that the current state of things is insufficient. Especially consider that this is Apple's main value: design. reply freitasm 15 hours agoparentprevYou should have multiple access to important accounts. Yubikeys are great for this. I keep a couple in an envelope with important instructions. reply grenran 9 hours agoparentprevAndroid find does work without 2FA. Have tried it before reply XargonEnder 16 hours agoparentprevMy wife and I had this exact same sort of situation and now we're both logged into each other's accounts. We never have trouble finding our phones now but it does introduce other problems. Such as not knowing if some sort of security alert is being generated by my spouse and now we have to communicate about it. reply nemothekid 15 hours agorootparentDoes Android have an equivalent of the \"Find my Friends\" feature? I assume that would be easier/more secure than logging into each others accounts. reply Avery3R 13 hours agorootparentGoogle maps location sharing works cross-platform on android and iOS reply spurgu 15 hours agorootparentprevSeems like that's what was just released. reply nemothekid 15 hours agorootparentThe article mentions sharing accessories, but not the device. I assumed that maybe sharing the device feature already existed, but judging by GP's comment that doesn't seem to be the case. reply spurgu 15 hours agorootparentYeah I'm just assuming that it includes devices as well. reply lotsofpulp 15 hours agorootparentprevI have no idea why they sunsetted Latitude. I feel like there was nothing on Android after that, and everyone I know switched to Find my Friends and iOS. reply progbits 15 hours agoparentprevI understand your frustration, and device sharing is obviously a missing useful feature. But I absolutely don't want it to work without 2FA, and if your only 2FA is your phone you have other problems. reply GeekyBear 15 hours agorootparentA \"feature\" designed to help you find your lost phone that can not work when your phone is lost is clearly designed poorly. All Google had to do here was copy Apple's solution of having access to that one feature work with only the user name and password. reply gomox 14 hours agorootparentThis is just classic Google product management. reply Teever 15 hours agorootparentprevWhy? Why not just have it let you know via a message pop up that location tracking is active? If you don't get that message because you don't have your device that means you're not being tracked / are looking for your device and if you get that message because you have your device you've now been warned that someone has breached your account. reply gomox 14 hours agoparentprevCame to the comments section to point out this ridiculous fact, which has been this way for ages (2018 at least). I'm glad it looks like there is enough momentum for them to think about fixing it now. reply sf_rob 15 hours agoparentprevTangent, but my experience with this was when I needed to upgrade my mobile plan for international service... but I had 2FA via text message enabled for the account. reply MarkusWandel 16 hours agoparentprevAnd now I thought I'd try it with my device. It's on my work wifi and on mobile data. \"Play a sound\" works. But it still insists \"Can't reach device\" and won't tell me where it is. What am I missing? \"Find my device\" is toggled on in the settings. reply ryandvm 16 hours agoparentprevYeah, this seems like such an incredibly obvious oversight. Like... I know how it goes with fuzzy requirements, but who didn't nail down the acceptance criteria that the user should be able to trigger Find My Phone without 2FA. reply daghamm 15 hours agorootparentPlease don't. If you have 2FA, it should always be active. No exceptions. Hackers were able to steal private photos of half of Hollywood in 2014 because someone at Apple decided that 2FA is not needed for one particular iCloud function. reply IncreasePosts 15 hours agorootparentprevSo if someone steals my password, they can now find out exactly where I am? And factory reset my device now. Perhaps some combination of no 2FA IF you are a trusted contact that I've specifically added to the account would be safer. reply gomox 14 hours agorootparentThere are many design tradeoffs around this problem, and most of them are reasonable, except the one they chose (can't find your phone unless you have your phone). Especially since the choice of where the 2FA prompt will actually show up is out of your hands. reply tiagod 15 hours agorootparentprevIt's very common. I once lost a phone in an Uber car, and the only way to contact the driver was to log-in to Uber, which required SMS 2FA... reply 21 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The new Find My Device feature on Android enables users to locate lost devices and items by leveraging a vast network of over a billion Android devices.",
      "It provides multiple tracking options such as offline location tracking, Bluetooth tag support, proximity tracking, integration with Nest devices, and the capability to share accessories with contacts.",
      "The feature guarantees security, privacy, and works with devices operating on Android 9 and above, with forthcoming updates for headphones from brands like JBL and Sony to integrate into the Find My Device network."
    ],
    "commentSummary": [
      "The forum discusses various tracking devices such as Tile and Apple AirTags, comparing their features, limitations, and anti-theft effectiveness.",
      "Users address interoperability challenges among tech companies, privacy issues with location tracking, and the call for industry standardization.",
      "Suggestions include utilizing UWB technology, incorporating hardware switches for privacy control, and enhancing device tracking systems while expressing concerns about market domination by Apple and Google stifling innovation for smaller businesses."
    ],
    "points": 207,
    "commentCount": 276,
    "retryCount": 0,
    "time": 1712595834
  },
  {
    "id": 39968382,
    "title": "How Digital Equipment Corp. Engineers Saved Ethernet",
    "originLink": "https://spectrum.ieee.org/how-dec-engineers-saved-ethernet",
    "originBody": "THE INSTITUTE GUEST ARTICLE How Engineers at Digital Equipment Corp. Saved Ethernet Their groundbreaking learning bridge technology increased LAN performance ALAN KIRBY07 APR 20247 MIN READ Alan Kirby [left] and Mark Kempf with the LANBridge 100 with serial number 0001. ALAN KIRBY",
    "commentLink": "https://news.ycombinator.com/item?id=39968382",
    "commentBody": "How engineers at Digital Equipment Corp. saved Ethernet (ieee.org)206 points by hasheddan 19 hours agohidepastfavorite73 comments abraae 10 hours ago> Mark’s idea didn’t replace Ethernet—and that was its brilliance. By allowing store-and-forward switching between existing CSMA/CD coax-based Ethernets, bridges allowed easy upgrades of existing LANs. This reminds me of John Carmack's philosophy of great things coming from thinking locally and taking small steps. > Carmack subscribes to the philosophy that small, incremental steps are the fastest route to meaningful and disruptive innovation. He compares this approach to the \"magic of gradient descent\" where small steps using local information result in the best outcomes. According to Carmack, this principle is proven by his own experience, and he has observed this in many of the smartest people in the world. He states, \"Little tiny steps using local information winds up leading to all the best answers.\" reply Solvency 8 hours agoparentThat last paragraph is literally the same basic sentence about \"little steps\" written three times in slightly different ways. Did you get this quote from an AI written SEOspam site? reply abraae 8 hours agorootparentHa ha, it came from Wikipedia so possibly. reply itemize 8 hours agorootparentAs a barely relevant point, Using google, I can't find references to the quote (or John Carmack) by searching 'Little tiny steps using local information winds up leading to all the best answers'. Other search engines seems to do a bit better on retrieving the Wiki article. What gives, is google really becoming worse? reply foxylad 5 hours agorootparentKagi search shows this HN post, a link to kids encyclopedia page on Carmack, and his Wikipedia page as the first three links. I started using Kagi last month, and am really enjoying it - it's like Google at it's prime. reply jen729w 2 hours agorootparentprevThe Wikipedia article has a reference to where the quote came from. It’s a podcast, whose audio presumably has not been transcribed and indexed. https://lexfridman.com/john-carmack/ reply Solvency 7 hours agorootparentprevIt's been at its worst for like 6 years. reply gumby 13 hours agoprevWhat a great story. The spanning tree algo is under appreciated: this allowed people who didn’t understand networking to plug networks together the way you would plug extension power cables together,* making networking simple ( or alternatively insanely broken, when people had 400 computers on a single LAN with a rat’s nest of bridges and hubs…but unlike the extension cord case, nothing would catch literal fire). * don’t try this at home or work! reply jeramey 13 hours agoparentOccasionally people don't understand how to plug extension power cables together, either, especially during times of high stress and low sleep. Once upon a time, when I was in IT support, I got a call from someone in a satellite office across town saying that their computer wouldn't turn on. A new production had begun and everyone was a bit frantic, so this was an urgent request. After asking them to hold the power switch in for a few seconds and try to turn it on again, I asked them to make sure the power cable was secure and that the computer was plugged in. It was, of course, but the computer still wouldn't turn on, so it was time to jump on the bicycle and ride across town with a new power supply in tow, figuring it would be a quick fix. When I arrive, I see that, indeed, the computer was plugged in to a power strip. And that power strip was plugged in to itself. From then on, I always made sure to ask, \"Is the computer plugged into the wall?\" Saved myself a few bicycle trips that way. reply firecall 11 hours agorootparentI was really hoping for some never before known knowledge of how to connect Extension Cables and Power Boards together! But no, turns out someone didnt even manage the basics! LOL There's a reason the IT Crowd have the running joke of 'have you tried turning it off and on again...'! :-) reply jeramey 11 hours agorootparentIn all fairness, the person had obviously been awake for over 24 hours and was on their 1,001st cup of coffee. And since earlier in the summer I had crashed the entire ticket scanner network the night before the opening of the weekend festival we had put on by creating a network loop between a couple of non-spanning-tree-speaking network devices, I didn't feel I was in a place to be snarky about it! reply gumby 11 hours agorootparent> was on their 1,001st cup of coffee 9 cups is a lot but shouldn't cause cognitive disorder. reply patmorgan23 7 hours agorootparentThere are 10 types of people reply freedomben 12 hours agorootparentprevheh, with power strips that have very long cables, I've seen them plugged into themselves a few times as well. reply Arrath 11 hours agorootparentLike tying a shoelace: the long grey cord goes around the backside of the desk, turns and comes around the front, and back into its own powerstrip. ..Wait why won't it turn on? reply teleforce 10 hours agoparentprevThat's why the new Ethernet protocol 802.11aq is recommending Shortest Path Bridging (SPB) as a robust alternative to Spanning Tree Protocol (STP). Not only it's more robust, but it is more secure with extra resilience against broadcast storm. It also can support multi-cast function more easily and intuitively at data-link layer [1],[2]. Compared to the new SPB, STP looks like an immature hacks and why it took so long to be replaced is beyond me. [1]IEEE 802.1aq: https://en.m.wikipedia.org/wiki/IEEE_802.1aq [2] 802.1aq Shortest Path Bridging Design and Evolution: The Architect's Perspective: https://ieeexplore.ieee.org/book/6381532 reply throw0101d 10 hours agorootparent> That's why the new Ethernet protocol 802.11aq is recommending Shortest Path Bridging (SPB) as a robust alternative to Spanning Tree Protocol (STP). This is a dead letter standard: most folks who would 'need' SPB are probably using BGP EVPN to reduce the Layer 2 'blast radius'. It should also be noted that the IEEE was dragged kicking and screaming towards SPB: originally TRILL was proposed, but the IEEE rejected it, and so the IETF published: * https://en.wikipedia.org/wiki/TRILL_(computing) IEEE realized their mistake and published SPB, so now there are two L2 standards. Not many folks who either though, with anyone really needing 'large scale' stuff moving towards L3 solutions. reply inopinatus 9 hours agoparentprev> people who didn’t understand networking A couple of decades ago I witnessed a classic demonstration of Weinberg's Corollary¹ when a spanning tree misconfiguration on a freshly-installed BlackDiamond disabled LINX for hours, throwing half of Britain's ISP peering into chaos. The switch in question was evidently cursed: it'd been dropped on my foot earlier that week, thus disabling me for hours, and everyone else involved in that project was dead within two years. __ [1] \"an expert is a person who avoids the small errors while sweeping on to the grand fallacy\" reply dredmorbius 3 hours agorootparenteveryone else involved in that project was dead within two years. That's ... quite a legacy. reply non-chalad 10 hours agoparentprevToo late: Tried this at work; Did a marvelous job sharing satellite internet with the whole barracks. reply gumby 9 hours agorootparentSounds like things worked out, even if all you shared was your porn. reply 486sx33 11 hours agoparentprevNot recommended but these bad boys make a lot of things work, and cause a lot of damage! https://m.media-amazon.com/images/I/61A5WvzcgsL._AC_UL960_QL... reply Aloha 9 hours agoparentprevSTP is basically magic - and being able to reliably optimize it to reduce reconvergence time makes you a magician. reply jbernsteiniv 5 hours agorootparentI feel like STP is basically magic when you first learn it in a course (usually sponsored by Cisco) but the illusion is broken when you learn computer science principles like a graph data structure and how to build an acyclic graph. STP handles this through sending little messages with every node receiving the message appending themselves after whoever sent it. Then a node just checks if it sees itself in the message. If I DO see myself in the chain, I know now I need to not send information to the previous node that sent me the message. The message just needs to be something that'll never succeed but flow through the network. If I have three nodes labeled \"A\", \"B\", and \"C\" then I could send a message intended for \"D\" (which does not exist in my network). A has a path to B and A has a path to C. B has a path to C and vice versa. Each node can talk to another. A send to B. B checks if its in the chain and if not, it appends itself in the chain. A -> B. B knows it received the message from A so it will ignore this connection and send the message to all of its neighbors. C receives the message from B. C checks if its in the chain and if not, it appends itself in the chain. A > B -> C. C got the message from B so it ignores this path and sends the message to all of its other neighbors (in this case, A). A receives the message and checks if its in the chain (it is). A knows it received the message from C. A now knows that a cycle has been detected, so A disables its connection to C. Congrats! You've designed a tree! Its a graph that contains no cycles. All we had to do was send a message and check if we see ourselves in the chain. If we do, we disable our path to the last node that sent us a message. Of course STP gets more complicated from here as we factor in path costs as weights to measure the decision to axe a connection. Maybe A -> B is a 1G connection and A -> C is a 10G connection, in this case A may disable the path to B. I do agree that reducing reconvergence time is magic. I don't understand that one. reply Waterluvian 13 hours agoparentprev> but unlike the extension cord case, nothing would catch literal fire Somewhere, out there, is a story of an overburdened network setup literally catching fire, and it’s hopefully making its way to us. reply djbusby 8 hours agorootparentI saw one sorta catch fire. But it wasn't over burdened. It was in the kitchen, cause that was also where the utility closet is. and our 100+ year old building has crappy/dirty power, frequent brown-outs. So, a few weeks of splashing coffee, tea and other crap around, and our crap power the thing started humming (60Hz). Then one day POP! Cap blew out and there was a little smoke. reply genericone 12 hours agorootparentprevC'mon HN, I'm counting on you... reply inopinatus 9 hours agoprevThis is all very well but DEC's greatest contribution to my own networking happiness was perhaps the quad-port Tulip/21143 cards that were my go-to for building reliable FreeBSD-based white-box routers back in the day reply mannyv 7 hours agoprevOne side effect of the network switch was that you needed to buy switches. With the old way everyone could be on one wire. With the new way you needed one port per host. Switches also allowed for centralized management. Note that cable works on what essentially is token ring, at least conceptually; channel 0 (i believe, it's been a while) is the heartbeat. reply panick21_ 9 hours agoprevI always wondered why it took so long to go from 10 Mbit/s to 100Mbit/s. For sure in that time they invented things like Ethernet Switching. But still, it seems like that standard took forever to move on. And then once it did it went from 100Mbit/s to 1000Mbit/s per second pretty damn fast. reply topspin 3 hours agoparent100BASE-T marked the transition from Cat 3 to Cat 5, and a lot of sites were actually still making use of coax. That was necessary but was an impediment because it meant a high cost of adoption. This coincided with the fact that, at the time, it was largely unnecessary for the broader market of users: these were the days of IDE/ATA-1 disks, acoustic modems and 300 DPI black and white printers. You watched video with VCR. I recall using a business desktop back in the day with a common IDE disk, and also attached to a NetWare share. The NetWare share was much faster than the local drive although the network was only 10BASE-T. Around that time meetings began: how much was it going to cost to replace all the cable (half of which was still coax) and buy 100BASE-T switches? (Answer: a couple hundred dollars per drop.) That process took two years and another six months to do the work: a quarter of a decade. 1000BASE-T was specified for Cat 5: the floorboards didn't have to be torn up to replace all that cable everyone just paid for. The cost of PHY/SERDES components fell greatly during that time and made 1000BASE-T hardware affordable. Moving from 100BASE-T to 1000BASE-T was, therefore, cheap and easy and took correspondingly less time. reply devilbunny 8 hours agoparentprev10 Mbps was fine for, frankly, just about anything under 100 MB. And since 100 Mbps hubs were rare beasts, most being switches, they stayed a lot more expensive for a long time. Hell, my first home network was 10 Mbps on thinnet because it was so much cheaper than a 10 Mbps hub, and that was ca. 1997. I'd had 100 Mbps available in the dorms the year before, but you were looking at $250 for 100 vs $40 or less for 10 - which, in college student dollars, is a lot. When we had hard drives that barely topped 1 GB, which could be copied in under 17 minutes even on 10 Mbps, there just wasn't a huge incentive to upgrade speed. But once 100 made sense for homes - supporting 802.11g - the price of switches fell rapidly because it was no longer just businesses buying them. And so 1 Gbps fell more quickly, because the background hardware was getting cheaper. My 1998 computer with 100 Mbps and a 6.4 GB drive got used a few times as the piracy data exfiltration machine because it could pull down the 5 GB of stuff we'd accumulated so much faster, then plug it up to the three-apartment network we had set up and let everyone pull it to their own machines on the 10 Mbps we had there. Our outgoing connection was a Linux box on a 56k connection shared among six of us; it's not like 10 Mbps slowed any of that down. reply spintin 13 hours agoprev [42 more] [flagged] eej71 12 hours agoparentI think the great architectural challenge would be - how does one add that byte to the IP header in a non-breaking way? reply themerone 12 hours agorootparentThere's a surprising number of people who think you could magically expand the IPv4 address space in a backwards compatible manner. reply rini17 11 hours agorootparentBut this was what literally happened. All routers today support NAT and most of them actively use it. Isn't it a magical form of extended IPv4 address space? Could have been easily done in less band-aid fashion instead of chasing IPv6. reply ianburrell 10 hours agorootparentIt is impossible to address hosts behind NAT. Only the public IPv4 address is visible. It might have been possible to extend IPv4 by having each NAT hop add the internal IPv4 address as option header. Then it would be possible to refer to inside host directly with a list of addresses. That isn't worth doing now because it would require rewriting everything to deal with the new protocol. For one thing, lots of NAT boxes remove all the option headers. The new protocol wouldn't be reliable. reply inopinatus 7 hours agorootparentNot a counterexample exactly, but your remark reminded me of this eldritch horror: https://blog.cloudflare.com/cloudflare-servers-dont-own-ips-... TLDR: Cloudflare is using five bits from the port number as a subnetting & routing scheme, with optional content policy semantics, for hosts behind anycast addressing and inside their network boundary. Filed in my bookmarks under \"abominations\". reply jbernsteiniv 5 hours agorootparentFrom the link: \"A lot of Cloudflare's technology is well documented. For example, how we handle traffic between the eyeballs (clients) and our servers has been discussed\" Already 1 sentence in and we've already got Eldritchian horrors with customers described as just their watery orbs in their skull. reply djbusby 8 hours agorootparentprevNAT doesn't change IP Header size, just the contents - while staying the same size. The header size is a requirement for compatibility. reply vlovich123 11 hours agorootparentprevHow? reply aleph_minus_one 9 hours agorootparentprev> There's a surprising number of people who think you could magically expand the IPv4 address space in a backwards compatible manner. There is a (somewhat) backward-compatible way to do this at least for TCP and UDP: Instead of assigning an IPv4 network prefix (or one single IP address as a special case), assign a tuple (IPv4 network prefix, port prefix), i.e. by firewall rules not every source port can be used from every IP address for TCP and UDP connections. Is this a better solution than IPv6? I clearly don't think so. But it is an ugly, hacky solution that is theoretically possible if one really insists that one wants to keep using IPv4 (only), and needs to magically expand the address space. reply Alupis 10 hours agorootparentprevI believe it's more along the lines of the concepts in IPv4 are easier to grasp than IPv6, starting with the actual addresses themselves. IPv6 breaks all backwards compatibility. So, it's not unreasonable for people to ask why we can't just break it in a more familiar way? Extending IPv4 into say, IPv7 and using familiar addressing schemes, well understood routing/NAT/DHCP techniques, etc, while providing the same usable address range as IPv6 is possible. reply zamadatix 10 hours agorootparentIPv6 breaks less backwards compatibility than many people think. Many get caught up in all of the other changes you can (and often do) do because it makes life better and conflate that with IPv6 not letting them do things the same way. About the only generally true \"it breaks backwards compatibility\" is the format of the address being longer for the longer address. Netmask, gateways, DHCP, static assignment, and neighbor discovery are all about as 1:1 as one could ask if that's all you care about but it's just a dumb way to do things if you're upgrading everything so then you have SLAAC, a more present link local, DHCP-PD, and so on to hear/think about as well. E.g. you can still NAT the massive IPv6 private address space with static and/or DHCP assignments without having to change your understanding (addresses would even be darn short too!) but... it's just silly to do. reply kccqzy 8 hours agorootparentprevWhat are you exactly asking for? Sure it's not recommended but you can have NAT66 for IPv6, and DHCPv6 for IPv6. You can choose to configure your own IPv6 network in a way that's familiar to IPv4. Not exactly best practice but doable. reply _factor 8 hours agorootparentprevAt this rate, why not just use UTF-8 and combing IP and DNS into one. reply eej71 12 hours agorootparentprevIt would be great if its true! Perhaps its really there and we're all just blind to it? Unlikely, but I'm willing to be surprised. reply kemotep 12 hours agorootparentWell I don’t see how you could do it without upgrading equipment and software to support it. And then to start using the new ip scheme. Which starts to sound an awful lot like an IPV6 migration. reply bluGill 12 hours agorootparentprevUnfortunately none of those people have explained how it could be done in enough detail that I could try it. Most walk away when pressed but a few press on telling me it is easy so shut up and do it . reply mixmastamyk 11 hours agorootparentSimilar to how it has been done with phone numbers. I saw this done in Brazil for example. You add a digit to the front and put all existing address on 0.*. Short number dials are assumed to be 0.*. Update OS and hardware. Then you allocate across the new digit much later as time goes on. The thing with phone infrastructure though is that it is centralized. So may happen in a reasonably coordinated rollout. Global internet is a lot more distributed so it would take a very long time. The open question is, would it take longer than IP6 has? Maybe not. Part of the reason I didn't care to use it early was because of the long addresses. If we could get a five byte address written in hex it would be somewhat user friendly. reply Dylan16807 10 hours agorootparentIf you had to deal with a hundred million different kinds of dialing software, that method wouldn't work out so well. > If we could get a five byte address written in hex it would be somewhat user friendly. For local addresses, you can use fec0::zzzz. Do you need to memorize or hand-type global addresses very often? reply mixmastamyk 9 hours agorootparent> that method wouldn't work out so well How well did IPv6 turn out by that criteria? > memorize or hand-type We read things many more times than we type them. Yes, gibberish at 4x the length is substantially harder to deal with. I do write addresses occasionally and with IP4 it is at least possible, if not desirable. reply Dylan16807 9 hours agorootparent> How well did IPv6 turn out by that criteria? About the same. So it's good we made it big for the future. > Yes, gibberish at 4x the length is substantially harder to deal with. I find copying and pasting to be pretty easy, and if you're looking at your own machines you can organize them all with the last 4 characters. I don't have to compare random internet servers very often. reply KerrAvon 10 hours agorootparentprevThe IPv4 address field is fixed size. You can't simply add a digit and deal with it at the telephony company premises like you can with a phone number. You have to rev every piece of equipment and software that can ever touch a packet. At that point, why are you not also fixing other architectural flaws and ensuring that the address space is large enough to accommodate any future needs? reply thwarted 10 hours agorootparentOne interesting thing about this idea is that, from a higher level, that's exactly how IPv6 works; the \"leading zero\" is the IP protocol version field. If that leading zero, er IPv4, is there, the fields of the IP packet are interpreted using IPv4 semantics. If the version field is IPv6, then the IP packet is interpreted using IPv6 semantics. This is how you configure telephone dial plans. Earlier dialed numbers influence the interpretation of later numbers. You dial a 1, an area code is expected next. You dial 9, you get an outline line. Dial plans are a pretty decent setup and allow scoped dialing, but are limited in their extensibility (you can't have a local number start with 1). In IP, the IP protocol version field influences the interpretation of later fields, logically similar to dial plans. reply Alupis 10 hours agorootparentprev> The IPv4 address field is fixed size. So? Make it bigger, call it IPv7, then enjoy. IPv6 has 128 bits for addresses, why can't IPv7 have 128 bits as well, but still use more familiar patterns/techniques borrowed from IPv4? IPv6 threw just about everything out the window... for what reason? Two decades of confusion and resistance... I think at this point in time, people are afraid to say \"ya, we overthought the hell out of IPv6\". reply ianburrell 10 hours agorootparentNothing in the world can work with IPv7. That means need to update all the software and replace all the hardware. This takes years of effort, and years of time. What more familiar techniques? How are they going to be worth the millions of man-hours to implement? How are they going to be so much better that people will abandon IPv6 and switch from IPv4? Could this be accomplished by changing part of IPv6? It is quite possible that you are using IPv6 to access Hacker News without knowing it. reply Alupis 9 hours agorootparent> Nothing in the world can work with IPv7. That means need to update all the software and replace all the hardware. This takes years of effort, and years of time. Uh, you mean like IPv6? IPv6 once was new, and it was radical at the time (still mostly is). IPv4 should have just been extended to a 128 bit address space, breaking changes implemented around that, and then everything else would have been easier to adopt. No relearning everything - just rationalizing about larger address space. > It is quite possible that you are using IPv6 to access Hacker News without knowing it. No I am not, because our IT Dept. disables IPv6 on all workstations and doesn't support them at our gateways. IPv6 was the culprit in a lot of networking issues that just magically \"go away\" when disabled... so, they disable. The decades and decades of knowledge built around IPv4 is immense. IPv6 asked everyone to forget almost all of it and start over. It's really not surprising IPv6 is still not well adopted... reply bauruine 5 hours agorootparent>IPv6 was the culprit in a lot of networking issues that just magically \"go away\" when disabled... so, they disable Why do you think your IPv7 will magically just work? There will be problems with it and IT departements will still disable it. reply starspangled 4 hours agorootparentprevI know nothing of networking, could you explain why you can't use 4 bytes of option to extend the address space by 8 bits without invalidating existing addresses or require routes to them to understand the new option? Do routers not reject unknown options? reply samatman 10 hours agorootparentprevThere are a bit more than a hundred unused protocol numbers, and the options field has more than enough room to stuff some more address space into it. You'd need enough buy-in to get the protocol number accepted as valid, and could introduce better routing gradually: as long as the OG destination address knows how to send the packet on to the extended-address computer, it'll get where it's going. The problems, and they are real, show-stopping problems, are more social than technical. reply CodeWriter23 8 hours agoparentprevWhy didn’t Douglas Comer, Bob Metcalfe, Vint Cerf and John Postel (in no particular order) think of that? reply firecall 11 hours agoparentprev [–] Has IPv6 been a failure? All I know is that IPv4 is still around. And without knowing the IP Addresses of devices on my LAN, by home network would be harder to manage! reply ianburrell 10 hours agorootparentHacker News added IPv6 support recently. It is quite possible people are using currently using it to access the site. Google has IPv6 support at 50%. Most of the major sites I use have IPv6. Google, Facebook, Amazon, Youtube. reply ndriscoll 10 hours agorootparentprevYou can know the IP addresses on your LAN. If you'd like you can assign multiple addresses per interface, so that you have global and local addresses. e.g. local addresses fc00::1 for your router, fc00::2 for your desktop, fc00::3 for your phone, etc. If you want, you can use your global prefix with ::1 ::2 and ::3 for your global addresses too. You don't have to use privacy extensions if you don't want to. Things like peer to peer calling can actually work though without NAT. reply Dylan16807 10 hours agorootparentAddresses starting with fc are reserved. I would suggest using either fec0 or fd00. Which is still not best practices, but it's a lot closer. (fec0 is deprecated, and fd is supposed to be followed by a randomly chosen 40 bit site ID but you can lie and say your random number was 0) reply ianburrell 9 hours agorootparentfc00::7 is the ULA range. It is not reserved. It is the right range to use for an internal network. There is no reason to prefer fc00 over fd00. fec0 is the site-local addresses. Nobody should be assigning addresses in that range since they are made automatically. Using site-local addresses makes sense for single subnet network like most home networks. reply Dylan16807 9 hours agorootparent> fc00::7 is the ULA range. It is not reserved. It is the right range to use for an internal network. There is no reason to prefer fc00 over fd00. fc00::/7 is the ULA range. It is made out of two halves. fc00::/8 and fd00::/8 fc00::/8 is reserved. People are only allowed to use fd00::/8 > fec0 is the site-local addresses. Nobody should be assigning addresses in that range since they are made automatically. Using site-local addresses makes sense for single subnet network like most home networks. None of my adapters have automatic fec0 addresses, they only have global and fe80. So can you elaborate on that? reply ianburrell 8 hours agorootparentI don't know that about ULA range. I got fe80 and fec0 confused. It looks like the latter is the deprecated site-local range. It is unlikely that will be reassigned, but lots of people said the same with IPv4 and got burned. It is safer to use ULA. reply Dylan16807 7 hours agorootparentAh, yeah, fe80 is automatic but it's link-local so you can't do very much with it. reply ianburrell 9 hours agorootparentprevI use mDNS for hosts on home network. They have IPv6 addresses, multiple ones, but I don't care what they are because I have never used them. reply 9front 6 hours agorootparentprevNo, IPv6 has not been a failure, just a newer version of IP. IPv5 was skipped, so will be IPv7. Maybe IPv8 will unify IPv4 & IPv6. reply CamperBob2 5 hours agorootparentprevIPv6 == the new ISDN: I Still Don't Need it... although I will say ISDN was a godsend in the years before my community was finally wired for cable. reply simoncion 9 hours agorootparentprev [–] > All I know is that IPv4 is still around. So what? IPv6 coexists with IPv4 just fine. As has been made abundantly clear over the years, there's no need to have a flag day for a big cutover in order to adopt IPv6. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Engineers at Digital Equipment Corp. created a revolutionary learning bridge technology enhancing LAN performance and preserving Ethernet.",
      "Alan Kirby and Mark Kempf played key roles in developing this innovative technology."
    ],
    "commentSummary": [
      "The conversation delves into diverse networking topics such as Ethernet technology evolution, network speed upgrades, and debates on transitioning from IPv4 to IPv6 or even IPv7.",
      "It also discusses challenges like expanding address space, site-local addresses in IPv6, and the pros and cons of adopting new networking protocols.",
      "Overall, the discussion highlights the intricacies and factors to consider when enhancing and fine-tuning networking infrastructure."
    ],
    "points": 206,
    "commentCount": 73,
    "retryCount": 0,
    "time": 1712573520
  },
  {
    "id": 39969448,
    "title": "Cloudflare Enhances Resilience After Data Center Power Outage",
    "originLink": "https://blog.cloudflare.com/major-data-center-power-failure-again-cloudflare-code-orange-tested",
    "originBody": "Major data center power failure (again): Cloudflare Code Orange tested 04/08/2024 Matthew Prince John Graham-Cumming Jeremy Hartman 9 min read Here's a post we never thought we'd need to write: less than five months after one of our major data centers lost power, it happened again to the exact same data center. That sucks and, if you're thinking \"why do they keep using this facility??,\" I don't blame you. We're thinking the same thing. But, here's the thing, while a lot may not have changed at the data center, a lot changed over those five months at Cloudflare. So, while five months ago a major data center going offline was really painful, this time it was much less so. This is a little bit about how a high availability data center lost power for the second time in five months. But, more so, it's the story of how our team worked to ensure that even if one of our critical data centers lost power it wouldn't impact our customers. On November 2, 2023, one of our critical facilities in the Portland, Oregon region lost power for an extended period of time. It happened because of a cascading series of faults that appears to have been caused by maintenance by the electrical grid provider, climaxing with a ground fault at the facility, and was made worse by a series of unfortunate incidents that prevented the facility from getting back online in a timely fashion. If you want to read all the gory details, they're available here. It's painful whenever a data center has a complete loss of power, but it's something that we were supposed to expect. Unfortunately, in spite of that expectation, we hadn't enforced a number of requirements on our products that would ensure they continued running in spite of a major failure. That was a mistake we were never going to allow to happen again. Code Orange The incident was painful enough that we declared what we called Code Orange. We borrowed the idea from Google which, when they have an existential threat to their business, reportedly declares a Code Yellow or Code Red. Our logo is orange, so we altered the formula a bit. Our conception of Code Orange was that the person who led the incident, in this case our SVP of Technical Operations, Jeremy Hartman, would be empowered to charge any engineer on our team to work on what he deemed the highest priority project. (Unless we declared a Code Red, which we actually ended up doing due to a hacking incident, and which would then take even higher priority. If you're interested, you can read more about that here.) After getting through the immediate incident, Jeremy quickly triaged the most important work that needed to be done in order to ensure we'd be highly available even in the case of another catastrophic failure of a major data center facility. And the team got to work. How'd we do? We didn’t expect such an extensive real-world test so quickly, but the universe works in mysterious ways. On Tuesday, March 26, 2024, — just shy of five months after the initial incident — the same facility had another major power outage. Below, we'll get into what caused the outage this time, but what is most important is that it provided a perfect test for the work our team had done under Code Orange. So, what were the results? First, let’s revisit what functions the Portland data centers at Cloudflare provide. As described in the November 2, 2023, post, the control plane of Cloudflare primarily consists of the customer-facing interface for all of our services including our website and API. Additionally, the underlying services that provide the Analytics and Logging pipelines are primarily served from these facilities. Just like in November 2023, we were alerted immediately that we had lost connectivity to our PDX01 data center. Unlike in November, we very quickly knew with certainty that we had once again lost all power, putting us in the exact same situation as five months prior. We also knew, based on a successful internal cut test in February, how our systems should react. We had spent months preparing, updating countless systems and activating huge amounts of network and server capacity, culminating with a test to prove the work was having the intended effect, which in this case was an automatic failover to the redundant facilities. Our Control Plane consists of hundreds of internal services, and the expectation is that when we lose one of the three critical data centers in Portland, these services continue to operate normally in the remaining two facilities, and we continue to operate primarily in Portland. We have the capability to fail over to our European data centers in case our Portland centers are completely unavailable. However, that is a secondary option, and not something we pursue immediately. On March 26, 2024, at 14:58 UTC, PDX01 lost power and our systems began to react. By 15:05 UTC, our APIs and Dashboards were operating normally, all without human intervention. Our primary focus over the past few months has been to make sure that our customers would still be able to configure and operate their Cloudflare services in case of a similar outage. There were a few specific services that required human intervention and therefore took a bit longer to recover, however the primary interface mechanism was operating as expected. To put a finer point on this, during the November 2, 2023, incident the following services had at least six hours of control plane downtime, with several of them functionally degraded for days. API and Dashboard Zero Trust Magic Transit SSL SSL for SaaS Workers KV Waiting Room Load Balancing Zero Trust Gateway Access Pages Stream Images During the March 26, 2024, incident, all of these services were up and running within minutes of the power failure, and many of them did not experience any impact at all during the failover. The data plane, which handles the traffic that Cloudflare customers pass through our data centers in over 300 cities worldwide, was not impacted. Our Analytics platform, which provides a view into customer traffic, was impacted and wasn’t fully restored until later that day. This was expected behavior as the Analytics platform is reliant on the PDX01 data center. Just like the Control Plane work, we began building new Analytics capacity immediately after the November 2, 2023, incident. However, the scale of the work requires that it will take a bit more time to complete. We have been working as fast as we can to remove this dependency, and we expect to complete this work in the near future. Once we had validated the functionality of our Control Plane services, we were faced yet again with the cold start of a very large data center. This activity took roughly 72 hours in November 2023, but this time around we were able to complete this in roughly 10 hours. There is still work to be done to make that even faster in the future, and we will continue to refine our procedures in case we have a similar incident in the future. How did we get here? As mentioned above, the power outage event from last November led us to introduce Code Orange, a process where we shift most or all engineering resources to addressing the issue at hand when there’s a significant event or crisis. Over the past five months, we shifted all non-critical engineering functions to focusing on ensuring high reliability of our control plane. Teams across our engineering departments rallied to ensure our systems would be more resilient in the face of a similar failure in the future. Though the March 26, 2024, incident was unexpected, it was something we’d been preparing for. The most obvious difference is the speed at which the control plane and APIs regained service. Without human intervention, the ability to log in and make changes to Cloudflare configuration was possible seven minutes after PDX01 was lost. This is due to our efforts to move all of our configuration databases to a Highly Available (HA) topology, and pre-provision enough capacity that we could absorb the capacity loss. More than 100 databases across over 20 different database clusters simultaneously failed out of the affected facility and restored service automatically. This was actually the culmination of over a year’s worth of work, and we make sure we prove our ability to failover properly with weekly tests. Another significant improvement is the updates to our Logpush infrastructure. In November 2023, the loss of the PDX01 datacenter meant that we were unable to push logs to our customers. During Code Orange, we invested in making the Logpush infrastructure HA in Portland, and additionally created an active failover option in Amsterdam. Logpush took advantage of our massively expanded Kubernetes cluster that spans all of our Portland facilities and provides a seamless way for service owners to deploy HA compliant services that have resiliency baked in. In fact, during our February chaos exercise, we found a flaw in our Portland HA deployment, but customers were not impacted because the Amsterdam Logpush infrastructure took over successfully. During this event, we saw that the fixes we’d made since then worked, and we were able to push logs from the Portland region. A number of other improvements in our Stream and Zero Trust products resulted in little to no impact to their operation. Our Stream products, which use a lot of compute resources to transcode videos, were able to seamlessly hand off to our Amsterdam facility to continue operations. Teams were given specific availability targets for the services and were provided several options to achieve those targets. Stream is a good example of a service that chose a different resiliency architecture but was able to seamlessly deliver their service during this outage. Zero Trust, which was also impacted in November 2023, has since moved the vast majority of its functionally to our hundreds of data centers, which kept working seamlessly throughout this event. Ultimately this is the strategy we are pushing all Cloudflare products to adopt as our data centers in over 300 cities worldwide provide the highest level of availability possible. What happened to the power in the data center? On March 26, 2024, at 14:58 UTC, PDX01 experienced a total loss of power to Cloudflare’s physical infrastructure following a reportedly simultaneous failure of four Flexential-owned and operated switchboards serving all of Cloudflare’s cages. This meant both primary and redundant power paths were deactivated across the entire environment. During the Flexential investigation, engineers focused on a set of equipment known as Circuit Switch Boards, or CSBs. CSBs are likened to an electrical panel board, consisting of a main input circuit breaker and series of smaller output breakers. Flexential engineers reported that infrastructure upstream of the CSBs (power feed, generator, UPS & PDU/transformer) was not impacted and continued to act normally. Similarly, infrastructure downstream from the CSBs such as Remote Power Panels and connected switchgear was not impacted – thus implying the outage was isolated to the CSBs themselves. Initial assessment of the root cause of Flexential’s CSB failures points to incorrectly set breaker coordination settings within the four CSBs as one contributing factor. Trip settings which are too restrictive can result in overly sensitive overcurrent protection and the potential nuisance tripping of devices. In our case, Flexential’s breaker settings within the four CSBs were reportedly too low in relation to the downstream provisioned power capacities. When one or more of these breakers tripped, a cascading failure of the remaining active CSB boards resulted, thus causing a total loss of power serving Cloudflare’s cage and others on the shared infrastructure. During the triage of the incident, we were told that the Flexential facilities team noticed the incorrect trip settings, reset the CSBs and adjusted them to the expected values, enabling our team to power up our servers in a staged and controlled fashion. We do not know when these settings were established – typically, these would be set/adjusted as part of a data center commissioning process and/or breaker coordination study before customer critical loads are installed. What’s next? Our top priority is completing the resilience program for our Analytics platform. Analytics aren’t simply pretty charts in a dashboard. When you want to check the status of attacks, activities a firewall is blocking, or even the status of Cloudflare Tunnels - you need analytics. We have evidence that the resiliency pattern we are adopting works as expected, so this remains our primary focus, and we will progress as quickly as possible. There were some services that still required manual intervention to properly recover, and we have collected data and action items for each of them to ensure that further manual action is not required. We will continue to use production cut tests to prove all of these changes and enhancements provide the resiliency that our customers expect. We will continue to work with Flexential on follow-up activities to expand our understanding of their operational and review procedures to the greatest extent possible. While this incident was limited to a single facility, we will turn this exercise into a process that ensures we have a similar view into all of our critical data center facilities. Once again, we are very sorry for the impact to our customers, particularly those that rely on the Analytics engine who were unable to access that product feature during the incident. Our work over the past four months has yielded the results that we expected, and we will stay absolutely focused on completing the remaining body of work. We protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust. Visit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer. To learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions. Discuss on Hacker News Post MortemOutage",
    "commentLink": "https://news.ycombinator.com/item?id=39969448",
    "commentBody": "Major data center power failure (again): Cloudflare Code Orange tested (cloudflare.com)179 points by gmemstr 20 hours agohidepastfavorite38 comments decasia 14 hours agoAs always, it's really impressive to see how much technical detail they release publicly in their RCAs. It sets a good example for the industry. Also — quite impressive to make major infrastructure and architecture changes in a few months. Not every organization can pull that off. reply zamalek 8 hours agoparent> quite impressive to make major infrastructure and architecture changes in a few months And have it work first time round. reply Waterluvian 12 hours agoparentprevI feel there’s a sweet spot where if you do it too quickly, it’s a bad sign. And if it takes years, risk just keeps going up and up until it becomes basically impossible to do smoothly. reply qmarchi 15 hours agoprevTwo major outages less than half a year a part, but with wildly different outcomes. It's definitely showing their engineering capabilities were targeted at the correct outcomes. Would definitely be interested to see the detailed RCA on the power side of things. Not many people really think about Layer 0 on the stack. reply mike_d 12 hours agoparentCloudflare is in EdgeConneX Portland, you can try poking around but I haven't seen a RCA of what happened. Details are usually only shared with direct customers because it is bad for the brand. https://www.edgeconnex.com/wp-content/uploads/2018/10/ECX-22... reply virtuallynathan 9 hours agorootparentThey called out Flexential in the post? reply Jamie9912 8 hours agorootparentYes reply mannyv 14 hours agoprevSomeone set up the breakers incorrectly way back when, and they were never adjusted. I'll bet it's not possible to adjust those without powering off the downstream equipment. It reminds me of the amazon guy discovering that there was no way to fail back power without an outage, then them going off and building their own equipment. reply amluto 9 hours agoparentI would guess the devices in question are some sort of electric trip unit, along these lines: https://www.eaton.com/content/dam/eaton/products/electrical-... I don’t see anything in the manual about needing to power it down for reconfiguration. All the relevant buttons, set screws, etc should be accessible without removing any dead fronts, so they can be safely accessed while the system is live. reply SteveNuts 14 hours agoparentprev> there was no way to fail back power without an outage, then them going off and building their own equipment. Anywhere I can read about that? reply hx833001 14 hours agorootparenthttps://perspectives.mvdirona.com/2017/04/at-scale-rare-even... reply michaelt 14 hours agorootparentprevMaybe [1] which is about per-rack UPSes, to reduce the blast radius of UPS failure? Pretty sensible IMHO - I live in a country with a reliable electricity grid, and outages due to UPS malfunction are about as common as power outages. [1] https://www.datacenterdynamics.com/en/news/aws-develops-its-... reply nyrikki 14 hours agorootparentIn large data centers, rack level UPSs are impractical for many reasons like cost and efficiency, but the big problem is that modern power densities are so high that you want rows to fail if cooling isn't available. It doesn't take long without cooling to cook equipment to the point of failure or reduced reliability. 7 to 16kW per rack is common even in these older colo facilities. And there never would have been enough UPS to make up for not enough replacement breakers on site. reply throw0101d 10 hours agorootparent> It doesn't take long without cooling to cook equipment to the point of failure or reduced reliability. IIRC, it's about 30 minutes—irrelevant of room size. This is because while smaller rooms have less air volume as a 'buffer', they also you can't fit much gear. While huge data centres have huge volumes of air, they also have lots of gear. So there's a coincidental proportional relationship what you (as a rule of thumb) have 30 minutes before things cook, regardless of room size. (At least for 'general purpose' computing: GPUs are probably another matter.) reply michaelt 12 hours agorootparentprevBut isn't the UPS only expected to last for 15 minutes or so, to give the backup generators time to start up? Or to perform a fast-but-graceful migration when the generator doesn't start up? I thought most DCs just pause the cooling until the generator comes up, rather than running the cooling on battery power? reply throw0101d 10 hours agorootparent> But isn't the UPS only expected to last for 15 minutes or so, to give the backup generators time to start up? They are expected to last that long, but if the batteries are on year 4 of their 5-year life, that may not happen. What also may not happen is the generator starting up. Or the automatic transfer switch (ATS) not working properly: it should be on either input Feed A or Feed B, but when it tries to throw itself over (making a loud kah-chunk sound), it gets stuck in between—this happened to us once. \"The perversity of the universe always tends toward a maximum.\" — Finagle's Law of Dynamic Negatives reply jeffbee 12 hours agorootparentprevGoogle has been using rack-level UPS for many years, so I'm curious to hear why you think that is impractical specifically in large datacenters. https://cloud.google.com/blog/products/compute/google-joins-.... reply lathiat 9 hours agorootparentProbably because Google are designing the entire hardware (its not a Dell rack server) and data center together, they are also responsible for the entire workload if something goes wrong and likely also have data center redundancy for an entire power loss for the entire workload there. In third party data centers you're co-located with other customers that have mostly comodity server hardware which won't have those options, they generally (at least in my experience) all provide UPS power as part of the facilitiy so you have it already and something like a fire if it did happen, would impact lots of other customers who likely don't have redunancy between data centers or fire compartments. Additionally depending on your exact setup (it's possible you may have dark fibre directly out of the facilitiy, but often not) you're likely to be uplinked through powered equipment at the same facility, that wouldn't have the extra UPS power either. Plus as others mentioned, the loss of cooling is the biggest problem. In fairness maybe that would buy you a few minutes for the kindof automatic switchover they talked about. But that's quite a bit of co-ordination to have your own power, to not rely on anyone else being powered and then to be able to co-ordinate your shutdown before the room gets too hot - and not be at risk of overheating the room for others. Plus there are safety issues like emergency power out for fire fighters etc which again, I'm sure Google could deal with at a facility-wide level, but if you're a small fish in a much bigger data center its harder to co-ordinate that kindof thing. So while I'm sure it's all possible, there are obvious headwinds in many directions. reply justsomehnguy 11 hours agorootparentprevYour typical company, even if it has hundreds of racks, usually at the scale and workloads needed at Google, where you can plug off the racks and not really notice it. reply jabart 12 hours agorootparentprevPer-Rack UPS is not allowed in our local Flexential DC. Reason being is that in case of a fire, the whole room needs to go dark on power for fire control. We do have two redundant AC circuits on two different breakers. But our DC was from a company that got bought out by Flexential so maybe, hopefully, its setup different. reply iJohnDoe 7 hours agorootparentIt’s a good idea not to have per rack UPS in a colo. Customers are really bad at maintaining equipment Making sure customers replace batteries every five years might be difficult. Also, in my opinion, all UPS manufactures make horrible UPS equipment. There can be so many different types of failures or problems or glitches. You don’t want a UPS leaking or exploding. Also, there are laws in some states that only allow so many pounds of batteries in a building. reply mysteria 13 hours agorootparentprev> Pretty sensible IMHO - I live in a country with a reliable electricity grid, and outages due to UPS malfunction are about as common as power outages. Hence why modern servers and their associated networking equipment have dual power supplies which could be connected to two seperate UPS systems. It would be very unlikely to have them both fail at once. In a less important home/small business scenario typically one supply is connected to the UPS and the other is connected to the wall via a surge protector. reply Dylan16807 13 hours agorootparentprevYou normally reduce the blast radius of UPS failures by having two supplies for each server. So I don't think that's it. The link someone else put about overriding faults might be it, if OP misremembered the problem. reply michaelt 12 hours agorootparentWell in principle, sure. And yet here we are, reading an article about \"a total loss of power [...] following a reportedly simultaneous failure of four [...] switchboards serving all of Cloudflare’s cages. This meant both primary and redundant power paths were deactivated across the entire environment.\" reply Dylan16807 11 hours agorootparentI'm not sure what you mean. Wouldn't we have the same level of outage with per-rack UPSes? reply alberth 10 hours agoprevSingle Point of Failure Is PDX still a single-point-of-failure for Cloudflare services? It was 5-months ago [0], and if I understand the post - it sounds like it still is. If anyone knows, I'd be curious to hear. [0] https://news.ycombinator.com/item?id=38113503 reply vlovich123 9 hours agoparentIt’s complicated and it’s not supposed to be. PDX is where the config plane for the edge network and services is stored. Some of this information is transported to the edge via QuickSilver [1] (e.g. auth tokens). This information is replicated to Europe and fail over is possible. The challenge with the previous outage was a combination of things (as it always is) whereby certain services had a dependency that relied on PDX as a single point of failure (if I recall correctly). That underpinned enough services that a good chunk of Cloudflare’s config plane went down. Additionally, the cutover to Europe didn’t go smoothly because it was instantaneous instead of gradual which resulted in traffic amplification as retries for previous requests & current requests were shuttled into the online data center resulting in a thundering herd. What this blog post talks about it is how this time nothing went down (or at least cut over within minutes due to presumably automated systems noticing & doing it) with the exception of analytics data which does have a single dependency and that’s determined to be “ok” (i.e. it’s an acceptable failure mode for that product & that product only). There are additional failure complications required because PDX (& Europe) is composed of several independent data centers which aren’t supposed to all fail simultaneously. What’s pretty clear from the implied “we’re not happy either” is that Cloudflare isn’t pleased with their vendor’s separately located data centers still having correlated failures. [1] https://blog.cloudflare.com/introducing-quicksilver-configur... reply alberth 8 hours agorootparent> Cloudflare isn’t pleased with their vendor’s separately located data centers still having correlated failures. I hear you, but … … what’s now core fabric of the internet (cloudflare) shouldn’t be suseptable to a single vendors data center going down to - knock out their service. It create a situation where an attacker now knows all they need to do is target that single data center (PDX), cloudflare will go down, and now they can attack any website previously protected by cloudflare. I’m not trying to be a hater. I’m just trying to call out, it’s a bit unfair to solely point the figure at this data center provider when architecturally this issue shouldn’t exist in the first place. reply MallocVoidstar 8 hours agorootparent>cloudflare will go down, and now they can attack any website previously protected by cloudflare. Neither this outage nor the previous broke Cloudflare's protection. reply internetter 9 hours agoparentprevFrom what I understand of what I read, it was a single point of failure for one product instead of 15 reply andrewaylett 13 hours agoprevI can very definitely empathise with the experience of having worked hard at fixing the issues underpinning high priority incidents, then noticing that what previously would have taken hours to fix is now only visible as a blip on a graph. reply llbeansandrice 12 hours agoprevA single k8s cluster spanning multiple datacenters feels mind boggling to me. I know it's not exactly uncommon for HA even if you just have a little one in your cloud provider of choice but I'm sure it's a totally different beast than the toy ones I've created. reply ekimekim 8 hours agoparentAt its core Kubernetes is mostly a HTTP API being used to sync state between nodes. I see no reason that part shouldn't work even across the world, albeit at the cost of slower syncing of state (eg. pods taking longer to be created after a scale-up). That HTTP API is backed by etcd which uses Raft and that is where running over a large area is more likely to cause problems. One approach would be to keep the etcd instances in one region (and probably also co-locate the scheduler, controller manager there), while having far-flung worker nodes. This creates a risk of losing the control plane but in most cases services would keep running (but would be unable to react to any further issues until the control plane recovers). You would also want to carefully design your workloads with topology constraints and region-specific services to avoid high application layer latencies though. Overall it's a fun thought experiment. In practical terms I think cross-datacenter in a small geographic area would work fine but I probably wouldn't want to run a single worldwide cluster, both for the reasons above and for other scaling reasons. reply oceanplexian 6 hours agorootparentYou could implement geographic taints and tolerations and constrain certain workloads to certain regions. Lots of places span clusters across several AZs and have the same problem in theory. However I don’t personally like it, because you’re engineering a cross-datacenter failure domains which is usually a bad design decision unless done for specific reasons. And as for the the control plane, for it to reliably span multiple locations while also tolerating failures, you now have to deal with all kinds of weird split brain scenarios, so most just run multiple clusters instead of rewriting k8s for this kind of design. reply iampims 8 hours agoparentprevIt’s no different than running EKS in Multi-AZ configuration, except for the cross-az egress fees… reply Havoc 10 hours agoparentprevI guess if you have really fast fibre connections between them then the separation isn’t all that separate even with distance reply Jamie9912 8 hours agoprev [–] Interesting, I didn't even hear about that second outage reply slyall 4 hours agoparent [–] Same. My employer was highly impacted by the November 2nd outage but this latest one didn't appear to have affected us at all. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Cloudflare faced a significant power outage at their Portland data center, the second within five months, but thanks to enhancements, they recovered faster with minimal disruptions to services.",
      "The outage resulted from overly sensitive overcurrent protection in switchboards, now rectified, as the team prioritizes finalizing the resilience program for the Analytics platform to avoid manual interventions and teams up with Flexential for operational enhancements.",
      "Cloudflare remains committed to delivering robust services, creating job prospects, and striving for operational excellence."
    ],
    "commentSummary": [
      "Cloudflare experienced a significant data center power failure, leading to talks about deploying rack-level UPS systems for future incident prevention.",
      "Emphasis was placed on power redundancy, equipment maintenance, and safety in third-party-managed data centers, following Cloudflare's outage history and mitigation strategies.",
      "Discussions explored security risks, single points of failure, and solutions like Kubernetes clusters, geographic taints, tolerations, and the possibility of multiple clusters over a single worldwide cluster for improved reliability in distributed architectures."
    ],
    "points": 179,
    "commentCount": 38,
    "retryCount": 0,
    "time": 1712582436
  },
  {
    "id": 39970501,
    "title": "Reverst: Exposing Services Securely with Reverse Tunnels in Go",
    "originLink": "https://github.com/flipt-io/reverst",
    "originBody": "reverst: HTTP reverse tunnels over QUIC Ti esrever dna ti pilf nwod gnaht ym tup i Reverst is a (load-balanced) reverse-tunnel server and Go server-client library built on QUIC and HTTP/3. Go Powered: Written in Go using quic-go Compatible: The Go client package is built on net/http standard-library abstractions Load-balanced: Run multiple instances of your services behind the same tunnel Performant: Built on top of QUIC and HTTP/3 Use-case Reverst is for exposing services on the public internet from within restrictive networks (e.g. behind NAT gateways). The tunnel binary is intended to be deployed on the public internet. Client servers then dial out to the tunnels and register themselves on target tunnel groups. A tunnel group is a load-balanced set of client-servers, which is exposed through the reverst tunnel HTTP interface. Usage reverst tunnel server ➜ reverst -h COMMAND reverst USAGE reverst [FLAGS] FLAGS -l, --log LEVEL debug, info, warn or error (default: INFO) -a, --tunnel-address STRING address for accepting tunnelling quic connections (default: 127.0.0.1:7171) -s, --http-address STRING address for serving HTTP requests (default: 127.0.0.1:8181) -g, --tunnel-groups STRING path to tunnel groups configuration file (default: groups.yml) -n, --server-name STRING server name used to identify tunnel via TLS (required) -k, --private-key-path STRING path to TLS private key PEM file (required) -c, --certificate-path STRING path to TLS certificate PEM file (required) --max-idle-timeout DURATION maximum time a connection can be idle (default: 1m0s) --keep-alive-period DURATION period between keep-alive events (default: 30s) Client Building go install ./client/... Usage See ./client directory for more details. Server Building go install ./cmd/... Testing Reverst uses Dagger to setup and run an integration test suite. dagger call test --source=. The test suite sets up a tunnel, registers a server-client to the tunnel and then requests the service through the tunnels HTTP interface. Running The following walks through experimenting with the ./examples/simple server example. This directory contains a number of things needed to stand up reverst and a registering client server: The example service in ./examples/simple/main.go. Simple self-signed TLS private key and certificate. A tunnel-groups file for routing decisions. Running reverst The following runs the tunnel server with: The QUIC tunnel listener on 127.0.0.1:7171 The HTTP serving listener on 127.0.0.1:8181 Logging with debug level A TLS server-name of flipt.dev.local Some tunnel group definitions with a single tunnel group The group has the name flipt.dev.local The group is reachable under the same host name The group requires basic username and password authentication The dummy TLS certificates go run ./cmd/reverst/... -l debug \\ -n flipt.dev.local \\ -g examples/simple/group.yml \\ -k examples/simple/server.key \\ -c examples/simple/server.crt Running example server Now you can run the example server. This is a simple HTTP server that responds to all requests with PONG. It is setup to use the server client to register as a listener on the tunnel. go run ./examples/simple/main.go --username user --password pass Making requests You can now curl the tunnel and requests will be forward all the way through to the example server. Be sure to include the Host header, as this is used to route requests to the respective tunnel-group. curl -H 'Host: flipt.dev.local' 127.0.0.1:8181/fo",
    "commentLink": "https://news.ycombinator.com/item?id=39970501",
    "commentBody": "Reverst: Reverse Tunnels in Go over HTTP/3 and QUIC (github.com/flipt-io)172 points by todsacerdoti 18 hours agohidepastfavorite22 comments roshanj 18 hours agoThis is great! I've had this exact idea for a specific robotics use-case but never got around to implementing it: a fleet of robots that each expose an HTTP service for debugging purposes. These robots connect to the internet through cellular or hop around among a set of wifi access points, such that long-lived connections are often interrupted and each robot IP address intermittently changes. Many other reverse proxy / tunneling solutions use TCP-based protocols or require the target hosts to be accessible by the proxy server, but in this case QUIC connection migration avoids the reconnection handshakes needed for dropped TCP connections, and your client->server model allows the robots to register themselves from restrictive networks. The only missing feature would be to allow some sort of auth plugin - perhaps as a sub-request made to an external auth service that contains the identifier of the client the request will be routed to, similar to nginx's auth_request (https://nginx.org/en/docs/http/ngx_http_auth_request_module....) reply debarshri 13 hours agoparentGenerally people use Zerotier for this kind of usecase in the industry. It is pretty robust. reply screamingninja 15 hours agoparentprevHave you considered wireguard / tailscale? reply GeorgeMac 18 hours agoparentprevWould love a feature request GH issue for that! Seems totally doable! reply zilti 16 hours agoparentprevBut why http? reply yjftsjthsd-h 15 hours agorootparentNetwork effects? (No pun intended.) HTTP has a big ecosystem. Ex. Everyone already has curl installed. Edit: This isn't hypothetical, either; I literally use curl regularly to query services. Sure, there are other options, but HTTP generally works, so... reply dovholuknf 10 hours agoprevVery neat. Lots of similar tools listed on https://github.com/anderspitman/awesome-tunneling. Seems similar to zrok.io, ngrok, cloudflare tunnels, tailscale funnels and zrok although you're using http/3 explicitly. Personally I work on two similar projects you might want to check out: zrok and OpenZiti. Similar projects, but zrok is closest to what you did here. reply GeorgeMac 1 hour agoparentAmazing! There really is an awesome- for everything haha. Definitely checking these out. reply erebe__ 14 hours agoprevNice project :) HTTP3 may not be suitable for all environments, as UDP is pretty commonly filtered. If you are in such scenario, you may want to take a look at wstunnel, it allows you to do the same (and more) over websocket or HTTP2. https://github.com/erebe/wstunnel reply ongy 5 hours agoparentI just saw, judging by Nick you are probably the author :) Do you have the protocol defined somewhere? Wstunnel is one of our options, and we'll likely add a golang library for the solution we chose. Would be easier if I don't have to figure out the frame format from code. reply ongy 5 hours agoparentprevWe recently ran into issues with http2. Specifically the lack of support in zscaler. We are still looking into something like wstunnel and websockets, though I'm preparing myself for the day when we have to add \"normal\" http1.1 support :( reply fractalnetworks 9 hours agoprevNice, check out the selfhosted-gateway if you're looking for something similar based on nginx + caddy + wireguard: https://github.com/fractalnetworksco/selfhosted-gateway reply sekh 17 hours agoprevThis is a great idea! There are multiple tentatives of achieving the same feature with HTTP/2. A RFC is even being written https://www.ietf.org/archive/id/draft-bt-httpbis-reverse-htt... HAProxy implements something similar in HTTP/2 with the 'rhttp@' keyword. edit: wrong URL. reply GeorgeMac 16 hours agoparentThis is great! Thanks for sharing! reply sekh 15 hours agorootparentI just updated the URL which was the wrong one. reply xyst 18 hours agoprevNot a bad way to exfil data from target server. reply tru3_power 15 hours agoparentAdd it to the toolkit! reply ocdtrekkie 14 hours agoparentprevAny good \"restrictive network\" blocks QUIC. This helps potentially in places the restriction is structural (CGNAT) but not where the restriction is security-based. reply jsiepkes 14 hours agorootparentYou can easily setup a reverse tunnel with something like websockets or HTTP/2 (TCP). For example with wstunnel[1]. [1] https://github.com/erebe/wstunnel reply 20thr 16 hours agoprev [–] This is very cool. We built something similar in https://github.com/namespacelabs/breakpoint but the more general purpose nature here is great. reply GeorgeMac 16 hours agoparentThis is very cool. Checking it out! Thanks! reply therein 13 hours agoparentprev [–] This is very useful. I needed this at least 10-20 times in the past but didn't know it existed. I no longer need it in a CI context but I could imagine this getting really handy when some weird thing happens during the build stages of a docker container too. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Reverst is a load-balanced reverse-tunnel server and Go server-client library based on QUIC and HTTP/3, facilitating service exposure on the public internet from restricted networks.",
      "It supports multiple service instances behind one tunnel, where the tunnel binary is placed on the public internet, and client servers join target tunnel groups.",
      "To use Reverst, run the tunnel server and example server, and send requests via the tunnel using curl."
    ],
    "commentSummary": [
      "The post introduces Reverst, a project using reverse tunnels in Go over HTTP/3 and QUIC, beneficial for debugging when dealing with frequently changing IP addresses and interrupted long-lived connections.",
      "Alternative tools like Zerotier, wireguard, and wstunnel are also mentioned as options to achieve similar objectives.",
      "It provides suggestions for enhancements, references related projects, and additional resources for further exploration."
    ],
    "points": 172,
    "commentCount": 22,
    "retryCount": 0,
    "time": 1712588652
  },
  {
    "id": 39968384,
    "title": "Securing KDE6: Addressing D-Bus and Polkit Vulnerabilities",
    "originLink": "https://security.opensuse.org/2024/04/02/kde6-dbus-polkit.html",
    "originBody": "KDE6 release: D-Bus and Polkit Galore Apr 2, 2024 • Matthias Gerstner, Filippo Bonazzi (proofread) Table of Contents Introduction D-Bus Overview Polkit Overview Security Relevance of D-Bus and Polkit The KDE KAuth Framework Problematic use of QVariantMap Serialized Data Problems with generated D-Bus drop-in Configuration Snippets Legacy fontinst D-Bus service Woes with “unexpected” Polkit Settings Problematic File System Operations in sddm-kcm6 Unsafe Operations on File System Paths Provided by the Unprivileged D-Bus Client Changes in configuration files owned by the sddm service user Going Forward from Here KWalletManager: Pseudo-Authentication to Protect the Configuration Improvements in DrKonqi Conclusion References Change History Introduction The SUSE security team restricts the installation of system wide D-Bus services and Polkit policies in openSUSE distributions and derived SUSE products. Any package that ships these features needs to be reviewed by us first, before it can be added to production repositories. In November, openSUSE KDE packagers approached us with a long list of KDE components for an upcoming KDE6 major release. The packages needed adjusted D-Bus and Polkit whitelistings due to renamed interfaces or other breaking changes. Looking into this many components at once was a unique experience that also led to new insights, which will be discussed in this article. For readers that are new to D-Bus and/or Polkit, the following sections offer a summary to get a better idea about these systems. D-Bus Overview The D-Bus message bus system provides a defined way to implement remote procedure calls in applications. On Linux it is usually only used locally, although the D-Bus specification also allows for operation over the network. A D-Bus service is a program that provides one or more interfaces that can be invoked by clients to obtain information, trigger operations and so on. The D-Bus specification defines a set of data types that can be passed to and returned from D-Bus method calls. D-Bus applications reach each other by connecting to a shared bus of which there exist two predefined types: the system bus and the session bus. Services that perform system wide tasks connect to the system bus. These services often run as root or as dedicated service users. A session bus, on the other hand, is created for each (graphical) user session, and only applications running with the privileges of the logged-in user can connect to it. No special privileges are involved with the session bus. Its main purpose is to provide a defined API for session wide services, like a desktop search engine. Polkit Overview Polkit is an authorization framework that allows (privileged) applications to decide whether a user in the system is allowed to perform a specific action. These actions allow for a more fine-grained authorization model when compared to a plain root vs. non-root decision. Examples could be an action to enable a Bluetooth device in the system, or to mount a removable storage device. A Polkit policy configuration file declares actions used by a certain application domain and the authentication requirements for it. When an actor in the system asks an application that uses Polkit to perform an action, then this application in turns asks the system-wide Polkit daemon whether this actor is privileged to do so. Depending on the context this can, for example, lead to a password prompt being displayed in a user’s graphical session to authorize the operation. Polkit is independent of D-Bus, but the combination of both is a very common pattern. Other manners in which Polkit can be used is in setuid-root binaries or via the sudo-like pkexec utility. Security Relevance of D-Bus and Polkit The typical setup of D-Bus and Polkit is as follows: a system daemon is running with full root privileges and registers a service on the D-Bus system bus. An unprivileged user that is logged into a graphical session asks the daemon via D-Bus to perform an activity. This triggers the Polkit authentication process to determine whether the caller is allowed to do this. Security-wise, there is quite a number of things that can go wrong in this scenario. The following sections investigate typical issues that can arise. Covering all Privileged Code Paths The system daemon actually needs to implement the Polkit authorization check properly for every sensitive D-Bus method it offers. Polkit is not something that is magically turned on, but the privileged component needs to identify all the code paths that need to be protected by it. Some applications deliberately offer a mix of unauthenticated and authenticated D-Bus methods. In these cases it can sometimes be hard to keep all the possible side effects and outcomes in mind, which can lead to security issues when something is overlooked. Acting as root on Behalf of Unprivileged Users The privileged D-Bus service component often needs to act on behalf of an unprivileged client. An example could be mounting a file system in the caller’s home directory, or processing a file provided by the caller. This is a classic crossing of privilege boundaries. Developers of such services are often not aware of the problems that can arise, especially when accessing user controlled paths as root. Similarly, if a privileged D-Bus service stores data from multiple users in a shared system directory, then information leaks can occur by storing files with too open permissions, or by mixing up different user contexts. The Integration of Polkit can be Hard Polkit has its own nomenclature and design principles that one needs to get into, to fully understand it. Apart from this, even if Polkit is correctly asked for permissions, the privileged service still needs to correctly evaluate the result. A typical mistake that can happen in this area is when a privileged service does ask Polkit correctly for authentication, but the result is simply ignored and the privileged operation continues regardless. Everybody can Access the D-Bus System Bus By default, all local users can access the D-Bus system bus and talk to most of the privileged services. Individual D-Bus service configuration files can limit the scope of users that are allowed to invoke a D-Bus service’s methods. This setup is the exception, however, as the majority of D-Bus services is accessible to all users. This increases the attack surface notably, as not only an interactive user account that is running an authorized local session can talk to these services, but also e.g. the nobody user account. These days, many system daemons running on the network only have limited privileges or even use dynamically allocated users provided by systemd. If one of these network daemons with low privileges can be exploited, then weaknesses in privileged D-Bus system services can offer the possibility to further escalate privileges. This is one of the reasons why, as part of a defense-in-depth strategy, the SUSE security team looks closely also into these components that aren’t directly attached to the network. The KDE KAuth Framework The KDE desktop environment is a heavy user of D-Bus services both on the system and on the session bus. It adds further abstractions on top of D-Bus and on top of Polkit. The base component for this is the KAuth framework. KAuth generates D-Bus configuration files and some glue code to integrate D-Bus and Polkit into KDE applications. In KAuth, a privileged D-Bus service running as root is called a KAuth helper. We performed a dedicated follow-up review of it for the KDE6 release. A former member of the SUSE security team had found a major security flaw in this glue code in 2017. Since the audit at the time was comprehensive, we did not expect to find any major issues in the core authorization logic anymore, and in fact we didn’t. Problematic use of QVariantMap Serialized Data A peculiarity of KAuth is that, instead of the native D-Bus data types, only binary blob objects are transferred on D-Bus level, that are based on the QVariantMap data type offered by the Qt framework. During the review we noticed that the implementation of this feature in KAuth is a bit shaky, since potentially attacker controlled data is processed during Qt data type deserialization, before the actual D-Bus function callbacks are even invoked. In 2019, the upstream authors had already identified this problem, that can lead to side effects like image data being deserialized, where actually only strings and integers are expected. The KAuth code currently meddles with internal Qt framework state to prevent such side effects. Problems with generated D-Bus drop-in Configuration Snippets Only late in our review efforts we realized that a change introduced with the KDE6 release of KAuth leads to overly open D-Bus configuration files being generated. Per-package configuration snippets for D-Bus are installed in “/usr/share/dbus-1/system.d”. These configuration files serve as a kind of firewall configuration for the D-Bus system bus. They define who is allowed to register a D-Bus service for a certain interface and also who is allowed to talk to it. Here is a proper example taken from systemd-network’s “org.freedesktop.network1.conf”:This allows only the dedicated service user “systemd-network” to register the D-Bus interface “org.freedesktop.network1”, while any other users in the system may talk to it. The KAuth KDE6 release candidate generated this configuration instead:The ramifications of this can easily be overlooked: this states that everybody is allowed to talk to everything on the D-Bus system bus. It also affects other D-Bus services that should not be influenced by the drop-in configuration snippet shipped for individual KDE packages. While most D-Bus services running on the system bus are “public”, i.e. everybody is allowed to talk to them, some services follow a different security model in which only dedicated users are allowed to interact with the service. We identified ratbagd as one such D-Bus service that would be negatively affected by this defect in KAuth. This shows that the security posture of unrelated packages is at stake. Luckily we identified this issue in time before the KDE6 release was finished, and the issue was fixed before it reached production systems. We also checked any non-KDE D-Bus configuration files we ship on openSUSE Tumbleweed for the same issue, but luckily found no further files containing this issue. These side effects are also in some sense shortcomings of the D-Bus configuration scheme, since developers of a specific D-Bus service don’t expect that their configuration file has a global influence. A similar issue exists for logrotate where settings in drop-in configuration files in “/etc/logrotate.d” can influence global settings that affect the complete system. This can lead to hard to find bugs in both cases, D-Bus and logrotate, because the outcome also depends on the order in which the configuration file snippets are parsed. Legacy fontinst D-Bus service Most of the KDE components that we have been requested to look into for the KDE6 release had already been reviewed by us in recent years. A few of them are legacy packages though, since they were already in stock when we introduced packaging restrictions for D-Bus and Polkit. At the time we didn’t have enough resources to check all of them in one go. One such legacy component we encountered while looking into the KDE6 release was the “org.kde.fontinst.service” which is part of the “plasma6-workspace” package. What we found there is a single D-Bus method “org.kde.fontinst.manage” that actually multiplexes a whole range of sub-methods, based on a “method” string input parameter. This is bad design since it undermines the D-Bus protocol, and thus makes the individual method calls less visible and less manageable. This is reinforced by the fact that also only a single Polkit action is used to authenticate all the sub-methods. This way there is only an all-or-nothing setting for the various code paths that are hidden behind this single D-Bus method call. The available sub-methods in this service nearly make up a generic file system I/O layer, especially when we remember that this service is running with full root privileges: install: this can be used to copy arbitrary file paths to arbitrary locations, the new files end up with mode 0644. uninstall: this allows to remove arbitrary file paths, as long as their parent directories have a writable bit set. move: this allows to move arbitrary paths complete with new owner uid and group gid to arbitrary new locations. toggle: this takes raw XML that also seems to specify font paths that are to be enabled or disabled. removeFile: does what is says on the label; another way to remove files. configure: saves modified font directories and invokes a small bash script fontinst_x11 that prepares font directories and triggers a font refresh at the X server. The core business logic of the fontinst service should be managing system wide fonts provided in the system. To achieve this, ideally only the necessary high level logical operations should be offered like: Install a font from provided data, remove a system font by name. Copying, removing and moving arbitrary files is way outside of the scope of what this service is supposed to do. The single Polkit action “org.kde.fontinst.manage” requires auth_admin_keep authorization by default i.e. anybody that wants to invoke this method needs to provide admin credentials. Still, if an admin decides to lower these requirements, because users should be able to e.g. install new fonts in the system, then this interface does not only allow that, but also allows to gain full root privileges by copying arbitrary files around (e.g. by creating a new “/etc/shadow” file). This service requires a larger redesign. KDE upstream was not able to come up with that in time for the KDE6 release. We hope that it will still happen though, as the API is in a rather worrying state. Woes with “unexpected” Polkit Settings The situation in the fontinst service regarding the auth_admin setting is a common pattern that we see when reviewing D-Bus services and Polkit actions. Developers believe that requiring auth_admin authentication for a Polkit action is enough to justify overly generic APIs or unsafe file system operations carried out as root. In some cases it might be justifiable to say that an action should never have weaker authentication requirements than auth_admin, since it otherwise causes uncontrollable security issues. One should not forget that Polkit is a configurable authentication framework, though. There are default settings shipped by applications, but system integrators and admins are allowed to change these requirements. The (open)SUSE Linux distributions are the only ones we know of, that offer a defined mechanism for admins to override the Polkit defaults for individual actions via profiles and overrides. This works via the polkit-default-privs package. Our experience with this shows that upstream developers mostly neither consider the security consequences of lowering the Polkit authentication requirements, nor test what happens when the authentication requirements are raised for hardening purposes. Raised authentication requirements lead to additional password prompts, and some applications implement workflows involving Polkit actions that lead to very unfortunate behaviour in such cases. A common example of this is a package manager like Flatpak, that attempts to acquire Polkit authentication for a repository refresh action upon login into a graphical session. The developers only test this with the default yes Polkit authentication requirement, which makes this authentication process invisible to users. When raising this to auth_admin, then suddenly a password prompt pops up during login and users are confused and annoyed. There are ways to deal with this: for example, services using Polkit can ask it whether an action can be authorized without user interaction. If this is not the case, then a package manager could choose not to refresh repositories just now. Also multiple actions can be authenticated in groups using the “org.freedesktop.policykit.imply” annotation to avoid multiple password prompts coming up for a single workflow. It is understandable that the configuration management of many different Polkit configurations is hard to test for upstream developers. Increased awareness of the general problems in this area would help to avoid these issues in the first place. It seems that developers just want to “cram in” authentication into their software, though, and stop thinking about it once they’re done. Granted, Polkit and D-Bus are far from simple when you’re new to them. Still, every authentication procedure should be given careful thought. The take home lessons for developers implementing Polkit should be: Polkit is a configurable authentication framework and the settings intended by developers might not be what actually happens during runtime. When modelling Polkit actions, one should take advantage of the possibility to make them fine grained, to allow users to fine tune the requirements for individual activities. Each Polkit authorization that happens in an application should be given some thought in both directions: what happens if the authentication requirement is lowered and what happens if it is raised? Another aspect that hasn’t been discussed yet is the topic of authentication messages shown to the user. They should clearly state what exactly is being authorized in a form that non-technical users can understand. Polkit also supports placeholders in messages to fill in runtime information, like a file that is being operated on. Sadly, this feature is used very rarely in practice. Problematic File System Operations in sddm-kcm6 This component is a KDE Configuration Module (KCM) for the SDDM display manager. It contains a D-Bus service “org.kde.kcontrol.kcmsddm.conf”. We reviewed it already in the past and did so again for the KDE6 release. The service has two major problems, discussed in the following sections. Unsafe Operations on File System Paths Provided by the Unprivileged D-Bus Client Multiple of the D-Bus methods provided by the sddm-kcm6 KAuth helper expect file system paths as input parameters. Such passing of paths to privileged D-Bus services is another problematic pattern that is often encountered. In the openConfig() function, the provided path to a SDDM theme configuration file will be created by the helper, if necessary. If it already exists, then a chmod() of the path to mode 0600 is performed, which is also following symlinks. To see how this can be problematic, consider what happens if “/etc/shadow” is passed as theme configuration path. Operating as root, on files that are under control of an unprivileged user, is notoriously hard to get right, and requires careful use of lower level system calls. Often developers aren’t even aware of this problem. KDE components have had a number of problems in this area in the past. We believe this has deeper roots, namely in the design of the Qt framework’s file system API, which on the one hand doesn’t allow full control over the lower level system calls (owed to the fact that Qt is also a platform abstraction layer), and on the other hand does not document exactly what can be expected of its APIs in this regard. Furthermore the Qt framework itself isn’t aware of the fact that it runs as root, possibly operating on files owned by other users. The Qt libraries are designed for implementing feature rich GUI applications and don’t really consider handling untrusted input, operating with raised privileges and crossing privilege boundaries. An elegant way to avoid the path access issue in the first place is by not passing file paths, but already opened file descriptors over D-Bus. This is possible since D-Bus uses UNIX domain sockets internally, and they can be used to pass file descriptors. So instead of passing a string from client to service suggesting “Open this file, trust me, it’s fine”, the client passes a file descriptor, opened using its own low privileges, to the privileged service. With this, many path access issues are gone in an instant. There are cases that still require care, however, for example if recursive file system operations need to be carried out. Unfortunately the KAuth framework used by KDE shows a limitation in this area. Since the KAuth helper’s D-Bus API only transfers binary blobs that result from serializing QVariantMap, there is currently no possibility to pass an open file descriptor. Changes in configuration files owned by the sddm service user The other problem is not found in the D-Bus API, but in the implementation of the sync() and reset() D-Bus methods. Once any input parameters from the client are processed, the helper operates in the home directory belonging to the sddm service user. Here is some condensed code taken from the reset() and sync() functions: // from SddmAuthHelper::reset() QString sddmHomeDirPath = KUser(\"sddm\").homeDir(); QDir sddmConfigLocation(sddmHomeDirPath + QStringLiteral(\"/.config\")); QFile::remove(sddmConfigLocation.path() + QStringLiteral(\"/kdeglobals\")); QFile::remove(sddmConfigLocation.path() + QStringLiteral(\"/plasmarc\")); QDir(sddmHomeDirPath + \"/.local/share/kscreen/\").removeRecursively(); // from SddmAuthHelper::sync() QString sddmHomeDirPath = KUser(\"sddm\").homeDir(); QDir sddmCacheLocation(sddmHomeDirPath + QStringLiteral(\"/.cache\")); if (sddmCacheLocation.exists()) { sddmCacheLocation.removeRecursively(); } QDir sddmConfigLocation(sddmHomeDirPath + QStringLiteral(\"/.config\")); if (!args[QStringLiteral(\"kscreen-config\")].isNull()) { const QString destinationDir = sddmHomeDirPath + \"/.local/share/kscreen/\"; QSet done; copyDirectoryRecursively(args[QStringLiteral(\"kscreen-config\")].toString(), destinationDir, done); } A compromised sddm service user can exploit these operations to its advantage: it can cause a denial-of-service by e.g. placing directory symlinks to have the D-Bus service operate in completely different file system locations. This attack is limited though, since the final path components used in removal calls need to match, like kscreen. it can cause the “kscreen-config” to be copied to arbitrary locations by placing a symlink in “~/.local/share/kscreen”. To make these operations safe, it would be best to temporarily drop privileges to the sddm user. Going Forward from Here KDE upstream was not able to come up with a redesign of this D-Bus service in time for the KDE6 release. In this instance, the unsafe operations in the sddm user’s home directory would formally even justify assignment of a CVE. Since all the D-Bus methods are guarded by auth_admin Polkit authentication requirements, the issues can at least not be exploited in default installations. KWalletManager: Pseudo-Authentication to Protect the Configuration KWalletManager is KDE’s password manager. It features a GUI and, as one would expect, runs in the context of the graphical user session of a logged-in user. It ships a “savehelper” service that offers a single D-Bus method “org.kde.kcontrol.kcmkwallet5.save”. So what does a service helper running as root need to save here? Let’s look at the implementation: ActionReply SaveHelper::save(const QVariantMap &args) { Q_UNUSED(args); const qint64 uid = QCoreApplication::applicationPid(); qDebug() << \"executing uid=\" << uid; return ActionReply::SuccessReply(); } Turning this piece of code carefully to all sides will lead to the insight that it does nothing. We asked upstream to remove this unused helper, but we’ve been told that this is not a mistake, but on purpose. They want to protect against the following attack scenario: a user leaves their computer alone and unlocked, a random person gets by and, of all things, wants to change KWalletManager’s settings. To prevent this from happening, the GUI is asking the service helper to authenticate the action requiring Polkit’s auth_self authorization, and doesn’t continue if this fails. This cannot stop a real attacker, though, since the KWalletManager configuration is stored in the unprivileged user’s home directory and can still be edited directly, or using a modified version of KWalletManager that simply does not ask for this authentication. Not to talk about all the other things that an attacker could do in such a situation. So where should one draw a line to stop? We don’t even see this as a hardening, it is fake security and confusing. If such a fake authentication is really needed then at least a way should be found to implement it, without requiring an authentication helper running as root that does nothing. Upstream seems to disagree, but we asked our packagers to remove this logic from our packaging via patches. Improvements in DrKonqi DrKonqi is KDE’s crash handling utility. These days, it interacts with systemd-coredump to access core dumps of applications. Our previous 2022 review of it led to a finding in systemd-coredump itself. In the meantime DrKonqi obtained additional D-Bus service logic to copy a private core dump (e.g. from a process that was running as root) into the session of an unprivileged user for analysis. The implementation of this is unusual for a KDE component in so far as it doesn’t rely on KAuth: it directly uses the Qt framework’s D-Bus and Polkit facilities. The likely reason for this is the shortcoming of KAuth with regard to passing file descriptors, as discussed above. The single excavateFromToDirFd() D-Bus method actually accepts a file descriptor. It is supposed to be a file descriptor referring to a directory under control of the unprivileged caller, where the selected core dump is to be copied to. Even though this means that DrKonqi cannot benefit from the common framework features of KAuth, it is security-wise a good example of how to improve the robustness of a D-Bus service running as root and operating in the file system. Unfortunately, even with file descriptors issues can arise, as this example also shows. The permission handling for directories is different from regular files. Directories generally can only be opened in read-only mode (O_RDONLY). Write permissions are only checked at the time a write attempt is made, like when calling renameat() in the case of the DrKonqi helper. This is too late. The unprivileged caller can open just any directory it has read access for and pass it to the D-Bus service. The D-Bus service running as root will now happily create new files in the directory even if the caller doesn’t have any write permissions for it. There is a constructive discussion discussion going on with upstream that led to various improvements in detail in this D-Bus service that are about to be merged. The issue with the dir file descriptor was only found late in the process, but hopefully a solution for the problem will be found soon. Conclusion D-Bus and Polkit have their share of complexities that need to be understood and managed well. This is important as a defense in depth measure even beyond the local security of a Linux system. Putting additional layers on top, like in the KAuth framework, can cause long-term problems, as can be seen from the lack of support for passing file descriptors with the current KAuth API. It was helpful that our KDE packagers and upstream approached us early about the KDE6 release candidate and the changes it introduces. In some areas, like the badly generated D-Bus KAuth configuration files, upstream quickly reacted and applied fixes, thus avoiding that the problematic code was ever released in a production version of KDE6. In other areas, like the legacy fontinst D-Bus service or the sddm-kcm D-Bus service, the complexity of fixing API issues has obviously been too high for upstream to come up with something better in time. We decided not to ask for CVE assignments for the findings in these services, since the attack vectors are not reachable to regular users in the default Polkit configuration. By now most KDE6 packages should have reached openSUSE Tumbleweed and can be used in production. References D-Bus Project Page Polkit Manual KDE KAuth Framework Documentation Change History 2024-04-05 Minor spelling fixes; inserted an introductory paragraph to Unsafe Operations in sddm-kcm6.",
    "commentLink": "https://news.ycombinator.com/item?id=39968384",
    "commentBody": "KDE6 release: D-Bus and Polkit Galore (opensuse.org)163 points by milliams 23 hours agohidepastfavorite68 comments 1oooqooq 19 hours agothe actual end user security could get the same love. the privilege escalation dialog is mostly a windows 10 copy, but just shows: allow dbus.something.something\"? the name is always meaningless and have no parameters. and there's zero way to get more information. windows at least shows the binary or PowerShell command plus the arguments. reply milliams 17 hours agoparentI agree, it always feels strange putting my root password into a box that just popped up. True, it's only ever been in direct connection with a command line program I'm running, but there's nothing hard linking the terminal with the GUI window. I think that either the terminal program should print something like a PIN which is repeated in the window to cross-check, or the window should be able to use some restricted window manager feature (like Windows' fullscreen greying out thing) to prove that it's not just a random Qt/GTK window from an attacker's script. reply Cu3PO42 17 hours agorootparentThe problem is that in a sense it is just some random Qt/Gtk window. I have recently written my own PolKit agent. It runs purely in userspace without any special privileges. An attacker could kill your legitimate PolKit agent, register itself as your agent, even act as one, and also steal your password the next time you do actually authenticate something. In that sense a PIN linking the command line to the agent window doesn't save you. I agree this situation needs improving, the problem just runs much, much deeper. On the other hand: if an attacker placed a modified sudo that steals your password in ~/.local/bin, it would also be Game Over. Much of the current security model breaks as soon as the attacker has code execution. EDIT: I'd also like to highlight this: > or the window should be able to use some restricted window manager feature (like Windows' fullscreen greying out thing) to prove Your Wayland compositor also runs in userspace. Even if it is supposed to check that some calls come from euid 0, an attacker may be able to circumvent them if the attacker runs in the same context as the compositor. Again, the security problem runs very deep. reply kaba0 14 hours agorootparentAs much as I love linux, it is very painful to accept the state of “security”, this is beyond criminal. Especially that many people just put their head into the sand. The age-old xkcd about linux being secure only about your video card driver is still true to this age, with no clear sign of improvement. Also, frankly I don’t understand why there is no more cross-pollination between android and linux userspace - the former has actually solved this issue properly. I’m afraid the answer is that “it is not written in c”. reply tredre3 12 hours agorootparent> I’m afraid the answer is that “it is not written in c”. I don't think it has anything to do with it. Android's security comes from design, not language. Each application runs under its own UID and not all applications can draw arbitrary content on your screen, for example. Those things could be on the Linux Desktop. And maybe flatpak will bring it to us. But for now I can already hear the screeching about \"freedom\" and \"the Unix philosophy\". reply hulitu 3 hours agorootparent> Android's security comes from design \"This application requires storage permission. Grant ?\" Yes. Comes from design. /s reply KETHERCORTEX 3 hours agorootparentStill better than Flatpak's \"install time permissions\". reply DEADMINCE 17 hours agorootparentprev> the window should be able to use some restricted window manager feature (like Windows' fullscreen greying out thing) No FOSS desktop is even close to offering this. The windows 'greying out thing' is an entirely separate desktop instance, for example. A lot of security consideration was put into designing it. I don't think any FOSS desktop is working on anything even remotely similar, which is a shame. reply vbezhenar 15 hours agorootparentWhat prevents any random Windows application to make a screenshot and apply \"greying out\" filter to it? I mean, I launch a game and it puts full-screen image of whatever it wants. It doesn't take any special permissions to make a screenshot either. The only true security feature I remember was implemented in Windows many years ago. After you got security prompt, you would need to press Ctrl+Alt+Delete and type password there. Supposedly only Windows itself could handle that combination. reply asimops 11 hours agorootparentNothing will prevent that. The question is, what would you do with the stolen credentials in terms of privilege escalation? Suppose we are in a situation where the UAC is on and asks for every action on the secure desktop. Even if you were to authenticate with the stolen credentials, you would end up in an unprivileged session due to UAC's token filtering mechanisms. If you were then to request elevation, the UAC would prompt the user on the secure desktop, which you cannot control from within the user session. It is therefore not enough to imitate the prompt. You also need to convince the user to accept the real prompt. reply aseipp 11 hours agorootparentprevIf you write a piece of malware that spoofs the secure desktop prompt, then you can steal the users password or whatever. But that does not grant you higher privileges. Only the secure desktop can grant your process higher privileges. The password only authenticates you. It does not authorize you. So now your malware tries to use the credentials it stole to perform a restricted task afterword, but it will not have the proper privileges (elevated security token), so the user will get forced into the UAC prompt on the Secure Desktop. You cannot stop this from happening once you attempt to do it. The exact scenario you are describing, spoofing the secure desktop prompt, was literally the basics of the UAC prompt thought up over 15 years ago. It's like the 101 example. In contrast, on Linux, if you put a shell script in the users $PATH called 'sudo' that simply captures their password, you can (assuming the default PAM configuration) immediately use that to elevate privileges anytime you want with absolutely no intervention whatsoever. Because the password not only authenticates you, it authorizes you as well. reply DEADMINCE 14 hours agorootparentprev> What prevents any random Windows application to make a screenshot and apply \"greying out\" filter to it? I mean, I launch a game and it puts full-screen image of whatever it wants. It doesn't take any special permissions to make a screenshot either. Because you would be able to alt+tab to other tasks, and that application would be able to still execute code as it was able to before. It's not any kind of security 'behind the scenes'. > The only true security feature I remember was implemented in Windows many years ago. Or perhaps you didn't realize all the engineering that was taking place behind just what you see. This[0] blog post gives a good overview of some of the security architecture that went into making that elevation prompt. Honestly, MS has been leading the pack in OS security for some time. They are leagues ahead of MacOS and Linux. [0] https://learn.microsoft.com/en-us/archive/blogs/uac/user-acc... reply vbezhenar 14 hours agorootparentWhy would I do \"alt-tab\"? Do you think users typically type alt-tab when asked for admin permissions? I never did that. It's all security theater. reply DEADMINCE 12 hours agorootparentIt isn't security theater at all. Did you read the link? You wouldn't be able to switch to another application is the point. What you suggest greying out, would easily allow other applications to hijack that prompt or replace it with their own. That's a large part of what 'Secure Desktop' aims to prevent, and it succeeds at doing so. reply asimops 11 hours agorootparentThat's not the point they were trying to make. They were asking what would prevent malware from imitating the UAC prompt to steal administrator privileges. reply Scion9066 8 hours agorootparenthttps://learn.microsoft.com/en-us/windows/security/applicati... > If the policy setting is set to Prompt for credentials, malware imitating the credential prompt might be able to gather the credentials from the user. However, the malware doesn't gain elevated privilege and the system has other protections that mitigate malware from taking control of the user interface even with a harvested password. So just knowing the credentials is not enough, they have to be used in the right context (the secure desktop). reply DEADMINCE 8 hours agorootparentprevI understand and answered that. The answer is Secure Desktop, details on which can be found in the link I posted in my earlier comment. reply binkHN 21 hours agoprevI don't know much about openSUSE, but it's nice to see the security effort that goes on before importing a large update like KDE6. reply brnt 20 hours agoparentIt must be nice to get such a review of your code and design, and it looks like KDE appreciated it (probably they expected it, hence the early reach out). Kumbaya but non sarcastic :) reply malkia 14 hours agoprevWhy was this not done with RAII - https://invent.kde.org/frameworks/kauth/-/commit/fc70fb0161c... - if there was an exception between here QVariantMap args; QDataStream s(&arguments, QIODevice::ReadOnly); s >> args; Then it won't restore the global. Also ... global ugh reply surajrmal 19 hours agoprevThese sorts of articles reaffirms to me that there is a dire need to switch to capability based security models. Managing the security with the set of tools we have available in the legacy model leaves lots of room for error. reply DEADMINCE 17 hours agoprevI hesitate to use the word 'bloat', but ever since DKE4 with that Avahi service or whatever it was, that's the impression I've had of KDE. It almost feels like a separate OS on top of an OS. I guess it's just not for me. About a year ago I discovered AwesomeWM and just how flexible and configurable it is - I can truly have a 100% completely customized desktop down to every detail. Even without that though I'd probably opt for something like XFCE if I wanted something with a desktop and taskbar. There's just no good reason a desktop has to be as heavy as more popular options. Even the Windows desktop isn't as heavy. reply Shared404 16 hours agoparent> Even the Windows desktop isn't as heavy. Have you used Windows recently[0]? KDE is definitely lighter than Windows, in both mental load and RAM usage. At least as of late Windows 10 and early Windows 11, which were the last times I used it with any regularity. I'll stick with sway though for now, and I'm excited for the new Cosmic version coming soon. [0] Recently is relative of course. It's been a while for me. reply sho_hn 15 hours agoparentprevIf you haven't revisited it since those days, Plasma 5 eventually achieved a reputation for being relatively light-weight - a default session was, at some points (maybe still is), lighter than XFce, getting it design wins on lower-powered ARM devices like the Pinebook at the time. This was the result of projects like Plasma Mobile, and also the community intentionally focusing on hardware vendors and embedded as a growth area (as now seen with deployments like the Steam Deck or Mercedes-Benz cars). Criticism of KDE 4 as bloated is well-taken, but we've since done quite a lot to repent :-) Disclaimer: I'm a KDE contributor. reply DEADMINCE 14 hours agorootparent> If you haven't revisited it since those days, Plasma 5 eventually achieved a reputation for being relatively light-weight - a default session was, at some points (maybe still is), lighter than XFce That is very interesting! I will definitely give it another look. Is it still running that service that started with 'a'? What was the reason for that? I always found that so frustrating. I remember fighting with it to try and kill it or disable it way back in the day. > Criticism of KDE 4 as bloated is well-taken, but we've since done quite a lot to repent :-) Disclaimer: I'm a KDE contributor. Thank you for all the work you do! reply milliams 14 hours agorootparentI think you're thinking of \"Akonadi\", a service used for calendars, contacts, emails, RSS etc. It famously originally came with a MySQL server dependency (though I think that it might now work with SQLite). reply jraph 13 hours agorootparentI just checked, Akonadi still uses MySQL on Plasma 6. But you can probably disable it (in settings > background services > Account manager (translated from French so possibly not accurate)) if you don't use any PIM feature of KDE (contacts, mails). Now, it's quite idle, so one wouldn't need to worry about it I think. Baloo, Nepomuk's successor, has also improved a lot and file indexing can be disabled, or folders excluded from indexing. And I agree about Plasma being lightweight: that's what I run on a 12 year old tablet which has 1GB of RAM because it has been the only touch-friendly DE that was lightweight enough for years. Now, we have Plasma mobile and Phosh that might do the job better. reply DEADMINCE 13 hours agorootparentRequiring MySQL for a desktop service running in the background honestly seems crazy to me. reply jraph 12 hours agorootparentApparently one can switch databases and use PostgreSQL or SQLite instead. Not sure why SQLite is not the default. Maybe they expect several applications to write to the DB concurrently and expect that SQLite might cause perf issues. Though there's nothing in the doc about this. reply kaba0 1 hour agorootparentprevWhy? I mean, if there is a single, even slightly heavy db running for a desktop environment that would seem fine. Of course each service having its own is an overkill. reply redeeman 11 hours agorootparentprevthey used mysql embedded... many softwares on windows uses sql server embedded, for just specific applications reply MSFT_Edging 16 hours agoparentprevI moved to i3 then to Xmonad after years of Gnome, I felt similar, just too much junk. Recently I set up a few new laptops and decided to try KDE as a \"base layer\" for various utilities I need, as its easier to install the DE and get the utilities as deps than download them all myself. The annoying truth about tiling WMs that I ignored for years is man it takes so much time to get set up exactly how you like it. I could drop in my xmonad setup like I've done on other machines, but I want to try one of the wayland offerings and just the time commitment is making me put it off. That being said, the most recent KDE plasma desktop is extremely snappy and polished feeling. More so than the windows desktop, and I feel like its 85% as smooth as say MacOS. I've yet to test battery life compared to a bare bones suckless tiling setup, but I'm still getting very good battery life without it. reply bityard 15 hours agoparentprevI have not tried KDE 6 yet, but I have been running KDE 5 on Debian for about 1.5 years and with few exceptions, I find it to be fast, relatively bloat-free, and almost infinitely configurable. On my systems, it uses something like 1.2 GB of memory out of the box. Meaning you have a _very_ usable machine even with only 4 GB of RAM, as long as you can avoid the temptation to run Electron apps. Pretty sure that is impossible with Windows. I've tried basically every Linux DE and WM since I first started using Linux in 1996, and basically haven't been happier with the overall experience than I am now. Which scares me a little bit, because every time I get comfortable with a DE, someone decides to throw it all out and rewrite the thing from scratch. (GNOME 2->3, KDE 3->4, etc.) reply tredre3 12 hours agorootparent> Pretty sure that is impossible with Windows. A fresh Windows 10 install on a system with 4GB of ram will idle at around 1GB used. Perhaps your comment is correct in regards to Windows 11, though. KDE6 has been pretty good to me, you should try it! Or wait for 6.1. reply not_your_vase 16 hours agoparentprevI feel you. I have become a sway and xmonad user on my machines after years of riding KDE. I was daily driving Neon (the official KDE distro) for about 2 years before I have had enough of the bloat and the general direction of the desktop env. I have a more and more stronger feeling that most of these projects are developed without an actual usecase. Which is fine of course, OSS is about scratching your own itch, not other's... but nowadays it's almost a sin to mention that something is buggy/unusable. reply natrys 15 hours agoparentprevOn the flip side, AwesomeWM or Xmonad users like us are stuck with Xorg for the foreseeable future. Maybe not a problem for here and now, but a concern in the long term for sure. Shame that the barrier to entry to creating an independent WM for Wayland is so high that very few exist yet and none that can substitute those two (yes I am aware of Sway). reply orhmeh09 16 hours agoparentprevJust FYI, XFCE has a dbus dependency, so it might not be what you are looking for if that's a problem for you. reply lyu07282 5 hours agoparentprevI don't think they are comparable anyway, plasma is a desktop environment not a window manager. I mean plasma5 has an incredible amount of features, and it's extensible by Qt/QML widgets that are very easy to write. Sure you can say you don't really need its features, like it's bluetooth integration, its audio mixer, multihead display settings, notifications with individually configurable alerts, per-application or window customization, integrated screenshot and screen recording, MTP/NFS/SMB/etc. integration, file content fts, etc. etc. And I'm sure you could achieve all of it somehow in awesomewm/i3wm/etc. but to me it just feels like a waste of time and a bad user experience. Perhaps I'm just getting old and rather focus on more interesting things than the configuration files of my window manager. Disclaimer: I've been using Linux on the desktop exclusively since before Windows XP. reply dnAGged 15 hours agoparentprevWhy do you think KDE is not configurable? reply DEADMINCE 14 hours agorootparentI don't, but it, nor any DE like it come close to customizability when compared with AwesomeWM. It's not just about settings are available, I can hook into and change whatever functions I like, add custom code easily, replace every single aspect of the interface, etc. reply fooker 12 hours agorootparentCan you give a specific example of what you could do with Awesome WM that you can't replicate easily with plasma 5 or an available extension? reply xcv123 14 hours agoparentprev> Even the Windows desktop isn't as heavy LOL. No. Try Window 11 then try KDE 5 or 6. No comparison. You are comparing KDE4, a 10 year old release which was not well optimized. KDE has become more efficient while Windows has become more bogged down and bloated with ads and telemetry and crapware. It is unusable before running a debloating script. reply DEADMINCE 8 hours agorootparentI don't know about Windows 11, but Windows 10 is still very snappy on old hardware with low ram. Windows has a lot of issues but desktop responsiveness isn't typically one of them. reply redeeman 11 hours agorootparentprevand probably kde 4.0 or 4.1, not the later releases where things were kinda good reply yarg 13 hours agoprevThe update doesn't seem too bad - but I did (initially) make the mistake of calling zypper from within KDE, which leads to a crash and leaves the system in an invalid state. (ctrl+alt+f4 from the login screen allows you to get to a command line without starting KDE, and that allows for the upgrade to complete.) I do think that this shouldn't be allowed; zypper should exit gracefully and inform the user how to safely perform the upgrade. reply asimops 11 hours agoparentWhy could zipper not update the stuff on disk, then prompt you to logout/login to load the new binaries? Windows and MacOS afaik don't require you to change to a tty to update the GUI. reply yarg 9 hours agorootparentI've got no idea, but allowing partial updates seems to be the worst of all options. reply lyu07282 19 hours agoprevI'm positively surprised someone is looking at all so deeply into potential desktop local privescs, I just assumed the extreme complexity of your average default plasma desktop vs. the relative few users, meant it is probably full of vulnerabilities just not worth the effort of finding them. reply creshal 20 hours agoprev [–] Complexity addicts come up with more complexity to handle the extra complexity of trying to wrap an excessively complex design-by-committee excess into another excessively complex framework. Yaaay. reply genter 20 hours agoparentHow did you get your web browser to work in DOS? reply creshal 19 hours agorootparentPolkit is entirely redundant (dbus has its own permission system, not that you need it) and it's trivial to build a linux desktop without it. (None of my systems have it installed, e.g.) Dbus is harder to replace, since the complexity addicts at Redhat and KDE decreed it to be The Default Linux IPC, but realistically, most programmers who think they need could do with a solution 10% as complex and 10x as fast. (dbus-broker is a step in the right direction, but not radical enough.) reply kps 18 hours agorootparentSome of it is… impressive. “These side effects are also in some sense shortcomings of the D-Bus configuration scheme, since developers of a specific D-Bus service don’t expect that their configuration file has a global influence.” reply bluGill 18 hours agorootparentprevThe problem is everyone's 10% solution is different and so the complexity is easier than the sum total of all the others you would have. reply creshal 18 hours agorootparentDBus is really quite a magical place, I'm sure if you actually dig through how open source projects use it, you'll find that the vast majority of projects don't really interact with most of its features and just do the bare minimum the protocol forces them to declare (object paths vs object class hierarchies, dbus perms vs polkit perms, ...), or they're stuff that sounded like a good idea 20 years ago and hasn't been for 15 (defining its own data serialization layer, incompatible to everything else). reply SSLy 18 hours agorootparentFinally some actual feedback. Which alternative IPC + capability delegation protocol do you wish linux used instead? reply creshal 4 hours agorootparentHonestly just a wrapper that makes it easy for programmers to have RPC over unix sockets (spawn multiple with different ACLs for fine-grained permissions, if you really need to - but usually you don't. cap delegation is overrated). And then pick any popular serialization library that is already supported in all languages and stick with it. JSON would be nice for the easy users, but you do wany to exchsnge binary data over IPC, so that would maybe be a bit too simpld. I don't have any strong preferences when it comes to binary serialization/rpc protocols, though. reply enriquto 17 hours agorootparentprev> Which alternative IPC + capability delegation protocol do you wish linux used instead? Why \"alternative\"? Aren't file-based operations good enough? A program reads and writes data to files/devices, which it can or cannot do according to the permissions that it sees. From the point of view of the program, nothing else is needed beyond fopen,fread and fwrite. If finer control is needed, it's not for the program to implement, but for the OS to expose. An improvement to the dbus+polkit mess would be something that programs don't link to, something that is entirely invisible to programs beyond the possibility to open certain files at runtime. reply freedomben 17 hours agorootparent> Why \"alternative\"? Aren't file-based operations good enough? For some things yes, for other things no. If you're just making a basic config, yeah writing to a text file can be sufficient. Though, that requires defining a protocol/standard and following that. You could use a socket, but similarly that requires a protocol, possibly at multiple levels in the stack. Besides adding unnecessary overhead, that also loses all the OS-level information such as which user/privilege level is the caller running as? Now suppose you need it to be highly and quickly reactive (or what we used to call \"event driven\"). Suppose you also need to impose authentication and access controls. You could use the built-in file stuff for authn/authz, but not really event driven and most solutions to that are slow and bolted-on, and there's no pre-defined schema (so back to protocol defining). You could use socket but no built-in authn/authz. I rejoice in simplicity, and strongly prefer it when possible. But limiting only to simple solutions means there's a lot of stuff you can't do, and when you need to do those things, this is not great. And if you have to learn and use multiple solutions based on the level of complexity of the use case, then it may be simple from the dev's point of view, but it's certainly not from the operator's point of view. reply gunapologist99 16 hours agorootparentBut if we look at the operations that dbus is actually used for, they don't seem to be as complex as all of the capabilities that dbus provides. It's like saying your init should basically be another entire operating system instead of just a service orchestrator; by reducing the API surface area, we not only increase security and decrease footprint, we also end up with composability and replaceable layers because other components might even be simpler but with the same API. By defining this as a huge, complex problem that needs to be solved, the solution area naturally has to be much larger and encompass a far greater API and provide a lot more capabilities instead of the minimum capabilities to accomplish the far more limited goal. Growing the problem domain and thus the solution is always a huge temptation but ultimately results in the opposite of modularization and good system design. reply freedomben 15 hours agorootparent> But if we look at the operations that dbus is actually used for, they don't seem to be as complex as all of the capabilities that dbus provides. I'm not sure this is true, certainly not for desktop users of Gnome and KDE. Just NetworkManager makes extensive use of D-Bus, and if you are writing an application that needs to care about (read or write) the state of the network, D-Bus is very useful and offers some powerful functionality. On my Fedora machine I jsut ran `busctl listwc -l` and got 175 (meaning 174 buses available), so it is being used quite extensively. Watching `sudo busctl monitor org.freedesktop.NetworkManager` is fascinating. I'm certain that there are applications using it that aren't using the full capabilities, but if it already needs to be in place, I don't think it adds complexity to use it for simple situations where a file might be sufficient. In fact I think somewhat the opposite. If everything were in D-Bus, there's only one place to look and you learn how to use it once and it's immediately useful for many things. If every application is using a homegrown/custom/bespoke IPC solution, you have to get into the details for every single thing. That sounds a lot more complex to me. But that said, I bet we probably agree more than we disagree. If the application just needs to take in some configuration at runtime and doesn't need IPC, then using D-Bus is overkill and a bit cringey. I would much, much prefer a config file. If there's runtime changes though, that's where D-Bus starts to make sense to me. reply tremon 14 hours agorootparentprevI'm not sure what use case you're thinking of here. All requirements that you list are bread and butter for filesystem-based operations. - which user/privilege level is the caller running as? the kernel already knows this. Which file-based operation doesn't know the privilege level of the caller? - highly and quickly reactive (or what we used to call \"event driven\") inotify/fsnotify? - impose authentication and access controls built-in to the filesystem, or use SO_PEERCRED on sockets? - and there's no pre-defined schema What do you mean with \"schema\" here? Are you saying that /proc and /sys have no pre-defined schema? AFAIK, the only thing that files cannot do by default is data broadcast: if you have multiple readers on a file or socket, only one of them will receive the data written. For that, you would need to keep track of the number of readers and give each (passive) listener its own connection. reply freedomben 13 hours agorootparent> What do you mean with \"schema\" here? Are you saying that /proc and /sys have no pre-defined schema? If all you're doing is reading/writing kernel stuff, then those will be sufficient. But if you're building a user-space service or daemon that will read/write to/from other user-space processes, then /proc and /sys don't help. If you want to use files you will have to agree and standardize on a format. For example, use YAML? ok, but which properties are mandatory, which are optional? What will their hierarchy look like? Even what is (are) the file name(s)? How do you handle adding/removing properties, maintaining backward compatibilty? > inotify/fsnotify? You can get a lot done with inotify and fsnotify, but there's a lot of overhead involved too. It is also somewhat fragile, some might say \"hacky\". It's certainly not something I would want to build on top of for a production important service. If you want a reliable Pub-Sub, (so for example, your new application1 wants to subscribe to a number of different types of events/notifications about network changes from Network Manager) you have to hope that NetworkManager is writing that to files, find out which files it is, register a watch on them, read them when they change, parse the data (and have to agree on a data format or \"schema\" with NetworkManager), and do your thing. Now want to send something back to Network Manager based on what you read? Gotta write to a file, and hope that NetworkManager is already coded to watch that file with inotify/fsnotify, and already agrees with the data format (or \"schema\"). When you control both sides it's not hard, but when both sides are completely independent with their own goals/priorities and preferences, it can get hairy. These are largely problems that go away with something like D-Bus. reply akvadrako 3 hours agorootparentprevWe had a solution already and it's still used when the overhead or dependency on dbus is unacceptable, like with graphics and audio. It's putting Unix sockets in /var/run with the correct permissions. reply _factor 19 hours agorootparentprevIt’s hard to sell enterprise support if your product is simple and easy to understand. reply Scarbutt 17 hours agorootparentprevWhat do you use as alternatives to xdg-desktop-portal and its backends which require rtkit/polkit? reply bandrami 17 hours agorootparentI can't speak for polkit, but in the absence of rtkit the gid/uid limits set in limits.conf work just fine even over a portal. I honestly never understood what was allegedly gained by moving that logic from one place to another. (For that matter the analogy with polkit and group permissions works for a lot of things.) reply creshal 4 hours agorootparentprevWayland and snaps/flatpaks were more overly complicated mistakes, the alternative will be to go back to drawing board and not come back until there's a solution less stupid. reply brnt 20 hours agoparentprev [–] The reviewed helped find the complexity and give pointers on how to reduce it, is the way I read it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article examines the security implications of D-Bus and Polkit in the KDE6 release, focusing on vulnerabilities and enhancements in these components.",
      "It emphasizes the issues related to insecure D-Bus services in legacy packages, the need for secure authentication in Polkit actions, and risks associated with file system operations in KDE components.",
      "Recommendations include using the KAuth framework, addressing security risks from D-Bus services running with root privileges, and adopting secure practices like dropping privileges and using file descriptors over paths in KDE operations."
    ],
    "commentSummary": [
      "The KDE6 release sparks discussions on security, focusing on D-Bus and Polkit vulnerabilities, and users are concerned about PolKit agents' vulnerability and the lack of information in privilege escalation dialogs.",
      "Comparisons are drawn with Android and Windows security features, highlighting the significance of design for robust security measures.",
      "Users also delve into the evolution of KDE, enhancements in Plasma Mobile, and the lightweight nature of Plasma 5, sharing experiences in switching desktop environments and exploring potential improvements in Linux systems like alternative IPC and capability delegation protocols for simpler solutions."
    ],
    "points": 163,
    "commentCount": 68,
    "retryCount": 0,
    "time": 1712573521
  },
  {
    "id": 39973296,
    "title": "Stow Project Seeks Co-maintainer for Sustainability",
    "originLink": "https://savannah.gnu.org/bugs/index.php?65569",
    "originBody": "Not Logged in Login New User This Page Language Clean Reload Search Area to search in Groups People Support Bugs Tasks Patches Hosted Projects Hosting requirements Register New Project Full List Contributors Wanted Statistics Site Help User Docs: FAQ User Docs: In Depth Guide Get Support Contact Savannah GNU Project Help GNU All GNU Packages Dev Resources License List GNU Mirrors Help us protect your freedom and the rights of computer users everywhere by becoming a member of the FSF. Join Now! Free Software Foundation Coming Events Free Software Directory Cryptographic software legal notice Copyright infringement notification Related Forges Savannah Non-GNU Puszcza GNU Stow - Bugs: bug #65569, Stow needs a co-maintainer Group Main Main View members Search Homepage Download Docs Browse Submit Edit Digest Export Search Mailing lists Source code Use CVS Browse Web Pages Repository Use Git Browse Git repository Bugs Submit new Browse Reset to open Digest Dependencies Export Get statistics Search News Browse Atom feed Submit Manage bug #65569: Stow needs a co-maintainer Submitter:Adam Spiers Submitted:Sun 07 Apr 2024 10:49:12 PM UTCCategory:None Severity:4 - Important Item Group:None Status:None Privacy:Public Assigned to:aspiers Open/Closed:Open * Mandatory Fields (+)Post a Comment (-)Discussion Sun 07 Apr 2024 10:49:12 PM UTC, original submission: Quote (Also posted to https://github.com/aspiers/stow/issues/104 for greater visibility.) The current situation where I'm the only maintainer of Stow is not a healthy or sustainable situation for the long term. I have in the past received one or two kind offers to help out, but at the time was regrettably too busy with professional and personal stuff to give them the attention they deserved. If you are one of those people, please accept my apologies and I'm glad you're reading this. If not, I need to dig out those old messages, contact the people involved, and see if they are still interested. But in the mean time, anyone who reads this and may be interested in helping out, or just has suggestions for the best way forward, is welcome to add a comment below. What would be needed of a new co-maintainer? Hopefully I'm not asking for too much here! If someone has significant gaps in the below list, then most likely bringing them on as co-maintainer would be counter-productive, because if I had to spend time getting someone up to speed on the basics before getting to the complex stuff then it would be more effective to use that time to do the work myself. At least a decent grasp of Perl A decent understanding of Stow and the ability to learn how the implementation works without too much external assistance (I will be very happy to answer good questions about stuff which isn't obvious, but I don't have time to explain simple things which are already documented or easy to deduce by reading the existing code and docs) Plenty of experience doing code reviews Plenty of experience with git, GitHub, and the pull request workflow A public track record of contributing to FOSS projects Strong communication skills, to be able to help write release notes and announcements, and help support the community Good situational awareness and ability to coordinate with me, so that we don't end up duplicating work or causing unnecessary merge conflicts or other inefficiencies How much time / commitment would be needed? I don't think we need to be too prescriptive about this to be honest. If someone has the ability to do excellent work but only on an occasional basis, that's still great. We could potentially even have a team of 3-4 trusted co-maintainers all working sporadically, as long as they can all do a great job of coordinating in a professional manner. Or if there is one awesome person out there with a lot of free time who can carry a lot of weight, that would also be fantastic. I'm open to all offers and suggestions. However the one thing I do want to avoid is someone making a very loose commitment just because they like the idea of having maintainer privileges and then not following through. I'm sure that's a very unlikely scenario, but please don't offer to help unless you're sure that you have the capacity to help. Ideally you would have already contributed to Stow by reviewing and/or submitting PRs.Adam Spiers(-)Attached Files (Note: upload size limit is set to 16384 kB, after insertion of the required escape characters.) Attach Files: Comment: No files currently attached (-)Dependencies Depends on the following items: None found Items that depend on this one: None found (+)Mail Notification Carbon-Copy List (+)Votes Please enter the title of George Orwell's famous dystopian book (it's a date): (+)History Copyright © 2024 Free Software Foundation, Inc. Verbatim copying and distribution of this entire article is permitted in any medium, provided this notice is preserved. The Levitating, Meditating, Flute-playing Gnu logo is a GNU GPL'ed image provided by the Nevrax Design Team. Page source code Powered by Savane 3.13-319f. Corresponding source code",
    "commentLink": "https://news.ycombinator.com/item?id=39973296",
    "commentBody": "GNU Stow needs a co-maintainer (savannah.gnu.org)147 points by nequo 13 hours agohidepastfavorite75 comments kstrauser 12 hours agoFrom https://www.gnu.org/software/stow/ : \"GNU Stow is a symlink farm manager which takes distinct packages of software and/or data located in separate directories on the filesystem, and makes them appear to be installed in the same place.\" The idea is that instead of installing package foopkg directly into /usr/local, you could install it to /opt/foopkg-v1.2.3. Then you can run stow to make a bunch of symlinks like /usr/local/bin/foo -> /opt/foopkg-v1.2.3/bin/foo. Upgrade it to a new version, re-run stow, and now all the symlinks point to /opt/foopkg-v4.5.6/bin/foo and so on. It's pretty nifty. However, I used it more for managing dotfiles in my home directory than anything else, making links like ~/.vimrc -> ~/src/my-config-repo/.vimrc . I much prefer using chezmoi for that now. reply colonwqbang 10 hours agoparentI've found that stock git works great for managing dotfiles, without any extra tools needed. Just a few lines of gitconfig and a shell alias is enough. It's all explained here: https://www.atlassian.com/git/tutorials/dotfiles Perhaps other people have more complex use cases than me. reply wdfx 3 minutes agorootparentyadm https://yadm.io/ uses git for file management, but also provides some convenience on top of that reply kstrauser 10 hours agorootparentprevThere's nothing wrong with that setup. It falls over when you start pushing it across multiple machines with substantial differences. Then Chezmoi's templating is so handy. For example, 99% of my config is the same between my desktop Mac and my various Linux servers, but I use different ssh_config settings on the 2 OSes. Chezmoi makes that very easy. Stock git doesn't. I could script something up to do handle that for me automatically, and before I knew about Chezmoi, that's exactly what I did. Now I'd prefer to let someone else write and maintain all that for me so I can move on to working on other things. reply blueflow 4 hours agorootparentWith `Match exec ...` you can define arbitrary machine-specific sections. I had more problems with differences between versions than with differences between machines. tmux is the sort of program where the available config directives change every major release and there is no full backwards compatibility. reply Ferret7446 8 hours agorootparentprevThat's what branches are for. I have two personal machines and two work machines, all of which have diverging configs, which I push to two remotes (one work specific), and I merge changes between them. This is how Git was designed to be used. reply _ikke_ 19 minutes agorootparentUntil you want to introduce a change that affects both machines. You need to start rewriting history or cherry-pick the changes on both branches. The further the history diverges, the harder this becomes. Using branches for this does not scale. reply markstos 9 hours agorootparentprevI evaluated Stow and tried Chezmoi for a while but settled on YADM. It’s the bare git repo idea with a little more sugar sprinkled on top. Perfect for my needs. reply dngray 7 hours agorootparentI switched from Yadm to Chezmoi. The main reason was because there invariably is always something slightly sensitive you don't want in a dotfile and the rest of the file is okay. Yadm uses third party tools to do jinja templating. The first one envtpl stopped being maintained, and the second one j2cli (both jinja2 templaters) aren't very well maintained either. With chezmoi I just use the golang text/template templater. I know it will always be maintained. The integrated password manager functionality for chezmoi also works awesome too. I did initially use stow, but symlinks is just bad. You end up with all sorts of problems with that I can't even remember. My whole dotfiles is 7MB, so if a copy is made from a \"source tree\" to my home dir that's okay. Chezmoi also encouraged me to do things more deterministically based on hosts and reduce the number of \"scripts\" that I run significantly, which led to less bugs. I use the same set of dotfiles across a number of my systems. reply eternityforest 1 hour agorootparentprevI currently use Borg in Vorta to manage my dotfiles. I only have about 5, and half of them have secrets. I could do it all with shell... but every few likes of custom scripting and config I can ditch is one less thing to worry about, and I don't really need VCS for just a few kb that rarely change. I've been using a lot more VSCode extensions lately though, so perhaps I'll want to do something for that. reply Twirrim 12 hours agoparentprev>It's pretty nifty. It's how Amazon's stuff used to work (though not using Stow) back several years ago. No idea if they've migrated from that approach to containers, or similar, yet. Every application you deployed would have the necessary components deployed (or re-used if you had something else that already used it), and then build the application space from symlinks to those parts. Worked really well. reply RGamma 10 hours agorootparentAmazon has an internal Nix/Guix? Probably not much public info on this... reply paholg 4 hours agorootparentIt's not the same. It's been a while, so I'm sure to get things wrong, but you basically had different package groups you could set up. So a service would have its group that it could update, test, and deploy with at once. But like if I produced a package, and another team depended on it, there was no guarantee that the group I ran in CI had versions in common with the group that they deployed. I also remember some weirdness just within one package, like maybe your PR build was based on your local group setup and not anything \"official\". The coolest thing about it is that you could make a PR against multiple repos at the same time, even if one depended on the other. Like you could add a function to a library in one repo and call it from another repo in one PR. reply Twirrim 4 hours agorootparentprevYes. From what I was told, it was already of reasonable vintage when I joined in 2013. reply kstrauser 9 hours agorootparentprevAlternatively, Amazon is using stow, a common GNU utility whose info page refers to a version of Perl released in 1992, or something similar to it, instead of Nix and Guix which didn't exist when Amazon started. reply coldtea 18 minutes agorootparentIt used neither, but it's own implementation of the concept (\"symlink all the things!\", which is much older than both, and has lots of other implementations too, aside from Nix and Stow. It's how gobolinux works too, for example. reply RGamma 9 hours agorootparentprevWould Amazon rely on stow when it is in this peril? Unless they really do rely on it and maintain an internal fork, which would make this situation even worse. Or they use it anyway... reply fnordpiglet 9 hours agorootparentprevWhich is also the software this submission is about reply tubs 11 hours agorootparentprevStill works on link farms, yep. And works pretty well! reply thrtythreeforty 11 hours agorootparentprevAh, symlink farms, how I love to hate thee! They are alive and well. reply yjftsjthsd-h 10 hours agorootparentWhy do you hate them? (FWIW this is a sincere question; given the number of these things I touch, I would very much like to know if there are problems I need to know about and/or better alternatives) reply hawski 2 hours agoparentprevIs there an overlap of functionality between GNU Stow and OSTree? reply blackeyeblitzar 11 hours agoparentprevWhat is the use case for this? Is the idea that it can automatically turn a package into a portable package, effectively? Or is it so you can install multiple versions of the same software without conflict between them? reply bandrami 2 hours agorootparentYou install libfoo 4.7 to /usr/local/stow/libfoo-4.7 (such that you have /usr/local/stow/libfoo-4.7/bin/foocfg and /usr/local/stow/libfoo-4.7/lib/libfoo-4.7.0.so and /usr/local/libfoo-4.7/share/man/man1/foocfg.1.gz), and then libfoo 4.8 to /usr/local/stow/libfoo-4.8. Then from /usr/local/stow you run `stow libfoo-4.7` and all the contents mentioned above are symlinked into /usr/local appropriately. Then if you want to switch libraries you unstow that one and stow version 4.8. It's highly configurable, so you can do a lot more with it than that, but that was the idea behind it 25 years ago. There were whole distros based on that, though it fell out of favor when containers became a bigger thing in the late oughts. reply kevin_thibedeau 8 hours agorootparentprevIt lets you have informal package management of self-compiled binaries in parallel with your distro's package manager. With Stow you can install updated libs and applications into /usr/local and don't have to be concerned with conflicts. At worst you may need to set LD_PRELOAD to bypass system libs. Very useful with Debian stable when you need a new feature in something and don't want to wrestle with backports. reply kazinator 6 hours agorootparentI've never had a problem installing self-compiled stuff into /usr/local without conflicts. I'd have to have the problem of installing multiple versions (upgrades) and needing to be able to roll back easily, along with any config files and such. reply ttyprintk 5 hours agorootparentIn the distant past, /usr/local/stow was a NFS mount. Each machine could maintain different symlink trees. reply BeetleB 4 hours agorootparentprevCurious: How do you uninstall stuff you manually installed in /usr/local? reply kazinator 4 hours agorootparentAnything I install in /usr/local is stuff is going to be something I really need, and for which there isn't a distro package. It stays for the life of the system. If I wanted to install something into /usr/local that would be suspected of needing removal later, I'd build it with /usr/local as a prefix, but install it in a temporary directory, then make a tarball package out of that to keep track of the file list. That could be used to remove it. I could trivially generate an uninstall script by using find in the package directory to get a list of relative paths; converting each to a rm command. The uninstall script would be put into /usr/local and run from there. reply BeetleB 4 hours agorootparent> If I wanted to install something into /usr/local that would be suspected of needing removal later, I'd build it with /usr/local as a prefix, but install it in a temporary directory, then make a tarball package out of that to keep track of the file list. That could be used to remove it. I could trivially generate an uninstall script by using find in the package directory to get a list of relative paths; converting each to a rm command. The uninstall script would be put into /usr/local and run from there. Compared to using stow, this is two orders of magnitude more complicated :-) With stow, you simply install it anywhere, and stow will make the symlinks into /usr for you. When you want to uninstall it, stow will remove all the symlinks. This way, I would install each package into its own directory. When I want to remove it, I use stow to delete all the symlinks, and then just delete the directory. No need for \"make uninstall\", etc. reply kazinator 3 hours agorootparentIt's only theoretical; I've never uninstalled anything out of a /usr/local. What Stow is doing, by the way, is better achieved with overlayfs, which didn't exist when Stow was first introduced. With overlayfs you can specify multiple directories that are merged together. Multiple packages that are rooted at /usr/local can be mapped there with overlayfs. reply fiddlerwoaroof 4 hours agorootparentprevI’ve used this a lot on Debian systems so that I could just use apt to remove the manually-compiled version: https://en.m.wikipedia.org/wiki/CheckInstall These days I mostly use Nix which basically elominates this problem. reply jonhohle 11 hours agorootparentprevAnd so your development environment artifacts can be linked in to the environment root just like any other package (really awesome). reply blackeyeblitzar 10 hours agorootparentSorry I’m not super clear on how this works. Could you explain what it means to be linked in to the environment root? reply tfsh 9 hours agoprevI've been using Stow for about two years now to manage my dotfiles. In order to improve the UX I've wrapped it within a small utility which allows me to define packages (directories of dotfiles that should be managed) such as zsh (containing .zshrc, .zshenv, .zlogin, etc) and their respective locations [1]. There's a few other niceties such as modular Zsh file sourcing, allowing encapsulation by OS, automatic git-crypt support, and dependency resolution across mac and debian. I've been tempted to try NixOS due to its emphasis on config-as-code, but this isn't something I can feasibly do as this would fragment my dotfiles across different ways of thinking, which conflicts with my desire to have one repo for every device. Which I've achieved pretty successfully as these dotfiles exist on my personal/work macs, WSL, work/personal linux workstations and a few colleagues devices. All of which works out of the box with a single $ git clone and invocation of a bootstrap.zsh file which installs and sources everything you'd need. The real magic behind all of this is Stow, so I'll always be eternally grateful to its maintainers. If I wasn't a complete stranger to the GNU ecosystem, I'd step up and offer my help. For anyone who's curious here's my dotfiles: https://github.com/o-y/dotfiles 1: https://github.com/o-y/dotfiles/blob/main/bootstrap.zsh#L38-... reply kazinator 3 hours agoprevThe GNU Stow documentation has a curious blind spot; it doesn't mention the DESTDIR convention for installing in a separate directory: $ ./configure --prefix=/usr/local $ make $ make install DESTDIR=/usr/local/stow/whatever Using \"make prefix=...\" is not the main mechanism for overriding the install location; DESTDIR is. DESTDIR is widely supported, and documented in the GNU Coding Standards, in the Makefile Conventions section of the Release Process chapter: https://www.gnu.org/prep/standards/html_node/DESTDIR.html reply tommiegannert 2 hours agoparent(Referring to https://www.gnu.org/software/stow/manual/stow.html#Other-FSF...) They serve two different purposes. DESTDIR places files in a staging directory, e.g. to be packaged into a tarball. In your example, you will end up with the program at /usr/local/stow/whatever/usr/local/bin, which I'm guessing Stow is trying to avoid, because it's ugly and the extra directories are unnecessary. Not wrong, though. With their approach, it ends up at /usr/local/stow/whatever/bin. reply kazinator 2 hours agorootparentRight. So if the package supports install time prefix with no hassle, it could be done with make DESTDIR=/usr/local/stow/whatever prefix=/ to get rid of the usr/local components. If the prefix override causes a problem, then just live with the extra components. I'm guessing that in that case you tell stow that your package root is at /usr/local/stow/whatever/usr/local. Stow doesn't care about the extra components; your package can be anywhere you like, right? reply grumbel 1 hour agorootparentThat's not right either. The --prefix needs to be /usr/local or /usr/local/stow/package-1.0, otherwise many packages won't find their own files. The prefix path will get compiled into the binary or configuration for a lot of packages, it's not just an install time thing. Using --prefix=/usr/local/stow/package-1.0 is problematic whenever you have a package with plugins, themes or other stuff, as those go to /usr/local/share/package/... while the app is looking in /usr/local/stow/package-1.0/share/package/ Using DESTDIR and manually removing the usr/local from the directory tree is what I would consider the correct way, even if a bit annoying. Either way, these days I would just recommend to use Nix instead, which is a much more complete solution for what stow tries to do. reply kazinator 1 hour agorootparentI understand the compile-time prefix. But in some projects, you can override the configured prefix variable during \"make install\" without changing anything in the package; the install steps will just accept those paths, as a hack for shortening the paths, in packages where that works. The Stow documentation mentions this also. reply OJFord 7 hours agoprevI used Stow (some years ago) until I discovered the XDG directory spec. It can be a bit more painful on macOS, but generally enough respects it that it makes more sense to me to use it by default and fix/carve workarounds for what doesn't than to use tools like Stow to essentially put everything in the latter category. That said, I don't imagine it was built for 'dotfiles' as everyone is and will discuss it being used for, so perhaps it does deserve to live on. reply throwiforgtnlzy 7 hours agoprevPoor stow. We used it in the stone age about 20 years ago for unpackaged, shared software management on academic research clusters (Think \"/usr/{{other hierarchy}}\" over NFS). The problem with it is it depends entirely on symlinks and some programs get confused or just don't like them. Nix, hab/habitat, containers, or overlay filesystems (such as with flatpak, etc.) are options that might work better that get around this problem. reply mike_d 12 hours agoprevSite is getting hugged to death. Maybe we can update the URL? They posted the same notice to GitHub: https://github.com/aspiers/stow/issues/104 reply kazinator 3 hours agoprevI think you can merge multiple directories onto /usr/local using overlayfs. That seems to be what Stow is simulating, using symlinks. Symlinks have the advantage that they persist; you don't have to recreate them after every reboot. Stow also has ignore lists, which isn't something overlayfs will do, or not nicely; it is oriented toward directories. In the package installation use case, you could just keep unwanted cruft out of the individual installation directories that are being combined. reply malobre 8 hours agoprevA few years ago I read a blog post [0] on using GNU Stow to manage your dotfiles, I loved the idea and it inspired me to create xdot, a minimalist dotfiles manager [1] which I have been using ever since. [0]: https://brandon.invergo.net/news/2012-05-26-using-gnu-stow-t... [1]: https://github.com/malobre/xdot reply BeetleB 12 hours agoprevstow was a really useful tool for me once at work. I had a \"local\" usr/* in my home directory for custom packages I'd install. Occasionally I'd need to swap different versions of the same library, etc. stow made the process a lot more manageable. reply saghm 11 hours agoparentI've found it useful occasionally for when I've needed to install something via source that didn't include an \"uninstall\" target in their build configuration. Being able to \"unstow\" all of the symlinks will clean up all of the system directories, at which point you can just delete the entire folder where the actual installation occurred if you don't want to keep it around at all. reply BeetleB 4 hours agorootparentExactly! reply Karellen 10 hours agoparentprev> I had a \"local\" usr/* in my home directory I always just used `--prefix=\"$HOME\"` so that everything went into `~/bin`, `~/lib`, `~/man`, etc... I did look into stow a couple of times, and would have been fine with it dropping symlinks going into those dirs if I'd have used it. (A few years after XDG started being commonly used I moved everything in my `~/etc` into `~/.config`, and `~/etc` is now a symlink to it. I occasionally wonder if doing it the other way around and setting up XDG_CONFIG_DIR would be more old-skool, before catching my reflection in my monitor and realising how daft that thought is.) reply BeetleB 4 hours agorootparent> I always just used `--prefix=\"$HOME\"` so that everything went into `~/bin`, `~/lib`, `~/man`, etc... Forgive my ignorance, but how do you uninstall stuff a year or two later? reply mistrial9 11 hours agoparentprevI saw a german after-midnight guy setup postgresql like this once, on linux.. rather quickly ;-) All the significant system parts were in an alternate LD_LIBRARY_PATH plus the postgresl libs reply CGamesPlay 8 hours agoprevI evaluated Stow for dotfiles, but I wanted something simpler to deploy (single binary). I built a solution myself several years ago, which shares many features with Stow, and it’s still kicking along. https://github.com/cgamesplay/dfm reply malobre 8 hours agoparentLooks great! Seems like GNU Stow inspired many devs, especially around the dotfiles use case reply xyst 7 hours agoprevSeems like a zombie project at this point. Maybe a sign that it needs to be put down? - written in perl - 1 maintainer - not much activity on GH or mailing lists - pivot from initial purpose as “symlink farm” to dot file management It’s had a nice run but seems much better alternatives exist now (as mentioned in comments) reply jjgreen 1 hour agoparentI used to do quite a bit of Perl, and occasionally have need to run scripts which are 10, 15 years old -- I can't remember a case where one of those didn't work. A two-month old Python script, that's 50/50 reply mid-kid 4 hours agoparentprevIt's a simple tool with no dependencies besides perl (which will be around forever) and no cybersecurity footprint. Even if it were to be abandoned for a decade it would still work fine and serve a purpose. There also exists no better alternative for symlink-farm style management. Where do you people keep coming from? reply fiddlerwoaroof 4 hours agorootparentPerl is not web scale reply jjgreen 1 hour agorootparentNo, it's bigger than that https://xkcd.com/224/ reply Dalewyn 4 hours agorootparentprev>Where do you people keep coming from? The future. reply qalmakka 43 minutes agoparentprev> written in perl Perl scripts are surprisingly resilient. I have seen Python modules only a handful years old turning to garbage, and unmaintained Perl modules still working fine after > 20 years. reply johnisgood 1 hour agoparentprev1. What is your issue with Perl with regarding to this project? 2. Is the project not more or less complete? 3. See above. 4. What exactly is the issue? As for alternatives, I do not see any mentioned. I did hear about https://zolk3ri.name/cgit/zpkg/ though. reply freedomben 12 hours agoprevI think Savannah is getting an HN hugrsh reply shp0ngle 5 hours agoparentprevI'll ping Jigar reply benced 11 hours agoparentprevCame here to say exactly this reply xyst 7 hours agoparentprevToo soon, bro reply crest 11 hours agoparentprev [–] Give him some time to recover first. After all the haters destroyed the fruits of his tireless labour of love to which he dedicated years of his life. /s reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Stow project's current maintainer is looking for a co-maintainer due to sustainability issues, requiring proficiency in Perl, familiarity with Stow, skills in code reviews, git, and open-source projects, strong communication, and effective coordination.",
      "The new co-maintainer must commit effectively, avoiding offering help without follow-up, with flexibility in the commitment level.",
      "Those interested are urged to get involved by reviewing and submitting pull requests (PRs)."
    ],
    "commentSummary": [
      "Users are debating the utility of GNU Stow in handling packages and dotfiles, alongside alternative tools such as YADM, Chezmoi, and Nix.",
      "Opinions vary on the effectiveness of Stow for package management, leading to discussions on different approaches to configuring software installations on Unix systems.",
      "The conversation delves into managing symlinks, software versions, the use of Perl scripts, and the longevity of programming languages."
    ],
    "points": 147,
    "commentCount": 75,
    "retryCount": 0,
    "time": 1712607613
  }
]
