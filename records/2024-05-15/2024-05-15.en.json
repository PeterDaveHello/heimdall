[
  {
    "id": 40358041,
    "title": "Veo: Google's Powerful Generative Video Model",
    "originLink": "https://deepmind.google/technologies/veo/",
    "originBody": "Pause Play Technology Veo Our most capable generative video model Sign up to try on VideoFX Veo is our most capable video generation model to date. It generates high-quality, 1080p resolution videos that can go beyond a minute, in a wide range of cinematic and visual styles. It accurately captures the nuance and tone of a prompt, and provides an unprecedented level of creative control — understanding prompts for all kinds of cinematic effects, like time lapses or aerial shots of a landscape. Our video generation model will help create tools that make video production accessible to everyone. Whether you're a seasoned filmmaker, aspiring creator, or educator looking to share knowledge, Veo unlocks new possibilities for storytelling, education and more. Over the coming weeks some of these features will be available to select creators through VideoFX, a new experimental tool at labs.google. You can join the waitlist now. In the future, we’ll also bring some of Veo’s capabilities to YouTube Shorts and other products. Pause Play Prompt: A lone cowboy rides his horse across an open plain at beautiful sunset, soft light, warm colors Pause Play Prompt: A fast-tracking shot down an suburban residential street lined with trees. Daytime with a clear blue sky. Saturated colors, high contrast Pause Play Prompt: Extreme close-up of chicken and green pepper kebabs grilling on a barbeque with flames. Shallow focus and light smoke. vivid colours Pause Play Prompt: Timelapse of the northern lights dancing across the Arctic sky, stars twinkling, snow-covered landscape Pause Play Prompt: An aerial shot of a lighthouse standing tall on a rocky cliff, its beacon cutting through the early dawn, waves crash against the rocks below Greater understanding of language and vision To produce a coherent scene, generative video models need to accurately interpret a text prompt and combine this information with relevant visual references. With advanced understanding of natural language and visual semantics, Veo generates video that closely follows the prompt. It accurately captures the nuance and tone in a phrase, rendering intricate details within complex scenes. Pause Play Prompt: Many spotted jellyfish pulsating under water. Their bodies are transparent and glowing in deep ocean Pause Play Prompt: Timelapse of a common sunflower opening, dark background Pause Play Prompt: extreme close-up with a shallow depth of field of a puddle in a street. reflecting a busy futuristic Tokyo city with bright neon signs, night, lens flare Controls for film-making When given both an input video and editing command, like adding kayaks to an aerial shot of a coastline, Veo can apply this command to the initial video and create a new, edited video. Pause Play Prompt: Drone shot along the Hawaii jungle coastline, sunny day Pause Play Drone shot along the Hawaii jungle coastline, sunny day. Kayaks in the water In addition, it supports masked editing, enabling changes to specific areas of the video when you add a mask area to your video and text prompt. Veo can also generate a video with an image as input along with the text prompt. By providing a reference image in combination with a text prompt, it conditions Veo to generate a video that follows the image’s style and user prompt’s instructions. Prompt: Alpacas wearing knit wool sweaters, graffiti background, sunglasses Pause Play Prompt: Alpacas dancing to the beat The model is also able to make video clips and extend them to 60 seconds and beyond. It can do this either from a single prompt, or by being given a sequence of prompts which together tell a story. Watch Prompts: A fast-tracking shot through a bustling dystopian sprawl with bright neon signs, flying cars and mist, night, lens flare, volumetric lighting. A fast-tracking shot through a futuristic dystopian sprawl with bright neon signs, starships in the sky, night, volumetric lighting. A neon hologram of a car driving at top speed, speed of light, cinematic, incredible details, volumetric lighting. The cars leave the tunnel, back into the real world city Hong Kong. Consistency across video frames Maintaining visual consistency can be a challenge for video generation models. Characters, objects, or even entire scenes can flicker, jump, or morph unexpectedly between frames, disrupting the viewing experience. Veo's cutting-edge latent diffusion transformers reduce the appearance of these inconsistencies, keeping characters, objects and styles in place, as they would in real life. Pause Play Prompt: A panning shot of a serene mountain landscape, the camera slowly revealing snow-capped peaks, granite rocks and a crystal-clear lake reflecting the sky Pause Play Prompt: moody shot of a central European alley film noir cinematic black and white high contrast high detail Pause Play Prompt: Crochet elephant in intricate patterns walking on the savanna Built upon years of video generation research Veo builds upon years of generative video model work including Generative Query Network (GQN), DVD-GAN, Imagen-Video, Phenaki, WALT, VideoPoet and Lumiere, and also our Transformer architecture and Gemini. To help Veo understand and follow prompts more accurately, we have also added more details to the captions of each video in its training data. And to further improve performance, the model uses high-quality, compressed representations of video (also known as latents) so it’s more efficient too. These steps improve overall quality and reduce the time it takes to generate videos. Responsible by design It's critical to bring technologies like Veo to the world responsibly. Videos created by Veo are watermarked using SynthID, our cutting-edge tool for watermarking and identifying AI-generated content, and passed through safety filters and memorization checking processes that help mitigate privacy, copyright and bias risks. Veo’s future will be informed by our work with leading creators and filmmakers. Their feedback helps us improve our generative video technologies and makes sure they benefit the wider creative community and beyond. Watch Preview of our work with filmmaker Donald Glover and his creative studio, Gilga. Note: All videos on this page were generated by Veo and have not been modified. How Veo is powering text-to-video products across Google Sign up to try VideoFX Acknowledgements This work was made possible by the exceptional contributions of: Abhishek Sharma, Adams Yu, Ali Razavi, Andeep Toor, Andrew Pierson, Ankush Gupta, Austin Waters, Daniel Tanis, Dumitru Erhan, Eric Lau, Eleni Shaw, Gabe Barth-Maron, Greg Shaw, Han Zhang, Henna Nandwani, Hernan Moraldo, Hyunjik Kim, Irina Blok, Jakob Bauer, Jeff Donahue, Junyoung Chung, Kory Mathewson, Kurtis David, Lasse Espeholt, Marc van Zee, Matt McGill, Medhini Narasimhan, Miaosen Wang, Mikołaj Bińkowski, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Nick Pezzotti, Pieter-Jan Kindermans, Poorva Rane, Rachel Hornung, Robert Riachi, Ruben Villegas, Rui Qian, Sander Dieleman, Serena Zhang, Serkan Cabi, Shixin Luo, Shlomi Fruchter, Signe Nørly, Srivatsan Srinivasan, Tobias Pfaff, Tom Hume, Vikas Verma, Weizhe Hua, William Zhu, Xinchen Yan, Xinyu Wang, Yelin Kim, Yuqing Du and Yutian Chen. We extend our gratitude to Aida Nematzadeh, Alex Cullum, April Lehman, Aäron van den Oord, Benigno Uria, Charlie Chen, Charlie Nash, Charline Le Lan, Conor Durkan, Cristian Țăpuș, David Bridson, David Ding, David Steiner, Emanuel Taropa, Evgeny Gladchenko, Frankie Garcia, Gavin Buttimore, Geng Yan, Greg Shaw, Hadi Hashemi, Harsha Vashisht, Hartwig Adam, Huisheng Wang, Jacob Austin, Jacob Kelly, Jacob Walker, Jim Lin, Jonas Adler, Joost van Amersfoort, Jordi Pont-Tuset, Josh Newlan, Josh V. Dillon, Junwhan Ahn, Kelvin Xu, Kristian Kjems, Lois Zhou, Luis C. Cobo, Maigo Le, Malcolm Reynolds, Marcus Wainwright, Mary Cassin, Mateusz Malinowski, Matt Smart, Matt Young, Mingda Zhang, Minh Giang, Moritz Dickfeld, Nancy Xiao, Nelly Papalampidi, Nir Shabat, Oliver Woodman, Ollie Purkiss, Oskar Bunyan, Patrice Oehen, Pauline Luc, Pete Aykroyd, Petko Georgiev, Phil Chen, RJ Mical, Rakesh Shivanna, Ramya Ganeshan, Richard Nguyen, Robin Strudel, Rohan Anil, Sam Haves, Shanshan Zheng, Sholto Douglas, Siddhartha Brahma, Tatiana López, Tobias Pfaff, Victor Gomes, Vighnesh Birodkar, Xin Chen, Yaroslav Ganin, Yi-Ling Wang, Yilin Ma, Yori Zwols, Yu Qiao, Yuchen Liang, Yusuf Aytar and Zu Kim for their invaluable partnership in developing and refining key components of this project. Special thanks to Douglas Eck, Nando de Freitas, Oriol Vinyals, Eli Collins, Koray Kavukcuoglu and Demis Hassabis for their insightful guidance and support throughout the research process. We also acknowledge the many other individuals who contributed across Google DeepMind and our partners at Google. Related posts Imagen 3 Our highest quality text-to-image model. Company New generative media models and tools, built with and for creators We’re introducing Veo, our most capable model for generating high-definition video, and Imagen 3, our highest quality text-to-image model. We’re also sharing new demo recordings created with our... 14 May 2024 SynthID Robust and scalable tool for watermarking and identifying AI-generated images. Explore our other teams and product areas Google AI Google Research Google Cloud LABS.GOOGLE",
    "commentLink": "https://news.ycombinator.com/item?id=40358041",
    "commentBody": "Veo (deepmind.google)1428 points by meetpateltech 16 hours agohidepastfavorite408 comments salamo 7 hours agoThe first thing I will do when I get access to this is ask it to generate a realistic chess board. I have never gotten a decent looking chessboard with any image generator that doesn't have deformed pieces, the correct number of squares, squares properly in a checkerboard pattern, pieces placed in the correct position, board oriented properly (white on the right!) and not an otherwise illegal position. It seems to be an \"AI complete\" problem. reply arcticbull 7 hours agoparentSimilarly the Veo example of the northern lights is a really interesting one. That's not what the northern lights look like to the naked eye - they're actually pretty grey. The really bright greens and even the reds really only come out when you take a photo of them with a camera. Of course the model couldn't know that because, well, it only gets trained on photos. Gets really existential - simulacra energy - maybe another good AI Turing test, for now. reply porphyra 4 hours agorootparentHuman eyes are basically black and white in low light since rod cells can't detect color. But when the northern lights are bright enough you can definitely see the colors. The fact that some things are too dark to be seen by humans but can be captured accurately with cameras doesn't mean that the camera, or the AI, is \"making things up\" or whatever. Finally, nobody wants to see a video or a photo of a dark, gray, and barely visible aurora. reply exodust 4 hours agorootparent> nobody wants to see a video or a photo of a dark, gray, and barely visible aurora Except those who want to see an accurate representation of what it looks like to the naked eye. reply stkhlm 3 hours agorootparentLiving in northern Sweden I see the northern lights multiple times a year. I have never seen them pale or otherwise not colorful. Green and reds always. That is to my naked eye. Photographs do look more saturated, but the difference isn't as large as this comment thread make it out to be. reply peanut_merchant 51 minutes agorootparentEven in Northern Scotland (further south than northern Sweden) this is the case. The latest aurora showing was vividly colourful to the naked eye. reply shwaj 2 hours agorootparentprevThat mirrors my experience from when I used to live in northern Canada reply jabits 2 hours agorootparentEven in Upper Michigan near Lake Superior we sometimes had stunn, colorful Northern Lights. Sometimes it seemed like they were flying overhead within your grasp reply DaSHacka 1 hour agorootparentMost definitely, it's quite common to find people hanging around outside up towards Calumet whenever there's a night with a high KP Index. I highly recommend checking them out if you're nearby, the recent auoras have been quite astonishing reply paxys 6 hours agorootparentprevThat's not true at all. I have seen northern lights with my own eyes that were more neon green and bright purple than any mainstream photo. reply cryptoz 5 hours agorootparentThere's a middle ground here. I saw the northern lights with my own eyes just days ago and it was mostly grey. I saw some color. But when I took a photo with a phone camera, the color absolutely popped. So it may be that you've seen more color than any photo, but the average viewer in Seattle this past weekend saw grey-er with their eyes and huge color in their phone photos. (Edit: it was still super-cool even if grey-ish, and there was absolutely beautiful colors in there if you could find your way out of the direct city lights) reply goostavos 5 hours agorootparentThe hubris of suggesting that your single experience of vaguely seeing the northern lights one time in Seattle has now led to a deep understanding of their true \"color\" and that the other person (perhaps all other people?) must be fooling themselves is... part of what makes HN so delightful to read. I've also seen the northern lights with my own eyes. Way up in the arctic circle in Sweden. Their color changes along with activity. Grey looking sometimes? Sure. But also colors that are so vivid that it feels like it envelopes your body. reply lpapez 1 hour agorootparent> The hubris of suggesting that your single experience of vaguely seeing the northern lights one time in Seattle has now led to a deep understanding of their true \"color\" and that the other person (perhaps all other people?) must be fooling themselves is... part of what makes HN so delightful to read. The H in HN stands for Hubris. reply stavros 2 hours agorootparentprevThey did say \"the average viewer in Seattle this past weekend\", not \"all other viewers\". Then again, the average viewer in Seattle this past weekend is hardly representative of what the northern lights look like. reply hoyd 4 hours agorootparentprevI can see what you mean, and that the video is somewhat not what it would be like in real. I have lived in northern Norway most of my life, and watched Auroras a lot. It certainly look green and link for the most time. Fainter, it would perhaps sorry gray I guess? Red, when viewed from a more southern viewpoint.. I work at Andøya Space where perhaps most of the space research on Aurora had been done by sending scientific rockets into space for the last 60 yrs. reply pmlarocque 7 hours agorootparentprevThat not true, they look grey when they aren't bright enough, but they can look green or red to the naked eyes if they are bright. I have seen it myself and yes I was disappointed to see only grey ones last week. see: https://theconversation.com/what-causes-the-different-colour... reply arcticbull 7 hours agorootparent> [Aurora] only appear to us in shades of gray because the light is too faint to be sensed by our color-detecting cone cells.\" > Thus, the human eye primarily views the Northern Lights in faint colors and shades of gray and white. DSLR camera sensors don't have that limitation. Couple that fact with the long exposure times and high ISO settings of modern cameras and it becomes clear that the camera sensor has a much higher dynamic range of vision in the dark than people do. https://www.space.com/23707-only-photos-reveal-aurora-true-c... This aligns with my experiences. The brightest ones I saw in Northern Canada I even saw hints of reds - but no real greens - until I looked at it through my phone, and it looked just like the simulated video. If I looked up and saw them the way they appear in the simulation, in real life, I'd run for a pair of leaded undies. reply Kiro 4 hours agorootparentThat is totally incorrect which anyone who have seen real northern lights can attest to. I'm sorry that you haven't gotten the chance to experience it and now think all northern lights are that lackluster. reply Maxion 4 hours agorootparentprevGreens are the more common colors, reds and blues occur in higher energy solar storms. And yes, they can be as green to the naked eye in that AI video. I've seen aurora shows that fill the entire night sky from horizon to horizon, way more impressive than that AI video with my own eyes. reply Tronno 7 hours agorootparentprevI've seen it bright green with the naked eye. It definitely happens. That article is inaccurate. reply kortilla 4 hours agorootparentprevThis is such an arrogant pile of bullshit. I’ve seen very obvious colors on many different occasions in the northern part of the lower 48, up in southern Canada, and in Alaska. reply blhack 5 hours agorootparentprevHave you ever seen the Northern Lights with your eyes? If so I'm curious where you saw them. I echo what some other posters here have said: they're certainly not gray. reply poulpy123 31 minutes agorootparentprevthat's a bad example since the only images of aurora borealis are brightly colored. What I expect of an image generator is to output what is expected from it reply darkstar_16 2 hours agorootparentprevNorthern lights are actually pretty colourful, even to the naked eye. I've never seen them pale or b/w reply sdenton4 7 hours agorootparentprevThat doesn't seem in any way useful, though... To use a very blunt analogy, are color blind people intelligent/sentient/whatever? Obviously, yes: differences in perceptual apparatus aren't useful indicators of intelligence. reply shermantanktop 7 hours agorootparentAs a colorblind person…I could see the northern lights way better than all the full-color-vision people around me squinting at their phones. Wider bandwidth isn’t always better. reply Ferret7446 3 hours agorootparent> I could see the northern lights way better than all the full-color-vision people around me How would you know? reply squeaky-clean 1 hour agorootparentQuote the entire sentence, not just a portion of it. reply simonjgreen 3 hours agorootparentprevTo be fair, the prompt isn’t asking for a realistic interpretation it’s asking for a timelapse. What it’s generated is absolutely what most timelapses look like. > Prompt: Timelapse of the northern lights dancing across the Arctic sky, stars twinkling, snow-covered landscape reply laserbeam 6 hours agorootparentprevFor decades, game engines have been working on realistic rendering. Bumping quality here and there. The golden standard for rendering has always been cameras. It’s always photo-realistic rendering. Maybe this won’t be true for VR, but so far most effort is to be as good as video, not as good as the human eye. Any sort of video generation AI is likely to have the same goal. Be as good as top notch cameras, not as eyes. reply 22c 7 hours agorootparentprevI've only ever seen photos of the northern lights and I also didn't know that. reply garyrob 5 hours agorootparentprevEven in NY State, Hudson River Valley, I've seen them with real color. They're different each time. reply Kiro 4 hours agorootparentprevShouldn't the model reflect how it looks on video rather than our naked eye? reply sdenton4 7 hours agoparentprevThis strikes me as equally \"AI complete\" as drawing hands, which is now essentially a solved problem... No one test is sufficient, because you can add enough training data to address it. reply salamo 7 hours agorootparentYeah \"AI complete\" is a bit tongue-in-cheek but it is a fairly spectacular failure mode of every model I've tried. reply smusamashah 1 hour agorootparentIdeogram and dalle do hands pretty well reply sabellito 2 hours agoparentprevPer usual the top comment on anything AI related is snark on \"it can't to [random specific thing] well yet\". reply kmacdough 5 minutes agorootparentTiring, but so is the relentless over-marketing. Each new demo implies new use cases and flexible performance. But the reality is they're very brittle and blunder most seemingly simple tasks. I would personally love an ongoing breakdown of the key weaknesses. I often wonder \"can it X?\" The answer is almost always \"almost, but not a useful almost\". reply svag 13 hours agoprevAn interesting thing that Google does is to watermark the AI generated videos using the [SynthID technology](https://deepmind.google/technologies/synthid/). It seems that the SynthID is not only for AI generated video but for image, text and audio. reply padolsey 5 hours agoparentIt seems really clever, especially the encoding of a signature into LLM token probability selections. I wonder if synthid will trigger some standarization in the industry. I don't think there's much incentive to tho. Open-source gen AI will still exist. What does google expext to occur? I guess they're just trying to present themselves as 'ethically pursuing AI'. reply bardak 8 hours agoparentprevI would like a bit more convincing that the text watermark will not be noticeable. AI text already has issues with using certain words to frequently. Messing with the weights seems like it might make the issue worse reply Tostino 8 hours agorootparentNot to mention, when does he get applied? If I am asking an llm to transform some data from one format to another, I don't expect any changes other than the format. reply ugh123 13 hours agoprevFrom a filmmaking standpoint I still don't think this is impactful. For that it needs a \"director\" to say: \"turn the horse's head 90˚ the other way, trot 20 feet, and dismount the rider\" and \"give me additional camera angles\" of the same scene. Otherwise this is mostly b-roll content. I'm sure this is coming. reply larodi 49 minutes agoparentPerhaps the only industry which immediately benefits from this is the short ads and perhaps TikTok. But still it is very dubious, as people seem to actually enjoy being themselves the directors of their thing, not somebody else. Maybe this works for ads for duner place or shisha bar in some developing country. I’ve seen generated images used for menus in such places. But I doubt a serious filmography can be done this way. And if it can - it’d be again thanks to some smart concept on behalf of humans. reply qingcharles 12 hours agoparentprevI can see using these video generators to create video storyboards. Especially if you can drop in a scribbled sketch and a prompt for each tile. reply ancientworldnow 9 hours agorootparentThat sounds actively harmful. Often we want story boards to be less specific so as not to have some non artist decision maker ask why it doesn't look like the storyboard. And when we want it to match exactly in an animatic or whatever, it needs to be far more precise than this, matching real locations etc. reply gregmac 6 hours agorootparentI hadn't thought about that in movie context before, but it totally makes sense. I've worked with other developers that want to build high fidelity wire frames, sometimes in the actual UI framework, probably because they can (and it's \"easy\"). I always push back against that, in favor of using whiteboard or Sharpies. The low-fidelity brings better feedback and discussion: focused on layout and flow, not spacing and colors. Psychologically it also feels temporary, giving permission for others to suggest a completely different approach without thinking they're tossing out more than a few minutes of work. I think in the artistic context it extends further, too: if you show something too detailed it can anchor it in people's minds and stifle their creativity. Most people experience this in an ironically similar way: consider how you picture the characters of a book differently depending on if you watched the movie first or not. reply sbarre 9 hours agorootparentprevI know you weren't implying this, but not every storyboard is for sharing with (or seeking approval from) decision makers. I could see this being really useful for exploring tone, movement, shot sequences or cut timing, etc.. Right now you scrape together \"kinda close enough\" stock footage for this kind of exploration, and this could get you \"much closer enough\" footage.. reply shermantanktop 6 hours agorootparentI think of it in terms of the anchoring bias. Imagine that your most important decisions are anchored for you by what a 10 year old kid heard and understood. Your ideas don’t come to life without first being rendered as a terrible approximation that is convincing to others but deeply wrong to you, and now you get to react to that instead of going through your own method. So if it’s an optional tool, great, but some people would be fine with it, some would not. reply cpill 27 minutes agorootparentprevI guess this will give birth to a new kind of film making. Start with a rough sketch, generate 100 higher quality versions with an image generator, select one to tweak, use that as input to a video generator which generates 10 versions, coffee one to refine etc reply chacham15 8 hours agoparentprevI dont think \"turn the horse's head 90˚\" is the right path forward. What I think is more likely and more useful is: here is a start keyframe and here is a stop keyframe (generated by text to image using other things like controlnet to control positioning etc.) and then having the AI generate the frames in between. Dont like the way it generated the in between? Choose a keyframe, adjust it, and rerun with the segment before and segment after. reply GenerocUsername 7 hours agorootparentThis appeals to me because it feels auditable and controllable... But the pace these things have been progressing the last 3 years, I could imagine the tech leapfrogs all conventional understanding real soon. Likely outputting gaussian splat style outputs where the scene is separate from the camera and ask peices can be independently tweaked via a VR director chair reply 8note 6 hours agorootparentprevSo a declarative keyframe of \"the horses head is pointed forward\" and a second one of \"the horse is looking left\" And let the robot tween? Vs an imperative for \"tween this by turning the horse's head left\" reply imachine1980_ 8 hours agoparentprevStock videos are indeed crucial, especially now that we can easily search for precisely what we need. Take, for instance, the scene at the end of 'Look Up' featuring a native American dance in Peru. The dancer's movements were captured from a stock video, and the comet falling was seamlessly edited in. now imagine having near infinite stock videos tailored to the situation. reply rzmmm 6 hours agorootparentStock photographers are already having issues with piracy due to very powerful AI watermark removal tools. And I suspect the companies are using content of these people to train these models too. . reply evantbyrne 12 hours agoparentprevThey claim it can accept an \"input video and editing command\" to produce a new video output. Also, \"In addition, it supports masked editing, enabling changes to specific areas of the video when you add a mask area to your video and text prompt.\" Not sure if that specific example would work or not. reply Eji1700 8 hours agoparentprevThere's also the whole \"oh you have no actual model/rigging/lighting/set to manipulate\" for detail work issue. That said, I personally think the solution will not be coming that soon, but at the same time, we'll be seeing a LOT more content that can be done using current tools, even if that means a dip in quality (severely) due to the cost it might save. reply SJC_Hacker 7 hours agorootparentThis lead me to the question of why hasn't there been an effort to do this with 3D content (that I know of). Because camera angles/lighting/collision detection/etc. at that point would be almost trivial. I guess with the \"2D only\" approach that is based on actual, acquired video you get way more impressive shots. But the obvious application is for games. Content generation in the form of modeling and animation is actually one the biggest cost centers for most studios these days. reply sailfast 10 hours agoparentprevFor most things I view on the internet B-roll is great content, so I'm sure this will enable a new kind of storytelling via YouTube Shorts / Instagram, etc at minimum. reply aetherson 8 hours agoparentprevYeah, I've made a lot of images, and it sure is amazing if all you're interested in is, like, \"Any basically good image,\" but if you start needing something very particular, rather than \"anything that is on a general topic and is aesthetically pleasing,\" it gets a lot harder. And there are a lot more degrees of freedom to get something wrong in film than in a single still image. reply gedy 8 hours agoparentprevI think with AI content, we'd need to not treat it like expecting fine grained control. E.g. instead like \"dramatic scene of rider coming down path, and dismounting horse, then looking into distance\", etc. (Or even less detail eventually once a cohesive story can be generated.) reply teaearlgraycold 8 hours agoparentprevEverything I’ve heard from professionals backs that up. Great for B roll. Great for stock footage. That’s it. reply lofaszvanitt 4 hours agoparentprevI can't wait what will the big video camera makers gonna do with tech similar to this. Since Google clearly have zero idea what to do with this, and they lack the creativity, it's up to ARRI, Canon, Panasonic etc. to create their own solutions for this tech. I can't wait to see what Canon has up its sleeves with their new offerings that come in a few months. reply loudmax 15 hours agoprevThe videos in this demo are pretty neat. If this had been announced just four months ago we'd all be very impressed by the capabilities. The problem is that these video clips are very unimpressive compared to the Sora demonstration which came out three months ago. If this demo was announced by some scrappy startup it would be worth taking note. Coming from Google, the inventor of the Transformer and owner of the largest collection of videos in the world, these sample videos are underwhelming. Having said that, Sora isn't publicly available yet, and maybe Veo will have more to offer than what we see in those short clips when it gets a full release. reply alex_duf 1 hour agoparent>these sample videos are underwhelming wow the speed at which we can be blasé is terrifying. 6 months ago this was not possible, and felt this was years away! They're not underwhelming to me, they're beyond anything I thought would ever be possible. are you genuinely unimpressed? or maybe trying to play it cool? reply fakedang 15 hours agoparentprevHonestly, if Veo becomes public faster than Sora, they could win the video AI race. But what am I wishfully thinking - it's Google we're talking about! reply spaceman_2020 3 hours agorootparentThe cost to switch to new models is negligible. People will switch to Sora if its better instantly I’ve switched to Opus from GPT-4 for coding and it was non-trivially easy reply SilverSlash 30 minutes agorootparentExcept your single experience doesn't mean it's generally true, bud. For instance I have not switched to Opus despite claims that it is better because I don't want to go through the effort of cancelling my ChatGPT subscription and subbing to Claude. Plus I like getting new stuff early that OpenAI occasionally gives out and the same could apply for Google's AI. reply ndls 3 hours agorootparentprevI think you used non-trivially wrong there, bud. reply Jensson 14 hours agorootparentprev> But what am I wishfully thinking - it's Google we're talking about! Google the company known to launch way too many products? What other big company launches more stuff early than them? What people complain about Google is that they launch too much and then shut them down, not that they don't launch things. reply xnx 13 hours agoprev60 second example video: https://www.youtube.com/watch?v=diqmZs1aD1g reply candiddevmike 8 hours agoparentFor some reason this video reminds me of dreaming--details just kind of pop in and out and the entire thing seems very surreal and fractal. reply jprete 7 hours agorootparentSame impression here. The scene changes very abruptly from a sky view to following the car. The cars meld with the ground frequently, and I think I saw one car drive through another at one point. reply londons_explore 5 hours agoparentprevLooks like in places this has learned video compression artifacts... reply exodust 3 hours agorootparentFunny if true. Perhaps in some generated video it will suddenly interrupt the sequence with pretend unskippable ads for phone cases & VPNs. reply nixpulvis 7 hours agoparentprevSo… much… bloom. I like it, but still holy shit. I hate that I like it because I don’t want this art form to be reduced by overuse. Sadly, it’s too late. I’ll just go back to living under a rock. reply mccraveiro 15 hours agoprevThey didn't show any human videos, which could indicate that the technology struggles with generating them. reply chubot 14 hours agoparentIt's also probably that it's easier to spot fake humans than to spot fake cats or camels. We are more attuned to the faces of our own species That is, AI humans can look \"creepy\" whereas AI animals may not. The cowboy looks pretty good precisely because it's all shadow. CGI animators can probably explain this better than I can ... they have to spend way more time on certain areas and certain motions, and all the other times it makes sense to \"cheat\" ... It explains why CGI characters look a certain way too -- they have to be economical to animate reply karmasimida 15 hours agoparentprevActually there is one in the last demo, it is not an individual one, but one shot in the demo where a team uses this model to create a scene with human in it, where they created an image of black woman but only up her head in it I would generally agree though, it is not normal they didn’t show more human reply revscat 15 hours agoparentprevI’m sure part of the reason, beyond those given already, is that they want to avoid the debate around nudity. reply himinlomax 12 hours agoparentprevThey're probably still wary of their latest PR disaster, the inclusive and diverse WW2 Germans from Gemini. reply mjfl 12 hours agoparentprevthank goodness. reply dyauspitr 14 hours agoparentprevYou know why and it’s not that their technology struggles with it. reply lewispollard 3 hours agorootparentPlease elaborate, because I certainly don't. reply blinky88 2 hours agorootparentI think he's talking about the diversity controversy reply popcar2 15 hours agoprevNot nearly as impressive as Sora. Sora was impressive because the clips were long and had lots of rapid movement since video models tend to fall apart when the movement isn't easy to predict. By comparison, the shots here are only a few seconds long and almost all look like slow motion or slow panning shots cherrypicked because they don't have that much movement. Compare that to Sora's videos of people walking in real speed. The only shot they had that can compare was the cyberpunk video they linked to, and it looks crazy inconsistent. Real shame. reply latexr 14 hours agoparent> Not nearly as impressive as Sora. Sora was impressive because the clips were long and had lots of rapid movement The most impressive Sora demo was heavily edited. https://www.fxguide.com/fxfeatured/actually-using-sora/ reply jsheard 13 hours agorootparentTo Shy Kids credit they made it clear the Sora footage was heavily edited, but OpenAIs site still presents Air Head without that context. https://www.youtube.com/watch?v=KFzXwBZgB88 (posted the day after the short debuted) https://openai.com/index/sora-first-impressions (no mention of editing, nor do they link to the above making-of video) reply seoulmetro 11 hours agorootparentThere is now on that second link: >The videos below were edited by the artists, who creatively integrated Sora into their work, and had the freedom to modify the content Sora generated. reply jsheard 11 hours agorootparentHa, here's an archive from yesterday for posterity. https://web.archive.org/web/20240513050023/https://openai.co... They also just added a link to the making-of video. reply Aeolun 10 hours agorootparentIf you modified something because it got some attention on HN, at least have the guts to own up to it :/ reply seoulmetro 8 hours agorootparentprevThat's hilarious. Your comment clearly got seen by someone. reply hanspeter 3 hours agorootparentprevI believe it was clear that Air Head was an edited video. The intention wasn't to show \"This is what Sora can generate from start to end\" but rather \"This is what a video production team can do with Sora instead of shooting their own raw footage.\" Maybe not so obvious to others, but for me it was clear from how the other demo videos looked. reply rvz 13 hours agorootparentprevInteresting to see that OpenAI was successful in creating their own reality distortion spells, just like Apple's reality distortion field which has fooled many of these commenters here. It's quite early to race to the conclusion that one is better than the other when not only they are both unreleased, but especially when the demos can be edited, faked or altered to look great for optics and distortion. EDIT: It appears there is at least one commenter who replied below that is upset with this fact above. It is OK to cope, but the truth really doesn't care especially when the competition (Google) came out much stronger than expected with their announcements. reply ijidak 11 hours agorootparentWell, as a counterpoint, Apple did become a $2 trillion dollar company... Distortion is easiest when the products really work. :) reply adventured 11 hours agorootparentApple got up to $3 trillion back in 2023. reply turnsout 9 hours agorootparentIndeed, and they’re at 2.87T today… Built largely on differentiated high-margin products, which is not how I would describe OpenAI. I should clarify that I’m a fan of both companies, but the reality is that OpenAI’s business model depends on how well it can commoditize itself. reply fkyoureadthedoc 13 hours agorootparentprevnext [2 more] [flagged] latexr 13 hours agorootparentHN guidelines ask commenters to be kind and for the discussion to get more thoughtful and substantive as it progresses. If you believe a comment is so bad as to warrant shame and embarrassment, please explain why you think so, rather than being dismissive and spewing insults. On a related note, that is likely why you’re being downvoted. I wouldn’t be surprised if the comment is soon flagged. reply Jensson 15 hours agoparentprev> Sora was impressive because the clips were long and had lots of rapid movement Sora videos ran at 1 beat per second, so everything in the image moved at the same beat and often too slow or too fast to keep the pace. It is very obvious when you inspect the images and notice that there are keyframes at every whole second mark and everything on the screen suddenly goes in their next animation step. That really limits the kind of videos you can generate. reply lupire 14 hours agorootparentSo it needs to learn how far each object can travel in 1sec at its natural speed? reply Jensson 14 hours agorootparentIt also needs to separate animation steps for different objects so that objects can keep different speeds. It isn't trivial at all to go from having a keyframe for the whole picture to having separate for separate parts, you need to retrain the whole thing from the ground up and the results will be way worse until you figure out a way to train that. My point is that it isn't obvious at all that Soras way actually is closer to the end goal, it might look better today to have those 1 second beats for every video but where do you go from there? reply Aerroon 10 hours agorootparentThe best case scenario would probably being able to generate \"layers\" at a time. That would give more creative control over the outcome, but I have no idea how you would do it. reply TIPSIO 14 hours agoparentprevObjectively speaking (if people would be honest with themselves), both are just decent at best. I think comparing them now is probably not that useful outside of this AI hype train. Like comparing two children. A lot can happen. The bigger message I am getting from this is it's clear OpenAI won't have a super AI monopoly. reply TaylorAlexander 14 hours agorootparentComparing two children is a good one. My girlfriend has taken to pointing out when I’m engaging in “punditry”. They're an engineer like I am and we talk about tech all the time, but sometimes I talk about which company is beating which company like it’s a football game, and they call me out for it. Video models are interesting, and to some extent trying to imagine which company is gonna eat the other’s lunch is kind of interesting, but sometimes that’s all people are interested in and I can see my girlfriend's reasoning for being disinterested in such discussion. reply Jonanin 6 hours agorootparentExcept that many of the people involved do think of it like a football game, and thus it actually is like one. Of course the researchers and engineers at both OpenAI and Google DeepMind have a sense of rivalry and strive to one up another. They definitely feel like they are in a competition. reply TaylorAlexander 3 hours agorootparent> They definitely feel like they are in a competition. Citation needed? Although I did not work in AI, I did work at Google X robotics on a robot they often use for AI research. Maybe some people felt like it was a competition, but I don’t have much reason to believe that feeling is common. AI researchers are literally in collaboration with other people in the field, publishing papers and reading the work of others to learn and build upon it. reply Jensson 1 hour agorootparent> AI researchers are literally in collaboration with other people in the field, publishing papers and reading the work of others to learn and build upon it. When OpenAI suddenly stopped publishing their stuff I bet that many researchers now started feeling like it started to be a competition. OpenAI is no longer cooperating, they are just competing. They still haven't said anything about how gpt-4 works. reply Aeolun 10 hours agorootparentprevI’m fairly certain Google just has a big stack of these in storage but never released, or the moment someone pulls ahead it’s all hands on deck to make the same thing. reply motoxpro 12 hours agorootparentprevWhat would make this \"Good?\" reply ein0p 15 hours agoparentprevAlso Sora demos had some really impressive generations featuring _people_. Here we hardly see any people which likely means exactly what you’d guess. reply data-ottawa 14 hours agorootparentHas Gemini started generated impacted of people again? My trial has ended and I haven’t been following the issue. reply nuz 15 hours agoparentprevSora is also movement limited to a certain range if you look at the clips closely. Probably something like filtering by some function of optical flow in both cases. reply arcastroe 14 hours agoparentprev> The shots here [..] almost all look like slow motion or slow panning shots. I think this is arguably better than the alternative. With slow-mo generated videos, you can always speed them up in editing. It's much harder to take a fast-paced video and slow it down without terrible loss in quality. reply btown 10 hours agoparentprevA commercially available tool that can turn still images into depth-conscious panning shots is still tremendously impactful across all sorts of industries, especially tourism and hospitality. I’m really excited to see what this can do. reply pheatherlite 9 hours agoparentprevNot just that, but anything with a subject in it felt uncanny valleyish... like that cowboy clip, the gate of the horse stood out as odd and then I gave it some attention . It seems like a camel's gate. And whole thing seems to be hovering, gliding rather than walking. Sora indeed seems to have an advantage reply __float 8 hours agorootparentI thought a camel's gait is much closer to two legs moving almost at the same time. Granted, I don't see camels often. Out of curiosity can you explain that more? reply spiderfarmer 15 hours agoparentprevAlso the horse just looks weird, just like the buildings and peppers. It's impressive as hell though. Even if it would only be used to extrapolate existing video. reply dyauspitr 10 hours agoparentprevThey’re not showing people because that can get hairy quickly. reply dangoodmanUT 14 hours agoparentprevnext [2 more] [flagged] ipaddr 10 hours agorootparentI can't wait to see any weights. reply LZ_Khan 15 hours agoparentprevI imagine thats just a function of how much training data you throw at it. reply totaldude87 14 hours agoparentprevCould also be the doing of google. if Veo screws up , the weight falls on Alphabet stock. While open AI is not public and doesn't have to worry about anything . Like even if open AI faked some of their AI videos(not saying they did), it wouldn't affect them the way it would affect Veo--> Google-->Alphabet being cautious often puts a dent in innovation reply soulofmischief 14 hours agorootparentYou mean like how they faked some Gemini stuff? https://www.bbc.com/news/technology-67650807 reply sanjayk0508 4 minutes agoprevits a direct competition to sora reply mrcwinn 6 hours agoprevOpenAI has the model advantage. Google and Apple have the ecosystem advantage. Apple in particular has the deeper stack integration advantage. Both Apple and Google have a somewhat poor software innovation reputation. How does it all net out? I suspect ecosystem play wins in this case because they can personalize more deeply. reply xNeil 5 hours agoparent>Google and Apple have a somewhat poor software innovation reputation. I'm assuming you mean reputation as in general opinion among developers? Because Google's probably been the most innovative company of the 21st century so far. reply bugbuddy 4 hours agorootparentYes, I miss Stadia so much. It was the most innovative streaming platform I had ever used. I wished I could still use it. Please, Google, bring Stadia back. reply teaearlgraycold 2 hours agorootparentThey’re renting out the tech to 3rd parties reply miki123211 6 hours agoparentprevGoogle and Apple also have an \"API access\" advantage. It is similar to the ecosystem advantage but goes beyond it; Google and Apple restrict third-party app makers from access to crucial APIs like receiving and reading texts or interacting with onscreen content from other apps. I think that may turn out to be the most important advantage of them all. This should be a far bigger concern for antitrust regulators than petty squabbles over in-app purchases. Spotify and Netflix are possible (if slightly inconvenient) to use on iOS, a fully-featured AI assistant coming from somebody who isn't Apple is not. Google (and to a lesser extend also Microsoft and Meta) also have a data advantage, they've been building search engines for years, and presumably have a lot more in-house expertise on crawling the web and filtering the scraped content. Google can also require websites which wish to appear in Google search to also consent to appearing in their LLM datasets. That decision would even make sense from a technical perspective, it's easier and cheaper to scrape once and maintain one dataset than to have two separate scrapers for different purposes. Then there's the bias problem, all of the major AI companies (except for Mistral) are based in California and have mostly left-leaning employees, some of them quite radical and many of them very passionate about identity politics. That worldview is inconsistent with a half of all Americans and the large majority of people in other countries. This particularly applies to the identity politics part, which just isn't a concern outside of the English-speaking world. That might also have some impact on which AI companies people choose, although I suspect far less so than the previous two points. reply mirekrusin 6 hours agoparentprevNot mentioning Meta, the good guy now, is scandalous. X is not going to sit quietly as well. There is also the rest of us. reply riffraff 3 hours agorootparentX is tiny compared to Apple/Meta/Google, both in engineering size and in \"fingerprint\" in people's life. Also engineering wise, currently every tweet is followed by a reply \"my nudes in profile\" and X seems unable to detect it as trivial spam, I doubt they have the chops to compete in this arena, especially after the mass layoffs they experienced. reply mirekrusin 2 hours agorootparentBy X I mean one guy with big pocket who won't sit quietly - I wouldn't underestimate him. reply lowkey 6 hours agoparentprevGoogle has a deep addiction to AdWords revenue which makes for a significant disadvantage. Nomatter how good their technology, they will struggle internally with deploying it at scale because that would risk their cash cow. Innovator’s dilemma. reply frankacter 6 hours agorootparentGoogle Cloud and cloud services generated almost 9.57 billion. That's up 28% from prior: https://www.crn.com/news/networking/2024/google-cloud-posts-... They are embedding their models not only widely across their platforms suite of internal products and devices, but also computationally via API for 3rd party development. Those are all free from any perceived golden handcuffs that AdWords would impose. reply damsalor 3 hours agorootparentYea, well. I still think there is a conflict of interest if you sell propaganda reply hwbunny 4 hours agoparentprevahem...zzzzzzzz reply inasio 15 hours agoprevFrom a 2014 Wired article [0]: \"The average shot length of English language films has declined from about 12 seconds in 1930 to about 2.5 seconds today\" I can see more real-world impact from this (and/or Sora) than most other AI tools [0] https://www.wired.com/2014/09/cinema-is-evolving/ reply mattgreenrocks 15 hours agoparentThis is very noticeable. Watching movies from the 1970s is positively serene for me, vs the shot time on modern films often leaves me wonder, \"wait, what just happened there?\" And I'm someone who is fine playing fast action video games. Can't imagine what it's like if you're older or have sensory processing issues. reply psbp 13 hours agorootparentMy brain processes too slow for modern action movies. I can tell what's going on, but I always end up feeling agitated. reply MarcScott 3 hours agorootparentI'm okay with watching the majority of action movies, but I distinctly remember watching this fight scene in a Bourne movie and not having a clue what was going on. The constant camera changes, short shot length, and shaky cam, just confused the hell out of me. https://youtu.be/uLt7lXDCHQ0?si=JnVMjmu0WgN5Jr5e&t=70 reply earthnail 1 hour agorootparentI thought it was brilliant. Notice there’s no music. It’s one of the most brutal action scenes I know. Brutal in the sense of how honest it felt about direct combat. reply ryandrake 15 hours agorootparentprevObligatory: Liam Neeson jumps over a fence in 6 seconds, with 14 cuts[1]. 1: https://www.youtube.com/watch?v=gCKhktcbfQM reply aidenn0 14 hours agorootparentI'd like to fact check this amazing comment on that video, but it would require watching Taken 3: > Some of y'all may find how awful this editing gets pretty interesting: I did an Average Shot Length (ASL) for many movies for a recent project, and just to illustrate bad overediting in action movies, I looked at Taken 3 (2014) in its extended cut. > The longest shot in the movie is the last shot, an aerial shot of a pier at sunset ending the movie as the end credits start rolling over them. It clocks in at a runtime of 41 seconds and is, BY FAR, the longest shot in the movie. > The next longest is a helicopter establishing shot of the daughter's college after the \"action scene\" there a little over an hour in, at 5 seconds. > Otherwise, the ASL for Taken 3 (minus the end credits/opening logos), which has a runtime of 1:49:40, 4,561 shots in all (!!!), is 1.38 SECONDS . For comparison, Zack Snyder's Justice League (2021) (minus end credits/opening logos) is 3:50:59, with 3163 shots overall, giving it an ASL of 4.40 seconds, and this movie, at 1 hour 50 minutes, has north of 4,561 for an ASL of 1.38 seconds?!?! Taken 3 has more shots in it than Zack Snyder's Justice League, a movie more than double its length... > To further illustrate how ridiculous this editing gets, the ASL for Taken 3's non-action scenes is 2.27 seconds. To reiterate, this is the non-action scenes. The \"slow scenes.\" The character stuff. Dialogue scenes. The stuff where any other movie would know to slow down. 2.27 SECONDS For comparison, Mad Max: Fury Road (minus end credits/opening logos) has a runtime of 1:51:58, with 2646 shots overall, for an ASL of 2.54 seconds. TAKEN 3'S \"SLOW SCENES\" ARE EDITED MORE AGGRESSIVELY THAN MAD MAX: FURY ROAD! > And Taken 3's action scenes? Their ASL is 0.68 seconds! > If it weren't for the sound people on the movie, Taken 3 wouldn't be an \"action movie\". It'd be abstract art. reply throwup238 14 hours agorootparentIt's worth noting that Taken 3 has a 13% rating on Rotten Tomatoes, which is well in to \"it's so bad it's good\" territory. I don't think the rapid cuts went unnoticed. reply nimithryn 13 hours agorootparentYeah, this sequence is a meme commonly cited to show \"choppy modern editing\" reply llmblockchain 13 hours agorootparentprevMore chops than an MF DOOM track. reply kristofferR 14 hours agorootparentprevThe top comment makes a really good point though: \"He's 68. I'm guessing they stitched it together like this because \"geriatric spends 30 seconds scaling chainlink fence then breaks a hip\" doesn't exactly make for riveting action flick fare.\" Lingering shots are horrible for obscuring things. reply lupire 14 hours agorootparentMovies have stunt performers. And Neeson was only 60 when filming Taken 3. reply troupo 12 hours agorootparentprevKeanu Reeves was 57-8 when he shot the last John Wick. IIRC Bob Odenkirk was 58 in Nobody. Neeson was 60 in Taken 3. There ways to shoot an action scene with an aging star that doesn't involve 14 cuts in 4 seconds. You just have to care about your craft. reply nineteen999 11 hours agorootparentprevIs it Liam Neeson, or his stunt double? reply kemitchell 11 hours agorootparentprevEnjoy some Tarkovsky. reply jsheard 15 hours agoparentprevEven if the shots are very short you still need coherency between shots, and they don't seem to have tackled that problem yet. reply chipweinberger 9 hours agoparentprevIn 1930 they often literally had a single camera. Just worth keeping that in mind. You could not just switch between multiple shots like you can today. reply lobochrome 10 hours agoparentprevShot length, yes - but the scene stays the same. Getting continuity with just prompts seems not yet figured out. Maybe it's easy, and you feed continuity stills into the prompt. Maybe it's not, and this will always remain just a more advanced storyboarding technique. But then again, storyboards are always less about details and more about mood, dialog, and framing. reply joshuahedlund 13 hours agoparentprevHow many of those 2.5 second \"shots\" are back-and-forths between two perspectives (ex. of two characters talking to one another) where each perspective is consistent with itself? This would be extremely relevant for how many seconds of consistent footage are actually needed for an AI-generated \"shot\" at film-level quality. reply runeks 2 hours agoprevDidn't the model ever fail to generate realistic-looking content? If I don't know better I'd think you just cherry-picked the prompts with the best-looking results. reply carschno 2 hours agoparentWhat you see there is a product, not the scientific contribution behind it. Consequently, you see marketing material, not a scientific evaluation. reply tsurba 1 hour agorootparentUnfortunately also the majority of scientific papers for eg. image generation have had completely cherry-picked examples for a long time now. reply Octokiddie 6 hours agoprevOddly enough, I predict the final destination for this train will be for moving images to fade into the background. Everything will have a dazzling sameness to it. It's not unlike the weird place that action movies and pop music have arrived. What would have been considered unbelievable a short time ago has become bland. It's probably more than just novelty that's driving the comeback of vinyl. reply jmathai 5 hours agoparentIt's a lot more than novelty. It's dedicating the attention span needed to listen to an album track by track without skipping to another song or another artist. If that sounds dumb, give it time and you'll get there also. It's not just technology though. Globalization has added so many layers between us and the objects we interact with. I think Etsy was a bit ahead of their time. It's no longer a marketplace for handcrafted goods - it got overrun by mass produced goods masquerading as something artisan. I think the trend is continuing and in 5-10 years we'll be tired of cheap and plentiful goods. reply rjh29 6 hours agoparentprevEven this site just did not impress me. I feel like it's all stuff I could easily imagine myself. True creativity is someone with a unique mind creating something you would never had thought of. reply damsalor 3 hours agorootparentGet a life reply mFixman 2 hours agoparentprevAI generated images and video are not competing against actual quality work with money put into it. They are competing against the quick photoshop or Adobe Aftereffects done by hobbyists and people learning how to work in the creative arts. I never heard HN claiming that Copilot will replace programmers. Why do so many people believe generative AI will replace artists? reply hwbunny 4 hours agoparentprevYeah, but if you bring up a generation or two on this trash, they will get used to it and think this will be the norm and gonna enjoy it like pigs at the troughs. reply SoftTalker 14 hours agoprevVaguely unsettling that the thumbnail for first example prompt \"A lone cowboy rides his horse across an open plain at beautiful sunset, soft light, warm colors\" looks something like the pixelated vision of The Gunslinger android (Yul Brynner's character) from the 1973 version of Westworld. See 1:11 in this video https://www.youtube.com/watch?v=MAvid5fzWnY Incidentally that was one of the early uses of computer graphics in a movie, supposedly those short scenes took many hours to render and had to be done three times to achieve a colorized image. reply AceJohnny2 14 hours agoparentCan't say I see a visual similarity. In any case, \"Cowboy silhouette in the sunset\" is a pretty classic American visual. But the parallel you made between android Brynner's vision and the generated imagery is fun to consider! reply indy 15 hours agoprevAs someone who doesn't live in the US this year's Google IO feels like I'm outside looking in at all the cool kids who get to play with the latest toys. reply roynasser 15 hours agoparentVPN'd right into that playground, turns out the toys were pretty blah reply numbers 14 hours agoparentprevdon't feel left out, we're all on the wait lists reply htrp 16 hours agoprevWas anyone else confused by that Donald Glover segment. It felt like we were going to get a short film, and we got 3-5 clips? reply jsheard 15 hours agoparentAnd those clips mostly look like generic stock footage, not something specific that a director might want to pre-vis. This is what movie pre-vis is actually like, it doesn't need to be pretty, it needs to be precise: https://www.youtube.com/watch?v=KMMeHPGV5VE reply ZiiS 15 hours agoparentprevAlso it is either very good at generating living people or they need to put more though into saying \"Note: All videos on this page were generated by Veo and have not been modified\" reply jsheard 15 hours agorootparentThat \"footage has not been modified\" statement is probably to get ahead of any speculation that it was \"cleaned up\" in post, after it turned out that the Sora demo of the balloon headed man had fairly extensive manual VFX applied afterwards to fix continuity errors and other artifacts. reply iamdelirium 15 hours agorootparentWait, where did you hear this? I would assume something like this would have made somewhat of a splash. reply jsheard 15 hours agorootparentThe studio was pretty up front about it, they released a making-of video one day after debuting the short which made it clear they used VFX to fix Soras errors in post, but OpenAI neglected to mention that in their own copy so it flew under the radar for a while. https://www.youtube.com/watch?v=KFzXwBZgB88 https://www.fxguide.com/fxfeatured/actually-using-sora/ > While all the imagery was generated in SORA, the balloon still required a lot of post-work. In addition to isolating the balloon so it could be re-coloured, it would sometimes have a face on Sonny, as if his face was drawn on with a marker, and this would be removed in AfterEffects. similar other artifacts were often removed. reply curiousgal 15 hours agoparentprevExactly! \"Hey guys big artist says this is fine so we're good\" reply Keyframe 15 hours agoparentprevIt felt AI-generated. reply htrp 15 hours agorootparentI wish it were AI Donald Glover talking and the \"Apple twist\" at the end was that the entire 3 minute segment was a prompt for \"Donald Glover talking about how Awesome Gemini Models are in a California vineyard\" reply thisoneworks 15 hours agoparentprevYeah that wasn't obvious what they were trying to show. Demis said feature films will be released in a while reply miohtama 12 hours agoprev> Veo's cutting-edge latent diffusion transformers reduce the appearance of these inconsistencies, keeping characters, objects and styles in place, as they would in real life. How is this achieved? Is there temporal memory between frames? reply hackerlight 8 hours agoparentProbably similar to Sora, a patchified vision transformer, you sample a 3d patch (third dimension is time) instead of a 2d patch reply NegativeLatency 13 hours agoprevShoulda used youtube to host their video, it's all broken and pixelated for me reply tauntz 14 hours agoprevUh.. First it tells me that I can't sign up because my country is supported (yay, EU) and I can sign up to be notified when it's actually available. Great, after I complete that form, I get an error that the form can't be submitted and I'm taken to https://aitestkitchen.withgoogle.com/tools/video-fx where I can only press the \"Join our waitlist\" button. This takes me to a Google Form, that doesn't have my country in the required country dropdown and has a hint that says: \"Note: the dropdown only includes countries where ImageFX and MusicFX are publicly available.\". Say what? Why does this have to be so confusing? Is the name \"Veo\" or \"VideoFX\"? Why is the waitlist for VideoFX telling me something about public availability of ImageFX and MusicFX? Why is everything US only, again? Sigh.. reply pelorat 13 hours agoparentWe can blame the EU AI act and other regulations for that. reply yoyopa 1 hour agoprevstop with the ridiculous names just some code numbers like BMW reply aragonite 14 hours agoprevWith so much recent focus by OpenAI/Google on AI's visual capabilities, does anyone know when we might see an OCR product as good as Whisper for voice transcription? (Or has that already happened?) I had to convert some PDFs and MP3s to text recently and was struck by the vast difference in output quality. Whisper's transcription was near-flawless, all the OCR softwares I tried struggled with formatting, missed words, and made many errors. reply jazzyjackson 13 hours agoparentYou might enjoy this breakdown of the lengths one person went through to take advantage of the iOS vision API and creating a local web service for transcribing some very challenging memes: https://findthatmeme.com/blog/2023/01/08/image-stacks-and-ip... discussed on HN: https://news.ycombinator.com/item?id=34315782 reply nunez 6 hours agorootparentThis is a work of fucking art. reply aragonite 12 hours agorootparentprevThis is so good - thanks for sharing this! reply thesandlord 12 hours agoparentprevWe use GPT-4o for data extraction from documents, its really good. I published a small library that does a lot of the document conversion and output parsing: https://npmjs.com/package/llm-document-ocr For straight OCR, it does work really well but at the end of the day its still not 100% reply aragonite 11 hours agorootparentThanks! look forward to checking this out as soon as I get home. reply willsmith72 15 hours agoprevall of this stuff i'll believe when it's ready for public release 1. safety measures lead to huge quality reductions 2. the devil's in the details. you can make me 1 million videos which look 99% realistic, but it's useless. consumers can pick it instantly, and it's a gigantic turn-off for any brand reply aprilthird2021 12 hours agoparentThere'll always be a market for cheap low-quality videos, and vice versa always a market for shockingly high quality videos. K. Asif's Mughal-e-Azham had enormous ticket sales and a huge budget spending on all sorts of stuff, like actual gold jewelry to make the actors feel that they were important despite the film being black and white. No matter how good AI gets, it will never be the highest budget. Hell, even technically more accurate quartz watches cannot compete price wise with mechanical masterpiece watches of lower accuracy reply belval 15 hours agoprevWhile it's cool that they chose to showcase full-resolution videos, they take so long to load I thought their videos were just a stuttery mess. Turns out if you open the video in a new tab the smoothness is much more impressive. reply TIPSIO 15 hours agoprevSeems like ImageFX, VideoFX (just a Google form and 3 demos), MusicFX, and TextFX at the links are down and not working. Huge grammar error on front page too. reply sys32768 14 hours agoprevI assume for consumers to use this, we must agree to have product placements inserted into our productions every 48 seconds. reply axblount 15 hours agoprevI hate to be so cynical, but I'm dreading the inevitable flood of AI generated video spam. We really are about this close to infinite jest. Imagine TikTok's algorithm with on demand video generation to suit your exact tastes. It may erase the social aspect, but for many users I doubt that would matter too much. \"Lurking\" into oblivion. reply jprete 14 hours agoparentAt the bottom of the text blurb on the Veo page: \"In the future, we’ll also bring some of Veo’s capabilities to YouTube Shorts and other products.\" So...you're not cynical, it's an explicit product goal. reply lordswork 15 hours agoparentprevIt's already here. There are communities forming around generating passive income from mass producing AI videos as tiktoks and shorts. reply axblount 15 hours agorootparentI saw one of those where a guy just made videos about increasingly elaborate AI generated cakes. You're right, I guess we're mostly there. But those still require some human input. I'm imagining a sort of genetic algorithm for video prompts, no human editing, input, or curation required. reply tikkun 10 hours agorootparentprevWhat's the subreddit? reply barbariangrunge 15 hours agoparentprevYouTube’s endgame is to not need content creators in the loop any more. The algorithm will just create everything reply belter 14 hours agorootparentHenry Ford II: Walter, how are you going to get those robots to pay your union dues? Walter Reuther: Henry, how are you going to get them to buy your cars? reply esafak 14 hours agorootparentprevThe endgame of that is that people will leave. reply darby_eight 14 hours agorootparentI'm somewhat surprised people still watch YouTube with the horrible recommendations and non-stop spam reply layer8 13 hours agoparentprevIf it really suited my exact tastes, that would actually be great. But I don’t see how we’re anywhere close to that. And they won’t target matching your exact taste. They will target the threshold where it’s just barely interesting enough that people don’t turn it off. reply LZ_Khan 15 hours agoparentprevI had the same thought regarding infinite jest recently reply beacon294 14 hours agoparentprevCan you explain this aspect of infinite jest to me without spoiling the book? reply _xander 14 hours agorootparentIt's introduced early on (and not what the book is really about): distribution of a video that is so entertaining that any viewer is compelled to watch it until they die reply redml 14 hours agoparentprevI think of it as we're replacing the SEO spam we have right now with AI spam. At least now we can fight that with more AI. reply sph 2 hours agorootparentThere's a naive statement to make. reply Invictus0 14 hours agoparentprevThis basically already exists for porn reply rm_-rf_slash 15 hours agoparentprevAnd somehow our exact tastes would also include influencer coded advertisements. reply TheAceOfHearts 4 hours agoprevImageFX fails at both of my tests: 1. Generating an image of \"a group of catgirls activating a summoning circle\". Anything related to catgirls tends to get tagged as sexual or NSFW so it's censored. Unsurprising. 2. The lamb described in Book of Revelation. Asking for it directly or pasting in the passage where the lamb is described both fail to generate any images. Normally this fails because there's not much art of the lamb from Book of Revelation from which the model can steal. If I gave the worst of artists a description of this, they'd be able to come up with something even if it's not great. Overall, a very disappointing release. It's surprising that despite having effectively infinite money this is the best that Google is able to ship at the moment. reply SomaticPirate 4 hours agoparentI think this comment is peak Hackernews… dripping with sarcasm and minimizing a significant engineering accomplishment reply barbariangrunge 15 hours agoprevThe company that controls online video is announcing a new tool, and ambitions to develop it further, to create videos without need for content creators. Using their videos to make a machine that will cut them out of the loop. reply infinitezest 9 hours agoparentMales the very long Acknowledgments section at the bottom extra rich. reply Keyframe 15 hours agoprevKind of sucks to be google. Even they're making good progress here, and have laid the foundations of a lot if not most things.. their products are, well there aren't any noteworthy compared to rest. And considering google is sitting on top of one of the largest if not THE largest video database, along with maps, traffic, search, internet.zip, usenet, vast computing resources vertically integrated.. they have the whole advantage in the world. So, the hell are they doing? Why isn't their CEO already out? Expectations from them are higher than from anyone else. reply Workaccount2 15 hours agoparentI don't know how more people don't talk about the 1M context tokens. While the output is mediocre for cutting edge models, you can context stuff the ever living hell out of it for some pretty amazing capabilities. 2M tokens is even crazier. reply lordswork 15 hours agorootparentIt is pretty amazing. I've been using it every day. I do wish you could easily upload an entire repo into it though. reply bongodongobob 13 hours agorootparentHave it write a program to output a repo as a flat file. reply Keyframe 13 hours agorootparentprevThat's a good point. Gemini gatekeeping me on so many answers made me forget about this extraordinary feature of it. reply rm_-rf_slash 15 hours agorootparentprevAnything approaching the token limit I turn into a file and upload to a vector store. Results are comparable between Chat and Assistants. reply InfiniteVortex 15 hours agoparentprevGoogle search has been absolutely ruined in terms of quality. You're right, they've built the base in terms of R&D for many of the AI breakthroughs thats powering competing alternative products.... that happen to be better than Google's own products. Google went from \"Don't be evil\" to just another big corporate tech company. They have so much potential. Regrettable. reply CraftingLinks 15 hours agorootparentThey are fast on their way becoming IBM 2.0. reply jason-phillips 13 hours agorootparentMore like Xerox reply dyauspitr 14 hours agorootparentprevIf anything google search with the Gemini area on the top has been very good for me. reply atleastoptimal 15 hours agoparentprevBecause they punish experimentation as it eats into their bottom line. AI is a tool for ads in the mind of executives at Google. Ads and monetization of human productivity, not an agent of productivity on its own. reply lolinder 15 hours agorootparent\"Laser-focused on the bottom line at the expense of all else\" is not how I'd describe Google, now or at any point in the past. They have a lot of dysfunction, but if anything that dysfunction stems from too much experimentation and autonomy at the leaf nodes of the organization. That's how they get into these crazy places where they have to pick between 5 chat apps or whatever. If Google were as focused on ads as you seem to think we'd at least see some sort of coherent org-wide strategy instead of a complete lack of direction. reply criddell 15 hours agorootparentI'd describe Google as focused on the bottom line after they put the ads guy in charge of search. I'm referring to this article that was posted here recently: https://www.wheresyoured.at/the-men-who-killed-google/ reply khazhoux 14 hours agorootparentThe person now in charge of Search is Elizabeth Hamon Reid, a long-time googler who came up through the ranks from engineer (in Google Maps) to VP over 20 years. She's legit. reply criddell 14 hours agorootparentIs Wikipedia out of date then? https://en.wikipedia.org/wiki/Prabhakar_Raghavan reply khazhoux 13 hours agorootparentAh, according to this, she’s head of Search but reports to Prabhakar. I thought from recent reports that she’d taken search over from him. Nonetheless, she was a good engineer and a good manager, back when we crossed path many moons ago. https://searchengineland.com/liz-reid-google-new-head-of-sea... reply lolinder 14 hours agorootparentprevThat was a decision that prioritized the bottom line over other things. But saying that Google is \"focused\" on the bottom line implies that there's a pattern of them putting the bottom line first, which is simply not true if you look at Google as a whole. Search specifically, maybe, but not Alphabet. reply khazhoux 15 hours agorootparentprevC'mon, Google doesn't \"punish\" experimentation. Google X, Google Glass, Daydream, Fuschia, moonshots, the lab spinoff (whose name I can't remember)... hell, even all the abandoned products everyone here always complains about. The experiments often/usually fail, but they do experiment. reply Koffiepoeder 14 hours agorootparentIf you prune all the branches, where will the fruits grow? reply khazhoux 14 hours agorootparentThe branches were dead and could bear no fruit. New branches will sprout next season. reply saalweachter 8 hours agorootparentFor grapes, the conventional wisdom is to prune all the old branches at the end of each season. reply softwaredoug 14 hours agoparentprevIt's often said you need to disrupt your own business model. Google had blinders on. They didn't relentlessly focus on reinventing their domain. They just milked what they had. Gradually losing site of the user experience[1] to focus on monetization above all else. 1 - https://twitter.com/pdrmnvd/status/1707395736458207430 reply dyauspitr 14 hours agoparentprevTheir CEO is generating massive, growing profits every quarter while releasing generative technology, all the while threading a fine line in what those models generate because it can be pretty devastating for a large corp like Google. reply Keyframe 14 hours agorootparentyou think it's because of him or despite him? reply rishav_sharan 15 hours agoprevNow that the first direct competitor to Sora has been announced, I am sure Sora will be suddenly ready for public consumption, all it's ai safety concerns forgotten reply sebastiennight 12 hours agoparentI think there's a tremendous compute cost associated with both models still... I can't see how either company could withstand the instant enormous demand, even if they tried to command crazy prices. Even at $1 per 5-second video, I think some use cases (including fun/non-business ones) would still overwhelm capacity. reply infinitezest 9 hours agoprev> A fast-tracking shot through a bustling dystopian sprawl How apropos... reply thih9 13 hours agoprevIs there any non slow motion example? The cyberpunk video seems better in that aspect, but I wish there were more. reply s1k3s 13 hours agoprevThis looks really good for promo videos. All scenes in here are basically that. reply makestuff 14 hours agoprevIs there any good blogs/videos that ELI5 how these video generation models even work? reply hipadev23 15 hours agoprevI've never had to click \"Sign in\" so many times in a row. reply flying_whale 15 hours agoparent...and then fill out an actual google form at the end, _after_ you've already signed in, to be added to the waitlist :sigh: reply throwup238 13 hours agorootparent...and enter your email into the form again despite being logged into a Google account. reply aaroninsf 15 hours agoprevIt's mildly interesting how many of the samples shown fail to fully conform to the prompts. Lots of specifics are missing. Kudos to Google for if not foregrounding, being entirely transparent, about this. reply 158 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Veo is a cutting-edge generative video model by Google, capable of producing high-quality videos in various styles from text prompts, utilizing advanced language and vision understanding.",
      "It features editing commands, masked editing, and focuses on visual consistency, developed over years of research to enhance performance, promoting responsible design with watermarks and safety filters for risk reduction.",
      "Veo collaborates with other Google tools like Imagen 3 and SynthID to pioneer text-to-video products, incorporating feedback from creators and filmmakers for future enhancements."
    ],
    "commentSummary": [
      "The debate revolves around the accuracy and limitations of AI-generated images, particularly in recreating natural phenomena like the northern lights.",
      "Discussions include the perception of colors in the northern lights, AI's role in filmmaking, challenges faced by companies like Apple and OpenAI, and generating film shots with AI.",
      "Concerns arise about the quality of AI-generated content, Google's reliance on AdWords revenue, and the future implications of AI in content creation and consumption."
    ],
    "points": 1428,
    "commentCount": 408,
    "retryCount": 0,
    "time": 1715709506
  },
  {
    "id": 40361128,
    "title": "Ilya Sutskever Departs OpenAI after Ten Years",
    "originLink": "https://twitter.com/ilyasut/status/1790517455628198322",
    "originBody": "After almost a decade, I have made the decision to leave OpenAI. The company’s trajectory has been nothing short of miraculous, and I’m confident that OpenAI will build AGI that is both safe and beneficial under the leadership of @sama, @gdb, @miramurati and now, under the…— Ilya Sutskever (@ilyasut) May 14, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40361128",
    "commentBody": "Ilya Sutskever to leave OpenAI (twitter.com/ilyasut)829 points by wavelander 11 hours agohidepastfavorite484 comments zoogeny 5 hours agoInteresting, both Karpathy and Sutskever are gone from OpenAI now. Looks like it is now the Sam Altman and Greg Brockman show. I have to admit, of the four, Karpathy and Sutskever were the two I was most impressed with. I hope he goes on to do something great. reply nabla9 3 hours agoparentTop 6 science guys are long gone. Open AI is run by marketing, business, software and productization people. When the next wave of new deep learning innovations sweeps the world, Microsoft eats whats left of them. They make lots of money, but don't have future unless they replace what they lost. reply chx 3 minutes agorootparent> When the next wave of new deep learning innovations sweeps the world, that won't happen, the next scam will be different it was crypto until FTX collapsed then the usual suspects led by a16z leaned on OpenAI to rush whatever they had on market hence the odd naming of ChatGPT 3.5. When the hype is finally realized to be just mass printing bullshit -- relevant bullshit, yes, which sometimes can be useful but not billions of dollars of useful -- there will be something else. Same old, same old. reply fnordpiglet 3 hours agorootparentprevI don’t feel that OpenAI has a huge moat against say Anthropic. And I don’t know OpenAI needs Microsoft nearly as much as Microsoft needs OpenAI reply cm2187 2 hours agorootparentBut is it even clear what is the next big leap after LLM? I have the feeling many tend to extrapolate the progress of AI from the last 2 years to the next 30 years but research doesn't always work like that (though improvements in computing power did). reply huygens6363 23 minutes agorootparentNot saying it’s going to be the same, but I’m sure computing progress looked pretty unimpressive from, say, 1975 to 1990 for the uninitiated. By the 90s they were still mainly used as fancy typewriters by “normal” people (my parents, school, etc) although the ridiculous potential was clear from day one. It just took a looong time to go from pong to ping and then to living online. I’m still convinced even this stage is temporary and only a milestone on the way to bigger and better things. Computing and computational thought still has to percolate into all corners of society. Again not saying “LLM’s” are the same, but AI in general will probably walk a similar path. It just takes a long time, think decades, not years. Edit: wanted to mention The Mother of All Demos by Engelbart (1968), which to me looks like it captures all essential aspects of what distributed online computing can do. In a “low resolution”, of course. reply benterix 1 hour agorootparentprevExtrapolating 2 years might give you a wrong idea, but extrapolating the last year suggests making another leap that was GPT3 or GPT4 is much, much more difficult. The only considerable breakthrough I can think of is Google's huge context window which I hope will be the norm one day, but in terms of actual results they're not mind-blowing yet. We see little improvements everyday and for sure there will be some leaps, but I wouldn't count on a revolution. reply nabla9 2 hours agorootparentprevThere are no moats in deep learning, everything changes so fast. They have the next iteration of GPT Sutskever helped to finalize. OpenAI lost it's future unless they find new same caliber people. reply sk11001 1 hour agorootparent> They have the next iteration of GPT Sutskever helped to finalize How do you know that they have the next GPT? How do you know what Sutskever contributed? (There was talk that the most valuable contributions came from the less well known researchers not from him) reply bamboozled 1 hour agorootparentprevThey seem to have a huge \"money moat\" now. Partnerships with Apple and MS mean they have a LOT of money to try a lot of things I guess. Before the Apple partnership, maybe it seemed like the moat was shrinking, but I'm tno sure now. Likely they have access to a LOT of data now too. reply gunalx 3 hours agorootparentprevI Don't know. Being able to get azure credits has payed out really well for openai as a business in constant need of computer. reply fnordpiglet 3 hours agorootparentWhich is a very short term advantage. And Anthropic gets aws credits which would you rather have? reply ralfd 2 hours agorootparentprevHow important are top science guys though? OpenAI has a thousand employees and almost unlimited money, and llm are better understood, I would guess continous development will beat singular genius heroes? reply benterix 1 hour agorootparent> OpenAI has a thousand employees and almost unlimited money You could say the same about Google - and yet they missed the consequences of their own discovery and got behind instead of being leaders. So you need specific talent to pull this off even if in theory you can hire anybody. reply wg0 55 minutes agorootparentI am just curious how it happened to Google? Like who were the product managers or others who didn't see an opportunity here exactly where the whole thing was invented and they had huge amounts of data already, whole web basically and the amount of video that no one else can ever hope to have? reply pembrook 7 minutes agorootparentI’m 100% positive lots of people at Google were chomping at the bit to productize LLMs early on. But the reality is, LLMs are a cannibalization threat to Search. And the Search Monopoly is the core money making engine of the entire company. Classic innovators dilemma. No fat-and-happy corporate executive would ever say yes to putting lots of resources behind something risky that might also kill the golden goose. reply jack_riminton 40 minutes agorootparentprevI think the discovery of the power of the LLM was almost stumbled upon at OpenAI, they certainly didn't set out initially with the goal of creating them. Afaik they had one guy who was doing a project of creating an LLM with amazon review text data and only off the back of playing around with that did they realise its potential reply andy99 43 minutes agorootparentprevData volume isn't that important, that's becoming clearer now. What OpenAI did was paid for a bunch of good labelled data. I'm convinced that's basically the differentiator. It's not a academic or fundamental thing to do which is why google didn't do it, it's a pure practical product thing. reply aramattamara 42 minutes agorootparentprevIt's hard to invest millions in employees who are likely to leave to a competitor later. That's very risky, aka venture. reply thebytefairy 37 minutes agorootparentprevA lot of it was the unwillingness to take risk. LLMs were, and still are, hard to control, in terms of making sure they give correct and reliable answers, making sure they don't say inappropriate things that hurt your brand. When you're the stable leader you don't want to tank your reputation, which makes LLMs difficult to put out there. It's almost good for Google that OpenAI broke this ground for them and made people accepting of this imperfect technology. reply l5870uoo9y 1 hour agorootparentprevDifficult to quantify but as an example the 2017 scientific paper “Attention is all you need” changed the entire AI field dramatically. Without these landmark achievements delivered by highly skilled scientists, OpenAI wouldn’t exist or only be severely limited. reply belter 10 minutes agorootparentAnd ironically even the authors did not fully grasp at the time the paper importance. Reminds me of when Larry Page and Sergey Brin, tried to sell Google for $1 million ... reply RandomLensman 25 minutes agorootparentprevMost of every large business isn't science but getting organized, costs controlled, products made, risk managed, and so forth. reply mrklol 1 hour agorootparentprevThey definitely need them to find new approaches which you won’t find normally. reply AdamN 2 hours agorootparentprevAgreed - it's good to have some far thinking innovation but really that can be acquired as needed so you really just need a few people with their pulse on innovation which there will always be more of outside a given company than within it. Right now it's all about reducing transaction costs, small-i innovating, onboarding integrations, maintaining customer and stakeholder trust, getting content, managing stakeholders, and selling. reply dkjaudyeqooe 15 minutes agorootparentprev> Open AI is run by marketing, business, software and productization people. AKA 'the four horsemen of enshitification'. reply loldomsa 14 minutes agorootparentprevI honestly think that is the best course of actions for humanity. Even less chance to see AGI anytime soon if he leaves. reply larodi 45 minutes agoparentprevKarpathy is still a mountain in the area of ML/AI, one of the few people worth following closely on Twitter/X. reply albertzeyer 2 hours agoparentprevGreg Brockman is a very good engineer. And that's maybe even more important in the current situation. reply gdiamos 4 hours agoparentprevI don’t think people give Dario enough credit reply davedx 3 hours agoparentprevI love Karpathy. He's like a classical polymath, a scholar and a teacher. reply yu3zhou4 2 hours agoparentprevJakub Pachocki is still in OpenAI though reply ascorbic 4 hours agoprevJan Leike has said he's leaving too https://twitter.com/janleike/status/1790603862132596961 reply DalasNoin 4 hours agoparentThere goes the so called superalignment: Ilya Jan Leike William Saunders Leopold Aschenbrenner All gone reply pcurve 3 hours agorootparentResignations lead to more resignations....unless mgmt. can get on top of it and remedy it quickly, which rarely happens. I've seen it happen way too many times working 25 years in tech. reply sk11001 52 minutes agorootparentThis might not be bad from the perspective of the remaining employees, it might be that the annoying people are leaving the room. reply e_i_pi_2 16 minutes agorootparentprevRelying on specific people was never a good strategy, people will change but this will be a good test of their crazy governance structure. I think of it similar to political systems - if it can't withstand someone fully malicious getting in power then it's not a good system reply jeanlucas 5 minutes agorootparentSame applies to Sam Altman as well? Thing felt like a cult when he was forced out and everyone threatened to resign. reply belter 6 minutes agorootparentprevSo Satya Nadella paid $13 billion to have....Sam Altman :-)) reply bamboozled 1 hour agorootparentprevI guess if they really thought we had something to worry about, they would've stayed just to steer things in the right direction. Doesn't seem like they felt it was required. Edit: I'd love to know why the down votes, it's an opinion, not a political statement. This community is quite off lately. Is this a highly controversial statement ? People are truly worried about the future and this is just an anxiety based reaction ? reply reducesuffering 3 hours agorootparentprevDaniel “Quit OpenAI due to losing confidence that it would behave responsibly around the time of AGI” “I think AGI will probably be here by 2029, and could indeed arrive this year” Kokotajlo too. We are so fucked reply OtomotO 3 hours agorootparentI am sorry, there must be some hidden tech, some completely different attempt to speak about AGI. I really, really doubt that transformers will become AGI. Maybe I am wrong, I am no expert in this field, but I would love to understand the reasoning behind this \"could arrive this year\", because it reminds me about coldfusion :X edit: maybe the term has changed again. AGI to me means truly understanding, maybe even some kind of consciousness, but not just probability... when I explain something, I have understood it. It's not that I have soaked up so many books that I can just use a probabilistic function to \"guess\" which word should come next. reply n_ary 3 hours agorootparentDon't worry, these are the \"keeping the bridge intact\" speak of people leaving a glorious or so workplace. I have worked at several places, and when people left(usually most well paid ones), they post linkedin/twitter posts to say kudos and inspire that, the corresponding business will be in forefront of the particular niche this year or soon and they would like to be proud of ever being part of it. Also, when they speak about AGI, it raises their(person leaving) marketing value as someone else already know they are brilliant to have worked at something cool and they might also know some secret sauce, which could be acquired at lower cost by hiring them immediately[1]. I have seen these kinds of speak play out too many times. Last January, one of the senior engineers from my current work place in aviation left citing about something super secret coming this year or soon, and they immediately got hired by a competitor with generous pay to work on that said topic. reply reducesuffering 3 hours agorootparent> Also, when they speak about AGI, it raises their(person leaving) marketing value Why yes, of course Jan Leike just impromptu resigned and Daniel Kokotajlo just gave up 85% of his wealth in order not to sign a resignation NDA to do what you're describing... reply Shrezzing 2 hours agorootparentWhile he'll be giving up a lot of wealth, it's unlikely that any meaningful NDA will be applied here. Maybe for products, but definitely not for their research. There's very few people who can lead in frontier AI research domains - maybe a few dozen worldwide - and there are many active research niches. Applying an NDA to a very senior researcher would be such a massive net-negative for the industry, that it'd be a net-negative for the applying organisation too. I could see some kind of product-based NDA, like \"don't discuss the target release dates for the new models\", but \"stop working on your field of research\" isn't going to happen. reply reducesuffering 1 hour agorootparentKokotajlo: “To clarify: I did sign something when I joined the company, so I'm still not completely free to speak (still under confidentiality obligations). But I didn't take on any additional obligations when I left. Unclear how to value the equity I gave up, but it probably would have been about 85% of my family's net worth at least. Basically I wanted to retain my ability to criticize the company in the future.“ > but \"stop working on your field of research\" isn't going to happen. We’re talking about NDA, obviously no-competes aren’t legal in CA https://www.lesswrong.com/posts/kovCotfpTFWFXaxwi/?commentId... reply darkwater 22 minutes agorootparent> Unclear how to value the equity I gave up, but it probably would have been about 85% of my family's net worth at least. Percentages are nice, but with money and wealth absolute numbers are already important enough. You can leave a very, very good life even if you are losing 85% if the remaining 15% is USD $1M. And if not signing that NDA will help you landing another richly paying job + freedom to say whatever you feel it's important saying. reply Miraltar 8 minutes agorootparentprevThis paper and other similar works changed my opinion on that quite a bit. It shows that to perform text prediction, LLMs build complex internal models. https://news.ycombinator.com/item?id=38893456 reply truculent 8 minutes agorootparentprev> truly understanding… when I explain something, I have understood it When you have that feeling of understanding, it is important to recognize that it is a feeling. We hope it’s correlated with some kind of ability to reason, but at the end of the day, you can have the ability to reason about things without realising it, and you can feel that you understand something and be wrong. It’s not clear to me why this feeling would be necessary for superhuman-level general performance. Nor is it clear to me that a feeling of understanding isn’t what being an excellent token predictor feels like from the inside. If it walks and talks like an AGI, at some point, don’t we have to concede it may be an AGI? reply TaylorAlexander 3 hours agorootparentprevNo I’m with you on this. Next token prediction does lead to impressive emergent phenomena. But what makes people people is an internal drive to attend to our needs, and an LLM exists without that. A real AGI should be something you can drop in to a humanoid robot and it would basically live as an individual, learning from every moment and every day, growing and changing with time. LLMs can’t even count the number of letters in a sentence. reply kgeist 24 minutes agorootparent>LLMs can’t even count the number of letters in a sentence. It's a consequence of tokenization. They \"see\" the world through tokens, and tokenization rules depend on the specific middleware you're using. It's like making someone blind and then claiming they are not intelligent because they can't tell red from green. That's just how they perceive the world and tells nothing about intelligence. reply vintermann 2 hours agorootparentprevFrom that AGI definition, AGI is probably quite possible and reachable - but also something pointless which there are no good reasons to \"use\", and many good reasons not to. reply astrange 1 hour agorootparentprevLLMs could count the number of letters in a sentence if you stopped tokenizing them first. reply ben_w 53 minutes agorootparentprev> maybe the term has changed again. AGI to me means truly understanding, maybe even some kind of consciousness, but not just probability... when I explain something, I have understood it. The term, and indeed each initial, means different things to different people. To me, even InstructGPT manages to be a \"general\" AI, so it counts as AGI — much to the confusion and upset of many like you who think the term requires conscious, and others who want it to be superhuman in quality. I would also absolutely agree LLMs are not at all human-like. I don't know if they do or don't need the various missing parts in order to be in order to change the world into a jobless (u/dis)topia. I also don't have any reason to be for or against any claim about consciousness, given that word also has a broad range of definitions to choose between. What do you mean by \"truly understanding\"? reply bbor 3 hours agorootparentprevAs something of a (biased) expert: yes, it’s a big deal, and yes, this seemingly dumb breakthrough was the last missing piece. It takes a few dozen hours of philosophy to show why your brain is also composed of recursive structures of probabilistic machines, so forget that, it’s not neccesary, instead, take a glance at these two links: 1. Alan Turing on why we should never ever perform a Turing test: https://redirect.cs.umbc.edu/courses/471/papers/turing.pdf 2. Marvin Minsky on the “Frame Problem” that lead to one or two previous AI winters, and what an Intuitive algorithm might look like: https://ojs.aaai.org/aimagazine/index.php/aimagazine/article... reply re 1 hour agorootparent> Alan Turing on why we should never ever perform a Turing test Can you cite specifically what in the paper you're basing that on? I skimmed it as well as the Wikipedia summary but I didn't see anywhere that Turing said that the imitation game should not be played. reply _nalply 3 hours agorootparentprevI think what's missing: - A possibility to fact-check the text, for example by the Wolfram math engine or by giving internet access - Something like an instinct to fight for life (seems dangerous) - some more subsystems: let's have a look a the brain: there's the amygdala, the cerebellum, the hippocampus, and so on, and there must be some evolutionary need for these parts reply t4ng0pwn3d 2 hours agorootparentAGI can’t be defined as autocomplete with fact checker and instinct to survive, there’s so so so much more hidden in that “subsystems point”. At least if we go by Bostroms definition… reply vintermann 2 hours agoparentprevThe guy with the \"Bad universal priors and notions of optimality\", which did to Hutter's MIRI program what Gödel did to Hilbert's program. reply debatem1 2 hours agorootparentAny chance you can eli5? I'm familiar with the Godel/Hilbert side but not the relationship to these developments. reply snowbyte 5 hours agoprevWhen walking around the U of Toronto, I often think that ~10 years ago Ilya was in a lab next to Alex trying to figure things out. I can't believe this new AI wave started there. Ilya, Karpathy, Jimmy Ba, and many more were at the right time when Hinton was there too. reply izend 5 hours agoparentAnd none of them build AI companies in Toronto. I’m Canadian and disappointed at how ineffective we are at building successful companies. reply langsoul-com 4 hours agorootparentWhy would anyone start the game on hard mode when easy mode is a border drive away? Us is so outrageously better than the rest that people fly across oceans to start businesses there. Canada, being next door, doesn't have the distance moat to at least slow down the brain drain reply threatofrain 3 hours agorootparentWith regards to talent, there's no particular reason why software centers couldn't be in any major established city in the world. It's not like it takes billions of dollars on a highly uncertain bet like creating a car company, rocket reuse company, or a CPU company. A small crew of people could potentially build the next WhatsApp. On Erlang. reply resonious 51 minutes agorootparentThere are definitely many good programmers all over the world, but there are more in the US, because that's where all the best companies are. So if you're trying to make a good company and you want good programmers, where do you go? reply ForHackernews 45 minutes agorootparentprevIn the case of AI it absolutely does take billions and billions of dollars on an uncertain bet. They bet that throwing more data, more hardware, more GPU cycles at the problem would yield results and it has. reply raverbashing 1 hour agorootparentprevAs much as people on this site like to complain about Europe (and a lot of it is merited) - I've found that Canada manages to be worse. Even having lower on avg bureaucracy reply cmrdporcupine 1 hour agorootparentprevCanada is addicted to rent seeking, monopoly businesses, corporations that push regulatory capture on the gov't and then parasitize, and -- most of all -- ripping resources out of the ground and selling them cheap, or doing the same with real estate. My latest annoyance is all the moaning and groaning about the latest capital gains tax increase. People complaining on one hand about how the Canadian economy lacks productivity, and then screaming to high heaven about tax policy that mostly only impacts people making quick speculative cash. Investment takes no risks in this country because they don't have to. They just dump money into real estate or oil & gas instead and then hang at the lake in the Muskokas. reply nightowl_games 4 hours agorootparentprevCouldn't a company like that get a huge tax benefit from the SRED program? reply loopdoend 4 hours agorootparentThe amount of paperwork involved makes it unworkable reply MassiveQuasar 4 hours agorootparent10B$ says you're wrong. reply llm_trw 4 hours agorootparent10B is going to old mates mates. No new startups are getting it. It's also like pulling teeth trying to explain to people that if we don't offer compensation commensurate with what they get in the US people will just leave for the US. There is some form of brain damage where even people who know how to code assume that because you can get a crud developer for $80k a year you should get an AI researcher for $150,000. It's nearly double after all. reply tbossanova 3 hours agorootparentCompensation isn’t just money. Staying near family and friends is hard to measure. I’ve never moved country for $ alone. reply x-complexity 2 hours agorootparentprevThere is a vast underestimation of how tedious & time-intensive these tax credit programs are when applying for them. A large company can do so because they can hire the people to solely go after them; A new startup (with a headcount that can fit in one hand) is too busy in actually keeping the business alive to pursue these programs, which often times come with conditions too arbitrary for startups to fulfill. reply cmrdporcupine 1 hour agorootparentprevSRED is basically a subsidy for companies that do your SRED paperwork for you, not the company doing the engineering itself. There's a whole industry of this. No evidence that SRED has done anything ever for actual R&D. I've seen people get SRED for making web pages in JavaScript&HTML. When I had to fill in the SRED stuff it was ridiculous. Someone doing actual innovation would throw their hands up in the air. reply typon 4 hours agorootparentprevThe Canadian Dream is to get a great education and then move of the US. You might want to blame the government or this or that but I think as a Canadian I've finally come to reckon with the fact that it's just not in the Canadian ethos to do risky things like make startups. Of course there are exceptions to the rule but they are very very rare. Canadian investors don't want to take big risks and the Americans are just next door waiting to gobble up the talent in search of capital. reply eggdaft 4 hours agorootparentFor folks without responsibilities like kids, aging parents, etc. I really don’t think startups are very “risky”. What’s the worst that happens? It doesn’t work out and after five years you go get a job in boring corp corp with an incredible skillset and vast life experience. You’ve sacrificed some income perhaps, but so what? People make choices like that all the time. Your working career could easily be 40 or 45 years, 5 is not that much and it’s not like you went bankrupt. Your skillset might even mean you more than make up for lost time. I don’t understand the talk of “risk” unless you’re Elon Musk betting the farm on your businesses and facing bankruptcy. Work in your spare time until you have something Angel worthy, then get a modest salary to get to the next level and on you go. Or just bootstrap. Is it easy? No, it’s the hardest thing you’ll ever do. Is it risky? Not so much. So why do Canadians and Brits see it as a risky thing to do? I think they don’t. What they see is _uncertainty_ - where will I be in six months? What if it doesn’t work out? What if I fail and people judge me? They don’t like uncertainty. That is conservative with a small c. Probably it’s a cultural artefact rather than anything remotely rational. The problem is you end up in an equilibrium where the society is conservative (“what you wanna do wasting your time with that”) so the ambitious people just leave and go to somewhere like (parts of) the US where people want to change things, make things, improve the world. And the conservative society gets more conservative until it is ossified. Startups carry high uncertainty but not high risk. reply StrauXX 3 hours agorootparentThere are countries where the business culture makes you unemployable and almost impossible for you to get a loan for the rest of your live if you have ever failed a business (bad enough). Many countries aren't as open to failure as the US. reply vasco 4 hours agorootparentprevThis depends on what you see as risk. If I can safely earn for 5 years way above national average and build a strong savings egg that can provide income forever. Or I can fail at a startup and be close to zero five years later, the fact that you aren't homeless and starving and can get another job doesn't mean it wasn't risky, you still wasted a bunch of years compared to slow and steady accumulation. I've read the majority of millionaires in the US get created like this, working and saving through decades. You're basically repeating investor kool-aid, because for their model to work, 100 people will fail and 1 succeed, and so they tell you to not worry if you're in the 99. reply ghaff 4 hours agorootparentOf course you could probably say at least some of the same things about grad degrees that may not really translate into appreciable different/better career outcomes. Of course some say exactly that, especially about PhDs. reply prmoustache 3 hours agorootparentprevI can't speak for Canada and I may be a wrong, but it seems to me harder to loan money for business than in NA. Banks are the ones that don't want to take risks, not necessarily the people with ideas. Also failures aren't considered the same in every job market. reply fire_lake 4 hours agorootparentprevThe increased cost of living in the last few years has changed this somewhat. That 5 years of lower earnings now means less nice groceries, fewer holidays and being under the yoke of landlords for considerably longer. reply baq 4 hours agorootparentprevTurns out there’s only enough people with this mindset to fill a couple hubs around the world. The rest prefers less volatility and happily takes on less downside risk for capped reward and/or less upside risk. reply crucialfelix 3 hours agorootparentprevAlso: All your comedians move to the US to make it big. reply titanomachy 4 hours agorootparentprevYeah Canada just spends a ton of taxpayer money to create great institutions like U of T and Waterloo, so that their graduates can all go to Silicon Valley and make 2-3x the money. reply llm_trw 4 hours agorootparent> 2-3x the money. That's if you're stuck in tech support. When you start doing actual ground breaking work it starts at x10 and goes up significantly. reply titanomachy 2 hours agorootparentIf you’re a top-tier AI researcher like Ilya, for sure. I was thinking about your run-of-the mill FAANGish senior engineer. reply newzisforsukas 3 hours agorootparentprevAs if the tech economy in the US is unique for some reason reply MrBuddyCasino 3 hours agorootparentIt kind of is? reply sal_regalier 2 hours agorootparentI think that was the joke. reply saithound 4 hours agorootparentprevMaybe the majority of Canadians think that having great higher education institutions and thr people who work in them is a good fit for their way of life, but having Silicon Valley companies and people making SV salaries around epuld make their lives worse? If so, this is great: Canadians don't want to live with the tech crowd, so they provide them with the skills so they can move elsewhere, make their dreams come tuee, and not bother the majority that don't want their presence. NB some actual Canadians in this thread have voiced this possibility. reply vasco 4 hours agorootparentThat makes zero sense, governments invest in education to improve their own country, not to train other countries work forces. If you read anything about Canada ever you will also know they have a bunch of policies to try and stop the brain drain and to recruit tech workers from abroad. reply saithound 3 hours agorootparent> That makes zero sense, governments invest in education to improve their own country The idea is precisely that not having SV types around _improves_ the country, i.e. makes it closer to the preferences of Canadians. And yes, having a foreign tech worker doing 9-to-5 in a large legacy company for thoroughly average salaries is very different from having a SV-style startup culture. There is very little process in Canada to make life difficult for the former style of company, and plenty of process to make operations difficult for the latter. If not having SV folk improves Canada for Canadians, and hqving SV folks improves America for Americans, then this is just mutually beneficial trade. Efforts to try and stop brain drain still makes sense: it's even better if you can convince the citizens you trained to engage in the economic activity you actually want instead of economic activity that you find undesirable, but if you're unable to convince most of them, letting them go is still better than having them stay and engage in their undesirable behavior anyway. Compare: if a large minority of Icelanders wanted to work for the Baby (which Iceland doesn't have), theb stopping the brain drain (convincing them to work in the Merchant Fleet) is the best outcome, but funneling them out (training them in merchant navigation and watching them join the Danish Navy) would still be preferable to them engaging in their desired behavior anyway (form their own pirate gang preying on the very Merchant Fleet you're trying to advantage). reply vasco 3 hours agorootparent> And yes, having a foreign tech worker doing 9-to-5 in a large legacy company for thoroughly average salaries is very different from having a SV-style startup culture Immigrants coming into countries start companies at a disproportionate rate compared to natives. Other than unquantifiable statements about what \"Canadians want\" everything you mentioned so far to justify this idea of \"canada doesnt care if tech graduates leave\" is falsifiable by data. reply saithound 3 hours agorootparentOne last time, the claim is not that \"Canada doesn't care\". It's that it prefers it to the alternative of SV-style companies operating from Canada. Which is consistent both with data, facts on the ground (yes, Canada has laws and administrative processes designed to make SV-style startups difficult to start there, that's precisely what people complain about above!), and the comments of actual Canadians in this very thread. You're welcome to present data falsifying the actual claim if you think you have it (instead of the \"Canada doesn't care\" straw man or misunderstanding that you repeat above, noting that so far you have not even refuted your own straw man by presenting any data). reply vasco 3 hours agorootparent> Maybe the majority of Canadians think that (...) having Silicon Valley companies and people making SV salaries around epuld (sic) make their lives worse This is your claim that I engaged with. If your claim is true it literally means that Canadians do not care if those people leave, in fact they would prefer it. My argument is that you're wrong and Canada and it's people would rather have more tech workers and more tech companies. I don't believe I'm misunderstanding so I think we should probably both give up at this point. reply saithound 2 hours agorootparentI don't tuink it implies that they don't care, it only implies that they find it preferable to one certain alternative (staying AND turning Vancouver into north-SF; the conjunction is load-bearing), and I think this much looks true and well-supported by the facts and revealed preferences. They're not willing to change the rules and procedures that people complain about here, and if you propose they do so, as many have, they say no to that explicitly. But I agree that we should probably disengage, so (barring exceptional new insights on my end) will leave this as my last post in the thread. Thanks for the chat. reply vintermann 2 hours agorootparentprevIt's not as if Canada doesn't benefit from machine learning advances. It just doesn't by having many ML start-ups as a tax base. Canada's skilled immigration policy is a train wreck, but that's another issue. reply newcan1 1 hour agorootparentCanada's skilled immigration policy is amazing. It is attracting some of the best talent in the world. What it is not able to do is retain the talent and is just ending up as a stepping stone to the US. All it needs to do is two things: 1. Provide tax deductions for rent and interest on home loans for new home buyers. 2. Reduce the average taxes to just slightly less than the US tax rate by 5-10% upto 500k. Then watch the magic happen. reply ren_engineer 10 hours agoprevseemed inevitable after that ouster attempt, probably just working out the details of the exit. But the day after their new features release announcement? reply ptero 10 hours agoparent\"Get next major feature to release and you can go as a friend\" might have been part of an earlier agreement. reply ru552 9 hours agorootparentMore like they iced him for the last 6 months to ensure he wasn’t taking their lead to a competitor. He probably hasn’t touched anything in that time. reply coffeebeqn 8 hours agorootparentSitting on the roof? reply djbusby 8 hours agorootparentRest and Vest baby! reply gordon_freeman 7 hours agorootparentprevremember that from the Silicon Valley (HBO) episode. :) reply teaearlgraycold 8 hours agorootparentprevThat was literally me at Google reply lulznews 8 hours agorootparentHow can I get this gig at Google? I’m willing to not work for mid to high six figures. reply dclowd9901 7 hours agorootparentYou have to be someone who’s worth more not working for someone else than not working for Google. reply cmrdporcupine 1 hour agorootparentprevYou don't want it. They'll simultaneously pay you big money to stop you from working somewhere else, and crush your soul at the same time. Also they won't do that anymore, I'm sure. reply golergka 8 hours agorootparentprevSilicon Valley is one of the most underrated documentaries of the last decade. reply cm2187 2 hours agorootparentIf you extend the window a little bit, along Yes Minister, Idiocracy and Demolition Man (the last two being a documentary of our time filmed 20y ago). reply golergka 1 hour agorootparent> Idiocracy I definitely enjoyed this movie, and it's understandable that people of any era, starting with ancient Greece, enjoy lamenting at how stupid people are becoming. However, as long as videos explaining quantum physics and 4-hour long interviews with historians and engineers are still one of the most popular kinds of content on Youtube, I would suggest that it's not a documentary, at least not yet. reply renewiltord 7 hours agorootparentprevYeah, that’s Google’s reputation. Probably the most famous retirement home in the Bay. reply VirusNewbie 7 hours agorootparentbut it's not true. Most people work a lot at G. Maybe some teams in search coast or something. But for every slacker I know 8 people who stay late a fair bit. reply mtnGoat 4 hours agorootparentMy experience differs greatly, company I used to work for did NDA, alpha, beta projects with Google. I was always impressed at how little anyone knew, the fact that nothing was delivered on time, scope creep to the point of almost everything being delayed and most projects were not well thought out nor well architected. I warned one API update would break things if it went live, and it did. Why was the guy from another company the only one able to see that? I’m sure they were working hard at something but it wasn’t ever clear what. reply zaphirplane 2 hours agorootparentBut ask a dynamic programming question and watch it get smashed out None of the points you raise relate to being really good at dynamic programming reply I_ 1 hour agorootparentHaha if only dynamic programming was what least to greatness in software engineering. reply mike_hearn 3 hours agorootparentprevI've actually met someone who was \"on the roof\" at Google once. I asked what he was working on and he admitted he hasn't had a project for the last six months. Until that point I thought the roof was a joke. reply bruce511 5 hours agorootparentprev\"Most\" leaves a lot of room there. Of course there are lots of hard workers at Google. You suggest only about 10% are slackers. But that's 10% of a -lot-. I'm thinking there's a market for an Android app that let's one schedule limited roof space... reply hehdhdjehehegwv 3 hours agorootparentprevStrong disagree, I’ve never seen people work less than my time there. reply barbazoo 6 hours agorootparentprev“Working a lot” doesn’t necessitate “staying late” reply refulgentis 6 hours agorootparentprevNah man come on lol. (source: worked at Google 2016-2023) reply sailfast 7 hours agorootparentprevI have to say I'm a bit surprised \"gardening leave\" is not more of a norm in tech like it is in investment banking or finance. reply TheKarateKid 5 hours agorootparentIt's already become the norm in Big Tech for layoffs to avoid filing WARN notices. reply refulgentis 6 hours agorootparentprevBig Tech wasn't what I expected, it seems there's almost a forced perspective that individuals don't make a difference (they do) reply FuckButtons 5 hours agorootparentOne of the inputs into any business is labor, if labor is replaceable then business can function much more cost efficiently, since market forces on a replaceable commodity will reduce its cost. So, big tech acts as though labor is replaceable, because it’s in its economic interest to have that be true, hence the desire for standardization, procedure and systematization of labor, if an individuals output is not unique, they can be replaced, and if they are a replaceable commodity, then market forces will reduce their costs. reply petre 4 hours agorootparentSure business can fire the drones, but institutional know how goes away with the top talent. reply szundi 4 hours agorootparentprevNot the leaving one is the only individual that matters. The org is made of them too, deals have to be made. Would be easier without having to regulate egos. reply beeboobaa3 10 hours agorootparentprevSounds like a threat. reply johnbellone 9 hours agorootparentWhen you take a shot at the king, you better not miss. reply karma_pharmer 5 hours agorootparentOr, in this case, when you take a shot at Machiavelli. reply tjpnz 5 hours agorootparentprevLest you find yourself in a private jet careening into the ground. reply csours 7 hours agorootparentprevI mean, people can also get attached to a feature release. \"I want to work with the team to get this thing done\" reply twobitshifter 7 hours agoparentprevI believe Omni was his work based on an interview he gave about end to end multimodal training being needed to move to the next level of understanding. reply gallerdude 9 hours agoparentprevI would imagine he’d been thinking about it for a while, and maybe with all the buzz about him at the same time of the release, he was asked to decide. reply CooCooCaCha 9 hours agoparentprevCould be a clever play. They sandwiched google io with news which has taken attention from Google. Plus they just had a big announcement so the negative news hits a little less hard. reply treprinum 3 hours agoprevThe usual fate of idealistic people who build something great only to be discarded by management in a power struggle. How often did this repeat? reply wruza 1 hour agoparentWhat do you mean how often, that is a foundation for the most successful economic model in humans. Some may not be discarded, but they will never get enough credit compared to a clueless head with a $1M smile talking to clueless heads with $1B wallets. We should thank god/nature that people who understand and do things exist in our species at all. reply TyrianPurple 4 hours agoprevMeta's next for him? There's lots of money being poured into their AI division and there's lots of compute & being able to do any kind of research he might want. reply jakozaur 2 hours agoprevJakub Pachocki is amazing. He was in top 20 in Polish algorithm competition: https://oi.edu.pl/contestants/Jakub%20Pachocki/ reply rfoo 2 hours agoparentWait, TIL Jakub Pachocki == meret [1], never made the connection. [1] https://codeforces.com/profile/meret reply fhd2 1 hour agoprevNot to be a conspiracy theorist, but the phrase \"So long, and thanks for everything\" used in the tweet reminds me of \"So long, and thanks for all the fish\" from the dolphins in The Hitchhiker's Guide To The Galaxy. The background there is that dolphins are secretly more intelligent than humans, and are leaving Earth without them when its destruction is imminent (something the humans don't see coming). I did once leave a company with a phrase just like that :P A few people there actually got the reference and congratulated me for the burn. reply DalasNoin 3 minutes agoparentIn that metaphor, is openai the humans or are actual humans the humans? So is openai about to be destroyed or humanity? reply ed_mercer 10 hours agoprevHow good or bad is this for OpenAI? reply samspenc 10 hours agoparentA few years ago? Probably catastrophic, he was Chief Scientist after all. Now? Probably not too much, they have enough investment, and additionally talented people wanting to join. I mean, Andrej Karpathy also joined and left OpenAI twice and it didn't impact operations much. I think OpenAI is now where Google was at or just before its IPO, a few key players leaving isn't going to impact them as much as it would have in its earlier founding days, and there is plenty of talent who are ready to jump in to fill the shoes of anyone who leaves. reply cm2187 1 hour agorootparentThat may be true in term of engineering, but I think everyone had switched to google as their search engine by then. I am not sure openai has captured the market quite in the same way, as I think people are still mostly experimenting with AI, the integration time in any large company is much slower than the rate of progress of AI. And it's not clear to me that there is much of a vendor lockin to use the openai API vs an equivalent competitor. reply andsoitis 8 hours agoparentprevAccording to Mr Altman’s tweet (https://twitter.com/sama/status/1790518031640347056) they had not just one but TWO of the greatest minds of this generation. After this change they will have only one. reply aleph_minus_one 7 hours agorootparentSam Altman's tweet only implies that they had >= 2 greatest minds of this generation, and now they have at least one of this breed of people. reply bamboozled 7 hours agorootparentTwo is one and one is none. Not actually sure how much Ilya was doing in the end, but clearly he did a bit, so it's likely a big a loss whatever way you look at it. reply spoonjim 4 hours agoparentprevHe is the smartest guy in AI but the sum of OpenAI’s talent is greater than his. But he could easily be the next great advancement in the field. reply lr4444lr 9 hours agoparentprevDepends on how tight his non-compete is. reply cjbprime 9 hours agorootparentI think those are illegal now. They have been in California for a long time. https://www.ftc.gov/news-events/news/press-releases/2024/04/... reply brianjking 8 hours agorootparentCould still cover him, no idea though. Via the FTC link: \"Existing noncompetes for senior executives - who represent less than 0.75% of workers - can remain in force under the FTC’s final rule, but employers are banned from entering into or attempting to enforce any new noncompetes, even if they involve senior executives. Employers will be required to provide notice to workers other than senior executives who are bound by an existing noncompete that they will not be enforcing any noncompetes against them.\" reply zeroonetwothree 7 hours agorootparentDoesn’t matter since CA has broader laws anyway reply astrange 1 hour agorootparentIt's constitutional in CA. reply zeroonetwothree 7 hours agorootparentprevNon competes are illegal in CA reply darkerside 6 hours agoprevMaking sure generalized AI benefits everybody is the new Don't Be Evil reply unraveller 2 hours agoparent\"We want to put AI in your hands\" to keep?? NO! whatever gave you that idea, evil doer... Open AI, as in, open your hands and beg for another hit of AI through thick rubber gloves and plexiglass. reply seydor 5 hours agoprevWhat next? Meta? reply visarga 5 hours agoparentMaybe Microsoft, for being so close with OpenAI. Maybe Apple, who really needs a tech lead for AI. Maybe Google, his previous workplace, or work for Elon, who was successful in poaching Andrej in the past. Or a startup, he can raise billions if he so wishes. Wherever he goes in a year will compete with OpenAI. Previous time lead researchers had a philosophical disagreement with Sam they left and created Anthropic, which recently caught up to OpenAI. That's the risk of letting Ilya go. And where Ilya goes, other top researchers will go too. reply Tenoke 4 hours agoparentprevIlya cares about AI Safety and AGI. Meta's whole positioning is to dismiss it. No way he goes there. reply can16358p 4 hours agorootparentPerhaps that's exactly why he might go there: to change it for a reason (a new company path long term, or just upcoming potential regulations etc.) I don't believe it either, but in case it happened, it might make some sense that way. reply seydor 1 hour agorootparentprevMaybe the best to guarantee safety is to openly share the science. Lecun is also more 'academic style' than most competing labs reply surfingdino 5 hours agoparentprevIt depends on the anti-compete clauses in his contract. reply navane 5 hours agorootparentI don't know how to word it, but a company that ignores all content rights enforcing a non compete seems ironic to me. reply meowtimemania 5 hours agorootparentprevAren’t those non enforceable now? reply mtnGoat 4 hours agorootparentMight still be enforceable at this level. reply cellis 3 hours agorootparentHypothetically: Could someone play for both the Los Angeles Lakers and Golden State Warriors? Something tells me those non competes are unenforceable. reply mhowland 4 hours agorootparentprevNot really a thing in CA, largely unenforceable. reply surfingdino 4 hours agorootparentThanks for pointing that out, I didn't know it. What about NDAs? reply jessenaser 7 hours agoprevAt least now we know GPT-5 has finished development and is now in training from this (I would hope that Iyla got to add all that he hoped to before leaving). Ilya, thanks for all you have contributed within OpenAI! reply unraveller 3 hours agoparentGPT-5ANDBAG more like it He wouldn't have left if he could advance hoomanity further there, the guy has like a 800ms delay for each word and that does not make for a very good liar, perhaps a dutiful one. reply nomad-nigiri 6 hours agoprevMy bet is he joins Ive reply EasyMark 5 hours agoparentpretty sure he's going to Microsoft reply zandrew 4 hours agorootparentThe \"personally meaningful to me\" spells to me that it's probably a personal project? reply zaps 6 hours agoparentprevMaybe together they can use AI to make a less shitty Christmas tree reply nabla9 7 hours agoprevIf he goes to Microsoft next it was all prearranged a year ago. reply delf 10 hours agoprevNvidia should snatch him. reply rakejake 6 hours agoparentI have a feeling Apple will make a play for him. Apple is considered to be seriously lagging behind in ML. Just his name alone is probably enough for the time being - They can give him his own lab to do whatever he wants. Ilya will attract enough talent, at least some of whom will be willing to take up responsibility over commercial stuff in the coming years. reply seydor 5 hours agorootparentI have a feeling he would like to publish some stuff, and apple doesnt do that reply rakejake 5 hours agorootparentIlya is very much in favor of closed source AI albeit for different reasons. I don't see a problem here. reply robotresearcher 4 hours agorootparentprevTimes have changed. https://machinelearning.apple.com/research reply jack_riminton 2 hours agorootparentprevI think so too, GTP-4o but replacing Siri would be world-changing for mobile reply mft_ 7 minutes agorootparentThat, or something like it, might well be coming at WWDC next month... reply keyle 8 hours agoparentprevThey sell the shovels and the buckets, they're not digging for gold. reply samsartor 6 hours agorootparentThey do participate pretty heavily in ML research from what I've seen. To continue your metaphor, they try to invent as many gold digging techniques as possible which exclusively work with their own shovels and buckets. reply m_mueller 5 hours agorootparentYep, see for example ‘Earth 2’. reply twobitshifter 7 hours agorootparentprevIf you look at deep learning super sampling, they are doing digging and being pretty successful at it. reply djbusby 8 hours agorootparentprevThey've got money for a very BIG experiment tho reply brokencode 8 hours agorootparentI feel like they’re not going to want to go into direct competition with their most valuable customers. reply entangledqubit 7 hours agorootparentHaven't most of the most valuable customers already started rolling their own Nvidia hardware replacements? reply seabrookmx 6 hours agorootparentHedging maybe. But there's no real competition currently. reply whimsicalism 8 hours agorootparentprevtheir most valuable customer is cloud providers and they’re already taking big stakes and picking winners reply WithinReason 2 hours agorootparentprevThey would if they could reply seydor 5 hours agorootparentprevDoesnt hurt to also sell the gold reply yumraj 5 hours agoprevHis phone must be ringing non-stop from all the VCs. reply quyleanh 9 hours agoprevTesla also lost top AI lead [0]. Will they come to Apple? [0] https://news.ycombinator.com/item?id=40361350 reply 23B1 10 hours agoprevAnd just like that, a drawbridge across OAI's moat. reply surume 1 hour agoprevThe future of the company doesn't depend on one engineer. If he left, it's likely because he had a vision that wasn't in line with Sam or Microsoft. Others will take his place and OpenAI will likely reach Elon Musks' recent prediction that AI will improve 100x in the next few years. reply VoVAllen 10 hours agoprevWhy now? reply Tenoke 4 hours agoparentGiven that he went radio silent since the voting out Altman fiasco exactly 6 months ago, it's clearly due to that. reply cowsaymoo 3 hours agoparentprevOne idea could be the product launch dev day, which is something that originally was a point of tension (overcommercialization vs research). Launching GPT-4o at a dev day basically asserts Sam is picking up no compromise on where they were 6mo ago. Good time to finally leave if protesting that is what he believes in. reply EasyMark 5 hours agoparentprevcash out and live the good life? Start his own AI company or support company? Build the next better AI? The sky is the limit reply surfingdino 5 hours agorootparentThere may be a small print limiting his options. reply grugagag 10 hours agoparentprevWhy not? We don’t know details that could involve financial agreements reply Bjorkbat 8 hours agoprevProbably not related, but it's worth pointing out that Daniel Kokotajlo (https://www.lesswrong.com/users/daniel-kokotajlo) left last month. But if it were related, then that would presumably be because people within the company (or at least two rather noteworthy people) no longer believe that OpenAI is acting in the best interests of humanity. Which isn't too shocking really given that a decent chunk of us feel the same way, but then again, we're just nobodies making dumb comments on Hacker News. It's a little different when someone like Ilya really doesn't want to be at OpenAI. reply photochemsyn 7 hours agoparentWell it might be in the best (long-term) interests of humanity to have autonomous flying killer robots powered by OpenAI secret military contracting work cut the human population in half, in the name of the long-term ecological health of the planet, and to cull those not smart or fast enough to run away, thus improving the breeding stock. That's why I don't trust people who run around claiming to be serving the best interests of humanity - glassy-eyed futurists with all the answers should be approached with caution. reply Simon_ORourke 3 hours agorootparent> Well it might be in the best (long-term) interests of humanity to have autonomous flying killer robots powered by OpenAI secret military contracting work cut the human population in half, in the name of the long-term ecological health of the planet, and to cull those not smart or fast enough to run away, thus improving the breeding stock. I love these \"kill 'em all and let God sort them out\" arguments. reply petre 4 hours agorootparentprevWe already have tools to cut the human population in half even without AI. Acting in the best interests of humanity is really a cheesy way to frame it. I'm sure they also told Oppenheimer he was acting in the best interests of humanity. reply starship006 7 hours agorootparentprevWhat? How is this not saying \"Well, it might be in the best interests of humanity for OpenAI to do [hypothetical thing that seems pretty bad that OpenAI has never suggested to do], and because they may consider doing said thing, we shouldn't trust them\"? reply robbomacrae 7 hours agorootparentI think OP is just pointing out that \"acting in the best interests of humanity\" is fairly ambiguous and leaves enough room for interpretation and spin to cover any number of sins. reply FabHK 6 hours agorootparentLike the effective altruists bought themselves a castle with SBF's money - in the best interests of humanity, obviously. reply xyzzy123 6 hours agorootparentprevIf we can't even align OpenAI the organisation full of humans then I'm not sure how well AI alignment can possibly go... reply starship006 3 hours agorootparentprevOkay this is reasonable, thanks for clarifying reply LMYahooTFY 8 hours agoparentprevWhy would that be presumable when his goodbye statement clearly states the opposite? This is baseless fear mongering given that. reply paxys 7 hours agoprevWhy does everyone here think that the guy who quit/lost his job at OpenAI because he didn't agree with their corporate shift and departure from the original non-profit vision is going to be lining up for another big corporate job building closed for-profit AI? reply mnk47 7 hours agoparent>the guy who quit/lost his job at OpenAI because he didn't agree with their corporate shift and departure from the original non-profit vision There is no evidence of this being true. He is one of the biggest proponents of keeping AI closed-source, by the way. reply nicce 7 hours agorootparent> He is one of the biggest proponents of keeping AI closed-source, by the way. From quite different reasons than profit, tho reply surfingdino 5 hours agorootparentprevThat's a naive way of thinking. Keeping it closed source would only make it available to the highest bidder on the black market. reply stale2002 3 hours agoparentprevThe big reason is that when push comes to shove, most of these people don't have any principles. Sure, if they are in a position of power they will wield it how they want. When he caused the whole fiasco, he probably thought it was going to work. But if the choice is between losing the position of influence, or deciding between what position of influence to accept next, well you'll see that the principles are very flexible. We already saw this happen with a few of the \"safety\" researchers that got fired from OpenAI, and yet started working on X AI (I think?), which is definitely not know for \"safety\". reply sahila 7 hours agoparentprevMaybe “better the devil you know than the devil you don't” applies? reply paxys 7 hours agorootparentThen...he would have stayed at OpenAI. reply twobitshifter 7 hours agoparentprevI am hoping he goes open source or to Meta reply Atotalnoob 10 hours agoprevI’m not surprised with what happened with Sam Altmans ousting. He missed the king. I’m surprised he lasted this long. reply hackerlight 10 hours agoparentMira Murati also \"missed the king\" and just delivered the keynote reply okdood64 10 hours agorootparentSeems like she was more appointed, than actually trying to make moves? reply hackerlight 10 hours agorootparentNot according to the reporting reply mikeg8 9 hours agorootparentWhere was this reported? reply hackerlight 8 hours agorootparenthttps://www.nytimes.com/2024/03/07/technology/openai-executi... reply bkyan 8 hours agorootparentprevReid Hoffman provided some clear (at least to me) evidence for Mira's non-involvement → https://youtu.be/IgcUOOI-egk?si=FiSPt87v3pM3lfKt&t=851 reply brandall10 5 hours agorootparentSomeone just above posted this, which shows that she did reach out to the board with concerns about his leadership style prior to the ouster: https://www.nytimes.com/2024/03/07/technology/openai-executi... reply zombiwoof 10 hours agorootparentprevnext [5 more] [flagged] edmundsauto 8 hours agorootparentWhat makes you say that? I have barely any idea who she is outside of this thread. reply vagab0nd 7 hours agorootparentprevCare to elaborate? And who's the biggest? reply blackeyeblitzar 8 hours agorootparentprevI’ve heard this sentiment from others but don’t know much about it. Can you say more? Is it because of her background (product management)? If she doesn’t have any skills why do you think Sam Altman keeps her around - if she a Sam supporter? reply __lbracket__ 9 hours agorootparentprevDont say that out loud though... reply mckirk 10 hours agoparentprevGreat, now I have that whistling stuck in my head again. Thanks for the reminder though, been a while since I've thought of The Wire :) reply dpflan 7 hours agorootparentOh, indeed. reply shmatt 9 hours agoprevFunny enough people will still call OpenAI “an engineering led company” when very obviously it’s slowly being taken over by the same MBAs as Google reply chasd00 7 hours agoprevI wonder if he thinks LLMs are an AGI dead end and he's not interested in selling a product. There's some academic papers floating around coming to the same conclusion (no links. sorry, studying for a cert exam). reply dcchambers 7 hours agoparentThat's been my assumption since the beginning of this drama last year. He seems to have one goal: real AGI. He knows that while LLMs may make something that seems like AGI there's nothing actually intelligent about it and its never going to get them there. OpenAI wants to pivot and sell sell sell because all they see is potential trillions of dollars, and it's time to make money instead of burning more millions/billions chasing a dream. Yet all the AI weirdos on Twitter seem convinced that Ilya \"saw something\" (AGI) and got scared and wanted to pull the plug...lmao. reply Davidzheng 44 minutes agorootparentThis is counter to every interview ilya has ever given since gpt3--he believes scaling llms can get there that's why they scaled to gpt4 scales at all. reply MVissers 5 hours agorootparentprevThere is more money in AGI than LLMs. Whatever it is, language seems key to intelligent algorithms. Nah, he departed due to politics (failed coup) and shift from research first to profit first. Same with Karpathy I believe. He’ll most likely go somewhere where he can get a lot of compute and go back to research first. reply Jensson 4 hours agorootparent> There is more money in AGI than LLMs. That doesn't matter, they know how to do LLMs, they don't know how to do AGI. To modern capitalists that means you invest in one and the other someone else can do. OpenAI was exploratory until they found something that could make them rich, then they went closed and for profit, now you should see them as just another for profit. reply astrange 1 hour agorootparentThey are a nonprofit. reply hugg 42 minutes agorootparenthttps://www.forbes.com/sites/jamesbroughel/2023/12/09/openai... reply msikora 4 hours agorootparentprevLike Google? reply surfingdino 5 hours agorootparentprev> it's time to make money instead of burning more millions/billions chasing a dream. The investors want their money before people realise they have been oversold the dream/threat of AGI. > Yet all the AI weirdos on Twitter seem convinced that Ilya \"saw something\" (AGI) and got scared and wanted to pull the plug...lmao. The market to believe in made-up stories is alaways strong. reply EasyMark 5 hours agoparentprevIsn't it consensus that AGI will never arise from LLMs? reply qp11 4 hours agorootparentJust based on current energy usage never going to happen. You just have to ask them to show you their energy bills alongside their demos. reply danielbln 3 hours agorootparentA plane takes a LOT more energy than a bird, yet both fly, one of them a lot faster. reply ifdefdebug 2 hours agorootparentA plane is good at hauling cargo and goes fast, but general flying skills? Doesn't even come close. reply danielbln 1 hour agorootparentExactly, a giant LLM is not even close to be as power efficient as a human brain, in the same vein as a plane isn't as good at general flying. Yet it provides huge value and in many dimensions (that are important to us as humans) a plane can do way more than any bird. reply Tenoke 4 hours agorootparentprevThere is no such consensus. reply woopsn 4 hours agorootparentprevNo... A good number of folks will even go so far as to say that \"all we do\" is token prediction too. It's worth noting -- OpenAI founder Elon Musk claims in a lawsuit that the company has achieved AGI. Make of that what you will, but certainly there are many people on this site who believe in the general potential of LLMs. reply petre 3 hours agorootparentElon also claims that Tesla has solved full self driving, has announced robotaxis, while the SEC is investigating him for \"inappropriate forward-looking statements\". reply KennyFromIT 7 hours agoparentprevYeah, I study* that way too. *procrastinate reply freecodyx 7 hours agorootparentFunny reply 227 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ilya Sutskever is departing from OpenAI after nearly ten years, expressing trust in the current leadership's capability to advance safe and beneficial artificial general intelligence."
    ],
    "commentSummary": [
      "Key figures' departure from OpenAI, like Ilya Sutskever, sparks concerns about the company's future direction in deep learning leadership.",
      "Discussions involve speculation on advanced AI models, investing in emerging tech challenges, and the potential for AGI development.",
      "Concerns about OpenAI's mission, ethical implications, and a shift towards profit-oriented goals are debated, along with Elon Musk's AGI claims and Tesla's tech, in a conversation including humor and criticism in the tech industry."
    ],
    "points": 829,
    "commentCount": 485,
    "retryCount": 0,
    "time": 1715727686
  },
  {
    "id": 40358309,
    "title": "Glider: Low Latency eInk Monitor Explained",
    "originLink": "https://github.com/Modos-Labs/Glider",
    "originBody": "Glider Open source Eink monitor with an emphasis on low latency. Note: This repo only contains the hardware design, the gateware running on the FPGA is my open-source Caster EPDC design. This README also contains information about the Caster as well. This is a long document, containing not just information about this project, but also pretty much everything I know about Eink. Given it's a bit hard to gather information about Eink online, I think this is the right thing to do. Use the following table of contents to navigate around. Eink is a registered trademark and brand of E Ink Corporation. All the contents provided in this repo are based on publicly available information online and original research. They are not endorsed by Eink in anyway and they may contain errors and/ or inaccurarcies. If you are interested in Eink or any other display technogies, I have a Discord server for that. Feel free to join: https://discord.gg/rtT7euSHQS . (This Discord server is also not endorsed by Eink or any other company. It's not a customer support server.) Table of Contents Overview Features Hardware Components Eink Screens Basic Theory of Operation Advantages and Disadvantages The Role of Eink Controller Screen Panel Types Using Screen with Integrated Controller Using Screen without Integrated Controller Understanding Waveform Greyscale Display Color Display Dithering Eink Screen Generations Caster/ Glider Design Low Latency Drive Hybrid Greyscale Mode Limitations Hardware Design Decisions Gateware Architecture Firmware Functions Resources Utilization Building PCB FPGA Bitstream MCU Firmware Working with Waveforms Compatible Screens References License Appendix Using Screens without Datasheet Screen List Overview Features Complete solution for low-latency/ high-refresh-rate EPD monitor Supports electrophoretics display panels with parallel I/F (Eink(R), SiPix and DES) Supports both monochrome and color-filter-array (such as Kaleido(TM)) based color screen Extremely low processing delay of 200MP/s when disabled The board is designed with KiCad. You may need the latest stable version of KiCad to open the source file. Components This repo hosts the PCB design, firmware source code, and a reference 3D-printable case design. The RTL code is in a seperate repo: https://gitlab.com/zephray/Caster/. Eink Screens Eink is the brand of a family of paper-like electrophoretic displays. The underlying technology is invented in the MIT Media Lab between 1995 and 1997 by Barrett Comiskey, J.D. Albert, and Joseph Jacobson. They later founded the E Ink Corporation to commercialize this technology. Nowadays they are commonly used on e-readers and electronic shelf labels. You’ve probably seen them on Kindle, or in stores, or maybe in some train stations as well. eReader/ Tablets Electronic Shelf Label Digital Signage(Source: https://www.eink.com/application, image copyright Eink corporation) This section gives an overview of the electrophoretics displays, including the screen panels available and underlying technology. Note this project obviously doesn't and can't support all electrophoretic screens. This documentation also solely focuses on using existing off-the-shelf screen panels rather than the physics or manufacturing process of one. Basic Theory of Operation In the simplest form, you have charged particles with different colors, dispersed in some oil in some transparent container. By applying electric fields the particles can be moved up or down to produce either black or white, or a mixture of that. (Source: https://www.eink.com/tech/detail/How_it_works , copyright Eink Corporation) There are multiple technologies based on this basic concept, namely Eink’s micro-capsule display, SiPix (now acquired by Eink)’s micro-cup display, and WFT’s DES display. They differs in specifics ways of confining the particles in containers, but otherwise very similar. The pixels on the screen are typically arranged as a 2D array, driven with TFTs. The pixels are scanned/ driven periodically at a fixed refresh rate, typically ranging from 50Hz to 120Hz. Applying positive voltage on the pixel will typically drive the particles towards the white state, while applying negative voltage will drive the particles towards the black state. This is similar to active matrix TN/IPS LCDs, which also uses 2D TFT arrays and uses electrical fields for changing state. However unlike LCDs, EPDs maintain their state after the electrical field is removed. So unlike LCDs which require continuously refreshing, the EPDs only need to be refreshed till the pixels are fully driven. In terms of driving the screen panel, depending on the pixel value (1 or 0), each pixel would be driven either with a positive voltage or a negative voltage. A global counter can be used to count the frames elapsed, and stop driving the pixels after a predefined period of time (for example, 100ms). Two framebuffers are typically used for determining if the pixel has changed color or not. If not, then the pixel does not need to be driven. Advantages and Disadvantages In terms of display quality, EPDs are no match for modern IPS LCDs. The following is a comparison table of key parameterics. Specific number would vary depending on the screen used, but should be within the same ballpack.Monochrome EPD CFA-based Color EPD Transmissive TFT IPS LCD Reflective TFT TN LCD Contrast Ratio ~17:1 ~14:1 ~1000:1 ~14:1 Colors 16 (Greyscale) 4096 16M 256 Color Gamut ~1.5% sRGB ~99.9% sRGB Reflectivity ~45% ~25% ~15% Response Time ~150ms ~150ms ~10ms ~15ms It has a few advantages. It reflect lights instead of emitting lights, so it generally consumes less power and can be used outdoors, etc. It’s also bistable, means that it retains the image after the power has been removed. Personally, the biggest differentiating factor for me (author of this README) is that it looks like paper. The image above shows a comparison between reflective TFT LCD (SHARP memory LCD in this case) and Eink. The LCD has a mirror-like texture which changes reflectivity drastically in different angles, while the Eink is more paper-like. ZBD LCD Ch LCD STN LCDBistable, reflective, high contrast, no greyscale, ~10s refresh Bistable, reflective, lower contrast, up to 32 level greyscale, ~5s refresh Volatile, reflective, lower contrast, up to 32 level greyscale, ~100ms response There are many other reflective or bistable display technologies. They are all interesting displays on their own, but none of them feels like paper (yet). Overally, there is no single perfect display technology. Each has its own unique strength. Pick the right one for your project. The Role of Eink Controller The Eink controller is in some ways similar to the display controller (DC/ CRTC) + timing controller (TCON) in a typical LCD based system. It takes the raw image data and convert it to signals required to drive the screen. To understand the actual work of an eink controller, start from the basic concept. The color of a pixel can be changed by applying positive or negative voltage for a finite period of time. From the controller’s perspective, depending on the current state of the pixel and the desired state of the pixel, there are 4 possibilities. Current State Target State Action Black Black No operation Black White Apply positive voltage White Black Apply negative voltage White White No operation The controller need to store and maintain the screen state inside of its own buffer memory, so it would typically have a large on-chip SRAM or an off-chip SDRAM controller. The controller should also have a timer to ensure the screen doesn't get overdriven or underdriven. Controller often use the so-called \"waveform\" to replace the action colume of the previous table. Instead of hardcoding the action for state transition, the actions are stored into a look-up-table (LUT) which can be modified at runtime to allow higher flexibility. Controllers may also offer more advanced features such as dithering acceleration, multiple region update, automatic LUT selection, etc. Screen Panel Types As discussed in the previous section, an Eink screen need to be coupled to an Eink controller to function. Aside from that, screen also needs high voltage drivers to drive the TFTs and the pixels. Virtually all E-paper panels use either COG (Chip-on-Glass) or TAB (Tape Auto Bonding) to integrate some chips onto the screen panel itself. Most of the screens available today can be divided into two categories based on whether or not the controller is integrated in: Here is a non-exhaustive list of the type based on their size: (the size or resolution is not related to or limited by the type, it is just for a certain size, the vendors tend to make them the same type.) Screens without controller: 4.3\", 6.0\", 7.8\", 8.0\", 9.7\", 10.3\", 13.3\", 25.3\", 31.2\", 42\" Screens with controller: 1.02\", 1.54\", 2.13\", 2.6\", 2.9\", 3.71\", 4.2\", 5.65\", 5.83\", 7.5\", 12.48\" One may notice that almost all e-readers/ e-ink cellphones use screens without controller, while almost all e-ink electronic shelf labels (ESL) use screens with controller. This gives some hints about the advantages and disadvantages of two types:Without Controller With Controller System Cost High. A dedicated controller or SoC with integrated contoller is usually required. Needs a dedicated power supply. Low. Virtually any MCUs could drive the screen directly, and the power supply is integrated in. Greyscale Levels Generally 16 (4bpp), up to 32 (5bpp) Generally 2 (BW only) or 4 (2bpp), with some hack, up to 16 (4bpp) Refresh Speed Generally fast (100ms~300ms) for BW. Depends on the screen used and the system architecture Generally fast (100ms~300ms) for BW if the partial refresh is enabled. Greyscales much slower, BWR or BWY screens would be even slower. Total Update Latency Generally the same as refresh time. Depends on the system architecture Slow. Ranging from 100ms to several seconds based on the resolution. Please keep in mind the discussion is about off-the-shelf screens you can buy today. These tradeoffs do not necessarily come from the fact the controller is integrated or not. Note that I mentioned the refresh speed and total update latency. They are different: The refresh speed refers to the time it takes to start refreshing the screen: from starting seeing screen changing, to the screen finish showing the new content. The total update latency refers to the latency when the processor needs to update the screen, to the screen finish showing the new content. As you could see, this is the biggest issue for screens with controllers. This is the main reason why they are almost never used on e-readers or cellphones or PC monitors. This diagram illustrates the difference between two. It should be noted that the screens without controller have the flexibility to be driven quickly, but the system designer might not architect the system for low latency. Using Screen with Integrated Controller Screens with integrated controller have almost everything already integrated. Common display panels in this type only need few external capacitors, inductors, and MOSFETs to support the integrated bipolar power supply circuit, then it could be hooked up to MCUs or MPUs using common interfaces like SPI or I2C. There are a lot of driving boards and examples of these screens available online. To be expanded (TODO) Using Screen without Integrated Controller This could get complicated. Note I used a lot of \"generally\" in the previous comparison table because there are many things one could do to drive them. Some of them would certainly impact the performance. The main issue here the controller chip. There are three solutions to drive these screen: Using a dedicated controller chip to drive the screen Using an SoC that has an integrated controller Using a fast MCU/SoC to emulate the controller with GPIO (software timing controller) Then, again here is a comparison between them:Specialized controller chip SoC with integrated controller MCU + Software TCON Resolution UXGA+ UXGA+ Limited by MCU RAM. Up to XGA with SRAM, UXGA with PSRAM, UXGA+ with DDR Greyscale Up to 32 Up to 32 Up to 32 Partial Update Yes Yes Yes Total Update Latency Depends. Could be very close as refresh speed, could be slow like screens with controller Same as refresh speed Same as refresh speed if data is internally generated (not streaming from an external device such as a PC) Suitable Applications IoT devices, E-readers, cellphones, E-ink monitors. possibly E-ink laptops Advanced IoT devices, E-readers, cellphones, E-ink typewriters, possibly lower performance E-ink laptops When using MCU: IoT devices, large ESLs, simple DIY E-readers. When using MPU: Same as SoC with integrated controller When using a dedicated controller, it could accept data from external devices. This allows it to be used in various different types of applications. Ranging from IoT devices, ESLs, to PC monitors with relatively fast refresh rate and low latency. When using SoC or MCU, the display content is generated by the SoC or MCU itself, which ultimately is limited by the capability of the SoC or MCU. Given the current SoCs with E-ink display controllers are usually limited in performance, the application is limited. The same goes for MCU, it does what an MCU could do. You could find ways to stream video data into SoC or MCUs by using USB, camera interface, WiFi, etc., but this might not be optimal. Existing Solutions Specialized controller chip Closed-source EPSON S1D13xxx: Widely used EPD controller in early E-readers. Proprietary, no documents available. Probably EOL. IT8951: Used on waveshare EPD Hat. Documents available, works with large EPDs up to 2048x2048. The drawback is the speed as the interface between processor and IT8951 could be slow. This is similar to the situation on screens with integrated controller T1000: Also known as IT8957, upgraded model of IT8951. It supports even higher resolution. It features higher speed MIPI DSI interface to mitigate the slow speed of IT8951. Waveshare HDMI driver board: FPGA-based controller. Closed source but easily purchasable, could be integrated into larger projects as a module. Open-source This project (Caster + Glider): FPGA-based controller, multiple update modes, ultra low latency processing, wide range of screen support. https://hackaday.io/project/21607-paperback-a-desktop-epaper-monitor: FPGA-based controller. However, doesn't support partial update mode and slower speed. https://hackaday.io/project/21168-fpga-eink-controller: FPGA-based controller, supports vendor waveform with reasonable speed. SoC with integrated controller RK29xx: Fairly old, Cortex-A8 based (RPi 1 level performance), 55nm, EOL RK3026/RK3028: Fairly old, Cortex-A9 based (RPi 2 level performance), 40nm, EOL i.MX 50: Fairly old, Cortex-A8 based (RPi 1 level performance), 65nm, in production i.MX 6ULL: Cortex-A7 based (RPi 1 level performance), 40nm, in production i.MX 6S/D: Fairly old, Cortex-A9 based (RPi 2-3 level performance), 40nm, in production i.MX 7S/D: Cortex-A7 based (RPi 2 level performance), 28nm, in production i.MX 8ULP: Cortex-A35 based (RPi 2 level performance), 28nm FD-SOI, in production AW B200: Cortex-A53 based (RPi 2 level performance), 28nm, in production MT8113: Cortex-A53 based (RPi 2 level performance), 12nm, in production RK3566/RK3568: Cortex-A55 based (RPi 3 level performance), 22nm, in production RK3576: Cortex-A72 + A53 based (RPi 4-5 level performance), 8nm, in production MCU/SoC + Software TCON http://essentialscrap.com/eink/waveforms.html: One of the earliest e-ink hack. Limited in performance but still could be used as a reference NekoCal: One of the earliest e-ink software TCON with greyscale support. Used to be available as a DIY kit. No longer updated, still could be used as a reference InkPlate 6/10: Commercially available. Based on ESP32. EPDiy: Based on ESP32, supports a lot of different screens, recommended if want to build some device with ESP32+Eink or embedding it into a larger project. Interface Signals and Timing The interface signals and timing are fairly similar to LCDs without controller. Following is the list of signals typically found on EPDs: GDOE/ MODE: Gate driver output enable GDCLK/ CKV: Gate driver clock (like HSYNC in LCD) GDSP/ SPV: Gate driver start pulse (like VSYNC in LCD) SDCLK/ XCL: Source driver clock (like PCLK in LCD) SDLE/ XLE: Source driver latch enable (like HSYNC in LCD) SDOE/ XOE: Source driver output enable SDCE/ XSTL: Source driver start pulse (like DE in LCD) SD: Source driver data (8-bit or 16-bit) SD signals goes into the source driver, typically the X direction. GD signals goes into the gate driver, typically the Y direction. It's a 2D array, gate driver selects one line at a time, and the source driver output the voltage for all the pixels in that line. Conceptually, it's like raster scan on a CRT. To send one field of data, both GD and SD are reset to the start position by using the start pulse signal. Data are then transmitted into the source driver 4 or 8 pixel at a time. Once the line has been fully transmitted, the source driver is reset to the beginning position by start pulse signal, and the gate driver moves to the next line by a pulse on the gate driver clock. Once all lines have been scanned, the entire process repeats for the next field. One notable difference with LCD is that each pixel is represented by 2 bits. This, however, doesn't mean each pixel is 2bpp or 4-level greyscale. The 2-bit per pixel is used to encode the voltage applied to the pixel: 00: No voltage 01: Negative voltage 10: Positive voltage 11: No voltage Just like CRT/ LCD, ther are also blanking periods in the entire timing (means it's just waiting without active pixel data being sent). They have identical meaning to CRT/ LCD systems: (Source: https://projectf.io/posts/video-timings-vga-720p-1080p/, Copyright Will Green) The following is a piece of pseudo-code implementing the Eink timing: #define DATA_BUS_WIDTH 8 // 8bit wide bus #define PIXEL_PER_CYCLE (DATA_BUS_WIDTH / 2) #define VFP 12 // Vertical front porch #define VSYNC 1 // Vertical sync length #define VBP 2 // Vertical back porch #define VACT 758 // Vertical active lines #define HFP 72 // Horizontal front porch #define HSYNC 2 // Horizontal sync length #define HBP 2 // Horizontal back porch #define HACT (1024 / PIXEL_PER_CYCLE) void pulse_h_clock() { sdclk = 1; sdclk = 0; } void drive_line(bool v_in_act) { sdce = 1; gdclk = 0; for (int i = 0; i 6:1 2008 TTL 33 33P-AED050SU3V220 800x600 PearlTTL 39 ED052TC2320 960x540 Carta 45% 16:1 2016 TTL 40 ED052TC4 VB3300-EBA 320 1280x720 Carta 1.2 45% 16:1 2017 TTL 50 EC058TC1 SA1452-EHA 320 1440x720 Kaleido / Carta 24% 15:1 2020 TTL 50 ED058TC7320CartaTTLED058TC8 VB3300-EHB 320 1440x720 CartaTTLED060SC12.1 800x600 TTL 39 39P-BED060SC3V100 800x600 VizplexTTL 39 39P-BED060SC4V110 800x600 Vizplex 35% >6:1 2008 TTL 39 39P-BED060SC7V220E 800x600 Pearl 40% 12:1 2010 TTL 34 34P-BED060SCAV110 800x600 VizplexTTLED060SCEV220/V220E 800x600 PearlTTL 34 34P-BED060SCFV220 800x600 PearlTTL 34 34P-AED060SCGV220E 800x600 PearlTTL 34 34P-BED060SCNV220E 800x600 PearlTTL 34 34P-AED060SCS 800x600 TTL 34 34P-BED060SCPV220 800x600 PearlTTL 34 34P-AED060SCQV220 800x600 PearlTTLED060SCS 800x600 TTLED060SCT320 800x600 CartaTTL 34 34P-BED060SD1320 800x600 CartaTTLED060XC3V220 1024x758 PearlTTL 34 34P-A Yes ED060XC5V220 1024x758 Pearl 35% 12:1 2011 TTL 34 34P-AED060XC8V320 1024x758 CartaTTL 35 35P-A Yes ED060XC9 1024x758 TTL 34 34P-AED060XCD320 1024x758 CartaTTLED060XCG VD1405-FOA 320/400 1024x758 Carta 1000 / 1200 40% 17:1 2020 TTLED060XCH VD1405-FOE 400 1024x758 Carta 1200TTLED060XD4320 1024x758 CartaTTL 34 34P-A Yes ED060XD6 1024x758 TTL 34 34P-AED060XG1V110/V220 1024x758 Vizplex / Pearl 40% 12:1 2012 TTLED060XG2V220 1024x758 PearlTTLED060XG3320 1024x758 CartaTTLED060XH2 1024x758 TTL 34 34P-AED060XH7320 1024x758 Carta 1.2 45% 17:1 2015 TTLED060XH9 VB3300-FOG 320 1024x758 CartaTTLED060TC1320 1448x1072 CartaTTL 35 35P-AED060KC1320 1448x1072 Carta 46% 17:1 2014 TTL 34 34P-AED060KC4320 1448x1072 CartaTTLED060KD1320 1448x1072 CartaTTL 34 34P-A Yes ED060KG1320 1448x1072 Carta 47% 17:1 2015 TTL 34 34P-AED060KH4320 1448x1072 CartaTTLED060KH6 VB3300-FOE 320 1448x1072 CartaTTLED060KHC 1448x1072 TTLEC060KH3 SA1452-FOA1448x1072 KaleidoTTLED061KC1 VD1405-FAA 400 1648x824 Carta 1200TTLED067KC1 VB3300-FGA 320 1800x900 Carta 45% 16:1 2020 TTL 50 50P-BEC067KC1 SA1452-FGA1800x900 KaleidoTTL 50 50P-BED068TG1320 1440x1080 Carta 44% >19:1TTLED070KC2320 1680x1264 Carta 1100 >47% >16:1TTLED070KC3320 1680x1264 Carta 1100TTLED070KC4 VD1400-GOC 400 1680x1264 Carta 1200TTLED070KH1320 1680x1264 Carta 1100TTLEC070KH1 SC1452-GOA1680x1264 Kaleido PlusTTLLB071WS1 1024x600 7:1TTLET073TC1V320 750x200 Carta 2016 TTLED078KC1 1872x1404 Carta 1.2 45% 16:1 2016 TTL 40 40P-AED078KC2 VB3300-GHC 320 1872x1404 CartaTTL 40 40P-AED078KH1320 1872x1404 CartaTTL 40 40P-AED078KH3320 1872x1404 Carta 1.2TTL 40 40P-AED078KH4 VB3300-GHB 320 1872x1404 CartaTTL 40 40P-AEC078KH3 SC1452-GHA1872x1404 Kaleido PlusTTL 40 40P-AEC078KH4 SC1452-GHB1872x1404 Kaleido Plus ?TTL 40 40P-AEC078KH5 SC1452-GHC1872x1404 Kaleido Plus ?TTL 40 40P-AEC078KH6 SC1452-GHD1872x1404 Kaleido 3TTL 40 40P-AEC078KH7 SC1452-GHE1872x1404 Kaleido 3TTL 40 40P-AED080XC1V110 1024x768 VizplexTTLED080TC1V220 1600x1200 PearlTTLEC080SC2V250 600xRGBx800 Triton 2TTL 40 40P-A Yes ES080KC2 VD1400-HOB 400 1920x1440 Carta 1200TTLES080KH1AC080KH1 AD1004-HOA HAL3 1920x1440 Gallery 3MiniLVDSED097OC1V110A 1200x825 Vizplex 35% 7:1 2008 TTL 33 33P-AED097OC4V110A/V220 1200x825 Vizplex / PearlTTL 33 33P-AED097OD2V220 1200x825 PearlTTL 33 33P-AED097TC1V220 1200x825 PearlTTL 33 33P-AED097TC2 VB3300-JGA 320 1200x825 Carta 1.2 42% 16:1 2016 TTL 33 33P-AEL097TR2 EA2220-JGB1200x825 Spectra 3000TTLED100UC1 VB3300-KOA 320 1600x1200 Carta 45% 16:1 2020 TTL 40 DIRECTES103TC1 VB3300-KCA 320 1872x1404 Carta 1.2 40% 12:1 2016 TTL 40 DIRECTED103TC2 VB3300-KCD 320 1872x1404 Carta 43% 14:1 2019 TTL 40 DIRECTES103TD1320 1872x1404 CartaTTLES103TD3320 1872x1404 CartaTTLEC103TD1 SA1452-KCC1872x1404 KaleidoTTLEC103TH2 SC1452-KCB1872x1404 Kaleido PlusTTLEC103KH2 SC1452-KCD2480x1860 Kaleido 3TTLES107KC1 VD1400-KGA 400 2560x1920 Carta 1200TTLES108FC1320 1920x1080 Carta 46% 16:1 2017 TTL 50 50P-CES108FC2320 1920x1080 CartaTTLED113TC1 VB3300-LCA 320 2400x1034 Carta 35% 12:1 2017 TTL 50 50P-AED113TC2 VB3300-LCB 320 2400x1034 Carta 1.2 35% 12:1 2019 TTL 50 50P-AEC113TC1 SC1452-LCA2400x1034 Kaleido Plus ?TTL 50 50P-AED115OC1V220 2760x2070 Pearl 35% 12:1 2012 TTL 40 DIRECTAC118TC1 AD1004-LHA Gallery 3MiniLVDSES120MC1 VD1400-MOA 400 2560x1600 Carta 1200TTL 40 ES133UT1V220 1600x1200 Pearl 35% 12:1 2013 TTL 39 39P-A Yes ES133UT2320 1600x1200 CartaTTL 39 39P-A Yes ES133UE2320 1600x1200 CartaTTL 39 39P-AED133UT2 VB3300-NCB 320 1600x1200 Carta 1.2 45% 16:1 2016 TTL 39 39P-AED133UT3 VB3300-NCC 320 1600x1200 Carta 45% 16:1 2019 TTL 39 39P-AES133TT3320 2200x1650 Carta 1.2 40% 12:1 2016 TTL 39 ES133TT5 VH1948-NCC 450 2200x1650 Carta 1250TTL 39 EC133UJ1 SD1452-NCB1600x1200 Kaleido 3 OutdoorTTL 39 39P-AAC133UT1 AA1020-NCA1600x1200 Gallery / Gallery 4000 35% 10:1 2020 TTL 39 39P-AEL133US1 1600x1200 Spectra 3000TTL 39 39P-A Yes EL133UR1 EA2220-NCC1600x1200 Spectra 3000 33% 15:1 2020 TTL 39 39P-AEL133UF1 ED2208-NCA1600x1200 Spectra 6QSPIED140TT1 VB3300-IDA 320 1440x300 CartaTTLAC253TT1 AA1020-PEA3200x1800 Gallery Plus / Gallery 4000 35% 10:1 2020 MiniLVDS 51x2 EL253EW1 ED2208-PEA3200x1800 Spectra 6MiniLVDSEC253TT1 SD1452-PEA3200x1800 Kaleido 3 OutdoorMiniLVDSED253TT1 VB3300-PEA 320 3200x1800 Carta 1.2MiniLVDS 51x2 ED253TT2 VB3300-PEB 320 3200x1800 Carta 1.2MiniLVDS 51x2 ED253TT3 VB3300-PEC 320 3200x1800 Carta 1.2MiniLVDS 51x2 EL253TV1 EB2200-PEA3200x1800 Spectra 3100MiniLVDS 51x2 ED280TT1 VB3300-PHA 320 3840x1080 Carta 1.2 40% 12:1 2020 MiniLVDS 51x2 ED312TT2 VA3200-QAA V220 2560x1440 PearlTTL 50x4 ED312TT3 VA3200-QAB V220 2560x1440 Pearl 40% 12:1 2018 TTL 50x4 EC312TT2 SB1452-QAA2560x1440 TritonTTL 50x4 EL315TW1 ED2208-QBA2560x1440 Spectra 6QSPIED420TT1V220 2880x2160 PearlTTL 50x2 ED420TT3 VB3300-RBA 320 2880x2160 Carta 1.2 45% 16:1 2020 TTL 50x2 ED420TT5 VB3300-RBB 320 2880x2160 Carta 1.2TTL 50x2 Note: Carta 1.2 is also known as Carta 1000. If the table cell says Carta it also likely means Carta 1000 (could be 1100 as well, I don't know for sure).",
    "commentLink": "https://news.ycombinator.com/item?id=40358309",
    "commentBody": "Glider – open-source eInk monitor with an emphasis on low latency (github.com/modos-labs)567 points by mistercheph 15 hours agohidepastfavorite95 comments snvzz 6 hours agoEvery eINK controller sucks. This person took upon themselves to fix that, and released the result, which is now the state of the art, as open source hardware. I love people and projects like this. reply birdlogic 4 hours agoprevIt’s a fun coincidence that this is named “Glider”, since a fair number of glider (sailplane) pilots use e-ink displays (rooted kobo/kindle readers usually) due to great sunlight readability, commonly running something like XCSoar: https://www.xcsoar.org/hardware/ reply exceptione 13 hours agoprevImpressive breadth and depth of information in just the README alone. When this kind of stuff gets in the open like it does here, I expect rapid innovation and disruption from the crowd. reply abdullahkhalids 12 hours agoparentI would hope that the Pine Note people look at it. Progress for them has been quite slow (I follow the discord), and they are struggling with a lot of basic stuff. reply tadfisher 6 hours agorootparentBecause someone needs to be paid to land Rockchip drivers in mainline, and Rockchip aren't going to do it. reply 3abiton 3 hours agoparentprevYou were not kidding, it's much more detailed than I anticipated down to the physics theory of they work. reply dheera 10 hours agoparentprevAt least this is real ePaper and not that previous bull that was trying to pass off monochrome LCD as low-latency ePaper. reply beacon294 5 hours agoprevI was interested in e-ink solutions for a long time due to eye fatigue and dry eye. It turns out I had a very minor stigmatism. My eye doctor did not recommend correcting it, but upon correction my dry eye and eye fatigue completely went away. So, get your minor astigmatism corrected via computer glasses, regardless of the eye doctor, best practice for minimum prescription strength. reply Baeocystin 3 hours agoparentSeconding this. I have a very minor astigmatism in my right eye. Getting it corrected with custom reading glasses instead of just using regular readers, even though perceptually it's fine with the cheap ones, is night and day in terms of eye strain and headache. I was genuinely surprised when I figured out how much of a difference it makes. reply isoprophlex 1 hour agorootparentWhoa. So, on a whim, I got my eyes measured when my wife was picking up her glasses. Very slight astigmatism in my right eye. And like you say, even though it's a minor abberation, the difference it makes in fatigue is incredible, especially after a day of coding work behind a screen..! Straight up the best 200 bucks I spent in terms of QOL improvement. Well that and a very nice thick blanket to sleep under. reply snvzz 4 hours agoparentprev>computer glasses What is this? reply jdietrich 33 minutes agorootparentGlasses dispensed for reading often don't work well for computer use. Historically \"reading\" meant looking at a book held in your hands, but computer monitors are further away and at a higher angle than a book. If you spend a lot of time using a computer, you need to discuss this with your dispensing optician when you buy glasses to make sure you get the right lenses for your needs. You may benefit from a dedicated pair of glasses for computer use, or a properly designed varifocal lens. This is especially important if you're over 35 and are likely to have some degree of presbyopia (look for an \"add\" value on your prescription, or different prescriptions for near and distance). There are a lot of special lens technologies and coatings that are advertised as being able to reduce eye strain when using digital devices. Most of these have little or no evidence to support their efficacy, so be wary if they try to upsell you on expensive extras. reply idle76 4 hours agorootparentprevGlasses for working at a computer. The same as reading glasses. The word is directly translated from the common name for it in my native language. reply snvzz 2 hours agorootparentOh. So the glasses correct the astigmatism. Understood. reply tonymet 13 hours agoprevI've been using a Kindle for 10+ years now , but the poor responsiveness has always irked me. I can't tell if it's a hardware or software issue. I'm glad to see this project is focused on reducing latency on the hardware side. Does anyone know why the Kindle is such a bad product? I use it because I like e-ink and the e-book market is comprehensive, but I don't think it's actually a good device. reply stronglikedan 13 hours agoparentIt's responsive enough to do what it was purpose built for - read a book. It can do other things, but it's not made or marketed to do them, so they keep the cost low by not innovating on responsiveness. Instead, they make it more comfortable to use in other ways, such as how it's held and navigated, and the backlight. reply tonymet 13 hours agorootparentThere are many high-quality products at a competitive cost. That's a pathetic excuse. A lot of time was spent integrating social features that no one uses. That time could have been spent on quality & latency. I understand their business goals and objectives. It's still a low-quality product. A profitable product can also be terrible. reply hex4def6 11 hours agorootparentAs someone who worked on them a decade ago: To be clear, the displays are not created by Amazon / Lab126. Instead, they're a product of Eink Holdings, Inc. From what I remember, most of the screen refresh algorithms etc are Eink IP. And by the way, the cost of the display module alone was eye-watering, especially when compared with LCD displays... With e-ink, you can drive it faster, at the expense of massive power consumption or terrible ghosting / artifacting. You're not going to get the 6 weeks of use out of a battery doing that. For reading a book, smudges / ghosting sucks, so they optimize for full screen refreshes just often enough to clear that up (that's when the screen goes black then white, followed by the update). It's kind of a physics based fundamental limitation -- the display is closer to a mechanical display of old than an LCD. The kindle is a product that does one thing well: display static text in any lighting condition with a similar quality to the printed page. reply klabb3 9 hours agorootparent> Instead, they're a product of Eink Holdings, Inc. From what I remember, most of the screen refresh algorithms etc are Eink IP. And by the way, the cost of the display module alone was eye-watering Layman here, but what you describe sounds very much like innovation held back by patents: At the core, it’s really promising tech with actual major advantages over LCDs with applications already in many domains and possibly many more in the future; all you’d need really is incremental improvements, similar to the journey of LCD. Remember the shitty TFT(?) monitors from 20 years ago? Ghosting, low resolution, delay, low contrast, backlight bleeding, etc. If we hypothetically had 20 companies competing the traditional way, throwing international manufacturing and material science know-how on these bad boys, I’d bet $100 that we’d see massive gains in ability at a fraction of marginal cost – from incrementalism alone – way before you reach physical limitations. And with a bit of luck, there might be a breakthrough in the core tech as well. > It's kind of a physics based fundamental limitation -- the display is closer to a mechanical display of old than an LCD. I hear you. But brilliant people have been wrong about these statements in all kinds of areas before. Could you share more detailed what those hard limitations might be? reply Qwertious 7 hours agorootparent>Layman here, but what you describe sounds very much like innovation held back by patents It's not patents, it's economy of scale: LCDs ship billions per quarter and are used in phones/watches/laptops/PC monitors/TVs/car-dashes/coffee-machines/fridges/kiosks/etc etc etc, whereas e-ink screens are used in e-readers, supermarket tags, e-notes (stylus tablets), and basically nothing else. When LCDs ship orders of magnitude more SKUs, they inevitably have lower costs. That's just economics. Besides which, Amazon ships Kindles at-cost, there's no way they'd be price-gouged - if E-Ink tried to screw them then they'd buy E-Ink Corp. It wouldn't even be the first passive display company they bought. See: LiquaVista. >Could you share more detailed what those hard limitations might be? The ink in the e-ink needs to be shuffled up and down with each refresh, but if they're pushed too quickly then they pound the capsule they're in and damage it, or get permanently stuck. Either will break the display. And it's powder not a solid object, so the display needs to move all the ink, down, or you'll have ghosting. reply treflop 9 hours agorootparentprevThe core patents have already expired. AFAIK manufacturing e-ink displays is still difficult. I also have worked with e-ink displays for hobby projects and you’re flipping tiny balls of ink. Unfortunately e-ink displays are extremely slow and it only gets worse if you want colors or anything you might want in a display. E-ink displays look cool and sound cool but really suck to work with. reply carlosjobim 8 hours agorootparentprevThere are high tech eInk devices that can refresh much faster than a Kindle and that you can buy right now, but any eInk discussion on Hacker News lives in a parallel universe where those devices should never be mentioned and we should pretend that the technology is where it was 10 years ago. With the rapid rate of development recently, I would expect eInk displays to break the \"magic\" 24 fps barrier in 2025 and hit the mainstream in a major way. Considering that offices worldwide have been constructed to block out sunlight to accommodate display use, this tech has the potential to change everything. reply klabb3 6 hours agorootparentVery cool! Do you have a link to a demo to see what it would look like? reply carlosjobim 6 hours agorootparentCheck out the Boox devices, there's a YouTuber called \"mydeepguide\" who makes extensive reviews. reply faeriechangling 5 hours agorootparentprevI've tried to read manga and graphic novels on e-readers which makes EVERY page turn take longer, and it's very clear to me that e-ink latency is a huge problem. Boox is notable for having faster page turns since they're essentially just customised android tablets with an e-ink screen. reply Tagbert 12 hours agorootparentprevfrom what I've read, responsive screen refresh is inversely related to battery life. A more responsive screen results in less battery life. Amazon, and pretty much all other eink readers have prioritized battery life over absolute responsiveness. They have made significant improvements in responsiveness over the years. Do you remember how slow screen refreshes were on the original Kindle? It's just that that is not a high priority for their main use case of linear reading. If you want to use it for reading PDF reference books, you probably should look to one of the eink Android tablets that are more general purpose devices and may have a faster refresh rate. reply jsheard 12 hours agorootparentThere's also an inverse relationship between response time and image quality with e-ink, speeding up the display comes at the cost of more smearing/ghosting. There's often a software option to tweak that balance one way or the other depending on your preference. reply andrewmutz 6 hours agorootparentprev> A lot of time was spent integrating social features that no one uses. That time could have been spent on quality & latency. Goodreads integration is the best! I track books I want to read with it and then when I’m ready for a new book use the integration to download a sample on the spot reply tonymet 4 hours agorootparentthat's good maybe I should give it another review. It seemed like a neglected part of the OS reply poulpy123 29 minutes agoparentprev> I can't tell if it's a hardware or software issue. it's neither, it's a physics issue. Movie electrically charged particles isn't going as fast as making current flow flow through semi conductors reply green-salt 12 hours agoparentprevIt becomes a much more stable device when you jailbreak it and put something like KOReader. You can put books on it with Calibre or just SFTP afterwards. reply t0bia_s 3 hours agoparentprevI recently bough Onyx Boox tablet. After degoogling device I'm really enjoy possibilities of android system more then closed one like Kindle or Kobo or Pocketbook. Main purpose is reading a lot of sheet music documents, making annotations and share with choir/orchestra. I discovered that Calibre is excellent tool for organisation of large database of sheet music documents. What surprised me most is how responsive drawing/writing by stylus is. Weird part is that apps need optimalisaton for fast responsivness which some apps have and some don't. Which mean eink technology capabilities are limited by SW quite significantly. reply seanp2k2 13 hours agoparentprevWhat are you trying to do with it that you’re concluding it’s a “bad product” due to the slow refresh times? Kindles have always been the benchmark ebook reader and the most common piece of e-ink technology that you can actually buy. Hardly a “bad product” in any dimension that matters in business terms. reply tonymet 13 hours agorootparentSuccessful doesn't necessarily mean good. the UI is slow. It crashes with large books. The hardware is seemingly under-powered. The OS degrades in usability over time. Search indexing is poor. The lack of responsiveness makes the keyboard unusable. I've heard similar pitfalls about Kindle Scribe, the write-able Kindle. reply pnw 12 hours agorootparentI use a Kindle Scribe almost every day, have read dozens and dozens of books and documents on it. Maybe we have different expectations but I love mine and take it everywhere. It's never crashed. When I am trying to focus on reading a book, I appreciate that the Kindle doesn't have too many bells and whistles. I don't want notifications popping off and the distraction of fast Internet access. reply Qwertious 7 hours agorootparentThe Kindle Scribe is way more expensive than normal kindles (and has to handle the more computationally demanding task of vector graphics), so it would have a faster processor, and would be able to handle ebooks way easier than a cheap Kindle. Not saying you're wrong, just that your anecdote doesn't mean much about $100/$50 Kindles. reply darby_eight 13 hours agorootparentprevThey've been the benchmark for amazon kindle books. They suck for pdfs or anything with graphics. reply tonymet 12 hours agorootparentAgain, you're confusing market success with quality. Many beautiful products fail and many awful products succeed. I'm talking about aesthetics. In this case, elegance, utility, responsiveness, durability, efficiency . reply Tagbert 12 hours agorootparentprevyou are confusing your needs with the use case of the Kindle which is heavily focused on linear reading of text, mostly fiction. Graphics and PDFs are much lower on the priority scale. reply t-3 11 hours agorootparentPDFs and comics are not a small use case at all - the push to larger screens is mostly driven by people who want to read scientific papers, business documents, etc which come in pdf form, or manga and other graphical works (the drive for color ereaders seems to come almost entirely from this segment). The smaller \"ebook only\" readers are much cheaper and marketed less aggressively. reply darby_eight 12 hours agorootparentprev> Graphics and PDFs are much lower on the priority scale. Unless of course you read books that have graphics or come in pdf form. reply kimixa 3 hours agorootparentprevI think e-readers are pretty much a solved problem for books that consist of solely text - it's got to the point where they only differentiate themselves on things more related to personal taste than any technical merit. And price, of course. I consider myself a heavy user of ereaders, probably averaging over an hour a day, and have had a few in my time (Kindle paperwhite, Kindle Voyage, Kobo Aura H2O, latest the Kindle Oasis - which I got in Jan 2019). The only real time I felt like I got an \"upgrade\" was the slightly denser screen on the voyage, then the slightly larger screen on the Aura. The Oasis feels functionally the same for my use case (again, 99.9% text, no images or diagrams or animations, so can't compare them). But really the only reason I \"upgraded\" after the voyage is leaving it on a train or got damaged in my luggage one time. And what can they really improve on it? There are denser screens - but they don't make the font size I use actually look any better. There are larger screens, but it's already about the limit of what I want for portability, and much larger would be harder to read not easier. There are faster refreshing screens, but I've never even noticed the page refresh speed, as it's lost in the time it takes to realign my eyes to the top of the page anyway. So what's left? Looks? \"Premium\" materials? Thinness? The oasis is already at the point where any thinner or bezels any smaller and you couldn't hold it. I don't read comics on it, so don't really need a higher resolution, or colour panel (though can see the advantage if you did). Same with flash size - every book I've ever read on it (nearly 3000) is still on my 8gb model, and it's not even at 50% capacity. And now my Oasis battery is starting to fade - it doesn't more than a few days anymore - and somehow it feels weird considering buying the same thing again - or pry open the back off and try to replace it - though who knows what that would do to it's water resistance. I guess the newer models have an amber backlight option, which would be nice, but still doesn't feel like an upgrade to something I purchased over 5 years ago now? Or even more as I only got that because the (functionally equivalent) predecessor got crushed on a plane. So I think my point is that for the \"Reading Books\" use case, a kindle has already reached the maxima. Any \"improvement\" would almost be a waste - why improve refresh times when it won't actually affect the user experience one bit? Is there something I'm missing? Is this just a local maxima where you'd look back on and feel stupid for not seeing the \"obvious\" improvement path? Or is ereader \"development\" just a waste now as it's now just a commodity? reply Qwertious 2 hours agorootparent>Is there something I'm missing? Is this just a local maxima where you'd look back on and feel stupid for not seeing the \"obvious\" improvement path? A handle. My hand is not L-shaped. Make it hand-mirror shaped and shift as much weight as possible into the handle. Preferably a fold-down handle. A folding screen. Or rather, two screens with a hinge, no need for fancy flexible screen tech. It doesn't matter if there's a visible bezel between the screens, you're only displaying text. Ideally, you'd cram it just small enough that it can fit in your pocket when folded. Also, once you have a handle you don't need bezels. reply kimixa 2 hours agorootparentA handle is an interesting one - I'd be a bit worried by having that you'd make it so you can't hold it any other way. As I read I naturally reposition my hands, sometimes both on the device on each side, sometimes one gripping the side. Sometimes my thumbs supporting it from the bottom, sometimes the thumb over the top corner. It's light enough you don't really need that much \"grip\". Sometimes not really gripped at all, but leant against sometime, like my hand or the back of a seat tray table on a train. I do the same thing with paperbacks. I can't imagine holding a hand mirror for any length of time comfortably - especially if there's only a single orientation you can grip it with the screen upright - but that may be the weight distribution as you mentioned. Folding would be useful for putting it in a bag or fitting it somewhere, which hasn't really been a big issue for me. A flat profile is already fine when putting it in a laptop bag - it's thin enough it just slides down the side of the pocket. I wouldn't be surprised if decreasing the total volume is pretty much at it's limit already with being able to be held - but maybe putting that is a more square package when stored would be easier in some cases? reply ByThyGrace 13 hours agoparentprevE-ink devices have improved a lot over the last 10 years, across the board: refresh rate, latency, computing power, responsiveness, you name it. reply tonymet 13 hours agorootparentIndeed, and though Kindle has improved, it hasn't improved by much. I've owned 4 generations and they are all a bit better, especially when new. It's the same complaint people make about iOS devices degrading to force upgrades. I don't think it's deliberate but I do think it's deliberately neglected. reply vbezhenar 12 hours agoparentprevI used few ebooks and Kindle is the only one that actually works as expected. Some ebooks I used drained battery in few days, not delivering promise of long life. Some ebooks were just crap and broke after few months. Kindle works few weeks from one charge for my use (1-2 hours of reading per day), it's water-proof so I can read my books while taking a bath (priceless). I never had any particular issues with it. Its UI seems oriented to promote Amazon Store and I never used it, sending books over e-mail and deleting after read, that's OK with me. I'd prefer for its library to have folders and I'd prefer for it to work as USB stick like other ebooks do, so I can connect it to PC and organize things inside as I want, but those are not necessary. So may be Kindle is bad, but rest are worse, I don't know a single ebook brand of Amazon scale. They all seem to be Chinese no-names which come and go without investments to quality and reputation. reply tonymet 12 hours agorootparentI agree with all of this, and I've noticed as much with the other readers. Some users promote Kobo reader as a quality alternative, but I haven't tried it. reply lidavidm 11 hours agorootparentKobo works quite well: you can set it up without an account (needs a bit of manual fiddling) if you really want, and either way after that you can just plug it in and drag-and-drop epubs to the device. Battery life, responsiveness, etc. are all fine to me (the older devices actually did a bit better IMO and it mostly only gets bogged down for comics, regular books are fine) reply callalex 8 hours agorootparentprev>I'd prefer for it to work as USB stick like other ebooks do Have you tried Calibre? https://calibre-ebook.com/ reply abdullahkhalids 11 hours agorootparentprevAfter registration, I have never connected my Kindle to the internet - 6-7 years now. This has prevented updates that degrade performance, and Amazon getting my usage data. I just copy over ebooks using usb, optionally after converting to mobi using Calibre. Works perfectly. reply hex4def6 11 hours agorootparentprevYeah. I like tools that do one thing well. The Kindle has hit that spot for a long time. There were incremental improvements (faster processor, 3G/4G, front light, higher DPI / contrast, etc), but it's surprising how similar a 2010 kindle is to a 2024 one. reply graypegg 12 hours agoparentprevI use a pocketbook right now, and was using quite a few kobos over the years prior. At least with those, I've noticed that we've kinda reached some ceiling for responsiveness, and I think it's the software/computing hardware not the screen hardware causing it. Stuff like page turns can be quite fast, but closing out of a book and opening another really feels like you're straining the poor thing. Pocketbook particularly takes ages to reflow text if you rotate it. I think it's reflowing the entire ebook to get page numbers + chapter positions? Very annoying if you forget to turn off the accelerometer. reply Liskni_si 9 hours agorootparentTry using KOReader instead of the built-in reader software. It's a bit faster (doesn't wait to reflow the entire book if you change fonts, margins, orientation, whatever), and also it's easier to achieve consistent rendering of books (fonts, spacing, etc.) as it supports overriding the stylesheet, and just generally handles HTML/CSS better. Oh, and speaking of responsiveness, I found that it depends on temperature a lot. E-ink apparently has a sweet spot at room temperature, but when I use the device outdoors, in either sub 10 ˚C temperatures or over 30 ˚C the screen changes noticeably slower. reply karunamurti 6 hours agoparentprevI think it's always e-ink, their patents, their evergreening patents, and their lawyers. reply pquki4 12 hours agoparentprevI don't know which kindle you have, but my Scribe is noticably faster than the Oasis 2nd gen from 2017. Almost makes me want to replace the Oasis with the latest Paperwhite. reply tonymet 12 hours agorootparentmy paperwhite 2022(ish) seemed faster than the previous one (2017?) but now it's nearly as bad reply dools 12 hours agoparentprev> Does anyone know why the Kindle is such a bad product? Because it’s made by Amazon reply danielscrubs 7 hours agorootparentIsn’t it funny how the CEO is always saying that user experience is their primary focus, and you get such a mediocre experience? reply mcast 12 hours agorootparentprevTo be fair, the Kindle is primarily used for reading books and doesn't require a fast refresh rate. It also lasts for weeks (months?) without charging. reply tonymet 12 hours agorootparentIt's not about refresh rate it's about responsiveness. As close to 0ms as possible. On kindle there's a frustrating amount of input lag reply TillE 12 hours agorootparentThe response you're waiting for is a refresh of the screen. The Kindle excels at being a low-cost device for reading novels, or linear non-fiction with no graphics etc. For anything fancier, it's simply not built for that. reply kjkjadksj 13 hours agoparentprevKindle is now the kleenex of ebook readers, certainly not a bad product by any stretch. reply tonymet 13 hours agorootparentwhat qualities are good? I admit they have a good ebook selection , networking features are good, and the price point is good when on discount. But the software is awful and the application of the eink hardware is terrible too. reply sokoloff 13 hours agorootparentIt’s a better book than actual books for 98+% of books I’ve read on it. Taking as many books as I want onto an airplane for a business trip is great. My kids and wife read theirs literally daily. I’d have to look up how old they are, but the newest one is 2 years old and I think mine is around 10 (it’s a first gen paper white). The e-ink display updates fast enough to not distract me from reading. The battery lasts multiple business trips, even on my very old unit. I’m surprised how negatively you feel about it, given my and my family’s very good experiences. reply amenhotep 11 hours agorootparentYou really threw me for a second with talking about your kids reading and then saying \"I'd have to look up how old they are, but the newest one is two years old\"! reply tonymet 12 hours agorootparentprevThe ability to take a library with me is great, obviously. I also primarily read on kindle. The complaints I have are all qualities within their control that seem to be lagging due to neglect (responsiveness, UI, stability, degrading performance over time, keyboard, search) Now ignore the library aspect and compare reading a single book to reading a single e-book and the gaps will be more visible. reply sokoloff 11 hours agorootparentI was recently given a physical book as part of a work-related book club and took it along with my Kindle on a family vacation in April, so I ended up doing a direct comparison. The physical book was not better than a Kindle version would have been. It was bigger, heavier, was only one book, was harder to read in the evening, harder to highlight passages and find them later. I think the only thing is the around double resolution of the print version, where the Kindle's ~300dpi is entirely passable, and the fact that the book's \"battery life\" is >100 years while the Kindle needs charging once per month. Still a big Kindle win. reply kjkjadksj 12 hours agorootparentprevDepends on your perspective. If you sell kindles you probably are pretty pleased with almost 100 million units sold. Not a lot of products get those numbers. It sold quite successfully I'd say. reply paulcole 6 hours agorootparentprevIncredible book selection. Ease of buying books. Great battery life. Great screen and light. Great reading experience. I read 100+ books a year on my Kindles. reply tonymet 4 hours agorootparentYes the cloud sync and the marketplace are great , with some reservations on licensing and ownership. The device itself is neglected, underpowered, unresponsive, unreliable , sluggish with a terrible UI. Decompose the concept. reply ElijahLynn 12 hours agoprevSo so grateful for the open sourcing of all the knowledge about Eink in your brain zephray!!! So much great information in your readme about Eink. I've already read a decent bit of it, and am going to reference it for the years to come! reply feverzsj 2 hours agoprevYes, you can make eInk refresh faster, but with ghosting and limited grayscale level. eInk hasn't advanced much since its advent, and no sign it will, as it's controlled by one company. reply girvo 2 hours agoparent> eInk hasn't advanced much since its advent Yes, it has. Even purely from the consumer facing side: the latest panels are far better in grayscale depth, sharpness and refresh rate (and their controller still kind of suck). Not to mention colour e-ink panels in consumer products seeing rapid improvements gen over gen. > and no sign it will, as it's controlled by one company. That's less true today, and there are multiple implementations of this idea that are being commercialised. Though, sadly, e-ink the company bought one of them... reply LeoPanthera 13 hours agoprevI want to make a Compact Mac clone with an eink display like this. How wonderful would that look. reply JKCalhoun 12 hours agoparentIt can look good [1] but I haven't yet seen the refresh rate of eInk that I thought could handle a moving cursor. Maybe with the right driver you can these days. [1] https://engineersneedart.com/systemsix/systemsix.html reply jwells89 6 hours agoparentprevA machine with the 12” Macbook form factor and an e-ink display with optional orange glowlight for night running something akin to Classic Mac OS would make for a very zen, highly portable writing machine. Would probably get battery life measured in weeks, too. reply afandian 13 hours agoparentprevI’d settle for an eink Newton. reply WillAdams 12 hours agorootparentI'd be glad of a contemporary device w/ a b/w LED --- still saddened that there wasn't a replacement for the Asus Eee Note EA800 --- and I'm still annoyed that Apple has yet to make a device to replace my Newton (at a minimum, I'd want Apple Pencil support on an iPhone (or iPod Touch if they'd bring that back) or Mini iPad), but using something other than an LCD for daylight viewability would be something I'd be glad of (trying to out-bright the sun on a battery-powered device is just as stupid as it sounds to my mind). reply tonymet 12 hours agoparentprevor reboot Palm pilot reply Max-q 14 hours agoprevThis is an amazing piece of work and the documentation is a great introduction to EPD. reply karmakaze 9 hours agoprev[I know nothing for real in this area.] The Limitations section was interesting to read about. As I was thinking about it, I began to wonder if 1 memory cell per pixel could be used in an analog fashion (if it could mimic the panel response) with a high speed ADC. Seems more complicated and less accurate that what's described, and perhaps not cheaper if possible at all. reply rkagerer 11 hours agoprevWhat a great primer in the readme! reply OptCohTomo 11 hours agoprev\"Optical teardown of a Kindle Paperwhite display by OCT\": https://arxiv.org/abs/1605.05174 OCT = Optical Coherence Tomography This paper shows what is going on inside the display. reply localfirst 14 hours agoprevim not familiar with this industry but how far are we from magazine quality look and feel using eInk? like there is a scene from an old 80s sci-fi movie dude whips out a game magazine and the screenshot of games are fully animated videos... ive been waiting 30 years now for this tech reply PaulStatezny 14 hours agoparentGiven the couple years I spent using a Dasung Paperlike e-ink monitor, I think we're pretty far off from that. There seems to be a direct tradeoff between contrast/image-brilliance and latency/frames-per-second. Many monitors can be switched along that spectrum, but the current tech doesn't seem to be able to deliver both simultaneously at a reasonable price. reply ant6n 2 hours agorootparentAs a not so proud owner of a Dasung color I have to agree. The product is absolutely atrocious. It has like 8 colors, not thousands. The contrast is shit, „white“ is brown, black is … some darker brown. It’s reflective. It heavily relies on unstable dithering (so moving the mouse will jitter around a large area of nearby pixels), which also makes everything unreadable. The color processing is shit, it won’t even try to approximate displayed colors with the ones it can display. You have to fight with the settings to just get something that is vaguely usable, and set up the OS with custom colors and an accessible theme. But even then it’s so much more straining to use than a normal monitor, because of the reflections and shit contrast. I think you could probably get something more useful by building a 28“ wall paper of game boy colors. Worst purchase ever. reply Rebelgecko 13 hours agoparentprevA lot of the components you'd need for that sorta kinda exist: Flexible e-ink displays Color eink displays High-ish refresh rate eink displays The problem is AFAICT there's no device that covers all 3. Color displays have pretty big tradeoffs in terms of resolution/contrast/latency. However they're still way ahead of where they were 5-10 years ago. reply sedatk 14 hours agoparentprevI believe we need to wait for e-ink patents to expire. reply Qwertious 7 hours agorootparentThe core patents already have expired. They have competition, it's called Reinkstone (and their DES/cofferdam tech). The reason e-inks are so expensive and progressing so slowly is that it's a niche product which doesn't drive much R&D spending (compared to LCDs, which sell billions per quarter and you're reading this comment on). reply kjkjadksj 13 hours agorootparentprevI'm surprised the eink patent holders are seemingly content to lose their patent in time instead of licensing it and making real money reply megous 12 hours agoprevOn the other end of the spectrum, there are production devices driving eInk displays using a regular LCD controllers. :))) Anyway, I appretiate the waveform format documentation and tools. Might kick me back to working on my Pocketbook display driver. reply carterschonwald 14 hours agoprevThis is great! reply thetinymite 13 hours agoprev [–] I think this is the original repo: https://gitlab.com/zephray/glider based on this tweet: https://twitter.com/zephray_wenting/status/17901730074884506... reply SushiHippie 13 hours agoparent [–] The GitHub repository description also says: > Open-source E-ink monitor. Mirror of https://gitlab.com/zephray/glider reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The \"Glider\" open-source Eink monitor prioritizes low latency for electrophoretic display panels like E Ink, SiPix, and DES.",
      "The document compares EPDs to LCDs, discussing Eink technology, controller options, advantages, and disadvantages, emphasizing the importance of choosing the right tech for specific projects.",
      "It covers driving E-ink screens, FPGA-based controllers, SoC options, and display panel data transmission, providing detailed specifications for various Eink display models, resolutions, and technologies."
    ],
    "commentSummary": [
      "The discussion revolves around e-ink technology, notably the Glider open-source eInk monitor, showcasing user-reported benefits like reduced eye strain and fatigue, especially when using custom reading glasses.",
      "It explores the limitations, future possibilities, and compares e-ink displays like Kindles with physical books while touching on alternative e-ink devices and the influence of expired patents on the tech.",
      "Emphasis is placed on the significance of selecting appropriate glasses for computer usage in this technological landscape."
    ],
    "points": 567,
    "commentCount": 95,
    "retryCount": 0,
    "time": 1715710741
  },
  {
    "id": 40358071,
    "title": "Gemini Flash: Breakthrough AI Model for Speed and Efficiency",
    "originLink": "https://deepmind.google/technologies/gemini/flash/",
    "originBody": "Gemini Flash Our lightweight model, optimized for speed and efficiency Build with Gemini Lightweight, fast and cost-efficient while featuring multimodal reasoning and a breakthrough long context window of up to one million tokens. Lightweight, fast and cost-efficient while featuring multimodal reasoning and a breakthrough long context window of up to one million tokens. Performance in a flash Designed to be fast and efficient to serve at scale. Built for speed Sub-second average first-token latency for the vast majority of developer and enterprise use cases. Quality at lower cost On most common tasks, 1.5 Flash achieves comparable quality to larger models, at a fraction of the cost. Long-context understanding Process hours of video and audio, and hundreds of thousands of words or lines of code. Pause Play Longer context Flash has a one-million-token context window by default, which means you can process one hour of video, 11 hours of audio, codebases with more than 30,000 lines of code, or over 700,000 words. Build with 1.5 Flash Relentless innovation Our research team is continually exploring new ideas at the frontier of AI, building innovative products that show consistent progress on a range of benchmarks. Our newest model is Gemini 1.5 Flash. Capability Benchmark Description Gemini 1.0 Pro Gemini 1.0 Ultra Gemini 1.5 Pro (Feb 2024) Gemini 1.5 Flash General MMLU Representation of questions in 57 subjects (incl. STEM, humanities, and others) General MMLU Representation of questions in 57 subjects (incl. STEM, humanities, and others) Gemini 1.0 Pro 71.8% Gemini 1.0 Ultra 83.7% Gemini 1.5 Pro (Feb 2024) 81.9% Gemini 1.5 Flash 78.9% Code Natural2Code Python code generation. Held out dataset HumanEval-like, not leaked on the web Code Natural2Code Python code generation. Held out dataset HumanEval-like, not leaked on the web Gemini 1.0 Pro 69.6% Gemini 1.0 Ultra 74.9% Gemini 1.5 Pro (Feb 2024) 77.7% Gemini 1.5 Flash 77.2% Math MATH Challenging math problems (incl. algebra, geometry, pre-calculus, and others) Math MATH Challenging math problems (incl. algebra, geometry, pre-calculus, and others) Gemini 1.0 Pro 32.6% Gemini 1.0 Ultra 53.2% Gemini 1.5 Pro (Feb 2024) 58.5% Gemini 1.5 Flash 54.9% Reasoning GPQA (main) Challenging dataset of questions written by domain experts in biology, physics, and chemistry Reasoning GPQA (main) Challenging dataset of questions written by domain experts in biology, physics, and chemistry Gemini 1.0 Pro 27.9% Gemini 1.0 Ultra 35.7% Gemini 1.5 Pro (Feb 2024) 41.5% Gemini 1.5 Flash 39.5% Reasoning Big-Bench Hard Diverse set of challenging tasks requiring multi-step reasoning Big-Bench Hard Diverse set of challenging tasks requiring multi-step reasoning Gemini 1.0 Pro 75.0% Gemini 1.0 Ultra 83.6% Gemini 1.5 Pro (Feb 2024) 84.0% Gemini 1.5 Flash 85.5% Multilingual WMT23 Language translation Multilingual WMT23 Language translation Gemini 1.0 Pro 71.7 Gemini 1.0 Ultra 74.4 Gemini 1.5 Pro (Feb 2024) 75.2 Gemini 1.5 Flash 74.1 Image MMMU Multi-discipline college-level reasoning problems Image MMMU Multi-discipline college-level reasoning problems Gemini 1.0 Pro 47.9% Gemini 1.0 Ultra 59.4% Gemini 1.5 Pro (Feb 2024) 58.5% Gemini 1.5 Flash 56.1% Image MathVista Multi-discipline college-level reasoning problems MathVista Mathematical reasoning in visual contexts Gemini 1.0 Pro 45.2% Gemini 1.0 Ultra 53.0% Gemini 1.5 Pro (Feb 2024) 52.1% Gemini 1.5 Flash 54.3% Audio FLEURS (55 languages) Automatic speech recognition (based on word error rate, lower is better) Audio FLEURS (55 languages) Automatic speech recognition (based on word error rate, lower is better) Gemini 1.0 Pro 6.4 Gemini 1.0 Ultra 6.0 Gemini 1.5 Pro (Feb 2024) 6.6 Gemini 1.5 Flash 9.8 Video EgoSchema Video question answering Video EgoSchema Video question answering Gemini 1.0 Pro 55.7% Gemini 1.0 Ultra 61.5% Gemini 1.5 Pro (Feb 2024) 63.2% Gemini 1.5 Flash 63.5% For developers Build with Gemini Integrate Gemini models into your applications with Google AI Studio and Google Cloud Vertex AI. Build Google AI Studio An easy way to develop model prompts and build quickly with the Gemini API. Build Vertex AI Purpose-built tools for data scientists and machine learning engineers. Get the latest updates Sign up for news on the latest innovations from Google DeepMind. Email address Please enter a valid email (e.g., \"name@example.com\") I accept Google's Terms and Conditions and acknowledge that my information will be used in accordance with Google's Privacy Policy. Sign up Explore our other teams and product areas Google AI Google Research Google Cloud LABS.GOOGLE",
    "commentLink": "https://news.ycombinator.com/item?id=40358071",
    "commentBody": "Gemini Flash (deepmind.google)390 points by meetpateltech 16 hours agohidepastfavorite110 comments simonw 12 hours agoI upgraded my llm-gemini plugin to provide CLI access to Gemini Flash: pipx install llm # or brew install llm llm install llm-gemini --upgrade llm keys set gemini # paste API key here llm -m gemini-1.5-flash-latest 'a short poem about otters' https://github.com/simonw/llm-gemini/releases/tag/0.1a4 reply xianshou 15 hours agoprevLooking at MMLU and other benchmarks, this essentially means sub-second first-token latency with Llama 3 70B quality (but not GPT-4 / Opus), native multimodality, and 1M context. Not bad compared to rolling your own, but among frontier models the main competitive differentiator was native multimodality. With the release of GPT-4o I'm not clear on why an organization not bound to GCP would pick Gemini. 128k context (4o) is fine unless you're processing whole books/movies at once. Is anyone doing this at scale in a way that can't be filtered down from 1M to 100k? reply Workaccount2 15 hours agoparentWith 1M tokens you can dump 2000 pages of documents into the context windows before starting a chat. Gemini's strength isn't in being able to answer logic puzzles, it's strength is in its context length. Studying for an exam? Just put the entire textbook in the chat. Need to use a dead language for an old test system with no information on the internet? Drop the 1300 page reference manual in and ask away. reply ianbicking 14 hours agorootparentHow much do those input tokens cost? According to https://ai.google.dev/pricing it's $0.70/million input tokens (for a long context). That will be per-exchange, so every little back and forth will cost around that much (if you're using a substantial portion of the context window). And while I haven't tested Gemini, most LLMs get increasingly wonky as the context goes up, more likely to fixate, more likely to forget instructions. That big context window could definitely be great for certain tasks (especially information extraction), but it doesn't feel like a generally useful feature. reply mcbuilder 14 hours agorootparentThat per exchange context cost is what really puts me off using cloud LLM for anything serious. I know batching and everything is needed in the data center, and important for keeping around KVQ cache, you basically need to fully take over machine to get an interactive session to get the context costs to scale with sequence length. So it's useful, but more in the case of a local LLaMA type situation if you want a conversation. reply falcor84 12 hours agorootparentI wonder if we could implement the equivalent of a JIT compilation, whereby context sequences which get repeatedly reused would be used for an online fine-tuning. reply sp332 7 hours agorootparentNo, but you can just cache the state after processing the prompt. https://github.com/ggerganov/llama.cpp/tree/master/examples/... reply HumanOstrich 7 hours agorootparentprevnext [2 more] [flagged] nostrebored 7 hours agorootparentThey are asking if you can take the context being passed per interaction and train it into a session in real time (via an online algorithm). Essentially bake the context passed in to the attention layer so that you can pass only the relevant chat context. Your post wasn’t a particularly charitable interpretation. reply lxgr 14 hours agorootparentprevIs there a way to amortize that cost over several queries, i.e. \"pre-bake\" a document into a context persisted in some form to allow cheaper follow-up queries about it? reply simonw 13 hours agorootparentThey announced that today, calling it \"context caching\" - but it looks like it's only going to be available for Gemini Pro 1.5, not for Gemini Flash. It reduces prompt costs by half for those shared prefix tokens, but you have to pay $4.50/million tokens/hour to keep that cache warm - so probably not a useful optimization for most lower traffic applications. https://ai.google.dev/gemini-api/docs/caching reply dragonwriter 12 hours agorootparent> It reduces prompt costs by half for those shared prefix tokens, but you have to pay $4.50/million tokens/hour to keep that cache warm - so probably not a useful optimization for most lower traffic applications That's on a model with $3.5/1M input token cost, so half price on cached prefix tokens for $4.5/1M/hour breaks even at a little over 2.5 requests/hour using the cached prefix. reply inlined 14 hours agorootparentprevThough I'm not familiar with the specifics, they announced \"context caching\" reply gcanyon 11 hours agorootparentprevDepending on the output window limit, the first query could be something like: \"Summarize this down to its essential details\" -- then use that to feed future queries. Tediously, it would be possible to do this chapter by chapter in order to exceed the output limit building something for future inputs. Of course, the summary might not fulfill the same functionality as the original source document. YMMV reply bredren 13 hours agorootparentprevCan anyone speculate on how G arrived at this price, and perhaps how it contrasts with how OAI arrived at its updated pricing? (realizing it can't be held up directly to GPT x at the moment) reply tk90 12 hours agorootparentprevIsn't there retrieval degradation with such a large context size? I would still think that a RAG system on 128K is still better than No Rag + 1M context window, no? (assuming text only) reply afro88 27 minutes agorootparentNot sure why you've been downvoted. Needle in a haystack testing exists for a reason reply tulip4attoo 14 hours ago [flagged]rootparentprevYou don't really use it, right? There's no way to debug if you're doing it like this. Also, the accuracy isn't high, and it can't answer complicated questions, making it quite useless for the cost. reply dang 14 hours agorootparentPlease make your substantive points without crossing into personal attack. Your comment would be fine without the first sentence. https://news.ycombinator.com/newsguidelines.html reply leetharris 13 hours agoparentprevThere's no way it's Llama 3 70b quality. I've been trying to work Gemini 1.5 Pro into our workstream for all kinds of stuff and it is so bad. Unbelievable amount of hallucinations, especially when you introduce video or audio. I'm not sure I can think of a single use case where a high hallucination tiny multimodal model is practical in most businesses. Without reliability it's just a toy. reply dibujaron 12 hours agorootparentSeconding this. Gemini 1.5 is comically bad at basic tasks that GPT4 breezes through, not to mention GPT4o. reply dragonwriter 14 hours agoparentprev> With the release of GPT-4o I'm not clear on why an organization not bound to GCP would pick Gemini. Price for anything, particularly multimodal tasks that with OpenAI GPT-4o is the cheapest model, that doesn't need GPT-4 quality. GPT-3.5-Turbo — which itself is 1/10 the cost of GPT-4o, is $0.5/1M tokens on input, $1.50/1M on output, with a 16K context window. Gemini 1.5 Flash, for prompts up to 128K, is $0.35/1M tokens on input, and $0.53/1M tokens on output. For tasks that require multimodality but not GPT-4 smarts (which I think includes a lot of document-processing tasks, for which GPT-4 with Vision and now GPT-4 are magical but pricy), Gemini Flash looks like close to a 95% price cut. reply thefourthchime 14 hours agoparentprevI tried to use the 1M tokens with Gemini a couple of months ago. It either crashed or responded ___very__ slowly and then crashed. I tried a half dozen times and gave up, I hope this one is faster and more stable. reply killerstorm 12 hours agoparentprevI guess it depends on what you want to do. E.g. I want to send an entire code base in a context. It might not fit into 128k. Filtering down is a complex task by itself. It's much easier to call a single API. Regarding quality of responses, I've seen both disappointing and brilliant responses from Gemini. Do maybe worth trying. But it will probably take several iterations until it can be relied upon. reply mupuff1234 14 hours agoparentprevI think that's a bit like asking why would someone need a 1gb Gmail when 50mb yahoo account is clearly enough. It means you can dump context without thinking about it twice and without needing to hack some solutions to deal with context overflow etc. And given that most use cases most likely deal with text and not multimodal the advantage seems pretty clear imo. reply tedsanders 13 hours agorootparentLong context is a little bit different than extra email storage. Having 1 gb of storage instead of 50 mb has essentially no downside to the user experience. But submitting 1M input tokens instead of 100k input tokens: - Causes your costs to go up ~10x - Causes your latency to go up ~10x (or between 1x and 10x) - Can result in worse answers (especially if the model gets distracted by irrelevant info) So longer context is great, yes, but it's not a no-brainer like more email storage. It brings costs. And whether those costs are worth it depends on what you're doing. reply treprinum 11 hours agoparentprevGPT-3.5 has 0.5s average first-token latency and Claude3 Haiku 0.4s. reply chimney 12 hours agoparentprevPrice. reply causal 15 hours agoprev1M token context by default is the big feature here IMO, but we need better benchmarks to measure what that really means. My intuition is that as contexts get longer we start hitting the limits of how much comprehension can be embedded in a single point of vector space, and will need better architectures for selecting the relevant portions of the context. reply dragonwriter 14 hours agoparent> 1M token context by default is the big feature here IMO, but we need better benchmarks to measure what that really means. Multimodality in a model That's between 4-7% the cost per token of OpenAI’s cheapest multimodal model is an important feature when you are talking about production use and not just economically unsustainable demos. reply leetharris 13 hours agorootparentThe problem is that even 1.5 Pro seems completely useless for long context multimodal stuff. I have tried it for so many use cases in video / audio and it hallucinates an unbelievable amount. More than any other model I've ever used. So if 1.5 Pro can't even handle simple tasks without hallucination, I imagine this tiny model is even more useless. reply refulgentis 14 hours agorootparentprevIn preview, can't be used in production, they already rug-pulled people building on Gemini w/r/t cost and RPM, and they're pointedly not putting any RPM or cost on the page. (seriously, try finding info on cost, RPM, or release right now, you're linked in circles.) Agree on OpenAI multimodal but it's sort of a stilted example itself, it's because OpenAI has a hole in its lineup - ex. Claude Haiku is multimodal, faster, and significantly cheaper than GPT 3.5. reply dragonwriter 14 hours agorootparent> they're pointedly not putting any RPM or cost on the page 360 RPM base limit, pricing is posted. > seriously, try finding info on cost, RPM, or release right now, I wasn't making up numbers, its on their Gemini API pricing page: https://ai.google.dev/pricing reply refulgentis 14 hours agorootparentNice, thanks (btw, I didn't think you were making it up, it was in the keynote!) reply causal 14 hours agorootparentprev+1 on Haiku being oft overlooked. reply verdverm 14 hours agorootparentShows the power of the brand and the limit of names consumers will recall long term \"Who are the biggest soda or potato chip makers?\" reply refulgentis 14 hours agorootparentnext [9 more] [flagged] memothon 14 hours agorootparentCan you explain this comment further? I don't really understand your point here. reply refulgentis 10 hours agorootparentGiven the one comment, I wouldn't draw too many conclusions re: Anthropic.* SNR ratio on AI everywhere is low-ish, and optimizing for a low cost multimodal model is a business-directed need that's rather niche currently. * I was very pleasantly surprised to see it ranking as high as #4, and its still top #50, in App Store. reply verdverm 14 hours agorootparentprev1. Not an AI specific comment series, it applies generally 2. Comments like yours are against the guidelines of HN 3. You don't know what I do and are incorrect in your assessment of my work and experience (another common HN mishap) --- (re: #2/3) Please make your substantive points without crossing into personal attack. https://news.ycombinator.com/newsguidelines.html reply anoncareer0212 13 hours agorootparentWhat do you mean? A) This fills a gaping hole for cheap multimodal models, OpenAI doesn't have one B) Anthropic's Haiku is a good choice. You) wow A didn't know Anthropic. Goes to show power of brands, much like snack foods B) Eh I wouldn't conclude anything from A. Its one comment. some people don't know what an Anthropic is because there's high interest in AI relative to interest in AI APIs. you can expect a low SNR ratio, even on HN You) Stop personally attacking me! It's against the rules!! reply verdverm 11 hours agorootparent> You) Stop personally attacking me! It's against the rules!! Your comment history certainly shows a disregard for the HN guidelines https://news.ycombinator.com/newsguidelines.html reply refulgentis 10 hours agorootparentI don't think its helpful to broaden the aspersions: you had a couple hours to cool off, and also got signal from the community that you went overboard. It's clear I was talking to you, not about you, you can tell because it's a reply to you. I do understand it would feel awful if I replied to you, ignoring you, and instead voicing to some anon 3rd person you don't know Anthropic pricing. Someone with looking to bully could possibly use that to say you are an amateur who doesn't know what you're talking about w.r.t to AI and only knows a brand name or two. If anyone does that, let me know, I'll correct their bullying...pulls on sunglasses...with extreme prejudice. reply verdverm 9 hours agorootparent> signal from the community that you went overboard. What signal are you referring to? That you and one other person are making low brow comments that do not enhance the conversation? reply refulgentis 8 hours agorootparent> What signal are you referring to? 1. The comment where you began getting upset and attacking people is grayed out, the only one in this thread. 2. The other person straightforwardly explaining to you I was replying to you, not talking about you. (which, I do understand why you're taking personally, but humbly, it isn't a rule violation or close, or \"low-brow\", it's well within bounds of adult conversation) 3. Me agreeing the reply was a reply, I don't know you or judge you, and if anyone does, I gave a nice playful comment telling you I'd fight for your honor. I humbly suggest taking a step back from this thread: it can be incredibly frustrating when you feel you were wronged and no one is listening, I feel ya. I tried giving you an e-hug without being condescending with the sunglasses comment. We both agree its outside the bounds of HN to continue this. Yell at me on Twitter for being sneaky and lying and saying you're bad at AI and making low-brow comments, @jpohhhh. reply shoelessone 1 hour agoparentprev> My intuition is that as contexts get longer we start hitting the limits of how much comprehension can be embedded in a single point of vector space, and will need better architectures for selecting the relevant portions of the context. Is it possible to explain what this means in a way that somebody only roughly familiar with vectors and vector databases? Or recommend an article or further reading on the topic? reply WhitneyLand 8 hours agoparentprevLimitations of single point in vector space of what dimension? I’m not sure it’s public knowledge, but it’s an architecture choice. They choose how big to make the embedding dimension. My point is just that there’s no limitation in principle, it’s just a matter of how they design it and resource constraints. reply causal 6 hours agorootparentThanks for responding to that point - it's the one most on my mind. So OpenAI's large embedding model has 3072 dimensions, though in practice far fewer are probably used. Clearly you can't compress 1M tokens down to 3072. Yet those 3072 numbers are all you've got for capturing the full meaning of the previous token when predicting the next one; including all 1M tokens of modifying context. So perhaps human language is simply never complex enough to need more than 3072 numbers to represent a given train of thought, but that doesn't seem clear to me. Edit: Since Gemini is relevant here, it looks like their text embedding model is 768 dimensions. reply refulgentis 14 hours agoparentprevYeah it's not very good in practice, you can get a halfway decent demo out of it (\"look I gave it 6.5 harry potters and it made an SVG map connecting characters with annotations!!\"...some of the characters...spare annotations...cost $20). Just good enough to fool you a couple times when you try to make it work 10 times. reply nightski 13 hours agoprevA lightweight model that you can only use in the cloud? That is amusing. These tech megacorps are really intent on owning your usage of AI. But we must not let that be the future. reply kherud 13 hours agoprevNow that context length seems abundant for most tasks, I'm wondering why sub-word tokens are still used. I'm really curious how character-based LLMs would compare. With 2 M context, the compute bottleneck fades away. I'm not sure though what role the vocabulary size has. Maybe a large size is critical, since the embedding already contains a big chunk of the knowledge. On the other hand, using a character-based vocabulary would solve multiple problems, I think, like glitch tokens and possibly things like arithmetic and rhyming capabilities. Implementing sub-word tokenizers correctly and training them seems also quite complex. On a character level this should be trivial. reply AaronFriel 12 hours agoparentThe attention mechanism is vastly more efficient to train when it can attend to larger, more meaningful tokens. For inference servers, a significant amount of memory goes into the KV cache, and as you note, to build up the embedding through attention would then require correlating far more tokens, each of which is \"less meaningful\". I think we may get to this point eventually, in the limit we will want multimodal LLMs that understand images and sounds down to the pixel and frequency, and it seems like for text, too, we will eventually want that as well. reply thomasahle 11 hours agorootparentMaybe you could just use a good-old 1D-CNN for the bottom 3-4 layers. Then the model has been able to combine characters into roughly token length chunks anyway. Just make sure to have some big MLPs at the start too, to enrich the \"tokens\" with the information currently stored in the embedding tables. reply yk 12 hours agorootparentprev> a significant amount of memory goes into the KV cache Is there a good paper (or talk) how inference looks at scale? (Kinda like ELI-using-single-gpus) reply darby_eight 12 hours agoparentprev> On a character level this should be trivial. Characters are not the semantic components of words—these are syllables. Generally speaking, anyway. I've got to imagine this approach would yield higher quality results than the roman alphabet. I'm curious if this could be tested by just looking at how LLMs handle English vs Chinese. reply inbetween 12 hours agorootparentThe minimal semantic parts of words are morphemes. Syllables are phonological units (roughly: the minimal unit for rhythmic purposes such as stress, etc) reply darby_eight 11 hours agorootparentOnly in languages that have morphemes! This is hardly a universal attribute of language so much as an attribute of those that use an alphabet to encode sounds. It makes more sense to just bypass the encoding and directly consider the speech. Besides, considering morphemes as semantic often results in a completely different meaning than we actually intend. We aren't trying to train a chatbot to speak in prefixes and suffixes, we're trying to train a chatbot to speak in natural language, even if it is encoded to latin script before output. reply joaogui1 12 hours agoparentprevI would say 2 big problems are: 1. latency, which would get worse if you have to sequentially generate more output 2. These models very roughly turn tokens -> \"average meaning\" on the embedding layer, followed by attention layers that combine the meanings, and feed forward layers that match the current meaning combination to some kind of learned archetype/prototype almost. When you move from word parts to characters all of that becomes more confusing (what's the average meaning of a?) and so I don't think there are good enough techniques to learn character-based models yet reply novaRom 12 hours agoparentprevIn AI music generation we have much better results with large vocabulary sizes of 10^6 order, my uneducated guess is that's because transformers are not universal pattern recognizers, they can catch patterns on a certain granularity level only. reply zone411 8 hours agoprev15.3 On NYT Connections benchmark: GPT-4 turbo (gpt-4-0125-preview) 31.0 GPT-4o 30.7 GPT-4 turbo (gpt-4-turbo-2024-04-09) 29.7 GPT-4 turbo (gpt-4-1106-preview) 28.8 Claude 3 Opus 27.3 GPT-4 (0613) 26.1 Llama 3 Instruct 70B 24.0 Gemini Pro 1.5 19.9 Mistral Large 17.7 -----> Gemini 1.5 Flash 15.3 Mistral Medium 15.0 Gemini Pro 1.0 14.2 Llama 3 Instruct 8B 12.3 Mixtral-8x22B Instruct 12.2 reply ukuina 5 hours agoparentSo many high-performing, yet poorly-named OpenAI models in that list. reply mrcwinn 6 hours agoprevI will say Google certainly has the better branding team. I like Gemini, Gems, and so on. “ChatGPT” is quite a clunky mess. OpenAI just feels like a faceless entity. All things that could change but seems late in the game at this point. They certainly had the money to be more creative as they came to market. reply precompute 1 hour agoparent\"ChatGPT\" is like \"Google\". \"Gemini\" is never replacing that. reply ukuina 5 hours agoparentprevOpenAI desperately needs a marketing consult. \"GPT4o\"? Seriously? Even \"GPT4 Omni\" is easier in conversation, and that's what the \"o\" stands for! They severely underestimate the number of casual users they have. reply zarzavat 4 hours agorootparentOpenAI doesn’t need marketing because everybody knows who’s the best. Same reason that if I asked you what’s the best violin you would say Stradivari, even though you’ve never seen an ad for one. OpenAI could call their model the “[poo emoji] 5000” for all the difference it would make. reply cpeterso 4 hours agorootparentprevGoggle “Gemini” is a much better product name than any name OpenAI has, but the Gemini product family could use some structure: Gemini Advanced (“with Ultra 1.0”) Gemini Ultra Gemini Pro Gemini Flash Gemini Nano-1 Gemini Nano-2 reply michaelteter 5 hours agoprevIf Gemini Flash is just faster Gemini, then I would say that bad answers aren't better when delivered more quickly. I ran Gemini Pro side by side with ChatGPT 4 for a few months on practical coding, systems architecture, and occasional general questions. ChatGPT was more useful at least 80% of the time. Gemini was either wrong or laboriously meandering in reaching a useful answer that it wasn't worth using, in my experience. Faster isn't what I needed... Maybe it's also \"smarter\" (more useful) too now? reply alephxyz 12 hours agoprevNot very informative. They're selling it as the fast/cheap option but they don't benchmark inference speed or compare it with non-gemini models. According to https://ai.google.dev/pricing it's priced a bit lower than gpt3.5-turbo but no idea how it compares to it. reply quantisan 12 hours agoprevPrice (input) $0.35 / 1 million tokens (for prompts up to 128K tokens) $0.70 / 1 million tokens (for prompts longer than 128K) Price (output) $0.53 / 1 million tokens (for prompts up to 128K tokens) $1.05 / 1 million tokens (for prompts longer than 128K) --- Compared to GPT-3.5 Turbo Input US$0.50 / 1M tokens Output US$1.50 / 1M tokens reply numbers 14 hours agoprevIt's ironic that when you ask these AI chatbots what their own context size is, they don't know. ChatGPT doesn't even know about 4o existing in 4o. reply advisedwang 13 hours agoparentAsk a human how many neurons they have. Hell, over history humans haven't even consistently understood that the brain is where cognition happens. reply simonw 12 hours agoparentprevThe models didn't exist when their training data was collected. But... that's not really an excuse any more. Model vendors should understand now that the most natural thing in the world is for people to ask models directly about their own abilities and architecture. I think models should have a final layer of fine-tuning or even system prompting to help them answer these kinds of questions in a useful way. reply SoftTalker 14 hours agoparentprevDoes a monkey know that it is a monkey? reply verdverm 14 hours agorootparentI think \"yes\" is the most likely answer here animals have a lot more intelligence than they typically get attributed Tool use, names, language, social structure and behavior, even drug use has been shown across many species reply chaorace 13 hours agorootparentOkay, but the monkey doesn't know that it knows that it's a monkey. reply verdverm 11 hours agorootparentare you sure? Many animals recognize themselves and their species as separate concepts reply keefle 2 hours agorootparentHe meant something more meta I believe. Knowing you are a monkey is one thing, and knowing that you know you are a monkey is a another thing. It's about being cognisant of the fact that there is something called knowledge and you have it reply fourthark 8 hours agorootparentprevHow do you know? reply chefkd 9 hours agoprevAny links to companies working on a fully local only AI? reply nojvek 13 hours agoprevWill wait for Meta to release Flash equivalent weights. Multi-Modal modals running offline on mobile devices with millisecond latencies per token seems the future. Where is Apple in all of this. Why is Siri still so shit? reply visarga 12 hours agoparentApple made a deal with OpenAI for GPT4o, the stakes are indeed high, can't be caught with pants down. iPhone needs to remain the premium brand. reply stan_kirdey 7 hours agoprevI've been diligently trying to use Gemini 1.5 Pro, and it is not even on the level of Llama3-70B. I really hope Gemini improves, even if it gets reduced context length. reply ukuina 5 hours agoparentFAIR really swung for the fences with Llama3. It's a very impressive model, but the 8K context size is quite limiting for most use-cases. reply eru 10 hours agoprevThe website talks about a specific benchmark: > Python code generation. Held out dataset HumanEval-like, not leaked on the web What I find interesting here is that for this particular benchmark _not_ publishing the benchmark is advertised as a feature (instead of as a sign of 'trust me, bro, we have a great benchmark'), and I can understand why. Still these are strange times we live in. reply webprofusion 7 hours agoprevUh guys, yeah.. Adobe are on the phone saying something about trademark infringement, apparently Flash is something else? I don't know, I've never heard of it.. reply exodust 3 hours agoparentInterestingly, until your comment I hadn't made any connection with old Flash, even though I spent hundreds of hours making Flash games. This suggests names don't stick around for long and can be re-used. Perhaps Google could bring back \"Buzz\" and \"Wave\" since enough time has passed! reply objektif 11 hours agoprevDoes Goog have anything like openai assistant via API? If they had I would definitely give it a try. reply cynicalsecurity 15 hours agoprevFeed 1 mln tokens @ Get blocked by some silly overly sensitive \"safety\" trigger reply gpm 14 hours agoparentLast I checked you could disable the safety triggers as an API user with gemini (which doesn't alleviate your obligation to follow the TOS as to the uses of the model). reply VS1999 14 hours agorootparentI'm not working with a company that can just write in the ToS \"we can do anything we want. lol. lmao\" and expect me to follow it religiously. Corporations need less control over speech, not more. reply zxexz 2 hours agorootparentI mean, you are using a service they're providing - many would say they they're exercising their rights by gatekeeping how it's used. There are pretty good models out there you could use however you want for your own purpose, whatever it is. I occasionally fine-tune Mixtral on HN posts+comments and chat with comments. An emergent Dang actually once told me off for flame-baiting a free speech comment. reply cs702 11 hours agoprevWe're witnessing a race to the bottom on pricing as it's happening. Competition based solely or mainly on pricing is a defining characteristic of a commodity market, i.e., a market in which competing products are interchangeable, and buyers are happy to switch to the cheapest option for a given level of quality. There's an old saying that if you're selling a commodity, \"you can only be as smart as your dumbest competitor.\" If we want to be more polite, we could say instead: \"you can only price your service as high as your lowest-cost competitor.\" It seems that a lot of capital that has been \"invested\" to train AI models is, ahem, unlikely ever to be recovered. reply daghamm 11 hours agoparentIs this race to the bottom or just Googles new TPUs being extremly efficient? reply Delmololo 11 hours agoparentprevBut the race to the bottom has an opposition right? So people expect to see a return of investment which will create the bottom of pricing (at least as soon as the old money ran out) I'm also curious if AI is a good example because ai will become fundamental. This means if you don't invest you might be gone therefore it's more like a fee in case the investment would not pan out. reply __loam 11 hours agorootparentSupply and demand determines price, not the hopes and dreams of investors. reply r0m4n0 11 hours agoparentprevGoogle is building on top of and integrated with their cloud offerings. Having first party solutions like this gives big cloud customers an easy way to integrate. For Google it’s just another tool in the chest that gets sold to these big enterprises. Many go all in on all the same cloud products. Also the models are only the building blocks. Other cloud products at Google will be built with this and sold as a service Not so sure about Open AI though… reply rmbyrro 11 hours agoparentprevGoogle figured it can't beat OpenAI technically, but they sure know they can beat them financially and infrastructurally. reply tfsh 10 hours agorootparentIn technical terms they're on par. But you're correct about Google being able to bet on their decades of infrastructure reply __loam 11 hours agorootparentprevIs infrastructure and scale not an expression of technical ability? It should have been obvious that Meta and Google would bury a tiny company with less than 1000 employees given the amount of capital they can leverage for compute, talent, and data. Google literally invented GPT. reply Aloisius 11 hours agoparentprevPrice competition isn't limited to commodities. reply cs702 11 hours agorootparentI never said it was. reply Aloisius 11 hours agorootparentThen why imply that it is a commodity because they (partly) compete on price? Fungibility is the defining characteristic of commodities. While these products can be used to accomplish the same task, we're not near real fungibility yet. reply EGreg 11 hours agorootparentprevYou never said it wasn't, either :-P reply __loam 11 hours agoparentprevYou're saying the quiet part out loud here. reply refulgentis 14 hours agoprev [–] It's absolutely unconscionable that Gemini Ultra got memory-holed. I can't trust anything that Google says about benchmarks. It seemingly existed only so in December 2023, Gemini ~= GPT-4. (April 2023 version) (on paper) (\"32-shot CoT\" vs. 5-shot GPT-4) reply CSMastermind 14 hours agoparentAnyone who uses both products regularly will tell you that Gemini Advanced is far behind GPT-4 and Claude 3 Opus. Pretending that they have a model internally that's on par but they're not releasing it is a very \"my girlfriend goes to another school\" move and makes no sense if they're a business that's actually trying to compete. reply summerlight 14 hours agoparentprev [–] Gemini Ultra is 1.0 with 8k window. This is 1.5 with 1m window. Your feeling is based on incorrect assumption. reply anoncareer0212 13 hours agorootparent [–] And? You're replying to a comment that points out Gemini Ultra was never released, wasn't mentioned today, and it's the only model Google's benchmarking at GPT-4 level. They didn't say anything about feelings or context window. reply dontreact 13 hours agorootparentGemini Ultra has been available for people to try via Gemini Advanced (formerly Bard) for a few months reply cma 11 hours agorootparentIt says it may fall back to a worse model under load and there is no way to tell which you are getting. I think chatgpt has at times done something similar though. reply summerlight 13 hours agorootparentprev [–] > You're replying to a comment that points out Gemini Ultra was never released What are you even talking about? How do you know it's memory-holed if you haven't used it? The API is not GA, but the model can be used through the chatbot subscription. GP is talking about their lack of trust on Google's claim of 1M context token, not GPT-4 level reasoning. If you're expect GPT-4 level performance with cost-efficient models, that's another problem. reply refulgentis 7 hours agorootparent [–] Idk why you're so aggro, they're right, I meant the GPT-4 level reasoning reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Gemini Flash model is a lightweight, fast, and cost-efficient AI model known for its breakthrough long context window of up to one million tokens, making it ideal for handling vast amounts of data like video, audio, and codebases.",
      "It offers high performance and quality comparable to larger models on various tasks, showcasing excellence in natural language processing, math, reasoning, image analysis, multilingual translation, and audio and video processing.",
      "Developers can easily integrate Gemini models into their applications using Google AI Studio and Google Cloud Vertex AI, leveraging its optimized efficiency and performance."
    ],
    "commentSummary": [
      "The discussion encompasses diverse topics on AI models, such as the Gemini Flash plugin, context caching, and the constraints of models like Gemini 1.5 Pro and GPT-3.5.",
      "There are debates on character-based models, morphemes, and the utilization of OpenAI models within the discussion.",
      "Users are expressing doubts about the capabilities and dependability of Google's AI models like Gemini Ultra and GPT-4, alongside concerns regarding AI ethics, pricing strategies, Google's competitiveness, and AI safety triggers."
    ],
    "points": 390,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1715709659
  },
  {
    "id": 40356751,
    "title": "Creating Touchable 3D Midair Plasma Displays with Femtosecond Lasers",
    "originLink": "https://spectrum.ieee.org/femtosecond-lasers-create-3d-midair-plasma-displays-you-can-touch",
    "originBody": "CONSUMER ELECTRONICS NEWS Femtosecond Lasers Create 3-D Midair Plasma Displays You Can Touch Floating dots of plasma make tiny, touchable imagesEVAN ACKERMAN26 JUN 20153 MIN READ IMAGE: YOICHI OCHIAI/UNIVERSITY OF TSUKUBA",
    "commentLink": "https://news.ycombinator.com/item?id=40356751",
    "commentBody": "Femtosecond lasers create 3D midair plasma displays you can touch (2015) (ieee.org)246 points by jagged-chisel 17 hours agohidepastfavorite113 comments denton-scratch 53 minutes ago> laser-induced plasma displays that ionize air molecules to create glowing points of light. The New Scientist used to have a column inside the back page called Daedalus. It usually described some kind of weird or wonderful \"invention\" that probably wasn't practical. One of them was for street lighting: you'd have pairs of ultraviolet lasers on either side of the street, intersecting in mid-air. The pairs of lasers would have frequencies that were not exactly the same, so that at the point of intersection lower-frequency \"beats\" would occur. If the frequency of the beats were adjusted to match the excitation energy of (e.g.) O2, the oxygen would emit light. Another Daedalus \"invention\" I remember was to use plant tropins to control the growth of wood. You could then grow e.g. an armchair, or a table, or even a house. No glue needed, no saw needed. I don't know who used to write that column. reply elric 7 minutes agoparentControling the growth of wood in that manner is a pretty common fantasy trope, usually of the Elven variety. Or treesinging in Wheel of Time, for example. It's one of those that occasionally pops into my head. Would be pretty neat from a CO2-sequestration point of view, but unless the wood is stressed appropriately during growth, the resulting furniture (or housing) would likely crumble pretty easily. reply cyberax 15 hours agoprevI have a slight eye damage in one eye from working with lasers that were just a bit outside the safe limits. And I realized that only years after getting it. So the last thing I want, is to be near unconfined lasers powerful enough to ionize the air. reply rolandog 13 hours agoparent> So the last thing I want, is to be near unconfined lasers powerful enough to ionize the air. I wholeheartedly agree. Just thinking about how the potentially-unregulated cheaply-manufactured knock-off projectors will result in having to wear welding glasses when walking around the street to avoid being blinded by the 3D advertisements that are being shot at your face... reply lyu07282 4 hours agorootparent> result in having to wear welding glasses when walking around the street to avoid being blinded by the 3D advertisements that are being shot at your face... but I'm sure the welding glasses would fit the overall cyberpunk aesthetic of that future very well reply cyanydeez 9 hours agorootparentprevUnfortunately, we couldn't even protect the dumbs from staring at a solar eclipse. Safety standards are ridiculously hard to calibrate. reply PhasmaFelis 5 hours agorootparentUnrelated, how bad is it to look without protection during totality? We got the approved eclipse glasses and diligently kept them on through the whole lead-up, then couldn't see shit once totality started, so we took them off and gazed at the loveliness and then hastily put them back on t moment the sun started to re-emerge. I get that it's not good to tell the public \"don't look at an eclipse unprotected except for this precisely-timed period,\" because that would lead to a statistically significant increase in permanent eye damage, but what we did seems to have gone fine, at least when exposures are circa once a decade. reply LegionMammal978 4 hours agorootparentThe general advice (at least from everything I've seen) is that you can and should remove your eclipse glasses during totality; after all, the bright surface of the sun is totally blocked. I personally looked into the research on solar eye damage a few years ago (when the 2017 eclipse fervor had died down), and my impression is that all recorded cases result from focusing directly at the sun for at least several seconds straight; glancing very briefly at the sun, or seeing it in your peripheral vision, is only uncomfortable at worst. Eye damage is mainly associated with eclipses since they motivate many people to stare at the sun, rather than the partially-eclipsed sun being uniquely dangerous to look at. But when the time comes around, the messaging gets very black-and-white, as you said. (Not that it's a bad idea to avoid looking at the sun as much as possible!) reply schoen 3 hours agorootparentI've also been interested in this in the past since I slightly damaged my eye by staring at a partial eclipse as an adolescent. (I can no longer perceive any damage, but it might have permanently reduced my retinal acuity in one spot.) > Eye damage is mainly associated with eclipses since they motivate many people to stare at the sun, rather than the partially-eclipsed sun being uniquely dangerous to look at. Most sources seem to agree with that, and that's basically my impression. But I do know of another theory, which is that damage is associated with something like J/m² absorbed energy, and so roughly with (W/m² × s) of exposure, but the blink or discomfort reflex is associated with something more like total W entering the eye. The way that the sun is obscured during a partial eclipse is not by dimming (which would reduce both the total power entering the eye and the total power absorbed by a given region of the retina by the same factor), but instead by making a portion of the solar disc invisible, while leaving other portions fully visible. Those portions are still causing a comparable absorption of energy per second on the parts of the retina where they are focused, but the partially-obscured sun, in addition to being much more interesting, might be less uncomfortable to stare at because of the total amount of light being lower. Sorry if that account was too wordy. A more concise way of putting it might be: Your desire to look away after a relatively short time is based on the total amount of light entering your eye (which is reduced during a partial eclipse), but the sun's ability to damage your eye is based more on the total amount of light focused on a given part of your retina (which is not reduced very much, since the parts of the sun you can still see are just about as bright as usual). I don't know whether that interpretation is right, but it's another wrinkle on the \"you're more inclined to stare at the sun during an eclipse than you usually would be\" issue: it might not be exclusively because it's more interesting, but also because the total light is less than usual, so your discomfort is less than usual, but your risk of injury per second is only very slightly decreased. Related to this, we don't have pain receptors directly on the retina itself, so the feeling of discomfort we get when looking at a bright light isn't directly indicating whether the retina is being damaged or not. reply throwaway290 1 hour agorootparentDon't our eyes adjust, so if it seems like the sun is obscured the pupils would dilate increasing potential damage? reply qwertox 13 hours agoparentprevI think the same happened to me with LEDs. I have an odd feeling that I can't really see a small section somewhere close to the center of focus, not directly at it. I remember an odd sensation in the eye for days after dealing with a white LED. reply elric 5 minutes agorootparentI have a similar thing. Have had numerous tests for it, and no one (so far) has been able to give me a satisfying explanation. I've gotten used to it, but it freaked me out when I first noticed it. reply fy20 13 hours agorootparentprevAre you sure thats not just the blind spot? It's quite big - roughly the size of a rubbish bin lid on the other side of a bedroom. Here's how to spot it: Hold you hands together, with thumbs up, at arms length. Close your left eye, and slowly move your right thumb away to the right, while looking at your left thumb. At around 6 inches it will dissapear. It's kind of hard to notice when my thumb is static, but it's more obvious if I wiggle my thumb while doing it. reply qwertox 13 hours agorootparentWow, cool stuff! But no, its much closer to the center, and it's not something I can really spot. I just know that it started after I dealt a bit with newly bought \"high power\" leds. Not really high power and I never really looked directly into them (them facing at me). It also might have been some bright, red ones on breadboards. Happened over 10 years ago. reply jijijijij 12 hours agorootparentYou may be able to better pin it down along sharp contrast lines, maybe moving, or flickering patterns. Eg. a black and white grid, stripes, or small checkerboard pattern on an LCD screen. And of course isolate eyes for these tests. Your brain can fill in a lot of voids before you notice, especially for monotonous areas and static impressions (as mentioned the blindspot or the blood vessels on your retina are usually \"invisible\" until you provoke awareness through unusual lighting changes, or defined peripheral accounting experiments). You likely won't notice acquired \"blindspots\" looking at a white wall, or chaotic fallen leaves on the ground, especially where the other eye provides missing information, but at the edges of highly predictable patterns, one eye closed at a time, you may trick your brain to fuck up, eg. blur or indent otherwise clearly defined areas, when it can't decide which color to fill. Reading texts with on eye closed may also highlight \"dancing\" letters or distortions around your center of vision. Worth noting, such defects may be caused by progressive conditions like retina detachment or even ocular melanoma, and the association with laser/light accidents may be incidental. If you spot a spot, do not brush it off as a limited loss! Have it checked, even with a likely attributable cause. You may prevent full blindness through medical intervention in case of disease! Edit: You can see the blood vessels when you look a white wall and steadily move a (smartphone) flashlight in and out of the field of vision, slowly waving the light next to your head, illuminating from your ears to the side of your nose and consequentially your eyes at a shallow angle. This will cause an unusual blood vessel shadow, now meandering through your vision. The blood vessels are also very visible during eye examinations when the doctor moves the slit lamp around (go check it out ;) Very weird seeing the insides of the very eye seeing, by ... well ... seeing. reply seabass-labrax 9 hours agorootparentSome of my earliest memories are of these kinds of perception, including the 'phosphenes' caused by internal pressure on the eyes when one looks to the side (they appear as fleeting, roundish flashes). It's curious to me that such formative memories would be triggered by something entirely 'internal' - not a measure of external stimulus involved. Perhaps in a similar way, someone else's earliest memory might be that of becoming aware of their heart beating! reply jijijijij 9 hours agorootparentI don't think there is much of a brain when the heart starts beating. Edit: Never mind; misread. reply AdamH12113 12 hours agorootparentprevOptometrists have cameras that can take pictures of your retina and see if there's any obvious damage. You might consider getting your eyes checked out. reply jijijijij 9 hours agorootparentThey won't see \"dead pixels\" unless it's severe damage, or a different underlying cause. All bets are off on the optical nerve, since MRI resolution may at best allow to spot a tumor. They can, however, do an extended version of the blind spot experiment above for the whole field of vision, where they project light dots into a hemisphere in an unpredictable but iteratively somewhat exhaustive fashion. Very tiring and challenging test, since you need to keep your eyes from wandering, fixated at a boring reference point for more than half an hour. Like a hearing test, but for your eyes... Laser beams or high energy radiation in general may also damage vision elsewhere in the optical pathway. Like opacification in the cornea or vitreous body when proteins get denatured by the heat. The body is very bad a repairing any damage in the optical apparatus since the eyes do have their own blood barrier, so macrophages usually don't have access to clean up \"junk\", and most tissues involved aren't really regenerative. Worse, damaged proteins tend to slowly spread the faulty structure to their neighbors. Don't fuck with your eyes! reply philipswood 6 hours agorootparentprevTry to map out the spot: Make a grid in a drawing program with crosshairs in the centre. Close one eye. Keep looking at the crosshairs. Scan from the central focus point using your mouse and click when you can't see the mouse cursor. With a bit of trial and error you should be able to map out large regions you can't actually see in. (My dad did this years ago for a retinal detachment spot in his one eye.) More importantly: get an optometrist to check out your retina. reply relaxing 6 hours agorootparentprevI’d be surprised if you damaged your eyes with any LEDs in the visible spectrum. Walking around in midday sun exposes you to orders of magnitude more energy. Maybe if you had focusing lenses on them, and they were really high power, like the kind that needs heat sinks to operate safely. If you had say UV LEDs, all bets are off since it wouldn’t trigger the pupillary light reflex to close down the iris. reply Zelizz 3 hours agorootparentprevThat's really cool. I tried it with an angled mirror, it was really weird wiggling my thumb and seeing it in the reflection but not outside of it. reply istjohn 13 hours agorootparentprevWoah, that's neat! reply solardev 13 hours agoparentprevWarning: Do not touch laser with remaining hand. reply m463 6 hours agorootparentnot that one, the other one. reply hi-v-rocknroll 9 hours agoparentprevWhat sort of work? Industrial processes like cleaning or cutting, or physics/engineering bench firing? reply colordrops 11 hours agoparentprevThis article is 9 years old. Probably went nowhere because of the reason you state. reply robertclaus 6 hours agorootparentLol, I completely missed this. Thanks for the callout. That WAS right around when everyone was trying to (or started to) use gimmicky volumetric displays at the trade shows I attended. reply CodeArtisan 15 hours agoparentprevHow did you realized you had eye damage? reply utensil4778 15 hours agorootparentTypically you'll notice a persistent black spot in your vision. Particularly if you're looking at a bright field like a blank white page on your monitor. reply jessriedel 14 hours agorootparentCite? My vague understanding of how vision works is that your brain will tend to interpolate over missing data rather than perceive it as black. So you need to actually test whether you can discriminate things in the affected field of view. reply philipswood 6 hours agorootparentMy dad has quite a severe spot where his retina detached in the one eye. Right in the centre of his vision. He reports it looking like a visible hole with surrounding distortion. Apparently similar to some renderings of black holes. That said, this is a large defect in the centre of vision - it's pretty much un-interpolatable. I understand smaller defects not in the macula just get interpolated away. reply atomicnumber3 13 hours agorootparentprevVision is complicated. Everyone's vision experience is slightly different. So different people might notice different things. reply __MatrixMan__ 11 hours agorootparentprevThis is not a citation, but... I have this spot in my right eye that resembles dead pixels. Just tiny blackness. If I let my eyes try to focus on it, it moves with my eyeball, so then they try to focus on the new location... ad infinitum. So they sort of jitter on a path upwards and to the right. I can only see it in certain lighting. Eye doctor couldn't see anything wrong with the area and suggested it might be a floater that got attached and would go away on its own within 6mo. That was 4yr ago and it's still around. It's not getting worse so it doesn't bother me, it just reminds me of the saa from the Wheel of Time. I would have expected interpolation versus vision artifacts as well, but apparently that's an \"only sometimes\" thing. reply lurquer 8 hours agorootparentYou have a speck of gunk in your vitreous near the focal point. Not retina. You can’t ‘see’ damage to your retina… you brain just interpolates. The gunk in your vitreous can stay there a long time… it’s gel, more or less, and can take a long time to move. reply cyberax 11 hours agorootparentprevIf I look at a straight line with the right eye, I see a \"kink\" in it. Grids look like they are warped in near the center. Turns out, that I have a \"pinhole\" damaged area in the right eye's fovea. It's small enough that the brain can \"interpolate\" over it, especially when both eyes can see the object. reply TrainedMonkey 14 hours agorootparentprevDon't know how OP did it, but I've been tested a couple of times at the optometrist by field of view machines*. You concentrate on a central point while the machine shows you a pictogram in a random location with a random timing. You click a button every time you see one. Note: not an actual name, I don't know how these are called. reply prodias2 14 hours agorootparentI believe the machine you're referring to is called a Humphrey visual field analyzer. https://en.wikipedia.org/wiki/Humphrey_visual_field_analyser reply jijijijij 9 hours agorootparentprevThese tests are no fun at all. reply BikiniPrince 15 hours agorootparentprevcheck engine light in the upper left hand corner of his vision. reply CamperBob2 14 hours agoparentprevGee, if only they had thought of this rather obvious point and addressed it in the article. reply tflol 16 hours agoprev> The researchers found that a pulse duration that minuscule doesn't result in any appreciable skin damage unless the laser is firing at that same spot at one shot per millisecond for a duration of 2,000 milliseconds Haha so if this thing is misconfigured badly enough it can just melt you reply itishappy 11 hours agoparentIt'll engrave you. The spot size is small and the material removal rate is low. Still not fun, but it's not gonna take a finger. reply King-Aaron 9 hours agorootparent> Still not fun, but it's not gonna take a finger. It might take your retina though reply yreg 11 hours agorootparentprevSounds like a promising topic for an Electroboom video. reply vlovich123 13 hours agoparentprevThat’s very awkward wording. Is it saying a 1ms pulse every 2s? reply throwaway11460 12 hours agorootparentI understand it as one pulse (shorter than a ms) each ms for 2 seconds. reply itishappy 12 hours agorootparentprevfs pulse duration, 1ms repetition rate, 2s run duration reply golergka 16 hours agoparentprevHow much does an 0-day cost for a device that can literally kill? reply kjkjadksj 16 hours agorootparentAsk an automaker or an airline reply moffkalast 14 hours agorootparentBoeing's hitman could not be reached for comment. reply glitchc 14 hours agorootparentI hear they will pay a visit though if you hashtag \"whistleblower\" and \"Boeing\" reply solardev 13 hours agorootparentDo they just show up and remove all your door hinges? reply jagged-chisel 17 hours agoprevThis technology came up recently in conversation so I searched the web to see what became of it. All I can find are articles dating from summer 2015 and nothing of substance after that. I’m sure we can all come up with some reasons this wouldn’t be practical in many situations, but the tech seemed promising. Do any of you have knowledge of what happened here? Was it smoke and mirrors? Just really not practical? I would really like to read some details about why this seems to have vanished. reply PaulHoule 17 hours agoparentGo into a lab in the first floor of the physics building at my Uni and you are likely to find a femtosecond laser inside which sprawls over an optical bench which takes up most of the room. At the heart of it is a fiber laser that costs about $250,000 but they usually have added a lot of stuff to it. A femtosecond laser for medical use is about $400,000 https://crstoday.com/articles/2014-sep/pricing-the-laser-cat... which is one reason you might not see this become widespread. reply jandrese 17 hours agorootparentIronically that seems promising. This could just be an engineering problem where the only people making the required laser equipment are only making lab/medical grade gear with enormous markups. It's possible someone comes up with a practical to manufacture solid state version that unlocks these displays for the masses. There is one consideration however where one failure mode for this device includes a situation where the processing locks up and leaves the laser pointed at a single spot continuously, burning a hole and possibly starting a fire. reply PaulHoule 16 hours agorootparentLasers used for laser light shows already have interlocks that will stop the laser if the mirror stops moving. My understanding is that femtosecond lasers have some parts that are physically large because they use prisms, diffraction gratings, and such, to spread out a pulse so the laser can effectively amplify it and then recompress the pulse to use it. See https://en.wikipedia.org/wiki/Pulsed_energy_projectile note a workable laser \"gun\" could plausibly be \"set to stun\" because if the energy of a pulse sparks the air near your skin or clothing, the plasma absorbs almost all the energy causing a plasma explosion which can knock you down plus cause severe pain from an electromagnetic pulse. reply emchammer 16 hours agorootparentprevThe pulses are not entrained to software. They are inherent to the laser's design, such as with Q-switching. reply vlovich123 13 hours agorootparentI think ops point still holds - actively modulated q switches could still have a failure that pushes the laser outside the safety parameters, no? I don’t think OP was necessarily suggesting a software failure. reply emchammer 10 hours agorootparentQ-switching does not operate like a transistor's base. It is more like the charge pump on a camera flash. It will not produce a high-power pulse if operated outside of parameters. There are other ways with different physics but comparable principles. reply yetihehe 16 hours agorootparentprev> where the processing locks up and leaves the laser pointed at a single spot continuously, burning a hole and possibly starting a fire. Already solved for 3d-printers locking up and starting a fire. reply alt0_ 16 hours agorootparentprevI'm incredibly curious as to what that even looks like. Where do you study? Can you give us a photo? reply PaulHoule 15 hours agorootparentSee https://www.oist.jp/image/researchers-femtosecond-spectrosco... or https://www.mp.aau.dk/research/laboratories/physics-lab/shor... or https://hajim.rochester.edu/optics/sites/guo/virtual-lab.htm... reply zonkerdonker 16 hours agoparentprevSeems like the tech is still likely alive and kicking in the military space: https://patents.google.com/patent/US20200041236A1/ US Navy submitted a patent in 2018 for a jet-mounted plasma decoy projector, I'm sure they have many more applications that I wasn't able to find as well reply JumpCrisscross 16 hours agorootparent> the tech is still likely alive and kicking in the military space I mean, it's a field that burns things that wander into it. I'm genuinely blown away this was considered for a display technology. It's closer to the risk profile of fireworks than an OLED. reply traverseda 7 hours agoparentprevRemember all those UFOs people were reporting? That was probably this, military took it over. reply vonzepp 13 hours agoprevThis is quite hard to do safely. Generating air breakdown with IR femto requires a reasonable amount of fluence. They are probably using galvo scanners to shift the beam in xy, but to get the required intensity they have to also shift the focal plane. Given the distances they are projecting away from the car that NA is low meaning they would need a reasonable power. Probably why they are running it at 1kHz. Given its 2015 it is probably a ti-sapphire at 80fs, galvos and an optotune lens. The reason this isn't a thing is that it very hard to make this eye safe. reply lainga 16 hours agoprevWeren't these things ear-splittingly loud from the constant formation of plasma balls in midair? That's what I recall from a demonstration In The Day reply mey 15 hours agoparentLooking into the plasma/laser paper and it mentions 77.2 dB at an insanely close 22mm distance. Background was 55.7. http://arxiv.org/pdf/1506.06668v1 Page 9. It sounds (hah) like it wouldn't be pleasant if you have your head near a full display in operation, especially since the noise scales with the resolution/brightness. It would be interesting to see how quickly the noise drops off over distance. You are looking at something between a home sound system, vacuum cleaner, or highway traffic at that volume level. Tolerable but not pleasant. reply Filligree 15 hours agorootparentI wonder if it could be modulated to function as a sound system? reply ben_w 15 hours agorootparentYes. You may enjoy the similar use of Tesla coils: https://youtu.be/rd3bH_xNYYQ?si=TXO_kA0PZ6YkUZyt reply doublerabbit 15 hours agorootparentPersonally I'm more the floppy drive fan. https://m.youtube.com/watch?v=8fh1CIupVXU But Tesla coils are cool, as are van de graaff generators. reply aidenn0 4 hours agorootparentprevA friend of mine built speakers that ionized the air and then used some mechanism (electrostatics? voice coil?) to move the air directly; kind of a coneless speaker. It didn't get that loud and made a lot of ozone. reply dotnet00 14 hours agoprevReminds me of the story from a few years ago where the US military was looking into scaring people via screaming balls of plasma, which gave rise to the joke that the CIA was trying to make religious extremists think their god was talking to them using the tech. https://newstarget.com/2018-04-24-u-s-military-creating-non-... reply farkanoid 14 hours agoparentI remember this! I'm glad someone else does too. reply max_ 13 hours agoprevIf you're interested in that project. Here is an equally cool project form Voxon Photonics. That you can buy today and is also very safe.[0] [0]: https://voxon.co/ reply farkanoid 14 hours agoprevI often wonder the same about the technology used in a mysterious video that appeared on youtube about 15 years ago titled \"Laser Induced Plasma Channel\". It appears to be an area denial weapon that uses femtosecond lasers to guide electric arcs across a hallway (think a metre-long ridiculously straight Taser discharge). The video had no description and comments on it were never answered - it was recently removed by the uploader, but there seems to be a reupload on DailyMotion[1] from 9 years ago. [1] https://www.dailymotion.com/video/x31ovvi reply jpgvm 16 hours agoprevThere are many display technologies that just didn't make it to mass production. A favourite of mine is Field Emission Display technology. However they are largely irrelevant now because OLED essentially fill the same role. reply worldsayshi 16 hours agoparentMine is transreflective LCD. I just really want to be able to work with my laptop in the sun. https://en.wikipedia.org/wiki/Transflective_liquid-crystal_d... Here it's used on the Adam Tablet that never took off: https://www.youtube.com/watch?v=pGUKHDBoTEc reply Woovie 15 hours agorootparentLTT just made a video about a modern display that is like this tech https://www.youtube.com/watch?v=c0TcGjzKbag reply worldsayshi 13 hours agorootparentInteresting! Although going for no backlight seem to make it about as impractical indoors as regular displays are outdoors? reply cycomanic 15 hours agorootparentprevHi had an Adam, the idea (and display) was pretty cool, but the build quality and software was not that great. Eink really has taken over that niche though, so not sure I really miss those displays. reply worldsayshi 13 hours agorootparentYeah it seems that TLCD never really got a chance to get developed properly. And Eink seem to me to have a lot of other clear drawbacks which makes it impractical for regular work use cases. reply tootie 15 hours agoparentprevWe played with lenticular displays and they are very good and very safe. But like it's just not that useful to have a 3d display. A 2d screen is perfectly good at conveying the same information. reply Animats 14 hours agoprevSee-through displays show up in movies so that the viewer can see the actors' faces. Not because see-through displays are useful. reply jcims 13 hours agoprevThis page has been around for a while. If you watch the YouTube video, you can see the skin on the person’s finger get burnt as they touch the little tiny plasma balls. https://youtu.be/AoWi10YVmfE?si=1zzwNba9A1l9Y2ld reply jagged-chisel 13 hours agoprevThis was supposed to be Ask HN but I didn’t realize I was on the wrong page for Ask submissions. So the title was changed to reflect the article. Anyway, my comment asked the question and we got some discussion, so it’s all good. reply karunamurti 6 hours agoprevI just want to point out holodeck episode in Star Trek preceded Star Wars by 3 years. reply aidenn0 4 hours agoparentIt was the animated series, if anyone else is like me and could not for the life of themselves remember a holodeck in the original series (Thanks Memory Alpha!). reply thesh4d0w 11 hours agoprevWorth calling out this article is from 8 years ago. reply Cadwhisker 10 hours agoprevThe video shows slightly burned skin on the tip of his finger where he has been touching the \"display\". reply bilsbie 12 hours agoprevWhatever happened to the idea of sending electricity through air with this technology? One application I heard of in the 90s was a long range taser. reply ffhhj 17 hours agoprevI remember IO2 Heliodisplay from around 2007. Now their site seems to be dead. > This holographic-style display works by projecting a 2D image onto a cloud of microscopic droplets (probably water, although this isn't confirmed) https://www.techpowerup.com/forums/threads/io2-launches-%E2%... reply Nursie 17 hours agoparentI followed them for ages, such cool tech for the early 00s when they first publicised it. But it never seemed to have gone anywhere. They were very cagey at first about the fact it used water vapour because they didn’t want people to think it was just projection onto a vapour screen when they had some sort of directed laser tech. It still needed the water though! If I were to guess why it didn’t reach any sort of momentum, I reckon they couldn’t control the costs or get the brightness good enough. But that’s really speculation. reply dmbche 16 hours agoprevVery cool! How energy efficient could it theoretically be? Is it somewhat relatable to touchscreens, or are we talking thousands of times more power per pixel per second? reply nirav72 11 hours agoprevI’m imagining blade runner style giant holograms above buildings if this tech matures reply RIMR 11 hours agoprevOP forgot to include it in the title, but this is from (2015). reply twic 14 hours agoprevWhile we're on the subject, what happened to uBeam? reply kragen 10 hours agoparentsame thing that happened to theranos, just in a lower-key fashion; they hadn't figured out how to do the thing they said they'd figured out how to do, and possibly nobody ever will in my book that makes them a fraud, though there are clearly a lot of ai company founders who disagree reply ErneX 14 hours agoprev(2015) reply ranger_danger 7 hours agoprevYou can also use femtosecond lasers and a streak camera to film the speed of light: https://www.youtube.com/watch?v=7Ys_yKGNFRQ Maybe one day something like this can be used to have true light-field cameras without microlens arrays (like Lytro had). reply awinter-py 10 hours agoprevthe large kind also do this in a sense reply alwa 10 hours agoprev(2015) reply ForOldHack 12 hours agoprevQue ads. reply m3kw9 12 hours agoprevUntil a kid put his face in it for fun reply 201984 14 hours agoprev(2015) reply bawolff 15 hours agoprev> However, a nanosecond-scale plasma burst still contains a significant amount of energy; you don’t want to go walking through one of these displays, because it will burn you. Yeah, i think that answers the headline question. Burning people is bad. reply jjk166 15 hours agoparentSo will touching a lightbulb, but we have those in abundance in the same application. I don't think that adequately answers the question. reply bawolff 15 hours agorootparentWe don't usually have lightbulbs suspended mid-air at human height without some sort of protective case. I guess i assume the point here is to have no protective case since the article made such a big deal about touching the display. reply jjk166 15 hours agorootparentThe overwhelming majority of lights are within human reach with no protective casing. There might be a lampshade to diffuse the light, and some frame to physically support the bulb, but nothing to stop a human hand from reaching in and touching it. Hell installing and replacing bulbs is typically done with bare hands. Free hanging bulb lights used to be quite common in closets and basements. reply Log_out_ 16 hours agoprevNow print circuitry mid air and energize.. reply chris_va 17 hours agoprevPfft, this is just a bunch of hot air. ... /s reply zoklet-enjoyer 16 hours agoprev [–] UFOs reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Femtosecond lasers are now capable of generating 3-D midair plasma displays, enabling the creation of small interactive images that can be physically interacted with."
    ],
    "commentSummary": [
      "Femtosecond lasers are used to generate 3D midair plasma displays, raising safety concerns regarding possible eye injuries from bright lights.",
      "Discussions include vision problems linked to prolonged exposure to intense lights, the brain's capacity to complete visual missing information, and the significance of routine eye check-ups.",
      "Topics cover optometric evaluations, the viability of femtosecond lasers, potential military uses, progress in display tech, holographic displays, and the risks posed by nanosecond-scale plasma bursts."
    ],
    "points": 246,
    "commentCount": 113,
    "retryCount": 0,
    "time": 1715703058
  },
  {
    "id": 40357681,
    "title": "Google Launches Model Explorer to Streamline ML Model Visualization",
    "originLink": "https://ai.google.dev/edge/model-explorer",
    "originBody": "Learn more about our Google AI Edge announcements from I/O Home Edge Model Explorer Send feedback Model Explorer Stay organized with collections Save and categorize content based on your preferences. A visualization tool that lets you analyze ML models and graphs, accelerating deployment to on-device targets. Get Started Try it in Colab Learn More Making edge development faster The constraints of edge devices often necessitate extra steps to convert and optimize models before they run efficiently, and visualization is one of the most effective ways to understand a model and identify targets for optimization. Conversion Quantization Optimization Model Explorer's side-by-side comparison feature makes it easier to spot conversion-related issues. Navigate the graph layer by layer, diving deeper into the graph by expanding and collapsing sections. Inspect the internal structure and connections within graphs at the granularity you need. Use Model Explorer to identify problematic operations affected by quantization. Sort ops by error metrics to find quality drops, get insights per layer, and compare different quantization results to find the ideal model size-quality trade-off. Use Model Explorer to better understand the output from your benchmarking and debugging tools. Gain insights into which ops can run on GPU, sort ops by latency, and compare per-op performance across accelerators. Support for large models Model Explorer is designed to render large models seamlessly. Thousands of nodes? No problem. The GPU-based rendering engine is capable of scaling up to smoothly render even very large models. And Model Explorer's unique approach to collapsing layers like a system of files and folders means that it's faster and easier to navigate. Features designed to help you work faster Search Split View Data Overlays Powerful regex-based search helps you locate, filter, and highlight specific nodes.Load models side by side in the same tab for easy comparison.Load custom, node-specific data into Model Explorer to quickly identify hot spots and other issues with your model. Export to .png Bookmarking Easy to access metadata With the click of a button, export an image of the graph to share with your team.Save your location in the graph by adding bookmarks, making it easy to jump between areas.View tensor shapes, trace inputs and outputs, highlight identical layers, see child node counts, and more. Two ways to use Model Explorer Run it locally Run it in a Colab notebook Follow the easy installation instructions on GitHub to set up Model Explorer on your local machine. It runs in a browser window and all your data stays local. Supports Linux, Mac and Windows. Model Explorer runs well in Colab, meaning you can integrate it into your existing model development workflow. Try the demo notebook or follow the installation instructions to add it to your own. Send feedback",
    "commentLink": "https://news.ycombinator.com/item?id=40357681",
    "commentBody": "Model Explorer: intuitive and hierarchical visualization of model graphs (ai.google.dev)245 points by antognini 16 hours agohidepastfavorite33 comments joaquincabezas 16 hours agoI normally use Netron for quickly inspecting models and making this 'mental picture' of the architecture, but this hierarchical approach seems a better fit for my needs. I'm just starting and the first impression is pretty good! reply tomrod 14 hours agoprevLooks like they have moved to https://ai.google.dev/edge/model-explorer reply dang 14 hours agoparentI'm not sure which link is more up to date, but since this one appears to give more background, we've switched to it from https://github.com/google-ai-edge/model-explorer above. Thanks! reply tomrod 7 hours agorootparentYou bet! They reference the github repo, but more info is in the link you moved it to. reply fabmilo 16 hours agoprevThese tools are eye candy and have been around from tensorflow/tensorboard 0.x 10 years ago but never used after just trying them for fun. You need to read the source code no easy way around it. reply swaptr 16 hours agoprevhttps://github.com/lutzroeder/netron https://netron.app/ reply anvuong 16 hours agoprevLooks cool but seems like it doesn't work on torch 2.0 \"AttributeError: module 'torch' has no attribute 'export'\" The torch.export API is currently in active development with planned breaking changes. The installation guide for this is still very minimal, anyone knows how to get it working on torch 2.0? reply frontierkodiak 12 hours agoparentI haven't managed to successfully export my custom ViT model yet, but I've not had an issue accessing the export methods in torch 2.3 within the nvcr.io/nvidia/pytorch:24.02-py3 container. I may have some more time to debug my trace tonight (i.e. remove conditionals from model + make sure everything is on CPU) and will update if I have any new insights. ``` from torch.export import export ... example_args = (dummy_input) exported_program = export(model, args=example_args) ``` Links: - torch.export docs: https://pytorch.org/docs/stable/export.html#serialization - Using 24.02 container: https://docs.nvidia.com/deeplearning/frameworks/pytorch-rele... reply westurner 14 hours agoprevgoogle-ai-edge/model-explorer//example_colabs/quick_start.ipynb: https://github.com/google-ai-edge/model-explorer/blob/main/e... XAI: Explainable AI: https://en.wikipedia.org/wiki/Explainable_artificial_intelli... reply germanjoey 12 hours agoprevIs there a demo of a model visualized using this somewhere? Even if it's just a short video... it's hard to tell what it's like from screenshots. reply brrrrrm 16 hours agoprevI've never really understood the point of these visualizer things. The idea that a model is always well represented by a directed acyclic graph seems extremely dated. I really would love a PyTorch/JAX profiler that shows, in annotated Python, where your code is allocating memory, using compute or doing device copies. reply lamename 16 hours agoparentIt may be that the way you like to think about things and the way others like to are different. I find that quickly grasping a new architecture is easiest with a graph-based diagram first. Then code for details. All with the goal of internalizing the information processing steps. Not memory allocation per se. In my mind, how the network implementation allocates memory is a different question. But I think both of our desires just reflect our jobs, our interests, and simply how our brains conceptualize things differently. reply brrrrrm 14 hours agorootparentI think it's a trap of visual elegance. When you start thinking of models this way you miss the way a lot of models are actually written. E.g. how do you represent an online fine-tuning process? I want to randomly switch between a reference impl and an approximation method, but when using the approx method I want to back-propagate so that it gets better over time. full disclosure: I've written plenty of these little visualizers and also fallen for the trap of \"everything should be a declarative graph.\" reply almostgotcaught 15 hours agoparentprevLol I interned inside pytorch a few years ago (you and I even met/talked about tangential things :)) and worked on tracking such allocations (although I didn't hook it up to profiler). Spoiler alert: you can't track such provenance because everything gets muddled in the dispatcher. EDIT: not completely accurate to say you can't do it. I prototyped a little allocator that would stamp every allocation (the pointer itself, in the unused bits, a trick I learned from zach) with the thread id and a timestamp (just an incrementing counter) and then percolate that up to the surface. Obv that didn't land lol. reply chillee 14 hours agorootparentWe actually do track such provenance now (https://pytorch.org/blog/understanding-gpu-memory-1/) - works pretty well I think :) reply almostgotcaught 14 hours agorootparentwell you should tell bram then :p but also while generally \"in all things i defer to horace\" (ok not really) so maybe i'm not looking closely enough (and missed it) but the bottom of that stack shows (roughly) the autograd dispatch key and not the python call site (or some such). and maybe it's a pedantic difference (depends on what bram wants) but i wanted provenance back to the TS op so that i could then do static memory allocation things with that representation (now i've probably fully de-anonymized myself...) and for that use-case, even what you have now, isn't enough (you can't get a total sum for how much each TS op or whatever allocates and when the corresponding free happens). reply brrrrrm 14 hours agorootparentprevI wonder if you could track provenance by operating at the highest layer of the dispatcher and capture any calls to GPU operations (ala Cuda Graph)? > in the unused bits I feel like this is a PT rite of passage :P reply hatthew 11 hours agoparentprevA DAG visualization of a model is a good abstraction to learn the general structure of the model to help contextualize the code you're reading. reply gedy 15 hours agoparentprevNot mocking you, but I'd make a guess you don't like to draw block diagrams when discussing designs or code architecture with others either? Some folks aren't \"visual\" thinkers, and took me a long time working to realize that some folks are like that. reply mathematicaster 15 hours agoparentprevI don't see any reference to acyclic as a requirement. reply brrrrrm 14 hours agorootparentI think it uses TF's graph construct which has that built in? it's like a weird mix of dataflow and control flow graphs. reply pdevr 16 hours agoprevOther than visualizing, creating custom nodes seems to be the most interesting available operation (at the time of writing this). API Guide: https://github.com/google-ai-edge/model-explorer/wiki/4.-API... Custom Nodes - User Guide: https://github.com/google-ai-edge/model-explorer/wiki/2.-Use... reply toddmorey 15 hours agoprevIs it becoming more and more common to launch open software like this without accepting contributions? reply kleiba 15 hours agoprevI'm confused - so it runs only in Edge or what's with the name?! reply inhumantsar 15 hours agoparentthe team is building tools to help run models on edge devices. embedded, mobile, laptops, etc. reply kleiba 14 hours agorootparentOkay, thanks for the explanation! reply spydum 16 hours agoprevNot to be confused with Microsoft Edge, which replaced Internet Explorer with Chromium (the browser that Google Chrome is based on). /s reply Veuxdo 15 hours agoprev [–] What is the \"AI\" part of this...? reply minimaxir 15 hours agoparentIt's an \"AI Model\" Explorer, not an AI \"Model Explorer.\" reply Veuxdo 15 hours agoparentprevAlso, what is the \"Google\" part of this? reply minimaxir 15 hours agorootparentIt's owned and operated by Google. reply nashashmi 15 hours agorootparentprevAnd why is this referring to MS Edge? reply akomtu 15 hours agoparentprev [–] Marketing & SEO. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google introduced Model Explorer, a tool aiding developers in managing ML models on edge devices for enhanced efficiency.",
      "Model Explorer enables visualization, analysis, and optimization of models for on-device deployment, supporting large models with features like search and split view.",
      "Users can utilize Model Explorer locally or in a Colab notebook, providing feedback and accessing extra resources for leveraging the tool effectively."
    ],
    "commentSummary": [
      "The Model Explorer is a tool that visualizes model graphs hierarchically, aiding users in comprehending model structures, utilizing export features in Torch 2.3, and designing custom nodes.",
      "Debate surrounds the effectiveness of visualizers versus code-based comprehension, with the tool linked to Google and targeting model deployment on edge devices.",
      "Users are uncertain about the tool's cross-platform and browser compatibility, as well as its significance in AI applications, leading to speculations about potential marketing and SEO motives in certain mentions."
    ],
    "points": 245,
    "commentCount": 33,
    "retryCount": 0,
    "time": 1715707759
  },
  {
    "id": 40361387,
    "title": "The Enigma of Jodie Chiffey: Unveiling Legitimacy Online",
    "originLink": "https://matt.sh/the-most-talented-person",
    "originBody": "so talent much person The Most Talented Person In The World I discovered a genius online. Just look at all her accomplishments: Jodie is a cross-functional 3D designer and blogger. She has special interests in open source 3D printing for R&D and has spent a lot of time at different projects across the globe to learn more about 3D printing. Most of what she has learned is from hands-on experience. Jodie Chiffey is a blogger, and mom who loves nothing more than testing out the latest grills, gadgets, and outdoor cooking tech. As passionate about food as she is about family, Jodie loves spending time outdoors and is always the one found hovering over the grill at parties, camping trips, and local community events. Jodie discovered her passion for grilling when she realized how much filler she was able to eliminate from her diet by grilling. So, she began to devote herself to learning more about grilling and mastering her grilling technique. Now, she spends most of her time outdoors grilling non-processed foods and enjoying fresh food on her plate. Jodie Chiffey - Mellifluent Performer at Guitar Space - Jodie fell in love with the guitar at a young age, but has hit a lot of bumps in the road with her journey. She knows all of the frustrations that come with learning the instrument without any formal lessons. But, she pushed through all of those challenges and she’s now here to teach other people too. If you’re looking for advice that’ll stick with you for a lifetime, Jodie’s here to share everything she knows. (mattnote: also lol at “Mellifluent” — ESL spam disease slipping in there) Jodie Chiffey loves the outdoors. If you aren’t sure where to find her, check the nearest dirt trail. She loves being outdoors and spends a good deal of time reviewing products like clothing, footwear, and other outdoor gear. Jodie is a full-time blogger who reviews a lot of the products for us. She writes many of our backpacking gear reviews, outdoor skills advice, and information, and helps visitors find the best destinations for backpacking, camping, hiking, and a lot more. If Jodie’s friends have a travel or outdoors question, they know who to ask and that’s why we featured on our website. Now you can get the same great advice her friends enjoy, so you know where to go and what gear to take with you. Jodie Chiffey - Travel Lover at RV Pioneers- Jodie Chiffey enjoys the outdoors. She goes mountain biking and loves spending time in nature and what better way to get there than to go in an RV? She shares her information here on RV Pioneers. Jodie knows quite a bit about beer and brewing it. From different types of hops and how they will affect your brew to her own opinion on various craft brews on the market. Jodie isn’t afraid to bring you her opinion on a new brew here at Beertannica, along with helping you decide if an ingredient or beer is the one for you. Her comprehensive guides will answer all your questions and more! As a fellow business owner and business strategist, Jodie shares valuable insight into the strategies and tactics business owners can use to increase their growth. Jodie’s run through a lot of shoes. After all, she’s a competitive racer! She understands all of the important features of an ideal running shoe and the biomechanics of the feet of different kinds of runners. You’ll be surprised to learn that Jodie doesn’t have a degree or diploma in athletic shoes; she’s just passionate about them and owns more than fifty pairs of running shoes herself. She’s dedicated her life to studying athletic shoes in detail and writing about them. Jodie has been a certified personal trainer for several years now and has a passion for wellness, health, fitness like no other. She’s been educating herself on nutrition since she was in high school. His boyfriend is a professional herbalist and food healer, so she takes the health and wellness discussions into the homefront. She uses and tests health products, protein supplements personally and shares her insights on topics such as protein, diets, muscle recovery, muscle building, supplements, and naturally-sourced products. Jodie is a nutrition and health expert who owns over 50 pairs of athletic shoes and puts them to good use. She is always looking for a new healthy recipe, loves juicing and tests new products at home, and reports back to us here at Alt Protein. Jodie’s articles and recipes can also be found on her successful blog called The Juice Chief. Jodie Chiffey understands how frustrating it can be to sort your way through long pieces of boring content just to try to get a simple explanation of the digital topic you’re looking to learn about. That’s why she’s here to break down all of the complex topics, to help you learn, understand, and grow in your own digital life. She wants to be there to give you all of the information you need to know to make your own personal decisions in regards to the digital topics you’re bringing into your life! Jodie Chiffey - Jodie is an avid traveler who loves camping and hiking. Why do you love mountain biking? I’ve had two separate passions for quite a long time - cycling and hiking. Whenever I was planning to spend some time outdoors, I had to decide which one to choose. It all made perfect sense when I discovered mountain biking, a beautiful blend of both of my favorite activities with the added value of adrenaline. What is your budget bike pick? If I were looking for a budget-friendly bike these days, I would undoubtedly go with GT Aggressor Expert. It’s super comfortable and lots of fun to ride. Nevertheless, Giant has some great entry-level mountain bikes these days too. What is your splurge bike pick? If I had an endless budget, I would probably buy a Santa Cruz 5010 - ideally in the exclusive X01 AXS RSV Carbon CC configuration. It’s one of the best bicycles I have had the chance to ride so far. What’s your favorite trail that you’ve ridden? Last summer, I enjoyed some tremendous single trails in the Austrian Alps around Nassfeld. Short but spicy Yannick Trail was probably my favorite. It was pretty demanding, and the views were truly priceless. Jodie Chiffey - Jodie Chiffey spends a lot of time helping people figure out how to get the perfect yard. With so many variants from climate, soil type, to the amount of rainfall, she researches and helps Turf and Till readers choose the best products for their needs. Why are you giving advice on Turf and Till? After spending three and a half years transforming my yard from a generic underestimated wasteland into the green haven of my dreams, I have felt two contradictory emotions: relief and sadness. I have realized that I will miss all the research, consideration, and hard work and started looking for new ways to approach this topic and make it a permanent part of my life. That was when I started helping others to fulfill their landscape-related dreams too. Since then, I have worked on numerous exciting projects and gained valuable experience. I see writing for Turf and Till as a natural continuation of my work in this field. Moreover, it is also an excellent opportunity to give back to the online community that has taught me a lot in my beginnings. What is your favorite part of landscaping? My favorite part is turning a blueprint into a reality. I am a huge overthinker, so planning is the most exciting yet also the most demanding and frustrating part of every project for me. When I am finally satisfied with the plan and can watch it turn into a real landscape, someone’s garden, or yard, it is the most gratifying moment I know. If you had to give one piece of advice to someone fixing up their yard, what would it be? Don’t be afraid to think big. There is always space for downgrades and compromises later, but don’t settle with any of it before you even start. If I earned a dollar for each “impossible” idea I have seen turning into a reality, I would be very rich by now. web spam bullshit All of those sites are labeled as “A Venture 4th Media Company” which has such wondrous important content all run by the same “writer” on: https://mtbinsider.com/author/jodie-chiffey/ https://turfandtill.com/author/jodie-chiffey/ https://www.betterwander.com/author/jodie-chiffey/ https://artofgrill.com/author/jodie-chiffey/ https://theathleticfoot.com/author/jodie-chiffey/ https://altprotein.com/team-members/jodie-chiffey/ https://digitalguyde.com/us/ https://total3dprinting.org/author/jodie-chiffey/ https://muckrack.com/jodie-chiffey/articles https://muckrack.com/jodie-chiffey-1 or is it “A Center Keel Media Company?” Either way, remember to take your amazon affiliate link horse supplements and buy your amazon affiliate link best snake traps of 2022 from a purely AI generated photoshop author image from your marketing satchel providing such important online content as https://wizerlist.com/ and https://americansportbike.com/ and https://birdinginsider.com/ and https://curatedcabinets.com/ and more auto-generated link farm low value crap clogging up the global search engine brain for affiliate commissions. “We specialize in developing and transacting digital media assets, providing the best possible visitor experience in every market vertical we enter. Our portfolio is diverse and balanced across multiple established and high growth market niches. We take a circumspect approach to building web assets, focused on the long-term value of acquiring and generating cash flow positive properties, uniquely positioned to capitalize on the maturation of online commerce.” pardon me while i vom Specialties: Search Engine Optimization, eCommerce, Content Marketing, and Internet Marketing of course it’s just a huge affiliate link bot farm network. There are a dozen sites with “Jodie Chiffey” as primary author each with dozens of pages about “The Top 10 X For Y!” each with low effort summaries and affiliate links everywhere polluting tons of search results. also, these “media companies” are two examples out of hundreds or thousands operating to manipulate global search results at scale for low-effort-low-return pennies-per-click ad or affiliate revenue. We’re going to need cryptographically authenticated proof-of-utility to be indexed by search engines fairly soon (or maybe not? who cares? google is happy to pollute the world with search garbage because more garbage means more ads means more google revenue and google is nothing without its $70 billion per year stock buyback program). Also if you haven’t searched for tech help over the past couple years, there are hundreds of fake “tech blog” sites doing the same thing: a site appearing as if it’s “a personal blog,” but having thousands of “helpful” pages in 300 different topic areas half scraped from github issues and half auto-generated. Also, by pure coincidence I’m sure, each page has 50 google ads or it bounces direct to spyware install sites when you navigate away. Each one of those sites is an “expert” in 100 different projects, platforms, programming languages, and strategies, yet they all are written from a first person “it’s just my blog lol!” point of view. Curiously, each of these sites has the exact same layout, looks driven from the same CMS, has the same headers and footers, has the same navigation everywhere, has the same vapid FAQs (“Mountain Biking Tips: WEAR A HELMET!!”), all copied across thousands of sites all ranking in the top results for common queries to every day questions… of course, this garbage is also flooding youtube as well with new emotionless and storyless and zero-personality autogenerated near-human voices all over the place: https://www.youtube.com/watch?v=oknB7TihO5A https://www.youtube.com/@top5reviews-te8eh/videos https://www.youtube.com/@5besstones/videos https://www.youtube.com/@yourreviews2613/videos https://www.youtube.com/@6BestOnesOfficial/videos people talk a lot about “dead internet theory” and how everything is becoming bots and scams and advertising bait, but those aren’t automatic outcomes. The “dead internet” is a process of direct information platform exploitation by manipulative people operating on unchecked-and-unauthenticated platforms drawn to running scams due to their life circumstances (we’re ignoring the other branch of politically controlled “information warfare” scams for now). Bot/scam/advertising/affiliate manipulation is born from usually living in low income societies where the most logical upward trajectory is just exploiting and manipulating high income people and societies who aren’t their own people (much like how a good way to stop spyware is changing your windows locale to russia because russian spyware firms don’t attack their own). Who cares if you make a million fake websites and destroy the information coherency of the planet if it earns you a passive $3 per day? The only future of the internet is, sadly, proof-of-person and proof-of-residence on every public network interaction. There’s no going back to “high trust anonymous internet” when half the world is willing to exploit the other half. We end up with real time global access from “low trust low income max exploitation because who is going to stop us” wankers to “high trust high income low questioning” societies and everything falls apart. the net of a million million lies is upon us and only you can prevent information fires. Welcome to the future of the entire Internet. Jodie the full time 3d-printing expert, grill master, expert guitarist, marketing wonder, brewqueen, personal trainer, avid camper, certified nutritionist, professional hiker, marathon runner, constant traveler, yard work enthusiast, mountain biker, rv driver, sportshoe expert, and mommy blogger welcomes you too.",
    "commentLink": "https://news.ycombinator.com/item?id=40361387",
    "commentBody": "The most talented person in the world (matt.sh)222 points by keyboardJones 10 hours agohidepastfavorite122 comments smusamashah 8 minutes agoThere was another article here on HN some time ago on how big media publishers are littering the web with spam recommendations https://news.ycombinator.com/item?id=39433451 (https://housefresh.com/david-vs-digital-goliaths/) This listed review spam from Dotdash Meredith which I used a clue to search and block all sites by them using uBlacklist. Edit: Found another article saying the same thing there are only few big media giant's dominating search https://detailed.com/google-control/ reply frabjoused 9 hours agoprevHas anyone built a search engine that uses LLMs to pre-grade every page with metrics such as: - Commercial bias (content compared to the source, which it learns about) - Insincere motives - Bloat (how many words it takes to say how little to penalize SEO bloat) I would assume that using LLMs, we can get a pretty good idea of what is SEO bloat and who the bad actors are by this point, and just penalize those results. reply pona-a 1 hour agoparentDid exactly that, hacked together a small pipeline in Nushell with simonw/llm. Even with GPT-4 turbo and given direct guidelines on common spam heuristics, it seems to perform worse than a Bayesian BoW. Endless trails of questions with no relation to the title it describes as informative, presence of affiliate links often gets forgotten in a mass of tokens in their tracking parameters, relevance is consistently near 0.8 even if there's relationship between title and content, and as for insincerity, our favorite BS generator cannot for the life of it correctly recognize its own creations. Your ideas for metrics are good, but LLMs seem to be quite terrible at any of these. A simple set of heuristics and maybe a tiny language model for named entity detection and \"vibe checking\" would serve you much better. Also, a lot of the worst offenders seem to use the same Q&A +- conclusion structure, which Viktor from marginalia.nu wrote a simple heuristic for, which I recall he said did wonders for pruning it. Solving SEO spam is easy when you aren't the one being optimized against. What's left is scaling and information retrieval. reply Uehreka 2 hours agoparentprevWhat makes you think that LLMs will be better at combating spam than they are at creating it? There’s no universal rule that innovations in AI will go hand in hand with innovations in detecting AI, yet I feel like I see people talking all the time like that’s the case. As of right now, LLMs are prolific but unreliable, which makes them extremely well suited for generating spam, but unsuited to detecting it without a large number of false positives and negatives. reply WithinReason 2 hours agorootparentClassification is easier than generation reply pona-a 1 hour agorootparentSadly, this didn't seem to track for LLMs. Even OpenAI gave up on trying to detect its own outputs. > As of July 20, 2023, the AI classifier is no longer available due to its low rate of accuracy. We are working to incorporate feedback and are currently researching more effective provenance techniques for text, and have made a commitment to develop and deploy mechanisms that enable users to understand if audio or visual content is AI-generated. reply WithinReason 1 hour agorootparentYou're not classifying just text, you're classifying entire web pages. If it's easy to tell for a human that it's SEO spam, it's easy for a classifier. reply Uehreka 2 hours agorootparentprevYeah no I’m gonna need more than that chief. Everything I know about LLMs says the opposite. reply WithinReason 2 hours agorootparentLook up Generative Adversarial Networks, that's their basic principle. You're not classifying just text, you're classifying entire web pages. reply TeMPOraL 1 hour agorootparentI'd say the whole point of GAN is that generation is cheaper than classification, therefore an effective brute-force way of making a good classifier is to generate an infinite supply of examples with a-priori known classification, and pit it against a classifier. reply itissid 4 hours agoparentprevSounds a bit more like you want to do something reranking-ish. Ideally, you would train a retrieval system to retrieve the most relevant pages which would inturn have been trained on a dataset not very different from MS-Marco. This would get you a small set of documents you want to rerank. For reranking to be able to detect commercial bias, insincerity or bloat you could use LLMs but IIRC you train a multiclass classifier for each and then combine the probabilities for each head(calibrate too?) into a score and use it in your ranking as weights? reply fxtentacle 3 hours agorootparentI think Kagi should add a feature where I can subscribe to the domain blocks of someone else. Every time I see a spam blog, I can easily prevent the domain from polluting future results. But it'd be great if I could also use my friends lists to rank their blocked domains to the end of my search results. reply anguspmitchell 5 hours agoparentprevI always assumed LLMs would result in more bloated content (stupid interns) but I think you’re right that it’ll lead to more efficient prioritization (hooray interns) reply mvkel 9 hours agoparentprevWe don't need to. We have LLMs now reply frabjoused 9 hours agorootparentI still would like to search and not get the dog crap that is Google's Internet, in addition to using LLMs myself. reply szszrk 8 hours agorootparentI know it's the other way around of what you are suggesting, but I feel I'm using Kagi for a while for the same reasons, with results you expect. Their search is much, much cleaner, that's for sure. But what made me stick (and mostly ditch DDG, which btw is also much cleaner than google), was how well their fastgpt works as a search tool. Summaries are very good, it includes recent events and news, it goes through pdfs, always cites it's sources. Does hallucinate for me sometimes, but I always can tell it's incorrect by the response itself. Plus it usually gives me links that easily clear out the confusion. Especially in IT field I can tell I'm fed with the source of my trouble (like initial GitHub issue that introduces broken functionality, source pdf of a study) and less discussion around it. Their search has some neat features as well, as you can simply choose to see less/no results from given site straight from list of search results. reply dmje 3 hours agorootparentYes - and the other killer feature in Kagi is being able to uprank your own choice of sites, and set contexts for this upranking. That to me is the killer thing about it reply voisin 8 hours agorootparentprevI agree. My issue with LLMs is that it isn’t clear when it is hallucinating versus when it isn’t. reply docandrew 7 hours agorootparentIt’s always hallucinating… but sometimes the hallucinations are of things that actually happened. reply TexanFeller 6 hours agorootparentprevSwitch to Kagi, IMHO it's worlds better than Google. It's worth the price, I get a lot more out of it than my Netflix subscription. reply komali2 8 hours agorootparentprevNon LLM efforts include alternative and even self hosted search engine indexes. I'm also curious if brave search's concept of \"goggles\" could work out, where you can write your own indexing logic and share it with others. reply frabjoused 7 hours agorootparentWe don't need self-hosted alternatives (just like the computer market doesn't need Linux tinkerers) as much as we need a real formidable competitor to dethrone Google and create proper website incentives so the Internet stops sucking. We need SEO companies to realize they will go out of business if they continue to generate crap filler content for their clients. We know Google won't do it. We need influence. We need effective results. Self-hosted is cute but ineffective at best, selfish at worst. reply A4ET8a8uTh0 6 hours agorootparentHmm, I reflexively disagree, but I disagree even after considering it from other different positions. Need is a strong term and it is likely doing a lot of work in that claim. We, technically, do not need much bar food, water and shelter. In that sense, the post is absolutely correct. Realistically, there is zero need for self-hosting, or linux, or anything much really. But even if we get past the need claim, why is setting up a duopoly a preferred option to people actually running their own preferred setups ( and maybe even learning something in the process )? More importantly, why on earth would I want yet another giant corporation in charge of my digital life?The only future of the internet is, sadly, proof-of-person and proof-of-residence on every public network interaction. I really hope we do not give up the internet's freedom as they suggest (and I doubt this would solve the spam problem). reply fxtentacle 3 hours agoparentAs much as I hate the idea, I'm convinced it would be a godsent for 99% of the population. Just imagine when you buy your SIM card the phone shop asks you: Do you want to limit incoming calls to people who you either called before or who have ever had a permanent residence in your country? 99% of spam and scam calls blocked, just like that. And just imagine how hilarious it would be if all those Nigerian prince emails had a note that says \"actually, the sender of this email has never been to Nigeria\" reply sebtron 2 hours agorootparent99% of the population is also happy with carrying a spyware device everywhere and allowing Google to know everything you do in order to serve you better ads. I am not saying this does not have upsides, but it would be a nightmare to have it imposwd on you. reply groestl 2 hours agorootparentprev> actually, the sender of this email has never been to Nigeria Conversely, what if all those emails actually originate from Nigeria? Would it make them more legit? reply anovikov 2 hours agorootparentThey actually indeed originate from Nigeria almost all of the time. reply groestl 2 hours agorootparentThat's what I'm hinting at :) reply zandrew 2 hours agoparentprevAgree, as much as we hate AI-generated content, what’s to say that the content isn’t helpful to some in some instances? Also, as long as search engines do their job, engagement on high quality pieces will always justify having a human write art reply porksoda 6 hours agoprevHow bad does it have to get before it stops working, I mean like phone a friend brittanica yellow pages bad? Quicker and more deeply we get there, the more effort and traction gets behind some breakout solution. Maybe it's just pure cynicism: burn it all. Remember email spam? It got so bad, that we fixed it. I mean email has its issues and how but spam isn't one of them. I built a spam juggernaught in my day (got bills don't I :)) and I feel like I contributed a tiny bit to our almost-spamless latter days. Progress! The world is on the march. reply jprete 6 hours agoparentI find personal email to be almost useless for anything other than mailing lists and online shopping messages. I don't consider it fixed; we already left it to burn. reply parpfish 6 hours agorootparentI use email constantly but I can’t tell you the last time I saw actual spam in my inbox Sure, there are occasional unwanted marketing emails but those are easily dispatched with unsubscribe and/or inbox filters reply brabel 3 hours agorootparentDepends on your provider. My GMail is fine, but my much older Hotmail is a spam cesspool. It's so bad I barely check it anymore the last couple of years, despite having been able to keep that email address since something like 1996. As I had \"exposed\" that to some email farms I probably got into some really bad spammers lists which Microsoft seems unable to stop. reply TeMPOraL 1 hour agorootparentprevSpam evolved. The filters catch most of the egregious kind, but our mailboxes are still flooded with the rest: marketing of legitimate companies, which you likely interacted with at some point, however briefly, and often mixed into transactional messages. This consists the majority of most people's inbox; I bet it's the case for you too. reply hamasho 8 hours agoprevI often google Reddit posts to find products or services. Posts with a lot of comments usually have better solutions. I also check out YouTube reviews, especially ones with few dislikes, using addons like \"Return YouTube Dislike.\" These methods are easy and pretty reliable. But it means the platform will soon be flooded with bot-generated spam. reply keyboardJones 8 hours agoparentOnce I heard of ReplyGuy, I knew it was the beginning of the end for forums like Reddit reply germinator 8 hours agorootparentIt's actually one of the perks of centralized platforms like Reddit: if they want to, they have the technology and the resources to investigate bad actors like that. I don't just mean just finding and blocking their IPs, but untangling their ownership, corporate structure, tooling, and so on. Hiring actual PIs if necessary. It's the bread-and-butter for many spam and abuse teams at Big Tech. That said, just because they can doesn't mean they will. It's possible that they've grown complacent and underfunded these capabilities (instead relying on community moderators to weed out bad actors). Or it's possible that they're too focused on the short term to see the existential risks. If I recall correctly, they couldn't resist the temptation of selling user content for LLM training, same as Stack Overflow. But in an internet overrun by spam LLMs, the future are curated, walled-garden communities, and Reddit could be the basis for that. reply IncreasePosts 6 hours agorootparentAnything which diminishes their usage numbers won't look good for investors, so one should assume no action will be taken by the company which diminishes their usage numbers. reply 256_ 7 hours agoprevSometimes I think about whether a nation-state may have an incentive to solve this problem. They could adopt a strict anti-SEO spam policy for their ccTLD and run a publicity campaign telling people about it, so they'd filter search results to that ccTLD, with the purpose of improving the country's reputation (by associating it with high-quality search results). That sounds kind of far-fetched, but it's hard to think of anything else. Maybe you could direct the blame at CAs too. For now, at least we can still put \"before:2020\" in our search queries. reply BobbyTables2 9 hours agoprevI’m amazed my medical websites… The top 5-10 sites all seem have identical wording for common conditions. In the age of copyright enforcement and DMCA antics, I don’t understand how this continues year after year. reply geerlingguy 7 hours agoparentI worked in a hospital system for a time—those documents are often integrated through a medical system where the hospital pays a recurring subscription, and the site developers just plug things through an API to display the documents from the central medical system. Thus, you have one document that's identical across dozens or hundreds of different hospital and medical system websites. A long time ago, I held more weight in results from reputable places like Mayo Clinic, but even their site seems to be the same as all the others now. reply DavidPiper 5 hours agoparentprevA family member works for a company that writes and publishes a mandatory handbook for GPs in Australia. One day the WHO found them and wholesale copied large chunks of their content verbatim into their online resources without asking or informing. They only found out because THEY went looking on the WHO website for the latest information on something. At that point I started wondering just how much of big companies' work and content is just plagiarism and gratuitous theft from reputable but less visible or popular sources (also highly dependent on country, language, etc). And that was before I discovered hbomberguy on YouTube. reply grugagag 9 hours agoparentprevThey’re all the same thing, same owners likely reply greenavocado 8 hours agoparentprevAll major public facing organizations are slowly bought out by extremely wealthy and influential groups when they get big enough for narrative control purposes because ultimately mind control is the largest source of wealth and power. reply roughly 9 hours agoprevThe reason all of this exists - the reason the entire Internet turned into an unusable amalgam of horseshit - is because we built the entire commercial side of the internet off ad revenue. It might be a death spiral at this point - I can’t imagine anyone actually being willing to Pay for whatever the fuck Google or Facebook have become, so there’s nothing left to do but keep inventing new ways to generate bullshit and new bots to view it for you. reply janalsncm 8 hours agoparentWhatever model of the internet you have isn’t going to disrupt affiliate links. And as long as you have affiliate links, you will have websites trying to game the rankings to get their affiliate links in front of eyeballs. It’s not just a Google problem. reply caseyy 9 hours agoparentprevThe internet is the largest and most elaborate monument to marketing we may ever build. reply flir 8 hours agorootparentYe of little faith. Some day we'll project that Coca-Cola logo on to the moon. reply the_sleaze_ 8 hours agorootparentsomeday we'll sell the edges of our peripheral vision for ad space in lieu of a universal basic income reply TacticalCoder 7 hours agorootparent> someday we'll sell the edges of our peripheral vision for ad space in lieu of a universal basic income Meta and RayBan are working on that and not far: the RayBans \"Meta take a picture\" / \"Meta take a movie\" are a thing. And I doubt very much that it's done with good motives. No HUD yet but they've already got a microphone to analyze your voice (for good reasons, we're sure) and speakers. reply notpachet 8 hours agorootparentprevAren't we already doing that? reply nicbou 2 hours agoparentprevI write really detailed content for a living. So long as no one pays for it, I must live through other means. Fortunately, I don’t need to peddle stupid products, but I can’t disentangle myself from affiliate partnerships without state fundi, and that presents its own challenges. We have the internet we pay for. reply acscott 9 hours agoprevThe equation to rank links is a poor substitute for an editor. Editors used to be the gateway of information that was disbursed via newspaper, radio, and TV. They were not without fault. So what is the least faulty filter? (Even your own brain, eyes, and ears are faulty, sorry perfectionists.) You cannot trust video or audio now due to deepfakes. What can you trust as a source of information more than 50%? There's still information in the noise, you have to become your own editor. reply rileymat2 4 hours agoparentGoogle originally used back links in the equation; where the goal is to take the information by editors on a massive aggregate to rank. Brilliant until gamed. reply EVa5I7bHFq9mnYK 4 hours agoprevTime to go back to Yahoo! search engine with manually added and verified links. Automated web crawling experiment has failed. Anyway, whatever I search, I end up in either reddit or wikipedia (incidentally, both are human-curated stores of knowledge). reply brabel 3 hours agoparentI don't know about Wikipedia, but Reddit is full of bots. They're sophisticated enough you may not recognize some comments are not from a human though. That's worse than when it was always obvious. reply necovek 6 hours agoprevWhile I understand the frustration of the author in searching the web today (I do too), I am not sure how they get to: > We end up with real time global access from “low trust low income max exploitation because who is going to stop us” wankers to “high trust high income low questioning” societies and everything falls apart. Obviously, there are \"low trust high income max exploitation\" folks doing the damage too, as one of the linked articles talks about frauds with wiring large sums (in 100ks of dollars) into Hong Kong, which is itself an expensive city. Similarly, from the other direction, you've got high-income communities \"exploiting\" lower-income communities by getting cheaper labour (like getting things produced in China) — paying less than they would locally for the same or larger work effort. The solution to this is obviously global equalization of salary bands, which is well under way due to an ability to do a lot of highly paid work remotely and globalization in general — but it will take some time (and it's also why China is becoming less appealing in particular: salaries are going up there as well). But that will lead to a new set of problems altogether. reply WarOnPrivacy 9 hours agoprevAll of those sites are labeled as \"A Venture 4th Media Company\" which has such wondrous important content all run by the same \"writer\" on: https://mtbinsider.com/author/jodie-chiffey/ https://turfandtill.com/author/jodie-chiffey/ https://www.betterwander.com/author/jodie-chiffey/ https://artofgrill.com/author/jodie-chiffey/ https://theathleticfoot.com/author/jodie-chiffey/ https://altprotein.com/team-members/jodie-chiffey/ https://digitalguyde.com/us/ https://total3dprinting.org/author/jodie-chiffey/ Each of which is registered by NameCheap, who can never seem to kick their addiction to bottom feeders. Each of which is behind Cloudflare, the official latrine of planet dysentery. disclosure: I use too. reply seanhunter 2 hours agoparentWhat's interesting is I keep my old blog up even though it is basically never updated. I receive 3 or 4 emails a week from people like Jodie Chiffey saying they would like me to consider hosting a \"guest article\" by them about some topic or other. The content and SEO mill is absolutely gigantic. reply udev4096 5 hours agoprevI have never come across \"AI generated\" content in duckduckgo reply api 9 hours agoprevThis kind of spam was already destroying the web before LLMs came along. Now it’s being accelerated by thousands of times. Mainstream web search is probably cooked. Kagi and other niche players might have a chance if the fact that they are not beholden to advertisers lets them introduce features to do things like downrank content with ads. Kagi has “small web” which I think includes this in its weighting. Open social media is probably cooked too. In the future it’s going to require proof of human identity and will be more heavily moderated. The future of social is closed forums. Even those are really having to fight bots though. The other pervasive awful trend of the moment is everything becoming like John Deere tractors: cloud connected, DRMed, with subscriptions and/or planned obsolescence. Capitalism is supposed to reward people for creating value, but today it seems like it’s far easier and more profitable to just extract rent or scam. I am not sure how to fix this. reply ChrisMarshallNY 9 hours agoparent> Even those are really having to fight bots though. We recently released a closed-system, iOS-only app, that has a fairly rudimentary, privacy-first system of user registration. It is currently restricted to the US and Canada. Each signup request is manually vetted. There is no automatic registration. The app is designed for a specific demographic, and we do our best to ensure that new accounts are real people, that fit the demographic. This is an iOS-only app (free), restricted to the North American continent, and with no accessible server API. The server is a bespoke server, and has no connections or dependencies that we don't control. We are flooded with bots, and, most likely, scammers. So far, they have been pretty easy to spot, but that could change. reply illusive4080 7 hours agorootparentWhat app and serving whom? reply ChrisMarshallNY 1 hour agorootparentI won’t mention it here. Like I said, each signup is individually vetted, and the last thing we need, is hundreds of curious geeks, signing up one-shot accounts. It’s for addicts, seeking Recovery. We’re not interested in scale; only quality. People’s lives can depend on it. The bots have declined, recently. I suspect that there’s a watcher bot, that triggers on new apps. When it first came out, we had a lot. reply munificent 9 hours agoparentprevThe hyper-enshittification stages look like: 1. People get on the web and make real content with care because they're just excited to share stuff with each other. 2. Advertisers talk them into putting some ads on their pages so they can get some compensation for their work. 3. Shitty people figure out you can just make content where the main incentive is to get people to go to the page and see the ads. 4. Those people then outsource writing the content to the lowest bidder. 5. The lowest bidder becomes an LLM. 6. Search engines cut out the middle-man entirely and just send your search query to an LLM, stuff some ads in, and show the result to the user without ever hitting the web (except to periodically scrape it for model training). 7. Because of 6, people stop putting new content on the web at all. The models get shittier and stupider with regards to current events. 8. To counter that, LLM companies make deals with news organizations and other primary source information provides and pay them to have direct access to content to train their models. 9. Those organizations get such a large fraction of their income from those deals that eventually they get out of the business of giving human readers direct access to it because it's not worth the effort. Newspapers become B2B companies. 10. The only way to get information is via a handful of giant tech companies sitting on top of huge LLMs saying who-knows-what trained on a slurry of actual information and giant piles of ads. I hope that somewhere in the process people start to get tired of talking to machines all day and hop off the ride entirely and starting calling up their friends and getting information the old fashioned way. The only consolation I have is the belief that people have a deep seated desire to connect to actual humans and know the real truth about the world. reply api 8 hours agorootparentIMHO a major part of the problem is that the Internet never had a mechanism for paying for good content. Everything is “free” therefore ads emerge as the only monetization strategy and you did a good job outlining the rest. I’ve started trying to pay for good journalism, especially good indie journalism. I also Patreon a bunch of podcasts, buy high quality software if the price is reasonable, buy albums of my favorite music, buy films, and so on, while actively avoiding both gratuitous subscription models and the ad web. Pay for it or it either doesn’t get made or it pays for you. Free is a lie and piracy undermines quality. Edit: All the paying for good stuff I outlined above averages out to around $100-$150/month. It’s less than I usually spend on restaurants and coffee shops and far less than groceries for our family. Restaurants in particular feel like a far more frivolous expense. reply eszed 7 hours agorootparentWho are some indie journalists you've found to be worth paying? I'd like to find some more work that's worth supporting. reply andrethegiant 6 hours agorootparentAndrew Callaghan of Channel 5 News reply BriggyDwiggs42 5 hours agorootparentHe makes good stuff, but do the allegations give you any pause? reply Terr_ 9 hours agoparentprevI think we'll end up moving away from shallow signifiers of trustworthiness to reputational networks and evidence someone has invested into an identity or entity. That's not to say it's a solved problem, even in the real world with thousands of years of battle tested strategies. A very simple example would be a web browser where I could blacklist chronically-unhelpful sites, and share that metadata among friends. reply worstspotgain 9 hours agoparentprevYou're assuming that LLMs will boost spam and scams more than LLMs will tone them down by marginalizing them automatically. I have the opposite view. SMTP spam used to be mostly unavoidable, then one day it was not. IMO, the number of engineers and moderators needed to offset one scammer is about to take a huge dive. reply seanhunter 1 hour agorootparentYour analysis overlooks a massive cost asymmetry[1] in favour of the spammer. THey only have to pay the LLM cost once to generate a message which they can use thousands/millions of times versus the receiving side would need to pay an LLM to check and classify each incoming message. [1]Either you pay a SaaS LLM provider or you pay the cost of compute to run the LLM yourself reply lupire 8 hours agorootparentprevWhat information led to your opinion? reply worstspotgain 8 hours agorootparentI think LLMs have higher complexity in the white-/black-hat \"struggle domain\" than the weakest-link problem, maybe O(N^2) vs. O(NlogN). So if it used to be O(N) white against O(NlogN) black, it's now evenly matched, with O(N^2) LLMs on both sides. reply keyboardJones 9 hours agoparentprevMy strategy been to put my money where my mouth is and start paying for services that provide value to me (Kagi is one example - I’m a paying customer, and actually found this article using their small web site) reply cal85 3 hours agorootparentStrategy for what? reply raymondgh 9 hours agoprevMaybe a social endorsement scheme could be an alternative to an invasive proof system. reply yen223 9 hours agoparentIf there's a social endorsement scheme that doesn't involve real-world identity, I don't see how you won't end up with a situation where Bot_A is counted as legit, being endorsed by Bot_B...Bot_Z reply ms-menardi 9 hours agorootparentbecause once Bot_J and Bot_Q are discovered to be bots, then everyone they've authenticated suddenly needs new people to vouch for them. problem is, how do you figure out someone's a bot? Ah well maybe someone will make an AI tool for that, we humans are too busy doing groceries and putting round cubes into square holes. reply raymondgh 9 hours agorootparentprevThat kind of verification could be off-protocol. Perhaps a reputation or incentive system could help as well. reply datadrivenangel 6 hours agorootparentMission fucking accomplished. [0] 0 - https://xkcd.com/810/ reply vineyardmike 5 hours agoparentprevLike maybe a website is considered better if it is linked to by other websites? Kinda like research papers... We could call it \"Page Rank\". I bet we could get a major university (eg. Stanford) to help fund the initial deployment. I think we should call it a really big silly number, like \"Gazillion\" to emphasize how much it knows. Obviously this won't have ads, and it should make an explicit point to not be evil.... Hmm. I think I've heard this before. reply 256_ 7 hours agoparentprevCAs tend to revoke a domain's certificates if it's used for crime. Maybe they should do the same for SEO spam. reply friend_and_foe 6 hours agoprevI was nodding my head as a member of the choir until > The only future of the internet is, sadly, proof-of-person and proof-of-residence on every public network interaction Yeah, go away. The problem is that Google has monopolized web aggregation. Without it these sites wouldn't be worth making. I've got to show ID to post something online to kick that can down the road? The way I see it this is a self resolving problem. Ad driven search engines rank ad driven blog spam, people get tired of it and use methods of finding content that don't show that garbage, these guys, along with their multinational trillion dollar benefactor, go out of business. Problem solved. reply vineyardmike 5 hours agoparent> Yeah, go away. The problem is that Google has monopolized web aggregation. Without it these sites wouldn't be worth making. So how will you find things? > Ad driven search engines rank ad driven blog spam We don't have actual concrete proof that Google (or others) rank content with ads higher because of the ads. As a contrary corollary, generally as society grows, we've seen an increase in \"proof-of-person and proof-of-residence on every public X\". Want welfare checks or charity, prove yourself. Want to buy cough syrup or booze, prove yourself. Want to drive, want to shoot a gun, want to XXXX.... get an ID. In early America, men used to vote by everyone going into a big room and shouting for a while (some minor exaggeration). Now you have to register in advance and show ID, and they maintain registries of everyone and their affiliated party. Showing identity comes with a lack of trust, and volume + anonymity decreases trust as it's slowly abused. reply friend_and_foe 3 hours agorootparentLook, there's a difference between showing ID to vote or get a free paycheck from someone and showing ID to shout my ideas from a rooftop. I don't need your trust to say things on the internet. If you don't trust me, don't read what I have to say. You can continue trusting Google though, I won't, this is a bait and switch and they're the source of the problem, not your lack of my identifying documents. We have no proof that they're doing it in purpose, I don't care about the intent, I care about the results. How do I find things. I'm already living life without much google in it. There are a lot of ways to find things. Aggregators like this one have a better signal to noise ratio than google or most places that publish a lot of information. There are search engines that actively blacklist anything with SEO in it. There are community groups that focus on topics of interest. I find that I only use big search engines nowadays to find a git repo for something or find out what time some place closes, that's all they're good for nowadays. I trust people more than faceless services, and I don't care anything about who any of those people are in real life. reply __MatrixMan__ 9 hours agoprev> The only future of the internet is, sadly, proof-of-person and proof-of-residence on every public network interaction. I'm going to be that pedant and point out that the Internet is not the same as the Web, and it's the Web that's sick. The Internet is fine. It's a distinction that matters because the Internet is expensive things like satellites and undersea cables. It's an investment that's too large to just walk away from, so perhaps its future is our future. The Web is just a bunch of conventions about how to use the Internet, it's not binding in any way. We can write a different protocol without laying new cable, we can make it less profitable for abusers, and then we can abandon the sick version that we're currently using. reply caseyy 8 hours agoparentYes, but we all get what the author means. UDP or TCP, SMTP or HTTP — most of it is transporting low value sludge whose purpose is to exploit its consumers. It involves everything from somewhat benign forms of surveillance/profiling to aggressively malicious scams. You could use the infrastructure for better and many people do. But most of the content on the internet isn’t that. reply jauntywundrkind 8 hours agoparentprevThe rest of the internet has even less reputation & ability to assess than the web. Nothing else has links worth a damn, and links while fakeable also do say something sometimes. reply lupire 8 hours agoparentprevThe post you replied to didn't say anything about the Internet being sick. reply dalemhurley 9 hours agoprevGoogle has an incentive to keep these links going. Google makes money by you continually returning back to Google. If you get the prefect result on your first click, then you move on with your day. If you get junk, then you go back to Google and click the next junk link and repeat. Each time someone visits one of these content farms and returns back to Google, then it is more advertising dollars for Google. reply munificent 9 hours agoparentThis is a classic argument and appears in many many forms: * \"Psychologists don't want to fix your problems because then you'll stop needing therapy every week.\" * \"Dating sites don't actually want you to find a long-term partner because then you'll stop using the site.\" * \"The mechanic's not trying to actually fix your car, just get it running for a few weeks so it breaks down again and you come back.\" Etc. etc. Any time there is information asymmetry and leaving a customer not fully satisfied might lead to future sales, this old canard comes up. I'm sure in some cases it's true. But, like, people aren't entirely stupid. Consumers generally won't keep repeatedly going back to the same business if the service is kinda sucky. And businesses generally figure out that reputation matters and the most economically viable long-term strategy is just to give people what they want. reply lmm 8 hours agorootparent> But, like, people aren't entirely stupid. Consumers generally won't keep repeatedly going back to the same business if the service is kinda sucky. And businesses generally figure out that reputation matters and the most economically viable long-term strategy is just to give people what they want. This isn't a law of nature. It's the result of particular conditions. Businesses in high-trust and low-trust cultures behave differently, and the descent of the US from a high-trust to a low-trust culture is going to have consequences. reply lupire 8 hours agorootparentHigh trust? The US sold literal \"snake oil\" reply lmm 7 hours agorootparentThe Wild West times were low trust. The New Deal era was high-trust. We're swinging back to low trust. reply Towaway69 9 hours agorootparentprevDon’t forget that rubbish cleaners throw the most rubbish on the streets. If there would be no rubbish on the streets, they would be out of a job. Now think crime and police: without crime, the police would be out of a job. A consultant has no interest that the project they are consulting on is ever completed. Of course these examples aren’t to be taken seriously, they merely illustrate some potential conflicts of interest of roles within society. reply roughly 9 hours agorootparentprev> long-term strategy This is where the theory falls apart. When “long-term strategy” and “short-term quarterly earnings” get into the boardroom together at a public company, it ain’t “long-term strategy” that’s walking out. reply vineyardmike 5 hours agorootparentIt can be though. \"Short Term\" tends to win when CEO pay is contingent upon hitting certain share price at certain times. Founders tend to not have this issue, and I'm sure other CEOs can have pay packages crafted this way if we desired. reply roughly 5 hours agorootparentThere’s nothing stopping it except that investors solely want short term gains and are rewarding CEOs and management for delivering that and punishing them when they don’t. reply giantg2 9 hours agorootparentprevIt's not necessarily that Google or whoever is doing it on purpose. It might just be that Google gets lazy because rhe money is still coming in. Or the psychiatrist, chiropractor, etc actually believes they are helping you and feel they can continue to be useful (or don't want to turn you lose too early in case of a bad consequence). There's all sorts of unintentional stuff that can still result in a bad outcome that seems predatory. reply xyzzy123 9 hours agorootparentprevBoth things can be true at the same time, it depends on the time preference of the business owner. reply WarOnPrivacy 9 hours agorootparentprev> this old canard comes up. Your examples 1 & 3: I have personally witnessed those negative outcomes. Regarding 3, A/C repair shops (drain system first, then discuss pricing) and transmission shops (disassemble first, then discuss pricing) are kind of notorious for it. And yet there are mechanics and therapists who have earned my unquestioning trust. I've not used 2. I may have bias. reply m463 6 hours agoprevI am a dynamic figure, often seen scaling walls and crushing ice. I have been known to remodel train stations on my lunch breaks, making them more efficient in the area of heat retention. I translate ethnic slurs for Cuban refugees, I write award-winning operas, I manage time efficiently. Occasionally, I tread water for three days in a row. I woo women with my sensuous and godlike trombone playing, I can pilot bicycles up severe inclines with unflagging speed, and I cook Thirty-Minute Brownies in twenty minutes. I am an expert in stucco, a veteran in love, and an outlaw in Peru. Using only a hoe and a large glass of water, I once single-handedly defended a small village in the Amazon Basin from a horde of ferocious army ants. I play bluegrass cello, I was scouted by the Mets, I am the subject of numerous documentaries. When I’m bored, I build large suspension bridges in my yard. I enjoy urban hang gliding. On Wednesdays, after school, I repair electrical appliances free of charge. I am an abstract artist, a concrete analyst, and a ruthless bookie. Critics worldwide swoon over my original line of corduroy evening wear. I don’t perspire. I am a private citizen, yet I receive fan mail. I have been caller number nine and have won the weekend passes. Last summer I toured New Jersey with a traveling centrifugal-force demonstration. I bat 400. My deft floral arrangements have earned me fame in international botany circles. Children trust me. I can hurl tennis rackets at small moving objects with deadly accuracy. I once read Paradise Lost, Moby Dick, and David Copperfield in one day and still had time to refurbish an entire dining room that evening. I know the exact location of every food item in the supermarket. I have performed several covert operations for the CIA. I sleep once a week; when I do sleep, I sleep in a chair. While on vacation in Canada, I successfully negotiated with a group of terrorists who had seized a small bakery. The laws of physics do not apply to me. I balance, I weave, I dodge, I frolic, and my bills are all paid. On weekends, to let off steam, I participate in full-contact origami. Years ago I discovered the meaning of life but forgot to write it down. I have made extraordinary four course meals using only a mouli and a toaster oven. I breed prizewinning clams. I have won bullfights in San Juan, cliff-diving competitions in Sri Lanka, and spelling bees at the Kremlin. I have played Hamlet, I have performed open-heart surgery, and I have spoken with Elvis. But I have not yet gone to college. - well known missive from decades ago reply neilv 6 hours agoparentThanks, I've not seen that college application essay in ages, and it's written better than most of what I've read since. https://archive.blogs.harvard.edu/sj/i-am-a-dynamic-figure/ reply ggm 9 hours agoprevBuries the lede. WHY would you make so many talanted instances of Jodie? Whats the upside. How do the financials work and is it a stake, or paid labour? reply roncesvalles 8 hours agoparentBlogspam sites are made by the exact same kind of \"hustle bros\" who run dropshipping companies. The business model is very simple and requires 3 ingredients: web design, SEO, and copyediting. The first two are one-time costs. The 3rd one is a COGS and there is a whole market of professional blogwriters who charge something per 1000 words. To answer your question, once you create a blog that starts printing money, it's in your best interest to just replicate it while changing as little of the \"template\" as possible, because you don't really know what element made it click. reply ggm 8 hours agorootparentSo if Jodie the expert in fishing works, you try cricket, music, cabbage-patch dolls on the assumption .. the ideation of Jodie worked? Feels like a big assumption that the magic was Jodie, and not the context of fishing. Ie your \"you don't really know what element made it click.\" above assumed Jodie was the fixed value, no matter what else. That's what I don't get: I click on lego spam because I like lego, not because Jodie is cool and I love his wordsmithing. That follows on. I'm probably being thick. Maybe it's because to the recipient Jodie is unique each time? It's \"this is the least important part of it\" so they don't change it because IT DOESN'T MATTER. reply WarOnPrivacy 9 hours agoparentprev> WHY would you make so many talanted instances of Jodie? One name is twice as easy to invent as two names. reply lupire 8 hours agorootparentChoosing a name isn't the bulk in the cost structure. reply adolph 8 hours agoparentprev> WHY would you make so many talanted instances of Jodie? Because Jodie is everywhere and got your girl back home https://taskandpurpose.com/military-life/brief-history-jody-... reply porksoda 6 hours agoprev [–] How bad does it have to get before it stops working, I mean like phone a friend brittanica yellow pages bad? Quicker and more deeply we get there, the more effort and traction gets behind some breakout solution. Maybe it's just pure cynicism: burn it all. Remember email spam? It got so bad, that we fixed it. I mean email has its issues and how but spam isn't one of them. I built a spam juggernaught in my day (got bills don't I :)) and I feel like I contributed a tiny bit to our almost-spamless latter days. Progress! The world is on the march. reply noobermin 6 hours agoparent [–] Did you accidentally post the same thing twice or are you making some meta point on spam. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Jodie Chiffey is a versatile expert in 3D design, grilling, guitar playing, outdoor gear reviews, RV travel, and beer brewing, sharing knowledge through blogs to assist others in enjoying these hobbies.",
      "The article raises concerns about the authenticity of Chiffey's content on various sites, highlighting the existence of fake tech blogs and affiliate link bot farms influencing search rankings for financial gain.",
      "Emphasizes the importance of transparency and trustworthiness online to prevent exploitation and scams, underscoring the significance of genuine and reliable content."
    ],
    "commentSummary": [
      "Hacker News discusses challenges in using language models to combat web spam, raising doubts about their effectiveness.",
      "Topics range from failed AI classifiers to manipulation concerns in search results, impacting internet content quality and the rise of low-quality content.",
      "Suggestions for fighting spam include alternative verification methods, reputation systems, and a \"Page Rank\" implementation."
    ],
    "points": 222,
    "commentCount": 122,
    "retryCount": 0,
    "time": 1715729845
  },
  {
    "id": 40355982,
    "title": "Firefox Enhances Search Data Collection While Prioritizing User Privacy",
    "originLink": "https://blog.mozilla.org/en/products/firefox/firefox-search-update/",
    "originBody": "Firefox See what’s changing in Firefox: Better insights, same privacy May 14, 2024 Mozilla Innovation and privacy go hand in hand here at Mozilla. To continue developing features and products that resonate with our users, we’re adopting a new approach to better understand how you engage with Firefox. Rest assured, the way we gather these insights will always put user privacy first. What’s new in Firefox’s approach to search data To improve Firefox based on your needs, understanding how users interact with essential functions like search is key. We’re ramping up our efforts to enhance search experience by developing new features like Firefox Suggest, which provides recommended online content that corresponds to queries. To make sure that features like this work well, we need better insights on overall search activity – all without trading off on our commitment to user privacy. Our goal is to understand what types of searches are happening so that we can prioritize the correct features by use case. With the latest version of Firefox for U.S. desktop users, we’re introducing a new way to measure search activity broken down into high level categories. This measure is not linked with specific individuals and is further anonymized using a technology called OHTTP to ensure it can’t be connected with user IP addresses. Let’s say you’re using Firefox to plan a trip to Spain and search for “Barcelona hotels.” Firefox infers that the search results fall under the category of “travel,” and it increments a counter to calculate the total number of searches happening at the country level. Here’s the current list of categories we’re using: animals, arts, autos, business, career, education, fashion, finance, food, government, health, hobbies, home, inconclusive, news, real estate, society, sports, tech and travel. Having an understanding of what types of searches happen most frequently will give us a better understanding of what’s important to our users, without giving us additional insight into individual browsing preferences. This helps us take a step forward in providing a browsing experience that is more tailored to your needs, without us stepping away from the principles that make us who we are. What Firefox’s search data collection means for you We understand that any new data collection might spark some questions. Simply put, this new method only categorizes the websites that show up in your searches — not the specifics of what you’re personally looking up. Sensitive topics, like searching for particular health care services, are categorized only under broad terms like health or society. Your search activities are handled with the same level of confidentiality as all other data regardless of any local laws surrounding certain health services. Remember, you can always opt out of sending any technical or usage data to Firefox. Here’s a step-by-step guide on how to adjust your settings. We also don’t collect category data when you use Private Browsing mode on Firefox. As far as user experience goes, you won’t see any visible changes in your browsing. Our new approach to data will just enable us to better refine our product features and offerings in ways that matter to you. We’re here to make the internet safer, faster and more in tune with what you need – just as we have since open-sourcing our browser code more than 25 years ago. Thanks for being part of our journey! Get Firefox Get the browser that protects what’s important",
    "commentLink": "https://news.ycombinator.com/item?id=40355982",
    "commentBody": "Firefox search update (blog.mozilla.org)216 points by murillians 18 hours agohidepastfavorite218 comments romanows 18 hours agoSeems like this is in support of \"Firefox Suggest\", which seems to show sponsored links. I really don't like advertising so I really don't like going down that road. Can't they just take Google's yearly $600,000,000 payment and build the best browser \"for the user\" while also addressing technical debt and organizational issues so it can continue as an open source project if/when the money ever dries up? reply aquova 17 hours agoparentI constantly see this take and I'm afraid I don't agree with it. Firefox is continuing to lose market share, and I think it's less anything they're doing and due simply to the fact that Google is a household name while Mozilla is not. When the user is already using a Google phone, email, search, maps, drive, and document editor, it follows to also use their browser. Simply being a solid browser isn't enough to motivate people to switch away. So I think they should try out some new interesting ideas in the hope that some of them take off. Now, I don't think this is the feature to do that, but I won't criticize them for trying. reply JeremyNT 16 hours agorootparentI agree with this take. It's not like the power users who currently use Firefox and yet dislike this stuff are going to switch away from Firefox, since there's nothing to switch to other than Chrome, which is clearly worse. Personal anecdote, I didn't like Pocket being added to Firefox, but eventually I did start using it - only because it came from Mozilla. And I currently pay Mozilla money for Relay (along with VPN), which are examples of them expanding outside of their core browser features. reply Sammi 8 minutes agorootparentI will switch away from Firefox to Ladybird as soon as Ladybird is half as good as Firefox. reply JumpCrisscross 15 hours agorootparentprev> not like the power users who currently use Firefox and yet dislike this stuff are going to switch away from Firefox Not a Firefox power user, but also not a fan of having to learn through HN that I need to disable some random opt-in to not have my browsing leaked by my browser. Was genuinely on the fence about deleting it and simplifying to Safari. reply threecheese 7 hours agorootparentIMO this holds Firefox to an impossible standard; it is vanishingly unlikely that Safari doesn’t perform this same type of user research and data collection, for search utilization wrt their relationship with Google and for other features of the browser. Your point about being surprised by this is certainly valid, if this is indeed opt-out/in by default. reply riedel 13 hours agorootparentprevI am a happy user of LibreWolf: they do the opt out for me. On android I use mull. I have to say that I am quite happy and really do not understand everyone moving to chromium based browsers. reply JohnFen 14 hours agorootparentprev> It's not like the power users who currently use Firefox and yet dislike this stuff are going to switch away from Firefox Sure they do. I know a lot that have, anyway. I'm one of a tiny percentage of my tech circle who uses it anymore. reply yjftsjthsd-h 16 hours agorootparentprev> When the user is already using a Google phone, email, search, maps, drive, and document editor, it follows to also use their browser. I think it's more that everyone used Google for search (because it at least was legitimately the best by a good margin) and then Google used that position to push Chrome. The other stuff may have helped, but I think search was 90% of it. reply ragnese 15 hours agorootparentWell, when Chrome first came out, it was definitely way faster than the other browsers for websites that had heavy JavaScript usage a la GMail. I imagine the gap has closed quite a bit, but I would suspect that it's probably still there, since they can coordinate development of both their web apps and the browser to work well together. More recently, there was that whole thing where YouTube was \"accidentally\" adding delays to non-Chrome browsers. reply merb 16 hours agorootparentprevActually I still use Firefox. But with stuff like that, it puts me rather off of it. Firefox isn’t better than chrome anymore and Mozilla acting as a Media Company and adding sponsored links and sponsors in their products is just not cool for a so called privacy focused company, pocket was already a strange thing and now that. reply xemdetia 16 hours agorootparentprevWhat I think I am waiting for from Mozilla is instead of this enrich the suggest experience just go full Yahoo like Microsoft did with Microsoft start. Make an organic 'good' home page, and let people who want to not use it switch it out and capture interest that way. Make links people can share so you can convince them to try Mozilla. You have to FIND Mozilla nowadays, and in comparison I look at what Opera has been doing lately in actively helping people find them. I am at least seeing a path for Opera to grow more than Firefox right now. reply Fatnino 6 hours agorootparentprevFirefox being solid is what got people to move off ie6 to ff1.5 And tabs. That was a cool idea. reply wakawaka28 8 hours agorootparentprevAt this point, most of the people still using Firefox desperately want a private browser. It would be nice for Mozilla to come up with innovative new features as they did in their heyday, but everything they have attempted for the last several years has broken beloved features and betrayed the privacy of users. reply saurik 16 hours agorootparentprevI just feel like they should be concentrating their time into adding features to Firefox which Google either will not or doesn't care to add to Chrome, not trying to compete with Google's entire business suite: that's a losing uphill battle and the pitch \"we're slightly more private\" is a hard one that they aren't even doing well... if they want to make a commitment to 0 data storage with no suggestions or analytics, that would be one thing, but they refuse to do that and so can't hold this narrative. Right now, Firefox's strategy seems to be focused on trying to follow in Google's footsteps and do everything they can to implement something almost as good as Chrome just without some--but not all!--of the extra things we hate about Google. The result of this strategy is there are simply way too few answers to the question \"why should I use Firefox instead of Chrome?\" that aren't \"because someone has to lest we lose the war, and it may as well be you (as I guess you drew the short straw today)\" :(. I want to be clear: these unique selling propositions can be really small. If you are using Linux on a computer with a touch screen, Firefox implemented good multi-touch with kinetic scrolling support for X11... it puts Chromium to shame, and so if you are using such a computer you are likely to use Firefox even if it is less performant or doesn't work with a few websites you like. The goal isn't to only target the majority by chasing analytics: it is to win a thousand 0.01% minorities that add up to 10%. The only other UI thing--and I'm using Firefox right now, and have been using Firefox as my primary browser for months now--that I can think of are container tabs. This one is interesting because, frankly, it doesn't buy me that much over Chrome's support for multiple profiles, and yet I do slightly prefer the feature, and clearly a bunch of other people do if you look around: implementing this feature won Firefox a bunch of users who now consider this part of their workflow and can overlook other faults. Firefox used to be really good at this: they owned the space of web developers due to Firebug--which was also a critical market as it meant websites tended to work in Firefox--but Chrome saw that and took it from them. If I were in charge of Firefox, my hail marys wouldn't be allocated to end user acquisition: it would be focused on what I can offer developers to get them back to using Firefox as their primary browser. But like, it isn't even clear to me Firefox right now cares about developers anymore :/. I mean... not only did they lay off the entire MDN writing team back in 2020--which to me was putting Firefox at the forefront of developers' minds (in the same way you mention users knowing about Google)--but, as far as I understand, they also laid off a lot of the dev tools team. Their website showing the features of Firefox for developers sounds strong, but I feel like Chrome also now has all of this stuff. I am excited to see that Firefox claims to have better support for CSS Grid debugging, I guess? I also say that, because another place Firefox used to have a unique selling proposition is that it was \"the hacker's browser\": you could easily alter any part of the interface due to its crazy XUL layer, and I knew a ton of developers and users alike who would sell you on Firefox due to the crazy Firefox-specific extensions you could install. But as Chrome added extension support, Firefox not only wanted to be compatible with Chrome's extensions... they dropped (almost) everything that was unique about Firefox. As it stands, they at least do retain some functionality that isn't just the same as what Chrome offers: support for synchronous fetch hooks (which I might be describing poorly) that is used by the more advanced ad blockers. This is a great USP because, of course, Google isn't going to support those... but Firefox stops there. I contend that it wouldn't be a big lift for them to add some extra Firefox-specific extension API surface and get, for example, the Tridactyl user community back to 100% on Firefox. And there is frankly a ton of uncharted territory on being able to make powerful web extensions. I used to be in charge of the iPhone native code extension community, and I seriously feel more crippled trying to easily modify a web page than I ever did with a native Objective-C app, and that's insane: I myself constantly run into roadblocks due to being unable to dig into the private data of JavaScript objects or closures, and I see other developers complain about being locked out of styling web components. Firefox should lean into \"it is easier to hack the web with Firefox\" as we know Google is going in the opposite direction. Despite the insane complexities of jailbreaking your iPhone, we had around a consistent ~10% marketshare; and no: that wasn't piracy! Not only did the US Copyright Office investigate and say we weren't the problem, we had a thriving ecosystem of paid native app extensions! (Though, frankly, if Firefox managed to hold ~10% marketshare entirely on the back of piracy, I'd be OK with that!) Otherwise, as it stands, Firefox seems to be removing unique selling propositions as they focus on narrowly re-implementing exactly the set of things offered by Chrome. They have decided that the only market worth targeting is the mass market, and so they are making the same analytics-driven decisions Google makes with respect to safety, streamlining, and prioritization that forsake developers and power users as part of a losing battle with Google for 90% of the web when they used to own the other 10%. reply ragnese 15 hours agorootparent> the pitch \"we're slightly more private\" is a hard one that they aren't even doing well... if they want to make a commitment to 0 data storage with no suggestions or analytics, that would be one thing, but they refuse to do that and so can't hold this narrative. Or... they can, because they are still more private and respectful than Chrome and Edge, at least. reply saurik 14 hours agorootparentAnd yet, every time this comes up people come out to bash Firefox for not going far enough on privacy. The top comments on this HN post are all in that category, as the people who care about privacy actually care about privacy and Firefox's half-assed attempts to walk the line are falling flat for that audience. Meanwhile, as can be seen in these very comments, people seem to have found alternatives: any of the now many forks of Chromium that entirely remove these anti-privacy features they don't want without incurring any other functionality tradeoffs. Firefox could take these users back, but they don't want to: their strategy of throwing away the minority markets so they can be Google-lite in the hope of guilting enough of the majority market into using Firefox is reliant on analytics and they seemingly can't help themselves with the attempts to upsell people on data harvesting services. reply philistine 12 hours agorootparentIf I didn’t know any better, I’d presume Firefox is waiting for Google to get in real bad trouble with the FTC so it has to sign a deal to promote other browsers or something. I would not hold my breath. reply romanows 5 hours agorootparentprevThis is a great comment. Firefox used to _stand_ for things that internet savvy folks cared about, and at least some of that would trickle down to make even the non-savvy user's browsing experience better. reply hulitu 2 hours agorootparentprev> and I think it's less anything they're doing and due simply to the fact that Google is a household name while Mozilla is not. Of course, changing the UI, using Google safebrowsing as default and other anti-user practices have nothing to do with it. /s reply pquki4 17 hours agoparentprev> Can't they just take Google's yearly $600,000,000 payment They can do that for as long as Google is willing to pay. Without additional revenue stream, the day Google decides to cut cost and stop sponsoring Mozilla, that's the day Firefox will run into big trouble. Any additional revenue stream is going to help. I am no CEO but that seems very clear to me. reply zamadatix 16 hours agorootparentIt's not a sponsorship, it's a sale.It's also made such deals with Yandex, Baidu, and Yahoo before. The sale is for eyeballs on your search (ad) service. Firefox is getting in a precarious situation with it though as the number of users on Firefox has been decreasing (in absolute terms) despite the number of web users increasing. reply xnyan 15 hours agorootparentIt's also a part of their anti-trust defense strategy. Firefox staying alive as a marginal but existent alternative to chromium based browsers is currently good for google. reply lxgr 17 hours agoparentprevIf you ever figure out a bulletproof solution to \"just taking the money and building a good product/company without any technical debt and organizational cruft\", please do share your results – this would be somewhat valuable for humanity! reply causality0 16 hours agorootparentI dunno, the CEO getting a 400% pay rise in the same time period the browser lost 85% of its market share might be a good place to start. reply BadHumans 18 hours agoparentprevYou want them to be Google's vassal state and wait for death instead of trying to become self-sustainable? Defeatist mentality if I've ever seen one. reply mardifoufs 17 hours agorootparentThat's what they are and have been for like 8 years. It's fine. It would be more productive to put it to good use on the browser. I'm not saying that they shouldn't have tried but... they did for almost a decade and they aren't remotely close to being able to sustain themselves without google. It's not defeatist to say that making up for the 600m$/year might be a bit of an unreachable goal at this point. Chasing ghosts isn't more sustainable than the money from google. Yes it's a business relationship. but I'm not sure how they aren't completely \"vassalized\", to use your term, at this point already. It's not a potential problem, if google stops paying they would shutdown in a matter of months. It's not a bad thing, Firefox as we know it wouldn't exist by now otherwise reply Kuinox 18 hours agorootparentprevI want them to not replicate google. I use Firefox to fight ads, not to get ads in my searchbar. reply deprecative 17 hours agorootparentIronic that ads are what Google uses to pay Mozilla so you(we) can use Firefox. reply immibis 15 hours agorootparentYou assume that Firefox can't exist without Mozilla. Thunderbird got a lot better after divorcing from Mozilla. reply dralley 13 hours agorootparentA) Firefox is vastly bigger and more complicated than Thunderbird B) Email protocols aren't a moving target to the degree that the web is C) Thunderbird has the benefit of being able to freeload off the base platform development that Firefox continues to do, although of course it's a lot of work even to adapt to those changes. reply autoexec 17 hours agorootparentprev> You want them to be Google's vassal state and wait for death instead of trying to become self-sustainable? There's no shortage of privacy respecting open source software that somehow doesn't have to choose between depending on Google and selling out their users. Firefox knows that most people won't opt out. They're choosing to take Google's money and screw over their users at the same time. reply sp332 17 hours agorootparentprevIf they just turn into another Google then what was the point? reply ddalex 18 hours agoparentprev> addressing ... organizational issues Obviously not. It's very difficult to make people understand something when their jobs depend on them not understanding it. reply not2b 16 hours agoparentprevAntitrust actions in the US and EU may force Google to cut off those payments (to Mozilla and Apple), and if that happens, Firefox needs to survive somehow. reply kjkjadksj 16 hours agorootparentSomehow open source browsers still get made without a $600 million “rich uncle.” Mozilla is pretty bloated. This can buy like 1200 good engineers. People make browsers with a couple people sometimes. reply zamadatix 16 hours agorootparentMost open source browsers are a reskin of actual open source browser development. E.g. Brave and Vivaldi are skins on top of the actual Chromium project, librewolf and others on Firefox's components, and Orion and others on Safari's components. Only a rare few browsers e.g. Dillo or Ladybird are actually independent and it shows in that those browsers are nearly unusably slow, compatibility limited, and insecure (that's not a diss on them, they are still awesome projects for their own reasons). reply camkego 16 hours agorootparentprevIt’s possible to repackage Chromium with a couple people. It’s not possible to build a browser from the ground up with a couple people. reply kjkjadksj 16 hours agorootparentUsing existing code is fine, that's how most software is built anyhow. Only masochists build from complete scratch. EDIT: I've been getting a lot of down votes for this stance, surprisingly. Why not share your position if you don't agree? There must be a bunch of hardcore people on here who are writing directly on the metal in machine code. Short of that, you are in fact using someone else's code in all your projects. reply Vinnl 1 hour agorootparentIt's the classic \"building on Chrome cedes control of what the web can do to Google\". But I think most people aren't interested in rehashing that discussion here - a search for \"independent browser engine\" should turn up plenty of arguments if you're interested in other positions. reply JohnFen 12 hours agorootparentprevThis seems to conflate two entirely different ways of \"using existing code\". Using tools written by others is technically \"using existing code\", but that's an entirely different thing than incorporating existing code into your projects. I think that when most devs hear \"using existing code\", they're thinking of the latter, not the former. reply kjkjadksj 6 hours agorootparentHow is calling a library and using a function someone else wrote not the same thing as using a function someone else wrote in any other way? Cite your sources by all means but still the difference is a semantical one. reply marginalia_nu 16 hours agorootparentprevWhy not? Ladybird seems like it is coming along very well. reply zamadatix 16 hours agorootparentLadybird is frickin awesome but give it 10 more years and it still won't be an equivalent to 2024 Safari/Firefox/Chrome without a lot more funding than it's already getting (~100-200k/year in sponsorships). reply mead5432 12 hours agorootparentprevSorry - was looking at an older version of the page and didn’t realize there were already a bunch of the same comment. My bad. ——— Will they though? Maybe some toy ones but the distribution of other browsers are largely built on chromium with a few on Firefox and one or two on WebKit. Beyond that, there’s Arc that’s made a splash with the HN crowd but IIRC, it’s VC funded so a whole different set of concerns. There will be other browsers but almost the entire browser market is essentially funded by Google. reply sunaookami 17 hours agoparentprevSadly Firefox already shows sponsored spammy & clickbaity \"articles\" on the default new tab page which always makes me cringe whenever I see it on other pcs. reply immibis 15 hours agoparentprevCapitalist organizations, even nonprofits, incentive moving money to the top. Mozilla has the same kind of cancer that Wikimedia has documented. Firefox is open-source, but Mozilla is not. reply jasonlotito 14 hours agoparentprev> Seems like this is in support of \"Firefox Suggest\", which seems to show sponsored links. Browsing history, bookmarks, clipboard, open tabs, shortcuts, search engines, suggestions from firefox, suggestions from sponsors. Each of those can be individually turned off and on. Why do you think Firefox Suggest using browsing history, bookmarks, clipboard, open tabs, shortcuts, search engines, suggestions from firefox isn't building the best browser \"for the user\"? Or were you just ignorant of what Firefox Suggest did and didn't bother to take a moment to look it up in Firefox? reply wavemode 17 hours agoprevI would respect the announcement more if they just came out and said it - \"we need cash to keep running and Google is offering to pay us a bajillion dollars for some anonymized search data. Sorry folks.\" reply kevincox 15 hours agoparentI don't know why they didn't say it in this article, but in https://www.mozilla.org/en-US/firefox/126.0/releasenotes/ they said: > This data will not be associated with specific users and will be collected using OHTTP to remove IP addresses as potentially identifying metadata. No profiling will be performed, and no data will be shared with third parties. So it seems they aren't selling it directly. But I wouldn't be surprised if aggregated numbers could be used for sales deals. (Hi Bing, we have 137M \"travel\" searches a month, I'm sure that you could put some big juicy ads next to those if you purchase the default search engine status) reply sunnybeetroot 14 hours agorootparentGood catch reply lxgr 16 hours agoparentprevIs that actually what's happening? Why would Google need this information? Don't they already get all of that and more in the queries themselves? Or is this about supplying Google with a user profile that persists beyond incognito tabs, cleared cookies/history etc.? I read it more as \"we, Mozilla, want to know what Firefox users use their browsers for\" rather than \"we want to hand this data to Google on a per-query level\". That said, it is incredibly vaguely worded. reply jasonlotito 14 hours agoparentprevI'm sorry, but \"Support Firefox with occasional sponsored suggestions.\" isn't good enough? More importantly: \"Google is offering to pay us a bajillion dollars for some anonymized search data\" Citation needed here. Your head isn't good enough. reply fallingsquirrel 18 hours agoprevStuff like this makes me wonder why I still cling to Firefox instead of switching to one of the actually privacy-focused Chromium forks. I've been using Arkenfox to turn off all the telemetry/etc but it increasingly feels like a game of whackamole. https://github.com/arkenfox/user.js reply roughly 17 hours agoparentI still use Firefox because I'd like there to be more than one web browser in the world, but boy oh boy they're trying as hard as they can to get me to quit. reply mdaniel 11 hours agorootparentIt's also much easier to compile, and thus hack upon since it doesn't require a datacenter-scale distcc setup for all the CVE-generation code C++ reply falcolas 18 hours agoparentprevFor me, it's the support for ad-blockers that actually work, plus greasemonkey. But it's been awhile, and maybe I need to look again. reply autoexec 17 hours agoparentprevFirefox is still the most secure browser you can get once you've beaten it into submission through hundreds of about:config changes. Last I checked you couldn't even fully disable service workers or WebRTC in chrome. reply computator 16 hours agorootparentDo you have a recommended list of the about:config changes to get optimal security and privacy? reply autoexec 15 hours agorootparentNot for most people. I deal with compromised/malware infested websites so my team has a document with over 600 entries that locks down as much as possible. Arkenfox is included in that and that should cover what most people need in terms of privacy. Firefox on my work PC has things disabled that most people would probably want like SVG, Wasm, JS, auto/form fill, saved passwords, searching from the address bar, reader view, mathml, browser.fixup, remote fonts, PDF viewer, etc. reply DavideNL 15 hours agorootparentprevThis might interest you... (if you know how to set it up): \"Firefox privacy, security and anti-tracking: a comprehensive user.js template for configuration and hardening \" https://github.com/arkenfox/user.js reply yjftsjthsd-h 17 hours agoparentprevSurely playing whackamole with Google is harder than Mozilla? reply omoikane 15 hours agoparentprevI use both Chrome and Firefox. Either it will reaffirm me that my preferred browser is indeed better, or I will find out what I have been missing out. I might not have been running multiple browsers daily if any one browser is perfect, but currently some websites seemed optimized for Firefox while others seemed optimized for Chrome. (And a small handful of websites are actually better with Lynx). reply medstrom 18 hours agoparentprevReally? Arkenfox user too and it seems to have stabilized the last couple of years. The moles have been whacked. reply fallingsquirrel 17 hours agorootparentI didn't mean to imply Arkenfox is unstable, it's great. I meant the Arkenfox maintainer is playing whackamole with FF. Every so often FF adds a spying feature (like this one), then Arkenfox has to react and turn it off and cut a release, then everyone has to run the updater script. My system auto updates Firefox, but not Arkenfox. So if I don't want any windows of vulnerability I'm stuck having to pay attention to updates and make sure I run the updater script at just the right time. reply atomicfiredoll 18 hours agoprevJust a shortcut for anybody trying to find it. \"Remember, you can always opt out of sending any technical or usage data to Firefox. Here’s a step-by-step guide [0] on how to adjust your settings.\" --- 1. Click the [hamburger menu button] and select Settings. 2. Select the Privacy & Security panel. 3. Scroll down to the Firefox Data Collection and Use section. 4. Check or uncheck the box next to Allow Firefox to send technical and interaction data to Mozilla. [0] https://support.mozilla.org/en-US/kb/share-data-mozilla-help... reply krono 16 hours agoparentThat still doesn't disable all telemetry from being sent. There's a whole slue of settings that you need to change in about:config and then there's still no guarantee. Even if you follow those steps you listed there and uncheck all those boxes you're shown in the standard settings pages, then Firefox will just continue to send out 'pings' when you interact with certain elements in the browser interface. And that's just the tip of the iceberg. Mozilla is like one of those politicians that just won't stop yapping about traditional conservative family values, who is later found to have several extramarital children and numerous affairs with same-sex partners themselves. reply idk1 2 hours agorootparentCould you tell us what other settings you need to change please? reply yabatopia 18 hours agoparentprevIt's a bit deceptive to claim you have a Privacy First policy when it's opt out. True privacy is not opt out, but opt in. reply cubefox 18 hours agorootparentAnonymization isn't opt out. reply JohnFen 17 hours agorootparentAnonymization is also weak sauce. reply stronglikedan 18 hours agoparentprevI wonder if you'll have to remember to do this after each update, like you have to do for most of Chrome's unwanted and unwelcomed features. reply Ennea 18 hours agorootparentProbably not. These settings have been around for a long time already anyway, and they never get overwritten once set, at least in my experience. reply wewxjfq 16 hours agorootparentWhen you haven't used Firefox for a while it shows a prompt that tricks you into resetting your profile. reply sunaookami 17 hours agorootparentprevThey already did this way back with their other telemetry stuff. You had to disable it through about:config and then Mozilla introduced new flags/renamed them and even added a flag if you have disabled telemetry that you had to disable again manually. reply saganus 18 hours agorootparentprevApparently not, at least for this update. I had the settings disabled, then updated to the latest version (\"126.0 (64-bit)\") and settings were kept disabled. So at least for now it looks like it respects your current settings. reply atomicfiredoll 18 hours agorootparentprevOnly time will tell, but I don't think I've ever had that issue with Firefox so far. reply hagbard_c 18 hours agorootparentprevIt might be possible to disable this feature through a policy file [1] just like other features can be disabled, e.g. here's how to disable auto-update: { \"policies\": { \"DisableAppUpdate\": true } } Create a text file named policies.json in a directory named distribution inside the Firefox installation directory. In my case that ends up looking like this: /opt/APPfirefox (package root) /opt/APPfirefox/firefox (currrent nightly) /opt/APPfirefox/bin/firefox-127.0a1 -> /opt/APPfirefox/firefox/firefox-127.0a1 /opt/APPfirefox/distribution/policies.json The browser sees this file and abides by its contents. Open about:policies#documentation in Firefox to see which policies can be configured this way. [1] https://support.mozilla.org/en-US/kb/customizing-firefox-usi... reply stevenicr 18 hours agoparentprevthanks for sharing - just found out you have to do this for each firefox profile if you have more than one. Note says it takes 30 days for them to remove your data history. I wonder if pihole or similar could just block the dns for this crap. reply Tarq0n 14 hours agorootparentHow could they remove your data if it's anonymized? That implies they're using pseudonymization instead. reply CaptainOfCoit 18 hours agoprev> Innovation and privacy go hand in hand here at Mozilla [...] Rest assured, the way we gather these insights will always put user privacy first [...] Remember, you can always opt out of sending any technical or usage data to Firefox Wouldn't actually putting \"user privacy first\" lead to the conclusion that gathering insights like this shouldn't be done on a opt-out basis and instead be opt-in, at the very least? Personally, I'd see \"privacy first\" as not needing to sell any user data at all, in the first place, but we're clearly beyond that already. reply SllX 18 hours agoparentPrivacy first is just not collecting the data. Everything else is just a compromise and an excuse. reply dylan604 18 hours agorootparentI don't have a problem if the server side is tracking metrics on what queries are being made. When it becomes a problem is trying to associate the queries based on a single user regardless on anonymization attempts. Of course there's no money in that. reply stevenicr 18 hours agorootparentWasn't like 15? years ago with the AOL search data anonymized release that people found multiple ways to connect dots to whittle it down to just one person or a small subset of people? Wasn't this also shown with anonymized taxi-cab data (released in NY?) many moons ago? Would it not be possible with knowing that you are tracking this data to funnel people into doing searches in a way that would reveal things? Directions to the out of state reproductive health clinic, combined with card data would be all it takes to do serious things to people in some states. Defaults matter. A lot. Anonymized data is not always anonymous, collected server side or otherwise. reply ziddoap 17 hours agorootparentYes, this is a process called (fittingly) data re-identification. There are many papers on the topic. One of the more popular examples is \"Robust De-anonymization of Large Sparse Datasets\" using the Netflix Prize Dataset. >We apply our de-anonymization methodology to the Netflix Prize dataset, which contains anonymous movie ratings of 500,000 subscribers of Netflix, the world’s largest online movie rental service. We demonstrate that an adversary who knows only a little bit about an individual subscriber can easily identify this subscriber’s record in the dataset. Using the Internet Movie Database as the source of background knowledge, we successfully identified the Netflix records of known users, uncovering their apparent political preferences and other potentially sensitive information. https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf This paper speaks about AOL in 2006, which I think you are referring to: https://digitalcommons.law.uw.edu/cgi/viewcontent.cgi?articl... However, it should be noted that the AOL dataset had a bunch of stuff that was identifiable by its nature (e.g. people searching for their full names or address), and the dataset wasn't scrubbed of those searches. So the controversy wasn't just re-identification of data, but also just a bunch of already-identifiable data. >Anonymized data is not always anonymous More importantly, in my opinion, is that data that is anonymous now is just one other dataset away from not being anonymous anymore. reply saghm 17 hours agorootparentprev> Anonymized data is not always anonymous, collected server side or otherwise If anything, I think it's both safer and more accurate to start from the assumption \"anonymized\" data can be de-anonymized and and require evidence to refute that rather than starting from a place of assumption that anonymization works and then trying to find a way to attack it. In practice, there's just not a good track record of this being done effectively, and I think people should generally be skeptical of whether this is even possible in many cases. reply JohnFen 14 hours agorootparentThere is only one way that data can really be \"anonymized\": if the individual data points are aggregated and the original collected data is deleted. Short of that, anonymization is basically illusory. The trouble is that we'd still have to take the word of the entity doing the data collection that they've done this properly, and it's clear that we can't take anyone's word for that. reply salawat 18 hours agorootparentprevAnonymization is effectively not achievable. Limited anonymoty may be possible within the scope of a particular dataset, but all it yakes is one enrichment pipeline to strip that all away. If you don't think that's what places like insurers do on a regular basis, you're a fool. reply unethical_ban 18 hours agorootparentprevDo you believe there is no utility in knowledge? Do you believe absolutism is the only philosophy for privacy? I think telemetry and the data software collects can help with usability, design, and product enhancement and that it's very likely this can be done to some extent without harming privacy. reply autoexec 18 hours agorootparent> I think telemetry and the data software collects can help with usability, design, and product enhancement It can, but that's too often little more than a justification to take as much data as possible much of which is misused. In this case, what a person searches for on the internet isn't helping one bit with \"usability, design, and product enhancement\" reply salawat 18 hours agorootparentprevYou know what? That's fine and good. Ask users to opt-in, and respect the wishes of those who opt-out. Anything short of that is you ignoring the concept of consent entirely, and doing whatever mental gymnastics you can to feel fine with doing so. reply Barrin92 17 hours agorootparentThis has nothing to do with mental gymnastics. Consent isn't a meaningful concept when it comes to each individual setting of a piece of software. You'd have to make 500 decisions every minute if you had to actively consent to everything your software does. In fact cookie banners show this. People hate them because they force meaningless choices on them. If you make a website with tracking as an opt-out option, almost everyone clicks \"accept all\". If you make a website with tracking as opt-in, almost every one clicks accept all. That shows that opt-in/out or consent does literally nothing ot reflect people's preferences, the act of making a choice completely dominates the actual decision. That means that if you want to respect user preferences you don't actually get around making default choices for them, and it's why consent is pretty much meaningless. reply bobthecowboy 16 hours agorootparent> In fact cookie banners show this. People hate them because they force meaningless choices on them. If you make a website with tracking as an opt-out option, almost everyone clicks \"accept all\". If you make a website with tracking as opt-in, almost every one clicks accept all. That shows that opt-in/out or consent does literally nothing ot reflect people's preferences, the act of making a choice completely dominates the actual decision. I disagree with this interpretation - the banners force themselves in front of the user before accessing the content. And then the choice is almost always \"Accept all\" and \"complete a checklist mini-game of things you don't want cookies for\". It's not a shock that people when confronted with this will click the easy button, and that doesn't mean it reflects their actual interests. It's just fatigue. If the \"accept our cookies\" button was off to the side of the page, and defaulted to \"none\" unless the user did something otherwise, I wonder what the \"accept all\" numbers would look like then. Actually, I don't. reply Barrin92 13 hours agorootparent> It's not a shock that people when confronted with this will click the easy button, and that doesn't mean it reflects their actual interests. Yes, but that was my actual point. If one simple UI design trick is enough to completely flip the choices of users, then consent forms aren't a robust way to collect preferences at all. In fact if you wanted to genuinely and in good faith provide access to granular preferences, giving a more complicated set of choices would be the only way to go about it, and that fatigue is still real even if the design has a legitimate purpose. What you're saying is true, the only way for the choice to be representative would be to have like a binary yes/no choice because that's simple, but that's not even necessarily what the user wants either. You're going to get a significantly more accurate view of people's real preferences by collecting data, like what Firefox is doing here, and then setting defaults accordingly. reply cubefox 18 hours agoparentprev> Wouldn't actually putting \"user privacy first\" lead to the conclusion that gathering insights like this shouldn't be done on a opt-out basis and instead be opt-in, at the very least? At the very least collection of non-anonymized data should be opt-in at most. So where is the problem? reply MattGaiser 18 hours agoparentprev> Personally, I'd see \"privacy first\" as not needing to sell any user data at all, in the first place, but we're clearly beyond that already. That requires users paying, something often suggested, but I have not heard of working commercially for anything where a competitor can supply a user as the product alternate. Privacy just isn't that valuable. reply autoexec 18 hours agorootparent> That requires users paying That's clearly not a solution either since expensive products and services with subscriptions still routinely collect every scrap of data they can get their hands on. Companies will always make more money by violating your privacy while also charging you as much as they possibly can so that's exactly what they do. reply beretguy 18 hours agorootparentprevI think a small fee subscription can be very attractive. Less than $5 a month. I already pay $5/month for better search experience. Sometimes Kagi finds things neither DDG nor Brave can. reply beretguy 18 hours agoprevI’d rather they make Firefox subscription based. I’d rather pay $5 a month and have no ads or tracking of any kind. They can even run a separate fork of Firefox or something for this purpose. If Firefox goes down Google will lock internet behind a DRM and we gonna need another rms or Torvalds to deliver us from this hell. Can you imagine a world without Linux, with Windows and Mac pretty much the only mainstream alternative? I’d rather pay a small fee not to have that. reply Marsymars 16 hours agoparentPaid web browsers have been tried... Opera and OmniWeb probably the most notable examples, and it just doesn't seem like there's a workable market there. reply beretguy 11 hours agorootparentYeah, but I don’t think we had Google trying to lock internet behind a DRM at the time. reply xnyan 15 hours agoparentprev>Windows and Mac pretty much the only mainstream alternative That's not the world we live in today? reply beretguy 11 hours agorootparentWe have Linux to choose from. Imaging we didn’t have that option. reply JohnFen 14 hours agoparentprev> I’d rather they make Firefox subscription based. Software subscriptions are a hard \"no\" for me. Even if I loved literally everything else about Firefox, if they did this I'd have to stop using it. reply fifteen1506 18 hours agoparentprevNo subscription for me tyvm. I do donate on average €2/month for various projects, though. reply beretguy 17 hours agorootparentYes, I understand. reply mr_machine 18 hours agoprevThe browser is, at this point, like a public utility. You can't get by without one. How is it okay to bake in privacy compromises? reply medstrom 18 hours agoparentDon't ask Mozilla that. Ask the others. reply yjftsjthsd-h 16 hours agorootparentMozilla is a browser vendor; the question is appropriate for them. It may also be a good question for every other browser vendor, but it's still valid for Mozilla. reply VancouverMan 18 hours agoprevAnyone who uses Firefox with the belief that it offers \"privacy\" should really read the Firefox Privacy Notice: https://www.mozilla.org/en-US/privacy/firefox/ Quite a few company and organization names are referenced. These include well-known ones like \"Google\" and \"Microsoft\", but also others that (at least to me) are far more obscure, such as \"our third-party ad platform Kevel\" and \"AdMarketplace (a third-party referral platform)\". Questionable words like \"send\", \"sends\", \"sending\", \"share\", \"shares\", and \"sharing\" also appear quite a few times. More broadly, the notice is quite long. A software product that truly respects its users' privacy should have a short privacy policy, mainly because it isn't collecting data to begin with, it isn't sending data to third parties, and so forth. I know some people will claim that the data collection and sending that Firefox does is somehow acceptable because some of it can be disabled, or because it might be less than what other browsers do. I don't buy into those arguments. A privacy-respecting browser would have users opt in to enable any functionality that might transmit user data, or just not even include such functionality at all by default (it would have to be voluntarily added via an extension, for example). reply AzzyHN 16 hours agoprevReading just the blog post... these feel like useless metrics. Why do you care what percentage of your users are searching about animals. What do you possibly have to gain from this? Are they planning to introduce animal-based features? And if it's not for their own metrics, but to sell... who's going to buy? Facebook tracking pixels and other statistics cookies are so much more valuable that this kind of data reply kevincox 15 hours agoparentI suspect it is mostly for selling things like default search engine and ads via Firefox Suggest. Some search categories are far more valuable than others for ads. reply binkHN 17 hours agoprevWow. Another write-up on how Firefox is planning on monetizing. How about a write-up on how Firefox plans to stay competitive and integrate/innovate on some of the best features from competing browsers? reply remram 9 hours agoprevOblivious HTTP is such a joke. The idea that an \"independent organization\" is your \"partner\" is a non-starter for me. How can you put those words in the same sentence? And then base the entire security of the whole feature on this premise? reply multimoon 16 hours agoprevI was a chrome user for over a decade and just last year switched to Firefox when googles decision to just flat share my data with ad brokers and call it a security feature finally pushed me over the edge and I switched to Firefox. I still think chrome is the better browser both in functionality and performance, but at least Firefox was 90-95% of the way there, and respected my desire to not sell my data. While this in of itself isn’t super egregious and has an easy opt out, what alternatives are there? I know there’s a bunch of chrome clones, but at that point why not just use raw chromium? reply jdlyga 16 hours agoprevI miss when Firefox was this forward thinking project to make a better browser with a better UI. It’s similar to what happened to Canonical with Ubuntu. Both projects are extremely solid, and are easy to recommend, but neither has been pushing the envelope in a long time. reply mrob 18 hours agoprevI don't even want my browser to support searching from the browser UI. If I want to send information to Google I'll visit their web page directly. I don't want to do it by a misclick. It's surprisingly difficult to disable searching from the browser UI in Firefox. Firefox insists you have at least one search engine enabled at all times. The best option I know of is to set the undocumented setting \"browser.urlbar.update2.engineAliasRefresh\" to \"true\" in about:config, which enables an \"Add\" button in the Search preferences for adding custom search engines. You can then add a fake search engine on localhost that will always fail, and set this to be the only enabled search engine. reply crtasm 17 hours agoparentI achieved this by turning off a handful of config options for the address bar's suggestions; setting the search input to be separate from the address bar, and hiding it via Customize Toolbar. I think these were the most important: keyword.enabled browser.fixup.alternate.enabled browser.urlbar.suggest.searches reply mrob 17 hours agorootparentFor many versions now, removing the separate search bar via Customize Toolbar reverts to combined URL bar and search bar. It also leaves the search option in the right-click menu. reply JohnFen 14 hours agorootparentOooh! That explains why I haven't been able to disable that stuff from the omnibar. I hate the omnibar -- I just want text entered there to be interpreted as a URL and nothing else -- but at some point, I wasn't able to configure it to be that anymore. If this is why, you've just made Firefox better for me! Thank you! reply crtasm 17 hours agorootparentprevThat may be the case, but typing into my address bar does not trigger or suggest searches. Good point about the right click. reply lagniappe 18 hours agoparentprev>You can then add a fake search engine on localhost that will always fail, and set this to be the only enabled search engine. YES! My friend, you have found my holy grail, thank you reply autoexec 17 hours agoparentprevThere are a bunch of about:config changes you can make to prevent searching from the address bar. I follow that up by going through and just deleting nearly every string with a URL in it. reply elaborate4013 14 hours agoparentprevAre you talking about the address bar search? Just disable keywords in about:config then. reply wopwops 17 hours agoprevIs Firefox being sabotaged from within? reply abcd_f 16 hours agoparentSure feels like that. For years now actually. reply pixxel 3 hours agoparentprevHighly paid executives paid by… [checks notes]…Google. Surely not! reply remram 8 hours agoprevI still can't get search in my own bookmarks most of the time, because they pushed bookmark tags hard, then have not implemented them on mobile after a decade. But at least they have a state-of-the-art snooping mechanism... reply JohnFen 17 hours agoprevI'm unclear about what search data they're collecting. Are they only talking about searches done through the omnibar, or are they including searches done by navigating to a search engine website and using that? reply parrellel 18 hours agoprevWelp, thanks for telling me what to turn off right now Mozilla. reply mst 18 hours agoparentProviding a link to their own comprehensive guide to turn everything off definitely earns style points in my book. reply UberFly 6 hours agoparentprevTurning off telemetry has always been easy in Firefox settings. If I'm reading it correctly, the same checkbox we've all be clicking off for years now also disables this too same as always. reply chatmasta 15 hours agoprev> Remember, you can always opt out of sending any technical or usage data to Firefox Why isn’t this opt-in? Seems to give away the plot. reply callroomlamp 18 hours agoprevLibrewolf* seems to be the answer reply UberFly 6 hours agoparentWho's watching the watchers?... reply mtmail 18 hours agoprev\"list of categories we’re using: animals, arts, autos, business, career, education, fashion, finance, food, government, health, hobbies, home, inconclusive, news, real estate, society, sports, tech and travel.\" \"This helps us [...] providing a browsing experience that is more tailored to your needs\" How would my browsing experience be different in any of the categories? Unless it's about showing me ads yet again. reply mjw1007 16 hours agoparentRight. If the categories were things like \"using search as the most convenient way to find a well-known website\", \"entering a term that's likely to appear in a news headline\", and \"asking a fully-formed question\", I'd find the claim that this is about ways to make a better \"browser experience\" more convincing. reply SoftTalker 18 hours agoparentprevIt's about showing you ads. reply cflewis 18 hours agoparentprevYeah, this is the rub of it. I don’t want my browser to know about me. It’s not necessary. What are they gonna do, release a gaming-aesthetic browser like Opera did? Can’t this data be gleaned some most definitely not identifiable way like download number for particular platforms (“hey, we’re getting a lot of use on Steam Deck”) or something? Mozilla is supposed to be better than this. I don’t really see anywhere else to go though if you want a web experience that works out of the box (non-free codecs and the like). reply JohnFen 12 hours agorootparent> I don’t want my browser to know about me. It’s not necessary. This. It's not necessary, and I consider it dangerous. Even if the browser maker is 100% on the same privacy page as I am, browsers are too complex to trust with personal data. reply ezoe 5 hours agoprevWhy is the web browser try to act like SNS web service? You are an user agent for SNS web service, not the SNS itself. Now be a good boy and fetch a SNS web page like a good little dog. reply autoexecbat 17 hours agoprevIs there a white-label firefox that I can install that has sane defaults? reply itscrush 17 hours agoparentArkenfox, Librewolf, Waterfox among others might be the best bet, plenty of trade offs like updater scripts at times. reply braiamp 17 hours agoprevHow can Mozilla guarantee some privacy? Well, they would be using separation of content and the source. The content is \"broad categories\" of search queries. They take your query, match it to a category (this is important, because it has to be local matching to make sense), encrypt it with the public key and send it to a broker. The broker knows where it is coming from, but not what it says. The broker then forward to Mozilla, which decrypts it with their private key. I don't see a problem as long as there's not enough bytes of information that can infer a person. Aggregating the data in the client would allow this, so I see some kind of time shifting to protect the activity patterns. reply flipbrad 18 hours agoprevHistorically it was always interesting and quite challenging legal work, reconciling some things a bit like this with privacy, lawful intercept and anti-remote-exploitation-tool (anti-trojan) laws, especially in Europe (think: the cookie rule, which goes beyond cookies and PII). Then again, more recently mobile apps and operating systems (on all platforms, e.g. desktop OSs) seem to be doing quite a lot of it, so maybe those legal concerns were overblown. reply autoexec 17 hours agoparent> Then again, more recently mobile apps and operating systems (on all platforms, e.g. desktop OSs) seem to be doing quite a lot of it, so maybe those legal concerns were overblown. I think its possible that the laws are still being violated, but good luck suing companies like MS or Google as an individual. Governments won't go after them because their spy agencies love the constant stream of data they can collect from it. reply pixxel 17 hours agoprevI have all Firefox/Mozilla URLs blocked at network level. I get Firefox install/updates from a Linux repo. Firefox is constantly pounding my network attempting to dial home. Thank you maintainers for giving me privacy. reply Woodi 18 hours agoprevSo, there is MS servers ip lists to block, now where is Mozilla's ? reply proactivesvcs 16 hours agoprev\"Mozilla Connect is a collaborative space for ideas, feedback, and discussions that will help shape future product releases. Whether you’re a longtime Mozilla fan or new to our products—welcome! This community is for you. Please use this space to share ideas, give constructive feedback, and participate in meaningful conversations so we can work together to build a better internet\" https://connect.mozilla.org/ When you make a comment here, do a quick search beforehand to check the topic doesn't already exist, and do abide by the spirit of the guidelines. reply ribhu97 17 hours agoprevSo I guess they're building their own index, just like Brave, Perplexity, Exa, and others. A much smaller arms race that seems to be going under the radar of most folks. reply roughly 17 hours agoprevWhoever is writing these press releases for Firefox needs to reconsider their style. By this point in the product cycle, the only people left using Firefox are sophisticated enough to read through this level of corporate bullshit. Either take \"privacy first\" seriously or take it out of the mission statement, but don't piss on our leg and call it rain. reply parasti 17 hours agoprevCome on, Mozilla. I just want a browser. I type in an address, the browser shows me the page. You need money? Let me donate to Firefox so I don't have to continuously opt out of crap like this. reply Fervicus 16 hours agoprevMight be unrealistic, but I can't wait for the the day when Ladybird browser becomes a viable alternative. Librewolf it is till then. reply michelangelo 17 hours agoprevInteresting to see this post has no author or signature... reply manifoldgeo 6 hours agoparentIt was written by Moe Zilla, obviously /s reply thepaulmcbride 16 hours agoprevAnyone know why this is only available in the US? reply abcd_f 16 hours agoparentBecause it's inherently anti-consumer (not something that anyone would voluntarily opt into) and the EU would have their balls in vices instantaneously? reply red_admiral 18 hours agoprevAs far as I can tell, this is about collecting aggregate data - they say at the country level - for example \"U.S. users search a lot more for 'government' but less for 'education' than Mexican ones\". This particular initiative doesn't seem to be about profiling individuals. And, of course, you can turn it off (even if you're not in a country covered by the GDPR). If you want to do some kind of high-level data collection, this seems like a fairly ethical way of doing it? It's certainly better by miles than what all of chrome, android and edge have been pushing recently. I have firefox suggest turned off on my machine (at least the one I have firefox as my main browser on), but that's just me. reply mrob 18 hours agoparentThe ethical way of doing this is opt in. It could be done like Debian's Popularity Contest[0], where you're offered a chance to install the data collection software, with the default being don't install. Firefox could do this with an extension. [0] https://popcon.debian.org/ reply doublepg23 17 hours agoprevI didn’t expect there to be a privacy side effect of mostly migrating to Apple products. I use Safari on everything now. reply ravivyas 18 hours agoprevI wonder if they are planning to get ads, there is little reason to collect behaviour data for anything else. reply elric 16 hours agoprevMaybe we should just go back to the 1990s where we just pay a one time fee to buy a piece software and own it forever? Without any data collection, without constant enshittification, without endless ads? Buy it, get a couple of yeard worth of security updates, and be done with it. It's clear that libre browsers have failed. They are not sustainable. They are too big, too complex, and apparently only two organizations are really capable of keeping one going. One is an ad company, the other is a patsy for the ad company. I don't see a way out of this that is at all sustainable. reply PreInternet01 17 hours agoprevSuggestion for a future 'Show HN': Send Mozilla an hourly (GDPR|CCPA) data request and notify you if there are any results. I'd pay a monthly recurring EU$ 0(.|,)99 for that! reply ekianjo 18 hours agoprevnobody knows how to read a room like the Mozilla folks. They know quite a few Firefox users hate telemetry with passion yet they always go back at it like it does not matter. reply fifteen1506 18 hours agoparentThey've reached peak power users usage. reply cassepipe 17 hours agoprevThe main reason I use Firefox is because it lets you deactivate search suggestions and the \"awesome bar\" will suggest only from history and bookmarks. Not having to go through a search engine for basic navigation is a killer feature, and better for the planet. Chrome does not let you do that for obvious reasons. Now I am afraid of enshittification, that they will try to shove search suggestions one day and hide/remove the option to deactivate them. reply Timber-6539 18 hours agoprevYou either die a hero or you live long enough to see yourself become the villain... reply mst 18 hours agoprevTitle feels a little bit misleading - they're not collecting 'categorised search data' so much as 'categorising searches and then collecting an aggregate of only the categories.' I'm not entirely sure how to phrase it better while keeping it pithy but the current wording reads to me like they're collecting the searches as well as the categories, and they deliberately aren't doing that. (I entirely understand that some people will reasonably have an issue with what they -are- doing, but it's at least worth understanding what that actually is before deciding whether you're one of them :) reply poszlem 18 hours agoprevOne thing that annoys me more than data collection is corporations pretending they do it for my benefit while \"respecting my privacy.\" Stop with the gaslighting, it's absolutely insufferable. Not to mention, the way the text is written makes me think it's ai generated and that makes it somehow even worse. reply exitzer0 17 hours agoparentThis. I invite everyone to go and look at the amount settings that must be turned OFF in Windows 10/11, Firefox, Chrome, whatever app from companies that claim to respect user privacy. For Windows it is literally dozens of toggles, and unsoliticed apps. Then there are the so called security features like malware scanning of URLs/file downloads, etc. Then the DNS replacements which, ya know, still see the URLs that you are asking for. Every company that claims to respect your privacy is full of shit. There is too much money to made in not respecting it and this is the sole reason that they exist. reply daft_pink 16 hours agoprevWhat could go wrong? reply cess11 17 hours agoprevThank you Mozilla for categorising me for my own good, really appreciate the effort I need to put in to try and shut it down. 'By the way, these values have come out in other contexts. I remember in an earlier war General Westmoreland saying “We had to destroy the village in order to save it”. But it was not an irony. He meant it. I mean, so did the early Christian communities that settled into this country mean it. That for a witches own good one had to dunk her repeatedly in water. Now we have come a long way since then haven’t we[?]'reply blackeyeblitzar 17 hours agoprevMaybe this is okay to help support it but only if there is a clear modal dialog asking users to consider donating or consider staying opted in. But a default opt in is deceptive. reply resource_waste 18 hours agoprevI installed firefox on my new computer and it reminded my of Microsoft Windows. Lots of ads. Its just a glimpse into the future of this company. We are past the peak. reply usernamed7 18 hours agoprevHere we go again. This is why i use waterfox. I switched when they put ads in the address bar. reply bloopernova 18 hours agoprevI don't want to be given a \"browsing experience\". I don't want to be treated as a product, sold to data brokers without my knowledge. My browsing experience is already tailored to me by my own actions. I guess I'll be migrating to LibreWolf. reply lxgr 17 hours agoparentHow does having a browsing experience imply being a product? Is it possible to use Firefox without having a browsing experience? It is a web browser! Seems like Mozilla really is fighting a losing battle: Whenever they as much as think about doing 1% of the monitoring that their competitors do (often for user research to guide and focus their development efforts; unclear if that's what's happening here though), they alienate a vocal part of their user base; when they don't, their software drifts further and further from what users actually want and need. reply kjkjadksj 16 hours agorootparentThe base they alienate just wants a web browser. You don’t need to iterate forever on a good tool that does the job. Software can be feature complete. Anyone wishing for a complete overhaul update for something like cp? Absolutely not, people want tools that work the same for years. reply lxgr 16 hours agorootparentBut a browser is never \"done\", for better or worse, nor are user expectations static. Stopping to iterate on a web browser is like stopping to iterate on an operating system: It'll go well for a while (and users even might love you for it for a year or two, since you can focus your effort on fixing bugs and not introduce new ones with new features), and then you'll slowly but steadily lose users to other browser that do adapt. Losing users for a web browser, means losing search referral revenue in the short term (literally Mozilla's lifeblood), and losing web developers in the long term, which will break the experience even further. Just one example that almost made me switch browsers: Web site translation. I was regularly using Chrome in parallel for that, but now Firefox fortunately supports it too (and in a privacy-preserving local way at that – a true innovation), so they keep me as a user. reply kjkjadksj 16 hours agorootparentAll you have to do is keep up with web standards. Most people don't use 99% of the browsers features I'd guess. About all I need are tabs and bookmarks and a history and thats it, same stuff since netscape basically, my three features for a complete browser experience that would make me very happy. I'm sure other users feel the same. reply lxgr 14 hours agorootparentSome things technically not part of the core feature set of a browser (at least not the one you've mentioned), yet even just one of them missing would make me immediately drop Firefox: - End-to-end encrypted tab, history, and bookmark sync across devices - Content translation, as mentioned above ideally in a privacy-preserving way - Plugin support - Cookie-jar-per-tab and proxy-per-tab support (Firefox allows doing both through Multi-Account Containers) I'm sure some other users also feel the same. reply JohnFen 11 hours agorootparentAnd I don't want or use any of those things. It's almost like a \"one size fits all\" browser can't actually exist. I'm hoping for a more minimal, but excellent, browser to come around to meet the needs of the sort of user I am. I think this is one of the adverse effects of Firefox changing how extensions work. In the Good Old Days, Firefox was a reasonably minimal browser and you could pick and choose which advanced functionality you wanted by choosing which extensions to install. But since extensions have been largely neutered now, a whole lot of that advanced functionality has to be built into the browser proper. This is fine if you want that stuff, but it's really uncomfortable if you don't. reply throwaway7356 4 hours agorootparentprev> About all I need are tabs and bookmarks and a history and thats it I'm impressed you don't even need a HTML renderer component and all that comes with it. reply SkyMarshal 16 hours agorootparentprevAs long as web standards keep evolving you can never stop iterating on browsers. And with Google Chrome pushing some of those web standards additions, Firefox has to keep up or die. Also, it is possible to continue iteratively improving a complex piece of software like a browser, beyond just the web standards race. Security, privacy, performance, and reliability/bugs/code correctness are areas where new computer science is constantly coming down the pipe and worth integrating. And maybe other things like AI or features for AR/VR systems, though it's more debatable whether those belong in general purpose browsers. Browsers aren't Unix utilities that should do one thing and just one thing and thus can theoretically reach a state of \"done\". But even there it's not always a certainty. For example, \"sudo\" being superseded by a simpler more secure \"doas\", and then more recently by SystemD \"run0\". Even simple utilities continue to evolve. reply kjkjadksj 16 hours agorootparentFirefox is doing a lot more than merely keeping up with web standards. Learning to abrogate a web standard is not an impossibility. They are standards for a reason: to show other engineers how to interface with them. I'd consider a browser finished if it has tabs, bookmarks, history, and print. Same as we had 25 years ago. reply jimbob45 16 hours agoparentprevThat's just not feasible for the vast majority of people. Being constantly behind on security updates is a sound recipe for losing your personal data unless there's nothing you care about on the machine you're using. reply spiffytech 17 hours agoprevI posted this on another story 6mo ago: Every time I hear \"anonymous data\", I think of that time AOL published anonymized search logs (for academic research). The anonymization was negligent, and an NYT reporter de-anonymized and tracked down one of the users with the local & personal info present in the search queries. https://en.wikipedia.org/wiki/AOL_search_log_release https://web.archive.org/web/20130404175032/http://www.nytime... reply lordofgibbons 18 hours agoprev> We’re ramping up our efforts to enhance search experience by developing new features like Firefox Suggest, which provides recommended online content that corresponds to queries. So, they need to collect data about my search habits so they can show me relevant ads in Firefox Suggest? At this point why do I even bother using Firefox? I seriously might as well just use Chromium. reply beretguy 18 hours agoparent> I seriously might as well just use Chromium Before you leave please give LibreWolf a shot. It’s a modified Firefox with “spying/tracking/etc” disabled. https://librewolf.net/ reply lordofgibbons 18 hours agorootparentI've looked into it in the past, but I have no idea who's maintaining that software or what's included in it. I understand their claims, but I can't verify that's all they're doing due to my lack of time/interest. It's more risk than I'm willing to take for a software I use or everything critically important to me - including banking access. reply Phelinofist 17 hours agorootparentprevQuoting another HN user regarding librewolf: > Binaries are unsigned, third party update service, Google safe browsing disabled unless you build from source, running unusual browser setups can actually make you more distinctive online, unencrypted DNS by default, speed of security patches is slower than base Firefox, etc. reply stuffinmyhand 18 hours agorootparentprevwill do reply cflewis 18 hours agoparentprevDepends what you want. Chromium also has hooks back to Google so if you are really data conscious you’ll need to look to something like ungoogled-chromium. reply CaptainOfCoit 18 hours agoparentprevOnly reason I still use Firefox is because of TreeStyleTabs/Sideberry, still nothing like it for Chrome so on Firefox I remain. reply TheFreim 18 hours agorootparentThe Vivaldi browser has a similar feature (not open source unfortunately). I've used it for a while, I quite enjoy the tab and bookmark management sidebars. reply jedisct1 18 hours agorootparentprevTry Arc. You won't be looking back. reply CaptainOfCoit 18 hours agorootparentI'd love to but it not being available on Linux and invite-only both makes it very difficult to even give it a try :) reply lars_francke 18 hours agorootparentprevArc being such a generic term it'd be helpful to include a link or explanation what that is. reply beretguy 18 hours agorootparentJust search for “arc browser”. https://arc.net/ reply beretguy 18 hours agorootparentprevWhy? It’s based on chromium anyways. Might as well use Vivaldi instead. reply bdzr 18 hours agoparentprevRemember when Firefox was the new hotness because it blocked ads? reply LikesPwsh 17 hours agorootparentFirefox mobile is still the hotness for that same reason. reply groovecoder 18 hours agoparentprevContainers? reply beretguy 18 hours agorootparentI love containers when developing! Especially when working with or testing authentication related stuff. No need to open private tabs all the time. reply add-sub-mul-div 18 hours agoparentprevYou can trivially opt out of both the telemetry and the suggestions. reply rabbits_2002 18 hours agoprevnext [2 more] [flagged] Andrex 18 hours agoparentWhen Epiphany got extension support it finally seemed to check all my boxes, personally. Wish there were an Android version with bookmark sync but watchagonnado reply yborg 18 hours agoprevIsn't this exactly what Google does in Chrome? reply braiamp 18 hours agoparentNo. Your queries aren't send to Mozilla, just the broad categories. Chrome is sending full queries to Google. reply CaptainOfCoit 18 hours agorootparentIf you're using Google for searches, it also doesn't matter if you use Firefox or Chrome, obviously they have access to the full queries you do on their properties. reply antiframe 18 hours agorootparentIt does matter becauae you can use an anonimizer for seach on Firefox just fine, but if you use Chrome then Google can collect everything you enter in a form and see where it's sent. Not sure if they do, but they could. One element of security is comparmentalization: avoid using a client and server from the same adversary, if you can, to minimize correlations. reply add-sub-mul-div 18 hours agorootparentprevAlso you can opt out of this, unlike (I believe, correct me if I'm wrong) the Chrome telemetry. reply mrbluecoat 18 hours agoprev [–] DuckDuckGo for the win! reply lordofgibbons 18 hours agoparent [–] This has nothing to do with what search engine you're using. It collects data about what you type in the search/URL bar. reply jmkb 18 hours agorootparent [–] I guess it's time to stop searching from the url bar (sorry, \"awesomebar\") and create a local html file as the startup and new tab page, with search boxes for all your fav engines. Just like the good old days. reply SoftTalker 17 hours agorootparentI did that for years, then it became difficult as browsers didn't want to let you set a local file as your default home page (for \"security reasons\"). I now just have \"Blank Page\" set for my Homepage, new windows, and new tabs. I just tried setting a local file in Firefox and it seems to work again, so I may go back to that. reply prophesi 16 hours agorootparentI was able to set a local file for new windows, but it looks like there's no built-in way to set a custom URL for new pages, not even in about:config (v125.0.3, Fedora). :/ I may have to simply use the URL bar with data collection opted-out, as I'd rather not bother with vetting and installing a custom open source extension to override the new tab behavior. reply autoexec 17 hours agorootparentprevabout:blank is the new tab nothing else loads faster! reply medstrom 18 hours agorootparentprev [–] You could even paste all your bookmarks into that page! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Firefox is implementing a new method to collect search data to enhance user experience while safeguarding privacy.",
      "The data will be categorized to grasp user requirements better and emphasize essential features.",
      "Users have the option to decline data collection, ensuring their browsing experience remains unaffected, showcasing Mozilla's dedication to privacy and pioneering tailored browsing encounters."
    ],
    "commentSummary": [
      "Users are worried about data privacy problems linked to browsers like Firefox and the inclusion of sponsored content in search results, prompting some to switch to more privacy-focused alternatives.",
      "Debates arise over the need for data tracking for targeted ads and suggestions for enhancing privacy features in browsers to address dissatisfaction.",
      "Balancing new features and customization with user-friendly browsing experience is crucial per discussions on maintaining a positive user experience."
    ],
    "points": 216,
    "commentCount": 218,
    "retryCount": 0,
    "time": 1715699261
  },
  {
    "id": 40355744,
    "title": "Pico: Production-Ready Open-Source Alternative to Ngrok for Kubernetes",
    "originLink": "https://github.com/andydunstall/pico",
    "originBody": "Pico is an open-source alternative to Ngrok. Unlike most other open-source tunnelling solutions, Pico is designed to serve production traffic and be simple to host (particularly on Kubernetes).Upstream services connect to Pico and register endpoints. Pico will then route requests for an endpoint to a registered upstream service via its outbound-only connection. This means you can expose your services without opening a public port.Pico runs as a cluster of nodes in order to be fault tolerant, scale horizontally and support zero downtime deployments. It is also easy to host, such as a Kubernetes Deployment or StatefulSet behind a HTTP load balancer.",
    "commentLink": "https://news.ycombinator.com/item?id=40355744",
    "commentBody": "Pico: An open-source Ngrok alternative built for production traffic (github.com/andydunstall)212 points by andydunstall 19 hours agohidepastfavorite38 comments Pico is an open-source alternative to Ngrok. Unlike most other open-source tunnelling solutions, Pico is designed to serve production traffic and be simple to host (particularly on Kubernetes). Upstream services connect to Pico and register endpoints. Pico will then route requests for an endpoint to a registered upstream service via its outbound-only connection. This means you can expose your services without opening a public port. Pico runs as a cluster of nodes in order to be fault tolerant, scale horizontally and support zero downtime deployments. It is also easy to host, such as a Kubernetes Deployment or StatefulSet behind a HTTP load balancer. NathanFlurry 17 hours agoThis is very cool! Trying to get it added to awesome-tunneling: https://github.com/anderspitman/awesome-tunneling/pull/149 Related -- we also built a simple (but not production-grade) tunneling solution just for devving on our open-source project (multiplayer game server management). We recently ran in to an issue where we need devs to be able to have a public IP with vanilla TCP+TLS sockets to hack on some parts of our software. I tried Ngrok TCP endpoints, but didn't feel comfortable requiring our maintainers to pay for SaaS just to be able to hack around with our software. Cloudflare Tunnels is awesome if you know what you're doing, but too complicated to set up. It works by automating a Terraform plan to (a) set up a remote VM, (b) set up SSH keys, and (c) create a container that uses reverse SSH tunneling to expose a port on the host. We get the benefit of a dedicated IP + any port + no 3rd party vendors for $2.50/mo in your own cloud. All you need is a Linode access token, arguably faster and cheaper than any other reverse tunneling software. Source: https://github.com/rivet-gg/rivet/tree/main/infra/dev-tunnel Setup guide: https://github.com/rivet-gg/rivet/blob/main/docs/infrastruct... reply apitman 14 hours agoparentThis is a good candidate for the list. Most solutions don't really differentiate themselves much, but being designed for production environments is certainly unique amongst the open source options. I'll try to get this merged today. reply marssaxman 8 hours agoprevWhy on earth would you reuse such a long-established name? Pico has been around for 35 years and many distros include it by default (or symlink `pico` to nano, anyway). reply andydunstall 6 hours agoparentI didn't know there was already a long-established project called Pico :) As someone suggested below, I'll rename to 'Piko' reply inferiorhuman 3 hours agorootparentPost GPL3 Apple replaced GNU Nano (itself a Pico clone) with UW Pico. A step backwards perhaps, but nano is a symlink to pico. I'd steer clear of anything that looks/sounds like 'pico' including 'piko' which doesn't seem to clear anything up. reply andydunstall 5 hours agoprevAs commented below, Pico is already a well established name for a text editor so I've renamed to Piko: https://github.com/andydunstall/piko reply crims0n 15 hours agoprevAs a hobbyist and programmer, I love the project. As an infosec professional working in an enterprise environment... not so much. reply andydunstall 6 hours agoparentCould you elaborate? Do you mean tunnelling generally or this implementation? reply dmattia 17 hours agoprevSay I have a pico cluster with a few service nodes and a few upstream clients register themselves, and then I deploy a new version of the service nodes where all existing service nodes are taken down and replaced. Can the client still talk to the service nodes? Is this over the same tunnel, or does the agent need to create a new tunnel? What happens to requests that are sent from a proxy-client to the service nodes during this transition? Or at a much higher level: Can I deploy new service nodes without downtime? reply andydunstall 16 hours agoparentWhen Pico server nodes are replaced, the upstreams will automatically reconnect to a new node, then that node will propagate the new routing information to the other nodes in the cluster So if you have a single upstream for an endpoint, when the upstream reconnects there may be a second where it isn't connected but will recover quickly (planning to add retries in the future to handle this more gracefully) Similarly if a server node fails the upstream can reconnect reply kowlo 17 hours agoprevIs there a way to use this with a simple docker-compose, no Kubernetes? reply dmattia 17 hours agoparentLooks like there is from the instructions in the getting-started guide: https://github.com/andydunstall/pico/blob/main/docs/getting-... reply resoluteteeth 16 hours agorootparentThe instructions there say that it will create a cluster with three nodes, so while it is using docker compose I am guessing it is still using kubernetes reply andydunstall 16 hours agorootparentThat demo only uses docker compose: https://github.com/andydunstall/pico/blob/main/docs/demo/doc... reply abuani 16 hours agorootparentprevThe compose file is in the demo folder, you don't need to guess what it's going to do. https://github.com/andydunstall/pico/blob/main/docs/demo/doc... Looks like it's spinning up three replicas of Pico reply v3ss0n 13 hours agoprevCheck out ziti and zork projects, they are a lot more innovative and ambitious https://github.com/openziti/ziti reply PLG88 28 minutes agoparentmispelling, zrok - https://zrok.io/. Its open source and has a free SaaS (or paid if you want). reply andydunstall 6 hours agoparentprevYep I checked out overlay networks, its definitely a very cool project. However it also seems pretty complex to host. I think they are different use cases reply PLG88 25 minutes agorootparentzrok is a similar capability (though it can potentially do a lot more). OpenZiti is definitely a more complex project. In fact, zrok was built on top of OpenZiti. We did this as Ziti provides a platform to develop secure by default, distributed applications quicker, which is why zrok has been built by only 1 developer across about 18 months and is almost feature parity with Ngrok (which has been developed by many people for almost 10 years). reply baq 17 hours agoprevNot to be confused with https://en.m.wikipedia.org/wiki/Pico_(text_editor) - no comments on the project itself but the name is a bit unfortunate. reply keb_ 16 hours agoparentAlso not to be confused with https://github.com/picosh or https://github.com/alexeyraspopov/picocolors or https://github.com/fergalwalsh/pico reply xd1936 14 hours agorootparentOr PicoCSS https://github.com/picocss/pico reply keb_ 13 hours agorootparentOr the footballer: https://en.wikipedia.org/wiki/Pico_(footballer) reply andydunstall 16 hours agoparentprevYeah sorry I started Pico before realising... reply abuani 16 hours agorootparentCould also rebrand as Piko.... reply andydunstall 16 hours agorootparentGood idea - will do that! reply hobo_mark 16 hours agorootparentOr Pigo? it's written in go after all :D reply tmountain 14 hours agorootparentprevSmart to rename. Pico editor is ubiquitous. reply sipjca 15 hours agoprevLove this! Have been doing something similar with HAProxy + Cloudflare Tunnels, but would love to move off it at somepoint. Super curious to give it a run soon. Thanks for sharing! reply illiac786 13 hours agoparentI have been considering cloudflare a bit, but it’s basically a mitm no?They decrypt your entire traffic then. It’s a lot of trust to put in cloudflare… reply chadsix 9 hours agorootparent>but it’s basically a mitm no? Yes [1] You could try IPv6.rs (shameless plug). We provide a routed IPv6 IP and reverse proxy for IPv4. We made it easy to run servers with Cloud Seeder [2], our open source server manager. [1] https://blog.ipv6.rs/understanding-tls-mitm-and-privacy-poli... [2] https://github.com/ipv6rslimited/cloudseeder reply mrbluecoat 12 hours agoprev> you can expose your services without opening a public port But doesn't the Pico cluster have to expose a public port? reply andydunstall 5 hours agoparentFew things: - If your trying to access a customer network (such as for BYOC), exposing a public port in the customer network is likely a no-go (or would require complex networking to setup VPC peering etc) - The Pico 'proxy' port doesn't need to be public (and in most cases won't be), such as you can only expose to clients in the same network (which is one of the benifits of self-hosting) - The Pico 'upstream' port (that upstream services connect to) will usually need to be public, but that can use TLS and has JWT authentication reply sigmonsays 15 hours agoprevhow do you access http services locally? is there a socks5 proxy or something I can configure in my web browser? reply andydunstall 15 hours agoparentNot sure I follow Pico is a reverse proxy, so the upstream services open outbound-only connections to Pico, then proxy clients send HTTP requests to Pico which are then routed to the upstream services So as long as your browser can access Pico it should work like any other proxy (Theres a getting started guide if that helps: https://github.com/andydunstall/pico/blob/main/docs/getting-...) reply nodesocket 15 hours agoprev [–] Is there a helm chart? Didn’t see one off the bat. reply westurner 9 hours agoparentGiven a helm chart, also podman kube kubernetes YAML podman-kube-play: https://docs.podman.io/en/latest/markdown/podman-kube-play.1... helm template: https://helm.sh/docs/helm/helm_template/ \"RFE Allow podman to run Helm charts\" https://github.com/containers/podman/issues/15098#issuecomme... helm template --dry-run --debug . --generate-name \\ --values values.yamltee kube42.yml && \\ podman kube play kube42.yml; reply andydunstall 6 hours agoparentprev [–] Not yet (still quite a new project), its on the list to add one reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Pico is an open-source tunnelling tool like Ngrok, tailored for production traffic and simple deployment on Kubernetes.",
      "It enables services to register endpoints and route requests via outbound connections, removing the necessity of exposing public ports.",
      "Pico functions as a fault-tolerant cluster, enabling horizontal scalability and zero downtime deployments, ideal for hosting behind HTTP load balancers."
    ],
    "commentSummary": [
      "Pico is an open-source tool similar to Ngrok, tailored for handling production traffic and functioning as a reverse proxy for upstream services, offering fault tolerance and scalability.",
      "Recently renamed to \"Piko\" due to name similarity concerns with an established text editor, it can be seamlessly deployed on platforms like Kubernetes and Docker, supporting zero downtime deployments.",
      "Developers are intrigued by Piko's versatility for networking tasks like local HTTP service access and integration with tools such as Cloudflare Tunnels."
    ],
    "points": 212,
    "commentCount": 38,
    "retryCount": 0,
    "time": 1715697877
  },
  {
    "id": 40355227,
    "title": "Optimizing Linked Lists for Enhanced Performance",
    "originLink": "https://dygalo.dev/blog/blazingly-fast-linked-lists/",
    "originBody": "Lost in IRC About Blog Tags RSS feed Blazingly Fast Linked Lists 2024-05-13 :: tags: #rust #performance #data-structures #jsonschema © Tomas Castelazo, www.tomascastelazo.com / Wikimedia Commons / CC BY-SA 4.0 Linked lists are taught as fundamental data structures in programming courses, but they are more commonly encountered in tech interviews than in real-world projects. In this post, I'll demonstrate a practical use case where a linked list significantly outperforms Vec. We will build a simple data validation library that shows the exact error location within invalid input, showcasing how a linked list can be used in graph traversals. This post mostly reflects my own exploration and mistakes in the jsonschema crates and therefore does not aim to provide a complete guide to linked lists but rather to present an idea of how they can be used in practice. Starting with a naive approach, we’ll progressively implement various optimizations and observe their impact on performance. Readers are expected to have a basic understanding of Rust, common data structures, and the concept of memory allocations (stack vs. heap). UPDATE (2024-05-14): Given the feedback I emphasized what ideas are objectively bad, clarified some sidenotes, and removed the imbl idea. To follow along with the implementation steps and explore the code, check out the accompanying repository Validation API Our library is fairly minimal, and follows JSON Schema semantics: use serde_json::json; fn main() -> Result> { jsonschema::validate( // JSON instance to validate &json!({ \"name\": \"John\", \"location\": { \"country\": 404 } }), // JSON schema &json!({ \"properties\": { \"name\": { \"type\": \"string\" }, \"location\": { \"properties\": { \"country\": { \"type\": \"string\" } } } } }), ).expect_err(\"Should fail\"); Ok(()) } In this example, 404 is not a string and this code should result in an error like this: 404 is not of type ‘string’ at /location/country|| --------------- || |Location within the JSON instanceFailing value This library and optimization ideas are derived from my jsonschema crate The validation process boils down to graph traversal. The input instance is traversed based on rules defined in the schema. At any traversal step, we should know the current location within the JSON instance to potentially report a meaningful error. Our primary goal is to implement location tracking while minimizing its impact on the library's performance. Without diving too deep into the Validator and Node implementations, let's see a simplified version of the validation process without location tracking: type ValidationResult = Result; fn validate(instance: &Value, schema: &Value) -> ValidationResult { Validator::new(schema) .expect(\"Invalid schema\") .validate(instance) } struct ValidationError { message: String, } impl Validator { /// Validate JSON instance against this validator. fn validate(&self, instance: &Value) -> ValidationResult { self.node.validate(instance) } } trait Node { fn validate(&self, instance: &Value) -> ValidationResult; } impl Node for Properties { fn validate(&self, instance: &Value) -> ValidationResult { if let Value::Object(object) = instance { // Iterate over properties and validate them if they are present. for (key, value) in &self.properties { if let Some(instance) = object.get(key) { // Delegate validation to the child validator. value.validate(instance)?; } } } Ok(()) } } impl Node for Type { fn validate(&self, instance: &Value) -> ValidationResult { // ... } } The validate function creates a new Validator instance (that is also a graph) from the provided schema and then calls its validate method with the JSON instance. The Node trait defines the validate method that each validation rule must implement. While this version provides error messages without location tracking, it serves as an upper bound for subsequent optimizations within the validate function. Benchmark setup To ensure the optimizations are relevant to the library's typical usage scenarios, we select inputs from different groups - valid and invalid instances of varying sizes. The schema contains 10 levels of nesting and is deliberately restricted to only the properties and type keywords, which are sufficient to demonstrate the overhead of path-tracking behavior while simplifying benchmarking and keeping our focus on performance rather than JSON Schema semantics. It's worth noting that the path-tracking behavior will remain largely the same for other keywords as well. { \"properties\":{ \"another\":{ \"type\":\"string\" }, \"inner\":{ \"properties\":{ \"another\":{ \"type\":\"string\" }, \"inner\":{ \"properties\":{ // And so on for up to 10 levels } } } } } } The instances have 0, 5, or 10 levels of nesting, following the schema's structure. Valid instances have a string value for the \"another\" property at the deepest level, while invalid instances have an integer. // Valid - 5 levels { \"inner\":{ \"inner\":{ \"inner\":{ \"inner\":{ \"another\":\"hello\" } } } } } // Invalid - 5 levels { \"inner\":{ \"inner\":{ \"inner\":{ \"inner\":{ \"another\":1 } } } } } To focus on the performance of the validation process itself, the schema is hardcoded in Validator::new, and Validator is built once and then reused. Let’s measure performance with criterion by running the following validation routine: const NUMBER_OF_ITERATIONS: usize = 10000; fn benchmarks(c: &mut Criterion) { // snip ... for instance in &benchmark.instances { c.bench_with_input( BenchmarkId::new(instance.kind, &instance.name), &instance.value, |b: &mut Bencher, value| { b.iter(|| { for _ in 0..NUMBER_OF_ITERATIONS { let _ = validator.validate(value); } }); }, ); } } Commit Valid Invalid 0 5 10 0 5 10 a030dcb 36.3 µs 553.8 µs 1.11 ms 475.2 µs 914.8 µs 1.48 ms The numbers in these microbenchmarks are not absolute and may vary depending on the hardware and the environment. Generally, the observed changes could be explained by stack traces in flame graphs, but you always need to check the benchmarks yourself. Bad Idea 1: Clone Vec Let's start from collecting traversed path segments in a vector adding a new segment each time validation goes one level deeper: fn validate(&self, instance: &Value) -> ValidationResult { // Start with an empty vector self.node.validate(instance, vec![]) } trait Node { // Add `path` parameter to track the current location in the JSON instance fn validate(&self, instance: &Value, path: Vec) -> ValidationResult; } impl Node for Properties { fn validate(&self, instance: &Value, path: Vec) -> ValidationResult { if let Value::Object(object) = instance { for (key, value) in &self.properties { if let Some((key, instance)) = object.get_key_value(key) { // CLONE! let mut path = path.clone(); path.push(key); value.validate(instance, path)?; } } } Ok(()) } } impl Node for Type { fn validate(&self, instance: &Value, path: Vec) -> ValidationResult { match (self, instance) { // ... Compare `instance` type with expected type _ => Err(ValidationError::new( format!(\"{instance} is not of type '{self}'\"), // Convert path to an iterator path.into_iter(), )), } } } The ValidationError struct now stores this path: struct ValidationError { message: String, /// Error location within the input instance. location: Vec, } impl ValidationError { /// Create new validation error. fn new( message: impl Into, // Accept an iterator and convert it to `Vec` location: impl Iterator>, ) -> Self { Self { message: message.into(), location: location.map(Into::into).collect(), } } } If you've been writing Rust for a while, you'll likely recognize the clone() call as a common \"solution\" to lifetime and mutability issues. While it is acceptable in certain situations, depending on your performance and maintainability constraints, it often signals an opportunity for optimization. Let's use such an opportunity here. Commit Valid Invalid 0 5 10 0 5 10 9ef7b4c 40.9 µs (+12%) 2.61 ms (+369.4%) 6.69 ms (+499.6%) 961.2 µs (+100.8%) 4.11 ms (+346.8%) 9.07 ms (+502.7%) This feature makes validation up to 6 times slower! NOTE: The table compares the current idea with the previous one. Cloning is generally a bad idea, but it is often seen in practice when developers are doing \"quick and dirty\" work or not aware about better options. Surely I used it many times. Let’s visualize the slowest \"valid\" benchmark using cargo flamegraph to understand what is going on. As expected, the memory reallocations show up in the flame graph: cargo flamegraph --bench jsonschema -o naive-10.svg -- --bench \"^valid/10 levels\" Flame graphs are amazing for visualizing stack traces, see more details here OK Idea 2: Reuse allocations However, you may note that we don't have to clone the data - what if we mutate the same vector, pushing and popping segments as we traverse? trait Node { fn validate( &self, instance: &'a Value, path: &mut Vec, ) -> ValidationResult; } impl Node for Properties { fn validate( &self, instance: &'a Value, path: &mut Vec, ) -> ValidationResult { // ... path.push(key); value.validate(instance, path)?; path.pop(); // ... } } impl Node for Type { fn validate( &self, instance: &'a Value, path: &mut Vec, ) -> ValidationResult { match (self, instance) { // ... Compare `instance` type with expected type _ => Err(ValidationError::new( format!(\"{instance} is not of type '{self}'\"), path.iter().copied(), )), } } The lifetime annotations ('a) are needed here because the path parameter is a mutable reference to a Vec that contains references to the instance parameter. The lifetime 'a ensures that the references in path do not outlive the instance they refer to. Commit Valid Invalid 0 5 10 0 5 10 7c94736 vs Idea 1 40.2 µs (-1.7%) 1.24 ms (-110.4%) 2.46 ms (-171.9%) 951.7 µs (-1.0%) 2.39 ms (-71.9%) 4.16 ms (-118.0%) vs Base +10.7% +124.2% +121.6% +100.2% +161.4% +181.0% Indeed, using &mut Vec can significantly improve performance compared to the naive approach, by reusing a single heap allocation instead of creating multiple clones. However, this approach requires a bit more bookkeeping and somewhat more lifetime annotations. This is our baseline idea for comparison. This akin to what I originally committed to the jsonschema crate. Better Idea 3: Linked list However, is it even necessary to allocate heap memory for a vector during traversal? Consider this: for each traversed node in the input value, there's a corresponding validate function call with its own stack frame. As a result, path segments can be stored in these stack frames, eliminating the need for heap allocations completely. Taking a step back and looking from a different angle can uncover ideas that may not be apparent at a lower level. By storing all segments on the stack, when an error happens, the previous segments can be traced back. This approach involves connecting each segment to form a path and collecting them when necessary. This sounds like a linked list: /// A node in a linked list representing a JSON pointer. struct JsonPointerNode { segment: Option, parent: Option>, } impl JsonPointerNode { /// Create a root node of a JSON pointer. const fn new() -> Self { JsonPointerNode { segment: None, parent: None, } } /// Push a new segment to the JSON pointer. fn push(&'a self, segment: &'a str) -> JsonPointerNode { JsonPointerNode { segment: Some(segment), parent: Some(self), } } /// Convert the JSON pointer node to a vector of path segments. fn to_vec(&'a self) -> Vec { // Callect the segments from the head to the tail let mut buffer = Vec::new(); let mut head = self; if let Some(segment) = &head.segment { buffer.push(*segment); } while let Some(next) = head.parent { head = next; if let Some(segment) = &head.segment { buffer.push(*segment); } } // Reverse the buffer to get the segments in the correct order buffer.reverse(); buffer } } Now we can replace &mut Vec: fn validate(&self, instance: &Value) -> ValidationResult { self.node.validate(instance, JsonPointerNode::new()) } trait Node { fn validate( &self, instance: &'a Value, path: JsonPointerNode, ) -> ValidationResult; } impl Node for Properties { fn validate( &self, instance: &'a Value, path: JsonPointerNode, ) -> ValidationResult { // ... value.validate(instance, path.push(key.as_str()))?; // ... } impl Node for Type { fn validate( &self, instance: &'a Value, path: JsonPointerNode, ) -> ValidationResult { match (self, instance) { // ... Compare `instance` type with expected type _ => Err(ValidationError::new( format!(\"{instance} is not of type '{self}'\"), path.to_vec().into_iter(), )), } } } No heap allocations during traversal! Does it help? Commit Valid Invalid 0 5 10 0 5 10 91ec92c 35.0 µs (-14.8%) 663.5 µs (-46.4%) 1.32 ms (-46.6%) 958.9 µs (+1.8%) 2.54 ms (+5.1%) 4.58 ms (+9.9%) Woah! That is significantly better for valid inputs! Invalid ones are roughly the same, however, we have not optimized the linked list implementation yet. In our case, we are leveraging the fact, that the path is only needed when an error occurs. This allows us to avoid the overhead of maintaining the path during the entire traversal process and heap allocate only when necessary. Note that linked lists have worse cache locality compared to vectors, which can lead to slower performance in some scenarios. Good Idea 4: Precise memory allocation Let's find out why the linked list implementation is not as performant for invalid inputs by looking at JsonPointerNode first: Apparently, the problem is in memory reallocations inside JsonPointerNode::to_vec. We can avoid them by allocating the exact amount of memory needed. This will require an extra linked list traversal to calculate the capacity, but the performance gain from avoiding reallocations outweighs the cost of the extra traversal: impl JsonPointerNode { pub(crate) fn to_vec(&'a self) -> Vec { // Walk the linked list to calculate the capacity let mut capacity = 0; let mut head = self; while let Some(next) = head.parent { head = next; capacity += 1; } // Callect the segments from the head to the tail let mut buffer = Vec::with_capacity(capacity); let mut head = self; if let Some(segment) = &head.segment { buffer.push(*segment); } while let Some(next) = head.parent { head = next; if let Some(segment) = &head.segment { buffer.push(*segment); } } // Reverse the buffer to get the segments in the correct order buffer.reverse(); buffer } } Commit Valid Invalid 0 5 10 0 5 10 10ae4f1 39.1 µs (+11.2%) 667.9 µs (+0.5%) 1.30 ms (-1.7%) 899.7 µs (-7.5%) 1.96 ms (-23.3%) 3.49 ms (-24.3%) Great! Precise memory allocation improves performance for invalid inputs, bringing it closer to the performance of valid ones. Allocate exactly as needed, whenever possible, to avoid the overhead of memory reallocations. Good Idea 5: Avoid temporary Vec At this point, the most significant slowdown is for invalid cases. If we take a closer look at the ValidationError implementation we’ll see the collect call, which means that first, we build Vec in JsonPointerNode::to_vec and then almost immediately build Vec from it, which means allocating Vec twice. Why don’t we just build Vec in the first place: impl ValidationError { fn new(message: impl Into, location: Vec) -> Self { Self { message: message.into(), location, } } } impl Node for Type { fn validate( &self, instance: &'a Value, path: JsonPointerNode, ) -> ValidationResult { match (self, instance) { // ... Compare `instance` type with expected type _ => Err(ValidationError::new( format!(\"{instance} is not of type '{self}'\"), path.to_vec(), )), } } } impl JsonPointerNode { pub(crate) fn to_vec(&self) -> Vec { // ... if let Some(segment) = &head.segment { buffer.push((*segment).to_string()); } while let Some(next) = head.parent { head = next; if let Some(segment) = &head.segment { buffer.push((*segment).to_string()); } } // ... } } This optimization leads to a visible performance improvement for invalid cases: Commit Valid Invalid 0 5 10 0 5 10 d3d2182 39.7 µs (-0.2%) 652.3 µs (-2.7%) 1.35 ms (+2.2%) 765.1 µs (-14.2%) 1.83 ms (-6.9%) 3.33 ms (-5.9%) Maybe Good Idea 6: Struct size optimization Sometimes it is worth trying to reduce struct sizes, especially when the struct is passed by value frequently. Smaller structs lead to less memory usage and faster function calls, as less data needs to be copied on the stack. If we were to track not only keys in JSON objects but also indexes in arrays, we would need to use an enum like this: enum Segment { /// Property name within a JSON object. Property(&'a str), /// Index within a JSON array. Index(usize), } struct JsonPointerNode { segment: Option>, parent: Option>, } Then, the JsonPointerNode struct would occupy 32 bytes: assert_eq!(std::mem::size_of::(), 32); However, by avoiding Option in the segment field, we can reduce its size to 24 bytes. The idea is to put some cheap value in the root node and never read it: struct JsonPointerNode { segment: Segment, parent: Option>, } impl JsonPointerNode { /// Create a root node of a JSON pointer. const fn new() -> Self { JsonPointerNode { // The value does not matter, it will never be used segment: Segment::Index(0), parent: None, } } fn push(&'a self, segment: Segment) -> JsonPointerNode { JsonPointerNode { segment, parent: Some(self), } } /// Convert the JSON pointer node to a vector of path segments. pub(crate) fn to_vec(&self) -> Vec { // ... if head.parent.is_some() { buffer.push(head.segment.to_string()) } while let Some(next) = head.parent { head = next; if head.parent.is_some() { buffer.push(head.segment.to_string()); } } // ... } } This technique is directly used in the jsonschema crate to reduce the size of the JsonPointerNode struct, but not used here for simplicity. More ideas that didn't make it I think that we can also gain a bit more performance by avoiding the reverse call in JsonPointerNode::to_vec, as it moves the data around. One way to achieve this is by assigning segments starting from the back of a vector filled with default values. However, the extra bookeeping needed for writing in the reverse order could outweigh the gains, so it's important to profile and benchmark any changes to ensure they provide measurable benefits for your use case. Another idea is to store references to path segments inside ValidationError instead of cloning the strings: struct ValidationError { message: String, location: Vec, } This way, we can avoid cloning the path segments and instead store references to them. This could lead to performance improvements, especially when dealing with long paths or large numbers of errors. However, this approach would make ValidationError less flexible, as it would be tied to the lifetime of the input JSON data. Excellent Idea 7: Maybe you don't need a linked list? This idea was suggested by @Kobzol, who noted that the error path could be collected lazily from the call stack in the error propagation path. I've implemented the idea based on the suggestion and the original code snippet: impl ValidationError { pub(crate) fn push_segment(&mut self, segment: String) { self.location.push(segment); } pub(crate) fn finish(mut self) -> ValidationError { self.location.reverse(); self } } fn validate(&self, instance: &Value) -> ValidationResult { if let Err(error) = self.node.validate(instance, 0) { // Reverse the path segments in the `finish` method Err(error.finish()) } else { Ok(()) } } impl Node for Properties { fn validate(&self, instance: &Value, level: u32) -> ValidationResult { // ... for (key, value) in &self.properties { if let Some(instance) = object.get(key) { if let Err(mut error) = value.validate(instance, level + 1) { error.push_segment(key.to_string()); return Err(error); // ... } } impl Node for Type { fn validate(&self, instance: &'a Value, level: u32) -> ValidationResult { // ... _ => Err(ValidationError::new( format!(\"{instance} is not of type '{self}'\"), Vec::with_capacity(level as usize) // ... } } However, I was not able to get a stable improvement in benchmarks yet :( Small improvement may come from the fact that we no longer use the ? operator as it involves the From/Into conversion, but we only have a single error type and don't need to convert it. This is the reason why serde has its own tri! macro that is used instead of ?; Conclusion In the end, we achieved ~1.9x / ~1.3x improvements from the baseline implementation for valid / invalid scenarios. Overall this feature adds 18% / 95% on top of the path-less version! Some optimizations in this article may not seem immediately beneficial, especially if you already know where the bottlenecks are. However, exploring simpler optimizations can sometimes reveal unexpected opportunities for improvement. Even if an optimization doesn't directly pay off or even makes your code slower in some cases, it may open up new possibilities for further optimizations. Takeaways: A naive approach could be good enough Look at the problem at different scales Search for a data structure that fits your problem Allocate exactly as needed and when necessary Reduce the size of values you pass around a lot Don't use \"Blazingly fast\" in your post titles If you have any more ideas to improve this use case or have any suggestions, please let me know! In the next article, I will dive into HTML trees and why you should try to build your own data structure for it. Thank you for your attention! Dmitry © 2021-2024 Dmitry Dygalo",
    "commentLink": "https://news.ycombinator.com/item?id=40355227",
    "commentBody": "Fast linked lists (dygalo.dev)176 points by dmitry_dygalo 20 hours agohidepastfavorite116 comments duped 18 hours agoIt strikes me the bottleneck for this problem isn't Vec or List, it's the serde_json Value type that needs to be used. This is useful for serializing/deserializing values into Rust types but if you're trying to validate JSON against a schema you don't actually need the JSON value data, just the types of the nodes (or more specifically, you only need some of the value data, and probably not much of it, so don't pay for it when you don't have to). If you implemented your own parser for the schema and the JSON and only used an AST to validate + span information (which can just be a pair of u16s for start/end of a token) then you can collect your error messages very, very quickly and generate the error message once validation is finished. Heavily optimized compilers will do this for semantic analysis and type checking, where the IR they use can be constructed quickly and then used with the input text to get helpful messages out, while the guts of the algorithm is only working with semantic information in a structure that's compact and easy to access/manipulate. All that said, serde_json is incredibly convenient and giving up to write your own parser is a big hammer for a problem that probably doesn't need it. reply ComputerGuru 16 hours agoparent> All that said, serde_json is incredibly convenient and giving up to write your own parser is a big hammer for a problem that probably doesn't need it. I had a thought in my reply [0] on this that actually might let him eat his cake and have it too in this regard. I think you can heavily abuse serde::de::Visitor to schema validate without actually parsing (or with less parsing, at any rate). I went into more detail in my comment but I wanted to ping you (@duped). [0]: https://news.ycombinator.com/item?id=40357159 reply ww520 14 hours agoparentprevI've seen a lexer/parser scheme that encodes the lexer token type along with the token file location information into a u64 integer, something like struct Token { token_type: u8, type_info: u8, token_start: u32, // offset into the source file. token_len: u16 } It's blazing fast. The lexer/parser can process millions of lines per second. The textual information is included, and the location information is included. reply duped 6 hours agorootparentI think the key insight is that the true benchmark is bytes/second (bandwidth) of the lexer/parser, so reducing the size of the output data (tokens/AST nodes) is a massive gain in the amount of data that you can process in the same amount of time. The fewer bytes you can pack data into the more data that you can process per second. Computers may be the most complex machines ever built but the simple fact of having fewer things to touch means you can touch more things in the same amount of time remains true. reply acidx 11 hours agorootparentprevThis is roughly what my JSON parser does. It does type-checking, albeit without using JSON-schema, but an object descriptor that you have to define to parse and generate JSON. It's been developed for embedded systems (it was written originally for a NATS implementation in the Zephyr RTOS), so it's a bit limited and there's no easy way to know where some parsing/type validation error happened, but the information is there if one wants to obtain it: https://github.com/lpereira/lwan/blob/master/src/samples/tec... reply dmitry_dygalo 16 hours agoparentprevThat is really cool idea! Thank you reply EGreg 17 hours agoparentprevJust use capn’proto. No deserialization needed ! reply aabhay 17 hours agorootparentWhats your experience like using it? Is it ergonomic or does it require you to do lots of type gymnastics? reply cabronerp 15 hours agorootparentThis repo has a nice pub/sub implementation based on capnp: https://github.com/commaai/cereal/blob/master/log.capnp reply evmar 18 hours agoprevThe \"maybe you don't need a linked list\" proposal at the bottom seems significantly better than the options presented in the post: - almost no cost in the non-erroring path - no extra data structures to manage - a lot less code I think the post would benefit from a better analysis of why this doesn't work for them. reply dmitry_dygalo 16 hours agoparentIndeed, I agree with your points. This idea was added after I wrote the post and wasn't taken from my own optimization efforts in `jsonschema`. Originally, in `jsonschema` the output type is actually a badly composed iterator and I intended to simplify it to just a `Result` for the article, but with this output, there are actually way better optimizations than I originally implemented. If I'd discovered this idea earlier, I'd probably spend more time investigating it. reply vouwfietsman 16 hours agoparentprevIndeed, also building a linked list over the stack like that is a crafty but very weird design. Keep it simple. reply ComputerGuru 17 hours agoprevNice post, Dmitry! Two suggestions: it’s not immediately obvious whether subsequent benchmark result tables/rows show deltas from the original approach or from the preceding one (it’s the latter, which is more impressive). Maybe call that out the first couple of times? Second, the “using the single Vec but mutating it” option would presumably benefit from a reserve() or with_capacity() call. Since in that approach you push to the vector in both erroring and non-erroring branches, it doesn’t have to be exact (though you could do a bfs search to find maximum depth, that doesn’t strike me as a great idea) and could be up to some constant value since a single block memory allocation is cheap in this context. (Additionally, the schema you are validating against defines a minimum depth whereas the default vec has a capacity of zero, so you’re guaranteed that it’s a bad choice unless you’re validating an empty, invalid object.) But I agree with the sibling comment from @duped that actually parsing to JSON is the biggest bottleneck and simply parsing to the minimum requirements for validation would be far cheaper, although it depends on if you’ll be parsing immediately after in case it isn’t invalid (make the common case fast, assuming the common case here is absence of errors rather than presence of them) or if you really do just want to validate the schema (which isn’t that rare of a requirement, in and of itself). (Edit: I do wonder if you can still use serde and serde_json but use the deserialize module’s `Visitor` trait/impl to “deserialize” to an enum { Success, ValidationError(..) }` so you don’t have to write your own parser, get to use the already crazy-optimized serde code, and still avoid actually fully parsing the JSON in order to merely validate it.) If this were in the real world, I would use a custom slab allocator, possibly from storage on the stack rather than the heap, to back the Vec (and go with the last design with no linked lists whatsoever). But a compromise would be to give something like the mimalloc crate a try! reply dmitry_dygalo 14 hours agoparentThanks! > Two suggestions: it’s not immediately obvious whether subsequent benchmark result tables/rows show deltas from the original approach or from the preceding one (it’s the latter, which is more impressive). Maybe call that out the first couple of times? Agree! > Second, the “using the single Vec but mutating it” option would presumably benefit from a reserve() or with_capacity() call. Since in that approach you push to the vector in both erroring and non-erroring branches, it doesn’t have to be exact (though you could do a bfs search to find maximum depth, that doesn’t strike me as a great idea) and could be up to some constant value since a single block memory allocation is cheap in this context. (Additionally, the schema you are validating against defines a minimum depth whereas the default vec has a capacity of zero, so you’re guaranteed that it’s a bad choice unless you’re validating an empty, invalid object.) Oh, this is a cool observation! Indeed it feels like `with_capacity` would help here > But I agree with the sibling comment from @duped that actually parsing to JSON is the biggest bottleneck and simply parsing to the minimum requirements for validation would be far cheaper, although it depends on if you’ll be parsing immediately after in case it isn’t invalid (make the common case fast, assuming the common case here is absence of errors rather than presence of them) or if you really do just want to validate the schema (which isn’t that rare of a requirement, in and of itself). My initial assumption was that usually the input is already parsed. E.g. validating incoming data inside an API endpoint which is then passed somewhere else in the same representation. But I think that is a fair use case too and I was actually thinking of implementing it at some point via a generic `Json` trait which does not imply certain representation. > (Edit: I do wonder if you can still use serde and serde_json but use the deserialize module’s `Visitor` trait/impl to “deserialize” to an enum { Success, ValidationError(..) }` so you don’t have to write your own parser, get to use the already crazy-optimized serde code, and still avoid actually fully parsing the JSON in order to merely validate it.) Now when I read the details, it feels like a really really cool idea! > If this were in the real world, I would use a custom slab allocator, possibly from storage on the stack rather than the heap, to back the Vec (and go with the last design with no linked lists whatsoever). But a compromise would be to give something like the mimalloc crate a try! Nice! In the original `jsonschema` implementation the `validate` function returns `Result` which makes it more complex to apply that approach, but I think it still should be possible. reply ertucetin 19 hours agoprev7-8 years ago I created GlueList (https://github.com/ertugrulcetin/GlueList) in order to build faster version of LinkedList + ArrayList. It was a fun effort. reply duped 18 hours agoparentIf I have this right, what you've built is this, storing M items in M / N nodes where N is the radix of the array? 0 1 M / Nth node [ N elem ] [.] -> [N elem] [.] -> .... -> [M - (M / N) elem] [null] And so if you want to index the `i`th element you chase the pointers index (node i) : acc = 0 while acc + NLinked lists are taught as fundamental data structures in programming courses, but they are more commonly encountered in tech interviews than in real-world projects. I beg to disagree. In kernels, drivers, and embedded systems they are very common. reply saghm 16 hours agoparentMost people who take data structures courses or perform tech interviews don't end up working on kernels, drivers, or embedded systems though. To me, it sounds like the point being made is that there are a large number of programmers who have learned about linked lists but haven't run into many cases where they needed them in the world world, and I think it's accurate. reply dmitry_dygalo 16 hours agorootparentThis was my intention reply SoftTalker 15 hours agorootparentAgree, I can't recall using anything more complicated than lists/arrays or hash tables (key/value stores) in practice, in many years of (mostly web application) programming. And even those I'm not coding from scratch, I'm using classes or functions that my programming language gives me. For anything more complicated than that, I'm using a database, which of course is using many data structures under the covers but I don't directly touch those. reply sumtechguy 14 hours agorootparentI used to use them all the time. However, now? I would be hard pressed to not use one of the many built in vector/list/dict/hash items in many languages now. I would have to be truly doing something very low level or for speed to use one. reply josephg 13 hours agorootparentAs a counterpoint, I’ve been working on collaborative text editing. I ended up implementing a custom b-tree because we needed a few features that I couldn’t find in any off the shelf library: - My values represent runs of characters in the document. - Inserts in the tree may split a run. - Runs have a size - 0 if the run is marked as deleted or the number of characters otherwise. The size changes as we process edits - Every inserted character has an ID. I need to be able to look up any run by its ID, and then edit it. (Including querying the run’s current position in the tree and editing the run’s size). It’s an interesting data structure problem, and it took a few weeks to have a good solution (and a few weeks more later rewriting it in safe rust & improving performance in the process). I love this stuff. I think it’s pretty rare to find a reason to code your own collection types these days, but it certainly comes up from time to time! reply samatman 11 hours agorootparentprevYou need a linked list to write hello world in any Lisp, though. Seems like the glaring exception to the rule! reply lispm 1 hour agorootparentAs source code, but not necessarily as running code. SBCL: * (defun hello-world () (write-string \"hello world\")) HELLO-WORLD * (disassemble #'hello-world) ; disassembly for HELLO-WORLD ; Size: 36 bytes. Origin: #x100311C85C ; HELLO-WORLD ; 5C: AA0A40F9 LDR R0, [THREAD, #16] ; binding-stack-pointer ; 60: 4A0B00F9 STR R0, [CFP, #16] ; 64: EAFDFF58 LDR R0, #x100311C820 ; \"hello world\" ; 68: 570080D2 MOVZ NARGS, #2 ; 6C: 29EC80D2 MOVZ TMP, #1889 ; 70: BE6B69F8 LDR LR, [NULL, TMP] ; WRITE-STRING ; 74: DE130091 ADD LR, LR, #4 ; 78: C0031FD6 BR LR ; 7C: E00120D4 BRK #15 ; Invalid argument count trap The actual code for this example is machine code (which references a string, which is a vector), here without linked lists. reply kazinator 6 hours agorootparentprevNo, you don't need to use linked lists to send a string to the standard output port in most Lisps. You just call a function. reply wesnerm2 16 hours agoparentprevLinked lists were heavily used in application software before the appearance of standard libraries and Java, which is when dynamically sizable array-based lists become common. There also wasn't a gap between the performance of linked lists and arrays before CPU became significantly faster than RAM. reply hi-v-rocknroll 7 hours agorootparentModern processor and cache performance lend themselves to vectors and SSA. Linked lists just don't scale well outside of niche uses. reply ComputerGuru 16 hours agoparentprevReally only because they’re so goddamn easy. I find myself using linked lists a lot less since adopting rust for embedded code (even with no_std and no allocator, but especially when alloc-only std data structures are within reach). reply ziddoap 16 hours agoparentprev>In kernels, drivers, and embedded systems they are very common. Out of all the programmers in the world, what percentage of them do you think work in the kernel/driver/embedded spaces? reply sfink 15 hours agorootparentFirst, my only guess is that everyone's guesses are going to be wildly wrong. People who work in such spaces will greatly overestimate. People who don't will greatly underestimate. (This is mostly due to how many comments I've read on HN that implicitly assume that most people's problems and perspectives are the same as the commenter's.) Second, linked lists are useful in a lot more places than that. Probably a better proxy would be low-level coders. You almost always want a linked list somewhere when you're dealing with memory addresses and pointers. Maybe not for the primary collections, but there are always secondary ones that need to maintain a collection with a different membership or ordering, and vectors of pointers don't have many clear advantages over intrusive linked lists for those secondary collections. reply josephg 13 hours agorootparentYeah intrusive collections in C is the biggest use I’ve seen. I played with a physics engine a few years ago (chipmunk2d) which made heavy use of intrusive linked lists to store all the objects in the world model. I suspect there’s some clever data structures out there that might have better performance, but the intrusive linked list approach was simple and fast! reply 9659 16 hours agorootparentprev1% reply coldtea 16 hours agorootparentMore like 0.01% -- if we consider enterprise programmers, web programmers, and application/game programmers which I'd expect to be the largest groups... reply hi-v-rocknroll 6 hours agorootparentYep. There aren't many software developers I know who have ever touched {Linux, macOS, FreeBSD, Windows} kernel code except for embedded devs, driver devs, security researchers, hobbyists, and SREs/PEs. The % who have touched kernel bits, wrote a triangle engine scene renderer, wrote a compiler, touched server metal in production, have worked on ASICs, and can put together ML/AI building blocks shrinks way, way down to a handful of living humans. reply TheCondor 15 hours agoparentprevThere are plenty of good uses for linked list and their variants. Like LRU lists come to mind; I couldn't bet that it's the most efficient way to implement them but they're pretty darn good. Then obviously things like breadth first search need a type of queue data structure. It often can come down to memory pressure, if you've got Gigs to spare, then allocating a contiguous block of memory for a list of something isn't a big deal, if memory is tight and maybe fragmented, linked lists can and will get it done. They have their places. I did start to encounter some fresh grads with degrees that said \"computer science\" on them that couldn't answer some basic linked list questions. I was beginning to think it was a bad set of questions until I hit those kids. If you claim to know \"computer science\" and don't know what a linked list is, especially beyond some text books stuff, I'm probably not interested. reply igammarays 16 hours agoparentprevWhy? Why would someone reach for a linked list in a kernel, driver, or embedded system? reply sratner 16 hours agorootparentNo memory allocation/reallocation, preallocated resources managed in e.g. a free list. Also for things like packetized networks, lists are handy for filling as you progress down the stack while using fixed sized packet buffers, or reassembling fragments. In embedded world, memory often needs to be exactly controlled, and allocation failures are fatal without a more complex MMU. In kernel world, I believe the main reason is that allocations can block. reply cyberax 15 hours agorootparentprevIn kernels, it's usually hard to get general-purpose allocation working reliably in all contexts. And you need that for resizable vectors. With lists, you just need to be able to grab an element-sized block. Quite often, it's even done with the memory page granularity. In addition, a lot of data structures might be shared across multiple cores. Linked lists can be traversed and mutated concurrently (although with a bit of care). reply dist1ll 13 hours agorootparentI wonder how much of that is due to the kernel history, and the influence of C idioms, and not because of some inherent design superiority. I'd be convinced once I see pure Rust kernels geared towards modern machines suddenly using linked lists everywhere. Otherwise I'm leaning towards it being a side-effect of the language choice and culture. Also because I've seen the same kind of reasoning applied to compilers (e.g. \"of course you need linked lists in compilers, they are extremely graph traversal heavy\"). But one look at modern compilers implemented in Rust paint a very different picture, with index-based vectors, data-oriented design and flattened ASTs everywhere. reply akira2501 15 hours agorootparentprevO(n) iteration but pretty much guaranteed O(1) for every other operation. If that's the semantic you need, then linked lists are your friend. reply vineyardlabs 15 hours agorootparentprevAny time you have a computer interacting with the outside world in an asynchronous fashion you basically have to have some form of buffering which takes the form of a queue/fifo. A linked list is the most performant/natural way of modeling a queue in our ubiquitous computing infrastructure. I/e in a DMA-based ethernet driver, the ethernet MAC receives packets asynchronously from the processor, perhaps faster than the processor can ingest them. So the mac interrupts the processor to give it new packets, and the processor can't sit processing the packets in the interrupt context, so it needs to put them into some ordered list for processing later when it has downtime. In a true embedded system, the memory for this list is going to be fixed or statically allocated, but you still don't really want to have an array-style list with fixed indexing, as you'll have to manage what happens when the index wraps around back to 0 etc, so instead you just construct a linked list in that pre-allocated memory. I wouldn't say linked lists aren't really used in high-level applications, as I said they're used all over the place whenever you have external asynchronous communication, it's just that modern high-level frameworks/libs totally abstract this away from most people writing high level code. reply kevingadd 16 hours agorootparentprevIntrusive lists are really powerful for those kinds of scenarios, and technically are linked lists. They're widely used in the kernel, IIRC. reply dmitry_dygalo 16 hours agorootparentprevEasier to avoid allocation errors, e.g. in the Linux kernel. I think Alice Ryhl mentioned it here - https://www.youtube.com/watch?v=CEznkXjYFb4 reply SJC_Hacker 16 hours agorootparentHow do linked list prevent allocation errors? If anything it would seem to make them worse. My experience in embedded, everything is hardcoded as a compile time constant, including fixed size arrays (or vectors of a fixed capacity) reply sfink 15 hours agorootparentIntrusive linked lists eliminate the allocation entirely. With a vector, you have the Obj allocation and then potential vector-related reallocations. With an intrusive linked list, you only have the Obj allocation. So your code that adds/removes list entries does no additional allocation at all, it reuses a pointer or two that was allocated as part of the original Obj allocation. Often the list manipulation happens at a time when allocation failures are inconvenient or impossible to handle. reply sratner 15 hours agorootparentprevIn more complex embedded software you are likely to see free lists used to manage pools of preallocated resources (like event structs etc) or pools of fixed sized memory buffers. reply torusle 12 hours agorootparentprevIn embedded, you often need message queues. A common way to implement these is to have an array of messages, sized for the worst case scenario and use this as the message pool. You keep the unused messages in a single linked \"free-list\", and keep the used messages in a double linked queue or fifo structure. That way you get O(1) allocation, de-allocation, enqueue and dequeue operations for your message queue. Another example for this paradigm are job queues. You might have several actuators or sensors connected to a single interface and want to talk to them. The high level \"business\" logic enqueues such jobs and an interrupt driven logic works on these jobs in the background, aka interrupts. And because you only move some pointers around for each of these operations it is perfectly fine to do so in interrupt handlers. What you really want to avoid is to move kilobytes of data around. That quickly leads to missing other interrupts in time. reply hi-v-rocknroll 7 hours agoparentprevI beg to disagree^2. Tasks, threads, and processes are often structured as rings where there is always a \"next\" to maintain simplicity of task switching. The overall architecture of resources is modelable as cyclic graphs but implemented as rings, deques, single LLs, and other data structures. reply dmitry_dygalo 16 hours agoparentprevI'd say most developers don't write kernels/drivers or embeds, at least from what I've seen. I am not saying that there are not many devs like this, but rather that there are fewer kernel devs than web devs. reply davexunit 15 hours agoparentprevI don't do any of those things and I still use lists constantly. Kinda strange to learn that many others don't use them at all it seems. reply nequo 11 hours agorootparentWhat kinds of things are you using them for usually? Is it mostly in C/C++? reply waynesonfire 14 hours agoparentprevlinked lists shine when you can perform a O(1) remove operation if you have a reference to an object on the list. This is very common when using C structs and not possible in Java for example. reply adgjlsfhk1 14 hours agorootparentthese cases are usually cases where you want to use a (hash) Set. If you're Ok changing everyone's indexing, the indexing didn't matter. reply hi-v-rocknroll 7 hours agoprevLinked lists are often the wrong choice because they're rarely performant or efficient when compared to vecs in the real world. In general, use vecs until you absolutely can't. Perhaps there are a few uses for LL's in limited circumstances. reply jkaptur 18 hours agoprevAnother cool aspect of this and (if I understand correctly), where Rust really helps you is that you can explore multiple branches in parallel, since the linked list is immutable. reply ComputerGuru 16 hours agoparentTo be fair, lack of concurrency safety never stopped C and C++ devs from boldly doing just that, anyway! reply eimrine 16 hours agoparentprevWhat if the linked list is cycled or doubly-linked? reply jkaptur 13 hours agorootparentI was talking about this case in particular, where we know the list is basically isomorphic to the call stack. reply amelius 16 hours agorootparentprevThen Rust isn't the right tool for the job. Rust is great for tree-like structures which is 99% of what you encounter anyway. Unless you're writing a kernel or something. reply osigurdson 15 hours agoprevLink lists can move items in O(1) but their O(N) search can be bad because of all of the cache line misses. reply sfink 16 hours agoprevLinked lists get a bum rap. Yes, if you have a simple choice between a vector and a linked list, then the vector is vastly superior due to locality and (for non-intrusive linked lists) allocations. So much so that vectors often win even when you're doing lots of O(n) deletions that would be O(1) with a linked list. But that doesn't mean that linked lists are useless! A vector gives you a single ordering. What if you need multiple? What if you need a free list, which you're never going to be iterating over but will just grab off an item at a time? I find it quite common to have one \"natural\" order, for which I will use a vector (or equivalently, a bump allocator of fixed-size items), and then one or several auxiliary orders like the entries belonging to a particular owner or the ones that will need to be visited for some sort of cleanup or an undo list or some sort of stack or queue of pending items to process. Your common iteration will be in memory order, but that doesn't mean you won't ever want to do different iterations. It annoys me that this is always omitted, with the attitude that linked lists are obsolete and useless because vectors be moar better faster gooder, to the point that it's a waste of time to learn how to manipulate linked lists anymore. I guess a lot of this is probably due to the popularity of high level languages, where you're just dealing with references to everything in the first place. But in those, the arguments in favor of vectors are often not valid because that blessed golden vector of Objects is giving you no more locality than giving your Object a `next` field: the vector of Objects is represented internally as a vector of pointers to the actual Object data, so your oh so nicely cached lookups are doing memory reads that are 100% pure overhead compared to following a `next` link from an Object that fits into a cache line. In both cases, your performance is going to be limited by the locality of your Object data, which is the same whether you have a vector of pointers or Object data with an intrusive pointer. Also, if you have an existing setup and need to introduce a new list, it is sometimes far easier to drop in a `next` (and maybe `prev`) field than to refactor everything to accommodate a new vector. Especially since the vector will move all of your data when resizing the vector, which invalidates any pointers you might be using for other purposes. If you'll be iterating that list frequently, then the vector may very well be a good idea. If it's just for error cases or slow paths, then linked lists really aren't bad at all. I'm not trying to argue for linked lists here so much as arguing against the blanket arguments against them.reply PartiallyTyped 15 hours agoparent> What if you need multiple? You can very much have a single vector owning the memory, and do all other ordering over auxiliary vectors of indices. Should be cheaper and faster than holding more linked lists. If you want to remove elements, you can very much use tombstones to flag deleted elements and then clean up after some threshold. reply sfink 14 hours agorootparent> You can very much have a single vector owning the memory, and do all other ordering over auxiliary vectors of indices. Should be cheaper and faster than holding more linked lists. Cheaper in space depends on the percentage of elements in the auxiliary list. An intrusive list has space for every element to be in the list, which is wasteful if few are. A vector that grows by doubling could waste nearly half of its elements. Indexes can often be smaller than pointers, though, which favors the vector approach. Faster is debatable. Iterating the vector of indexes is quite fast, but indirecting into the data in a random order is still slow. An intrusive linked list doesn't need to do the former, only the latter. (Then again, it also bloats up the data slightly, which matters for small items since fewer fit in cache.) The main reason why linked lists could still be at an advantage is if you don't want allocations in your auxiliary list manipulation path. Maybe you don't want the unpredictable timing, or maybe you can't handle failure. I agree on tombstoning, but note that you're giving up some of the vector advantages by doing so. Your processing loop now has a much less predictable branch in it. (Though really, the vector probably still wins here, since the linked list is built out of data dependent accesses!) Sometimes these auxiliary lists need to contain things that are no longer in the main list, too, as in the case of a free list. (And if you swap such things to the end, then you've just invalidated all of your indexes.) And non-intrusive linked lists can point to variable-size elements (though they lose most of the other benefits of an intrusive linked list.) Anyway, it's the usual \"use the right tool for the right job\", I just claim that linked lists are sometimes the right tool. (Random side note: I use \"indexes\" instead of \"indices\" these days, for a silly reason—\"indices\" is a fine word, I have no issue with it, but it seems to encourage some people to use the non-word \"indice\" which is an abomination.) reply thewakalix 17 hours agoprevWouldn't using push_back prevent the need to reverse the Vec at the end? reply kolbe 19 hours agoprevLinked list benchmarks are amazing.... if you don't thrash your cache on inserts so all its elements are contiguous. You get all the benefits of a vector and a linked list, without the reality that linked lists mostly don't get populated 100% consecutively, and thus can be anywhere in memory. reply bjoli 18 hours agoparentMost languages with linked lists as an important part (lisps mostly) all have well optimized linked lists that end up with a lot better memory locality. reply wavemode 18 hours agorootparentHow does that work? I don't follow how any runtime or compile-time optimization can solve the problem of locality for a linked list. If the data wasn't allocated sequentially, then it's simply not going to be sequential (unless you move it). reply jerf 16 hours agorootparentInstead of thinking of a linked list structurally, think of it functionally. You can have a token that represents a location in the linked list. From that token you have a Fetch() operation that will fetch what is at that location, a Next() operation that will fetch the next token or some indication that you are done, and depending on your language, some sort of insert or append operation (mutability factors in here). While there is a natural encoding of this process into a pointer to the target value, and a pointer to the next node, it is not the only encoding possible by any means. A good exercise if you are having trouble with this is to implement a little linked list that performs those operations on something that simply backs to an array, including the blind O(n) copy when appending another list. It should be quite short. But that is not the only alternate implementation, either, and you can easily build others where the interface is maintained but you pay only log n additional factors maximum for operations, easily recovered from the constant factors which on modern hardware are often staggering. Once you break the entanglement between memory representation and the API a data structure offers in your mind, many ways become obvious as to how to possibly improve this structure while maintaining the same operations, many of which of course already exist and have names. reply Arch485 16 hours agorootparentLinked lists are, by definition, a value with a pointer to the next. That's what makes it \"linked\". The API you're describing is the iterator pattern, which indeed can be backed by almost anything (be it a linked list, array, tree, etc.). reply jerf 15 hours agorootparentBoth replies as I write this are ignoring the question I was answering. The question was, how can a compiler implement a linked list as anything other than a linked list? The answer is what I gave. Being a compiler, it may also do things like an analysis of the use cases to prove that there are no relevant changes to performance (or that performance only improves) and fall back to a \"true\" linked list if it can't prove how it is being used, or, being a compiler, it may just tell you to get stuffed and deal with the performance differences. Depends on the choices the compiler author(s) made. But just because Lisp has cons and cdr does not mean the compiler is obligated to only and exactly use things that structurally look like linked lists in the actual implementation. You need to break the relationship between memory layout and API to understand what is going on here. You may choose later to put them back together; the naive memory layout is often a good one. (Linked lists nowadays are arguably one of the larger exceptions to that, but there are still circumstances even so where that may not be an issue; see Erlang's memory management for instance and ponder how that impacts linked list locality.) But you can't understand what compilers may be doing if you do not realize that there is a distinction. reply wavemode 16 hours agorootparentprev> Instead of thinking of a linked list structurally, think of it functionally If your argument is that a linked-list-like data structure can be implemented using something other than a linked list, then I agree with you. Vector tries (for example) are great for that use case. But a vector trie isn't a linked list, it's a vector trie. As such, it will be faster for some usage patterns, equal for some, and completely degenerate for some others. Just like any other data structure that isn't a linked list. It wouldn't really be an \"optimization\" to implement my linked list code with something other than a linked list - it would be rewriting my code. reply kolbe 15 hours agorootparentEvery linked list discussion I've ever gotten into ends with someone finding a way to modify the std::list data structure in such a way that totally breaks the definition of the LL data structure. We may as well call it \"the law of linked list arguments\" reply hayley-patton 18 hours agorootparentprevSBCL tries its hardest to allocate sequentially, then moves lists to be sequential in GC. reply s17n 17 hours agorootparentThe entire theoretical advantage of linked lists over vectors (constant time insertion and deletion) comes from not sequentially allocating them. reply screcth 17 hours agorootparentIf you have a compacting GC you are going to move the nodes anyway, so why not reallocate them sequentially? reply stonemetal12 15 hours agorootparentThat implies you took the time to sort them, so your O(1) insertion time just became O(N log N). Sure, that cost is spread over how ever many inserts you did between GCs but it isn't O(1) anymore. reply hayley-patton 15 hours agorootparentSBCL cheats and copies starting from the first cons cell that is referenced from outside the list; this tends to be the start of the list, and so traversing just works. The contiguous copying also ends when a cons cell which was already copied is found, so there can't be more discontinuities than there are cons cells referenced from outside the list, regardless of which order we discover the references. reply twic 15 hours agorootparentprevI don't think this is what hayley-patton meant (although it's not crystal clear). I think SBCL does memory management with a bump allocator and a moving collector. So if you build a linked list sequentially, the cons cells will be allocated sequentially, and will be sequential in memory. If you build it in random order, they won't be. But when the collector runs, it will move the whole list, and it will move the cells in sequential order, and then it will be. reply thefaux 17 hours agorootparentprevThere are other advantages of linked lists over vectors besides those that you've listed. reply bjoli 15 hours agorootparentprevWell, I once wrote a small scheme that did loop unrolling and allocated lists in as many steps as the unroll went, which was capped at a maximum of 5. That worked surprisingly well, but there were lots of edge cases since I am a professional bassoonist and not a compiler guy. I generalized it and made all lists allocated in chunks of 16 and then let the GC truncate it. I spent a stupid amount of work doing the first thing and the second thing was done in an afternoon. Then there are all kinds of reply zelphirkalt 12 hours agorootparentprevIf needed, linked list can be backed by a vector, that is resized, when needed. reply Someone 17 hours agorootparentprevA sufficiently smart compiler can detect that a new node gets allocated and then appended to a list, and allocate the node near the other nodes of that list. To have a good chance that there is space near the other nodes, you’d have to reserve memory up front for each list you’d want to do that with, though, and that will kill cache locality before that optimization sets in. Corrections welcome, but I don’t see that (easily) being a net win. But hey, with a sufficiently smart compiler at hand, you can do wonders. (Edit: when looking at pure functions, it many times may not be that hard to discriminate between “this allocation will be returned” and “this allocation is temporary”. For example, if you use map to apply a function to all elements in a list, and that function is pure, detecting that only the return values of that function will be visible after the function returns isn’t that hard) An easier win is that lisp garbage collectors tend to move memory, and thus can try to move nodes that reference each other close together. You get that for free with a semispace collector, but that has quite a few disadvantages, so you don’t want to use that. A generational collector also will do it a bit, though (say you create a few million nodes of garbage to build a list of a few hundred results. Then, after generational collection, those few hundred cells will be closer together than before) reply twic 14 hours agorootparentprevIf anyone knows of a detailed writeup of a language implementation like this, please link to it. The thread under this comment is disappointingly full of \"a sufficiently smart compiler could ...\" type stuff. reply Xeamek 17 hours agoparentprevWhy cant we 'simply' get processor extension to mark data as pointer so that the prefetcher would actually know what to fetch. From my understanding this is what led to the recent 'unpatchable' exploit in Apple's M1, but rather then trying to guess it by some heuristic, why not just give compilers option to make that optimization? reply favorited 15 hours agorootparentApple Silicon does this. If it prefetches something that looks like a pointer, it will also fetch the pointed-to memory. It's a cool feature, and is especially useful for Apple, since their Objective-C collections only store pointers – but it also can re-open the door for certain timing attacks by violating preconditions of constant-time cryptography algorithms. https://en.wikipedia.org/wiki/GoFetch reply Joker_vD 17 hours agorootparentprevI don't think making CPU issue (likely bogus) pre-fetches for every field in the cache line that's marked as a pointer is really that good idea. At best, you save couple of cycles because the fetches are started a one or two instructions earlier before the actual load instruction for \"loading the linked pointer\" is issued. At worst, you keep thrashing your cache loading data you're not going to read, delaying fetching the data you will read. reply Xeamek 15 hours agorootparentFor everything? Obviously no. But for all the crazy optimizations modern compiler do, I don't see how marking pointers for more then couple of them in a raw is that crazy reply Joker_vD 10 hours agorootparentBecause if you're issuing a bogus pre-fetch, you can't cancel it, can you? So that's 90 or something cycles that's the fetch for your actual data is being delayed. Pointer chasing already strains the memory bandwidth, trying to request even more data from memory will only worsen things. And unrolling loop for traversing linked lists can be done, if you use a sentinel node instead of nullptr to signal then end: beqz a0, .end .loop: ld a1, 0(a0) ; a1 = curr->data ld a0, 8(a0) ; curr = curr->next ; do something with payload in a1 here bnez a1, .loop .end: becomes la s1, sentinel beq a0, s1, .end ld a1, 0(a0) ld a2, 8(a0) ld a3, 0(a2) ld a4, 8(a2) ld a5, 0(a4) ld a6, 8(a4) beq a6, s1, .trail .loop: ld t0, 0(a6) ld t1, 8(a6) ld t2, 0(t1) ld t3, 8(t1) ld t4, 0(t3) ld t5, 8(t3) ; do something with three payloads in a1, a3, a5 here mv a1, t0 mv a2, t1 mv a3, t2 mv a4, t3 mv a5, t4 mv a6, t5 bne t5, s1, .loop mv a0, a2 beq a2, s1, .end .trail: ld a1, 0(a0) ld a0, 8(a0) ; do something with payload in a1 here bne a0, s1, .trail .end: As you can see, \"ld t3, 8(a2)\" is almost right after to \"ld t1, 8(a6)\", with intervening load from 0(a2), so prefetch won't noticeably help here, and if the address that ends up in t3 is not in the cache, then \"ld t5, 8(t3)\" will stall no matter what. And moving the speculative loads up in the loop body before processing the payloads (using even more registers, as you can see) somewhat hurts the latency of processing the first three payloads. Oh, and if you want to see something really crazy, look at e.g. splitting the branch instruction into prediction and resolution instructions [0]. [0] https://zilles.cs.illinois.edu/papers/branch_vanguard_isca_2... reply infamouscow 17 hours agorootparentprevThis was the idea of Itanium. It failed mostly because of economics. It turns out programmers, or rather their employers, don't really care about using hardware efficiency. They care about shipping things yesterday, because that's how business deals get closed, and making software efficient is secondary to money changing hands. Performance never really matters to business people after the checks are cashed. Multicore computers have been ubiquitous for more than a decade, yet the overwhelming majority of software built today is single-threaded microservices, where in they spend most of their time serializing and deserializing message payloads. This is all really to say that most performance is already being left on the table for the majority of what computers are used for today. reply ComputerGuru 16 hours agorootparentI do want to say that I think the Itanic would have fared way, way better in a post-LLVM world where the importance of smart, optimizing compilers is much more valued and understood and language designers actively work hand-in-hand with compiler devs far more often (with much more significant back-and-forth from hardware manufacturers). reply gpderetta 12 hours agorootparentI don't think LLVM is particularly good at optimizing VLIW code. Very good optimizing compilers existed before LLVM. Intel had one specifically for Itanium. It wasn't enough. reply ComputerGuru 6 hours agorootparentWhy would llvm be particularly good at optimizing vliw code when there’s no demand for it to be? You can’t believe everything else would remain the same in the hypothetical I posed. reply gpderetta 2 hours agorootparentA) optimizing for VLIW is hard. B) the null hypothesis would be no change. reply Xeamek 17 hours agorootparentprevI mean sure, I don't doubt 99% of end-user programmers wouldn't look twice at something like this, but compilers designers probably would care. And it's not like the companies arent trying this idea (again, M1 exploit). But for whatever reason they want to keep cpus as black box, perfect abstract machines, even though we know they aren't reply kolbe 16 hours agorootparentprevGiven how much of today's computer needs are dependent on a database query, this is no surprise. Who cares about the micros you gain with added efficiency while there's a 100ms db query return in the path? reply Xeamek 15 hours agorootparentApparently Apple and Intel do, since they introduced those changes into their silicon reply kolbe 15 hours agorootparentNot every pipeline involves a db query. reply gpderetta 12 hours agorootparentprevWhere do you think DBs run? reply thefaux 17 hours agoparentprevAn exception to this is precisely parsing and validation in which linked lists can and ideally should be populated consecutively (using an arena allocator). I agree that general purpose linked lists are rarely optimal but they are hard to beat for parsers where they hit a pareto sweet spot of implementation simplicity and good performance. reply dpc_01234 15 hours agoparentprevThis is such an important point. Benchmarking is hard, and very often is done in ideal conditions, which never materialize in real use cases. reply kevingadd 19 hours agoprevI was hoping to see optimization of the actual linked list manipulation and traversal (pipelining? i'm not sure what you'd do), but this is still a neat post. It's cool to see thought put into various parts of the problem, like reallocation/preallocation, stack allocation, etc. reply cogman10 17 hours agoparentreallocation/preallocation actually does increase manipulation and traversal performance. The major slowness of linked lists is cache inconsistency. New nodes can be put all over the memory space. However, if you can make sure all or part of the list exists in contiguous blocks of memory, then there's a good chance that when the CPU loads up the next node it will also grab the next 3 nodes in a cache line. The closer these node addresses are in memory, the faster things will be. reply tomck 17 hours agoprevThis article is disingenuous with its Vec benchmark. Each call to `validate` creates a new Vec, but that means you allocate + free the vec for each validation. Why not store the vec on the validator to reuse the allocation? Why not mention this in the article, i had to dig in the git history to find out whether the vec was getting reallocated. This feels like you had a cool conclusion for your article, 'linked lists faster than vec', but you had to engineer the vec example to be worse. Maybe I'm being cynical. It would be interesting to see the performance of a `Vec` where you reuse the vector, but also a `Vec` where you copy the path bytes directly into the vector and don't bother doing any pointer traversals. The example path sections are all very small - 'inner', 'another', 5 bytes, 7 bytes - less than the length of a pointer! storing a whole `&str` is 16 bytes per element and then you have to rebuild it again anyway in the invalid case. --- This whole article is kinda bad, it's titled 'blazingly fast linked lists' which gives it some authority but the approach is all wrong. Man, be responsible if you're choosing titles like this. Someone's going to read this and assume it's a reasonable approach, but the entire section with Vec is bonkers. Why are we designing 'blazingly fast' algorithms with rust primitives rather than thinking about where the data needs to go first? Why are we even considering vector clones or other crazy stuff? The thought process behind the naive approach and step 1 is insane to me: 1. i need to track some data that will grow and shrink like a stack, so my solution is to copy around an immutable Vec (???) 2. this is really slow for obvious reasons, how about we: pull in a whole new dependency ('imbl') that attempts to optimize for the general case using complex trees (???????????????) You also mention: > In some scenarios, where modifications occur way less often than clones, you can consider using Arc as explained in this video I understand you're trying to be complete, but 'some scenarios' is doing a lot of work here. An Arc approach is literally just the same as the naive approach but with extra atomic refcounts! Why mention it in this context? You finally get around to mutating the vector + using it like a stack, but then comment: > However, this approach requires more bookkeeping and somewhat more lifetime annotations, which can increase code complexity. I have no idea why you mention 'code complexity' here (complexity introduced by rust and its lifetimes), but fail to mention how adding a dependency on 'imbl' is a negative. reply thefaux 17 hours agoparent> how about we: pull in a whole new dependency ('imbl') that attempts to optimize for the general case using complex trees (???????????????) To me this is a self answering question. reply ww520 14 hours agoparentprevYes, when I saw the immutable path is cloned and appended a key, I knew the benchmark was a strawman. reply dmitry_dygalo 15 hours agoparentprev> This article is disingenuous with its Vec benchmark. Each call to `validate` creates a new Vec, but that means you allocate + free the vec for each validation. Why not store the vec on the validator to reuse the allocation? Why not mention this in the article, i had to dig in the git history to find out whether the vec was getting reallocated. The idea comes back to [0] which is similar to one of the steps in the article, and before adding `push` & `pop` I just cloned it to make things work. That's what Rust beginners do. > This feels like you had a cool conclusion for your article, 'linked lists faster than vec', but you had to engineer the vec example to be worse. Maybe I'm being cynical. Maybe from today's point in time, I'd think the same. > It would be interesting to see the performance of a `Vec` where you reuse the vector, but also a `Vec` where you copy the path bytes directly into the vector and don't bother doing any pointer traversals. The example path sections are all very small - 'inner', 'another', 5 bytes, 7 bytes - less than the length of a pointer! storing a whole `&str` is 16 bytes per element and then you have to rebuild it again anyway in the invalid case. Yeah, that makes sense to try! > This whole article is kinda bad, it's titled 'blazingly fast linked lists' which gives it some authority but the approach is all wrong. Man, be responsible if you're choosing titles like this. Someone's going to read this and assume it's a reasonable approach, but the entire section with Vec is bonkers. > Why are we designing 'blazingly fast' algorithms with rust primitives rather than thinking about where the data needs to go first? Why are we even considering vector clones or other crazy stuff? The thought process behind the naive approach and step 1 is insane to me: > 1. i need to track some data that will grow and shrink like a stack, so my solution is to copy around an immutable Vec (???) > 2. this is really slow for obvious reasons, how about we: pull in a whole new dependency ('imbl') that attempts to optimize for the general case using complex trees (???????????????) That's clickbait-y, though none of the article's ideas aim to be a silver bullet. I mean, there are admittedly dumb ideas in the article, though I won't believe that somebody would come up with a reasonable solution without trying something stupid first. However, I might have used better wording to highlight that and mention that I've come up with some of these ideas when was working on `jsonschema` in the past. > I understand you're trying to be complete, but 'some scenarios' is doing a lot of work here. An Arc approach is literally just the same as the naive approach but with extra atomic refcounts! Why mention it in this context? If you don't need to mutate the data and need to store it in some other struct, it might be useful, i.e. just to have cheap clones. But dang, that indeed is a whole different story. > I have no idea why you mention 'code complexity' here (complexity introduced by rust and its lifetimes), but fail to mention how adding a dependency on 'imbl' is a negative. Fair. Adding `imbl` wasn't a really good idea for this context at all. Overall I think what you say is kind of fair, but I think that our perspectives on the goals of the article are quite different (which does not disregard the criticism). Thank you for taking the time and answer! - [0] - https://github.com/Stranger6667/jsonschema-rs/commit/1a1c6c3... reply varispeed 18 hours agoprevnext [3 more] [flagged] dymk 17 hours agoparentOpposite for me; I can read C/C++ fine including messy template code, but Rust's syntax is overall easier for me to read, and has a much simpler grammar. reply actionfromafar 17 hours agoparentprevI often think about this - my pet theory is that the kinds of smart people who can create new languages don't have a problem with a new syntax. While us plebes struggle, learning both new concepts and new syntax at the same time. :) reply ILoveQaWolf 18 hours agoprev [–] this is interesting! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article delves into utilizing linked lists in a data validation library to enhance performance and accuracy in error reporting for JSON schema validation.",
      "Code snippets and benchmarks are provided to compare optimization strategies, including reducing memory allocations and minimizing struct sizes.",
      "Emphasis is placed on avoiding redundant operations like clone() in Rust code to boost performance, with suggestions for readers to investigate further optimizations."
    ],
    "commentSummary": [
      "The discussion on dygalo.dev centers on enhancing JSON data serialization/deserialization in Rust through custom parsers and Abstract Syntax Trees (ASTs) to optimize output size and memory allocation efficiency.",
      "Debates include improving speed and reducing complexity in various programming scenarios by considering alternative data structures like vectors or custom data structures instead of linked lists.",
      "The importance of comprehending and leveraging linked lists in programming for situations where their benefits surpass disadvantages is underscored in the discussion."
    ],
    "points": 176,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1715694686
  },
  {
    "id": 40354130,
    "title": "Protein-Based Gel Reduces Blood Alcohol Levels",
    "originLink": "https://ethz.ch/en/news-and-events/eth-news/news/2024/05/press-release-new-gel-breaks-down-alcohol-in-the-body.html",
    "originBody": "Press Enter to activate screen reader mode. Homepage Navigation Search Content Footer Contact Sitemap Header Main Navigation Menu Homepage News & events ETH Zurich Studies at ETH Zurich Doctorate Research Industry & Knowledge Transfer Campus Services Student portal Alumni association Staffnet Contact lock Login Search search search en en de Departments ETH Zurich Select a department Departments D-ARCH: Architecture D-BAUG: Civil, Environmental and Geomatic Engineering D-BSSE: Biosystems Science and Engineering D-INFK: Computer Science D-ITET: Information Technology and Electrical Engineering D-MATL: Department of Materials D-MAVT: Mechanical and Process Engineering D-BIOL: Biology D-CHAB: Chemistry and Applied Biosciences D-MATH: Mathematics D-PHYS: Physics D-ERDW: Earth Sciences D-HEST: Health Sciences and Technology D-USYS: Environmental Systems Science D-MTEC: Management, Technology and Economics D-GESS: Humanities, Social and Political Sciences Language Selection en English Deutsch You are here Homepage chevron_right News & events chevron_right … ETH News chevron_right All articles chevron_right 2024 chevron_right May chevron_right New gel breaks down alcohol in the body New gel breaks down alcohol in the body Press release Food sciences Researchers at ETH Zurich have developed a protein-based gel that breaks down alcohol in the gastrointestinal tract without harming the body. In the future, people who take the gel could reduce the harmful and intoxicating effects of alcohol. 13.05.2024 by Christoph Elhardt mode_comment Number of comments shareShare Twitter Facebook Linkedin emailEmail linkCopy link The gel could prevent the blood alcohol level from rising. (Adobe Stock, edited with AI) In brief Researchers at ETH Zurich have developed a gel made from whey protein fibrils that uses individual iron atoms to convert alcohol in the intestine into harmless acetic acid before it enters the bloodstream. They showed that in mice, the gel reduces blood alcohol levels by up to 50 percent and protects the body from damage. While further tests are necessary before the gel can be used in humans, the researchers are confident that these will be a success and have already applied to patent the gel. Most alcohol enters the bloodstream via the mucous membrane layer of the stomach and the intestines. These days, the consequences of this are undisputed: even small amounts of alcohol impair people’s ability to concentrate and to react, increasing the risk of accidents. Drinking large quantities on a regular basis is detrimental to one’s health: common consequences include liver disease, inflammation of the gastrointestinal tract and cancer. According to the World Health Organization, around 3 million people die every year from excessive alcohol consumption. Researchers at ETH Zurich have now developed a protein gel that breaks down alcohol in the gastrointestinal tract. In a study recently published in the journal Nature Nanotechnology, they show that in mice, the gel converts alcohol quickly, efficiently and directly into harmless acetic acid before it enters the bloodstream, where it would normally develop its intoxicating and harmful effects. Reducing health damage caused by alcohol “The gel shifts the breakdown of alcohol from the liver to the digestive tract. In contrast to when alcohol is metabolised in the liver, no harmful acetaldehyde is produced as an intermediate product,” explains Professor Raffaele Mezzenga from the Laboratory of Food & Soft Materials at ETH Zurich. Acetaldehyde is toxic and is responsible for many health problems caused by excessive alcohol consumption. In the future, the gel could be taken orally before or during alcohol consumption to prevent blood alcohol levels from rising and acetaldehyde from damaging the body. In contrast to many products available on the market, the gel combats not only the symptoms of harmful alcohol consumption but also its causes. Yet, the gel is only effective as long as there is still alcohol in the gastrointestinal tract. This means it can do very little to help with alcohol poisoning, once the alcohol has crossed into the bloodstream. Nor does it help to reduce alcohol consumption in general. “It’s healthier not to drink alcohol at all. However, the gel could be of particular interest to people who don’t want to give up alcohol completely, but don’t want to put a strain on their bodies and aren’t actively seeking the effects of alcohol,” Mezzenga says. Alcohol degradation in the body with and without the new gel. (Visualisations: ETH Zurich / Adobe Stock) Main ingredients: Whey, iron and gold The researchers used ordinary whey proteins to produce the gel. They boiled them for several hours to form long, thin fibrils. Adding salt and water as a solvent then causes the fibrils to cross-link and form a gel. The advantage of a gel over other delivery systems is that it is digested very slowly. But to break down the alcohol, the gel needs several catalysts. The researchers used individual iron atoms as the main catalyst, which they distributed evenly over the surface of the long protein fibrils. “We immersed the fibrils in an iron bath, so to speak, so that they can react effectively with the alcohol and convert it into acetic acid,” says ETH researcher Jiaqi Su, the first author of the study. Tiny amounts of hydrogen peroxide are needed to trigger this reaction in the intestine. These are generated by an upstream reaction between glucose and gold nanoparticles. Gold was chosen as a catalyst for hydrogen peroxide because the precious metal is not digested and therefore stays effective for longer in the digestive tract. The researchers packed all these substances – iron, glucose and gold – into the gel. This resulted in a multi-stage cascade of enzymatic reactions that ultimately converts alcohol into acetic acid. Gel works in mice The researchers tested the effectiveness of the new gel on mice that were given alcohol just once as well as on mice that were given alcohol regularly for ten days. Thirty minutes after the single dose of alcohol, the prophylactic application of the gel reduced the alcohol level in the mice by 40 percent. Five hours after alcohol intake, their blood alcohol level had dropped by as much as 56 percent compared to the control group. Harmful acetaldehyde accumulated less in these mice, and they exhibited greatly reduced stress reactions in their livers, which was reflected in better blood values. In the mice that were given alcohol for ten days, the researchers were able to demonstrate not only a lower alcohol level but also a lasting therapeutic effect of the gel: the mice that were given the gel daily in addition to alcohol showed significantly less weight loss, less liver damage and hence better fat metabolism in the liver as well as better blood values. Other organs in the mice, such as the spleen or the intestine, as well as their tissues also showed much less damage caused by alcohol. Patent pending In an earlier study of administering iron through whey protein fibrils, the researchers had discovered that iron reacts with alcohol to form acetic acid. As this process was too slow and too ineffective at the time, they changed the form in which they attached the iron to the protein fibrils. “Instead of using larger nanoparticles, we opted for individual iron atoms, which can be distributed more evenly on the surface of the fibrils and therefore react more effectively and quickly with the alcohol,” Mezzenga says. The researchers have already applied for a patent for the gel. While several clinical tests are still required before it can be authorised for human use, the researchers are confident that this step will also be successful, as they already showed that the whey protein fibrils that make up the gel are edible. Reference Su J, Wang P, Zhou W, Peydayesh M, Zhou J, Jin T, Donat F, Jin C, Xia L, Wang K, Ren F, Van der Meeren P, García de Arquer P, and Mezzenga R. Single-site iron-anchored amyloid hydrogels as catalytic platforms 1 for alcohol detoxification. Nature Nanotechnology. DOI: external page10.1038/s41565-024-01657-7call_made Contact Raffaele Mezzenga Phone phone+41 44 632 91 40 E-Mail emailraffaele.mezzenga@hest.ethz.ch ETH Zurich Switzerland remove add Show more Show less Franziska Schmid Media Relations Phone phone+41 44 632 89 41 E-Mail emailfranziska.schmid@hk.ethz.ch contactsvCard Download ETH Zurich Switzerland remove add Show more Show less Downloads Download vertical_align_bottom Press release (PDF, 139 KB) Newsletter subscription chevron_right Get the latest ETH News everyday Share article Twitter Facebook Linkedin email Send article by mail link Copy link Similar topics Food sciences Materials science Comments Leave a comment Leave a comment We are happy if you comment on articles on ETH channels, ask questions or respond to comments from other readers. Please note our comment policy when doing so. Comment No comments yet Footer Recommended links Media information Search Keyword or person search Follow us Services Student portal Alumni association Staffnet Contact lock Login Departments D-ARCH Architecture D-BAUG Civil, Environmental and Geomatic Engineering D-BIOL Biology D-BSSE Biosystems Science and Engineering D-CHAB Chemistry and Applied Biosciences D-ERDW Earth Sciences D-GESS Humanities, Social and Political Sciences D-HEST Health Sciences and Technology D-INFK Computer Science D-ITET Information Technology and Electrical Engineering D-MATH Mathematics D-MATL Department of Materials D-MAVT Mechanical and Process Engineering D-MTEC Management, Technology and Economics D-PHYS Physics D-USYS Environmental Systems Science Table of contents and legal Sitemap Imprint Accessibility Statement Disclaimer & Copyright Data protection © 2024 Eidgenössische Technische Hochschule Zürich",
    "commentLink": "https://news.ycombinator.com/item?id=40354130",
    "commentBody": "New gel breaks down alcohol in the body (ethz.ch)174 points by geox 20 hours agohidepastfavorite197 comments circlefavshape 15 hours agoSomething to sober you up if you're already drunk would be amazing, but I don't think this will do that reply fluxist 14 hours agoparentDihydromyricetin[1] can accomplish this remarkably effectively. It's available on Amazon. Also works great for hangovers. [1] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3292407/ reply dynm 12 hours agorootparentJust a warning to everyone: This effect doesn't seem to have much scientific support beyond the cited paper. Other work has followed up and was not able to replicate: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8603706/. reply jajko 2 hours agorootparentThe test is trivial - take a breathalyzer and compare results after say 2, 3, 5 beers before and after. Some mental / IQ test could be added to make sure we don't have folks driving around passing breath test while being very drunk. Something tells me if they can't provide such a trivial result, it ain't working as folks expect (and god knows what nasty side effects it can have). reply manmal 13 hours agorootparentprevGreat suggestion. There is also Kislip, which seems to be based on probiotics and, like DHM, also helps metabolize acetaldehyde. Acetium (a Finnish product) also claims to lower acetaldehyde, but that might be a localized effect (mouth/nasopharynx + GI tract). reply mrtesthah 3 hours agorootparentZbiotics gives you genetically modified probiotic bacteria that secrete ALDH into your gut. reply cogman10 1 hour agorootparentI'm not sure how that's helpful. It looks like ALDH is responsible for breaking down acetaldehyde. However, the pathway for ethanol to become acetaldehyde happens primarily in the liver with ADH. So while this may flood your stomach with ALDH, that's mainly only useful in the blood stream after alcohol is metabolized right? Is the theory here that because the digestive tracks is a large organs filled with blood the resulting ALDH would interact? My next question is do these, and how many, probiotic bacteria survive the acid wash of the stomach? Are there studies around this? reply losteric 11 hours agorootparentprevNote the study involves injections. The oral route is subject to digestion. reply colechristensen 13 hours agorootparentprevI learned about this and it does work in my experience, best if taken both at the beginning and end of the night. While it doesn’t alleviate all of the hangover symptoms it does nearly eliminate all of the most unpleasant ones. reply skrebbel 53 minutes agorootparentWhich are the pleasant hangover symptoms? reply cess11 16 minutes agorootparentModerate lethargy, satisfaction from eating fatty and protein rich foods, things like that. If it wasn't pleasant, not that many people would endure it regularly as there are. reply asdfman123 10 hours agoparentprevWhat we really need is something that helps quickly break down acetaldehyde, which is what alcohol is broken down to in your bloodstream and is responsible for all the negative side effects. reply jghn 10 hours agorootparentCheck out a product named ZBiotics [1] which claims to do this. I can't tell you that it for certain works as advertised, but between myself and several other people now in my friend group, all but one swears by the results in terms of the morning after. [1] https://zbiotics.com reply warble 7 hours agorootparentConfirmed. Zbiotics works great. It's a very noticeable difference. reply ivanb 13 minutes agorootparentprevLemon juice. reply dhbanes 5 hours agorootparentprevN-acetyl cysteine reply aantix 15 hours agoparentprevSulforaphane. Take something like Broccomax - or eat lots of Brussels sprouts, and broccoli. reply BobbyTables2 7 hours agorootparentI’d probably have to drink heavily before I could eat significant quantities of Brussels sprouts… reply satvikpendem 7 hours agorootparentBrussel sprouts are quite good these days, because they've been genetically engineered over the past 30 years to be less bitter. They're not your parents' sprouts anymore (especially roasted for that Maillard reaction, not boiled as they would've likely done). reply romafirst3 4 hours agorootparentAre you working for Big Sprout? I seem to remember seeing that exact phrase on here in the last couple of months. \"Not your parents sprouts anymore\" For the record I loved them 30 years ago and still love em. My recipe is very different to the modern fashion, boiled until mushy with a slice of smokey bacon, drain and eat with butter. They literally melt like butter - delicious. reply tayo42 4 hours agorootparentprevBrussel sprouts at restaurants are basically candy lol. Yeah they're roasted and crispy and cooked with bacon fat and usually some sweet sauce added lol. reply petesergeant 7 hours agorootparentprevBritish people combine both at Xmas reply zolbrek 14 hours agorootparentprevShould I take it when I'm ready to sober up or before drinking as well if I want to make sure I don't get too drunk but still have a good buzz? reply aantix 14 hours agorootparentTake it a couple of hours before drinking. You won't feel as drunk and will have less of a hangover. reply voisin 13 hours agorootparentWhat about all the other negative impacts though - like liver impact and blood pressure. Isn’t it better to just moderate your drinking rather than drinking to excess and then dialing back the feel of it with broccoli? reply aantix 9 hours agorootparentAgree. Of all the negatives, I think diminished sleep is the one that doesn’t get enough attention. It’s superficial sleep at best. You’re up peeing 3,4,5 times a night. You never enter REM sleep. You’re dragging the entire next day. All to do it over again to your body the next day? Of the people that I know that drink actively almost every day, they look like shit, and lack motivation. Their body is crying out for rest and better food but they just keep drinking through their signals. reply agos 1 hour agorootparentnot a sleep expert by I think the biggest impact is on deep sleep, not REM reply iamthirsty 13 hours agorootparentprevProbably. reply gerad 10 hours agorootparentNot to mention the cost. reply eps 12 hours agorootparentprevAn older method is to eat a couple of tablespoons of butter 30 min before the first drink. Works quite well to reduce the effects of alcohol. reply lukan 11 hours agorootparentOr any other food with lots of fat. The fat makes the body absorb the alcohol slower. And drinking lots of water just before sleeping greatly reduces hangovers. What also works, is drinking in moderation .. reply zolbrek 13 hours agorootparentprevThanks, I'll give it a go. reply glandium 10 hours agoparentprevTurmeric-based drinks we can find here in Japan are said to work miracles against hangover. I rarely drink alcohol, so I can't tell whether that's true. reply duskwuff 5 hours agorootparentUnlikely. The alleged medical benefits of turmeric are largely illusory; the only medically interesting property that it's been conclusively shown to have is that it interferes with a wide range of biochemical assays. https://pubs.acs.org/doi/10.1021/acs.jmedchem.6b00975 reply klausa 6 hours agorootparentprevI don't know if it's a placebo effect; but I find that they _do_ work. _But_ they also make you less drunk and making the drunkenness less... pleasant? Sample size of like... 4 nights out having nomihoudai. reply cvdub 15 hours agoparentprevAgreed, not seeing many good use cases. It could be prescribed to alcoholics who can’t/won’t stop drinking to help them taper off alcohol, but people would just end up drinking more to counter it. The best use I can think of is for undercover agents to drink heavily and avoid intoxication! reply prmoustache 3 hours agorootparentAs long as it doesn't leave your palet with a taste of the gel, the point is to enjoy the taste of these drinks without the effects of alcohol. Not everybody drink alcohol for the alteration effects and the non-alcohol versions of most beverages is so shitty you don't really want them. My mother for example hate the feeling of being intoxicated, even a little. So she only drink one glass on occasions and barely finishes it. She would definitely enjoy it more if she could have different wines over the course of a meal. It would also be useful if you want to enjoy the taste of some drinks with alcohol but you need to drive or you are in an on-call schedule. reply Derbasti 5 hours agorootparentprevYou could perhaps drink and drive. reply readthenotes1 14 hours agorootparentprevThat's enough of a good use case! It would have made my college years much better... reply scrumper 7 hours agorootparentUseful too for sales people at business dinners, execs doing corporate entertainment, journalists, even politicians - anyone who's employment requires them to engage in quasi-social activities with 3rd parties where it's awkward to refuse a drink or six. reply TacticalCoder 14 hours agoparentprev> Something to sober you up if you're already drunk would be amazing ... But it's the fun of alcohol! What'd be amazing would be something that really works against the headaches and/or prevent vomiting. Basically something for the next day or even the next two days (when you get old, if you party too hard it can take two days to feel good again: I hate it so at 50 I very rarely party hard anymore: maybe once a year). reply hylaride 14 hours agorootparentHangovers are a good part caused by by the body reacting to the \"damage\" that alcohol does: - Alcohol loosens the blood vessels, whereas a lot of the effects of a hangover are caused by the body then \"over-constricting\" when the aclohol goes away, causing headaches and nausea. The cause of the headache is similar for a brain-freeze from too much ice cream causing the veins in your neck and throat to constrict, though the brain-freeze goes away as you warm back up. - There's also the dehydration as alcohol throws off the balance of water in your system as it makes you want to pee more, but interferes with the body's ability to actually absorb water. - Alcohol causes your body to pump out more \"feel good\" hormones, which then lead to a crash later. So the way to prevent a hangover is to not get drunk in the first place, same as always. If you want to drink and limit the effects of alcohol (including the initial \"benefits\") then this has potential. It may prevent vomiting in the sense that vomiting is your body trying to eject the poison as it's building up faster than it can process it (eg when you're already trashed an on the train to hangoverville). However, people often tend to drink for the \"good\" effects that this gel prevents. If what you want is alcohol's fun, then you're going to need another cure (essentially an IV drip, and drugs to replace the hormones and loosen your blood vessels - all of which are not readily available for other good reasons). reply filleduchaos 8 hours agorootparentA reliable way to prevent hangovers in my experience is to simply pace yourself properly as well as eat and drink plenty of water. People go out partying on near-empty stomachs and take crazy amounts of alcoholic drinks with not so much as a drop of water all night; it's no surprise that their bodies react that extremely. Also, sticking to drinks with low levels of congeners helps immensely. Ethanol is already toxic enough without throwing other more dangerous alcohols into the mix. reply kijin 5 hours agorootparentI wonder if there's scientific consensus on how much water is \"plenty\". Like, what's the ABV I should target, averaged over every type of liquid I drank that night? Regular beer contains about 20x as much water as it does ethanol (by volume), but apparently that's not enough. It's certainly better than wine, though, which only contains 6-8x the water and an entire catalogue of congeners. reply prmoustache 4 hours agorootparentUnless it is a strong beer with 2 digits of alcohol per volume, I rarely get drunk enough on beers alone because of all I need to pee to get to that point. reply maeil 2 hours agorootparentprevI'm sure I'm not the only one who just really enjoys the taste of alcohol and would love a way to drink more of it without its effects, both the \"positive\" and the negative side. It's genuinely one of my big wishes! I'd pay 5x the normal price for a bottle of whisky that neither has the \"good\" nor \"bad\" parts, as if it was water but with the same taste. reply pcl 1 hour agorootparentI don’t think they do whiskey yet, but ISH Spirits makes some tasty alcohol free products. https://ishspirits.com/ reply Nursie 6 hours agorootparentprevSome of it is down to that. Some is down to the metabolic products of ethanol, specifically aldehydes. There are other alcohols (like tert-Amyl alcohol) that people have used recreationally which don't cause hangovers in the same way. There are other great reasons not to mess with these substances (tert-Amyl is very long lasting and as little as 30ml might kill you), but they do show that some of the hangover is down to the specifics of ethanol's breakdown in the body. reply felipemnoa 12 hours agorootparentprev>>What'd be amazing would be something that really works >>against the headaches and/or prevent vomiting. If you start vomiting then you overdid it. Your body is trying to save you from poisoning. To prevent the headaches just drink lots of water. reply bradleyjg 14 hours agorootparentprev> But it's the fun of alcohol! For a while it is. But sometimes you don’t want to keep being drunk. Maybe you want to drive home, or your childcare ends, or you just have other things to do that day. If I could just end the whole thing at will, including the hangovers you mention, that would be ideal. reply neves 14 hours agorootparentprevI'd love to take one of these when I'm going home. There's no use to keep absorving alcohol when I want to go to sleep. reply somesortofthing 11 hours agorootparentprevI have really awful genetics for drinking(I'm 23 and already experience the thing you're describing with taking multiple days to feel good again, and instead of physical symptoms I get crushing depression) so I'd love to just take some of this stuff before a night out to drink the same volume as other people without having to deal with the aftereffects. reply OSI-Auflauf 8 hours agorootparentThats interesting. Are you ok for a 1on1 talk about this topic? reply voisin 13 hours agorootparentprevI find marijuana to be the fun of alcohol without the impacts you mention. reply beeboobaa3 13 hours agorootparentprev> maybe once a year you sure you didn't just lower your tolerance? reply psunavy03 13 hours agorootparentYour tolerance naturally changes as you get older and the hangovers last longer. reply Arrath 12 hours agorootparentIts cyclical. The tolerance for hangovers and the recovery period went down (god do I hate it) therefore I drink less, thus I have a lower tolerance for alcohol. reply navaati 12 hours agorootparentprev> But it's the fun of alcohol! Meh, I like wine more and more and being drunk less and less, one is seriously limiting the other :) reply DennisP 12 hours agorootparentI'm the same way with whisky. I might have to imitate the professional tasters: just swirl and spit. Seems like a travesty but less so than letting my collection gather dust indefinitely. reply idontpost 11 hours agorootparentprevI now alternate between regular beer and NA beer. I get to drink as much as I want, I do enjoy the taste, and I don't get drunk or hungover. reply prmoustache 4 hours agorootparentI am not looking at being drunk while having beers but haven't found a non-alcoholic beer yet that would be nearly drinkable. reply m463 12 hours agoparentprevdisulfiram will do it ... in a more philosophical sense. also known as antabuse. https://en.wikipedia.org/wiki/Disulfiram (if you take it, you will get sick if you drink alcohol) Personally, tequila works on me in a similar way when I smell it. reply dbbk 11 hours agorootparentNaltrexone is a good alternative to this that doesn't make you physically sick. It just dulls the effect to the point where you kinda just get bored of drinking. reply petesergeant 7 hours agorootparentAlso it’s half of the weight-loss drug Contrave, for similar reasons reply binary132 10 hours agoprevSounds like a good way to save someone from severe alcohol poisoning. Not sure I see much of a point otherwise, tbh. reply jszymborski 10 hours agoparentSome folks, like myself, love the taste of a peaty whiskey, but have bodies that don't agree with alcohol. reply notnmeyer 10 hours agorootparenthrm, smoke peat cigarettes instead? reply test6554 7 hours agorootparentprevI can’t stand the taste of any alcohol. I literally only consume it for one reason. The intoxicating effect. I get enough to get the job done then stop. So this gel seems like drinking less booze with extra steps. I do see a use for women and undercover cops though. Or to make people think you are drunker than you are. reply dullcrisp 5 hours agorootparentBecause everyone who isn’t you is a woman or an undercover cop? reply SOLAR_FIELDS 10 hours agoparentprevThey already noted in the article that once it’s passed into your bloodstream this approach is useless. I see it more like being a better form of naltrexone, someone takes it before going on a night out and saves themselves a nasty hangover while getting to party. I’ve always disliked the lack of availability of low alcohol beers in the States (1-2%) and this could be a decent solution for that. As is the easiest thing to do for moderation when going out in the States for me is to do the One On One Off where you alternate between one moderate alcohol drink and one NA drink. It would be more ideal to just pop a pill right before I go out that lets me tailor the level of absorption exactly as I want it. reply pdonis 10 hours agorootparent> someone takes it before going on a night out and saves themselves a nasty hangover while getting to party But if their way of partying is to get drunk, this gel will prevent them from partying, since getting drunk requires the alcohol to get into your bloodstream. reply tristor 10 hours agorootparentprevI was unaware of people using naltrexone for hangover prevention and did a bit of spot research, it seems naltrexone only prevents the buzz/euphoria from alcohol consumption but doesn't prevent impairment or hangover symptoms, such as those caused by dehydration, nor does it prevent liver damage. reply SOLAR_FIELDS 9 hours agorootparentIf you’ve never taken it before: it reduces cravings too. If your goal is to get drunk, it won’t stop you, but it does help to quell the dopamine beast if that’s not your goal reply arpa 1 hour agoparentprevpeople with autobrewery syndrome come to mind immediately. reply adfm 12 hours agoprevJim Koch, founder and brewer of Sam Adams, says that he mixes a teaspoon of bakers yeast (not sure which) and yogurt to break down the alcohol before it hits the bloodstream and has been doing it for years if not decades. https://www.npr.org/sections/thesalt/2014/07/10/327854051/al... reply joshuahaglund 11 hours agoparentOne guy says it works. The rest of the article is about a small experiment that showed otherwise. Then it quotes experts who explain why it doesn't work and called it an urban legend. reply adfm 7 hours agorootparentYou are correct! reply Bognar 9 hours agoparentprevI heard a different variant of this, which is that Jim Koch the brewer at Sam Adams is a functioning alcoholic who drinks a lot throughout the day. The yogurt and yeast is an unrelated thing that he claims to work. reply somesortofthing 11 hours agoparentprevHypothetically, if the yeast could work fast enough(I'm 99% sure it can't unless he's leaving the mixture in a sealed container for a few months before drinking it) wouldn't this result in straight vinegar reply pavel_lishin 11 hours agoparentprevDoesn't yeast produce alcohol? edit: ah, I read TFA: > \"Yeast can degrade ethanol,\" says microbiologist Benjamin Tu of the University of Texas Southwestern Medical Center. \"But they love other sugars — glucose, maltose — more. When those sugars are around, the cells turn off the genes needed for alcohol degradation.\" reply qwerty456127 5 hours agoprevAs an alcohol-hating wine-lover I can't wait to get this product. Hopefully something similar is going to be invented to break down sugar as well. reply stockboss 3 hours agoparentwhat for? sugar is already broken down in your body into glucose to be absorbed. there are already enzymes you can take to help break it down faster, but it's all going to be absorbed regardless. perhaps what you really want is something to block glucose absorption? reply Jensson 3 hours agorootparentYou could invent resistant bacteria that eats all the sugar in your stomach. There are already such but they are lethal if you don't do much about them, but maybe it is possible to make nice versions of them that just eats sugar and don't kill you by eating everything. reply misja111 1 hour agorootparentEhm, we humans need a certain amount of blood sugar to survive. Having such a resistant bacteria living in my stomach seems a terrible idea .. reply jawns 15 hours agoprevIf you're an alcoholic, and your body is physically dependent on alcohol consumption, would this allow you to satisfy your physical cravings without the deleterious effects normally associated with alcohol addiction? Kind of like how a nicotine patch helps satisfy the physical addiction? reply sctb 14 hours agoparentThat's exactly what benzodiazepines are for, but only for short-term taper protocols as the dependence/withdrawal profile over the long term is worse than alcohol. Medium-term can be covered by gabapentin. reply readthenotes1 14 hours agorootparentI know an alcoholic who was also addicted to gabapentin. It didn't reduce his alcoholism, and even when he tried to stop drinking, he would get stoned on the gabapentin (and was largely unaware of his diminished state) reply fragmede 9 hours agorootparentHow did ozempic treat him? reply hn72774 12 hours agoparentprevNo, there would be a risk of dt's, seizures, and death. Quitting alcohol requires medical supervision for those who are deep into alcohol use disorder. reply sandworm101 15 hours agoparentprev>> breaks down alcohol in the gastrointestinal tract without harming the body. Likely no. This stuff will destroy the alcohol before it enters the bloodstream. So it sounds useful to stop people absorbing more alcohol (pumping the stomach situations) but won't do much for alcohol already in the blood and causing effects in the body. reply gus_massa 13 hours agorootparentWoudn't people just consume more alcohol to compensate the one destroyed by this invention? My guess is that some persons drink until they are drunk, not until they took N glasses of drinks. reply rldjbpin 2 hours agoprevi wonder if it is also effective against methanol to help those who consumed denatured alcohol. reply SCAQTony 4 hours agoprevWith great advantage comes equal disadvantage, I wonder what the side effects are? reply throwup238 15 hours agoprev> In the future, people who take the gel could reduce the harmful and intoxicating effects of alcohol. I understand harm reduction but what is the point of reducing the intoxicating effects of alcohol? What's the point? reply aredox 15 hours agoparentAlcohol is a solvent for many flavours. That's why you have a lot of it in perfumes, and that's why alcohol-free drinks don't taste as good as the originals. In Europe, we drink a lot of alcohol with food exactly for that reason (wine dissolves the fats of the sauce or just the meat and mixes and enhances the taste). I'd really like to enjoy it without any drunkenness afterwards nor any effect on my health. reply pavlov 14 hours agorootparentYeah, if there was a product that would let me enjoy real wine but the effect of the alcohol would be similar to a 3% mild beer, I would use that fairly often. A typical wine at 11-12% is a bit too strong for many situations. reply emmanuel_1234 15 hours agoparentprevFrom alcohol, I love: - drinking it: I really enjoy the feeling of just ingesting beer, wine or spirits, especially with friends or family - having a nice buzz from it. However, those two things are, for me, incompatible. If I start drinking a little bit, I usually don't stop until I'm way beyond the \"nice buzz\". As the joke goes, \"one beer is not enough, two beers are just enough, three beers really aren't enough\". Having something that would allow me to keep drinking without jeopardizing my body, my mind, and the day after would be a huge game changer. reply CaptainOfCoit 15 hours agorootparentSounds like maybe the \"alcohol free\"/\"0.0%\" beer (not really 100% alcohol free) is something you should try. Tastes and looks like beer, but doesn't come with the buzz (which is the thing that your brain hooks into and uses to tell you it isn't enough yet). reply iamthirsty 13 hours agorootparent> Tastes and looks like beer I have tried almost every major non-alcoholic beer in the U.S., and none of them truly taste exactly like beer. reply oarsinsync 1 hour agorootparent> I have tried almost every major non-alcoholic beer in the U.S., and none of them truly taste exactly like beer. Agreed, but if you drink them exclusively for a few weeks, your taste buds and brain will reframe around them, and it'll cease to be a problem. I did this with Brewdog's Punk AF, and after 2 weeks on that, most non-alcoholic beers triggered my brain's \"I'm drinking beer!\" response. It's a lot of work for overpriced soft drinks, and isn't for everyone. Also, I'm back to drinking real beer again anyway. reply pjot 13 hours agorootparentprevThere’s a brand I’ve seen in stores called “Athletic” and it’s nearly indistinguishable reply badgersnake 12 hours agorootparentprevOver in the UK I think Adnams Ghost Ship 0.5% is the best one commonly available that I’ve had. Most of them are far too sweet. reply Nursie 6 hours agorootparentprevThey are improving all the time. Some are really good. If you like german weissbeer then Franziskaner Alkoholfrei is more or less indistiguishable to me. There are other good ones too. reply xgkickt 7 hours agorootparentprevTry alternating between water and beer. You’ll drink half as much, and at a better pace. reply swozey 14 hours agorootparentprevMicrodosing psilocybin (shrooms) is amazing at letting me go out and just have 1-2 beers while still giving the slight affect of a nice anxiety-free buzz. I'm what I'd call a nervous drinker. I'm ADHD and sitting still can be rough for me, so at bars I tend to drink a lot very fast because the only fidgeting I can do without looking strange is .. cup to face over and over. Obviously not something everyone wants to do or can do, it's legal for me, but it's great for basically taking my interest in alcohol away after a beer or two. Microdosing is usually 1/10th or less what a \"normal\" light dose would be (going with 1-2g dose here, so 0.1/0.2). No weird/visual/hallucination effects or anything like that. You just feel a bit lighter and relaxed. Been a big game changer for me since I go out all the time. A bunch of my service industry friends and I do it and they've all started drinking significantly less. Naltrexone is great too but it's probably easier to get someone to microdose than ask their doctor for that which is unfortunate. You won't get a buzz with that, you'll just get bored of drinking by your 2nd beer and move on to doing other things which can also be nice at times. reply joekrill 15 hours agoparentprevThere's many occasions where I don't want to drink, but not doing so can make things uncomfortable. It's unfortunate, but there's lots of occasions where abstaining can make other people uncomfortable, or cause them to give you a hard time, or any number of other things. I would love to be able to take something that allows me drink but with none of the side effects, just to avoid potential uncomfortable situations. Again, I'm not condoning the behavior and it's unfortunate that this is how society is sometimes. But it would be nice to have this option. There's also people who have difficulty controlling their drinking once they start. This would certainly help in those scenarios. Another example is a situation where a woman may be pregnant so doesn't want to drink, but may not be far enough along that she wants people to know. Not drinking would get people wondering, but drinking could harm the fetus. I don't know if this gel would make drinking safe in that scenario - but if it does, that would be a great situation to use it in. reply hn72774 15 hours agorootparentClub soda with lime. No one knows there's not vodka in it but you. reply colecut 14 hours agorootparentClub soda and bitters has been a recent goto reply FredPret 15 hours agorootparentprevIf people are uncomfortable with you not drinking, that's very much on them reply joekrill 15 hours agorootparentAgreed. And ideally that shouldn't matter. But in the real world there are other consequences and side effects. Maybe I don't want to make them uncomfortable. Maybe I just don't want to deal with the nagging, \"busting my chops\", or the questions about why I'm not drinking. Maybe I want to avoid some preconceived notions certain people may have about people who don't drink. So even if it's on them, there are still personal reasons I may want to avoid the situation. reply FredPret 14 hours agorootparentPeople will take their cue from you as to how to respond. If you act embarrassed about it, they’ll react accordingly. If it’s no big deal to you, same thing for them. I quit cold turkey years ago. I was worried about the same things. But even my most macho & hard-drinking friends just accepted it without comment. reply gwbas1c 13 hours agorootparentprevI encountered a lot of peer pressure to drink when I was in college. (I didn't drink much because a lot of alcohol makes my stomach upset.) Don't surround yourself with people who make you uncomfortable. It's a lesson I realized as I got older: It has little to do with drinking; but if the people around you make you feel uncomfortable not drinking, then you're around the wrong people. reply FredPret 11 hours agorootparent> Don't surround yourself with people who make you uncomfortable. I wish I learnt this at age 0. This goes for online life as well. reply Tepix 11 hours agorootparentprev> There's many occasions where I don't want to drink, but not doing so can make things uncomfortable. People don't care as much if/what others drink as you think. reply zoklet-enjoyer 15 hours agorootparentprevStop giving in to peer pressure. You'll be happier. reply rfrey 14 hours agoparentprevThe hardest part of reducing my alcohol consumption is that I love red wine so much. The flavor, the aroma, the acidity, everything. I have stopped drinking all alcohol except red wine because of the health effects. I would be delighted to have a way to drink wine without any intoxication. reply testless 14 hours agorootparentRed wine being healthy is a myth. But misunderstood, see below… reply 0xffff2 13 hours agorootparentGP is saying they have stopped drinking most alcohol (all except red wine) because of the [negative] health effects. reply rfrey 10 hours agorootparentprevI know red wine is just as bad for me as any other alcohol, which is to say quite bad, and in any dose. I meant I've largely quit because of that. But I just can't quit you, Red Wine. reply rpmisms 12 hours agorootparentprevIt has health benefits. The alcohol in it is no better for you than any other alcohol. reply standardUser 5 hours agoparentprevI like really strong drinks like overproof whiskey and IPAs and craft cocktails, which can be shockingly alcoholic sometimes. It would be nice to have essentially a higher alcohol tolerance so I could drink the drinks I like most more freely. Now, I usually just eat a huge meal before I drink a lot, but a pill would be much better. reply alamortsubite 15 hours agoparentprevNon-alcoholic beer sales are close to $40B/year globally and going up. Generally speaking, the flavor of non-alcoholic beer pales in comparison to the real thing. Then there's also the market for wine and spirits. reply 2024throwaway 15 hours agorootparentThere are some new, very good, NA beers on the market these days. Untitled Art and Athletic are two brands that are doing great work. reply cpfohl 6 hours agorootparentThe Athletic seasonals are way better than their \"always available.\" And the Free Wave is better than the Run Wild for the always available beers. They had an Irish Stout (Emerald Coast) that was indistinguishable. I could have blind taste tested them. There's some fantastic single-hop beers too. I couldn't get into the Untitled Art. Just too sweet. reply alamortsubite 15 hours agorootparentprevYes, those are great for NA beers (De Halve Maan's Brugse Sportzot is better). Unfortunately, they're mediocre among normal beers. reply ahahahahah 15 hours agorootparentprevYes, those both make some good NA beers. I wouldn't categorize them as good beers, but they are drinkable. reply ravenstine 15 hours agorootparentprev> Generally speaking, the flavor of non-alcoholic beer pales in comparison to the real thing. Ehhh... that's a bit of an overstatement, though I mostly agree. I just happened to be doing a crapload of bar hopping this last month and got into trying different non-alcoholic beers because they're way more prevalent than they used to be. They're certainly a lot better than the nonalcoholic beers of yore. And it's also obvious that you're not drinking a traditional beer. Undoubtedly, it has to do with the lack of ethanol present, but I can't help but think it's got to do with the process of producing them. Nonalcoholic beers, from what I've noticed, are usually much less foamy than alcoholic beer. I will generally still drink alcoholic beer since I don't seem to have alcoholism in me, but I think it's great that people are being given options. When I was younger, I really hated it when people would pressure me to drink or act like I'm an alien for not wanting to drink, and nonalcoholic beer is inconspicuous enough that people can drink socially without the peculiar attitude. Although that attitude seems to have largely gone away anyway. reply postcynical 15 hours agorootparentThere's this Danish startup that claims they can put the missing flavour back into the beer using their enzymes: https://evodiabio.com/yops/ https://science.ku.dk/english/press/news/2022/researchers-ma... reply triceratops 14 hours agorootparentprevWhy drink grain juice when you could drink fruit juice? reply lallysingh 10 hours agoparentprevAlmost any social drinking setting is going to have some people drink more than they're comfortable with. Having some way to mitigate the effects is wonderful. reply Maximus9000 15 hours agoparentprevTake this gel at the end of the night so that you don't have a hangover the next day? reply sureIy 7 hours agoparentprevIntoxicating. If you get the root of that word you might figure out the answer. reply colecut 15 hours agoparentprevTo drive home? =) reply zolbrek 14 hours agorootparentI would love a supplement with that effect. It's not always fun having to slowly sober up while watching your friends drinking. reply odiroot 13 hours agoparentprevTo enjoy consuming alcoholic drinks without being intoxicated? reply DennisP 12 hours agoparentprevIt's great for spies. As I've learned from movies, it's important for them to hold their booze better than their sources. reply sva_ 15 hours agoparentprevYeah people who want that could just drink beer with a low alcohol content (so called \"alcohol-free beer\", which usually still has some 0.5%) reply patall 11 hours agoprevReminds me of Ro15-4513, an antagonist of alcohol that stop (some of) the symptoms of intoxication. https://en.wikipedia.org/wiki/Ro15-4513 reply ginko 13 hours agoprevI wonder how that would affect the calories of the alcohol. Apparently the ethanol gets broken down into acetic acid, which I believe can't be digested further? Does that mean you also wouldn't gain weight when drinking dry beer or wine? EDIT: Apparently acetic acid _does_ have calories. Didn't know that. reply grujicd 13 hours agoparent> Apparently acetic acid _does_ have calories. Didn't know that. Calories are measured by burning the substance. While it very precisely determines contained energy in the physics sense, it's a question whether all that energy is used by digestion? Especially if it's something the body treats as a toxin and wants to remove as soon as possible? reply filleduchaos 7 hours agorootparentA bit pedantic but burning is a chemical reaction, and redox reactions are redox reactions no matter where they're happening. Also, substances like ethanol and acetic acid are not digested, they pretty much just go straight into the bloodstream. There is a limit on how much e.g. ethanol the liver can process per hour, which does put a cap on how many of ethanol's calories are released as ATP + body heat. Acetic acid is a normal temporary metabolite though and so its consumption is much more complete (it gets used by cells all over). reply mft_ 13 hours agoparentprevInteresting question! From this article[0] it seems ~60% of the calories in lager come from the alcohol - presumably ~40% come from carbohydrates. And from Google, acetic acid is 349 kcal / 100g, versus pure ethanol at 700 kcal / 100g. So if this approach converted 100% of alcohol to acetic acid, you'd drop the calories from lager by ~30% overall. To your question, dry wine or lower-carb beer would be proportionally even better. The flip-side of this is perhaps consumption of too much acetic acid! It's impossible to calculate potential toxicity without understanding the strength of the acetic acid generated, though. [0] https://www.researchgate.net/publication/331119759_Nutrition... reply Tade0 2 hours agorootparent> And from Google, acetic acid is 349 kcal / 100g, versus pure ethanol at 700 kcal / 100g. Metabolism matters. Gasoline is over 800kcal/100g, but you wouldn't get that much from it (if anything at all). Alcohol has a particularly long metabolic pathway, which after ingesting approximately 20-30g gets cut short to one where acetic acid is excreted (commonly known as \"breaking the seal\") and the overall upper energy yield limit becomes approximately 110kcal/100g. Executive summary is that on a given session it's the first two 12oz (330ml) beers which provide most of the calories from alcohol and their contribution is on par with a snickers bar. reply gwbas1c 13 hours agoprevI honestly don't \"get\" why someone would use this? It doesn't reverse intoxication, and it doesn't prevent alcohol from entering the bloodstream. I get that alcoholic drinks taste better than their non-alcoholic counterparts (frozen margaritas are so much better than slush puppies,) but this won't prevent the buzz / intoxication. reply Tepix 11 hours agoparentJust one example: Closing a business deal in China or Japan might require you to drink a lot of alcohol. reply JohnMakin 11 hours agorootparentThis is precisely the first thing I thought of. reply aeturnum 11 hours agoparentprevDepending on the time of action it could be very useful in addressing the \"lag\" in alcohol hitting your system. If someone is drinking heavily, they regularly reach an uncomfortable level of intoxication before all the alcohol in their stomach has been absorbed. This could help folks in that situation. reply jonwinstanley 12 hours agoparentprevAgreed, something that can sober you up or reduce hangovers would be incredible. reply rootedbox 8 hours agoprevmix it with your syrup for a real hangover helper pancake. reply jebarker 12 hours agoprevI honestly wonder whether people would drink alcohol if there weren't intoxicating effects. What I mean is, does anyone really believe that alcohol is pleasant to drink or is that purely a trick of the mind/body to get you to satisfy the craving? reply noirbot 11 hours agoparentIt was mentioned elsewhere in the thread, but alcohol is an amazing chemical for absorbing other potent flavors. It's why it's commonly used for herb extracts and medicines. Part of what I like about many alcoholic drinks is they have pungent flavors that are hard to find elsewhere. An herbal liquor is going to often have much stronger flavor of the herbs than you'd get from making a tea or a soda. There's also more diversity in the options because it's a natural preservative. Much easier for a store to stock niche-flavored spirits in small quantities when they functionally never go bad, which in turn helps keep a market for niche products that wouldn't exist if they needed steady high demand. Even something as popular as Fernet Branca doesn't really have a flavor equal in any other beverage class. reply aeturnum 11 hours agoparentprevIt's both right? Alcohol is a dopaminergic[1] in all amounts. Your brain, to some degree, likes you drinking (which may be offset by side effects). A much smaller group of people genuinely enjoy the taste and physical experience of drinking alcohol. Non-alcoholic taste-matched substitutes are common. I strongly suspect that, if alcohol was banned, you would see a bigger market for taste-matched \"mocktails\" with the same sharp, chemical undertones. However, there's no market now because moderate drinkers can just get a drink and people who don't like they taste don't want them. [1] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6826820/ reply abtinf 11 hours agorootparent> no market Mocktail mixers are a small but readily available product, e.g. Seedlip. reply aeturnum 11 hours agorootparentOh! That's great. The mocktails I've seen are generally complex, well-crafted drinks that are missing the bite that makes alcoholic drinks so satisfying to sip. I guess I need to check out more mocktails. reply swatcoder 12 hours agoparentprevFermentation, distillation, and aging can all create unique and incredibly rich flavors that may be very compelling on their own or as complements to other flavors in food and drink. Perhaps you haven't tried very many and haven't run into any you like, but they're sure out there. From a culinary perspective, it's no accident that wines and beers and spirits represent enormous industries around the world and have done so for centuries. If it was all just about getting tipsy or drunk, you'd expect there to be just a few \"winners\" of the market rather than the incredible variety we seem to have instead. reply jebarker 11 hours agorootparentI drink, probably too much, and feel like I enjoy the flavors of many different drinks. But I don't think that's evidence that it's not really just a chemical dependence. I certainly remember there was a time when I didn't enjoy the flavors I believe I do now. Your point about variety is good but, again playing devil's advocate, maybe that's necessary to keep drinking socially acceptable and sustain the illusion that there's more to it than just addiction? The most convincing argument though is that we do enjoy the flavors of fermented things that aren't intoxicating. reply sodapopcan 12 hours agoparentprevI certainly don't speak for everyone but I certainly drank for the effects. I do like the flavour of beer but it was all about the effects. I could never have just one or two (which is why I no longer drink). Again, I don't speak for everyone. I do know people who drink frequently without ever getting drunk, but obviously even one drink has some effect. reply abtinf 11 hours agoparentprevNon-alcoholic beer is an entire product category and popular in the Middle East. Russians have a fermented bread drink called Kvass that isvisual perception latency will be noticeable, and my words will start a bit getting in the order wrong. That one glass will also hit me like a sleeping pill within an hour. And if it's beer or red wine, I'll enjoy itchy arms, chest, and legs for a couple of hours. So partially a reaction to alcohol (the sleepiness for sure) and partially an allergic reaction to tannins/hops/etc. I love a riesling or a mint mojito, but I keep to probably one drink every few months... and then regret it all over again. On the other hand, there's so many alcoholics in my extended family and my best friends growing up, that I consider my body's rejection of alcohol to be a strong contributor to me being successful in life and work. reply abtinf 11 hours agoparentprevI enjoy peaty Islay scotches. I can sip a single serving over an hour, savoring the flavor, with zero intoxicating effects. reply jjgreen 20 hours agoprev... convert alcohol in the intestine into harmless acetic acid before it enters the bloodstream. You could put it on your chips post-pub and eliminate the need for vinegar! reply tguvot 15 hours agoprevpretty sure it's what zbiotics do reply striking 15 hours agoparentZbiotics targets acetaldehyde, not ethyl alcohol. From the article: > The gel shifts the breakdown of alcohol from the liver to the digestive tract. In contrast to when alcohol is metabolised in the liver, no harmful acetaldehyde is produced as an intermediate product,” explains Professor Raffaele Mezzenga from the Laboratory of Food & Soft Materials at ETH Zurich. Acetaldehyde is toxic and is responsible for many health problems caused by excessive alcohol consumption. Using this gel would manifest as simply not being as intoxicated, whereas Zbiotics is intended to allow for intoxication but prevent the hangover afterwards. reply tguvot 15 hours agorootparenti meant conceptually even if mechanism of action is different. reply striking 14 hours agorootparentSure, if the difference between being intoxicated and not is merely conceptual to you. For me it is not, but to each their own. reply tguvot 5 hours agorootparentdifferent people have different threshold for intoxication. for me it rather high. yet zbiotics take care of day after and liver reply James_K 11 hours agoprevAlcohol-free drinks already exist. They're a much easier sell than alcoholic drinks + some weird gel. reply chefandy 10 hours agoparentYes: I also think we must vocally, expressly reject anything that I don't find to be completely necessary. Other use cases == doing it wrong. reply hi-v-rocknroll 11 hours agoparentprevThis seems to be useful for people who are either allergic to alcohol or who are trying to keep up or competitively pressure others into intoxication. Otherwise, it would be wiser to just avoid EtOH that has no safe lower intake threshold and is conspicuously expensive. reply qwerty456127 10 hours agoparentprevAlcohol-free beer can be fairly good but all the rest of alcohol-free drinks are too different in taste from their alcoholic originals. reply _djo_ 11 hours agoparentprevMaybe, but so what? If this enables people to have a few drinks after work in the evening and be fully sober for work the next day, I’m all for it. Also, anything that can rapidly break down alcohol in the body will help as a medical treatment for alcohol overdoses. reply teekert 12 hours agoprevAlcohol is a drug, it can be addictive and it’s a carcinogen. Sure fermented stuff is nice but you just shouldn’t have too much. Just alternate a drink and water and don’t drink much. Who needs a shitty gel in their body just to consume more poison? Show some discipline and restraint, tale good care of your body. reply shepherdjerred 12 hours agoparentHave you ever struggled with addiction (food, drink, drugs, media, games, sleep, porn etc.)? I can't provide a study, but anecdotally I suspect nearly everyone does. > Show some discipline and restraint This is such unhelpful, unsympathetic advice. reply try_the_bass 11 hours agorootparent> This is such unhelpful, unsympathetic advice. I think writing this advice off as unhelpful is actually more harmful than offering it in the first place. I think this is very real and very helpful advice. Is it hard to follow? Speaking from experience: absolutely. Self-control is like a muscle: exercise it frequently, and it gets stronger. It also gets tired and needs rest, and it atrophies with disuse. And like exercise, it's almost always beneficial. Even folks with physical disabilities see very real benefits from exercise, even when it's hard and painful! I used to live next door to a man who walked with a cane and very obviously struggled to go up and down stairs... And yet any time I would offer him help, he would refuse, because he knew the effort would keep him as mobile and active as he could be, given his circumstances--and do accept that help would actually harm him in the long run by accelerating the decline in his abilities. I doubt I would have his level of discipline were I in his situation, and to this day I envy that of him. I think going so far as to say \"telling someone to exercise self-control is unhelpful/unsympathetic\" is exactly analogous to telling someone exercise is harmful. Not \"too much exercise is harmful\", but \"any exercise is harmful\", which is obviously untrue. I'll be the first to acknowledge that humans are innately lazy, and that exercise is hard/boring/inconvenient/whatever. However, we do no one justice by giving them reasons to excuse that laziness. Justifying a lack of internal effort/ability should be and explanation of last resort, not the baseline. Put differently: very few people are physically incapable of doing a pushup (or whatever other basic exercise you want to reference) due to actual physical limitations. Most who cannot simply haven't put in the work to reach the point where they can. [E] This turned out longer than I anticipated. It turns out I feel strongly about this, and feel like this is one of the most toxic aspects of the society I feel like I inhabit. People should be encouraged to push their abilities, not given excuses not to. It's all too easy to accept those excuses as truth, and this prevents us all from reaching our highest potential. This feels like a net harm to society and a driver of very real inequality reply KittenInABox 11 hours agorootparent> Self-control is like a muscle: exercise it frequently, and it gets stronger. It also gets tired and needs rest, and it atrophies with disuse. Addiction is not a matter of self-control. reply wubrr 11 hours agorootparentIt's 100% a matter of self-control. reply filleduchaos 8 hours agorootparentAddiction is a psychological disorder - it is not at all a matter of self-control. Just as being sad is not the same thing as being clinically depressed, and worrying about a review is not the same thing as having an anxiety disorder, people need to stop equating really liking something and being addicted to said thing. reply wubrr 6 hours agorootparent> Addiction is a psychological disorder - it is not at all a matter of self-control. And self-control is not psychological? > people need to stop equating really liking something and being addicted to said thing. Self control is precisely the difference between really linking something and consuming a reasonable/healthy amount of it vs being addicted to it and overconsuming. reply teekert 1 hour agorootparentprevWhatever addiction, you’ll only get past it by assuming and believing it is under your control, paradoxically. Playing victim is, as very often in life, the opposite of empowering. reply fragmede 8 hours agorootparentprevNo it's not. That's a lazy way of thinking. Addiction isn't that simple, and deluding yourself into thinking it's that simple does yourself a disservice. It's not a moral failing, it's a disease. reply wubrr 6 hours agorootparentSelf control is not 'simple', nor did I say anything about a 'moral failing'. 'Deluding yourself' by arguing against a position you imagined does yourself a disservice and just seems like a simple straw-man. reply deelowe 11 hours agorootparentprevFew things are. The world would be a better place if people would try to help each other more. reply vl 11 hours agorootparentprevThis gel, if anything, will enable addiction. reply Spivak 10 hours agorootparentprevThis reminds me so much of the advice people give folks with adhd, just get organized, use a calendar, use a pomodoro timer. If that stuff worked for me I wouldn't have adhd. If exercising restraint worked no one would have alcoholism. It's not a moral failing, it's a physical condition. reply BxGyw2 37 minutes agorootparentall of those things you listed are good methods that generally help adhd people work better?? reply jszymborski 3 hours agoparentprevHow does alternating with water alleviate the carcinogenicity (and other health-related issues) of alcohol? Perhaps this gel can help? Perhaps trials will show long term health benefits. Also, please don't conflate addiction with a lack of discipline or restraint. reply reactordev 11 hours agoparentprevNot everyone can self regulate or their body isn’t the dependable vehicle yours is. This is for those people. Enjoy yourself and keep switching between white claws and water. reply therobots927 9 hours agoparentprevSo people who smoke cigarettes might as well rip out the filters? Drug users at raves shouldn't use test kits? People on motorcycles shouldn't wear helmets? reply fuzzfactor 4 hours agoparentprevAlcohol is a solvent. reply thetoon 2 hours agorootparentSo is water, btw. reply hi-v-rocknroll 11 hours agoprev [–] Spy and drink competition ringer toolkit. Back it up with ~25 year old hangover prophylaxis (2 before + 2 with ea drink) https://www.ru21.com/. reply BobaFloutist 10 hours agoparent [–] Holy shit, four aspirins with every drink? How much do you hate your liver? reply hi-v-rocknroll 10 hours agorootparent [–] It's not aspirin. That would be toxic and fucking stupid. It's a mix of vitamins and amino acids. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "ETH Zurich researchers created a protein-based gel that breaks down alcohol in the gastrointestinal tract without causing harm to the body.",
      "The gel, consisting of whey protein fibrils and iron atoms, converts alcohol into harmless acetic acid, lowering blood alcohol levels by up to 50% in mice.",
      "Although human trials are necessary, this innovation shows promise in mitigating the negative impacts of alcohol consumption."
    ],
    "commentSummary": [
      "Research is ongoing on products and methods to reduce the harmful impacts of alcohol, like hangovers, by targeting acetaldehyde with gels and promoting non-alcoholic drinks.",
      "The discussion includes views on addiction, self-regulation, and community assistance concerning alcohol use management.",
      "The primary aim is to develop strategies that allow responsible alcohol consumption while minimizing its adverse effects."
    ],
    "points": 174,
    "commentCount": 197,
    "retryCount": 0,
    "time": 1715687177
  },
  {
    "id": 40353963,
    "title": "LightVM: A Safer and Lighter Virtual Machine Solution",
    "originLink": "https://dl.acm.org/doi/10.1145/3132747.3132763",
    "originBody": "research-article Open Access Share on My VM is Lighter (and Safer) than your Container Authors: Filipe Manco NEC Laboratories Europe NEC Laboratories Europe View Profile , Costin Lupu Univ. Politehnica of Bucharest Univ. Politehnica of Bucharest View Profile , Florian Schmidt NEC Laboratories Europe NEC Laboratories Europe View Profile , Jose Mendes NEC Laboratories Europe NEC Laboratories Europe View Profile , Simon Kuenzer NEC Laboratories Europe NEC Laboratories Europe View Profile , Sumit Sati NEC Laboratories Europe NEC Laboratories Europe View Profile , Kenichi Yasukata NEC Laboratories Europe NEC Laboratories Europe View Profile , Costin Raiciu Univ. Politehnica of Bucharest Univ. Politehnica of Bucharest View Profile , Felipe Huici NEC Laboratories Europe NEC Laboratories Europe View Profile Authors Info & Claims SOSP '17: Proceedings of the 26th Symposium on Operating Systems PrinciplesOctober 2017Pages 218–233https://doi.org/10.1145/3132747.3132763 Published:14 October 2017Publication History 182citation 24,383 Downloads Metrics Total Citations182 Total Downloads24,383 Last 12 Months1,905 Last 6 weeks280 Get Citation Alerts New Citation Alert added! This alert has been successfully added and will be sent to: You will be notified whenever a record that you have chosen has been cited. To manage your alert preferences, click on the button below. Manage my Alerts New Citation Alert! Please log in to your account Publisher Site eReader PDF SOSP '17: Proceedings of the 26th Symposium on Operating Systems Principles My VM is Lighter (and Safer) than your Container Pages 218–233 PreviousChapterNextChapter ABSTRACT Containers are in great demand because they are lightweight when compared to virtual machines. On the downside, containers offer weaker isolation than VMs, to the point where people run containers in virtual machines to achieve proper isolation. In this paper, we examine whether there is indeed a strict tradeoff between isolation (VMs) and efficiency (containers). We find that VMs can be as nimble as containers, as long as they are small and the toolstack is fast enough. We achieve lightweight VMs by using unikernels for specialized applications and with Tinyx, a tool that enables creating tailor-made, trimmed-down Linux virtual machines. By themselves, lightweight virtual machines are not enough to ensure good performance since the virtualization control plane (the toolstack) becomes the performance bottleneck. We present LightVM, a new virtualization solution based on Xen that is optimized to offer fast boot-times regardless of the number of active VMs. LightVM features a complete redesign of Xen's control plane, transforming its centralized operation to a distributed one where interactions with the hypervisor are reduced to a minimum. LightVM can boot a VM in 2.3ms, comparable to fork/exec on Linux (1ms), and two orders of magnitude faster than Docker. LightVM can pack thousands of LightVM guests on modest hardware with memory and CPU usage comparable to that of processes. Skip Supplemental Material Section Supplemental Material my_vm_lighter.mp4 mp4 2.2 GB Download References Amazon Web Services {n. d.}. Amazon EC2 Container Service. https://aws.amazon.com/ecs/. ({n. d.}).Google Scholar Amazon Web Services {n. d.}. AWS Lambda - Serverless Compute. https://aws.amazon.com/lambda. ({n. d.}).Google Scholar Paul Barham, Boris Dragovic, Keir Fraser, Steven Hand, Tim Harris, Alex Ho, Rolf Neugebauer, Ian Pratt, and Andrew Warfield. 2003. Xen and the Art of Virtualization. SIGOPS Open Syst. Rev. 37, 5 (Oct. 2003), 164--177. Google ScholarDigital Library J. Clark. {n. d.}. Google: \"EVERYTHING at Google runs in a container\". http//:www.theregister.co.uk/2014/05/23/google_containerizationtwobillion/. ({n. d.}).Google Scholar Patrick Colp, Mihir Nanavati, Jun Zhu, William Aiello, George Coker, Tim Deegan, Peter Loscocco, and Andrew Warfield. 2011. Breaking Up is Hard to Do: Security and Functionality in a Commodity Hypervisor. In Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles (SOSP '11). ACM, New York, NY, USA, 189--202. Google ScholarDigital Library Docker {n. d.}. The Docker Containerization Platform. https://www.docker.com/. ({n. d.}).Google Scholar John R. Douceur, Jeremy Elson, Jon Howell, and Jacob R. Lorch. 2008. Leveraging Legacy Code to Deploy Desktop Applications on the Web. In Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation (OSDI'08). USENIX Association, Berkeley, CA, USA, 339--354. http://dl.acm.org/citation.cfm?id=1855741.1855765 Google ScholarDigital Library D. R. Engler, M. F. Kaashoek, and J. O'Toole, Jr. 1995. Exokernel: An Operating System Architecture for Application-level Resource Management. In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles (SOSP '95). ACM, New York, NY, USA, 251--266. Google ScholarDigital Library Erlang on Xen 2012. Erlang on Xen. http://erlangonxen.org/. (July 2012).Google Scholar Google Cloud Platform {n. d.}. The Google Cloud Platform Container Engine. https://cloud.google.com/container-engine. ({n. d.}).Google Scholar A. Grattafiori. {n. d.}. Understanding and Hardening Linux Containers. https://www.nccgroup.trust/us/our-research/understanding-and-hardening-linux-containers/. ({n. d.}).Google Scholar Cameron Hamilton-Rich. {n. d.}. axTLS Embedded SSL. http://axtls.sourceforge.net. ({n. d.}).Google Scholar Poul henning Kamp and Robert N. M. Watson. 2000. Jails: Confining the omnipotent root. In In Proc. 2nd Intl. SANE Conference.Google Scholar J. Hertz. {n. d.}. Abusing Privileged and Unprivileged Linux Containers. https://www.nccgroup.tmst/uk/our-research/abusing-privileged-and-unprivileged-linux-containers/, ({n. d.}).Google Scholar Jon Howell, Bryan Parno, and John R. Douceur. 2013. Embassies: Radically Refactoring the Web. In Presented as part of the 10th USENTX Symposium on Networked Systems Design and Implementation (NSDI13). USENIX, Lombard, IL, 529--545. https://www.usenix.org/conference/nsdil3/technical-sessions/presentation/howell Google ScholarDigital Library Yun Chao Hu, Milan Patel, Dario Sabella, Nurit Sprecher, and Valerie Young. 2015. Mobile Edge Computing - A key technology towards 5G. ETSI White Paper No. 11, First edition (2015).Google Scholar IBM. {n. d.}. Docker at insane scale on IBM Power Systems. https://www.ibm.com/blogs/bluemix/2015/ll/docker-insane-scale-on-ibm-power-systems. ({n. d.}).Google Scholar IBM developerWorks Open {n. d.}. Solo5 Unikernel. https://developer.ibm.com/open/openprojects/solo5-unikernel/. ({n. d.}).Google Scholar Intel. {n. d.}. Intel Clear Containers: A Breakthrough Combination of Speed and Workload Isolation. https://clearlinux.org/sites/default/files/vmscontainers_wp_v5.pdf. ({n. d.}).Google Scholar Avi Kivity, Yaniv Kamay, Dor Laor, Uri Lublin, and Anthony Liguori. 2007. KVM: the Linux Virtual Machine Monitor. In In Proc. 2007 Ottawa Linux Symposium (OLS '07).Google Scholar Avi Kivity, Dor Laor, Glauber Costa, Pekka Enberg, Nadav Har'El, Don Marti, and Vlad Zolotarov. 2014. OSv---Optimizing the Operating System for Virtual Machines. In Proceedings of the 2014 USENTX Annual Technical Conference (USENIX ATC '14). USENIX Association, Philadelphia, PA, 61--72. https://www.usenix.org/conference/atcl4/technical-sessions/presentation/kivity Google ScholarDigital Library E. Kovacs. {n. d.}. Docker Fixes Vulnerabilities, Shares Plans For Making Platform Safer. http//:www.securityweek.com/docker-fixes-vulnerabilities-shares-plans-making-platform-safer. ({n. d.}).Google Scholar Simon Kuenzer, Anton Ivanov, Filipe Manco, Jose Mendes, Yuri Volchkov, Florian Schmidt, Kenichi Yasukata, Michio Honda, and Felipe Huici. 2017. Unikernels Everywhere: The Case for Elastic CDNs. In Proceedings of the 13th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments (VEE '17). ACM, New York, NY, USA, 15--29. Google ScholarDigital Library Horacio Andrés Lagar-Cavilla, Joseph Andrew Whitney, Adin Matthew Scannell, Philip Patchin, Stephen M. Rumble, Eyal de Lara, Michael Brudno, and Mahadev Satyanarayanan. 2009. SnowFlock: Rapid Virtual Machine Cloning for Cloud Computing. In Proceedings of the 4th ACM European Conference on Computer Systems (EuroSys '09). ACM, New York, NY, USA, 1--12. Google ScholarDigital Library LinuxContainers.org {n. d.}. LinuxContainers.org. https://linuxcontainers.org. ({n. d.}).Google Scholar Anil Madhavapeddy, Thomas Leonard, Magnus Skjegstad, Thomas Gazagnaire, David Sheets, Dave Scott, Richard Mortier, Amir Chaudhry, Balraj Singh, Jon Ludlam, Jon Crowcroft, and Ian Leslie. 2015. Jitsu: Just-In-Time Summoning of Unikernels. In 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI '15). USENIX Association, Oakland, CA, 559--573. https://www.usenix.org/conference/nsdil5/technical-sessions/presentation/madhavapeddy Google ScholarDigital Library Anil Madhavapeddy and David J. Scott. 2013. Unikernels: Rise of the Virtual Library Operating System. Queue 11, 11, Article 30 (Dec. 2013), 15 pages. Google ScholarDigital Library Y. Mao, J. Zhang, and K. B. Letaief. 2016. Dynamic Computation Offloading for Mobile-Edge Computing With Energy Harvesting Devices. IEEE Journal on Selected Areas in Communications 34, 12 (Dec 2016), 3590--3605. Google ScholarDigital Library Joao Martins, Mohamed Ahmed, Costin Raiciu, Vladimir Olteanu, Michio Honda, Roberto Bifulco, and Felipe Huici. 2014. ClickOS and the Art of Network Function Virtualization. In 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI '14). USENIX Association, Seattle, WA, 459--473. https://www.usenix.org/conference/nsdil4/technical-sessions/presentation/martins Google ScholarDigital Library McAffee. 2016. Mobile Threat Report. https://www.mcafee.com/us/resources/reports/rp-mobile-threat-report-2016.pdf. (2016).Google Scholar MicroPython {n. d.}. MicroPython. https://micropython.org/. ({n. d.}).Google Scholar Microsoft. {n. d.}. Azure Container Service. https://azure.microsoft.com/en-us/services/container-service/. ({n. d.}).Google Scholar Microsoft Research. {n. d.}. Drawbridge. https://www.microsoft.com/en-us/research/project/drawbridge/. ({n. d.}).Google Scholar minios {n. d.}. Mini-OS. https://wiki.xenproject.org/wiki/Mini-OS. ({n. d.}).Google Scholar A. Mourat. {n. d.}. 5 security concerns when using Docker. https://www.oreilly.com/ideas/five-security-concerns-when-using-docker. ({n. d.}).Google Scholar Vlad Nitu, Pierre Olivier, Alain Tchana, Daniel Chiba, Antonio Barbalace, Daniel Hagimont, and Binoy Ravindran. 2017. Swift Birth and Quick Death: Enabling Fast Parallel Guest Boot and Destruction in the Xen Hypervisor. In Proceedings of the 13th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments (VEE '17). ACM, New York, NY, USA, 1--14. Google ScholarDigital Library MAN page. {n. d.}. Linux system calls list. http://man7.org/linux/man-pages/man2/syscalls.2.html. ({n. d.}).Google Scholar Rumpkernel.org {n. d.}. Rump Kernels. http://rumpkernel.org/. ({n. d.}).Google Scholar Sandvine. {n. d.}. Internet traffic encryption. https://www.sandvine.com/trends/encryption.html. ({n. d.}).Google Scholar Mahadev Satyanarayanan, Paramvir Bahl, Ramón Caceres, and Nigel Davies. 2009. The Case for VM-Based Cloudlets in Mobile Computing. IEEE Pervasive Computing 8, 4 (Oct. 2009), 14--23. Google ScholarDigital Library Justine Sherry, Shaddi Hasan, Colin Scott, Arvind Krishnamurthy, Sylvia Ratnasamy, and Vyas Sekar. 2012. Making Middleboxes Someone Else's Problem: Network Processing As a Cloud Service. In Proceedings of the ACM SIGCOMM 2012 Conference on Computer Communication (SIGCOMM '12). ACM, New York, NY, USA, 13--24. Google ScholarDigital Library Stephen Soltesz, Herbert Pötzl, Marc E. Fiuczynski, Andy Bavier, and Larry Peterson. 2007. Container-based Operating System Virtualization: A Scalable, High-performance Alternative to Hypervisors. SIGOPS Oper. Syst. Rev. 41, 3 (March 2007), 275--287. Google ScholarDigital Library S. Stabellini. {n. d.}. Xen on ARM. http//:www.slideshare.net/xen_com_mgr/alsf13-stabellini. ({n. d.}).Google Scholar Udo Steinberg and Bernhard Kauer. 2010. NOVA: A Microhypervisorbased Secure Virtualization Architecture. In Proceedings of the 5th European Conference on Computer Systems (EuroSys '10). ACM, New York, NY, USA, 209--222. Google ScholarDigital Library A. van de Ven. {n. d.}. An introduction to Clear Containers. https://lwn.net/Articles/644675/. ({n. d.}).Google Scholar Akshat Verma, Gargi Dasgupta, Tapan Kumar Nayak, Pradipta De, and Ravi Kothari. 2009. Server Workload Analysis for Power Minimization Using Consolidation. In Proceedings of the 2009 USENIX Annual Technical Conference (USENIX ATC '09). USENIX Association, Berkeley, CA, USA, 28--28. http://dl.acm.org/citation.cfm?id=1855807.1855835 Google ScholarDigital Library VMWare. {n. d.}. vSphere ESXi Bare-Metal Hypervisor. http//:www.vmware.com/products/esxi-and-esx.html. ({n. d.}).Google Scholar Michael Vrable, Justin Ma, Jay Chen, David Moore, Erik Vandekieft, Alex C. Snoeren, Geoffrey M. Voelker, and Stefan Savage. 2005. Scalability, Fidelity, and Containment in the Potemkin Virtual Honey-farm. SIGOPS Oper. Syst. Rev. 39, 5 (Oct. 2005), 148--162. Google ScholarDigital Library Andrew Whitaker, Marianne Shaw, and Steven D. Gribble. 2002. Scale and Performance in the Denali Isolation Kernel. SIGOPS Oper. Syst. Rev. 36, SI (Dec. 2002), 195--209. Google ScholarDigital Library Dan Williams and Ricardo Koller. 2016. Unikernel Monitors: Extending Minimalism Outside of the Box. In 8th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud '16). USENIX Association, Denver, CO. https://www.usenix.org/conference/hotcloud16/workshop-program/presentation/williams Google ScholarDigital Library Wei Zhang, Jinho Hwang, Shriram Rajagopalan, K.K. Ramakrishnan, and Timothy Wood. 2016. Flurries: Countless Fine-Grained NFs for Flexible Per-Flow Customization. In Proceedings of the 12th International on Conference on Emerging Networking EXperiments and Technologies (CoNEXT '16). ACM, New York, NY, USA, 3--17. Google ScholarDigital Library Cited By View all Index Terms My VM is Lighter (and Safer) than your Container Software and its engineering Software organization and properties Contextual software domains Operating systems Software infrastructure Virtual machines Recommendations Transparently bridging semantic gap in CPU management for virtualized environments Consolidated environments are progressively accommodating diverse and unpredictable workloads in conjunction with virtual desktop infrastructure and cloud computing. Unpredictable workloads, however, aggravate the semantic gap between the virtual ... Read More Container-based operating system virtualization: a scalable, high-performance alternative to hypervisors EuroSys '07: Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007 Hypervisors, popularized by Xen and VMware, are quickly becoming commodity. They are appropriate for many usage scenarios, but there are scenarios that require system virtualization with high degrees of both isolation and efficiency. Examples include ... Read More Container-based operating system virtualization: a scalable, high-performance alternative to hypervisors EuroSys'07 Conference Proceedings Hypervisors, popularized by Xen and VMware, are quickly becoming commodity. They are appropriate for many usage scenarios, but there are scenarios that require system virtualization with high degrees of both isolation and efficiency. Examples include ... Read More Comments Login options Check if you have access through your login credentials or your institution to get full access on this article. Sign in Full Access Get this Publication Information Contributors Published in SOSP '17: Proceedings of the 26th Symposium on Operating Systems Principles October 2017 677 pages ISBN:9781450350853 DOI:10.1145/3132747 Copyright © 2017 Owner/Author This work is licensed under a Creative Commons Attribution International 4.0 License. Sponsors In-Cooperation Publisher Association for Computing Machinery New York, NY, United States Publication History Published: 14 October 2017 Permissions Request permissions about this article. Request Permissions Check for updates Author Tags Virtualization Xen containers hypervisor operating systems specialization unikernels virtual machine Qualifiers research-article Research Refereed limited Conference Acceptance Rates Overall Acceptance Rate131of716submissions,18% Upcoming Conference SOSP '24 Sponsor: SIGOPS ACM SIGOPS 29th Symposium on Operating Systems Principles November 5 - 8, 2024 Austin , TX , USA Funding Sources Other Metrics View Article Metrics Bibliometrics Citations182 Article Metrics 182 Total Citations View Citations 24,383 Total Downloads Downloads (Last 12 months)1,905 Downloads (Last 6 weeks)280 Other Metrics View Author Metrics Cited By View all PDF Format View or Download as a PDF file. PDF eReader View online with eReader. eReader Digital Edition View this article in digital edition. View Digital Edition Figures Other Share this Publication link https://dl.acm.org/doi/10.1145/3132747.3132763 Copy Link Share on Social Media Share on 0References",
    "commentLink": "https://news.ycombinator.com/item?id=40353963",
    "commentBody": "My VM is lighter (and safer) than your container (2017) (acm.org)162 points by fanf2 22 hours agohidepastfavorite165 comments epr 21 hours agoNo mention of user namespaces whatsoever, which is the primary security isolation mechanism for containers on linux. This is what enables \"rootless\" mode. Of course, this is from 2017, but user namespaces were released with linux 3.8 in February 2013. Docker particularly has always required extra work to run in rootless mode because it was released soon after in March 2013, and for whatever reason it hasn't been a priority to rework the codebase to make that the default. I switched to podman for exactly this reason as my go-to oci implementation and haven't looked back. The linux kernel features that enable various forms of isolation all require root privileges (CAP_SYS_ADMIN). Once user namespaces were a thing, that allowed you to use user namespaces to get around the root requirements for all the other isolation namespaces. All of the below still require CAP_SYS_ADMIN: CLONE_NEWCGROUP: cgroup namespace, for resource control (mem/cpu/block io/devices/network bandwith) CLONE_NEWIPC: ipc namespace for sysv ipc objects and message queues CLONE_NEWNET: network namespace, for isolated virtual networking CLONE_NEWNS: mount namespace, for isolated mounting (filesystems, etc.) CLONE_NEWPID: pid namespace, for isolated view of running processes CLONE_NEWUTS: unix timesharing system namespace, for isolation of hostname and domain name see: https://man7.org/linux/man-pages/man2/clone.2.html reply ledgerdev 20 hours agoparentThanks for the suggestion, I was unaware of podman and will be trying it out because root has always bothered me. reply angra_mainyu 15 hours agorootparentPodman is fantastic! It can also handle kubernetes and generate systemd units for containers. reply sureglymop 20 hours agoparentprevNowadays it is fairly straightforward to set up docker in rootless mode. reply epr 20 hours agorootparentExactly, \"set up\". Many people (not all) don't want to fiddle with things, they just want it to work out of the box. The importance of secure defaults can't be overstated, especially when there are virtually no downsides. reply zerof1l 21 hours agoprevDocker has a big community, lots of guides, and ready-to-use containers. It became pretty much a de facto standard for self-hosting things. You also have a very high chance of getting a piece of software to work out of the box as intended with Doker. The only way this or some other way of running stuff will overtake Doker is if it will match the Docker in these aspects. As much as I'd love to try this lightweight VM idea, I don't have the time or energy to convert 20+ projects I'm self-hosting into this and then keep everything updated. I'd rather invest this time into learning Docker more and making my existing setup more secure and robust. reply fhuici 20 hours agoparentMaybe try out kraft.cloud: we take Dockerfiles as input and automatically convert to lightweight VMs/unikernels when deploying (disclaimer: I'm one of the paper's authors and one of the people behind KraftCloud). reply sureglymop 20 hours agorootparentI recently built a similar thing for learning purposes using firecracker + the firecracker go api. I wrote a small init system in rust and combined that with filesystem images derived from the Debian, Ubuntu etc. container images (that can be extended with more layers). What really surprised me the most, is how quick and simple it is to compile the linux kernel. Cloned a tag with --depth 1, configured it and then it took ~ 5 minutes to build vmlinuz.bin. As someone who is too young to have had to regularly do that, I had heard multiple stories of how long that's supposed to take but it really doesn't. I then tried to move from firecracker to qemu microvms but didn't get that far yet since I didn't have more time. All in all a great learning experience and if I wasn't an undergrad student with no time, I'd love to build a service/business around it. reply jetbalsa 19 hours agorootparentit /used/ to take forever back in the Core 2 Duo days, with the amount of cores and the sheer speed of the IPC its gotten a ton better. reply FridgeSeal 20 hours agorootparentprevWow! I came across Unikraft a while ago and went “wow that’s cool, but I have no idea how to use this”, cloud offering and docs you have up there now look amazing! Will 100% be giving this a go first thing tomorrow!! reply __jonas 20 hours agorootparentprevI thought that’s what fly.io is, although I’m not familiar with it, am I missing something? reply FridgeSeal 20 hours agorootparentFly: takes your docker image, converts it into a Firecracker VM and runs that: kernel boundaries etc are all the same as before (and the same as running your container locally). Kraft Cloud: takes your docker image, and turns it into a “unikernel”, and runs that. In a unikernel, your application _is_ the kernel. There’s no process boundary, no kernel-space/userspace split there’s a single address-space etc. I believe the idea is that you get a perf benefit-as your application is often the only one running in the container, security is provided by the hupervisor anyways, so may as well cut out all the middle layers that aren’t getting you much. Seems some of the authors/founders of Unikraft are in the comments, they can explain much better than I. reply fhuici 14 hours agorootparentHey, author/founder here, thanks for providing that answer, all correct there :) . I would also add that KraftCloud unikernels are built using Unikraft, and that its modularity allow us to tailor/specialize those images to obtain great perf. Finally, we also had to design and implement a controller from scratch -- nothing out there provided the millisecond semantics and scalability we needed (plus we also did tweaks to network interface creation and a few other things to get the end to end experience to be fast). reply FridgeSeal 12 hours agorootparentThat sounds amazing! Very keen to give it a try. My work had a product that was doing builds and hosting for arbitrary client code, you’re doing all that, plus more. I’ve got massive respect for that, because there were some hard problems to solve, even in our pretty vanilla environment- looks like you guys have done a far better job than we did, plus more! reply surajrmal 19 hours agorootparentprevIt sounds like consequences of bugs like memory corruption are far more challenging to deal with in the Kraft cloud situation. Sometimes isolation has other benefits. reply rakoo 16 hours agorootparentIsn't that better isolation though ? A memory corruption will at worse break the OS which is the app and nothing else. Push the model further and you can have one unikernel per user and reduce even further the consequences of bugs reply kylecordes 20 hours agorootparentprevThat is a very clever idea! reply jve 20 hours agoparentprevAhem, this is a research paper. You should look at this stuff as \"Innovation\" and someone may just consider building a tool or product on the idea... or not. reply fhuici 14 hours agorootparentAuthor here, we did this, first by continuing the research alongside the creation of the Unikraft LF OSS project -- the result of which was the Eurosys 2021 best paper award (https://dl.acm.org/doi/10.1145/3447786.3456248). Commercially, we leverage Unikraft on kraft.cloud to provide a cloud platform with millisecond semantics. reply jeltz 20 hours agoparentprevThe ready-to-use is only true for the most popular software like PostgreSQL. Anything else is often broken and/or unmaintained. reply gbalduzzi 20 hours agorootparentIn web development I had never experienced an issue with the commonly used softwares reply k8sToGo 20 hours agoparentprevEspecially with something like linuxserver.io reply DEADMINCE 19 hours agoparentprev> making my existing setup more secure and robust. The first step should be to not be so entirely reliant on Docker. reply BobbyTables2 20 hours agoparentprevIt’s amazing what a few hundred million in VC funding can do! reply uticus 20 hours agoprevOS: I provide isolation where needed, handle safely interacting with outside world, and abstract away all the pesky stuff so programmers can just get stuff done. Container / VM: I provide isolation where needed, handle safely interacting with outside world, and abstract away all the pesky stuff so programmers can just get stuff done. I get that a dev machine (OS) isn't usually suitable for deployment or shared development (Container/VM). But seems to me the promise of the Operating System has fallen short, if we are striving to meet so many of the same goals of the OS, with something on the OS that tries to abstract away the OS. reply vilunov 20 hours agoparentI guess this came to be due to the poor original security model of classic OSs, which led to prolification of viruses and complex management of shared resources. Users, groups and access flags are not enough to manage security of a system. Linux tried to fix that with namespaces and it turned out to be more or less successful, but Linux is not an OS, it's just a kernel, and it's up to real OSs built atop Linux to use namespaces as an implementation detail for real application isolation. One way to do that is OCI-containers, the other way is Flatpak. Neither of those is not a proper OS yet, but you could call Kubernetes an operating system which uses containers as means for application and resource isolation. Naturally that means Kubernetes is a complex beast, but that's what it takes to provide what users expect from an OS. Android also comes to mind, they managed to isolate applications between each other quite safely. reply clan 20 hours agorootparentI say this with great care as I do not want to launch a flamewar. If you do not consider Linux with namespaces an OS (because of fragmented userland): Would you then consider FreeBSD with jails or Solaris with zones for fully fledged? If you still consider those flawed (maybe because thet do not force you into jails/zones) should we at least no consider OS/390 or z/OS as proper operating systems to that/your (not meant inflamatory!) standard? Yes. Though you do not mention them directly DOS and Windows has ruled the world for years and they opened the door for the nasties. But they were not all there was - only the popular/easy choice. Everything is a trade off. reply vilunov 20 hours agorootparentIsolation mechanisms is not what makes an OS. It's the stable ABI that application developers can depend on and which provides a way to use shared resources: disk, CPU, RAM, GPU, network, screen space, push notifications, GUI integrations, your favorite LLM integration, so on, so forth... Yes, it might have an imperfect security model, but nothing's perfect under the sun. Raw Linux without userspace could be considered an OS, but it has the ABI only in form of syscalls and the minimal standard FS. That's barely enough for anything other than, say, a statically linked Go binary, which is why it's seldom used by app developers as a target. To most of your examples I say – yes, that's an OS, and jails or zones have nothing to do with it. Although I'm not familiar with them other than FreeBSD, so I'm relying on your short description and your implied criteria for selecting these examples. reply jayd16 20 hours agoparentprevContainers are an OS feature, though. The OS is fulfilling the promise of environment isolation with containers. reply jsheard 22 hours agoprevFrom 2017, before rootless containers caught on I think. The conclusions on safety might be due for re-evaluating. reply vegardx 21 hours agoparentI don't really see how rootless containers change anything at all. You're still \"just\" one kernel privilege escalation away from breaking out. The level of isolation is much better in virtual machines, and the performance penalty is comparable these days. The virtual machine images are a bit heavier, since you need a kernel and whatnot, but it's negligible at best. The memory footprint of virtual machines with memory deduplication and such means that you get very close to the footprint of containers. You have the cold start issue with microvms, but these days they generally start in less than a couple of hundred milliseconds, not that far off your typical container. reply jfindley 20 hours agorootparentMemory de-dup is computationally expensive, and KSM hitrate is generally much worse than people tend to expect - not to mention that it comes with its own security issues. I agree that the security tradeoffs need to be taken seriously but the realworld performance/efficiency considerations are definitely not negligeable at scale. There are also significant operational concerns. With containers you can just have your CI/CD system spit out a new signed image every N days and do fairly seamless A/B rollouts. With VMs that's a lot harder. You may be able to emulate some of this by building some sort of static microvm, but there's a LOT of complexity you'll need to handle (e.g. networking config, OS updates, debugging access) that is going to be some combination of flaky and hard to manage. I by no means disagree with the security points but people are overstating the case for replacing containers with VMs in these replies. reply fhuici 21 hours agorootparentprevAnd these overheads are even smaller if you use unikernels as per the paper. Eg, cold starts of a few milliseconds depending on the app/size of the image. reply vegardx 20 hours agorootparentI'm struggling a little bit to grasp all the concepts when we start talking about unikernels, wasm and so on. Hopefully that's just a sign of the maturity of it, and not a sign of my mental decline. But on paper (as I understand it) it looks /so cool/. reply epr 20 hours agorootparentUnikernels aren't too complicated conceptually. They're more or less a kernel stripped down to the bare minimum required by a single application. The complete bundle of the minimal kernel and application together is called a unikernel. The uni- prefix means one as in the kernel only supports one userspace application, instead of something like linux, which supports many. The benefits, as mentioned in the paper and in this thread are that you can run that as a vm, since it contains it's own operating system, unlike a container which is dependent on the host operating system. Also, they boot very quickly. reply fhuici 20 hours agorootparentprevAgree with epr's definition of a unikernel (and no, no mental decline on your part, this isn't always well defined). First off, a unikernel is a virtual machine, albeit a pretty specialized one. They're are often based on modular operating systems (e.g., Unikraft), in order to be able to easily pick the OS modules needed for each application, at compile time. You can think of it as a VM that has a say NGINX-specific distro, all the way down to the OS kernel modules. VMs provide what's called hardware-level isolation, running on top of a hypervisor like KVM, Xen or Hyper-V. Wasm runs higher up the stack, in user-space, and provides what's called language-level isolation (i.e., you could even create a wasm unikernel, that is, a specialized VM that inside runs wasm (eg, see https://docs.kraft.cloud/guides/wazero/). Generally speaking, the higher you go up the stack, the more code you're running and the higher the chances of a vulnerability. reply Aardwolf 21 hours agoparentprevWhy weren't containers rootless from the start anyway? What did they need that user space doesn't provide? Wine, emulators and VMs didn't require it either (with the exception of some VMs needing a kernel module for performance reasons like memory management, which I also find stupid, the OS should provide all the performance in user space). reply epr 20 hours agorootparentAs I mentioned in another comment, the linux kernel feature (user namespaces) that enables \"rootless\" containers was released in February 2013, and Docker was released soon after in March of that year. For whatever reason, they haven't made it a priority to make rootless the default, although it is technically doable. If you are annoyed by this, I'd suggest checking out podman, which has done a lot of work to be basically a drop in replacement with a similar workflow to docker. reply BobbyTables2 20 hours agorootparentprevYou mean for the container launcher to not require root? Root isn’t required — look at Podman. Docker spent so much on marketing, the world is too blind to pivot to a superior alternative! reply zokier 20 hours agorootparentPeople were running containers for a decade before rootless podman came around. There has been lot of sharp corners around userns and related tech that needed to get resolved. Notably Debian& Ubuntu disabled unprivileged userns for some legitimate security concerns reply epr 19 hours agorootparentFunny, the original commit message for that suggests it was simply a precaution. It's not out of the ordinary to avoid newer kernel features just in case. > This is a short-term patch. Unprivileged use of CLONE_NEWUSER is certainly an intended feature of user namespaces. However for at least saucy we want to make sure that, if any security issues are found, we have a fail-safe. from: https://web.archive.org/web/20211022013829/https://kernel.ub... reply Aardwolf 19 hours agorootparentprevI really don't get that: having to run something substantial as root seems a much bigger security concern, than what it is shielding from user space (example: hosting a web server at port 80) reply imtringued 14 hours agorootparentprevBecause the docker developers hate security. The idea of the docker group is insane, for example. You can mount any directory into a container so being in the docker group is like having a root account. reply TrueDuality 20 hours agoprevThere is a lot of discussion on here about the different isolation levels available, but these micro-VMs aren't playing in the same field and can't be compared apples-to-apples. If you go read the paper this requires a specialized Xen kernel, which in turn requires processor virtualization extensions directly available where you're running these containers. Those extensions aren't generally available if you're already running inside of a VM. This is a solution that only works on bare metal which I would bet money the vast majority of people using containers, outside of development environments at least, are not running their containers in bare metal but in an existing VM such as on AWS or GCP where this solution is simply a non-starter. Neat, niche, and doesn't operate in the same world as containers. reply _joel 22 hours agoprevShould be tagged with [2017] reply 1vuio0pswjnm7 2 hours agoprevFirecracker micro VM uses musl not glibc. Is it true. reply macspoofing 20 hours agoprev>On the downside, containers offer weaker isolation than VMs, to the point where people run containers in virtual machines to achieve proper isolation. That's not really why containers are deployed in VMs, especially in the context of on-prem enterprise software. I think that's more of a legacy issue. For example, for on-prem enterprise software, the enterprise already invested millions into their VM infrastructure so deploying a containerized stack means deploying into their VM infrastructure. I think when centralized container orchestrators get enough market penetration with properly trained IT, you'll probably see that change. Also, very few people choose containers for security and isolation. Typically it's for flexibility in deployment, and control of the environment (no more dependency hell). reply hi-v-rocknroll 11 hours agoprevContainers seem light and cheap, but they have subtle problems lacking solid guarantees, prioritization, or limits on compute, network, and storage resources that type-1 v12n provides. reply wg0 20 hours agoprevLooking around Kraftcloud, it looks like fly.io but vocabulary is lot simpler and I can totally see it as a viable upcoming PaaS player. Seems like you can run state full workloads too. reply datadeft 20 hours agoprevIs LightVM actively developed or used? The repo is empty. reply codedokode 21 hours agoprev\"VM\" means it has its own kernel? Why have 2 kernels on the same machine? All processes in a proper OS are already isolated and there is no need for VM. reply Cthulhu_ 21 hours agoparentIsolated, but are they isolated enough? The article states that containers offer weaker isolation than VMs. (it doesn't quantify it though and I don't know this kind of thing offhand) reply macspoofing 20 hours agorootparentWho is complaining? And if containers do not offer enough of an isolation, why would you think VMs do? There are use cases where you have to have host-level isolation - for example, if you want to build a HIPAA-compliant cloud service, your customer data has to be isolated at the host level and VMs are not enough. reply codedokode 21 hours agorootparentprevProcesses run in a userspace and cannot do anything without OS approval. reply kevincox 21 hours agorootparentThe Linux kernel has far too large of an attack surface to be trusted as a hard security boundary. It is good enough to prevent mostly trusted software from accidentally interfering with each other but I would not trust it to protect me from an untrusted workload. For example GCP and AWS both have container running services. They both use hardware VMs to isolate different tenants. You will never share a kernel with another customer (I don't even think you will share one with yourself by default). reply fhuici 21 hours agorootparentprevI agree with the other comments. On the cloud, the VM is still the golden standard for strong (hardware-level isolation): if you deploy a container in the cloud, you can almost be sure there's a VM underneath. Given this, what we tried to do in that paper, in the LF Unikraft project (www.unikraft), and on kraft.cloud, is ensure that each VM only has the thinnest possible layer between the application and the hypervisor underneath -- strong isolation and hopefully max efficiency. We do use Dockerfiles to have users specify the app/filesystem, but then we transparently convert them to unikernels (specialized VMs) at deploy time. reply akdev1l 21 hours agorootparentprevThe kernel can be attacked and exploited. Container escape exploits are more common than VM escape exploits. reply brap 20 hours agorootparentprevEverything you said is correct, in theory. In practice, however... reply nderjung 21 hours agoparentprevCorrect -- and you can run multiple kernels on the machine with virtualization extensions. Even Docker Desktop does this. You'd do this for _real_ isolation purposes. reply hi-v-rocknroll 11 hours agoparentprevIt depends on the type of v12n. Paravirtualization and similar, the answer is sort-of while hard emulation is definitely yes. There are efficiencies in memory usage because the often will share the same kernel code and userland code, which are memory pages that can be deduplicated at the hypervisor level. Read more about type-1 v12n. > All processes in a proper OS are already isolated and there is no need for VM. No. This is not how things work in reality. (Ideally, yes because hypervisors are OS \"duct tape\" but there is no such readily-available OS with strict resource limits and hard enforced VFS and network isolation.) Isolation, sharing, and hard limits on RAM, CPU, networking, and storage (bandwidth, block devices, and IOPS) is beyond the capabilities of every major OS. This is why VMware and similar type-1 hypervisors exist. reply flemhans 20 hours agoprevDid something come out of it? What's the best way to run thousands of superlight VMs in 2024? WebAssembly? reply koprulusector 20 hours agoprevFor those that didn’t take the time to read, this is about unikernels reply nashashmi 21 hours agoprevStupid question but forgive me: Whats the difference between a container and a VM? reply volkadav 21 hours agoparenthigh level, a vm is an entire virtual machine with its own kernel/operating system/filesystem/etc. a container is a process (and associated files/archived filesystem) with a (more or less) isolated view of the world (network/filesystem/etc.) running on top of the same kernel/os as other processes on the same machine. examples: a) vm - an entire windows install running in a window on my linux workstation so i can use tax software once a year. two kernels running at the same time. (N+1 for N VMs) b) container - a small python service, its dependencies, and various filesystem bits from alpine-minimal packaged into a file that docker/containerd/whatever can turn into the service running in a little isolated portion of my machine. no matter how many i run, one kernel. the various processes just don't see the host or other procs' files/memory/etc. via namespace trickery (unless there's a security problem, lol) reply akdev1l 21 hours agoparentprevA VM is a virtualized instance with virtual hardware and can therefore run its own operating system with its own kernel to interface with the virtual hardware. A container is basically a process restricted by multiple kernel namespace isolation mechanisms. It shares the same kernel with the host and does not present any “virtual hardware”. reply belter 21 hours agoparentprevTechnically, and simplifying enormously, the VM emulates the whole machine while the Container scopes the OS process. I prefer the analogy of an office building. Your VM is your whole office building and overnight maybe a whole new company can move in but still using the whole building. Your Container is a set of rules, somebody told you when arrived to the reception desk. About where is the only office in the building you can use, plus maybe some common access to shared areas once in a while, like WC and Kitchen. :-) reply chadcmulligan 20 hours agoparentprevThis is a nice little overview https://youtu.be/eyNBf1sqdBQ reply jonahbenton 22 hours agoprev(2017) reply mrAssHat 22 hours agoprevIntegrate that with kubernetes and I'm sold. reply _joel 22 hours agoparentWhat, like https://firecracker-microvm.github.io/ ? reply d3m0t3p 21 hours agorootparentI think it's from the AWS team, they made firecracker (micro VM) So it does exist. Funnily that's what fly does: take your container uncompress it to a full micro VM and run it on their infra reply iamstan23 21 hours agorootparentfly.io uses Firecracker. Firecracker is Open Sourced with an Apache 2 license. It's faster than LightVM mentioned in the post. Firecracker also has containerd support (https://github.com/firecracker-microvm/firecracker-container...). There are a few ways to run Kubernetes with Firecracker, including FireKube. reply throwaway482945 21 hours agorootparentIs it really faster? I thought firecracker boot times were something like 100ms. LightVM claims 2.3ms? reply fhuici 21 hours agorootparentBack when we did the paper, Firecracker wasn't mainstream so we ended up doing a (much hackier) version of a fast VMM by modifying's Xen's VMM; but yeah, a few millis was totally feasible back then, and still now (the evolution of that paper is Unikraft, a LF OSS project at www.unikraft.org). (Cold) boot times are determined by a chain of components, including (1) the controller (eg, k8s/Borg), (2) the VMM (Firecracker, QEMU, Cloud Hypervisor), (3) the VM's OS (e.g., Linux, Windows, etc), (4) any initialization of processes, libs, etc and finally (5) the app itself. With Unikraft we build extremely specialized VMs (unikernels) in order to minimize the overhead of (3) and (4). On KraftCloud, which leverages Unikraft/unikernels, we additionally use a custom controller to optimize (1) and Firecracker to optimize (2). What's left is (5), the app, which hopefully the developers can optimize if needed. reply tpetry 21 hours agorootparentprevLightVM is stating a VM creation of 2.3ms while Firecracker states 125ms of time from VM creation to a working user space. So this comparing apples and oranges. reply fhuici 20 hours agorootparentThe 125ms is using Linux. Using a unikernel and tweaking Firecracker a bit (on KraftCloud) we can get, for example, 20 millis cold starts for NGINX, and have features on the way to reduce this further. reply imtringued 14 hours agorootparentprevI know it's cool to talk about these insane numbers, but from what I can tell people have AWS lambdas that boot slower than this to the point where people send warmup calls just to be sure. What exactly warrants the ability to start a VM this quickly? reply nderjung 21 hours agoparentprevWe ended up doing this over at https://unikraft.io :-) reply crabbone 21 hours agoparentprevThere are NVidia's Kata containers: https://docs.nvidia.com/datacenter/cloud-native/gpu-operator... . I'm not sure you need the physical GPUs to run them though. Most likely not. I'm wondering though what value will Kubernetes add beside integrating with existing (presumably Kubernetes-based) infrastructure? At least, this is my understanding of the rationale for Kata containers. Other than that, it seems like it'd be just getting in the way... reply basemi 20 hours agorootparentSe also: https://katacontainers.io/ reply bjconlan 21 hours agorootparentprevI believe this work originated at Intel as \"clear containers\" (which I believe started life from an acquisition (but could be mixing this up...my memory isn't what it used to be). Either way it's great they are being used like this and at Nvidia (I know Alibaba cloud also use this tech) reply fhuici 21 hours agorootparentYes, Kata started as clear containers. And yes, the main purpose is compatibility with containers -- though generally speaking, adding layers to the cloud stack never helps to make a deployment more efficient. On kraft.cloud we use Dockerfiles to specify app/filesystem, but then at deploy time automatically and transparently convert that to a specialized VM/unikernel for best performance. reply andix 21 hours agoprev [–] I think containers are often misunderstood: The main benefit is not isolation and security, it's defined and reproducible environments and builds. If there is some additional isolation required, just run the container in a VM. reply brabel 21 hours agoparentBut if you can get isolation, security AND reproducible environments using a VM, specially one that's nearly as fast as a OS process, the case for using containers instead pretty much disappears. I don't know this LiteVM thing but I will definitely investigate that, specially given that on my Mac I need to use a VM anyway to run containers! reply fhuici 21 hours agorootparentCheck out kraft.cloud and the accompanying LF OSS project www.unikraft.org :) (disclaimer: I'm one of the authors of the paper and one of the people behind that cloud offering). On KraftCloud we use Dockerfiles so users can conveniently specify the app/filesystem, and then at deploy time transparently convert that to a unikernel (specialized VMs). With this in place, NGINX cold starts in 20 millis, and even heavier apps/frameworks like Spring Boot inFetching dependencies for unikraft/cli/kraftkit: aarch64-elf-binutils, gmp, mpfr, aarch64-elf-gcc, coreutils, gettext, readline, gawk, gnu-sed, pcre2, grep, make, capstone, dtc, mpdecimal, ca-certificates, openssl@3, sqlite, python@3.12, glib, libunistring, libidn2, p11-kit, libnghttp2, unbound, gnutls, jpeg-turbo, libslirp, libssh, libusb, ncurses, snappy, vde, qemu, socat, wget, x86_64-elf-binutils and x86_64-elf-gcc Most should already exist on your mac if you do development... it seems to rely on qemu, unsurprisingly... openjdk as well (probably to support Java out-of-the-box?), imagegick etc. Took a few minutes to finish installing... the CLI seems to be based on the Docker commands (build, clean, run, 'net create', inspect etc.), some package-manager like commands ('pkg info', 'pkg pull', 'pkg list' etc.), a bunch of \"cloud\" commands (I suppose that's the non-free part) and \"compose\" commands just like docker-compose. Interesting stuff. Note for the parent commenter: the Lua link in the landing page is broken: https://github.com/unikraft/catalog/tree/main/examples/http-... I tried to run the C hello world example... I get an error, it wants to run Docker?!?! I thought the whole point was to avoid Docker (and containers)?? Here's the log: i creating ephemeral buildkit container W could not connect to BuildKit client '' is BuildKit running? W W By default, KraftKit will look for a native install which W is located at /run/buildkit/buildkit.sock. Alternatively, you W can run BuildKit in a container (recommended for macOS users) W which you can do by running: W W docker run --rm -d --name buildkit --privileged moby/buildkit:latest W export KRAFTKIT_BUILDKIT_HOST=docker-container://buildkit W W For more usage instructions visit: https://unikraft.org/buildkit W E creating buildkit container: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?: failed to create container PS. running the hello-world pre-built \"image\" worked: > kraft run unikraft.org/helloworld:latest EDIT: A lot of stuff looks broken on MacOS. For example, `kraft menu` doesn't work (error \"no choices provided\", even though the docs show it working fine without \"choices\"?)... `kraft run --elfloader loaders.unikraft.org/strace:latest ./my_binary` also doesn't work (the docs show it working). Error: \"unknown flag: --elfloader\". Seems like the product is still in alpha?! reply alickz 21 hours agorootparentprevi think at that point it's kind of just semantics i don't think devs care if they use containers or VMs, as long as it's easy and they don't have to worry about which version of Python the host is running reply jackcviers3 21 hours agorootparentThis. It's why vagrant was popular before the container revolution. The killer app of Docker isn't the container, it's the depth and uniformity of the UX surrounding the container system. When that is broken by something on the host (non x86 cpu was a major pain for a while before popular images were x-built) and emulation gets in the way and is not as easy, or just mildly different (windows behind corporate firewalls that assign ips used by the docker engine for example), the ease of use falls away for non-power users and it's all painful again. Tech like Docker for windows and Rancher Desktop and lima has largely matured at this point, but somebody could make a new machine and then the process of gradual improvement starts all over again. reply usrusr 20 hours agorootparentprevCertainly depends a lot on what the term \"VM\" actually means in the context. If it's something as specialized as the JVM, or a native virtualization with an extremely trimmed down guest, then at some point you'll find yourself in need of something more heterogenous, e.g. running a tool on the side that does not fit the VM. Then you're back at square one, only this time with containers (or back in some irreproducible ad-hoc setup). Going with containers from the start, containers that may or may not contain a VM, and that may or may not actually do more than what a VM could supply, that's much less hassle than changing horses at a later point. reply byteknight 21 hours agorootparentprevNo it didn't. I want to rebuild my image using a different base. Docker? One line possibly. VM? Afternoon (unless I want to write ansible too) reply pwagland 21 hours agorootparentIn this sort of scenario, then you can also use something like cloud-init: https://cloudinit.readthedocs.io/en/latest/ reply bluGill 21 hours agorootparentprevVMs in general use more CPU power as you have two OSes each doing things like updating their real time clock... There are VM aware OSes that will not do this, but it needs special code and CPU support which means you are often lagging behind the latest (to be fair this is rarely important) A container will normally be slightly faster than a VM never slower (assuming a reasonable OS - I can write an exception if I was malicious) and so there is a lot of interest if they are good enough. reply bartekrutkowski 21 hours agorootparentprevYou don't need a VM on Mac to run containers, check out OrbStack, they provide a Docker compatible engine that is using native MacOS capabilities for running containers without the hidden Linux VM. reply pxc 20 hours agorootparentI don't know where you got that idea. OrbStack absolutely runs a Linux VM. That Linux VM then uses Linux containerization technologies (namely LXD) for each separate OrbStack 'machine' you set up, which is how you get such fast startup times for your OrbStack 'machines'. For Docker, OrbStack does the same thing as Docker Desktop, Podman Desktop, Rancher Desktop, etc., which is set up a Linux VM running Docker and then present a native socket interface on macOS which relays everything it receives to the Docker socket inside the VM. macOS doesn't have native capabilities for running containers, which is why the nearest thing you can get to containerd on it requires you to disable SIP so it can use a custom filesystem to emulate bind mounts/null mounts: https://darwin-containers.github.io/ If you read the PRs where the principal author of the Darwin Containers implementation is trying to upstream bits of his work, you'll see containerd comparing his approaches to others and complimenting them by calling them 'the most containerish' because real capabilities aren't there. (I believe I've read rumors here on HN that Apple has those features internally, fwiw. But they've evidently never released them in a public copy of macOS.) Another clue in all this is to just run uname in any of your Docker containers in OrbStack; you'll see they're Linux machines. Some operating systems have Linux syscall emulation layers (WSL1, FreeBSD's Linux emulation, Illumos' LX Zones) that could perhaps be used to run Linux containers without hardware emulation or paravirtualization in combination with some native containerization capabilities. Afaik Illumos' LX Zones is the only implementation where that's a supported, intended use case but maybe FreeBSD can do it. At any rate, macOS has never had that kind of syscall compatibility layer for Linux, either. So when you run `uname` in a 'macOS container' and see 'Linux', you can be certain that there's a VM in that stack. PS: Aside from the fact that it's proprietary, I really do quite like OrbStack. It's the nicest-to-use implementation of something like this that I've tried, including WSL2 and Lima. The fact that it makes the VM machinery so invisible is very much to its credit from a UX perspective! reply bartekrutkowski 1 hour agorootparentInteresting! I'd swear that in the early days of OrbStack somewhere on their website I've read they're using native MacOS frameworks without the need of Linux VM, but I can't find that anymore (they don't mention Linux VM either, but the language still differs from what I remember). reply tpetry 21 hours agorootparentprevBut then also monitoring gets harder. With containers you could see (and monitor) all those processes running in container easy on the host. With VMs? You now need a way to get data from within the VM which in most lightweight VM implementations just not possible. reply fhuici 21 hours agorootparentOn kraft.cloud we have unikernels (specialized VMs) with Prometheus exporters that can be scraped, and other monitoring facilities. reply andix 20 hours agorootparentprevWhere is the tooling to build and distribute lightweight vms like containers? How can I copy one html file into an nginx VM, built this vm image with multiple architectures (I have arm and x64 servers), publish it, pull it, and run it multiple times? Once again: Containers are not about isolation or security, they are a package format for shipping applications. The packages are easy to build, distribute, multiarch, ... And requiring a Linux-VM on macOS to run Linux containers, is not particularly surprising. reply hmottestad 21 hours agorootparentprevI wouldn't discount the massive user base of containers among developers. reply Timshel 21 hours agorootparentprevAt the time of publication of the article the tool used to create the minimalistic VM Tinyx was not released and as far as I can see was never released. reply fhuici 20 hours agorootparentCorrect, we never did release Tinyx, mostly because it was in a very unclean/researchy state = not ready for public consumption. In retrospect, we probably should have either (a) made it available in whatever state it was in or (b) put more cycles into it. reply ravenstine 20 hours agorootparentprevI agree in principle, but this is not so easy when shared disk IO is introduced, in my experience. reply beeboobaa3 20 hours agorootparentprev> specially given that on my Mac I need to use a VM anyway to run containers Take it up with Apple. reply nderjung 21 hours agoparentprevContainers are perfect for build environments and for creating the root filesystem. The issue is that the kernel these days are super bulky and are intended for multi-user, multi-process environments. Running a container runtime on top just makes it worse when you're looking for \"isolation\". This paper argues that when you build a extremely minimal kernel (i.e. ditch Linux entirely) and link your application against necessary bits of code to execute _as_ a VM, then you'll get better performance than a container and you'll get that isolation. This is in fact true based on performance studies, the follow up paper to this shows so: https://arxiv.org/pdf/2104.12721 (Disclosure, co-author of the linked paper.) We ended up taking this to real workloads if you want to see it in action: https://unikraft.io/ reply pdimitar 21 hours agorootparentYour pricing page has a mistake in the Free tier. The number 1 which is supposed to be a superscript is instead shown as HTML markup (1). reply byteknight 21 hours agorootparentCan confirm it's present on mobile. reply mark_l_watson 20 hours agorootparentprevI am looking at the examples. They all have a Docker file. If that just for local development on my laptop? Using the deploy command line tool is the Docker file used to determine dependencies for the hosted VM? What if a developer is using an unusual programming language, like Common Lisp. Is that doable? reply rad_gruchalski 20 hours agorootparentA Dockerfile is just a file with a bunch of commands to execute and get a working \"computer\". https://github.com/combust-labs/firebuild is fairly aged translation of the Dockerfile to a VM rootfs. reply posix_monad 21 hours agorootparentprev> build a extremely minimal kernel (i.e. ditch Linux entirely) and link your application against necessary bits of code It would be nice, but this is really hard to do when modern software has so many layers of crud. Good luck getting say, a PyTorch app, to work doing this without some serious time investment. reply jerf 21 hours agorootparentBut you don't need to write against all the layers of crud. You only have to write against the bottom layer, the kernel API. This sort of software would have no need to specifically support \"libxml\" or \"TLS\", because that is multiple layers above what this sort of software does. The flip side is that if you want something like low-level access to your specific graphics card you may need to implement a lot of additional support. But of course nothing says you have to use this everywhere at the exclusion of everything else. There's plenty of systems in the world that from the kernel point of view are basically \"I need TCP\" and a whole bunch of compute and nothing else terribly special. reply fhuici 20 hours agorootparent[Author of the paper here] You hit the nail on the head, this is precisely what we do (kernel API compatibility) with the LF Unikraft project (the evolution of the 2017 paper) at www.unikraft.org, and kraft.cloud, a cloud platform that leverages Unikraft. reply bluGill 21 hours agorootparentprevMost of that effort should be sharable. if you know you will only have one python process you can get rid of a lot of cruft. If you know you will be running in a VM then you only need the driver for the network interface the VM provides not every network interface every designed (often including ones that your hardware doesn't even physically support). So while there is serious time investment it isn't nearly as much as it would be to write a competitor to linux. reply ascar 21 hours agorootparentI'm not sure if I missed a bit here, but I have some colleagues doing research on unikernels for HPC and the point is that this unikernel is running directly on the hardware or hypervisor and not inside another VM. The unikernel is effectively a minimal VM and the network stack is one of the things they struggle the most with due to sheer effort. reply fhuici 20 hours agorootparent[One of the authors of the paper] I wouldn't recommend writing a network stack from scratch, that is a lot of effort. Instead, with the Unikraft LF project (www.unikraft.org) we took the lwip network stack and turned it into a Unikraft lib/module. At KraftCloud we also have a port of the FreeBSD stack. reply melenaboija 21 hours agorootparentprev> Running a container runtime on top just makes it worse when you're looking for \"isolation\". The point of the poster was pretty clear: “The main benefit is not isolation and security” reply cduzz 20 hours agoparentprevI tell people \"An OCI container is a way to turn any random runtime into a statically linked binary.\" It is very useful for managing dependency hell, or at least moving it into \"API dependencies\" not \"Library dependencies\", it is handy for pickling a CI/CD release engineering infrastructure. It's not a security boundary. (I'm 100% agreeing with parent, in case I sound contentious) reply scarby2 20 hours agorootparent> It's not a security boundary. It is a security boundary, just not necessarily the best one. reply andix 20 hours agorootparentI would even claim it's a pretty good security boundary, good enough for most applications. reply bayindirh 20 hours agorootparentprevIt's an incidental security boundary because CGroups happen to isolate the process fairly well. reply vilunov 20 hours agorootparentYeah, but that's not an incidental property of *namespaces* (of which cgroups is only one isolation axis), that was the requirement when namespaces were designed. reply bayindirh 20 hours agorootparentYeah, I know. Namespaces are pretty cool outside containers too. My comment was more of a soft jab against using containers as the ultimate \"thing\" for anything and everything. I prefer to use them as \"statically linked binaries\" for short lived processes (like document building, etc.). But, whenever someone abuses containers (like adding an HTTPs fronting container in front of anything which can handle HTTPS on its own) I'm displeased. Relevant XKCD: https://xkcd.com/1988/ reply ajross 20 hours agorootparentprevAll security boundaries are \"incidental\" in that sense, though. Virtualization isn't a \"purpose-designed\" security boundary either, most of the time it's deployed for non-security reasons and the original motivation was software compatibility management. The snobbery deployed in this \"containers vs. VMs\" argument really gets out of hand sometimes. Especially since it's almost never deployed symmetrically. Would you make the same argument against using a BSD jail? Do you refuse to run your services in a separate UID because it's not as secure as a container (or jail, or VM)? Of course not. Pick the tools that match the problem, don't be a zealot. reply bayindirh 19 hours agorootparent> All security boundaries are \"incidental\" in that sense, though X86 protected mode, processor rings, user isolation in the multi user operating systems, secure execution environments in X86 and ARM ISAs, kernel and userspace isolation, etc. are purpose built security boundaries. Virtualization is actually built to allow better utilization of servers, which is built as a \"nested protected mode\", but had great overhead in the beginning, which has been reduced over generations. Containers are just BSD jails, ported to Linux. This doesn't make containers bad, however. They're a cool tech, but held very wrong in some cases because of laziness. reply ajross 19 hours agorootparentThe motivation for MMU hardware was reliability and not \"security\". Basically no one was thinking about computer crime in the 1970's. They were trying to keep timesharing systems running without constant operator intervention. reply nolist_policy 20 hours agorootparentprevDepending on your container runtime (Kata container or gVisor), it's a exceptionally strong security boundary too. reply bluGill 21 hours agoparentprevThere is no such thing as a reproducible build environment anymore. You can get a temporary reproducible build environment, but any sane security policy will have certificates that expire and that in turn means that in a couple years your build environment won't be reproducible anymore. reply cesarb 20 hours agorootparent> but any sane security policy will have certificates that expire and that in turn means that in a couple years your build environment won't be reproducible anymore. \"Reproducible\" is usually defined as \"identical output except for the cryptographic signature at the end\" (and that should be the only use for a certificate in your build environment, a high-quality build environment should be self-contained and have no network access). That is, once you remove the signature, the built artifacts should be bit-by-bit identical. reply bluGill 18 hours agorootparentI said environment not build. reply andix 21 hours agorootparentprevIf you run multiple instances of a container image, you get a reproducible environment. If you run a docker build multiple times, and copy a few files into the container, you get a reproducible container image. It is not a hash perfect duplicate, but functionally equivalent. If builds of your favourite programming language are reproducible or not, is not really related to VM vs. Container. reply flanked-evergl 20 hours agoparentprevHere are some projects that run docker containers on top of micro/lightweight VMs: - https://github.com/firecracker-microvm/firecracker-container... - https://github.com/kuasar-io/kuasar - https://github.com/kata-containers/kata-containers - https://github.com/QuarkContainer/Quark - https://github.com/google/gvisor - https://github.com/containers/libkrun reply lttlrck 20 hours agoparentprevThe main advantage in my use case is in fact isolation (network and volumes) and a well defined API enabling management of those containers in production (not k8s, a tiny subnet of that perhaps). The isolation could be achieved using namespaces directly. But the API, tooling and registry add a lot of value that would otherwise require a lot of development. Also last time I looked hypervisors aren't possible on all cloud vendors, unless you have a bare metal server. This matters in my case. Maybe it has changed in the past 3 years. When docker fits it's great. Same can be said of k8s, where there are a whole bunch of additional benefits. Swings and roundabouts. reply dweekly 21 hours agoparentprevIf this were true, then wouldn't folks just need an application binary that statically links all of its required libraries and resources into a giant, say, ELF? Why even bother with a container? reply Repulsion9513 21 hours agorootparentProgrammers discover the benefits of static linking, and then programmers discover the benefits of dynamic linking, and then programmers discover the benefits of static linking, and then... Anyway containers go quite a bit further than just static linking, most people aren't out there linking all the binaries that their shell script uses together? reply TDiblik 21 hours agorootparentprevFirst thing that comes to mind is the need to link against libraries across platforms. Imagine that my app depends on opencv, if I wanted to statically link everything on my Windows machine, I need to compile opencv for Linux on my windows machine (or use pre-compiled binaries). Also, if you link against libraries dynamicaly, it's likely you can compile them on the host machine (or in a container) with more optimizations enabled. And the last thing is probably the ability to \"freeze\" the whole \"system\" environment (like folders, permissions, versions of system libraries). Personally, I use containers to quickly spin-up different database servers for development or as an easy way of deployment to a cloud service... reply jcelerier 21 hours agorootparentprevWell yes, but try turning some random python, java or ruby service into a single binary .. now do that 12 times. Or try with a native app that leverages both the GPU and libLLVM, and enjoy finding out the kind of precautions you have to take for LLVM to not blow up on a computer where your GPU driver was built with a different LLVM version. reply Cthulhu_ 21 hours agorootparentprevThat's exactly the summary of this. That said, it makes sense from a developer POV; if, during development, you don't need the isolation you can run multiple containers (with on paper fast boot times and minimal overhead) on your development box. There's plenty of cases to imagine where you need the containerization but not necessarily the isolation. reply jbverschoor 20 hours agorootparentThat what I do. I use https://github.com/jrz/container-shell so my projects are contained, and I'm a little bit protected against supply chain attacks reply andix 20 hours agorootparentprevWhat if you application is not just one binary. What if it's a pipeline of complex tasks, calls some python scripts, uses a patched version of some obscure library, ... It's not possible to package half a Linux distribution into a single binary. That's why we have containers. reply dailykoder 21 hours agorootparentprevBecause static libraries ain't a big thing anymore. Maybe they will become popular again. This would make it easier to have reproduceable build without a container. But I think containers are the new static libs now reply indymike 21 hours agorootparentInterpreted languages and their associated dependencies are more of an issue than static linking with compiled languages. reply shawabawa3 21 hours agorootparentprevpeople would absolutely love to do that, but it's difficult, and the UX sucks so people use containers instead (even if the container literally only contains a single statically linked binary) reply otabdeveloper4 21 hours agorootparentprevYes, indeed, and that is golang's main reason for existing. reply dooglius 21 hours agorootparentprevglibc doesn't work statically linked and lots of stuff depends on glibc reply nimbius 21 hours agoparentprevwe arguably already had this with things like python venv. the articles main point still remains, containers are a slow and bloated answer to this problem. I concede youll need containers for Kubernetes, and Kubernetes on the surface is a very good idea, but this level of infrastructure automation exists already in things like foreman and openstack. designs like shift-on-stack trade simplicity of traditional hardware for ever byzantine levels of brittle versioned complexity...so ultimately instead of fixing the problem we invoke the god of immutability, destroy and rebuild, and hope the problem fixes itself somehow...its really quite comical. baremetal rust/python/go with good architecture and CI will absolutely crush container workloads in a fraction of disk, CPU, RAM, and personal frustration. reply jchw 21 hours agorootparentPython venv is language specific, doesn't handle the interpreter version and doesn't handle C libraries. I really don't understand why people do this: I get having a distaste for containers but some people, seeing the massive success of OCI images, mainly seem content on trying to figure out how to discredit its popularity, rather than trying to understand why it's popular. The former may be good for contrarian Internet forums, but the latter is more practically useful and interesting. I say this with some level of understanding as I also have a distaste for containers and Docker is not my preferred way to do \"hermetic\" or \"reproducible\" (I am a huge Nix proponent.) I want to get past the \"actually it was clearly useless from the start\" because it wasn't... reply jbverschoor 21 hours agorootparentprevNot really, you'd still need a proper chroot / etc. Check out https://github.com/jrz/container-shell reply benreesman 21 hours agoparentprevContainers have their place, if you’re racking and running your own gear life gets a lot better than screwing around with IPMI. But reproducible infrastructure as code is just orthogonal to that: everything from Salt to Nix is credible in that role. Containerizing on top of a Xen hypervisor never made sense to me. reply oceanplexian 20 hours agorootparentAll the younger engineers I talk to think you would need to be Albert Einstein to bootstrap a bare metal server. As someone who made a living doing this at scale, where we would build a new datacenter every 2-4 weeks using 100% open source or off the shelf tools, I completely disagree. I think PXE booting some servers and running a binary on them is 90% easier than most container orchestration engines, Kubernetes control plane, and all the other problems engineers seem to have invented for themselves. I also think it’s almost always much more performant. Engineers don’t have an intuition to realize that their XXLarge-SuperDuper instance is actually a 5 year old Xeon they’re sharing with 4 other customers. Cloud Prociders obfuscate this as much as possible, and charge a King’s ransom if you want modern, dedicated hardware. reply benreesman 11 hours agorootparentI’ll urge that we don’t compare Albert Einstein to a k9s expert. No one will ever be considered for a Nobel over Kubernetes. reply fhuici 21 hours agoparentprev[disclaimer: I'm one of the authors of the paper] I 100% agree, containers are an amazing dev env/reprodicble env tool! In fact, we think they're the perfect marriage to the unikernels (specialized VMs) we used in the paper; on kraft.cloud , a cloud platform we built, we use Dockerfiles to specify apps/filesystems, and transparently convert them to unikernels for deployment. The end result is the convenience of containers with the power of unikernels (eg, millisecond cold starts, scale to zero and autoscale, reduced TCB, etc). reply andrewpolidori 21 hours agoparentprevWhile reproducible build envs are a nice feature of using containers, they aren't the primary benefit. The primary benefit is resource usage and orchestration. Rather than duplicating entire aspects of an OS stack (which might ne considered wasteful) they allow for workloads to share aspects of the system they run on while maintaining a kind of logical isolation. This allows for more densely packed workloads and more effective use of resources. This is a reason why the tech was developed and pushed by google and adopted by hyperscalers. reply belter 21 hours agoparentprevYou are absolutely correct, and the creators of Docker did mention that was the core reason. Unfortunately your comment comes 10 years too late for many. reply BobbyTables2 20 hours agoparentprevI agree with you, but the world seems to think otherwise! reply kkfx 21 hours agoparentprevNixOS and Guix System offer a far lighter and more reproducible approach, who also not push many running images build by unknown on the internet direct in production, full of outdated deps, wasting in the meantime storage and cpu resources... reply oneplane 21 hours agorootparentYet it doesn't even come close to a fraction of the adoption scale of containers, no matter how good it is. Ecosystems matter more than individual quality. reply bigstrat2003 19 hours agorootparentI think that's a gross overstatement. Ecosystems matter, yes. But they don't matter more than quality. reply kkfx 21 hours agorootparentprevThat's because some interested parties have advertised containers, because they are good to sell as pre-built stuff, nice to sell VPS and alike etc, while pure IaC is useful for anyone and invite NOT to be dependent on third party platforms. It's not a technical matter, it's a human, economical matter and actually... Most people are poor, following the largest scale means following poverty not a good thing. reply nailer 21 hours agoparentprev [–] > If there is some additional isolation required, just run the container in a VM. No. Running a container in a VM gets you no additional isolation. Containers share kernel space and as such have limited isolation to VMs, which have isolated kennels. In exchange for this Lack of additional isolation, you’ve added a Bunch of extra Complexity. Pardon the extra caps I am using iOS voice dictation. reply kevincox 21 hours agorootparent [–] I think they mean run a VM with one container inside. So you do get strong isolation. This is similar to how managed container IaaS works. They launch a VM and run your container in it. It is extra complexity but has a few advantages. 1. People already have a convenient workflow for building container images. 2. The base OS can manage hardware, networking and whatever other low-level needs so that the container doesn't need to have these configurations. 3. If you want to trade of isolation for efficiency you can do this. For example running two instances of a container in the same VM. The container doesn't need any changes to support this setup. reply fhuici 20 hours agorootparentThe model of a single container within a VM just adds overhead. The ideal case would be to remove the container layer and have the application(s) within the container run directly in the VM (which hopefully only includes the libs and OS modules needed for the app to run, and nothing more). This is the approach we take at kraft.cloud (based on the LF Unikraft project): use Dockerfiles to specify app/filesystem, and at deploy automatically convert to a lightweight VM (unikernel) without the container runtime/layer. reply nailer 17 hours agorootparentprev [–] > So you do get strong isolation. No, you don’t. There is no benefit the container is providing, because The only feature of the container is isolating you from the zero other containers running on the VM. reply kevincox 17 hours agorootparent [–] The isolation I am referencing is from the VM, not the container. Containers don't provide strong isolation, that is why the VM is required in this model. reply nailer 9 hours agorootparent [–] The VM is an isolated environment itself. You do not need to be isolated from it. Using two levels of userland isolation makes about the same sense as using 457 levels of userland isolation. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The research article delves into the balance between isolation and efficiency in virtual machines (VMs) and containers, presenting LightVM as a solution leveraging Xen for quick boot times and optimal performance.",
      "It covers multiple research papers and resources on VM-based cloudlets, container-based OS virtualization, and secure virtualization architecture, emphasizing the importance of strong isolation and efficiency for various applications.",
      "With 182 citations, it addresses CPU management challenges in diverse workloads within virtualized settings, available online or for download in PDF format."
    ],
    "commentSummary": [
      "The ACM website debate compares virtual machines (VMs) and containers, emphasizing Linux container security measures like user namespaces.",
      "Discussions include topics like Docker's \"rootless\" mode, the shift to Podman for enhanced security, and the potential of lightweight VMs and unikernels.",
      "Platforms such as kraft.cloud and projects like Unikraft are praised for advancing secure and efficient application deployments, highlighting the ongoing discourse on security, performance, and operational trade-offs among containers, VMs, and unikernels."
    ],
    "points": 162,
    "commentCount": 165,
    "retryCount": 0,
    "time": 1715685842
  },
  {
    "id": 40358588,
    "title": "Introducing APT 3.0 Solver: solver3 and Its Innovative Approach",
    "originLink": "https://blog.jak-linux.org/2024/05/14/solver3/",
    "originBody": "Blog of Julian Andres Klode May 14, 2024 The new APT 3.0 solver APT 2.9.3 introduces the first iteration of the new solver codenamed solver3, and now available with the –solver 3.0 option. The new solver works fundamentally different from the old one. How does it work? Solver3 is a fully backtracking dependency solving algorithm that defers choices to as late as possible. It starts with an empty set of packages, then adds the manually installed packages, and then installs packages automatically as necessary to satisfy the dependencies. Deferring the choices is implemented multiple ways: First, all install requests recursively mark dependencies with a single solution for install, and any packages that are being rejected due to conflicts or user requests will cause their reverse dependencies to be transitively marked as rejected, provided their or group cannot be solved by a different package. Second, any dependency with more than one choice is pushed to a priority queue that is ordered by the number of possible solutions, such that we resolve a|b before a|b|c. Not just by the number of solutions, though. One important point to note is that optional dependencies, that is, Recommends, are always sorting after mandatory dependencies. Do note on that: Recommended packages do not “nest” in backtracking - dependencies of a Recommended package themselves are not optional, so they will have to be resolved before the next Recommended package is seen in the queue. Another important step in deferring choices is extracting the common dependencies of a package across its version and then installing them before we even decide which of its versions we want to install - one of the dependencies might cycle back to a specific version after all. Decisions about package levels are recorded at a certain decision level, if we reach a conflict we backtrack to the previous decision level, mark the decision we made (install X) in the inverse (DO NOT INSTALL X), reset all the state all decisions made at the higher level, and restore any dependencies that are no longer resolved to the work queue. Comparison to SAT solver design. If you have studied SAT solver design, you’ll find that essentially this is a DPLL solver without pure literal elimination. A pure literal eliminitation phase would not work for a package manager: First negative pure literals (packages that everything conflicts with) do not exist, and positive pure literals (packages nothing conflicts with) we do not want to mark for install - we want to install as little as possible (well subject, to policy). As part of the solving phase, we also construct an implication graph, albeit a partial one: The first package installing another package is marked as the reason (A -> B), the same thing for conflicts (not A -> not B). Once we have added the ability to have multiple parents in the implication graph, it stands to reason that we can also implement the much more advanced method of conflict-driven clause learning; where we do not jump back to the previous decision level but exactly to the decision level that caused the conflict. This would massively speed up backtracking. What changes can you expect in behavior? The most striking difference to the classic APT solver is that solver3 always keeps manually installed packages around, it never offers to remove them. We will relax that in a future iteration so that it can replace packages with new ones, that is, if your package is no longer available in the repository (obsolete), but there is one that Conflicts+Replaces+Provides it, solver3 will be allowed to install that and remove the other. Implementing that policy is rather trivial: We just need to queue obsoletereplacement as a dependency to solve, rather than mark the obsolete package for install. Another critical difference is the change in the autoremove behavior: The new solver currently only knows the strongest dependency chain to each package, and hence it will not keep around any packages that are only reachable via weaker chains. A common example is when gcc- packages accumulate on your system over the years. They all have Provides: c-compiler and the libtool Depends: gccc-compiler is enough to keep them around. New features The new option --no-strict-pinning instructs the solver to consider all versions of a package and not just the candidate version. For example, you could use apt install foo=2.0 --no-strict-pinning to install version 2.0 of foo and upgrade - or downgrade - packages as needed to satisfy foo=2.0 dependencies. This mostly comes in handy in use cases involving Debian experimental or the Ubuntu proposed pockets, where you want to install a package from there, but try to satisfy from the normal release as much as possible. The implication graph building allows us to implement an apt why command, that while not as nicely detailed as aptitude, at least tells you the exact reason why a package is installed. It will only show the strongest dependency chain at first of course, since that is what we record. What is left to do? At the moment, error information is not stored across backtracking in any way, but we generally will want to show you the first conflict we reach as it is the most natural one; or all conflicts. Currently you get the last conflict which may not be particularly useful. Likewise, errors currently are just rendered as implication graphs of the form [not] A -> [not] B -> ..., and we need to put in some work to present those nicely. The test suite is not passing yet, I haven’t really started working on it. A challenge is that most packages in the test suite are manually installed as they are mocked, and the solver now doesn’t remove those. We plan to implement the replacement logic such that foo can be replaced by foo2 Conflicts/Replaces/Provides foo without needing to be automatically installed. Improving the backtracking to be non-chronological conflict-driven clause learning would vastly enhance our backtracking performance. Not that it seems to be an issue right now in my limited testing (mostly noble 64-bit-time_t upgrades). A lot of that complexity you have normally is not there because the manually installed packages and resulting unit propagation (single-solution Depends/Reverse-Depends for Conflicts) already ground us fairly far in what changes we can actually make. Once all the stuff has landed, we need to start rolling it out and gather feedback. On Ubuntu I’d like automated feedback on regressions (running solver3 in parallel, checking if result is worse and then submitting an error to the error tracker), on Debian this could just be a role email address to send solver dumps to. At the same time, we can also incrementally start rolling this out. Like phased updates in Ubuntu, we can also roll out the new solver as the default to 10%, 20%, 50% of users before going to the full 100%. This will allow us to capture regressions early and fix them. Reactions from Mastodon Copyright © 2018-2020 Julian Andres Klode, articles licensed under CC BY-SA 4.0. Comments are provided by Mastodon and copyright of their authors. This website does not store any personally identifiable information. As part of standard web server access_log logging, it stores requests and the user agents and shortened IP addresses used to make them. It does, however, load some avatars from mastodon. Powered by Hugo, and the Ernest theme.",
    "commentLink": "https://news.ycombinator.com/item?id=40358588",
    "commentBody": "The new APT 3.0 solver (jak-linux.org)159 points by todsacerdoti 15 hours agohidepastfavorite86 comments amelius 14 hours agoCan anyone explain why libc versions are so often a problem when installing software? I understand that all libraries in an executable need to use the same version of malloc. But otherwise I don't understand the reason for these clashes. Even malloc can be swapped out for another function with the same functionality. And I also don't understand why libc needs to be updated so frequently, anyway. reply codexon 14 hours agoparentThey are a problem because gcc automatically links to the latest version of glibc. As to why they don't add an option to specify an older version? I don't know either and it is rather annoying to have to use docker images of older OSes to target older glibc versions. It's just one of many things that prevents linux from being as popular as windows for desktop users. reply zX41ZdbW 11 hours agorootparentI've made a library named \"glibc-compatibility\": https://github.com/ClickHouse/ClickHouse/tree/master/base/gl... When linking with this library before glibc, the resulting binary will not depend on the new symbol versions. It will run on glibc 2.4 and on systems as old as Ubuntu 8.04 and CentOS 5 even when built on the most modern system. reply thrtythreeforty 10 hours agorootparentHow does this work? reply threecheese 8 hours agorootparentIn case parent didn’t see your q: https://www.lightofdawn.org/wiki/wiki.cgi/NewAppsOnOldGlibc reply adastra22 8 hours agorootparentprevYou are a hero. reply bregma 14 hours agorootparentprevHmm. The MSVCRT.DLL/MSVCRTD.DLL not being binary compatible between releases of Visual Studio is the same thing, except of course you can't even combine some modules compiled for debug with modules compiled without debug in the same executable. The Windows problem has always been so so much worse that pretty much all developers simple resorted to shipping the OS system runtime with every package and it's just expected nowadays. It's where the phrase \"DLL hell\" originated, after all. Not to say the ABI problem isn't real if you want to combine binary packages from different Linux-based OSes. Plenty of solutions for that have cropped up as well: containers, flatpacks, snaps, the list goes on. reply delta_p_delta_x 13 hours agorootparentThis comment is full of inaccuracies and mistakes, and is a terrible travesty of the Windows situation. > except of course you can't even combine some modules compiled for debug with modules compiled without debug in the same executable. There's a good reason for this, which IMO Unix-like compilers and system libraries should start adopting, too. Debug and Release binaries like the standard C and C++ runtimes and libraries cannot be inter-mixed because they have have different ABIs. They have different ABIs because the former set of binaries have different type layouts, many of which come with debug-specific assertions and tests like bounds-checking, exception try-catch, null-dereference tests, etc. > The Windows problem has always been so so much worse that pretty much all developers simple resorted to shipping the OS system runtime with every package and it's just expected nowadays. This is not true at all. There are several layers to the counterargument. Firstly, UCRT has supplanted MSVCRT since Visual Studio 2015, which is a decade old this year. Additionally, UCRT can be statically linked: use `/MT` instead of `/MD`. And linking back to the previous quote, to statically link a debug CRT, use `/MTd`. Set this up in MSBuild or CMake using release/debug build configurations. UCRT is available for install (and maintained with Windows Update) in older versions of Windows going back to Vista. Next, Windows by default comes with several versions of C++ redistributables going back to Visual Studio 2005. All of these redistributables are also regularly maintained with Windows Update. Finally, Windows SDK versions targeting various versions of Windows are available for all supported Visual Studio developer environments. The oldest currently available in Visual Studio 2022 is Windows XP SP3[1]. These all serve to thoroughly solve both combinations of backward-forward compatibility, where (a) the runtime environment is newer than the developer environment, and (b), the runtime environment is older than the developer environment. It is perfectly possible to compile a single `.exe` on Visual Studio 2022 in Windows 11, and expect it to run on Windows XP SP3, and the vice versa: compile a single `.exe` on Visual Studio 6, and expect it to run on Windows 11. No dynamic libraries, no DLLs, nothing; just a naked `.exe`. Download from website, double-click to run. That's it. No git clone, no GNU Autotools, no configure, no make, no make install (and then f*cking with rpaths), nothing. Prioritising binary-only software distribution means Windows has prioritised the end-user experience. > It's where the phrase \"DLL hell\" originated, after all. This is also incorrect. 'DLL hell' is a pre-NT problem that originated when packagers decided it was a good idea to overwrite system binaries by installing their own versions into system directories[2]. Sure, the versioning problem was there too, but this is itself a result of the aforementioned DLL stomping. [1]: https://learn.microsoft.com/en-us/cpp/build/configuring-prog... [2]: https://en.wikipedia.org/wiki/DLL_Hell#DLL_stomping I fully daresay writing C and C++ for Windows is an easier matter than targeting any Unix-like. For context, see what video game developers and Valve have done to check off the 'works on Linux' checkbox: glibc updates are so ridiculously painful that Valve resorted to simply patching WINE and releasing it as Proton, and WINE is the ABI target for video games on Linux. reply jraph 11 hours agorootparentThat Windows is generally better at handling compatibility is one thing, but I'm curious about the following part: > There's a good reason for this, which IMO Unix-like compilers and system libraries should start adopting, too. Why? I'm understanding from your comment that you can't mix debug and release objects on Windows because the ABI is different. That's not a feature, that's a limitation. If it works on Linux to mix debug-enabled objects with \"release\", what use would it have to make it not work anymore? IIUC debug symbols can be totally separated from the object code, such that you can debug the release if you download the debug symbols. A well configured GDB on distros that offer this feature is able to do it automatically for you. It seems very useful and elegant. Why can't Windows do something like this and how is it an advantage? (Genuine question, I have a remote idea on how ELF works (wrote a toy linker), not much how DWARF works, and not the slightest idea on how all this stuff works on Windows) reply jcelerier 6 hours agorootparent> If it works on Linux to mix debug-enabled objects with \"release\" it definitely does not. MSVC's debug mode is akin to for instance using libstdc++ with -D_GLIBCXX_DEBUG which does change the ABI. Just passing \"-g\" which enable debug symbols is very different from what Microsoft calls Debug mode, which adds very extensive checks at all levels of the standard library (for instance, iterators become fat objects which track provenance, algorithms check preconditions such as \"the input data is sorted\", etc.) reply gpderetta 2 hours agorootparentNote tough that libstdc++ has an ABI compatible debug mode that still adds a significant amount debug checks (and it is meant for production deployment). reply josephg 11 hours agorootparentprevYes, I wonder that too. The comment says that debug and release builds have ABIs because they have different type layouts. But why do they have different type layouts? Bounds checking and assertions shouldn’t change the type layout. It seems to me that debug flags should generally only modify code generation & asserts. This is usually the case on Linux, and it’s extremely convenient. If windows is going to insist on different libraries in debug and release mode, I wish the development version of the library bundled debug and release builds together so I could just say “link with library X” and the compiler, linker and runtime would just figure it out. (Like framework bundles on the Mac). Windows could start by having a standard for library file naming - foo.obj/dll for release and foo-debug.obj/dll for debug builds or something. Then make the compiler smart enough to pick the right file automatically. Seriously. It’s 2024. We know how to make good compiler tooling (look at go, Swift, rust, etc). There’s no sane reason that C++ has to be so unbelievably complex and horrible to work with. reply tlb 2 hours agorootparentWindows debug builds add extra sanity checks for which it needs extra members in types. For instance, a vector::iterator is just a T* in a regular build, but in a debug build it also keeps a pointer to the vector so it can check bounds on every access. But yes, C++ punts a lot of things to the build system, partly because the standard has to work on embedded systems where shared libraries don’t exist. A better build system could fix most of these things, but every build system that tries ends up massively complicated and slow, like Bazel. reply josephg 22 minutes agorootparentBut fixing it in Windows doesn’t mean they also need to fix it in embedded systems. Microsoft is in control of their whole ecosystem from the kernel to userland to visual studio. Microsoft could make C++ on windows sane without anyone else’s permission. Their failure to do that is on them and them alone. I think browser vendors have the right idea when it comes to evolving standards. Vendors experiment using their own products and then come together and try and standardise their work at committee. I think that would be a much better idea than either doing nothing or, as you say, trying to boil the ocean. reply delta_p_delta_x 10 hours agorootparentprev> I wish the development version of the library bundled debug and release builds together Almost all do. Look for binary library releases; they almost always supply Debug and Release binaries together. > Windows could start by having a standard for library file naming - foo.obj/dll for release and foo-debug.obj/dll for debug builds or something. Funnily enough, it does. foo.lib for Release; food.lib for Debug. reply DHowett 9 hours agorootparentprev> Bounds checking and assertions shouldn’t change the type layout. Any bounds checks and assertions that rely on storing additional data such as valid iterator ranges or mutation counters would need to change the type layout, wouldn't they? Even if the STL were purely a header-only library (and influenced only by code generation changes for debug builds), there's still the problem of ABI compatibility across different translation units--or different libraries--which might be built with different options. EDIT: One of your sibling comments goes into greater detail! reply delta_p_delta_x 11 hours agorootparentprevThere's a bit of a conflation here; partially my fault. Allow me to clarify... To generate debug symbols for a given binary (whether executable or library) on Windows and MSVC's cl.exe (and Clang on Windows), compile with `/DEBUG`[1] and one of `/Z7`, `/Zi`, or `/ZI`[2]. This is equivalent to `-g` on Linux gcc/clang. In particular, `/Z7` generates separate `.pdb` files, which contain debug symbols for the binary in question. The options that the parent commenter and I were discussing, i.e. `/MD`, `/MDd`, /MT`, and `/MTd`[3] have to do with the C and C++ runtime link configuration. These correspond to multithreaded dynamic, multithreaded dynamic debug, multithreaded static, and multithreaded static debug respectively. Therefore, the small `d` refers to debug versions of the C and C++ runtimes. The differences between the debug and release versions of the C and C++ runtimes are listed in the following links[4][5][6][7][8]. The last link in particular demonstrates the debug CRT's functionality. Conventionally on Windows, debug binaries are linked to the debug versions of the C and C++ runtimes; ergo the requirement that 'Release and Debug binaries on Windows cannot be combined'. This convention is respected by all maintainers who release binary libraries on Windows. There is no equivalent on Unix-likes: it'd be like having 'debug' versions of libc.so.6/libstdc++.so/libc++.so/libpthread.so with different ABIs. If you wanted to change between release/debug here, you would have to at least re-link (if not re-compile) everything. Imagine having `-cstdlib=libc-debug` and `stdlib=libc++-debug` options. Both sets of options (debug symbol options and C runtime link options) are orthogonal, and may be freely combined. Hence, it is perfectly possible to link the debug versions of the C and C++ runtimes to a 'release' executable, although it would be pretty weird. For instance, `/O2 /LTCG /arch:AVX2 /MTd`. Equivalent imaginary GNU-style command: `-O3 -flto=thin -march=x86-64-v3 -cstdlib=libc-debug stdlib=libc++-debug -static`. You can see what I mean, I hope. [1]: https://learn.microsoft.com/en-gb/cpp/build/reference/debug-... [2]: https://learn.microsoft.com/en-gb/cpp/build/reference/z7-zi-... [3]: https://learn.microsoft.com/en-gb/cpp/build/reference/md-mt-... [4]: https://learn.microsoft.com/en-gb/cpp/c-runtime-library/c-ru... [5]: https://learn.microsoft.com/en-gb/cpp/c-runtime-library/crt-... [6]: https://learn.microsoft.com/en-gb/cpp/c-runtime-library/debu... [7]: https://learn.microsoft.com/en-gb/cpp/c-runtime-library/run-... [8]: https://learn.microsoft.com/en-gb/cpp/c-runtime-library/crt-... reply forrestthewoods 11 hours agorootparentprev> If it works on Linux to mix debug-enabled objects with \"release\", what use would it have to make it not work anymore? There is no difference between Linux and Windows here. The debug/release issue is ultimately up to the API developer. C++ has has the standard template library (STL). libstdc++, libc++, and MSVC STL are three different implementations. STL defines various iterators. A common choice is for a release-mode iterator to be a raw pointer, just 8 bytes on 64-bit. But the debug-mode iterator is a struct with some extra information for runtime validation, so it's 24 bytes! The end result is that if you pass an iterator to a function that iterator is effectively two completely different types with different memory layouts on debug and release. This is a common issue with C++. Less so with C. But it's not a platform choice per se. > IUC debug symbols can be totally separated from the object code, such that you can debug the release if you download the debug symbols. A well configured GDB on distros that offer this feature is able to do it automatically for you. It seems very useful and elegant. Why can't Windows do something like this and how is it an advantage? MSVC always generates separate .pdb files for debug symbols. Windows tooling has spectacular tooling support for symbol servers (download symbols) and source indexing (download source code). It's great. reply o11c 9 hours agorootparentThe difference is that on Linux, \"compile my program in debug mode\" does not enable libstdc++'s expensive (and incompatible) mode. reply jcelerier 6 hours agorootparent> The difference is that on Linux, \"compile my program in debug mode\" \"Linux\" does not have a \"compile my program in debug mode\" magic toggle (or Release or whatever for what it's worth). Different IDEs and toolchains may have different defaults and expectations. \"g++ -g\" is not debug mode, it's debug symbols. reply forrestthewoods 9 hours agorootparentprevMy point is the real difference is the implementation of the three libraries. It's not a fundamental platform difference. reply forrestthewoods 13 hours agorootparentprev> The Windows problem has always been so so much worse Hard, hard disagree. The problems are somewhat comparable. But if any platform is more painful it's Linux. Although they're similar if you exclude glibc pain. At least in my personal experience of writing lots of code that needs to run on win/mac/linux/android. > pretty much all developers simple resorted to shipping the OS system runtime with every package Meanwhile Linux developers have resorted to shipping an entire OS via docker to run every program. Because managing Linux environment dependencies is so painful you have to package the whole system. Needing to docker to simply launch a program is so embarrassing. > except of course you can't even combine some modules compiled for debug with modules compiled without debug in the same executable That's not any different on Linux. That has more to do with C++. reply graemep 13 hours agorootparent> Meanwhile Linux developers have resorted to shipping an entire OS via docker to run every program. > Needing to docker to simply launch a program is so embarrassing. I have never needed docker \"just to launch a program\". Docker makes it easy to provide multiple containerised copies of an identical environment. Containers are a light alternative to VM images. I assume you find the existence of Windows containers just as embarrassing? https://learn.microsoft.com/en-us/virtualization/windowscont... reply forrestthewoods 12 hours agorootparent> Docker makes it easy to provide multiple containerised copies of an identical environment. Correct. The Linux architecture around a global of dependencies is, imho, bad and wrong. The thesis is it's good because you can deploy a security fix to libfoo.so just once for the whole system. However we now live in a world where you actually need to deploy the updated libfoo.so to all your various hierarchical Docker images. sad trombone > Containers are a light alternative to VM images. A light alternative to Docker is simply deploy your dependencies and not rely on a fragile, complicated global environment. > I assume you find the existence of Windows containers just as embarrassing? Yes. I know my opinion is deeply unpopular. But I stand by it! Running a program should be as simple as downloading a zip, extracting, and running the executable. It's not hard! reply rixed 5 hours agorootparentI wish you all had experienced the golden age when running a program was as simple as apt-get install program and run it. I find it hard to discuss the merits of Linux va windows with regard to deploying software without addressing the elephant in the room which is the replacement of a collectively maintained system that ensure software cohabitation by the modern compartmentalised collection of independent programs. reply LtWorf 11 hours agorootparentprevI guess your software is not libre, right? In that case, I think you should stick to snap and flatpak. reply fullspectrumdev 12 hours agorootparentprevInstead of Docker, static linking is a much more elegant solution if library/dep management is painful. Switching to static linking using a sane libc (not glibc) can be a pain initially but you end up with way less overhead IMO. reply munchler 11 hours agorootparentStatic linking is a good way to avoid the problem, but I’d hardly call it “elegant” to replicate the same runtime library in every executable. It’s very wasteful - we’re just fortunate to have enough storage these days to get away with it. reply forrestthewoods 9 hours agorootparent> I’d hardly call it “elegant” to replicate the same runtime library in every executable. It’s very wasteful - we’re just fortunate to have enough storage these days to get away with it. I think you need to quantify \"very wasteful\". Quite frankly it's actually just fine. Totally fine. Especially when the alternative has turned out to be massive Docker images! So the alternative isn't actually any better. Womp womp. An actually elegant solution would be a copy-on-write filesystem that can deduplicate. It'd be the best of both worlds. reply jcelerier 6 hours agorootparentprev> Switching to static linking using a sane libc (not glibc) can be a pain initially but you end up with way less overhead IMO. how does that work when you app needs access to the gpu drivers ? reply Sesse__ 12 hours agorootparentprevLinking to the latest version of glibc is, in itself, not a problem -- glibc hasn't bumped its soname in ages, it is using symbol versioning instead. So you only get a problem if you use a symbol that doesn't exist in older glibc (i.e., some specific interface that you are using changed). As for using an older version of glibc, _linking_ isn't the problem -- swapping out the header files would be. You can probably install an old version of the header files somewhere else and just -I that directory, but I've never tried. libstdc++ would probably be harder, if you're in C++ land. reply fweimer 12 hours agorootparentRecent libstdc++ has a _dl_find_object@GLIBC_2.35 dependency, so it's not exactly trivial anymore to link a C++ program against a older, side-installed glibc version because it won't have that symbol. It's possible to work around that (link against a stub that has _dl_find_object@GLIBC_2.35 as a compat symbol, so that libstdc++ isn't rejected), but linking statically is much more difficult because libstc++.a (actually, libgcc_eh.a) does not have the old code anymore that _dl_find_object replaces (once GCC is built against a glibc version that has _dl_find_object). This applies to other libraries as well because there are new(ish) math functions, strlcpy, posix_spawn extensions etc. that seem to be quite widely used already. reply ynik 1 hour agorootparentI find it's best to treat this as a case of \"cross-compilation to old glibc version\". That is, you don't want to just link against an old glibc version, you want a full cross-compilation toolchain where the compiler does not pick up headers from `/usr/include` by default (but instead has its own copy of the system headers), and where libstdc++ is also linked against that old glibc version. We used https://crosstool-ng.github.io/ to create such a \"cross-compiler\". Now it doesn't matter which distribution our developers use, the same source code will always turn into the same binary (reproducible builds, yay!) and those binaries will work on older distributions than the developers are using. This allows us to ship dynamically linked linux executables; our linux customers can just unzip + run, same as our windows customers, no need to mess with docker containers. The downside is that we can't just use a library by `apt install`ing it, everything needs to be built with the cross compilation toolchain. reply sunshowers 3 hours agorootparentprevIf you're building Rust, check out cargo zigbuild (yes, zigbuild) which lets you target old glibc. reply IshKebab 14 hours agorootparentprevI think the main reason they don't offer a `--make-my-binary-compatible-with-the-ancient-linux-versions-users-often-have` is that GCC/glibc is a GNU project and the are philosophically against distributing software as binaries. I don't think there's any technical reason why it couldn't be done. To be fair to them though, Mac has the same problem. I worked at a company where we had to keep old Mac machines to produce compatible binaries, and Apple makes it hard to even download old versions of MacOS and Xcode. I guess the difference is MacOS is easy to upgrade so you don't have to support versions from 13 years ago or whatever like you do with glibc. reply fweimer 12 hours agorootparentI used to think that binary compatibility benefits proprietary applications, but I'm not so sure anymore. From a commercial perspective, when we break binary compatibility (not that we want to), it's an opportunity for selling more stuff. Many distributions do periodic mass rebuilds anyway and do not need that much long-term ABI compatibility. Binary compatibility seems mostly for people who compile their own software, but have not automated that and therefore couldn't keep up with updates if there wasn't ABI compatibility. reply IshKebab 3 hours agorootparentI agree. It's annoying for closed source apps but they generally have the resources to deal with it anyway. E.g. with Questa I can just unzip it and run it. No trouble. It's disproportionately annoying for open source projects who don't want to waste their time dealing with this. reply codexon 14 hours agorootparentprev> I think the main reason they don't offer a `--make-my-binary-compatible-with-the-ancient-linux-versions-users-often-have` is that GCC/glibc is a GNU project and the are philosophically against distributing software as binaries. You don't have to statically compile glibc, gcc just needs an option to tell the compiler to target say, version 2.14 instead of the latest one. The newest glibc has all the older versions in it. That's why you can compile on say ubuntu 14 and have it run on ubuntu 24. reply saurik 13 hours agorootparentNo like, the point is that the only reason you (and I: I do this all the time, including with my open source software... like: no judgment) want to target some old version of glibc is so you can distribute that binary to people without caring as much about what version of the OS they have; but that would be unnecessary if you just gave them the source code and have them compile their own copy for their system targeting the exact libraries they have. reply codexon 13 hours agorootparentUnfortunately most people don't want to bother compiling, myself included. I tried gentoo one time and it took 1 hour to compile 5 minutes worth of apt-get on ubuntu. reply fweimer 12 hours agorootparentprevOnly the dynamically linked bits, the statically linked startup code and libc_nonshared.a are missing from newer versions. Most programs don't need them (who needs working ELF constructors in the main program?). The libc_nonshared.a bits can be reimplemented from scratch easily enough (but we should switch them over to header-only implementations eventually). reply forrestthewoods 14 hours agorootparentprev> They are a problem because gcc automatically links to the latest version of glibc. As to why they don't add an option to specify an older version? Because glibc and ld/lld are badly designed. glibc is stuck in the 80s with awful and unnecessary automagic configure steps. ld/lld expect a full and complete shared library to exist when compiling even though it expects a different shared library to exist in the future. Zig solves the glibc linking issue. You can trivially target any old version for any supported target platform. The only thing you actually need are headers and a thin, implementation free lib that contains stub functions. Unfortunately glibc is not architected to make this trivial. But this is just because glibc is stuck with decades of historic cruft, not because it's actually a hard problem. reply einpoklum 12 hours agorootparent> awful and unnecessary automagic configure steps Steps taken when? When building glibc? And - what steps? > ld/lld expect a full and complete shared library to exist when compiling ... But ld and lld are linkers... > Zig solves the glibc linking issue. But Zig is a language. Do you mean the Zig standard library? The Zig compiler? > The only thing you actually need are headers and a thin, implementation free lib that contains stub functions. Why do you need stub functions at all, if you're not actually using them? reply ploxiln 6 hours agorootparentThe zig compiler can compile C and C++, using llvm, and it also packages various libc implementations, including glibc, musl, mingw, and msvc, and more for other OSes. Some people use it as a more convenient golang-like cross-compiler. And this whole combination is a smaller download and install than most other toolchains out there. It just took some inspiration and grunt work, to dissect the necessary (processed) headers and other bits of each libc ... hats of to the zig devs. Random blog post I just found about it: https://ruoyusun.com/2022/02/27/zig-cc.html reply LtWorf 11 hours agorootparentprevchroot has existed for many years. reply cookiengineer 6 hours agoparentprevThe problem is actually: nobody adhering to semantic versioning. Hence the reason why there is a separate API/ABI level embedded into a shared object file, which is pretty much always completely different from the library's package version because they broke compatibility so often. Additionally there's lots of libraries that break their symbols and hash tables on every subminor update. (Usually when they use LLVM, but I'm not saying it's LLVM's fault) For example, harfbuzz and libicu are a regular offender of this, meaning they fuck up every downstream project because the method signatures contain a randomized chunk which changes on every single subminor version. reply braiamp 9 minutes agorootparentlibicu is deliberately doing this because one of its purpose is to offer Unicode ordering. If consumers try the new ordering on non-rebuild objects, it could lead to data loss. That's what happened with glibc 2.28. [0] If postgres prefers explicit library versions to prevent data corruption, then icu is doing things correctly and making sure that downstream consumers don't shoot themselves in the foot. [0]: https://wiki.postgresql.org/wiki/Locale_data_changes reply fweimer 12 hours agoparentprevYou don't have to update, it's just that developers seem to like to run the latest stuff, and many appear to build production binaries on their laptops (rather than in a controlled build environment, where dependencies are tightly managed, and you could deliberately stick to an older distribution easily enough). The dependency errors indicate real issues because of the way most distributions handle backwards compatibility: you have to build on the oldest version you want to support. Those errors happen if this rule is violated. For glibc-based systems, the effect is amplified because package managers have become quite good at modeling glibc run-time requirements in the package-level dependencies, and mismatches result in install-time dependency errors. Admittedly, I'm biased, but I strongly suspect that if we magically waved away the glibc dependency issues, most applications still wouldn't work because they depend on other distribution components, something that's just not visible today. reply o11c 9 hours agoparentprevBecause: * Developers like to be on the latest-and-greatest distro, and rarely perform builds in a chroot. Sometimes this is sheer apathy; other times it is because backporting library dependencies is annoying. * End-users can't even be assumed to be on the latest LTS. Related, almost nobody understands `rpath` and why it should always be used instead of static linking (assuming normal dynamic linking doesn't work of course). There's some common FUD going around but unless you're a setuid or similar program it's not actually true (and even then, it's only an issue with some uses of rpath). reply david_draco 14 hours agoprevAfter reading so many GPT news item, I was confused what APT is, no explanation on the page, no link ... took me a while to realise it is about debian linux' packaging tool apt. reply westurner 10 hours agoprev> Solver3 is a fully backtracking dependency solving algorithm that defers choices to as late as possible. It starts with an empty set of packages, then adds the manually installed packages, and then installs packages automatically as necessary to satisfy the dependencies. [...] > If you have studied SAT solver design, you’ll find that essentially this is a DPLL solver without pure literal elimination DPLL algorithm: https://en.wikipedia.org/wiki/DPLL_algorithm reply westurner 9 hours agoparentprefix-dev/pixi is a successor to Mamba and Conda which wraps libsolv like dnf. https://github.com/prefix-dev/pixi OpenSUSE/libsolv: https://github.com/openSUSE/libsolv OpenSUSE/zypper may have wrapped or may still wrap libsolv? TIL libsolv also supports arch .apk and .deb; there's a deb2solv utility to create libsolv .solv files: https://github.com/openSUSE/libsolv/blob/master/tools/deb2so... \"[mamba-org/rattler:] A new memory-safe SAT solver for package management in Rust (port of libsolv)\" (2024) https://prefix.dev/blog/the_new_rattler_resolver https://news.ycombinator.com/item?id=37101862 Pixi is integrating uv: https://github.com/astral-sh/uv/issues/1572#issuecomment-194... astral-sh/uv [1] solves versions with PubGrub [2], which was written for Dart: [1] https://github.com/astral-sh/uv [2] https://github.com/pubgrub-rs/pubgrub uv's benchmarks are run with hyperfine, a CLI benchmarking tool: https://github.com/sharkdp/hyperfine astral-sh/rye was mitsuhiko/rye, and rye wraps uv with \"fallback to unearth [resolvelib] and pip-tools\": https://github.com/astral-sh/rye https://lucumr.pocoo.org/2024/2/15/rye-grows-with-uv/ So, TIL it looks like rye for python and pixi for conda (python, r, rust, go,) are the latest tools for python packaging, not apt packaging. fpm is one way to create deb packages from python virtualenvs for install with the new APT solver, which resulted in this research. reply gsich 14 hours agoprevFrom the guy that brought you the keepassxc downgrade. https://github.com/keepassxreboot/keepassxc/issues/10725 reply j1elo 12 hours agoparentWait so this guy decided to fork the project, and seemingly abused his position to supplant the previous version with his own opinionated fork? All this while disregarding the opinion of upstream devs themselves and being arrogant and stubborn in his replies. What an spectacular way to break things for end users. If there's one thing to learn and apply from Linus, IMHO, is his attitude about NEVER breaking userspace in the Kernel. This lesson can be adapted to most software, and we really should strive to more of it, not less (obviously adjusting to each case; here, replace \"userspace\" with \"user setups\") reply layer8 11 hours agorootparentFirst, this is not in Debian stable (yet). Issues like this is one reason why Debian testing/unstable exist. Secondly, it is common and expected for distributions to select the feature flags they deem appropriate. It is not a fork. The mistake here was not to provide a compatible upgrade option in addition to the new default. reply Spivak 5 hours agorootparentWhen I found out that it was just in unstable my position flipped on it. The guy has no business being as crass as he is in the comments section of the GH issue but lack of tact from an opinionated software dev in internet discourse is nothing new. And the point behind the petty of switching to package/package-full, which is pretty standard in the Debian world, is perfectly reasonable so long as it happens on a major version bump. I don't disagree that it's annoying to the upstream devs but c'est la vie when you have a bunch of 3rd parties repackaging your code. They won't be the first to have a \"uninstall your distro's package and install the upstream version before reporting a bug\" in their issue template. reply bjoli 13 hours agoparentprevI never thought I would say this but: I am a pretty recent flatpak convert. This thing just made me more convinced about how it is a good thing to let the Devs deliver the apps themselves and let the distro people do the distro stuff. After accepting flatpak I started using fedora silverblue and then switched to opensuse aeon and I have been very happy. The only pain point was getting Emacs working properly. reply k8sToGo 14 hours agoparentprevWhat downgrade are you referring to? reply gsich 14 hours agorootparenthttps://github.com/keepassxreboot/keepassxc/issues/10725 reply k8sToGo 14 hours agorootparentThanks. I love opensource drama. reply noisy_boy 8 hours agoparentprevSeems like another Pottering in making - my way or highway developer. reply yjftsjthsd-h 14 hours agoprev [–] > The most striking difference to the classic APT solver is that solver3 always keeps manually installed packages around, it never offers to remove them. Y'know, when you put it like that it does seem rather obvious, doesn't it? Principle of least surprise and all that. Edit: To be clear, not knocking anyone for it; hindsight is 20/20 and I've never written a dependency solving algorithm so I'm in no position to judge. reply layer8 13 hours agoparentIt’s not that clearcut, because sometimes you forget all the packages you installed manually over the years and aren’t using anymore, and it can make sense to be offered to remove one when it helps resolving a conflict with a newer package that you actually want to use now. reply inopinatus 12 hours agorootparentThe healthy choice is to rebuild rather than upgrade any long-lived OS installation (e.g. your desktop) on distro release. Corollary: make it easy on yourself: mount /home separately (and maybe /var too), and keep /etc in scm. reply layer8 11 hours agorootparentI’ve been upgrading Debian in-place for almost two decades now and prefer that approach. Security updates are automated (daily), major-version upgrades have almost no downtime, and only very rarely is there a critical configuration that needs to be adjusted. The installed packages can be listed with `dpkg --get-selections`, and that list can be replayed should it be necessary to recreate the installation, plus a backup of /etc/. But I never had to do this, Debian just works. reply inopinatus 11 hours agorootparentI was previously a maintainer of certain Debian packages, and of similar vintage, so this advice comes with the extra salt of having seen how the sausage is made. I shudder to think how many abandoned files, orphan packages, and obsolete/bad-practice configurations might be lurking in a system that has only been release-upgraded for decades. Yes, no doubt it functions. By the same token, people can live in their own filth. Should they? I choose not to. That said, I may do a speculative dist-upgrade on a snapshot to reveal & prepare for conflicted conffiles in advance, but I'll throw that away, I won't rely on the merged result across a release upgrade. reply calvinmorrison 9 hours agorootparentBut when there's a configuration change dpkg-configure asks you if you want the upstream one, no? reply hsbauauvhabzb 11 hours agorootparentprevDebian releases are somewhat slower than Ubuntu or similar - given infinite time, esoteric configurations will break on update due to some edge case 4 dist-upgrades ago. reply josephg 11 hours agorootparentprev> The healthy choice is to rebuild rather than upgrade any long-lived OS installation (e.g. your desktop) on distro release. I wish there was more rigor (and testing) with this sort of thing. Generally systems should have the invariant that “install old” + “upgrade” == “install new”. This property can be fuzz tested with a bit of work - just make an automated distribution installer, install random sets of packages (or all of them), upgrade and see if the files all match. /etc makes this harder, since you’d want to migrate old configuration files rather than resetting configuration to the defaults. But I feel like this should be way more reliable in general. And people want that reliability - if the popularity of docker and nix are anything to go by. reply inopinatus 10 hours agorootparentAlas that many packages will not upgrade conffiles gracefully when their structure/semantics change, and many are beyond the ability of any package manager's built-in diffing tools to apprehend. That is why I place /etc under scm for long-lived hosts, because what happens next instead looks an awful lot like doing a conflicted three-way code merge following a breaking-change vendor release. The greatest OMFGFFS in this realm, by far, came with the big lurch over to systemd. reply pxc 8 hours agorootparentrelated: https://etckeeper.branchable.com/ > The greatest OMFGFFS in this realm, by far, came with the big lurch over to systemd. I think by the time I started with it, NixOS was already on systems, but reportedly that transition went off without a hitch. Before the transition, NixOS configs were in charge of generating OpenRC scripts or whatever, then afterwards the same configs generated systemd units instead. When the system has total control over the config files like that you get to bypass certain challenges! You might notice that the old announcement for that feature includes no special care other than the requisite reboot to change init systems: https://web.archive.org/web/20200423143059/https://releases.... reply LtWorf 11 hours agorootparentprevIn theory yes, but this hits with the fact that defaults change. For example a new system won't have pulseaudio, but it won't be removed and replaced automatically because that would be potentially disruptive to existing users. reply miki123211 9 hours agorootparentprevThe even healthier choice is to store all your system state in a config, you can then just occasionally clean up the config. This also lets you put extensive comments on why a package is needed or even create a submodule for a specific project. Shame that the only bistro that lets you do this is extremely arcane and difficult to learn if you do not have a PhD in mathematics. reply pxc 8 hours agorootparent> Shame that the only distro that lets you do this is extremely arcane and difficult to learn if you do not have a PhD in mathematics. Hey, there are two! Remember GuixSD reply firewolf34 10 hours agorootparentprevWhat do you use to \"keep /etc in scm\"? reply pxc 8 hours agorootparentetckeeper was the standard tool last time I really ran the kind of distro that benefits from it reply firewolf34 3 hours agorootparentThanks for the recommendation! :) reply dylan-m 10 hours agorootparentprevThis is one of my favourite things about Fedora Silverblue and rpm-ostree. If I run `rpm-ostree status`, I get a list of all the packages I’ve installed or removed compared to the base image. Makes it really easy to keep track of what’s different in my system compared to the current default. reply josephg 11 hours agorootparentprevI don’t know if other distributions do this, but I quite like gentoo’s “world” file. Its a text file listing all the manually installed packages. Everything in the list is pinned in the dependency tree, and kept up to date when you “emerge update”. I constantly install random stuff for random projects then forget about it. The world list lets me easily do spring cleaning. I’ll scroll through the list, delete everything I don’t recognise and let the package manager auto remove unused stuff. I think nix has something similar. I wish Debian/ubuntu had something like that - maybe it does - but I’ve never figured it out. reply adgjlsfhk1 7 hours agorootparentI think OSes really should take inspiration from the newest generation of programming languages here. Rust and Julia (as well as a few others) do a lot better than the OS. Features like environments (e.g. for installing per user packages), and a clean separation of a project and an environment would be very nice. reply yjftsjthsd-h 2 hours agorootparentI hesitate to say it because it comes up in approximately every single conversation about package management, but... NixOS does all of those things. Whole list of packages in configuration.nix, whole of /etc encoded into configuration.nix, flakes let you pin everything, (therefore) it's trivial to check the whole OS config (packages, versions, configurations) into version control, home manager lets you do all this per-user (or you can do it in the system-wide configuration.nix if you really want), and you can drop a flake.nix+flake.lock in any project directory and scope things that way. reply skrause 10 hours agorootparentprevDebian has debfoster. reply stock_toaster 10 hours agorootparentprevAlpine has a similar /etc/apk/world file. Further, you can edit this file and then `apk fix` will update the system to reflect any changes. reply o11c 9 hours agorootparentprevOn Debian-derived systems, the list can be found with: aptitude search '?installed ?not(?automatic)' (or filter with that within the program, etc.) There's probably a way without aptitude, but using Debian without aptitude is like riding a bicycle without gears. reply yjftsjthsd-h 12 hours agorootparentprevYeah, I've hit that exact situation on one of my machines running Void Linux (so obviously completely different package manager). I personally think it's best to tell the user that it can't make the given constraints work and suggest which package(s) are the problem, and then if the user is willing to part with them then they can `apt remove foo && apt upgrade` or w/e (which is more or less my experience of how Void's xbps does it). reply giancarlostoro 13 hours agoparentprev [–] I love when this happens when I find a new product, I think to myself would be even better if they did this really simple tweak. Then a week or two later they do the same exact change. Apt probably moves a little slower on “big” changes, since theres many implications. I cant even imagine the number of obscure scripts that something like this breaks because the output and behavior is slightly off somehow. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Julian Andres Klode introduces the new APT 3.0 solver, solver3, utilizing a backtracking dependency solving algorithm with deferred choices.",
      "The blog post details the solver's functionality, contrasts it with SAT solver design, and outlines behavior changes and additional features.",
      "Future plans entail enhancing error information storage, passing the test suite, and incorporating replacement logic, with a gradual user rollout and feedback collection."
    ],
    "commentSummary": [
      "The text addresses challenges when linking to different glibc versions in Linux software installation, prompting users to resort to solutions like Docker for compatibility.",
      "It contrasts Linux with Windows development, emphasizing Windows' ease of compatibility due to redistributables compared to Linux's complexities.",
      "Discussions include debug vs. release builds, C++ development, STL implementations, dynamic vs. static linking, managing dependencies, package tools, upgrades, and the importance of clean configurations for efficient package management in operating systems."
    ],
    "points": 159,
    "commentCount": 86,
    "retryCount": 0,
    "time": 1715712050
  }
]
