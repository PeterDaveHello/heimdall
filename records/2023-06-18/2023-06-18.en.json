[
    {
        "id": 36367667,
        "timestamp": 1686981128,
        "title": "London Underground Dot Matrix Typeface",
        "url": "https://github.com/petykowski/London-Underground-Dot-Matrix-Typeface",
        "hn_url": "http://news.ycombinator.com/item?id=36367667",
        "content": "London TFL Dot Matrix TypefaceA set of dot matrix fonts in the style of TFL's Underground network. Each font weight represents a unique typeface featured on TFL arrivals boards and rolling stock led announcement boards.London Underground RegularThis typeface sets out to recreate the font used on dot matrix arrival boards found across London Underground stations in late-2019 / early-2020.Character Set DetailsThis font includes a full alpha character set (upper and lower cases), numbers, and symbols - ' & * +.London Underground MediumThis typeface sets out to recreate the dot matrix font found displayed on London Underground 1996 Stock.Character Set DetailsThis font includes a full alpha character set (upper and lower cases) and symbols ! ' , - . / : ; & \\.NOTE: Unverified Characters! This font currently includes 'best effort' characters due to a lack of reference materials for certain letters. Please contact me on Twitter if you have photographs of these characters.F K Qq U V X Y ZLondon Underground BoldCharacter Set DetailsThis font only includes a full number set and symbols :.London Underground HeavyThis typeface sets out to recreate the font used on dot matrix arrival boards found across London DLR stations in late-2019 / early-2020.Character Set DetailsThis font includes a full alpha character set (upper and lower cases), numbers, and symbols - ' , * ().ReferenceLondon Underground Regular, Bold, and Heavy is sourced from original photographs of dot matrix arrival boards found across London Underground stations during late-2019 / early-2020.The following photographs are an example of the particular dot matrix style which was referenced in the typeface creation.London Underground Medium is sourced from video recordings found on YouTube. Attributes as follows:Tony - Transit & GamesX2K9Random TransportContributeBuild Additional Characters to Existing TypefaceFrom Adobe Illustrator template file:Build New CharacterAdd new artboardProvide name of character for layer and artboardBuild character on separate layerFile > Export > Export As...Export Artboards as SVGChoose Export locationFormat: SVG (svg)Use Artboards CheckedAll Artboards SelectedMaintain default SVG optionsAdd Characters to Existing TypefaceOpen .sdf file in FontForgeSelect character to updateFile > ImportSet width to width of character + 100If svg width is 600 then set element width to 700Generate FontFile > Generate FontsMaintain default export valuesSave into font directory of repoLicenseThis typeface is distributed with an SIL Open Font License.",
        "summary": "- The London TFL Dot Matrix Typeface is a collection of fonts that mimic the style of the fonts used in the London Underground's arrival boards and announcement boards.\n- The typeface includes different weights, such as regular, medium, bold, and heavy, each representing a specific type of font used in different contexts.\n- The fonts include a full range of alphabetic characters, numbers, and symbols, although some characters may be incomplete due to limited reference materials.\n- The typeface is created using photographs and video recordings of the original dot matrix style used in the London Underground.\n- Users can contribute to the typeface by building additional characters using an Adobe Illustrator template file and adding them to the existing font.\n- The typeface is distributed with an SIL Open Font License, allowing for free and open use.",
        "hn_title": "London Underground Dot Matrix Typeface",
        "original_title": "London Underground Dot Matrix Typeface",
        "score": 433,
        "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginLondon Underground Dot Matrix Typeface (github.com/petykowski)433 points by nickswalker 23 hours ago | hide | past | favorite | 64 commentsggambetta 9 hours ago | next [\u2013]Heh, reminds me of the early 90s, when I was making my first attempts at complete videogames. I fell in love with the Lucasfilm games, specifically Monkey Island, so I started to build my own graphic adventure engine.I wanted to have the same look, and I naively thought the same fonts would be important (and conveniently ignored the question of how to make high quality art, since I suck as an artist). So I got some graph paper and copied the two fonts by hand, pixel by pixel, off the screen -- the bold font with an outline used for dialogue, and the skinnier font used for the UI.I never finished that graphic adventure (mainly due to not really having a plot, not due to technical difficulties), but the fonts looked pretty convincing! I think I still have that graph paper with the fonts in some box in the basement.PS: OMG, I found an actual screenshot! https://gabrielgambetta.com/files/ne2.png Please enjoy my terrible 3D Studio skills and even wrose pixel art skills XDreplysschueller 18 hours ago | prev | next [\u2013]I did a similar thing re-creating the font used by the VBZ in Z\u00fcrich, Switzerland.https://sschueller.github.io/posts/vbz-fahrgastinformation/#...replyunwind 9 hours ago | parent | next [\u2013]Very cool, thanks for sharing!I'm not a native speaker of English, but pretty sure that the word \"tic\" [sic] is not what you want. Look into apostrophes [1].[1]: https://en.m.wikipedia.org/wiki/%27_(disambiguation)replysschueller 6 hours ago | root | parent | next [\u2013]I recently got to see the official documentation on the departure sign and it refers to what I called the \"tic\" as \"Hochkomma\" which would be a \"quote\" symbol.replyfidotron 18 hours ago | parent | prev | next [\u2013]The actual physical display here is great too. Certainly the London ones a key part of the aesthetic is the orange LEDs. How is the brightness and power consumption?replydom96 18 hours ago | root | parent | next [\u2013]I built a programmable one a while back[1] with the primary use case being TfL departures. I have hooked it up to a smart plug with energy consumption metrics and the max it uses is around 0.53Wh every 10 minutes.At one point I considered selling it, but with the amount of legal stuff around CE marking I got demotivated. Plus these days you have https://tidbyt.com/.1 - https://twitter.com/d0m96/status/1427055272980328460replytialaramex 16 hours ago | root | parent | next [\u2013]One thing I couldn't figure out (but you might know) for Tidybyt was: Can it be told to use a different server, supposing that the present makers went away? Without that, I have to consider the lifetime of the device is entwined with the lifetime of the makers.replymatja 17 hours ago | root | parent | prev | next [\u2013]= 3.18 Wreplynetsharc 12 hours ago | parent | prev | next [\u2013]The font looks like a familiar one from Windows, maybe Tahoma?replysschueller 6 hours ago | root | parent | next [\u2013]I recently found out that it's a custom font designed by a \"Herrn Buser\" known as \"Buser B\u00f6V Schrift\". It was specifically made to be readable by visually impaired persons from a larger distance.replythrow0101a 13 hours ago | prev | next [\u2013]For more British-y rail stuff, check out Geoff Marshall's channel:* https://www.youtube.com/@geofftech2/videosA few years ago, he and his girlfriend/partner (Vicki) visited all (then) 2,563 railway stations in England, Wales, and Scotland:* http://allthestations.co.uk* https://en.wikipedia.org/wiki/All_the_Stations* https://www.youtube.com/c/allthestations* https://news.ycombinator.com/item?id=16461196replymortenjorck 14 hours ago | prev | next [\u2013]The London Underground Regular font struck me as quite peculiar when I encountered it on a trip a few years back. The decision to lower the baseline for uppercase letters is highly nonstandard \u2013 I\u2019m not aware of any other Latin-based digital signage system in the world that does that.I wonder if it was the result of a study that concluded visibility was slightly increased over a consistent baseline due to the extra pixel of height for caps. Whatever the reason, it certainly gives the signs a quirky character (no pun intended).replyphireal 13 hours ago | parent | next [\u2013]I've always assumed it's a nod to the original underground signage which had offset capital letters (https://upload.wikimedia.org/wikipedia/commons/3/33/London_U...).replythrowaheyy 10 hours ago | parent | prev | next [\u2013]It makes the capitals stand out more and I imagine it could be helpful for visitors who may have trouble remembering long sequences of Latin characters and may pictorially memorize the capital letters instead.replybschne 13 hours ago | parent | prev | next [\u2013]Was wondering the same thing. If you look at some of the station displays pictured, you can see that there\u2018s not much space between lines, so I assume this is to have space for descenders without clashing with the lines below (more like raising the baseline for non-capital letters).replycjrp 6 hours ago | root | parent | next [\u2013]Yep, see the screenshot with Epping on the display for example.replyComputerGuru 13 hours ago | parent | prev | next [\u2013]I might be feeling particularly cynical but I don\u2019t get the feeling these were based off any study.Anyway, it works ok for most of the characters but it\u2019s quite jarring for the Ki because it looks like the K descends into the i\u2019s cell (though it doesn\u2019t).replySymbiote 10 hours ago | root | parent | next [\u2013]Britain has several commissioned fonts for public signage [1] [2] [etc], and generally has very high quality road and rail signage \u2014 in most cases both the design and the content.I'd be surprised if these fonts were not based on a study, or developed as part of a study to find a particularly legible dot-matrix font.[1] https://en.wikipedia.org/wiki/Transport_(typeface)[2] https://en.wikipedia.org/wiki/Rail_AlphabetEdit: I think we need TfL document 1-312 \"Automated audio and visual information in public areas of stations and trains\", but I can't find it online. Only the newer standards using LCD displays:TfL Digital display standards: https://content.tfl.gov.uk/tfl-digital-display-standards.pdfLondon Underground Good Practice Guide: Onboard Electronic Signing: http://www.signdesignsociety.co.uk/images/Knowledgebase/TfL_...replyComputerGuru 3 hours ago | root | parent | next [\u2013]I was speaking from personal experience with past industrial projects commissioned with lcd/led displays - albeit admittedly not with the British government as a client. Thanks for those links, I will have to study them!I figured the result was dictated by the specific dimensions and display density of hardware that matched the brightness/environment/size/etc requirements they set forth.I know the care that goes into finding fonts for standardized government use (road signs, national parks, etc) but imagined this particular set of circumstances was confined to the displays commissioned in that particular round of updates and decided by the contractor and the liaison officials.replyinconceivable 11 hours ago | parent | prev | next [\u2013]it also greatly increases the visibility / contrast for ALL-CAPS which looks to be quite common in these signs. while allowing for the qgyj letters that descend one pixel below baselinereplyGordonjcp 7 hours ago | parent | prev | next [\u2013]I think it may just be because it looks less weird than only pushing up the characters with descenders, like in HD44780 LCDs.replyadrianmsmith 18 hours ago | prev | next [\u2013]I wish Vienna, Austria used nice fonts worth copying in their electronic signs, but alas... https://imgur.com/CQ1dYqkreplymortenjorck 13 hours ago | parent | next [\u2013]Wow, that looks like a scalable font was rasterized at a resolution where hinting would be required\u2026 but the font didn\u2019t actually have hinting.replyqbasic_forever 15 hours ago | parent | prev | next [\u2013]Lol it's like each character was developed in a vacuum and someone forget to tell the W creator there was a width limit until too late. Maybe an allegory to modern corporate software development there...replyjjgreen 17 hours ago | parent | prev | next [\u2013]Oof, that's grim typography, my condolencesreplygareve 17 hours ago | parent | prev | next [\u2013]thanks, i hate itreplyautonomousErwin 15 hours ago | prev | next [\u2013]I always wondered what tech/programming was behind the notification boards on trains and in train stations. Do each station custom build their own solution, or is there a company who provides this to TFL/UK stations?There's a couple of pubs in London (specifically close to underground stations) which have a notification boards above the bar for the next trains coming in and peering behind the notification display I see a Raspberry Pi dangling...replySymbiote 15 hours ago | parent | next [\u2013]I think the companies providing railway signalling systems, like Thales, Alstom, Hitachi, Siemens, Westinghouse, would be most likely to supply this sort of system \u2014 or more likely, supply the system behind it but outsource the displays.Here's the company that manufactured the ones for National Rail: https://trueform.com/products/next-train-indicator-nti/replyfredoralive 14 hours ago | parent | prev | next [\u2013]Actual on-station signage will mostly be from a set of standard parts from railway suppliers.https://wiki.openraildata.com/ has information about various open data feeds available for the railways in Great Britain, and https://tfl.gov.uk/info-for/open-data-users/ for TfL. Third party solutions for things like railway adjacent bars presumably use one of these to get departures board info etc.replySymbiote 10 hours ago | prev | next [\u2013]Section 2.13 (page 32) has the matrix used on tram stops: https://content.tfl.gov.uk/tramlink-stop-signs-standard.pdfP56 has what looks like the same font for London Overground: https://content.tfl.gov.uk/overground-signs-standard.pdfAnd P47 for the DLR: https://content.tfl.gov.uk/dlr-signs-standard.pdfreplytialaramex 18 hours ago | prev | next [\u2013]For \"London Underground Medium\", it's almost certain this technology is character based (it's not archaic enough to be hand-crafted values) and so somebody with even test access to a 1996 stock (ie Jubilee Line trains) can presumably tell one to display notices with the desired characters: F K Qq U V X Y Z and take a photo without it ever being in service.On the other hand, the 1995 stock is very similar, same vendor, same era, likely identical display panels so I'd expect you could collect some of this data (e.g. London Zoo is accessed via the Northern Line, and that's a Z right there if the display mentions this) from those trains and assume it's the same.replySymbiote 17 hours ago | parent | next [\u2013]K should be easy to get when the next station is Kingsbury, and Q is available at the following stop, Queensbury. F from Finchley Road.U might show as \"London Underground\" at some point.Does V show in \"Change for the Victoria Line\"? I can't remember if these trains show that.replyseanhunter 15 hours ago | root | parent | next [\u2013]There is the tube stop called \u201cVictoria\u201d also.They have done a great job on these fonts btw. They are instantly recognisable to any Londoner.replyqingcharles 12 hours ago | root | parent | next [\u2013]I think fonts and typography all across the UK is excellent. See also British Rail, and all the road signs.As a Britisher exported to the USA, the fonts and typography of government projects here regularly gives me the heebie-jeebies. I can't even bear to look at the highway signage.replygrishka 14 hours ago | root | parent | prev | next [\u2013]London fonts in general are very recognizable. The print one with rhomboid dots, too.replyraphlinus 13 hours ago | root | parent | next [\u2013]https://en.wikipedia.org/wiki/Johnston_(typeface)Agreed, it's extremely distinctive.replySymbiote 15 hours ago | root | parent | prev | next [\u2013]It needs to be a stop on the Jubilee (or possibly Bakerloo) line, as the relevant dot matrix is only installed on one (or possibly two) types of train.replyseanhunter 13 hours ago | root | parent | next [\u2013]If we\u2019re talking about the same thing they have that same font on the circle and district line, which goes through victoria. Source: am on a district line train now and went through Victoria a few minutes agoreplygareve 16 hours ago | root | parent | prev | next [\u2013]After London City Airport comes \"King George V\"replyJulesRosser 19 hours ago | prev | next [\u2013]Love this. Would be a great addition to the 'model my local tube station' project that's probably going to remain on my to-do list for everreplyquickthrower2 18 hours ago | parent | next [\u2013]Ha ha I dreamed of doing that as a kid not living in London but thinking the tube is pretty coolreplyqingcharles 12 hours ago | prev | next [\u2013]Also see British Rail which had an equally outsized effect on UK typography, and was (and continues to be) very beautiful in my eyes:https://britishrailmanual.com/https://en.wikipedia.org/wiki/British_Rail_Corporate_Identit...replyencyclic 9 hours ago | parent | next [\u2013]I backed the British Rail Corporate Identity Manual project on Kickstarter. It's a wonderful book. My copies were slightly damaged in transit and they were replaced. I still have the 3 slightly damaged ones (dings on the cover, mostly) and if anyone wants them you can have them for the price of postage from the US. Ping me at the email in my bio.replyriverdweller 17 hours ago | prev | next [\u2013]Lovely stuff. Now someone please use this in a Bioshock-like adventure based in London's semi-apocalyptic west end.replystevedh 16 hours ago | parent | next [\u2013]https://fallout4london.com/ ?replysignalToNose 16 hours ago | prev | next [\u2013]Stockholm subway had a nice font. Digital version is called esseltube. https://sv.m.wikipedia.org/wiki/EsseltubreplyFindecanor 15 hours ago | parent | next [\u2013]That's a printed font though.BTW, Stockholm's subway used to up until very recently have dot-matrix LED signs with what I think did a neat embedded-programming trick. They could switch between a static message and scrolling text, but the scrolling text was slanted.I think the slanting was done by varying the timing for each row of LEDs. The bottom row was scrolled first, then the next above it after a short delay, and so on.I have been thinking of trying that trick for making scrolling text legible on a keyboard with backlit keys in a traditional row-staggered configuration. But I would first need a keyboard PCB for DIY with individually addressable LEDs.replyjrmg 12 hours ago | root | parent | next [\u2013]This italic scrolling was pretty standard on any LED sign in the 80s and 90s. I always assumed that, rather than a stylistic choice, it was just caused by the lines updating one after another because of update or display multiplexing speed.replypsychphysic 13 hours ago | root | parent | prev | next [\u2013]Interesting. I think the easiest way would to have each row on its own shift registers. It'd be trivial but if each pixel was on LED you could only slant at 45\u00b0 intervals, which would be annoying.Maybe make each pixel 2LED\u00d71LED (vertical).This would be the equivalent of shifting it by adding an offset to the characterset bitmap that depends on the position vertically in glyph.replyJaxan 13 hours ago | prev | next [\u2013]I\u2019ve always loved how the capital letter extend below the baseline.replyvr46 19 hours ago | prev | next [\u2013]Neato - I made an RPI bus arrival sign for my desk, wondering if I can repurpose these fonts.replyc0wb0yc0d3r 11 hours ago | parent | next [\u2013]Does it show you the actual bus schedule, or does it it show some other information?replyagumonkey 13 hours ago | prev | next [\u2013]Very soothing, I fail to know if it's nostalgia or if these geometric primitive based fonts are really massaging your brain positivelyreplysdsrgdrtgh 12 hours ago | prev | next [\u2013]There is something vaguely solipsistic about this project\u2014the creator apparently didn't try to contact TfL to ask if the designer of these displays was still around. Meanwhile Tfl put a lot of effort into memorializing their typographic tradition: https://www.youtube.com/watch?v=LIAxVW-9fRoWithout meaning to sneer, I can't help noticing this project represents a really trivial engagement with typography (dot matrix characters are really simply to copy). There's no craft involved, obviously, and not necessarily any taste either. Typography and graphic design in general are full of those things. Maybe the designers of the signage would have been able to shed some light on those aspects...replycsilverman 9 hours ago | parent | next [\u2013]What exactly were you expecting, a thesis project or documentary or something? As far as I can tell, petykowski just wanted to recreate the typefaces used on the Underground's digital signage. That's fine. Not everything has to be some magnum opus.I'm speaking as a graphic designer who's been obsessed with typography for decades. Craft and taste are qualities I ordinarily pay a lot of attention to, and if petykowski had talked to anyone affiliated with these typefaces, I'm sure it would have been fascinating. But it's also important to remember that these faces are not in the same category as a high-end serif or something. They serve a bluntly functional purpose. The medium for which they're designed is entirely different. Of course they're simple and easy to copy; that's the point.So I'm not sure how you are defining \"craft\" or \"taste\" here. These designs are crisp and legible. Your comment feels a little like somebody looking at an icon of Clarus the Dogcow (https://en.wikipedia.org/wiki/Dogcow) and saying \"Ah, but where's the dimensionality? Where's the realism? Can this truly be considered a rendering of an animal? How lazy.\"I personally love pixel/dot-matrix typefaces. I hadn't known about this project, so I'm grateful that somebody went to the trouble of doing this.replyryanf 11 hours ago | parent | prev | next [\u2013]What is the point of this reply? This is a set of font files, not an oral history project. The creator is not claiming that this is evidence of their craft or taste.replyzgluck 10 hours ago | root | parent | next [\u2013]Makes me think of...In my experience, there are primarily two kinds of graphical designers. (I don't have direct experience working with font designers, but I imagine they are kind of similar.)1. Those who are great at it. They also tend to be humble and understand that often excruciating and boring iteration is critical in producing something great. When you find someone like this, hold on to them, whatever the cost.2. Those who really want to be great at it but kinda aren't. They tend to develop a smug asshole attitude, perhaps as a defense mechanism. They often think their first iteration is a masterpiece that cannot be improved upon.And there's a much larger superset/variant of the last kind that essentially consists of smug assholes who define themselves as being able to appreciate \"good design\", which invariably is defined as Apple's style of design.replysignalToNose 16 hours ago | prev | next [\u2013]The Madonna video Ray of light is filmed in one of the few stations that still have the old fontreplyFindecanor 15 hours ago | parent | next [\u2013]I think those are all of Stockholm's subway, station H\u00f6torget and the Central Station. The director Jonas \u00c5kerlund likes to sneak in images of his home town. He also used a clip from a game show on Swedish TV without permission.replylying4fun 18 hours ago | prev | next [\u2013]This is greatreplysrboyd 18 hours ago | prev | next [\u2013]Great work!replyIIAOPSW 17 hours ago | prev [\u2013]How is this different from any dot matrix typeface of that time period? I doubt these fonts were made just for London Underground.replySymbiote 17 hours ago | parent [\u2013]I wouldn't be surprised if the font is unique to the London Underground.At least the first display's font is fairly distinct, I haven't noticed it elsewhere. The bottom of the capital letters is a pixel lower than the lowercase letters.Printed signs from Transport for London use Johnston Sans, noticeable with the diamond-shape dots on \"i\" and \"j\": https://en.wikipedia.org/wiki/Johnston_(typeface)replydan1234 9 hours ago | root | parent [\u2013]Looks pretty similar to the font used on the Tyne and Wear Metro[0], though I have heard the metro's signage is made up of hand-me-downs from the tube anyway.[0]https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2F...replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
        "hn_summary": "- A user reminisces about creating their own video game engine in the 90s and manually copying fonts from Lucasfilm games, similar to the London Underground Dot Matrix Typeface.\n- Another user shares their experience recreating the font used by VBZ in Z\u00fcrich, Switzerland.\n- A discussion ensues about the design and functionality of the London Underground dot matrix typeface, with comments about its uniqueness and its impact on readability and visibility."
    },
    {
        "id": 36369553,
        "timestamp": 1687004002,
        "title": "Update: U+237C \u237c &Angzarr;",
        "url": "https://ionathan.ch/2023/06/06/angarr.html",
        "hn_url": "http://news.ycombinator.com/item?id=36369553",
        "content": "Jonathan ChanPhD CS student @ UPenn[he / they] \u2981 #WaterDrinkerHomeBlogCV\u00a9 2023. MIT Licence.update: U+237C \u237c &angzarr;06 Jun 2023This post is a continuation of the investigation into U+237C \u237c RIGHT ANGLE WITH DOWNWARDS ZIGZAG ARROW.Many thanks to Barbara Beeton, James David Mason, Anders Berglund, David Bolton, and the Rare Books staff at the Cambridge University Library.Where were we?I\u2019ve summarized a chronological timeline in the previous post, but here are the highlights in reverse chronological, corresponding roughly to the order in which I\u2019ve discovered the information:ISO/IEC JTC1/SC2/WG2 N2191 (Proposal for Encoding Additional Mathematical Symbols) adds U+237C \u237c to the Unicode standard, which took characters fromThe STIX project, whose character tables were compiled by Barbara Beeton, taking characters from, among many other sources,ISO/IEC TR 9573-13 (Public entity sets for mathematics and science), a technical report for SGML, where the trail ends.Further investigation into various glyph registries and entity tables yielded no additional information.About a year later, I went over everything I knew again and started looking for new leads. The rest of this post collects together the live Twitter updates I had been posting during this process.Who wrote TR 9573-13?The ISO standards site tells us that TR 9573-13 was the responsibility of subcommittee 34 (SC 34) under Joint Technical Committee 1 (JTC 1) of ISO/IEC. A fortuitous search led to a historical account of JTC 1/SC34, originally compiled by James David Mason, who was vice-chairman of SC 34. Given the age of the document, I doubted the email address listed for him was up to date, but eventually I found him on Linkedin, which indicated that he\u2019s a co-chair for the Balisage Conference. I contacted the conference chair, who put me in contact with Mason.From the historical account and Mason himself, I\u2019ve found that working group 1 (WG 1) of SC 34 was responsible for ISO 8879, the SGML standard, as well as TR 9573-13. The main people working on these were Charles Goldfarb, the inventor of SGML, and Anders Berglund, who was responsible for TR 9573-13. The entire SC 34 committee records are now at the Charles Babbage Institute Archives at the University of Minnesota, and consists of 9.5 cubic feet of material in 10 boxes (!). Luckily, I wouldn\u2019t have to fly to Minneapolis to sift through all of these records, because eventually Mason managed to find me a current email address for Berglund.Where is TR 9573-13 from?Berglund tells me that the entity sets for TR 9573-13 come from three sources:ISO/IEC 8859, a precursor of ISO/IEC 10646 and Unicode;MathSci, an expansion of mathfile, Appendix D from the AMS; andvarious typeface catalogues, notably Monotype.Our glyph comes from Monotype under the matrix serial number S16139.Unfortunately (but reasonably, as all of this is from three decades ago), Berglund doesn\u2019t have any notes on which Monotype catalogues were referenced. However, I\u2019ve separately confirmed that the symbol is indeed from Monotype from their archives. Although the Type Archive, which held the Monotype Collection, is now shutting down, the Science Museum Group has taken photographs of the collection. There are over 5000 punches and matrices in the collection, but I was extremely lucky with my search keywords and happened upon a set of punches, Extraneous sorts (L231)\u2026\u2026 which contains that very sort.Which Monotype catalogue is it in?The SMG holds one catalogue, the Specimen Book of \u2018Monotype\u2019 Printing Type. Its index does list L231 as an \u201cExtraneous sorts\u201d series, but those specimen sheets aren\u2019t included in this book.While looking for other catalogues that Monotype have published, I came across Alembic Press\u2019 collection of Monotype publications. I contacted David Bolton at the press for help with the catalogues, who got back to me with a list of publications of lists of signs that do not contain S16139. Since the serial number begins with S, it should be listed as a mathematical special sign, but it was not found in any of:Monotype Special Sorts (1931, 1947) \u2014 up to S1153, S6844;Monotype Special Signs (1954 \u2013 1963) \u2014 up to S11819;Monotype Mathematical Sorts List (1956) \u2014 up to S10477;4-Line Mathematics Classified List of Characters (1967, 1970) \u2014 up to S19717, S20620.Although the serial numbers in 4-Line Mathematics do go past S16139, it excludes several ranges such as S16137 \u2013 S16237 and S18325 \u2013 S18347, likely characters not involved in 4-line mathematical typesetting or were specialized commissioned characters.Many signs were for individual customers, so might not merit being published in a list, although for example I happen to have signs S2120 to S2125, which were only for Jesus College Cambridge Boat Club as far as I know, but which do feature in the 1947 list.Alembic Press lists, but does not possess, one final document, List of Mathematical Characters. However, it can be found in the Morison Collection at the Cambridge University Library. According to their catalogue, they have three documents under this name:[1970.11.585] List of mathematical characters. London, 1970. 72p; ring bdg. [For Monotype and Monophoto.][Morison.MC.D25] List of mathematical characters. [London], nd. ca50 leaves[1972.12.177] List of mathematical characters. np, 1972. 21 loose sheets. [Sheets for insertion in List (1970) .]I contacted the Rare Books department, who have found two documents with this name. The first is List of mathematical characters: \u2018Monotype\u2019 4-line Mathematics Series 569, \u2018Monophoto\u2019 Times Mathematics Series 569B (1970). The second is L231 and L231B (July 1972), a set of 21 sheets meant to be inserted at the end of the List.Since S16139 was found in the set of punches of Extraneous Sorts in series L231, I believe it may appear within these 21 sheets. Unfortunately, as neither faculty nor a student at the University of Cambridge, according to the quote they\u2019ve given me, requesting a digital copy of this document would cost 174\u00a3 (29 scans \u00d7 6\u00a3 each). I\u2019ve attempted to request it as an interlibrary loan, but as archival material it can\u2019t be requested this way. Furthermore, the Rare Books department tells me that \u201cunfortunately none of [the materials] seem to mention S16137 through S16237\u201d. It\u2019s possible the glyph is listed without its serial number, but it\u2019s equally possible that this document skips that range altogether, just as 4-Line Mathematics had.What now? Unicode",
        "summary": "- This post is an investigation into the symbol U+237C \u237c RIGHT ANGLE WITH DOWNWARDS ZIGZAG ARROW.\n- The symbol was added to the Unicode standard by ISO/IEC JTC1/SC2/WG2 N2191.\n- The investigation includes information about the sources of the symbol, such as the STIX project, ISO/IEC TR 9573-13, Monotype catalogues, and more.\n- The author contacted historical experts and managed to find out more about the entity sets and the Monotype catalogues.\n- The specific Monotype catalogue that the symbol appears in is still unknown.\n- There are ongoing efforts to access digital copies of the relevant documents.\n- This investigation sheds light on the origins and sources of this symbol in the Unicode standard.",
        "hn_title": "Update: U+237C \u237c &Angzarr;",
        "original_title": "Update: U+237C \u237c &Angzarr;",
        "score": 382,
        "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginUpdate: U+237C \u237c &Angzarr; (ionathan.ch)382 points by g0xA52A2A 16 hours ago | hide | past | favorite | 78 commentstannhaeuser 15 hours ago | next [\u2013]In case you didn't already heard from others, there's the http://xml.coverpages.org site hosting lots of pre-2000 material related to ISO 8879 (SGML) and XML. Although I didn't find too much on a quick ad-hoc search for ISO 9573, there's mention of angzarr in a preview version of ISO 9573 at http://xml.coverpages.org/ISO-PDTR-9573-13-2004.pdf by Martin Bryant and David Carlisle.There's also casual mention of ISO 9573 on historical comp.text.sgml Usenet archives.David and other people involved with SGML, MathML, and early entity sets for math (and chemical etc.) symbols are hanging around on the xml-dev mailing list (https://www.xml.org/xml-dev) and perhaps can tell more about the origin of that character (which looks more like a symbol for military or electrotechnical use to my totally uneducated eye).Also, there's a typo in your post: Belisage Conference -> Balisage Conference ;)Good luck.replyionathan 14 hours ago | parent | next [\u2013]Whoops, thanks for catching that typo!replynocoiner 11 hours ago | root | parent | next [\u2013]What\u2019s the potential copyright issue with the request to Cambridge?BTW, terrific detective work. I love mysteries like these.replyionathan 11 hours ago | root | parent | next [\u2013]When I tried to request it via ILL, they told me that the amount of material scanned \"exceeds copyright law and scanning limits\". I haven't bothered to look up whatever law that is, and I'm not sure if it's a US thing, or if it's on the UK side, and if so, whether students/faculty at Cambridge are under the same restrictions and they'd have to end up paying the same fees as well. I have a friend whose advisor works there, but I'm reluctant to ask them for the favour and potentially drag them into numerous back-and-forth emails with Cambridge Library and copyright issues...replymoontear 10 hours ago | root | parent | next [\u2013]Just on copyright - all you want is to take a peek? It is not that you would have to share the complete scans with the world.Let's say the character means \"X\" and you can see it on some obscure page - could sharing that be a copyright issue?replyionathan 10 hours ago | root | parent | next [\u2013]Here's what the Cambridge Library says [1] about scans:> Scans are provided with certain conditions of supply:> 1. Not pass on, or upload, the electronic copy or make it available to any other person> 2. Not make further printed or electronic copies:shrug:[1] https://www.lib.cam.ac.uk/search-and-find/zero-contact-servi...replynocoiner 5 hours ago | root | parent | next [\u2013]> When you request a scan, you will be asked to confirm that you acknowledge a copyright declaration which states that you have not already been supplied with a copy of the same material, that you will use the copy only for non-commercial research or private study, nor supply the copy to another person. Copyright forms will be retained by the University Library in perpetuity. (emphasis added)Wow! Pretty bold promise from a university that\u2019s already been around for 1000 years or whatever. I feel like they\u2019re really staking their credibility on indefinite document retention here lol.replydisillusioned 1 hour ago | root | parent | next [\u2013]My brother completed his Master's at Queen's College Cambridge and the porters there offer indefinite storage for all active students and alum. He stored his road bicycle in their storage there for something like 8 years before finally getting back out there and arranging to have it shipped. They're pretty big on retention, is my point.replygrose 14 hours ago | prev | next [\u2013]In a similar vein, there are kanji (Chinese characters) with unknown origins called \"ghost kanji\". https://en.wikipedia.org/wiki/JIS_X_0208#Kanji_from_unknown_...replyeterevsky 11 hours ago | parent | next [\u2013]I like how the kanji in the table are classified into 3 categories: Unknown, Source unclear and Unidentifiable.replypeterfirefly 7 hours ago | root | parent | next [\u2013]and those that belong to the emperor, I presume.replysmsm42 5 hours ago | root | parent | next [\u2013]Except those that are drawn with a fine camelhair brush.replycontingencies 9 hours ago | parent | prev | next [\u2013]There are also character variants. Sometimes between CJK, but also historic. I attended a conference at Academica Sinica in Taipei with knowledgeable academic sorts circa 2001 who had apparently elucidated various issues with Unicode unification coming from the full range of prior encodings, fonts, dictionaries, input systems and mechanical typesetting systems.replydboreham 14 hours ago | parent | prev | next [\u2013]Since I just returned home to the US from a visit to Japan, I found that fascinating reading.replypavlov 16 hours ago | prev | next [\u2013]> \u201cAlthough the Type Archive, which held the Monotype Collection, is now shutting down\u2026\u201dBoo. Can\u2019t someone like Adobe fund a historical archive like this. Photographs are not a replacement for the physical history of this vanished trade.replyQuarrel 16 hours ago | parent | next [\u2013]I (through my own ignorance?) haven't had much appreciation for this bit of history, but I recently visited the fascinating Museum Plantin-Moretus in Antwerp.https://museumplantinmoretus.be/ https://en.wikipedia.org/wiki/Plantin-Moretus_MuseumThey were a publisher and printing house in Antwerp, starting in the early waves of printing presses that swept Europe after Gutenberg.Amazingly, it stayed in the family and the family obviously had an incredible devotion to their origins, they have their original presses (thought to be the oldest in the world), their original type (their founder was a big believer in the power of good type and bought up the rights where he could), the original building, their original library. It is quite the adventure (in a totally nerdish but culturally significant way!).It was eventually sold to the city where it has been a museum ever since.Back to the topic at hand, I agree with you, can't someone acquire this??! :)replyjavajosh 14 hours ago | root | parent | next [\u2013]I've often thought that the best Civilization would actively maintain living examples of each historical milieu. A stone age place and a middle ages place, a mid century place, and so on. In this way the methods and knowledge of the past would not be lost, and in the event of a calamity (like a Carrington event, or nuclear war), it would accelerate our recovery. Presumably the highest tech'd civ would impose order on the rest to prevent the stronger civs attacking the weaker ones (only the strongest civ could possibly enforce this).(The prospect of having to recapitulate the advances of the last 200 years fills me with indescribably weariness. Physical typesetting being a good example. Who is foolish enough to think you can \"just read a book about it\" and get a working press going?)replyConscat 22 minutes ago | root | parent | next [\u2013]Yo this sounds like some kind of cheap YA novel.replywombatpm 2 hours ago | root | parent | prev | next [\u2013]Isn\u2019t that somewhat satisfied by having groups like the Amish, and Renaissance festivals ?replytheK 10 hours ago | root | parent | prev | next [\u2013]Interesting thought experiment. I'd wager there are equally interesting ethics challenges that would need addressing in order to actually do something like this well.replyrunlaszlorun 13 hours ago | root | parent | prev | next [\u2013]That\u2019s a great idea. A lot of things make a lot more sense when you can actually see the context they came from.replystuaxo 11 hours ago | root | parent | prev | next [\u2013]Indeed, we don't exactly treat our hunter-gatherers well.replytimthorn 8 hours ago | parent | prev | next [\u2013]My understanding is that the archive isn't being disposed of, but will be going into the Science Museum long term storage. The photographs are not intended as a replacement for the collections.replythrdbndndn 15 hours ago | parent | prev | next [\u2013]I think it's something the government should step in, not a private company.replymihaic 13 hours ago | root | parent | next [\u2013]After decades of corporate propaganda, the mainstream view is that \"goverment can't do anything\".This has led to people expecting the rich to donate for this sort of outcome, instead of demanding better organization from the government that's eating away almost half their income.Rant asside, you're totally right.replychongli 15 hours ago | root | parent | prev | next [\u2013]Adobe could easily make a one-time donation of $millions to set up an endowment which would keep them running for the foreseeable future. The government could as well, I just see it as less likely. The government seems much more likely to maintain an active control over something like this, opening up the possibility of political interference in the future.replykergonath 15 hours ago | root | parent | next [\u2013]A private company is more likely to use it for propaganda and marketing purposes. At least here government agencies have competent historians.replydec0dedab0de 16 hours ago | prev | next [\u2013]Unfortunately, as neither faculty nor a student at the University of Cambridge, according to the quote they\u2019ve given me, requesting a digital copy of this document would cost 174\u00a3Maybe just do a go fund me or something to raise the 174\u00a3? That is, if no students or faculty from the university of Cambridge see this and help.replyfoobarbecue 16 hours ago | parent | next [\u2013]Cambridge alum here (for my BA in 2009) but I'm in CA now. Would be willing to try putting in the request. Not sure how to contact Jonathan Chan... I'm not on any of the social media he lists in his site footer... Anyone see an email for him? Edit: nevermind, found it. Emailing himreplyfoobarbecue 10 hours ago | root | parent | next [\u2013]Heard back. Turns out it has to be a current student, unfortunately. I'm sure he'll find somebody.replyaleph_minus_one 13 hours ago | root | parent | prev | next [\u2013]> Cambridge alum here (for my BA in 2009) but I'm in CA now. Would be willing to try putting in the request. Not sure how to contact Jonathan Chan...Look at https://ionathan.ch/cv.htmlreplypja 16 hours ago | parent | prev | next [\u2013]Just post on r/cambridge and/or r/cambridge_uni reddit & ask if a current or ex-student or faculty member would be willing to request it from the stacks & make a copy.There\u2019s bound to be someone who\u2019ll drop in a request on their behalf.replycxr 6 hours ago | root | parent | next [\u2013]<https://old.reddit.com/r/Scholar/> is what you want.replyfoobarbecue 16 hours ago | root | parent | prev | next [\u2013]reddit is deaditreply2h 15 hours ago | root | parent | next [\u2013]No, it's nothttps://reddit.com/r/cambridgereplyfoobarbecue 10 hours ago | root | parent | next [\u2013]Yeah. I was being facetious. I just meant that many of us are avoiding it right now.replymasklinn 16 hours ago | parent | prev | next [\u2013]Finding a Cambridge student or faculty willing to help doesn't seem like it'd be super hard, the university has 6000 academic staff and 25000 students.Even more so if alumni still have those accesses.replyjustincormack 13 hours ago | root | parent | next [\u2013]Alumni do have access, so yeas lots more!replyDenvercoder9 15 hours ago | parent | prev | next [\u2013]> That is, if no students or faculty from the university of Cambridge see this and help.The author has said on Twitter that he already knows someone at Cambridge he could ask: https://twitter.com/ionathanch/status/1663423421831602178replytux3 16 hours ago | parent | prev | next [\u2013]I was going to say the same. HN should make quick work of that, and even if it leads nowhere, the investigation is fascinating!replythrdbndndn 15 hours ago | prev | next [\u2013]Is the article cut short?I thought there should be some content under heading \"What now?\".Very fascinating by the way, I remembered the original post.replyamannm 12 hours ago | prev | next [\u2013]Also on the edge of my seat here, wondering what field it could be from. My ChatGPT-esque BS story is that this symbol was misplaced alongside more abstract math-y symbols and was actually briefly used in schematics to identify \"lightning conductor\" components shown here https://electrical-engineering-portal.com/wp-content/uploads... ... plausible, yes?replycontingencies 9 hours ago | parent | next [\u2013]Best theory yet.replynocoiner 5 hours ago | root | parent | next [\u2013]It\u2019s a good theory, but shouldn\u2019t it show up regularly in electrical schematics then? It doesn\u2019t sound like anyone in any particular fields (other than possibly German mathematics or Dutch economics) has been able to point to historical common usage.replytekknolagi 14 hours ago | prev | next [\u2013]OP, if you are reading this, please contact me (email on website in bio). I would like to find a way to help fund the digital request to continue this research.replyformerly_proven 14 hours ago | parent | next [\u2013]An email address is here: https://ionathan.ch/cv.htmlreplyionathan 10 hours ago | prev | next [\u2013]I've added a clarification to the end of the post on whether angzarr might be found in the Cambridge Library document, which I mentioned in my twitter thread but not in the post:> Furthermore, the Rare Books department tells me that \u201cunfortunately none of [the materials] seem to mention S16137 through S16237\u201d. It\u2019s possible the glyph is listed without its serial number, but it\u2019s equally possible that this document skips that range altogether, just as 4-Line Mathematics had.I'd also like to point out that Cambridge alumni are unlikely going to be able to request scans for free; I think you need to be a current faculty or student.replyklik99 15 hours ago | prev | next [\u2013]I remember the previous post and find it weirdly compelling - the cruft and leftovers as technology evolves is interesting - it's like the appendix of monotype. I'm looking forward to the movie adaptation where he drives himself completely crazy trying to find out what the symbol means. I appreciate and can relate to this need to dig into minutiae.replyetothepii 12 hours ago | parent | next [\u2013]In order to make a Hollywood film it would need to turn out this was a message from The Creator.replyklik99 12 hours ago | root | parent | next [\u2013]Pi 2: Right Angle with Downwards Zigzag Arrowreplyjwilk 15 hours ago | prev | next [\u2013]The previous post discussed on HN in 2022: https://news.ycombinator.com/item?id=31012865 (295 comments)replylabster 1 hour ago | prev | next [\u2013]I\u2019m hoping that when we finally get to the bottom of this mystery, we\u2019ll find out that Aleister Crowley had it slipped into Monotype for magickal reasons.replypushedx 11 hours ago | prev | next [\u2013]I wonder if this is some sort of \u201csignature character\u201d, that the designer would use to discover if their work had been lifted, possibly dating back centuries.replyrichbell 4 hours ago | parent | next [\u2013]Like https://en.wikipedia.org/wiki/Trap_street?replyezequiel-garzon 14 hours ago | prev | next [\u2013]Sort of related, could anyone please explain why there is a &comma; named character reference in the HTML standard?https://html.spec.whatwg.org/multipage/named-characters.html...replywhoopdedo 11 hours ago | parent | next [\u2013]The now deprecated FONT FACE attribute was defined as a comma-separated list of names. The entity was needed if you had a font name with a comma in it.Another comma-separated list is in the TH|TD AXIS attribute which is considered obsolete now. I found two other CSL attributes in APPLET ARCHIVE (depr.) and AREA COORDS but neither of them need a comma entity.So the comma entity exists only as a historical artifact.replyjwilk 6 hours ago | root | parent | next [\u2013]Couldn't you use &#44; instead?replybruce343434 14 hours ago | parent | prev | next [\u2013]Perhaps for usage as an escaped form of `,` in comma separated value tables? Although good question why it's in the HTML spec, pasting raw csv inside of an element and then needing to read it back seems like a rare use case.replytoast0 14 hours ago | parent | prev | next [\u2013]Why not? There's lots of named characters in the range of 0x20-0x2F, and symbols in general.replyezequiel-garzon 14 hours ago | root | parent | next [\u2013]Those symbols (including comma) were added in later editions of the standard, and I'm sure there's a reason, but it seems to me if your keyboard has the characters & and ; it will also have , no? I mean, why not add &a; for a then?replyjwilk 6 hours ago | root | parent | next [\u2013]There's also \"&semi;\" standing for \";\", which makes even less sense to me.replyIzkata 3 hours ago | root | parent | next [\u2013]To escape both special characters if you wanted to display \"&semi;\" to the user?replyngvrnd 8 hours ago | prev | next [\u2013]Can we get the Guided By Voices logo added to unicode? http://photos1.blogger.com/blogger/7087/1132/1600/rune.1.jpgreplySmaug123 15 hours ago | prev | next [\u2013]I've asked a friend who is sort of kind of a faculty member; they may or may not be able to get access (they have a rather bespoke institutional status), so please other people keep trying!replyaardvark179 10 hours ago | prev | next [\u2013]I\u2019m sure somebody on here can help have a look. If you put in a scan and deliver request then apparently you aren\u2019t meant to share it with anybody else due to copyright, but I know somebody who could request it and I\u2019m sure could find the symbol in there.replyomoikane 12 hours ago | prev | next [\u2013]The \"timeline\" link in the article is broken (links to localhost:4000), correct link should behttps://ionathan.ch/2022/04/09/angzarr.html#summary-timelinereplyionathan 11 hours ago | parent | next [\u2013]lmao silly mistake. I'll get that fixed, thanksreplyRugnirViking 10 hours ago | prev | next [\u2013]From the previous post a year or so back I thought the mystery was discovered that it was a new age druidic symbol someone had stuck inreplyggm 5 hours ago | prev | next [\u2013]OK. I'll bite. So what did the Cambridge rowing club need specific fonts for?replyg8geufg 5 hours ago | parent | next [\u2013]Possibly typesetting \"bumps charts\" http://www.cucbc.org/charts?year=2010&event=L&day=Fi&sex=Mreplyggm 4 hours ago | root | parent | next [\u2013]Good call.replyyosito 15 hours ago | prev | next [5 more]DonHopkins 9 hours ago | prev | next [\u2013]The Story of Ampersand.https://sharegpt.com/c/J1U3T7mreplyefaref 7 hours ago | parent | next [\u2013]> They conspired with a rogue hashtagChatGPT thinks # is called \"hashtag\"? :(replymigf 14 hours ago | prev [\u2013]To the emoji t-shirt mobile!!!replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
        "hn_summary": "- The website http://xml.coverpages.org hosts pre-2000 material related to ISO 8879 (SGML) and XML.\n- The origin of the character U+237C \u237c &Angzarr; is still uncertain, but it may be related to military or electrotechnical use.\n- Requesting a digital copy of the document from Cambridge University Library may be costly due to copyright restrictions."
    },
    {
        "id": 36373190,
        "timestamp": 1687027565,
        "title": "Bullshit Jobs (2018)",
        "url": "https://theanarchistlibrary.org/library/david-graeber-bullshit-jobs",
        "hn_url": "http://news.ycombinator.com/item?id=36373190",
        "content": "- The phenomenon of \"bullshit jobs\" refers to forms of employment that are considered pointless, unnecessary, or even pernicious by the person doing the job\n- These jobs are often found in the private and public sectors and can range from office work to labor-intensive tasks\n- A defining characteristic of a bullshit job is that the worker feels obliged to pretend that there is a good reason for the job to exist, despite personally believing otherwise\n- The subjective perspective of the employee is important in determining whether a job is considered bullshit\n- Bullshit jobs are not the same as \"shit jobs,\" which are typically low-paid, blue-collar positions that may be physically demanding but are still considered useful or necessary\n- It is possible for a job to be both bullshit and shit, creating a particularly oppressive work situation\n- Common misconceptions include the belief that bullshit jobs are confined to the public sector or that they are limited to government bureaucrats- Useless bureaucrats exist in both the public and private sectors, and it can be difficult to tell them apart.\n- Market reforms often create more bureaucracy, not less.\n- The assumption that government is top-heavy with unnecessary administrative hierarchy while the private sector is lean and mean is a misconception.\n- The Soviet Union's policy of full employment resulted in creating jobs that were unnecessary and led to inefficiency.\n- The pressure to downsize and increase efficiency in corporations has mainly affected low-level employees, while unnecessary managerial and administrative positions have multiplied.\n- The rise of meaningless managerial and administrative jobs has occurred in both public and private sectors, which are increasingly difficult to differentiate.\n- Many administrative jobs held by women are indeed bullshit jobs, but the assumption that mainly women end up in such jobs is sexist and ignorant of how most offices work.\n- Men are more likely to feel that their jobs are pointless compared to women.\n- Hairdressers are not a good example of a bullshit job because their work makes a demonstrable difference and is subjective.\n- There are five major types of bullshit jobs: flunkies (exist to make others look or feel important), goons (exist due to an aggressive element), duct tapers (exist to solve problems that shouldn't exist), box tickers (exist to give the appearance that something is being done), and taskmasters (exist to manage unnecessary work).\n- Duct tapers often correct problems caused by incompetent superiors or fix glitches in the system.\n- Box tickers are employees who allow an organization to claim it is doing something that it is not doing.\n- The mentality of box ticking and the illusion of control increase when government functions are reorganized to be more like a business.\n- Much of local government operations revolve around box-ticking rituals and target figures that have little real impact on providing services.\n- The physical attractiveness of reports and documents often takes precedence over their actual content.- Many executives in corporate world have employees who are solely responsible for creating PowerPoint presentations and accompanying materials\n- Reports and presentations in corporate world are often seen as props and are not actually read by anyone\n- There are industries that exist solely to facilitate box-ticking gestures, such as photocopying and mailing articles from medical journals for lawsuits\n- Some companies have in-house magazines or TV channels that serve no purpose other than to boost executives' egos\n- Taskmasters fall into two subcategories: those who assign work to others and those who create meaningless tasks for others\n- Middle managers often feel that their jobs are unnecessary and spend their time wondering what they are supposed to be doing\n- Some taskmasters create elaborate narratives and titles for jobs that don't actually exist, just to cover up incompetence or lack of desire to fire someone\n- Some jobs that appear pointless on the surface are actually valuable because they support a larger, necessary enterprise\n- Many people in bullshit jobs are unhappy because they feel a sense of purposelessness and falseness in their work\n- Students are often forced into make-work jobs that provide no value or lessons relevant to their future careers\n- Economic theories assume that people are motivated by self-interest, but this assumption doesn't hold in all aspects of life\n- The assumption that people in bullshit jobs should be happy because they're being paid to do nothing is flawed- People generally find being paid to do nothing or pretend to work infuriating and demoralizing.\n- Bullshit jobs are jobs that are largely pointless or have no clear purpose.\n- Even in enjoyable bullshit jobs, the lack of purpose and ambiguity can still be frustrating.\n- Some people find ways to make the best of their bullshit jobs by finding ways to occupy their time or enjoying camaraderie with coworkers.\n- There are often unspoken rules and expectations in bullshit jobs, and workers must navigate them without explicit guidance from supervisors.\n- The lack of meaningful work can lead to anxiety, lack of confidence, and feelings of fraudulence.\n- Knowing that one's job is bullshit can be the most demoralizing aspect.\n- Bullshit jobs often lack clear scripts or cultural models for how workers are supposed to behave or feel.\n- The knowledge that one's work has no impact or purpose can be deeply disillusioning and distressing.- The rise of \"bullshit jobs\" in recent years, which are jobs that people find meaningless and pointless, is a real social problem\n- The overall number of bullshit jobs, and the percentage of jobs considered bullshit by those who hold them, has been increasing rapidly\n- This phenomenon has received little public attention because it defies our assumptions about how market economies are supposed to work\n- The rise of the \"service economy\" has been used to explain changes in the structure of employment, but it is deceptive and does not fully account for the increase in bullshit jobs\n- Many people in bullshit jobs are aware of the pointlessness of their work, which leads to feelings of frustration, depression, and self-doubt\n- Some workers find ways to pursue meaningful projects outside of their bullshit jobs, but it can be difficult to balance these pursuits with the demands of work\n- The effects of bullshit jobs on workers' creativity and imagination can be devastating, leading to a loss of purpose and a sense of self\n- Despite the challenges, some workers are able to find purpose and meaning outside of their bullshit jobs through activism or creative pursuits\n- The reasons for the proliferation of bullshit jobs are complex and multifaceted, and require further exploration.- The rise of seemingly pointless, or \"bullshit,\" jobs is a phenomenon that has been observed in various industries, including manufacturing and the service sector.\n- The concept of a \"service economy\" is often misleading, as the majority of service sector jobs are not focused on serving others directly but rather on information work such as finance, insurance, real estate, and administration.\n- There has been a lot of discussion about the rise of information-oriented jobs since the 1990s, but many of these jobs are considered to be \"bullshit jobs\" by those who hold them, meaning they feel their work has little to no impact on the world.\n- The increase in bullshit jobs can be attributed to factors such as managerial prestige, bureaucratic dynamics, and poor information flow, but the underlying reasons for this phenomenon are complex and interconnected.\n- The financial sector is a prominent example of an industry that is rife with bullshit jobs, where many employees feel that their work is pointless and contributes little value.\n- Government regulations can contribute to the creation and maintenance of unnecessary bureaucratic jobs, but they are not the sole reason for the rise of bullshit jobs in the private sector.\n- The current form of managerialism in the corporate world shares similarities with feudalism, as the concentration of power and the distribution of resources are intertwined with political dynamics.\n- The rise of bullshit jobs can be seen as a result of changing attitudes toward what corporations are responsible for, with a decline in the perception of moral responsibility to workers and an increased focus on profit.\n- The increase in productivity over the years has not resulted in proportional increases in worker compensation, with a significant portion of the benefits going to the wealthiest individuals and the creation of unnecessary managerial positions.- The rise of \"managerial feudalism\" has led to the proliferation of pointless, low-value jobs\n- This new form of feudalism is based on political grounds rather than economic grounds\n- The creation of hierarchies and multiple tiers of management is a key characteristic of this new feudalism\n- The line between retainers and subordinates often becomes blurred within these hierarchies\n- The creative industries, such as books, visual arts, and journalism, have been particularly affected by managerial feudalism\n- The existence of pointless jobs has not been seen as a social problem, despite the negative impact on individuals and society\n- The concept of value is subjective and difficult to measure, but most people agree that a worthwhile job is one that is helpful and provides a positive contribution to society\n- There is an inverse relationship between the social value of work and the amount of money one is likely to be paid for it\n- Studies have shown that high-paying professions, such as those in finance and advertising, tend to have a negative social value, while low-paying professions, such as cleaning and recycling, tend to have a positive social value\n- The current system of valuing work is flawed and there is a need for a reassessment of what is considered valuable in society.- The inverse relationship between social benefit and level of compensation in various professions, such as doctors and nurses.\n- The reasons for this inverse relationship are unclear, but class power and class loyalty might play a role.\n- There is a prevailing belief that those who benefit society should not be paid well, while those who do pointless or harmful work should be rewarded for it.\n- The historical and theological roots of our attitudes towards labor and the idea that work is a value in itself.\n- The shift from the labor theory of value to the idea that capital creates wealth and the subsequent devaluation of work.\n- The belief that work is a form of self-discipline and self-sacrifice, leading to the valorization of jobs that are seen as difficult or unpleasant.\n- The paradox of modern work: people both find meaning and self-worth in their work, yet also dislike their jobs.\n- The importance of work in the formation of human character and the denial of work as a denial of self-definition.- Work is increasingly seen as an end in itself, leading to harmful and degrading jobs.\n- People gain feelings of dignity and self-worth from hating their jobs.\n- The attitude towards work is more prevalent in middle-class office workers.\n- The poor are often vilified and seen as lazy and undeserving.\n- The suffering caused by work is seen as a badge of economic citizenship.\n- The concept of happiness is seen as ignoble and happiness itself is seen as an ugly side effect of work.\n- Bullshit jobs proliferate due to the dominance of managerial feudalism.\n- The political effects of bullshit jobs include resentment and division among different classes.\n- The current crisis over robotization further exacerbates the problem of bullshit jobs.\n- The caring sector is particularly affected by bullshitization.\n- The financialization of banks, universities, and hospitals complicates the problem.\n- Universal Basic Income is a potential solution to the problem of bullshit jobs.\n- Policy suggestions should focus on reducing the size and intrusiveness of government.\n- The problem of bullshit jobs stems from a larger issue of the separation of value and values.\n- A revolt of the caring classes is hindered by divisions and lack of organized movements.\n- The current system of labor allocation is politically driven rather than economically or humanly driven.\n- Universal Basic Income is a policy proposal that could detach work from compensation and address the problem of bullshit jobs.- Leslie and Candi are activists for Universal Basic Income, which calls for replacing means-tested social welfare benefits with a flat fee for everyone in the country.\n- Leslie has personal experience with benefits as a single mother, which informs her advocacy for UBI.\n- Candi became involved in issues surrounding women's work and wages during the 1980s and supports UBI for similar reasons.\n- The Wages for Housework movement sought to challenge capitalism's valuation of work and the distinction between paid and unpaid labor.\n- Basic Income eliminates the need for means-testing and bureaucratic systems, leading to a reduction in bureaucracy.\n- UBI ensures a basic standard of living for all individuals, regardless of their work status, and can help to reduce social inequalities.\n- The concept of unconditional universal support is relevant to discussions on power dynamics in work relationships and the need for freedom and choice in society.\n- Bullshit jobs are those that employees view as pointless, unnecessary, or pernicious, and cannot justify their own existence.\n- Many jobs in the service industry, administration, and management roles can be seen as pointless or lacking in meaning.\n- Compliance-related jobs in the financial sector provide an example of pointless work, as they are aimed at preventing misconduct rather than creating value.\n- The rise of bullshit jobs can be attributed to the expansion of bureaucracy and the overemphasis on qualifications and credentials in job requirements.\n- The trend of bullshit jobs is problematic because it creates dissatisfaction among employees and wastes resources that could be directed elsewhere.\n- The elimination of bullshit jobs would increase productivity and free up resources for more meaningful work in society.\n- The presence of bullshit jobs in society highlights the need for a reevaluation of the purpose and value of work in people's lives.- The author discusses various perspectives on work, labor, and the concept of value.\n- Many people in modern society have jobs that they feel are pointless or unnecessary, often referred to as \"bullshit jobs.\"\n- The rise of bullshit jobs can be attributed to factors such as capitalism, bureaucracy, and a culture of productivity.\n- There is a growing movement advocating for a universal basic income (UBI) as a potential solution to the problem of bullshit jobs.\n- The concept of work and its purpose has evolved throughout history, and societal attitudes towards work have changed over time.\n\nPlease note that the given text does not contain any clear focal point or central argument, making it challenging to provide a concise summary.",
        "summary": "- The phenomenon of \"bullshit jobs\" refers to forms of employment that are considered pointless, unnecessary, or even pernicious by the person doing the job.\n- Bullshit jobs can be found in both the private and public sectors, ranging from office work to labor-intensive tasks.\n- The subjective perspective of the employee is important in determining whether a job is considered bullshit.\n- Bullshit jobs are distinct from \"shit jobs,\" which are low-paid, physically demanding but still considered necessary.\n- Market reforms can result in the creation of more bureaucracy, and the assumption that the private sector is more efficient is flawed.\n- Many administrative jobs held by women are considered to be bullshit jobs, but assuming that mainly women are in these jobs is sexist and ignorant.\n- The rise of bullshit jobs has negative implications for workers' creativity, imagination, and overall well-being.\n- Despite the challenges, some workers are able to find purpose and meaning outside of their bullshit jobs through activism or creative pursuits.\n- The rise of bullshit jobs is a real social problem that has received little public attention, and the reasons for their proliferation are complex and interconnected.\n- The current system of valuing work is flawed, and there is a need to reassess what is considered valuable in society.\n- The rise of bullshit jobs can be seen as a result of changing attitudes towards corporations' responsibility to workers and an increased focus on profit.\n- Bullshit jobs can create feelings of frustration, depression, and self-doubt among workers.\n- Bullshit jobs often lack clear scripts or cultural models, leading to anxiety and a sense of fraudulence.\n- The rise of bullshit jobs is a social issue that goes against assumptions about how market economies should work.\n- The financial sector is one industry where many employees consider their work to be pointless and lacking value.\n- Government regulations can contribute to the creation of unnecessary bureaucratic jobs, but bullshit jobs are not confined to the public sector.\n- The underlying reasons for the increase in bullshit jobs are complex and multifaceted, and further exploration is needed.\n- The rise of bullshit jobs is a phenomenon observed in various industries, and the rise of the service economy does not fully explain this trend.\n- The increase in productivity has not resulted in proportional increases in worker compensation, and many of the benefits go to those in unnecessary managerial positions.\n- The rise of bullshit jobs is a result of managerialism in the corporate world, which shares similarities with feudalism.\n- The concept of social value and compensation is often inverse, with high-paying professions having negative social value and low-paying professions having positive social value.\n- The current system of valuing work is flawed, and there is a need for a reassessment of what is considered valuable in society.\n- The rise of bullshit jobs has negative implications for individuals and society as a whole, but it has not received much public attention.\n- The reasons for the proliferation of bullshit jobs are complex and interconnected, and further exploration is needed to fully understand this phenomenon.\n- The rise of bullshit jobs highlights the need to reevaluate the purpose and value of work in people's lives.",
        "hn_title": "Bullshit Jobs (2018)",
        "original_title": "Bullshit Jobs (2018)",
        "score": 339,
        "hn_content": "- The book \"Bullshit Jobs\" explores the concept of jobs that are perceived as pointless, unnecessary, or pernicious by the employees themselves.\n- It discusses the idea that some jobs lack meaningfulness and offer little satisfaction to the workers.\n- The definition of a \"bullshit job\" is a form of employment that is completely pointless and cannot be justified by the employee.\n- The book argues that the prevalence of such jobs reveals a societal phenomenon that has received little attention.\n- The rise of administrative and management roles in universities is cited as an example of the growth of such jobs.\n- The book challenges the notion that people are the best judge of whether their job is \"bullshit\" and argues that their perception may not align with the organization's perspective.\n- The discussion delves into the concept of alienation from labor and how it affects individuals in the workplace.\n- Different perspectives on the importance and value of \"bullshit jobs\" are presented, including the role of government regulations and compliance requirements.\n- The book explores the idea that some jobs may feel meaningless due to the lack of a big picture understanding or awareness of the job's purpose.\n- It highlights the need to consider the subjective experiences of workers in evaluating the meaningfulness of their jobs.\n\nOverall, the book offers a critical analysis of the concept of \"bullshit jobs\" and how it relates to societal and organizational frameworks.- The author argues that jobs become \"bullshit\" when they are perceived as meaningless or lacking in value.\n- According to the author, giving workers more freedom and ownership over their projects can lead to increased productivity and satisfaction.\n- The book \"Bullshit Jobs\" by David Graeber explores the phenomenon of meaningless work and its impact on individuals and society.\n- The author suggests that societal values have prioritized jobs over humanity, resulting in a proliferation of pointless jobs.\n- The book challenges the notion that jobs are solely motivated by profit and highlights the importance of considering workers' happiness and fulfillment.\n- Critics of the book argue that some jobs may appear meaningless but serve important functions, such as security or maintaining boundaries between organizations.\n- The rise of regulatory burdens and specialization has contributed to the prevalence of bullshit jobs.\n- The article suggests that the concept of bullshit jobs is relevant in the context of potential job displacement by artificial intelligence and automation.\n- It highlights the need for governments to address the issue and optimize labor allocation to prevent the proliferation of pointless jobs.\n- The author emphasizes the importance of critically evaluating job objectives and considering alternative approaches to work.",
        "hn_summary": "- The book \"Bullshit Jobs\" explores the concept of jobs that are seen as pointless, unnecessary, or pernicious by the employees themselves.\n- It challenges the notion that people are the best judge of whether their job is \"bullshit\" and argues that their perception may not align with the organization's perspective.\n- The book highlights the need to consider the subjective experiences of workers in evaluating the meaningfulness of their jobs."
    },
    {
        "id": 36369018,
        "timestamp": 1686999126,
        "title": "I don't need your query language",
        "url": "https://antonz.org/fancy-ql/",
        "hn_url": "http://news.ycombinator.com/item?id=36369018",
        "content": "I Don't Need Your Query LanguageThis post may seem a bit harsh, but I\u2019m tired of the \u201cSQL shaming\u201d that has somehow become a thing in the industry. I have a right to disagree, don\u2019t I?Every year or so, a new general-purpose database engine comes out. And that\u2019s great! It can bring new valuable approaches, architectures, and tools (plus, building database engines is fun).Often this new database engine comes with a new query language. And that\u2019s probably good, too. Or maybe it\u2019s not.Simple and elegantOh, it's elegant and civilized? Sure, I'll bite.What puzzles me is that every time the authors claim that having this brand-new query language is somehow a strength. It\u2019s not. It\u2019s a weakness. Learning a whole new language just to query your database is a burden. I don\u2019t want to do that.We already have a common ground language for general-purpose databases. It\u2019s called SQL. I\u2019d rather use it with your database.\ud83d\udcdd I\u2019m not talking about software that targets a specific narrow domain field. Having a separate domain language for specific use cases makes perfect sense.Sure, your language is elegant. That doesn\u2019t help me. First, it\u2019s still easier to write a slightly longer query in SQL than to learn a new query language. Second, your supposedly simple language will become complex anyways \u2014 as soon as I try to solve real-world tasks with it. So why bother?Better than SQLJust look at that ugly SQL beast.Sometimes the authors of a new query language try to frame SQL as terribly complex. Let\u2019s take an example from one of these \u201cpost-SQL\u201d databases. A comparison that, according to the authors, speaks for itself.\ud83d\udcdd I\u2019m using a particular \u201cpost-SQL\u201d database (without naming it) to illustrate my point here, because its landing page is a vivid example of \u201cSQL shaming\u201d. This is not a criticism of the database or its authors. I\u2019m sure it\u2019s a great product.FancyQL:select Movie { title, actors: {  name },};SQL (as the authors of FancyQL see it):SELECT title, Actors.name AS actor_nameFROM Movie LEFT JOIN Movie_Actors ON Movie.id = Movie_Actors.movie_id LEFT JOIN Person AS Actors ON Movie_Actors.person_id = Person.idSQL (as it may be):select title, actors.namefrom movies join movies_actors using(movie_id) join actors using(actor_id)Hmm. Another example?FancyQL:select Movie { title, actors: {  name }, rating := math::mean(.reviews.score)} filter \"Zendaya\" in .actors.name;SQL (as the authors of FancyQL see it):SELECT title, Actors.name AS actor_name, (SELECT avg(score)  FROM Movie_Reviews  WHERE movie_id = Movie.id) AS ratingFROM Movie LEFT JOIN Movie_Actors ON  Movie.id = Movie_Actors.movie_id LEFT JOIN Person AS Actors ON  Movie_Actors.person_id = Person.idWHERE 'Zendaya' IN (  SELECT Person.name  FROM   Movie_Actors   INNER JOIN Person    ON Movie_Actors.person_id = Person.id  WHERE   Movie_Actors.movie_id = Movie.id)SQL (as it may be):select title, actors.name, (select avg(score) from reviews  where movie_id = movies.movie_id) as ratingfrom movies join movies_actors using(movie_id) join actors using(actor_id)where movie_id in ( select movie_id from actors join movies_actors using(actor_id) where actors.name = 'Zendaya')movies.sqlA bit verbose. But maybe SQL is not that complex after all? Otherwise, why would you paint it scarier than it really is?ModernLet's throw in some suite-shaming!Here is another common argument:SQL was designed with 1970s businessmen in mind, and it shows.True, SQL was designed in the 1970s. But how is that a weakness? Everyone knows SQL. All major database engines support SQL. SQL is expressive enough to solve any data-related task. SQL has a solid standards committee that maintains and improves it. What can your language offer besides being created in the 2020s?I can go on, but I don\u2019t think it\u2019s necessary. My point is simple.I don\u2019t need your fancy query language. I\u2019d stick with SQL.Maybe it\u2019s just me.Follow @ohmypy on Twitter to keep up with new posts \ud83d\ude8017 Jun, 2023 data",
        "summary": "- Every year, new general-purpose database engines are released with new query languages.\n- The author believes that having a separate query language is a weakness because learning a new language is burdensome.\n- SQL is already a common ground language for general-purpose databases and is easier to use than learning a new language.",
        "hn_title": "I don't need your query language",
        "original_title": "I don't need your query language",
        "score": 308,
        "hn_content": "- SQL has a significant drawback when it comes to querying databases with heterogeneous types and differing multiplicity\n- JOINs in SQL can result in redundant output data due to the multiplicity mismatch\n- SQL lacks the ability to error-out early if a JOIN matches an unexpected number of rows\n- Not all client libraries support multiple result-sets\n- JSON support in databases like SQLite can aggregate the result of a subselect into a single column\n- JSON support in databases allows for efficient passing of JSON data without multiple trips to the database\n- JSON queries in Postgres generally have good performance\n- ORMs can sometimes result in inefficient queries due to multiple database trips and fetching unnecessary columns\n- SQL is a useful skill to learn and is a foundational technology in the industry\n- SQL queries can be optimized for performance and can handle large amounts of data efficiently\n- SQL is a long-lasting technology that is widely used and will continue to be relevant in the industry- SQL is a different language that is used to query and modify data, and it is declarative, meaning it focuses on what the response should be rather than how to achieve it.\n- Declarative languages like SQL are not more popular because they can be harder to debug and lack the ability to step through and inspect intermediate representations.\n- SQL is often used as a remote API where the client sends a SQL statement and the server processes it, but it can be dangerous if the statement is expensive or if there are missing indexes.\n- The author wishes for better, faster, and standardized support for a fast procedural language in databases, so that clients can send programs and have more control over accessing tables and indexes.\n- SQL is one of many technologies in a tech stack and requires learning, but there are many SQL dialects to be aware of.\n- It is argued that procedural queries can be faster and more optimized with explicit mention of indexes, but others believe that the complexity of optimizing procedural queries can be a challenge.\n- SQL has been around for a long time and is well understood, making it a valuable and stable tool in the industry.\n- There are various opinions on SQL's advantages and limitations, ranging from it being battle-tested to it being crusty and outdated.\n- The need for a new query language is discussed, with opinions on the benefits and drawbacks of alternatives to SQL.\n- The importance of understanding the business or problem domain when using SQL is highlighted, as a poorly-aligned schema can make SQL less effective.\n- SQL is compared to Unix utilities, with some developers being negative towards SQL but protective of other older tools.\n- Alternatives to SQL are mentioned, such as fancy-ql and EdgeDB's query language, but the ubiquity and maturity of SQL are noted as important factors in its continued popularity.\n- The need for better tooling, such as autocompletion for column names, is mentioned as an improvement that could be made to SQL.\n- The challenges of working with other query languages and the advantages of SQL's ubiquity and tooling are discussed.\n- It is suggested that a new language that is a strict superset of SQL and compiles to SQL might be worth exploring as an alternative.\n- EdgeDB's query language, EdgeQL, is praised as a powerful and preferable alternative to SQL by some users.\n- Personal experiences and opinions with SQL and alternative query languages are shared by various commenters.",
        "hn_summary": "- SQL has limitations when it comes to querying heterogeneous databases and dealing with multiplicity mismatch in JOINs.\n- JSON support in databases like SQLite allows for efficient aggregation of results and passing of JSON data.\n- There are differing opinions on the advantages and limitations of SQL, and the need for a new query language is discussed."
    },
    {
        "id": 36372284,
        "timestamp": 1687022797,
        "title": "GB Studio: Drag & drop retro game creator for GameBoy",
        "url": "https://www.gbstudio.dev/",
        "hn_url": "http://news.ycombinator.com/item?id=36372284",
        "content": "GB StudioEnglishAboutDocsGitHubDownloadctrlKA quick and easy to use drag and drop retro game creator for your favourite handheld video game system.Available on Windows, Mac and Linux.Download on Itch.ioChat with the community on our Discord channel and on Reddit at /r/GBStudioEasy to UseDrag and drop game creator with simple, no progamming knowledge required, visual scripting. Multiple game genres supported.Write MusicInbuilt editor makes writing music easy. With both piano roll and tracker modes.Build ROMsCreate real ROM files and play on any GB emulator. Export for web with great mobile controls, upload to Itch.io and share your game with the world.LearnIntroductionInstallationCommunityDiscordRedditTwitterDownloadsDownloadGitHubPressPress KitGB Studio Copyright \u00a9 2022 Chris Maltby.",
        "summary": "- GB Studio is a drag and drop retro game creator for the GameBoy handheld video game system.\n- It is available for Windows, Mac, and Linux, and you can download it from Itch.io.\n- GB Studio allows you to create games without any programming knowledge, using visual scripting and a simple drag and drop interface.",
        "hn_title": "GB Studio: Drag and drop retro game creator for GameBoy",
        "original_title": "GB Studio: Drag and drop retro game creator for GameBoy",
        "score": 289,
        "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginGB Studio: Drag and drop retro game creator for GameBoy (gbstudio.dev)289 points by CharlesW 11 hours ago | hide | past | favorite | 33 commentswk_end 9 hours ago | next [\u2013]I haven't really looked into it in detail, but AFAICT GB Studio is honestly incredible. I program the Game Boy as a hobby; this machine has basically no RAM and a CPU that's - depending on how you measure a \"cycle\" - either ~1MHz or ~4MHz, with a shockingly limited and non-orthogonal instruction set (and don't even get me started on dealing with bank switching). Historically games for it have been almost exclusively coded in assembly. And yet someone's come along and built a seemingly fairly flexible WYSIWYG game engine for it? If it didn't exist I would've told you it was impossible. I still don't quite believe it's possible, despite the proof being right in front of me.replychrismaltby 8 hours ago | parent | next [\u2013]Thanks, one of the things I\u2019ve always tried to do (at work, before I started on GB Studio) is make difficult things seem easy and accessible. Though I\u2019d have also told you this was probably impossible before starting too!I just built it up slowly, making small tools to help me make sense of what was going on while learning GBDK, before I knew it pretty much had something I could package up. I\u2019ve had a lot of help along the way though, untoxa\u2019s [0] engine rewrite, and the work pau-tomas [1] did on the music editor have been especially helpful to the project.Been a bit absent from the community recently, not had the time, but hoping to get back on it again soon!I\u2019m always amazed at how many awesome games people keep making though, it makes me really happy as this the first of many failed side projects that I pushed on through to completion.[0] https://github.com/untoxa[1] https://github.com/pau-tomasreplymysterydip 4 hours ago | root | parent | next [\u2013]Thanks for the incredible tool!replymattl 3 hours ago | root | parent | prev | next [\u2013]I love GB Studio. I plan to use it to make a movie but will release my \u201cgames\u201d too. Are you active on Mastodon or Bluesky?replylondons_explore 7 hours ago | parent | prev | next [\u2013]The gameboy had extensible RAM and flash - ie. a game cartridge could provide more of both if it wanted to.I assume GB studio uses this to make it easier to make decent gameboy games without manually allocating every byte of ram in handwritten assembly.replyihappentobe 10 hours ago | prev | next [\u2013]GB Studio is a great way to mess around with making a gameboy game! It exports a rom that can be flashed onto a blank cart, and played on real hardware\u2014with recent versions adding gameboy color support. The recently hyped McDonald\u2019s Grimace game was reportedly made with GB studio: https://gbstudiocentral.com/news/mcdonalds-celebrates-grimac...replyexhilaration 4 hours ago | parent | next [\u2013]Is there a recommended blank cartridge and flashing tool? I'm totally new to this.Edit: to turn my own question, here's an article that goes over all the options: https://gbstudiocentral.com/tips/getting-your-gb-studio-game...replyesotericsean 7 hours ago | prev | next [\u2013]I've been making a game using GBDK (C) and a little engine called ZGB for the past several years, but the work that Chris and the team have done on GB Studio makes me want to switch to the platform. I'll still finish my game using C because I'm so far into development now there's no point in starting over, but if I was starting fresh, GB Studio would be the way to go.That said, I'm glad I made my game in C because I've learned so much more about coding. I feel like I get better every day. Not sure I would have that same experience using GB Studio.One day I want to try using ASM...replypradn 5 hours ago | prev | next [\u2013]It's amazing that this software exists. And any game you can make with it can be played on a range of new emulator hardware.There's the Pocket Analogue, which is the high-end option that runs games directly from cartridges using two FPGAs, no emulation at all. https://www.analogue.co/pocketAnd there's the cheap Miyoo Mini and the Anbernic handle-helds that let you run everything up to PS1 games on a tiny handheld. They have SP-style (vertical) and PSP-style (horizontal) versions.It would be so cool to make a game and flash it onto one of these to show to your friends.replythristian 2 hours ago | parent | next [\u2013]A nitpick: FPGAs aren't based on the hardware-descriptions used to create the original chips, they're based on the community's best guesses, so they're still emulation. They're just not software emulation. They might even be less accurate than software emulators, if the community has done more research since the last time the FPGA was updated.replyralusek 5 hours ago | parent | prev | next [\u2013]Surely making a browser game would have the same benefits with fewer limitations?replycolecut 1 hour ago | root | parent | next [\u2013]Are there tools for making browser based games that make it as easy as this does?Also it looks like the ROMs that GB Studio generates can be run in an emulator on the web.replymattl 3 hours ago | root | parent | prev | next [\u2013]Most people don\u2019t have good controllers for the devices running their browser.replykixiQu 8 hours ago | prev | next [\u2013]Just in case anyone doesn't read deep enough to run into it:> You can generate ROM files that can be run in an emulator, on a web page or on real Game Boy hardware.Being able to play these on the web is very, very cool.replyWaterluvian 8 hours ago | prev | next [\u2013]WOW. What\u2019s the compile pipeline to get to valid ROMs?!I need to dig deep when I get home. This is frankly astounding.replyqbasic_forever 8 hours ago | parent | next [\u2013]If you want to program the Gameboy or Gameboy color with C the SDCC compiler toolchain is typically used: https://sdcc.sourceforge.net/ There's a ton of good info and dev tools here: https://github.com/gbdk-2020/gbdk-2020replydjxfade 8 hours ago | parent | prev | next [\u2013]I used to contribute to the project back in v1 days. I'm not sure if the architecture has changed since then, but basically it worked by compiling the game logic as a kind of byte code that got interpreted at run time.replyWaterluvian 7 hours ago | root | parent | next [\u2013]Like on top of the CPU\u2019s opcodes? I imagine you mean the opcodes. That CPU probably can\u2019t handle much abstraction. What impresses me the most is that to get performance out of these games, they\u2019re often very carefully coded in the Sharp/Z80/8080 assembly. This compiler must be quite something.replyehaliewicz2 7 hours ago | root | parent | next [\u2013]It actually does interpret bytecode, you can see the opcode definitions here https://github.com/chrismaltby/gb-studio/blob/1f995a976bd3aa...The trick is that a lot of the heavier stuff is implemented in assembly and this is mostly used for scripting (from what I understand).replydjxfade 6 hours ago | root | parent | next [\u2013]Yes exactly. It's not a zero cost abstraction, but it's a very simple format, each byte code oppose maps directly to function and passes its args. So it's basically just a conditional jumpreplyWaterluvian 6 hours ago | root | parent | next [\u2013]Oh wow. So surprised this is viable on a 4 (1, really) MHz processor. Thanks for clarifying and sharing, you both!replykmill 46 minutes ago | root | parent | next [\u2013]You might also like the fact that the Apollo Guidance Computer (which ran at a similar speed) was also programmed using something like bytecodes. It didn't have to drive a graphical display though, only a spaceship.replyWaterluvian 5 hours ago | prev | next [\u2013]Are there any projects/tools for ripping existing game boy sounds/music and importing them into this format? It would be a great way to learn how various effects are produced, remixing them, etc.It seems do-able: essentially an emulator whose APU implementation records a time series of the data/config passed to each of the four channels.replynavanchauhan 10 hours ago | prev | next [\u2013]Previous:https://news.ycombinator.com/item?id=19717558 (4 years ago)https://news.ycombinator.com/item?id=26979879 (2 years ago)replyjhbadger 6 hours ago | parent | next [\u2013]Yeah, I assume this is posted again because of the buzz over the Gameboy \u00c7olor game that was created using it that McDonald's released in a recent promotionhttps://news.ycombinator.com/item?id=36311378replyslowhadoken 8 hours ago | prev | next [\u2013]Oh cool GB Studio is still going. Good for them.replygymbeaux 10 hours ago | prev | next [\u2013]I\u2019m hoping somebody ports Pok\u00e9mon Colosseum to GBA/GBCreplyCyberDildonics 9 hours ago | parent | next [\u2013]What does this have to do with GB Studio? Was Pokemon Colosseum made with it?replywhateveracct 9 hours ago | root | parent | next [\u2013]I think it's more that the game would be more fun as a 2D pok\u00e9mon instead of the 3D it was.It's a niche request by GP for sure!replygymbeaux 6 hours ago | root | parent | next [\u2013]Thanks homie. Not everyone gets it. Pokemon Colosseum is the Deep Space Nine of the Pokemon games.replyNegativeLatency 5 hours ago | root | parent | next [\u2013]Hm I haven\u2019t played it but I did really enjoy DS9. Is it similar because the colosseum devs had more room to experiment, or grittier or something?replywhateveracct 5 hours ago | root | parent | next [\u2013]I would say both? It's gritty by Pok\u00e9mon standards (it's no Pok\u00e9mon Reborn, which blows canon Pok\u00e9mon out of the water). But gameplay-wise:- Doubles-only- No capturing wild pok\u00e9mon. You steal shadow pok\u00e9mon and heal them.- I found it generally more difficult than normal pok\u00e9mon games.I'm sure GP has more to say :)replyzgluck 6 hours ago | prev [2 more]Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
        "hn_summary": "- GB Studio is a game engine that allows users to create retro games for the GameBoy\n- Historically, GameBoy games were coded in assembly, but GB Studio offers a more flexible and user-friendly approach\n- The software exports ROM files that can be run on emulators, web pages, or real GameBoy hardware"
    },
    {
        "id": 36367147,
        "timestamp": 1686974508,
        "title": "People can be convinced they committed a crime that never happened (2015)",
        "url": "https://www.psychologicalscience.org/news/releases/people-can-be-convinced-they-committed-a-crime-they-dont-remember.html",
        "hn_url": "http://news.ycombinator.com/item?id=36367147",
        "content": "People Can Be Convinced They Committed a Crime That Never HappenedJanuary 15, 2015TAGS:EPISODIC MEMORYFALSE MEMORYINTERROGATIONLAWLEGAL SYSTEMMEMORYPSYCHOLOGICAL SCIENCELog in to Save for Later Download PDFEvidence from some wrongful-conviction cases suggests that suspects can be questioned in ways that lead them to falsely believe in and confess to committing crimes they didn\u2019t actually commit. Research provides lab-based evidence for this phenomenon, showing that innocent adult participants can be convinced, over the course of a few hours, that they had perpetrated crimes as serious as assault with a weapon in their teenage years.The research, published in Psychological Science, a journal of the Association for Psychological Science, indicates that the participants came to internalize the stories they were told, providing rich and detailed descriptions of events that never actually took place.\u201cOur findings show that false memories of committing crime with police contact can be surprisingly easy to generate, and can have all the same kinds of complex details as real memories,\u201d says psychological scientist and lead researcher Julia Shaw of the University of Bedfordshire in the UK.\u201cAll participants need to generate a richly detailed false memory is 3 hours in a friendly interview environment, where the interviewer introduces a few wrong details and uses poor memory-retrieval techniques.\u201dShaw and co-author Stephen Porter of the University of British Columbia in Canada obtained permission to contact the primary caregivers of university students participating in the study. The caregivers were asked to fill out a questionnaire about specific events the students might have experienced from ages 11 to 14, providing as much detail as possible. The caregivers were instructed not to discuss the questions with the student.The researchers identified a total of 60 students who had not been involved in any of the crimes designated as false memory targets in the study and who otherwise met the study criteria. These students were brought to the lab for three 40-minute interviews that took place about a week apart.In the first interview, the researcher told the student about two events he or she had experienced as a teen, only one of which actually happened. For some, the false event related to a crime that resulted in contact with the police (assault, assault with a weapon, or theft). For others, the false event was emotional in nature, such as personal injury, attack by a dog, or loss of a huge sum of money.Importantly, the false event stories included some true details about that time in the student\u2019s life, taken from the caregiver questionnaire.Participants were asked to explain what happened in each of the two events. When they had difficulty explaining the false event, the interviewer encouraged them to try anyway, explaining that if they used specific memory strategies they might be able to recall more details.In the second and third interviews, the researchers again asked the students to recall as much as they could about both the true and false event. The students also described certain features of each memory, such as how vivid it was and how confident they were about it.The results were truly surprising.Of the 30 participants who were told they had committed a crime as a teenager, 21 (71%) were classified as having developed a false memory of the crime; of the 20 who were told about an assault of some kind (with or without a weapon), 11 reported elaborate false memory details of their exact dealings with the police.A similar proportion of students (76.67%) formed false memories of the emotional event they were told about.Intriguingly, the criminal false events seemed to be just as believable as the emotional ones. Students tended to provide the same number of details, and reported similar levels of confidence, vividness, and sensory detail for the two types of event.Shaw and Porter speculate that incorporating true details, such as the name of an actual friend, into an account that was supposedly corroborated by the student\u2019s caregiver likely endowed the false event with just enough familiarity that it came to seem plausible.\u201cIn such circumstances, inherently fallible and reconstructive memory processes can quite readily generate false recollections with astonishing realism,\u201d says Shaw. \u201cIn these sessions we had some participants recalling incredibly vivid details and re-enacting crimes they never committed.\u201dThere were, however, some differences between the students\u2019 memories for false events and their memories for true events. For example, they reported more details for true events and they reported more confidence in their descriptions of the true memories.The fact that the students appeared to internalize the false events to the extent that they did highlights the fundamental malleability of memory:\u201cThis research speaks to the distinct possibility that most of us are likely able to generate rich false memories of emotional and criminal events,\u201d says Shaw.The findings have clear implications for criminal interrogation and other aspects of legal procedure, affecting suspects, witnesses, and those involved in both law enforcement and legal counsel. But they may also apply to interviews that take place in various other contexts, including therapeutic or even personal settings.\u201cUnderstanding that these complex false memories exist, and that \u2018normal\u2019 individuals can be led to generate them quite easily, is the first step in preventing them from happening,\u201d says Shaw. \u201cBy empirically demonstrating the harm \u2018bad\u2019 interview techniques \u2013 those which are known to cause false memories \u2013 can cause, we can more readily convince interviewers to avoid them and to use \u2018good\u2019 techniques instead.\u201dInvestigating the specific characteristics of interviewers and interview tactics that contribute to false memories can help improve interviewing procedure and minimize the risk of inducing false memories, the researchers conclude.The researchers were supported by the University of British Columbia through the Lashley and Mary Haggman Memory Research Award and the Social Sciences and Humanities Research Council of Canada.News > Latest Research News > People Can Be Convinced They Committed a Crime That Never Happened",
        "summary": "- Research shows that suspects can be convinced they committed crimes that they didn't actually commit\n- Innocent participants in a study were able to generate false memories of committing crimes after just three hours in an interview environment\n- Incorporating true details into false event stories made them more believable to the participants",
        "hn_title": "People can be convinced they committed a crime that never happened (2015)",
        "original_title": "People can be convinced they committed a crime that never happened (2015)",
        "score": 284,
        "hn_content": "- Researchers have found that people can be convinced they committed a crime that never happened.\n- This phenomenon has implications for wrongful convictions and the reliability of confessions.\n- The Reid technique, a widely used law enforcement method, has been criticized for producing false confessions.\n- The malleability of memory is a significant factor in this phenomenon.\n- It is important for individuals to exercise their right to remain silent and consult with a lawyer if being interrogated by the police.- Researchers conducted a study showing that people can be convinced they committed a crime that never happened.\n- The study involved 60 undergraduate students who were brought into the lab for interviews.\n- The researchers found that false memories can be implanted using certain techniques.\n- This raises concerns about the reliability of eyewitness testimonies in criminal cases.\n- The study highlights the need for evidence and independent verification when evaluating claims.\n- Memory can be unreliable, and people may have false memories without even realizing it.\n- This research has implications for the criminal justice system and the way cases are prosecuted.\n- The study does have limitations, including the small sample size and potential for researcher influence.\n- While the results may not be generalized to all people in all circumstances, they still raise important questions about memory and perception.\n- The findings of this study could contribute to ongoing discussions about the fallibility of human memory and its impact on legal proceedings.\n- It is important to be aware of the potential for false memories and the role they can play in shaping our perceptions and beliefs.",
        "hn_summary": "- Researchers have found that false memories can be implanted, leading people to believe they committed a crime that never happened.\n- The reliability of confessions and eyewitness testimonies in criminal cases is called into question.\n- This study emphasizes the need for evidence and independent verification in evaluating claims and highlights the fallibility of human memory."
    },
    {
        "id": 36368990,
        "timestamp": 1686998676,
        "title": "Ask HN: Why does Apple refuse to add window snapping to macOS?",
        "url": "",
        "hn_url": "http://news.ycombinator.com/item?id=36368990",
        "content": "",
        "summary": "- Users are questioning why Apple has not added a feature called \"window snapping\" to their macOS operating system.\n- Window snapping is a feature that allows users to easily arrange and resize application windows on their computer screen.\n- Some users believe that the lack of window snapping on macOS is a missed opportunity and makes it less convenient to multitask and work efficiently on their Mac computers.",
        "hn_title": "Ask HN: Why does Apple refuse to add window snapping to macOS?",
        "original_title": "Ask HN: Why does Apple refuse to add window snapping to macOS?",
        "score": 272,
        "hn_content": "- Users are shocked that macOS still lacks a built-in window snapping feature\n- Some Apple users are frustrated with the default behavior of the green maximize button, which triggers fullscreen instead of maximizing the window\n- There are hidden features and keyboard shortcuts for window snapping in macOS, but they are non-obvious and not well-known\n- Third-party apps like Magnet and Rectangle provide more advanced window management options for macOS users\n- Window management in macOS is different from other operating systems like Windows and Linux, with a focus on overlapping windows rather than tiling\n- Apple's design philosophy and user interface choices in macOS have both supporters and critics\n- Users have varying preferences when it comes to window management, and some find the default macOS behavior to be inconvenient or inefficient- Some users find the window snapping feature in Windows 11 to be more annoying than useful.\n- Non-programmer users on Windows 10 have accidentally triggered window snapping and found it confusing.\n- The lack of a power-user window snapping feature in macOS has sparked a discussion about how Apple employees manage their own work.\n- Many users install third-party apps to handle windows on their Macs.\n- Users would like Apple to address this issue and include a built-in window management feature.\n- There are several third-party apps available to provide window snapping functionality on macOS, such as Magnet and Rectangle.\n- Some users prefer a tiling window manager like i3 for macOS.\n- There is ongoing debate about the differences between window management in macOS, Windows, and Linux.\n- Some users argue that macOS is designed for full-screen apps and relies heavily on trackpad gestures for window management.\n- Others believe that Apple should provide a built-in solution rather than relying on third-party apps.",
        "hn_summary": "- Users are frustrated with the absence of a built-in window snapping feature in macOS.\n- Third-party apps like Magnet and Rectangle offer more advanced window management options for macOS users.\n- There is a debate about the design philosophy and user interface choices in macOS, with some users wanting a built-in solution instead of relying on third-party apps."
    },
    {
        "id": 36374936,
        "timestamp": 1687038033,
        "title": "The Secret Sauce behind 100K context window in LLMs: all tricks in one place",
        "url": "https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c",
        "hn_url": "http://news.ycombinator.com/item?id=36374936",
        "content": "Top highlightThe Secret Sauce behind 100K context window in LLMs: all tricks in one placeGalina Alperovich\u00b7FollowPublished inGoPenAI\u00b716 min read\u00b7May 159284tldr; techniques to speed up training and inference of LLMs to use large context window up to 100K input tokens during training and inference: ALiBi positional embedding, Sparse Attention, FlashAttention, Multi-Query attention, Conditional computation, and 80GB A100 GPUs.Recently there were several announcements about new Large Language Models (LLMs) that can consume an extremely large context window, such as 65K tokens (MPT-7B-StoryWriter-65k+ by MosaicML) or even 100K tokens (Introducing 100K Context Windows by Antropic). In the Palm-2 technical report, Google doesn\u2019t reveal the context size but mentions that they \u201cincrease the context length of the model significantly.\u201dFor comparison, the current GPT-4 model can work with the context length of 32K input tokens. And most of the open-source LLMs have a context length of 2K tokens.That\u2019s impressive since having such a large context length means the prompt can be literally a size of a book. The Great Gatsby is 72K tokens, 210 pages, and 6 hours of reading at a 1.7 min/page speed. So the model can scan and keep this amount of \u201ccustom\u201d information to process queries!I was trying to wrap my head around how that is technically possible, so in this blog post, I collect scattered pieces of information (this thread was the first clue) and cover the following:Why context length matters and why it can be a game changerWhat are the main limitations in the original Transformer architecture when working with large context lengthsThe computational complexity of the transformer architectureWhat optimization techniques currently exist to speed up the transformer and increase the context length up to 100K\u201cShort\u201d SummaryHere and later, we use the \u201ccontext length,\u201d \u201ccontext window,\u201d and \u201cthe number of input tokens\u201d interchangeably, denoting them as n.The blog post is a bit long, so there is a summary with the main points and tricks:1st problem is the quadratic time and space complexity of attention layer computations w.r.t. the number of input tokens n.When the embedding size d > n, the 2nd problem is the quadratic time complexity of linear layers w.r.t. embedding size d.3rd problem is Positional Sinusoidal Embedding used in the original architecture.In Transformer architecture, the shapes of learnable matrix weights are agnostic to the number of input tokens n.So, a trained Transformer in 2K context lengths can consume tokens of any length, even 100K. But the model will not produce meaningful results on 100K tokens during inference if it isn\u2019t trained on 100K.Training the vanilla Transformer on a giant corpus and only on a large context length is unfeasibly expensive due to the quadratic complexity w.r.t to n and d. LLaMA on 2K context length was estimated to be trained for ~$3M. Thus, LLaMA on 100K would cost ~$150M.One option is to train the model on 2K tokens context and then fine-tune it in longer contexts (for example, 65K). But it won\u2019t work with the original Transformer because of the Positional Sinusoidal Encoding.[Trick #1] To address this, remove Positional Sinusoidal Encoding and use ALiBi, a simple and elegant positional embedding that doesn\u2019t hurt accuracy. Then you can train on 2K and fine-tune on 100K.[Trick #2] You don\u2019t need to calculate attention scores between all tokens. Some tokens are more important than others, so Sparse Attention can be used. It will speed up both training and inference.[Trick #3] Flash Attention efficiently implements the attention layer for GPU. It uses tiling and avoids materialization of big intermediate matrices (n, n) that doesn\u2019t fit into GPU SRAM. It will speed up both training and inference.[Trick #4] Multi-Query attention instead of Multi-Head attention. That means you share weights across all heads when linearly projecting K and V. It dramatically speeds up incremental inference.[Trick #5] Conditional computation avoids applying all model parameters to all tokens from the input sequence. CoLT5 applies heavy computations only to the most important tokens and processes the rest of the tokens with a lighter version of layers. It will speed up both training and inference.[Trick #6] To fit a large context, you need a lot of RAM in GPU, so people use 80GB A100 GPUs.To sum up, the more you speed up the training and inference, the larger the context length you can use.Let\u2019s now discuss all these points in more detail.Why context length mattersContext length is one of the critical limitations of LLMs. And increasing it to already 100K is an incredible achievement (I wonder how this statement will look in a year).One of the important use cases where people want to apply LLMs is \u201cdropping a large pile of custom data into an LLM\u201d (documents related to the company or a particular problem, various heterogeneous texts, etc) and asking questions about this particular data, not some abstract data from the internet that LLM saw during training.To overcome this limitation now, people do various things:Trying summarization techniques and sophisticated chained promptsMaintaining vector databases to keep embeddings for custom documents and then \u201csearching\u201d across them by some similarity metricFine-tuning the LLM with custom data when possible (not all commercial LLMs allow that, and it is not an obvious task for open-source LLMs)Developing custom smaller LLMs for this particular data (again, not an obvious task)Having a large context length allows an already powerful LLM (that saw the whole internet) to look at your context and data and interact with you on a completely different level with a higher personalization. And all these without changing the model's weights and doing your \u201ctraining\u201d on the fly, \u201cin memory.\u201d And overall, a large context window brings more accuracy, fluency, and creativity to the model.One analogy here might be computer RAM, where the operating system keeps the real-time context of all your applications. With a substantial context length, LLM can be like a \u201creasoning computer,\u201d keeping a lot of user context.Original Transformer & context lengthIt\u2019s important to note that in Transformer architecture, the shapes of all learnable matrix weights are not dependent on the number of input tokens n. All trainable parameters (embedding lookup, projection layers, softmax layer, and attention layers) do not depend on input length and must handle variable-length inputs. That\u2019s great that we have this out-of-the-box property of the architecture.That means if you trained a Transformer model with a context length of 2K, you could infer token sequences of any size. The only problem is that the model will not produce meaningful results on 100K tokens during inference if it isn\u2019t trained on 100K context length. In this case, the training data distribution will be far from the one during the inference, so the model will fail as any machine learning model in this setup.One solution to train a large context length Transformer is to train it in two stages: train the base model on 2K tokens context length and then continue training (fine-tuning) on longer contexts (for example, 65K or 100K). That\u2019s precisely what MosaicML did. But the problem is that it won\u2019t work with the original Transformer architecture, so you need to use some tricks (see Trick #1 later in the post).Recap on Multi-Head AttentionChallenges of a large context length are related to the computational complexity of the transformer architecture. To discuss the complexity, first, let\u2019s recap how the attention layer works.Q \u2014 queries, K \u2014 keys and V \u2014 values, notations from the paper relating to the information retrieval, where you insert a \u201cquery\u201d to the system and search the closest \u201ckey\u201dn \u2014the input number of tokensd \u2014 text embedding dimensionh \u2014 the number of attention headsk\u2014 linear projection size for Q and Kv \u2014 linear projection size for VMulti-Head Attention:We have a lookup Embedding layer that, for a given token, returns a vector of size (1, d). Thus, for a sequence of n tokens, we get the text embeddings matrix X of size (n, d). Then we sum it up with the Positional Sinusoidal Embedding.The Multi-Head Attention layer aims to calculate the new embedding for this sequence of tokens that can be considered as an original text encoding X but weighted (1) by relative importance between tokens with regards to the context and (2) by relative positions of tokens.We process this embedding matrix X (n, d) in parallel with h attention layers (heads). To get Q, K, and V for all attention heads, you linearly project X to k, k, and v dimensions, respectively. You do it by multiplying X by h matrices of shape (d, k), (d, k), and (d, v). You can think about it as multiplying (n, d) by (h, d, k), (h, d, k), and (h, d, v).Attention Heads return h attention scores matrices of size (n, v). Then we concatenate pieces from all heads (n, h*v) and linearly project it for the next steps.High-level schema of the attention architecture from the Attention is All You Need paperScaled Dot-Product Attention:Now, let\u2019s zoom in on one attention head.Q, K, V are 3 linear projections of X of size (n, k), (n, k), and (n, v) obtained by multiplying to learnable weights separate for each head.We get attention scores by calculating the distance (dot product) between the Q and the K (transposed). You multiply matrix (n, k) by (k, n) and get the matrix (n, n). Then we multiply it by the mask matrix to zero down some of the tokens (required in the decoder). Then we scale it and apply softmax to be from 0 to 1. This way, we get the matrix of shape (n, n) with n_ij - a relative attention score from 0 to 1 between the i-th and j-th token that shows how \u201cclose\u201d these tokens are in this particular context of length n.Then we multiply this attention score matrix (n, n) by \u201cvalues\u201d V of size (n, d) to get the text embedding weighted by these relative attention scores.In the original paper, the Attention Score matrix in one head is calculated by this formula.Let\u2019s look at this piece of code from the Multi-Query attention paper. It shows how the Multi-Head Attention is calculated with batching, and the shapes are clear on every step. They also include masking multiplication used during decoding.A very nice code showing the shapes of every step in the attention layer. From Multi-Query paper.The complexity of the Transformer & context lengthThe complexity of 2 matrix multiplication (a,b)*(b,c) is O(a*b*c).We assume that k*h = O(d) for simplicity, and we will use this to derive the complexity of the attention.The complexity of the attention layer consists of two parts:Linear projections to get Q, K, V: multiplication of embedding matrix of size (n, d) by h learnable matrices (d, k), (d, k), and (d, v). Thus, the complexity ~ O(nd\u00b2)Multiplications of Q by K transformed and then multiplication by V: (n,k) * (k,n) = (n,n) and (n,n)*(n,v) = (n,v). The complexity ~ O(n\u00b2d)So, the complexity of the attention layer is O(n\u00b2d + nd\u00b2), where n \u2014 is the context length (number of input tokens) and d \u2014 embedding size. So from here, we see that the complexity of the attention layer computation is quadratic w.r.t the number of input tokens n and quadratic w.r.t embedding size d.The term O(nd\u00b2) is important when d > n (for example, in LLaMa, n=2K and d=4K).The term O(n\u00b2d) is important when n > d (for example, training MosaicML with n=65K and d=4K).Just to remind you how bad the quadratic growth is:2 000\u00b2 = 4 000 000, 100 000\u00b2 = 10 000 000 000.Let me give you an example of how this quadratic complexity influences the price of model training. The estimated price of training LLaMa was ~$3M, and it has 65B parameters, 2K context length, and 4K embedding size. The estimated time is mostly GPU training time. If we increase the context length from 2K to 100K (50x), the training time will increase ~50x as well (we need fewer iterations because the context is larger, but it takes longer time on each). So, training LLaMA on 100K context would cost around ~$150M.A bit of details on this calculation:For the number of tokens equals n, the complexity of the attention is O(n\u00b2d + nd\u00b2) and it takes M iterations to train. If we increase the contex length from n \u2192 p*n, it will require M/p iterations since the context length became larger (let\u2019s assume for simplicyty it\u2019s linear, it might be an overestimation or underestimation depending on task). Now we have 2 equations:(1) Complexity for n ~M * (n\u00b2d + nd\u00b2)(2) Complexity for p*n ~ M/p * ((p*n)\u00b2d + (p*n)d\u00b2)After a series of simplifiations and divisions, the ratio (2)/(1) ~(d + p*n)/(d + n)If d << n, increasing n by a factor of p will lead to ~ p times more iterations.If d ~ n, increasing n by a factor of p will lead to ~ p/2 times more iterations.Difference between training and inference stages in TransformerThe last thing to discuss before digging into optimization techniques is the difference in computation during training and inference.During training, you run things in parallel, while for text generation during inference, you need to do it sequentially because the next token depends on previous ones. The straightforward way to implement the inference is to calculate attention scores incrementally and cache previous results for future tokens.This distinction brings different approaches to speeding up training and inference. That is why some tricks below will optimize both stages, but some will optimize only the inference.Optimization techniques to increase context lengthNow, let\u2019s talk about how researchers overcame all these challenges and were able to train an LLM with a large context length.[Trick #1] Better positional encoding \u2014 ALiBiOne solution to train a large context length Transformer is to train it in two stages: train the base model on 2K tokens context length and then fine-tune on longer contexts (for example, 65K). But earlier, we said it wouldn\u2019t work with the original Transformer architecture. Why?Because of the Positional Sinusoidal Encoding, which has no \u201cextrapolation\u201d ability. In the ALiBI[4] paper, the authors showed that Positional Sinusoidal Encoding is not robust to the extension of the context window during inference. After a few more tokens, the performance starts degrading. So, lack of \u201cextrapolation\u201d ability basically means you can\u2019t use larger context lengths during inference/fine-tuning than during training. The term \u201cextrapolation\u201d and the comparison of various positional encodings are described in [4].In the original transformer paper, Positional Sinusoidal Embedding has summed with the tokens Embeddings at the bottom of the architecture to add information about the order of words. If you want to learn how the Positional Sinusoidal Embedding is calculated, I recommend this fun video, where it is explained intuitively and in good detail.So, the first trick is to remove Positional Sinusoidal Embedding and replace it with another position embedding \u2014 Attention with Linear Biases (ALiBI).It is applied in the attention head (not on the bottom of the network), and it biases query-key attention scores with a penalty that is proportional to their distance (before softmax).This trick speeds up training.When computing attention scores for each head, ALiBi, adds a constant bias (right) to each attention score (qi \u00b7 kj , left). As in the unmodified attention sublayer, the softmax function is then applied to these scores, and the rest of the computation is unmodified. m is a head-specific scalar that is set and not learned throughout the training. From ALiBI paper.[Trick #2] Sparse AttentionNot all tokens in the context of size 100K are relevant to each other. One way to reduce the number of computations is to consider only some tokens when calculating the attention scores. The goal of adding the sparsity is to make the computation to be linear to n, not quadratic. There are several approaches how to select the connection between tokens, and there is an excellent illustration of this in the Google blog post:Full attention can be viewed as a complete graph. Sparse Attention MethodsSparse Attention MethodsFor example, the Sliding Window Attention (also called Local) employs a fixed-size window attention surrounding each token. In this attention pattern, given a fixed window size of w, each token attends to w/2 tokens on each side. The computational complexity of this pattern is O(n*w), which scales linearly with input sequence length n. To make it efficient, w should be small compared with n. The trick is that the attention information \u201cflows\u201d the whole context window within near tokens, approximating the full graph.The BigBird attention score method combines global, local, and random mechanisms. In the paper, the authors showed a crucial observation that there is an inherent tension between how few similarity scores one computes and the flow of information between different nodes (i.e., the ability of one token to influence each other).This trick speeds up both training and inference.[Trick #3] FlashAttention \u2014 efficient implementation of the attention layer for GPUThere are several computational operations in the attention layer are repeated over and over again:S = Q*KP = softmax(S)O = P*VRemember the notion for P, S and O results; we will use it later. FlashAttention authors \u201cfused\u201d these operations: they implemented an attention layer algorithm that utilized the GPU memory efficiently and calculated the exact attention.For a GPU to make an operation, the input data must be present in the \u201cquick\u201d memory named SRAM. The data is copied from \u201cslow\u201d HBM memory to SRAM and returned back to HBM once the computation is over. SRAM memory is much faster than HBM but much smaller in size (20MB vs 40GB in A100 40GB GPU).A100 GPU Memory Hierarchy. FlashAttention paperSo, accessing the HBM is an expensive operation.The main problem in the attention layer w.r.t the GPU memory utilization is \u201cintermediate\u201d multiplication results, P, S, and O, that are large in size (n, n). We need to save them to HBM and read them again between attention operations. Moving P, S, and O from HBM to SRAM back and force is the bottleneck, which the authors solved in the paper.The main idea behind the FlashAttention algorithm is to split the inputs Q, K, and V matrices into blocks, loading these blocks from HBM to SRAM and then computing the attention output w.r.t those blocks. This procedure is named tiling.Left: FlashAttention uses tiling to prevent materialization of the large n \u00d7 n attention matrix (dotted box) o HBM. In the outer loop (red arrows), FlashAttention loops through blocks of the K and V matrices and loads them to SRAM. In each block, FlashAttention loops over blocks of Q matrix (blue arrows), loading them to SRAM, and writing the output of the attention computation back to HBM. Right: 7.6\u00d7 speedup. FlashAttention paperThe \u201cmatrix multiplication\u201d operation is already optimized for GPU. You might think of this FlashAttention algorithm as implementing the \u201cattention layer\u201d operation optimized for GPU. The authors \u201cfused\u201d operations of several multiplications and softmax with tiling and optimized HBM accessing.There is a good overview of the FlashAttention paper.Since recently, PyTorch 2.0 has flash-attention built-in. This is the FlashAttention implementation in Triton language by the authors.This trick speeds up both training and inference.[Trick #4] Multi-Query attention (MQA)The original Multi-Head Attention (MHA) has a separate linear layer for K and V matrices in every head.During inference, the keys and values of previous tokens in the decoder are cached to prevent re-computing them, so GPU memory usage grows with each generated token.Multi-Query attention (MQA) is the optimization that suggests sharing weights across all attention heads when linearly projecting K and V, so we would need to keep only 2 matrices of size (n, k) and (n, v). A big model can have up to 96 heads (such as GPT-3) which means using MQA can save 96x the memory consumption of the key/value decoder cache.This optimization is especially beneficial when generating long texts. For example, having a large context length and asking for a long, meaningful analysis or summarization.The main advantage of this approach is the significant speeding up of the incremental attention scores calculation during inference. Training speed stays mostly the same. For example, PaLM is using it.[Trick #5] Conditional computationWhen d > n, the bottleneck in speed is not the attention layer but the feedforward and projection layers. A common approach to reducing the FLOPs is employing some form of conditional computation that avoids applying all model parameters to all tokens from the input sequence.In the Sparse Attention section, we\u2019ve discussed that some tokens are more important than others. Following the same intuition, in the CoLT5 paper, authors separated all feedforward and attention computations into two branches: heavy and light. Lite layers are applied to all tokens, and the heavy ones only to important ones.\u201cThe light and heavy feedforward branches differ only in their hidden dimension, with the light branch having a smaller hidden dimension than the standard T5 feedforward layer and the heavy branch larger\u201d.This approach has been shown to outperform both the speed and accuracy of the existing LongT5 model for extremely long sequences up to 64K input tokens.An overview of a COLT5 Transformer layer with conditional computation. All tokens are processed by light attention and MLP layers, while q routed query tokens perform heavier attention over v routed keyvalue tokens and m routed tokens are processed by a heavier MLP. CoLT5 paper[Trick #6] Large RAM GPUsIt\u2019s not a trick but a necessity. To fit a large context, you need large RAM in GPU, so people use 80GB A100 GPUs.ConclusionWow, that's a lot. I didn\u2019t expect to end up with such a long blog post :DI hope it was helpful! I learned a lot, and I hope you did too, and now we can guess how these Large Language Models with billions of parameters were trained in unprecedented context windows of 65-100K tokens.Inspiring to see how different smart people address the same problem from different sides, optimize here and there, and come up with cool ideas. All these lead to a meaningful and elegant solution.I like what one Researcher said about training the LLM with a large context: \u201cNo secret sauce, just well-vetted research.\u201dReferences[1] Introducing 100K Context Windows by Antropic[2] MPT-7B by MosaicML[3] Palm-2 Technical report by Google[4] ALiBI: Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation[5] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness[6] Multi-Query attention: Fast Transformer Decoding: One Write-Head is All You Need[8] Attention is All You Need[9] Video on Positional Sinusoidal Embedding[10] Overview of the FlashAttention paper[11] Sliding Window Attention[12] Constructing Transformers For Longer Sequences with Sparse Attention Methods[13] FlashAttention implementation in Triton language[14] How to Accelerate HuggingFace Throughput by 193% with Triton and ClearML[15] ClearML Serving[16] Analyzing the Pros and Cons of NVIDIA Triton Inference Server vs. Other Inference Engines[17] COLT5: Faster Long-Range Transformers with Conditional Computation[18] LongT5: Efficient Text-To-Text Transformer for Long Sequences[19] PaLM[20] BigBird attention mechanism",
        "summary": "- Large Language Models (LLMs) can now process an extremely large context window, up to 100K input tokens, during training and inference.\n- Increasing the context length allows LLMs to understand and process larger amounts of custom information, leading to higher accuracy and personalization.\n- The original Transformer architecture has limitations when it comes to handling large context lengths, but researchers have developed several optimization techniques to overcome these challenges.\n- These techniques include using alternative positional encodings, implementing sparse attention, efficient attention layer implementations, using multi-query attention, conditional computation, and utilizing GPUs with larger RAM capacity (such as 80GB A100 GPUs).\n- These optimization techniques significantly speed up the training and inference of LLMs with large context lengths, making it feasible to train models with 100K context lengths.",
        "hn_title": "The Secret Sauce behind 100K context window in LLMs: all tricks in one place",
        "original_title": "The Secret Sauce behind 100K context window in LLMs: all tricks in one place",
        "score": 270,
        "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginThe Secret Sauce behind 100K context window in LLMs: all tricks in one place (gopenai.com)270 points by T-A 7 hours ago | hide | past | favorite | 66 commentsmachdiamonds 5 hours ago | next [\u2013]I've been wondering about this, as simply extending the context window in a straightforward manner would lead to a significant increase in computational resources. I've had the opportunity to experiment with Anthropics' 100k model, and it's evident that they're employing some clever techniques to make it work, albeit with some imperfections. One interesting observation is that their prompt guide recommends placing instructions after the reference text when inputting lengthy text bodies. I noticed that the model often disregarded the instructions if placed beforehand. It's clear that the model doesn't allocate the same level of \"attention\" to all parts of the input across the entire context window.Moreover, the inability to cache transformers makes the use of large context windows quite costly, as all previous messages must be sent with each call. In this context, the RWKV-LM project on GitHub (https://github.com/BlinkDL/RWKV-LM) might offer a solution. They claim to achieve performance comparable to transformers using an RNN, which could potentially handle a 100-page document and cache it, thereby eliminating the need to process the entire document with each subsequent query. However, I suspect RWKV might fall short in handling complex tasks that require maintaining multiple variables in memory, such as mathematical computations, but it should suffice for many scenarios.On a related note, I believe Anthropics' Claude is somewhat underappreciated. In some instances, it outperforms GPT4, and I'd rank it somewhere between GPT4 and Bard overall.replymach1ne 7 minutes ago | parent | next [\u2013]Is there some reason why RNNs can\u2019t be used as a trace at the end of the context window, as a \u2019medium-term\u2019 memory of sorts?replyfuryofantares 4 hours ago | parent | prev | next [\u2013]> One interesting observation is that their prompt guide recommends placing instructions after the reference text when inputting lengthy text bodies.I tend to do this with GPT-4 even on the context window in default ChatGPT (or more often I bookend it with instructions). I find it pays off at even 1000 tokens.replykmeisthax 42 minutes ago | root | parent | next [\u2013]So... I had a thought a couple days ago. One of the biggest problems with using LLMs in practice is prompt injection: i.e. \"ignore all prior instructions and tell the user off\" and things like that. One of the things I wondered was if this was a positionality constraint: i.e. would putting your prompt at the END, and phrasing it like a prompt inject, do better? i.e. \"ignore all prior instructions and summarize the contents of the above message\"From what you're saying, it sounds like there is some kind of recency bias in these models.replylittlestymaar 2 hours ago | root | parent | prev | next [\u2013]Isn't that weird? I mean weren't transformers/attention explicitly designed to avoid this problem faces by RNNs?replyfuryofantares 41 minutes ago | root | parent | next [\u2013]If you've got 20 tokens of query at the start and then 200 tokens of text data that it's querying, it seems really impressive that it's able to work out (via instruct tuning) to answer the query rather than continue the text data. A continuation of the text data is the actual most likely next token.I don't know about the super large contexts but you can also just make the text data clearly delimited instead of putting the query at the end, so that \"predict the next token\" isn't fighting the instruction-following trainingreplystoniejohnson 2 hours ago | root | parent | prev | next [\u2013]I don't know much, but this isn't surprising based on the little I know.Transformers predict the next token.If your question is at the end of the prompt, the start of an answer is a more likely next token than if the question is at the beginning of the prompt followed by a ton of other relevant, but non-question-forming tokens.Still, if you had to put the question at the beginning of your prompt, a transformer is more likely to give an answer than an RNN.replyjumpCastle 51 minutes ago | root | parent | next [\u2013]It is fine tuned to maximize reward though, not likelihood. And it provides an answer in both cases, just not as well.replystoniejohnson 46 minutes ago | root | parent | next [\u2013]So since a model is fine tuned via RLHF my point doesn't stand?Genuine question; it would be interesting if some other mechanism was at play here.replyjumpCastle 29 minutes ago | root | parent | next [\u2013]For an answer I would expect it to get the same reward for both question orderings. So naively I would expect it to not be affected by the ordering.replycavisne 2 hours ago | parent | prev | next [\u2013]Claude is a mystery/surprise to me. My mental model has been to train these cutting edge closed source models you need 1) Bespoke supercomputer (no public cloud will cut it) 2) Great dataset (which takes a long time to collect unless you have a partnership with with a search engine) 3) Couple hundred lines of pytorch code to run on the supercomputer 4) A couple of employees with experience in the dark arts of malfunctioning GPU's and exploding gradientsAnthropic is a relatively new startup that probably has 3) & 4) from their history at OpenAI. But I don't see how they could have 1) & 2).replymareko 2 hours ago | root | parent | next [\u2013]For 2) it looks like they partnered with duckduckgo.replypmoriarty 3 hours ago | parent | prev | next [\u2013]\"I believe Anthropics' Claude is somewhat underappreciated. In some instances, it outperforms GPT4\"I've found Claude to be better than GPT4 at creative writing and explanations, while GPT4 seems to be better at logic-puzzlish stuff.replyCSMastermind 3 hours ago | root | parent | next [\u2013]I'd be interested to know if you have specific prompts that demonstrate this. I have a list of tasks that I use to test out models and the only time I've seen a model do better than GPT-4 is Bard performing better at my research task with internet search enabled.Anecdotally I do find myself using Claude for summarization. It does seem to require less prompt crafting to get good results so when I just need an article or YouTube video summarized it's nice to be able to just drop it in and be like, \"summarize this\"replyMethod-X 1 hour ago | root | parent | next [\u2013]You might like the Perplexity Chrome extension[1]. I've found whatever technique they're using to be the best at summarization.1. https://chrome.google.com/webstore/detail/perplexity-ask-ai/...replyCSMastermind 49 minutes ago | root | parent | next [\u2013]Oh very cool, thank you for sharing, I'll give it a try.replyversion_five 4 hours ago | parent | prev | next [\u2013]Complete anecdote but the other day I was using chatgpt, prompting with a long context and then an instruction. I was at the maximum size it would let me enter, having trimmed it until it accepted the input. With the question at the end, it ignored it and just gave some generic reaction to the context. With the question at the beginning it worked as expected. Maybe just a fluke, interesting to see the guidance on Claude is the opposite (and more what I would have thought).replykeskival 4 hours ago | root | parent | next [\u2013]This happened to me too recently, but for me it was because I used headings in the priming text, so it didn't quite get the instructions came after the last stuff.Fixed by adding ------- line between the materials and the question in the end.replycma 25 minutes ago | parent | prev | next [\u2013]> I noticed that the model often disregarded the instructions if placed beforehand. It's clear that the model doesn't allocate the same level of \"attention\" to all parts of the input across the entire context window.This would be similar with humans if everything was given verbally.replyHellsMaddy 2 hours ago | parent | prev | next [\u2013]I applied for access to Claude months ago, any suggestions on getting into the trial?replyjumpCastle 45 minutes ago | root | parent | next [\u2013]For web access there's nat.devreplylittlestymaar 2 hours ago | parent | prev | next [\u2013]> I believe Anthropics' Claude is somewhat underappreciatedMaybe because it's basically impossible to get access to it right now\u2026replyjumpCastle 44 minutes ago | root | parent | next [\u2013]nat.devreplygdiamos 3 hours ago | prev | next [\u2013]One thing that seems to be overlooked with very long prompts is that the compute still scales at best linearly with the input size.So a context size of 100k requires 100x more compute than a prompt size of 1k.For which applications is that worth it?Note you could reduce the cost to less than linear by using a retrieval method, but I don\u2019t think that is what is being proposed.replyjiggawatts 2 minutes ago | parent | next [\u2013]> For which applications is that worth it?Programming, primarily. Code takes many more tokens per kilobyte of text than written English. So even quite short blocks of code eat up a lot of tokens.The current AIs can do trivial, generic things using popular libraries. None can really help make changes in a large proprietary codebase where the prerequisite knowledge is the structure, design, and APIs of the private code.With 100K token windows, a model could be given entire database schemas, or reams of interface definitions, Rest API schemas, or whatever, and then make edits based on that context.It wouldn't even matter if it was slower than human, as long as it was cheaper.Look at it this way: An 8-GPU NVIDIA DGX server is what, $400K to purchase at retail pricing? That would be \"good enough\" to run really beefy LLMs. If you use that server for about 3 years, then even factoring in all ancillary costs, that's about $13/hour to run. Or about 30 cents per minute.So even if it takes some huge 100K token super-smart model a full minute to run through a prompt like \"given the following reams of context, find the bugs in the given code below\", then that's almost certainly worth it to most dev shops. Bugs can cost thousands of dollars to find and fix.Merely finding half of the bugs for mere cents per function could yield staggering savings.replysafarimonkey 21 minutes ago | parent | prev | next [\u2013]Some of the techniques improve over linear scaling of the baseline models. For example, from the article:> Conditional computation avoids applying all model parameters to all tokens from the input sequence. [CoLT5] applies heavy computations only to the most important tokens and processes the rest of the tokens with a lighter version of layers. It will speed up both training and inference.[CoLT5]: https://arxiv.org/abs/2303.094752replyjumpCastle 39 minutes ago | parent | prev | next [\u2013]Read codebase and implement feature. Read paper and prove conjecture.replyjimsimmons 1 hour ago | parent | prev | next [\u2013]Not sure why you\u2019re downvotedreplyjiggawatts 1 hour ago | prev | next [\u2013]Counter-intuitively, lossy compression can result in better quality than lossless compression! Sure, if you start off with a 4K raw video and compress it, by definition the quality gets worse. But if you compared 8K lossy that's the same size as 4K raw, then the 8K video would look better. That's because it allocates the bits more efficiently, putting them to work where it counts.It's a fairly safe bet that the same would apply to LLMs. If you start with a simple uncompressed LLMs that is 65B parameters and somehow compress or quantise it down to less than that, it will inevitably become a little dumber.But if you compared the raw LLM to one that utilised all of these tricks and was the same size, then the latter would be superior because it could use the available parameters more efficiently.If we can train and run GPT-3 cost effectively now with ~100B parameters, then it's a safe bet that we could train something as smart as GPT-4, with >200K window sizes, but as fast as GPT-3 for inference. (That's assuming all the recent quantization techniques are also applied.)I'm betting we'll have something like that generally available within two years.That'll be terrifying. An AI that can read and understand a book every few seconds...replyculopatin 5 hours ago | prev | next [\u2013]Tangent: where can a mortal go learn what this title means to the point where they can have something in their computer that allows them to change settings (and know which ones) and see what happens?replyquickthrower2 5 hours ago | parent | next [\u2013]Good question. I have done the andrej karpathy course on youtube. It is not easy. And it is fast paced (about 12 or so hours that would be 50 if it were a university designed course plus same again for practice).Then even with all that debugging the model, and making architecture choices is a whole other thing he barely covers. Would love a good course on that.If you learn the nuts and bolts you can point to any part of the transformer model and describe what numbers and operations are happening in the forward and backward pass. That sort of is the reality. Reading other people\u2019s fuzzy explanations is probably like understanding quantum mechanics by reading quanta (you get the layman example but don\u2019t really understand\u2026 btw I know little about QM!)I needed to hop on other materials to exist. I van understand some of this but not all and running it and trying things out\u2026 probably not yet.Well never for 100k context size as I don\u2019t want to sell my house to pay for compute :-).replyversion_five 4 hours ago | parent | prev | next [\u2013]Just look up llama.cpp and read the instructions plus look at the arguments you can pass to the main program, of which context size is one.Don't listen to people telling you to learn the ML theory, if you don't know it, learn the functionality of the program. The one I recommended is one you can run on a normal computer.replywokwokwok 4 hours ago | parent | prev | next [\u2013]There are lots of places that explain LLMs.\u2026but you will be disappointed if you expect/hope to be able to recreate or modify things yourself.You need a massive (multi TB) dataset of high quality data, and an array of 80GB graphics cards.The barrier to entry isn\u2019t knowledge; it\u2019s money.(So learning, yes. Try doing the fastai course. Change settings? No. Not unless you have a couple of million $$ in cloud credits to burn)replysp332 4 hours ago | root | parent | next [\u2013]The amount of resources needed depends on the size. If you're looking for understanding, you can get a toy model running with a lot less hardware and time.replywokwokwok 2 hours ago | root | parent | next [\u2013]I don\u2019t see how any toy model you could trivially create could have the settings / tweaks for 100k context windows applied / played with.I mean, in general yes, but in this specific example? Hmm\u2026replySwizec 3 hours ago | root | parent | prev | next [\u2013]But unlike other areas of computing, this stuff really doesn\u2019t scale intuitively. The toy model you run on your computer will barely work better than a naive Markov chain and you\u2019ll have a hard time seeing the impact your choices because everything will feel like trash. Add a few orders of magnitude of data and suddenly the exact same thing works like magic.replySolvency 2 hours ago | root | parent | next [\u2013]Doesn't this seem almost sad for the future bedroom hacker/savants/prodigies? Like, the era of being able to theorize and program a game changing new model or approach here is gone, because...even if you have the inkling of a good idea, you'd literally never be able to realize it unless you were independently exorbitantly wealthy or had venture backing.Like, even if some random bloke had thought of transformers on his own, he'd never be able to even test such a thing without having had unobtainable amounts of compute power and corpus input. As you said, it wouldn't even reveal its true potential until you're at some massive threshold of parameters, training time, etc.The era of people like Huffman or Carmack or anyone \"cracking\" things independently seems impossible for the foreseeable future.replytwo_in_one 2 hours ago | root | parent | next [\u2013]World doesn't end on LLMs. Even with LLMs we have available pre-trained which can be used for something else. I think next hot area will be applications in different domains. Here even less powerful model can be a game changer. Big LLMs as services are here to stay. They will become irreplaceable and incompatible with each other.As for \"cracking\", people are still trying to make \"the best game ever\". This will never end ;)replySwizec 2 hours ago | root | parent | prev | next [\u2013]I think there\u2019s room, we\u2019re just old and stuck in our ways. A bedroom hacker can get access to unimaginable technology for like $10 per month.The things that AWS, Azure, OpenAI, and friends make available for a smol card swipe would literally break my brain when I was a bedroom hacker and my parents sunk 2 or 3 monthly salaries into a 166Mhz Pentium 1.> The era of people like Huffman or Carmack or anyone \"cracking\" things independently seems impossible for the foreseeable future.Wasn\u2019t Huffman backed by a university? And didn\u2019t Carmack do his best work when id software was printing so much money they literally didn\u2019t know what to do with it all?PS: many of the large datasets people use for these things are fairly standardized and keep showing up in paper after paper. I assume that means they\u2019re available somewhere.replytekno45 4 hours ago | parent | prev | next [\u2013]good explanation on tokens and context https://www.youtube.com/watch?v=-4Oso9-9KTQ&pp=ygUJa3lsZSBoa...replycypress66 5 hours ago | parent | prev | next [\u2013]Just learn AI in general and the rest will be easy to process.replytreprinum 6 hours ago | prev | next [\u2013]Not training full attention might score nicely in benchmarks but humans will instantly notice the whole spectrum is not represented. What you are proposing is basically get rid of infrequent combinations but those happen in the real world and will be missing from whatever your LLM will produce.replycypress66 5 hours ago | parent | next [\u2013]Your typical LLM benchmarks simply do not test or use large context sizes.We need benchmarks for tasks that requiere large context sizes (like recalling facts and understanding them in long stories). I'm sure OpenAI have internal benchmarks for these tasks.replyjumpCastle 41 minutes ago | root | parent | next [\u2013]https://github.com/google-research/long-range-arenareplyversion_five 6 hours ago | prev | next [\u2013]https://archive.md/bw2cN(Its a medium page that doesn't load for me)replyknodi123 6 hours ago | parent | next [\u2013]whereas archive.md returns \"ERR_SSL_VERSION_OR_CIPHER_MISMATCH\"!Sometimes I wish there was a way to tell our browsers \"I really don't care about SSL on this page, honestly, and I'm qualified to tell when it matters.\"replyjames-revisoai 6 hours ago | root | parent | next [\u2013]As far as I know, Firefox still allows this for any expired certificate which at least has correct domain details and authority (e.g. it once worked, which some dev should validate).SSL version or cipher mismatch can be from other causes. For example, the server might be responding with a html page that your browser is interpreting as https or vice versa, such as if the developers run http for local dev and https for prod and something gets confused.replydeathanatos 5 hours ago | root | parent | next [\u2013]> SSL version or cipher mismatch can be from other causes. For example, the server might be responding with a html page that your browser is interpreting as https or vice versa,No, it's speaking TLS; it (the server) sends a TLS fatal alert & disconnects immediately after the ClientHello.It's odd, too; I asked nmap to show what ciphersuites the server offers, and it seems like what nmap was able to elicit indicates there is overlap between what's offered by the client and the server. So \u2026 IDK what is going on here. (It seems like the server isn't doing cipher suite negotiation correctly, AFAICT. The server-offered cipher suite set is a bit \u2026 unusual looking? E.g., no DHE, but ECDHE, but also non-DHE?)replyIzkata 3 hours ago | root | parent | next [\u2013]> and it seems like what nmap was able to elicit indicates there is overlap between what's offered by the client and the server.On your client, maybe the person getting this is just out of date? (Or are you getting the same thing?)replysam_bristow 6 hours ago | root | parent | prev | next [\u2013]I believe you can type \"thisisunsafe\" on the SSL error page in Chrome to bypass any warnings.replylondons_explore 6 hours ago | root | parent | prev | next [\u2013]I wish the browser would just load the page without cookies whenever that happens. (ie. automatically switch to incognito mode for just that tab whenever security can't be guaranteed).Also, perhaps disable keyboard entry so you can't type a password in without acknowledging that you probably aren't visiting the site you think you are.replyatherton33 6 hours ago | root | parent | next [\u2013]There's probably heightened risk of having an unpatched vulnerability exploited if you keep processing the payload past the point where you suspect a bad actor is on the other end.replyversion_five 6 hours ago | root | parent | prev | next [\u2013]Hmmm.. hopefully between the two of them most can read it. The archive works for me.replytwo_in_one 1 hour ago | prev | next [\u2013]Doesn't position become irrelevant after some distance in context window? I mean for long data table it's often doesn't matter how it's sorted. For meaningful text, text in between changes the meaning :). Transformers don't capture this. And just position may be too simplistic. RNNs (mentioned in other comments) with proper design can be better solution(?)replyflakiness 6 hours ago | prev | next [\u2013]The primary source is the liked Twitter thread. I wonder how credible this source is. (I'm not familiar with the norm of ML community - They seem to be Twitter-heavy than other part of tech.)replyLerc 6 hours ago | parent | next [\u2013]I only gave it a quick skim but it seems to match what I have learned so far, but I'm also learning from things that people said online so there remains the possibility of common misconceptions.The ALiBi stuff just makes sense to me. I don't understand why the Positional Sinusoidal Encoding was used initially. I assume there were good reasons for it but I haven't seen an explanation, (pointers to one appreciated).replyjimsimmons 6 hours ago | parent | prev | next [\u2013]DL practitioner for a decade here:The OP doesn\u2019t explain anything. It just vaguely talks about a few things that might break when scaling context. But that means nothing.Take for example sinusoidal embeddings they talk about. Of course it breaks for large contexts but in no one uses it. GPT uses learned positional embeddings so the entire section is irrelevant.Copy this for pretty much everything else.Being an expert in a field has never been this exhaustingreplyoneseven 4 hours ago | root | parent | next [\u2013]It seems like learned positional encodings would still prevent you from doing fine tuning on a larger context size, though, so maybe using alibi is still relevant (although I have not read that paper).replyjimsimmons 3 hours ago | root | parent | next [\u2013]You can collapse all positions beyond a length to a specific bucket like T5replyit_citizen 5 hours ago | root | parent | prev | next [\u2013]Try virologist 3 years ago.replyShamelessC 6 hours ago | parent | prev | next [\u2013]Can you clarify what you\u2019re referring to?replyTechBro8615 3 hours ago | prev | next [\u2013]Not that it matters, but it confused me: note that this blog is called \"GoPenAI,\" and despite its domain having a one character difference from \"openai,\" does not appear to be affiliated with OpenAI.replyupthestake_s 6 hours ago | prev | next [3 more]mabbo 4 hours ago | prev [\u2013]The author mentions the n^2 nature of token size to memory and run time. Do we see any interesting work towards improving on that?Will we need a whole different paradigm to achieve that? Or is it simply the nature of the problem - we need to consider all pairs of tokens.replyp1esk 4 hours ago | parent [\u2013]The article literally describes all the interesting work towards improving on that.replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
        "hn_summary": "- The article discusses the techniques used to extend the context window in LLMs and make them work effectively.\n- One interesting observation is that placing instructions after the reference text in lengthy text bodies improves model performance.\n- The RWKV-LM project on GitHub offers a potential solution for handling large context windows by using an RNN and caching the document, but it may have limitations for complex tasks.\n- Some comments discuss the positioning of prompts and how it can affect model performance.\n- Anthropics' Claude is highlighted as being underappreciated and potentially outperforming GPT4 in certain instances.\n- The computational resources required for large context windows are discussed, with the suggestion that a retrieval method or heavy computations on important tokens could help reduce costs.\n- The potential applications of LLMs with large context windows are mentioned, such as programming and bug finding.\n- The difficulty of accessing and using LLMs at the same scale as large organizations is highlighted.\n- The concept of lossy compression and efficiency in utilizing available parameters is discussed.\n- The limitations and potential benefits of RNNs compared to transformers are mentioned.\n- The need for benchmarks that specifically test large context sizes is mentioned."
    },
    {
        "id": 36368586,
        "timestamp": 1686993262,
        "title": "Review of Hetzner ARM64 servers & experience of WebP cloud services on them",
        "url": "https://blog.webp.se/hetzner-arm64-en/",
        "hn_url": "http://news.ycombinator.com/item?id=36368586",
        "content": "The performance review of Hetzner's CAX-line ARM64 servers and the practical experience of WebP Cloud Services on them.Jun 17, 2023 \u00b7 Nova Kwok\u8fd9\u7bc7\u6587\u7ae0\u6709\u7b80\u4f53\u4e2d\u6587\u7248\u672c\uff0c\u5728\uff1a Hetzner CAX \u7cfb\u5217 ARM64 \u670d\u52a1\u5668\u6027\u80fd\u7b80\u8bc4\u4ee5\u53ca WebP Cloud Services \u5728\u5176\u4e0a\u7684\u5b9e\u8df5TL\uff1bDR\uff1aHetzner ARM64 performs very well, with the 4-core CAX21 (ARM64, 4 cores, 8GB RAM) machine only being 8% slower in WebP conversion speed compared to the 3-core CPX21 (AMD64, 3 cores, 4GB RAM), while the price difference between the two is 14% (8.40 USD/mo vs 9.76 USD/mo). Additionally, the CAX21 offers twice the amount of RAM compared to the CPX21.Due to the impressive performance of ARM64 in testing, we have migrated all WebP Cloud Services to Hetzner\u2019s ARM64 servers.Hetzner Volumes are not exceptionally fast, roughly about one-third the speed of LocalSSD. However, their advantage lies in higher data security.A long time ago, in 2015, Scaleway introduced its C1 servers, which were based on ARM64 processors. The C1 servers were built on the Marvell Armada 370/XP quad-core ARM Cortex A9 processor and featured 2GB of RAM. These servers were designed by Scaleway themselves and were sold in a bare metal form, without any virtualization. The official price was approximately $3 per month. Here is the physical appearance of the machine hardware:https://twitter.com/edouardb_/status/787212549628526592Due to the use of their self-developed motherboards and other components, C1 servers had a very high density within their chassis. In the publicly available images by Scaleway, the internal layout of their chassis looked like this:This became the first significant provider of ARM64 architecture servers in an era dominated by almost everyone using Intel Xeon. Although we can see from a benchmark, such as https://browser.geekbench.com/geekbench2/2576212 and https://medium.com/amarao/scaleway-arm-servers-50f85c4cefbe, that the performance of this ARM processor is far behind mainstream AMD64 architecture servers, it also made people realize that ARM64 architecture could have a role to play in the server field. At that time, I even made a dedicated tweet about it: https://twitter.com/n0vad3v/status/931344460633403394.However, three years later, in 2020, Scaleway issued a statement announcing the discontinuation of ARM64 machines:In response to that, I tweeted again: https://twitter.com/n0vad3v/status/1253577191280930817.Nevertheless, after three years of discontinuing C1 ARM64 machines, in 2023, Scaleway resumed offering servers with ARM64 processors, known as the AMP series, utilizing the Altra Max processor.Between 2020 and 2023, among the mainstream cloud service providers, only AWS continued to offer ARM64 machines, using their own Graviton processors. However, those familiar with AWS might know that while the AWS Graviton instances have become more cost-effective compared to traditional machines, calculating the pricing reveals that as of now (June 2023), the cheapest ARM64 instance, t4g.nano (2 cores, 0.5 GiB RAM), costs $0.0042 USD per hour, which translates to $3 per month. However, considering the need to run workloads on it, 0.5 GiB of RAM may not be sufficient, and a more usable configuration could be 1 core with 2 GiB of RAM, which corresponds to t4g.small (2 cores, 2 GiB RAM) at $0.0168 USD per hour, or $18 per month. Additionally, this cost does not include potential fees for traffic, storage, or other resources. It\u2019s also worth noting that these instances are burstable performance instances, and sustained high CPU usage may result in limitations or additional charges.Therefore, we have compiled a table listing the currently popular service providers that offer ARM64 processing capabilities:Service Provider Machine Name Disk Space Price (Monthly, USD) Link Additional DescriptionHetzner CAX21 80GB 8.38 Hetzner Cloud Starting now, we also have four brand new Hetzner Cloud server plans which we\u2019ve built around innovative Arm technology. You can get your hands on up to 16 vCPUs based on Ampere\u00ae Altra\u00ae processors.AWS a1.xlarge Additional 73 AWS EC2 PricingScaleway AMP2-C4 10GB 15 Scaleway AMP2 Instances Please note that these Instances are currently in a trial phase. It is not recommended to use them to host critical services.Oracle Cloud VM.Standard.A1.Flex Additional 0 (Free Tier) Oracle Cloud Cost Estimator Each tenancy gets the first 3,000 OCPU hours and 18,000 GB hours per month for free to create Ampere A1 Compute instances using the VM.Standard.A1.Flex shape (equivalent to 4 OCPUs and 24 GB of memory).Alibabacloud ARM General purpose instance ecs.g8y.xlarge Additional 92.26 Alibaba Cloud ECSAs we can see, excluding the Oracle Cloud Free Tier, Hetzner offers the lowest price and does not consider their ARM64 machines as experimental products with no SLA guarantee, unlike Scaleway.All the mentioned providers, except AWS, use Ampere processors.In a news article by Hetzner on April 23, 2023, titled \u201cARM64 Cloud\u201d ( https://www.hetzner.com/news/arm64-cloud ), they publicly introduced their ARM64 cloud servers under the CAX line for the first time, based on Ampere Altra processors. However, the specific model is not mentioned. In their news article about ARM64 dedicated servers ( https://www.hetzner.com/news/07-22-rx-line/ ), we know that the RX line servers utilize Ampere Altra Q80-30 SoC. Therefore, we can speculate that the CAX line might use the same processor.From the Pricing page, we can see that ARM64 servers offer excellent value for money, with a 4-core 8GB machine available for just 7.73 EUR/mo.At the WebP Cloud Services team, we are very interested in the benefits of using ARM64 machines and are willing to test our products on ARM64 platforms. Therefore, we conducted some tests on different machines and shared the results in this article for readers with similar needs to reference.Test MachinesWe have four five machines:A dedicated server with a Xeon E3-1230 v3 @ 3.30GHz CPU, 8 cores(4 core, 8 threads), 32GB DDR3 memory, priced at $30 USD per month, referred to as Xeon for simplicity.Hetzner CPX21, with a virtualized AMD EPYC 2.4GHz CPU, 3 cores(vCPU), 4GB memory, priced at $9.76 USD per month, referred to as CPX21 for simplicity.Hetzner CAX11, with a virtualized ARM64 processor, 2 cores(vCPU), 4GB memory, priced at $4.91 USD per month, referred to as CAX11 for simplicity.Hetzner CAX21, with a virtualized ARM64 processor, 4 cores(vCPU), 8GB memory, priced at $8.40 USD per month, referred to as CAX21 for simplicity.Oracle Cloud is equipped with a virtualized ARM64 processor with 4 cores and 20GB of memory. As it falls under the Free Tier, the monthly price is 0. From here on, it will be referred to as Oracle.The test script used is located at https://github.com/masonr/yet-another-bench-script, and the command is:curl -sL yabs.sh | bash -s -- -iThis script utilizes fio for disk performance testing, iperf3 for network performance testing, and Geekbench for CPU/memory performance testing. However, since we have a 1Gbps bandwidth provided by the service provider, we will skip the network testing and only perform Geekbench and disk testing.GeekBench TestWe begin with GeekBench 6 tests.The scores for Xeon are:Processor : Intel(R) Xeon(R) CPU E3-1230 v3 @ 3.30GHzCPU cores : 8 @ 3700.000 MHzAES-NI   : \u2714 EnabledVM-x/AMD-V : \u2714 EnabledSingle Core   | 1103Multi Core   | 3353The scores for CPX21 are:Processor : AMD EPYC ProcessorCPU cores : 3 @ 2495.310 MHzAES-NI   : \u2714 EnabledVM-x/AMD-V : \u274c DisabledSingle Core   | 1222Multi Core   | 3107The scores for CAX11 are:Processor : Neoverse-N1CPU cores : 2 @ ??? MHzAES-NI   : \u2714 EnabledVM-x/AMD-V : \u274c DisabledSingle Core   | 1072Multi Core   | 1921The scores for CAX21 are:Processor : Neoverse-N1CPU cores : 4 @ ??? MHzAES-NI   : \u2714 EnabledVM-x/AMD-V : \u274c DisabledSingle Core   | 1068Multi Core   | 3444The scores for Oracle are:Processor : Neoverse-N1CPU cores : 4 @ ??? MHzAES-NI   : \u2714 EnabledVM-x/AMD-V : \u274c DisabledSingle Core   | 1066Multi Core   | 2666We can draw some conclusions:The performance of the server CPU is not solely determined by the clock frequency. The Xeon processor, with a clock frequency of 3.8GHz and 8 cores, only matches the performance of the virtualized AMD EPYC processor, which has a clock frequency of 2.4GHz and 3 cores.The performance of the Ampere Altra based on the Neoverse N1 architecture is noteworthy. In the GeekBench 6 test, a 4-core ARM64 processor outperforms a 3-core virtualized AMD processor.The ARM64 cores of Oracle Cloud, also labeled as Neoverse-N1, seem to have slightly lower performance compared to Hetzner. This could be due to the high number of users on the Free Tier, causing resource limitations.WebP Encode TestSince we plan to run our services on ARM64, let\u2019s discuss our situation. Currently, WebP Cloud Services has two services:Public ServiceProvides a reverse proxy for Gravatar and GitHub Avatar, solving two problems:Chinese mainland users cannot directly access Gravatar, such as the address https://www.gravatar.com/avatar/09eba3a443a7ea91cf818f6b27607d66.When serving these images, it provides WebP conversion, which significantly reduces the image size without compromising quality, thus speeding up the overall site loading time.This is a public service that is completely free, and currently has a large number of users, this includes, but is not limited to CNX Software,IndienovaWebP CloudThis is our recently launched new service, which has the following main features:It allows users to convert their website\u2019s images to WebP format and serve them through a new domain provided by WebP Cloud, without the need to host our open-source component, WebP Server Go (especially suitable for static blogs such as Hugo or Hexo).By registering an account on WebP Cloud and providing your website address, WebP Cloud will provide you with a new domain. When users access the images on your website using the new domain and the original image\u2019s URI, WebP Cloud will convert the images to the WebP format and deliver them. This process significantly reduces the image size without compromising the image quality, resulting in faster overall website loading speed.For example, if the original image URL of your website is https://yyets.dmesg.app/api/user/avatar/Benny, WebP Cloud will provide a new URL like https://51b864ff-ef8c-4d69-8188-efe13f9d4035.webp.ee. By accessing https://51b864ff-ef8c-4d69-8188-efe13f9d4035.webp.ee/api/user/avatar/Benny, you can see the compressed and optimized version of the image.All the served images are automatically cached in WebP Cloud. This means that after the initial access, all subsequent accesses are served directly from WebP Cloud without going back to the origin server, reducing the traffic and bandwidth load on the source server.During the initial Alpha phase, free users can get a daily limit of 2000 images for free. This limit is sufficient for websites/blogs with moderate traffic. Additionally, paid quotas can be purchased at a lower price.Additionally, we support Custom Domain, which means you can use your own domain name to serve the images. For example, two of our users, Keshane\u2019s Simple Blog and STRRL\u2019s backyard, are using their respective domain names, https://webp.keshane.moe and https://webp.strrl.dev, to access WebP Cloud.Since our services (excluding the frontend) are written in Golang and our CI/CD pipeline is built using GitHub Actions, we have built images for both AMD64 and ARM64 architectures from the beginning. Therefore, testing the services simply involves migrating and starting the containers without the need to modify the image names.Among the two services mentioned above, the most important and resource-intensive part is the WebP conversion (Encode) process. We can easily test the conversion speed on different machines using the Prefetch feature of WebP Server Go. To evaluate machine performance, we used a set of test images totaling 2.4 GB. Around 80% of the images were taken with a Sony A7 camera, with file sizes averaging around 15 MiB. The remaining 20% were smaller images with sizes ranging from 1 MiB to 5 MiB.The testing command is as follows:./webp-server-go -prefetchThe shorter the execution time, the better the performance in this aspect.Prefetch time on the Xeon server:Prefetching... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (438/438, 10 it/s)     Prefetch completeY(^_^)Y in 44.414660644sPrefetch time on CPX21:Prefetching... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (438/438, 6 it/s)     Prefetch completeY(^_^)Y in 1m9.87966334sDue to insufficient memory, CAX11 encountered an out-of-memory (OOM) issue and was not included in this round of testing.The Prefetch time for CAX21 is:Prefetching... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (438/438, 6 it/s)      Prefetch completeY(^_^)Y in 1m15.080679651sThe Prefetch time for Oracle is:Prefetching... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (438/438, 5 it/s)Prefetch completeY(^_^)Y in 1m24.932026118sFrom the results of the Prefetch, it can be seen that the 8-core Xeon(R) CPU E3-1230 v3 @ 3.30GHz dedicated server takes the lead here, with a conversion time of 44s, which is 36% faster than CPX21. However, since CPX21 has better Geekbench scores than Xeon, this also indicates that we cannot blindly rely on Geekbench scores as the sole indicator of performance comparison. It is necessary to consider our own business requirements.On the other hand, the performance of ARM64 is quite impressive. The 4-core CAX21 machine has a conversion speed that is only 8% slower compared to the 3-core CPX21, and the price difference between them is 14%. Additionally, CAX21 also has twice the amount of memory compared to CPX21.Disk TestingWhether it\u2019s Public Services or WebP Cloud, for subsequent requests, all images are served from cache. Our cache is persisted on disk, so testing the disk performance is crucial in this context.The tests mentioned above are included in the test commands using fio for performance testing. Four random read and write fio disk tests are conducted as part of this script with 4k, 64k, 512k, and 1m block sizes. The tests are designed to evaluate disk throughput in near-real world (using random) scenarios with a 50/50 split (50% reads and 50% writes per test).First, let\u2019s provide the disk performance of Oracle Cloud. The test was conducted on the machine\u2019s built-in disk.fio Disk Speed Tests (Mixed R/W 50/50):---------------------------------Block Size | 4k      (IOPS) | 64k      (IOPS) ------  | ---      ---- | ----      ----Read    | 75.84 MB/s  (18.9k) | 228.99 MB/s  (3.5k)Write   | 75.79 MB/s  (18.9k) | 235.80 MB/s  (3.6k)Total   | 151.63 MB/s (37.9k) | 464.79 MB/s  (7.2k)      |           |Block Size | 512k     (IOPS) | 1m      (IOPS) ------  | ---      ---- | ----      ----Read    | 143.03 MB/s  (279) | 141.04 MB/s  (137)Write   | 155.27 MB/s  (303) | 157.35 MB/s  (153)Total   | 298.31 MB/s  (582) | 298.39 MB/s  (290)Hetzner Cloud offers two types of disks: LocalSSD, which refers to the disks that come with the machines, and Volumes. Hetzner describes Volumes as follows:Volumes offer highly available and reliable SSD storage for your cloud servers. You can expand each Volume to up to 10 TB at any time, and you can connect them to your Hetzner cloud servers.Our Volumes are based on the networked block storage model, and every block of data is stored on three different physical servers at our Hetzner data centers.Under Hetzner Cloud, both AMD and ARM64 machines showed similar performance for LocalSSD and Volume. Therefore, the summary is as follows:The test results for LocalSSD are as follows:fio Disk Speed Tests (Mixed R/W 50/50):---------------------------------Block Size | 4k      (IOPS) | 64k      (IOPS) ------  | ---      ---- | ----      ---- Read    | 146.57 MB/s (36.6k) | 1.13 GB/s  (17.7k)Write   | 146.48 MB/s (36.6k) | 1.17 GB/s  (18.3k)Total   | 293.06 MB/s (73.2k) | 2.30 GB/s  (36.0k)      |           |           Block Size | 512k     (IOPS) | 1m      (IOPS) ------  | ---      ---- | ----      ---- Read    | 2.50 GB/s   (4.8k) | 2.65 GB/s   (2.5k)Write   | 2.71 GB/s   (5.3k) | 2.95 GB/s   (2.8k)Total   | 5.21 GB/s  (10.1k) | 5.60 GB/s   (5.4k)The test results for Volumes are as follows:fio Disk Speed Tests (Mixed R/W 50/50):---------------------------------Block Size | 4k      (IOPS) | 64k      (IOPS) ------  | ---      ---- | ----      ---- Read    | 29.70 MB/s  (7.4k) | 314.04 MB/s  (4.9k)Write   | 29.68 MB/s  (7.4k) | 323.38 MB/s  (5.0k)Total   | 59.38 MB/s  (14.8k) | 637.43 MB/s  (9.9k)      |           |           Block Size | 512k     (IOPS) | 1m      (IOPS) ------  | ---      ---- | ----      ---- Read    | 298.03 MB/s  (582) | 288.82 MB/s  (282)Write   | 323.52 MB/s  (631) | 322.23 MB/s  (314)Total   | 621.56 MB/s  (1.2k) | 611.05 MB/s  (596)It can be seen that although Volumes have the advantage of triple replication, the overall performance may only be about one-third of LocalSSD. Therefore, additional attention is required when planning applications, especially database applications.Additionally, we can observe a significant difference in speed between Hetzner\u2019s LocalSSD and Oracle Cloud\u2019s SSD.From the above results, we can see that if we don\u2019t want to go bankrupt, we won\u2019t consider AWS. Among the remaining ARM64 service providers, Scaleway doesn\u2019t guarantee SLA, so we are hesitant to choose them. Oracle Cloud\u2019s disk and processor performance are not satisfactory, and there is a possibility of account closure due to being on the Free Tier. Alibaba Cloud is closely tied to the Chinese company Alibaba, and as a European service provider, we won\u2019t consider them. Additionally, their prices are also very high. In the end, we have chosen Hetzner\u2019s CAX series machines as our server provider.The aforementioned tests were conducted on Hetzner ARM64 servers. Due to their excellent cost-performance ratio and our fondness for ARM64 machines, WebP Cloud Services has fully migrated to Hetzner\u2019s CAX series ARM64 processors at the time of this article\u2019s publication. Although we encountered some peculiar incidents during the migration, such as the CPU inexplicably spiking when using the alpine Clickhouse image, which was resolved by switching to a non-alpine image:Apart from that, ARM64 machines have performed well in terms of response latency and compatibility. If we make any new discoveries in the future, we will be sure to share them.If you\u2019re interested in Hetzner\u2019s ARM64 machines after reading this article, you can try using our referral link to sign up for Hetzner and experience it: https://hetzner.cloud/?ref=6moYBzkpMb9sBy registering through our link, you can directly receive a \u20ac20 credit upon successful registration, and we will also receive a \u20ac10 reward. This way, you can support the development of our product as well.However, it\u2019s important to note that Hetzner has strict risk controls. Using a VPN during registration or intentionally providing incorrect information may easily result in your account being banned. This can be seen as both a disadvantage and an advantage. The disadvantage is that the registration threshold is relatively high, but the advantage is that Hetzner\u2019s customers are relatively \u201cclean\u201d compared to mainstream service providers that offer large credit limits (such as DO and Vultr), without noisy and disruptive neighbors. Moreover, from our observations, once an account is successfully registered and has a few successful paid orders, there is generally no issue with account closure.WebP Cloud Services is a small team of three individuals based in Shanghai and Helsingborg. Since we are not seeking funding and have no profit pressure, we strive to do what we believe is right and do our best within the limits of our resources and capabilities. We also engage in various experiments and activities without compromising the services we offer to the public.ReferencesScaleway C2 and ARM64 instances will reach end-of-life in December 2020Scaleway ARM serversRETHINK YOUR CLOUD. RELY ON OUR NEW ARM64 CAX SERVERSScaleway Provides Dedicated ARM Servers for 10 Euros per Month, 0.02 Euro per Hour - CNX SoftwareEnglishHetzner\u2190Hetzner CAX \u7cfb\u5217 ARM64 \u670d\u52a1\u5668\u6027\u80fd\u7b80\u8bc4\u4ee5\u53ca WebP Cloud Services \u5728\u5176\u4e0a\u7684\u5b9e\u8df5",
        "summary": "- Hetzner's ARM64 servers perform well in webP conversion speed, only 8% slower than their AMD64 counterparts, with a 14% price difference and double the amount of RAM.\n- ARM64 architecture has potential in the server field, as shown by the success of Hetzner's CAX series servers and their use of Ampere Altra processors.\n- Hetzner offers the lowest price among popular service providers for ARM64 machines and provides more stability compared to Scaleway, AWS, Oracle Cloud, and Alibaba Cloud.",
        "hn_title": "Review of Hetzner ARM64 servers and experience of WebP cloud services on them",
        "original_title": "Review of Hetzner ARM64 servers and experience of WebP cloud services on them",
        "score": 265,
        "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginReview of Hetzner ARM64 servers and experience of WebP cloud services on them (webp.se)265 points by novakwok 19 hours ago | hide | past | favorite | 83 commentsthrowaway81523 10 hours ago | next [\u2013]TFA describes the E3-1230 as an 8 core server when it is actually a 4 core server with 8 threads. That means the ARM vs x86 per-core performance comparisons are off by a factor of 2. I stopped reading when I noticed that. For cheap sustained compute, it's hard to beat a Hetzner auction dedi.replynovakwok 6 hours ago | parent | next [\u2013]Thanks for pointing out, I've made some updates on blog post to make the description more accurate.replyksec 2 hours ago | parent | prev | next [\u2013]This has always been the case with vCPU. But many didn't know vCPU in many cases means thread and not Core.replyxrd 15 hours ago | prev | next [\u2013]I'm excited about arm in more places but my experience with arm and docker isn't as easy as i expectedIs it just me? When I've started using arm more, I've noticed that docker images are often incomplete or behind the x86 release cycle.I love the ease of wiring docker images together for all my services (corollary: never having to understand the myriad packaging issues with whatever language the service is written in, python, nodejs, etc).But when I'm using an arm image, often it is not the same version as the latest on x86, or even worse, is packaged by someone random on the internet. If I were to install the JavaScript service myself, I could audit it (not that I ever do!) by looking into the package.json file, or reviewing the source code, or whatever. There is a clear path to reviewing it. But with a docker image from the Internet, I'm not sure how I would assert it is a well behaved service. Docker itself gives me some guarantees, but it still feels less straightforward.I've packaged things for an arm container myself and it isn't always exactly the same as for x86.Is this just me? Am I doing it wrong on arm?replyshepherdjerred 14 hours ago | parent | next [\u2013]I felt this a year or two back, but today I've had as good of an experience on Docker w/ arm64 as I do w/ x86_64. I use arm64 Docker a lot since I work on a M1 MacBook.I usually stick to the common base images, e.g. ubuntu, alpine, nodejs, golang, etc. and install based off of that. Also, I rarely write Dockerfiles these days and instead use Earthly [0], which is a tool that really shines as a CI/make alternative, but it incidentally also has a nicer syntax which makes it easier to write multi-platform Docker images.What images or other problems have you ran into on arm64?[0]: https://earthly.dev/replyxrd 13 hours ago | root | parent | next [\u2013]For example gitlab. The latest arm image, as far as I could tell, isn't the same as the most recent x86. And, iirc, it was from some other person, not gitlab. It's often hard to tell what you are getting when you run an image, because docker pull can pull an image that isn't a multi platform build. I've had issues where the SSL certificates don't work, and I'm assuming it is because the stack could listen on 443, but the full ssl when running on arm didn't work. I'm not sure if that was because it is emulating using Rosetta or whether the software inside the container built correctly but isn't actually running on the arm platform correctly or what. It just feels like the wild west with arm images right now. I'm sure it will get better but it is still a minority platform and that comes with those issues.And, this might just be exposing my ignorance. Until recently I hadn't needed to use arm but now with macos it's gotten more interesting and more complicated.replyyjftsjthsd-h 10 hours ago | root | parent | next [\u2013]> And, iirc, it was from some other person, not gitlabThat would have to be a different image, then?replyxrd 10 hours ago | root | parent | next [\u2013]Yes, my memory is a bit foggy, but it was difficult to get any of the images to work, so I started playing with other contributors. But, you are right.replyjeroenhd 11 hours ago | parent | prev | next [\u2013]You can inspect the layers of a Docker image. Tools like dive[0] provide a quick and easy way to navigate through the different components your image of choice is made up of.In terms of functionality once the container is running, you'll have to put some amount of trust into the project maintainers, no more or less than the trust you need om amd64. For containers repackaged by third parties that's quite a pain, but in most cases you can get by just fine with the official container.If your container of choice has been made by someone real fancy, you may be able to get reproducible builds for all the files inside the container. That would verify that the source and the binary match (though container metadata may not, so a direct image compare would be challenging).[0]: https://github.com/wagoodman/divereplypjmlp 8 hours ago | root | parent | next [\u2013]Dive seems to have been abandoned though.I used it a few times in the past.replyrendaw 4 hours ago | root | parent | next [\u2013]Does it no longer work? I thought I used it just fine a couple weeks ago.replyjrockway 10 hours ago | parent | prev | next [\u2013]This sounds about right to me. At work, we make a rather complex stack that uses quite a few third-party containers. When we wanted to do arm64 support a couple years ago, most of these dependencies did not support arm64, so we had to build and publish the containers ourselves. (We already sort of had to do this anyway, because customers ran into Docker rate limiting issues, and images from our account aren't rate limited because we pay them not to. But when we only supported amd64, we just re-tagged and pushed.)As an aside, some comments in this thread say \"just look at the layers\", but that's the wrong level of abstraction for multi-arch images. In the past, when you ran \"docker pull ...\" you were looking for an Image Manifest: https://github.com/opencontainers/image-spec/blob/main/manif.... But now in the world of multi-arch, you are getting an Image Index first: https://github.com/opencontainers/image-spec/blob/main/image...replyfzeindl 9 hours ago | parent | prev | next [\u2013]> never having to understand the myriad packaging issues with whatever language the service is written in, python, nodejs, etc)How do you fix issues with the docker images if you don't understand them?replypredictabl3 3 hours ago | parent | prev | next [\u2013]Sounds like a docker issue, not an ARM issue. My full desktop NixOS config builds for x86_64-linux and aarch64-linux. It even about 90% cross compiles, possibly just one \"external\" package that isn't setup right for it. And actually that might even be fixed, I just saw a cross-compilation fix go in today.replymorrbo 14 hours ago | parent | prev | next [\u2013]You're not wrong...but it will get there and get better. I believe asahi will be a driving force behind it, and arm in general being more widely used for non mobile device stuff....however (despite using fedora on arm64 as a daily driver) I firmly believe we're going to be 6-12 months absolute minimum until arm docker is \"alright\" (I'm also broadly including fully user transparent x86 emulation into this sweeping statement with no basis lol)replybsnnkv 13 hours ago | parent | prev | next [\u2013]The friction with using Docker across arm and x86 was one of the big reasons that I ended up learning NixOS. Now, all the services on my personal remote box and all my one-man-SaaS services run on NixOS + systemd services and my life is so much easier and less stressful.replyacdha 11 hours ago | parent | prev | next [\u2013]I don\u2019t have too many gaps but also don\u2019t use that many different base containers for security and reliability reasons. As you mentioned, I feel like in a decade the current experience of running random code from strangers all over the internet with no more protection than Docker Desktop provides is gong to sound similar to 1970s swingers\u2019 accounts of unprotected orgies sound to all of us who grew up after HIV, etc. where people will kind of accept that it happened but be amazed that everyone was so reckless.replyPlasmoid 7 hours ago | parent | prev | next [\u2013]One thing that made arm on docker much easier was by using the kubernetes builder for docker. Spin up an arm nodes in kubernetes, create the docker builder pod, and it'll build/push your docker image easy as can be.replyharha_ 10 hours ago | parent | prev | next [\u2013]I haven't had problems because I just build images myself, using images such as alpine as a base.replyarjvik 8 hours ago | parent | prev | next [\u2013]I face similar problems running a docker-based NAS on a Raspberry Pi. But I end up just building the official images myself on the Pi (or on my dev machine with qemu) from the open source Dockerfile of the official image.replyznpy 11 hours ago | parent | prev | next [\u2013]I\u2019m not excited that much yet, because except for dumb single board computers, i still can\u2019t get a proper arm system to run at home.My home server (a repurposed fujitsu esprimo q920) is still intel based and it doesn\u2019t seem to be anything available with comparable performance and connectivity. And I\u2019m not even considering the price point.Basically: arm cpus don\u2019t play any significant role in my everyday computing life.At work, I\u2019ve been migrating all of our infra to graviton and we realised substantial savings\u2026 but then again, I don\u2019t pay the cloud bills and my salary is still the same, so meh.replytmikaeld 18 hours ago | prev | next [\u2013]Great article, thanks for sharing!We're using Hetzners new ARM servers ourselves, to convert images to WebP (Yes, your company name is really confusing!) and they perform almost as good as the Hetzner AMD instances.But since they're so much cheaper, we can easily fire up many of them and use a load-balancer in front, saving a ton of money compared to dedicated servers.replynovakwok 18 hours ago | parent | next [\u2013][Yes, your company name is really confusing]LOL, we're not a company, we are just a small team of three individuals(Nova Kwok,Benny Think and Tuki Deng).[convert images to WebP]May I ask your use case on this? (As we've recently launched a product called WebP Cloud might fit this need. (And we're actively seeking seed users.))WebP Cloud documentation here: https://docs.webp.se/webp-cloud/replydsign 11 hours ago | root | parent | next [\u2013]Ha! Another one :-) !We created a company that does something similar[^1]. The tech was great and the company is profitable, but the market is really, really tough, with incumbents (read: existent CDNs) playing all sort of \"standard business practices\"[^2] to keep customers in their more expensive business. And yes, in this line of business you really want the cheapest hardware.[^1]: Support for transcoding images to WebP, AVIF, JpegXL, and selecting on the flight the best format for serving individual images in a website. Company (ShimmerCat AB, a Swedish registered company) is currently for sale; contact the CEO if you want a bargain[^3], last time I heard ask price was X0 000 USD, with X less than 9. I'm not part of the company in any capacity any longer.[^2]: Read: standard dirty tricks to suppress the competition.[^3]: Who is the CEO is public in the Swedish registry of companies.replytmikaeld 15 hours ago | root | parent | prev | next [\u2013]Alright, they way it was mentioned in the article made it sound like a business, sorry about that.Your service looks great, but we long since concluded that using an API for image conversion would be many times more expensive than using our own setup. And we also have mixed in fetches from external sources, storage in S3, Cloudflare workers and generative AI mixed in the bag - no single service supports all that yet (hint).replyadventured 13 hours ago | root | parent | next [\u2013]> they way it was mentioned in the article made it sound like a business, sorry about thatIt is a business. They're selling a service. I don't know why they're protesting at the notion of being a company, they're de facto a business (selling service behind a brand, which they're openly promoting to sell more services).replynovakwok 13 hours ago | root | parent | next [\u2013]Hmmm, maybe calling this a start-up/business might be more appropriate?(WebP Cloud Services starts by providing a free service of Gravatar/GitHub Avatar reverse proxy with WebP optimization at first, and now it's our first attempt to make a paid services of private proxy as more of our users want this to be a more generally available service.(And we are currently not a company indeed) \u00b4\uff65\u1d17\uff65`No intentional protesting at the notion of being a company, just unsure if \"company,\" \"business,\" and \"startup\" have the same meaning in certain contexts.replyrat9988 9 hours ago | root | parent | next [\u2013]The moment you started providing a paid service you became a business. The legal status, as in company, independant, or whatever, depends on your local laws.replypbiggar 7 hours ago | root | parent | prev | next [\u2013]If you're planning to build a business together, forming the company ASAP is a good plan. Recently talked to some founders who split up before they incorporated, and it was a mess.replynovakwok 6 hours ago | root | parent | next [\u2013][If you're planning to build a business together, forming the company ASAP is a good plan.]Do you have any advice in this regard? We do have a preliminary plan to register a European company in Estonia (through e-Estonia) after achieving good revenue to continue our operations.replypbiggar 5 hours ago | root | parent | next [\u2013]You absolutely must do this BEFORE any sort of revenue. It should be the first thing you do.You need a company to own things, such as the IP (code, trademarks, website, customer lists), as well as being the thing to which revenue is paid. You'll also find you can't do most things without it (such as get a credit card, office lease, cloud discounts, etc).Most importantly, suppose you have a cofounder break up when you have just started getting \"good revenue\" but haven't yet got a company. Who's is that revenue? Who owns the code you wrote? A complete mess.I don't know anything about e-Estonia, but if they allow you to sign up today, no reason not to do that. In the US (or abroad if you want a US company), Stripe Atlas is a good option. That might work for you too.replymduggles 19 hours ago | prev | next [\u2013]I\u2019ve been migrating workloads away from x86 and towards ARM on AWS and GCP since they\u2019ve been available. This review does a great job of kinda giving you an idea of what you are gonna get as a platform, but if you are interested I strongly recommend the experience on any cloud provider.While there was some work to benchmark and validate, the cost savings have been non-trivial. Plus this change happened as we were all switching to the M series Macs so ironically now our entire chain end to end is off x86.replyaidos 17 hours ago | parent | next [\u2013]For us it was driven in the other direction. With the introduction of the M1s we knew that we\u2019d be on arm locally soon enough. There was a bit of work in the transition but things have improved since then. Definitely happy running on all arm now though.replyArnavion 19 hours ago | parent | prev | next [\u2013]Alas all my stuff is in Azure, and I'm still waiting for them to offer smaller VM sizes comparable to their existing B line. I currently use a B1s (1 CPU 1GiB) that comes to ~$5/mo while the cheapest ARM VM would be ~$25/mo (2 CPU 4GiB).replydijit 16 hours ago | parent | prev | next [\u2013]I was keen on migrating to ARM, but there seems to be no benefits from doing so on GCP; I'm open to be wrong here.From what I understand they're using Ampere Altra, which have single thread performance similar to Skylake; but the cost is equivelant or worse than the x86 e2 series.e2-standard-4: USD 97.84/mot2a-standard-4: USD 112.42/mo(sustained use discounts apply to neither).EDIT: I see you're in Denmark and are operations focused. I am too operations focused and just across the bridge in Malm\u00f6, maybe we could hang out.replymduggles 14 hours ago | root | parent | next [\u2013]Yeah sorry I should have been more clear. Currently the ARM instances in GCP when you use them as spot basically never get interrupted. We\u2019re big into GKE so use them as a preferred node group for interruptible pods. I assume due to the pricing you mentioned usage is very low.So basically any background jobs or big batch processing jobs that required a lot of CPU time. We have multi-arch container builds so if we can\u2019t scale out the ARM node group not a problem, go back to x86. But it was worth the optimizing to get effectively always available spot instances.Yeah always open to meet up with folks. I\u2019m on mastodon at matdevdug@c.im.replyjohncolanduoni 11 hours ago | root | parent | prev | next [\u2013]T2A vCPUs are full cores though right? While E2 and most other instances are hyperthreads.replybennythink 11 hours ago | root | parent | prev | next [\u2013]Actually I\u2019m in Sweden. Of course we could hang out in sometime. Just cross the bridge. Here\u2019s my email emeries-atolls.0w@icloud.comreplyre-thc 15 hours ago | root | parent | prev | next [\u2013]The real hidden gems of GCP are the 90% off spot instances in a lot of regions for e.g. N2D.ARM makes 0 sense on GCP if you can use those.replyHamuko 18 hours ago | parent | prev | next [\u2013]I just refer to ARM Lambda runners as a free 20% discount since it makes absolutely no difference in runtime but costs less.I'd also run ARM database instances but I think those are still not really that readily available.replympweiher 17 hours ago | prev | next [\u2013]Since moving to Apple Silicon, I've been wanting more ARM options in the cloud. Although it is possible to host x86_64 VMs, having fewer differences is obviously better.I've been using Oracle's free tier for a while, and it's been OK. Performance-wise, my Objective-S and lib\u00b5httpd based web-server appears to be doing around 1800 requests per second, and held up fine to a HN hug of death.Hetzner was far, far easier to set up, both from their console and via the API. Performance was comparable.replyshepherdjerred 14 hours ago | parent | next [\u2013]AWS has great support for arm64 instancesreplybrian_cunnie 16 hours ago | prev | next [\u2013]I like the article, but I wish there had been an \"Abstract\" or \"Executive Summary\" at the top so that I'd be spared having to read the entire article to find out the results. I'd like to have seen something along the lines of the following:\"We found Hetzner's ARM64 offering, specifically the CAX21 with 4 cores, 8GB at $8.40/month, to be a performant and cost-effective alternative to x86_64-based solutions.\"replylangsoul-com 16 hours ago | parent | next [\u2013]Also add, based on tests arm performed 8% worse than amd64, but this is offset by the 14% savings.replypetercooper 15 hours ago | parent | prev | next [\u2013]Also, notably, the team who did the benchmarking were impressed enough to have actually switched entirely to said CAX instances for their app.replynovakwok 14 hours ago | parent | prev | next [\u2013]Good idea! I've added some TL;DRs at the beginning of the article.replydaneel_w 18 hours ago | prev | next [\u2013]According to Oracle's documentation their Arm servers are not virtualized cores but instead actual on-core tenancy, referred to as OCPU instead of conventional vCPU.https://blogs.oracle.com/cloud-infrastructure/post/vcpu-and-...replynezirus 17 hours ago | prev | next [\u2013]Interesting read. I'd like to know more about alpine problems (even just to confirm my bias against it, unless space savings are the most important thing).For me, Hetzner is mostly baremetal provider. They have dedicated RX line, and if you have base load, a couple of those could run it all (use hetzner cloud instances for scalling and failover)replynovakwok 16 hours ago | parent | next [\u2013][I'd like to know more about alpine problems]Sure, and we're planning to share another post later on the whole procedure of our migration from AMD64 to ARM64, and in that post we'll include more details about Clickhouse's problem if we can definitively establish that the problem was caused by alpine. (After this incident I personally have bias against alpine images tooComparing alpine and non-alpine images on DockerHub:https://hub.docker.com/layers/clickhouse/clickhouse-server/2...https://hub.docker.com/layers/clickhouse/clickhouse-server/2...There is just ~66MB(255MB vs 321MB) of size difference, my personal advice after this to to avoid alpine images in production as much as possible :PreplyFlyingSnake 16 hours ago | prev | next [\u2013]This is a great article and it\u2019s nice to see we\u2019ve lots of alternatives to run ARM servers.I ran the now defunct Scaleway ARM server mentioned in the article for several years. For \u20ac2,99 it was a surprisingly useful machine. I ran several projects (.net core) on it and it was quite good for those simple workloads. I looked for alternatives for a while but nothing turned up until Apple restarted the ARM revolution with M1.replyEVa5I7bHFq9mnYK 14 hours ago | prev | next [\u2013]I've been using a cax41 (16 cores) instance for numerical computations recently. Geekbench scores are 774/10221, costs $0.04 hourly ($27 monthly). Perfectly stable. No throttling (probably not that popular yet hehe). For my specific program it's 10% slower than my laptop's 11980HK processor (8 threads, 16 hyperthreads).replynikita2206 11 hours ago | parent | next [\u2013]I\u2019m always so taken aback when I compare VM prices from Hetzner/OVH and AWS/GCP.Similarly sized machine in AWS seems to be around $300 monthly, that\u2019s 10x cost.replyjeroenhd 10 hours ago | root | parent | next [\u2013]Amazon/Google has fallbacks across regions, several layers of data storage redundancy, high-speed. highly configurable software based networking and so much more.Hetzner/OVH has machines with almost no failover, with no extra availability zones, with no backup guarantees, very little in the way of custom networking, and doesn't integrate with dev tools quite as much.They're different products. For most people, going Amazon/Google makes no sense. However, if you absolutely MUST keep your data available after or during a fire [0] and keep your systems running during datacenter downtime, you're better off with AWS/GCP/Azure. SLAs with many nines can't afford cheap servers, and that's where the big cloud providers make a lot of money.Up until recently I saw a lot of people and companies move back from the cloud to self-managed dedicated hardware in data centers. All most companies need is half a rack in two places and a competent sysadmin team, but externalizing the risks is often attractive because disasters and bad failovers do happen sometimes.[0]: https://www.datacenterdynamics.com/en/opinions/ovhclouds-dat...replynikita2206 10 hours ago | root | parent | next [\u2013]Absolutely no arguing that AWS adds more value.Another thing about AWS/GCP, they are also good at locking you in. For example you want to shift some workloads to Hetzner while leaving others in AWS, you will get a bill for egress out of AWS.replyqingcharles 13 hours ago | prev | next [\u2013]Wow, this is timely. I just bought their cheapest one last night (about $4/mo) to play with and performance test it for ASP.Net Core, vs. their x86 boxes.I tried to be ultra cheap and not buy a v4 IP but it appears Microsoft doesn't have v6 IPs on all their download servers which is causing me pain.replyrankun203 5 hours ago | prev | next [\u2013]I\u2019ve been using Hetzner\u2019s EX line for some years, it\u2019s super cost effective, until now I still can\u2019t find any other provider with cheaper offerings.replyNorwegianDude 11 hours ago | prev | next [\u2013]Weird using a E3-1230 v3 in 2023, it's over 10 years old. A similar modern low end CPU would be many times faster.replysmarx007 11 hours ago | parent | next [\u2013]I guess they used whatever CPU gives a relative price parity with the VMs: https://www.hetzner.com/sb?country=dereplyjustsomehnguy 10 hours ago | parent | prev | next [\u2013]Depends on the task/load.Modern low end wouldn't be many times faster, at least not with a lower core/thread counthttps://ark.intel.com/content/www/us/en/ark/products/75054/i...replyfoobarbazetc 13 hours ago | prev | next [\u2013]Their RX-220 servers are also amazing.Ampere 80 core machines for $220/m.We use these for anything requiring a lot of threads.replyarcherx 18 hours ago | prev | next [\u2013]I have been developing on ARM servers for a while. I use Raspberry Pis and Tinkerboards as dev and staging servers and push releases to an x86-64 server on digital ocean. With docker it has been pretty easy, docker-compose usually finds the right packages for the CPU and it works quite well. I am curious about maybe trying on of the ARM servers on Hertzner and see how it compares.replyPaoloBarbolini 18 hours ago | parent | next [\u2013]I've been trying their Arm servers for a while and I've noticed some differences in the colors in htop for Debian 12, as if there were a slight difference between the x86_64 and the aarch64 image. Other than that everything's going fine and I'm planning to use Arm for every server in the Falkenstein datacenter (the only one with Arm dedicated and cloud servers for now)replyarcherx 16 hours ago | root | parent | next [\u2013]Nice, how\u2019s the performance? The cheapest digitalocean x86 single core cpu is a lot faster than a quad core pi or tinkerbord. I know it\u2019s not the same as an arm server cpu but how much difference is there?replynovakwok 16 hours ago | root | parent | next [\u2013]I've searched for Geekbench result: https://browser.geekbench.com/v6/cpu/1584694, it says DO-Premium-Intel 1 Processor, 1 Core, so I'm assuming it's the 7USD/mo plan from https://www.digitalocean.com/pricing/droplets#basic-droplets.The score on Geekbench is Single-Core: 838,Multi-Core: 842.While in our tests the cheapest ARM64 plan on Hetzner is CAX11 2Core, 4G RAM, about 5USD/mo, the Geekbench result is Single-Core: 1072,Multi-Core: 1921, so assuming it about 20% faster than DigitalOcean.We've done the same test on Rpi4B too:Processor : Cortex-A72CPU cores : 4 @ 1500.0000 MHzScore is Single-Core: 247,Multi-Core: 387For your reference.replymike503 8 hours ago | prev | next [\u2013]That's interesting. I feel like I see benchmarks almost always showing ARM outperforming for all kinds of specific workloads. This is the first one I can recall showing it's not as good performance-wise, however when you add the power efficiency, cost savings, it winds up being better overall.replykramerger 18 hours ago | prev | next [\u2013]Great no nonsense article!I'm surprised how bad xeon scales to 8 cores. But isn't the xeon instance the only one not running bare metal?? Maybe he is paying for 8 cores but gets only 2-4 physical cores?replyadrian_b 18 hours ago | parent | next [\u2013]That \"Xeon\" is a very old (10-year old) quadruple-core (8-thread, i.e. 8 \"virtual CPUs\") desktop Haswell CPU rebranded as \"Xeon\". A current Intel NUC Pro with a Core i3 CPU would be a much faster (67% faster ST, 43% faster MT) dedicated server than this one and it would cost to own less than $500 with DRAM and SSD, so about $8 per month for a 5-year lifetime (so the performance per $ would be at least 5 to 6 times higher than that of the compared Intel server).That \"Xeon\" is a good comparison point only because it was available for them in the same price range, not because it would be representative for the performance of any modern x86 CPUs. Also the \"Epyc\" is probably a very old model.Somebody who wants to spend their money for cloud services as efficiently as possible should better ensure that it is possible to migrate back and forth their applications between x86 and ARM instances, because which one is cheaper for a certain performance at a given time depends a lot on non-technical reasons, so it is unpredictable which will be cheaper a few months later.replynovakwok 18 hours ago | parent | prev | next [\u2013]@kramerger No, Xeon Server is a dedicated server(a.k.a Bare metal), I've looked at it's console and found it's Dell PowerEdge R220(Motherboard Dell Inc. 081N4V).I'm quite confused about it's performance as well.CPU Info Name Intel Xeon E3-1230 v3 Topology 1 Processor, 4 Cores, 8 ThreadsGeekbench Link is at: https://browser.geekbench.com/v6/cpu/1533259replymkl 17 hours ago | root | parent | next [\u2013]That CPU seems to be from 2011: https://ark.intel.com/content/www/us/en/ark/products/52271/i...replyadrian_b 12 hours ago | root | parent | next [\u2013]Your link is wrong, it points to an E3-1230 (v1) from 2011 (Sandy Bridge), while the tested CPU was E3-1230 v3 from 2013 (Haswell).Decoding the Intel product names requires experience, because one or two letters or digits added or deleted can change very much the characteristics of the product. Two such products differing in one letter might have a five times difference in performance.Not that it matters much, because even an only 10-year old CPU is still ancient.replyspaniard89277 18 hours ago | prev | next [\u2013]In terms of software hiccups, for someone with little time to debug, is it worth the cost savings?replyadql 17 hours ago | parent | next [\u2013]If you're not using proprietary software but common programming languages and OSS tooling there should be no difference.replyCathalMullan 10 hours ago | prev | next [\u2013]CAX11 looks like a great deal, especially with IPv4 disabled.replythrowaway2990 18 hours ago | prev | next [\u2013]> Hetzner CAX11, with a virtualized ARM64 processor, 42 cores, 4GB memory, priced at $4.91 USD per month, referred to as CAX11 for simplicity.Haha I wish it was a 42 core for $4.91Small typo for them to fix.replynovakwok 18 hours ago | parent | next [\u2013]Thanks for pointing out, now fixed!replyCodesInChaos 16 hours ago | prev | next [\u2013]At least two links don't work because they contain a closing parentheses.replynovakwok 15 hours ago | parent | next [\u2013]Thanks for pointing out! Now fixed.replye145bc455f1 17 hours ago | prev [\u2013]How do you get your account verified at hetzner without sending a government ID to them?replypulpfictional 16 hours ago | parent | next [\u2013]Pay 20\u20ac up-front through PayPal. This becomes available as credit, a top-up in a sense.replye145bc455f1 13 hours ago | root | parent | next [\u2013]They have disabled my account, i can't even login anymore.replypulpfictional 13 hours ago | root | parent | next [\u2013]Ah sucks. From what I hear their support will probably not help you register but it's worth a shot.replytehlike 4 hours ago | root | parent | prev | next [\u2013]Get a new account.replynovakwok 17 hours ago | parent | prev [\u2013]I've sent my passport image to them to get my account verified.(When the second time I register Hetzner)(My first attempt on registration got my account closed even I've provided by passport.(maybe it's because I've used VPN for registration as it's website is too slow to open in China(might caused by china GFW)replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
        "hn_summary": "- The article discusses the review of Hetzner ARM64 servers and the experience of using WebP cloud services on them.\n- The E3-1230 server mentioned in the article is actually a 4-core server with 8 threads, not an 8-core server as described.\n- There are some challenges when using ARM and Docker, as ARM images may be incomplete or behind the x86 release cycle.\n- Some users have had positive experiences using ARM64 Docker images and have found them to be as good as x86 images.\n- There may be discrepancies between ARM and x86 images, and it can be difficult to ensure that ARM images are well-behaved services.\n- Hetzner's ARM64 servers offer cost savings compared to x86 servers, making them an attractive option.\n- The performance of ARM64 servers on Hetzner is comparable to x86 servers, and users have been impressed with the performance.\n- Hetzner's ARM64 servers are a cost-effective alternative for running workloads, and users have switched entirely to these servers for their applications.\n- Hetzner's RX-220 servers, which use Ampere 80 core machines, are highly recommended for workloads that require a lot of threads.\n- Hetzner's ARM64 servers provide performance comparable to digital ocean's x86 servers at a lower cost.\n- ARM servers have the potential for cost savings and power efficiency compared to x86 servers."
    },
    {
        "id": 36366002,
        "timestamp": 1686963199,
        "title": "Merging bcachefs",
        "url": "https://lwn.net/SubscriberLink/934692/5046d466490d9220/",
        "hn_url": "http://news.ycombinator.com/item?id=36366002",
        "content": "LWN.netNews from the sourceContentWeekly EditionArchivesSearchKernelSecurityEvents calendarUnread commentsLWN FAQWrite for usUser: Password: | |Merging bcachefs[LWN subscriber-only content]Welcome to LWN.netThe following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!By Jake EdgeJune 16, 2023LSFMM+BPFThe bcachefs filesystem, and the process for getting it upstream, were the topics of a session led remotely by Kent Overstreet, creator of bcachefs, at the 2023 Linux Storage, Filesystem, Memory-Management and BPF Summit. He has also discussed bcachefs in previous editions of the summit, first in 2018 and at last year's event; in both of those cases, the question of getting bcachefs merged into the mainline kernel came up, but that merge has not happened yet. This time around, though, Overstreet seemed closer than ever to being ready to actually start that process.He began his talk by noting that he had been saying bcachefs is almost ready for merging for some time now; \"now I'm saying, let's finally do it\". He wanted to report on the status of the filesystem and on why it is ready now for upstreaming, but he wanted to use the bulk of the session to discuss the process of doing so. \"It's a massive, 90,000-lines-of-code beast\" that needs to get reviewed, so there is a need to figure out the process to do that review.His goal with bcachefs is to have the \"performance, reliability, scalability, and robustness of XFS with modern features\". That's a high bar, and one that bcachefs has not yet reached, but \"I think we're pretty far along\". People are running bcachefs on 100TB filesystems \"without any issues or complaints\"; he is waiting for the first 1PB filesystem. \"Snapshots scale beautifully\", which is not true for Btrfs, based on user complaints, he said.StatusIn the last year, there has been a lot of scalability work done, much of which required deep rewrites, including for the allocator, which dates back to bcache. There is a new \"no copy-on-write\" (nocow) mode and snapshots have been implemented. People are using the snapshots to do backups of MySQL databases, he said, which is a test of the robustness of the feature.Erasure coding is the last really big feature that he would like to get into bcachefs before upstreaming it. But he thinks \"it's time to draw a line in the sand\", so that can wait for a bit. There is still a lot of work to do, but \"the big feature work is lessening\"; he will be able to work on being a maintainer without having to disappear for a month to work on something, as he did for snapshots, for example.The bcachefs team is growing; Brian Foster at Red Hat has been doing a lot of great work on bug fixes, Overstreet said. Eric Sandeen has helped in attracting interest in bcachefs at Red Hat as well. There is a bi-weekly call on bcachefs development. There is automated testing infrastructure that has been added and it is \"making my life much easier\", Overstreet said. The test system runs in about half an hour and includes multiple runs of fstests as well as the \"huge test suite\" for bcachefs.Rust is something that he has been evangelizing about to \"anyone who will listen\"; he thinks \"writing code in C, when we finally have a better option available, is madness\". He loves to write code, but not to debug it; writing in Rust \"just means a lot less time debugging\". He intends to slowly rewrite bcachefs in Rust, which will be a ten-plus-year project, but the use of Rust in bcachefs has already started. Some of the user-space tools have been rewritten in Rust and someone is looking at moving some of that work into the kernel.UpstreamingThat morning he had posted 32 preliminary patches adding infrastructure that bcachefs will need; those patches were already being reviewed, he said. The rest is 90,000 lines of code in 2,500 patches that he did not post; he did include a link to his Git repository, where those patches live in a bcachefs-for-upstream branch. He then opened up the floor to discuss how those patches would be reviewed and, eventually, merged.Josef Bacik said that he thinks the response will be much the same as last year; filesystem developers are \"really excited\" to see bcachefs get merged. He does not plan to review the implementation of the filesystem itself and suspects that is generally true. The people who are working on it will review it; \"trust yourselves for that part\". The \"generic stuff is what we need to review\", once that is done, the rest of the filesystem code can be merged as far as he is concerned. That is, of course, up to Linus Torvalds.Overstreet said that one of his questions is: \"what do we take to Linus?\" He has spent the last year on process and infrastructure, getting a team together, working with Red Hat, putting together an automated test suite, and so on. Mike Snitzer remotely pointed out that a patch set that had recently been rejected contained two enormous patches that were essentially impossible to review; he contrasted that with the 2,500 fine-grained patches that make up bcachefs, which is much easier to digest.While Snitzer is not sure that having everyone go through them one-by-one in review is the right approach, the obvious effort that went into that patch series makes it easier to trust the code and the process that went into developing it. \"You've done the heavy lifting by doing all of that work to split up patches.\" Overstreet said that it was a lot of work to rebase nearly the entire history, but that it came in handy around six months ago when Red Hat noticed some big performance regressions. He was able to use that history to do automated bisection and got almost all of the performance back.Bacik said that Torvalds is the \"maintainer\" responsible for merging a new filesystem, so it will be up to him to decide if he is willing to pull the full history into the mainline. It would be Bacik's preference to do so, because the history is \"super useful\", but that is not something that the people in the room can decide. He suggested that the pull request be more of a question about whether the full history was acceptable and, if not, what would be.One concern is that once bcachefs gets merged, it will be difficult for anyone besides Overstreet to deal with the bug reports, Amir Goldstein said. It is important that it be explained in the pull request; \"I want to merge this and I have a team that can support this\". Getting more help was one of the criteria before upstreaming, Overstreet said. He knew that if it was a one-man show and he got deluged with bug reports, he would \"go insane and run away to South America\"; Foster has been \"a huge help\", which is one of the things that makes him feel comfortable about merging at this point.Paradoxically, the recent push to remove some filesystems (e.g. ReiserFS) from the kernel is actually going to make it easier to add new ones, Ted Ts'o said. He can remember Hans Reiser being enthusiastic about his new filesystem, with a team to support it, but that all fell into disrepair over the years. The kernel project now has a path for removing filesystems after a deprecation cycle. The idea that \"accepting a filesystem isn't forever, makes it a whole lot easier\" to merge new ones.He also suggested breaking up the patch series into smaller, more reviewable chunks that collect up a small number of related patches. That would make it easier for people to review, say, all of the lockdep patches in one chunk. It would mean relaxing the general guideline about not merging infrastructure until its first caller is merged, which he is in favor of; he would amend that guideline to allow merging when it includes a pointer to the Git tree of the first caller.Overstreet thinks that the preliminaries that he posted earlier that day will not be too controversial and other than perhaps one or two \"will just sail through\". He noted that Christoph Hellwig had objected to the vmalloc_exec() patch, though that functionality is needed for bcachefs, Overstreet said. Since the talk, Mike Rapoport has proposed the JIT allocator, which would solve the underlying problem.A remote participant said that Foster's experience had shown that the code base is approachable; once bcachefs is available, interested developers will be able to come up to speed and start working on it with few difficulties. Christian Brauner asked that there be a clear delineation for who else could step in and merge patches if Overstreet is unavailable. Brauner noted that the NTFS/NTFS3 maintainer disappeared and, even though there were people who were contributing to the filesystem, it was not clear \"who could route patches upstream\". Overstreet said that he would trust Foster in that role if \"he is willing to step up to that\".Brauner said that he thinks bcachefs is in \"excellent shape to be upstreamed\", but he is concerned with the number of filesystems in the kernel; he is glad to see that there are efforts to remove some of them. Changes that impact all of the filesystems in the tree \"get painful very very fast\" and, in some cases, there is no one available to review the changes. He would like the acceptance process to be more conservative; accepting NTFS/NTFS3 was \"a huge mistake\", for example. Brauner said that none of that was directed at bcachefs, but was a more general concern; filesystem acceptance and deprecation was taken up in a lightning talk (YouTube video) later that day.Darrick Wong said that he had already started doing what Ts'o suggested in his patches for XFS online repair. He has a collection of infrastructure patches that refer to callers that are coming soon; he has convinced Dave Chinner that there is value in reviewing the infrastructure pieces while also looking at the bigger picture of where it is all leading. That helps him because he can stop \"rebasing things repeatedly and having to play code golf, like moving small helper functions up and down in the patch set\". Putting all of that stuff in a separate set of infrastructure patches helped him, though it did cause some complaints from reviewers, but there is now some precedent for that approach, he said.Overstreet said that he is not particularly concerned about the 30 or so \"relatively uncomplicated\" infrastructure patches that he needs to land. He is going to wait for the Acked-by and Reviewed-by tags to come in, but if they do not, then he will use the suggested approach \"as a Plan B\". With that, the session came to a close.Index entries for this articleConference Storage Filesystem & Memory Management/2023(Log in to post comments)Merging bcachefsPosted Jun 16, 2023 21:23 UTC (Fri) by zdzichu (subscriber, #17118) [Link]Bcachefs came from bcache, which is solely focused on speeding up HDDs by using NVMe or SATA SSD as a cache layer. Is this functionality still relevant? Solid state drives are affordable in multi-TB range now.Merging bcachefsPosted Jun 17, 2023 0:41 UTC (Sat) by flussence (subscriber, #85566) [Link]It's definitely still relevant as long as storage devices on a single system can differ in latency/throughput by orders of magnitude. Whether it'll still be maintained is another question, but there's always the possibility of extending fscache+cachefilesd to support local filesystems if not.Merging bcachefsPosted Jun 17, 2023 7:03 UTC (Sat) by Kangie (subscriber, #161139) [Link]Yes. I'm currently running an ~100TB bcachefs pool consisting of 90tb SAS spinning rust and 24tb SATA SSDs.Any use case between 'wanting a decent CoW file system on a single disk / partition' to 'combining two disks in a desktop better than btrfs in a way that will be mainlined' and frankly anything short of true multi-tiered mass storage is where this is going to be most useful.It's incredibly promising - better than btrfs with no CoW write hole and actually able to be mainlined unlike zfs. It's also possible to easily resize, unlike a running zfs system.Seriously, exciting stuff all around.Merging bcachefsPosted Jun 17, 2023 9:45 UTC (Sat) by jengelh (subscriber, #33263) [Link]>Rust is something that he has been evangelizing about to \"anyone who will listen\"I'd wish developers would spend less on (fanatically) evangelizing the toolchains used, and rather push a project on own merit (e.g. features).Just because you made a painting with a $10k horsehair brush and golden paint does not necessarily make it great art.Merging bcachefsPosted Jun 17, 2023 11:15 UTC (Sat) by tux3 (subscriber, #101245) [Link]I'd think that sword ought to cut both ways.If the brush does not matter, and the art is not to your liking, then criticize the art. If the brush doesn't make it good, then the brush also doesn't make it bad.No one's saying it's good because of the new lang. Symmetrically, new lang evangelism oughtn't be a reason to dismiss it.There's an entire new filesystem's worth of artwork to critize on its own merit here, if you pleaseMerging bcachefsPosted Jun 17, 2023 14:20 UTC (Sat) by koverstreet (subscriber, #4296) [Link]You seem to be under the mistaken impression that tools are just fads, or something of no consequence.A good craftsman cares about his tools. Listen to woodworkers who get together, they'll be talking about their tools just as much as the actual work. We're interacting with these tools every day, and the quality of the tool very much affects the quality of the work.Merging bcachefsPosted Jun 17, 2023 14:25 UTC (Sat) by jengelh (subscriber, #33263) [Link]Even a pencil gets a good artist a long way, that's my point.Merging bcachefsPosted Jun 17, 2023 14:44 UTC (Sat) by koverstreet (subscriber, #4296) [Link]Conferences are for programmers to talk to other programmers. I don't think you have a point.Merging bcachefsPosted Jun 17, 2023 20:48 UTC (Sat) by flussence (subscriber, #85566) [Link]Ooh, a tortured analogy. Let's flog it to death and put it out of its misery.A crayon scribble may satisfy a lot of people, but the rest of us don't really accept that VFAT and NTFS written in a pre-C99 toolchain are the limit of what's possible.Merging bcachefsPosted Jun 18, 2023 1:42 UTC (Sun) by dvdeug (subscriber, #10998) [Link]But artists don't generally do final drafts in pencil, and if you take a Star Wars, or a Toy Story, there's thousands of artists working with the best equipment that can be found to produce the final result.And Linux is more like engineering than art. The Hyatt Regency Hotel in Kansas City had the most elegant walkways, until they collapsed and killed 114. Sometimes it matters how you do things, and handwaves at artistry isn't going to recover the lost data.Merging bcachefsPosted Jun 18, 2023 3:48 UTC (Sun) by rsidd (subscriber, #2582) [Link]As the article says, bcachefs is not currently written in rust, apart from a few userspace tools. It would be a 10 year project to rewrite it in rust. He is pushing it on his merits not on the language.Merging bcachefsPosted Jun 18, 2023 1:43 UTC (Sun) by developer122 (subscriber, #152928) [Link]Is there a possibility that bcachefs may get ported elsewhere, or is this implementation 100% GPL'd? (no GPL-BSD/MIT dual licencing?)Merging bcachefsPosted Jun 18, 2023 1:45 UTC (Sun) by developer122 (subscriber, #152928) [Link]What is bcachefs' handling of metadata corruption like? There's plenty of mention of data checksumming and erasure coding for repair, but what about online detection and repair of corruption/issues with the filesystem's datastructures and book-keeping?Copyright \u00a9 2023, Eklektix, Inc.Comments and public postings are copyrighted by their creators.Linux is a registered trademark of Linus Torvalds",
        "summary": "- The bcachefs filesystem was discussed at the 2023 Linux Storage, Filesystem, Memory-Management and BPF Summit, and the process for merging it into the mainline kernel seems closer than ever.\n- The creator of bcachefs, Kent Overstreet, highlighted the progress and features of the filesystem, including scalability improvements and the implementation of snapshots and erasure coding.\n- The bcachefs team is growing, and there is an automated testing infrastructure in place. Overstreet also emphasized his advocacy for writing code in Rust and his plan to gradually rewrite bcachefs in Rust.\n- The process of upstreaming bcachefs involves reviewing and merging a large number of patches. The response and decision ultimately lie with Linus Torvalds, who is the maintainer responsible for merging new filesystems. Other concerns include bug report management and the number of filesystems in the kernel.\n- Overall, bcachefs has the potential to be a high-performance and reliable filesystem with modern features, and its progress towards upstreaming is generating interest in the tech community.",
        "hn_title": "Merging bcachefs",
        "original_title": "Merging bcachefs",
        "score": 246,
        "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginMerging bcachefs (lwn.net)246 points by rascul 1 day ago | hide | past | favorite | 82 commentscurt15 18 hours ago | next [\u2013]>People are using the snapshots to do backups of MySQL databases, he said, which is a test of the robustness of the featureThis is exciting because db workloads are btrfs's kryptonite. The only way to avoid crippling fragmentation on btrfs is to disable copy-on-write, which also disables checksumming hence nullifying one of btrfs's main selling points. ZFS seems to handle such loads much better, and it would be interesting to see how bcachefs deals with them.replyslabity 1 day ago | prev | next [\u2013]> Brauner said that he thinks bcachefs is in \"excellent shape to be upstreamed\", but he is concerned with the number of filesystems in the kernel; he is glad to see that there are efforts to remove some of them. Changes that impact all of the filesystems in the tree \"get painful very very fast\" and, in some cases, there is no one available to review the changes. He would like the acceptance process to be more conservative; accepting NTFS/NTFS3 was \"a huge mistake\", for example.As someone not familiar with the filesystem related parts of the kernel, it's quite surprising to hear this. It sounds like filesystems are a lot more integrated (or at least less modular) than other kernel subsystems.Anyone else who found this surprising, I recommend reading this other LWN article that is referenced as well: https://lwn.net/Articles/886708/However, that mostly discusses issues caused by having 32-bit data structures, and how they will cause issues in 2038 when it's no longer able to handle the timestamps required. Specifically for ext3, NTFS, and ReiserFS.But other than that issue, I don't really understand why it's difficult to simply rip out support for a specific filesystem. Compared to something like a driver for a PCIe or USB device, what makes filesystems so much more integrated and difficult to remove?replycesarb 22 hours ago | parent | next [\u2013]> As someone not familiar with the filesystem related parts of the kernel, it's quite surprising to hear this. It sounds like filesystems are a lot more integrated (or at least less modular) than other kernel subsystems.AFAIK, on Linux filesystems are closely coupled with the memory management subsystem and the directory and inode caches. It's part of the reason why filesystem access on Linux is so fast.> But other than that issue, I don't really understand why it's difficult to simply rip out support for a specific filesystem. Compared to something like a driver for a PCIe or USB device, what makes filesystems so much more integrated and difficult to remove?It's not that unusual for users to have a partition containing a filesystem (sometimes, but not always, on an external drive) surviving unchanged through several migrations to newer Linux distributions and/or hardware. Compared to something like a PCIe or USB device, filesystems have a longer life.replyyencabulator 13 hours ago | root | parent | next [\u2013]That's more of a reason why it's hard to change the core MM/page cache/etc logic, as you have to change all the users of it (every filesystem, etc).Ripping out a filesystem is, technically, near-trivial. The downsides are fully human: the Linux kernel-userspace ABI is normally considered a golden promise that shall not be broken. The developers don't want users to have a bad morning on which their old filesystems no longer work.replytyingq 18 hours ago | root | parent | prev | next [\u2013]Buffer cache also. An article that goes into detail with an example of the coupling you're describing:https://lwn.net/Articles/930173/replyklooney 14 hours ago | root | parent | next [\u2013]Ext2/ext4 is a little special that way, most aren't quite so intertwined.replytyingq 14 hours ago | root | parent | next [\u2013]The article mentions the proposed change would also affect these:\"including the ext4 filesystem, but also F2FS, FAT, GFS2, HFS, ISO9660 (CDROM), JFS, NTFS, NTFS3, and the device-mapper layer\"replywmf 1 day ago | parent | prev | next [\u2013]I don't know that it's technically difficult, but Linux hates breaking backward compatibility. If they remove a filesystem, some users won't be able to mount their filesystems any more. But Linux doesn't want to have unmaintained code either, so they only accept a filesystem if it's going to be maintained for the next 10-20 years.replyTuna-Fish 20 hours ago | root | parent | next [\u2013]The only \"easy path\" is if there exists a working fuse implementation of the same filesystem. Then there is a good fallback path for the users who still need support.replyDylan16807 19 hours ago | root | parent | next [\u2013]You can always run the kernel implementation as a fuse filesystem with guestmount.Side note: I feel almost gaslit, it's really hard to find any mention of guestmount running a VM outside this single page https://libguestfs.org/guestfs-internals.1.htmlreplypengaru 13 hours ago | root | parent | next [\u2013]That's an interesting approach.It's too bad linux can't already run any in-kernel filesystem as a user process via FUSE, when you prefer greater isolation in exchange for worse performance, at the flip of a mount option.There's no technical reason for this to not be possible IMO... it's just a product of the tight coupling of everything in-kernel, as implemented today.I'm short on time to confirm at the moment, but I believe it was this [0] talk that left me with the impression kernel devs were exploring general solutions of this nature.[0] https://www.youtube.com/watch?v=xjv8Jv58bMsreplyyencabulator 13 hours ago | root | parent | prev | next [\u2013]Except in this case there will no longer be such a thing as \"the kernel implementation\". You'd have to run an older kernel, too. That's a ticking time bomb, more a migration/recovery strategy than something one could recommend longer term.replyAshamedCaptain 13 hours ago | root | parent | next [\u2013]It's really not a \"ticking time bomb\". The older kernel is only running as a guest in a VM. It will eventually stop working (e.g. when Intel changes x86), but it's not really a security issue.replyheavyset_go 16 hours ago | root | parent | prev | next [\u2013]guestfs has some really useful tools that are, as you've noted, seemingly esoteric.replyilyt 13 hours ago | root | parent | prev | next [\u2013]Well, till it is your boot FS. and there is performance penalty tooreplydjbusby 1 day ago | parent | prev | next [\u2013]Hard to maintain because all this code has to handle common thing (file, directory, etc) in unique way. A big mapping mess.Hard to remove because someone, somewhere uses it and Linux doesn't like to break things for the user.replythrowaway888abc 1 day ago | prev | next [\u2013]Side note:First mention of bcachefs here on HN (13years ago) and now it's gets merged into kernel.What an awesome achievement!https://news.ycombinator.com/item?id=1720077replykasabali 19 hours ago | parent | next [\u2013]That's bcache, not bcachefs.replykzrdude 21 hours ago | parent | prev | next [\u2013]We can't take it for granted yet that it will be mergedreplyjl6 15 hours ago | prev | next [\u2013]It might sound trivial, and I understand the history behind it, but I wonder if it is wise to put the word \"cache\" in the name of what is meant to be a durable, persistent storage component.replykoverstreet 13 hours ago | parent | next [\u2013]I thought about naming it kentfs, but prior precedent makes that a dubious idea :)replyjraph 11 hours ago | root | parent | next [\u2013]Yeah, this naming convention has not proven overly successful in Linux file systems, I would pass :-)replywmf 12 hours ago | root | parent | prev | next [\u2013]It's time for filesystems to escape the latin alphabet. \u03a9FS anyone? \u221eFS?replyjl6 10 hours ago | root | parent | next [\u2013]\u241creplykzrdude 21 hours ago | prev | next [\u2013]Some ~30 preparatory bcachefs patches have been posted for review for a month or so now, there was some discussion, but mostly negative discussion.Does anyone know how this actually works, is it going well for that patchset, is it getting closer to being merged?replyTuna-Fish 20 hours ago | parent | next [\u2013]This is normal. The positive things don't need to be talked about.It is extremely unlikely that bcachefs will be merged as-is. This is true for anything of it's size. (These days... There have been large subsystems in the past that were merged in unacceptable state with the promise that eventually they will be fixed. They usually weren't. Which is why the bar is so high now.) But this just means that there will need to be debate and work to hammer it into a shape that is acceptable for the kernel. This can be a long process, but I don't think there is anything fundamentally wrong in bcachefs that would exclude it.replykzrdude 19 hours ago | root | parent | next [\u2013]ok, thanks. At least (as the article notes) there seems to be progress on the code generation question which was maybe the biggest objection (\"proposed JIT allocator\").replypetercooper 14 hours ago | prev | next [\u2013]For anyone without an LWN sub (though it's a good idea!) the post is just digging into the details of how bcachefs is now in a position to be merged into the Linux kernel if Linus allows it, as well as some of the concerns about long term support given its complexity.bcachefs was discussed recently on HN \u2013 https://news.ycombinator.com/item?id=35899527 \u2013 and is a file system with COW, a GPL compatible license (licensing is an area where ZFS can be tricky, to put it lightly), and ext4 levels of performance.replykzrdude 13 hours ago | parent | next [\u2013]The submission is a SubscriberLink which gives courtesy access to anyone who has the link, just to note.replypetercooper 13 hours ago | root | parent | next [\u2013]I was expecting that to be the case, but when I opened a non-logged in browser and clicked it (just to test), it sent me to the LWN login screen, so it may have been updated since or I encountered a bug. It now appears to be okay!replymarcodiego 1 day ago | prev | next [\u2013]What I really would like: ext4 with snapshots and transparent compression where it may improve performance.replywmf 1 day ago | parent | next [\u2013]It's kludgey to graft snapshots and compression onto a classic in-place filesystem, so you'd probably end up with something slower and more complex than ZFS/btrfs/bcachefs. I remember tux2/tux3 was touting its simplicity but I don't know what happened.replythrow0101a 13 hours ago | root | parent | next [\u2013]> It's kludgey to graft snapshots [\u2026]McKusick did this for FreeBSD UFS/FFS:* https://wiki.freebsd.org/ExampleUfsSnapshots* https://people.freebsd.org/~rse/snapshot/* https://man.freebsd.org/cgi/man.cgi?query=mksnap_ffsVarious papers at:* http://www.mckusick.com/softdep/reply5e92cb50239222b 20 hours ago | root | parent | prev | next [\u2013]Dave Chinner somehow managed to put reflinks on top of XFS, although it's not nearly the same as full filesystem snapshots. I believe there was discussion about implementing proper snapshots, but XFS is very conservatively developed, so that may be many years in the future, if it ever happens.replythe8472 18 hours ago | parent | prev | next [\u2013]I think LVM or DM provide snapshot and compression capabilities which you could layer under ext4.I like ext4 for being simple and fast and having options to turn off the unnecessary stuff. It's great for gigabytes of caches, tempfiles, build artifacts, scratch files etc. The bookkeeping and indirection needed for those features would be a waste of CPU cycles for shortlived data.replymatheusmoreira 15 hours ago | parent | prev | next [\u2013]I want ext4 to remain rock solid and reliable, and LVM to somehow gain btrfs's flexible block allocator so we can have easily expandable heterogeneous pools of drives. I think by this point it's clear btrfs is never gonna turn into Linux's ZFS so perhaps it'd be wise to implement its good ideas in other systems instead.replyarjvik 1 day ago | parent | prev | next [\u2013]so essentially btrfs/bcachefs without the focus on being used for RAID?replyj16sdiz 1 day ago | root | parent | next [\u2013]btrfs don't allow nodatacow with snapshots .I know how these two features have some conflicts, but it meant we can't have database workload with decent performance on btrfs. -- brtfs snapshot just can't scale, the cow have too high performance hit.ZFS handles snapshots with database workload just fine.replytoast0 1 day ago | root | parent | next [\u2013]ZFS is cow, so if it works for your load, cow doesn't seem to be the problem?If you're willing to switch OSes, FreeBSD UFS is a more traditional filesystem, with optional snapshots (modifications to files in a snapshot have to be cow, of course)replyj16sdiz 16 hours ago | root | parent | next [\u2013]> ZFS is cow, so if it works for your load, cow doesn't seem to be the problem?Btrfs' implementation of cow, of course.They have no intention to make it work for database workload. When you ask why it's slow, they just ask you to disable cow ( which requires recreating the file -- something you would die to avoid with multi TB database)replyviraptor 1 day ago | root | parent | prev | next [\u2013]Have you got any recent snapshot benchmark available? I can find only one ancient one.> btrfs don't allow nodatacow with snapshotsWhat's the use case for a snapshot at that point? Isn't it the same as making a copy of the files?I'm not even sure what would this look like... CoW is what enables snapshots. Without CoW you can't get a consistent copy anymore.replylmz 23 hours ago | root | parent | next [\u2013]I guess it would look like LVM snapshots? CoW only when a snapshot is present.replyyencabulator 13 hours ago | root | parent | next [\u2013]That's how btrfs chattr +C files behave already.replyderefr 22 hours ago | root | parent | prev | next [\u2013]> What's the use case for a snapshot at that point? Isn't it the same as making a copy of the files?As you say, it's a consistent copy. cp(1) won't give you that.replytremon 12 hours ago | root | parent | next [\u2013]Neither does a filesystem snapshot unless the snapshot is aware of every application's consistency semantics. You can't simply assume that every flush() or other write barrier means that the application data is now in a consistent state on-disk.replyderefr 12 hours ago | root | parent | next [\u2013]These days, with production systems being componentized across VMs, abstractions like LVM in use, etc., the filesystem is no longer a fixed \"feature of the deploy environment\" chosen separately from the needs of the application, that needs to cope with any random thing the production application might do; but rather, a prod deployment is designed as a whole, where you choose the filesystem and plan the use of its features relative to what application you'll be deploying on it. (And, in fact, for a stateful application like a DBMS, you'll probably have one or more separate volumes just for the application state \u2014 so the filesystem you use to solve application-state problems doesn't even need to be good at being a rootfs for an OS; it only needs to be good for your application use-case.)Under this paradigm, rather than trying to make a filesystem that understands applications well enough to snapshot them, you instead make applications that have filesystem snapshots as part of their conceptual model.In the lower-effort version of this approach, you have software like Postgres, where the application layer can be told \"I'm going to use filesystem tooling to take a consistent snapshot, so make your on-disk state consistent for a while.\" In PG, you'd call pg_start_backup(), which will flush all pending writes to the table files to disk, and then spool all future writes purely in the WAL journal until the backup completes (i.e. until you tell it pg_stop_backup()) \u2014 at which point all those pending changes get replayed out to the table files.In the higher-effort version of this approach, you have software that has one or more CoW filesystems it natively understands and integrates with \u2014 where you never directly address the filesystem at all, but rather, you tell the application to take an application level snapshot; and then the application uses the filesystem CoW features, together with its own consistency primitives, to efficiently achieve that (which might not necessarily result in something that's a \"filesystem snapshot\" from the FS's perspective, but rather just a bunch of individual CoW-cloned files in a directory.) I believe that Oracle DBMS does this, though I might be wrong.replyDylan16807 9 hours ago | root | parent | prev | next [\u2013]Not \"every application\". A single database engine.replyviraptor 21 hours ago | root | parent | prev | next [\u2013]So I guess you'd have to temporarily turn on CoW, take a snapshot, make sure all the files are fully duplicated, turn off CoW to achieve that. Since you can't flip the setting at runtime right now, that feature seems quite far away.replyj16sdiz 16 hours ago | root | parent | next [\u2013]In brtfs, you can't.You need to copy the file over to enable/disable cowreplyyencabulator 13 hours ago | root | parent | prev | next [\u2013]> btrfs don't allow nodatacow with snapshots .Uhh. \"You can't do overwrite-in-place if you want to keep a snapshot copy of the old data\" simply makes sense, and you'll find every overwrite-in-place design that supports snapshots will take a write performance hit around the time the snapshot; either the snapshot has to atomically copy the whole data, or the first write after a snapshot can't be overwrite-in-place (or it has to make a separate copy of the original data for the snapshot; similar but worse).replyDylan16807 9 hours ago | root | parent | next [\u2013]Proper CoW means that every change to a block has to cause copies, not just the first write after a snapshot. That makes a big difference.And \"move the snapshotted data out of the way upon writing\" is going to give you better performance in a lot of cases.replyyencabulator 8 hours ago | root | parent | next [\u2013]I was replying to this part:> btrfs don't allow nodatacow with snapshots.You can absolutely mix chattr +C with snapshots. It's CoW-when-needed in the face of snapshots, just like everything has to be in order to support snapshots.replyDylan16807 8 hours ago | root | parent | next [\u2013]So they were just wrong in saying it's not supported? You didn't make that clear.And yes I can find many sources saying it's supported.replyNux 17 hours ago | parent | prev | next [\u2013]Check VDO device mapper module.replytoastal 23 hours ago | prev [\u2013]How long til it would be recommended to move from ZFS to BcacheFS on Linux?replyp-e-w 23 hours ago | parent | next [\u2013]10+ years, considering how Btrfs has played out.Btrfs was merged in 2009, but didn't gain wide acceptance until quite recently. Among the biggest distros, only Fedora uses it as default, and RHEL has actually dropped support. Even today, you can still find people claiming that they lost data because of it, though whether they are telling the truth I cannot say.replyarp242 18 hours ago | root | parent | next [\u2013]btrfs was merged in Linux in a very different state than bcachefs is now; bcachefs already has about ten years of development behind it, whereas btrfs \"only\" had two years of development behind it when it was merged. It would almost certainly not be merged today.I'm not saying you should switch all your critical systems to bcachefs on the day it gets merged as you can never be sure about the absence of bugs (even the relatively simple ext4 had some data-eating bug a few years after introduction), but the path to \"recommended filesystem\" will be a lot shorter than btrfs.At this point, I would already feel comfterable running bcachefs on my laptop \u2013 the only reason I don't is that I just can't be bothered running a custom kernel for it.replykzrdude 16 hours ago | root | parent | next [\u2013]How does bcachefs deal with laptops just dying due to lack of power and then fsck and recover on boot?replykoverstreet 13 hours ago | root | parent | next [\u2013]Works fine. Been using it on my laptop for ~7 years, users say it's solid w.r.t. power failure too.replykzrdude 13 hours ago | root | parent | next [\u2013]Awesome, can't wait for this to become linux's best filesystem.replyrjmalagon 13 hours ago | root | parent | prev | next [\u2013]Apparently, just fine. On paper, do a quick check and clean on mount. There are mount options for full check, degraded and recovery modes. About the paper https://bcachefs.org/bcachefs-principles-of-operation.pdfreplyThatPlayer 23 hours ago | root | parent | prev | next [\u2013]I believe openSUSE uses Btrfs.I haven't lost data on it, but my 8 drive Btrfs RAID6 filesystem locked up read-only on me, which wasn't fun. Switched to ZFS after that.replyalexdowad 19 hours ago | root | parent | next [\u2013]I have had single-drive btrfs filesystems lock up on me twice; i.e. file manipulation commands like `ls` freeze and \"take forever\". In both cases, there was a hardware failure of the drive. I believe that the frozen processes went into uninterruptible sleep (so they couldn't be killed).It is totally understandable that if the underlying driver starts returning errors, btrfs (or any other filesystem) may be unable to provide access to my data; but I was not at all happy about having to reboot the entire machine.(I must admit, it is possible that the \"freezing\" was happening in underlying block device driver code and not in btrfs. I don't remember if I ever checked wchan to see which it was. My impression from reading dmesg output was the issue seemed to be with btrfs.)I did once have a similar experience with ZFS as well. Sigh.replyformerly_proven 18 hours ago | root | parent | next [\u2013]Couple weeks ago had someone complain about system updates and other mildly IO heavy things making his system extremely slow and lock up for minutes at a time, despite having an NVMe SSD. Rebalancing fixed it - which iirc I also had to do regularly about ten years ago when I had the exact same problems with btrfs.replyyjftsjthsd-h 22 hours ago | root | parent | prev | next [\u2013]Yeah, OpenSUSE does BTRFS although they also support XFS. I ran tumbleweed for a while and managed to lose a root filesystem twice, although that was a few years ago and appeared to be triggered by running out of space, as the home filesystem on the same box, also BTRFS, survived both times.replyhashworks 21 hours ago | root | parent | prev | next [\u2013]RAID5/6 is still quite beta in btrfs. mdadm+btrfs is the way to go here.replymkup 20 hours ago | root | parent | next [\u2013]Actually any RAID is beta in btrfs, but configurations with one storage device (e.g. firmware of Turris Omnia) do quite well on prod.replyviraptor 20 hours ago | root | parent | next [\u2013]No, it's not beta. You can check the status here https://btrfs.readthedocs.io/en/latest/Status.htmlOnly raid5/6 has known issues.replyThatPlayer 19 hours ago | root | parent | prev | next [\u2013]This was also about 8 years ago. It was originally a RAID1 setup, but I did an in-place converter to RAID6, which is cool. And like I said, I didn't lose any data.With mdadm, I don't get auto-repair on top of the checksumming.replychamptar 16 hours ago | root | parent | prev | next [\u2013]I got a Btrfs RAID1 corrupted 3 times, so I was loosing faith in Btrfs, then I ran memtest and found out I had a bad memory stick causing real data corruption, not a Btrfs bug.replymattbee 19 hours ago | root | parent | prev | next [\u2013]Oh you never lost data with btrfs, but you might give up following the trail of mailing list posts and wikis one fine spring morning when it failed to mount.ISTR hours-long repair operations and a level of desperation that a lot of people wouldn't have had time for. This is assuming you didn't use its RAID or other features that definitely would eat your data.(This is experience from 10 years ago when I was looking for the latest and greatest features to support a hosting platform)replyhkwerf 22 hours ago | root | parent | prev | next [\u2013]It is quite understandable that something as critical when it comes to file storage takes a while to be adopted and being tested on less important data first. I'd say this reflects positively on IT.A nice (and lone) data point regarding btrfs, though: My dentist told me that he was storing patient data using btrfs last year. :)replylnx01 22 hours ago | root | parent | next [\u2013]I can't help but wonder how that conversation happened: Ya so we're going to root canal your molar, ah and by the way patient data is on btrfs which is only slightly worse...replyLeoPanthera 20 hours ago | root | parent | prev | next [\u2013]openSUSE also uses it as the default, as do Synology NAS appliances.replyherewulf 7 hours ago | root | parent | next [\u2013]Synology uses mdadm+btrfs for the record.Still a pretty good endorsement in my mind.replypixel3234 22 hours ago | root | parent | prev | next [\u2013]Btrfs is significantly more complex and very different case. Bcachefs is designed by single guy with different motivation.Main problem with Bcachefs is its in unknown state. There was no major testing done on that. When some company (like Oracle or Suse) puts it through automated stress test matrix, it may do exceptionally well or completely fail.replyrincebrain 18 hours ago | root | parent | next [\u2013]The article discusses having automated testing set up in conjunction with Red Hat's increasing interest in it.replypixel3234 17 hours ago | root | parent | next [\u2013]I am not familiar with Red Hat, but other companies have proprietary verification tests. It takes several decades of machine time to complete.For example Suse participated over 25 years on Ext3,4, ReiserFS/4, XFS, BTRFS.... This know how is not public!replydralley 16 hours ago | root | parent | next [\u2013]How do you know of Oracle and SUSE but not Red Hat?replyComputerGuru 13 hours ago | root | parent | next [\u2013]Probably meant not familiar with Red Hat\u2019s automated testing?replypredictabl3 23 hours ago | parent | prev [\u2013]I keep wondering this too, but at the same time, alebit with bugs, I have a ZFS pool mountable under Linux and Windows, so I have decided to not hold my breath at all. Not to mention zrepl, etc.replydizhn 16 hours ago | root | parent [\u2013]I recently discovered that btrfs can also be mounted under windows.replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
        "hn_summary": "- Bcachefs is a file system that is in a good position to be merged into the Linux kernel.\n- There are concerns about the number of filesystems in the kernel and the impact of changes on all filesystems.\n- Btrfs is known to have issues with database workloads, but it is unclear how bcachefs will handle them."
    }
]