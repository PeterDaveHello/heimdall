[
  {
    "id": 39962023,
    "title": "PumpkinOS: Modernizing PalmOS for x86 and ARM",
    "originLink": "https://github.com/migueletto/PumpkinOS",
    "originBody": "PumpkinOS PumpkinOS is a re-implementation of PalmOS that runs on modern architectures (x86, ARM, etc). It is not your average PalmOS emulator (it does NOT require a PalmOS ROM), but it can run m68K PalmOS applications. For a series of articles describing various aspects of PumpkinOS, look here: https://pmig96.wordpress.com/category/palmos/. Launcher is the first application that runs when PumpkinOS starts. It shows a panel from which you can start other applications. Preferences will eventually contain all preference options for configuring PumpkinOS. Command is a command shell, still experimental. This release contains the four PIM applications found on PalmOS: AddressBook, MemoPad, ToDoList and DateBook. The source code for these applications were distributed in one or more PalmOS SDks and were adapted for correct compilation on PumpkinOS. Records created by AddressBook and MemoPad should be compatible with their PalmOS counterparts. Because of differences in word size en endianness, however, records created by ToDoList and DateBook are not compatible. These applications were tested just to the point where I could create and edit a few records. There are still some quirks, and some functions were not tested at all. The goal here is just to offer a view of what to expect from PumpkinOS in the future. I am planing to setup a bug tracker to document enhancements and bugs. Licensing PumpkinOS is licensed under the GPL v3. The license directory contains information on specific licenses of the various components used in PumpkinOS. If you think something is missing and/or incorrect, please let me know. Building You have to build PumpkinOS from source. No IDE is required, you can build from the command line. If you use 64-bits Windows, you can use MSYS2 (https://www.msys2.org/). Download the installer and follow the instructions there. Open a MINGW64 terminal (the one with the blue 'M' icon) and install these additional packages: pacman -S gcc binutils make git Next clone the PumpkinOS repository: git clone https://github.com/migueletto/PumpkinOS.git Finally go to the source directory of the PumpkinOS repository you have just cloned and run the make script: cd PumpkinOS/src ./mk.sh Msys 64 If everything goes well, you will have a pumpkin.exe in the root directory, some DLLs in the bin directory, and some PRC files in the vfs/app_install directory. There is also experimental support for 32-bits Windows (Vista or later. It will not work on Windows XP). Open a MINGW32 terminal (the one with the gray 'M' icon) and install this additional package: pacman -S mingw-w64-i686-gcc From there, compile using (note that now argument is 32, for 32-bits): cd PumpkinOS/src ./mk.sh Msys 32 If you are using a 64-bits Linux-based OS (like Debian, Ubuntu, etc), you also need gcc, binutils, make and git. If you are a developer, there is a chance you already have those. If they are not installed, follow the instructions to download additional packages on your specific Linux distribution. You must also install the SDL2 development package (the package that contains the libraries and the headers). On a Debian distribution, it is probably something like: sudo apt install gcc binutils make git libsdl2-dev Again, you must clone the repository and compile it using: cd PumpkinOS/src ./mk.sh GNU/Linux 64 On Windows 11 and recent releases of Windows 10, it is also possible to build PumpkinOS on WSL (Windows Subsystem for Linux, version 2). Open a WSL terminal and follow the same instructions for a Linux build. Running On 64-bits Windows, run pumpkin.bat. On 32-bits Windows, run pumpkin32.bat. On Linux or WSL, run pumpkin.sh. PumpkinOS will open on a new window. On WSL you may need to run a X-Window Manager, otherwise the PumpkinOS window will not have a border. When you run PumpkinOS, all PRCs inside vfs/app_install will be removed and expanded into folders inside vfs/app_storage. Please keep in mind that everything is pretty much experimental at this stage, so expect a few issues here and there. After either a successful or an unsuccessful run, you will find a pumpkin.log file on the root directory. If something goes wrong, look for lines marked with an \"E\" on the third column of this file. You can reach me for questions (and send me your log file if you wish). The Windows version implements Drag & Drop functionality. You can drag a PalmOS PRC over the PumpkinOS window and hopefully it will be installed and show up in the Launcher. The Linux version lacks this functionality. For now, you have to manually copy PRCs to the vfs/app_install directory and restart PumpkinOS. If you really want to, you can debug PumpkinOS with gdb on Windows, Linux and WSL. On Windows, edit pumpkin.bat and change the last line to (you should also add the Windows equivalent of the /usr/bin directory of your MSYS2 installation the the PATH): gdb.exe --args .\\pumpkin.exe -d 1 -f pumpkin.log -s libscriptlua.dll script\\pumpkin_windows.lua On Linux and WSL edit pumpkin.sh and change the last line to: gdb --args ./pumpkin -d 1 -f pumpkin.log -s libscriptlua.so ./script/pumpkin_linux.lua I am writing a full Wiki article on source level debuging PumpkinOS.",
    "commentLink": "https://news.ycombinator.com/item?id=39962023",
    "commentBody": "PumpkinOS, a Re-Implementation of PalmOS (github.com/migueletto)316 points by rickcarlino 17 hours agohidepastfavorite81 comments PaulRobinson 3 minutes agoI love this project existing and prolonging the life of all that software that would otherwise never have a chance to execute again, but is this about nostalgia or about a real need/desire for that software? Given how much people wax lyrical about Palm, Blackberry, Psion, outside of niche applications (I know IMAX needs Palm emulators to run bits of that stack), do people genuinely yearn for going back to the old days? I was looking around for a \"modern\" Psion 5, and spotted some of the team had come up with the Gemini PDA which looks a bit like one, but is Android based. Some reviewers definitely find this a drawback. You can imagine a new EPOC operating system with support for modern connectivity would be a slam dunk for them, but then I found myself thinking I'd mostly run Linux on it (if I could find a new one in stock anywhere with a UK keyboard). But then, do we all really want that? There has been a lot of progress in the last 20+ years, and I can't help but feel if most people were offered PalmOS instead of Android or iOS as their daily driver it would be frustrating enough they'd hand it back within a week. What's the killer feature that makes that statement false? reply frellus 6 hours agoprevNothing made me feel older than going to the Computer History Museum in Mountain View, CA and seeing a Palm Pilot in the display case. It should be illegal to show things which were an integral part of your life, a short 30'ish years ago, as if they were uncovered in the ruins of some pre-civilization. Not fair at all. reply PaulRobinson 2 minutes agoparentAt the Science Museum in London, there is a collection of mobiles phones, computers and consoles in one gallery. My partner and I take great joy in pointing out all the ones we've owned over the years, it's great fun to see them again. reply codezero 6 hours agoparentprevI still have my original PalmPilot in a box in the attic. Its existence was a huge life lesson for me. I asked my boss to pay for it (he did) but he said: do you use anything to organize your life and projects right now? If you don't, I don't think a PalmPilot will help you. He was so right. reply bouncing 30 minutes agorootparentI find that often, having a tool that enhances something you never did doesn’t make you start doing that thing. But there are exceptions. I never had an address book or calendar until I had a Palm Pilot. It might have just been that I was becoming an adult at the time, but part of it was probably a barrier to use factor. The Palm was a small thing I could carry to class, keep near my phone, bring to my internship job, etc. It and the need for organizing did conspire to make it my first real organizer and my first time having that information organized at all. reply astrange 3 hours agorootparentprevI read a lot of ebooks on mine. reply JTyQZSnP3cQGa8B 3 hours agorootparentWe used to put ebooks on it to help us during university exams. I pretended it was a calculator. I know, I’m ashamed, but it was for a boring course on economics. reply bayindirh 2 hours agorootparentI was the opposite. I used it as a calculator, without adding any ebooks on top, but my professors were uneasy about it. Then I got a calculator to curb their anxiety. reply 0xEF 1 hour agorootparentI remember when calculators were the Forbidden Fruit because, according to maths teachers, we were \"not going to be walking around with calculators in our pockets all the time.\" reply johnwalkr 5 hours agoparentprevOn Reddit a few months ago there was a post about someone finding their grandfather’s old gameboy. reply tomashubelbauer 3 hours agorootparentThis one's funny because on one hand you have young people finding their grandparents Game Boys in the attic but on the other you'll have kids of the same age recording YouTube videos about GB modding, because that scene is still huge, diverse and very lively. reply vineyardmike 1 hour agorootparentprevAs someone who had a gameboy growing up, and now a child, I had to quickly do some math to console myself that I'm not yet \"Grandfather\" age. This is simply a (now) old man who had fun toys as a (normal age) adult. reply thombat 42 minutes agorootparentBut the (horrifying) maths checks out: say he got the Gameboy at the age of 12 when it launched in 1989, had a first child at twenty in 1997, that child has a child of its own also at the age of twenty in 2017, grandchild aged six or seven now excited to find granddad's Gameboy. Twenty is merely youngish for a first child, not the stuff of shotgun-wedding backblocks. reply akho 36 minutes agorootparentprevImagine them finding their grandfather's current PS5. reply therealfiona 3 hours agorootparentprevTo be fair, by grandfather had a Nintendo DS. reply finaard 27 minutes agoparentprevI still have a section on my homepage about how to create and flash custom ROMs for the Palm Vx - and somewhat surprisingly, I still now and then get emails from people asking for help with that. reply pjmlp 3 hours agoparentprevNow imagine what I feel when seeing paper tapes, 8\" floppies, assemble kits for Z80 in such museums. On the other hand, there some special sense of nostalagia for what I was doing back on those days. reply verdagon 11 hours agoprevThis PumpkinOS project is pretty incredible. I can't imagine how much effort it would take to be compatible with all the system calls that the average Palm app would expect. I remember Palm did some truly weird things with memory: anything moderately large would need to be put into a special memory block that the OS could rearrange at will, and one would need to lock the block's handle to keep it stable while accessing it. Stuff like that must have been challenging (and fun) to implement in PumpkinOS. This brings me back. I used to make little games for Palm OS, and I was so excited for the next version of the OS which would let one use the (then new) Palm OS Development Suite to make programs. It was also the last OS I've used where an app had a central event loop. Everything else today has UI frameworks that handle it for you. Things are easier now, but I still miss it. reply grishka 7 hours agoparent> It was also the last OS I've used where an app had a central event loop. Windows is still like that if you use Win32 APIs directly. All GUI toolkits ever made are like that, but in most of the modern ones, this queue and loop are usually internal and you can only infer their existence by looking at the stack in a debugger or when something crashes. reply p_l 2 hours agorootparentWindows doesn't actually have a central event loop, which makes it pretty unique. macOS/iOS/etc have a central event loop in Cocoa - what more only initial thread is allowed to talk with windowserver! Xlib pretty much enforced single event loop per connection - XCB allowed more. In comparison, win32 applications can create an event loop (\"message pump\") per thread, and you can use GUI calls completely independently on them. reply pjmlp 3 hours agorootparentprev> Windows is still like that if you use Win32 APIs directly. Which is basically the only option for C and C++ developers, when using vanilla Visual Studio, unless they want to write libraries to be consumed by .NET instead, or use a third party framework. It is either raw Win32 or MFC, don't even bother with WinUI. reply jsheard 11 hours agoparentprev> anything moderately large would need to be put into a special memory block that the OS could rearrange at will, and one would need to lock the block's handle to keep it stable while accessing it Didn't 16-bit Windows and classic Mac OS do something similar? If you're doing multitasking on a system without an MMU then I think that kind of live heap defragmentation would have been practically required. reply wmf 11 hours agorootparentPalm was started by ex-Apple people and borrowed a lot of ideas and mistakes from the Mac. reply MaulingMonkey 8 hours agorootparentprev> Didn't 16-bit Windows and classic Mac OS do something similar? I assume this is what `{Local,Global}{Lock,Unlock}` were for when combined with `{Local,Global}Alloc({L,G}MEM_MOVEABLE)` Similar idioms occasionally persist in modern code - e.g. when dealing with FFI in GCed languages (C#'s `fixed` statement pins memory in place.) reply asveikau 8 hours agorootparentThe lock/unlock metaphor is also used when sharing buffers with video or a/v apis in Windows. And the \"safe array\" type from COM/OLE. reply kmeisthax 8 hours agorootparentprevYes. The idea wasn't to get away with not having an MMU, though - it was to get away with shipping the Mac with an ungodly low amount of RAM for a machine with a GUI. I believe the original idea was to ship with like 64k or something? Obviously, with the state of mobile hardware back then relocatable blocks were also similarly necessary in order to save RAM. For anyone wondering, no, this isn't the thing that made classic Mac OS unfit for multitasking. The MMU is necessary to keep applications from writing other apps' heaps, not to do memory defragmentation. You can do some cool software-transparent defragmentation tricks with MMUs, but if you're deciding what the ABI looks like ahead of time, then you can just make everyone carry double-pointers to everything. reply NegativeLatency 6 hours agorootparentI actually installed an aftermarket MMU in my Macintosh II, since it's required for A/UX. https://retrocomputing.stackexchange.com/questions/10931/wha... https://aux-penelope.com reply Someone 1 hour agorootparentprev> I believe the original idea was to ship with like 64k or something? Yes. Both the 6809 based design and the first 68k one targeted 64k. See https://folklore.org/Five_Different_Macs.html reply spc476 4 hours agorootparentprevWell, there's also the fact that the MC68000 in the original Mac didn't have an MMU, and it was difficult to add an external MMU to a 68000 system [1]. You could use an MMU sanely starting with the MC68010, and it wasn't until I think the MC68030 that the CPU came with an integrated MMU. [1] Because exceptions on the 68000 didn't save enough information to restart the faulting instruction. You could get around this, but it involved using two 68000s as insane hack ... reply JNRowe 6 minutes agorootparentJust to muddy the waters some more there was also an EC variant¹ of the 030 without the MMU. The EC variant was available right through to the 060, and I'd be curious to know how prevalent the line was. I suspect the EC versions far outnumbered the \"full\" chips, because they appeared in all kinds of industrial systems. I'm basing that entirely on working for a company that was still shipping products with MMU-less 68k and coldfire this century, not any real data. ¹ https://en.wikipedia.org/wiki/Motorola_68030#Variants reply kalleboo 3 hours agorootparentprev> For anyone wondering, no, this isn't the thing that made classic Mac OS unfit for multitasking Yeah, the way to port classic MacOS apps to native OS X apps was called Carbon, and it was basically 80% of the classic MacOS toolbox just ported to OS X, Handles and QuickDraw and all. Classic MacOS apps written to CarbonLib would \"just run\" natively in OS X (and the same binary in Classic MacOS). Carbon even kept working on Intel MacOS, but they finally killed it with the 32-bit deprecation a year or two before Apple Silicon was released. Apple could have worked in multitasking in classic MacOS if they really wanted to, but their management was totally dysfunctional in the 90's where there was no point seen in investing in boring old MacOS since there was always a revolution just around the corner in the form of Pink, Taligent, Copland etc, projects which due to the aforementioned management never went anywhere. reply xmm 10 hours agorootparentprevClassic MacOS did, but it's definitely not something needed for multitasking without an MMU. For instance AmigaOS didn't do this, but instead effectively had a single shared heap. reply AshamedCaptain 1 hour agorootparentMac OS, Win16, PalmOS all have shared heaps too. This is precisely why you need defragmentation (after an application quits, the heap is a fragmented mess, full of holes) and therefore some system so that the other applications keep \"movable handles\" to heap blocks instead of raw pointers (which would become invalid after the heap undergoes one round of defragmentatino). If an OS does not do this you are basically indirectly setting a limit to its uptime, as eventually this global heap's fragmentation will prevent launching any new programs. Having local heaps does not solve this, as you still have to allocate these local heaps from somewhere. Having an MMU will allow you to do transparent defragmentation without handles as raw pointers (virtual addresses) become your handles. Having an MMU with fixed page size will allow you to outright avoid the need for defragmentation. reply alexey-salmin 5 hours agorootparentprevHow do you free the shared heap when an application quits? reply vidarh 4 hours agorootparentVery carefully. It's in fact one of the biggest issues with AmigaOS that made it incredibly hard to add proper MMU support. The OS is heavily message-passing based, and it's not at all always clear \"from the outside\" who the owner of a given structure passed via a message port (which is little more than a linked list) is, and so the OS doesn't even know which task (process/thread - the distinction was pretty meaningless due to the lack of memory protection) owns a given piece of memory. Later versions added some (optional) resource tracking to make it easier to ensure resources are freed, but if an application crashed or was buggy you'd frequently leak memory, and eventually have to reboot. It was not great, but usually less awful than it sounds with sufficiently defensive strategies. [I have at various points when e.g. doing some work on AROS way back, argued that it is is quite likely possible to largely untangle this; partially because for a lot of cases, the ownership changes are clear and rules that fit actual uses can be determined; partially because the set of extant AmigaOS apps is small enough you could \"just\" add some new calls that does ownership tracking, declare the old ones legacy, and map ownership changes for the rest one by one and either patch them, or, say, add a data file for the OS to use to apply heuristics; had the remaining userbase been larger maybe it'd have been worth it] reply kazinator 4 hours agorootparentThat situation doesn't prevent an MMU and virtual memory. It prevents multiple address spaces. Multiple address spaces per process are not a requirement for virtual memory, as such. They are a requirement for getting some of the protection benefits of virtual memory. Not all the benefits. With a single address space for all applications, there can still be user/kernel protection: userland not being able to trash kernel data structures. (Of course with important system functions residing in various daemons, when those processes get trashed, it's as good as the system being trashed.) reply vidarh 3 hours agorootparentIt doesn't \"prevent\" an MMU and virtual memory, you're right, but it does severely limits what you can do with it hence why I wrote \"proper\" MMU support. There are virtual memory solutions for AmigaOS, though rarely used. There are also limited MMU tools like Enforcer, but it was almost only used by developers. AmigaOS4 has some additional MMU use, and there has been work on trying to add some more protection elsewhere as well, but it is all fairly limited. Specifically in terms of the comment I replied to, you categorically can not automatically free memory when a task (process/thread) ends in AmigaOS without applications-specific knowledge without risking causing crashes, because some memory \"handoffs\" are intentional. > With a single address space for all applications, there can still be user/kernel protection: userland not being able to trash kernel data structures. Yes, you could if the OS was designed for it, and it was done at a point where most of the application developers were still around to fix the inevitable breakage. The problem with doing this in AmigaOS without significant API changes or auditing/patching of old code is that there is no clear delineation of ownership for a lot of things. This includes memory in theory \"owned\" by the OS, that a lot of applications have historically expected to be able to at least read, and often also write to. You also e.g. can't just redefine the \"system calls\" for manipulating lists and message queues to protect everything because those are also documented as ways to manipulate user-level structures - you can define your own message ports and expect them to have a specific memory layout. More widely, it includes every message sent to or received from the OS, where there's no general rule of who owns which piece of the message sent/received. E.g. a message can - and will often - include pointers to other structures where inclusion in the message may or may not imply an ownership change or \"permission\" to follow pointers and poke around in internals. To address this would mean defining lifecycle rules for every extant message type, and figuring out which applications breaks those assumptions and figuring out how to deal with them. It's not a small problem. reply pjmlp 3 hours agorootparentprevWindows 16 bit did, but it required a MMU anyway, at least since Windows 3, that was its big feature, 16 bit protected mode and a VM mode for running MS-DOS. reply skissane 2 hours agorootparent> Windows 16 bit did, but it required a MMU anyway, at least since Windows 3 Windows 3.0 supported three modes of operation: real mode (8086 minimum), standard mode (286 minimum), 386 Enhanced mode (386 minimum). Real mode was pretty limited, and a lot of apps could not fit in its rather limited memory, but it was not completely useless. I believe real mode Windows apps could use EMS, although I’m not sure if many actually did In Windows 3.1, real mode was removed, and only standard and 386 Enhanced were supported. So, 3.1 was the first version to “require an MMU”, if by that you mean a 286 or higher reply pjmlp 2 hours agorootparentYes you're right, but I never knew anyone that would get Windows 3, only to keep using it as Windows 2. reply Someone 2 hours agoparentprev> I remember Palm did some truly weird things with memory: anything moderately large would need to be put into a special memory block that the OS could rearrange at will, and one would need to lock the block's handle to keep it stable while accessing it. Stuff like that must have been challenging (and fun) to implement in PumpkinOS. That’s extremely easy on modern hardware with gigabytes of RAM (compared to 2 megabytes on the pal pilot III): just use malloc, never move memory around, and make locking and unlocking such blocks no-ops. If there is an OS call to determine lock state, you’ll have to store that somewhere, but that isn’t difficult, either. It also isn’t hard to implement the way they did back then; it ‘just’ complicates using it. reply NegativeLatency 6 hours agoparentprevOne nice thing about modern hardware would be that you wouldn't exactly be memory constrained. You'd get to implement a complicated API with whatever large size chunk of memory you wanted, since 128 MB of ram or how ever much they came with is peanuts today. reply toast0 3 hours agorootparent> since 128 MB of ram or how ever much they came with is peanuts today. The first Palm (Pilot 1000) had 128 kB. I think the biggest 68k Palm was the Palm Vx with 8MB. Towards the end of the (Intel) ARM Palms, they did have 128 MB models though. reply Tor3 2 hours agorootparentI think only the latest Treo had 128MB - the last PDA (Lifedrive) had 64MB, the TX 32MB. (One should remember though that there wasn't mass storage+RAM as we typically think of it - the memory of the Palm devices was storage and active memory in one. Battery-backup'ed until the very latest models. There wasn't a filesystem as such. So all this memory should be thought of as memory for applications, nor like storage in an Android device.) reply jeanchen 5 hours agoparentprevI loved Palm games! Those were the best mobile games, nothing modern compares to them at all. reply duskwuff 11 hours agoprevAlso possibly of interest: rePalm, a project to run PalmOS on ARM microcontrollers like the RP2040 - http://dmitry.gr/?r=05.Projects&proj=27.%20rePalm reply sillywalk 11 hours agoparentMore HN discussion on PumpkinOS PumpkinOS / 3 years ago|52 comments https://news.ycombinator.com/item?id=28466858 Related discussion on rePalm: PalmOS on Raspberry Pi 169 points by Tijdreiziger on Sept 10, 202186 comments https://news.ycombinator.com/item?id=28487817 reply rkagerer 6 hours agoprevMy heart thumped faster when I read this headline. Please make it work on Android so I can 'replace' my daily driver and go back to a better time! reply finaard 22 minutes agoparentI still miss the calendar and contact apps from the Palm. I stuck with Palm up until the Centro - and haven't found contact/calendar apps I'm as happy with as the Palm versions yet. They're either missing some simple basic features, or the UI is needlessly complicated. reply thot_experiment 8 hours agoprevSo hype to lose some hours playing Space Trader. I had a Palm Vx in middle school and I have some very fond memories of playing that game under my desk in class. reply alsobrsp 8 hours agoparentI have two Palm Vx in the garage. I love that game too. reply ethanpil 6 hours agoprevWhat would it take to get this on modern (or even last generation) phone hardware? I bet we could do everything we want with tremendous simplicity and out of this world battery life... Probably would make a PinePhone feel like a Rolls Royce. reply redundantly 6 hours agoparentIt would be nice if more effort was put into the optomisation and stabilisation on modern mobile operating systems. reply fragmede 6 hours agoparentprevthat is one helluva nerdsnipe reply amaccuish 1 hour agoprevWeird to think that Amazon's kindle format used to be based on .mobi, itself based on the Palm's database file type. reply tensor 11 hours agoprevI remember investing in Palm thinking that they'd eventually be the ones to build something like the iphone. Sadly, they didn't and when apple did that was it for them. reply lxgr 10 hours agoparentThey did have the Treo line! Arguably, what ultimately brought Palm down was their early success and the huge library of existing shareware and freeware tools: They desperately needed to try something new (Palm OS was just showing its age as a single-threaded, in-RAM, non-virtual-memory-based OS), but couldn't, since it would have alienated long-time fans by stranding their existing software libraries. They could never work their way through that chicken-and-egg problem (and all of the split ups (OS vs. hardware), forks/spin-offs (Handspring), and re-mergers didn't help either) until it was too late: Cobalt OS never saw any devices, and the Pre was an ambitious new start but would have had a tough time against the iPhone even if it would have launched earlier than that. reply snom380 1 hour agorootparentApple was able to manage this with a much bigger market and a lot more apps (when transitioning to OS X), so while it would be hard, I think Palm could have been able to do that as well. But as you say, the company structure, market position and a lot more worked against them (same thing with Nokia and Symbian). reply wpietri 4 hours agorootparentprev> They did have the Treo line! Thank you for mentioning it. It makes me feel like I'm taking crazy pills when people talk about how Steve Jobs invented the smartphone. I had a series of Treos starting with the Treo 270: https://en.wikipedia.org/wiki/Treo_270 As somebody who carried a Palm for years, it was so amazing to suddenly have the internet in my pocket. It still is, really. reply finaard 20 minutes agorootparentWith the Centro it was pretty amazing when they dropped a maps application later on - as it didn't have GPS. In built up areas they managed pretty impressive accuracy just by cell tower triangulation. reply toast0 3 hours agorootparentprev> Steve Jobs invented the smartphone Yeah, well people are misinformed. Unless there was something for the Newton that turned it into a phone, IBM was first to market in 1994 [1] The IBM Simon made calls, did data, had a paid 3rd party app, etc. Besides Palm, Symbian (Mostly from Nokia, but some other companies made Symbian handsets), RIM's Blackberry, and Microsoft's Windows Mobile (with handsets from many OEMs) had established smartphones before Apple. Of course, the iPhone had much better sales, and changed the market in many ways, but the category was 12ish years old when Apple entered. Hardly inventing or first to market. [1] https://en.m.wikipedia.org/wiki/IBM_Simon reply Tor3 3 hours agorootparentAnd, in any case, Japan had tons of what should be considered early smartphones. I was astonished the first time I saw one (and this was obviously before the iPhone). For some reason the iPhone managed to kill off the local Japanese industry though. Japan youngsters (and not only youngsters) are very fashion-oriented, which had more to do with the change than anything else. reply fragmede 6 hours agorootparentprevI think it was their lack of vertical integration that did it. there were too many pieces that every developer needed to do, that made it really hard to make apps for it. compare that to the app store where Apple just takes a 30% cut, which is steep, but they do things for that 30%. On the consumer end of things, the actually affordable data plan with att at launch, which is more vertical integration, rather than letting the carriers do their thing, is what did it, imo. reply floating-io 8 hours agorootparentprevWasn't that Handspring rather than palm? [edit: referring to Treo specifically, not the rest...] reply pushedx 8 hours agorootparentRight, it was ex-Palm employees at Handspring that created the Treo. Nice documentary about this with interviews: https://www.youtube.com/watch?v=b9_Vh9h3Ohw reply classichasclass 8 hours agorootparentprevOriginally, but then Palm bought them. reply causality0 7 hours agorootparentprevI'm going to blame it on Palm's flat refusal to move onto PalmOS 6. Every time a new device came out and it was still on PalmOS 5 the whole community was like \"what the fuck are you doing?\" reply ianburrell 10 hours agoparentprevOne big problem is that Palm split into PalmSource for software and Palm One for hardware. PalmSource went off to design Linux-based OS, got acquired, and disappeared. Palm One, renamed back to Palm One, made some early smartphones with PalmOS. But PalmOS was pretty obsolete by then, with 16-bit apps running on 32-bit OS. Then Palm developed webOS, which made some weird hardware decisions and couldn't compete with iPhone and Android. But it could have been a contender, better than Blackberry and Nokia that didn't make jump to capacitive screens. I think Palm's problem was being too late, but if they hadn't split and made PalmOS successor, they would have missed capacitive smartphones. reply tensor 10 hours agorootparentWebOS is great, at least it lives on in LG products for now. I far prefer it to android, despite the vastly smaller ecosystem. reply hedgehog 10 hours agorootparentprevwebOS was really nice, a lot of ideas that didn't come to iOS and Android until much later were present and nicely implemented back in 2010 or so. reply ianburrell 9 hours agorootparentI had a Palm Pre, and Treo before that. One problem is that webOS needed resources, and Palm chose limited hardware. I think webOS could have been the iPhone if Apple didn't exist. I think they could have taken the second spot from Android if had been earlier, open, and released conventional hardware. reply mrbluecoat 10 hours agoparentprevYeah, the GPL license cracked me up on this one. Can't imagine there's a danger of commercialization here.. reply seszett 5 hours agorootparentGPL isn't about commercialization though, it allows commercialization. It's all about preventing closed-source forks. reply AstroJetson 8 hours agoprevI was a Sprint customer from the start of the Palm phone era. Up until 2 years ago I was using colored Treo’s for my phone. I loved that mechanical keyboard it was so nice to use. ( I have slightly deformed fingers that make it hard for touch screens.). Merger with T-M killed the radio part. So sad to see it go. reply smm11 6 hours agoparentI think I used to work with you. You stated the Pre was the \"iPhone killer\" about six months after the iPhone came out, and Android phones appeared. You showed our deparment's first iPhone user the non-awesome stuff the Pre did by comparison, then stated that Palm's millions of users would show me. They sure did. reply tangus 11 hours agoprevAlso of interest: CloudpilotEmu - Palm emulator in your browser https://cloudpilot-emu.github.io/ When I installed it and could play Vexed again... ahh, the happiness! reply lxgr 10 hours agoparentCloudpilot is amazing, one of the most sophisticated PWAs I'm aware of! I haven't heard of Vexed, but for me, bringing back Space Trader made me very happy. reply DXXTHLOVESU 2 hours agoprevSo palm OS was an old mobile operating system, introduced around 1996, coded in C++... later extended to support smartphones? Can someone please explain more to me about this OS as it seems pretty interesting, and I have never heard of it. reply homero 8 hours agoprevWould be fun on Android reply yoelier23 5 hours agoprevHola reply Terretta 12 hours agoprev [–] Graffiti: https://github.com/search?q=repo%3Amigueletto%2FPumpkinOS+gr... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "PumpkinOS is a modern re-implementation of PalmOS, enabling the operation of contemporary applications on various architectures like x86 and ARM without needing a PalmOS ROM.",
      "Users need to construct PumpkinOS from the source by following platform-specific guidelines tailored for Windows and Linux systems.",
      "This experimental OS, licensed under GPL v3, provides basic PIM tools like AddressBook and MemoPad, with debugging supported via gdb on Windows, Linux, and WSL, offering a sneak peek at upcoming functionalities."
    ],
    "commentSummary": [
      "Online forum users reminisced about old technology, specifically PalmOS and devices like Game Boys and Palm Pilots, discussing memory management challenges in older operating systems.",
      "The conversation also explored Palm's journey in the smartphone market, the evolution of their operating systems, and missed chances to compete with Apple and Android, evoking a nostalgic tone for past innovations.",
      "Users expressed a sense of appreciation for the technological advancements of the past, highlighting the sentimental value of older tech."
    ],
    "points": 316,
    "commentCount": 81,
    "retryCount": 0,
    "time": 1712508916
  },
  {
    "id": 39960537,
    "title": "In-Memory PostgreSQL Mock Server for Testing: pgmock",
    "originLink": "https://github.com/stackframe-projects/pgmock",
    "originBody": "pgmock Demo — Discord pgmock is an in-memory PostgreSQL mock server for unit and E2E tests. It requires no external dependencies and runs entirely within WebAssembly on both Node.js and the browser. Installation npm install pgmock If you'd like to run pgmock in a browser, see the Browser support section for detailed instructions. Getting started You can run an in-memory server like so: import { PostgresMock } from \"pgmock\"; const mock = await PostgresMock.create(); const connectionString = await mock.listen(5432); Recommended: If you use node-postgres (pg on npm), pgmock provides you with a configuration object that doesn't require you to serve on a port (and also works in the browser): import * as pg from \"pg\"; const mock = await PostgresMock.create(); const client = new pg.Client(mock.getNodePostgresConfig()); await client.connect(); console.log(await client.query('SELECT $1::text as message', ['Hello world!'])); It is considered good practice to destroy the mock server after you are done with it to free up resources: mock.destroy(); Documentation Check the PostgresMock source file for a list of all available methods and their documentation. Browser support pgmock fully supports browser environments. While webapps can't listen to TCP ports, you can still use PostgresMock.createSocket and the node-postgres configuration. However, if your bundler statically analyzes imports, the default configuration may show a warning because of missing (optional) Node.js modules. Check examples/web-demo/next.config.mjs for an example on how to configure Webpack for bundling. If you're only looking to run a database in the browser, you might want to consider pglite instead. It is more performant and lightweight, but only has a limited feature set. pgmock is designed for feature parity with production PostgreSQL environments, as you would want in a testing environment. How does it work? There are two approaches to run Postgres in WebAssembly; by forking it to support WASM natively or by emulating the Postgres server in an x86 emulator. The former is more performant and uses considerably less memory, but only supports single-user mode (no connections), and no extensions. To prevent discrepancies between testing and production, and because performance is not usually a concern in tests, pgmock currently uses the latter approach. In the mid-term future, once native Postgres WASM forks mature, we plan to make both options available, and eventually, switch to native WASM as default. We don't expect there to be many breaking changes besides the APIs inside PostgresMock.subtle. pgmock differs from previous Postgres-in-the-browser projects by providing full feature-compatibility entirely inside the JavaScript runtime, without depending on a network proxy for communication. We did this by simulating a network stack in JavaScript that behaves like a real network, that can simulate TCP connections even on platforms that do not allow raw socket access. Wanna contribute? Great! We have a Discord server where you can talk to us. Can this run other Docker images or databases? In theory, yes. I just haven't tested them. Ping me on our Discord server if you're interested. Acknowledgements v86, the x86 emulator which makes this possible Supabase & Snaplet for building their own approach of running Postgres inside WebAssembly, which this is based on Stackframe for keeping me on a payroll while I was building pgmock",
    "commentLink": "https://news.ycombinator.com/item?id=39960537",
    "commentBody": "I open-sourced the in-memory PostgreSQL I built at work for E2E tests (github.com/stackframe-projects)283 points by n2d4 20 hours agohidepastfavorite69 comments n2d4 20 hours agoHey HN! For a few months, I've been building an in-memory version of Postgres at work. It has full feature parity with production databases. The cool thing about it is that you don't need any external processes or proxies. If your platform can run WASM (Node.js, browser, etc.), it can probably run pgmock. Creating a new database with mock data is as simple as creating a JavaScript object. It's a bit different from the amazing pglite [1] (which inspired me to open-source pgmock in the first place). pgmock runs an x86 emulator with the original Postgres inside, while pglite compiles a Postgres fork to native WASM directly and is hence much faster and more lightweight. However, it only supports single-user mode and a select few extensions, so you can't connect to it with normal Postgres clients (which is quite crucial for E2E testing). Theoretically, it could be modified to run any Docker image on WebAssembly platforms. Anything specific you'd like to see? Happy hacking! [1] https://github.com/electric-sql/pglite reply samwillis 13 hours agoparentThis looks really cool, awesome work! Correct on PGlite only being single user at the moment, and that certainly is a problem for using it for integration tests in some environments. But I'm hopeful we can bring a multi-connection mode to it, I have a few ideas how, but it will be a while before we do. There are a few other limitations with PGlite at the moment (related to it being single user mode), such as lacking support for pg_notify (have plans to fix this too). Whereas with this it should \"just work\" as it's much closer to a real Postgres. I think there is a big future for these in-memory Postgres projects for testing, it's looks like test run times can be brought down to less than a 1/4 with them. (I work on PGlite) reply negus 12 hours agoparentprevThat's great. But doesn't the whole concept of E2E test mean that you use real environment without mocking the components? reply refulgentis 12 hours agorootparentExplicitly mentioned in the comment as a drawback. In practice E2E means \"E2E as much as humanly possible\", and I'm glad to see any work that can help. reply sverhagen 9 hours agorootparentIt is, however, humanly possible, to run a real Postgres instance in a Docker container. You may use for example Testcontainers. So, I would not resort to an inferior Postgres \"mock\" for E2E testing. For unit testing (also mentioned in the tagline of the project on GitHub) I could see you wanting something snappier than a real Postgres in Docker, so then... maybe? Purists of some following will tell you to use a mocking framework instead, but I think that something closer to the real thing would be better in all cases. This might be it, just be careful not to lull yourself into a false sense of security about how close to the real thing this is (isn't). reply cqqxo4zV46cp 16 minutes agorootparentThe reality is that testing is as nuanced as anything else in software. Tradeoffs are different in different situations, and reasonable people can disagree about appropriate tradeoffs in a given situation. reply wpietri 5 hours agorootparentprevIt depends on the humans and what they're up to. Fast feedback is a very valuable attribute of a test suite, so reasonable people may well be willing to go down this road. reply justinclift 7 hours agorootparentprevIt does seem to be running a real version of Postgres though, unless I'm misreading? reply sverhagen 7 hours agorootparentOn first reading of the main GitHub page, I didn't take it that way. On second reading, I guess it could go either way. But there's talk about having reimplemented the network stack, so maybe that implies they just repurposed the rest of Postgres. I just don't know enough, happy to take your word. reply nborwankar 9 hours agoparentprevIf it could support the pgvector extension it would be a super fast vector database with all the power of Pg - the relational aspect brings the ability to add and query using rich domain specific metadata usually contained in relational databases. reply J_Shelby_J 5 hours agorootparentI spent last week trying to do that with some of the other pg embeded libs. And then lancedb released their embedded client for rust, so I went towards that. But it's still lacking FTS. So I fell back to sqlite. have some notes here https://shelbyjenkins.github.io/blog/retrieval-is-all-you-ne... reply justinclift 13 hours agoparentprevAs a data point, the online demo seems broken for queries it doesn't like: select foo(); Error.captureStackTrace is not a function That's when using Firefox 124.0.2 on Linux. reply rezonant 11 hours agorootparentYes, that's a nonstandard function provided by v8, so it wouldn't work on Firefox. [1] This can be worked around by just constructing an Error and taking it's stack property, captureStackTrace is just a convenience function, so hopefully they can fix that. [1] https://developer.mozilla.org/en-US/docs/Web/JavaScript/Refe... reply justinclift 3 hours agorootparentThanks for suggesting that. Looks like they've implemented that here: https://github.com/stackframe-projects/pgmock/commit/f80d9fa... It seems to have beendeployed out to the demo already as it's working now. :) reply n2d4 9 hours agorootparentprevMy bad, gotta fix that one! reply waldrews 16 hours agoparentprevOoh! The 'docker image on WASM' thing sounds promising for a wide range of problems. Recently I wanted to run a FFMPEG/SoX pipeline on the client - too many dependencies to easily recompile with Emscripten; could your approach help there? reply jasonjmcghee 14 hours agorootparentThere's already ffmpeg wasm. I've used it in projects. Works great. https://github.com/ffmpegwasm/ffmpeg.wasm reply waldrews 9 hours agorootparentThanks, looks good... doesn't look like there's a SoX though. reply jasonjmcghee 4 hours agorootparentI googled - haven’t tried it https://github.com/rameshvarun/sox-emscripten There’s also https://github.com/IPS-LMU/wasmsox Lots of cool software that people hack together for fun. Just site:github and search! reply n2d4 15 hours agorootparentprevYeah, that should be possible! Though, for audio processing, the performance will probably be terrible (because it's all emulated). reply MuffinFlavored 10 hours agoparentprev> pgmock runs an x86 emulator with the original Postgres inside Why can't Postgres compile to WASM instead of x86? reply anarazel 9 hours agorootparentPostgres uses multiple processes, shared memory etc. The single user thing that OP referenced is single user because of that... reply rsyring 15 hours agoprevWhy not just run Postgres with it's files on a ramdisk? Update: this can apparently run in a browser/Node environment so can be created/updated/destroyed by the tests. I guess I'm too much of a backend dev to understand the advantage over a more typical dev setup. Can someone elaborate on where/when/how this is better? reply n2d4 15 hours agoparentThat's more or less what happens inside the emulator (the emulated disk is an in-memory 9P file system). It's in WebAssembly because that makes it more portable (same behaviour across platforms, architectures, and even in the browser or edge environments), and there are no external dependencies (not even Docker). Because the emulator lets us boot an \"already launched\" state directly, it's also faster to boot up the emulated database than spinning up a real one (or Docker container), but this was more of a happy accident than a design goal. reply peter_l_downs 8 hours agorootparentCan you give a specific / concrete example of why I would want to use this instead of running a postgres server a different way (docker, binary, whatever) and having the tests connect to that server? I really don't understand when this would be useful. reply augunrik 6 hours agorootparentI have a customer that is not allowed to run Postgres natively or docker at all (bc of security). They could use this, I guess. reply peter_l_downs 6 hours agorootparentWow, ok thanks that makes sense — I never thought of an environment like that. reply lelandbatey 7 hours agorootparentprevThese kinds of in-process/in-memory versions of dependencies take the startup time of those test from minutes/seconds to milliseconds, allowing you to run your tests vastly faster. That's a game changer for developer productivity. What's great is that your code still just depends on \"postgres\", so you can test against this in-memory version most of the time then occasionally (such as in CI) run that same suite but either a \"real\" postgres as a way to make SURE you're not missing anything. reply peter_l_downs 6 hours agorootparentOK, that goal makes sense, thanks for explaining. For what it's worth I'm pretty sure you can do this with postgres, tmpfs, and template databases — see my project, pgtestdb [0]. I haven't done a formal perf measurement, but each test would get a fresh and migrated database in about 20ms on my local machine. The setup described runs postgres in a container for convenience, but you could probably also just run a postgres binary and store the data on a memory-backed filesystem. [0] https://github.com/peterldowns/pgtestdb reply stickfigure 5 hours agorootparentprevYou can bring \"real postgres\" test startup times back to milliseconds with CREATE DATABASE ... FROM TEMPLATE. Every test gets a fresh database (without having to run migration scripts) and the step takes millis. reply CogitoCogito 5 hours agorootparentYeah I’d be surprised if this method weren’t just as fast. And if it’s not, would the difference just be to run the server with a ramdisk and to maybe turn off some of the durable setting to speed things up ( https://www.postgresql.org/docs/current/non-durability.html ). reply danmur 8 hours agorootparentprevMe either. It's a couple hundred lines of code to make a very comprehensive fixture using the real postgres, and it supports all extensions, including exotic ones you make yourself. reply gchamonlive 12 hours agorootparentprevYou could also use memory state dump from a microvm manager like firecracker and have the state replicated reply zer00eyz 11 hours agoparentprevThe whole purpose of End to End testing is that your testing the system in a real state. It's an emulation of your live environment. Because of that you can do interesting things like find out what happens if you pull the plug or run out of disk or .... The moment that you shove a mock in there, your unit testing. Effective but not the same. One of the critical points of E2E is that without mocks you know that your tests are accurate. Because this isnt Postgres I'm testing it every time and not that system. >> Can someone elaborate on where/when/how this is better? If your building PG for an embedded, light weight, or under powered system then this would make sense for verification testing before real E2E testing that would be much slower. (a use case I have) Other than that its just a cool project and if you ever need a PG shim it's there. reply wpietri 5 hours agorootparentI think you're being a little absolutist about this. Swapping out a possibly equivalent database engine does not turn anything into a unit test, which is defined by testing individual units of code in relative isolation. You can argue that it's not true end to end testing. But almost every E2E test I've seen involves some compromises compared with the true production environment to save money, time, or effort. reply peter_l_downs 8 hours agorootparentprev> If your building PG for an embedded, light weight, or under powered system then this would make sense for verification testing before real E2E testing that would be much slower. (a use case I have) If this is actually just Postgres running in an x86 emulator (*edit: originally this said \"compiled to wasm\"), then how could this be faster than Postgres in any given environment? I don't understand — if it were faster, wouldn't you just want to deploy this in prod in your weird environment rather than Postgres? Why limit this to mocking? reply RussianCow 7 hours agorootparentPresumably, it's faster to boot and for tests because it doesn't need to access an actual file system; everything is in memory. That doesn't mean it would be any faster in production, and in fact, it wouldn't be useful in that environment even if it was. reply peter_l_downs 6 hours agorootparentUnderstood, thank you. reply majikandy 3 hours agorootparentprevUntil you trust every part of the mock behaves the same as every part of the real database you use… most often the db is your boundary with nothing further downstream. At that point it really is just a faster disposable database, and totally is valid acceptance tests for the e2e system. Also nothing stops you from using a mock for some tests and a real database for others. It just comes down to trust. reply lelandbatey 6 hours agorootparentprevNah, by having in-memory versions of your dependencies, in-memory versions which fulfill the same interfaces as those used in your E2E tests (or the majority of your E2E tests) you unlock running your entire E2E tests suite in milliseconds-to-seconds instead of minutes-to-seconds. And because they're E2E tests that work with any implementation, you can still run your exact same test suite against your \"real\" E2E dependencies in a CI step to be super sure both implementations behave the same. I've done this across multiple jobs, and it's amazing to be able to run your \"mostly-E2E\" tests in 1-2 seconds while developing and the same suite in the full E2E env in CI. It makes developing with confidence so fast and mostly stress free (diverging behavior is admittedly annoying, but usually rare). I highly recommend using these if feasible. reply orphea 15 hours agoparentprevI don't get it either. I feel like this is so much unnecessary code, an emulator, a network stack... Why not use something like https://testcontainers.com/? Is a container engine as an external dependency that bad? reply medellin 13 hours agorootparentIt is annoying is you want to run your teat inside a container for ci and now you are running a container in a container and all the issues that come with it. reply peter_l_downs 8 hours agorootparentWhy would the postgres container need to be nested inside another container? Why not just have the CI environment also run a Postgres container, along side your tests, and give your tests a `POSTGRES_URL` environment variable? Or why even bother running Postgres in a container, why not just run the Postgres binary on the host that's running your tests in the container? reply dorianmariefr 12 hours agorootparentprevwhich issues? reply c0balt 12 hours agorootparentDepending on the setup it can be a pain to get nested containers working sometimes. There is, e.g., Docker In Docker but this often required a privileged host container which is often not provided in CI/CD pipelines. reply j45 11 hours agorootparentWhich issues/pains with getting nested containers? I am aware of only a few settings that make a container nestle, or not, whether it is a vm, lxc/lxd type container, etc. reply bastawhiz 13 hours agorootparentprevIt's the same amount of code and on Mac you still run a full VM to load containers (with a network stack), so I'm not really sure what your point is. If anything it's less code because the notion of the container is entirely abstracted away, and the whole thing is entirely a wasm dependency that you load as a normal import. reply hamandcheese 9 hours agorootparentprevThe fact that this can run in-process is a big deal, as it means you don't have to worry about cleanup. As soon as you have external processes that your tests depend on, your tests need some sort of wrapper or orchestrator to set everything up before starting tests, and ideally tear it down after. In 90% of cases I see, that orchestration is done in an extremely non-portable way (like leveraging tools built in to your CI system) which can make reproducing test failures a huge pain in the ass. reply orphea 48 minutes agorootparentYour test framework probably provides you with hooks for one-time setup and cleanup. This is where you start and delete your external dependencies. reply theage 1 hour agoprevFor prisma/nodejs devs who just want postgres-in-a-can for local dev you are better off using the recently released serverizing of pglite, pglite-server: https://github.com/kamilogorek/pglite-server It's faster, can persist data to fs, though less stable under heavy use than the full x86 emu e2e test server. I found pglite-server uses only 150MB ram compared to 830MB for pgmock-server. You can then use dotenv to checkout a new .env.local with updated DATABASE_URL for all your nextjs/prisma package.json run scripts DATABASE_URL=\"postgresql://postgres@localhost:5432/awesomeproject\" \"db:pushlocal\": \"dotenv -e .env.local -- pnpm prisma db push\" Very easy to add to any project, No wonder neon is sponsoring this space. reply xlii 3 hours agoprevHate to be w downer but I’d never consider this for use. For trivial applications maybe it’d work, but with more complexity like anything that has risk of deadlocking or depends on the database shape and such solution subtracts from value as even small shift in behavior can snowball into critical problems. Today I lean towards resource constrained E2E environment so that local test runners have opportunity to break if someone write anything grossly underperforming. Not to mention that snapshotting DB after second and distributing this snapshot to test partitions is super fast and many times shaved multiple minutes from test suites. It’s an interesting idea and definitely great learning experience but I think that target audience is limited. reply rickette 15 hours agoprevI used to run all kinds of (custom) fake in-memory servers in my tests. Nowadays I just run the real thing using Testcontainers (https://testcontainers.com) reply tumidpandora 11 hours agoprevOff-topic, but the title confused me a bit - \"...I built at work.\" Doesn't this imply that the intellectual property for this project belongs to your employer, assuming you used resources from work? If so, are you technically allowed to open-source it? reply n2d4 9 hours agoparentWe're a startup, open-sourcing was as easy as getting the rest of the team's approval. reply Gracana 11 hours agoparentprevStackframe owns the repo and the LICENSE file says \"Copyright 2024 Stackframe.\" I think the author works at Stackframe. reply andy_ppp 3 hours agoprevWhy not just use Docker and have a different testing database? Elixir does this, and the testing framework wraps each test into a transaction that is rolled back for isolation. Be interesting to know the advantages of this approach! reply crakhamster01 8 hours agoprevThis is pretty neat! Some questions, if you're able to answer: * What was the inspiration for developing this project at work? Was running Postgres in a Docker container too slow? * What did your CI setup for E2E tests look like before and after integrating pgmock into the flow? * Was migrating over to this solution difficult? Thanks! reply drzaiusx11 11 hours agoprevHow does this compare to H2 in postgres compatibility mode? reply davvid 3 hours agoprevIf you're into this kinda thing and are in python land you might also appreciate my friend's project in a similar vein https://github.com/ugtar/pg_temp reply camgunz 1 hour agoprevWow very cool that you got to open source this, thanks! reply segmondy 8 hours agoprevDump your prod data, scrub all the sensitive data, truncate all the unneeded tables like your log tables. You have a good dev copy, replicate for dev ,qa, e2e, etc. Those extensions, triggers, functions, views, indexes, data are what you need for e2e. reply KingOfCoders 2 hours agoprevI've been using template databases in the past, to copy a new database for each test. Trying out some in-memory ideas, there was not too much difference to a fast SSD. reply geuis 8 hours agoprevI've used pgmem https://github.com/oguimbal/pg-mem for the last couple of years for the same thing. reply fullstackchris 1 hour agoprevThis is amazing! I've been looking for an in-memory postgres mock for integration tests for ages! reply herpdyderp 11 hours agoprevThis might be a stupid question, but do you know how this might be used with a Prisma client? reply andrelaszlo 15 hours agoprevCool! Which pg version is this based on? reply justinclift 13 hours agoparentThe online demo seems to be PG 14.5. Output from \"SELECT version()\" is: \"version\": \"PostgreSQL 14.5 on i686-buildroot-linux-musl, compiled by i686-buildroot-linux-musl-gcc.br_real (Buildroot 2022.08) 12.1.0, 32-bit\" reply vivzkestrel 7 hours agoprevsupport for orms like sequelize, prisma, drizzle for testing? reply revskill 5 hours agoprev [–] Just use docker. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "pgmock is an in-memory PostgreSQL mock server suitable for unit and end-to-end tests, operating on WebAssembly in Node.js and browsers.",
      "It offers complete PostgreSQL feature compatibility, ideal for testing scenarios, with intentions to transition to native WebAssembly for better performance.",
      "The tool replicates a network stack in JavaScript, allowing TCP connections on platforms restricting raw socket access; contributions are encouraged via their Discord server."
    ],
    "commentSummary": [
      "Developers are exploring in-memory versions of PostgreSQL for quicker end-to-end testing, debating the pros and cons of mock databases to enhance testing efficiency and developer productivity.",
      "Alternative solutions such as testcontainers and environment variables are being considered to handle sensitive data and modify database URLs.",
      "The discussion underscores the significance of mimicking actual environments for testing purposes and the benefits of reducing test execution durations."
    ],
    "points": 283,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1712495623
  },
  {
    "id": 39960717,
    "title": "Mixture-of-Depths: Efficient Compute Allocation in Transformer Models",
    "originLink": "https://arxiv.org/abs/2404.02258",
    "originBody": "Computer Science > Machine Learning arXiv:2404.02258 (cs) [Submitted on 2 Apr 2024] Title:Mixture-of-Depths: Dynamically allocating compute in transformer-based language models Authors:David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, Adam Santoro View PDF HTML (experimental) Abstract:Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50\\% faster to step during post-training sampling. Subjects: Machine Learning (cs.LG); Computation and Language (cs.CL) Cite as: arXiv:2404.02258 [cs.LG](or arXiv:2404.02258v1 [cs.LG] for this version) Submission history From: Adam Santoro [view email] [v1] Tue, 2 Apr 2024 19:28:11 UTC (1,763 KB) Full-text links: Access Paper: View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.LGnewrecent2404 Change to browse by: cs cs.CL References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) IArxiv recommender toggle IArxiv Recommender (What is IArxiv?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=39960717",
    "commentBody": "Mixture-of-Depths: Dynamically allocating compute in transformers (arxiv.org)243 points by milliondreams 18 hours agohidepastfavorite59 comments whimsicalism 17 hours agoI think more complicated routing is absolutely going to become more common. Specifically, I think at some point we are going to move to recursive routing, ie. pass back through a set of experts again. In the future, 'chain-of-thought' will happen internal to the model recursively reply optimalsolver 16 hours agoparentWe can name these hypothetical objects Recursive Neural Networks. reply whimsicalism 16 hours agorootparenti know you're jesting but RNNs are recursive along the sequence length where I am describing recursion along the depth. reply benreesman 5 hours agorootparentDepthwise RNN? reply refulgentis 12 hours agorootparentprevLike decode the next token, then adjust what you're paying attention to, then decode it again? reply nine_k 11 hours agorootparentIsn't it the only way to, say,understand a pun? reply refulgentis 10 hours agorootparentThat is exactly how LLM inference is performed, so I'm being cheeky (I'm 99% sure anyone proposing anything in this thread is someone handwaving based on limited understanding) reply whimsicalism 9 hours agorootparentYou would be wrong, but that is fine. Been working with attention since 2018. Why assume I know little and leave snarky comments (and basically a repetition of the prior joke at that, subbing RNN for transformer)? reply refulgentis 9 hours agorootparentTo playfully invite for you to participate in conversation further, so that I may humbly learn from you. \"I don't know what you're talking about\" seemed too spartan and austere and aggressive, and you reciprocated politely, if again sparsely, when the other person playfully invited you to elaborate. reply webmaven 7 hours agorootparentWell, you've now made your original intent specific, but in case you didn't draw the requisite lesson I'll make that explicit. Because text has less bandwidth than almost any other medium, certain forms of humor are much more likely to be understood (in this case, your \"gentle playfulness\" was taken to be snark, sarcasm, and point scoring). If you insist on using this and similar forms of humor that, ordinarily, depend quite strongly on intonation to convey intent, you'll have to be much more explicit to avoid being misunderstood. You are going to have actually state your intent explicitly as part of your communication. This need not entirely destroy the humor, for example, you might try something like this: And so I say to you (playfully, sir, playfully): etc. Or this: Yadda yadda yadda. (I kid, I kid!) The Internet-native forms of this are the humble ;-) or the newer j/k, but I find that it is all too easy to overlook a 3-character sequence, particularly if the passage being so marked is even as long as a single paragraph, but they can serve their purpose when used for the commonplace one-liner. reply anamax 5 hours agorootparent\"blah, blah, blah\" can be an expression of scornful boredom or the utterance of a vampire. reply refulgentis 4 hours agorootparentprevYou are painfully boring reply conradev 12 hours agorootparentprevYep: https://arxiv.org/abs/2305.13048 reply p1esk 5 hours agorootparentprevWe did: https://en.m.wikipedia.org/wiki/Recursive_neural_network reply miven 15 hours agoparentprevWhat you describe here sounds a little like the line of work centered around Universal Transformers, which basically process the input embeddings through a single transformer block multiple times with a separate module deciding when the embeddings have been cooked enough and can be pulled out of the oven so to speak. Even more in line with the idea of \"experts\" there's a paper from last year on Sparse Universal Transformers in which they combine a universal transformer with sparse mixture of experts, so it's up to the gating mechanism to decide which transformer blocks and in which order are to be used in shaping the embeddings. This really isn't my specialty but from what I gathered these are tricky to train properly, and require more overall compute during inference to reach comparable results to their vanilla transformer counterparts. It's an interesting direction nonetheless, having an upper bound on the number of computation steps per token is, in my opinion, one of the major downsides of the classical transformer architecture. reply rfdearborn 8 hours agoparentprevThe trendline is definitely toward increasing dynamic routing, but I suspect it's more so that MoE/MoD/MoDE enable models to embed additional facts with less superposition within their weights than enable deeper reasoning. Instead I expect deeper reasoning will come through token-wise dynamism rather than layer-wise -- e.g., this recent Quiet-STaR paper in which the model outputs throwaway rationale tokens: https://arxiv.org/abs/2403.09629 reply londons_explore 14 hours agoparentprevI think the reason this hasn't been done is you have no way to decide how many recursions are necessary at train time. And if you pick a random number/try many different levels of recursion, you 'blur' the output. Ie. the output of a layer doesn't know if it should be outputting info important for the final result, or the output that is the best possible input to another round of recursion. reply Nesco 1 hour agorootparentI have been thinking about this topic for some time. It might be done using the energy of the token. If it's still higher than an energy limit, then process it again, and increase the energy limit. The energy could be computed using log-sum-exp: https://openreview.net/pdf?id=Hkxzx0NtDB reply whimsicalism 13 hours agorootparentprevYes, I think training this model would be hard. Perhaps something akin to how MoEs are trained where you impose some sort of loss distribution to encourage equitable routing, but for recursion. reply hackerlight 2 hours agorootparentLook at the human brain for useful analogies? The default mode network does recursive/looping processing in the absence of external stimuli and world interaction. Multiple separate modules outside of the network are responsible for stopping and regulating this activity. reply pizza 13 hours agorootparentprevYou could just learn the right estimated number of recursions, also passing 'backtracking'/'state' information at the next nested level. Kind of like how state space models encode extractible information via a basis function representation, you could encode extractible recursion state information into the embedding. See also transformers that can learn to recognize n-deep balanced parentheses (Dyck-n languages) reply sdenton4 12 hours agorootparentprevThis is actually how EfficientNet trains, using random truncation of the network during training. It does just fine... The game is that each layer needs to get as close as it can to good output, improving in the previous activation quality. reply imranq 14 hours agoparentprevAttention is basically routing, these other routing schemes put a less fine-grained choice for the model, which potentially makes it easier to train reply whimsicalism 14 hours agorootparentHow is attention basically routing? reply visarga 13 hours agorootparentIt routes values based on linear combinations taken from the attention map. reply whimsicalism 12 hours agorootparentBut all of those values are created using an MLP with the same parameters, so there is no routing to different parameters. reply pizza 11 hours agorootparentprevThink of it like an edge flow matrix reply whimsicalism 10 hours agorootparentThat doesn't clarify it for me. The same parameters are being used for every layer for every token. Yes, there is this differentiable lookup in attention like in MoE - but routing is about more than just differentiable lookup, it is about selecting on parameters not state. reply mountainriver 6 hours agoparentprevMost of the original MoE implementations around LLMs were in fact recursive reply rajnathani 6 hours agorootparentCould you please elaborate? reply digdugdirk 16 hours agoparentprevSee, this is where my understanding of LLMs breaks down. I can understand one token going through the model, but I can't understand a model that has different \"experts\" internally. Do you have any resources or links to help explain that concept? reply HarHarVeryFunny 8 hours agorootparentThe \"mixture of experts\" goal is to add more parameters to the model to make it more powerful, without requiring any more compute. The way this is done is by having sections of the model (\"experts\") that are in parallel with each other, and each token only going through one of them. Think of it like a multi-lane highway with a toll booth on each lane - each car only drives on one lane rather than using them all, so only pays one toll. The name \"experts\" is a bit misleading, since each expert (\"highway lane\") is not really specialized in any obviously meaningful way. There is a routing/gating component in front of the experts that chooses on a token by token basis (not sentence by sentence!) which \"expert\" to route the token to, with the goal of roughly load balancing between the experts so that they all see the same number of tokens, and the parameters in each expert are therefore all equally utilized. The fact that the tokens in a sentence will be somewhat arbitrarily sent through different \"experts\" makes it an odd kind of expertise - not directly related to the sentence as a whole! There has been experimentation with a whole bunch of routing (expert selection) schemes. reply whimsicalism 16 hours agorootparentprevIt is still just one token going through the model. I actually think mixture-of-expert is a bit of a misnomer, the 'experts' do not really necessarily have super distinct expertise. Think of it more as how neurons activate in the brain - your entire brain doesn't light up for every query, now in neural networks the same thing happens (it doesn't fully light up for every query). Don't really know a resource besides the seminal Noam Shazeer paper, sorry - I'm sure others have higher-level. reply nl 5 hours agoprevMost important paper of 2024. The idea that we want models not to have to use the same amount of compute for every token has been around for a while. This is the first compelling mechanism I've seen for doing it. > Equipped with these new methods, we can sample autoregressively by choosing to route tokens to or around a block based on the router’s output, which does not depend on any information from future tokens. We provide empirical evidence that this is a relatively easy auxiliary task that quickly achieves 99% accuracy. Does anyone else find this is a bit surprising? reply namibj 2 hours agoparentSparse Universal Transformer is older and already did routing-based early termination... reply macrolime 1 hour agoprev\"This is more computationally efficient than performing a full content-based lookup across an entire memory buffer for each step in the future, and could be one step towards drastically increasing the context-length available for making a prediction.\" Is this how they get a context window of 10 million tokens? Or are they refering to even longer context windows in the future? reply panqueca 10 hours agoprevSimplified Intro Version: Imagine you have a smart assistant that can understand and process the words you say to it. Usually, this assistant pays equal attention to every word you say, no matter how important or unimportant each word is to the overall meaning of your message. Now, imagine that we found a way to teach the assistant to be smarter about how it uses its \"brain power.\" Instead of giving equal attention to every word, the assistant learns to focus more on the words that are most important for understanding what you mean. It can even adjust this focus on the fly, paying more attention to different words depending on the context of your message. To make sure the assistant doesn't get overwhelmed, we also set a limit on how much total \"brain power\" it can use at any given time. It's like giving the assistant a budget and saying, \"You can only spend your brain power on a certain number of words at a time.\" The assistant then has to decide which words are most important to focus on. Even with this limit, the assistant is still flexible in how it uses its brain power. It might spend more on certain words and less on others, depending on what you're saying. This means that while we always know the total amount of brain power the assistant is using, it can adapt to different situations and prioritize what's most important. When we teach the assistant using this method, it not only learns to focus its attention intelligently but also does so very efficiently. It can understand you just as well as an assistant that pays equal attention to every word, but it uses less brain power overall. This makes the assistant much faster at responding to you and processing new information. reply dannyw 5 hours agoparentI understand this is ELI5, but doesn’t attention already do this, in the way you described? It pays specific focus to the most contextual words in the prior sequence. reply xcv123 4 hours agorootparentThe way I understood it is that for each token, the attention mechanism itself consumes a fixed amount of processor time. The innovation here is to prioritize tokens so that some tokens have more or less processor time. reply mattmcdonagh 15 hours agoprevI wrote up a bit about it here, from what I could piece together: https://lifeinthesingularity.com/p/googles-breakthroughs-in-... reply datascienced 8 hours agoparentNice writing. Reminds me of New Scientist style. (I like NS so that is a compliment). I think the “explain as you go along but be brief style”. Which is nice for getting a feel for the space. reply yair99dd 2 hours agoprevhu-po does in-depth live-stream reviews of AI papers. highly recommended, here is his take on the mixture-of-depths paper discussed. https://www.youtube.com/watch?v=Teru_qIdB8Y reply rughouse 18 hours agoprevIt’s very similar to Mixture of Experts. But instead of routing tokens to multiple experts, you \"deploy to a single expert which can be dynamically skipped\" reply erikaww 16 hours agoparentMixing these would be pretty cool. Further reduced compute for MoE while keeping the performance. reply GaggiX 16 hours agorootparentIn the paper they already show a mixing of these two with Mixture-of-Depths-and-Experts (MoDE). reply kromem 9 hours agoprevEssentially the second law of thermodynamics for neural networks. Neat! reply nextaccountic 8 hours agoparentCan you elaborate on this analogy? reply maxrumpf 10 hours agoprevThe abstract and the rest of the paper don't really match imo. It's not really allocating more to some sequences, but just introducing ~dropout. Might be different sides to the same coin, but was still a weird read. reply adamsantoro 10 hours agoparentWe spent a fair bit of effort ensuring we were accurate with the language and claims, so we're happy to take any feedback and make updates in subsequent versions. However, I don't see where we claim that MoD allocates more to some sequences and not others (specifically, the abstract says \"transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence\". That said, it's a pretty simple change to make the approach work in the way you describe (allocating more to some sequences and not others) by changing the group across which the top-k works. In the paper we use the time (sequence) dimension, but one could also use the batch * time dimension, which would result in asymmetric allocation across sequences reply hackerlight 7 hours agoparentprevDropout is at train time this is at inference time. Dropout is random this is determined. Can't compare them. reply modeless 7 hours agoprevIt's a start but it's disappointing that half the layers still have to process every token. It seems like we ought to be able to get to 90% or even 99% savings when these models currently allocate the same compute for outputting \"the\" as they do for outputting the first digit of the answer of a complicated math problem. reply aiddun 7 hours agoparentSpeculative decoding does this to an extent - using a smaller model to generate its own predictions and putting them in the batch of the bigger model until they diverge https://huggingface.co/blog/whisper-speculative-decoding reply brrrrrm 6 hours agorootparentIt doesn’t. It simply trades compute efficiency by transposing matrix multiplications into “the future.” It doesn’t actually save FLOPs (uses more) and doesn’t work at large batch size reply barrenko 15 hours agoprev [–] Are we going to hit bullseye? reply ein0p 15 hours agoparent [–] This only cuts compute by “up to” 50% and only during inference. Quadratic dependence on context size remains, as do the enormous memory requirements. For something to be considered a bulls eye in this space it has to offer nonlinear improvements on both of these axes, and/or be much faster to train. Until that happens, people, including Google will continue to train bog standard MoE and dense transformers. Radical experimentation at scale is too expensive even for megacorps at this point. reply mdale 12 hours agorootparentMakes opportunities for smaller companies to innovative/experiment to offer solutions / acquisition targets where tighter inference compute requirements makes or breaks the experience but larger training cost is less of a concern (such as embedded or local runtime use cases) reply ein0p 12 hours agorootparentBefore those opportunities are available to you, someone would need to spend a few million dollars and train a competitive model with this, and then release it under a license that allows commercial use. This is out of reach for the vast majority of smaller companies. These models only excel at large parameter counts, even for narrow problems. This is especially true in the case of MoE, which is a way to push the overall parameter count even larger without lighting up the whole thing for every token. reply visarga 13 hours agorootparentprev [–] Yeah all attempts at reducing complexity from quadratic to linear failed, only Mamba still has a chance, but it's not tested on large models and only provides a speedup at for 2000+ tokens. That was to be expected as small sequences have very small memory requirements for transformers, but recursive architectures use the same hidden size. So when recurrent hidden size > sequence length, the old transformer is faster. reply ein0p 12 hours agorootparent [–] It's more subtle than that IMO. They haven't necessarily \"failed\" - they just don't have the \"superpowers\" that the metrics used to evaluate such systems are aimed at. E.g. no such linear method devised so far (that I know of, at least) is able to do very high recall point retrieval in long context _and_ effective in-context learning simultaneously. You get one or the other, but not both. But as far as the metrics go, high recall retrieval in long context is easier to for the researcher to demonstrate and for the observer to comprehend - a typical needle/haystack setting is trivial to put together. It is also something that (unlike in-context learning) humans are usually very bad at, so it's perceived as a \"superpower\" or \"magic\". In this case e.g. Mamba being more human like due to its selective forgetfulness is currently playing against it. But whether it's \"better\" per se will depend on the task. It's just that we do not know how to evaluate most of the tasks yet, so people keep trying to find the proverbial keys under the lamp post, and measure what they can to make progress, and thereby keep their efforts lavishly funded. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper introduces the Mixture-of-Depths method for allocating compute dynamically in transformer-based language models, optimizing efficiency and flexibility in FLOP allocation across model depth and time dimensions.",
      "This method caps the number of tokens participating in self-attention and MLP computations at each layer using a top-k routing mechanism, resulting in models that maintain baseline performance while demanding fewer FLOPs per forward pass and faster post-training sampling.",
      "It highlights the efficiency and effectiveness of the Mixture-of-Depths approach in compute allocation, demonstrating its potential in improving the performance of language models."
    ],
    "commentSummary": [
      "The forum discusses training models with recursive routing similar to Mixture of Experts (MoE), proposing the term \"Recursive Neural Networks\" for these models.",
      "Participants explore topics like Universal Transformers, sparse mixture of experts, and the challenges of training models with recursive processing, aiming to enhance computational efficiency and context-length for predictions.",
      "Analogies from the human brain and the concept of Mixture-of-Depths-and-Experts (MoDE) are examined concerning MoE, considering high memory demands and limited innovation options for smaller firms."
    ],
    "points": 243,
    "commentCount": 59,
    "retryCount": 0,
    "time": 1712497325
  },
  {
    "id": 39959946,
    "title": "Exploring Integer Square Root Instructions in Processors",
    "originLink": "https://retrocomputing.stackexchange.com/questions/29787/did-any-processor-implement-an-integer-square-root-instruction",
    "originBody": "Stack Exchange Network Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers. Visit Stack Exchange Loading… Tour Start here for a quick overview of the site Help Center Detailed answers to any questions you might have Meta Discuss the workings and policies of this site About Us Learn more about Stack Overflow the company, and our products current community Retrocomputing help chat Retrocomputing Meta your communities Sign up or log in to customize your list. more stack exchange communities company blog Log in Sign up Retrocomputing Home Questions Tags Users Companies Unanswered Teams Ask questions, find answers and collaborate at work with Stack Overflow for Teams. Explore Teams Create a free Team Teams Ask questions, find answers and collaborate at work with Stack Overflow for Teams. Explore Teams Teams Q&A for work Connect and share knowledge within a single location that is structured and easy to search. Learn more about Teams Did any processor implement an integer square root instruction? Ask Question Asked 3 days ago Modified 2 days ago Viewed 30k times This question shows research effort; it is useful and clear 34 Save this question. Show activity on this post. Has any processor ever implemented an integer square root instruction? Obviously, floating-point square root instructions are quite common, but I've never seen one specifically for integers. One close candidate I know of is the Nintendo DS, which contained a memory-mapped integer divider/square rooter. This was helpful for 3D calculations, given that its ARM processor had no FPU or hardware divider. However, it's not a native processor instruction. history instruction-set Share Improve this question Follow Follow this question to receive notifications edited 2 days ago Raffzahn 223k2222 gold badges631631 silver badges918918 bronze badges asked Apr 5 at 6:01 v-robv-rob 79711 gold badge55 silver badges1010 bronze badges 7 2 tough one; square rooting is something that you don't want to do combinatorically in a single clock cycle, or else your maximum clock rate will go dooooown, and it uses the same algorithmic components that an integer multiply and division need. This makes it a prime candidate for implementation in iterative microcode. Now, what's the difference between a CPU that implements an isqrt instruction in microcode vs software that implements a isqrt function on a CPU that has no isqrt instruction, precisely? – Marcus Müller 2 days ago 3 Non-iterative implementation for large input sizes get unhandy very quickly, and for small sizes, simple lookup tables might be quite performant. What is the minimum input size you would consider an implementation? – Marcus Müller 2 days ago By \"processor\" do you mean microprocessor? Or do you intend to include mainframes, DSPs, etc? – DrSheldon 2 days ago @DrSheldon I meant it in a pretty general sense of the word. If it's got an instruction set that includes an isqrt instruction, it'll probably do :) – v-rob 2 days ago 1 I add the isqrt instruction only because I noticed that the bignum library had it as a primitive. It was obvious to me that it's not trivial to implement, particularly if someone were to need that for large integers. I went through the code, in that bignum library and noticed it was poor. I put fixes in to make it faster, and then due to the IKEA effect of having made it mine by working on it, I had to expose it in the language. – Kaz 16 hours agoShow 2 more comments 3 Answers Sorted by: Reset to default Highest score (default) Date modified (newest first) Date created (oldest first) This answer is useful 40 Save this answer. Show activity on this post. Yes. The Harris RTX 2000 Forth CPU offered a multi-step square root instruction, as did its military-grade sibling, the RTX 2010. There may have been others, but that is the one I know of. See: https://users.ece.cmu.edu/~koopman/stack_computers/sec4_5.html Share Improve this answer Follow Follow this answer to receive notifications answered 2 days ago RichFRichF 9,22644 gold badges3232 silver badges5656 bronze badges 3 Nice, do you know any more detailed description? – Raffzahn 2 days ago 2 @Raffzahn Not off the top of my head. But I do see when searching, you should specify Harris as part of the search. Otherwise the results are mostly pages relating to a series of NVidia GPU boards. As far as I can tell, there is no commonality besides the general model number. – RichF 2 days ago 6 Interesting; according to the reference manual (users.ece.cmu.edu/~koopman/forth/…), it's an iterated square root where you have to run 1 setup instruction and then 15 step instructions to get the final answer. I've seen that done for division on other processors. And of course it would be a Forth CPU that would have such an esoteric instruction, heh. – v-rob 2 days ago Add a commentThis answer is useful 34 Save this answer. Show activity on this post. Is 1946 retro enough? From Wikipedia: ENIAC used four of the accumulators (controlled by a special multiplier unit) to perform up to 385 multiplication operations per second; five of the accumulators were controlled by a special divider/square-rooter unit to perform up to 40 division operations per second or three square root operations per second. ENIAC accumulators operated on decimal integers. Share Improve this answer Follow Follow this answer to receive notifications answered 2 days ago John DotyJohn Doty 2,47477 silver badges1313 bronze badges Add a commentThis answer is useful 17 Save this answer. Show activity on this post. It's not that easy. The most efficient method to calculate square root is to calculate inverse/reciprocal of the square root using Newton-Raphson iterations, and then multiply it with the original. This is best known as the \"Quake method\" (see also Where did Fast InvSqrt() come from?). The more modern version used by contemporary CPU and GPUs are generalized into two instructions, one for estimating the initial guess (e.g. frsqrte of ARMv8), another to run the following iterations (e.g. frsqrts of ARMv8). Single-instruction version of sqrt is a micro-coded or pseudo-instruction version of these two instructions. The prerequisite for all of this is a multiplier. If you want to calculate FP (i)sqrt, then you need a (fast) FP multiplier, which all FPUs have. If you want to calculate integer (i)sqrt, then you need a (fast) integer multiplier, which most CPUs don't have (historically). Otherwise it would be called a DSP. To make it better, you need a (fast) multiplier that is twice the width of your input to have sufficient precision, which most CPUs definitely don't have until \"relatively\" recently (relative to RetroComputing). And precision matters, or not? If you look at the \"Quake method\" closely, you notice that one of the iterations was commented out. There are a lot of use cases where the extreme precision isn't necessary and it'll be better to leave the choice of precision/speed trade off to programmers. isqrt was intentionally separated into fsqrte and fsqrts on ARMv8 exactly for this reason: so that the programmer can adjust the number of fsqrts for the desired speed and accuracy tradeoff. So I don't quite agree to the statement that single instruction sqrt is very common. It's there because the IEEEE754 and the C stand math library requires it (for the flag bits and exceptions), but that doesn't mean it's frequently used. Further reading SPRC542 TI's math library for C64x DSP (8-issue VLIW CPU with two 32x32=64 multipliers). In this library _iq _IQisqrt is implemented using Newton-Raphson iterations and _iq _IQsqrt is calculated by multiplying the original with the isqrt. The source code is available on request. SPRUGG9 TMS320C64x+ IQmath Library User's Guide.The user guide for SPRC542. My implementation of square root using binary search, that doesn't depend on a multiplier. Only basic ALU instructions are used. It is vigorously undocumented. I have no idea what I wrote but it seems to work. . unsigned int usqrt(unsigned int x){ unsigned int a=0; unsigned int masksq=0,mask=0; unsigned int mask_shift=15; for(masksq=1u>2,mask=mask>>1,mask_shift--){ if(x>=masksq){ a=mask; break; } } x-=masksq;//masksq==a*a; mask=mask>>1; mask_shift--; while(mask>0){ unsigned int step=(mask=step){ a|=mask; x-=step; } mask=mask>>1; mask_shift--; } return a; } Share Improve this answer Follow Follow this answer to receive notifications edited 2 days ago Stephen Kitt 122k1717 gold badges505505 silver badges463463 bronze badges answered 2 days ago user3528438user3528438 1,3791111 silver badges1111 bronze badges 11 3 What is recent for RetroComputing? The 8086 (from 1978) definitely did have an integer muliplier, even one that could multiply two 16 bit values into a 32 bit value. These instructions weren't really fast, though, and thus assembly books back then suggested to use the shift-register approach, as this was often faster than using the native imul/idiv instructions. – PMF 2 days ago 3 @PMF An integer multiply instruction doesn't say anything about a integer multiplier. – user3528438 2 days ago 2 In what sense do IEEE-754 and the C standard math library require a single instruction square root? As far as I know, all IEEE-754 cares about is things like memory layout and precision, and all C cares about is semantics. Unless there's something I'm missing, both will allow for the square root to be done in several instructions, a function call, or some memory-mapped funk like the one the Nintendo DS has. – Omar and Lorraine 2 days ago 5 @OmarandLorraine you're right that nothing in IEEE754 requires an instruction. You're wrong about IEEE754 not defining semantics: the standard very much defines operations like multiplications, conversion to integer and square roots, along with their correct result. – Marcus Müller 2 days ago @OmarandLorraine The IEEE 754 requirement for specific semantics of the operation (which are very much defined by the standard) heavily favors a single operation approach if not a single-instruction approach, because that helps ensure that programmers who need those semantics don’t have to think about what they need to do to get those semantics. – Austin Hemmelgarn 2 days agoShow 6 more comments You must log in to answer this question. Not the answer you're looking for? Browse other questions tagged history instruction-set . The Overflow Blog How do mixture-of-experts layers affect transformer models? What a year building AI has taught Stack Overflow Featured on Meta New Focus Styles & Updated Styling for Button Groups Upcoming initiatives on Stack Overflow and across the Stack Exchange network Linked 17 Where did Fast InvSqrt() come from? Related 14 Are there any articles elucidating the history of the POPCOUNT instruction? 12 Did any 360-compatible machine implement registers in core? 87 Why is the processor instruction called \"move\", not \"copy\"? 17 Has there ever been a instruction set architecture that did not require instruction decoding at all? 10 Did any core-memory computers have a read-and-erase instruction? 19 Have there been any instruction sets with an odd register width? 6 Were there any square monitors other than PLATO? 11 IMPI Instruction set: is there any reference? 11 Why did the VAX compatibility mode not implement the MARK instruction? Hot Network Questions Can I safely refinish 250 gallon propane tank? increasing section number independent of chapter Need Help in identifying- analog hall sensor with the numbers 958409 Inequality involving an absolute value Fill a matrix with randomly placed elements that are a certain distance apart My PhD supervisor is doing nothing and is probably involved in academic misconduct. What should I do? Should the banker hit? White noise fluctuation amplitude Why do we need to prove extension lemmas? Are “Data are fixed” in Bayesian viewpoint and “Data are random” in frequentist viewpoint talking about the same thing mathematically? What are examples of \"Official Observations\" in a passport? Variance of Estimator in Casella and Berger Understanding the risks surrounding empty tanks & containers (propane, natural gas, septic, etc.) How to write chords using correct enharmonic spellings? Are all validities isomorphic or equivalent to valid proofs? Any practical use for the underscore variable? Masyu: Four Colours Regressors Became Statistically Insignificant Upon Correcting for Autocorrelation Noise density in 1/f region Difficult sentence from Leibniz's Historia Inventionis Phosphori? Under what circumstances did Aziz Isa Elkun lose contact with his mother? Baseball caught: Ownership? Can one leave? Draw a Fibonacci Swoosh What international law did Ecuador break by storming the Mexican embassy? more hot questions Question feed Subscribe to RSS Question feed To subscribe to this RSS feed, copy and paste this URL into your RSS reader. Retrocomputing Tour Help Chat Contact Feedback Company Stack Overflow Teams Advertising Collectives Talent About Press Legal Privacy Policy Terms of Service Cookie Settings Cookie Policy Stack Exchange Network Technology Culture & recreation Life & arts Science Professional Business API Data Blog Facebook Twitter LinkedIn Instagram Site design / logo © 2024 Stack Exchange Inc; user contributions licensed under CC BY-SA. rev 2024.4.5.7373",
    "commentLink": "https://news.ycombinator.com/item?id=39959946",
    "commentBody": "Did any processor implement an integer square root instruction? (retrocomputing.stackexchange.com)216 points by rwallace 23 hours agohidepastfavorite61 comments corsix 19 hours agoAArch64 NEON has the URSQRTE instruction, which gets closer to the OP's question than you might think; view a 32-bit value as a fixed-precision integer with 32 fractional bits (so the representable range is evenly spaced 0 through 1-ε, where ε=2^-32), then URSQRTE computes the approximate inverse square root, halves it, then clamps it to the range 0 through 1-ε. Fixed-precision integers aren't quite integers, and approximate inverse square root isn't quite square root, but it might get you somewhere close. The related FRSQRTE instruction is much more conventional, operating on 32-bit floats, again giving approximate inverse square root. reply voidbert 17 hours agoparentWhat task benefits from using such a complex instruction so easily dividable in simpler ones for it to be present in aarch64? reply nanidin 17 hours agorootparentNeon is SIMD so I would presume these instructions let you vectorize those calculations and do them in parallel on a lot of data more efficiently than if you broke it down into simpler operations and did them one by one. reply voidbert 16 hours agorootparentYes, but the part that got me was the halving of the result followed by the clamping. SIMD generally makes sense, but for something like this to exist usually there's something very specific (like a certain video codec, for example) that greatly benefits from such a complex instruction. reply creato 16 hours agorootparentIt's probably not about avoiding extra instructions/performance, but making the range of the result more useful and avoiding overflow. Or in other words, the entire instruction may be useless if you don't do these things. reply epcoa 12 hours agorootparentprevThe halving and clamping is nothing particularly remarkable in the context of usefully using fixed point numbers (scaled integers) to avoid overflow. Reciprocal square root itself is a fundamental operation for DSP algorithms and of course computer graphics. This is a fairly generic instruction really, though FRSQRTE likely gets more real world use. reply ekelsen 6 hours agorootparentprevThe halving could come from an intended use in a Newton Raphson iteration of a square root refinement. See for example https://math.mit.edu/~stevenj/18.335/newton-sqrt.pdf The initial guess is the approximate square root, but it needs to be halved as part of the calculation. reply colechristensen 17 hours agorootparentprevInverse square root is for normalizing vectors particularly in computer graphics calculations, it needs to be run a whole lot very fast. https://en.m.wikipedia.org/wiki/Fast_inverse_square_root#Mot... reply hinkley 14 hours agorootparentFamously the magic constant in the Quake engine that nobody remembers inventing. That article does say there’s an SSE instruction rsqrtss that is better. reply fjfaase 21 hours agoprevIs it possible in a single clock-cycle. Yes, with a very large lookup table. It is probably possible to reduce the size depending on how many serial logical gates can be executed within the clock-cycle. Think about that the binary root of 10000 is rather similar to that of 100 only with respect to different number of zero's. reply Findecanor 20 hours agoparentFloating point reciprocal square root estimate (`frsqrte`) instructions are typically implemented as just such a table lookup, indexed by a few bits of the fraction and the LSB of the exponent. The precision is typically limited to similar to bf16 (ARM, RISC-V) or fp16 (x86), so programs are expected to do a few Newton-Raphson iterations afterwards if they want more. reply bonzini 21 hours agoparentprevYou can compute the integer square root in n/2 iterations where n is the number of bits in the source using just shifts and adds. For each step, check if a new bit has to be set in the result n_old by computing n2_new = (n_old + (1int8 table only needs 256 bytes, an int32 -> int32 needs 16 gigabytes. reply DaiPlusPlus 16 hours agorootparentFractal functions are pure, but don’t lend themselves well to memoization nor lookup tables. reply ajb 21 hours agorootparentprevIt isn't, because eventually the size of your logic or table becomes larger than the distance a signal can propagate in one clock tick. Before that, it likely presents practical issues (eg, is it worth dedicating that much silicon) reply willcipriano 18 hours agorootparentHave slower ticks. A planet size CPU that runs at .5 hz but can work on impossibly large numbers. reply Dylan16807 10 hours agorootparent> Have slower ticks. Yes, this solves the stated issue about huge lookup tables. > A planet size CPU that runs at .5 hz but can work on impossibly large numbers. This doesn't make much sense to me, though. If your goal is \"any algorithm\", you'll often have to go a lot slower than .5Hz. A hash-calculating circuit that's built out of a single mountain of silicon could have a critical path that's light-years long. But if your goal is just \"work on impossibly large numbers\", but it's okay to take multiple ticks, then there's no reason to drag the frequency down that low. You can run a planet-scale CPU at 1GHz. CPUs have no need for signals to go all the way across inside a single tick. reply Vecr 6 hours agorootparentYou'd need way better clocks and synchronization circuits than exist now though, but I don't see any pure physical barriers. reply Dylan16807 5 hours agorootparentThe whole thing doesn't need to be on the same clock domain. You can put clock crossings every inch. reply volemo 17 hours agorootparentprevAnd 27.7% [1] of the planet's crust is silicon already! [1] Britannica: Silicon reply gameshot911 18 hours agorootparentprevThat's actually a really fascinating science fiction idea! reply com2kid 15 hours agorootparenthttps://en.wikipedia.org/wiki/Matrioshka_brain :-D It's a topic that has been explored quite a bit in science fiction literature. reply karmakaze 14 hours agorootparentprevRelated to the fallacy of comparing what's slow in our world with what's computable in a simulation of it--there's no requirement for time to tick similarly. reply willcipriano 18 hours agorootparentprevIt's sort of the plot of the Douglas Adam's books. https://hitchhikers.fandom.com/wiki/Earth reply rcxdude 20 hours agorootparentprevWith a sufficiently large chip and a sufficiently slow clock, sure. reply wheybags 20 hours agorootparent\"Give me a lut large enough and an infinite period of time to execute, and I shall simulate the world\" reply dartos 18 hours agorootparent“Every cycle” reply robinduckett 21 hours agorootparentprevI like the Quantum BogoSort as a proof of this /s reply benlivengood 15 hours agoparentprevIt's not as bad for integer square root; you only need to store N^0.5 entries in a greater/lesser-than lookup table: N^2 for all the answers N. Feasible for 16-bit integers, maybe for 32-bit, not for 64-bit. reply treprinum 21 hours agoprevCan't you use the sequence 1 + 3 + 5 + ... + 2k + 1 to get the integer square root of any integer number? It's basically the k of the nearest lower number to your number in this sequence. reply maxcoder4 20 hours agoparentCan you explain your idea? Your algorithm is correct by definition, but doing this naively would be very slow (even for 32bit number). At this point it would be much faster to just binsearch it. reply __s 20 hours agorootparentFor an example of binsearch algo, I recently dipped into this while switching some code from floats to fixed point arithmetic (reducing overall wasm blob size) https://github.com/serprex/openEtG/blob/2011007dec2616d1a24d... Tho I could probably save binary size more by importing Math.sqrt from JS reply smcameron 14 hours agorootparentRecently, I had reason to dig around for a fixed point square root algorithm and found this: https://groups.google.com/g/comp.lang.c/c/IpwKbw0MAxw/m/N1xh... reply tomatocracy 18 hours agoparentprevBetter might be to use the expansion (x+y)^2=x^2+2xy+y^2 along with the observation that in any base, the square root of a 2n-digit number is at most n digits, as in the common method for calculating a square root \"by hand\" with pen and paper. If you did this 8 bits at a time then you only need a lookup table for roots of 8bit numbers. reply atq2119 20 hours agoparentprevAnd you would iterate through that sequence? That's exponential time in the bit length of the input... reply sublinear 20 hours agorootparentIt's sqrt(n) - 1 additions for the n you're trying to get the integer square root of. Memoization would make it constant time for any lesser n than the greatest n you've done this for. For greater n it's sqrt(new_big_n - prev_big_n) - 1 more additions to memoize. You're right this isn't practical, but fun to think about. Good refresher for those out of school for a while. reply klyrs 16 hours agorootparentSubtle detail of complexity analysis: the input is of length log(n), so taking n^k steps for any positive number k is exponential time. reply HPsquared 21 hours agoparentprevAnd this is one of those \"embarrassingly parallel\" tasks. reply sublinear 19 hours agorootparentYes. On a desert island we can have the whole village construct this table for newton-raphson guesses. Combined with a cutting tool attached to a worm drive we will precisely count our turns (big radius crank for extra precision!) and begin manufacture of slide rules. Can never have too many scales and this is just one we shall etch into them! reply msarnoff 14 hours agoprevIf you wanted to expand the definition of “processor” to electromechanical contraptions, the Friden SRQ could perform square roots using just additions and shifts, with not a single electronic component other than a motor. And since you had to position the decimal points manually, it would _technically_ be an integer operation… Video: https://youtu.be/o44a1ao5h8w reply moomin 19 hours agoprevYou need to read down a bit, but the answer “ENIAC” is hilarious. reply drpixie 8 hours agoparentSo many people assume that everything that came before they were at school was primitive, and barely chugged along :) A little reading shows the opposite. Most of our smart ideas were already used in 1940s/50s/60s computers, and are recycled on our fab new chips!! Pipelining, out of order exec, multiple cores, etc. That old-time hardware might have been a bit \"chunky\" but the architectures used some very smart techniques. reply xyst 19 hours agoparentprevhardware engineer humor lol reply jlarcombe 4 hours agoprevSemi-related and of interest to 6502 fans, exhaustive analysis of square root algorithms: https://github.com/TobyLobster/sqrt_test reply Dwedit 13 hours agoprev2 ^ (1/2 * Log2(X)) = sqrt(X) You can get a really really rough approximation if you replace Log2(x) with 'count leading zeroes'. With a better approximation of Log(2), you can get closer to the answer. reply pajko 13 hours agoprevARM VFP has VSQRT https://developer.arm.com/documentation/dui0473/m/vfp-instru... reply bryanlarsen 18 hours agoprevIIRC, most (all?) fixed point DSP's have a square root instruction and/or helper instructions. reply dahart 19 hours agoprev [–] For an approximate (very rough) answer, as opposed to one accurate to the nearest integer, a right shift by half the number of bits of the leading 1’s position will do, and of course nearly every processor has a shift instruction. I’m not sure how often processors haven’t had something like FLO (Find Leading One) or FFS (Find First Set) instruction, those seem ubiquitous as well. The super rough approximation for some uses can be approximately as good as an accurate answer. When you just need a decent starting place for some further Newton-Raphson iteration, for example. (Of course the right-shift trick is a nice way to seed a more accurate square root calculation. :P) reply lordnacho 19 hours agoparentIs this where the DOOM reference comes in? Somewhat famous Internet story by now featuring Carmack and a magic 32 bit number. reply epcoa 18 hours agorootparentYou mean Quake 3 and fast inverse square root? No. And it wasn’t Carmack. https://www.beyond3d.com/content/articles/15/ https://en.wikipedia.org/wiki/Fast_inverse_square_root reply dboreham 18 hours agorootparentI wondered about the name of that function: surely it isn't inverse square root? That would be \"squared\". It's \"1 over square root\" or something. So down the rabbit hole to see if it was always called that. Yup, in Wikipedia articles at least, but the first paper seems to be by Jim Blinn in 1997, without the term in the title. So let's read the paper... Frustratingly, although I am an IEEE member, and I did subscribe to Computer Graphics and Applications in 1997, it won't let me read the paper without paying again. So curious to hear from folks knowledgeable in the space if this was a mis-naming that stuck or I'm confused about the meaning of \"inverse\". In my universe we used inverse in the context of functions and their inverses, and \"invert\" in the context of binary values (1s compliment of x is also called x inverted). Never to describe reciprocal. reply epcoa 17 hours agorootparentIt was called \"inverse square root\" as it is just another practical \"layer\" on top of \"inverse\" which is just meant to mean multiplicative inverse. This is the original Blinn 97 BTW. https://web.archive.org/web/20130309073539/http://rufus.hack... This seems to have been common usage. I never really thought about it as it was just so normal to refer to reciprocal as \"inverse\" in this context. > In my universe we used inverse in the context of functions and their inverses Yes but, the other type of inverse that is so fundamental to CS in general, and especially geometry is a matrix inverse, which is again a multiplicative inverse, so it's not too surprising how this usage became assumed in many contexts. reply dahart 17 hours agorootparentprevI’ve heard inverse used to mean reciprocal often enough. And it’s technically accurate - a reciprocal is a multiplicative inverse. The problem is mainly that “inverse” is ambiguous, especially in this particular case (an inverse square root is a square!), whereas “reciprocal” is clear and unambiguous. Online you can find lots of uses of inverse, and questions about inverse vs reciprocal. So yes reciprocal is the better, more clear term to use here, but “inverse” to mean reciprocal does happen. reply dahart 19 hours agorootparentprevNot really, that’s a very clever trick used on floating point numbers, and does the approximate reciprocal square root. This right-shift thing is far simpler, not very clever, doesn’t involve magic numbers, and is much more well known than the “Quake trick”. There are easy ways to see it. One would be that multiplying a number by itself approximately doubles the number of digits. Therefore halving the number of digits is approximately the square root. You can get more technical and precise by noting that FFS(n) =~ log2(n), and if you remember logs, you know that exp(log(n)/2) = n^(1/2), so shifting right by FFS(n)/2 is just mathematically approximately a square root. reply creato 16 hours agorootparentThey are more closely related than you suggest. Both methods are using an approximation of log2 to get an initial guess. One gets it from \"FPS(n)\", the other gets it from the floating point representation of n, where you can roughly find the log2(n) by looking at the exponent of n in the float representation. You can also use the mantissa to refine the guess further. reply dahart 14 hours agorootparentThey are related a little, around the log2 (exponent), I totally agree. I guess I figured the magic number subtraction that turns it into n^(-1/2) is so wild that it puts the Quake trick in a bit of a different class. This right shift thing is a lot older and there’s probably a lineage of ideas from the right shift on integers to the Quake trick. I discovered another fun one on my own that is probably already well known in bit-math circles, but simply inverting the bits of a floating point exponent is a very rough approximation to the reciprocal, good for a Newton initial guess. reply winwang 16 hours agoparentprev [–] Fun fact, FFS (and its generalization, FNS) is in CUDA: https://docs.nvidia.com/cuda/cuda-math-api/index.html#group_... Another nice CUDA hardware intrinsic I like is log2. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Stack Exchange Network is a platform where developers can ask questions, share knowledge, and engage in discussions about computer programming and retrocomputing.",
      "A recent question on the platform delves into implementing integer square root instructions in processors, examining methods like the Quake method and binary search, along with discussions on efficiency, modern CPUs and GPUs, and trade-offs between precision and speed.",
      "Conversations in the comments revolve around processor instruction sets, specific instructions, and technical inquiries concerning computer systems."
    ],
    "commentSummary": [
      "The article delves into implementing square root estimation in AArch64 NEON processors, highlighting the URSQRTE instruction for approximating the inverse square root of fixed-precision integers.",
      "URSQRTE is advantageous for parallel vectorized computations in tasks such as DSP algorithms and computer graphics, involving square root algorithms, fixed-point arithmetic, and right shifts.",
      "The discussion includes a debate on the terminology \"reciprocal\" versus \"inverse\" in mathematical contexts and mentions historical smart techniques in old computers, with tidbits on CUDA hardware intrinsics."
    ],
    "points": 216,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1712487310
  },
  {
    "id": 39966743,
    "title": "Spotify Faces Backlash for Demonetizing Low-Stream Tracks",
    "originLink": "https://djmag.com/news/spotify-officially-demonetises-all-tracks-under-1000-streams",
    "originBody": "News Martin Guttridge-Hewitt 4 April 2024, 12:37 Spotify officially demonetises all tracks with under 1,000 streams Customers could soon be paying for more services, according to reports Spotify has officially demonetised all tracks with under 1,000 streams. The new policy came into effect for all artists in 1st April 2024. The revised revenue scheme was announced last year, and means music will only be included in the royalty pool calculation if it passes a threshold of 1,000 plays in the preceding 12 months. According to a Spotify blog post, 99.5% of all streams on the platform are of tracks that have above that many plays, with the platform claiming that these tracks will now earn more as a result. Additionally, Spotify now requires a minimum number of unique listeners for royalties to apply. This attempt to stop \"further manipulation by bad actors\" targets people using automatic and artificial plays to ramp up stream counts. Meanwhile, \"functional\" genres, such as white noise, are being targeted, too. Whereas before these types of recordings could generate income from as little as 30 seconds of play, this has now been increased to two minutes. However, the change has been met with anger and frustration from some corners of the music industry. This week, United Musicians and Allied Workers shared a post on X which suggested those numbers could be wildly overstated, arguing that 86% of all content on Spotify will now fail to meet the criteria for royalties based on play count. As of April 1st, Spotify has demonetized any track on the platform that receives less than 1000 streams per year. That’s an estimated 86% of tracks on the platform. This new policy will impact small artists the most — the same artists that Spotify claims to support. — United Musicians and Allied Workers (@UMAW_) April 3, 2024 United Musicians and Allied Workers recently spearheaded the Make Streaming Pay initiative, demanding fairer revenue split for artists using platforms including Spotify and Apple Music. The campaign is behind a new Living Wage For Musicians Bill, which was introduced to US congress in March and aims to \"ensure that artists and musicians can build sustainable careers in the digital age\". You can find out more about this here. While political and public pressure for better royalty payments grows, Spotify is looking to bring in more revenue to plug its own shortfall. Since going public on the stock market in 2018, the company has lost money every year. Just this week, it was reported that the streaming giant has confirmed intentions to revise and increase its pricing. Between $1 and $2 will be added to monthly bills for customers in several territories, including the UK, Australia, and Pakistan, Bloomburg reports. This is said to cover the cost of audiobooks, added to the platform late-2023. More recently, video learning content was introduced to further diversify the offering. A new basic tier package will be rolled out for those who do not want to access audiobooks, the first of several updated pricing options. This news led to a 4.6% jump in Spotify's share value, although it's unclear what the long-term impact will be. Spotify Music Streaming Music Sales Living Wage For Musicians Most Popular News Tomorrowland Thailand: ‘Nothing officially confirmed’, festival clarifies 2 How Allen & Heath’s Xone:92 wrote a new blueprint for DJ mixers 3 Akai unveils stem separation for legendary sampler, MPC Stems 4 Tomorrowland is coming to Thailand in 2026 5 The Top 100 Festivals of 2023, as voted by DJ Mag readers",
    "commentLink": "https://news.ycombinator.com/item?id=39966743",
    "commentBody": "Spotify demonetizes all tracks under 1k streams (djmag.com)214 points by buro9 3 hours agohidepastfavorite214 comments Manheim 3 minutes agoPaying a nominal amount for each stream is one matter; however, compensating nothing for smaller quantities while you pay for larger amounts presents a fundamentally different issue. This scenario is akin to an author not receiving royalties for the first 1,000 books sold in a bookstore, or a manufacturer not being paid for the first 1,000 candy bars sold at a grocery store. Such practices can be equated with theft. It's perplexing how current legislation permits this. reply snailmailman 2 hours agoprevI don’t know much about how music licensing works. But would this cause smaller musicians to decide to pull their stuff off Spotify? I listen to a lot of music on Spotify. And some of it is from smaller indie artists. Not a huge amount, but I’ve definitely listened to songs that are in thatThere is no monetary reason to be on spotify, the only reason we are on there is because fans asked. This isn't an excuse for Spotify not paying much for plays, but I bet having music on spotify helps the gig revenue though. In some ways this is also true for Taylor Swift et al - they also make thousands times the amount from playing gigs than 'selling' albums, streaming, and radio plays but that drives ticket sales. reply jfoster 42 minutes agorootparentIndie: No one pays you. Popular: Gigs pay you. Taylor Swift: Governments pay you millions of dollars for regional exclusivity. (essentially, play 2 - 3 concerts instead of 10+) https://www.theguardian.com/music/2024/mar/05/taylor-swift-s... reply jhrmnn 38 minutes agorootparentprevSpotify should then be run as ad business—charge artists to let them advertise on Spotify. reply whstl 1 hour agorootparentprevSimilar experiences (but from a few years ago). Even people who have Spotify end up buying stuff in Bandcamp, anyway. I never had merch but had CDs, and sometimes people would buy two physical copies, one to gift. For me there was never any difference between Spotify and piracy. reply Hoasi 1 hour agorootparentAbsolutely. I always buy music from artists to support them on Bandcamp. Spotify lets you discover bands that are little known, which is cool, but it's not a place that enables you to support those artists. Unfortunately, Bandcamp might also change since they are changing owners. reply asdof 1 hour agorootparentprevI'd wager the income from playing gigs would be substantially lower if you weren't on Spotify. reply tgv 54 minutes agorootparentYour arguments veers close to the \"for exposure\" payment. People don't go to indie concerts because of a spotify track. Nor to big name concerts. And 20 cents corresponds to around 50 to 100 plays according to the numbers mentioned here. reply jug 30 minutes agorootparentI'm not sure what your basis is for this claim? I've most certainly discovered artists from Spotify and later on bought e.g. merch to support them. I would in a heartbeat also go on their concerts if I was given the opportunity with travel distance, vacation plans etc. Spotify can't give you these experiences no matter how hard they try so this claim doesn't make much sense to me. A concert isn't exactly competing with streaming services or anything. reply helsinkiandrew 42 minutes agorootparentprev> And 20 cents corresponds to around 50 to 100 plays Unfortunately more like 5000+ plays. People tend go to concerts of music they listen to and for a lot of people that means hearing it on Spotify reply epolanski 44 minutes agorootparentprev> People don't go to indie concerts because of a spotify track Do you have numbers behind this claim? I definitely got aware of some unknown artists thanks to Spotify and YouTube, some of them had low hundreds views/listens. Ended up supporting them, going to concerts and buying merch. reply cqqxo4zV46cp 7 minutes agorootparentprevYour absolutist claim is already false by virtue of the fact that I have gone to an “indie” show after hearing a track on Spotify. Next? reply cal85 36 minutes agorootparentprevIt is extremely common and often perfectly rational for creatives to do things in exchange for exposure. There is nothing fallacious about this. > People don't go to indie concerts because of a spotify track. Yes they do. reply petecooper 37 minutes agorootparentprevReading this has made me buy the last two physical items on my Bandcamp wishlist. Bandcamp isn't perfect, but it fits well with my listening habits and I'm glad it earns _some_ money for you in the grand scheme of things. reply mkl 19 minutes agorootparentprevHow many Spotify plays was that? reply runiq 1 hour agorootparentprevI'm intrigued. What's your bandcamp page? :) reply atoav 1 hour agorootparentWarning — we are probably pretty niche with our noisy music, but here ya go: https://maschinkaput.bandcamp.com/ reply meindnoch 0 minutes agorootparentNice! Reminds me of Nazoranai. BLKNSLVR 33 minutes agorootparentprevListening on Bandcamp right now - the song 'drop'. I like some pretty out there stuff, and this is right at the outer limits, and I mean that as a compliment. I might not \"enjoy\" it as one would traditionally enjoy listening to music, but I enjoy that it exists and the uniqueness of the experience it provides. If you can handle LAMC by Tool, you can handle this. 'kabelbrand' is giving me flashbacks to Providence by King Crimson, just a bit more raw and industrial. All build-up without the instruments actually synchronising together for the payoff as per Providence. The band name is great; apt. reply runiq 32 minutes agorootparentprevNee, ich mag's. Danke für den Link, und Grüße nach Hamburg! reply jug 33 minutes agorootparentprevYeah, no cash in it but I figure it's mainly to gain exposure to hopefully later on get those into the \"playing gigs\" / merch audience, because reach is on the other hand massive on Spotify. reply kredd 2 hours agoparentprevI’m not sure how it can affect those artists given it’s like $0.5 for 1k streams. I mean, I guess if they have 100 songs all at like 999 streams, that’s about $50 loss/year if they never pass that threshold. It sucks obviously, but not enough to for artists to pull out. My understanding is they’re playing a cat-and-mouse game with auto generated content, that syphons away some percentage of the royalties. reply basisword 1 hour agorootparentWhile it might not have a big impact on smaller artists financially, the idea of a multi billion dollar company deciding unilaterally to steal a few cents from you could be enough to drive you away. reply lnxg33k1 1 hour agorootparentProbably if being actively exploited by a multi billion company is a problem for them, it might be better for them to change dream.. or species reply boxed 1 hour agorootparentprevSeems silly. The reason they have that problem is that their logic for funneling subscription revenue to streams is broken. This won't fix that fundamental problem. You can still use Spotify to scam money out of the system. reply Lewton 1 hour agorootparent> The reason they have that problem is that their logic for funneling subscription revenue to streams is broken Changing it to splitting out a users monthly payment to what they're actually listening to, would make the abuse they're trying to stop easier and not harder (as it would mean all the money they're trying to launder will go to the correct artist instead of just some of it) Not saying that that's a good argument for the current system, but it wouldn't fix the issue they're trying to solve reply doikor 8 minutes agorootparentYea there are better ways to fix the problem but the problem is selling that fix to the big labels. The current system gets them a larger share of the revenue so why would they be interested in changing it? reply kredd 1 hour agorootparentprevYou can, but the game isn’t to drive the abuse to 0. It’s about to bring it down, and hope abusers find an easier target to extract money from. It’s the same concept as “why would you steal a secured bike, when there’s an unsecured one right next to it”, and hope you’re the one who secured yours. reply isodev 1 hour agorootparentprevAs a part-time indie artist, one can do a lot with $50 you get on the side from streams. Also, what is Spotify going to do with these 50$? What if it happens twice, 100$... do they pay you back eventually? Perpetually free access to all their premium services? reply kredd 1 hour agorootparentI'll take this bad faith argument, and give you even a worse one - $50 is about 5 drinks or less at a bar with tips nowadays. I genuinely doubt a serious person who makes music on the side will look at that number as a potential loss of income. And if you have made 200 songs (so loss of $100), all streaming less than 1K each, maybe your current production is not marketable on Spotify, and yeah, maybe you should find another venue. Eventually there will be a marketplace for this type of production, and hopefully willing users to pay for it. reply spookie 25 minutes agorootparentYou're assuming every artist lives in a first world country. The problem seems a lot more severe under a more general lens. reply throwaway8414fr 1 hour agorootparentprevThat wss an extreme example I think. Most probably end up with less than $5 loss reply isodev 1 hour agorootparentit's not up to Spotify to decide what I do with that. What if I use it to pay a sub for some app I like and helps me create music? Then Spotify is taking value away from 2 Indies, not just one. reply SSLy 1 hour agorootparentprevI though labels job was to prevent content like that reply whstl 1 hour agorootparentSpotify allows people to self publish. Not to mention people can set up labels themselves, it's just a company like any other. I'm sure we don't want big labels being gatekeepers. They have their own set of problems, and make streaming even more difficult for everyone but very large artists/groups (and sometimes even for those, as they eat a big chunk of royalties). reply danpalmer 7 minutes agoparentprevThis is the reason I have stuck with Apple Music so long (and previously Google Play Music). I have a fair number of sentimental tracks that I've carried with me in my collection for decades which aren't on streaming sites. Spotify doesn't have an answer for this. Just don't listen to things outside the Spotify library. Apple Music (and GPM before it) lets me upload my own tracks and sync them between my devices. reply jzzskijj 2 hours agoparentprev> It surprises me how much of my Spotify library is no longer available. There’s at least a few dozen songs in my Spotify library that have been taken off the platform. It shows up in the list greyed out. A lot of good songs too. This probably has more to do with publishers and licensing contracts than artists pulling their music off from platforms. Sometimes even bigger artists' albums disappear when publisher is sold or goes out from the business. Or the licensing contract's period runs out. As sad it is, many artists don't own the rights to their music, and if the rights owner is defunct, then there are missing albums or even discographies. reply jillesvangurp 1 hour agorootparentA lot of that is also bad meta data. I have a few playlists for around 10 years or so. Every so often I have to go in and hunt down greyed out tracks that are no longer there but are available in identical versions elsewhere. Publishers apparently regularly update what they have on the platform and there's a lot of duplication as well between best off albums, remastered albums, etc. And some artists, like Neil Young, actually pulled most of their content. Spotify seems very happy to just break everybody's playlists continuously. reply geraldhh 54 minutes agorootparent> Spotify seems very happy to just break everybody's playlists continuously. definitely possible to avoid this, but given it would be a follow-up cost from the failing core business, it probably could not be done. hell, they didn't even get around to making basic meta-data reliably present. reply nmeagent 13 minutes agorootparentprev> Spotify seems very happy to just break everybody's playlists continuously. I remember this being particularly infuriating with movie soundtracks or other compilations, where individual tracks would often evaporate one day for bogus (licensing) reasons. > And some artists, like Neil Young, actually pulled most of their content. Yeah, I actually left Spotify with Neil. Apparently though he and other artists like Joni Mitchell that left at the time are now back on the service as of a few weeks ago. reply pydry 27 minutes agorootparentprev>Spotify seems very happy to just break everybody's playlists continuously. This is an artefact of the industry and not something they have much control over. Labels for larger artists just don't just sell global streaming rights in perpetuity. They will carve it up by region and time in order to try and maximize profit. reply snailmailman 2 hours agorootparentprevIt’s infuriating. Spotify for instance had some Spotify exclusive “DJ mixes” that were all an hour of well-mixed playlists. Spotify got artists to curate them and would have “XYZ’s DJ Mix”. They were perfect for seamless music listening at the gym. No breaks between songs. Non stop music. I listened to them all the time. But one day a few months ago they all just vanished. Every one. I had added several to my Spotify library. For a bit they were playlists of grayed out songs. And then the playlists themselves vanished too. This content was all Spotify exclusive- the music is just gone now. It’s not available elsewhere. It was all “mixed” versions of songs with smooth transitions in an out. Luckily I had added most of the original songs to other Spotify playlists that I had. But still, the mixes themselves are gone. reply input_sh 2 hours agorootparentYou should know: Apple Music filled that niche to perfection. Hands down, better than it ever was on Spotify. More choices, easier discovery, more collaboration with external brands, etc. https://music.apple.com/us/curator/apple-music-dj-mixes/1441... reply snailmailman 1 hour agorootparentInteresting! I’ll definitely take a look. I’ve been meaning to try Apple Music. reply jonathantf2 46 minutes agorootparentIt's the reason I use both. Spotify's recommendations and social features are much better than Apple's - but the exclusive DJ sets and recordings of concerts etc are a great value add. reply WickyNilliams 46 minutes agorootparentprevNot sure what genre you listen to, but soundcloud and mixcloud are the place to go for high quality mixes imo. That's where the DJs post. It's not immune to takedowns and tracks disappearing. But I have a playlists of hundreds of mixes curated over years and they're all still there. Many even with an option to download if you so wish reply znpy 1 hour agorootparentprevFrankly, this is why I always ignored Spotify and other streaming services and keep a jolly roger above my head. If you had pirated or ripped those mixes off Spotify, you'd still have them. And you probably paid enough months of subscription to feel okay with that. Because in the end https://xkcd.com/488/ reply dieselgate 1 hour agorootparentThanks for the comic link and enjoy the riff off the book title “steal this book”. Just looked into deleting my Spotify account through the app and appears that’s not an (obvious?) option. Ugh reply lotsofpulp 1 hour agorootparentThe comic does not make sense for iTunes, since if you buy music from Apple, you get the non DRM file that is yours to use however you want. reply xerox13ster 45 minutes agorootparentIt’s xkcd number 488, it was probably made when iTunes was still selling or just after it stopped selling DRM music. reply lotsofpulp 38 minutes agorootparentGood point, the comic was published Oct 2008 according to xkcd and Apple stopped selling DRM music in Jan 2009. reply richrichardsson 21 minutes agoparentprevI have roughly 25-30 tracks on Spotify. They send me a monthly email detailing the number of plays I received as an artist, and it averages around 300. So each track is getting around 10 plays per month. So none of them come anywhere close to the minimum threshold. I made around $10/year from Spotify plays, it's a pittance, but over all the streaming platforms I would get enough for a new plugin each year, this makes it \"worthwhile\". I absolutely will not allow Spotify to have content on their platform without reimbursement. The only people losing from this are the handful of fans who like my music. reply realprimoh 2 hours agoparentprevI use YouTube Premium 90% of the time now for this reason. All the remixes and other indie mixes that I listen to are on YouTube and YouTube has so much more that I find it a hard value proposition to beat. reply klausjensen 1 hour agorootparentSlightly off topic, but where do you go to find this now? reply baby 1 hour agoparentprevIt’s not just about the money. It’s about the exposure! If you’re not a big band you’re making money from touring. More people listening and discovering you online = more people attending your tours and maybe reaching a threshold where you can make good money reply easyThrowaway 1 hour agoparentprev>I don’t know much about how music licensing works. But would this cause smaller musicians to decide to pull their stuff off Spotify? A lot of smaller artists don't have a say regarding licensing and distribution. They're usually managed by a local minor label, which in turn has specific agreements with bigger players (Columbia, UMG) once your music sells more than, let's say 50.000 copies and they need a partner to publish a release on more territories. A few artists also get their own label. For example, Peggy Gou tracks are usually released trough Dudu Records (Her own label) -> Ninja Tune -> XL Recordings, which is basically the same label that nowadays publishes Adele and Radiohead. Adele went even a step further, and her music is handled by Columbia Records for the US market. Until you reach this level of clout, you have very little direct control over your syncs and streaming rights in the \"regular\", Billboard-tracked, market. reply bashwizard 1 hour agoparentprevI have a few thousand of streams per month and I make pennies from it. It's not like I care about being demonetized on spotify and losing a few pennies. reply kunley 1 hour agoparentprevThe mechanism there is kind of \"dontcareism\": yes, indie musicians will go away, but Spotify doesn't care, because ultimately this won't stop listeners from using it, over time most of them will forget what they really wanted reply pydry 39 minutes agorootparentThey do care, they're just exploiting the indie musicians' lack of leverage. Together they matter a lot, they're just powerless individually. It's the same dynamic companies use against ununionized labor. reply countmora 1 hour agoparentprevI think royalties fromThere’s at least a few dozen songs in my Spotify library that have been taken off the platform. It shows up in the list greyed out. As infuriating as this is, it's the reason I still use Spotify. A music service that hides their catalog changes by subtly modifying my playlists is a no-go. Having the songs still appear lets me know, as it gives me the opportunity to find those songs elsewhere. reply junon 1 hour agorootparentYeah I have to admit, Spotify uses the least amount of cruddy dark patterns. Their model is very upfront about paying, so they have never nagged me for anything. Their interface isn't the best but it's consistent. These days there's a lot of worth in that, the bars been set quite low. reply TonyTrapp 2 hours agoparentprev> But would this cause smaller musicians to decide to pull their stuff off Spotify? It's already happening. Even some artists which are above the limit pull out to show their support for smaller artists. reply Sparkyte 1 hour agoparentprevOr it would force small musicians to advertiwse more. Which is mutually beneficial to Spotify and the user. 1k is a relatively low number to hit at least they didn't set the demonizations level the same as other platforms. reply cranberryturkey 2 hours agoparentprevBack to torrenting I guess. If I’m not mistaken snoop said he made 45k off 1 billion plays. Thats ridiculous reply reliablereason 2 hours agorootparentThat was apparently a track that was not his own, he was only a partial co author of that track. reply ansc 2 hours agorootparentprevTo be fair, that's because the label siphons the rest. Spotify pays $4 per 1000 streams. reply djxfade 1 hour agorootparentWhere do you get those numbers from? I'm a self published artist. I had 150k streams on Spotify last year. I only earned roughly $450. There's no label or other middleman here taking a cut. Spotify is just cheap. reply ssl-3 1 hour agorootparentThey say: $4 / 1,000 = $0.004 per streamed track. You say: $450 / 150,000 = $0.003 per streamed track. The two sets of numbers don't seem so far apart that the discrepancy cannot be explained by rounding errors. reply Dylan16807 1 hour agorootparentprevSnoop's number was $0.045 per thousand plays. ansc's number was $4 per thousand plays. Your number is $3 per thousand plays. It sounds like you basically agree with ansc. While Spotify might be cheap, that cheapness is not the problem Snoop is having. reply Martinussen 1 hour agorootparentprevIt's somewhere around $3.5/1k streams - you're not exactly super far off their estimate either, though. It varies a bit based on (listener, or artist?) regions too, I believe. reply auggierose 1 hour agorootparentprevThat's not too far off, is it? 150 * $4 = $600 ≈ $450. reply Mashimo 1 hour agorootparentprev> Back to torrenting I guess. Or just buy it on bandcamp friday and give it all to the artists :) reply jstummbillig 2 hours agorootparentprevAh. Music income numbers, where the details are not actually details. He did say that[1], but the statement is misleading (and has little to do with Spotify). A very rough global average per-stream payout is often estimated at around $0.003 to $0.005. That brings the payout for 1 billion streams between $3,000,000 and $5,000,000. Let's go with the lower end. This is actual money paid out by Spotify for 1 billion streams, as in, a bank transfer of $3,000,000. How do you get from there to $45,000? - As with any legacy artist, the record label gets most (maybe 70%). Why that is, oh well. It's less shocking if you think of the label as an employer and how little an average employee gets compared to what they produce. Do you need an employer? Absolutely not. Do you need a record label? Absolutely not. Anyway, a 70% cut would leave the artist with $900,000. - Expenses: There are various costs associated with producing, distributing, and promoting music, hotel costs. Let your imagination run wild. Safe to say: This is a black hole, depending on how you want the process to look. If Snoop wants to sit in a big studio for months, he pays prices, for month, but that's not required to make music. Renting a high end studio on end is lifestyle. Sure, it's an expense, but it's like complaining about your profit while driving a Porsche. Awkward. Let's assume a 20% deduction here, reducing the revenue to $720,000. - Focus on Publishing Royalties: Publishing royalties are just one part of the total royalties an artist earns. So there's quite a bit of double accounting going on if you want to pretend that the above costs are all deducted from your Spotify income. If we assume publishing royalties make up only 10% of the total, that brings the figure down to $72,000. - tldr: Snoop is living the big life, confused about money, and then proceeds to confuse others about money. [1] https://economictimes.indiatimes.com/news/international/us/s... reply addicted 16 minutes agorootparentA record label is probably much more of a VC than it is an employer. The business has changed since streaming and self publishing but the labels took bets on artists and gave them expensive recording studios and marketing dollars, gave them some basic money, and in return took the majority of rights to the first few albums. They also knew that if the artist truly made it big they would likely go to a different label where they could get better terms or start their own where they’d keep all the money after the initial contract which drove their side higher. The record label model clearly doesn’t work as well in modern days but it was successful for good reasons at the time it really flourished. reply tibu 50 minutes agorootparentprevIf the ownership is with the publishing studio then AFAIK they pay the producing, distribution and promotion. It is an investment into the artist, so they shouldn't have these costs. reply amelius 1 hour agorootparentprevI hear taylor swift is a billionaire. Is she on Spotify? reply Daz1 1 hour agorootparentprevAh yes performative postering by people who 'really care' about artists getting paid. reply paranoidrobot 2 hours agoprevI think this is the wrong approach. If they have issues with people abusing the revenue model by publishing white noise and generating fake plays - go after that. However the 1K streams per track thing is going to negatively impact small artists who might have relatively large collections, but few over 1K annual streams. If it's a processing cost issue then make it so that payouts need to meet some reasonable minimum threshold. > between $1 and $2 will be added to monthly bills for customers in several territories, including the UK, Australia, and Pakistan, Bloomburg reports. This is said to cover the cost of audiobooks, added to the platform late-2023. More recently, video learning content was introduced to further diversify the offering. I don't want, and never wanted, Audiobooks or Podcasts or News or other crap on the music app. They mention that there will be another tier added which doesn't have these -- great, so long as it keeps the audio quality for music at the same level, and doesn't have ads. reply VS1999 2 hours agoparentThis is an issue I've seen with every platform that primarily exists online. They buy every digital service and increase prices with or without the customer's consent. Aside from being annoying and largely pointless, it's anti-competitive because if you subscribe to Spotify you are discouraged from subscribing to some other, better podcast service you actually want because you're already paying for one. It gets even worse as more services get bundled together. reply _3u10 2 hours agorootparentYou’re free to cancel, and I’m sure customer service would give you the refund on the extra cost for the first month. I’ve noticed this in restaurants too, they just change the price like a normal business in an inflationary environment. I bet you get raises too, or ask for them and move on if you don’t get them. reply VS1999 2 hours agorootparentI have no idea how that relates to what I wrote. Edit: Actually I see now, you're replying to where I said they bought services and raised prices without the customer consent. If we're using a restaurant comparison, it'd be like your local burger shop raising prices because your meal comes bundled with discounted carwashes. Consumers generally don't want bundles, they want to buy the thing they're interested in. reply lotsofpulp 1 hour agorootparent> Consumers generally don't want bundles, they want to buy the thing they're interested in. They have been able to buy the individual song for $1 to $2 for a long time. reply iamacyborg 1 hour agoparentprevJust cancel your Spotify subscription. I did, and now use a combination of Roon and Tidal. I have a much better listening and discovery experience through Roon and artists get paid more with Tidal. reply shmageggy 28 minutes agorootparentIf only Tidal (or Qobuz, or...) would implement a Connect feature that allows me to control my PC desktop from mobile, then I would, but it's been on the \"promised features\" list for years now. It's a deal-breaker for me. reply TomK32 0 minutes agorootparentDid you try KDEConnect? wokkel 22 minutes agorootparentprevI use deezer and when I play a track on mobile I can continue it on PC. Is that what you mean? reply iamacyborg 21 minutes agorootparentprevYeah Spotify Connect really is solid but that’s why I also went with Roon as it allows me to do that and more. reply shmageggy 7 minutes agorootparentYeah it looks like Roon can do this. Might be worth checking out, thanks! reply wkat4242 1 hour agorootparentprevThe problem is that Spotify is the only one that really works well on Foss with libspotify / spotify-qt. All the others don't have clients for my OS and I don't have DRM in my browser which they require. reply iamacyborg 54 minutes agorootparentAh annoying, I'd assumed Roon had a Linux client but it appears not to, despite having a compatible server and a bridge client. reply baxtr 27 minutes agorootparentprevSame here. I am on Apple Music. I like it. The automated playlists are way better on Spotify though. reply vachina 1 hour agorootparentprevlol. Tidal, the guys that tried to sell us snake oil like MQA. That said you sound like a Meridian shill. reply iamacyborg 59 minutes agorootparentErr, thanks. Heaven forbid someone shares software that they actually like. reply vmfunction 1 hour agoparentprevAll that is listed are what reasonable customer and people would have assumed. However, things in corporate world seems to have the mind of it's own. It is normally in the interest of centralisation in the name of efficiency, and sadly at cost of human productivity and buyer and seller interests. reply HenryBemis 2 hours agoparentprev> I don't want, and never wanted, Audiobooks or Podcasts or News or other crap on the music app. Regarding Spotify, which I use daily, I also dislike their updates. I remember some months back I could disable/hide \"Recommendations\". Now in all my (personal & private) playlists I get a list of Recommendations. It's an annoyance because when I scroll fast to reach the bottom \"to listen to that one song\" it end up surfing the Recommendations (did I mention I dislike them?). So, it makes me want to go to apkpure, find an apk from 2020 and use THAT (slim chance it'll work) because most updates are \"to increase engagement\" (pester you/me/all) with 'new and exciting content!!!' (bleh) Sidenote: for podcasts I am using a 2021 version of the Podcast Addict (the one displaying the full screen ad)(which you can block with NoRoot Firewall) reply ben_w 6 minutes agorootparent\"Recommendations\" never seem even close to what I want, on any platform. The worst is Facebook (I have friends and family who don't use any other mode for regular contact), where it often behaves as if the ideal way to engage me is, in order of appearance on the timeline: • \"People you may know\" (usually devoid of people I've ever even met, and when I do know them they're not people I'm going to add) • One real post • A suggestion of a corporation or group to follow • An advert for something I'm not even capable of getting (\"Are you an American living in the UK who wants to renounce their citizenship for tax purposes?\" — no on all counts) • Three more suggestions of a corporation or group to follow • More ads for things even less relevant than the email spam I got in 1998 -- Even YouTube, where I can recognise the value by comparing what I see when I'm logged in to not logged in, the recommendations are still only 50% useful and 50% dross. (Not logged in it's 98% dross, and yes I did just count 50 items before finding one video I might watch in order to not be a statistic made up on the spot). reply snailmailman 2 hours agorootparentprevMy Spotify app almost daily shows me a full screen advertisement for some new album when I open the app. It is clearly labeled sponsored content. I can then click a tiny 3 dots menu, click “stop showing me this” and “I don’t want to see sponsored content” and it will say “sure thing! We will no longer show you these” and the next day it will show me a new advertisement and I repeat the process. So annoying. The whole premise of premium is that there are no ads! But now I’m constantly getting visual ads in the app. Every artists page has ads for their merch. Spotify notifies me about exclusive merch for artists I listen to. And is constantly trying to get me to go to concerts of artists I listen to. reply HenryBemis 2 hours agorootparent\"NoRoot Firewall\" and I have allowed IPs and URLs that I've trialed-and-errored to Allow. Then . to block everything else. Fun fact: most of the garbage/ads/albums/pushes/etc, as well as your Annual Wrap-Up (or whatever it is called) it is provided by a tracker (I can't remember/see because I haven't individually blocked it - it's blocked under the . rule)(first 10-15 IPs & URLs are allowed, all else is blocked). I believe that using an Android phone without a Firewall must be a horrible experience/it increases the positive experience by A LOT. It also helps to see which apps are sneaky and run on their own - even if you disable the 'background run' (e.g. TuneIn Radio starts itself bypassing battery/power settings) reply avtolik 1 hour agorootparentprevI really like Podcast Addict. Its price is like 10$ per year to hide a small ad on the bottom and to get a choice which screen is the first to show. reply noirscape 1 hour agoprevIMO the problem is that Spotify is fundamentally breaking the \"spirit\" of the deal that allowed them to get in the position they are right now. Spotify's entire promise was \"we solve the music inequality problem by just pooling all subscriber money together and then we do an equal(-ish, record labels iirc got slightly different deals) split depending on how many people listen to your music.\" It kinda sells the idea that if you're just popular enough, you can make it big on Spotify. Of course practically that's been a lie for ages (numbers showcased that only the top 0.1% could afford to live off of Spotify alone, and all those songs are owned by the established record companies anyway), so you could say this is just dispensing with the charade to avoid transaction costs. I do wonder about the ripple effect this could cause for indie artists; Spotify just told them to go fuck themselves and there's pretty much no incentive to use Spotify anymore now that they pulled this stunt. If you want to support artists directly, it's still always better to just buy the albums. Most of them have Bandcamp pages and for now, Bandcamp provides a good deal (and as a customer you can just download the FLAC files). reply quitit 51 minutes agoparentEssentially Spotify now pays small artists in “exposure”. Looks a lot like abuse of market position. reply estomagordo 2 hours agoprevThis change quite obviously can't negatively impact small artists in any meaningful way. When googling for how much Spotify pay artists per stream, ranges vary a bit. But it seems like a high number is $0.005 per stream, or 200 streams to the dollar. Even if a very prolific artist has 200 songs, averaging 500 plays per year, this would only ever amount to $500 per year. It is not peanuts for a hobby musician, but it would represent an unusually productive artist who happens to be just under this limit with most (or all) their songs, and also who manages to somehow get paid in the upper parts of the span. reply Shrezzing 7 minutes agoparentThe change in isolation from Spotify may not impact individual bands. However if this is adopted across all the seven major streaming services, then artists can in theory have 7,006 streams per track without generating a penny of income, even though the streaming vendors have generated revenues. If YouTube, Spotify, Apple, Tidal, Deezer, Soundcloud, and Amazon all adopt this practice, taking your edge-case artist, they're out of pocket for $3,500 per year. That makes a situation where the more highly concentrated Spotify's monopoly is, and the less productive an artist is, the fairer the outcome for that small artist. That's a quite uniquely perverse economic outcome. reply epolanski 41 minutes agoprevI'm not understanding the harsh criticism here. Pretty much all platforms have some kind of soft threshold before paying. This is actually very common in real world as well, e.g. in sales you don't get commissions if you don't meet certain thresholds too. reply konschubert 0 minutes agoparentI think it makes sense to have a lower threshold on \"plays per artist\", since it involves billing overhead etc. But plays per song? Why? Are they trying to fight spam? And even so, it's so little money and little overhead, just do it for the goodwill. reply Cthulhu_ 3 minutes agoparentprevNo same, I don't get it; 1000 streams a year is not an unrealistic goal, and Spotify is flooded with low-effort garbage - and this will get worse with AI generated stuff. reply nonrandomstring 11 minutes agoprevAccording to this artist [0] Spotify will be irrelevant soon because infinitely streaming generative AI music will cater to all your \"ever changing\" desires. [0] https://news.ycombinator.com/item?id=39967745 reply Cthulhu_ 4 minutes agoparentCool prediction, but false; AI generated music will be added to the pile of genres. Honestly there's an existing factory of low effort music being churned out already, AI will become part of that existing pipeline. There will be more music produced, but it won't become mainstream. Music isn't only about music, it's about the creators and their stories as well. Anyone claiming AI will replace traditional music creation is but a functional consumer. It's the same type that exclusively drinks soylent and wears black turtlenecks to avoid having to think about things or taste anything. reply logro 55 minutes agoprevI think \"theft\" closer describes this rather than \"demonetization\" reply Cthulhu_ 2 minutes agoparentNah; the artists entered an agreement with Spotify, it's legally on board. If the artist believes Spotify breached contract they can either end it one-sidedly, or try and sue but I'm confident Spotify's side is rock solid. reply megalomanu 1 hour agoprevI highly recommend trying Qobuz! Artists are paid more, the catalog is rich and comprehensive (I recommend to early users who might have felt a lack of content to give it another try, as it's now much richer), most albums are available in Hi-Res Audio, the UI is nice, there is no fluff (just music, nothing more), and most importantly, content is curated, with many articles written by critics and journalists, artist interviews, and, for each album, a small review or a piece of text informing about the album's significance. Compared to the awesome but \"industrial\" recommendation system of Spotify, this is something more personal and curated which, in my opinion, better helps to understand some music genres. reply truegoric 1 hour agoprevStreaming services will be regulated in the coming months in EU https://www.theverge.com/2024/1/17/24041343/eu-music-streami... EDIT: as it’s been pointed out, the article doesn’t mention a time, which is a speculation on my part reply code_runner 1 hour agoparentThat’s not quite what the article linked says. If anything is done it could be years. It’s also unclear if they have any price per stream in mind and how they even think it should be calculated on an ongoing basis. Sounds pretty early stage reply dcb_lu 1 hour agoprevI hope this doesn't affect Matt Farley[1] too badly, as it seems like he relies on having many songs with only a few streams each as his main strategy for generating income from Spotify. I once commissioned a Mother's Day song from him -- sent him 10 facts about my Mom and our family and he came back a few days later with a funny and charming little tune. Best $30 I've ever spent. [1] https://www.nytimes.com/2024/03/31/magazine/spotify-matt-far... reply egeozcan 16 minutes agoprevCan someone ELI5: Why is it so hard to just split my monthly subscription money on all the artists that I listened to, proportional to their air time to the total listening time? I really don't get it reply fundatus 11 minutes agoparentBecause that would be extremely complex and expensive to do for a negligible effect. reply VS1999 2 hours agoprevHow much of an issue is this? Does it work like ad revenue on youtube? If something has under 1k streams I have to assume that was only a few dollars hobbyists get occasionally, if even that. reply ojosilva 2 hours agoparentAs a side-gig musician and composer - my wife and I as a matter of fact - this is a non-issue. It's a few bucks a year, maybe $5 to $15, depending if we release some new album. It's symbolic, not even a tip, and we can't cash it until it reaches I think $60 or more iirc. Spotify is not (or barely) making money out of our work, but we otoh are \"making money\" by using its platform and visibility as means to reaching to our small fan base that will finance our work through concerts. For Spotify otoh, these few bucks add up to a long-tail loss of an increasing and disturbing magnitude. Additionally, bad actors are in fact exploiting it, ruining the experience for me as a musician and as a user. The issue, if any, is that in all fairness we should all be making money, proportionally, from the platform. But under 1000 streams, as an artist, you are probably getting more value from having your work uploaded and streamable. reply basisword 1 hour agorootparent>> we can't cash it until it reaches I think $60 or more iirc This is highly dependent on your distributor. It can be as little as $10. >> Spotify is not (or barely) making money out of our work, but we otoh are \"making money\" by using its platform and visibility as means to reaching to our small fan base that will finance our work through concerts. This is an odd way to look at it. Without music, Spotify doesn’t exist. People subscribe to Spotify and listen to their ads in exchange for unlimited music access. So Spotify is making money from your music. reply kmlx 30 minutes agorootparent> So Spotify is making money from your music. not really: https://www.statista.com/chart/26773/profitability-developme... > Since the beginning of 2017, the streaming service has generated a positive net balance in eight quarters, two of them in 2021. reply basisword 6 minutes agorootparentI would guess the huge sums they've blown on exclusive podcast deals and the repeated awful unnecessary redesigns go a long way towards that. reply ojosilva 1 hour agorootparentprev> This is an odd way to look at it. Without music, Spotify doesn’t exist. People subscribe to Spotify and listen to their ads in exchange for unlimited music access. So Spotify is making money from your music. Spotify has expenses too and is hosting and streaming our music, distributing and maintaining an app and api for access, all basically for free for the musician. So \"making money from your music\" from my 800 streams a year is a very optimistic view of how the whole thing works out, I'd say we are cooperating fairly. reply quitit 41 minutes agoparentprevI think that’s a too-narrow way of looking at it. For an individual artist it’s not much. For Spotify it’s huge, all of those nickels and dimes add up quickly. It also doesn’t matter if their current plan is to redistribute 100% of the gains to other artists, since this becomes profit they didn’t need to sacrifice to appease big acts. I.e. small artists pay so Spotify doesn’t have to. Most importantly the artists didn’t sign up for this arrangement when they first joined Spotify, but now due to Spotify’s size they may feel compelled to give away their work for free for the chance at being successful or keeping fans happy. reply baxtr 24 minutes agoprevI find it funny/interesting/annoying that Spotify plays the hero when it comes to condemning Apple for its App Store policies yet has no issues to be the villain when they have a chance to show to handle such issues. reply easyThrowaway 2 hours agoprevThis should be renamed to \"Spotify admits demonetizing all tracks under 1k streams with no previous contractual agreements\". Next step would be explaining why artists with the same amount of streams (but different labels) have wildly different payouts. reply schainks 15 minutes agoprevSeems like a second order effect: Google cloud is squeezing them, and now they are squeezing artists. reply fimdomeio 36 minutes agoprev\"You wouldn't stream a car for free would you?\" This is just playing with meanings but for most musicians having their music on Spotify or on torrents is pretty much the same. the only difference is that only in one case there's a company profiting in the process. reply anentropic 38 minutes agoprev> Since going public on the stock market in 2018, the company has lost money every year they don't even have a business model the whole thing is a massive scam that devalues music just to earn some VCs a big payout it's like if I said I was going to \"disrupt the supermarket industry\" by giving away free food reply qnleigh 2 hours agoprevWhen I read the headline, I assumed this was to disincentivise uploading thousands of cheap AI-generated songs. There's tons of this on the platform, and it looks like it was generated automatically in bulk. But the article makes it sound like this was not a factor? reply supernes 2 hours agoparentI'm inclined to believe that filling the service with an endless stream of generated garbage is their ultimate endgame - why bother with unreliable and costly human creators when you can just emulate them well enough for undiscerning listeners. No substance, just vibes. reply whstl 1 hour agorootparentEven Spotify is smart enough to know that people will leave them if this ever happens. reply reducesuffering 54 minutes agorootparentNo, that's the end game here. People already mostly have parasocial relationships with artists. There's already famous AI personality Instagrams and Vtubers. With the advent of Generative AI video like Sora and Generative AI music like Suno, people will find their perfect generative AI niche and follow an artist story/interaction that they can't even tell is real or not. Generative AI will create millions of niches, it will hit that spot. reply OscarTheGrinch 1 hour agoprevWhat is the going rate for a service that \"organically\" plays a track / album 1000 times? To misquote Bezos: Your silly policy is my buisness oppertunity. reply geraldhh 33 minutes agoparentthe going rate is probably what spotify pays for that 1000 plays, minus tax reply nottorp 1 hour agoprevCan you see how many streams an artist has? I'm generally happy to let Spotify stream whatever for background noise, but it looks like I should play some local bands manually once in a while. reply scary-size 1 hour agoprevKristin Graziani wrote a comprehensive explainer on this in November: https://consequence.net/2023/11/spotify-royalty-model-op-ed/ reply max_ 55 minutes agoprevThe market is hungry for a well-done music streaming service. I don't know why there is no venture backed competitor. reply geraldhh 37 minutes agoparenti guess the economics are not favorable because the music industry is already well positioned to extract wealth from populism and will give a hell of a legal/drm battle if threatened. the long-tail of indy artists doing essentially nothing for the bottom line doesn't help either. reply lotsofpulp 32 minutes agoparentprevBecause after 18 years, Spotify’s net income looks like this: https://www.macrotrends.net/stocks/charts/SPOT/spotify-techn... And their market cap looks like this: https://www.macrotrends.net/stocks/charts/SPOT/spotify-techn... And your competitors are Apple/Amazon/Alphabet. And your vendors with whom you have zero negotiating power are Universal/Warner/Sony, and their whole interest in supporting a music streaming business like Spotify is to maintain leverage against Apple/Amazon/Alphabet, so music sellers only need to keep the streaming business limping along. There is no play here without reform to copyright laws that limit copyright to 10 years. reply fifticon 34 minutes agoprevisn't this partly to combat people who commit search listing flooding by abusing the upload ability to upload 100.000 variations of the same thing? reply cynicalsecurity 55 minutes agoprevWhy use Spotify at all when it's so much more helpful to subscribe to patreon of new and promising artist? reply ctenb 48 minutes agoprevAre there any other comparable (but more ethical) platforms out there that I can switch to? reply milesjag 42 minutes agoparentbandcamp is a bit more of a level playing field - the artists can decide whether they want to use the platform to stream or sell their music, while being given some control over the number of streams per listener before a song will be unavailable to stream (although this does lean on cookies, I think, so not difficult to get around...) reply geraldhh 43 minutes agoparentprevjust pirate and donate directly if you really mean it. reply geraldhh 47 minutes agoprevacceptable as a reaction to the cost of tracking engagement and paying the long-tail of artists. then again this could probably be fixed with a minimum payout policy and a donation button in the app. reply ulrikrasmussen 1 hour agoprevEven if this doesn't really impact the small artists, Spotify is not even profitable for shareholders yet AND still doesn't pay a meaningful amount to most artists. It is only going to get worse from here. I do not believe that their business model will ever do anything but fuck over the artists. It's also deeply disturbing to me that there are only a few options left for people to actually own their music and the rest is rent-seeking platforms. reply Helmut10001 1 hour agoparentSelf-host Funkwhale [1] * (or something else). Curate your own music library. This is the only way forward. * I self-host this internally, for my family and me, since 2019. One of the best designed interfaces / UX you can find. [1]: https://www.funkwhale.audio/ reply mfashby 1 hour agoprevDoes anyone know of a service where I can feed in my Spotify playlists, and it tells me where I can buy those tracks? reply geraldhh 31 minutes agoparentidk but this seems usefull https://github.com/spotDL/spotify-downloader reply rain_iwakura 2 hours agoprevI think they should basically turn it into Twitch, where I can \"sub\" to an artist and maybe get their tracks earlier than others or some other additional content that costs artist $0 to produce for the most part (podcast, making of, e.g. a la Patreon). In addition, if you add \"donate\" button you'd see a lot of artists being showered with cash. It's a shame that these hacks (streamers) become multimillionaires while real talents make pennies off actual art. reply rnestler 1 hour agoparent> In addition, if you add \"donate\" button you'd see a lot of artists being showered with cash. There are donation buttons for some artist on Spotify. I guess the artist need to enable it? (\"Signum Regis\" is an artist that has a Donate button for example) reply RCitronsBroker 20 minutes agoprevthat’s BS and in no way solving their very real bot problem. This wasn’t reported on worldwide, but here in Germany, it got some press due to local hip-hop labels being involved. Malicious labels use click farms to push up the numbers on their artists, either by manipulating all platforms or locking their artist into Spotify. This, in addition to other forms of manipulation such as playlists-aaS, isn’t just a ploy to promote their music. As long as farming clicks is cheaper that the payout rate, they can get paid for laundering illicit gains that way. reply cjk2 2 hours agoprevI suspect the truth around this is that it contributes to their eternal losses actually paying artists so they worked out another way of not doing it for their balance sheet. I was at a gig for a moderately successful band recently and they said rather loudly they were bastards and buy one of their CDs instead even if you never play it because they need to eat. reply Hendrikto 1 hour agoparent> they said rather loudly […] buy one of their CDs instead even if you never play it because they need to eat They could try making music people actually want to listen to. If they can‘t, that career may not be for them. reply cjk2 1 hour agorootparentI don't think you know how the music industry works reply LightHugger 33 minutes agoprevWhen they say demonetize, are they saying they aren't playing any ads on them or are they just not giving creators a cut of the ads? Those are very different things. These platforms have every incentive in the world to continue monetizing them but just cut the creator out of that monetization their work earned. reply kkarpkkarp 1 hour agoprevClickbait song titles enters the scene... reply ykonstant 1 hour agoparentYou won't believe believing is without you you won't believe anymore~ reply basisword 2 hours agoprevThis shouldn’t be legal. It’s a massive abuse of power. Small artists need to be on Spotify (pulling their music in protest is not viable and not going to lead to change anyway). What gives Spotify the right to arbitrarily decide “you make so little money it’s not worth our time paying you”? There’s no other business you could get away with this in. Scum. reply fundatus 5 minutes agoparentSeriously, if you're a small artist with less than 1,000 streams per YEAR on your tracks, Spotify (or any streaming service for that matter) is already today not a relevant income stream at all. reply silisili 1 hour agoparentprevIt depends on the amount here, which I'm still not actually sure of. Lots of companies today only pay you out once you reach $X, usually some nominal amount. It sounds like that could be the case here. Why deal with payment processors, billing disputes, etc for a customer only earning pennies? reply whstl 1 hour agorootparent> Lots of companies today only pay you out once you reach $X, usually some nominal amount. It sounds like that could be the case here. This seems to already be the case, in practice. Spotify claims that there's about 40 million in unpaid royalties for people who can't withdrawal yet. https://artists.spotify.com/en/blog/modernizing-our-royalty-... reply basisword 1 hour agorootparentprev>> Why deal with payment processors, billing disputes, etc for a customer only earning pennies? Spotify doesn’t have to deal with these things. Artists work with distributors who then deal with Spotify on their behalf. The distributor pays the artist. For me the issue here is they’ve changed the deal to the detriment of the littlest of the little guys. A company that just signed a non-exclusive $250m deal for Joe Rogans podcast, can’t afford to pay $10 a year to a whole bunch of independent artists. That’s outrageous. For me it’s the ethics of it rather than the amount. reply lawgimenez 1 hour agoprevSo which platform pays the highest royalty? reply nicholassmith 1 hour agoparentI did some research on this recently and it's a bit tricky to pin down as some of the information is out of date or doesn't include certain services, but the three that appear to be the best on payouts are: Tidal, Quobuz and Napster. reply Mashimo 1 hour agoparentprevBandcamp :) reply kypro 47 minutes agoprevThis seems quite reasonable. I have quite an uncommon taste in music so I tend to listen to artists with very small followings. I know most songs I listen to only have a few thousand views in total on YouTube, so I was initially worried that it would be similar on Spotify so just took a look... Spotify seems to be about x10 whatever the stats are on YouTube so almost all of the songs I listen to qualify for monetisation. The only exceptions are tiny self-recording artists I've randomly stubbled on from browsing Soundcloud or other corners of the internet. I find it hard to see how this decision would impact small artists to be honest. Maybe if an artist had thousands of tracks all with ~500 streams then it might impact them, but I'm guessing small artists who are currently monetised are making almost nothing anyway from their handful of songs with under <1,000 streams – assuming they only have 2-3 albums worth of music. Supporting monetisation on all songs is probably more headache than it's worth when you're dealing with paying out a few cents. Honestly the best thing you can do to support small artists isn't stream their music, but share it with your friends, donate to them and buy their merch (assuming they have merch because many won't). reply andrewstuart 1 hour agoprevIt's THEIR money. reply dclowd9901 1 hour agoprevHow is Spotify not the loser here? Isn’t it more important to say “we have basically every song ever made” than save a few bucks over small fish streaming? reply rightbyte 2 hours agoprev\"Additionally, Spotify now requires a minimum number of unique listeners for royalties to apply. This attempt to stop \"further manipulation by bad actors\"\" What a load of manure. The only reason manipulation of royalties even works is that they pool the streams such that the consumer wont pay the artist he listens too. Just tie the royalties to what each user listens to and what they pay. Solved. No abuse possible. But no. Have to have an opaque algorithm and do hacks upon hacks to obscure its funamental flaws. reply loctal 2 hours agoparentIf this is true, I demand that I only pay Spotify if I listen to more than 1000 songs a month reply high_priest 2 hours agoparentprevI've recently learned how extraction of funds from gift cards and hacked accounts works. What I've come to understand that obfuscation of monetization model, with simple info for the creators \"beyond this many discoveries, we'll pay you some portion of general activity\" is actually a good way to avoid abuse and allow for easier management of apps funds in general. reply Y_Y 2 hours agorootparentIf that's all they wanted to achieve, then maybe they can pool that \"demonetized\" money and raffle it to the smaller musicians. If the outcome is only that Spotify stops paying out on something they used to pay for, by unilaterally changing the terms for the users without any bargaining power, then it feels like any effect other than increasing net profit is just the cherry on top. You can't just say \"in order to improve your experience we've decided to stop paying you\" and expect people to trust your motives. reply 7bit 2 hours agoparentprevFree Users don't pay anything. But Spotify still wanna money from ads. So tying royalties to what users pay would be unfair to the musicians. reply klabb3 1 hour agorootparentPut a dollar value on the free accounts then? You could even base it off the amount of ads played per account. This is such an easy problem to solve. The pooling of streams is the main main incentive problem, like parent says. reply joshuamorton 49 minutes agoparentprevSomething like 15% of spotify's revenue is from ads, and so even if you do that you have to address the ad-fraud problem. You also aren't even addressing one of the issues here, which is functionally \"listen-time\" related fraud on Spotify, but simple solutions don't really work, since yt has the same issue in reverse. As a general rule, if you think something like \"No abuse possible.\", you are unfamiliar with the problem space. reply estomagordo 2 hours agoparentprevWait, what. Are you saying that the costs of using the service should be tied to how much listening one does? Because if you're not, then what you are saying should have no bearing at all on how much royalties anyone gets. reply Dylan16807 2 hours agorootparentAssuming a fixed fee, it's a tradeoff. Do you let the people with more plays steer everyone else's money, or do you let plays from different people be worth different amounts? I think the first option is worse. Neither option pays a fixed amount per play, so that factor doesn't favor either choice. reply jfim 2 hours agorootparentprevOne way it could work is if Spotify divided say $8 per user across all the streams that user listened to that month. So if a user only listens to Justin Bieber this month, he'd get the full $8 in stream royalties, while someone listening to more artists would redistribute their $8 across all of them. reply rightbyte 1 hour agorootparentprevNo I'd like my subscription (which I canceled due to the new UI and new ToS) should go only to the artists I listen to, proportionally to the amount of minutes played. reply vichle 55 minutes agorootparentSearch for white noise and you'll understand why this is not a great idea. reply hmry 2 hours agorootparentprevI read this as splitting each user's monthly payment only across the artists they themselves listened to (like how YouTube premium works) instead of pooling everything together. So artists with more paying listeners would get more, while artists who only get listened to by free bot accounts wouldn't get anything. reply rain_iwakura 2 hours agorootparenthow would this work if i listen to 1000 bands? the split would be miniscule. reply cuu508 2 hours agorootparentDo you mean, how do you handle $0.001 monthly payouts? Spotify could set a payout threshold. If the threshold is not passed in a given month, the balance carries over to the next month. reply esafak 2 hours agorootparentprevThe split would be fair, and if the same algorithm applied to every subscriber, popular musicians would get paid more. reply Dylan16807 2 hours agorootparentprevIf you listen to one or two songs each from a thousand bands, the amount each band gets is minuscule with either method. reply rain_iwakura 2 hours agorootparentthis was basically what I was asking. would this scheme actually make more money for the musicians? thanks for a reply. reply Dylan16807 1 hour agorootparentThe total amount doesn't change, but I think it would be fairer. reply foota 2 hours agoparentprevYou could just listen to your buddie's streams on loop while your asleep so they get a portion of your fees. reply klabb3 1 hour agorootparentYes, but it would generate less than the current system. Today you can generate more revenue for your friend than your account costs, ie a pure financial arbitrage opportunity. This has been successfully exploited already, and I assume they anti-abuse systems in place today. reply chgs 1 hour agoprevI spend £17 a month on Spotify. If all I do is listen to “Bob Bobson sings Bob Bob Bob”, then my £17, minus commission (1%, 5%, 30%, whatever), should go entirely to Bob Bobson That’s not how Spotify works. Instead a lot of it goes to Taylor Swift, who I don’t listen to. reply Arkhaine_kupo 1 hour agoparentEven worse like 90% of the money comes from paid users, but most of the streams comes from free users with ads. Some some barber shop is playing drake 24/7 having ads and 16£ of you 17£ are going to drake because you only listened to Bob bobson 20 minutes today instead of 23 hours:59 minutes like the barber shop. The pay model is ridiculous, every update is terrible and there is no mechanism to let the company know they are running the product into the ground. It used to be heaven for power users,now if they simplify it any more the app will just pick what it thinks you should listen to, give the money to ed sheeran regardless of what you end up listening to and not even let you pause cause an extra button might be too complicated for you. reply Lacerda69 2 hours agoprevAnd the enshitification begins. Remember that you as users are next. Find a sustainable alternative now. I personally recommend bandcamp + youtube combo. Yt to explore and find new music, Bc to buy the music I enjoy and __own__ it. reply Semaphor 2 hours agoparentWhat works well for me is a metal discord community (run by writers of a review blog [0]). Tons of interesting recommendations, including sometimes non-metal stuff, my favorite album of the year so far is chamber folk / Americana [1]. For new music, I tend to at least give a few seconds to every new release that is of an even slightly interesting sub-genre, older stuff sometimes shows up in what people post they are listening to, but there are also listening pods (me and two others are currently doing a one album a day discography run of Necrophobic), and adopted months: March was Screamarch, where two people curated a list of Screamo and adjacent albums/eps for every day, now it’s Finlapril where a Finn curated 1-2 albums a day moving through the history of Finnish metal. Since joining, my monthly bandcamp purchases went pretty far up, though. And I’m only buying digital albums, there are quite a few who do vinyl purchases for even higher bills. [0] https://www.angrymetalguy.com/ [1] Vera Sola - Peacemaker https://verasola.bandcamp.com/album/peacemaker reply cageface 1 hour agorootparentI hardly ever listen to metal any more but I still often check Angry Metal Guy just because it's such a well run independent music review and community site. Unfortunately such sites are increasingly rare these days. reply Semaphor 1 hour agorootparentAnd no ads/payment whatsoever. Purely volunteer-run, for the fun of writing/reviewing. reply paranoidrobot 2 hours agoparentprevI find new music through YT fairly regularly - it's what got me into a whole bunch of artists I'd never seen/heard before. Unfortunately with YT I've had to cancel my Premium subscription on - they want $34/month, and I'm not paying that. I'll move to using third party apps/adblockers, and paying some of the creators I watch directly. reply dacryn 2 hours agorootparentwhy do they want 34? I pay 12 reply nottorp 1 hour agorootparentProbably some dark pattern where they hide the actual price for just youtube music and show you the subscription for a bundle of services you don't want... reply Minor49er 2 hours agoparentprevDiscogs and RYM are great for discovery. Slsk is also still alive and well reply xlbuttplug2 1 hour agorootparentsputnikmusic is my go-to. reply baumschubser 2 hours agoparentprevIt is not exactly the new kid on the block, but I find tons of interesting music on the music blog aggregator Hype Machine. YMMV depending on your preferred genre. reply wiz21c 2 hours agoparentprevOwning the music is the thing. reply AtNightWeCode 2 hours agoprev [–] The 1k streams limit will probably only financially effect \"artists\" that spams the platform with new tracks. It is not like it will make any difference for real artists. reply wokkel 25 minutes agoparent [–] So by your definition a real artist should have more than 1000 plays. I listen to small artist from my country which will probably never reach a global audience and they definitely do not get 1.000 plays but are verified artists according to spotify. This policy basically drives more drivel music and leaves out alternative and indie bands. It's a bad policy that needs an investigation by the authorities to force Spotify not to discriminate smaller artists. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Spotify has decided to demonetize songs that have less than 1,000 streams, causing dissatisfaction among some music industry professionals.",
      "The platform aims to boost its income by raising prices and rolling out fresh services, which has led to an uptick in Spotify's stock value.",
      "United Musicians and Allied Workers are pushing for more equitable revenue sharing for artists in response to these developments."
    ],
    "commentSummary": [
      "Spotify's payment policy for artists, especially small indie musicians, raises concerns about fair compensation within the music industry.",
      "Users discuss the impact of streaming services like Spotify on indie artists and propose Bandcamp as a more supportive alternative platform.",
      "The conversation delves into issues with Spotify playlists, Apple Music, music licensing, ad placement on social media, and music streaming apps, offering suggestions to enhance artist compensation and explore better music listening experiences on alternative platforms."
    ],
    "points": 214,
    "commentCount": 214,
    "retryCount": 0,
    "time": 1712557191
  },
  {
    "id": 39960107,
    "title": "D-Link NAS Devices Vulnerable to Command Injection",
    "originLink": "https://github.com/netsecfish/dlink",
    "originBody": "Command Injection and Backdoor Account in D-Link NAS Devices Vulnerability Summary: The described vulnerability affects multiple D-Link NAS devices, including models DNS-340L, DNS-320L, DNS-327L, and DNS-325, among others. The vulnerability lies within the nas_sharing.cgi uri, which is vulnerable due to two main issues: a backdoor facilitated by hardcoded credentials, and a command injection vulnerability via the system parameter. This exploitation could lead to arbitrary command execution on the affected D-Link NAS devices, granting attackers potential access to sensitive information, system configuration alteration, or denial of service, by specifying a command,affecting over 92,000 devices on the Internet. Corresponding CWE CWE-77 (Command Injection) and CWE-798 (Use of Hard-coded Credentials). Affected Devices: DNS-320L Version 1.11, Version 1.03.0904.2013, Version 1.01.0702.2013 DNS-325 Version 1.01 DNS-327L Version 1.09, Version 1.00.0409.2013 DNS-340L Version 1.08 Vulnerability Details: The vulnerability exists in the nas_sharing.cgi CGI script, which leads to: Backdoor through Username and Password Exposure: The request includes parameters for a username (user=messagebus) and an empty password field (passwd=). This indicates a backdoor allowing unauthorized access without proper authentication. Command Injection through the System Parameter: The system parameter within the request carries a base64 encoded value that, when decoded, appears to be a command. Exploitation: Craft Malicious HTTP Request: Prepare an HTTP GET request targeting the /cgi-bin/nas_sharing.cgi endpoint. GET /cgi-bin/nas_sharing.cgi?user=messagebus&passwd=&cmd=15&system= Actual Result Impact: Successful exploitation of this vulnerability could allow an attacker to execute arbitrary commands on the system, potentially leading to unauthorized access to sensitive information, modification of system configurations, or denial of service conditions. Fix Recommendation: Apply available patches and updates from the device manufacturer.",
    "commentLink": "https://news.ycombinator.com/item?id=39960107",
    "commentBody": "Command injection and backdoor account in D-Link NAS devices (github.com/netsecfish)199 points by campuscodi 22 hours agohidepastfavorite103 comments starky 18 hours agoI'd assume that every consumer NAS device is insecure these days. I had a Terramaster NAS and was hit with a ransomware attack because of the poor security of their OS through a feature I had turned off. It caused me to look into it more and realized that all of the consumer NAS devices have had similar security issues. You are far better off getting cheap hardware and running TrueNAS or Unraid on it as they actually get regular software updates and don't have a history of major security issues. reply Ao7bei3s 16 hours agoparentUnraid is not confidence inspiring either. It's just more commercial closed source software, developed behind closed doors and with a slow update cadence (~3 months). They have made questionable security choices anywhere you can see, and I have strong doubts that their code quality is any better. The PHP scripts certainly are a horrible mess, in all ways. For example, shell injection prevention is based on using escapeshellarg at each call site... that pattern is _exactly_ the structural root cause for vulnerabilities like the one D-Link had. In no particular order, and obviously not exhaustive: Everything runs directly as root - nginx, php-fpm, smb, ... No AppArmor/SELinux. There is no Secure Boot support (especially unfortunate since boot is from USB stick). No HTTPS access to web frontend by default. SMB protocol defaults are insecure. SMB shares default to public. SSH allows password-based root login. Pools are unencrypted at rest by default. They have a checkbox to enable telnet for management! Very permissive iptables rules. Almost any features that real competitors like Synology would officially provide come from third parties via a moderately shady app store. Note it's not about any of these individual points. I see above as signal that they are not security experts and see security as an afterthought, rather than as something that deserves a team of experts that specifically cares about it. (There's certainly other fields they also aren't experts in, like UX - their predominant UI pattern is \"list of dropdown fields\". Even in storage, one could have a longer discussion how their Array feature - the true core of their product -, compares to modern solutions. There's a reason they've evolved cache pools to just pools as a separate thing, and some users do pool-only Unraid...) That's all quite understandable since it's a small team with only 2-3 coders (https://unraid.net/about). But nevertheless. reply ffsm8 15 hours agorootparent> Everything runs directly as root - nginx, php-fpm, smb, For the record: you need root on Linux to open ports below 1000. By necessity, these programs need at least one thread that runs as root just from that. Can't comment on the rest. As I never used it. Fedora server + cockpit UI was enough for me when I switched from my Synology NAS the other day reply arp242 13 hours agorootparentYou can use cap_net_bind_service to bind ports 1024 and redirect in iptables, or even with a trivial TCP proxy. There are options on pretty much any system, but certainly on Linux with capabilities. None of these require direct support from the application (dropping root after binding does). You almost never need to run anything as root, especially not with these \"run 6 different types of services in a box\" type of appliances. None of this is new; this was already widely considered best practice when I was starting out 25 years ago. reply duskwuff 12 hours agorootparent> There are options on pretty much any system, but certainly on Linux with capabilities. If you're using systemd, you can grant the appropriate capability to the process by setting: [Service] AmbientCapabilities=CAP_NET_BIND_SERVICE in its service file. Note that this will necessarily allow the process to listen to any port; there is, unfortunately, currently no way to lock it down to a single port. reply matthews2 15 hours agorootparentprevYou can drop root after binding, or you can use capabilities to allow a particular program to bind on privileged ports. php-fpm could listen on a UNIX socket instead of a TCP socket. reply Ao7bei3s 14 hours agorootparentExactly. A more modern secure approach is to let the init system open the socket and pass it as an FD. This has some side benefits too (not even temporary root for daemon, less custom code, standard&declarative config, socket activation). (Of course Unraid, being based on Slackware, has a legacy init system that doesn't support this scheme. But there are enough other options.) reply 0x073 17 hours agoparentprevTerramaster is a cheap young Chinese nas brand. Never had issues with Synology. reply starky 17 hours agorootparentAs I said, while that was the brand I had, brands like QNAP and Asustor have also had the exact same issues. These devices are all just examples of poorly supported hardware, where a solution where you control the hardware and software installed on it is far more likely to be supported going forward. reply syntaxing 16 hours agorootparentQNAP is known to have garbage security though. I personally have a Synology NAS with no internet access (blocked using my firewall) reply m463 11 hours agoparentprevI think people should learn how to configure their network in the first place. - Don't get a cloud router (ever!) - learn about vlans and use them. - never hook a critical device like a NAS to the wider internet reply IYasha 13 minutes agorootparentWhat good is VLAN if an attacker breaks into your router? (non-rhetorical question) He could forge any packets and networks after that. I also have doubts in my separate wire setup (data LAN + management LAN) because any device with both cables attached can potentially be made into a bridge. reply ajross 17 hours agoparentprev> You are far better off getting cheap hardware and running TrueNAS or Unraid on it as they actually get regular software updates and don't have a history of major security issues. Unpopular opinion: really you'd be \"far better off\" using Dropbox or Drive or whatever. By demanding to store your junk yourself you've already walked away from the clearly most robust solution. Obviously this will engender the standard HN freakout about privacy and Big Tech and whatever, and I won't engage. But at the narrow level of robustness and security, the cloud experts are going to do this objectively better than anything under your personal control, probably by more than an order of magnitude. reply derekp7 17 hours agorootparentYou are leaving out the other primary reasons for a local NAS -- upload bandwidth, data caps, and special usage scenarios. Not to say that cloud-hosted solutions purely for backup purposes don't have their place, but it is just one component of a 3-2-1 backup policy. reply starky 17 hours agorootparentprevThose aren't an equivalent solution at all. A NAS is for storage of lots of data, even the biggest options from the companies you mentioned wouldn't provide as much data as I have on my NAS. If I were to host the data in a real solution for larger amounts of data such as with Backblaze I'd have to pay nearly $1000/year. There are multiple different tiers of data that I keep: on device, encrypted snapshots, cloud, and NAS. There is differing amounts of money and effort I'm willing to spend depending on the type of data, but no one solutions is sufficient for everything. reply dvngnt_ 11 hours agorootparentseems pretty cheap tbh reply beeboobaa3 17 hours agorootparentprevHave you seen how much those services charge for even just 10tb of storage? It's insane. reply ajross 16 hours agorootparentLooks like $15 a month at Dropbox (it's true that the other providers don't count that high[1] in their standard rates, so you end up having to do it as more expensive overage). Seems like a bargain for not getting backdoored to me. But then I did say it was an unpopular opinion. [1] I do chuckle at the use of \"just 10tb\". Obviously the real issue here is that these NAS boxes are deployed in service of movie and porn collections that people don't want to leave with a third party. But my point was about reliability and security, not legality or propriety. reply arp242 13 hours agorootparentIt shows \"€12/user/month\" for Business (9TB), but that's with a 3 user minimum, so €36/month. The \"Essentials\" package for individuals is €16.58/month, but has only 3TB of storage. I don't know what it would cost to add extra storage for that (doesn't show a price), but I would be surprised if it was less than €25/month, and wouldn't be surprised if it was quite a bit more than that. reply beeboobaa3 16 hours agorootparentprevFrom what I can tell dropbox will charge you E14.50*3=43.5/month (Business plan with a minimum of 3 users) or E522/year for 9TB, but maybe I'm missing something in their pricing? > I do chuckle at the use of \"just 10tb\" It's 2024. 2TB SSDs can be bought cheaply, and so can 16TB HDDs. reply IshKebab 14 hours agorootparentYeah but the fact that you can buy a 10TB disk doesn't mean it's normal to need to store 10TB of data. I think bandwidth is a bigger issue with cloud storage. Most people have terrible upload bandwidth. reply BenjiWiebe 10 hours agorootparentMy upload bandwidth is 3Mbps. My NAS is an old computer running Linux and samba. Due to the bandwidth issue cloud is a complete non-starter. reply beeboobaa3 10 hours agorootparentprevIt's not normal for you. reply IshKebab 3 hours agorootparentIt's not normal full stop. A tiny fraction of people might need it but it's definitely not the norm. reply aborsy 15 hours agorootparentprevDropbox speed is low. Even in Dropbox plus, ordinary features are behind paywall. Not interesting. Also, it’s not a good idea to store TBs of personal information on someone else’s computer or potentially the internet. reply lupusreal 16 hours agorootparentprevIf you're using a NAS as regular storage as I do, having it on your LAN gives you far superior performance. NFS exposed only over wireguard is secure enough for me and I've never had any worries about \"robustness\". Inb4 the old \"dropbox/ftp HN suggestion\" meme. I'm not recommending my setup for common users, or even for you. It suits my needs and that's all that I really care about. reply NKosmatos 20 hours agoprevThe thing of interest is that although the DNS-320L, and the other D-Link NASes, is EOL (End Of Life), there are more than 90,000 devices still operating out there! The bad thing here is that many manufacturers, even big ones, tend to forget “old” products and drop support for them. Usually it’s a market/business decision, but this is what happens with closed systems :-( Quoting from https://www.dlink.com/uk/en/products/dns-320l-sharecenter-2-... : >> This product was phased out on: 13/11/2017 >> This product's last date of support is on: 13/11/2019 Being an owner of 320L, I don’t expect D-Link to offer us an updated firmware any time soon. reply pierrec 18 hours agoparent>I don’t expect D-Link to offer us an updated firmware any time soon Don't worry, some botnet will infect all of these and fix the vulnerability in order to prevent competing malware from gaining access. This fix will deploy within an impressively tight timeline. reply bchanudet 17 hours agoparentprevSometime ago I found an alternative \"OS\" for my DNS-320L called Alt-f [0]. Granted its UI is very old-school but it has worked flawlessly. There has been no release since 2022 but at least it's more recent than the last official firmware (and I assume is not impacted by the vulnerability). IIRC I didn't even had to format or move the data I had on it before installing. [0] https://sourceforge.net/projects/alt-f/ reply captn3m0 19 hours agoparentprevIs there a better way to track D-Link EOLs? Would be nice to have this up at the endoflife.date. reply tmoertel 21 hours agoprevAre there any hardware manufacturers that can be relied upon not to have big security problems? reply mtlynch 20 hours agoparentThese problems all go away if you don't expose crappy management software directly to the Internet. Don't allow anonymous inbound traffic on your firewall to hit your NAS (or any other interface that you're not updating and monitoring rigorously). If you need remote access to your NAS, install a VPN (e.g., Tailscale, ZeroTier) on the NAS or a different host on your internal network, and then access it remotely over an SSH tunnel. I understand that the casual home user doesn't know how to install a VPN or do SSH tunneling, so they end up exposing their NAS through the firewall, and that's why these issues are widespread. But for people who are a bit more technically savvy, you can massively reduce your attack surface by denying any inbound connections through your home firewall. reply kevincox 20 hours agorootparentWhile \"don't let any possible attacker see the device\" is good defence in depth it is a pretty shitty only level of security. Even if it is protected outside of your local network many people have crappy IoT devices or even just laptops with malware on their network. In top of that this won't even really prevent this vulnerability as it can be triggered by a GET request. This means that if you are connected to the VPN any site you visit can trigger it. (If they can guess the hostname or IP) reply mtlynch 20 hours agorootparent>While \"don't let any possible attacker see the device\" is good defence in depth it is a pretty shitty only level of security. If we're talking about a company, I agree. For a home, I think VPN + firewall is sufficient security against insecure management software on network devices. >Even if it is protected outside of your local network many people have crappy IoT devices or even just laptops with malware on their network. If there's a malicious laptop within your network, what difference does it make if your NAS has a security vulnerability? If your laptop is compromised, it's game over anyway because the attacker can steal your cookies or install a keylogger. If it's a housemate's laptop that doesn't have access to the NAS, they can do ARP poisoning[0] and MitM your NAS to steal your credentials (unless you're also vigorous about managing your own TLS CA). >In top of that this won't even really prevent this vulnerability as it can be triggered by a GET request. This means that if you are connected to the VPN any site you visit can trigger it. (If they can guess the hostname or IP) Other websites can trigger the behavior, but they can't read any responses because of same origin policy, so they can't exploit anything.[1] Correction: I overlooked in the writeup that the GET request allows RCE. So, you're right is that malicious websites can exploit this despite SOP. Still, firewall + VPN reduces your risk by many orders of magnitude. Instead of the attacker trivially scanning for your device or looking it up on Shodan, they have to trick you into visiting a malicious site. [0] https://en.wikipedia.org/wiki/ARP_spoofing [1] https://developer.mozilla.org/en-US/docs/Web/Security/Same-o... reply kevincox 19 hours agorootparent> they can do ARP poisoning[0] and MitM your NAS to steal your credentials (unless you're also vigorous about managing your own TLS CA) All traffic to my NAS is secured, even over the local network. I don't trust the network and I don't think it should be game-over for my NAS because my housemate installs some malware. I use SSH with pinned keys and TLS (thanks Let's Encrypt). I think if any NAS appliance doesn't provide some basic security like this it is a failing of that vendor. Even a self-signed TLS cert to provide TOFU validation would go a long way. (And actually in many ways be more secure than a public CA issued certificate.) reply mtlynch 19 hours agorootparentI guess I'm not really sure what you're advocating. These measures sound good for you, but what percentage of HN readers can do that? How many threats does it eliminate in practice beyond what you'd get from basic VPN + firewall? There's always additional security measures you can take, but the costs go up and the marginal benefits go down. It's a bit like saying it's not sufficient for other HN users to deadbolt their doors because it's weaker than the armed guards that patrol your property 24/7. reply kevincox 19 hours agorootparentWell what are you advocating? It seems like you are suggesting that device security isn't important because it is hard to do and a firewall will solve most problems. But at the same time we are looking at a vulnerability that works through a firewall. I am just suggesting that even in a presence of a firewall we should expect appliances to be secure on a hostile network. That is the standard we should hold vendors to. Much like Android, iOS, Windows and macOS hold themselves to this standard. I'm also not saying that you should test this standard, defence in depth is very valuable. But I don't think saying \"it is hard and a firewall solves many issues\" is a good reason to forgive these vendors for shit security. reply mtlynch 18 hours agorootparent>Well what are you advocating? It seems like you are suggesting that device security isn't important because it is hard to do and a firewall will solve most problems. I'm advocating firewall with no inbound traffic + VPN for remote access. It mitigates most of the risk of devices on your network having security vulnerabilities. My original comment in this thread was that it's more important to firewall + VPN your network than it is to pick a network appliance with a good security record. I agree that there certainly are additional precautions you can take, but I think firewall + VPN defends against most practical attacks the average HN user would encounter. Firewall + VPN is something a large proportion of HN (40%?) is capable of doing with about an hour of effort. I think it's a pretty small segment of HN that's even capable of the precautions you're talking about (separate VLANs, custom certificates, hardening every host), and it would take person-days of effort to replicate. >But I don't think saying \"it is hard and a firewall solves many issues\" is a good reason to forgive these vendors for shit security. I don't forgive vendors for shit security. I agree with you that we should hold vendors accountable. RCE on an unauthenticated GET request is absurdly incompetent and irresponsible, and D-Link deserves punishment. That said, I'm not in a position to influence D-Link, but I am in a position to help users protect themselves if appliances on their network have mistakes like this. reply asveikau 17 hours agorootparentprev> For a home, I think VPN + firewall is sufficient security against insecure management software on network devices. If you have some iot devices which phone home on the same network, then this theoretically could be a vector. It can be hard to know if that's the case. On the more paranoid side, be sure your guest wifi can't access these devices. reply Dalewyn 20 hours agorootparentprev>crappy IoT devices Those should all go on their own vlan, or for a more Joe Average(tm) approach the guest network. >laptops with malware on their network. At that point you've got bigger fish to fry than shitty firmware, all bets are off. reply kevincox 19 hours agorootparentWhy are all bets off if you have malware on my network. My devices are hardened with the expectation of being on hostile networks. My computers, phone and home server all assume that they are exposed to malicious actors. Of course defence in depth is also important, but there is no reason that all bets should be off if an attacker can access your device over the network. reply fulafel 19 hours agorootparentThis is the only sane way. You are then able to reason about impact and reasonably investigate when you inevitably end up dealing with a incident. Relying on a \"trusted internal network\" went out of fashion a long time ago. reply Dalewyn 19 hours agorootparentprevDo you consider yourself a malicious actor to yourself? Because that's what you're saying. I am assuming the hypothetical compromised laptop is your own, and thus assuming it contains credentials. What credentials? Everything. All your credentials are belong to us. At that point, the only way to defend against that is to have never trusted yourself. That's a ridiculous stance for any normal person; their home network is a trusted space. That's why I'm saying all bets are off, you have bigger fish to fry than caring about some shitass firmware written by some third rate bank clerk in Bangladesh in a plastic box made by the finest sweatshops in China. reply kevincox 19 hours agorootparentWhy would you assume the compromised laptop is my own? What if I have roommates? Or a friend comes over for a LAN party? I want them on my real network for Steam game transfers and game traffic. Maybe I want them to pull mods off of my NAS. reply Dalewyn 19 hours agorootparentIn that case, yes that network isn't a home network and should be untrusted as such. Personally were I in your situation, I would have individual computers for each network as needed for total and proper segregation of trust; a NAS for the home and a NAS for the public (eg: LAN party) network. I would not trust a single computer with all sorts of data to safely handle two wildly different levels of trust simultaneously like that. I consider the home network a trusted enclave. Thus, all bets are off if a compromised laptop is in the home network. I assumed the laptop was your own as a worst case scenario (Credentials! In plain arm's reach!). If an untrusted third party laptop breached or was invited into your home network, that's not immediately as bad but the bets are still off. Basically: I see worrying about shitass firmware when you haven't secured the network as pointless. The hypothetical situation presented was a compromised laptop in the network; forget the firmware, you need to deal with that laptop and then commence cleanup of the network first. reply dvdkon 18 hours agorootparentWe seem to have very different approaches to security. I'd instead say \"forget about the network, you need to secure your servers\". I fundamentally dislike \"secure enclaves\", since they amount to giving up at some point. You can instead meaningfully exercise \"defence in depth\" by trying to keep file servers secure against unauthenticated attackers, or even authenticated ones; by not running unneeded network services on your desktops; etc. Securing one layer perfectly might be tempting, but it's much more work than securing multiple layers \"well enough\". In other words, \"shared WPA2 WiFi + an up-to-date TrueNAS system + multiple accounts for different users\" will be much less work and more practical than any network-only solution you can suggest. A layered approach also means that you don't need to worry so much about someone else's screwups. A friend had malware on their laptop, but didn't have credentials for the NAS? Not much need to worry, maybe just change the WiFi password and you're probably fine. They had access to a limited account? Probably only the data on that account is compromised. Most times you won't even get to check logs to confirm or start some top-to-bottom \"cleanup\", since you'll never know this even happened. reply Dalewyn 6 hours agorootparentAt some point you have to put more trust into your home network than you would a public network. Not as far as accepting all communications, obviously, but in the sense that you have to open ports and services in order to accept incoming authentication requests and subsequent authenticated connections. A locked door is weaker than a stone wall, after all. You're trusting that it's safe to have a locked door than a stone wall. reply amluto 18 hours agorootparentprevThere are two major problems with this approach: 1. With gadgets like this, the gadget is likely to be your firewall. 2. You are almost certainly running a sandboxed agent inside your firewall that runs untrusted code: your web browser. CSRF attacks taking over crappy devices like the devices in question are a thing. reply wepple 18 hours agorootparentprevThis advice massively reduces the largest risk, but “problems all go away” is wildly wrong There are a ton of remote access and update functionality in these things that make them fully exploitable even with no internet exposure. Theyll do crazy things like VPN back to home servers, where every single device is on a flat /8 with no additional security. So anyone who dumps the VPN keys from one is basically on your eth. Or update unsigned firmware from domains that end up abandoned and left hanging Etc etc reply mtlynch 16 hours agorootparent>This advice massively reduces the largest risk, but “problems all go away” is wildly wrong By the problems all go away, I meant the problems associated with a network appliance handling malicious input that leads to a security issue. If the device isn't exposed to the Internet, it can't bungle malicious input. >Theyll do crazy things like VPN back to home servers, where every single device is on a flat /8 with no additional security. So anyone who dumps the VPN keys from one is basically on your eth. > >Or update unsigned firmware from domains that end up abandoned and left hanging Are there cases of that happening with products that have wide usage? Like 1M+ devices in the field? The only time I can recall hearing of anything like that, was for a very small time vendor.[0] Every security vulnerability I've heard about around NAS devices required the device to be exposed to the public Internet or for the attacker to be on the same local network as the vulnerable device. [0] https://news.ycombinator.com/item?id=34325695 reply wepple 16 hours agorootparentYeah, there have been a bunch. The last one I remember was a fairly well used security camera. VPN was how they streamed video back to storage. A really “simple” solution to turning a LAN-based video camera into an internet-connected one. I’ll see if I can drag up some references, a quick google doesn’t show what I’m looking for. reply rfoo 21 hours agoparentprevWhat kind of hardware? For home networking? Ubiquiti is okay. As bugs in these enterprise/SMB networking gear worth a little $$$ so there's at least people looking for low hanging fruits. The best option however, is to find a recent MediaTek-based (because they are currently the most upstream-friendly WiFi AP SoC vendor) home router with vulnerable firmware (for easier flash lol) and replace the firmware with OpenWRT. For NAS just build your own, or buy used rack-mount servers off eBay and leave them in basement. reply wepple 21 hours agoparentprevAmazon, Apple, Google tend to ship hardware that at least doesn’t having gaping 1997 style vulnerabilities everywhere. Cisco have had some doozies but overall aren’t terrible. Everything else is probably bad. reply jwells89 19 hours agorootparentI wish Apple hadn’t dropped out of the router business. The only good replacements all seem to be considerably more involved, e.g. a full Ubiquiti setup or one of the more mainstream routers with third party firmware. AirPorts were excellent for shoving into a corner and not having to think about aside from the odd firmware update every few years. reply hwbunny 21 hours agoparentprevD-Link always was an almost trash kinda company. reply arp242 19 hours agoparentprevTo the best of my knowledge there has never been a remote exploit for a Commodore 64. reply basementcat 19 hours agorootparentMorning standup at a state sponsored hacking organization. Bob: I hope everyone had a good weekend. I’d like to start out by giving some kudos to Fred and Jane for the D-Link NAS back door! A round of polite clapping. Bob: Let’s start with Igor, how's it going? Igor: Bah, my target is running web server on Commodore 64 so I spend all weekend to write 6502 remote exploit for Contiki OS. Unfortunately, I found out target keeps secrets on ZX81 but I don’t know Z80 assembly. Bob: Fortunately we’re a state sponsored hacking organization so we have considerable resources. Federico, do you think you could give Igor a hand? Federico: Certamente! reply candiddevmike 21 hours agoparentprevUse generic hardware or build your own, and install an OS you can manage yourself reply tomaskafka 19 hours agoparentprevMikrotik seems to be pretty regularly updated, even old devices - assuming they're not NSA frontend :) reply jqpabc123 17 hours agoparentprevShort answer --- no. The key term here is *hardware manufacturers*. They have no financial incentive to worry too much about software. The solution to every problem is to buy new hardware. Software bugs are just added incentive. reply panja 20 hours agoparentprevUse your own hardware and something like Unraid reply codedokode 21 hours agoprevAs I understand, the problem is that authentication used users from /etc/passwd and allowed to log in as any user, even as system user like \"messagebus\" which has no password. It is annoying that linux software uses system database for authorization, for example, Postgres and Samba do this and there is always a risk that you have some system user you don't know about which can be used to access your system. reply segfaultbuserr 20 hours agoparent\"No password = locked account\" is today's default policy on most systems, for a reason. reply jesprenj 19 hours agorootparentNot really. To be really sure, set ! as a password in /etc/shadow. That's what most distributions do. PAM has another security measure, if user's shell is not in /etc/shells, it's also considered a locked account and will not allow login. reply segfaultbuserr 19 hours agorootparentOf course, follow the best practices if you want to be really sure, e.g. set the password field to \"!\" so it's invalid, set the shell to /sbin/nologin, set the home directory to /dev/null (doesn't really do anything but it's a convention), etc, etc, etc. The \"must have password\" policy is for preventing accidental mistakes by people who don't know anything about what we just said, and my experience is that it's an effective one. reply 0x0 19 hours agorootparentSetting the home directory to a special device node file \"/dev/null\" sounds like a good way to shoot yourself in the foot, and I've never heard of this practice before. On Debian, at least, it seems like setting the home directory to \"/nonexistent\" is more common. For example, running \"deluser\" later could end up performing a \"remove home\" cleanup operation, and you certainly don't want to remove the /dev/null file. reply TacticalCoder 17 hours agorootparentprevYup and for outgoing traffic firewalling rules can also be put in place to only allow users authorized to emit traffic. Drop or reject anything by default, then only allow some users to send packets (like \"john\" and \"_apt\" [to update packages], etc.). It's not incompatible with the other measures you described. FWIW I also log, for every \"user\" on the system, anything from a user that's not supposed to access the net. reply candiddevmike 21 hours agoparentprevUnraveling this and dumping NSS would probably have a measurable increase in security and isolation. A backdoor xz thing in libnss would be catastrophic. reply duskwuff 12 hours agorootparentYou're probably thinking of PAM (passwords and authentication), not NSS (hostname lookup). Both are direly in need of modernization, though. reply jra_samba 16 hours agoparentprevSamba doesn't use the passwords or users in /etc/passwd directly. You have to map any SMB users into /etc/passwd users in Sambas database. Without that mapping they don't exist for Samba. reply okl 20 hours agoparentprevSamba has its own user DB with the default auth backend and allows per share restrictions. Same with Postgres. \"PostgreSQL database user names are logically separate from user names of the operating system in which the server runs.\" [https://www.postgresql.org/docs/current/client-authenticatio...] reply cqqxo4zV46cp 21 hours agoparentprevYeah. It’s a pretty old-hat way of thinking. The ‘unified authentication database’ ship sailed long ago, for better or worse. reply aborsy 16 hours agoprevSynology DSM and apps are closed source. It’s a Taiwanese company, and I wonder to what extent it can be trusted? Anyone has information on the security of DSM? Like, is it compliant for use in sensitive departments? reply bonzini 16 hours agoparentI honestly don't know, but the fact that they give updates for 10 years is impressive compared to everyone else. I bought one in fall 2012 and it got the DSM 7.0 update on 2022 or 2023; even though the UI is really really slow, it gives me a lot of confidence that they know what they're doing. reply eigenvalue 18 hours agoprevI’m surprised that there aren’t projects that aggregate a bunch of these known exploits together with these security search engines to find vulnerable devices and use them to create a TOR-like network of proxy server nodes. Presumably most of these vulnerable devices are running in homes of consumers with residential internet, making the traffic hard to identify as being from a VPN service. Not that I’m suggesting anyone actually do this since it would be highly illegal… reply smashed 17 hours agoparentI've seen compromised iot devices used to run proxy servers. They are mostly resold on black market as residential IPs for web scrapping and such. Since there is a black market for it they mostly get resold, instead of put to use on the free TOR network I guess. Residential ISPs usually monitor the blacklisting of their IP addresses and will contact/suspend the customer once the IPs get listed on public black lists. reply galangalalgol 17 hours agorootparentHow unlikely is it that some major iot device manufacturers outsource the firmware to someone who puts botnets in to start with? Who writes fridge firmware? Does ge or Samsung do that in house? Ovens? Definitely some liability there, but I'm not sure which way that drives it, out or in. reply tadfisher 17 hours agoparentprevWhy do that for free? Folks are perfectly happy to pay to be part of NordVPN's proxy network. reply wolverine876 13 hours agoprevRemember the recent stories about how FOSS is inherently less secure than proprietary systems - and because of the xz exploit, which was infinitely more complex? I'm trying to remember a FOSS system with a hardcoded, passwordless backdoor - it's a big world; there must be some. I almost expect a backdoor (though with a password!) in a consumer NAS. I don't knee-jerk reject proprietary solutions. And they might have an advantage if there was liability for this sort of thing. D-Link should be paying hefty fines for selling this obviously substandard, unprofessional crap to the public. reply speedylight 18 hours agoprevI see IoT devices are still the weakest link in any network. reply TacticalCoder 17 hours agoparent> I see IoT devices are still the weakest link in any network. That's why IoT stood for, since the very beginning, for \"Internet of shitty insecure Things\" : ) reply IYasha 6 minutes agorootparentOr just \"Insecurity of Things\" ) reply srgseg 18 hours agoprevThis is why I always use an encrypted file system, where the encryption keys are only known to the client (and not the NAS or other storage provider). reply prmoustache 17 hours agoparentI just use an old computer full of drives on which I ssh mount. I don't even care setting up samba these days. Even NAS distros/OSes are usually overkill for home and less than 5 different users. In a typical family context you usually only need one filesystem for each user + 1 shared fs with same RW for everyone. reply srgseg 14 hours agorootparentExactly - just a Linux box, mounted with SSHFS. To be fair, I have spent some time over the years writing my own scripts for SMART alerts, but the end result is better monitoring and alerting than a NAS would provide. reply jcpham2 21 hours agoprevWhen my bosses want to know why a quote for a device like a NAS is thousands and thousands of dollars instead of hundreds of dollars I use examples like this. Running consumer gear, especially public facing internet consumer gear is just asking for trouble. TrueNAS/FreeNAS whatever it’s called these days - a real OS with real vendor and community support keeping the project alive and up to date is just necessary. Buying these consumer devices that are set and forget with limited or zero firmware updates is BAD. Not to mention the code quality and unknown closed source backdoors reply lyu07282 21 hours agoparent> TrueNAS/FreeNAS whatever it’s called these days - a real OS with real vendor and community support keeping the project alive and up to date is just necessary. and for routers there is OpenWRT. For example Turris is built on that and provides automatic firmware updates, that's a huge security benefit for those devices: https://www.turris.com/en/ reply jcpham2 20 hours agorootparentHuge OpenWRT fan I’d almost even be tempted to deploy an OpenWRT device somehow as a NAS over any prebuilt consumer device. It’s build it yourself (and get what you pay for) or buy junk and similarly roll the dice. It’s not a terrible difficult value proposition to explain but there’s always the person in the room that knows just enough to not know this is something you do not introduce on your corporate network; you leave it home tucked away behind a firewall reply yjftsjthsd-h 19 hours agorootparent> I’d almost even be tempted to deploy an OpenWRT device somehow as a NAS over any prebuilt consumer device. No ZFS, but it appears to support mdadm and btrfs so that could work if you can get good hardware for it. reply fx1994 18 hours agoprevI bought used DNS-320L and first thing is to load it with ALT-F alternative firmware. Worked great but device was too slow for my needs. reply GEBBL 21 hours agoprevHow did this pass code review? reply riedel 21 hours agoparentWalter Shao [0] says lgtm... (There is simply little awareness and not even minimum standards in the industry) [0] https://forum.qnap.com/viewtopic.php?f=45&t=160849&start=450... reply steve1977 21 hours agorootparentouch… reply mmsc 21 hours agoparentprevWhat code review? reply hwbunny 21 hours agorootparentExactly. reply cqqxo4zV46cp 21 hours agoparentprevThis is completely usual for IoT/networking devices. It’s the longstanding reality. They just come at this stuff differently. reply cqqxo4zV46cp 21 hours agoprev [–] Have I woken up in some universe where the word “backdoor” doesn’t mean what it used to? I am used to backdoor implying malicious intent on the part of the vendor / author, or something left by an attacker. I’m just not seeing that here. This just looks like utterly unsurprising incompetence on the part of a consumer networking / IoT gear manufacturer. Yes, I know what a literal ‘back door’ is, and yes, I know that “make it look like incompetence” could be a strategy. I’d partially blame people being all hot and bothered about xz, but I can see that these files were committed two weeks ago. reply II2II 20 hours agoparentI also get the impression that the definition has shifted, though I don't recall malicious intent from the vendor/author being a part of it. Classical backdoors include things like service accounts that were intended for the vendor, which were genuinely intended for service. Granted, those passwords were intended to be changed. (Also, that's not really appropriate for consumer grade gear though, since consumers have not received that level of support in decades and were not connected to networks when that level of service actually existed.) Either way though, I do seem to recall intention being part of the definition. Ah well. It's just a new meaning to get used to! reply BobbyTables2 19 hours agorootparentThey knowingly created a password less account because of their authentication system! If it isn’t a true backdoor, then it is utter stupidity and negligence! When it comes to security, why is EOL allowed by society? We don’t give 5 or 10 yr old cars a free pass to blow up or stop working without a manufacturer recall. Software is even easier to fix! reply II2II 17 hours agorootparentThe tricky part is determining how long software should be supported for, at least with respect to security updates. I'm sure that most people would agree that 5 years is insufficient. Twenty five years, many would say is too long. As for the car analogy, it is a weak comparison. First of all, the owner is expected to do some upkeep. A car is more likely to be deemed unroadworth from a lack of upkeep as it is from a design flaw or manufacturer defect. The other consideration is that the notion of secure software is a shifting target. In the case of cars, physics is a passive adversary. In the case of computer security, the adversary is active. Is software security to be judged by the security standards of when it is developed, or of when it is used? The former will catch cases like this, but will prove ineffective in the long term. The latter will bankrupt the industry. reply halJordan 19 hours agoparentprevI think you went to sleep in some other universe and are waking up in the real world where backdoor has always meant \"bypasses the front door.\" reply rfoo 21 hours agoparentprev> This just looks like utterly unsurprising incompetence on the part of a consumer networking / IoT gear manufacturer. Then how about Xiongmai NVRs [1]? Can I say it's just an incompetently implemented debug feature left enabled in production? [1] https://habr.com/en/articles/486856/ reply cjbprime 18 hours agoparentprev [–] I'm surprised it wasn't \"authentication bypass\" too. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A vulnerability in D-Link NAS devices enables command injection and backdoor entry via hardcoded credentials in the nas_sharing.cgi script, permitting attackers to run unauthorized commands and potentially compromise data or disrupt services.",
      "Over 92,000 devices are impacted by this security flaw.",
      "The manufacturer has issued patches to address the issue and provide fixes for the affected devices."
    ],
    "commentSummary": [
      "GitHub discussion highlights security vulnerabilities in NAS devices such as command injection, backdoor accounts, ARP poisoning, and RCE attacks.",
      "Recommendations include using alternative firmware, VPNs, firewalls, and regular software updates to enhance security.",
      "Debate on cloud vs. local storage, vendor accountability for secure devices, IoT security risks, and importance of updating software for optimal security are also covered."
    ],
    "points": 199,
    "commentCount": 103,
    "retryCount": 0,
    "time": 1712489957
  },
  {
    "id": 39961910,
    "title": "John von Neumann's Legacy at Los Alamos",
    "originLink": "https://3quarksdaily.com/3quarksdaily/2020/10/what-john-von-neumann-really-did-at-los-alamos.html",
    "originBody": "What John von Neumann really did at Los Alamos Posted on Monday, Oct 26, 2020 2:20AMMonday, October 26, 2020 by Ashutosh Jogalekar by Ashutosh Jogalekar John von Neumann (Image: Life Magazine) ad During a wartime visit to England in early 1943, John von Neumann wrote a letter to his fellow mathematician Oswald Veblen at the Institute for Advanced Study in Princeton, saying: “I think I have learned a great deal of experimental physics here, particularly of the gas dynamical variety, and that I shall return a better and impurer man. I have also developed an obscene interest in computational techniques…” This seemingly mundane communication was to foreshadow a decisive effect on the development of two overwhelmingly important aspects of 20th and 21st century technology – the development of computing and the development of nuclear weapons. Johnny von Neumann was the multifaceted intellectual diamond of the 20th century. He contributed so many seminal ideas to so many fields so quickly that it would be impossible for any one person to summarize, let alone understand them. He may have been the last universalist in mathematics, having almost complete command of both pure and applied mathematics. But he didn’t stop there. After making fundamental contributions to operator algebra, set theory and the foundations of mathematics, he revolutionized at least two different and disparate fields – economics and computer science – and made contributions to a dozen others, each of which would have been important enough to enshrine his name in scientific history. But at the end of his relatively short life which was cut down cruelly by cancer, von Neumann had acquired another identity – that of an American patriot who had done more than almost anyone else to make sure that his country was well-defended and ahead of the Soviet Union in the rapidly heating Cold War. Like most other contributions of this sort, this one had a distinctly Faustian gleam to it, bringing both glory and woe to humanity’s experiments in self-elevation and self-destruction. The origins of Johnny’s far-reaching accomplishments lay in the Manhattan Project. But this fact alone is curious: Von Neumann was never part of the regular cast of characters at Los Alamos which included Robert Oppenheimer, Hans Bethe, Edward Teller, Richard Feynman, Enrico Fermi and other world-renowned scientists; he features as a relatively minor player in most standard histories of the project. He visited as a consultant a few times a year. And yet in some sense, his contribution equalled or even exceeded in the long-term the contributions made by his fellow scientists. This discrepancy between his role in the Manhattan Project and the importance of his work speaks to both his awesome intellect and, more interestingly, to a core element in the history and philosophy of science in which ideas and technologies unexpectedly intersect and piggyback on each other. To understand this more fully, it’s important to understand von Neumann’s origins. From Budapest to Los Alamos via Princeton Johnny at age 11 with his cousin (Image: Michael Vonneumann and Cantor’s Paradise) ad Von Neumann was the quintessential product of turn-of-the-century Jewish affluence and intellectual achievement in the Austro-Hungarian empire. Born in 1903 to a wealthy banker and his wife in Budapest, Johnny was one of history’s great child prodigies, speaking half a dozen languages and learning calculus by the time he was barely past the first decade of his life. He had an amazing photographic memory and an intense interest in history that stayed with him all his life and dazzled his friends and colleagues; as the story goes, by the time he was eight he had read and annotated all forty-three volumes of a comprehensive world history written by the German historian Wilhelm Oncken, and he used to stun his parents’ friends by reciting entire pages from the phone directory as a child. As he grew up he collected around himself some of the great Hungarian minds of the 20th century – Eugene Wigner, Leo Szilard, Edward Teller. Later all of them became émigrés to the United States, and their superior intelligence led others to joke that they were Martians who had learnt to perfectly mimic human beings. Toward the end of his life, Eugene Wigner who won a Nobel Prize for his work on nuclear structure was asked why a tiny country like Hungary produced so many scientific geniuses. Wigner said that Hungary had produced only one genius – Johnny von Neumann. After getting two degrees in Zurich and Budapest, one in mathematics and one in chemical engineering – the latter to to please his father who, like many Jewish fathers, worried that his son might not be able to get a job in spite of his great intellect but due to his Jewish background – von Neumann became an assistant to David Hilbert, one of the 20th century’s greatest mathematicians. While his early forays were in set theory and operator algebra, in a short time he left a blazing trail of contributions whose depth and breadth would be unequalled in the annals of 20th century mathematics and science. By his 30th birthday, he had solved Hilbert’s fifth problem for compact groups, proved the mean ergodic theorem, provided a mathematical foundation for quantum mechanics, proved the minimax theorem, started laying the foundation for game theory in economics and done important work in the foundations of mathematics. He had already become acutely prescient about the deteriorating political situation in Europe. After Hitler became chancellor of Germany in 1933, Johnny started making trips to the United States where he had been invited to lecture at the Institute for Advanced Study. The institute had been set up as a kind of pure thinkers’ heaven by the renowned educator Abraham Flexner, and there were few fields more suited to pure thought than mathematics, philosophy and theoretical physics. Flexner went to great lengths to snag the greatest fish of them all – Albert Einstein. By 1933 Einstein had already been the target of virulent anti-Semitic attacks and was looking for a new home. After visiting a few universities in the United States and England, Flexner lured him to Princeton with the promise of a very generous salary and complete freedom to explore his interests without any administrative or teaching responsibilities. But even by 1933, when Einstein was fifty-four, the twenty-nine-year-old von Neumann was considered important enough to be made one of the first professors at the institute, along with Einstein, Hermann Weyl and Oswald Veblen, a mathematician who was as adept at hacking away at the brush in the Princeton woods as he was at problems in geometry and topology. Veblen was to become an important mentor to Johnny. When the institute’s plans to hire Weyl fell through, Johnny was hired as a permanent faculty member. Ever since he moved to the United States, Johnny showed a great love for his adopted country. His home was the scene of weekly raucous parties, and he loved fast cars, expensive suits and hobnobbing with the rich and famous. His friend Eugene Wigner who had shipped from Hungary as a joint appointment with von Neumann recalled how Johnny was refreshed by the new world and by the youthful enthusiasm he saw there which rejected the reactionary and orthodox ideas of old world Europe, a motivation not uncommon among American émigrés. As Europe crumbled in the face of fascism in the next few years, Johnny was joined by European scientists like Bethe, Fermi and Weisskopf who became household names, started great schools of science and catapulted America into the front ranks of science and technology, a position that it continues to hold in no small part due to such immigrants. Oskar Morgenstern and von Neumann published “Theory of Games and Economic Behavior” in 1944, when Johnny was already deep into defense work (Image: ThatsMaths) Because of his intense interest and background in history, Johnny could already see in 1935 that there would be some kind of war between Germany and other countries. By 1937 he had started expressing interest in the more applied parts of mathematics. His work on the minimax theorem and the foundations of quantum mechanics was already dazzling, especially for a pure mathematician, and in Princeton he had struck up a friendship with another Austrian emigre, the economist Oskar Morgenstern. During the next few years he would embark on the writing “The Theory of Games and Economic Behavior” with Morgenstern which is now considered the foundation of game theory. What accounted for Johnny’s forays into applied mathematics and away from pure mathematics? One reason was certainly his capacious mind that roamed over diverse domains out of sheer burning intellectual curiosity, but I also personally feel that it was partly a disillusionment with the foundations of mathematics, a disillusionment that was made acutely clear by the work of his friend and institute colleague Kurt Gödel who in 1932 had struck a blow at the foundations of mathematics through his famous incompleteness theorems. Johnny had extended Gödel’s first theorem in short order, only to find that Gödel had already gotten there. Later he did what he could to ensure Gödel’s appointment at the institute; the two friends now lie only a few feet from each other. Gödel’s work might have convinced Johnny that making deep contributions to pure mathematics was perhaps not his forte. Fortunately for him, the escalating situation in Europe brought a new opportunity that would make full use of his intellectual powers. Shaped charges and the Aberdeen Proving Ground Charles Munroe (Image: Wikipedia) In 1888, an American chemist working for the navy named Charles Munroe discovered something quite wonderful and intriguing. Monroe found that when he detonated a block of explosive with the company’s name engraved on it with raised letters near a metal plate, the raised letters somehow got “transferred” into the metal plate. What was happening was that the explosive blast from the block took the shape of the letters – and stayed that way for some time. Later Munroe demonstrated this effect even more convincingly with a metal safe. As he described it in a 1900 ‘Popular Science’ article, “Among the experiments made … was one upon a safe twenty-nine inches cube, with walls four inches and three quarters thick, made up of plates of iron and steel … When a hollow charge of dynamite nine pounds and a half in weight and untamped was detonated on it, a hole three inches in diameter was blown clear through the wall … The hollow cartridge was made by tying the sticks of dynamite around a tin can, the open mouth of the latter being placed downward.” Thus was born the principle of the Munroe Effect and the shaped charge, a phenomenon which still has a little bit of a whiff of magic to it. The shaped charge essentially enables a precisely geometrically-shaped explosive to ephemerally create a small pocket of air that is packed with deadly force – hell in a puff of air. When this puff hits a target, even a metal safe cannot help but be cast in its shadow. Shaped charges soon started seeing use in many peaceful applications of explosives to create controlled, precisely shaped, powerful cavities in materials like rocks and metals. By the time the 1930s came along, scientists in many countries had gotten interested in them. A particularly important contribution was made by R. W. Wood, a Johns Hopkins physicist whose initial interest stemmed from morbid accidental deaths by shaped charges. An illuminating history of shaped charges tells the story of how the technology gradually percolated from America to Britain and back to America. But there was an ominous detour in Germany. An Austrian physicist named Rudolf Thomanek had become interested in the British and American work and was trying to develop an anti-tank gun. By 1932, he had become convinced that the Austrian authorities were “halfwits” who were uninterested – why not try to sell the concept to the much more enthusiastic German authorities? In fact, why not go straight to the top? By the end of 1935, Thomanek had pulled enough strings to find himself presenting the idea to an audience consisting of Hitler, Himmler, Goering and the General Staff of the Third Reich’s armed forces. Contrary to his later, well-deserved image as a strategist who routinely came up with unworkable and outlandish schemes, Hitler was extremely detail-oriented and well-versed when it came to weaponry. Thomanek had put a dented plate on Hitler’s desk. The German version of the American bazooka – the ‘Panzerfaust’ (Image: HistoryNet) “I am in a hurry and will express my opinion immediately”, Hitler said. “This proposal could be the solution I have always wanted to give the individual soldier a weapon to defeat tanks. Maybe the same principle could be used in bombs and torpedoes.” The Führer had grasped the significance of the new idea immediately. So did scientists, engineers and politicians in Britain, the Soviet Union and the United States. In the United States the shaped charge saw incarnation as the fabled anti-tank weapon the Bazooka, a weapon whose detailed workings were kept secret until 1945 when the core principle was revealed by a reprint of Munroe’s 1900 Popular Science article. The point of this slight digression is to indicate that by 1937, shaped charges were a well-studied form of anti-tank weaponry. They were also ideally suited to von Neumann’s particular skill set, and it was at the Aberdeen Proving Ground that they found fertile landfall. The Aberdeen Proving ground in Maryland had been set up during World War 1 for ballistics research. Ballistics research was a clear priority for any army utilizing artillery. The main area of research for ballistics specialists was firing tables that indicated the velocity and the angle at which to fire a projectile to hit both stationary and moving targets. In 1918, Oswald Veblen had been hired to oversee scientific work at the site, and among others had recruited von Neumann’s fellow mathematical prodigy Norbert Wiener to work with him. By the late 1930s, Aberdeen was in a good position to contribute to the war effort and Veblen invited von Neumann to join him. The newly naturalized Johnny was eager enough that he applied to take an exam as a lieutenant in the army reserve in 1939. Amusingly, even though he passed the exam, he was turned down because of an age cutoff. Senator William Smathers from New Jersey expressed his perplexed reaction to FDR’s Secretary of War, Harry Woodring: “Mr. von Neumann is internationally known as a mathematician, and I cannot understand how a man with so much potential value to the American armed forces should be turned down for a technicality.” The reason Smathers and Veblen considered Johnny so valuable to a potential war effort was simple. Although von Neumann had no training in war-related work, his singular skill as a universalist mathematician would be invaluable in analyzing the nonlinear phenomena involved in shaped charge detonation. Crudely speaking, nonlinear phenomena deal with cases in which a given stimulus produces a disproportionate response. Most complex phenomena in the real world of interest to scientists are nonlinear, and in that sense the name ‘nonlinear’ is a misnomer: As von Neumann’s best friend Stanislaw Ulam who had escaped Poland before the Nazis invaded it once said, “Using a term like nonlinear science is like referring to the bulk of zoology as the study of non-elephant animals.” Nonlinear phenomena required mathematical manipulation at the highest level because of the complexity of the equations which usually involved partial differential equations in multiple dimensions. Both the versatility and speed of von Neumann’s mind made him an ideal candidate for analyzing these. As we will see later, Johnny’s interest in nonlinear equations was to have an unexpected and revolutionary impact on modern technology. As the United States entered World War 2 after Pearl Harbor, most European émigré scientists wanted to do their part to help the war effort. Johnny’s point of departure into what became his most important contribution arose from Hans Bethe and Edward Teller’s work on shock waves. In 1941, Teller and Bethe had analyzed how the change in conditions around a shock wave, both in its front and back, can be analyzed in terms of characteristics like the medium’s temperature, pressure etc. To a simplifying approximation, Teller and Bethe found that the conditions at the front of the developing wave can be represented accurately by knowing the conditions at the back of the wave. Johnny was aware of this work, and in May 1941 wrote a letter to Bethe in which he asked the physicist if he and Teller could extend their work to air at 25,000 degrees centigrade. The scientists were finding out, interestingly, that the so-called equations of state for materials that relate pressure, temperature and density to each other were actually simplified at higher temperatures compared to lower ones because of a kind of equalizing behavior of different materials at those temperatures. Johnny’s main contribution had been to figure out that the behavior of the detonation of a shaped charge could be interpreted as the propagation of a shock wave followed by chemical reaction of the materials in the medium. We thus know that even before Pearl Harbor, Johnny was already making himself an expert in the behavior of shock waves at high temperatures. From shaped charges to implosion Curiously, von Neumann wasn’t involved in the early developments leading to the Manhattan Project, perhaps he already had much on his plate at that point. Not only was he consulting for multiple departments of the army and navy, but he was also making trips to England as a consultant. The early steps leading to the bomb project were all taken by physicists like Leo Szilard, Edward Teller, Robert Oppenheimer and Enrico Fermi. After the news of fission came to the United States in 1939, Szilard convinced his friend Einstein to write a famous letter to FDR impressing on him the urgency of starting research into uranium fission. By the summer of 1942, Oppenheimer had been picked by Leslie Groves, the head of the project, to oversee a preliminary theoretical effort into a fission bomb. At Berkeley that summer, Oppenheimer, Bethe, Teller and others convinced themselves that theoretically, a simple fission bomb would be possible. By the end of 1942, Enrico Fermi and his team had done a proof-of-concept experiment and built the world’s first nuclear reactor at the University of Chicago. With Fermi’s experiments the Manhattan Project kicked into high gear, and by March 1943 an isolated site set among the majestic mesas of Los Alamos in New Mexico had been selected for the top-secret project. As has been well documented, the basic principle of building a fission bomb was clear by 1943. You had to fire two subcritical pieces of uranium into one another, and once the critical mass had been exceeded there would be an explosion. It was also understood that this process needed to be exceptionally fast to prevent pre-detonation or a fizzle which would give suboptimal yield. The main challenge was always considered the separation of uranium isotopes and the production of plutonium, a novel material which would be even more fissile. By the time the scientists settled in at Los Alamos, Oppenheimer’s close collaborator and confidant Robert Serber had given a set of lectures summarizing the state of knowledge. It was then that a physicist named Seth Neddermeyer came up with an entirely novel approach to detonating a bomb – an approach called implosion. Instead of crashing two pieces of fissile material together, implosion called for bringing together those pieces inward, preferably in three dimensions to maximize efficiency. It says something about how novel and speculative this approach was that Neddermeyer’s idea met with fierce opposition from the likes of scientists like Bethe, Fermi and Feynman, men who weren’t known for a deficiency of new ideas. Deke Parsons, a naval physicist who was called in to help with ordnance, derisively called it the “beer can experiment”: the challenge was to symmetrically implode or squeeze a beer can from all sides without having the beer squirt out. But Oppenheimer was more prescient and he asked Neddermeyer to conduct preliminary experiments. Stymied by complex three dimensions, Neddermeyer started working with cylinders in two dimensions. By September 1943, the work was trudging along and Neddermeyer was not a particularly harmonious individual to work with, sticking to his ideas and seldom entertaining others. More importantly, what had been already suspected by Oppenheimer’s team in 1942 now became clear – that when it came to building a bomb, understanding the hydrodynamics or flow of shock waves was as important as understanding the nuclear physics and flow of radiation and particles like neutrons. Enter Johnny von Neumann. Oppenheimer had written a letter to von Neumann in July 1943, and the contents of that letter make clear both the high regard in which he held Johnny and the difficulties he was facing: “We are in what can only be described as a desperate need of your help. We have a good many theoretical people working here, but I think that if your usual shrewdness is a guide to you about the probable nature of our problems you will see why even this staff is in some respects critically inadequate…I would like you to come as a permanent, and let me assure you, honored member of our staff. A visit will give you a better idea of this somewhat Buck Rogers project than any amount of correspondence.” At this point it’s worth noting von Neumann’s salient qualities that were considered to be helpful in such a project. I have noted these qualities in detail before in an essay about his major contributions to computing. In a nutshell, while Johnny’s mind was as original as anyone else’s, his main strength was being able to jump ten steps ahead of everyone with his lightning fast mind, look at the big picture and connect disparate ideas together. This enabled him, in his friend Edward Teller’s words, “to identify solutions where most people didn’t even notice the problems.” All this not only made von Neumann a supremely reliable mind to seek advice from but lent his words enormous prestige. Johnny agreed to visit Los Alamos, not as a permanent member but as a consultant in September 1943. When not at Los Alamos, he would work out of the office of the National Academy of Sciences in Washington, D.C. What Johnny essentially did in his critical visit was anoint implosion with his blessings. He confirmed that the best way to achieve implosion would be to spherically place shaped charges around a uranium or plutonium core; the Munroe Effect operating in three dimensions would then squeeze the core to the unearthly densities required to bring about the fission reaction. He also made the critical suggestion that implosion could be made much faster by increasing the ratio of the explosive in the shaped charges to the nuclear material. The faster assembly would not only prevent pre-detonation or a fizzle by making sure that the material has already reached critical mass before the nuclear fission reaction has been initiated, but the higher explosive to fissile material ratio would mean that you would need to use less of exceedingly precious enriched uranium or plutonium. In fact, as Johnny discovered because of his friend Teller who was very knowledgeable about the behavior of materials at high pressure, at the pressures that would exist in the core, materials would get squeezed so hard that they would become not just critical but supercritical, greatly accelerating the efficiency of the resulting explosion. Because of his prestige and confidence in his calculations, von Neumann breathed fresh air into what was until then a creaky, uncertain program that Neddermeyer had been stubbornly sticking to. Fermi, Bethe and other senior scientists who had until then been skeptical of Neddermeyer’s scheme not only trusted von Neumann but regarded his dazzling mind with an awe that would be extraordinary for men who themselves were some of the leading scientific minds of the 20th century. Perhaps the ultimate tribute to von Neumann’s intelligence was paid by Bethe when he remarked after the war that “Von Neumann’s mind seemed to indicate that he belonged to a new species, an evolution beyond man.”, and Fermi once told his student Richard Garwin that von Neumann left him feeling he knew no mathematics at all. After von Neumann’s trip Charles Critchfield, a theoretical physicist on Oppenheimer’s team, said, “Johnny woke everyone up. He was a very resourceful person, at least twenty years ahead of his time.” Once von Neumann made his suggestions, it was quite clear that the implosion program had to be given top priority. There were two main challenges associated with implementing it. One was to work out the complicated nonlinear hydrodynamical equations associated with the shock wave. The other was to find explosive materials which would sustain an imploding wave and have it impinge on the center of the fission target uniformly. To aid with the latter task, Oppenheimer invited George Kistiakowsky from Harvard who was a world-class expert in explosives; in his spare time Kistiakowsky would use his explosives knowledge to raze trees and create ski slopes on the mesa for recreation for the scientists. He also set aside separate groups that would develop the diagnostics and assembly techniques for implosion, including the development of novel detonators that would assure an instantaneous detonation on multiple points of a sphere. None of these innovations or work was trivial, but von Neumann left everyone with the feeling that it was possible. The biggest challenge in putting implosion into practice was to design an assembly of explosives that would ensure a symmetric shock wave – an asymmetry as little as 5% would lead to a “beer can experiment”. The problem with a standard explosion is that it sets off a diverging shock wave. To ensure that this wave impinged on the uranium or plutonium at the center, it had to somehow be turned from diverging to converging. How to achieve this? At this point another valuable character showed up, from England. James Tuck was part of the British contingent to Los Alamos, a small cadre of highly accomplished physicists who were to help with the bomb. The proof-of-concept for fission and critical mass had actually been worked out in England before the United States, so it was natural for British scientists to be involved. Tuck had experience working with explosive materials of differing densities. College physics says that when a light wave passes through materials of varying densities, it bends or refracts differently. Similarly a shock wave would “refract” or bend differently with different densities. And just like a material that refracts and transmits light is a lens, so an explosive that bends and transmits shock waves is an “explosive lens”. Tuck suggested an ingenious arrangement in which a diverging shock wave could be turned into a converging one by layering explosives burning at different rates together. By starting the detonation in fast-burning explosive and then maneuvering the resulting wave inside using shaped charges of slow-burning explosives, thirty-two different waves from thirty-two different points of detonation could be made to converge to a pinpoint at the center, squeezing the material to unearthly densities and triggering the fission reaction. It was again von Neumann who worked out the mathematics of explosive lenses and played a major role in their design. An explosive lens in an implosion weapon (Image: Wikipedia) The real role played by von Neumann, Tuck, Kistiakowsky and others became apparent when a critical observation made it clear that implosion would be the only possible way to assemble the plutonium weapon. In the fall of 1944, Emilio Segrè, a protege of Fermi’s, discovered that the plutonium-239 being used in the bomb contained an isotope, plutonium-240, that was an inevitable byproduct of the reactor-bred plutonium in the reactors at Hanford, Washington state. Pu-240 underwent spontaneous fission without any initiation, and it turned out that the gun method that was being used for the uranium bomb would be too slow to prevent pre-detonation or a fizzle. Implosion would be the only possible way to go for plutonium. Without implosion the plutonium bomb would likely have been abandoned at this stage. Implosion was still considered such a novel concept that the design was tested in New Mexico’s Alamogordo desert on July 16, 1945. Von Neumann wasn’t done with his ideas though. A crucial albeit rather dark contribution that he made was to simplify the calculations used to estimate the pressures from a bomb. Von Neumann knew that for large explosions, the effects of the detonation are not dictated by the length of the pulse but only by the excess pressure that results; this made the relationship between bomb yield and effects a much simpler one. More importantly, using his knowledge of shock wave reflections, Johnny helped to estimate the optimal height at which a nuclear bomb should be exploded to cause maximum damage. On August 6, 1945, the Little Boy uranium bomb exploded at a height of about 1900 feet on top of Hiroshima. About 80,000 to 100,000 people were instantly killed, providing a real-life demonstration of von Neumann’s calculations. Von Neumann, Feynman and Ulam at Los Alamos: Their individual attire seems consonant with their personalities (Image: Atomic Heritage) Johnny enjoyed being at Los Alamos. He reconnected with his old friend Edward Teller and invited a new friend, Stan Ulam, to help with the hydrodynamics calculations. Ulam was almost as versatile a mathematician as Johnny and the two were best friends – significant parts of Ulam’s wonderful memoir, “Adventures of a Mathematician”, are dedicated to describing conversations and travels with Johnny. They tried to recreate the famous cafe atmosphere in Eastern Europe that had fueled late night mathematical bull sessions in the early 20th century before it all came crashing down. In keeping with his predilection for wearing expensive, tailored three-piece suits, Johnny once wore one even when riding a mule in the Jemez mountains near the lab. He also went for walks with Richard Feynman, and in his own memoir “Surely You’re Joking Mr. Feynman”, Feynman points out that it was von Neumann who planted the idea in his mind that knowing how uncertain the consequences of our actions are, you should not have them weigh too heavily on your conscience. Knowing the rather grim endeavor that the physicists were involved with, this was psychologically soothing advice for Feynman. “I am thinking about something much more important than bombs; I am thinking about computers.” For all his critical ideas, the most important and far-reaching thing to come out of von Neumann’s work at Los Alamos had little to do with bombs. Once Johnny suggested using spherical implosion, it quickly became clear that the calculations involved would be too complex for individuals to perform. In England and before, Johnny had already been introduced to early computing machines, and he had been impressed with Alan Turing’s seminal paper on Turing machines and even tried to recruit Turing as his assistant when Turing visited Princeton to finish his PhD before the war. Now when it became clear that computing machines might be needed to handle the complex physics of implosion, Johnny was again in the right place at the right time. Stan Ulam who came to Los Alamos on Johnny’s invitation captured the problem well: “The hydrodynamical problem was simply stated but very difficult to calculate, not just in its details but even in order of magnitude.” He remembered a discussion in which von Neumann and others had suggested all kinds of ingenious shortcuts and theoretical simplifications, and he wondered whether “more simpleminded brute force, that is, more realistic, numerical work” might not be better. To do this kind of work, the first IBM computing machines arrived in April, 1944 to help with the calculations. The implosion problem involved the integration of a hyperbolic partial differential equation in one space and one time coordinate. The integration was carried out using punched cards; Richard Feynman was in charge of this effort. Johnny himself spent two weeks programming the machines and getting a feel for the calculations. He found rewiring the tabulators especially cumbersome, and this frustration had an impact on his later crucial thinking about general-purpose computers. Johnny also knew that Howard Aiken at Harvard had had IBM build one of the first programmable computers, and he arranged for a numerical integration of a second-order partial differential equation to be run simultaneously on the Los Alamos computer and the Harvard computer as a contest; the Los Alamos computer finished first, but the Harvard computer calculated to more decimal places. His experience at Los Alamos immediately suggested to the thinking-ten-steps-ahead Johnny that computers were going to become an invaluable asset in doing complex math and science. Quite fortuitously, around the same time he also came in contact with the pioneering engineers Presper Eckert and John Mauchly who were building one of the first pioneering general-purpose computers, the ENIAC, at the University of Pennsylvania. I have described Johnny’s accidental introduction to computers and his subsequent work as one of the fathers of modern computing in a separate essay, but it is clear that it was his work at Los Alamos that crystallized in his mind the value of computers as an enabling tool for science and technology. It was characteristically far-sighted of him that even before the war ended he was writing to a colleague, “I am thinking about something much more important than bombs; I am thinking about computers.” Johnny’s legacy Johnny von Neumann’s work at Los Alamos had a direct impact on the weapons of mass destruction that leveled Hiroshima and Nagasaki and that still decorate the world’s nuclear arsenals. Implosion is still the predominant technique used in fission bombs, and even in thermonuclear weapons it’s used as the primary fission device that ignites the fusion secondary. After the war Johnny kept on advising the United States government as a high-level consultant; at one point he advised every agency except the Coast Guard. He made valuable contributions not just to fission weapons but to the thermonuclear weapons that are today capable of killing hundreds of millions in a matter of minutes. Just before his death, he became one of the prime movers behind the recommendation to accelerate the United States’s nuclear ICBM program, a development that immediately made the entire world vulnerable to the threat of nuclear armageddon. He was convinced that the Soviet Union posed an existential danger to the security of the United States and advocated not just a preemptive but a preventive strike on that country; thankfully that part of his advice was not taken. Von Neumann receiving the Medal of Freedom from President Eisenhower (Image: University of Texas) He remained a steadfast patriot, often politically at odds with his more liberal colleagues but always friends with everyone, and was held in such esteem that during the last year of his life when cancer unexpectedly struck him, he received the Medal of Freedom from President Eisenhower and spent his last days in a special, guarded suite at Walter Reed Hospital that Eisenhower had specially requisitioned for him. On his deathbed lay a manuscript of a set of lectures comparing the computer with the brain. In it lay ideas whose seeds germinated at Los Alamos and whose branches now extend into artificial intelligence, robotics and neuroscience. Perhaps his most enduring idea in this regard, and one which is very much still waiting to play out, was the idea of self-reproducing automata which could be sent out into space and would populate the cosmos with a diversity of exploding life. Von Neumann and Oppenheimer in front of the Institute for Advanced Study computer in Princeton (Image: IAS) As the eminent historian of science George Dyson put it in his superb book “Turing’s Cathedral”, “Bombs made computers, and computers made bombs.” If designing the implosion lens for nuclear weapons were to be Johnny’s biggest legacy from Los Alamos, it would be a morally dubious one. But his unexpected recognition of the value of computers takes his contributions to a completely new level. While initially he did encourage using computers to simulate the workings of first fission and then fusion weapons, he made seminal contributions to charting out the stored-program concept, random access memory and what is today called the von Neumann architecture. With a talented team of engineers he designed a pioneering computer at the Institute for Advanced Study. Using this computer, his team made forays into a remarkable number of important and fascinating topics: weather simulation, artificial life, fundamental mathematical research, geophysics. The branches that these explorations sent out continue to thrive. Johnny von Neumann’s Los Alamos story shows us that often, the most important impact of a new technology is another new technology. Often this new technology is wholly unanticipated. When Johnny came to Los Alamos, he and his colleagues thought they would be designing bombs, but what they didn’t know was that they would need computers to design those bombs. And that the computers would be far more important than the bombs even in the short term. Ultimately a hundred or a thousand years from now, if humanity still survives and the world’s nuclear arsenals are a dim memory in a history textbook, we will still be using computers, and computers will still be using us. Recommended further reading: John von Neumann and the Origins of Modern Computing – William Aspray Critical Assembly: A Technical History of Los Alamos during the Oppenheimer Years, 1943-1945 – Lillian Hoddeson, Paul Henriksen, Roger Meade and Catherine Westfall John von Neumann: Selected Letters – Edited by Miklós Rédei The Making of the Atomic Bomb – Richard Rhodes Adventures of a Mathematician – S. M. Ulam John von Neumann – Norman Macrae Prisoner’s Dilemma – William Poundstone What John von Neumann really did for modern computing – Ashutosh Jogalekar Von Neumann in 1955 and 2020: Musings of a cheerful pessimist – Ashutosh Jogalekar The Unparalleled Genius of John von Neumann – Jørgen Veisdal John von Neumann – 1966 documentary by the Mathematical Association of America that includes great recollections by Johnny’s eminent colleagues (Eugene Wigner, Hans Bethe, Paul Halmos, Herman Goldstine, Edward Teller and others) Tagged computers, history of science, john von neumann, nuclear weapons, technology",
    "commentLink": "https://news.ycombinator.com/item?id=39961910",
    "commentBody": "What John von Neumann did at Los Alamos (2020) (3quarksdaily.com)195 points by fanf2 17 hours agohidepastfavorite105 comments jonplackett 12 hours agoI’m curious, since I never studied maths past A level. Is there a difference to the way it’s taught now compared to whenever von Neumann was studying. What I mean is, the way I was taught maths and science in general was with a feeling of “look we basically know everything just learn this and you’ll be fine”. Rather than that there are MASSIVE open questions and things we don’t understand at all. Please learn maths and physics and fix them, will you? I’m basically curious how a fairly young person gets into their head a thought like “Hey, why don’t I just revolutionise a few things? That seems totally reasonable for me to do that.” reply mhh__ 11 hours agoparentIt's hard to imagine von Neumann not being great today (I think some stories are probably made up, but he was obviously in the top 1 minds of his era), but it is probably true that the \"easy\" stuff has been mined - where I would define easy as things that can stem from an application of brains around the coffee table rather than intense application of years of whittling at the conference stage. reply vishnugupta 3 hours agorootparentWould the present bureaucracy allowed him to flourish to the same extent though? That’s the question. reply bmitc 2 hours agorootparentI honestly think not. Today's climate sidelines generalists. reply titanomachy 5 hours agorootparentprev> in the top 1 minds of his era Not sure if that’s a typo reply alex_smart 2 hours agorootparentProbably not. The figure is the right order of magnitude. reply prerok 5 hours agorootparentprevNot the author, but probably top 1 percent? reply vermilingua 4 hours agorootparentI don’t think it’s unreasonable to say that JvN was the #1 mind of his era reply xxs 5 hours agorootparentprevtop 1% would be such way out of range. top 0.0001% would still not make it justice. reply hiAndrewQuinn 4 hours agorootparent1 out of every million... Yeah, that sounds about right to me as an actual lower bound. reply alex_smart 2 hours agorootparentNot sure if you’re being facetious? There were not 100 people of the same calibre as JvN in the US or even the world surely. reply lordnacho 2 hours agorootparentJvN, the finished product, probably you're right. But of the people born with his tools? How many were born into a setting that even cared for intellectual things? How many of them would be born into a wealthy family that could afford to hire private tutors for all their kids? reply kgwgk 4 hours agorootparentprevMore likely top 10 - if it’s indeed a typo. reply empath-nirvana 10 hours agoparentprevEventually you start specializing and learning more and more about a particular topic until you know as much about it as anybody else, and then you just keep going. There's always massive open questions because every question you answer generates more questions. reply epolanski 36 minutes agorootparentThis. Which is also why most science published is hard to verify. Pretty much every topic out there is niche enough that the amount of people able to review it (let alone test it) is extremely small and they all know each other. reply kqr 3 hours agoparentprevI strongly recognise this sentiment. It's why it was so refreshing to read The War Trap[1]. Granted, it's 40 years old at this point, but the author took data I have access to, used maths I know, and derived new results on a complicated topic. It's incredibly inspiring. What other complicated fields are just waiting for the right person to come along and discover their fundamental laws? I have a vague suspicion that partial differential equations are criminally underused in modeling for software developmemt and system design. [1]: I wrote a short review at https://two-wrongs.com/war-what-is-it-good-for.html if anyone is curious. reply randomcarbloke 1 hour agoparentprevActually yes, the gymnasium at which he studied in Hungary during the turn of the C20 had a very fast and formulated approach to mathematical learning, it churned out a handful of truly exceptional minds, there is literature but it is sparse. reply chpatrick 11 hours agoparentprevhttps://en.wikipedia.org/wiki/The_Martians_(scientists) There was an incredible cadre of scientists from Hungary in the early 20th century. Sadly they were exiled by fascism. reply wslh 8 hours agorootparentJust Fascism? They would have been exterminated. This subthread shows a lot of ignorance about what we are talking about. Just watch [1]. [1] https://en.wikipedia.org/wiki/Sunshine_(1999_film) reply javajosh 11 hours agorootparentprevOne wonders what fascism could have done had it not alienated intellectuals. But one also wonders if that alienation is core to fascism, even definitive. Fascism is notoriously hard to define, but \"alienates the best thinkers\" seems like a good litmus test. reply tonyarkles 10 hours agorootparentWell... for curiosity-sake I clicked through the list of scientists on the Martians page. Every single one of them has Jewish ancestry. So while fascism more generally (right-wing nationalist authoritarianism) doesn't strictly prohibit intellectuals, World War II fascism alienated these folks because of their heritage. There were still lots of smart people in Nazi Germany, as evidenced by Operation Paperclip which brought ~1,600 Nazi scientists over to the US to work on rockets and help the US fight the USSR in the Cold War and space race. reply epolanski 21 minutes agorootparentAnti semitism is not a core or a common ingredient of what we historically define as fascist governments. Spain under Franco and Portugal under Salazar did not persecute Jews, in fact allowed them safe passage from Vichy France. Italy really turned antisemitic after more than a decade with the alliance with Germany to appease Hitler mostly. There's a nice book on the topic, in Italian, \"Mussolini Razzista Riluttante\" by Antonio Spinosa. reply hattmall 5 hours agorootparentprev>Every single one of them has Jewish ancestry. Well, yes, as that's the defining characteristic of the group and why they moved to the United States and became known as the martians. reply ThomasBHickey 9 hours agorootparentprevA friend of mine wrote a book about what happened in Germany: Scientists under Hitler : politics and the physics community in the Third Reich by Alan Beyerchen. I'm about a third of the way through it, but they mixed politics with the physical sciences to a remarkable degree, driving away many of the best people and isolating German scientists. reply digdugdirk 8 hours agorootparentOut of curiosity, did they start by politicizing the social sciences first? reply vasco 7 hours agorootparentSocial \"sciences\" are always political because they are mostly not science and so is easy to co-opt them. Math doesn't have different results depending on countries political system but social sciences do - tells you all you need to know. reply defrost 8 hours agorootparentprevIs that a well formed question? Of Comte, Durkheim, Marx, and Weber, the people credited with the establishment of \"modern\" (pre WWI & WWII) sciences of society all were \"political\" philosphersthinkers that looked at the behaviour of societies. Max Weber (1864 – 1920) https://en.wikipedia.org/wiki/Max_Weber was a German of some influence, and the pre and inbetween World War periods were a time of intense political debate in France, Germany, and surrounds. Who are \"they\" to whom you refer, and what is \"political\" as opposed to \"social\" ? reply kjellsbells 8 hours agorootparentprevYou mean the Nazis? Yes. (As did the Stalinists in their totalitarian state at the same time). One might guess (and its just that, a guess) that such regimes target the social sciences first because such departments (eg economics, political studies) are adjacent to live politics, the regime's people think they understand them and can make contributions (in reality, trying to take control of course) and of course because the hard sciences require the payment of a hard tax of mathematical skills to say anything useful, which most political hacks do not have. reply globalnode 8 hours agorootparentprevthe fascists distrust thinking that doesnt mirror their own -- the whole \"youre either with us or against us\" mentality that ironically we see all around the western world right now reply nonrandomstring 10 hours agorootparentprevI like the rawness of your thinking/questions. > alienation is core to fascism Fascism, a neighbour of Futurism, idolises technology, of only for its semiotic power. But then so did Soviet Communism. Stalin executed all the \"bourgeois engineers\" and Bolsheviks who built the industrial engine of the CCCP. Alternatively, the great leap forward in China and Pol Pot in Cambodia both rounded up the thinkers, teachers and engineers first. So I don't think it's unique to fascism. What is common? Seems it's a hallmark of the treachery of totalitarianism. Tyrants fear but use intellectuals. They need actual smart people to gain power, but then who is more threatening than people who know how the system works because they built it for you? So they are always betrayed. Consider Oppenheimer. Many of the great scientists and artists are put out to pasture once they've done what's useful for the party. > \"alienates the best thinkers\" seems like a good litmus test. Almost the whole Frankfurt School were Jewish intellectuals who fled to America. I've written (probably in Digital Vegan) on how the Silicon Valley Billionaires, once they've built their \"Social Media Digital Utopia\" - a pageant of vanity, fear and cybernetic governance - would simply dispose of the coders (perhaps not as directly as Stalin). Maybe an indicator of that came early with the layoffs when some got a bit too moist about \"AI\". I think they hope that \"AI\" will be the way to pull up the ladder, leaving no challenging class. reply nonrandomstring 1 hour agorootparentSome wannabe treacherous tyrants didn't like that being said out loud. reply zztop44 9 hours agorootparentprevThank you for this reminder to buy Digital Vegan. I’ve had an open tab on the checkout page for about a year now. reply nonrandomstring 9 hours agorootparentThanks. Wish I could go back in time and rewrite some parts - it's one of those \"needed to write it as a way of thinking\" projects. Better to follow us on the Cybershow now. PM me there and I'l send you a DRM free ebook if you like. reply dzolob 8 hours agorootparentprevI wonder where in the fascist scale is an atomic bomb that actually got thrown to innocent people. reply CamperBob2 8 hours agorootparentBomb somebody else's harbor next time, kthxbai reply sgregnt 8 hours agorootparentprevI wonder if writing needs to update the innocence definition. By that measure, it is very easy to claim one. reply somat 6 hours agoparentprevOften, I feel things are taught poorly. Most just want to sort things out, then teach it systematically from bottom to top. I understand this, it is a lot simpler, but that is not how we figure things out. there are false starts, things that did not work The best teachers, guide you in this process of discovery. reply 0xEF 52 minutes agorootparentSagan's old quote about making an apple pie applies, here, I think. Building on your sentiment, I have always felt that schools (in the US, at least) tend to teach students what to think rather than how to think. There is a huge emphasis on \"here is the answer, now go find the questions\" where I posit more focus should be placed on guiding a student to problem solve, think critically, and discover a few things for themselves, especially at the younger ages reply rowanG077 4 hours agorootparentprevIt's not only simpler. It's also faster. We don't want people exiting high school and they just learned of the concept 0. reply lordnacho 2 hours agoparentprevIt's like that all the way through undergrad, you're just learning things that have been known for a long time. You need to know the basics before you can contribute anything new, and there's a lot of basics nowadays. Once you get to the edge, the game changes a fair bit. You'll need to know the actual experts, rather than just watching YouTube lectures or reading textbooks. In terms of difference at the level you're talking about, the one big difference is that von Neumann's parents recognized how smart he was, and could afford to hire appropriate tutors to accelerate his learning. There's this thing called Bloom's Two Sigma, though I'm no expert on the area it would appear that having a tutor improves results considerably. Now imagine one of the smartest kids ever born having parents who got him tutors. reply j7ake 6 hours agoparentprevI guess the issue is that being able to articulate what is the frontier problems of any field requires a certain amount of jargon and technical knowledge. I think one can learn math and physics without being at the frontier. One can appreciate the beauty of solutions of classical problems by discussing the problem in a historical context. reply k1t 3 hours agoparentprevMy memory of school at that age was they'd teach you one thing as though it was the absolute truth, then the following year tell you it was all a lie (or over-simplification) and actually it works like this. Then the next year they'd do it again. We'd occasionally catch them at it when in Chemistry they'd tell us electrons worked one way and in Physics they'd say they worked another. Anyway, I can definitely see how an enquiring mind might want to move past all this and find out The Truth. reply mkl 46 minutes agorootparentI (in NZ) remember that happening in physics and chemistry, but not maths. Maths has unambiguously and provably correct answers, so while more general methods and topics are introduced, old results don't change. (Maybe this is why I became a mathematician!) reply _glass 2 hours agorootparentprevI hated this. I started school in Saxony (state in Germany), and then my family relocated to Bavaria. The curriculum switched from a science to a language focus, so I already knew all the next-level Chemistry, and was really annoyed by the style of teaching it wrongly. reply sublinear 6 hours agoparentprevContrary to all the fawning in this thread I can only say the somewhat obvious that if you were alive then you'd too think that was a reasonable thing for you to do. reply Projectiboga 9 hours agoparentprevI forget the time frames, but you won't get far out of the 19th century undergrad in math like calculus. Statistics or stuff in Comp Sci you will but not in Calc or Differential Equations. reply prerok 5 hours agorootparentThat's somewhat true, however the way the math is taught has changed. Anecdottally, my father was only introduced to vectors and vector based calculations when at the uni. My generation was taught vectors in primary school already. I think they opted for the different approach to make a more visual representation of the concepts, so younger minds could more easily grasp them. reply keithalewis 5 hours agoparentprevThere are still massive open questions. The current educational system does not providing the tools you need to address them. Maybe you should look into Laslo Polgar. Another Hungarian who taught his daughters how to beat chess grandmasters. reply tekla 12 hours agoparentprevSome people just seem to be born with the gift and are simply better. I recommend reading about Srinivasa Ramanujan https://en.wikipedia.org/wiki/Srinivasa_Ramanujan reply mycologos 11 hours agorootparentRamanujan is an interesting comparison. He was an outstanding mathematician, but it feels like his prowess came from a deep love of math and concomitant time spent on it. There's an anecdote somewhere about child Ramanujan working his way through thousands of elementary lemmas for fun, and I wonder if this gave him the almost occult intuitive ability reported by his collaborators. The stories about him have a tone of awe, but not fear. In some contrast, von Neumann seems to have been able to take his enormous fluid intelligence and speed and apply it to pretty much whatever problem he decided, and he did it across many areas, including non-scientific ones. The stories about him are tinged with a little unease, and it's a little harder to see the human underneath his achievements. reply rowanG077 4 hours agorootparentYou are really deluding yourself if you think anyone can reach Ramanujan levels of skill by simply trying hard enough. This takes insane innate talent in addition to trying hard. reply laichzeit0 3 hours agorootparentYeah it’s like people thinking anyone can be a Micheal Jordan or Tiger Woods if they just tried hard enough, started young enough, were born in the right place to the right parents enough. Some people just have the gift, it’s like neural networks: some random weight initialisations are just better, some maybe even so close to optimum that almost no training is needed at all. reply andxor_ 3 hours agorootparentprevRamanujan was extra-terrestrial. His note books have produced so many papers and PhD theses. Probably will continue to do so for many more years. reply jonplackett 12 hours agorootparentprevThis is fascinating. Thanks for sharing. reply nradov 8 hours agorootparentprevEdward Witten probably falls into that category as well. https://en.wikipedia.org/wiki/Edward_Witten reply dukeofdoom 6 hours agoparentprevNot really an answer to your question. But here's a picture of math entrance exam from 1869 to Harvard. https://imgur.com/a/bDGRU reply mkl 57 minutes agoprevThe YouTube video linked in reference 11 has been taken down, but I found it on the Internet Archive: https://web.archive.org/web/20200626032839/https://www.youtu... I've just watched it, and it was very interesting (though a low quality copy). From 1966, it has many people who knew him talking about him. reply randcraw 11 hours agoprevBTW, the author of this piece is an amazing writer. The rest of his blog and the blog roll at 3quarksdaily are worth a closer look. reply nmwnmw 12 hours agoprevHighly recommend The Man from the Future: The Visionary Life of John von Neumann by Ananyo Bhattacharya. Talks about this time plus many of his other contributions to e.g. game theory. reply stevenwoo 5 hours agoparentI read that and really wanted more details on the science stuff so I guess it's a good intro to get one started investigating, and the part where the author introduces the woman who was the love of his life (up to that point) and then they were married was unintentionally hilarious because she was not mentioned up to that point in the book despite knowing each other since childhood and prior pages had stories about him carousing at university, IIRC. reply serjester 4 hours agoparentprevPersonally I found this underwhelming- the book read like a long CV. I wish it talked more about him as a person. But to each their own. reply AlbertCory 10 hours agoprev> “Using a term like nonlinear science is like referring to the bulk of zoology as the study of non-elephant animals.” Gotta remember that one. Even though it wasn't JvN. reply hilux 3 hours agoprevInteresting: Oswald Veblen, the recipient of the von Neumann letter, was the nephew of Thorstein Veblen, who gave a name to \"conspicuous consumption,\" among other things. reply credit_guy 14 hours agoprevPersonal speculation: without John von Neumann's contributions to the Plutonium bomb, there's a good chance we would not have hydrogen bombs to this day. Here's why: the US had already had a feasible bomb design, the Uranium-235 based bomb, that was dropped at Hiroshima. Not only that, but it had an alternative to the Plutonium bomb too, the U-233 bomb. Glenn Seaborg, the guy who discovered Plutonium and a bunch of other elements, was tasked with doing a feasibility study of a bomb with U-233. He found out it can be done, and sure enough, a few years after the war the US build such a bomb and tested it. It was not done because the Plutonium bomb became possible (with von Neumann's help). But again, if von Neumann had decided to spend his time on other problems, then the US could have focused on the U-233 bomb instead. This leaves us at the end of the war, when the Soviets decided to steal the secret and build their own bomb. Stalin decided they'll build an exact replica of the Nagasaki bomb, which they did and tested in 1949. If only U-233 and U-235 were on the table, they would have picked one of those, rather than explored an uncertain design. From uranium bombs to boosted uranium bombs there's a small step. So the world would have seen much bigger bombs than the ones dropped on Japan. Fission bombs were built and tested that got close to one megaton. But the hydrogen bomb is fundamentally an implosion bomb. The hydrogen bomb was a side-effect of the deeper understanding of the implosion design. In particular, the US figured out that if you can do implosion with conventional explosives, you can do it even better with nuclear explosives, so it designed and tested a two stage fission bomb, the Castle Nectar bomb. It's the only non-thermonuclear bomb ever detonated that had a yield above 1 MT (it was 1.8 MT). The research into this two-stage bomb is what Ulam was doing, and he told Teller that maybe what works for a second stage that is a fission bomb could work for a second stage that's a fusion bomb. Teller added his own insights, and eventually it was done. reply dilyevsky 5 hours agoparentImplosion design would have been worked out eventually with JN or not. Computer models make this a lot easier and they probably could have gotten there even without via trial and error. reply Animats 2 hours agorootparentThere was a lot of trial and error involved. Huge numbers of explosive tests were made out in the desert. A technique was developed of taking X-ray movies of implosions to see what was going on in there. reply philwelch 13 hours agoparentprevWikipedia claims that Von Neumann and Klaus Fuchs also had a preliminary design for a hydrogen bomb that also used the implosion bomb as a first stage, though in a different way than the Teller-Ulam design. Fuchs leaked the Neumann-Fuchs design to the Soviets along with everything else, but just like the US, the Soviets set it aside and independently came up with the Teller-Ulam design instead. So it would seem that, despite Teller and Ulam winning out, Von Neumann was still deeply involved in hydrogen bomb development beyond simply developing the prerequisite plutonium implosion bomb. reply barelyauser 9 hours agoparentprevThis is an absurd take, to say the least. It offends the intelligence of virtually everyone involved in the bomb development. Plutonium usefulness would be noticed eventually. Implosion of fusion fuel using X-Rays thermal transport would have been discovered as well. The world does not have a single point of failure based on a single person, like you paint it. reply silverquiet 9 hours agorootparentAbsurd isn't really the word I would use. I think it's more of a naive/common misconception about how science advances. People tend to think of a single genius making breakthroughs when in fact a lot of ideas were building on prior work and in collaboration with others; essentially the ideas eventually have their time, and you get a sense that the specific people who discovered them, while impressive, are a bit of a historical detail rather than an essential ingredient. To me, the biggest example would be evolution - Darwin was famous for it, but you will find others speculating about it before him, and famously he sat on his work and only published it once Alfred Russel Wallace was about to independently publish his discovery of the same phenomena \"Connections\" by James Burke is a wonderful documentary that first helped me to really understand this. reply credit_guy 8 hours agorootparentprevSlow down a bit. First: it took many years for the Ulam-Teller design to be discovered. Teller came up with the idea of a fusion bomb in 1942. The actual Ulam-Teller design was invented in March 1951, basically 9 years later. In this 9 years, for at least 2 years, people were searching frantically for a workable design. It's very easy to say, after the fact, that \"X-Rays thermal transport would have been discovered as well\" because we know this is what worked in the end. But before the fact, nobody was looking for X-Ray implosion. Second: During WW2, plutonium was considered superior to uranium because it was cheaper to manufacture. U-235 was being manufactured via a very expensive separation process. But U-233 can be bred in a thorium reactor just like Plutonium is bred in a uranium reactor. They both come with their challenges (U-232 for U-233, Pu-240 for Pu-239), but in a scenario where the Manhattan project did not figure the implosion design in a hurry, the US would have shifted the resources to U-233. Here's a quote from wikipedia [1] > A declassified 1966 memo from the US nuclear program stated that uranium-233 has been shown to be highly satisfactory as a weapons material, though it was only superior to plutonium in rare circumstances. It was claimed that if the existing weapons were based on uranium-233 instead of plutonium-239, Livermore would not be interested in switching to plutonium. Now, I have no doubts that fission boosting would have been discovered. But with fission boosting, bombs would have gotten to the 1 MT yield, and that is plenty destructive for all war scenarios. Crucially, such a weapon did not need to use any type of implosion. In a scenario where the US does not develop the implosion knowledge because it can build a 1 MT weapon without, then it is not at all a given that someone else would have looked for an implosion-based design of a thermonuclear weapon. We would still have had the layer-cake design, but that is not a game changer, and it's not clear that the extra yield was worth the extra complexity. Now, in today's world: Most of US's nukes have yields around 100 kT [2]. They are thermonuclear but the same effect can be achieved with (boosted) fission bombs. The largest current US nuke, the B83, has a yield of 1.2 MT, but the US is looking to retire it, and replace it with B61, with a maximum yield of 400 kT. My point is that a superpower can service all its deterrence needs with nuclear weapons with yields that are achievable with boosted fission. Why did we then go and build thermonuclear monster bombs in the 50's and 60's? Because at the time the ICBM precision was limited, and you needed something with a mile-sized fireball to make up for the lack of precision. But the ICBM technology advanced at an incredible pace. If we had been 10 years late coming with the design for a thermonuclear bomb, it would not have been needed at all. Yet another thing is this: the destructive power of a nuke does not grow linearly with its yield. Ten bombs with a 100 kT yield are more destructive than on single bomb with a 1 MT yield. Oppenheimer knew that, and advised the US to focus on more rather than bigger. He was not listened to, and the US build both more and bigger. But we do know that you can devastate the world with 100 kT bombs, just as much as you can devastate it with 1 MT bombs. [1] https://en.wikipedia.org/wiki/Uranium-233#Weapon_material [2] https://en.wikipedia.org/wiki/W76 reply surprisetalk 12 hours agoprevIf you’re looking for a dramatic, fictionalized biography of Von Neumann and surrounding characters, I highly recommend The MANIAC: https://bookshop.org/p/books/the-maniac-benjamin-labatut/196... reply RcouF1uZ4gsC 11 hours agoprevvon Neumann was the genius's genius. Even the people a at Los Alamos had a suspicion that von Neumann operated on a different level from themselves. From wikipedia: Nobel Laureate Hans Bethe said \"I have sometimes wondered whether a brain like von Neumann's does not indicate a species superior to that of man\".[29] Edward Teller observed \"von Neumann would carry on a conversation with my 3-year-old son, and the two of them would talk as equals, and I sometimes wondered if he used the same principle when he talked to the rest of us.\" reply Jach 7 hours agoparentRelayed by Nick Metropolis: Fermi and von Neumann overlapped. They collaborated on problems of Taylor instabilities and they wrote a report. When Fermi went back to Chicago after that work he called in his very close collaborator, namely Herbert Anderson, a young Ph.D. student at Columbia, a collaboration that began from Fermi's very first days at Columbia and lasted up until the very last moment. Herb was an experimental physicist. (If you want to know about Fermi in great detail, you would do well to interview Herbert Anderson.) But, at any rate, when Fermi got back he called in Herb Anderson to his office and he said, \"You know, Herb, how much faster I am in thinking than you are. That is how much faster von Neumann is compared to me.\" That and a few other bits in http://infoproc.blogspot.com/2012/03/differences-are-enormou... reply zero_k 9 hours agoparentprevI am 100% with you. I have infinite admiration for von Neumann's talents. He was the genius of the geniuses. I wish I could have a 10 minute chat with him. He'd blow my mind, I'm sure. I'd love to explain some of the research issues I'm working on to him, I have a feeling he'd nail them to the ground. His resume is absolutely insane. The dude invented merge sort and it's an effin' side-note to his work. Like, WAT. His achievements could fill the resume of 50 Nobel Laureate scientists. Plus apparently he had a CHARACTER, like a real one. Would have loved to shoot the sh&t with him. reply satori99 9 hours agorootparentThere were others in his league who have been mostly lost in history. When von Neumann first published a description of a stored-program binary computing machine in 1945, he cited only one paper -- which was co-authored by Walter Pitts; https://nautil.us/the-man-who-tried-to-redeem-the-world-with... reply pedrosorio 7 hours agorootparentprev> The dude invented merge sort and it's an effin' side-note to his work. Like, WAT. Inventing merge sort is not a particularly impressive intellectual achievement. reply dchftcs 57 minutes agorootparentIt's trivial in an era where the concept of algorithms is established but less so back in the 1940s. You really had to be at the forefront to know that sorting algorithms are meaningful things to think about. But indeed mergesort is really not even a footnote of von Neuamnn's contributions to computing, which was perhaps not even 1/3 if his career. That's how much a giant he was. reply mkl 29 minutes agorootparentPeople have done merge sort, insertion sort, etc., by hand, for a very long time, probably centuries, and re-invented by many people. I would be very surprised if the steps had not been written down earlier. Algorithms done by hand, e.g. numerical methods, have been around for centuries too, so it's not that the concept was missing. reply 48864w6ui 48 minutes agorootparentprevSorting was already a big thing in the punched card world reply 48864w6ui 49 minutes agorootparentprevEspecially because merging was how card machines sorted large decks reply reddog 8 hours agoparentprevvon Neumann also advocated for an immediate, surprise U.S. nuclear first strike on the Soviet Union: “If you say why not bomb them tomorrow, I say why not today? If you say today at 5 o’clock, I say why not one o’clock?”. According to game theory you see, it was only rational. The genius's genius. reply batushka3 43 minutes agorootparentLiving in once soviet ocupied country, I would rather taken nukes. The Great chance to erase the malignance missed. reply keiferski 6 hours agorootparentprevYes, the genius hero worship can be tiring. The man was intelligent, no doubt – but that doesn’t seem to have translated into any real world ethical understanding. A good example of why “meritocratic” systems based on raw intelligence probably don’t lead to wise outcomes. reply latency-guy2 6 hours agorootparent> but that doesn’t seem to have translated into any real world ethical understanding. Why? Ethics gets muddy, inconsistent, and nonsensical quite quickly and I am not so certain you have figured it all out compared to anyone else, so you'll find it hard for me to believe this position. von Neumann may have had a certain game theory understanding of the world at the time, and you might as well in the opposite direction today, which one is correct is still up in the air and you won't know, and maybe in fact, you will never know. von Neumann's fear was that nuclear war was inevitable and that the entire world would die from it should the communist parties of the world, especially the one that ran in the Soviet Union were maintained. I don't think he was far off given a few years after his death the Cuban missle crisis. Then the decades after with the US and Soviet Union clashing in various capacities and interfering respectively reducing each other's influence, or failing to do so. And today with the Soviet Union's successor. reply tuyiown 2 hours agorootparent> von Neumann's fear I'm glad we don't rely on a single man's fears, whatever are his genius and rationalizations. reply keiferski 5 hours agorootparentprevI didn’t claim to be have more ethical understanding than he did, but frankly, if his reported positions on nuclear strikes were actually what he thought, then yes, I think I do. You may think ethics is muddy and nonsensical, but guess what? So is reality. That’s the point. Reality is nuanced and more complex than a single human mind can comprehend. Extremely intelligent people (particularly at math and other abstract fields) often seem to not understand this, preferring to believe that their variation of game theory is accurate. All I can say is that I’m extremely glad that Von Neumann was in no position to launch weapons. Analysis has its limits. reply latency-guy2 5 hours agorootparent> I didn’t claim to be have more ethical understanding than he did You just made the same value claim against his position, twice. Your claim that you made no claim is a lie. > All I can say is that I’m extremely glad that Von Neumann was in no position to launch weapons. Analysis has its limits. Neumann was a part of the same council that launched nuclear weapons, twice. reply keiferski 5 hours agorootparentNo, I made the claim that Von Neumann lacked ethical wisdom, if his reported positions are accurate. I didn’t say I had more ethical understanding. One can point out the flaws in something without involving oneself. This is…not complicated? This is exactly the sort of criticism that arises from hero worship. “Who are you to criticize the genius?” And no, a council did not launch nuclear weapons. They advised the president, who made the final decision. reply viciousvoxel 7 hours agorootparentprevIt's a bit funny to imagine him as so sophomoric that he really thought this was the optimal game-theoretic strategy, which is true only in the most simplistic scenario. reply jandrese 5 hours agorootparentReading that passage I’m not sure if he wasn’t just pointing out a danger of game theory simplifications leading people to absurd conclusions. reply skrebbel 3 hours agorootparentIt’s well established that von Neumann wanted to kill millions of people (in the USSR), and would’ve done so had he been given the chance. That quote is not an isolated case. He was a genius with no respect for human life, he was so blindly opposed to totalitarianism of all kinds that he thought no amount of collateral damage would be too much. reply EVa5I7bHFq9mnYK 1 hour agorootparentBy the time of him writing this, totalitarianism killed 50 million people. After he wrote this, totalitarianism killed tens of millions more. Right now totalitarianism kills hundreds of thousands and is very likely on path to kill millions. So looks like he was on point with his math, again. reply TMWNN 9 hours agoparentprevUnsurprisingly, von Neumann was one of The Martians.reply ChrisArchitect 13 hours agoprev(2020) Some discussion then: https://news.ycombinator.com/item?id=28673376 reply dang 9 hours agoparentThanks! Macroexpanded: What John von Neumann Did at Los Alamos - https://news.ycombinator.com/item?id=28673376 - Sept 2021 (152 comments) reply 082349872349872 13 hours agoprevsee also https://en.wikipedia.org/wiki/Klára_Dán_von_Neumann Lagniappe: http://anappendage.blogspot.com/2010/02/autofac-by-phillip-k... reply CamperBob2 7 hours agoparentShe died in 1963 when she drove from her home in La Jolla to the beach and walked into the surf and drowned. Well, that got dark towards the end. reply 082349872349872 3 hours agorootparent10 Nov 1963 (1 day before Armistice Day) to be specific. Maybe it was something personal, but (as someone who spent their youth worrying that the Big One might arrive anytime in the next 5-30 minutes, depending upon flight time) I see a lot of geopolitics going on around that date. Now I was fortunate: the US was already out of Vietnam before I was draftable, and I emigrated early in this century, before it was apparent that Afghanistan and Iraq would become slogs, so my life has had a lot of peace, with only minor engagements[0] of either of my countries. Compare KDvN: born in 1911, WWI+Kun 1914-1919, Hitler+WWII 1933-1945, Cold War 1947-, Korea 1950-1953, US involvement in Vietnam 1961-, plus Bay of Pigs 1961, Cuban Missile Crisis 1962, and chaos in Vietnam mid 1963[1]; worked on at least the A-bomb[1,5] and therefore knew what the H-bomb was capable of, but had no empirical evidence that MAD[2] would turn out to be effective, as that acronym was only coined in 1962 and doctrine announced sometime after. If I try to take her priors (as someone who had already started fleeing communism at 8, and had had, it seems, a life full of hope only between the ages of 9 and 22?) I wouldn't have put a lot of weight on peaceful outcomes then either. (then again, Stefan Zweig also committed suicide[3], so I'm not sure how much of a culture shock the New World was to former Austro-hungarians in general) [0] Lebanon, Grenada, Tanker War, Panama, Gulf I, Somalia, Bosnia&Croatia, Haiti, Kosovo (+ Libya raid & misc. no-fly's) [1] https://en.wikipedia.org/wiki/Vietnam_War#Kennedy's_escalati... ; she fortunately missed Kennedy getting cancelled by 11 days. [1,5] EDIT: JvN died young from cancer, which, as an atomic scientist, leads to obvious suspicions. [2] https://en.wikipedia.org/wiki/Mutual_assured_destruction [3] having left The World of Yesterday as a book-length note? reply tyre 9 hours agoprevMinor side point, did anyone else find it odd that the author almost exclusively referred to the subject as \"Johnny\"? reply mycologos 8 hours agoparentIIRC that's the name that friends and colleagues used to refer to him. It pops up in a lot of their quotes. reply mkl 27 minutes agorootparentMany of the quotes are from https://web.archive.org/web/20200626032839/https://www.youtu..., where they do say Johnny a lot. reply unsigner 5 hours agoprev [5 more] [flagged] atoav 3 hours agoparentAs someone from a neutral country I guess then you just would have been the (even more?) evil ones in the eyes of history. Not to defend Stalin or anything, that man wasn't to be trusted, which was part of the reason why many other nations had their problems with them — but can we agree that: 1.) Most nations (or at least their populations) perceive themselves as the good guys 2.) If you take actions for which you would call other nations out, but it is a \"necessary evil\" when you or your allies do it, you might not be the good guys 3.) Independent from morality we can discuss the geostrategic value of everything, but if your nation is mainly acting based on geostrategic interest it has no grounds of claiming the moral high ground reply awestroke 3 hours agoparentprevI would not want to live in a world where nobody opposes the US reply skrebbel 3 hours agoparentprevI’ll never understand this sentiment. You’re actually for killing all the Russians? Everything the Nazis did pales in comparison, how can anyone be this evil? I don’t understand it about von Neumann and I don’t understand it about comments like these. reply archagon 3 hours agoparentprev [–] You're OK with tens of millions murdered civilians? Shame on you. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "John von Neumann, a remarkable mathematician and physicist, made significant contributions to various fields, including economics, computer science, and nuclear weapons development.",
      "His work at Los Alamos during the Manhattan Project greatly influenced technology and defense strategies in the Cold War era.",
      "Von Neumann's expertise in shock waves, implosion, and computing technology left a lasting impact, contributing to advancements in artificial intelligence and general-purpose computers."
    ],
    "commentSummary": [
      "The discussion on 3quarksdaily.com centers on the impact and brilliance of John von Neumann at Los Alamos, covering his math skills, contributions to nuclear bomb development, and ethical implications of his work.",
      "Topics explored include verifying scientific research, the link between fascism and intellectual detachment, and the significance of teaching critical thinking in education.",
      "The debate also examines the balance between innate talent and hard work in excelling in mathematics, the influence of figures like Ramanujan and Witten, as well as the history of nuclear bomb development in WWII, and Von Neumann's personal life and broader ethical and geopolitical themes."
    ],
    "points": 195,
    "commentCount": 105,
    "retryCount": 0,
    "time": 1712508124
  },
  {
    "id": 39960069,
    "title": "3M settles multi-billion dollar lawsuit over PFAS in drinking water",
    "originLink": "https://www.cbsnews.com/minnesota/news/3m-pfas-drinking-water-settlement/",
    "originBody": "Local News Court approves 3M multi-billion dollar settlement over PFAS in public drinking water systems By WCCO Staff, The Associated Press Updated on: April 1, 2024 / 10:48 PM CDT / CBS/AP Report finds some water filters can reduce PFAS from tap water Report finds some water filters can reduce PFAS from tap water 01:54 MINNEAPOLIS — Minnesota-based chemical manufacturer 3M will begin payments this summer to many U.S. public drinking water systems as part of a multi-billion-dollar settlement over contamination with potentially dangerous chemicals, the company said. Communities in the east metro are especially impacted by the contamination. 3M announced Monday that last year's lawsuit settlement received final approval from the U.S. District Court in Charleston, South Carolina. The agreement called for payouts through 2036. Depending on what additional contamination is found, the amount paid out will range from $10.5 billion to $12.5 billion. RELATED: Despite historic 3M PFAS payout, Minnesota communities need millions more for cleanup \"This is yet another important step forward for 3M as we continue to deliver on our priorities. The final approval of this settlement and continued progress toward exiting all PFAS manufacturing by the end of 2025 will further our efforts to reduce risk and uncertainty as we move forward,\" 3M's chairman and CEO, Mike Roman, said in a news release. Six years ago, 3M settled with the state for $850 million for disposing the chemicals and contaminating drinking water and our environment — one of the largest settlements of its kind in the country. The deal compensates water providers for pollution with per- and polyfluorinated substances, known collectively as PFAS — a broad class of chemicals used in nonstick, water- and grease-resistant products such as clothing and cookware. PFAS have been described as \"forever chemicals\" because they don't degrade naturally in the environment. They've been linked to a variety of health problems, including liver and immune-system damage and some cancers. The compounds have been detected at varying levels in drinking water nationwide. The Environmental Protection Agency in March 2023 proposed strict limits on two common types, PFOA and PFOS, and said it wanted to regulate four others. Water providers would be responsible for monitoring their systems for the chemicals. RELATED: PFAS in Minnesota: How \"forever chemicals\" changed the state of water A new state law passed last year will ban PFAS in some consumer products starting in 2025 with a full ban in 2032. But prevention is only one part of the solution. Some communities like Woodbury and St. Louis County also want lawmakers to approve funding for PFAS mitigation in their infrastructure package this year. The 3M settlement first announced in June came in a lawsuit by Stuart, Florida, one of about 300 communities that had filed similar suits against companies that produced firefighting foam or the PFAS it contained. The payment will help cover the costs of filtering PFAS from systems. Some of the settlement money will help additional water systems test for contamination from PFAS, said Scott Summy, one of the lead attorneys for those suing 3M and other manufacturers. They have until June 2026 to apply for compensation if contamination is found. \"That's great news for American citizens who drink from that water,\" Summy said. \"It'll help rid our public drinking water systems of PFAS, and that's the most important thing about the settlement.\" 3M pledged in late 2022 that the company would stop manufacturing and using PFAS by the end of 2025. More from CBS News Drought conditions cause lower than usual water levels, DNR warns boaters Elderly customers, those with disabilities worry about Uber, Lyft leaving Minneapolis as companies attempt to fill the void St. Paul school district proposes major budget cuts next year, including staff reductions and food menu changes Wisconsin's bar association agrees to change diversity definition in settlement In: Health Charleston Lawsuit WCCO Staff The WCCO Staff is a group of experienced journalists who bring you the content on WCCO.com. Twitter Facebook Instagram First published on April 1, 2024 / 5:28 PM CDT © 2024 CBS Interactive Inc. All Rights Reserved. This material may not be published, broadcast, rewritten, or redistributed. The Associated Press contributed to this report.",
    "commentLink": "https://news.ycombinator.com/item?id=39960069",
    "commentBody": "Court approves 3M multi-billion dollar settlement over PFAS in drinking water (cbsnews.com)191 points by geox 22 hours agohidepastfavorite211 comments BytesAndGears 21 hours agoI’m more worried about the fact that it was allowed for so long. Sure, we’re doing the right thing by quitting PFAS and starting major cleanup efforts. But the system existed where people were incredibly negligent for years, knew they were hurting people, made boatloads of money, and retired. That same system exists in many oil-and-gas industries, drug marketing, and more. There are dozens of examples where this has happened in recent years, and that’s just the ones at large enough scale to be prosecuted. It’s still happening to this day. How do we start treating intentional negligence that leads to mass suffering and death as seriously as murder? It is individuals making these decisions, and they need to be held personally liable. The corporate veil is useful, and shouldn’t go away completely. But when an individual makes a decision that had a high probability of doing major harm, they should at least stop and raise the issue to a government agency, to limit their liability. If they do not take appropriate precautions, like getting explicit approval from government authorities, then they should be personally liable for the negligence. reply Buttons840 21 hours agoparent> But the system existed where people were incredibly negligent for years, knew they were hurting people, made boatloads of money, and retired.... It is individuals making these decisions, and they need to be held personally liable. Maybe people who have more than, say, 20 million dollars should be personally liable. This seems like a very conservative middle ground, because, as you say, these people have sometimes knowingly made decisions they knew would result in deaths. Taking away only some of their money is the least of punishments, and they would still be left rich and very comfortable. This should apply to almost anyone. I get that regular people deserve some protection from liability for just doing their job, but at some point society is paying you so much money that you owe us a little personal responsibility in return. Maybe it could be structured such that when a company is sued and loses, the company can then sue past employees who made the wrong decisions. Again, this liability would not be able to reduce an individual's wealth to less than 20 million, so employees don't have to worry about being completely ruined, but people shouldn't be able to control society warping amounts of wealth when the source of that wealth was incorrect and possibly corrupt decisions. Let's try including some responsibility with the merit. reply ivan_gammel 20 hours agorootparentCorporate negligence that results in significant damage to environment or public health must be criminalized with most severe punishment going to higher level decision makers, including some accountability for their superiors. If CEO knew and acted against public interests, then Board must be held accountable. If board knew and acted against public interests, then majority shareholders who voted for this board must be at least fined. reply Eddy_Viscosity2 20 hours agorootparentNot just that the CEO or board \"knew\" because that is a hard thing to prove. Better to have it be \"reasonably should have known\". If this is a thing that the company is doing and has been doing for a long time then the senior managers should know about it and are therefore liable and responsible. Such a rule would also incentivize managers from 'looking the other way' at wrong-doing because the saying 'I didn't know' won't matter. reply tomp 18 hours agorootparentThis is how it already works in e.g. foreign bribery for UK and US companies. Executives just “letting” it happen is a crime, they have to explicitly ensure employees are educated and told not to do it. Source: I worked in finance where we had tons of such useless courses. reply edkennedy 18 hours agorootparentI agree with the sentiment here, education is needed and finance / bribery is a good example. At a certain point though, you have to wonder what is wrong with society where \"Don't kill a bunch of people through negligence\" becomes something that needs to be educated. reply mistrial9 17 hours agorootparentperhaps it relates to a concept of LD50 in chemical testing.. that is, run tests and show a distribution of content and application, that results in \"greater than fifty percent\" fatality or \"less than fifty percent\" overall mortality.. continue to evaluate through the entire market system.. No action or formulation is one hundred percent pure and beneficial, though many actions or formulations are certainly one hundred percent fatal, now or over time. This is true in products created by companies, and markets push towards maximum profitability, not maximum safety. reply Drakim 20 hours agorootparentprevThis is very important. Each time there is a leak of a bunch of personal data companies whip out the \"we have not found any evidence of the leak being ours\" or something to that tune, which incentivizes them not look for evidence. reply RobotToaster 20 hours agorootparentprevIt often feels like the only requirement to be an executive at a large corporation is the ability to create plausible deniability via willful ignorance. reply ryandrake 19 hours agorootparentCEO on stage at a trade show: \"Look at all these great things my company has done! Let me share with you all the details!\" Same CEO during a deposition: \"I have no idea what my company even does! Do we make soup or something?\" reply username332211 20 hours agorootparentprevIronically enough, the government has been moving in the opposite direction - No fault compensation schemes like NCVIA, anti-lawsuits bills like PLCAA, limits on torts. I do wonder why? Must be Big Business (in the NCVIA case, Big Pharma), right? reply forgetfreeman 18 hours agorootparenthttps://en.wikipedia.org/wiki/Regulatory_capture reply rapjr9 12 hours agorootparentAh, so the regulators should share in the punishment when wrong doing is found, not just the CEO's. reply forgetfreeman 8 hours agorootparentDon't threaten me with a good time. reply ljm 19 hours agorootparentprevThe buck can and should stop with the executive level anyway. They are paid extremely handsomely, in huge disproportion to the rest of the staff at their company, in part because fucking up at this level means you won’t see the C level again for a long time. It’s the ultimate accountability in the organization. The accountability piece rarely happens now, unless you happen to defraud other investors, yet the compensation for the job continues to grow wildly out of proportion. reply sofixa 16 hours agorootparentHard disagree. High level executives are ultimately responsible for everything that happens, and must suffer the consequences - I really want to see C-levels regularly being sent to jail for the crimes comitted by the companies they led - but the low-level employees that actually performed the dangerous/illegal/immoral acts aren't magically innocent because someone higher up ordered them so (\"just following orders\" is not a valid defence). Example at Boeing: the CEO and all related high level execs that fostered a culture of bean counting and cutting corners to save costs are guilty and liable for deaths. The actual people that designed a deadly system, those who approved it; similarly, those that forgot to put in bolts holding critical stuff together... they are guilty too. They knowingly endangered human lives. reply ljm 1 hour agorootparentI think we both agree, you just went into more detail on it. The executive level shouldn’t be able to defer all of the accountability to employees lower down the ladder (e.g blaming it on an intern or whatever usual CYA excuse is rolled out). It’s one thing for an employee to go rogue, but totally another thing for leadership to either brush it under the rug or to create the environment for such behaviour in the first place. reply tomxor 17 hours agorootparentprev> Maybe people who have more than, say, 20 million dollars should be personally liable. This seems like a very conservative middle ground, because, as you say, these people have sometimes knowingly made decisions they knew would result in deaths. Taking away only some of their money is the least of punishments, and they would still be left rich and very comfortable. Monetary punishment is not enough, and in fact I would say any kind of punishment is not useful enough. We need to switch our thinking to focus on preventative measures. In this case all of the money ever made from PFAS will never be enough to clean up the pollution or care for all of the human health side effects. You could make all of the employees and execs very uncomfortable, claw back every single penny ever made and it still wont be enough by a long shot. In some cases It's easy to do a huge amount of damage for (relatively) far less personal gain. Some of the world's regulatory systems already at least attempt to work from a preventative perspective such as drug approval processes for humans. As more of society wises up to the fact that the environment is an extension of us, it shouldn't be that hard to convince people to take preventative strategies towards caring for it. reply rapjr9 11 hours agorootparentI agree, and prevention can't be a one time thing. In the 1930's if you were against using coal to produce electricity you would have been labeled crazy. Seems like what is needed is constant re-assessment to determine if new harms have been noted, along with ongoing research and testing to look for toxins, environmental effects, etc. As you state, preventive strategies. For example, ConsumerLab.com is testing herbal supplements, not just once, but regularly for lead, potentcy and more. This is something that should have already been ongoing for decades, not just started as a business in the last few years. Another example is national security as it relates to imported products. If we're not testing imported foods and other products (kids toys) reliably for lead and other poisons then a hostile country could poison your population before you know about it. Impaired people = impaired soldiers. Managing this in a cost effective way is difficult, but it seems possible. reply tomxor 10 hours agorootparentReassessment is needed for sure. To classify the other strategies you are describing I think I would call them \"increased focus\", which would be a positive move for sure. But the draw back of this approach is it's more of the same, it's reactive, it has to chase industry, and it's effectiveness is dependent on available resources not being outpaced by industry (unlikely). An alternative regulatory system that doesn't need to chase industry would require industry to come to them by law when for example they want to release a new class of chemical into the environment. For all of it's flaws, I'm basically talking about something like the FDA, but hopefully less corrupt. reply thfuran 17 hours agorootparentprev>Again, this liability would not be able to reduce an individual's wealth to less than 20 million $20,000 I might accept, but that's ridiculous. Edit: Really, I don't think any kind of limit is a good idea. We already have bankruptcy law and different sorts of fines or penalties being or not being dischargeable under forms of bankruptcy. Sticking to that framework seems best. reply Buttons840 16 hours agorootparentAre you a programmer? Would you accept having most of your money, retirement, possessions, and wealth taken away if it is discovered you made a mistake and there's a serious big in your code? I'm not talking about just criminal negligence, I'm taking about honest mistakes too. If you want to be ultra rich, more than 20 million, then you have to be competent enough to not make big mistakes. For everyone else, they get to keep their retirement, etc. That's why I said 20 million. And I'm thinking about more than punishment. It's about compensation. A CEO should be compensated for making good long term decisions, and if it turns out they have not make good decisions, they should retroactively lose compensation, but not be left in ruin over honest mistakes. As for criminal cases, apply criminal penalties, which are beyond what I was suggesting. reply notjulianjaynes 15 hours agorootparentMy issue with your argument is in what world is a net worth of $20 million not \"ultra rich.\" The median net worth of an individual near retirement age is $400,000. Under 35 it's a tenth of that. I don't really have an opinion about statutory fines for executives in situations like this, but the idea that reducing someone's net worth to a measly 50x that of what most people have is somehow punitive is one of the most ridiculous things I have ever heard. reply Buttons840 15 hours agorootparentIt's an arbitrary number, but I believe it should be fairly high in this idea I propose. (And this is all pie in the sky theory crafting anyway.) My goal is not punishment as much as it is compensation. I want a world where if someone has a billion dollars, it's because they made good decisions and have proven to be good in the long term. Currently that is not the case, if someone has a billion dollars it might be because they made greedy short term decisions and then bailed and deployed their golden parachute and left others to clean up their mess. I also think it's about liability. I like not being personally liable for mistakes I make a work, but I don't get paid millions of dollars either. I think once enough money is involved people should no longer be able to disconnect financial liability from their personal money. I also proposed it as a conservative high number, because if this ever were to happen, you know talking heads would be screaming about it, but regular people are going to scoff when they learn the only threat is to take away wealth in excess of 20 million. reply thfuran 16 hours agorootparentprev>I'm not talking about just criminal negligence But that was the topic of the discussion. Actually, the context was gross negligence rather then mere negligence, let alone just ordinary mistakes. reply erickj 16 hours agorootparentYes, but a once society crossed that boundary then it will be on you and your lawyer to prove that it wasn't criminal negligence, just the regular old kind reply twoodfin 14 hours agorootparentprevCongratulations: All your products are now made with chemicals designed and manufactured in China. reply asadotzler 13 hours agorootparentNot really. Sovereign countries can block 100% of China imports any time they want. That your lack understanding of trade policy is no excuse for such a reductionist claim. One can also imagine that people might personally refuse to import deadly products or products that harm their children's future environment. There are plenty of reasons this might not just be the race to the bottom you imagine and profess. Do the harder work next time instead of wasting everyone's attention on such worthless replies. reply twoodfin 13 hours agorootparentThe government can also ban chemicals at any time, for import or domestic production. The issue is they didn’t, at the time, but 3M or similar was supposed to have “known better”. Massive retrospective penalties for “should have known better” non-crimes will just result in an off-shoring to markets where these penalties do not apply. reply alistairSH 19 hours agorootparentprev…this liability would not be able to reduce an individual's wealth to less than 20 million… Why? $20 million is a LOT of money. We shouldn’t allow greedy sociopaths to accumulate even that much wealth at the expense of the rest of us. The Sackler family is a good example. Why should they walk away with even $20 million (let alone the $11 billion they kept after negotiating a $4.5 billion settlement). It’s crazy. reply TreetopPlace 19 hours agorootparentYou think that if someone makes $20 million, then the money has been taken away from someone else? I see this sentiment on TikTok and Reddit alot, and it makes no sense. reply alistairSH 15 hours agorootparentWe’re talking about people who, via a corporate shield, are polluting, or peddling drugs, or whatever else. The wealth they’re accumulating is at the expense of society at large. If you want to make a billion dollars, fine, just do it without fucking over the rest of us. reply Hikikomori 17 hours agorootparentprevWhere did it come from if not someone else? reply erickj 16 hours agorootparentThe economy is not a zero sum game (Not defending the Sacklers, fuck that family) reply forgetfreeman 18 hours agorootparentprevIf it doesn't make sense then you apparently haven't come to terms with the fact that capitalism is a zero sum endeavor. We're getting off topic though quibbling over where to plant the goalposts. reply rayiner 17 hours agorootparent> If it doesn't make sense then you apparently haven't come to terms with the fact that capitalism is a zero sum endeavor How ignorant do you have to be to still say that in 2024? We literally spent the last century conducting worldwide social experiments to show exactly the opposite. At this point objecting to capitalism itself (I’m not talking more or less regulation within a capitalist system) is like being a flat-earther. Leftist westerners loved my home country—as an object of their virtuous pity—when it was socialist. You’d see commercials begging for $1/day to feed people. Thanks to capitalism the country has changed completely in the last two decades. Vietnam and China both abandoned communism for capitalism and their prosperity soared. reply hackable_sand 11 hours agorootparentTo be pedantic, the argument is that violence is necessary to maintain the fiction of property. The violence is abstracted now that capitalism has matured so we only see it as numbers but death is most definitely zero-sum. reply rayiner 9 hours agorootparentThat’s a non-sequitur, because violence is required to maintain any kind of society. reply forgetfreeman 8 hours agorootparentprev\"At this point objecting to capitalism itself (I’m not talking more or less regulation within a capitalist system) is like being a flat-earther.\" You're welcome to that delusion but you're cheerleading a system that among it's many flaws is predicated on continuous growth that's based on a closed system with finite resources to draw from. Three guesses how that ends. reply jnordwick 7 hours agorootparentlol. people have been predicting the end to natural resources like oil, gas, minerals, and ores for over 100 years now, but they keep getting cheaper showing that they are more abundant now than ever. go read about the Simon and Ehrlich wager where Ehrlich just got humiliated. reply forgetfreeman 6 hours agorootparentNow who's ignorant? You're confusing the fruits of the combination of technological advancements in extraction and exploitation of the 3rd world with endless abundance. 150 years ago oil wells were routinely dug with nothing more complicated than a pick axe and some dynamite. You think companies invested in the complexity and cost of off shore drilling (as an example) in spite of simpler more cost effective methods being available? Pull your head out. reply orangecat 17 hours agorootparentprevyou apparently haven't come to terms with the fact that capitalism is a zero sum endeavor This is a ridiculous claim. Do we have the same amount of total wealth as we did a hundred or a thousand years ago, and the only difference is how it's distributed? reply int_19h 15 hours agorootparentIt is still a zero-sum endeavor because all this new wealth didn't materialize out of thin air. It was created by people, and specifically by people doing productive work with their hands and their brains - which does not describe the suits on top, no matter how much they try to convince us otherwise. The reason why the latter end up with most of this newly produced wealth in their pockets is simply because they have access to a wide assortment of mechanisms to collect economic rent from the people who actually produce - this is the zero-sum part. The notion that, without the suits, the wealth wouldn't have been produced at all is nonsense. reply forgetfreeman 8 hours agorootparentprevThis is a ridiculous response. Have we extracted more natural resources than we had a hundred or a thousand years ago? Are the biomes that our species utilize for foodstuffs more or less healthy than they were a century ago? reply wyre 14 hours agorootparentprevSounds like you haven’t came to terms with it being zero sum either. Zero-sum: adj, of, relating to, or being a situation (such as a game or relationship) in which a gain for one side entails a corresponding loss for the other side. GDP can still increase, but doesn’t disprove capitalism isn’t zero-sum. If you haven’t been paying attention we are currently living in a period of the most wealth inequality in recent history. reply jnordwick 7 hours agorootparent> currently living in a period of the most wealth inequality in recent history. Th feudal period would like a word with you. reply forgetfreeman 1 hour agorootparentDid the conversation slip into geologic timescales or something? Unless you're advancing the argument that modern day corporate structures bear some kind of significant relationship to feudalism the phrase was recent history. I'm fairly certain reasonable people wouldn't classify, for example, the War of the Roses as recent events. reply lttlrck 17 hours agorootparentprevThat does seem like much of a deterrent to be honest. reply lostlogin 16 hours agoparentprev> they should at least stop and raise the issue to a government agency, to limit their liability. Many countries have a revolving door between industry and government oversight. New Zealand consistently scores well, having low corruption. Currently we have a government that has a very interesting relationship with the tobacco industry, with freight/roading, gambling and with horse and dog racing. I’m sure there are many more. reply calibas 18 hours agoparentprevWhen the food industry intentionally sabotages health and nutrition science for their own gain, they should be liable. Their actions are responsible for the early deaths and sicknesses of millions of people. https://www.npr.org/sections/thetwo-way/2016/09/13/493739074... reply Y_Y 21 hours agoparentprevIs there a told-you-so list somewhere that catalogues these ongoing negligences? I'd love to see a comprehensive of the things that are known now, but not well-known yet. reply sokoloff 12 hours agorootparentI suspect we're going to find plenty of food items to put on this list. Are we planning to jail or bankrupt the peddlers of sugar water? Sugar itself? Refined flour? Non-nutritive sweeteners? Fast food? Fast casual? Buffets? Potato chips? Chocolate? Beer? Wine? (Stop me when we get to some pseudo-vice that you sometimes enjoy.) At some point, I think you have to say \"society decided at the time that selling Coke, Pepsi, fast food, potato chips, and wine was okay, and the people who sold that aren't subject to retroactive punishments for selling something that we pretty much knew was damaging to health of millions of people, but they chose to eat it anyway...\" reply dec0dedab0de 19 hours agoparentprev* How do we start treating intentional negligence that leads to mass suffering and death as seriously as murder?* I think a combustion of very public corporal and capital punishment is the only answer here. reply dec0dedab0de 16 hours agorootparentcombination* reply sofixa 16 hours agorootparentNo, no, combustion works very well too. If Dave Calhoun legitimately feared he'd be set on fire in public, he'd probably actually pay attention to the shit he's creating instead of being sure that worst case scenario he'd retire with a few tens of millions of USD more. reply londons_explore 16 hours agoparentprevI think the key thing is people doing things that aren't yet illegal, but probably should be. I think we should simply backdate fines and punishments in this case. Ie. if you were doing something harmful to people or the environment but it was technically legal to do at the time you did it, you should still get punished (although a smaller punishment, especially if you can reasonably argue you didn't know). reply pirate787 16 hours agorootparentThis is dystopian. The better approach is the \"precautionary principle\" which requires owners of new tech or chemicals prove they are safe before use reply YawningAngel 16 hours agorootparentprevIn the US, this is constitutionally forbidden. reply londons_explore 16 hours agorootparentYet harming people is still illegal. So the deal could be \"We're gonna fine you $$$ instead of prosecuting you for Poisoning. Penal Code 347(a)(1).\" reply fallingknife 16 hours agorootparentprevAnd after we implement that we can start arresting people for precrime! reply mattmaroon 19 hours agoparentprevThere is not strong evidence that the levels of PFAs most people are exposed to cause any health issues at all. It is, of course, concerning. Nobody likes the idea of materials building up in their bodies. But we can't say they knew they were hurting many people, because we still don't. reply akaru 19 hours agorootparentYet a drug—that people willingly and optionally take as opposed to this—must be proven NOT to create harm before it is allowed in the wild. Your argument sounds like it is in bad faith. reply mattmaroon 19 hours agorootparentYou are conflating very different things. Go into any Whole Foods and there is an entire aisle of pills that are not proven to not create harm, in fact, some of them most certainly do. Pharmaceuticals play by a different set of rules than other chemicals because humans are particularly gullible when it comes to health claims. \"Snake oil salesman\" is a common term for a reason. We don't let big pharma make certain claims without ample test data for a whole host of obvious reasons. In this context we were talking about punishing people for \"harming\" without having proven that they've really harmed at all. This would be more like locking up all the Chinese restaurant owners for using MSG. Your argument sounds like it is in bad logic. reply zug_zug 19 hours agorootparentThat really feels like a bad-faith and misinformed comparison. Can you please at least do a very basic amount of research before you take sides on this? Perhaps watch a documentary (e.g. the devil we know) or read at least 1 wikipedia article -- it was clear from the outset that workers at these factories were giving birth to children with birth-defects for example. [1] These companies had engineers who were actively concerned about its health effects, circulating internal memos to that effect [1], but just because there wasn't a finalized published study are able to claim \"well.... it's not proooooven technically for 100% sure to be bad for you in small doses...\" Of course small doses of anything can take a decade to be proven harmful to newborn children. These are now being linked to ADHD, that's something that takes years of very intentional research to find (because how old does a kid have to be to get that diagnosis). https://www.ucsf.edu/news/2023/05/425451/makers-pfas-forever... reply mattmaroon 18 hours agorootparentDocumentaries are not research. I have read countless actual papers on this. You can find them online. I am not taking the side you think I am, you simply are reading things into what I said that aren’t there. I am sure 3M did awful things. I am sure high levels of the stuff are dangerous, certain occupations were exposed to it at dangerous levels, the company willingly ignored that. I’m sure they deserve this fine. Fines of this magnitude hit corporations where they live in the way that jailing a COO cannot. I just think “mass harm” is not proven. Perhaps some simply define that differently than I do, I don’t consider the relatively tiny amounts of factory workers impacted to be mass harm. Awful, sure. But the hysteria about this, and what I think most people are talking about when they use that term, is the level all of us are exposed to via drinking water, food packaging, etc. There’s simply no proof (or even solid evidence) of mass harm by that definition, just hysteria. Proof may arrive one day, I do not know. As you point out, it may take years or even decades. (Though it has been around nearly a century, and in large quantities for some time.) The word “linked” is always common in bunk science. When you see that frequently on any topic, you can be sure there’s hysteria. I don’t think it’s reasonable to expect every chemical manufactured (they all can end up in water systems) to be clinically tested. Those chemicals are too important to modern life to yank them all. There are too many. reply zug_zug 13 hours agorootparent>> I just think “mass harm” is not proven. Well clearly it's been proven to the satisfaction of the court, whatever your \"personal research\" says. It's also been proven to an independent science council: The C8 Science Panel, which was funded by DuPont as part of a lawsuit settlement, conducted extensive epidemiological analysis on human health outcomes from exposure to PFOA, a type of PFAS. This panel, composed of independent scientists, found links to several health harms, including two types of cancer and pregnancy-related issues, after studying blood samples from nearly 70,000 residents in the mid-Ohio River Valley. This area was significantly exposed due to its proximity to a DuPont Teflon factory (EWG). More papers come out every year finding stronger links. I wonder what your horse in this race is, my friend? [1] https://www.ewg.org/news-insights/news/science-pfas-rebuttal... reply mattmaroon 13 hours agorootparentYou simply do not understand my argument and perhaps it is my failing for that. In any case, I can’t make it any clearer than I already have so I’ll stop trying. reply rayiner 17 hours agorootparentprevEvery single western society takes a different approach to drugs that people directly ingest and industrial chemicals that might build up in peoples’ bodies. The calculus seems to be that people would rather have better non-stick pans and are okay with a few more people dying from them. You might disagree with that, but OP is just stating what’s apparently common sense. That’s the exact opposite of “bad faith.” reply lazide 16 hours agorootparentWell, people buy non stick pans despite there being evidence people may die from it, so I’d say the evidence is strongly in your favor. reply mattmaroon 15 hours agorootparentMy girlfriend is simultaneously outraged that there are PFAs in the water supply and has entire sets of pans that are disintegrating Teflon that she even puts in the dishwasher. When I point out the irony she shrugs. reply JumpCrisscross 19 hours agorootparentprev> must be proven NOT to create harm before it is allowed in the wild This isn’t how we regulate chemicals in America. Perhaps it should be. reply hyperdimension 19 hours agorootparentI think that that was the implication. We should elevate the standard for new chemicals that might enter our body through the environment to a similar standard we already have for drugs (chemicals we deliberately put into our body.) reply lazide 16 hours agorootparentprevI’m not sure how that is even possible, frankly. What sort of test suite would THAT look like? reply rayiner 17 hours agorootparentprevTo my knowledge, that’s not how we regulate industrial chemicals anywhere. I agree it probably should be. Those chemists are witch doctors. reply lazide 16 hours agorootparentprevThat is definitely not the bar for drugs, or most drugs would not be allowed to be sold. The FDA requires that drugs provably and consistently have an effect, and that side effects are generally well known and not excessively bad so as to outweigh the benefits. Drugs without known side effects are the worst in the eyes of the FDA (except those with no redeeming properties whatsoever anyway), because it doesn’t allow for manageable prescribing. If they were proven to be safe, they wouldn’t require a prescription and would be sold over the counter. Though even that bar isn’t really required, or St. John’s Wort, Tylenol, etc. wouldn’t be allowed to be sold. reply mikeiz404 15 hours agorootparentprev> But we can't say they knew they were hurting many people, because we still don't. I’d highly recommend reading this article. It’s quite long but worth the read. https://highline.huffingtonpost.com/articles/en/welcome-to-b... reply cpncrunch 19 hours agorootparentprevMost of the comments here are highly disturbing, bring out the pitchforks. Its sad that this is on HN. reply mattmaroon 18 hours agorootparentMost people don’t know the difference between a Netflix documentary and research. reply rayiner 17 hours agorootparentThe problem is that the American left is full of feelers and emoters. Thats not a criticism of leftism—quite the opposite. Protecting people from harmful chemicals is too important to leave to feelers who are reacting based on Netflix documentaries. reply speedylight 18 hours agorootparentprevThat’s exactly why it’s bad, we don’t require businesses to prove X chemical/compound is safe before we start using it for whatever, no, we use it and just hope that it’s safe. reply mattmaroon 18 hours agorootparentThe problem is, there are very many chemicals, those chemicals in many ways power our modern world, and they can all end up in drinking water or ingested by some other means. If you took every chemical off of the market that didn’t go through a randomized controlled double blind study, billions of people would die. Literally. How many people are fed thanks to fertilizers and pesticides that we know would be actively harmful if you consume them in quantity? Or at least don’t know would be unharmful. It’s not practical to ban everything that we don’t know can be ingested safely. It’s just not even an intelligent idea. It’s merely a response to hysteria. reply speedylight 11 hours agorootparentI am not suggesting that we ban all chemicals not properly studied, but we can do a lot to at least get a sense of what X chemical may be capable of doing to us if ingested. I don’t think it would’ve been incredibly difficult or burdensome to think that a chemical that can with stand 900+ degrees celsius won’t be metabolized and broken down very well inside the body, the next obvious supposition would be well is the body going to be able to eliminate it in some way or will it just stay there? What happens if it stays there, that doesn’t sound very good? These are all basic questions 3M chemists could’ve asked themselves before they created this mess, or may be they did and no one paid attention. reply esafak 15 hours agorootparentprevThese chemicals were suspected to be harmful but allowed to be used anyway. This is not an innocent mistake. Same playbook as the tobacco industry. Throw the book at them, they deserve it. https://theintercept.com/2018/07/31/3m-pfas-minnesota-pfoa-p... reply forgetfreeman 18 hours agorootparentprevAbsolute nonsense. We've known since at least the 90s that this shit posed significant health risks. See Also: Parkersburg, West Virginia. reply mattmaroon 18 hours agorootparentWe can be reasonably certain that very high exposure is harmful, yes. Only a small percent of the population achieved that level of exposure, thankfully. reply forgetfreeman 11 hours agorootparentThings we know: high concentrations are catastrophically harmful, it persists in the environment indefinitely, and this shit is bio accumulative. That should be the end of any discussion about product safety. reply fingerlocks 6 hours agorootparentEverything you just said is also true about water. reply forgetfreeman 1 hour agorootparentIt's also true of lead. Care to eat a brick of it and report your findings? reply mikeiz404 15 hours agorootparentprev> There is not strong evidence that the levels of PFAs most people are exposed to cause any health issues at all. It is, of course, concerning. Nobody likes the idea of materials building up in their bodies. I mean the research is still relatively new and ongoing [1][4] where “most of the hundreds of PFAS currently in commerce have limited or no toxicity data” [5]. And in 2022 the EPA posted “…updated advisory levels, which are based on new science and consider lifetime exposure, indicate that some negative health effects may occur with concentrations of PFOA or PFOS in water that are near zero.” [2]. On top of that there are lots of PFAs which cannot be individually tested for — In 2019 the EPA “can now measure 29 chemicals”[6] out of “thousands of PFAS chemicals” [7] — which means many of those tests are only checking for a small subset of this class of chemical (though the older and more widely used ones have been prioritized for testing). Also where do you get your control group from for low level exposure? It takes time to bioaccumulate and most people are already exposed to some degree —“Nearly all people in the United States have measurable amounts of PFAS in their blood” [4] — through many different sources — “There are thousands of PFAS chemicals, and they are found in many different consumer, commercial, and industrial products. This makes it challenging to study and assess the potential human health and environmental risks.” [7]. Some other interesting links/articles: - https://www.ewg.org/interactive-maps/pfas_contamination/map/ - https://www.consumerreports.org/toxic-chemicals-substances/i... - https://www.su.se/english/news/it-s-raining-pfas-even-in-ant... - https://highline.huffingtonpost.com/articles/en/welcome-to-b... - https://cen.acs.org/environment/persistent-pollutants/US-EPA... - https://www.fda.gov/news-events/press-announcements/fda-anno... Sources: 1: https://www.atsdr.cdc.gov/pfas/health-effects/index.html 2: https://www.epa.gov/sdwa/drinking-water-health-advisories-pf... 4: https://www.atsdr.cdc.gov/pfas/resources/pfas-information-fo... 5: https://www.epa.gov/assessing-and-managing-chemicals-under-t... 6: https://www.epa.gov/newsreleases/epa-announces-new-method-te... 7: https://www.epa.gov/pfas/pfas-explained reply whall6 20 hours agoparentprevCan you explain how this is analogous to oil and gas? Not sure I am there. reply digdugdirk 18 hours agorootparentOil and gas companies knew that their product would cause large scale global climate impacts half a century ago. They suppressed that information, and then proceeded to go the extra step and create the current \"climate change isn't real\" worldview through a carefully planned and executed influence campaign. It's a remarkably effective strategy that appears all over different industries, including tech. Keep an eye out for large political battles, and wishy-washy scientific studies funded by the industry that profits off the issue at hand. reply mikeiz404 16 hours agorootparentIf someone is looking for sources, this article goes into great detail though it is extremely long but worth the read. Other publications have posted similar stories. https://highline.huffingtonpost.com/articles/en/welcome-to-b... Ground water and soil contamination is just one source of exposure. EWG has created an interactive map of the US which tracks this. https://www.ewg.org/interactive-maps/pfas_contamination/map/ reply fallingknife 16 hours agorootparentprevOther people besides oil companies knew 50 years ago too, so this is hardly relevant. Nothing would have changed if they had made this public. Hell, we all know now and still aren't doing much. This whole conspiracy theory is just cope from people who won't admit that it is us and our oil consumption that is the problem and the companies are just there to give us what we want. reply int_19h 15 hours agorootparentWe aren't doing nearly as much as we should be, but we're certainly doing something - and at time spans like 50 years, said \"something\" can compound quite significantly. Even if we've ultimately had the same discussion about climate that we ended up having today, but several decades earlier, that would already be a fairly big improvement in the current state of affairs. You also completely ignore the other, and arguably more important, part where those companies didn't just bury the research - no, they knowingly developed highly effective agitprop, often intentionally disguised as science, to blunt the public reaction when the story eventually broke for the general public. They have straight up bought politicians to peddle said agitprop, too. reply digdugdirk 9 hours agorootparentprevIt's the difference between your car's brakes failing and you plowing through a crosswalk and tragically killing a pedestrian, and being a serial killer. Intentionality is key, and ExxonMobil is a highly profitable psychopath in this regard. reply rayiner 17 hours agoparentprevDo PFAS kill people? reply pengaru 18 hours agoparentprev\"land of opportunity\" reply phkahler 20 hours agoprevTo all the comments wanting some kind of personal accountability or justice, let's just point out that the government won't even ban stuff like this quickly. Companies are given time to phase it out. Just making a hard ban effective immediately is seen as too harsh, yet it is the most obvious thing to do and you're asking for much more. reply mattmaroon 19 hours agoparentThat's also a function of lack of strong evidence of harm. We're not even sure that people exposed to high levels of PFAs are suffering harm, let alone the other 99% of us. There's no mesothelioma-level smoking gun here even for firefighters, factory workers, etc., let alone those of us who are only exposed to it via drinking water. It's a mass hysteria, and while I'm certainly not saying it's benign, I've read the studies, and they remind me of the nutritional ones from the 90's we've all since abandoned where they just throw a bunch of claims at the wall, show a bunch of weak correlations in completely uncontrolled studies, and say \"more research is needed\". reply forgetfreeman 18 hours agorootparentYour comment also reminds me of the late 80s and early 90s when tobacco companies pumped massive amounts of cash into cancer research to muddy the water around the harms being caused by their products. reply mattmaroon 18 hours agorootparentVery different. The evidence was much stronger there. The correlations were so high as to be impossible to ignore. (It also helped that smoking was so prevalent across race/class/age etc that it practically controlled for all the other variables inherently, which is true of drinking water exposure to PFAs too.) There were conditions like emphysema that were basically unheard of in non-smokers (or people who worked in certain occupations that involved a lot of combustion byproducts) and common in smokers. The correlations here (at the levels most of us are exposed to) are incredibly weak. Don’t take my word for them, look them up. This is more like the link between aspartame and cancer. There’s no smoking gun, no high correlations. If you look up the symptoms “linked” to it, there are hundreds at least. Not all correlations are equal. But, I install R/O filters in my house because I am not certain it is not harmful and why take the chance? I’m just not certain it is harmful and I would need to be to support jailing people or banning the chemicals immediately and entirely. reply alwayslikethis 17 hours agorootparentAbove all, you should not be able to dump things in the environment which doesn't break down. They also accumulate in your body and is not possible to remove quickly. Moreover, they knew since the 70s [1] that it was probably harmful, but kept making it anyways pretending it was safe. There needs to be corporate responsibility for the cleanup, which may well bankrupt the compnay, and personal criminal responsibility for those who made the decision. 1. https://qz.com/1643554/3m-knew-pfas-was-contaminating-us-foo... reply lazide 16 hours agorootparentA bit of a straw man here - but water doesn’t meaningfully break down. Sodium chloride (salt) also doesn’t meaningfully break down. It’s not that simple. reply alwayslikethis 13 hours agorootparentWell, just the things that do not exist naturally in that location then. reply lazide 12 hours agorootparentSo no concrete? Stainless steel? Glass? No silicates? reply forgetfreeman 11 hours agorootparentIs there a particular purpose you're pursuing with this obvious pedantry or is it merely performative? reply therealdrag0 10 hours agorootparentIf a lay man’s vague proscription is so easily counter acted perhaps one should reconsider its meaningfulness. reply forgetfreeman 8 hours agorootparentSo performative then, gotcha. reply therealdrag0 8 hours agorootparentWhat does that even mean in the context of a discussion like this? Doesn’t make any sense lol. reply forgetfreeman 1 hour agorootparentDoesn't it? I think we all agree performative behaviors have taken center stage over the last couple decades (see also: online influencers, prepper culture, and the shitstorm over \"woke\" culture). Quibbling over bullshit minutia with no specific larger goal in mind, especially in public forums, is just another manifestation. reply Zigurd 18 hours agorootparentprevAlso leaded gas, also asbestos. Leaded gas left a trace in the Earth's crust about which future archeologists will remark of our fallen civilization \"Oh yeah, this kind of explains the PFAS layer, too.\" reply mattmaroon 18 hours agorootparentThere is always a time where a harmful substance is introduced, suspicions develop, and data is inconclusive. As I mentioned, that may be where we are with PFAs. But for each of these there are dozens of examples where people are hysterical about some new substance, correlations are shown, and it turns out to be nothing. We could be there with low level exposure to PFAs too. With asbestos, like with cigarettes, there was a form of cancer that almost only appears in people exposed to it. That’s a solid smoking gin that’s totally absent here. reply Hikikomori 17 hours agorootparentSo it's okay to expose people to whatever might kill them and only do something decades later when proof overwhelms companies and politicians trying their best to do nothing in the name of profit? reply qwerasdf5 16 hours agorootparentI see you used the word 'when' instead of 'if'. Are you from the future? How do you know this? reply Hikikomori 15 hours agorootparentSure, proof might never overwhelm corporate power and politics and people will be exposed to something bad forever. reply qwerasdf5 15 hours agorootparentSorry, that's not what I meant. What I mean is: You are predicting that the overwhelming proof will ever exist. This prediction may turn out to be true, but why should I believe your prediction over that of anyone else's? Do you have some insider knowledge or expertise that you can share? Edit: Also, what do you give the odds of your prediction, by severity of outcome? I want to see how the odds you provide stack up against the economic and thus social consequences of under-regulation or over-regulation. reply Hikikomori 9 hours agorootparentAbout pfas? I dont know. Wasn't commenting about that specifically, just how bad our system works in general. reply forgetfreeman 11 hours agorootparentprevI hear what you're saying but foundationally this is a bullshit argument. The entire debate evaporates if you simply shift the burden of proof where it belongs and state that Things must comprehensively prove their safety before being released onto the market or otherwise into the wild. reply lazide 16 hours agorootparentprevThat profit pays your paycheck, at least a little, and is part of your retirement. And the product is part of the product you and everyone else decided they wanted to own. reply Zigurd 15 hours agorootparentAnd the cleanup and health care costs are deducted from that. reply Zigurd 17 hours agorootparentprevThey knew and they carried on to the detriment of all of humanity. The penalties we have are insufficient. Perdue pharma got away with mass murder. There's a perverse incentive to carry on with grossly detrimental products, because an admission that they are detrimental could bring on claims of liability. The penalties have to be harsh enough to overcome that reluctance. reply mattmaroon 13 hours agorootparentThey knew what? The really important question (do these chemicals in the levels we see in the body of average Joe cause harm?) is still unknown. Yet half the population is ready to tar and feather 3M execs due to the hysteria. (Never mind that a dozen other chemical companies are still on the hook too.) Purdue pharma might not be off the hook yet (didn’t the judgment that protected them from criminal liability get reversed? I admit I don’t know as much about that case.) and did commit mass murder, but this isn’t that. We don’t know that any sizable number of people have even been harmed by these yet, that’s my point. What does one evil corporation have to do with an entirely related other one? Are we supposed to just jail the execs of any corporation who we think may have done something wrong just because of Purdue Pharma? We know PFAs and the like are in most people in low levels. We know there are a bunch of very weak correlations with various health defects with no indication of causality. The penalty was harsh. That’s a whopping figure even for a mega corp. I just don’t see the case for jail time here like I do with the Sacklers. reply wyre 14 hours agorootparentprevYour overrationalization on the safety of PFAS and your defense of the people making the decisions to keep poisoning the water supply is gross. reply siliconc0w 17 hours agoprevThere is some evidence (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9228135/) that PFAS causes trans-generational dysfunction. Even if we somehow get it out of our environment - which we will never do - the damage already done can stay with us indefinity. They not only poisoned several generations, they poisoned future ones as well. There is no amount of money that can be be paid here. reply kurthr 15 hours agoparentHaving read the article it's really not clear this it supports that analysis, \"As expected, environmentally relevant PFAS levels did not affect survival.\" In particular, although the high levels of exposure (of one or more PFOS/PFOA) showed effects, there was reverse correlation at low levels. Of course this is in Zebra fish so I'd hesitate to guess the relevance for mammal or human populations. It does seem to most directly affect lipid metabolism including steroids, and is multi-generational. I'd be most worried where the chemical (waste) byproducts were used as fire suppressants rather than the small quantities found elsewhere. Those concentrations (hundreds of ppt) are much more worrying. People who sprayed their clothes, couches, tires with scotchguard were probably also already exposed to 100s of times higher lifetime doses than they will ever see through environmental sources atCorporate entities and the legal protection they provide are accessible to only the wealthy and elite. This is the part of the previous comment to which I was responding, giving just one example of how it's not the case. Limited Liability corporations protect business owners of all wealth levels. Just correcting a patently false claim. I understand your concern about thread topic, but it's just not really applicable to my point above. reply asadotzler 13 hours agorootparentYour point above was not topical and only served to derail the conversation or gaslight the people who you mislead with it. reply NoPicklez 8 hours agorootparentI don't think so, I think it was topical just perhaps not in favour of your view. The person they were replying to and to which their point related to, was very much narrowing down and exaggerating corporate entities to a \"small group of people\" of the \"wealthy and elite\". While these larger corporations limit larger amounts of liability, most people can create a business and have it as a company, giving them limited liability. Yes, large corporations have the money and resources to better limit their liability than a smaller corporation. They weren't disagreeing with that person, but providing a more balanced view in which liability limitations benefit companies of all sizes, not just larger ones. reply stonogo 16 hours agorootparentprevWhy would anyone believe that legal remedies are not more accessible to the rich? That's the foundation of the cash bail system, among many, many other differences. reply generic92034 17 hours agorootparentprevIs that limited liability in the US really that general? Why were James Robert Liang and Oliver Schmidt serving a sentence in prison? reply andrewpolidori 17 hours agorootparentIn general, it's not a protection from gross negligence and/or fraud. The problem is that in cases like these it's extremely difficult to prove such a thing, especially considering it's over many decades and potentially dozens of people. There's also a bit of practical concerns where criminal prosecutions don't want to pick cases that are tough to win as it'll be resource intense to pursue them. Of course like the Volkswagen situation you reference (in Germany) where the corporation operates may impact criminal prosecution. reply konschubert 21 hours agorootparentprevBut this would have to be limited to cases where you can prove personal intent or personal gross negligence by the executive. Otherwise you’re effectively making companies illegal. reply ndsipa_pomu 20 hours agorootparent> Otherwise you’re effectively making companies illegal. It's more like making illegal behaviour by companies to be illegal. There's a disconnect where behaviour that would result in jail time for people, doesn't result in jail time if performed by companies and that seems wrong to me. If an individual poisons a water supply, they're not going to be getting just a fine. reply konschubert 20 hours agorootparentI don’t think a lot of ppl would want to take a job where they are liable for the actions of others. reply ndsipa_pomu 20 hours agorootparentIsn't that exactly what is required of CEOs and executives? If a company doesn't have liability for upper management, then where's their incentive to not instruct employees to behave illegally? reply Wolfenstein98k 20 hours agorootparentInstructing employees to behave illegally is illegal reply ndsipa_pomu 17 hours agorootparentThat doesn't appear to be well enforced as otherwise companies wouldn't be able to get away (or at least maybe receive a fine at some point) with illegal activity. If a company is illegally dumping chemicals into the water, then there has to have been at least one instance where a higher up instructed employees to do so. reply user_7832 19 hours agorootparentprevIn some (not all of course) of these discussions, the CEOs/other decision making executives have said things that were fairly explicitly \"dangerous\"/illegal. I'm not talking about \"hold ceo responsible for something s/he wasn't aware of\", I'm talking \"hold them responsible when they instructed something even though they knew people would die\" (eg see Ford Pinto). reply int_19h 15 hours agorootparentprevIt depends on the risk/reward ratio. It would certainly make megacorps non-viable, but I don't see why that is undesirable. If the average company size is scaled down to the level where such liability is actually manageable, it would also largely solve the problem with monopolies, so it's a win-win. reply littlestymaar 20 hours agorootparentprevDon't underestimate the appeal of a 7 digit salary and the position of power you get with it. There a orders of magnitude more candidates for CEOs positions than there are jobs, so it's not like we would run out of people to do the job if we increased the responsibilities. reply ryandrake 19 hours agorootparentprevIt would be better for the world if those people do not take that job and tell/incentivize their employees to do illegal things. So win-win. reply panta 18 hours agorootparentprevIf you develop or sell something that has even a remote potential to kill someone, it's your responsibility to apply a cautionary principle and gather enough evidence that the probability of that outcome is negligible. Otherwise that IS gross negligence. reply wouldbecouldbe 21 hours agoparentprevFor these cases of gross misconduct affecting health of people, criminal charges should actually be made against individuals who made the decisions. reply elaus 21 hours agorootparentAre you talking about the _responsibility_ that is the justification for paychecks in the millions? It seems to end somewhere before jail sentences for most execs... reply sp332 21 hours agoparentprevIf they could have raised prices, they would have already done it. reply mattmaroon 20 hours agorootparentRight, they aren't the only manufacturer. There are a dozen major ones. People always miss this about competitive markets. They are already charging the highest price they believe they can. reply backwardation_b 21 hours agoparentprevThe system self corrects by companies moving to a different supplier. If 3M has to increase prices (they will) to offset the settlement costs, then other suppliers will be comparatively cheaper. 3M share price and profit will decrease which incentivizes the board and or shareholders to encourage safer (read: legal) business practices. reply thayne 21 hours agorootparentThat clearly doesn't work. The fines are often less than the profits made. And the individuals and shareholders have already made a fortune by the time the payments come due, and can just move on to the next company and do the same thing there. reply checkyoursudo 20 hours agorootparentprevHow does your model account for the probability that cheaper competitors are simply bad actors who have not yet been caught? In fact, without something like criminal penalties, why would the board encourage safer/legal business practices? Seems like they might equally say, well that was a good run now find us another dastardly product that can bring our prices back down to our competitors. reply rolandog 21 hours agorootparentprevThe people affected by these \"corrections\" are the workers that would be laid off due to decreasing revenue,... In order to maintain the expected profits that result in dividend payments to shareholders. So, one could make the case that the workers get punished by the board of directors' bad decisions. reply int_19h 15 hours agorootparentprevAnd those \"other suppliers\" will be 3M - same owners, same board, same executives - under a different name. reply ClumsyPilot 21 hours agorootparentprevYou are selling a fantasy, a spherical cow. As the OP said, The original people responsible made bank and retired. The original shareholders cashed out. this does not encourage anything. reply lettergram 21 hours agoparentprevThe fine was the 1/3 of yearly revenue, in 2023 their revenue was $32.68B, net income was -$7B (that’s a negative seven billion). Im not sure of the payment timeline or if they have insurance, but it seems likely this will wipe out a few years of profitability and may force the sale of business units. That said, I do think personal culpability is probably a better incentive and more ethically reasonable depending on the circumstances. Though that’d have to be criminal neglect, using laws we probably already have. In other words, this is a civil penalty, not the same as a criminal case with higher burdens of proof and motives need to be established. reply rahimnathwani 21 hours agorootparentthis will wipe out a few years of profitability They've already recognized most of the loss, so most of the impact on profits is in the past. From the Q2 2023 10-Q: During the first six months of 2023, as a result of ongoing review and recent developments in ongoing environmental matters and litigation (including the proposed PWS Settlement), the Company increased its accrual for PFAS-related other environmental liabilities by $10.3 billion and made related payments of $38 million. As of June 30, 2023, the Company had recorded liabilities of $10.9 billion for “other environmental liabilities.” These amounts are reflected in the consolidated balance sheet within other current liabilities ($0.3 billion) and other liabilities ($10.6 billion). The accruals represent the Company’s estimate of the probable loss in connection with the environmental matters and PFAS-related matters and litigation described above. The Company is not able to estimate a possible loss or range of possible loss in excess of the established accruals at this time. reply cfiggers 21 hours agoparentprevI dunno man, $12 BILLION is a pretty serious speeding ticket reply idiotsecant 21 hours agorootparent3M sells about 1.3 billion USD of the stuff yearly at about 16% margin, so figure about 200 million USD in profit per year. 12 billion represents about 60 current profit-years of production.They invented the stuff in the 30s but have been churning it out in quantity since the 50s. 12 billion isn't a speeding ticket but it also isn't even all the profit they made on it. In addition to that, is their obligation to pay their profits or to make whole a world soaked in forever chemicals? Because the latter would be way more than 12 billion. reply tome 21 hours agorootparentI'm not entirely sure what you're saying. Are you saying that 60 years of profit on a product that's been churned out in quantity for 70 years is a small fine? reply justinclift 19 hours agorootparent> 12 billion isn't a speeding ticket but it also isn't even all the profit they made on it. They literally said \"it isn't even all the profit they made\". That seems pretty clear. reply tome 14 hours agorootparentIt's not clear to me! reply justinclift 13 hours agorootparentThey're being fined less than the profit they made. Surely they should be getting fined more than how much they profited? Preferably (at a minimum) several times the amount, so they (and others) aren't keen to do it again. reply asadotzler 13 hours agorootparentprevnext [2 more] [flagged] justinclift 10 hours agorootparent> That this simple idea seems a stretch for you might be a signal to you that your reality is distorted in this area and could use a re-examining. That seems pretty unwarranted. :( reply ClumsyPilot 21 hours agorootparentprevIndeed, the real justice would be to make them pay for cleanup efforts for the next 200 years reply ben_w 20 hours agorootparentHow are you planning on enforcing that given: 1) bankruptcy is a thing, and 2) that period exceeds the lifespan of not only those responsible, but also any kids they might have conceived in extreme old age, and people can just… choose not work for (or invest in) a company that's been stuck with a long-term obligation. I can see how it would be possible to make criminal liability stick on all members of a chain of command, corporate or otherwise, responsible for some decision. I can't see how setting a multi-lifetime punishment could possibly work. reply ClumsyPilot 20 hours agorootparent> 1) bankruptcy is a thing Students loans, famously, can’t be discharged in bankruptcy, and follow you your entire life. Should we have one set of law for the capital class and another for mere peasants? reply mattmaroon 19 hours agorootparentWhen you sign up for student loans, you are aware (or at least should be) that they can't be discharged in bankruptcy. They aren't dischargable in bankruptcy because if they were, far fewer would be made, and far fewer non-rich people would go to college, and we don't want that. Nothing would perpetuate class divisions more than restricting education to those with money. Nobody would ever get a student loan for an anthropology degree again. I don't necessarily agree with this system, as an 18 yr old probably does not have the life experience to make such large financial decisions, and I think perhaps far fewer people should go to college for things of no economic value, but the system has logic to it. It's meant to benefit the \"peasants\". Rich people don't need student loans and can pay for their kids' degrees. Fines aren't the same. A company only has so much money. Many of the owners of the company are pensions funds, etc., that are held by the middle class. You can't reasonably squeeze money from everyone who had an ETF that contained 3M. You need to allow corporations to file bankruptcy to have a healthy economy, in fact, America's bankruptcy laws are (according to some economists) one of the reasons we have such a strong economy. reply int_19h 15 hours agorootparentWhat you can do is liquidate the company and set up a non-profit fund with that money with explicit mandate to do the clean-up. Let the non-profit invest said money ethically to fund its operations. Ideally, of course, the initial capital should also include the personal wealth of all the top management personally responsible for this. > Many of the owners of the company are pensions funds, etc., that are held by the middle class. You can't reasonably squeeze money from everyone who had an ETF that contained 3M. And why not? Do you not see how this kind of system is deliberately engineered to produce pushback under just such a pretense by, effectively, diluting and spreading responsibility wide enough across society? OTOH if we stop taking the \"ohnoes, it will hurt all those middle class investors\" argument seriously, it would actually encourage people to invest ethically instead of just blindly giving their money to whichever random sociopath offers the highest return. (And yes, I do have considerable personal investments of my own.) reply ben_w 13 hours agorootparent> And why not? Because the biggest voting block in many democracies is the pensioners, so this being possible is a good way to lose elections. > Do you not see how this kind of system is deliberately engineered to produce pushback under just such a pretense by, effectively, diluting and spreading responsibility wide enough across society? OTOH if we stop taking the \"ohnoes, it will hurt all those middle class investors\" argument seriously, it would actually encourage people to invest ethically instead of just blindly giving their money to whichever random sociopath offers the highest return. Everything I have seen in my life leads me to think that people are pretty blind to risks when you dangle the possibility of a large return in front of them. Ponzi schemes, Brexit, lotteries, wars, unrestricted AI — at every scale, people will enthusiastically go for all of them in the name of some hoped-for benefit and call you names if you point out the risks they're taking. reply ben_w 16 hours agorootparentprev1) That's a reason to change the student loan rules where you live. 2) When a corporation goes bankrupt, it can get dissolved. This is the corporate equivalent of the death penalty, which you may approve of, but it still isn't going to help: all the assets get sold, and all the investor groups (a) know that (b) can get the stuff cheap because it's a fire-sale, and (c) know the exact size and shape of the market opportunity created by the dissolution. And there's a workforce right there, who were all laid off and are looking for work, who have exactly the skills you need and are already conveniently located and familiar with the equipment. The debt doesn't get fully paid off, there's just nobody to do the paying. reply skrebbel 21 hours agorootparentprevI think the argument is that people doing crime go to jail, so when a company does crime, maybe the same should happen. reply jajko 19 hours agorootparentprevDo you see the company struggling to stay afloat, breaking up, selling parts to competition just to survive? For poisoning half of planet and killing god knows how many, yes thats a (bit bigger) speeding ticket, nothing more. reply konfusinomicon 21 hours agorootparentprevnot when you have 3M billions reply alexashka 15 hours agoparentprevWhat makes you think the system isn't working as intended? What would 'correct' even look like? Occasional human sacrifice to appease the masses as is being called for on here? reply tejohnso 20 hours agoprevWho were the top level executives while this was going on? Why don't these articles ever lay blame properly? And what are the consequences for those individuals? And what is being done to ensure it doesn't happen again? reply ein0p 16 hours agoprevI’m not even sure what a true “settlement” would look like in a case like this. Usually a settlement serves to at least partially repair the damage to the plaintiff. In this case the damage can’t be repaired at all. If there was a crime committed people need to go to jail in such cases. Unequal application of justice: someone steals a donut and ends up in jail, yet, another person person dumps tons of toxic sludge in a river and they get to “settle” using the money that’s not even their own. reply superkuh 17 hours agoprevI would like to see 3M forced to pay the DNR of Minnesota and Wisconsin a similar amount for making the St. Croix and Mississippi rivers polluted enough that we can't eat the fish again (like the 1980s/90s before the mercury settled out). reply VoodooJuJu 21 hours agoprevWe unfortunately see it far too often - corporate crimes that are punished with simple fines that become the cost of doing business. But we all know it's not justice. Justice would be criminal charges against the people within the corporation that actually did the crime. We're long overdue for some change here. Are there are any politicians or bills (past or present) that have floated the idea of exposing shareholder skin in the corporate game? If these people were threatened with serious prison time, capital punishment, or even worse - being forced to drive a used Kia the rest of their lives (materialists couldn't bear the shame), we'd see far less harm being done. reply CJefferson 21 hours agoparentAnother option would be easier corporate death — all shares instantly voided. The government could decide what to do with the company, fold it, or sell it for scraps. Yes, there might be motivation for the government to “kill” companies for profits, but there is exactly the same (and proved many times!) issue with putting people in prison, so I don’t see why companies should get a special exception, they are “people” as well, or so I’m told. reply always2slow 18 hours agorootparentI think you're on the right track here.. corporate death, shares voided, assets frozen in perpetuity, buildings, land, machinery. Everything frozen forever to stand as a warning to the next executive or employee before doing something like this. reply anonymousab 17 hours agorootparentYou can even start with corporate jail. No business operations of any kind allowed for weeks/months, be that preforming services for clients, servicing debts, contracts or payroll. And if that results in an existential threat to the company or a lot of lawsuits for failing to perform their obligations, then that would be the just results of the company's decisions. It would be as existentially threatening and crippling to a company as it currently is to the average member of society. reply thayne 20 hours agoparentprevUnfortunately, I think there is a lot of overlap between shareholders and politicians. The people making the laws have an incentive not to expose shareholders to personal liability. reply EarthLaunch 21 hours agoparentprevIf powerful or opportunistic criminals were at risk of being criminally charged by the government, they're then utterly incentivized to influence and control said government. And that has already happened since forever, which is why you have to ask if there are politicians threatening this. It's all already tied together. reply alwayslikethis 17 hours agoparentprevWe just need to impart fear on the executives to not make harmful decisions. The first step is an independent investigation to let their names be known to the public. The system should then be able to self-correct. reply 23B1 18 hours agoprevI wonder how many HN readers are complicit in this same style of abuse towards customer data. reply aranchelk 17 hours agoparentIn my mind privacy breaches don't even live on the same planet as large scale long term environmental damage. reply SoftTalker 17 hours agorootparentBut the same thinking leads to both. \"Not my problem\" or \"just doing what the boss asked me to do, that must be OK\" etc. reply 23B1 11 hours agorootparentprev...yet. And I'm not just talking about privacy breaches, I'm talking about conscious, willful abuse. reply calvinmorrison 21 hours agoprev [–] I would like to see particularly untrustworthy companies stripped of all assets, buildings razed and executives jailed reply idiotsecant 21 hours agoparentJail executives? Sure. Tear down 3m? Why? 3M is useful. They make a lot of stuff we need. reply calvinmorrison 20 hours agorootparentCool! Seize the assets and auction them to companies who don't do crime. Maybe a bigger fuck you would be to revoke all thier intellectual property. Seize the assets under asset forfeiture and release them into the public domain. reply JumpCrisscross 19 hours agorootparent> Seize the assets and auction them to companies who don't do crime Like one of the dozen other companies that manufacture PFAS, which continue to be totally legal in America. reply CamperBob2 19 hours agorootparentWhat is the actual evidence that justifies a complete ban, in your view? reply JumpCrisscross 19 hours agorootparent> What is the actual evidence that justifies a complete ban, in your view? That’s outside my circle of competence. It’s just interesting to note the predictable drumbeat of calls for jail time, even in cases where we forgot to make it illegal. reply bogtog 21 hours agorootparentprevI liked the part where we even razed their buildings reply yuppiepuppie 21 hours agorootparentprevIm guessing they want a mechanism to hold shareholders feet to the fire. I venture to guess that there are better options for this. reply struant 21 hours agorootparentThese fines should just be paid by shareholders (backdated to whoever profited from their misdeeds) reply ben_w 20 hours agorootparentThat kind of thing is called an \"unlimited company\", which exists, but is rare and puts off investments (but apparently makes borrowing easier?): https://en.wikipedia.org/wiki/Unlimited_company Random \"unsophisticated investors\" (e.g. me) are supposed to get some degree of protection from catastrophic mistakes we can't be expected to recognise heading our way, in order to encourage us to participate in the stock market without having to worry we might inadvertently be taking on some fractional responsibility for a debt exceeding our net worth. Likewise, governments don't like it when pension funds are bankrupted because of what they invested in. reply struant 3 hours agorootparentThe people benefiting from any behavior ranging from deceptive to criminal should be at least partially held liable (at minimum to not receive any profit whatsoever from those misdeeds). Will this change investor behavior and industry culture? Yes, and that is not only a net good but absolutely necessary. If you want capitalism as a financial system then holding the capitalists liable for wrongdoing that benefits them is the only way you avoid turning the planet into a hellscape. reply ben_w 31 minutes agorootparentYou need to be very careful with how you phrase that and what exactly you mean — as (for example) 3M and Boeing pay taxes and make stuff broadly useful across society, the set of people benefiting from misbehaviour which these companies are deemed responsible for is… basically everyone. Even in a more narrow case with no complications (even narrower than SBF, because he distributed money to good causes who can't pay it back), you're including the defence lawyers. I'm (caveat: not a lawyer) more in favour of specifically criminal behaviour being handled by \"piercing the corporate veil\", putting the penalty directly on the humans who actually did the crime without letting them hide behind corporate personhood: https://en.wikipedia.org/wiki/Piercing_the_corporate_veil But of course, it can be hard to prove who knew what, so sometimes \"follow the money\" is easier. And yes, I also agree with you that there are misaligned incentives in capitalism that lead to bad outcomes and we should work to improve that — just don't throw out the baby with the bathwater. reply calvinmorrison 20 hours agorootparentprevLimited liability and other contraptions are just tools capital and feudalists use to avoid liability. In fact these should be reveresed. In fact most of these should be tried as RICO cases reply ben_w 16 hours agorootparentFeudalists avoid liability by conscripting peasants and ordering them to fight their creditors' peasants, or handing out literal piracy (privateer) licenses, or steal money from monasteries after denouncing the Pope or accusing the specific holy order in question of practicing witchcraft. RICO came after 159 years of US experience with all the benefits of limited liability and in the midst of the Cold War where the US was fighting for the freedom to keep government out of the way of business, so selecting that as your magic legal phrase is about as sensible as trying to fight Covid lockdown restrictions by putting a printout of Magna Carta in a shop window. reply tomrod 18 hours agorootparentprevWhy? And please, cite sources when making the case for reasonability. reply ClumsyPilot 20 hours agorootparentprev> 3M is useful. They make a lot of stuff we need. Doctors are even more useful, but they do end up in prison if they commit murder reply jajko 20 hours agorootparentLess than you would think (in case we talk about manslaughter / kill by negligence, outright murder may be a slightly different topic). Literally every seasoned doctor has what they call a 'small cemetery' of their failures. Got this from a GP, you don't need to be a surgeon for this to apply. In fact GPs have probably highest chance of saving/not saving lives due to volume of cases and potential severity of those, even if they don't do any invasive medicine themselves. reply abecode 14 hours agorootparentActually I heard the opposite... I was a premed student and I did and internship at a hospital and one of the doctors said it's actually fairly difficult to kill someone by accident, ie you have to mess up very badly or the patient must already be gravely ill for an accident to result in death. Still, the comment implies that doctors do in fact make mistakes and it was one of the experiences I had in that internship that made me decided to switch majors. reply egl2021 19 hours agoparentprev [–] And send their employees to penal colonies. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "3M settled a multi-billion dollar lawsuit regarding PFAS contamination in public drinking water systems, with payments scheduled from this summer until 2036.",
      "The settlement intends to compensate water providers for pollution caused by PFAS, also known as forever chemicals, which are potentially hazardous.",
      "In addition to monetary compensation, 3M committed to ceasing the production and utilization of PFAS by the conclusion of 2025, aiding in filtering PFAS from water systems and contamination testing to support affected communities."
    ],
    "commentSummary": [
      "The debate centers on the accountability of corporations and individuals for causing harm, addressing environmental and public health issues like PFAS contamination.",
      "Suggestions include criminalizing corporate negligence, penalizing senior executives severely, and holding individuals personally liable for harmful actions.",
      "Key points discussed are wealth distribution, regulating harmful substances, corporate accountability, and systemic changes necessary to avoid future harm, underscoring the call for better safeguards, ethical choices, and individual responsibility in the business sphere."
    ],
    "points": 191,
    "commentCount": 211,
    "retryCount": 0,
    "time": 1712489212
  },
  {
    "id": 39960645,
    "title": "Writer's Sleep Study: How Sleep Affects Blog Posts",
    "originLink": "https://breckyunits.com/sleepWriting.html",
    "originBody": "Blog Posts, Sorted by Sleep April 5, 2024 — Have you ever examined the correlation between your writing behavior and sleep? I've written some things in my life that make me cringe. I might cringe because I see some past writing was naive, mistaken, locked-in, overconfident, unkind, insensitive, aggressive, or grandiose. I now have a pretty big dataset to identify my secret trick to write more cringe: less sleep. For this post I combined 2,500 nights of sleep data with 58 blog posts. A 7 year experiment to see how sleep affects my writing. Interactive version. ~7 Hours is the Cutoff Most posts above 7 hours of sleep do not need a sleep disclaimer. Most posts below 7 hours do. Not to say there is no value in the posts made with under 7 hours of sleep, it's just less rigorous writing (and thinking). On the plus side, writing with little sleep can be more concise at times. It might exaggerate the key ideas, but nevertheless identify them fearlessly and concisely. Interactive Table Static image of table above. More Posts. Similar Word Counts. Higher Scatteredness. Similar IQ. Higher Confidence. I actually post slightly more when I sleep less (Pearson correlation coefficient of -.14), but fewer words per post, which is indicative of a more \"scattered\" thinking state. I was surprised to see that I don't generally generate a whole lot more words in deprived sleep states. I perceive my writing to be smarter during those times, but looking back it's clearly not. Other Social Media Besides this blog, I have long written and posted content to HackerNews, Reddit, other discussion forums, and at times Twitter, Instagram, Facebook, YouTube, and LinkedIn. I haven't done the data grunt work, but if my memory serves me correctly I am confident my publishing behavior on those platforms mirrors the same patterns as my blogging behavior, with regards to sleep. Public vs Private Writing There have been stretches where I published little publicly but was generating a similar amount of tokens, just in private groups. My writing patterns in private groups also mirrors my patterns on this blog, with regards to sleep. Tangent: when I've been lucky to be a part of brainiac private organizations (such as Microsoft, YCombinator, Our World in Data, academia, and so on), I got to read so much brilliant writing by people who rarely post publicly, and every time I think about that I am humbled. There is so much well written content on the public web, and to think it is only a fraction of the great content ever written, is humbling. Sleep Disclaimers I realize I already have an unofficial \"sleep disclaimer\" policy. I have de-indexed (but kept published) at least a couple of sleep-deprived posts, and added a disclaimer/correction to at least 2 others. Now with this dataset I am sure I will append a few more sleep disclaimers. With sleep disclaimers, I can say, \"hey, might be interesting ideas here, but don't train too heavily on this\". Grateful for Git I am happy with my decision to use git for this blog so that I always keep an honest history, while still being free to down weight sleep deprived content and try and keep my more thought-out out ideas front and center. Benefits of Peer Review I don't have a column for it (yet), but it does seem my better posts often were the ones where I took the time to get friends and/or colleagues to review, IRL. Sleep deprived posts I would generally blast out without talking to anyone. Peer review is a great filter, and a great forcing function to put more effort in. On the other hand, because the importance of ideas varies by so many orders of magnitude (there are \"black swan\" ideas), you could make an argument that spending too much time in one area of ideas isn't the optimal strategy, and publishing things as you go, improving them later, is an approach with merit. Writing data reflects the current phenomena in your brain It seems when I sleep less, my brain is in more of a pleasure seeking state, has a bias to action (\"don't think, just do\"), and feels less pain than in a more rested state. Less sleep means less critical thinking. Less sleep seems to make me less willing to invest the time in rewiring my brain to correct mistaken thought habits. Grateful for FitBit I started wearing a Microsoft Band when it first came out in November 2014. Then a Band 2, then FitBit Charge, Ionic, Versa, and now Sense 2. I am grateful for all the people involved with creating these things. I think continued progress in the wearable sensor field is the best bet for improving human health. View source Built with Scroll v75.0.0",
    "commentLink": "https://news.ycombinator.com/item?id=39960645",
    "commentBody": "Blog posts, sorted by sleep (breckyunits.com)190 points by breck 19 hours agohidepastfavorite44 comments ssgodderidge 17 hours agoFascinating. I would love to see my breakdown of bugs-per-commit, or words-spoken-in-meetings, compared to sleep to see if there any correlations. Anecdotally, sleep seems to be the number one factor that influences my productivity. More so than diet, exercise, or even mental health. reply pawelduda 11 hours agoparentI remember when played a lot of DotA 2, trying to climb the ranks.Anecdotally, sleep seems to be the number one factor that influences my productivity. More so than diet, exercise, or even mental health. These are far from independent variables though. reply tiagod 13 hours agoparentprevI have chronic insomnia that got worse every year. Eventually I stopped being able to sleep without being absolutely exhausted, usually after 35h or more awake hours. Almost drove me crazy. Paranoia, auditory and visual hallucinations, extreme touch sensitivity, you name it. Lucky, modern medicine pretty much saved my life. (trazodone, which was combined with mirtazapine a few years later.) I later was diagnosed with ADHD, and after medication I find it a bit easier to sleep (although I still need the other to have consistent sleep). Biggest problem was my brain just wouldn't shut up (and got noiser and noiser the longer I was awake.) reply OccamsMirror 9 hours agorootparentWhen you say \"after medication\" do you mean the anti-depressants or ADHD medication? If the latter, what were you prescribed? reply ramijames 12 hours agoparentprevIt took me a long time to get there, but I finally accepted that when I am tired, I shouldn't work. I should rest. The quality of the work that I do when tired is abysmal, and usually requires that it be redone anyway. reply gsuuon 7 hours agorootparentYup - it's very easy for distracted, tired work to become negative work. reply CoffeeOnWrite 16 hours agoprevSigh, missed opportunity to use sleep sort to do the ordering. https://web.archive.org/web/20151231221001/http://bl0ckeduse... reply HPsquared 1 hour agoprevThe big question is, does the lack of sleep cause the increased levels of cringe scatteredness in the blog posts (and presumably elsewhere too), or do (temporary) high levels of cringe and disorganization in one's life cause a lack of sleep? Or, there could be some hidden variable driving both. Some stressor causing both lack of sleep and scattered thinking. reply pedalpete 10 hours agoprevWe work in the sleep space, and I was having a discussion about insomnia with relative stranger. He also has insomnia and we were discussing how it is often spoken of as an issue with anxiety, but we both concluded that, even though we are aware of the negative impacts, we liked the thinking time alone, it is not stress induced. It is a time we get our best thinking done. Or more thinking is perhaps a better description. I didn't have a great take-away from this post, but to suggest that post activity could also have a correlation with sleepless thoughts. I'm positive a Balmer Peak exists for the quality of these posts. reply ehnto 8 hours agoparent> I'm positive a Balmer Peak exists for the quality of these posts. Aha! I have thought this for some time, that there is a level of tiredness where I actually get quite a bit more done. Likely because I am too tired to split my focus and have my mind wander. That said I find my effectiveness at decision making deteriorates, so it would only be good for tasks where the right path is obvious. For important complex decisions I think I am still better off well rested. reply Esus 4 hours agorootparentI think I might have saw something about this in the book When, but might have been a different one. They described it as when tired, the guards of judgement that normally stand when we're well slept are less active. Not sure how accurate it is, still trying to test it. > For important complex decisions I think I am still better off well rested. 100%. reply foolswisdom 6 hours agorootparentprev> That said I find my effectiveness at decision making deteriorates, so it would only be good for tasks where the right path is obvious. For important complex decisions I think I am still better off well rested. My experience is the same. Sadly it's not just specific decision making, but all the important parts of my life depend on me having good executive function or they might happen even if I know the way forward. reply jasonjmcghee 4 hours agorootparentprevI find myself quite productive in situations where my body wants to do something- eat, drink, sleep, etc and I’m putting it off until i finish some key thing. Some impending implicit deadline… reply pedalpete 3 hours agoparentprevThere are a few responses to this comment that suggest people being better on less sleep. You should also consider that your mind tricks you into thinking you are better on less sleep. Your own subjective measure of your performance is hindered. reply tandav 16 hours agoprevIt's annoying that there's no easy way to export data from the Apple Watch. The only option is to export complete data from the Apple Health app, which results in a large ZIP file. This file takes about 10 minutes of preprocessing before the whole archive becomes available. It would be much better if I could export only the new records, like those from the last day. reply CharlesW 14 hours agoparentYou might find this helpful: “Share your health and fitness data in XML format”, https://support.apple.com/guide/iphone/share-your-health-dat... reply baapercollege 2 hours agorootparentHelpful. Do you know of any way to export the screen time from iphone/ipad? reply stephenbez 12 hours agoparentprevI use health auto export and it works pretty well. https://www.healthexportapp.com/ reply spaceywilly 16 hours agoparentprevSounds like a good idea for an app. I believe all that data is available through HealthKit. ChatGPT it up :) reply 082349872349872 16 hours agorootparentI believe most of us sleep fewer than 3 times per day, so writing down times and doing a few subtractions and a little data entry once a week should be under 1 min/day to have everything digitised. (that said, https://xkcd.com/1205/ suggests it'd be worth spending up to 21 hours to fully automate) reply gcr 16 hours agoparentprevIt's not quite what you're asking for, but you can retrieve heath records using a Shortcut. I'm not sure if detailed sleep data is retrievable this way, but you at least can get times to bed, times woken up, etc. reply Dibby053 12 hours agoprevI think rather than less sleep causing worse blog posts, both having a common cause is a more likely explanation. For instance stress, or spending too much time writing careless blogposts instead of sleeping :P reply jldugger 7 hours agoparentIndeed there is a massive confounding factor in OP's dataset: they have recently written about their bipolar disorder diagnosis, which usually pairs less need for sleep with overconfidence and high self-esteem, but probably isn't causal in the way you or I might write poorly when sleep deprived. As an example: consider the datapoint \"Turning Down the Nobel Prize\"[1], which appears to be the pre-launch announcement for their startup to literally cure cancer, drops a bunch of names, then implies they deserve an instant Nobel prize -- but not before another personal hero and pioneer gets one. This data may be useful for OP in their own diagnosis and self-care, but is still n=1 in one very important dimension relevant to the rest of us. [1]: https://breckyunits.com/turning-down-nobel-prize.html reply foobarbecue 11 hours agoparentprevYeah... also this person writes about some pretty intense psychological trouble. Seems like their manic episodes are one cause of both poor sleep and aggressive writing. reply Esus 4 hours agoprevThis is a great visual. Wonder if you could do sentiment analysis too? I use a bunch of sleep trackers and have spent 5 years to correlate poor sleep to bad listening/more misunderstandings. Wouldn't have noticed it without the data. reply neilv 13 hours agoprevI have some (imperfect) sense of how sharp I am at some time, or am likely going to be in the near future, which influences when I tackle tasks. Algorithm that must be correct? It's not going to be working through that first thing in the morning, nor squeeze it into the 30-minute gap between 2 meetings. And hopefully it doesn't have to be done on that day I didn't sleep well, because I can tell I'm not at my best. When something that needs hard thinking must be done right then, I can do it (and probably still better than most people, if it's something I'm good at). But I often have the feeling that I'd be thinking of more possibilities, or not making \"careless\" oversights, were I not fatigued or distracted. Occasionally, I've pulled off some of my most meticulous work despite being fatigued. But it felt like more exertion than it should, and presumably I wasn't spotting all the opportunities that I would've under better conditions. That said, as someone who doesn't blog, I don't much filter my more casual, off-the-cuff Internet posts by sleep/sharpness. Actually, Internet posts are more likely to be a morning warm-up, or a break between tasks. Maybe, if Internet posts paid better, I'd start optimizing sharpness for them. reply tipiirai 3 hours agoprevAm I the only one who feels the opposite? I sleep less when excited and have something important to say or develop. I sleep more when I'm bored. reply modeless 12 hours agoprevI want to see a graph of page views vs sleep. reply deathanatos 13 hours agoprevhttps://www.cdc.gov/niosh/work-hour-training-for-nurses/long... Sleep deprivation is basically a form of impairment, but it gets a \"this is fine\" pass from society in most cases¹. I think anyone who has done SRE knows this: I'm an idiot if I'm up at 4a trying to debug something, and the risk of dumb errors is pretty high. … what kills me is when people schedule stuff to be done … basically, while impaired. I work in healthtech, and (considerably ironically, given the article above also coming from healthcare…) providers will schedule migrations at 1 to 4am in the morning. SWE is difficult enough as it is, but you want me to think about IPSec and CGNAT at 1am? The bad reason given across the industry is always \"we can't do it during the day, because that's when users are using the system!\" Your processes are broken, then, if you have so little faith in your ability to deploy new stuff. (And it is possible; my company, in healthtech, used to regularly do mid-day deployments, because we had processed in place such that a failed deployment would usually get caught. The ones that didn't, well, deploying them at 1am wouldn't have made it better. In fact, we had at least one PM from an outage where that it went out after-hours made the outage worse.) Rimworld (the video game) has an interesting system where pawns have a \"consciousness\" value; normally is 100%, but things can lower it, e.g., drinking, drugs, brain injuries. A pawn with a lower consciousness value is simply worse at everything. And there are days, and times — e.g., when I'm sleep deprived after a long night of battling prod — that yeah, I am definitely operating at like 60% consciousness. Everything suffers as a result. > but [where I] was generating a similar amount of tokens, > I can say, \"hey, might be interesting ideas here, but don't train too heavily on this\". … good grief, just no. I for one do not \"generate tokens\" nor do I \"train\" on blog posts. Language like that devalues the abilities of a mind. ¹there are a few spots, like truck drivers, where society starts to care. reply ehnto 8 hours agoparentI appreciate the reference to Rimworld, and I agree with your thoughts on late night deploys. We also don't do deploys at night, since as you alluded to it is the time when you are least supported and least capable (assuming you are normally asleep then at least) This tends to mean that you put your best team members on the 1am job just in case something goes wrong, which is a pretty poor reward for being effective. reply elric 15 hours agoprevThat seems like a really wide range of sleep time. 4.3-10.8 hours. I can't image either of those extremes being good for you? The only times I ever sleep that little, or that much, is after a lot of heavy drinking. reply jzb 14 hours agoparentCongrats? I can’t remember the last time I slept 10+ hours. Probably once in the last few years when I was exceptionally sick. My range would be more like 3-7.5 hours, with (thankfully) few nights in the 3-hour range. 4.5 is not uncommon, as I am a light sleeper and once I wake up due to temperature, noise, or whatever, I rarely manage to get back to sleep. Fork in the disposal. reply kaashif 10 hours agoparentprevI sleep that little/much while recovering from travelling across 5+ hour time differences, which I do somewhat regularly. Commenting because I imagine this is a common reason for sleep deprivation. Lots of 2 hour nights due to not being tired at the right time, then complete exhaustion as I try to make it to a normal bedtime the next day, usually resulting in passing out early and getting 10+ hours in total. reply reducesuffering 4 hours agoparentprevThe author has a medical condition causing it, it isn't ideal. reply nicbou 13 hours agoprevCould it be because of a correlation between sleep hours and time off? I'd be more likely to write on a Sunday, and I'd also be more likely to sleep late on a Sunday. reply mft_ 16 hours agoprev> I think continued progress in the wearable sensor field... I'm interested in people's general experience with wearable sensors - of whichever type. I wear a Fitbit (mostly for sleep tracking) but it's mostly a curiosity rather than something that offers meaningful, actionable insights. And I'm experimenting with deliberately tracking my metrics less in other areas. As a cyclist, it's easy to get sucked into the metrics world, with easy access to HR and power, training plans, and sharing your performances online... but I found I wasn't enjoying the activity of cycling very much. A friend hypothesised that the inherent comparison and competition might be to blame, and suggested to try removing most of the data tracking; it's early days yet though. > ...is the best bet for improving human health. I'd posit that avoidance of behaviours which are well-known to cause dierct harm to one's health is probably a better place to start. reply bpye 8 hours agoparentI use an Apple Watch for sleep tracking and it seems to do relatively well. Whilst I haven’t found the data directly actionable, it has been useful when talking to healthcare providers in the past. reply Liftyee 16 hours agoprev [–] Could someone clarify what the writer means by \"locked-in\" when describing the writing? reply sdwr 14 hours agoparentI take it to mean that his viewpoint is pre-decided and unchangeable. Weird phrasing though, I've only heard it in a positive context. reply quenix 15 hours agoparentprev [–] A state of flow and intense focus. reply 8n4vidtmkvmk 15 hours agorootparent [–] Sounds like \"in the zone\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author analyzes how their writing quality is linked to sleep by studying 2,500 nights of sleep data and 58 blog posts spanning 7 years.",
      "Posts crafted with less than 7 hours of sleep tend to be more concise but less thorough according to the findings.",
      "The author acknowledges the significance of platforms like Git for blogging, peer review for enhancing content quality, and wearable sensors for monitoring sleep patterns."
    ],
    "commentSummary": [
      "The blog post delves into the connection between sleep quality and productivity, featuring personal stories about insomnia's impact on work performance.",
      "It highlights the link between sleep deprivation and cognitive difficulties, while also mentioning the potential advantages of fatigue for productivity.",
      "Suggestions for extracting data from Apple devices, discussions on mental well-being, cognitive functions, and the risks of sleep deprivation are included, emphasizing the significance of sufficient rest and avoiding detrimental behaviors to health."
    ],
    "points": 190,
    "commentCount": 44,
    "retryCount": 0,
    "time": 1712496747
  },
  {
    "id": 39960125,
    "title": "Google Public DNS Boosts Security Against Cache Poisoning",
    "originLink": "https://security.googleblog.com/2024/03/google-public-dnss-approach-to-fight.html",
    "originBody": "Security Blog The latest news and insights from Google on security and safety on the Internet Google Public DNS’s approach to fight against cache poisoning attacks March 28, 2024 Tianhao Chi and Puneet Sood, Google Public DNS The Domain Name System (DNS) is a fundamental protocol used on the Internet to translate human-readable domain names (e.g., www.example.com) into numeric IP addresses (e.g., 192.0.2.1) so that devices and servers can find and communicate with each other. When a user enters a domain name in their browser, the DNS resolver (e.g. Google Public DNS) locates the authoritative DNS nameservers for the requested name, and queries one or more of them to obtain the IP address(es) to return to the browser. When DNS was launched in the early 1980s as a trusted, content-neutral infrastructure, security was not yet a pressing concern, however, as the Internet grew DNS became vulnerable to various attacks. In this post, we will look at DNS cache poisoning attacks and how Google Public DNS addresses the risks associated with them. DNS Cache Poisoning Attacks DNS lookups in most applications are forwarded to a caching resolver (which could be local or an open resolver like. Google Public DNS). The path from a client to the resolver is usually on a local network or can be protected using encrypted transports like DoH, DoT. The resolver queries authoritative DNS servers to obtain answers for user queries. This communication primarily occurs over UDP, an insecure connectionless protocol, in which messages can be easily spoofed including the source IP address. The content of DNS queries may be sufficiently predictable that even an off-path attacker can, with enough effort, forge responses that appear to be from the queried authoritative server. This response will be cached if it matches the necessary fields and arrives before the authentic response. This type of attack is called a cache poisoning attack, which can cause great harm once successful. According to RFC 5452, the probability of success is very high without protection. Forged DNS responses can lead to denial of service, or may even compromise application security. For an excellent introduction to cache poisoning attacks, please see “An Illustrated Guide to the Kaminsky DNS Vulnerability”. Cache poisoning mitigations in Google Public DNS Improving DNS security has been a goal of Google Public DNS since our launch in 2009. We take a multi-pronged approach to protect users against DNS cache-poisoning attacks. There is no silver bullet or countermeasure that entirely solves the problem, but in combination they make successful attacks substantially more difficult. RFC 5452 And DNS Cookies We have implemented the basic countermeasures outlined in RFC 5452 namely randomizing query source ports and query IDs. But these measures alone are not sufficient (see page 8 of our OARC 38 presentation). We have therefore also implemented support for RFC 7873 (DNS Cookies) which can make spoofing impractical if it’s supported by the authoritative server. Measurements indicate that the DNS Cookies do not provide sufficient coverage, even though around 40% of nameservers by IP support DNS Cookies, these account for less than 10% of overall query volume. In addition, many non-compliant nameservers return incorrect or ambiguous responses for queries with DNS Cookies, which creates further deployment obstacles. For now, we’ve enabled DNS Cookies through manual configuration, primarily for selected TLD zones. Case Randomization (0x20) The query name case randomization mechanism, originally proposed in a March 2008 draft “Use of Bit 0x20 in DNS Labels to Improve Transaction Identity”, however, is highly effective, because all but a small minority of nameservers are compatible with query name case randomization. We have been performing case randomization of query names since 2009 to a small set of chosen nameservers that handle only a minority of our query volume. In 2022 we started work on enabling case randomization by default, which when used, the query name in the question section is randomized and the DNS server’s response is expected to match the case-randomized query name exactly in the request. For example, if “ExaMplE.CoM” is the name sent in the request, the name in the question section of the response must also be “ExaMplE.CoM” rather than, e.g., “example.com.” Responses that fail to preserve the case of the query name may be dropped as potential cache poisoning attacks (and retried over TCP). We are happy to announce that we’ve already enabled and deployed this feature globally by default. It covers over 90% of our UDP traffic to nameservers, significantly reducing the risk of cache poisoning attacks. Meanwhile, we maintain an exception list and implement fallback mechanisms to prevent potential issues with non-conformant nameservers. However we strongly recommend that nameserver implementations preserve the query case in the response. DNS-over-TLS In addition to case randomization, we’ve deployed DNS-over-TLS to authoritative nameservers (ADoT), following procedures described in RFC 9539 (Unilateral Opportunistic Deployment of Encrypted Recursive-to-Authoritative DNS). Real world measurements show that ADoT has a higher success rate and comparable latency to UDP. And ADoT is in use for around 6% of egress traffic. At the cost of some CPU and memory, we get both security and privacy for nameserver queries without DNS compliance issues. Summary Google Public DNS takes security of our users seriously. Through multiple countermeasures to cache poisoning attacks, we aim to provide a more secure and reliable DNS resolution service, enhancing the overall Internet experience for users worldwide. With the measures described above we are able to provide protection against passive attacks for over 90% of authoritative queries. To enhance DNS security, we recommend that DNS server operators support one or more of the security mechanisms described here. We are also working with the DNS community to improve DNS security. Please see our presentations at DNS-OARC 38 and 40 for more technical details. No comments : Post a Comment    Labels   Archive  Feed Follow @google Follow Give us feedback in our Product Forums. Google Privacy Terms",
    "commentLink": "https://news.ycombinator.com/item?id=39960125",
    "commentBody": "Google Public DNS's approach to fight against cache poisoning attacks (googleblog.com)185 points by tatersolid 22 hours agohidepastfavorite45 comments LinuxBender 21 hours agoI see most of the bot traffic hiding behind Google and Cloudflare. I appreciate that. They take a lot of load off my servers. If I had one request of Google it would be to remove the TTL cap of 21600 or raise it to 86400 to further reduce the traffic that comes through from them as my TTL records are very high on purpose for my own reasons that I will not debate. I know memory is still a limit. CF seem to honor my TTL's with the caveat I have to wait for each node behind their anycast clusters to cache the record but that's fine. As a side note, whatever group in Google are running DNS are doing a great job. I do not see malformed garbage coming from them. Some people watch birds when they retire, I watch packets ... and critters. reply freedomben 15 hours agoparent> my TTL records are very high on purpose for my own reasons that I will not debate. Why are your records high? Is the load on your servers too intense otherwise? (not trying to debate, am curious because I've really appreciated your comments/insights in the past) reply oooyay 10 hours agorootparentAsk and you shall receive: https://news.ycombinator.com/item?id=21436448 reply contingencies 15 hours agorootparentprevGuess - possibly some sort of outbound port-knock alternative: if (SYN->dest port && TTL > X) { ACK }; reply ThePowerOfFuet 11 hours agorootparentHow is this supposed to work, exactly? reply contingencies 5 hours agorootparenthttps://en.wikipedia.org/wiki/Port_knocking https://wiki.archlinux.org/title/Port_knocking reply egberts1 13 hours agorootparentprevthis reply yuliyp 10 hours agoparentprevA recursive resolver is not a database. They're under no obligation to cache for a day. reply Waterluvian 8 hours agorootparentOkay stupid question… I thought TTL was a count of hops, not time in seconds. Or do people use it differently where instead of subtracting 1, their routing devices subtract n seconds? reply yuliyp 8 hours agorootparentTTL is a general term standing for \"time to live\". In the context of an IP packet it's a number of hops. In the context of a cache, it's a time until expiration. reply ongy 3 hours agorootparentIP is weird. TTL is still in seconds, but every hop has to decrease it by at least one. I'm not sure if there is any implementation out there that cares, but if e.g. wifi retries or a massive buffer queue lead to a packet spending more than a second on a hop, it should decrease by two. reply Waterluvian 8 hours agorootparentprevDerp. Right. We’re talking about caching. reply WirelessGigabit 20 hours agoparentprevI aspire to be you when (if) I retire. reply mjl- 18 hours agoprevWondering what prompted the blog post. The recent publication of RFC 9539? It would be interesting to hear how often the google dns servers see attempts to poison their cache. The mitigations probably prevent folks from even trying, but he numbers would be interesting. The OARC 40 presentation PDF mentions cookies deployment is low for large operators but open source software has compliant implementations. Are large operators writing their own dns servers, but badly? I would think there wouldn't be many custom implementations, and that you would be able to detect which software nameservers are running, each with known capabilities. But from the way the numbers are presented it seems they only look at behaviour without considering software (versions). reply mike_d 6 hours agoparent> Are large operators writing their own dns servers, but badly? Yes. It is trivial to build a DNS server, and near impossible to write a correct DNS server. Eventually your organization gets large enough that someone thinks it is a good idea without understanding the implications. > you would be able to detect which software nameservers are running, each with known capabilities There are pseudo-standards for asking an authoritative server what software it is running, but everyone turns that off because somehow it makes you \"more secure.\" What you end up having to do is probe auth servers by replaying user queries on the side, measuring if the responses you get are correct, and then keeping a database somewhere of which servers support which flags. reply ape4 20 hours agoprevGood to hear the world's infrastructure relies on a kludge like case randomization. reply akira2501 14 hours agoparentThat's how you maintain backwards compatibility while improving security. I'm not sure lamenting the imperfection is valuable, but it is a worthwhile lesson for those designing new protocols. If it becomes popular enough you will certainly face future security challenges you failed to even imagine. Leave some room for that. Otherwise, this is great work. reply bradfitz 17 hours agoparentprevLonger domain names are more secure! reply jamespwilliams 16 hours agorootparentAnd conversely, short domains like Google’s own g.co are less secure! reply rainsford 18 hours agoprevI'm a little surprised Google only implemented case randomization by default in 2022 considering it's been around since 2008. Presumably they had concerns about widespread compatibility? Although my understanding is that for a lot of DNS servers it just worked without any specific implementation effort...but maybe there was a long tail of random server types Google was concerned about. reply spydum 18 hours agoprevThis is so weird to see. Just this morning I was checking thru my public authoritative NS query logs, and noticed the random capitalization. I had also noticed this in a similar work environment roughly end of 2023, but attributed it to people just doing DNS wordlist bruteforcing to find stuff (couldn't explain the case, but figured it was some evasion). Today I let my curiosity dive deeper and quickly found the ietf publication on 0x20 encoding and this article. Just odd to see others post it to hn on the same day.. coincidences are weird. reply MarkSweep 13 hours agoprevThe that the longer your domain name, the less susceptible is it to cache poisoning attacks, right? Since there are more possible case variations. reply AndyMcConachie 20 hours agoprevWeird that they don't even mention that Google's public DNS performs DNSSEC validation. It does, and that's the ultimate defense against cache poisoning attacks. reply tptacek 20 hours agoparentNo, it obviously isn't, because less than 5% of zones are signed, and an even smaller fraction of important zones (the Moz 500, the Tranco list) are. That's why they don't mention DNSSEC: because it isn't a significant security mechanism for Internet DNS. It's also why they do mention ADoT: because it is. I think this is also why DNSSEC advocates are so fixated on DANE, which is (necessarily) an even harder lift than getting DNSSEC deployed: because the attacks DNSSEC were ostensibly designed to address are now solved problems. Note also that if ADoT rolls out all the way --- it's already significantly more available than DNSSEC! --- there won't even be a real architectural argument for DNSSEC anymore, because we'll have end-to-end cryptographic security for the DNS. Thanks for calling this out! That Google feels case randomization is more important than DNSSEC is indeed telling. reply teddyh 7 minutes agorootparentMany years ago, when HTTPS adoption was in the 5% range, if someone would have said “HTTPS is the ultimate defense against web page spoofing”, would you have argued that it was false, just because the adoption was so low? DNSSEC is the ultimate defense against cache poisoning attacks, no matter the adoption percentage. reply Hazelnut2465 19 hours agorootparentprev> there won't even be a real architectural argument for DNSSEC anymore ADoT relies on NS records to be DNSSEC signed. The TLS certificates that ADoT relies on need to be hashed into TLSA records (DANE, DNSSEC). reply tptacek 15 hours agorootparenthttps://educatedguesswork.org/posts/dns-security-adox/ reply Hazelnut2465 14 hours agorootparentAnd? The IETF RFC draft for ADoT still specifies that it relies on DNSSEC. https://datatracker.ietf.org/doc/draft-dickson-dprive-adot-a... reply tptacek 9 hours agorootparentCheck out the DPRIVE working group (ekr's comments in particular) for some of the backstory. DNSSEC isn't happening either way, but I think ADOX might. reply tialaramex 19 hours agorootparentprev> the attacks DNSSEC were ostensibly designed to address are now solved problems. Maybe you can help figure out where you went wrong here by explaining what - in your understanding - were the problems that DNSSEC was \"ostensibly designed to address\" ? > because we'll have end-to-end cryptographic security for the DNS. In 1995 a researcher who was annoyed about people snooping his passwords over telnet invented a protocol (and gave away a free Unix program) which I guess you'd say delivers \"end-to-end cryptographic security\" for the remote shell. Now, when you go into a startup and you find they've set up a bunch of ad hoc SSH servers and their people are just agreeing to all the \"Are you sure you want to continue ..?\" messages, do you think \"That's fine, it's end-to-end cryptographic security\" ? Or do you immediately put that on the Must Do list for basic security because it's an obvious vulnerability ? reply 8organicbits 18 hours agoparentprevFor end users, TLS is the key protection. I don't care if my DNS is poisoned, MITMed, or malicious: if the IP address I connect to can't present a valid TLS cert, then I don't proceed. If you can't securely authenticate your server (as HTTPS/TLS does) you have other problems too. reply singpolyma3 15 hours agorootparentUnfortunately it's quite easy for some actors to get a valid TLS cert https://notes.valdikss.org.ru/jabber.ru-mitm/ reply tsimionescu 3 hours agorootparentQuite astonishing that someone managed to get valid certs from Let's Encrypt for domains that they didn't own. Has Let's Encrypt issued any statements about how this might have happened, and how those specific certificates were validated by them? Still, good to see that monitoring the CT logs would have caught this problem much sooner. reply phicoh 14 hours agoparentprevDNS is where the web would have been if browsers basically didn't force websites to support HTTPS. The reasoning is that DNS is not important enough to go through the trouble of deploying DNSSEC. These days TLS is often cited as the reason DNSSEC is not needed. At the same time we see a lot of interest in techniques to prevent cache poisoning and other spoofing attacks. Suddenly in those cases DNS is important. If all DNS client software would drop UDP source port randomization and randomized IDs, then lots of people would be very upset. Because DNS security was more important than claimed. DNS cookies are also an interesting case. They can stop most cache poisoning attacks. But from the google article, big DNS authoritatives do not deploy them. reply tptacek 14 hours agorootparentThe key thing to note is that anti-poisoning countermeasures deployed at major authority servers scale to provide value without incurring cost for every (serverside) Internet \"user\", and DNSSEC doesn't. A lot of these thing seem like (really, are) half-measures, but their cost/benefit ratio is drastically different, which is why they get rolled out so quickly compared to DNSSEC, which is more akin to a forklift upgrade. reply phicoh 13 hours agorootparentThere is no quickly in the Google article. It took them ages to roll out 0x20. Cookies are not very well supported. And then the elephant in the room is the connection between the stub resolver and the public DNS resolvers. The interesting thing is what happens when BGP is used to redirect traffic to DNS servers: https://www.thousandeyes.com/blog/amazon-route-53-dns-and-bg... reply tptacek 12 hours agorootparentDid it take 25 years? That's the baseline. :) reply throwaway458864 12 hours agorootparentprevIf we didn't have the web, all networking above OSI L4 on all operating systems would have been encrypted by default. A simple set of syscalls and kernel features could have enabled it. But since the web was there, and popularized a solution for secure communications (TLS + HTTP), everyone just jumped on that bandwagon, and built skyscrapers on top of a used books store. The weird irony is it's the old \"worse is better\" winning again. HTTP and TLS are fairly bad protocols, in their own ways. But put them together and they're better than whatever else exists. It's just too bad we didn't keep them and ditch the browser. reply phicoh 2 hours agorootparentAssuming you are talking about IPSEC, that uses a model that is very hard to deploy. The problem is that applications typically use TCP connections, but IPSEC works at the IP level. Early on, the (BSD socket) kernel API was basically fixed at the IP level instead of associating it with a TCP socket. So the whole thing became too complex (also for other reasons). So SSL and SSH were created to have simple things that worked. SSL took many iterations to get any kind of security, so IPSEC had plenty of time to get it right and take over. But as far as I know, there just never happened. It also doesn't help that TLS is trivial to combine with NAT, and for IPSEC that is quite tricky. reply tsimionescu 3 hours agorootparentprevIsn't the Linux kernel at least very unhappy with the idea of adding encryption logic inside, especially in a way where they expose this to user space? reply tptacek 12 hours agorootparentprevCan you articulate what you believe is bad about TLS? reply LinuxBender 20 hours agoparentprevJust guessing but it could be the lack of adoption. Despite having climbed rapidly in the last few years [0] the percentage is still very low. [1] [0] - https://www.verisign.com/en_US/company-information/verisign-... [1] - https://www.statdns.com/ reply omoikane 16 hours agorootparentThe low adoption of DNSSEC might be due to posts like these: https://news.ycombinator.com/item?id=36171696 - Calling time on DNSSEC: The costs exceed the benefits (2023) And also many news regarding validation failures: https://hn.algolia.com/?q=dnssec reply tptacek 15 hours agorootparentThe rabbit hole on people gradually pulling up stakes on DNSSEC goes deeper than that; I'd say the canary in the coal mine is probably Geoff Huston switching from \"of course we're going to DNSSEC everything\" to \"are we going to DNSSEC anything?\": https://www.potaroo.net/ispcol/2023-02/dnssec.html (Geoff Huston is an Internet infrastructure giant.) But really it all just boils down to the fact that the DNS zones that matter --- the ones at the busy end of the fat tail of lookups --- just aren't signed, despite 25 years of work on the standard. IPv6 is gradually mainstreaming; in countries where registrars auto-sign zones, DNSSEC is growing too, but very notably in countries where people have a choice, DNSSEC deployment is stubbornly stuck in the low single digit percentages, and the zones that are getting signed are disproportionately not in the top 10,000 of the Tranco list. reply jeffbee 17 hours agoprev [–] I first heard about this 0x20 scheme around 2015 when I was working on a DNS cache (also at Google, but not for the public DNS team). I noticed and had to work around the fact that some servers were responding in vixie-case even when the requests were not. Those servers would be broken if the requests were paying attention to 0x20, right? I wonder what software was doing that. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google Public DNS blog highlights the significance of security in the Domain Name System (DNS) and their strategies to combat cache poisoning attacks.",
      "Implemented measures include DNS Cookies, case randomization, and DNS-over-TLS to enhance user protection and thwart potential attacks.",
      "The goal is to offer a more secure and dependable DNS resolution service for global users."
    ],
    "commentSummary": [
      "Google Public DNS is actively fighting cache poisoning attacks using TTL records and security protocols in collaboration with Cloudflare.",
      "The effectiveness of DNSSEC and ADoT in preventing attacks is debated, discussing adoption rates and cost-effectiveness against other security solutions.",
      "Geoff Huston's change in promoting DNSSEC is highlighted due to deployment challenges and validation issues in the DNS security landscape."
    ],
    "points": 185,
    "commentCount": 45,
    "retryCount": 0,
    "time": 1712490415
  },
  {
    "id": 39959790,
    "title": "Enhance Text Analysis with SentenceTransformers Framework",
    "originLink": "https://www.sbert.net/index.html",
    "originBody": "Sentence-Transformers Star Overview Installation Quickstart Pretrained Models Pretrained Cross-Encoders Publications Hugging Face 🤗 Usage Computing Sentence Embeddings Semantic Textual Similarity Embedding Quantization Semantic Search Retrieve & Re-Rank Clustering Paraphrase Mining Translated Sentence Mining Cross-Encoders Image Search Training Training Overview Loss Overview Matryoshka Embeddings Adaptive Layers Multilingual-Models Model Distillation Cross-Encoders Augmented SBERT Training Datasets Training Examples Semantic Textual Similarity Natural Language Inference Paraphrase Data Quora Duplicate Questions MS MARCO Unsupervised Learning Unsupervised Learning Domain Adaptation Package Reference SentenceTransformer util quantization Models Losses Evaluation Datasets cross_encoder Sentence-Transformers » SentenceTransformers Documentation Edit on GitHub SentenceTransformers Documentation¶ SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. The initial work is described in our paper Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similarity, semantic search, or paraphrase mining. The framework is based on PyTorch and Transformers and offers a large collection of pre-trained models tuned for various tasks. Further, it is easy to fine-tune your own models. Installation¶ You can install it using pip: pip install -U sentence-transformers We recommend Python 3.8 or higher, and at least PyTorch 1.11.0. See installation for further installation options, especially if you want to use a GPU. Usage¶ The usage is as simple as: from sentence_transformers import SentenceTransformer model = SentenceTransformer(\"all-MiniLM-L6-v2\") # Our sentences to encode sentences = [ \"This framework generates embeddings for each input sentence\", \"Sentences are passed as a list of string.\", \"The quick brown fox jumps over the lazy dog.\" ] # Sentences are encoded by calling model.encode() embeddings = model.encode(sentences) # Print the embeddings for sentence, embedding in zip(sentences, embeddings): print(\"Sentence:\", sentence) print(\"Embedding:\", embedding) print(\"\") Performance¶ Our models are evaluated extensively and achieve state-of-the-art performance on various tasks. Further, the code is tuned to provide the highest possible speed. Have a look at Pre-Trained Models for an overview of available models and the respective performance on different tasks. Contact¶ Contact person: Tom Aarsen, tom.aarsen@huggingface.co Don’t hesitate to open an issue on the repository if something is broken (and it shouldn’t be) or if you have further questions. This repository contains experimental software and is published for the sole purpose of giving additional background details on the respective publication. Citing & Authors¶ If you find this repository helpful, feel free to cite our publication Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks: @inproceedings{reimers-2019-sentence-bert, title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", author = \"Reimers, Nils and Gurevych, Iryna\", booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\", month = \"11\", year = \"2019\", publisher = \"Association for Computational Linguistics\", url = \"https://arxiv.org/abs/1908.10084\", } If you use one of the multilingual models, feel free to cite our publication Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation: @inproceedings{reimers-2020-multilingual-sentence-bert, title = \"Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\", author = \"Reimers, Nils and Gurevych, Iryna\", booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\", month = \"11\", year = \"2020\", publisher = \"Association for Computational Linguistics\", url = \"https://arxiv.org/abs/2004.09813\", } If you use the code for data augmentation, feel free to cite our publication Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks: @inproceedings{thakur-2020-AugSBERT, title = \"Augmented {SBERT}: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks\", author = \"Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes and Gurevych, Iryna\", booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\", month = jun, year = \"2021\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/2021.naacl-main.28\", pages = \"296--310\", } Overview Installation Install SentenceTransformers Install PyTorch with CUDA support Quickstart Comparing Sentence Similarities Pre-Trained Models Training your own Embeddings Pretrained Models Model Overview Semantic Search Multi-Lingual Models Image & Text-Models Other Models Pretrained Cross-Encoders MS MARCO SQuAD (QNLI) STSbenchmark Quora Duplicate Questions NLI Publications Hugging Face 🤗 The Hugging Face Hub Using Hugging Face models Sharing your models Sharing your embeddings Additional resources Usage Computing Sentence Embeddings Prompt Templates Input Sequence Length Storing & Loading Embeddings Multi-Process / Multi-GPU Encoding Sentence Embeddings with Transformers Semantic Textual Similarity Embedding Quantization Binary Quantization Scalar (int8) Quantization Additional extensions Demo Try it yourself Semantic Search Background Symmetric vs. Asymmetric Semantic Search Python util.semantic_search Speed Optimization Elasticsearch Approximate Nearest Neighbor Retrieve & Re-Rank Examples Retrieve & Re-Rank Retrieve & Re-Rank Pipeline Retrieval: Bi-Encoder Re-Ranker: Cross-Encoder Example Scripts Pre-trained Bi-Encoders (Retrieval) Pre-trained Cross-Encoders (Re-Ranker) Clustering k-Means Agglomerative Clustering Fast Clustering Topic Modeling Paraphrase Mining Translated Sentence Mining Marging Based Mining Examples Cross-Encoders Bi-Encoder vs. Cross-Encoder When to use Cross- / Bi-Encoders? Cross-Encoders Usage Combining Bi- and Cross-Encoders Training Cross-Encoders Image Search Installation Usage Examples Training Training Overview Network Architecture Creating Networks from Scratch Training Data Loss Functions Evaluators Loading Custom SentenceTransformer Models Multitask Training Adding Special Tokens Best Transformer Model Loss Overview Loss modifiers Distillation Commonly used Loss Functions Matryoshka Embeddings Use Cases Results Training Inference Code Examples Adaptive Layers Use Cases Results Training Inference Code Examples Multilingual-Models Available Pre-trained Models Usage Performance Extend your own models Training Data Format Loading Training Datasets Sources for Training Data Evaluation Citation Model Distillation Knowledge Distillation Speed - Performance Trade-Off Dimensionality Reduction Quantization Cross-Encoders Examples Training CrossEncoders Augmented SBERT Motivation Extend to your own datasets Methodology Scenario 1: Limited or small annotated datasets (few labeled sentence-pairs) Scenario 2: No annotated datasets (Only unlabeled sentence-pairs) Training Citation Training Datasets Datasets on the Hugging Face Hub Training Examples Semantic Textual Similarity Training data Loss Function Natural Language Inference Data SoftmaxLoss MultipleNegativesRankingLoss Paraphrase Data Datasets Training Pre-Trained Models Work in Progress Quora Duplicate Questions Pretrained Models Dataset Usage Training MultipleNegativesRankingLoss MS MARCO Bi-Encoder Cross-Encoder Cross-Encoder Knowledge Distillation Unsupervised Learning Unsupervised Learning TSDAE SimCSE CT CT (In-Batch Negative Sampling) Masked Language Model (MLM) GenQ GPL Performance Comparison Domain Adaptation Domain Adaptation vs. Unsupervised Learning Adaptive Pre-Training GPL: Generative Pseudo-Labeling Package Reference SentenceTransformer util quantization Models Losses Evaluation Datasets cross_encoder Next © Copyright 2024, Nils Reimers • Contact Built with Sphinx using a theme provided by Read the Docs.",
    "commentLink": "https://news.ycombinator.com/item?id=39959790",
    "commentBody": "SentenceTransformers: Python framework for sentence, text and image embeddings (sbert.net)181 points by tosh 23 hours agohidepastfavorite53 comments anon373839 20 hours agoThese are extremely useful embedding models, and some are small enough to use in the frontend (with transformers.js) for on-device semantic search. One issue I’ve run into is that they produce a very good ranking, but the actual cosine similarity scores appear to be meaningless. (E.g., “chocolate chip cookies” and “PLS6;YJBXSRF&/” could have a similarity of 0.8.) Consequently, I’ve had a hard time selecting sensible cutoff values to balance recall and precision. Has anyone found a good approach to this? reply refibrillator 18 hours agoparentEven if you choose a more sophisticated similarity measure as suggested by another commenter, you’ll still need to set a threshold on that metric to perform binary classification. In my experience there are two paths forward, the one I recommend is to train an MLP classifier on the embeddings to produce a binary classification (ie similar or not). The advantage is that you no longer need to set a numeric threshold on a distance metric, however you will need labeled training data to define what is “similar” in the context of your use case. The other path is to calculate the statistics of pair-wise distances for every record in some unlabeled dataset you have, and use the resulting distribution to inform choice of threshold. This will at least give you an estimate of what % of records will be classified as similar for a given threshold value. reply whakim 18 hours agorootparentI’ve had a lot of success training a classifier on top of these embeddings. An MLP works well, as does an SVM. If you don’t have labeled data, I’ve also found that various active learning techniques can produce extremely strong results while only requiring a small amount of human labeling. reply lmeyerov 12 hours agorootparentYep, we do precisely this, st + SVM => pre filtering / routing => RAG pipelines for some of louie.ai In our cases,I haven't noticed this behavior in such extreme ways as you. I should have clarified that it really depends on the model. I have had this problem to a greater extent with the GTE and BGE embeddings. But I use them anyway, because they’re so strong overall. PCA is an interesting idea, and worth looking at. reply fermisea 11 hours agoparentprevBootstrap a pvalue by creating a set of thousands of random words, calculate the metric for those and either explicitly keep these numbers to compute the rank, or fit some normal distribution and use this mean and std to estimate the probability that it's similar. reply refulgentis 12 hours agoparentprevSomething is off if you're seeing behavior like that, 0.1-0.15 with MiniLM L6 V3 are good for \"has any relevancy whatsoever\" reply anon373839 11 hours agorootparentI should have added that it depends on the model. MiniLM didn’t exhibit this behavior, but it unfortunately didn’t perform as well on recall or ranking as other models that did. GTE comes to mind. You can try the demo widget on HuggingFace and see this: https://huggingface.co/thenlper/gte-large As an example, against “Chocolate chip cookies”, “Oreos” has a cosine similarity of .808, while “Bubonic plague” is at .709. reply refulgentis 10 hours agorootparentAmazing lol: for what it's worth, it's hugely, hugely, shocking to me how little anyone outside sentence-transformers bothers noting whether their embedding model is for asymmetric vs. symmetric search. i.e. in all seriousness, afaik, 90% of people are using the \"wrong\" embeddings, and either handwave about it, don't care, or think it's being picky. So of course I want to reach to blame that here. I'm doing a cross-platform app pipeline that basically search API => get pages => embed => run against prompt, so unfortunately I don't have much more insight. MiniLM V3 is good enough, the nearest constraint at this point is more \"will I download all 10 search results in time?\" than \"am I grabbing the absolutely best passages?\" -- good on you for legitimately testing those things. edit: actually, I do have one more thought: sometimes I end up with embeddings for the last part of a web page that match ~anything. i.e. imagine i have a 1005 word web page and split it into 250 word chunks to embed. Then I'll have 4 \"full\" vectors, and one that represents just 5 words. That 5 word tends to match almost ~anything, i.e. I'll see ones from previous search queries match the current search query. Maybe you're seeing noise because its just 3-5 words? But then again, you were probably just illustrating without splatting in huge chunks of texts. reply anon373839 8 hours agorootparentThat’s a good point! Actually I’ve been using these in the asymmetric search setting, and the queries are very short phrases (not full questions like you might see in a RAG application). reply ilaksh 20 hours agoparentprevmaybe the normalize_embeddings flag on encode? reply Clueed 19 hours agorootparentNot sure about the specific implementation here but the very definition of cosine similarity includes normalization. [0] [0] https://en.wikipedia.org/wiki/Cosine_similarity reply PaulHoule 21 hours agoprevI use these all the time. For many of the classification tasks I do it works very well to pass an image or text through an embedding and then apply some kind of classical machine learning like the SVM. Model training is super reliable and takes maybe 3 minutes to train multiple models and cross-validate. In maybe 45 minutes I can train a single fine-tuned model but the results are really hit and miss. reply spxneo 15 hours agoparentwould love to try this out myself. seems like there are lighter solutions than relying a giant LLM reply gregw134 13 hours agoprevHow are you guys deciding what parts of a document to turn into embeddings? I've heard paragraph embeddings aren't that reliable, so I'm planning on using tf-idf first to extract keywords from a document, and then just create embeddings from those keywords. reply nestorD 13 hours agoparentTake your document, cut it (cleanly!) into pieces small enough to fit into your sentence embedder's context window, and generate several embeddings that all point to the same document. I would recommend against merging (averaging, etc.) the embeddings (unless you want a blurry idea of what your document contains), as well as feeding very large pieces of text to the embedder (some models have massive context lengths, but the result is similarly vague). reply gregw134 12 hours agorootparent> [recommend against] feeding very large pieces of text to the embedder Sounds right, I've heard this from multiple sources. That's why I'm leaning towards just embedding the keywords. reply therealdrag0 12 hours agoparentprevKeyword would leave you with semantics of word definitions but lose sentence meaning/context right? reply gregw134 12 hours agorootparentI'm sure it would lose a ton of meaning, but for me it's easier to fit into a traditional search pipeline. reply nmstoker 22 hours agoprevShould probably have \"(2019)\" appended to the title as per HN conventions, given the date in the citation paper links (1x 2019 and 2x 2020) and the fact that this site has been around for quite a few years... reply zwaps 21 hours agoparentIt should be mentioned that the original author Nils Reimers has moved on and the repo has been stale since 2021/2022. It has recently (e.g. end of 2023) gotten a new team and has since seen updates. This is obviously quite significant given how important sentence embedding models are. reply estreeper 18 hours agoprevI'm curious how people are handling multi-lingual embeddings. I've found LASER[1] which originally had the idea to embed all languages in the same vector space, though it's a bit harder to use than models available through SentenceTransformers. LASER2 stuck with this approach, but LASER3 switched to language-specific models. However, I haven't found benchmarks for these models, and they were released about 2 years ago. Another alternative would be to translate everything before embedding, which would introduce some amount of error, though maybe it wouldn't be significant. 1. https://github.com/facebookresearch/LASER reply VHRanger 17 hours agoparentThe transformer models handle multilingual directly. For good old embedding models (eg. GLoVe), you have a few choices: 1. LASER as you mentioned. The performance tends to suck though. 2. Language prediction + one embedding model per supported language. Libraries like whichlang make this nice, and MUSE has embedding modes aligned per language for 100ish languages. Fastembed is a good library for this. Note that for most people, 32 dimension glove is all they need if you benchmark it. As the length of the text you're embedding goes up, or as the specificity goes up (eg. You have only medical documents and want difference between them) you'll need richer embeddings (more dimensions, or a transformer model, or both) People never benchmark their embeddings and I find it incredible how they end up with needlessly overenginneered systems. reply gregw134 13 hours agorootparentAny idea which model has the best performance across languages? I'm checking out model performance on the Huggingface leaderboard and the top models for English aren't even in the top 20 for Polish, French and Chinese reply VHRanger 13 hours agorootparentDepends on what your usecase is. For the normal user that just wants something across languages, the minilm-paraphrase-multilingual in the OP library is great. Of you want better than that (either bigger model, or specifically for a subset of languages, etc.) then you need to think about your task, priorities, target languages, etc. reply andai 18 hours agoprevWhat is everyone using embeddings for, and which mdels? I built a RAG last year (for searching long documents) but found it a bit disappointing. I tested it with OpenAI and with SentenceTransformers (instructor-xl or a smaller variant). Apparently they've come a long way since then though. Currently I'm working on an old fashioned search engine (tf-if + keyword expansion). Apparently that can work better than vector databases in some cases: https://news.ycombinator.com/item?id=38703943 reply deepsdev 19 hours agoprevI have used this library for a few years and is reliable. As someone mentioned, sometimes 2 things that are not related can have the same cosine similarity. Easy to use and get started with. reply itissid 16 hours agoprevI recall paragraph2Vec. It was the earliest way to experiment with position embeddings that I recall reading. At first when I read it, it felt kind of crazy/weird that it worked: You feed in the paragraph ID(yes just an integer of the para where the word was) via its own layer along with the word to CBOW/SkipGram set up. Then throw away that part after training. Then, during inference, you attach a new randmly initialized layer for where that old layer was and \"re-train\" just that part before generating the embedding for words. reply edshiro 20 hours agoprevI don't have much experience with embeddings... Could someone more knowledgeable suggest when it would make sense to use the SentenceTransformers library vs for instance relying on the OpenAI API to get embeddings for a sentence? reply montebicyclelo 20 hours agoparentIt's fairly easy to use, not that compute intensive (e.g. can run on even a small-ish CPU VM), the embeddings tend to perform well and you can avoid sending your data to a third party. Also, there are models fine tuned for particular domains on HF-hub, that can potentially give better embeddings for content in that domain. reply estreeper 18 hours agorootparentJust to add to this, a great resource is the Massive Text Embedding Benchmark (MTEB) leaderboard which you can use to find good models to evaluate, and there are many open models that outperform i.e. OpenAI's text-embedding-ada-002, currently ranked #46 for retrieval, which you can use with SentenceTransformers. https://huggingface.co/spaces/mteb/leaderboard reply edshiro 20 hours agorootparentprevI see - thanks for the clarifications I presume if your customers are enterprise companies then you may opt to use this library vs sending their data to OpenAI etc. And you can get more customisation/fine-tuning from this library too. reply VHRanger 17 hours agoparentprevYou should basically never use the openAI embeddings. There isn't a single usecase where they're better than the free models, and they're slower, needlessly large, and outrageously expensive for what they are. reply rolisz 20 hours agoparentprevUp until a month ago, the OpenAI embeddings where very poor. But they recently released a new model which is much better then they're previous one. Now it depends un specific usecase (domain, language, length of texts) reply tinyhouse 20 hours agoparentprevEmbeddings is one of those things that using OpenAI (or any other provider) isn't really necessary. There are many small open source embedding models that perform very well. Plus, you can finetune them on your task. You can also run locally and not worry about all the constraints (latency, rate limits etc) of using an external provider endpoint. If performance is important for you, then you'll need a GPU. The main reason to use one of those providers is if you want something that performs well out of the box without doing any work and you don't mind paying for it. Those companies like OpenAI, Cohere and others, already did they work to make those models work well on various domains. They may also use larger models that are not as easy to deal with yourself. (although as I mentioned previously, a small embeddings model fine-tuned on your task is likely to perform as well as a much bigger general model) reply marban 19 hours agoprevAlso to be used with https://maartengr.github.io/BERTopic/index.html reply gillesjacobs 21 hours agoprevUsed this library quite a bit. I still have no idea if there is a good reason this API is not packaged within huggingface/transformers. Probably historic reasons, anyway solid 9/10 API. reply jacky2wong 8 hours agoprevI love this library - I'm surprised cohere doesn't jump in and have its logo in the front! reply mapmeld 20 hours agoprevMajor kudos to this library for supporting Matryoshka/nested/adaptive embeddings (https://huggingface.co/blog/matryoshka), which I needed to train a model recently reply sroussey 6 hours agoparentIs there an open source Matryoshka embedding model? reply mapmeld 4 hours agorootparentThese are the models on HF which have \"Matryoshka\" in the name, such as the PubMed one; I worked on a protein sequence one https://huggingface.co/models?search=matryoshka reply andai 18 hours agoprevHas anyone used FlagEmbedding? I'm testing a model that comes with examples for both SentenceTransformers and FlagEmbedding, but it's hard to find any information about it. reply spxneo 15 hours agoprevhow are people using this with/without LLM? is the feature more of \"reliable and accurate without hallucination\" than LLM? where are you using it? text interface? reply riku_iki 18 hours agoprevit doesn't look they fine-tune those models on any of modern foundational models, which likely produces huge performance gap compared to OpenAI embedding for example.. reply Der_Einzige 16 hours agoprev [–] >tfw it's 2024 and people STILL aren't using span compression to implement a \"medium term\" memory (RAG is long term and the context length is short term) for LLMs. >tfw it's 2024 and we just accept that the context \"falls out\" of the model if we push it beyond it's regular context length So everyone forgot that we can put large N number of tokens into small N number of embeddings because??? reply register 15 hours agoparent [–] What do you mean by span compression? We have experimented with various embedding contes lengths and we have found that bigger embeddings aren't the ones providing the best recall. We have hit the best results with something between 65% to 75% of the maximum embedding context length. We have been using OpenAI embeddings models though. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "SentenceTransformers is a Python framework that creates advanced embeddings for sentences, text, and images, aiding in tasks like semantic textual similarity and paraphrase mining.",
      "It is built on PyTorch and Transformers, enabling simple fine-tuning of models for various applications.",
      "The framework provides detailed documentation for installation, usage, training, evaluation, and more."
    ],
    "commentSummary": [
      "The text delves into utilizing SentenceTransformers to create embeddings and the difficulties associated with cosine similarity scores.",
      "Recommendations involve training classifiers on these embeddings, studying pair-wise distances, and employing active learning methods.",
      "Varied models such as MiniLM are reviewed for effectiveness, addressing concerns about noise in embeddings and the advantages of open-source tools like SentenceTransformers."
    ],
    "points": 181,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1712485431
  },
  {
    "id": 39964590,
    "title": "Groq Transitions from Hardware Sales to AI Cloud Services",
    "originLink": "https://www.eetimes.com/groq-ceo-we-no-longer-sell-hardware/",
    "originBody": "designLines AI & Big Data Designline Groq CEO: ‘We No Longer Sell Hardware’ By Sally Ward-Foxton 04.05.2024 0 Share Post Share on Facebook Share on Twitter MOUNTAIN VIEW, CALIF.—Groq CEO Jonathan Ross is adamant his company no longer sells hardware—the data center AI chip startup is now an AI cloud services provider. “Long term, we always wanted to go there, but the realization was, you cannot sell chips as a startup, it’s just too hard,” Ross told EE Times in a recent in-person interview. “The reason is the minimum quantity of purchase for it to make sense is high, the expense is high, and no-one wants to take the risk of buying a whole bunch of hardware—it doesn’t matter how amazing it is.” Jonathan Ross (Source: Groq) Groq’s customer is now the AI developer. Following a number of viral social media posts showcasing the latency of its rack-scale AI inference systems, the company currently has 70,000 developers registered for its real-time large language model (LLM) inference cloud service, GroqCloud, with 19,000 new applications running. “You get the sort of developer traction we’ve gotten, and people want to buy hardware, but we are no longer selling hardware, because why would we at this point?” Ross said. “It’s not a pivot—we always intended to have a cloud service, we just expected we would do both.” Partner Content View All Nuvoton drives the EV market with its cutting-edge battery monitoring chipset solution By Nuvoton Technology Corporation Japan 04.03.2024 Improved Power Efficiency and AI Inference in Autonomous Systems By Shingo Kojima, Sr Principal Engineer of Embedded Processing, Renesas Electronics 03.26.2024 Leveraging Advanced Microcontroller Features to Improve Industrial Fan Performance By Dylan Liu, Geehy Semiconductor 03.21.2024 If customers come with requests for high volumes of chips for very large installations, Groq will instead propose partnering on data center deployment. Ross said that Groq has “signed a deal” with Saudi state-owned oil company Aramco, though he declined to give further details, saying only that the deal involved “a very large deployment of [Groq] LPUs.” This strategy from a startup is not entirely unprecedented. AI chip startup Cerebras already partners with G42 to sell excess capacity on G42’s Cerebras-based AI supercomputers—though it is unusual. “The U.S. government and its allies are the only ones we’d be willing to sell hardware to,” Ross said. (Groq’s chips are fabricated and packaged in North America.) “For anyone else, we’re setting up commercial clouds [together], but that’s it.” Ross is optimistic about Groq’s ability to scale out to meet demand, in part because Groq’s chip does not use high-bandwidth memory (HBM). Two of the three HBM makers, SK Hynix and Micron, have said they have sold out their entire 2024 capacity, with Micron even saying recently that 2025’s capacity is almost gone. Competing solutions, including Nvidia GPUs, rely on HBM. “Groq is in a very unusual position because we’re the only ones who not only don’t use HBM, but also have a practical solution where the software works,” he said. “That means we can achieve a level of scale that no-one else can.” A cluster of Groq Racks. (Source: Groq) In terms of scale, Ross said Groq plans to deploy 42,000 language processing unit (LPU) chips itself this year as part of GroqCloud, with Aramco and other partners “in the process of finalizing” their deals for approaching the same number of chips. “We have the capacity to [make] 220,000 LPUs this year, and we are looking for partners to work with us on that,” Ross said. “We have the ability to do 1.5 million by next year [including the 220,000 this year], but only about a million is unallocated at this point, and that is getting rapidly eaten up now that people know we’re doing big deals.” Groq acquired Palo Alto startup Definitive Intelligence last month; the company is an AI-powered business insights firm. Definitive Intelligence CEO Sunny Madra now leads GroqCloud, and the team will focus on expanding capacity, improving efficiency, forming partnerships, and building out the developer platform. By becoming a cloud services provider who also makes its own chips, is Groq effectively trying to mimic the hyperscaler model? “There might need to be a new term, because by the end of next year we’re going to deploy enough LPUs that compute-wise, it’s going to be the equivalent of all the hyperscalers combined,” he said. “We already have a non-trivial portion of that.” In terms of performance, GroqCloud is benchmarked by artificialanalysis.ai at 467 tokens per second for Mixtral 8x7B, while other GPU-based services did not get above 200. Demos for 7B models seen by EE Times went as high as 750 tokens per second. Groq is working on further speedups by implementing features like speculative decoding, where smaller models are used combined with a bigger model to check their output and temperature—a parameter which controls whether the most likely tokens are picked and influences the so-called ‘creativity’ of the model. Groq’s first-gen chip, the LPU. (Source: Groq) Groq, whose first-gen silicon is almost five years old at this point, is also working on its next-gen LPU. “We are taping out this year,” Ross said, noting that Groq gen two will skip several process nodes from 14 nm to 4 nm, so customers should expect a big boost in performance. While the new silicon will be optimized for generative AI via Groq’s in-house design-space exploration tool, it will not have specific features for LLMs, Ross said. Groq has run other types of AI on its LPUs from the start, including CNNs and LSTMs, plus generative AI in multiple modalities. In the mean time, Groq seems to be confident enough with its five-year–old silicon. Following Nvidia’s announcement of the next-gen Blackwell GPU architecture, which promises 30× training performance for generative AI, Groq put out a press release with a simple two-word response: “Still faster.” RELATED TOPICS: AI AND BIG DATA Share this: Twitter Facebook LinkedIn Sally Ward-Foxton Sally Ward-Foxton covers AI for EETimes.com and EETimes Europe magazine. Sally has spent the last 18 years writing about the electronics industry from London. She has written for Electronic Design, ECN, Electronic Specifier: Design, Components in Electronics, and many more news publications. She holds a Masters' degree in Electrical and Electronic Engineering from the University of Cambridge. Follow Sally on LinkedIn 0 comments Post Comment Leave a Reply You must Register or Login to post a comment. This site uses Akismet to reduce spam. Learn how your comment data is processed.",
    "commentLink": "https://news.ycombinator.com/item?id=39964590",
    "commentBody": "Groq CEO: 'We No Longer Sell Hardware' (eetimes.com)162 points by frozenport 11 hours agohidepastfavorite118 comments geor9e 8 hours agoI don't understand why the comments are trash-talking Groq. They are the fastest LLM inference provider by a big margin. Why would they sell their hardware to any other company for any price? Keep it all for themselves and take over the market. 95% of my LLM requests go to Groq these days because it's 0.25 seconds round trip for a complete answer. In comparison, \"Claude Instant\" takes about 4 seconds. The other 5% of my requests go to Claude Opus and GPT-4, when I'm willing to wait an excruciating 5+ seconds for a better answer. I hate waiting. Latency is king. Groq wins. reply fitzn 8 hours agoparentWhat open source model are you using when you hit groq? I just benchmarked some perf for some of my larger context window queries last week and groq's API took 1.6 seconds versus 1.8 to 2.2 for OpenAI GPT-3.5-turbo. So, it wasn't much faster. I almost emailed their support to see if I was doing something wrong. Would love to hear any details about your workload or the complexity of your queries. reply bee_rider 4 hours agorootparentWhat context did I miss that implies they are using an open source model? reply vineyardmike 1 hour agorootparentIf you go to GroqChat (which is like a demo app), they offer Gemma, Mistral, and LLaMa. These are all open-weights models. reply laserbeam 5 hours agorootparentprev> 1.6 vs 1.8-2.2 seconds I believe certain companies would kill for 20% performance improvements on their main product. reply gpapilion 4 hours agorootparentI have lots of questions about how important latency is since you may be replacing many minutes or hours of a person’s time with undoubtedly a quicker response by any measure. This seems like a knee jerk reaction assuming latency is as important as it’s been with advertising. I’m not convinced latency matters as much as groqs material tries to claim it does. reply verdverm 30 minutes agorootparentGoogle won search in large part because of their latency. I stopped using local models because of latency. I switched from OpenAI to VertexAI because of latency (and availability) reply michaelt 50 minutes agorootparentprevDepends on your application. For example, if you're a game company and you want to use LLMs so your players can converse with nonplayer characters in natural language, replacing a multiple-choice conversation tree - you'd want that to be low latency, and you'd want it to be cheap. reply w-ll 3 hours agorootparentprevWhen has latency ever not mattered? Let alone 'chat' use cases, but holding a reponse up for N*1.2 longer than it could holds all sorts of other resources up/down stream. reply ben_w 1 hour agorootparentWhen it's already faster than I can absorb the response, which for me as an organic brain includes the normal token generation rate of the free tier of ChatGPT. If I was using them to process far more text, e.g. summarise long documents, or if I was using it as an inline editing assistant, then I'd care more about the speed. reply qeternity 1 hour agorootparent> When it's already faster than I can absorb the response Streaming a response from a chatbot is only one use-case of LLMs. I would argue the most interesting applications do not fall into this category. reply ben_w 44 minutes agorootparentNumber of different use cases (categories) I'd agree; I'm not so sure about use (volume)… …not yet anyway. Fast moving area, lots of blue water outside the chat interface. reply frozenport 3 hours agorootparentprevI guess its tool calling? When you chain the LLMs together? reply robrenaud 4 hours agorootparentprevModel quality matters a ton too. They aren't serving OpenAI or Anthropic models, which are state of the art. reply verdverm 28 minutes agorootparentResearch suggest most answers and use cases do not require the largest, most sophisticated models. When you start building more complex systems, the overall time increases from chaining and you can pick different models for different points reply metadat 4 hours agorootparentprev\"kill\", .. why would anyone kill for a fraction of a second in this case? Informed folks know that LLM hosters aren't raking in the big bucks. They're selling dreams and aspirations, and those are what's driving the funding. reply vineyardmike 2 hours agorootparentGoogle has used LMs in search for years (just not trendy LLMs), and search is famously optimized to the millisecond. Visa uses LMs to perform fraud detection every time someone makes a transaction, which is also quite latency sensitive. I'm guessing \"informed folks\" aren't so informed about the broader market. OpenAI and Anthropic's APIs are obviously not latency-driven. Same with comparable LLM API resellers like Azure. Most people are likely not expecting tight latency SLOs there. That said, chat experiences (esp. voice ones) would probably be even more valuable if they could react in \"human time\" instead of with few seconds delay. Integrating specialized hardware that can shave inference to fractions of a second seems like something that could be useful in a variety of latency-sensitive opportunities. Especially if this allows larger language models to be used where traditionally they were too slow. reply EVa5I7bHFq9mnYK 46 minutes agoparentprev>> why the comments are trash-talking Groq they probably bought NVDA stock :) reply huac 8 hours agoparentprevwhy don't you stream the results? reply tpetry 2 hours agorootparentYou still have to wait for the end of the streamed response until you can continue with your task. reply latchkey 9 hours agoprevInteresting, I guess that is why I never got a response back from them about buying their stuff. My guess is that they realized that just selling hardware is a lot harder than running it themselves. Deploying this level of compute is non-trivial, with very high rates of failure, as well as huge supply chain issues. If you have to sell the hardware and support people buying it, that is a world of trouble. > no-one wants to take the risk of buying a whole bunch of hardware I do! Nobody has stated it yet, but this is probably great news for tenstorrent. Disclosure: building a cloud compute provider starting with AMD MI300x, and eventually any other high end hardware that our customers are asking for. reply gpapilion 4 hours agoparentIt’s basically their minimum cluster size for a reasonable model requires 8ish racks of compute. Semi analysis did some cost estimates, and I did some but you’re likely paying somewhere in the 12 million dollar range for the equipment to serve a single query using llama-70b. Compare that to a couple of gpus, and it’s easy to see why they are struggling to sell hardware, they can’t scale down. Since they didn’t use hbm, you need to stich enough cards together to get the memory to hold your model. It takes a lot of 256mb cards to get to 64gb, and there isn’t a good way to try the tech out since a single rack really can’t serve an LLM. The cloud provider path sounds riskier since that’s two capital intensive businesses, chip design and production and running a cloud service provider. reply latchkey 3 hours agorootparentThis is some fantastic insight. That would also inflate the opex (power/space/staffing) needs as well as people need to consider all the networking gear to hook this stuff together. 400G nic/switches/cables aren't cheap and in some cases, very hard to obtain in any sort of quantity. It does seem like an odd move in that case. I liken this to a company like Bitmain. Why sell the miners when you could just run them yourselves? Well, fact is that they do both. But in this case, Groq is turning off the sales. Who knows, maybe it just ends up being a temporary thing until they can sort all of the pieces out. reply ukd1 5 hours agoparentprevHow do y'all compare to https://tensorwave.com? reply latchkey 5 hours agorootparentTo be totally honest, I have no idea. When I first learned about them, I reached out to the CEO privately on LinkedIn, he asked what I was up to, I told him probably more than I should have (I come from an open source and transparent background), then he stopped talking to me entirely. Since then, one of the co-founders blocked me on Twitter for pointing out that despite their claims, they were not the first MI300x to production. Neither were we, ElioVP gets that trophy, then Lamini, then GigaIO. Making us 4th and them 5th. I could go on and on with weird stuff I've seen them do, but it just isn't productive here. Anyway, I think we have some overlap since we both are one of the few startups on the planet that actually has MI300x. But beyond that, I believe strongly that this space is large enough for multiple players and I don't see a need to be weird with each other. Apparently, I'm not on the same page though. ¯\\_(ツ)_/¯ reply ametrau 5 hours agorootparentFor what it’s worth, to me, your approach is the one I’d prefer as a customer. reply latchkey 4 hours agorootparentThank you. I've been on HN since 2009. The most successful people I've seen here, are the ones that are transparent, honest and ethical. I'm not trying to point fingers, I'm just focused on building a sustainable business and listening to my customers needs. The only way I can do that is by communicating with everyone around me as clearly and openly as I can. All our customers will know exactly where they stand, at all times. I post a lot of open information on r/AMD_Stock and the feedback that I've gotten there has been exceptional. People are excited to see if AMD can claw back a bit of the market. For the safety and success of AI, we don't need team blue vs. team red, we need everyone to work towards having as many options as possible. This is one way that I think we are going to differentiate ourselves. We won't just have MI300x, we will have every best-of-the-best chunk of hardware that we can get our hands on. No longer will super computers be tied up behind govt/edu grants. We want to democratize it. It has long been a goal of mine to build a super computer, and here is my chance. I'm excited. One thing that sets us apart is that my co-founder and I have a ton of experience deploying, managing and optimizing 150,000 AMD GPUs and 20PB+ of storage. We did it ourselves, all through covid and all of the supply chain issues. I'm not sure many others have done that and this is something that we are well versed at doing. I'm also seeing my competitors hiring a ton, while we are staying lean and mean with a very small team. I'd rather automate everything we deploy and focus all of our investors money on buying compute. We also have a pool of previous people we can hire from, which I think is quite an advantage over blanket hiring. reply elbear 4 hours agorootparentJust wanted to say that I like how you see things. Also, if you need help with infrastructure automation and are interested in using Nix for increased reproducibility, get in touch. I know you said you want to keep lean and already have a pipeline of potential candidates, but just in case. reply latchkey 4 hours agorootparentHi Lucian, thank you for stepping up to the plate. Recognized and appreciated. We aren't quite in the hiring phase today, still bootstrapping and focused on growing with customer demand, but I have absolutely added you to my list for the future. Cheers! reply elbear 3 hours agorootparentSure. Thank you for answering. Good luck with what you're doing! reply aleph_minus_one 9 hours agoparentprev> If you have to sell the hardware and support people buying it, that is a world of trouble. What is the difference between this and having to sell the cloud access and supporting the people who buy a subscription? reply alephnerd 9 hours agorootparent> What is the difference between this and having to sell the cloud access and supporting the people who buy a subscription Margins. Pricing for cloud compute is much higher and servicing and management for the provider is much cheaper. If I sold hardware directly, then I'm often on the hook for support contracts which can get pricy with hardware and distract from shipping future facing product features, as customers who purchase directly have longer upgrade windows due to logistical overhead. reply latchkey 9 hours agorootparentThe pricing isn't necessarily much higher. The pricing is set up to amortize the cost of running things over a period of time. If you're only going to use it for a short time, you pay more than someone who commits to a 3 year contract. It also isn't just the hardware capex, it is everything involved under the covers. Market pricing also factors into that as well. This is something I've struggled with myself quite a bit. I know all of my costs and what I'd like to charge, but because my offering is so brand new, until my competitors announced their pricing, I wasn't sure what the market would tolerate. Selling hardware directly is hard for exactly what you state though. Service contracts are a pain in the butt. All of this latest AI hardware has high rates of failures too. Up until recently with AMD coming to market with a great offering, the only thing people want any sort of quantity on are nvidia products. Groq probably realized that people buying 1-2 cards at a time, wasn't going to be profitable. reply alephnerd 9 hours agorootparent> it is everything involved under the covers Yep! You explained it better than me! > Groq probably realized that people buying 1-2 cards at a time, wasn't going to be profitable Yep! And if would have bogged them down by slowing down R&D cycles and even fulfilling orders as they obviously are not placing orders the same size as Nvidia or AMD. I wonder what this portends for SambaNova and other similar vendors as well. reply latchkey 9 hours agorootparent> I wonder what this portends for SambaNova and other similar vendors as well. Time will tell. This is definitely an interesting development. reply htrp 7 hours agorootparentprev>I wonder what this portends for SambaNova and other similar vendors as well. They're focused on services based full stack deployments afaik. They come in with a rack and sell you on models as well. reply themoonisachees 9 hours agorootparentprevA lot. It's the same reason amazon doesn't sell servers and instead gives you access to a single instance that everyone pretends is the same but in reality is massively transient. reply latchkey 4 hours agorootparentWe give full bare metal access. If you just want one GPU, we give you a VM with PCIe pass through. If you take a whole box, we can give BMC and can give you access to the PDU itself, to hard reboot things remotely. It is as if you own the whole machine yourself. reply ethbr1 8 hours agorootparentprev> What is the difference between this and having to sell the cloud access and supporting the people who buy a subscription Knowledge/training. If you're shipping a brand new hardware arch, exposed as raw hardware, then you're on the hook for training everyone in the world and fixing all their weird edge case uses. I.e. are you willing to invest in Intel/AMD/Nvidia-scale QA and support? If you're exposing a PaaS (or even IaaS), then you have some levers you can tweak / mask behind the scenes, so only your team need be experts at low-level operations. For a fast-paced company, the latter model makes a lot more sense, at least until hardware+software stabilizes. reply latchkey 6 hours agorootparent> I.e. are you willing to invest in Intel/AMD/Nvidia-scale QA and support? At least in our experience, the first line of support is from the chassis vendors. You don't go to a store and buy MI300x. You buy them from someone like SMCI/Dell, who provides the support. Of course, behind the scenes, they might be talking to AMD. Even those chassis companies often have other providers of their gear (like Exxact) as another line of defense as well. In the case of Groq, it would have been death by 1000 cuts to have to support end users directly, especially if they are selling small quantities. It is much easier to just build data centers full of gear, maintain it yourself and then just rent the time on the hardware. reply latchkey 9 hours agorootparentprevGood question. Not much. If anything, what I'm doing is even harder because I will have multiple sources for the hardware. I have to deal with all of the hardware and data center issues, as well as the customers who rely on us to provide them access. Good thing that I'm a glutton for punishment. reply winwang 4 hours agoparentprevHow's the overall software support for MI300 series? The hardware itself looks great. (also, +100 to valuing honesty and transparency) reply latchkey 4 hours agorootparentThe hardware is actually pretty amazing. 192GB (or 1.5TB in a chassis), is a game changer. I'll let you know once I get my hands on them again. There really isn't enough public information about them at all. So far, my friends at ElioVP [0] have published a blog post. Still with not enough detail for my taste, but I'm pretty sure he is limited by what he can talk about. Luckily, I am not. I mention in another comment below that my current goal is to get a bunch of people to perform testing on them and then publish blog posts along with open source code. This way, we can start a repository of CI/CD tests to see how things improve with time. ROCm 6.1 is rumored to be quite an improvement. [0] https://www.evp.cloud/post/diving-deeper-insights-from-our-l... reply winwang 4 hours agorootparentNice! I'm pretty interested in GPGPU applications and MI300A, but I'm also just glad for more competition. Love that you hit up the LocalLLaMa sub. Do you know if anyone's tested CuPy stuff on MI300X? reply latchkey 4 hours agorootparentWe haven't spec'd to buy A's quite yet as you're actually the first person I've heard even suggest them. If you're truly interested, hit me up personally. By default, we are putting dual 9754's in the chassis, along with 3TB ram and 155TB nvme. A pretty beefy box. However, if you want to work with us, we can customize this to whatever customers need. Effectively, we are the capex/opex for something that requires a lot of upfront funding and want to work with businesses that would rather focus on the software side of things. reply winwang 3 hours agorootparentWas mostly just checking to see if someone had already tested GPGPU, though I know some HPC labs like the MI300A. While I am starting a business, I'm not at the point of shipping software just yet (I wish!). Will definitely keep you in mind for if/when we get to AMD -- it's something I'd want, though that depends on achieving any modicum of success, haha. reply rnts08 9 hours agoparentprevThat's interesting, something that I've been really wanting to get into as well, but where I am there is literally no venture capital to raise for this at the moment. I'd be interested to know more and/or bounce some ideas though. reply latchkey 9 hours agorootparentExtremely capital intensive, but also requires relationships in the industry at many levels. Luckily, I happen to have both and they are crazy enough to put their trust in me. I feel very grateful for that. reply htrp 9 hours agoparentprevthat or if you put chips in the hands of your customers, they may start to benchmark it against other equivalent solutions reply latchkey 9 hours agorootparentFunny you should mention that. ;-) https://www.reddit.com/r/LocalLLaMA/comments/1bpgrdf/wanted_... I've got about a dozen people signed up. Just working through some hardware issues right now (see above about high rate of failures), and hope to have this resolved next week, so that I can get people onto them and doing their testing. reply htrp 7 hours agorootparentShiny! You have you guys gotten MI300X's working for non-inference use cases? reply latchkey 6 hours agorootparentWe got them, then gave them to a customer to use for a week, got them back and now we are having some hardware issues that we are in the process of sorting out. I literally haven't had more than a few hours time on them yet. They _should_ work fine for both training and inference, but since nobody has done much in the way of public in-depth benchmarks yet... I was hoping to get people to do it for us in order to stay as unbiased as possible. noticing now: strange that my previous comment was downvoted. Would be nice to understand what someone didn't like about what I said! reply gandalfgeek 10 hours agoprevThey're calling the lie on needing bleeding edge hardware for performance. 5 yr old silicon (14 nm!!) and no hbm. Their secret sauce seems to be an ahead-of-time compiler that statically lays out entire computation, enabling zero contention at runtime. Basically, they stamp out all non-determinism. https://wow.groq.com/isca-2022-paper reply ipsum2 9 hours agoparentIt's not really a lie though. They require 20x more chips (storing all the weights in sram instead of hbm is expensive!) than Nvidia GPUs for a ~2x speed increase. Overall the power cost is more expensive for groq than GPUs. reply ethbr1 8 hours agorootparentAre power and 20x 14-nm chip capacity limiting factors currently? It's not inconceivable that's a better trade-off than leading-node and HBM requirements. reply ipsum2 3 hours agorootparentprevEdit: 200x more chips, not 20x. reply rattray 9 hours agorootparentprevSource? Or how do you know that? reply wmf 8 hours agorootparentThis was discussed extensively in previous threads, e.g. https://news.ycombinator.com/item?id=39431989 reply halflings 10 hours agoparentprevNo HBM because they use tons of fast SRAM instead. Isn't that the main driver for performance here? (the way I understood it => it's still cost effective at scale due to throughput increase this brings) reply gandalfgeek 9 hours agorootparent> No HBM because they use tons of fast SRAM instead. Isn't that the main driver for performance here? No doubt fast SRAM helps, but from a computation pov imho its that they've statically planned computation and eliminated all locks. Short explainer here: https://www.youtube.com/watch?v=H77tV1KcWIE (Based on their paper). reply germanjoey 9 hours agorootparentprevcost effective in what sense? groq doesn't achieve high efficiency, only low latency. but that's not done in a cost-effective way. compare sambanova achieving the same performance with 8 chips instead of 568, and with higher precision. reply torginus 3 hours agoparentprevI wonder if the use of eDRAM (https://en.wikipedia.org/wiki/EDRAM), which is essentially embedding DRAM into a chip made on a logic process would be a good idea here. EDRAM is essentially a tradeoff between SRAM and DRAM, offering much greater density at the cost of somewhat worse throughput and latency. There were a couple of POWER cpus that used EDRAM as L3 cache, but it seems to have fallen out of favor. reply grandmczeb 2 hours agorootparentIt fell out of favor because it lost the density advantage in newer processes. reply dralley 10 hours agoparentprevSo Itanium and its \"sufficiently smart compiler\" but functional? reply ethbr1 8 hours agorootparentFrom skimming the link above, it seems like they accepted it's extremely difficult (maybe impossible) to generate high ILP from a VLIW compiler on complex hardware (what Itanium tried to do). So they attacked the italicized portion and simplified the hardware. Mostly by eliminating memory-layer non-determinism / using time-sync'd global memory instructions as part of the ISA(?). This apparently reduced the difficulty of the compiler problem to something manageable (but no doubt still \"fun\")... and voila, performance. reply anon291 7 hours agorootparentThe problem set groq is restricted to (known size tensor manipulation) lends itself to much easier solutions than full blown ILP. The general problem of compiling arbitrary Turing complete algorithms to the arch is NP hard. Tensor manipulation... That's a different story. reply alted 9 hours agoprevCustom state-of-the-art silicon is ridiculously expensive. For a minimum 100 wafers = 10k chips, Groq may have paid $100M = $10k/chip purely in amortizing design costs. Chip design (software + engineer time) and fabrication setup (lithography masks) grow exponentially [1][2] with smaller nodes, e.g., maybe $100M for Groq's current 14nm chips to ~$500M for their planned 4nm tapeout. Once you reach mass production (>>1000 wafers, which have ~150 large chips each), wafers are $10k each. On top of this, it takes ~1 year to design then have prototypes made. (These same issues still exist on older slower nodes, albeit not as bad.) This could be reduced somewhat if chip design software were cheaper and margins were lower, but maybe 20% of this cost is due to fundamental manufacturing difficulty. (disclosure: I don't work with recent tech nodes myself; this is my best guess) [1] https://www.semianalysis.com/p/the-dark-side-of-the-semicond... [2] https://www.extremetech.com/computing/272096-3nm-process-nod... reply latchkey 4 hours agoparent> Custom state-of-the-art silicon is ridiculously expensive. Think about the amount of money being dumped into \"AI\" at this point. If you've got the technology and people to make stuff faster/better/cheaper, finding investors to dump money into your chip making business is probably not as hard as it was 2 years ago. Groq is making this change for other reasons than the expense of tapping out chips. reply shrubble 8 hours agoparentprevThe report I read said that latest TSMC is 17K per wafer. How much less it is for 14nm I don't know. reply karma_pharmer 8 hours agorootparentThe masks are the expensive part, not the wafers. reply thrtythreeforty 5 hours agorootparentThey are both fabulously expensive. reply mlazos 9 hours agoprevThe smoke and mirrors around groq are finally clearing. Truth is that their system is insanely expensive to maintain. hundreds (> 500 iirc) of chips to get wild tokens/s but the power and maintenance expense is crazy high for that number of chips. TCO just isn’t worth it reply ein0p 5 hours agoparentYou don't know that. For one thing, their silicon costs are going to be relatively cheap. It's an old reliable, 14nm process, and compared to even Google's TPU this is a relatively simple chip. For another they _could_ be putting all that silicon to a good use, and by all indications they are. Because there's far less local memory movement, and weights are distributed throughout the system, even this 14nm system could be energy efficient. 9/10ths of all power in a conventional system does not go towards compute - it's wasted in moving data back and forth. This is especially bad in transformers, which, because of their size, largely defeat the memory hierarchies the architects worked so hard to perfect. IOW, all your caches are useless and you're unnecessarily wasting 90% of your energy while also getting worse latency and worse throughput (due to memory bus bandwidth constraints). Oops. These folks seem to be offering something that nobody else does - a feasible, proven way to get out of jail free. I wish them all the success they can get, because all the other currently available architectures are largely unsuitable for high throughput transformer inference, and they work in spite, instead of because, of their design. reply mlazos 3 hours agorootparentPeak H100 power consumption is 700W. Average power consumption of the groq card (from their own website) is 240W. With 576 chips it just doesn’t look good. How much is that millisecond perf gain worth it to end users? That said I think their arch is super interesting. I just think that demo was way too hype when the actual system is pretty impractical. reply ein0p 2 hours agorootparentSo? They aren't performing the same computation. You can't compare the two. What you can compare is power draw at an equivalent tokens/sec on the same model for the entire system. But you don't have that number. reply Oribi 3 hours agoparentprevWhy would they want to run it themselves if the TCO didn’t work out reply b-side 2 hours agorootparentBecause they rather operate at a loss with high revenue rather than have 0 revenue and loss? reply mrkeen 2 hours agorootparentprevI thought that was par for the course these days. Operate at a loss. Get a big valuation. Cash out. reply rnts08 9 hours agoprevSounds like they're looking to get bought up to me. I'm sure they could monetize their current hardware, and build to sell just like other niche hardware vendors. Anyone remember the hype around big \"cloud\" storage boxes 10 years back? reply ilaksh 6 hours agoprevI'm not able to get consistent replies from the API. It's lightening fast for like ten minutes and then starts freezing up for several seconds. I want to use it, but it's been very unreliable. I have been using Claude 3 and thinking about together.ai with Mixtral. reply QuadrupleA 5 hours agoparentSame, it's great when it's quick / available, but they seem underprovisioned for busy times and I often get long 10-30 second stalls. reply Havoc 9 hours agoprevGiven that their hardware is different I can kinda see how they don’t want to deal with supporting customers. > what do you mean I can’t just drop a CUDA docker image in? reply htrp 9 hours agoparentif you're a hardware startup that doesn't sell hardware, what are you? reply aleph_minus_one 9 hours agorootparent> if you're a hardware startup that doesn't sell hardware, what are you? A hardware startup that sells cloud access to its hardware. :-) reply Havoc 2 hours agorootparentprevHardware setup that produces superior hardware and extracts the benefit in house ? reply dsrtslnd23 10 hours agoprevSo unless there are new Croq datacenters coming, this is only interesting for North American users. Otherwise H100 based latency optimized solutions would be faster - in particular for time-to-first-token sensitive applications. reply LoganDark 9 hours agoparent> latency optimized solutions would be faster - in particular for time-to-first-token sensitive applications Do you have any idea how fast Groq is? Go try it. Consistently over 400 t/s for most of the models that they support, and extremely low latency. reply huac 8 hours agorootparenttime to first token != tokens per second remember that EU -> US is ~150ms unavoidable latency, for example. then your comparison is local H100 vs Grok + 150ms latency to first token. reply nl 4 hours agorootparentI'm in Australia. I have 249ms of unavoidable latency and I'd still use the groq API if I could. It's that much faster than other inference solutions. reply LoganDark 7 hours agorootparentprev> time to first token != tokens per second I said \"and extremely low latency\" because I know they are different. Groq's TTFT is still consistently competitive with any other provider, and lower than most of them. Here's some benchmarks: https://github.com/ray-project/llmperf-leaderboard#70b-model... reply zetazzed 9 hours agoprevMan, I want to appreciate a nice new hardware approach, but they say such BS that it is hard to read about them: > “There might need to be a new term, because by the end of next year we’re going to deploy enough LPUs that compute-wise, it’s going to be the equivalent of all the hyperscalers combined,” he said. “We already have a non-trivial portion of that.” Really? Does anyone seriously believe they are going to be the equivalent of all hyperscalers in compute next year? (Where Meta alone is at 1 million H100 equivalents.) In the same article where they say it's too hard for them to sell chips? And when they literally don't have a setup to even accept a credit card today? reply wmf 8 hours agoparentYou don't put a million-dollar rack on a credit card. I'm not sure they want retail customers for their API either. reply scosman 10 hours agoprevWildly fast inference. And current chips are 14nm so headroom to get a lot better. reply jsheard 10 hours agoparentNote that SRAM density doesn't scale at the same rate as logic density, and Groqs \"secret sauce\" is putting a ton of SRAM on their chips. Their stuff won't necessarily see the full benefits of switching to denser nodes if the bottleneck is how much SRAM they can pack onto each chip. https://www.tomshardware.com/news/no-sram-scaling-implies-on... IIRC the last big jump for SRAM density was at 7nm, so they do still have that card to play, but progress has slowed to a crawl beyond that. TSMC 3nm SRAM is barely denser than TSMC 7nm SRAM. reply LoganDark 9 hours agoprevThat sucks. I wanted to save up for a couple years and get some hardware for home, but I guess the \"AI\" space moves so fast you barely get a couple months reply wmf 8 hours agoparentSave up for Tenstorrent instead. reply LoganDark 6 hours agorootparentI'll look into it, though seeing \"contact us\" always makes me think they're not going to sell a single unit to a home user. (With that said, Groq probably wouldn't either. You can technically buy LPUs for 20k each, without an expectation of support, but it takes tens of them to run Mixtral.) Tenstorrent also looks incredibly Python-specific (as in, everything including their SMI seems mostly Python-based) which doesn't seem promising? reply ojn 4 hours agorootparentMost of the low-level pieces are in Rust, the TUI is written in Python and most of the remaining pieces are getting lowered down to the Rust libraries over time. (It was all Python up until ~6 months ago) EDIT: Oh, and you can buy the Grayskull cards online now, without contacting anyone. reply latchkey 3 hours agorootparentEven talking to people there, my experience is that they are super nice! reply LoganDark 3 hours agorootparentprev> EDIT: Oh, and you can buy the Grayskull cards online now, without contacting anyone. I actually don't mind having to contact, I only mind if they won't want to sell to me due to being a non-bulk order. > Most of the low-level pieces are in Rust That's awesome! reply sipjca 5 hours agorootparentprevfwiw as a consumer I have a tenstorrent card in my machine reply latchkey 4 hours agorootparentthat is great. please hit me up privately, i'd love to chat with you about it. reply creato 9 hours agoprev> If customers come with requests for high volumes of chips for very large installations, Groq will instead propose partnering on data center deployment. Ross said that Groq has “signed a deal” with Saudi state-owned oil company Aramco, though he declined to give further details, saying only that the deal involved “a very large deployment of [Groq] LPUs.” What? How does this make sense? reply shrubble 8 hours agoparentOil & gas has large data needs, they had petabyte-scale data 2 decades ago. reply theturtletalks 9 hours agoparentprevIf you read on, Groq said they would only sell hardware to US companies and outside companies would get cloud services, not the LPUs. I think the US government told them to keep the LPUs in-house since they could be the secret sauce for scale. reply creato 9 hours agorootparentI'm not questioning the deployment strategy, I'm wondering why Saudi Aramco wants to access so much compute power that is highly specialized(?) for generative AI workloads. Or is it more general than that? reply thawab 8 hours agorootparentIt’s about material discovery[0], Aramco built a 250b model to help them improve efficiency [1]. [0] https://deepmind.google/discover/blog/millions-of-new-materi... [1] https://www.aramco.com/en/news-media/speeches/2024/leap24--r... reply mike_hearn 59 minutes agorootparentWhat's the connection there? The second link says it's not about materials discovery at all but rather their GigaPOWERS model, which is a physics simulation used to optimize CO2 injection into their fields (i.e. optimizing recovery). POWERS is old, it was in development for decades already. Given that they don't plan to use Groq for LLMs but simply for its parallel computation abilities. I wonder to what extent this deal - if it goes through - will seriously drain Groq, actually, as POWERS would not be like the code Groq was designed to run and so much of their performance comes from the way they tightly optimize for very specific calculations. reply tome 36 minutes agorootparentHi there, I work for Groq. Groq's system was designed to run arbitrary high performance numerical workloads. In the past it has been used for a variety of scientific computation tasks, including nuclear fusion and drug discovery. https://www.alcf.anl.gov/news/argonne-deploys-new-groq-syste... reply aleph_minus_one 9 hours agorootparentprev> I'm wondering why Saudi Aramco wants to access so much compute power that is highly specialized(?) for generative AI workloads. Or is it more general than that? For vanity reasons and because AI is the future (not every company acts that rationally for huge buying decisions). reply brookst 7 hours agorootparentAlso diversification. They’re smart people; oil isn’t the future. Comparatively small investments to hedge make perfect sense. reply theturtletalks 9 hours agorootparentprevMaybe Saudi doesn’t want to rely on OpenAI and other APIs and wants to run a fine-tuned Mixtral model on the cloud or their hardware. International companies will probably opt for an open source model since the data is sensitive and OpenAI could pass that to intelligence. reply recursivecaveat 5 hours agorootparentprevPartly supply-chain security I imagine. If generative AI does indeed become the next big thing much nicer to have a giant pile of hardware physically in your country than buying a drip feed from a foreign company. reply BoorishBears 10 hours agoprevRead: We're forcing someone's hand in acquiring us. Groq is still under a 30 request per minute rate-limit, which drops to 10 requests per minute if you have all day usage. Billing has been \"coming soon\" this whole time, and while they've built out hype enabling features like function calling, somehow they can't setup a Stripe webhook to collect money for realistic rate limits. They couldn't scream \"we can't service the tiniest bit of our demand\" any louder at this point. _ Edit: For anyone looking for fast inference without the smoke and mirrors, I've been using Fireworks.ai in production and it's great. 200 tk/s - 300 tk/s is closer to Groq than it is to OpenAI and co. And as a bonus they support PEFT with serverless pricing. reply arthurcolle 10 hours agoparentthey don't even let us pay them, it's insane I just have free API access with no ability to add a credit card. reply brcmthrowaway 10 hours agoparentprevWhat are you using all this for? Whats the product? reply BoorishBears 10 hours agorootparentI run an AI story telling site and an AI ideation platform. The story telling site alone averaged 27k requests a day this week, so about double what their current request limit is, and honestly not even that popular of a site. You can't run much more than a toy project on their current rate limits. reply karma_pharmer 8 hours agoprev [–] Another casualty of AI KYC. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Groq, previously a data center AI chip startup, now offers AI cloud services through GroqCloud, attracting 70,000 users and hosting 19,000 new applications.",
      "Collaborating with corporations like Saudi Aramco, Groq is expanding its reach and enhancing its services for large-scale implementations.",
      "The company is developing its next-generation chip and striving to compete with major cloud providers in computational efficiency and effectiveness."
    ],
    "commentSummary": [
      "Groq CEO shifts focus away from selling hardware to prioritize speed and low latency in inference processing, earning user accolades, particularly in natural language processing.",
      "Debate ensues on the significance of latency, the role of open-source models, and the value of specialized hardware, amid discussions on challenges in cloud computing sales and competition in the supercomputing sector.",
      "Groq's deployment methods with clients such as Saudi Aramco, scalability concerns, and services availability are examined, with mentions of alternative solutions like Tenstorrent."
    ],
    "points": 162,
    "commentCount": 118,
    "retryCount": 0,
    "time": 1712529650
  },
  {
    "id": 39962184,
    "title": "Xemu: Play Original Xbox Games on Windows, macOS, and Linux",
    "originLink": "https://xemu.app/",
    "originBody": "Original Xbox Emulator A free and open-source application that emulates the original Microsoft Xbox game console, enabling people to play their original Xbox games on Windows, macOS, and Linux systems. Download for Linux Version 0.7.120 (Mar 23, 2024) Alternative download options Previous Next Open Source The source code for xemu is available on GitHub. You are invited to help improve the project! Learn more here. Cross Platform xemu runs natively on Windows, macOS, and Linux platforms. Binaries are available for all platforms, or you can build from source if desired. Learn more here. Low Level Emulation xemu emulates the hardware of the original Xbox, providing superior compatibility with kernels, titles, and homebrew applications. Controller Support Built on SDL2, xemu supports virtually all controllers. Connect up to 4 controllers at any time, just like a real Xbox. Learn more here. Snapshots (Save States) No need to wait for game checkpoints. xemu supports saving the current machine state and loading it back up at any time. Learn more here. Render Scaling Breathe new life into your original Xbox games by easily increasing the resolution that games render at, on the fly. Scale up from 480p to 1080p at the click of a button. Networking Connect to other instances of xemu and real Xboxes, locally or over the Internet. Supports tunneling services and Xbox Live recreation projects. Learn more here. Community xemu has a thriving online community of original Xbox fans. Set up multiplayer matches, get help running xemu, and more by joining our community on Discord! Compatibility Note: Title compatibility status is provided by volunteer reporters in the community, as the reporter experienced the title in the current version of xemu on their computer at time of reporting. As the project evolves, reports may need to be updated. You are invited to help improve the project by submitting an updated compatibility report. Join the Discord server to learn how to contribute! Unknown Broken Intro Starts Playable Perfect Titles 1% 2% 12% 80% 4% Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Perfect Playable Perfect Playable Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Starts Playable Intro Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Perfect Playable Playable Playable Playable Playable Playable Intro Playable Playable Playable Starts Starts Playable Playable Starts Playable Playable Playable Playable Playable Playable Perfect Playable Playable Playable Playable Playable Playable Playable Perfect Playable Playable Playable Starts Perfect Starts Playable Playable Playable Playable Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Playable Starts Playable Playable Playable Playable Perfect Playable Playable Playable Playable Playable Playable Playable Perfect Playable Playable Playable Playable Playable Perfect Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Perfect Playable Playable Playable Playable Starts Playable Playable Playable Playable Perfect Playable Playable Playable Playable Starts Playable Playable Playable Playable Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Starts Playable Playable Playable Playable Playable Starts Playable Playable Playable Starts Perfect Playable Playable Playable Playable Playable Playable Perfect Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Perfect Playable Perfect Starts Starts Playable Playable Playable Playable Intro Intro Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Playable Starts Starts Playable Playable Playable Playable Playable Starts Playable Playable Playable Playable Perfect Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Starts Playable Playable Playable Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Playable Broken Starts Playable Playable Playable Playable Starts Broken Playable Playable Playable Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Playable Playable Starts Playable Playable Playable Playable Playable Starts Playable Playable Intro Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Perfect Playable Playable Playable Playable Playable Playable Playable Playable Perfect Playable Playable Perfect Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Perfect Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Playable Playable Playable Starts Playable Playable Starts Starts Playable Playable Playable Playable Playable Playable Playable Playable Starts Starts Playable Intro Playable Playable Playable Starts Playable Playable Playable Playable Playable Playable Starts Starts Starts Playable Broken Playable Playable Playable Playable Playable Playable Playable Starts Perfect Playable Playable Starts Playable Perfect Perfect Starts Broken Playable Broken Broken Playable Playable Playable Playable Playable Playable Playable Starts Starts Starts Starts Playable Playable Playable Playable Playable Playable Playable Playable Starts Playable Starts Starts Playable Playable Playable Playable Starts Playable Playable Perfect Perfect Playable Playable Playable Playable Starts Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Playable Playable Playable Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Intro Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Playable Playable Starts Starts Playable Playable Playable Playable Playable Perfect Playable Playable Playable Playable Playable Playable Broken Playable Playable Starts Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Intro Playable Playable Perfect Playable Playable Playable Playable Perfect Starts Starts Playable Playable Playable Playable Playable Playable Intro Starts Playable Playable Starts Playable Playable Starts Starts Playable Playable Intro Playable Starts Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Starts Playable Playable Broken Starts Starts Playable Playable Starts Perfect Playable Playable Playable Perfect Playable Playable Playable Intro Playable Starts Playable Playable Playable Playable Intro Playable Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Perfect Starts Playable Playable Perfect Starts Playable Playable Playable Playable Playable Playable Broken Starts Playable Playable Playable Playable Starts Playable Starts Starts Playable Playable Starts Playable Playable Playable Playable Playable Starts Starts Playable Starts Playable Starts Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Playable Starts Playable Playable Playable Perfect Playable Playable Playable Playable Playable Playable Playable Playable Starts Starts Playable Playable Playable Playable Playable Playable Playable Playable Intro Starts Perfect Playable Playable Playable Playable Starts Starts Perfect Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Perfect Starts Playable Playable Playable Playable Playable Playable Playable Intro Intro Playable Playable Playable Perfect Playable Playable Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Starts Playable Playable Playable Playable Playable Playable Perfect Playable Playable Playable Playable Playable Starts Playable Playable Playable Playable Starts Starts Playable Playable Playable Playable Playable Starts Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Starts Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Starts Starts Playable Perfect Playable Broken Playable Playable Starts Playable Playable Playable Playable Playable Starts Perfect Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Starts Playable Intro Perfect Playable Playable Playable Playable Starts Playable Playable Playable Perfect Playable Playable Playable Playable Playable Broken Playable Playable Playable Starts Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Starts Playable Playable Perfect Perfect Playable Perfect Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Playable Perfect Playable Playable Starts Playable Disclaimer: All product names, logos, and brands are property of their respective owners. All company, product and service names and/or images used in this website are for identification purposes only. © 2024 xemu Project",
    "commentLink": "https://news.ycombinator.com/item?id=39962184",
    "commentBody": "Xemu: Original Xbox Emulator (xemu.app)160 points by InitEnabler 16 hours agohidepastfavorite59 comments accrual 15 hours agoFun fact - Morrowind for the Xbox occasionally rebooted the whole console when running low on memory. A splash screen would remain displayed, so the user would have no idea this was happening. The Xbox only had 64MB of DDR1 RAM. https://hackaday.com/2021/04/14/morrowind-rebooted-the-origi... reply userbinator 14 hours agoparentWhen the original Xbox was released (2001), its specs were comparable to a low-end PC of the time, but with a somewhat better GPU. reply sillywalk 10 hours agorootparentGood write up (along with the rest of their articles) on the X-Box architecture. https://www.copetti.org/writings/consoles/xbox/ reply giancarlostoro 14 hours agoparentprevI had talked to a dev who worked on one of the Fable games and he mentioned that he was only alloted x amount of the total memory available on the 360 which surprised me, since I was primarily a PC gamer I didnt think about how little memory the console had. Fun reminder that we got to the moon on significantly less memory. reply rbut 11 hours agoparentprevThey could've implemented their own virtual memory and paged it to disk. Very easy to do. That's what we had to do to load 64mb ROMs when creating an n64 emulator for Xbox. reply darknavi 11 hours agorootparentSure, but deadlines. And game hacks are fun! reply hilsdev 8 hours agorootparentAnd also… Bethesda reply 01HNNWZ0MV43FF 7 hours agorootparentprevI read it was a memory leak reply costanzaDynasty 15 hours agoprevI hate the fact that Xbox One killed Xbox's momentum so much that people are dismissing or not recognizing some of the things that Xbox has going for it right now with Xbox GamePass and cloud gaming. This is a transitional generation, but Xbox is positioned a lot better than most people seem to understand if they only travel in the areas where console wars is the religion of the day again. I've been replaying Halo 1 & 2 again and just remembering the infancy of console online gaming and how fun it was as compared today were its either absolute silence or mute worthy trash. reply anonymousab 15 hours agoparent> recognizing some of the things that Xbox has going for it right now with Xbox GamePass and cloud gaming Or they don't recognize them as inherently good things. Game pass has potentially cannibalized \"real\" game sales on the platform and primed its userbase to \"just wait for it to come to gamepass\"; with the gamepass honeymoon developer deals drying up, this has had dire implications for third party support going forward. Xcloud has been a mixed bag - you can find countless reports of it lagging behind PS cloud streaming and various PC cloud gaming vendors in performance (latency) and image quality (though I personally haven't seen much difference between most of them). For a lot of us, the current generation Xbox platform has been doing everything just as bad or even worse than it was with the Xbox One - they've stopped iterating on backwards compatibility, they've pushed Gamepass above all, and they've spent unbelievable amounts of money on M&A instead of building up their own existing studios and releasing more new original exclusive games. The quality of some of their major trumpeted releases has also been incredibly suspect, despite repeated claims of high quality from their internal tastemakers before release. The dumb social media console war stuff has unfortunately gotten in the way of some important self-review and introspection that the Xbox team should be doing. As a longtime Xbox fan, it has been tremendously disappointing to see. reply dagmx 15 hours agoparentprevThe Xbox team have just had terrible leadership choices. The One was a miss from the beginning , trying to be more than a gaming console and falling short on all fronts. The name didn’t help either. Then they picked an even worse name for the Series X and and Series S. while implementing a necessity for parity. confusing the market and holding back devs. The 360 was effectively a flash in the pan IMHO, helped by a significant misstep on Sony’s part with the PS3 design, and Nintendo moving to create their own market with the Wii that effectively made it a companion console rather than a competing one. reply jwells89 14 hours agorootparentWith the One they also made the mistake of trying to permanently tie discs to consoles in an attempt to destroy the used game market for the console, which got leaked pretty early on and led to Microsoft and the Xbox brand taking a massive reputation hit, even though they quickly backtracked. For that generation, a lot of people who would’ve otherwise bought Xboxes instead bought PS4s out of principle. reply CBarkleyU 12 hours agorootparentThat E3 was crazy. Anyone remember the video that Sony posted on \"how to share games with friends\" while the Xbox debacle on locked discs and always online was in full force? (it was literally a guy giving a game disc to another guy) This is why I am still against the MS acquisitions. Imagine that whole debacle happening but you're still forced to buy an anti consumer console, because God forbid you like to play CoD, Fallout, Starfield... reply Lammy 12 hours agorootparentNow Sony just lock the entire disc drive behind online activation instead: https://www.playstation.com/en-us/support/hardware/ps5-disc-... “Internet connection required to pair disc drive and PS5 console upon set up.” Cue defenders “but it's only once and then it works offline!!” like that will make a difference in the far future once the activation system is dead and gone. reply dagmx 10 hours agorootparentBit disingenuous to compare the two because it’s also only if you’re upgrading from a disc-less model. reply hermitdev 12 hours agorootparentprevAnd others of us are old enough to remember the Sony rootkit DRM on their CDs and refuse to go anywhere near Sony products. reply dagmx 10 hours agorootparentCompletely different product segment though. I don’t think that really had any outsized influence on PlayStation buyers at the time. reply pjmlp 2 hours agorootparentprevAnd now they are playing a SEGA, turning XBox into a brand. I don't believe that they still care about the console as much they say they do. reply TMWNN 10 hours agorootparentprev>The name didn’t help either. It all goes back to Microsoft not naming the 360 \"Xbox 3\" with some lame excuse for why it did so. Yes, everyone would have laughed, but no one would remember or care today that the \"Xbox 5\" isn't actually the fifth Xbox. reply Macha 9 hours agorootparentThe Xbox 360 was only the second xbox. Though at the time we joked they just needed a triangle based word in the name for the follow up and they'll have all the main PS buttons covered in the title. reply TMWNN 9 hours agorootparent> The Xbox 360 was only the second xbox. thatsthejoke.gif reply rasz 13 hours agorootparentprevAnd to top it all off I heard MS just put Surface people in charge of designing the next one, to copy Switch success I guess?. reply dleslie 15 hours agoparentprevIt's funny that you should note that they are doing positive things in a transitional era, only to finish off by noting that you are playing decades-old games on their hardware. While I am a fan of GamePass, and I own two XBox One consoles, I have no desire to own a Series console or the forthcoming refresh. Everything I want to play I can play on the One or via Cloud; but more importantly: the difficulty of discovering compelling experiences in this era of XBox is too damn high. There are too many straight-up _bad_ indie games swamping GamePass, and too many B and AAA quality games that are phoning it in. reply bluescrn 14 hours agoparentprevAny game developer could have told them early on that Kinect was next-to-useless, despite the technology being interesting. Even operating simple menus was problematic. You could point, but had no buttons to click. It was never going to work beyond a few niche cases (dance games and simple minigames). reply musicale 13 hours agorootparent> It was never going to work beyond a few niche cases (dance games and simple minigames). It really is superior for dance games. reply philistine 8 hours agoparentprevIt's not that people are not recognizing the good things about it. There's this underlying theory that we had one chance at a digital game library. The PS3 and Xbox 360 were way too early for people to get attached to their digital purchases; after all, few large titles released digitally day and date with their disc versions. So people bought a few indie titles digitally, but they still had a big disc library. But the PS4 generation was the digital generation. The majority of sales were digital then (probably not dollars-wise, but certainly units-wise). So the theory goes that people have this large lock-in with PS4, which leads to them directly going to the PS5 without looking at the distinguished competition's offerings. I'm not sure I agree. reply andrewmcwatters 13 hours agoparentprevI’m sad for kids who don’t get to experience high-quality couch multiplayer. Also making an account for every little stupid thing. No you don’t need account systems. At least make them opt-in. reply skibz 13 hours agoprevThis emulator has brought me hours of fun. A huge thank you to the developers. I also strongly recommend checking whether Insignia supports the games you're playing on Xemu. If it does, grab a retail dashboard hard disk image and try it out! I think Insignia even recently added support for Halo 2, which is pretty huge. https://insignia.live reply bdcravens 16 hours agoprevThe original Xbox was essentially a PC of the time running a heavily customized version of Windows. Does this emulate that kernel without using any Microsoft code? reply iforgotpassword 16 hours agoparentI think most people familiar with the original Xbox hw (like people working on xemu :)) take offense when you say it's just a PC. It's PC hardware, but nothing much is shared with the \"IBM PC\" architecture-wise. Even the software side is very different. It's not running a customized version of Windows, but the Windows kernel. One stripped down to 256kb, including the boot up animation and sound. That design for example required that a lot of things get linked into a game's executable, like network drivers and the entire TCP/IP stack. If it were that easy, we'd have had a wine pendant for Xbox binaries 20 years ago. Cxbx is pretty much the wine approach to Xbox emulation and it's far from perfect. reply Dwedit 6 hours agorootparent> nothing much is shared with the \"IBM PC\" architecture-wise Gate A20 begs to differ. reply fps_doug 2 hours agorootparentIt's built into the CPU, what did you expect them to do, burn it with a laser? .... well in hindsight that might've been a smart thing to do :o) reply Dwedit 2 hours agorootparentGate A20 famously was responsible for the original XBOX getting hacked. It redirected execution from the special hidden ROM into ordinary flash ROM, bypassing all security. reply anthk 15 hours agorootparentprevLinux would ran on that with ease. And, yes, Wine ran, but just Win32 PE binaries, not XBE ones. OFC it was an \"IBM PC\". Some people could run ReactOS on it. reply bri3d 15 hours agoparentprevThere are two major original Xbox emulators - cxbx/cxbx-reloaded, which originally tried to do both LLE and HLE but these days does exclusively high-level emulation like you are describing, and xemu (this one), which does mostly low-level emulation. So no, in this case Xemu runs the original Xbox kernel. This is why it's more compatible and generally much more robust than cxbx in terms of game support. There are a lot of challenges with doing HLE for Xbox. While there is a kernel, its HAL is very low-level. Games are statically linked against the entire stack of DLLs that you'd usually find in Windows (the Xbox SDK), and depending on when they were released, can have one of about a gazillion versions of the Xbox SDK in them. So doing HLE is a matter of finding and hooking an enormous number of SDK functions, but also keeping up with the plethora of minor changes which were made constantly through the life of the console. Emulating the kernel doesn't get you very far, because it still provides low-level interfaces to (for example) manipulating GPU state, so you end up having to implement the GPU hardware in LLE anyway. reply jsheard 16 hours agoparentprevPer the documentation you have to provide dumps of the original Xbox bootroms, and it can optionally use a HDD image taken from a real Xbox, but the latter isn't required because they provide a basic cleanroom reimplementation of the dashboard you can use instead. https://xemu.app/docs/required-files/ The CPU in the Xbox was literally an off-the-shelf Intel processor (some crazy people even soldered in faster CPUs that were never meant for the Xbox) but as I understand it the GPU is weird, and that's where most of the difficulty with emulation comes from. It's a unique mashup of GeForce 2 and GeForce 3 IP that was never used anywhere else, so the existing efforts to reverse engineer GF2 and GF3 for Linux drivers were of limited use. reply IntelMiner 15 hours agoparentprevThis is a myth that is incorrect. The original Xbox's BIOS bares little to no resemblance to Windows 2000 (its closest contemporary) and runs from a 1MB flash chip at most. Games run Bare Metal on the system reply accrual 15 hours agoparentprevThe Xbox hardware was also quite advanced for its release date (2001). Many PCs at the time still had 100MHz FSB but the Xbox ran at 133MHz. The Xbox also had DDR RAM, not much of it (64MB), but DDR wouldn't be commonplace until the Pentium 4 and Athlon XP platforms a couple years later. Most PCs were still running regular single-channel SDRAM. I've built a near-period correct PC with a later and faster CPU (Tualatin at 1.0GHz vs Xbox's Coppermine at 700MHz), more RAM (512MB), faster GPU (GF4 Ti4400), but it still can't achieve the raw memory and GPU bandwidth the Xbox had. reply jsheard 15 hours agorootparentThe fun thing about the RAM was that although the retail units only had 64MB, they shared the motherboard design with the devkits which had 128MB, and if you sourced a second set of RAM chips you could fully populate the board and build a 128MB retail unit. That was useful for some homebrew, and also made it possible to play games built for the Sega Chihiro arcade platform, which used the Xbox architecture but had the full 128MB of RAM installed and allowed games to use all of it. If you were brave enough to replace a BGA chip you could upgrade the CPU as well, the stock one was an off-the-shelf 733mhz Pentium which happened to have a 1.4ghz counterpart, and surprisingly swapping the CPU out for the faster one mostly just worked. The faster chips were electrically compatible, but Intel only packaged those as socketed desktop parts, so people designed these wacky interposer boards to re-route the pins to the correct places on the Xboxes BGA footprint. https://i.imgur.com/nvJdKfM.jpeg reply jwells89 13 hours agorootparentI love hackery like this that pushes hardware capabilities to their limits. It’s kinda like extreme overclocking, except somehow both more crazy and usually more likely to have an end result that’s practical for day to day usage. reply flohofwoe 16 hours agoparentprevIf I remember right, most of the \"operating system\" was linked statically into the game images (AFAIK at least the DirectX libs, not sure about any low level \"firmware\" stuff). I'm pretty sure there was no concept of \"hardware drivers\" though. reply skinatro 16 hours agorootparentThe original xbox actually had a real os based on the windows 2000[1] unlike it's competitors like the PSX [1]-https://www.theverge.com/2020/5/21/21265995/xbox-source-code... reply dagmx 15 hours agorootparentThe original XBox didn’t compete against the PSX. It was one generation later competing with the PS2, DreamCast and GameCube reply Narishma 14 hours agorootparentThe Dreamcast was discontinued just as the Xbox was preparing to launch, so they didn't really compete. reply skinatro 16 hours agoparentprevIf I am not wrong this is a LLE (low level emulator) rather than a hle so it should emulate the hardware rather than the kernel. So you need the dump of the microsoft's xbox kernel for it to work reply erickhill 4 hours agoprevThat name had been taken... https://65site.de/emulator.php reply sshagent 16 hours agoprevkung fu chaos time! Such a great game, a little insensitive perhaps...glorious sofa multipleyer game though I kept that disk hoping they'd backwards compat it for many years reply blakes 15 hours agoparentOut of all the og Xbox games, Kung Fu Chaos provided the most fun and laughs with friends. It honestly still holds up gameplay wise. I still play it sometimes. The minigames were the best. reply rolandog 15 hours agorootparentFuzion Frenzy as well! reply darknavi 11 hours agorootparentPlaying the Fuzion Frenzy demo on the XB Magazine discs and the Halo demo menu were a ton of fun too. Wish more game demos existed today. reply ChristopherDrum 10 hours agoprevIs there some way to set to an alarm to let me know when Steel Battalion becomes playable? reply christkv 14 hours agoprevI’ve been replaying the thing using xemu and it works very well. reply pipeline_peak 15 hours agoprev [–] With the Master Chief Collection on PC, I’d only use this to play Metal Arms and Mercenaries. The first Xbox had a pretty lukewarm backlog. reply sirwhinesalot 14 hours agoparentIt had loads of good games! Ninja Gaiden, Otogi 1 & 2, Far Cry Instincts, Crimson Skies, Panzer Dragoon Orta, Metal Wolf Chaos, Project Gotham Racing, Jade Empire, Fable, Outrun 2006, Splinter Cell, Burnout, Chronicles of Riddick, Conker, Psychonauts, Unreal Championship 2, Oddworld, Forza, etc. reply fps_doug 2 hours agorootparentNinja Gaiden was so fraking hard! At some point I used an invincibility hack and still gave up at some point! But it was pretty cool. Played Halo 1 & 2, PGR a lot, JSRF was just awesome even though I never finished that either. Fable just wasn't my thing, but had a lot of fun in DoA3 with friends. reply sirwhinesalot 23 minutes agorootparentI managed to beat Ninja Gaiden as a teen. I have no idea how I pulled it off (it wasn't in Ninja Dog mode either). Sadly I no longer have the necessary hand-eye coordination :( reply clircle 12 hours agorootparentprevFor me, it was the timed exclusivity of MGS2 substance reply smrq 1 hour agoparentprevTo me, Jet Set Radio Future was worth buying the entire console back in the day. Halo was just a nice bonus. reply darknavi 11 hours agoparentprev [–] Metal Arms, what a blast from the past! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The xemu project is a free and open-source emulator for the original Xbox that enables users to play Xbox games on Windows, macOS, and Linux.",
      "It offers features like low-level emulation, controller support, save states, render scaling, and networking capabilities, with robust compatibility with the original Xbox hardware.",
      "Users can enhance the project by improving compatibility reports and participating in the lively Discord community."
    ],
    "commentSummary": [
      "The debate centers on original Xbox limitations, compared to modern platforms like Xbox One, and the influence of Xbox GamePass on the gaming community.",
      "Users criticize Xbox for not focusing on backward compatibility updates, overly emphasizing GamePass, and facing reputation issues from leadership decisions.",
      "The discussion delves into the evolution of Xbox consoles, difficulties in emulating the original Xbox hardware, and favorite games, evoking nostalgia and gratitude among users."
    ],
    "points": 160,
    "commentCount": 59,
    "retryCount": 0,
    "time": 1712510317
  },
  {
    "id": 39960788,
    "title": "Homemade Vegemite: A DIY Adventure in Australian Spread",
    "originLink": "https://daveon.design/adventures-making-vegemite.html",
    "originBody": "Adventures Making Vegemite To non-Australians, Vegemite is one of the strangest food products you’re likely to ever encounter. It’s a dark brown, almost black, shiny paste with a pungent scent, and a very strong, almost indescribable taste that is strongly umami and very salty. It is a little like soy sauce in solid form. We tend to eat it on toast for breakfast. The trick is not to spread it thickly like peanut butter; instead, you take about a quarter of a teaspoon’s worth and scrape it thinly over the entire slice. It also goes very nicely in a toasted cheese sandwich. It sounds unique, but perhaps the strangest thing about it is that it is not: the British have Marmite, which tastes similar but is runnier, Australians have a second yeast extract spread called Promite, In my personal opinion, even better than Vegemite. the Germans and Swiss have Cenovits and Vitam-R, I haven’t tried either of these. and a Latvian friend once gave me a jar of a tan-coloured, honey-consistency thick liquid he told me was like Vegemite and I (sadly) found inedible. This leads to spirited debates about which is best and, like a true Australian, I mock Marmite to the British friend with whom I made Vegemite for this article, Marmite is the inferior product: it is not a solid paste but is runny, and has a slightly less strong taste. even as he keeps Marmite in his cupboard and would never touch Vegemite. Both of us quietly admit they’re very similar and I have many times bought Marmite when Vegemite was not available. When you live overseas, you make do with what you can get. In Australia, you’d never admit to eating Marmite. But among fraternity of overseas Aussie citizens, there’s an unspoken knowledge that we’ve all lowered ourselves to Marmite from time to time. But even if thick black pungent salted umami spreads are common, how are they made? And what is such a strange spread really made of? The jar claims to be a ‘yeast extract’, and Aussie rumour had it that it was made from leftovers from making beer. Very Australian. Since it was invented in the 1920s, I personally had a mental image of a specific strain of yeast fermenting to produce some precursor substance, Asimov-style. Then my Marmite-loving British friend sent me this video, From the How To Cook That Youtube channel by Ann Reardon. and one afternoon we made it. By ‘we made it’, it’s more that I’d casually mentioned I’d like to try, and then at the pub the previous evening he said he had all the ingredients and would I like to join him the next afternoon? In other words: his effort, his ingredients, his kitchen, and I took photos. Left to right: Marmite, our spread, our Czech friend's umami stock spread, and some nutritional yeast paste to round out the last slice. Ingredients A homebrewing friend provided: The yeast slurry from the bottom of a batch of beer (from a plain lager) A fermented malt wort made from dry malt mixed with water The malt was very liquid and we boiled it down to a thick syrup. You could likely use supermarket malt. Boiling the skimmed yeast slurry liquid. The yeast slurry in Ann Reardon’s video was separated into yeast and liquids via a home-made washing-machine centrifuge. But our slurry had been sitting in my friend’s fridge for a few days, and he simply poured off the top. This liquid, a pale clear tan, we boiled down into a thick, dark brown syrup too. Initially, the kitchen smelled like hot beer, but it soon changed to smell like something recognisable as a Vegemite-like scent. It was not unpleasant—if you like Vegemite—but it was strong, and at some point despite the winter outside every window in the apartment was opened. Left: the malt reducing to a syrup; right: the slurry liquid also being boiled down. A good half-liter or more of yeast-slurry-liquid turned into only a couple of tablespoons of dense brown syrup. This is definitely a recipe where large starting volumes are needed, and you won’t get a large quantity result. The resulting two syrups: on the left, heavily reduced malt wort to a very dark, sweet, flavourful syrup; on the right, the main Vegemite flavour comes from the boiled-down reduction from the yeast slurry. A good cooking blog would show these two dishes very clean, with no drip remnants, but all this stuff is thick, sticky, and messy. The final major ingredient is nutritional yeast. We mixed several tablespoons with warm water to make a light yellow sludge. The names for the ingredients—slurries, sludges—are as appetizing as many non-Australians claim Vegemite to be. Taste Testing A taste test gave: Boiled slurry liquid: Vegemite or Marmite overtones, but surprisingly bitter (perhaps the result of hops?) Boiled, thick malt: unexpectedly, a strong ‘this taste is not Vegemite but is in Vegemite’ response. Vegemite does not taste like malt, and you would never eat Vegemite and think of malt, yet boiled-down malt wort syrup distinctly resembles very sweet Vegemite. Mixing Adding malt to the boiled-down slurry liquid, and then adding salt and the nutritional yeast sludge. Drag ⇢ The Youtube video has no proportions, and even with large quantities of starting liquids we had only teaspoons of boiled-down results. Taking the entire boiled-down yeast slurry runoff liquid, perhaps three teaspoons or a little more, we added about half a teaspoon of malt, a generous dash of salt Vegemite is very salty. and about a teaspoon of nutritional yeast sludge. Some vigorous mixing, some adjustment (a tiny bit more malt, plus a bit more salt) Vegemite is very salty. and we deemed it done. The appetizingly-coloured result! It was not the right colour. Nor did it taste exactly like Vegemite, though it was very close. But the taste was exactly like Marmite! Taste Tests British friend: Bang on. Yeah. Feels like you should make it with beer that’s not very hoppy. Friend’s partner: Really really close. A little more bitter maybe. My wife: It’s softer, and more bitter, than Marmite. Marmite is very sticky, and this lacks the stickiness of Marmite. But it still tastes like Marmite. To me: There was a strong hit of salt and then it fades into distinct Marmite flavour. Then a slight unwanted bitter note. The aftertaste is just like Vegemite. The next day, cold from the fridge, it seemed more sharp in flavour than the previous day. Overall, very in the Vegemite/Marmite family. Variations In the same taste-test, we also tried a variant made by our Czech friend who supplied the beer slurry. He was not interested in recreating Vegemite per se, but in creating an umami stock he could use for cooking. So he’d boiled it down with stock cubes and a lot more malt: the colour was right (very dark due to the malt) but it was sweet and very much not Vegemite. For ours, the main consensus is that there is a hint more bitterness in our Mite than in Vegemite. This may be due to the beer that provided the slurry: it came from a well-hopped lager. Perhaps we need to add more malt to offset the bitterness; it’s a strong flavour though and risks overpowering the key Vegemite notes that come from the boiled-down beer slurry. I’d like to try again using slurry from making a stout or one of the sweeter porters, which may have fewer hops, a darker colour from the roasted grains, and could give a less bitter starting point. The majority of Australian beer is lager, and if Vegemite uses the most widely available beer remnants, it likely comes from some kind of light, hopped beer. I am curious how they get such a dark colour. Finally, Wikipedia claims Vegemite is made with celery and onion extracts. I’d like to try again using a stout/sweet porter as basis, and add celery salt and onion powder. Making Vegemite What did I get for an afternoon’s worth of watching beery liquids boil down in my friend’s kitchen? I came home with three quarters of a teaspoon of brown runny goop… and somewhat of a fascination for how people invented this stuff in the first place. Left to right: Marmite, our spread, our Czech friend’s umami stock spread, and some nutritional yeast paste to round out the last slice of toast.",
    "commentLink": "https://news.ycombinator.com/item?id=39960788",
    "commentBody": "Adventures Making Vegemite (daveon.design)160 points by vintagedave 20 hours agohidepastfavorite114 comments twic 17 hours agoI've come to think of miso paste as a distant cousin of the Marmite family. Less salty, less acrid, still packed with umami, and with much more discernible flavour. A more civilised cousin, really. Although it's not traditional, you can use them in similar ways; miso paste on buttered toast is quite nice. reply fifilura 15 hours agoparentI am probably way off here but I also think of Norwegian brunost or Swedish messmör. Essentially boiled down whey. It is certainly much sweeter than vege/marmite, and I am not sure the remainig taste could be classified as umami. But it has a very distinctive taste. And, if I am completely wrong, at least I can claim that the consistency and color is similar to vege/marmite. reply twic 14 hours agorootparentThat's a really interesting comparison. I think brunost has a lot of flavours that are nothing like Marmite - it's creamy, it's sweet, and it's a bit sour. But they are certainly similar in that they have a sort of concentratedness to them, and in that sensible people hate them. reply wiredfool 13 hours agorootparentIs that like gaitost, the brown whey cheese? It’s basically solid whey that’s gone through a Maillard reaction to make a sweet, salty cheese like stuff. We made some a few years back, but couldn’t get it to turn out as aell as the commercial stuff. reply fifilura 12 hours agorootparentI also made it once. The electricity for boiling all that liquid should be what makes up most of the manufacuring cost. Hm I should ask Fjällbrynt how they cope with that. If I remember correctly the trick was in the stirring. Either not stir at all (with the risk of burning), or stir all the time to keep it from crystalizing. Edit: and yes geitost is the goat-whey version. reply marssaxman 13 hours agoparentprevWhen I make a grilled cheese sandwich, I like to smear one slice of bread thinly with miso paste (and the other with dijon). Vegemite suddenly made sense once I realized it is used in much the same way. reply DyslexicAtheist 12 hours agoparentprevmiso paste with raw vegetable sticks like cucumber, carrot or cabbage is awesome. reply simondotau 8 hours agoparentprev20 years ago I introduced a Japanese person to Vegemite and, I think because I described it to him as like miso, he really liked it. I wonder if the shock value is derived from an expectation of chocolatey sweetness. If you’re expecting Nutella, Vegemite is guaranteed to disappoint. reply edem 15 hours agoparentprevi second this. to me marmite (and all the rest) is inedible, but i really like miso paste reply hilbert42 12 hours agoprevI'm writing this because like all Vegemite lovers I have to stand up and be counted as one. As Vegemite-ers know, there's absolutely no point discussing Vegemite with those from overseas, especially Americans, let alone trying to get them to taste it. It's a total waste of time! What's more it's potentially risky—a foreigner could likely file assault charges on grounds of attempted poisoning. For the record, I'll eat spoonfuls of the stuff and others around me have been known to hide it away save there being none left. Recently, I've taken to the salt reduced version and I actually rather like it (and I suppose 40% salt reduction is a good thing). Quite some years ago a relative of mine who's living in the US packed eight of the largest jars made to stockpile in his Vegemite-forsaken adopted homeland and apparently US Customs started prodding the jars looking for illicit substances. From what I can gather he wasn't arrested nor were the jars confiscated. reply naruhodo 5 hours agoparentIt's often remarked that American bread is reminiscent of cake, in that is loaded with sugar. I wonder if they would like it better on more traditional, wholemeal bread. I also note that the post falls to mention butter, which is a key ingredient in any Vegemite-on-toast preparation. Also, the article compares Vegemite to soy sauce. While Vegemite is salty, black and umami flavoured, I think it is more similar to Japanese miso soup. reply anakaine 4 hours agorootparentButter? Oh, no no no. I'm definitely a fan of just Vegemite, spread thick, and toast. Adding butter is just diluting the pure goodness of the product, adulteration it with a moderating flavour and texture if you will. Also, Vegemite and avocado is a great pair. As is Vegemite and a hard chrap cheese like tasty cheese. reply runsonrum 7 hours agoparentprev'Spoonfuls' doesn't help with the the impression that Vegemite needs to be portioned correctly. I would guess, most Americans are put off my Vegemite because their first notion is to grab a spoonful and shove it in their mouths with nothing else as they would with peanut butter. Of course it doesn't help thst, my guess from experience, is that most Australians would encourage the action while trying to hold back their laughter. Disclosure: I am an American, I live in Australia, I like my Vegemite thin and my peanut butter thick :-) reply mindok 6 hours agorootparentHave you tried thin Vegemite on top of thick peanut butter on top of thick butter? Thank me later. :) reply BoxFour 11 hours agoparentprevChecking in as an American who really loves vegemite and butter on toast in the morning. Thankfully I live in a large city where it’s easy to find. I generally like strong flavors (Matcha, fermented foods, heavy garlic, didn’t hate Durian) so it was a welcome find! reply hilbert42 10 hours agorootparentRight, there has to be some! I've even met a few myself. But I'll bet the per capita is mighty low. ;-) From my observation, there are (roughly) two types of people who love Vegemite, Marmite, Promite, etc.—those who like strong flavors like you and I do and those who are brought up on it from not long after they've been weaned. If you check out the YouTube link in my other comment you'll note it's aimed at kids. For example, when I was in primary school kids (me included) would often have Vegemite and lettuce sandwiches (kids just wouldn't eat lettuce sandwiches alone). As contrast, our second sandwich would be something quite different (honey and peanut butter, ham and tomato, etc.). I eat chilies, curries and such and was introduced to them very young too, so no doubt that helped. But not completely, some of my liking for strong flavors was innate. I recall when I was about 5 going on 6 my mother made me weak tea with lots of milk and sugar. One day I told her not to put sugar in my tea and only one week later told her to stop the milk and to make it stronger, ever since I've only ever drunk it sans milk and sugar (I simply won't drink tea if either is added). reply samplatt 9 hours agorootparentprevI must have had the wrong matcha. I've never found one that has much of a taste at all. It's nice, of course, but like chamomile or earl grey - subtle delicacies to be savoured, rather than reveling in the glorious boldness of it all. reply avidiax 3 hours agorootparentYou have definitely had the wrong matcha. Prepared the traditional way (powder, hot water, and a bamboo whisk), it is a bit foamy, strongly bitter, with grassy notes. reply fuzztester 8 hours agorootparentprevGarlic, a strong flavor? Try asafoetida. reply teapot7 55 minutes agoparentprevYeah, we like to think foreigners can't hack it - but we had an American friend staying with us for a while, and one day I came out to breakfast to find him having Vegemite and peanut butter layered on top of each other, on the same piece of toast. Jesus, Craig - what the hell were you thinking!?!? reply emmelaich 7 hours agoparentprevWhen I visited the US office of the company I worked for, I made vegemite on toast and offered it around. Most responses were meh but the one positive I got was from a person of Chinese ethnicity. I think the \"soy sauce in solid form\" is a pretty good beginner description but of course it is also quite distinct. There are schools of vegemite thought too; warm or hot toast? Slather the vegemite on or just add dabs on top of melting butter? reply alisonatwork 4 hours agorootparentSoy sauce in solid form is an excellent description. I actually do this in reverse in Taiwan, where they have a type of thick soy sauce called 醬油膏. It's used in the local cuisine to build umami flavor in sauces and stir-fries or sometimes as a condiment, but it also works as a marmite-replacement. I tend to put it in a dish with some olive oil for margarine-replacement and dip chunks of bread into it to get my \"foreign\" food fix. I imagine most people from soy sauce eating countries in Asia would understand the flavor profile, it might be more the combination with toast that feels unexpected. reply mango7283 4 hours agorootparentIn Malaysian Chinese restaurants a common dish is \"Marmite chicken\" which I imagine uses the same principle. We also dip buttered toast into softboil eggs with soya sauce and pepper so make of that what you will lol reply cnasc 8 hours agoparentprevVegemite tastes metallic to me. I much prefer marmite reply bufordtwain 9 hours agoparentprevAlso see: Branston Pickle reply jason-phillips 10 hours agoparentprev> there's absolutely no point discussing Vegemite with those from overseas, especially Americans, let alone trying to get them to taste it Not so fast my friend! Ich bin ein sheiz Ami who is no stranger to sesos, lengua, offal, Asian ingenuity and piping hot street food spanning four continents. Describe the Vegemite. On biscuits or toast? If with beer, a pilsner or stout? Please. reply hilbert42 10 hours agorootparentPerhaps so, but as I said above, I'll bet the per capita is low. Even Australians who aren't brought up with the stuff from a very early age just won't eat it either. reply merek 10 hours agorootparentprevI recommend trying it thinly spread on buttered toast reply mbo 9 hours agorootparentprevI would recommend a pilsner. Vegemite itself has stouty tones. The pilsener would provide some contrast. reply nickcw 19 hours agoprevI always wondered how Marmite was made and how it was invented and now I know both after reading the article. I'm tempted to give it a go! My family would say I'm obsessed with Marmite. I had Marmite on crumpets for breakfast. Only about 50% of the people in my family like Marmite - it is that sort of thing. Either you love it or hate it! When I was travelling I could only get Vegemite and I came to appreciate that too. It's not Marmite but it is quite similar and will certainly do :-) My father and sister used to enjoy eating Bovril ( https://en.wikipedia.org/wiki/Bovril ) which is similar to Marmite/Vegemite but made with meat extract instead of yeast extract. I've never been a big fan, and jokingly we used to call it BSE ( https://en.wikipedia.org/wiki/Bovine_spongiform_encephalopat... ) in a jar. The Marmite shortage in the pandemic was terrible but luckily I managed to score a catering sized tub which lasted us out the shortage. reply bloopernova 19 hours agoparentJust don't follow any of these recipes: https://www.youtube.com/watch?v=GSg6aVTm4_c (Taskmaster UK, Season 5: Make Marmite. The show is hilarious and I recommend season 5 as a jumping in point to see if you like it.) Oh, and the absolute best advert for Marmite: https://www.youtube.com/watch?v=JFLHialhZ8c reply bonki 15 hours agorootparentI am currently rewatching Taskmaster while waiting for the current episodes to drop and I watched this exact episode yesterday...best show ever! reply bloopernova 15 hours agorootparentI'm really liking everyone in the new season, Steve Pemberton is practically Bob Mortimer levels of hilarious. Unfortunately I find it difficult to look at Nick Mohamed's dracula makeup because I have an issue with distorted faces. reply Loughla 18 hours agorootparentprevHoly shit that's so good. reply JumpCrisscross 17 hours agoparentprevI tried a Marmite butter at the Goring in London, and it’s become my favourite compound butter. reply 082349872349872 18 hours agoprevThe first time my father was in the UK, he had just come from the Netherlands, where they put chocolate sprinkles (Hagelslag) on toast, so he thought the open Marmite jar must be something Nutella-like, and slathered it liberally onto his bread. He had just been engaged in the process of pondering how awfully civilised the Brits were when he bit into it. reply esafak 17 hours agoparentMy brother in law had a similar experience when he poured our tart ayran over his cereal. \"This milk's gone bad!\", he cried he as he spat it out. https://en.wikipedia.org/wiki/Ayran reply anjel 17 hours agorootparentSeems like a tangier Egg cream of NYC fame and lore. reply _zoltan_ 15 hours agorootparentthese two are worlds apart. reply Octabrain 15 hours agoparentprevSame thing happened to me when I moved to the UK: I went to the supermarket, bought one jar or Marmite, went back home, had a toast with it expecting to have some sweet-close-to-chocolate flavour and then I was shocked. I have to admit that I actually liked although I don't eat it regularly these days because that wild amount of salt cannot be healthy in any manner. reply ceejayoz 18 hours agoparentprevMy parents brought Vegemite to my elementary school cultural day. Some kid snuck a spoonful thinking it was chocolate. Vomiting ensued. reply caf 9 hours agorootparentA teaspoon of Vegemite, eaten rather slowly, took the edge off the worst hangover I've ever had. reply __d 9 hours agoparentprevAs a kid, and indeed as a young adult, eating a teaspoon of Vegemite was something I did fairly frequently. It was just a snack, with no mucking about, just instant food. A sort-of after school thing. It was way better than a spoonful of peanut paste, which was just over-the-top, way too much, and left your mouth and teeth gummed up and gagging. Milo was also not unknown, albeit a smaller teaspoon, because it too was pretty intense. But the king of the \"instant spoonful of food\" category was powdered milk. You had to more-or-less hyperventilate first, because you weren't going to be breathing for a while without making a lot of mess, but the gummy, sweet, mouthful of saliva-mixed-with-milk-powder was good enough that the difficulty of doing it successfully was kinda worth it. I'm somewhat tempted to try a teaspoon of Vegemite now, you know \"for science\", but perhaps it's better left as just a memory. reply caf 9 hours agorootparentDid you ever try a teaspoon of the Nestle Malted Milk drink powder? reply __d 5 hours agorootparentNo. Strawberry Qwik (sp?) was given a go though. Once. reply DEADMINCE 18 hours agoparentprevAt least that's an honest mistake. Unlike all those YouTubers who clearly know only to spread a little but pretend it's Nutella and dig in with a spoon so they can overreact. reply twic 17 hours agoparentprevMy dad had some American friends come to stay once, with similar assumptions, and a similar lesson. reply anotherevan 10 hours agoprevLate '80s we had some American's visiting who were curiously watching us spread this stuff that looked like axle grease on our toast. \"You want to try some?\" \"Sure!\" So one of us got a tablespoon and said, \"Open wide.\" The moral is, don't trust us Australians when it comes to local cuisine or fauna (or the intersection thereof.) reply hilbert42 10 hours agoparent\"...got a tablespoon\" Did they head straight for the airport? ;-) reply anotherevan 10 hours agorootparentStraight to the bathroom. The airport shortly thereafter. reply markx2 18 hours agoprevI was first introduced to Marmite by way of Twiglets. I love marmite, I loved twiglets. Then I was forced to go gluten-free. That ruled out both Marmite and Twiglets, also the default Vegemite. I've yet to find gluten-free Twiglets, but thankfully there are gluten-free alts for Marmite and a GF Vegemite. As a Brit, Marmite takes the crown though. https://en.wikipedia.org/wiki/Twiglets reply hilbert42 12 hours agoparent\"was first introduced to Marmite by way of Twiglets. \" As a Vegemite addict, I find Twiglets (despite the Marmite) compulsively addictive. Fortunately, they're hard to find over here, hence my addiction is under control. BTW, I also like Marmite but not as much a Vegemite. When I was a kid, we had both at home, my mother favored Marmite, and as the ad goes, the rest of us were happy little Vegemites: https://m.youtube.com/watch?v=LZF3FBh-n_8 reply johnsillings 14 hours agoparentprevI'm a big Twiglet fan. My partner thinks I'm playing some long-winded joke on her and pretending to like them. But they really are awesome! I need to try Marmite or Vegemite. reply roydivision 2 hours agoprevThe word Marmite is now used in British English to describe something that one either loves or hates, little or no middle ground. \"The band U2 are very Marmite\" reply mathewsanders 19 hours agoprevI have to give a shoutout to New Zealand Marmite which is totally different to British Marmite (I think that NZ marmite has some sweetness as well). Somehow i grew up in a mixed household that had both because my mum preferred Vegemite and I prefer Marmite. In NYC it’s fairly easy to get Australian Vegemite but sadly impossible to find NZ Marmite, instead I do a bulk order every couple of years so that I have a stockpile :) reply veb 11 hours agoparentNZ Marmite ftw! I had to tell my mum off for having Vegemite in her pantry the other day, but she had a decent enough excuse that made me giggle. \"I buy vegemite because it comes in glass jars and I can reuse them!\" oh... I guess that's... practical, Mum! reply caf 9 hours agorootparentIn the 80s Vegemite used to come in small glass jars with a press-on lid that were designed to be reused as drinking tumblers. It was common to see kitchen cupboards full of those tumblers. reply RowanH 15 hours agoparentprevThis. Imagine my suprise growing up on NZ Marmite, to open a jar of British Marmite and go 'wtf is that slightly gooey stuff'. There's 3 very distinct camps in NZ. Love Marmite - what's wrong with you Vegemite people Love Vegemite - what's wrong with you Marmite people None of the above - what's wrong with the lot of you :) reply julianz 9 hours agoparentprevNZ Marmite includes sugar, which I find horrible, but my family likes it so we have both in the house. I am Vegemite for life. reply 99112000 10 hours agoprevThe best vegemite I ever had was from a 6 foor 4 muscular man in Brussels reply __d 9 hours agoparentDid he speak your language? reply defrost 9 hours agorootparentда, товарищ: The Alexandrov Ensemble (RiP) https://www.youtube.com/watch?app=desktop&v=W_roE45-AIU reply awl130 8 hours agorootparentprevyou better run, you better take cover reply throwaway0665 10 hours agoparentprevThe muscular fellow? reply Niksko 14 hours agoprevKudos to the author for trying this. My approach would be to ignore the junk recipes online and see if there are any patents that describe the process. I'll have a look when I have time. My guess would be that there's some industrial process that completely separates any yeast from anything else so as to remove any potential hop bitterness. And that there's also likely been some updates to the process over the years. It may have started out as boiling things down in the 1920s, but may have moved on to an enzyme catalyzed process these days? On the idea of using a stout: you will end up with even more bitterness than the version you made, more than likely. Stouts (especially higher alcohol stouts) tend to be fairly generously hopped compared to a standard light lager, in order to balance the sweetness of the added malt. You just don't taste them as being super hoppy because a) the hops are added early in the boil for bittering and most of the volatile aromas boil off, and b) because, well, they're doing their job of balancing the sweet malts to make the beer not taste sickly sweet. reply vintagedave 14 hours agoparentAuthor here! Thanks, though the kudos really goes to the friend who followed up on the idea of doing it and asked me to pop round in the afternoon and make some. Re process, the linked video talks about using a centrifuge to separate the yeast and liquid, and in fact improvised a home-made centrifuge using a washing machine. It’s possible that simply skimming the top of a settled mixture is not nearly as effective and yeast contamination led to some of poor flavor notes. As for stouts: noted, thanks, looks like I need another approach. The main reason was not just bitterness but colour: to start with a dark liquid to get a black end product. Any thoughts on how both Vegemite and Marmite end up so black? reply jjgreen 18 hours agoprev*-mite addicts are a bit like alcoholics - Vegemite eater is like the obnoxious drunk that crashes your party - Regular Marmite is the homeless guy drinking whiskey under a bridge - XO Marmite [1] is the homeless guy drinking meths under a bridge I'm in the last category. [1] https://www.marmite.co.uk/p/marmite-xo-yeast-extract-extra-o... [edit: url fixed, thanks ofrzeta] reply ofrzeta 18 hours agoparentYour link takes me to the Marmite homepage. This works. No idea what kind of strange URL routing they practice. https://www.marmite.co.uk/p/marmite-xo-yeast-extract-extra-o... reply twic 17 hours agoparentprevI recall that one year, they did a Marmite made from Champagne years for Valentine's day. I vaguely remember that it was quite a bit milder than normal Marmite. reply jjgreen 16 hours agorootparentAnd they say romance is dead. reply vintagedave 15 hours agoparentprevAuthor here: wow. I will see if I can find some of that Marmite XO. Thankyou! reply KptMarchewa 16 hours agoparentprevCan't wait for Gran Reserva Marmite VSOP Bourbon Barrel Aged reply shoobs 19 hours agoprevI've noticed the umami flavour that tends to develop when cooking beer for a long time. An Irish stew with Guinness in it, or a gulas using beer as the staple liquid all develop Vegemite like flavours. I love it, but I think a lot of non-Aussies don't recognise the similarities. reply dekhn 17 hours agoparentif you haven't done it, try slowly concentrating guinness in the way you would concentrate stock. Then add a little meat juice. It makes a formidable sauce. reply marssaxman 14 hours agorootparentThank you for the idea! reply grudg3 14 hours agoprevI'm surprised no one mentioned it, but smooshing up a soft avocado mixing it with a small amount of Marmite then spreading it on toast is godlike. Top it with a poached egg and it's perfection. reply idontwantthis 14 hours agoparentAs an American I was baffled by marmite/vegemite until an Englishman finally explained to me your not supposed to eat it like peanut butter. A small amount mixed with anything savory is delicious. It’s basically msg, salt, and B vitamins. reply fredley 16 hours agoprevI am a certified (British) Vegemite fiend, I go out of my way to stock up from the supermarkets that carry it (it's tragic to see a whole metre of shelving, top to bottom, taken up with differently packaged marmite and its derivatives like marmite peanut butter, but not a single tray of vegemite). However, as a child I went through a phase of eating Bovril on toast. Looking forward with not a little apprehension to \"Adventures in making Bovril\". reply jon_adler 13 hours agoparentThis Aussie expat is very thankful for our local Tesco always having Vegemite stock. They also stock Bundaberg Ginger Beer (try it if you haven’t, another Aussie favourite). reply BuildTheRobots 11 hours agorootparentRoot beer is another one of those flavours you either love or hate, and Bundaberg's is particularly excellent. Sadly it's the only one I've found that contains liquorice, making all others seem lacking by comparison. Slightly surprised to find Bundaberg is actually drunk by Aussie's - cynical me always assumed it was marketing like Fosters. For people who like that sort of thing, it might be worth looking at Euthymol toothpaste. It tastes like a mix of root beer, jagermeister, germolene antiseptic and pink. reply caf 9 hours agorootparentBundaberg would definitely be the most popular ginger beer brand in Australia, and their rum is so ubiquitous that you'll hear \"bundy & coke\" far more than the generic \"rum & coke\". reply julianz 9 hours agorootparentAnd their lemon, lime & bitters is fantastic. reply bigger_cheese 6 hours agorootparentYes Bundaberg Lemon Lime and Bitters is excellent. I believe they were originally an alcohol company Bundaberg Rum, is a relatively popular spirit. reply tunnuz 16 hours agoprevAmazing! I love Vegemite ever since living in Australia. When we later moved to Vienna I bought some with me. My first born has never been anywhere near Australia but is addicted to butter and Vegemite on toast. We also use it to make vegan versions of Italian recipes requiring anchovies. reply kebman 18 hours agoprevSounds like Australians are the actual Swedes to Norwegians of the Brits, and Americans are kind of like the Danes. reply twic 17 hours agoparentThe Irish are the Icelanders? The Finns are ... South Africans?! reply cupantae 15 hours agoprevThe article might have been better with fewer asterisks* * This could just be the next sentence reply vintagedave 15 hours agoparentAuthor here. I may have gone overboard with the sidenotes.* [*] My wife told me the same thing :) reply LastNevadan 16 hours agoprevFor anyone out there about to try Vegemite on toast for the first time: the secret is to only apply a VERY THIN layer. The first time I tried it, I gooped it on like jam. Don't do that! I learned this from my Aussie friend. reply __d 9 hours agoparentThey lied. If you can see the texture of the toast, it's not thick enough. I'd forgive them though, perhaps. They might have been thinking to convert you slowly. The big reveal can be overwhelming over the age of one. reply devjam 8 hours agorootparentI agree, nice and thick, with lots of butter! Don't listen to the advice in the article :-) > The trick is not to spread it thickly like peanut butter; instead, you take about a quarter of a teaspoon’s worth and scrape it thinly over the entire slice. reply nzealand 9 hours agoprevAmerica actually has a beefy abomination called Beefy Bovrite, which kind of looks like it might be somewhat like Marmite and Vegemite, but sadly is not. reply emmelaich 7 hours agoprevFun fact. Vegemite is kosher and halal. reply DyslexicAtheist 12 hours agoprevaccording to Scottish comedian Billy Connolly, Whoppie Goldberg once said that \"Vegemite tastes like licking a cat's arse\". Although Connolly hasn't asked her about her research. I personally don't understand the war between marmite and vegemite. Nor the craze about reducing the salt. They are both an acquired taste but really staple food once you get used to it. I wouldn't want to start a day without marmite (or vegemite) on some hot buttered toast. reply anotherevan 10 hours agoparentAmanda Palmer celebrated Vegemite in a song called Vegemite (The Black Death). “I cannot hold a man so close, who spreads this cancer on his toast…” https://youtu.be/t_BkZYmTQ5c reply simondotau 8 hours agoparentprevIt’s like Pepsi vs Coke, but without the ease of appeal of 12% sugar content. Behind the imposing wall of salt, they have subtle yet significant differences in flavour. An alternative probably won’t trigger the same comfort memories. reply jacknews 19 hours agoprevThese are great in vegetarian 'minced meat' dishes like chili, to add a meaty umami taste. Having said that, Marmite is clearly vastly superior and has a deeper umami taste than the slightly fake industrial plastic taste of Vegemite. Careful though, I bought a jar of New Zealand Marmite once, thinking it was the same thing as real Marmite, given the same brand name, but, it was barely even passable as 'cooking marmite', let alone on hot buttered toast. reply mirsadm 17 hours agoparentI moved from Australia to the UK a few years ago. I never liked Vegemite very much before but find myself really enjoying Marmite. There's an off taste to Vegemite, like almost a strange medicinal flavour. reply __d 9 hours agorootparentI think that's the VB \"flavour\", a by-product of the original recipe. reply jaredhallen 17 hours agoparentprevAlthough I've been aware of it for many years, I've never had vegemite (or marmite for that matter), and never really knew what it was. Reading this (and watching the video) got me thinking along the same lines as you mention. Seems like it could be interesting used as a bullion/soup base, or maybe used like Worcestershire. reply willmadden 11 hours agorootparentIt tastes like soggy beef bullion cubes melted in beer manufacturing byproducts. Don't do it. reply DEADMINCE 18 hours agoparentprev> Having said that, Marmite is clearly vastly superior and has a deeper umami taste than the slightly fake industrial plastic taste of Vegemite. Huh. I'd say Vegemite is clearly vastly superior as it doesn't have that artificial sweetness taste added to it. reply piva00 17 hours agorootparentBritish Marmite doesn't have any sweetness to my palate, do you mean the NZ one? reply retsibsi 17 hours agorootparentWow, I just looked it up and there's a huge difference. UK Marmite has even less sugar than Vegemite, whereas NZ Marmite is 16.8% sugar by weight. reply DEADMINCE 16 hours agorootparentprevI've never tried the NZ one. British Marmite is noticeably sweeter than Vegemite to me. reply worthless-trash 19 hours agoprevI like the note that the author makes: * Vegemite is very salty. * Vegemite is very salty. reply denton-scratch 18 hours agoparentIt's significantly saltier than (UK) Marmite. reply jiehong 15 hours agoprevNever ever saw such product in France, though. reply nitin-pai 17 hours agoprevPromite is better than Vegemite. reply _joel 17 hours agoparentI prefer Thermite on my toast reply mkoubaa 12 hours agorootparentI prefer dynamite in my butter reply cynicalsecurity 17 hours agoparentprevIn which way? Is it less intense than Vegemite? reply willmadden 12 hours agoprevOh God, why do that? I bet you could embalm someone with vegemite in a pinch. reply 29athrowaway 18 hours agoprev [–] Vegemite is Australian, ask an Australian friend instead. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores Vegemite, a unique Australian spread, along with similar alternatives like Marmite, Promite, Cenovits, and Vitam-R.",
      "It explains the process of creating a DIY Vegemite using ingredients like yeast slurry, malt wort, and nutritional yeast, resulting in a taste close to Marmite but slightly bitter.",
      "Variations like utilizing different beer slurries or incorporating celery salt and onion powder were tested, highlighting similarities to Vegemite/Marmite with some flavor and texture distinctions."
    ],
    "commentSummary": [
      "The conversation on daveon.design explores the similarities among Vegemite, Marmite, miso paste, and Norwegian brunost, discussing taste preferences and ways to savor the spreads.",
      "Opinions vary on the flavor of Vegemite and Marmite, with some considering them salty and others praising their intense taste profiles.",
      "Participants share anecdotes, cooking tips, and compare various spread brands during the engaging discussion on different spreads."
    ],
    "points": 160,
    "commentCount": 114,
    "retryCount": 0,
    "time": 1712498102
  },
  {
    "id": 39961994,
    "title": "RPGP: Secure OpenPGP Implementation in Rust",
    "originLink": "https://github.com/rpgp/rpgp",
    "originBody": "rPGP OpenPGP implemented in pure Rust, permissively licensed rPGP is the only pure Rust implementation of OpenPGP, following RFC4880 and RFC2440. It offers a minimal low-level API and does not prescribe trust schemes or key management policies. It fully supports all functionality required by the Autocrypt 1.1 e-mail encryption specification. rPGP is regularly published as the pgp Crate and its RSA implementation lives under the collective RustCrypto umbrella. For ECC crypto support we are using Curve25519-dalek. Please note that the API is not well documented yet. You may check out the tests which exercise the API. Please open issues here if if you are attempting to use rPGP and need help. Status (Last updated: October 2019) rPGP and its RSA dependency got an independent security audit mid 2019, see here for the full report from IncludeSecurity. No critical flaws were found and we have fixed most high, medium and low risk ones. rPGP is used in production by Delta Chat, the e-mail based messenger app suite, successfully running on Windows, Linux, macOS, Android and iOS in 32bit (only Windows and Android) and 64 bit builds (for the other platforms). More details on platform and OpenPGP implementation status: OpenPGP Status document which describes what of OpenPGP is supported Platform status document which describes current platform support. Experimental WASM Support When enabeling the wasm feature, rpgp can be compiled to run using WASM in Node.js and the supported Browsers. Experimental bindings for this can be found in rpgp/rpgp-js. Developement To run the stress tests, > git submodule update --init --recursive > cargo test --release -- --ignored To enable debugging, add use pretty_env_logger; let _ = pretty_env_logger::try_init(); And then run tests with RUST_LOG=pgp=info. How is rPGP different from Sequoia? Some key differences: rPGP has a more permissive license than Sequoia, which allows a broader usage rPGP is a library with a well-defined, relatively small feature-set where Sequoia also tries to be a replacement for the GPG command line tool All crypto used in rPGP is implemented in pure Rust, whereas Sequoia by default uses Nettle, which is implemented in C. Minimum Supported Rust Version (MSRV) All crates in this repository support Rust 1.70 or higher. In future minimally supported version of Rust can be changed, but it will be done with a minor version bump. LICENSE MIT or Apache 2.0 Contribution Unless you explicitly state otherwise, any contribution submitted for inclusion in rPGP by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.",
    "commentLink": "https://news.ycombinator.com/item?id=39961994",
    "commentBody": "Rpgp: Pure Rust implementation of OpenPGP (github.com/rpgp)153 points by bpierre 17 hours agohidepastfavorite75 comments perlgeek 12 hours agoGiven how much of a huge pain it was to use PGP / gpg back in the days (for email encryption/signatures, to be precise), it's amazing how easy end-to-end encryption on Signal and Whatsapp is. And how it lacks that drama that somehow always surrounded GPG. Is it really because trust on first use is good enough for most cases? Or is email somehow so much different than chat? Or was PGP the proof-of-concept, and current e2e encrypted platforms are the v1.0? Or all of the above? Did I miss anything important? reply woodruffw 11 hours agoparentPGP as an ecosystem has always suffered from toolbox-itis: when a tool (especially a cryptographic tool) tries to be everything, it ends up doing nothing particularly well. This got ossified in the form of OpenPGP, and has basically remained the status quo for the last ~25 years. More generally, the OpenPGP world is in a bit of a double-bind: they can either fix things by breaking compatibility (at which point someone can reasonably observe that there's no good reason not to ditch OpenPGP entirely), or retain compatibility and accept that OpenPGP will never get much better than RFC 4880 and whatever smattering of drafts the GnuPG maintainers agree to implement. One way essentially results in an entirely different tool/standard that happens to be wearing PGP's skin; the other means keeping around misuse-prone and outright broken cryptographic primitives (and bad formats to boot). (To answer your actual question: email is just a bad substrate for attempting E2EE messaging. Latacora has a great explainer post on why[1]. TOFU is a mostly adjacent concern; trust/identity negotiation is hard, but the thing that makes WhatsApp, Signal, etc. actually work is that they eliminate manual key management and make cryptographic right choices for the user, rather than expecting the user to hold the tool correctly. In other words: they're misuse-resistant, where PGP as an ecosystem has historically not been.) [1]: https://www.latacora.com/blog/2020/02/19/stop-using-encrypte... reply tcfhgj 10 hours agorootparentthey arent miss-use resistant when it comes to mitm attacks reply woodruffw 10 hours agorootparentNeither Signal nor WhatsApp is MITM-able. Could you explain why you think they are? (Signal and WhatsApp also have forward secrecy properties that PGP, as an ecosystem, is nowhere close to providing. PGP very much still operates under the \"one master key that you must never ever lose or disclose\" paradigm.) reply sowbug 5 hours agorootparentMy guess is that the term \"MITM\" is being overfit to include the basic problem of verifying that A's identity really belongs to A rather than M. reply 7bit 3 hours agorootparentBut you can verify the other end. Not wanting so so that ist Not a Problem of the App or protocol. reply pvg 11 hours agoparentprevEmail being different from chat is a big factor and another one is that the entire system in Signal is designed and engineered specifically for the purpose, from the protocol on up. Retrofitting privacy, confidentiality, etc onto email is one of those noble 90's cryptography dreams that turned out to be unrealistic and impractical. reply vmfunction 2 hours agorootparentThese are the reasons project such as Wave (By google) and matrix.org are introduced. But E-mail dies hard. As one can see it just keeps on adding new bandages to try to make it work. And most user don't care about security or privacy until it is a mess. reply vlovich123 4 hours agorootparentprevI find it hard to believe that you couldn’t retrofit Signal into an email app. Encrypted comms is quite good at sitting on unencrypted untrusted channels and signal is already asynchronous. The challenge is that there’s no money to be made so you’d need a non profit like Signal to do it to disrupt that industry. reply pvg 4 hours agorootparentYou can make an email-like app out of Signal but nobody would call that email since it won't work with the thing everyone else calls email. You can't just add Signal to actual email and get something with the security and privacy properties of Signal, though. reply kseistrup 3 hours agorootparentThere is, however, an app like DeltaChat that looks more or less like WhatsApp, uses AutoCrypt for E2E encryption, and uses IMAP/SMTP as its storage and transport media. https://delta.chat/ reply dignifiedquire 2 hours agorootparentFun fact, the main reason I developed rpgp in the first place was to power the pgp portion pf deltachat, as we ran into the same problem of not having a good library based implementation. And spawning gnupg on a phone was never an option. reply vlovich123 4 hours agorootparentprevWhy couldn't you have a browser extension for gmail that checked whether the other end was registered & if it was encrypt the email body before it's sent? You get transparent upgrades for your friends who have the extension installed. reply zeehio 3 hours agorootparentThe closest solution I remember to this happened roughly 15 years ago when there was a browser extension that detected pgp on Email bodies and was able to get public keys and encrypt/decrypt emails automatically or with a single click, if I recall correctly. It felt a small team or a single person project. There was a cat and mouse game being played against Gmail interface changes, with the encryption being broken every now and then. It required me, a random user, to trust the extension developer(s) with my Gmail and gpg keys. I had fear of having an xz backdoor situation compromising my Gmail account and gpg keys. And for this extension to succeed I also had to convince all my Gmail friends and contacts to use it, and convincing them to trust the developers of the extension as I had done. While I felt confident that the extension was perfectly safe I didn't dare to convince my family and friends to use it because (a) I could not guarantee that the extension wouldn't be compromised in the future and (b) the chance of the extension being broken in the future due to a Gmail UI change was close to 100%. Also the more users the extension had the higher the chances for someone to attempt to infiltrate to attempt a backdoor... If I recall correctly, eventually the developer got tired of the Gmail UI changes and increasing demands of users and moved on, stopping the development. Anyone with time could try to develop such an extension, but there were drawbacks back in the day... reply pvg 4 hours agorootparentprevThe latacora link in the sibling comment covers most of that. reply vlovich123 4 hours agorootparentNope actually it doesn’t. It makes the case that pgp is a bad implementation but makes the leap to claim that means there’s no way to improve the situation. For example, you could run a proxy server to deliver the messages between parties. So if you’re emailing someone, the browser extension changes the address to be a server you control, sends you the encrypted email that has everything meaningful encrypted, you decrypt the part your able to (ie the intended receiver) and forward that. As far as either email server is concerned, your email server is sending out and receiving encrypted emails but there’s no metadata to connect anyone. The key handling uses your access to the email account to validate access same as signal uses the phone number. That seems like a pretty close system to how Signal works and I don’t see anything meaningfully different. If you do, can you actually elaborate? I don’t feel like the link you pointed me answers why my proposal would be larping security since the security and threat model feels very similar to Signal. Here are some choice quotes I pulled out of that link and why I have problems with the claims made: > Searchable archives are too useful to sacrifice, but for secure messaging, archival is an unreasonable default. Secure messaging systems make arrangements for “disappearing messages”. Signal and most apps except Snapchat (which isn’t e2e afaik) don’t archive by default. But yes, it is true that having encrypted web mail and search are incompatible whereas messaging apps store the data locally and thus can provide search. You’d need a dedicated client that could store all the encrypted messages locally but people aren’t as used to for that. > Some email clients have obscure tools for automatically pruning archives, but there’s no way for me to reliably signal to a counterparty that the message I’m about to send should not be retained for more than 30 minutes. Talk about security LARPing. Once the message is sent you’ve lost all control of it. Your just following social contracts that the app honors the request and the user is using an unmodified app. > No matter how good a job one does securing their own data, their emails are always at the mercy of the least secure person they’ve sent them to. Also true for messaging apps reply pvg 3 hours agorootparentFor example, you could run a proxy server to deliver the messages between parties. You could just email each other your Signal ids at that point. It's no longer the email everyone thinks of as email. reply littlestymaar 3 hours agorootparentprevYou can but if you sent the email yo someone that doesn't have the extension, they wouldn't be able to read it, and you have no way yo know who has the extension installed. That's the core problem here: pretty much nobody uses encrypted emails, so you can't send encrypted emails to them. The problem doesn't exist with Signal since everyone using Signal uses a client that supports encryption. reply vlovich123 1 hour agorootparentDiscovery is a solved problem same as how signal uses phone numbers: https://support.signal.org/hc/en-us/articles/360007061452-Do... Nothing stops you from implementing a similar mechanism based on email addresses. reply OJFord 12 hours agoparentprevIf Gmail had decided to care about it, the experience in email might be nice too (or at least for Gmail users). But instead it wants to be able to read it to suggest calendar events etc. i.e. WhatsApp and Signal care about UX and mainstream adoption. Nothing like that for email has tried to do PGP. reply littlestymaar 3 hours agoparentprev> it's amazing how easy end-to-end encryption on Signal and Whatsapp is They both use a centralized, authority for identification and key sharing, sidestepping the hardest part entirely. Also, they are their own protocol and as such you don't suffer the encrypted/plain-text-only email dichotomy, sidestepping another pain point. When you don't care about the decentralized nature of email and can force everything to be encrypted, then the problem becomes much easier. reply barfbagginus 11 hours agoparentprevSignal and WhatsApp are no good against state actors And that is where they get their usability reply bluish29 8 hours agorootparentState actors is just a term usually refer to threats from dedicated abd resourceful entities that have don't have to worry about funding, tracked by their government..etc. They usually are in the market for 0days and have dedicated resources to throw at their targets. So you can basically say the same about any tool. If I know that I am being targeted by a state actor while living in the US, the only thing that can help is me going to FBI. This assume of course that state actor is the US itself and assume that you know your are being targeted. On individual level, if you have being targeted or use a tool with 0day then you lost before you even know that there was a game. reply tptacek 8 hours agorootparentprevIn what way is Signal \"no good against state actors\"? reply aborsy 4 hours agorootparentOne way would be to install a zero-click Pegasus-like software on their phones. Better yet, ask Apple to activate code on their iPhone. It’s a criticism of the platform, not applications running on it. reply mdhb 5 hours agorootparentprevI could think of a few ranging from just asking Apple for a copy of your iCloud backups, asking you personally for a copy of the messages through to exploiting iOS itself. reply akerl_ 5 hours agorootparentGiven that iCloud backups don't contain Signal messages, and \"ask you for a copy\" and \"exploit the OS\" affect any application... can you clarify? reply mdhb 4 hours agorootparentThe context was what are things that those with more or less unlimited resources can do in order to read your signal messages. They are 3 particular paths that aren’t in any way open to most attackers but will work here. Saying it impacts all applications isn’t wrong but it’s also not relevant to what was asked. If whatever the hell you are doing requires a threat model where the NSA is interested in finding out who you are talking with and what you are saying the original comment of signal is inappropriate is 100% accurate. reply LtWorf 5 hours agoparentprev> it's amazing how easy end-to-end encryption on Signal and Whatsapp is Amazing that you consider a proprietary thing like whatsapp remotely secure. For all we know it's backdoored. And signal… not available on fdroid… thus mostly installed via apple/google… how difficult would it be to push a backdoored update to selected individuals? GPG is more secure reply aborsy 8 hours agoparentprevWhatsApp is a phone app for messaging. PGP is a protocol for email and file encryption in any operating system. It’s unclear why you are comparing them. PGP could have a phone app that will do easy encryption, such as protonmail. reply dale_glass 14 hours agoprevAh, if only this was a thing a decade or two ago. IMO, the way GPG was done killed what could have been a decent ecosystem. It's a combination of two factors: 1. Don't roll your own crypto. 2. The available code (gpg) is a colossal pain to use. So for instance, how does something like KMail deal with gpg? There's no libgpg originally. There's just the gpg tool, so you've got to call it as a sub-process, and it really sucks: 1. You have to deal with process management, multiple filehandles, text parsing, non-trivial interactions, etc. 2. It's slow. You pay startup costs every single time. This is a huge problem on something interactive like a mail client, and it's dependent on things like the amount of keys in the gpg store. 3. gpg has very specific ideas about how it wants to be used, and not everything fits. Say that you oh, want to do some stats on GPG keys. There's no libgpg to just read an .asc file and get the list of signatures from that, no. You have to call gpg, feed it the key, parse the result. For some things you might actually have to have gpg import the key first. Manage a fake home dir for GPG. Deal with the horrible performance as the keystore grows. A million keys at a gpg invocation per second is going to be around 2 weeks. Unfortunately it's only now that gpg is effectively dead that the problem started to get fixed. Also, at this point GPG is effectively a legacy technology anyway. Modern cryptographic thought considers GPG to be a terrible idea for a whole bunch of reasons that are deeply built into it, so the only solution for that is throwing it out. reply wiktor-k 14 hours agoparentI agree that gpg did not age well. If we compare it to a different project with similar history: curl, it's apparent that gpg chose wrong on several fronts. It should be a library first instead of a cli tool. Funny part is that even the library of gpg (gpgme) is internally calling the binary. I've played around with designing a higher level library to OpenPGP once (https://pypi.org/project/pysequoia/) and personally I think it yields more readable, faster and secure code. reply im3w1l 3 hours agorootparent> Funny part is that even the library of gpg (gpgme) is internally calling the binary. Sounds like a great way to transition things in a saner direction. You know it will be bug-for-bug compatible with calling the gpg-binary. Having one blessed text-parser with a lot of eyes on it is much better than everyone rolling their own. Getting people used to depending on a library instead of shelling out also means it becomes possible to move the library to independent implementations. reply Lammy 12 hours agoparentprev> Don't roll your own crypto. This is exactly the meme I would try to propagate if I had backdoored every popular crypto implementation. More people should roll their own crypto so they can stumble and learn and eventually get good at it. reply woodruffw 11 hours agorootparentThis assumes that implementation diversity stymies an attacker, which isn't really true in cryptography. Cryptographic errors have well-understood \"shapes,\" frequently present even when the raw algebra of the implementation is correct; adversaries are delighted when they discover yet another hand-rolled RSA implementation that's susceptible to BB'98 or BB'06. reply LoganDark 3 hours agorootparenthttps://en.wikipedia.org/wiki/Daniel_Bleichenbacher reply YoshiRulz 4 hours agorootparentprev> More people should roll their own crypto so they can stumble and learn and eventually get good at it. This would only work if it were always obvious when your implementation is unsecure, which is not the case. reply aborsy 3 hours agorootparentprevRolling your own crypto is obviously a bad idea. It turned out the cryptography implemented by developers not expert in cryptography sometimes was merely obfuscation. If you want to roll your crypto, become a cryptographer first. That sounds circular, but isn’t. reply tptacek 4 hours agorootparentprevSeems like a great way to get new brain surgeons, too! reply ognarb 12 hours agoparentprevDisclaimer: I work on GPG and I am one of the KMail core developer. The standard way to use GPG is via the gpgme librairie which then communicate with the Assuan protocol to the gpg deamon. Gpgme has official bindings to C++, Qt and python. In KMail we use the Qt bindings and it works fine. reply dale_glass 12 hours agorootparent> Disclaimer: I work on GPG and I am one of the KMail core developer. Nice work :) Looking forward to seeing further improvements to it, it's still got a few rough edges, but otherwise I really like it. > The standard way to use GPG is via the gpgme librairie which then communicate with the Assuan protocol to the gpg deamon. Yeah, but all GPGme does is calling the gpg binary and presenting a more palatable interface to applications. That solves the problem as far as the API goes, but performance is still terrible for some uses, and it's still liable to run into all kinds of weird problems in edge cases. But at least for something like mail clients it seems to work well enough. reply curt15 12 hours agoparentprevDoesn't git use GPG to sign commits? reply dale_glass 12 hours agorootparentYes, why? And I believe these days you can use SSH keys for signing commits as well. reply Zizizizz 12 hours agorootparentprevYou can use SSH keys as well. reply dheera 12 hours agorootparentprevJust use an ssh key for authentication and there is no need to sign commits. reply rmwaite 11 hours agorootparentHow does SSH authentication provide the same guarantees as signing commits? reply dheera 6 hours agorootparentssh itself is cryptographically secure. the ciphertext of the commit being pushed is as good as a signature reply defanor 4 hours agorootparentI guess you are considering the case where a single signer transfers commits over SSH to a single verifier, with both putting the same meaning into the act of transfer as people normally would into a signature. But git is a distributed VCS, with signatures handling many more scenarios: consider multiple committers and multiple signatures, transfer over multiple, different, untrusted channels, multiple verifiers. reply dignifiedquire 14 hours agoprevhey, author here, happy to answer questions, and would love to hear from anyone using the library :) reply Mortak 14 hours agoparentIt seems OpenPGP is still riddled with SHA-1. Git kind of avoided the problem (https://github.blog/2017-03-20-sha-1-collision-detection-on-...). What's your plan to deal with the issue? reply upofadown 9 hours agorootparentI am not aware of any outstanding SHA-1 issues that would require a change in the current RFC4880 OpenPGP standard. There was an obscure attack that involved generating two keypairs with colliding SHA-1 signatures and getting a third party to sign one of them but you can just use a different hash (say SHA256). The SHA-1 used in the MDC portion of the authenticated encryption mode doesn't and is very unlikely to ever represent any security weakness (the hash used there doesn't require any particular cryptographic properties). SHA-1 is used for the key fingerprint, but the use of a hash with collision resistance is not required in general for key fingerprints. An attacker could in theory create two different keys with the same fingerprint, but then they would just own two keys that would be hard for to distinguish from one another. You don't sign the fingerprints, you sign the actual public key. In general, it would be a bad idea to specify that the hash used for a key fingerprint required collision resistance as that would mean that the fingerprint would have to be something like an unusable 256 bits long to prevent birthday attacks. reply dignifiedquire 14 hours agorootparentprevI just finished implementing sha1collision detection, and it will be integrated into rpgp soon https://github.com/rpgp/rpgp/issues/293 This is the same algorithm used by git. There are higher level implementations that use the dates on signatures to straight out reject sha1 material, but that gives only a limited protection. reply LtWorf 14 hours agoparentprevDoes it support smart cards and such devices? reply dignifiedquire 14 hours agorootparentit‘s not builtin, but possible to integrate with @wiktor-k is working on a tool to use rpgp to provide a simple solution to work with smartcards reply wiktor-k 14 hours agorootparentYep. We've got it working with OpenPGP Card devices (Yubikeys, Nitorkeys, etc.). The signing part was actually pretty easy and the decryption required a bit more work but the maintainer was super responsive (https://github.com/rpgp/rpgp/pull/315). Overall I'm pretty happy with the codebase. The PoC for using cards in git is in https://github.com/wiktor-k/monkeybagel (excuse the silly name ;). reply rendaw 6 hours agorootparentHow does it interface with the cards? IIRC the rust pcsc library used by Sequoia needed C libraries. I've been doing some NFC stuff too and was looking for a pure-rust solution if there was one. reply wiktor-k 4 hours agorootparentIt uses this crate: https://crates.io/crates/pcsc On Windows and Mac it binds to system libraries. On Linux it works with pcsclite. Btw I'm not aware of any tooling that's used by Sequoia, rather wrappers that use Sequoia and pcsc crate. reply upofadown 8 hours agoprevGnuPG has by default started emitting keypairs with a preference for the LibrePGP version of the OCB block cipher mode. That mode is not compatible with what the other faction is doing and is not generally supported in any case. Arch[1] and other distributions have apparently patched this default out. Is Rpgp emitting any new block cipher modes or generating keys that might cause such emission in the future? The risk here is a sort of incompatibility nightmare where decryption becomes a crap shoot. My article on this mess: https://articles.59.ca/doku.php?id=pgpfan:schism [1] https://wiki.archlinux.org/title/GnuPG#Disable_unsupported_A... reply dignifiedquire 1 hour agoparentWhile rpgp is slowly gaining support for this format (the aid users to be able to talk to anyone they want) it will keep emitting the broadest compatible format for a while by default, until the whole ecosystem has upgraded. The aggressive stance from gnupg is really hurting people and removes one of the biggest benefits of pgp, broad interop with compliant implementations. reply wiktor-k 3 hours agoparentprevYep, the schism is really bad for the end users and based on my \"insider\" knowledge it doesn't seem like the resolution is near. The tl;dr is that GnuPG basically forked OpenPGP into https://librepgp.org/ reply woodruffw 14 hours agoprevEdit: The comment below is incorrect. I'm leaving it for transparency, but I misread where the padding is being applied. I could be missing something here, but I think this is vulnerable to DO'1985, a/k/a Desmedt-Odlyzko: https://github.com/rpgp/rpgp/blob/8e67756ebce780c91b8c2ffc7d... In particular, in the presence of an insufficiently wide hash, the absence of padding here means that RSA signature validation is not secure under EUF-CMA. Matt Green has a great post on why and when EUF-CMA matters[1]. (This isn't necessarily this implementation's fault, since PGP seemingly (!) encourages the stripping of padding from signatures. But I can't find another source for whether this is actually encouraged by OpenPGP, or whether implementations just widely allow it.) [1]: https://blog.cryptographyengineering.com/euf-cma-and-suf-cma... reply dignifiedquire 14 hours agoparentI am not sure if this is an actual issue, all auditors that looked at this so far haven’t mentioned this being a problem. But I will have to investigate what the exact state is. reply woodruffw 14 hours agorootparentAccording to `git blame`, this was introduced June 2023, i.e. after your audit in 2019. But maybe it was moved from an older piece of the codebase, I didn't dig too deep. (Looks like the IncludeSec folks did a decent job in 2019. Hi Eric!) reply dignifiedquire 14 hours agorootparentThis was allowed in the rust-rsa crate directly before, which is why it was introduced in that commit. reply woodruffw 14 hours agorootparentYep, I saw the upstream[1]. However, I misread this: I thought the padding was being done on the cleartext signing side, but this is padding of the signature itself. So there's some malleability here, but it isn't susceptible to DO'1985. I'll update my top-level comment. [1]: https://github.com/RustCrypto/RSA/issues/272 reply junon 13 hours agorootparentGlad people care to look, that's what matters. reply dignifiedquire 13 hours agorootparentprevThanks, appreciate the careful check! reply wiktor-k 14 hours agoprevRpgp is great (we're currently using it for a better git signer with smartcards) but I wonder why is it trending right now at HN? Maybe because it's currently #1 in the test suite? https://tests.sequoia-pgp.org/ reply dralley 14 hours agoparentI wish seqouia would use MPL-2 or something over LGPL. It's only a little weaker in licensing terms but vastly more convenient to use. reply wiktor-k 13 hours agorootparentIndeed. There were some discussions over choosing MPL but the parent foundation was more aligned with GPL and after it collapsed it seemed th3 idea of relicensing to MPL was dropped. Do note that sequoia and rpgp are different in many aspects so the licensing is just one thing. reply karma_pharmer 11 hours agoprev [–] This uses a flawed RSA implementation which is vulnerable to timing attacks: https://deps.rs/repo/github/rpgp/rpgp#vulnerabilities There is a reason why crypto primitives handling key material -- especially RSA and AES -- are not written in higher-level languages. reply mcspiff 11 hours agoparentI’m not sure you can really categorize rust as a high level language, unless you’re stating crypto primitives should only be written in assembly. EDIT: Given this attack was also applied to OpenSSL, amongst many others this high level language comment seems doubly odd/dubious EDIT2: another rust TLS implementation was among only 3 that was initially verified to be not vulnerable as well.. reply karma_pharmer 8 hours agorootparentunless you’re stating crypto primitives should only be written in assembly Yes, that's exactly what I'm stating. Have a look at openssl, boringssl, nspr, etc. They all implement the core modular arithmetic for RSA and the s-box table for AES using assembly language. There is no reliable way to prevent a C compiler from \"optimizing\" your constant-time code into non-constant-time code. another rust TLS implementation rustls uses assembler (from boringssl) for these routines. It is not 100% rust, and that's a good thing. reply karma_pharmer 8 hours agoparentprev [–] Genuinely curious, why the downvotes? reply LtWorf 5 hours agorootparent [–] Because you lack the understanding that different CPUs behave differently with the same instructions, so your \"solution\" to timing attacks doesn't solve anything until you force every device to have the same identical CPU. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "rPGP is a Rust-based OpenPGP implementation following RFC4880 and RFC2440, with Autocrypt 1.1 support and a minimal API, used by Delta Chat after a security audit.",
      "It differs from Sequoia concerning licensing, features, and exclusive use of pure Rust for cryptographic functions, requiring Rust 1.70+ and licensed under MIT or Apache 2.0."
    ],
    "commentSummary": [
      "The conversation highlights PGP's limitations in end-to-end encryption, especially in email systems, in contrast to more modern platforms like Signal and WhatsApp.",
      "Concerns about trust, compatibility, and security are discussed, along with the challenges of integrating privacy features into existing email systems.",
      "The dialogue explores issues with GPG, such as slow performance and complexity, debates on DIY cryptography, and security vulnerabilities, emphasizing the significance of constant-time code for cryptographic security."
    ],
    "points": 153,
    "commentCount": 75,
    "retryCount": 0,
    "time": 1712508630
  }
]
