[
  {
    "id": 40496858,
    "title": "WordPress Co-Founder Reflects on 21 Years of Innovation and Community Engagement",
    "originLink": "https://ma.tt/2024/05/wp21/",
    "originBody": "WP21 May 27, 2024AsidesMatt It seems like just yesterday WordPress was becoming a teenager, and in a blink of the eye it’s now old enough to drink! 21 years since Mike and I did the first release of WordPress, forking Michel’s work on b2/cafélog. There’s been many milestones and highlights along the way, and many more to come. I’ve been thinking a lot about elements that made WordPress successful in its early years that we should keep in mind as we build this year and beyond. Here’s 11 opinions: Simple things should be easy and intuitive, and complex things possible. Blogging, commenting, and pingbacks need to be fun. Static websites are fine, but dynamic ones are better. Almost every site would be improved by having a great blog. Wikis are amazing, and our documentation should be wiki-easy to edit. Forums should be front and center in the community. bbPress and BuddyPress need some love. Every plugin and theme should have all the infrastructure that we use to build WordPress itself—version control, bug trackers, forums, documentation, internationalization, chat rooms, P2, and easy pathways for contribution and community. We shouldn’t be uploading ZIPs in 2024! Theme previews should be great, and a wide collection of non-commercial themes with diverse aesthetics and functionality are crucial. We can’t over-index for guidelines and requirements. Better to have good marketplace dynamics and engineer automated feedback loops and transparency to users. Boundaries in functionality and design should be pushed. (But spam and spammy behavior deserves zero tolerance.) Feedback loops are so important, and should scale with usage and the entire community rather than being reliant on gatekeepers. Core should be opinionated and quirky: Easter eggs, language with personality even if it’s difficult to translate, jazzy. Everyone developing and making decisions for software needs to use it. It’s important that we all do support, go to meetups and events, anything we can to stay close to regular end-users of what we make. What would you add? Fun fact: On May 27, 2003 I blogged “Working backwards, earlier tonight was great. Put WordPress out, which felt great.” as one sentence in a 953-word entry written from the porch of my parent’s house where I was accidentally locked out all night until my Dad left in the morning to go to work. Had no idea WordPress would be as big as it is. Earlier that night had set up WP for my friend Ramie Speight, and done some phone tech support for another friend Mike Tremoulet I had met through the local blogger meetup. My friends from high school all had their own domains with WP and that feedback loop was magical for shaping the software. Share this: Click to share on Tumblr (Opens in new window) Click to share on Twitter (Opens in new window) Click to share on Facebook (Opens in new window) Click to share on LinkedIn (Opens in new window) Click to share on Pocket (Opens in new window) Click to share on Telegram (Opens in new window) Click to email a link to a friend (Opens in new window) Related Posts",
    "commentLink": "https://news.ycombinator.com/item?id=40496858",
    "commentBody": "WP21 (ma.tt)409 points by joshbetz 16 hours agohidepastfavorite153 comments askonomm 11 hours agoI just wish WordPress actually adhered to development standards, not actively tried to break them. Using globals everywhere and encouraging spaghetti code with its classic themes, and with its new themes it evidently learned nothing and is encouraging JSON inside HTML comments which obviously has no editor support, is very prone to errors, besides being just plain stupid (like seriously, some senior engineers I assume decided to have templating inside HTML comments as JSON?). If I had my tinfoil hat on, it's almost as if they actively try to kill the freelancer or digital agency market and push everything to its WYSIWYG site builder on WP.com. reply zelphirkalt 11 hours agoparentThey encourage lots and lots of bad practices. I mean, just look at themes and how they are built by default. Comments in a CSS file describe the theme metadata. Using concattenation instead of composition everywhere, so that parts of HTML are not reusable. CSS, inside JS inside HTML inside PHP ... Part of it is this implicit structure it defines, which file it loads adter ehich other file to make a whole page. It seems convenient at first, if only you know it, but it encourages people to not properly finish all their HTML elements on the same file, but split them up stupidly and end them in other files, never to be reused. It all seems very beginner-mistake like. But I guess they are stuck with this now, because thousands of themes rely on this, instead of having a structure based on composition. Contrast this for example with how one uses Jinja2 templates rendering blocks and macros. reply Tijdreiziger 6 hours agorootparentTheir mantra of ‘never break backwards compatibility’ is a double-edged sword. It makes them extremely friendly to non-technical users, which I think is the majority of their userbase. However, it makes it impossible for them to change technical decisions made in the past. reply askonomm 6 hours agorootparentThat would be fine, imo. But seeing as their new development efforts (Block/Gutenberg themes) are also making horrible decisions when they no longer have to (such as putting templating markup inside of HTML comments as JSON)[1] E.g: I've lost any faith I had in their core development teams competence. I would understand if development choices made 20 years ago weren't all that great by todays standards, and never breaking backwards compatibility would be the reason it still is bad today, but their new themes can be used independently from their old themes as an entirely different implementation, so they had a chance to finally do what is industry standard, and they chose to again make it horrible to write theme markup code that is prone to errors, has no editor support (what editor supports JSON in HTML comments?) and enforces more spaghetti to be made. [1]: https://developer.wordpress.org/block-editor/getting-started... reply noduerme 11 hours agoparentprevKill the freelancer? I'm a freelancer and my favorite thing to tell cheap potential clients for the past 15 years is do their site with Wordpress and to go find a Wordpress \"expert\". It exists for the broke and the cheap who are under the illusion that they need a simple website, which itself is a dead letter. reply tomthe 10 hours agorootparentWhy do you think they might be wrong when they want a simple website? reply DaSHacka 10 hours agorootparentBecause then he couldn't justify why these companies should continue hiring freelancers reply sbarre 7 hours agorootparentprevBecause most non-technical people greatly underestimate all the moving parts in a website, even a simple one. And yes some websites truly are simple and can be managed with a few Markdown pages and a static site generator (even that can be a barrier to non-technical folk), but any kind of advertising or small business website - even if it's a single page - needs the ability to be updated or managed by a non-technical person and those parts are unknown/invisible at first. Wordpress, for all it's problems, is one possible solution that can help those people have the website they imagine or want without the sticker shock that comes with the eventual realization that their website (as a whole) is not \"simple\". reply Tijdreiziger 7 hours agorootparentWordPress is simple for the user. It’s like a lightbulb. Of course, the manufacturing of lightbulbs is a highly technical topic. However, to the user, they present a simple interface (screw it in and flip the switch). I feel like this is a point that devs often miss. Simplicity from the POV of a user and the POV of a dev are completely different things. The dev finds a static site generator simple, and WordPress unnecessarily complex. The user finds WordPress simple, and an SSG unusable. reply deerfeeder 8 hours agoparentprevIf I have to work with WP I always use the timber framework instead of the block/classic version https://timber.github.io/docs/v2/ reply blain 6 hours agorootparentI have worked with themes that used Timber and it has its own set of quirks. I also have an opinion that using things like Timber in WP themes just adds to the inconsistency instead of helping it. Taking over website with Timber when you never heard of it is fun. I don't want discourage, use what you like. Just my 2 cents. reply n3storm 5 hours agorootparentOur choice for Timber is not about consistency, is about sanity and survival. reply donatj 7 hours agoparentprevI am by no means a Wordpress expert, but I have had to deal with moving clients sites from time to time and I have vivid memories of hardcoded absolute file system paths in the database embedded in serialized PHP that broke when trying to move their installation to a new server with mildly different pathing. reply k3vinw 1 hour agorootparentIf I recall correctly, it was double the fun when it points to the original copy of your other wp install while it’s still running. reply easyThrowaway 11 hours agoparentprevIf you adhere to development standards your users aren't locked in to your platform. They could move to better working, more manageable CMSes instead of begrudgingly having to pay someone or, better yet, move to your hosted solution to keep the platform from tripping on its own feet. Unfortunately that could negatively affect your \"Our platform powers 43.4% of all websites\" marketing pitch. reply askonomm 11 hours agorootparentActually market effects these days make it so that clients themselves demand WordPress, and are unwilling to try any alternative platforms at all. So either I decline the client and make no money or have to use WP. reply benatkin 11 hours agoparentprevI agree that WordPress has often failed to live up to development standards, but those aren't them, those are ones that can legitimately vary between platforms. reply ssalat 14 hours agoprevQuick judgments and strong opinions have unfortunately become part of the community. Having spent the last 2-3 months working excessively on WordPress development, I would like to say a word about the excellent isolation of code with blocks (\"Gutenberg\"). As standalone plugins or in combination with Advanced Custom Fields, these allow for perfect, modular websites and development flows (design system), where even the HTML is 100% in your own hands. I can recommend everyone to understand and learn WordPress properly. – no relation or connection to WordPress. reply easyThrowaway 10 hours agoparentI'm doing what you're proposing literally right now. I have a fascinating error 500 on production because somehow, somewhere, today Gutenberg and ACF w/Blocks are having a disagreement on parsing the content of a nested media field. Which could be ranging from \"the user added an image description where he shouldn't have\" to \"a global object from a plugin is polluting other global objects passed to acf_register_block_type()\". Maybe I should call the already irate client and tell him he should avoid quick judgments and strong opinions. reply easyThrowaway 9 hours agorootparentBy the way, the issue was cunningly appropriate for this thread: Considering that in wordpress EVERYTHING is stored in the DB as an article in the WP_POST table (Yes, even stuff like attachments, images, and the menus) an hook from a slider plugin was messing up with the image descriptions, which are stored as...the main body text of an article. Which is actually an image. In a classic wordpress install this would just cause some weird garbage in the output, but given that in a gutenberg+ACF setup the content data is passed to the React/Block rendering engine, it would absolutely go crazy. reply rrreese 11 hours agoparentprev>Quick judgments I think most peoples judgments have been formed over 21 years. WordPress initially gained a reputation for being a fast and easy way to setup a website, then gained a reputation for being a security nightmare. Maybe its not anymore but people are right to be sceptical. I sure do see a lot of CVEs in the weekly update I check - maybe they're all low risk, or relate to rarely used plugins. reply kunley 8 hours agorootparentI think the root of bad reputation was due to various plugins and their usage pattern: lots of non-tech users heard that they can use plugins X Y Z for fun and profit, so they started to use them, but no one told them that managing dependencies requires some skill or at least discipline; that the fact some 3rd party pluggable software exists doesn't automatically mean it's good, viable, maintainable and safe; and that things in IT don't work by means of cargo cult, copy-pasting without underdstanding and by crossing fingers. So, there was a fallacy: these people believed (and many believe until this day) that they can remain being non-tech users while maintaining their wordpress-with-plugins installation, but it's impossible; one needs to become tech-aware in the process. I am not sure what WP community did to dissolve this fallacy; maybe they did something. maybe didn't. reply Tijdreiziger 6 hours agorootparentprevPersonally (and this is just based on my gut feeling), I don’t think WP core is more insecure than other CMSes. The real problem is the plugin ecosystem, which is not impossible to navigate for the disciplined, but at times bears resemblances to the Wild West. So, what ends up happening is: 1. Cheap ’experts’ install every plugin under the sun. 2. One of these plugins inevitably gets pwned. 3. Headline: ‘WordPress backdoored’. reply yard2010 11 hours agoparentprevOne of my first programming moments when I was a child - I naïvely opened the wp index.php to understand how it works. I remember I couldn't understand a thing except the comment in the top of the file \"code is poetry\". Dear op thank you for changing my mentality about code, inspiring me and pushing me into it. reply o_____________o 11 hours agoparentprevIt sure seems that way until you need to get just a little bit deeper and you realize what a dumpster fire WordPress is. You want metadata on posts, you install ACF. You want to filter on that metadata, good luck if it's over a couple filters simultaneously, the SQL queries will time out. Take a look at WP's insane schema to figure that one out. Gutenberg promises to have WYSIWYG editable React components, which is a big deal, but they made insane decisions like storing the attributes in HTML, rendering HTML in the database, and requiring component developers to keep an array of deprecated changes when they want to modify anything on the component. There are some people trying to untangle Wordpress by refactoring and bolting Laravel onto it[1], but every layer is just a nightmare; the authors of different parts can barely assess why things randomly break. You might find WP appealing for the plugin ecosystem, but the plugins are completely random in implementation, so you're likely to get a bloated scramble of CSS and JS pushed to your users. I moved to Directus and Astro, but I would probably use a Laravel-based CMS like October or Statamic for more generalized PHP deployment. [1]: https://roots.io/ reply wwweston 13 hours agoparentprevCan you recommend some of the better ways to understand and learn it properly? (Worked with it extensively 2009-2011, including authoring/modifying plugins, but never felt like I really understood it, only vaguely understood/appreciated it) reply throw5345346 12 hours agorootparentNot parent but: https://fullsiteediting.com will help you. It's a great project. reply pbowyer 12 hours agoparentprevWhat's the best skeleton theme to build your own theme on in 2024, that supports Full site editing, Gutenberg etc? After a decade away from WP I wanted to set up a new site. The default 2024 theme doesn't meet my needs, and I couldn't find a modern skeleton theme where I could add in Tailwind and build what I wanted. I've used https://roots.io/sage/ previously but they're moving further and further away from the WP way of doing things. Edit: or instead of a skeleton theme, a good free FSE theme to build on? reply throw5345346 11 hours agorootparenthttps://fullsiteediting.com/themes/ has a list, including one or two skeletons. I am tempted by Anders Norén's Björk, which is also FSE: https://andersnoren.se/teman/bjork-wordpress-theme/ I think I did most of my learning with Carolina's own Jace theme. (FWIW I have always thought the roots.io stuff is a mistake, conceptually.) reply ska-dev 11 hours agorootparentprevI made this - it's not free, but it's FSE and Tailwind: https://sinukoduleheabi.ee/blocks/ reply JeanMarcS 10 hours agoparentprevWordPress is a great blogging CMS. The fact that people use it for something else is what create strong opinions. Example : woo commerce. Products ? In the WP_POST db. Orders ? In the WP_POST db. And in every post about WP it says that using WP_POST for storing your data is good practice. Well I don't think it is (but it's only my opinion. A strong one). And the problem is that a LOT of plugins and themes are like that. reply 5636588 8 hours agorootparentFYI, WooCommerce added the ability to store orders in a separate database table. reply paulryanrogers 6 hours agorootparentDo you mean table(s)? reply paulryanrogers 6 hours agorootparentprevPerhaps you mean 'table'? reply egman_ekki 8 hours agorootparentprev> Example : woo commerce. > Orders ? In the WP_POST db. No longer true, at least for new installations: https://woocommerce.com/document/high-performance-order-stor... reply JeanMarcS 7 hours agorootparentOk, sorry for this example. I last worked on one about a year and a half ago. reply V__ 8 hours agoparentprevThe thing is, there are a lot of other CMSs out there, which can do this as well with much less bloat. Statamic, Craft CMS etc. reply zelphirkalt 11 hours agoparentprevBut HTML is not in your own hands with WP. WP changes your HTML as it pleases. See my other comment as an example. reply rob74 6 hours agoprevDo you know what makes you feel old? When you read \"WP\" and think not of WordPress (which would now apparently be accepted as an adult in even the most conservative countries), but of WordPerfect (https://en.wikipedia.org/wiki/WordPerfect - which is now at \"midlife crisis\" age). reply atonse 4 hours agoparentI still miss WordPerfect. I used to prefer it to MS Word. And it's been 25 years since I last used it. reply JodieBenitez 14 hours agoprevI love Wordpress. People install it on their own, install dozens of useless, unsecure and buggy plugins (\"it's easy, just clic clic clic... I love my admin panel !\") and after a while their websites break and then we can charge them for a more secure and resilient solution. reply agumonkey 9 hours agoparentBack in 2011, I had the pleasure to take a peak at the sociable plugin (#2 on their plugin ladderboard) and it was one of the more brittle and bloated piece of code I ever saw. Felt like an unfinished weekend project that dragged too long and ended up published, 5 screens long for loops over massive globals, duplicated to then do whatever. reply eastbound 13 hours agoparentprevThey sure made it enticing to install plugins, if not just by putting this UI to the forefront. Also, when images aren’t compressed by default, it’s a perfect alliance with the various SEO analyzers which mandate you to compress them… reply jaydeegee 14 hours agoprevWP is the perfect tool for 95% of the job with the last 5% of tweaking being incredibly frustrating. I've used it extensively and it's longevity is a testament to its usefulness may it be around for another 21yrs. reply TBurette 11 hours agoprevWhat I find remarkable is that multiple people can say 'I am a Wordpress Developer' and it will translate into very different experiences and skillsets. For some it means clicking around installing theme, plugins then writing the content of pages through the Wordpress admin. For others it means old school php code to customize Wordpress behaviour with PHP template to write the HTML. This is called classic theme. For yet other people it means writing JS+React with docker, CI/CD,... This is the new block theme. reply DANmode 9 hours agoparentFor yet others, it could be all of these, if they have chosen a diverse enough clientbase. reply thisisjaymehta 15 hours agoprevTo be honest, I never found WordPress easy to use. It's all flowers and rainbows as long as I can find good themes and plugins. However, it starts going south as soon as I need to make a very small custom change. reply thomasfromcdnjs 15 hours agoparentI've always considered myself an above average web developer, my friends would always have Wordpress websites, and ask me if I can just tweak a few things. Without hesitation, I'd proclaim I could easily make those changes! Then I'd load their site, load their plugin/theme code and css files, struggle for hours to get the desired effect, and even if I got it to work, I would break every other part of their site. --- Denigrating anecdote aside, good job Matt, loved the story at the end. reply pests 14 hours agorootparentI had the exact same thing happen years ago and I've never felt so helpless. Simple banner / color change. Ended up rebuilding the entire site from scratch it was easier. Once you spend some time in the codebase and understand a bit of its legacy history it gets easier over time. A lot of plugins and solutions are aimed towards non-technical users and a lot of overlap. Where we might just write up some custom HTML others might install 3 plugins to make it work. reply kijin 14 hours agorootparentThe overlap between plugins is crazy. It looks like every plugin comes with an entire SEO toolkit, a performance optimizer, a firewall and the kitchen sink. Unless you're very careful, you quickly end up with an unusable house with 5 kitchen sinks, no fridge, and 1 3/4 ovens that are perpetually trying to burn one another. reply pests 14 hours agorootparentI think half the problem there is everyone offering half a house for free, but every company is offering a different half. No one wants to pay for the paid offerings that have all the features, they install one SEO plugin to get page metadata, another to get structured data / microdata, another to optimize keywords; when all they had to do was buy the premium version of Yoost, for example. Thankfully there was a recent consolidation in a plugin called Admin & Site Enhancements (ASE). Probably got rid of 10 others with this thing. Includes duplicate post/page, admin menu cleanups & reorganization, hiding dashboard widgets, cleaning up the top bar, changing the login URL, an easy way to setup SMTP delivery, media replacement, hides the annoying admin notices, and a bunch more. reply Helmut10001 14 hours agoparentprevInitial setup of wordpress is super easy, but it gets very hard to maintain after a while. Updates require manual intervention, themes must be fixed, plugins deprecate. All of this adds a burden I am not willing to accept, which is why I moved all my sites to Hugo/Jekyll/Mkdocs (etc.) since about 2017. reply throw5345346 13 hours agorootparentHave you looked at WP-CLI? A lot of the stuff you want to do can be maintained at the command line. And it even gives you a way to do some fleet management. reply paulryanrogers 5 hours agorootparentprevAutomated updates are easier now, at least with well tested plugins and themes. reply Helmut10001 2 hours agorootparentHow about PHP updates for major versions (e.g. 7 to 8), or how about maintainability for 10+ years? I was sick of all the work that emerged after a while for some simple sites. I have about 15 websites, all focus on a specific topic, with one or two updates per year. Wordpress is too much for this. Static sites can be moved around or hosted on the most basic nginx, whereas Wordpress requires a lot more. I am not against wordpress in general, just that I found it not suited for my purposes. reply jsxlite 13 hours agoparentprevI switched to ghost, at first it was a bit rough around the edges, but since they made the global cli. It has being quite nice. For blogs, would rather use that than WP. Of course I also prefer JS instead of PHP. reply dep_b 8 hours agoprevI prefer something like Django to build more complex websites against for clients, or just pure PHP for blog style stuff for myself. WP level sites wouldn’t hire me as two days of my work is as expensive as years of WP hosting. But I admire the way the web was democratized with these CMS’es. reply NKosmatos 13 hours agoprevI wish more companies would adopt the sabbatical policy that automattic has: https://automattic.com/benefits/sabbatical/ reply croisillon 10 hours agoparentnote that in the US they have 2 weeks vacations per year, so even with 3 months sabbatical less than in Europe reply gibrown 5 hours agorootparentThis isn't true at all. The vacation policy is \"Our time off policy is short: take the time that you need\" and \"There is no minimum or maximum, but we encourage you to take at least 25 days of time off per year\". reply Raed667 9 hours agoprevI will always fondly remember that time i was in college (~2011), and created a fully working social network for my classmates using WordPress and ungodly amount of Plugins, and ducktape code I barely understood. A user profile display was ~30 SQL queries (who needs cache), and my poor 5$ VPS was sweating ... learned a lot on what not to do! fun times ! reply debesyla 7 hours agoparent30 SQL queries are basically normally functioning WordPress, it's a bit more unusual when it gets to 150+ queries zone :) reply Raed667 6 hours agorootparentI remember laughing because one of the plugins was storing the HEX color value of a small banner in a DB and for every page load it needed to fetch it again even if the banner was nowhere on the page. Suddenly it changed my look on all all those \"easy\" knobs and fields from the admin panel and they looked very costly reply zelphirkalt 11 hours agoprevI have come across a situation, where an automatic Wordpress update (need to keep up to date, against vulnerabilities) made a site I maintain violate the law: I used a unicode symbol for something in text. Wordpress out of nowhere and by itself decided, that it would be better to replace that symbol with a bloody svg, that is loaded from some third party. At first I could not believe my eyes, then it dawned on me, how incredible reckless they acted with that update. They must really have no clue what they are doing. Then I scrambled to reverse this bs and tried various things, including editing the theme minimally, which originally I never wanted to do, because I do not want to maintain a theme in addition to the site. Well nothing worked, except for installing a plugin, whose sole purpose it is to reverse this stupidity. If I had not had functionality connected to the DOM structure around my unicode symbol, I might not have noticed it, because that functionality also broke. So there we go, WP automatically making the site violating the law by loading from third party without consent and also breaking my functionality and basically forcing me to install a plugin to correct WP core mistakes. Of course it is very clear now, that it is completely unfit for any business website, when the core developers make such bad decisions. It requires constant maintenance, even if you update nothing but WP itself. Alternatively you let it get outdated and get hacked due to vulnerabilities. Great. reply simonjgreen 11 hours agoparentAre you intentionally not mentioning which country and law? reply zelphirkalt 11 hours agorootparentGDPR, EU. You cannot simply load third party shit on your website, without asking for consent. By downloading an SVG from a third party provider, I would need to ask the visitor, whether transmitting their IP address is OK or not, since that is personal data. Aside form all the information associated with when someone accesses the site. reply eertami 9 hours agorootparent> You cannot simply load third party shit on your website, without asking for consent That's not how the GDPR works at all. If it were, there would be no content distribution networks operating in the EU. Linking to a third party image in document markup does not involve you transmitting anything. reply JimDabell 8 hours agorootparentBy including external references to third-parties, you’re effectively leaking your visitors’ IP addresses to the third-party. Those IP addresses are considered PII and are covered by the GDPR. https://www.theregister.com/2022/01/31/website_fine_google_f... reply simonjgreen 9 hours agorootparentprevGotcha, thanks reply hawski 11 hours agorootparentprevMy bet is EU and making it non-compliant with GDPR. reply j-krieger 11 hours agorootparentprevThey definitely are and I would bet money it's China. reply addandsubtract 11 hours agorootparentChina has laws against loading content from third parties without consent? Sounds more like an EU thing. reply hawski 11 hours agoparentprev> Of course it is very clear now, that it is completely unfit for any business website, when the core developers make such bad decisions. I'm not saying it is or it isn't, but I do wonder how many people are doing business with WP while never considering a donation or whatever value add for the project. At the same time maybe you do pay, maybe they already swim in money. reply JimDabell 8 hours agorootparentAutomattic is valued at $7.5BN. They don’t need your donations. reply nashashmi 10 hours agorootparentprevWordpress doesn’t take donations. They have a payable service attached to the open source platform. And that service does not work any differently. reply liampulles 11 hours agoprevWordPress is not the blogging solution humanity deserves, but the one that was needed. Customising a base and installing plugins as opposed to devving a blogging platform allows a huge portion of people to create internet businesses who otherwise wouldn't. I get that there are supposed better solutions now, but this was not the case a few years ago. And there is a lot of WordPress content out there for novices to learn web dev. My sister co-owns a very basic architecting firm, and they have figured out WordPress based on online materials to design a website that suits them pretty well. Maybe that is not the long term solution for them, but it is a very productive place to start for a small business. reply sureIy 10 hours agoparent> better solutions now What better solution for self-hosted websites that can be jump started by anyone and provide the kind of flexibility that WP provides? reply fastball 10 hours agorootparentIt really depends on your needs. Are there many alternatives more flexible than WP? No. But that is begging the question. The vast majority of people don't need the kitchen sink that is WP, and would be better served by utilizing one of the many products and platforms that have cropped up to fill more specific niches since the inception of WP. Just need a blog? Maybe want it to be a newsletter? Ghost, Medium, Substack, etc Want an online store? Shopify, Checkout Page, etc Need a sexy website that doesn't take coding? Squarespace, Framer, etc You get the idea. reply sureIy 8 hours agorootparentOut of all your examples, only Ghost is self hosted. Not everyone wants to or is able to afford a $20/month provider, especially small businesses outside the US. reply fastball 3 hours agorootparentGiven that your previous comment said \"can be jump started by anyone\", self-hosted is definitely not a reasonable requirement. Self-hosted WP cannot be jump started by anyone, and in the limited cases it can (very, very limited WP), you're probably not spending much less than $20/m. reply throw5345346 13 hours agoprevOne of the things that strikes me about WordPress is the way that web nerds expect to find it easy and are angered when it is not. Like everything, it takes learning. It has opinions. It has some crazy history (I really wish media items were not handled the way they are), but it also has methodology to it. If I said I know Go and JS and Perl and Java and Ruby and C, and I was enraged that Rust is so hard to learn, I'd be shot down for it, rightly. WordPress looks like it does a simple job, but actually it's a whole, quite broad platform. You might have to read some documentation for a bit. And if you've inherited a site using Elementor, ask the people who made it how to change the simple stuff, because they will be able to help. If you've inherited a site using Visual Composer or Divi... shoot the people who made it. If you think Gutenberg is bad (it is very much not, now!)... oh man, Divi was a time. reply easyThrowaway 8 hours agoparentI've \"inherited\" a website using Divi for all of its styling. It's absolutely one of the worst pieces of commercial software I've ever seen. Just saving a blog post is capable of putting the entire website in an unrecoverable condition if there's even the slightest timeout in the execution of the terrifying javascript UI they wrote on top of Wordpress. The italian and french localization is genuinely abysmal, on par with some japanese games from the '90s. Responsive options are absolutely non-working, unless by responsive you mean \"hide and show content on specific breakpoints\". And even then, everything is absolutely brittle given that the front-end \"theme\" is basically an unreadable dump of jquery-era javascript. I'm 100% sure nobody would use that if Elegant Themes (Divi authors) weren't massively spending on advertising. reply orf 13 hours agoparentprevWhat’s the crazy history? reply throw5345346 12 hours agorootparentWell, maybe not crazy. Just heavily legacy. There are a few deeply frustrating things, if you ask me: All the media files are stored in a single uploads/year/month (maybe year/month/day) directory, which can mean some very big directories of file variations There's code that cannot be fun to support anymore, like the Pluggable functions (that still let you get Wordpress to check some external login system) There's still really not enough of a sense of a \"model\" anywhere. It still (AFAIK) stores some things in the database using PHP serialization (which is unambiguously the most annoying serialization format on earth, and means that search and replace tasks must be done in PHP) I mean... it's hard to blame them for not wanting to break stuff, and the commitment to backwards-compatibility is very nearly unprecedented. I think WordPress is great, and I am not judging. I'm just saying, there are decisions that might have gone better with a little more foresight. But some of them are literally twenty years old and hard to change now. Not that WP is alone in that -- FreeCAD is just getting through its \"fix a two decade legacy problem\" as we speak! Matt is right about zip uploads. I mean it's better than explaining to random users how to upload nested hierarchies over FTP, but still. reply k3vinw 1 hour agoprevMy first computer software related job was creating Wordpress powered web sites from photoshop documents. Thank you to the creators for making that possible! reply MangoCoffee 14 hours agoprevWP is 21 years old, and people on HN still bishing and crying about it. How come no one has offered something that can beat WP in 21 years? reply ehnto 13 hours agoparentI think developers underestimate the ecosystem and community aspects. Software is definitely not a space where \"if you build it, they will come\". There is better codebases, but no one has held a candle to the ecosystem yet. I believe this is because the core users of WP are not it's developers, the users (admins, agency clients etc) hold a considerably larger stake in typical business engagements. I have been doing eComm agency work for years and even if the chosen eComm platform does have a CMS, we're very often asked to integrate a wordpress site for the company's marketing/content team to use. reply dmje 12 hours agoparentprevIt’s HN. This is where WordPress comes to get bashed :-) Although - to be fair, I’m not seeing nearly as many anti-WP comments as you’d expect here - most people are being reasonably balanced with their criticisms. As a long time WP agency owner I agree with a fair number of the comments. My main beef now with the platform is that there are three fairly distinct types of WordPress in 2024. 1) “Classic” WordPress with no Gutenberg: great for data rich sites where you want many custom post types and taxonomies 2) “Gutenberg” WordPress for rich front end editing 3) “FSE” WordPress for quickly throwing up a one pager or simple brochureware site I wish WP was a bit more vocal about explaining these types and how they differ. And in fact I think they’re sufficiently distinct that the installation path should be explicit about these types and which to choose. There are of course endless things that really should be in core and not provided by plugins - it’s sometimes galling to have a team pushing endless changes out to Gutenberg when the underlying software doesn’t have obvious stuff. Page duplication, acf style custom field support, rich seo, sitemaps, better media handling, etc - all of this should just be there without plugins. But - as said above, it’s easy to snipe and overall I bloody love most of the whole ecosystem :-) reply 9dev 13 hours agoparentprevNah, that’s not how it works. People are also complaining about the quality of Google‘s search results, but has anyone come up with a something that can beat them? Technically, yes! But practically, Wordpress has such a velocity, it’s mostly impossible to stop at this point. There’s an ecosystem of millions of plugins, themes, entire agencies around Wordpress, that’s something you can’t really solve. So just because Wordpress sticks around doesn’t mean it’s the superior solution; it just happened to suck less than the competition a decade ago. reply patates 12 hours agoparentprevPeople are also complaining about JS/PHP/Python, Excel, JIRA, people-complaining, Windows, Teams, Google, death of RSS, ... I think it helps some people re-evaluate their decisions, helps products to get some unfiltered feedback, and perhaps motivate entrepreneurs to analyze the market needs. What's there to complain? :) reply todotask 13 hours agoparentprevIt's more objective if you compare by carbon footprint among others and how you build your site. reply todotask 6 hours agorootparentMeaning there is always challenges is all software and everyone has different expectations, I don't see why I get downvoted. reply oblio 7 hours agoparentprev> WP is 21 years old, and people on HN still bishing and crying about it. How come no one has offered something that can beat WP in 21 years? https://en.wikipedia.org/wiki/Economic_moat https://en.wikipedia.org/wiki/Barriers_to_entry https://en.wikipedia.org/wiki/Sunk_cost#Fallacy_effect Something can be mind-numbingly bad, yet be almost impossible to dislodge. reply graiz 14 hours agoprevI wish wordpress did fewer things but did them better. There should be something that is a notch better Jekyll but doesn't get gross when you install a dozen plug-ins. Out of the box wordpress produces a very slow and bandwidth heavy site. reply throw5345346 13 hours agoparentIf you need a simple site now you barely need any plugins at all. Something for a contact form, maybe. You do have to lean in slightly to how Gutenberg and Full Site Editing works -- I recommend this amazing documentation site: https://fullsiteediting.com reply p4bl0 12 hours agoparentprevHave you given Dotclear a try? reply joshstrange 11 hours agorootparent* Google for “Dotclear”, get the first result with no description (“No information is available for this page. Learn why”) - not a good first impression * Dotclear website’s theme is ugly verging on purposefully ugly. At least WP default themes look decent * Click on “About” to see what this runs on (homepage doesn’t tell me), nothing loads. Oh! I have to click one of these header tabs/links, odd… “About” -> “Overview” still doesn’t tell me what language this is written in. Click “Pre-requisite” finally see it’s PHP+(MySQL/Postgres/SQLite) For software that’s been around since 2003 I kind of expect better and I expect have heard of it since back in 2009-2011-ish I was doing a fair bit of WP development. reply Kye 8 hours agoprev>> \"Almost every site would be improved by having a great blog.\" Unfortunately, WordPress seems to move further and further from this every year. I've used WordPress from near the start, and the editor is so top-heavy now that I get anxiety opening it. Switching to Ghost (with a nice little $4/month managed host[0,1]) was a breath of fresh air. It doesn't do all the things WordPress does, but it does the things it does do better. Especially newsletters and pay gating. [0] Referral link: https://magicpages.co?aff=9fLJierQBpnV [1] Non-referral link: https://magicpages.co reply muglug 15 hours agoprevIn a vertical where the barrier to entry is not high, being first to market with something simple that people can extend can create a decent-sized moat. The downside is that much of the code in WP core is effectively un-modernizable, given how much depends on things working a certain way. Key classes and functions haven’t been meaningfully updated in over a decade. reply p4bl0 12 hours agoprevI remember a time when WordPress was \"easy\". Nowadays starting from a clean install and customizing the appearance and modules has actually become quite difficult. Even the usage workflow feels bloated. These days I prefer Dotclear as a blog engine. It also has grown in complexity over the years, but not quite as much and still feels like nothing is getting on your way. This is from the point of view of someone knowing how to program and the languages involved (PHP, SQL, HTML, CSS, etc.) but who's not a professional web developer and even less a professional WordPress-based developer (since this is now actually a thing). reply karolist 12 hours agoprevMy gf has a WP ecommerce site that her business revolves around, it was built buy some local guys doing WP development who have an agency solving problems just with WP. She told me how fast they were able to iterate and solve all their problems. The site has a bunch of plugins integrating various social services, ad tracking, SEO and whatnot. The site generates PDF shipment labels for parcels, one day her sales got high enough to buy a label printer, one that spits out 10x15cm stickers. The problem - PDFs from the site come out as A4 and text gets tiny if squeezed to fit into that sticker. She asked her developers to fix it, they said it's impossible and refused. Now that's interesting, nothing is impossible I shouted with my nerd hat on. I'm in the tech space for a good few decades now, I have FAANG experience, complex systems are my thing! I spent 5 hours diving through tons of spaghetti code plugins masquerading as highly abstracted set of interfaces to arrive at the conclusion that these guys were right, the PDF blob comes from the shipment company's SOAP API, though it's obfuscated deep enough. In the end I solved it with a simple PyQT+fPDF UI utility to crop out the printable parts and project them onto the right sized canvas for printing, it took me 2h to complete with binary packaging and all, less time than it took to understand why the WordPress site can't do it natively, and much less than than it would have taken me to integrate this PDF modification into WordPress. These guys were basically right. Her site now backs up to 4GB zip with photo assets. I dread the day when her site goes down due to some \"hack\" but I have no idea how to replicate this functionality for this cost without WP. No way in hell I would say I can do it from scratch for her, my previous Web dev experience doesn't matter at this point. Shopify? Sigh. reply skilled 12 hours agoparent4GB? Those are rookie numbers. I have a site where /wp-content/ is 20GB. Tens if not hundreds of thousands of images that have been auto generated by WordPress because of how thumbnails and minifaction works, but also converting images to WebP. And if you delete old posts or any other content, the images stay. And it’s my understanding that there is no safe way to remove unused images. I have tried to do it using the in-built media manager but eventually I gave up because it’s tedious and I don’t want to risk leaving pages without images by accident. reply debesyla 10 hours agorootparentBut 20GB is not a big deal these days, even cheapest hosting solutions give \"unlimited\" storage (only limited by inodes mostly, but those go into absurd numbers too). In my eyes it's more optimal to just don't worry and take the WordPress site as simply a tool - yes, it will break after 5 years, but by that time you would have needed new site anyway. reply pxtail 11 hours agorootparentprevSurely there is a plugin for that. reply jddj 12 hours agoparentprevI have a handful of different personal web apps spinning away, and other critical-path ones for work. Nothing though gives me the feeling of dread that I get when my partner's business WP site goes down and she asks if I can take a look. reply karolist 11 hours agorootparentIt's amazing how much lock-in you get by developing ecommerce sites with WP for clients. Her site was recently down, just didn't load. She paid a few hundred for the agency to take a look, they said they updated plugins and \"removed viruses\" and all is good again. If you're someone not technical or without a huge sum of money to pay someone to replicate the functionality to migrate off WP you're on the hook for the life of business. You can migrate hosts but that's about all the freedom you have, paying Shopify 20-30 USD / mo is nothing compared to what you'll have to eventually pay with WP if you build your business around it IMHO. reply jddj 10 hours agorootparentSame thing happened here recently, connection limit on the shared hosting's mysql. If I hadn't named the exact issue for her in her communications I'm sure a consultant would have happily shaken her down for the same. To be fair though, we get the same at work with Salesforce consultants reply zubspace 13 hours agoprevI avoided wordpress like a plague. Used stuff like django, flask, grav and who knows what. But recently I had to make a website with lots of unknowns which needed to be authored by dummy users and I just said ** it and went with wordpress. What I struggled with are: 1) Interesting site design, 2) custom functionality through plugins, 3) making it easy to add pages and blog posts with a nice editor. I'm glad open source solutions exist, but I swear, they always have drawbacks. Either they are code heavy, plugins are out of date, maintenance takes a lot of time or there is no way to easily design a page. So for Wordpress I bought Oxygen, for which you can still buy a lifetime license for, and oh my god, even I can make a nice, responsive website without touching code. It's such a game changer. I think lots of people avoid wordpress because of security. But that is not primarily a wordpress fault. Linux & PHP complicates it a lot. And I'm sure those other open source projects have severe bugs, too, but nobody knows or talks about them. What could bring down wordpress, imho, are expensive license fees for plugins. Who wants to spend 50 to 100 bucks per year and per page for a builder plugin? Everyone wants one, but no one wants to pay yearly. And it seems that all plugin makers are starting to go that way. reply throw5345346 12 hours agoparentI agree about builder plugins -- they are expensive mossy lock-in. WordPress now has a full-site editing system that lets you GUI edit the templates that are in the normal flow. It is not what I would call easy to master at the code level, but there is a global styles system and a way to use Gutenberg blocks to control layout outside the main content flow. So we are getting towards a point where page builder plugins won't be needed for skilled shops. But IMO until there are really easy to use themes based around FSE (there may be some), small design shops are still likely to use Elementor, which is a slow, frustrating experience (slower and more frustrating than Squarespace can be) reply aussieguy1234 10 hours agoprevI was on the team that built news.com.au, which, along with the other news sites we set up in a multi tenant WordPress setup, was at the time the largest commerical WordPress site in the world, getting something like 500 million monthly page views. We made extensive use of caching to say the least. reply firefoxd 15 hours agoprevWordpress is the SMS of the Internet. There are hundreds of alternatives that came and went. reply diego_sandoval 14 hours agoparentIt's the PHP of the CMS world. reply fjuan 14 hours agoprevCongrats Matt! WordPress enabled lots of people to push their own boundaries in the internet. Looking forward to celebrate the next 21! reply Swizec 15 hours agoprevWordPress is my favorite example of \"It doesn't have to be perfect, it just needs to work\" So many cool projects die because people over-complicate the first steps. You can always make it better later if people start using your thing, but first you gotta ship. reply 9dev 13 hours agoparentFunny, I think it proves the contrary. Wordpress effectively made their whole code base the public API, so now they’re stuck with the legacy code they have for eternity, unable to meaningfully improve it, as plugins may depend on the existing state. It’s so bad, the PHP language developers are unable to implement some features/fixes in the language, as the Wordpress team refuses to migrate their code, which makes for a huge chunk of PHP usage. Imagine that. reply onli 12 hours agorootparentThis is stability. Stability is good. There is no need to change code that works just because it collides with modern taste. If WordPress has a stabilizing influence on php that's even better. All the breaking changes of the new versions are a nightmare for an established project I work on. reply 9dev 11 hours agorootparentCode is never just working. The environment it runs in changes, requiring refactoring things. We’re not talking about a showcase piece of artisan algorithm here, but bug-riddled legacy code reliant on outdated system packages, SQL queries that cannot use bound parameters for historic reasons and are ever-prone to injection attacks. Code that uses broken multibyte encoding, such that it is vulnerable to several attack classes. And that’s not even talking about performance. Are you seriously telling me software performance should not improved if the core functionality kinda, sorta, works? reply onli 8 hours agorootparent> Are you seriously telling me software performance should not improved if the core functionality kinda, sorta, works? I would not hesitate to take this position. Of course it depends, on how severe the bugs are, especially for the outer code (like plugins) calling it, and on how bad the performance is. But otherwise absolutely, never break user space. > The environment it runs in changes The web environment Wordpress runs in did not change all that much. The JS ecosystem simulates big changes, but that's all bullshit. Server code that worked 30 years ago still works - if projects like PHP don't go out of their way to break it. reply oblio 7 hours agorootparent> Server code that worked 30 years ago still works - if projects like PHP don't go out of their way to break it. I'd be horrified to expose ANY software written 30 years ago to the internet, if it touches money or valuable data in any way. reply 9dev 7 hours agorootparentprev> But otherwise absolutely, never break user space. Neither the Linux kernel, nor OpenSSL, or any other reasonably complex project manages to do that over a given time frame. Sometimes you need to adapt, and things break in the process. Nobody would expect a house built 30 years ago to not require some maintenance and upgrades over time. > The web environment Wordpress runs in did not change all that much. The JS ecosystem simulates big changes, but that's all bullshit. Server code that worked 30 years ago still works - if projects like PHP don't go out of their way to break it. That sure sounds good, but is simply not true. We went from HTTP and FTP deployments to TLS and containers, from dialup to gigabit consumer uplink; the browser isn't a remote document viewer but a platform-agnostic virtual machine for fully-fledged applications; the web is centred around a few enormous platforms; people regularly stream GBs worth of video and expect services to deliver web apps on a variety of devices; they don't post on bulletin boards and in news groups, but use chat services; scammers distribute ransomware, steal your identity, remotely take compromising pictures from your webcam, or order stuff from your shopping accounts online. The modern web has almost nothing in common with the one from 30 years ago. reply onli 7 hours agorootparent> That sure sounds good, but is simply not true. Sure it's true. Many users are still doing deployment FTP-style, even if it's not the original protocol anymore. That the pipes are bigger just meant we could up the thumbnail size, and the browser is still also a remote document viewer for sites that don't demand more. I just today answered a support question on a bulletin board, and so on. There are other aspects of the web today, but the old way still exists. > Nobody would expect a house built 30 years ago to not require some maintenance and upgrades over time. You can do upgrades of software in a way that does not break compatibility, and you can definitely always aim to minimize breakage. Wordpress is not a bad example for just that. HN itself counts as a further example. If it weren't possible we wouldn't have this thread to discuss in. reply noahtallen 10 hours agorootparentprevNewer PHP versions are certainly supported (and required by many hosts due to the significant performance improvements of PHP 8.0): https://make.wordpress.org/core/handbook/references/php-comp... reply throw5345346 11 hours agorootparentprevWordpress requires at least PHP 7.0... and will complain about it if it's not above 7.4 (which was released in 2019). So this to me looks like WP is progressing with PHP, just slowly. Frankly I don't have a massive problem with this. The speed of Laravel's PHP baseline change may be appropriate for Laravel, because it is git-managed, more easily run in a container etc., but it's absolutely inappropriate for WordPress to chase the edge. Compared to trying to stay up-to-date with needless Node.js changes and frameworks that get EOL'd as soon as there's something more fun to play with, it's a paradise. Why don't people build things with a decade of life in mind? reply hollerith 14 hours agoparentprevIsn't the fact that people started using WP about 25 years ago evidence against \"you can always make it better\"? reply DrSiemer 12 hours agoprevHaven't used it a lot in the last decade, but WordPress was great for quickly setting up a basic website that needs a nice beginner friendly CMS. Just make sure you stick to the base theme with a custom child theme and only use a select few plugins (like custom fields and contact form), that don't stab you in the back or begin to fight with each other later on. I just still don't understand how Gutenberg is an improvement. It's so clunky and confusing to get even the most basic things done with it. reply tambourine_man 14 hours agoprevI like Wordpress and it’s the CMS of choice for almost every situation, but Gutenberg has been out for a long time and it’s still far from good enough. Both as an enduser and as a developer. reply stevesearer 14 hours agoprevWould be interested to hear other people’s favorite paid WordPress plugins. A couple essential ones for me are Advanced Custom Fields Pro and Admin Columns Pro. reply huxflux 13 hours agoparentGravity Forms - https://www.gravityforms.com reply todotask 14 hours agoprevNo matter how long it has been, on the one hand, WordPress can be seen as a victim of its success, as widespread adoption has led to various challenges such as security vulnerabilities and performance issues after maintining clients' website for the past 3 years. There are remaining security that is essentials and yet not many website does it correctly. reply modernerd 11 hours agoprev21! At this point WP is “boring technology” which makes it a great choice for many sites. It still does much that's hard to get anywhere else: - The core update process. It's long been one-click. Almost no other CMS or self-hosted framework offers as smooth an update process. (With Laravel, for example, I end up paying for Laravel Shift and even then it requires manual intervention that would be hard for a non-dev to handle.) For WP, services now exist to do automated updates with health checks and rollbacks to counter potential plugin incompatibility[1]. - The plugin ecosystem. WP went from \"democratizing publishing\" to \"democratizing user-owned sites and businesses\". From learning management systems to stores to paid newsletters, it's pretty cool what people with no programming experience can spin up. Things I'd love to see for WP in the next 21 years (that we'll probably get sooner if enough people contribute): - Built-in multilingual support. The web is global but WP isn't really, yet, except via third-party plugins. It's on the roadmap[2] but it's been a long time coming. - Improved education around Full Site Editing (FSE) and the new editor. The tools are getting good now, but there's still an education gap. Lots of people are helping to close this, though. Jamie Marsland's YouTube videos do a great job of showing what's possible with FSE, for example. [3] - Background batch processing/queues. These are only available via third-party solutions (and bundled with things like WooCommerce), but should probably be built into core. - SQLite support. Already pretty good but not officially supported in core yet. - Built-in site migrations. Also on the roadmap.[4] - Enhanced Playground tech. Distributing WP as a single binary for dependency-free local development (i.e. without Node.js) feels achievable and worthwhile. - Version control. It's too hard to store a WP site's state in a git repo and keep that synced with production and staging environments, especially when those with admin access can install and upgrade plugins independently of the repo. [1] Like Automattic's own scheduled updater on WP.com (https://wordpress.com/blog/2024/05/20/scheduled-plugin-updat...) and WP Engine's Smart Plugin Manager (https://wpengine.com/smart-plugin-manager/). [2] Current state of multilingual sites in WP: https://developer.wordpress.org/advanced-administration/word... [3] I like Jamie's videos showing how to recreate famous layouts with FSE: https://www.youtube.com/watch?v=WrdXCSIP578 [4] Site Transfer Protocol: https://core.trac.wordpress.org/ticket/60375 reply smooc 11 hours agoprevI'm getting old. I really thought for a moment that the title was about Word Perfect 2.1. Dann I miss the underwater screen. reply tommica 14 hours agoprevWordPress is special - though I don't work with it anymore, I have enjoyed seeing newer developers solve problems with it reply skibz 11 hours agoprevKudos, WordPress. reply dotcoma 14 hours agoprev> 1. Simple things should be easy and intuitive, and complex things possible. Yeah. Like, for example, forcing users to install the \"Classic Editor\" plugin to use, well, a normal editor that is more than sufficient for a blog. reply nikolay 14 hours agoparentOr one plugin no working with another and having to try all permutations to see which breaks which - that's simple and intuitive! reply mattl 13 hours agoprevI checked it out around the same time Mark Pilgrim moved to it. Movable Type came and went but to their credit they tried a GPL version but sadly OpenMelody never caught on. reply henning 14 hours agoprevWordPress is proof that clean code doesn't matter. Security doesn't matter. Out of the box performance doesn't matter. At least none of these matter if you are the 800 lb gorilla in the market and everyone else is trying to take a piece out of you. reply MangoCoffee 14 hours agoparent>if you are the 800 lb gorilla in the market but it didn't start out as an 800 lb gorilla. Simply saying it's an 800 lb gorilla in the room is lazy. reply 9dev 13 hours agorootparentNo, but there weren’t any good contenders at the time, and Wordpress quickly grew from that cute baby gorilla into the massive beast we have now. We could also say that Google didn’t start as a world-spanning empire of search engine moat. Yet it is now, and it’s hard to build a new search engine unless you’re a multi-trillion dollar company (and it’s hard even for them apparently). reply prmoustache 11 hours agorootparent> No, but there weren’t any good contenders at the time, Well yes and no. They were very few good blog focused CMS at the time. A contemporary one that comes to my mind and that actually predate Wordpress was Dotclear (which is still alive and still focusing on blogging). More general CMS like Typo3, Drupal, SPIP, Plone to name only a few open source ones were already around too and much more capable than Wordpress was. What Wordpress did very well at the beginning, was focusing on one thing: blogging for non technical people. It was a much more limited software but that made it much easier for so much people. And it came exactly during the blog boom and only with time became a more general purpose CMS. But if you had to build a website for your business it was a very bad choice at the time. Bottom line: making good software is one thing, but timing is everything. reply throw5345346 2 hours agoparentprev> WordPress is proof that clean code doesn't matter. At least it isn't Magento. reply nikolay 14 hours agoparentprevThe only things that matter are a decent admin UI and the illusion of choice of free plugins - most plugins are useless in 2024 because they have a pro version. When you add up the annual costs of all pro versions, the \"free\" WordPress (which requires a beefy hosting plan) becomes the most expensive and hardest to keep up publishing system nowadays! reply 9dev 13 hours agorootparentCan’t blame the plugin authors, a freemium Wordpress plugin is about the best possible source of passive income there is. Have a security issue? Take your time, they are coming anyway! reply nikolay 11 hours agorootparentI blame them as they usually don't know how to price their plugins to get real revenue. reply bboygravity 13 hours agorootparentprevFrom a business perspective the significant part of the website costs are man-hours for setup and maintenance. Got something to say about how WP compares to others when it comes to man-hours (assuming you can buy any and all plugins you want)? Honest question, because I have no idea about CMS. reply nikolay 11 hours agorootparentBut they overprice and they forget people use multiple plugins, so I use a third-party service that leverages GPL to offer most premium themes and plugins for a low annual fee. reply rafark 9 hours agorootparentSo basically you just want the functionality without paying for it. > they overprice and they forget people use multiple plugins People who complain about the prices also forget that a lot of these plugins take months or years to develop. You’re (we’ll not you since you made it clear you don’t pay the devs) literally getting thousands of man hours for like 60 bucks per year. reply jesterson 11 hours agoparentprev> clean code doesn't matter. Security doesn't matter. Out of the box performance doesn't matter. Absolutely so. Those are things for people to play, while the only thing taht matters is value. Does clean code provide value? No. Does security? No either. reply Pikamander2 10 hours agoprevWordPress is simultaneously amazing and terrible. At its best, it's a highly extensible free and open source CMS that's incredibly easy to set up and customize. For that matter, its core has reasonably good security and performance before third-party themes and plugins are added into the mix. The problem comes in with how many basic GUI-based features it's still missing out of the box 21 years later. Take, for example, the curious case of the lack of post cloning; Why is there still no \"Duplicate Post\" button in the core after 21 years? Why are over 4 million websites being forced to keep a third-party \"Yoast Duplicate Post\" plugin active in order to access a very basic CMS feature? The same goes for other GUI-based tasks like logging outgoing emails (WP Mail Logging), or viewing the scheduled cron jobs (WP Crontrol), or letting an admin temporarily switch to another user's account (User Switching), or downloading a one-click backup of the site regardless of host (All In One WP Migration), or managing the SMTP settings (FluentSMTP), or managing URL redirects (Redirection), or enabling SVG uploads (SVG Support). The fact that many of those tasks can be accomplished through small code snippets in the child theme is great and all, but that doesn't help the average WordPress site owner who is barely tech literate and would be more likely to break the site than successfully copy a hook over to the correct file. It's not uncommon to find WordPress sites with 50+ plugins installed, a good chunk of which are abandoned and have multiple code vulnerabilities, yet still find the time to clog up the dashboard with useless \"notices\", AKA advertisements for their other products. I could understand a lot of the missing functionality if WordPress was still a small FOSS project with no real funding and a few irregular volunteers, but the fact that's it's grown into what it is without any real plan to address those issues is just so frustrating. I've made a few small contributions to the core and read through a bunch of tickets for longstanding issues, and it's clear that time or funding aren't the problem; it's that the maintainers have an attitude of \"We don't personally need that feature ourselves, therefore it can just be a third-party plugin\", which might sound fine on paper, but... Ever clicked on a Google link, only to catch a quick glimpse of the real site before being redirected to an \"UPDATE CHROME NOW\" or \"CRITICAL MICROSOFT ALERT\" or \"CHEAP PHARMA PILLS\" website? There's a 99% chance that site is running WordPress and has dozens of plugins and got hacked at some point, and if you try to message the site's owner to tell them what's happening and how they can fix it, they'll think you're crazy or a scammer and leave the malware there because it intentionally hides itself to logged in users. I'm not exaggerating about that percentage, either; WordPress runs a massive chunk of the modern web (excluding major social media websites), and the failure to quash the need for so many common plugins has made them a goldmine for bad actors to inject redirects and SEO spam. reply joshbetz 2 hours agoparent> Take, for example, the curious case of the lack of post cloning The percentage of users that need this functionality is exceptionally low. If WordPress bundled every feature that some percentage of users could ever need, we'd have the opposite problem. The GUI would be an endless mishmash of features that most people don't need. > There's a 99% chance that site is running WordPress and has dozens of plugins and got hacked at some point I don't know about these statistics, but I take your point. Although I'm not so sure this is as big a problem in recent years. The WordPress team has made big improvements in improving the quality of the WordPress plugin repository. The reality is that WordPress is a big target. Even if they included all the features from the top 1000 plugins (which would cause an uproar), there would still be a huge market for plugins and some of those plugins would have security vulnerabilities. reply throw5345346 1 hour agorootparent> The percentage of users that need this functionality is exceptionally low. I am not sure about this -- I do know users who rely on post-cloning to update their sites. And I don't think it's unreasonable that this functionality should be fore. But what I would say is, it's not a slam-dunk as a piece of generalised functionality, though it might be possible to implement it fairly completely for the core post types. It would almost certainly need new core hooks. There are questions for example about who can clone whose posts -- do the editing and ownership mechanisms need updating, etc. And it might need reassessing in the Gutenberg era. I've found post cloning plugins to be an adequate solution here. reply zwaps 13 hours agoprev> Nah reply nikolay 14 hours agoprev [–] WordPress is the biggest bowl of spaghetti code in production! 21 years of carbicide! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "WordPress co-founder Matt reflects on the platform's 21-year journey, emphasizing principles like simplicity, dynamic content, community engagement, and robust infrastructure for plugins and themes.",
      "He stresses the importance of maintaining these elements while innovating in functionality and design.",
      "Matt shares a personal anecdote about the early days and highlights the crucial role of user feedback in shaping WordPress."
    ],
    "commentSummary": [
      "Critics argue that WordPress's backward compatibility and outdated methods hinder technical progress, frustrating developers with maintenance and migration challenges.",
      "Despite its flaws, WordPress remains dominant due to its extensive plugin and theme ecosystem, backward compatibility, and community support, though security and performance issues persist.",
      "Alternatives like Directus, Astro, and Laravel-based CMSs are recommended, with users suggesting better feature explanations and essential tools during installation to improve usability."
    ],
    "points": 409,
    "commentCount": 153,
    "retryCount": 0,
    "time": 1716864029
  },
  {
    "id": 40497106,
    "title": "Nonprofit Industrial Complex: Corruption and Mismanagement in American Cities",
    "originLink": "https://americanaffairsjournal.org/2024/05/the-nonprofit-industrial-complex-and-the-corruption-of-the-american-city/",
    "originBody": "Summer 2024 / Volume VIII, Number 2 May 20, 2024 The Nonprofit Industrial Complex and the Corruption of the American City By Jonathan Ireland The act of naming is always a form of propaganda. When you name something, you are never perfectly describing what it is, but are instead influencing how it is perceived. Marketers know this better than anyone. Prior to 1977, there was no such thing as the Chilean sea bass; the fish was called, instead, the Pata­gonian toothfish. The Chilean sea bass isn’t a type of bass at all, and most of them do not come from Chile. It was purely a marketing invention: an entrepreneur named Lee Lantz intuited that the American market might enjoy the taste of the Patagonian toothfish, but would never buy it under its given name. First, he chose to falsely call it a “bass” because Americans were comfortable with that type of fish. He then rejected the names “Pacific sea bass” and “South American sea bass,” on the grounds that they were too generic, and eventually settled on “Chilean sea bass” as a more exotic alternative. The name of one of the most popular fish in the world therefore has nothing to do with what the fish really is. A type of cod that is primarily farmed near Antarctica became the Chilean sea bass as a Goldilocks branding compromise. The familiarity of the bass was married to the perceived exoticism of Chile so that an American entrepreneur could sell a fish nobody had ever heard of to high-end restaurants in the United States. This ploy worked so well that today nobody has ever heard of the Patagonian toothfish, while the Chilean sea bass has a secure and inalienable position on restaurant menus from sea to shining sea. So its name is propaganda, but nobody cares. A lie that makes money will always be preferable to a truth that does not. Once you realize that every name is propaganda, it becomes readily apparent how much misconduct, greed, and corruption can be concealed behind an innocuously disingenuous name, especially a name that successfully evokes positive emotions in the general public. Consider the word “nonprofit.” Whoever came up with the idea of calling these organizations “nonprofits” was a marketing genius on the level of Steve Jobs. When someone hears the word nonprofit, they assume that such an organization is working for the public good; that it serves the homeless, protects the weak, exists for the benefit and the betterment of society at large. Hearing that something is a “nonprofit” immediately gives a sense that the organization is trustworthy and the people running it are driven by a charitable agenda. It’s a word that shuts down the critical faculties and grants an instantaneous moral stature to any organization to which it is applied. Consequently, non­profits receive a benefit of the doubt that would not be granted to any other form of private corporation. Yet nonprofit organizations are frequently the exact opposite of what they appear to be. As a consequence of the benefit of the doubt provided to nonprofits, there is rarely enough oversight to guarantee that they are doing what we pay them to do. In some cities, upwards of a billion dollars of public funds are paid to nonprofit organizations every year with glaringly insufficient safeguards to ensure that the money is used in a manner likely to serve the public interest. This money is then spent in ways that would shock the taxpayers whose hard-earned dollars are being effectively stolen from them. Non­profits that self-righteously declare themselves providers of homeless services actively lobby to make homelessness worse in order to increase their own funding; nonprofit organizations hire convicted felons—including murderers, gang leaders, sex offenders, and rapists—who go on to commit more felonies while receiving hundreds of thousands of dollars in government contracts; and the executives of nonprofits, the very people in charge of institutions whose stated purpose is not to make money, earn millions of dollars while catastrophically failing to deliver the public services we are paying them to provide. And as all of that is going on, the nonprofits in question receive tax breaks from the IRS, ensuring that the incompetent organizations wors­ening your city’s homelessness crisis exert their corrupting influence all the way to the halls of power in Washington, D.C. Money for Nothing There is a notorious nonprofit in San Francisco called the Tenants and Owners Development Corporation, or todco for short. The Tenants and Owners Development Corporation, despite containing the word “development” in its name, has not developed a single property in approximately twenty years. More and more, todco isn’t spending its money to help its current tenants, either. The San Francisco Standard found that todco’s spending on resident services declined from 62 percent of revenue in 2012 to only 45 percent eight years later. The Standard interviewed tenants in one of todco’s buildings and was deluged with complaints about decaying accommodations and rodent infestations. A woman told them bluntly that there were rats in the walls and complaints to management went nowhere; the tap water tasted foul and she sometimes found roaches in her food. One man felt a bite on his neck and assumed at first that he’d been bitten by one of the multitude of vermin that crawled through the building’s light fixtures; in fact, he’d been accidentally shot and the bullet hole was still visible in his wall when reporters interviewed him several months after the fact. It turned out that instead of spending money on housing development and tenant support, todco boosted executive pay and funneled millions into lobbying. As todco’s spending on its tenants declined by 17 percentage points, executive pay quadrupled. Meanwhile, todco’s president, John Elberling, launched the Yerba Buena Neighborhood Consortium, a political lobbying organization. Between 2012 and 2020, todco’s direct lobbying of legislative bodies increased 95 times, from $5,000 to $470,000. The Yerba Buena Neighborhood Consortium spent another $1.35 million on ballot referenda between 2016 and 2021, and within the small pond of municipal politics, that much money, if strategically deployed, can buy a shocking amount of influence. Here’s where the story gets strange. Although todco’s nonprofit status is predicated on helping poor people afford housing, todco lobbies incessantly to prevent the construction of affordable units in some of San Francisco’s most expensive neighborhoods. In 2018, todco sued to prevent the construction of a mixed-use building on the grounds that it would cast “new shadows” on a community garden; todco then agreed to drop this lawsuit after the building’s developer paid them $98,000, raising questions as to whether todco was merely using San Francisco’s byzantine permitting process to extract a bribe from another developer. In another case, todco lobbied to block a 495-unit housing development that would have included over a hundred affordable units. In other words, an affordable housing nonprofit has repeatedly sued other developers to prevent the construction of the same affordable units that it is supposed to be working to provide. And then, in July of 2020, the strangest of todco’s fiascos took place. That year, todco prevented the construction of over one thousand new apartments, including 350 affordable units, so it could run a “racial equity study,” which it then never bothered to conduct. San Francisco Supervisor Dean Preston—a todco political ally—convinced the land use committee to put off the development for six months, during which time todco would supposedly analyze the development’s impact on minority residents in the neighborhood. By August 2022, todco had not even begun the study that was supposed to have been completed eighteen months earlier. When asked by reporters from the San Francisco Chronicle why the study had never been done, todco’s president told the Chronicle that Covid interfered with their plans and a consulting group they’d been relying on had dropped out. Both of these excuses are highly dubious. When todco lobbied to delay the housing development so it could run its mythic study, Covid had already been rampaging through the United States for six months, meaning that todco was not blindsided by Covid and cannot use it as an excuse for failure. Furthermore, one consulting group dropping out should not indefinitely delay a study when the organization running it has millions of dollars in annual revenue, tens of millions of dollars in total assets, and a bevy of political connections. If todco wanted to run the study they could have done so, but they have exhibited no sense of urgency despite the fact that their failure to conduct the promised study has indefinitely prevented the existence of 350 units of affordable housing. Todco is a nonprofit whose mandate is to provide affordable housing. Over the last twenty years, however, todco has produced no additional units of affordable housing, has allowed the units it already owns to decay, and has spent millions of dollars preventing other developers from building thousands of apartments and hundreds of affordable units. Paradoxically, a nonprofit meant to provide affordable housing is spending taxpayer money to prevent affordable housing from being built; an organization that exists with the explicit mandate to help alleviate San Francisco’s housing crisis is instead working tirelessly to make that crisis worse. How can this be explained? To understand todco’s behavior, you need to know something about the business model of affordable housing nonprofits. An afford­able housing NGO makes more money as rents rise in the area where its buildings are located. Government subsidies make up the difference between what the NGO’s tenants are paying and what they could be paying if the building charged them the market rate. This means that a nonprofit, despite its name, has the same profit incentive as any other landlord, in that a lack of housing construction increases its profit margins by driving up rents. The only difference is that a nonprofit benefits from high rents through government subsidies instead of from directly charging its tenants. And that is an obvious conflict of interest. Nonprofit housing providers benefit financially if less housing gets built because high rents increase their subsidies. Affordable housing nonprofits are therefore incentivized to work against housing affordability if they want to increase the amount their executives get paid. Everything todco is doing is a natural consequence of the nonprofit industrial complex. Todco’s subsidies rise in concert with rents; it then funnels the money it receives from the government back into lobbying the same politicians responsible for funding it; those politicians prevent the construction of housing on todco’s behalf, thereby ensuring that rents remain high and todco rakes in millions of additional dollars in government subsidies; and todco’s executives receive enormous boosts to personal compensation and buy million-dollar houses in the suburbs. It is a kickback scheme so ingenious it would make Al Capone green with envy. This propensity for nonprofits to privilege their own finances over the needs of the poor is not unique to San Francisco. Last year, an initiative to establish a public social housing developer in Seattle was opposed by the Housing Development Consortium, a lobbying organ­ization for affordable housing nonprofits. The Seattle Times’ reporting on this initiative is revealing: The Housing Development Consortium, a lobbying group whose members include King County’s major low-income housing de­velopers, financial institutions and governmental development agencies, doesn’t want to compete with a new organization for funding. The Housing Development Consortium argues that Seattle should focus its resources on the existing system . . . which in­volves a collaboration between the largely federally funded Seattle Housing Authority, local public development agencies and other nonprofit organizations [emphasis added]. In other words, a lobbying organization for affordable housing nonprofits advocated against a social housing initiative on the grounds that it would direct funds away from its members. Many of the people involved in these affordable housing nonprofits would describe them­selves as socialists, and yet they argued against a more socialist form of public housing development and in favor of the privatized, complex, and inefficient system that just so happens to financially benefit themselves. In the event that America tried to dramatically increase the amount of public housing, a policy advocated by many progressives, it is likely that some of the fiercest opponents would be nonprofit landlords who would be outraged at the prospect of direct competition from government-owned housing. This is normal behavior for a private organization that relies on the government as its primary revenue source, but it is directly contrary to what the term “nonprofit” would seem to imply. Despite their trustworthy name, nonprofits are not now—nor have they ever been—the uncorrupted agents of the public good that their defend­ers would have us believe. Crime Inc. Exacerbating the profound conflicts of interest created by outsourcing government services to private nonprofits is the near total lack of oversight of these nonprofits, particularly with respect to how their money is spent and whom they hire to provide their services. It is a regular occurrence that money given to nonprofits is misdirected in a way ruinously contrary to the public interest. In especially egregious cases, money given to nonprofits finds its way into the pockets of people who never would have been hired by a government agency due to either a lack of competence or a disqualifying criminal history. For instance, San Francisco gave tens of millions of dollars to a nonprofit called the United Council of Human Services over the course of two decades; their CEO proceeded to spend large sums of money in a totally illegal manner, buying at least five vehicles for herself and family members and driving around with a trunk full of expensive jewelry. She also allowed twenty of her friends, family, and employees to occupy government-subsidized apartments which were meant to be used as housing for low-income San Franciscans. There is always going to be some level of malfeasance when large sums of money are involved, but what makes the United Council’s criminality inexcusable is that Gwendolyn Westbrook, the CEO who stole taxpayer dollars and spent them on herself, pleaded guilty in 1997 to stealing thousands of dollars in parking lot collections while working for the Port of San Francisco. A nonprofit run by a woman with a proven history of stealing from government agencies was given tens of millions of dollars to provide affordable housing services, and San Francisco’s political class was somehow surprised when, true to her previous track record, she stole that money, too. Following Westbrook’s fall from grace, a San Francisco Standard investigation discovered that the city had paid tens of millions of dollars to nonprofits that were ineligible for government funding: $25 million went to charities that were either delinquent or suspended and another $65 million went to nonprofits that were later declared ineligible. At the time of the Standard’s investigation, San Francisco had another $300 million of future contractual obligations to NGOs it was not legally allowed to fund. Although San Francisco is one of the worst cities when it comes to nonprofits behaving badly, these same problems exist in every city that makes excessive use of the nonprofit sector. Seattle, in particular, has a rather distressing tendency to give exorbitant sums of taxpayer money to convicted felons, up to and including violent criminals and registered sex offenders. In 2001, a man named Khalid Adams was convicted of first degree theft in an incident in which he allegedly groped his victim while shouting racial slurs; two years later Adams was convicted again, this time of first degree robbery and unlawful possession of a firearm. Adams’s third—but not final—conviction came in 2021 when he pled guilty to unlawful possession of a firearm by a previously convicted felon. Only a year after Adams’s third conviction, however, he was hired to work as a “violence interrupter” by a government-funded Seattle-area nonprofit called Community Passageways. In November 2022, while receiving a salary from King County taxpayers to prevent gun violence, Adams broke into his ex-girlfriend’s apartment, held her new boyfriend at gunpoint, and was subsequently shot by the ex’s eighteen-year-old cousin. Adding a surreal element to this already incredible story, Savior Wheeler, the young cousin who shot Khalid Adams, was a client of Community Passageways, one of the same at-risk young people that Adams was supposed to keep away from gun violence. A Seattle nonprofit therefore hired a three-time convicted felon who was fresh out of prison to work as a mentor for at-risk youths, and he was subsequently shot by one of those very at-risk youths while threatening an ex-girlfriend at gunpoint. This is a surprisingly common occurrence in Seattle. In 2022, the city gave $260,000 dollars to a registered sex offender who was operating under a fake name and credentials to mentor at-risk young people. In 2020, they gave a $3 million no-bid contract—one of the largest grants in city history—to a nonprofit called Freedom Project to run a “racial equity study.” Freedom Project’s executive director at the time was David Heppard who was convicted as a teenager for taking part in the gang rape of a pregnant seventeen-year-old. Freedom Project’s finance director, Quddafi Howell, once shot up a man’s house as an intimidation tactic to prevent him from snitching on Howell’s drug dealing. Believe it or not, the $3 million Seattle handed over to a bunch of convicted felons was somehow mismanaged. A local political blog called Seattle City Council Insight looked into the paperwork behind this contract and discovered that hundreds of thousands of dollars were handed out to subcontractors for minimal work and the project’s lead was paid $300 per hour while publicly claiming to be a volunteer. In another case, a homelessness nonprofit called share (the Seattle Housing and Resource Effort) turned out to employ an unlicensed accountant at the same time they were handling millions of dollars in government funds. Share claimed that they were the victim in this case, as they were unknowingly defrauded by a man who held himself out as a legitimate accountant. Fair enough. Share, however, might have noticed that their accountant was unlicensed were it not for the fact that share’s treasurer, the man presumably responsible for handling all their money, was Lantz Rowland, an unqualified homeless man who lived in a tent. This nonprofit had total annual revenue of approximately a million dollars in 2016, which included grants from King County and the City of Seattle, and the people responsible for handling that money were an illegal accountant and a sixty-year-old homeless tent dweller with no discernible qualifications. Leaving the West Coast for a moment, one finds similar outcomes arising from the overuse of nonprofits in the Windy City. In Chicago, an anti-violence nonprofit called CeaseFire has had a shocking number of its “violence interrupters” arrested for serious crimes since its incep­tion. This ought to surprise nobody given that these “violence interrupters” are almost universally convicted felons. Ceasefire workers convicted of crimes while employed by the organization include a fifty-one-year-old arrested for illegal gun possession, whom authorities alleged was also moonlighting as a national gang leader; a convicted felon caught naked under his bed with $50,000, an illegal firearm, crack cocaine, and heroin; and a woman who was hired despite four prior felony convictions and subsequently stole $10,000 worth of diamonds. Chicago also has a state-funded “Peacekeepers” program in which “community-based organizations” deploy former convicts to act as vio­lence interrupters during times when tensions are expected to be high. Last Memorial Day, a “Peacekeeper” named Oscar Montes assaulted a motorist while on duty, leaving the man with potentially permanent eye damage. Montes was hired by the Peacekeepers program only a year after serving a ten-year prison sentence for shooting a rival gang member. What is instantly noticeable about the above examples is that none of these people could draw a government paycheck unless it was laundered through a nonprofit. A police department could never hire a convicted felon with long-term ties to street gangs, but a private nonprofit has looser standards regarding who is allowed access to public funds. This not only squanders money on people who are not capable of performing the roles they’re assigned, but is an active threat to public safety in circumstances where the state uses convict-staffed nonprofits for duties that ought to be reserved for the police. The Progressive Doom Loop At the start of this piece, I said that “the act of naming is a form of propaganda,” an aphorism which applies to nonprofits because the name they’ve been given is a marketing device, rather than an objective representation of their conduct and behavior. It’s important to recognize, though, that nonprofits aren’t the only group relevant to this story that has been given an inaccurate name as a marketing ploy. The political ideology that supports the nonprofit industrial complex is generally referred to as “progressivism,” which calls to mind the socialist-leaning Progressive movement of the early twentieth century. In spite of sharing a common name, however, today’s “progressivism” has nothing in common with the Progressive movements of the last century, is not socialist in any real sense, and is, if anything, an extremist libertarian movement that destroys the ability of the government to function, rather than using state power for the betterment of the poor. Once you start digging into the evidence, you find that the places where “progressives” wield the most power are some of the least socialist governments in the country. In 2022, San Francisco spent $5.8 billion on private contracts, over 40 percent of all city government spending, while the entire budget of Houston, a city 2.5 times as large, was only $5.7 billion. It is a strange form of socialism that runs more than two-fifths of its government through private contractors, instead of using publicly owned developers and social housing. Portland, Oregon, meanwhile, has been suffering from a serious trash crisis for the past several years, due both to the city’s soaring homeless population and the government’s refusal to enforce antidumping laws. Portland’s response to the festering trash piles now blighting a once-beautiful city has not been to dramatically increase the government’s capacity to pick up and process garbage; instead, Portland, in conjunction with the state of Oregon, has paid millions of dollars to nonprofits to deal with the trash problem. As Portland outsourced trash collection to private nonprofit organi­zations, the ability of the government to collect trash has been gutted by budget cuts and a lack of resources. According to local activist Frank Moscow, Portland used to sweep every street as a matter of course, but currently only has one functioning street sweeper in the entire city. Not that it matters much, since Portland’s Bureau of Transportation sus­pended all street sweeping activities last June after another series of budget cuts. Adding to Portland’s trash-addled misery is the city’s inability to stop anyone from dumping their trash where it is not legally allowed to do so. In 2016, the city issued thirty-one citations for illegal dumping; in 2021, they issued a grand total of one citation, for a measly $154. An opinion column published in the Oregonian in 2022 asserted confidently that “you could dump 10 large bags of garbage in Pioneer Square tonight and drive off without fear of being caught or penalized,” before going on to complain that Portland picks up trash from residential units every two weeks, instead of offering weekly trash pickup like almost every other city of comparable size. This is the state of affairs in almost every city where “progressives” have a large impact on local politics. Progressives claim to support government spending programs, but also have an anarchistic, anti-governmental attitude that can be seen in their support for policies like police abolition in 2020. Although progressives want the government to fund public programs, their opposition to centralized state power means they often don’t want the government to run the programs being funded. The cities progressives control therefore tend to underfund core government agencies in favor of “community-based organizations,” by which they mean NGOs and nonprofits. Once the government can no longer meet its responsibilities, progressive cities outsource those ser­vices to nonprofit organizations, effectively privatizing the government. A serious problem then occurs. Using several nonprofits instead of one government agency is inherently inefficient due to weak oversight and an inability to take advantage of economies of scale. That’s why cities on the West Coast spend so much on homelessness to no end whatsoever. San Francisco’s spending on homeless services increased from $200 million in 2016 to an astronomical $1.1 billion in 2021. Despite this incredible investment, on an average night there were a thousand more homeless San Franciscans in 2022 than there were in 2015. In fairness, there was a decline in the city’s homeless population between 2019 and 2022, leading the deputy director at the San Francisco Department of Homelessness and Supportive Housing to declare that “investment works.” Yet San Francisco’s population fell by seventy thousand resi­dents between 2019 and 2022, meaning the minuscule re­duction in homelessness was likely a mere by-product of massive population loss. Nor are the problems that cities like San Francisco, Portland, and Seattle have with falling population unrelated. By funding inefficient nonprofits instead of more centralized, accountable government initia­tives, progressive cities have high taxes but poor services; residents receive nothing in return for the taxes they pay. Portland has one of the highest municipal income tax burdens in the country, but is forty-eighth out of the fifty largest cities in police staffing, has piles of festering, uncollected garbage littering its streets, and only has two thousand shelter beds for a homeless population of 6,300, requiring four thousand homeless Portlanders to sleep outside even if every one of them wanted a bed. Contrary to the conservative assumption that high taxes are an inherent evil, people are often fine with higher taxes provided that the taxes are utilized to improve local living standards. What is taking place in America’s most performatively socialist urban areas is that taxes are constantly raised in order to fund public services, resulting in some of the most heavily taxed populations in the country. But this tax revenue is then squandered on private contracts to unaccountable nonprofit organizations whose activities do little to rectify the problems they are nominally being funded to address. Taxes soar in concert with the collapse in local living standards and the decay in public services. Public parks where children used to play fill up with homeless drug addicts who leave used needles near jungle gyms; a six-year-old girl in California mistook a syringe for a thermometer and put it in her mouth, and an eleven-year-old stepped on a needle while swimming in Santa Cruz. The streets grow increasingly unsanitary due to spiraling homelessness, reintroducing diseases once considered eradi­cated by civilized living. Los Angeles had three deaths from flea-borne typhus in 2022, the first such deaths in three decades; a reverend who works on Skid Row lost both legs to an infection he contracted simply walking the neighborhood’s streets; and Portland’s Old Town recently experienced an outbreak of Shigella, a disease mostly seen in developing countries that spreads through fecal matter. The inability of nonprofits to properly manage services results in European taxes for third-world state capacity. Residents don’t know what the problem is: they don’t know that their taxes go to “violence interrupters” who are convicted felons; they don’t know affordable housing nonprofits use taxpayer money to lobby against affordable housing; and they don’t know money is being misallocated due to insuf­ficient oversight of nonprofits. All they know is they pay high taxes for no reason. And so they leave. Oregon’s largest county lost 2.5 percent of its population between 2020 and 2022. As the population declines, the tax base shrinks. San Francisco’s sales tax revenue fell 22 percent between 2019 and 2022, with the worst decline seen downtown, where office buildings now have record-high vacancy rates. Many articles have been written recently about the threat such cities face of an “urban doom loop,” in which a falling population guts tax revenue, which forces cuts to city services, thus reducing livability and causing an accelerating population exodus in a vicious cycle. To my knowledge, however, nobody has ever argued that a major contributor to this death spiral is outsourcing government services to unaccountable nonprofit organizations, rather than increasing state capacity and im­proving the government’s ability to solve problems itself. In this contrarian narrative, American cities are not failing because they’re too socialist; they’re failing because they aren’t socialist enough. And that, alas, is the state of the great American city in the early twenty-first century, where nothing is as it seems. Ours is a country where “progressivism” has nothing in common with the movement from which it takes its name; where “socialists” privatize government services at every opportunity; and where countless “nonprofits” exist solely for the misbegotten profit of the people who run them. Again and again, the names we use to explain the realities of modern urban politics are propagandistic marketing terms and not an accurate representation of what is taking place. Just like with the invention of the Chilean Sea Bass, however—which is neither Chilean nor a bass—many people are making money off the scheme. Who cares if it’s all a lie? This article originally appeared in American Affairs Volume VIII, Number 2 (Summer 2024): 134–45. You have reached your limit of 1 free article this month. Please subscribe to receive unlimited access to the site and continue reading this article. Have an account? Sign In About the Author Jonathan Ireland is a freelance writer and documentary filmmaker. Previous Lessons in Development: Revisiting Martin J. Sklar’s Corporate Liberalism By Justin H. Vassallo Next Progressive Geography’s Intellectual Dead End By Joel Kotkin",
    "commentLink": "https://news.ycombinator.com/item?id=40497106",
    "commentBody": "The Nonprofit Industrial Complex and the Corruption of the American City (americanaffairsjournal.org)246 points by lalaland1125 15 hours agohidepastfavorite210 comments tptacek 11 hours agoI sort of believe the thesis behind this piece, but it's written so viciously that I have trouble taking it seriously. For instance, however ineffective the \"Freedom Project\" in Seattle is, its finance director, who \"once shot up a man’s house as an intimidation tactic to prevent him from snitching on Howell’s drug dealing\", did so 20 years ago, served his sentence, went to college, then grad school at Penn State, and started a career. It seems extraordinarily unlikely that the crimes he committed when he was 21 have much of an impact on \"Freedom Project\" now. The author knows that, but includes the innuendo anyways, which makes me think he can't make a better case than that. Ick. reply michtzik 3 hours agoparentI strongly disagree with your thinking that the author can't make a better case. The previous sentence before the one you quote is this: > Freedom Project’s executive director at the time was David Heppard who was convicted as a teenager for taking part in the gang rape of a pregnant seventeen-year-old. And here's what the author COULD have said: > On February 18, 1994, Heppard and five of his friends spotted a five-months pregnant 17-year-old who is referred to in court filings as “J.H.” Cecil Morton III, one of Heppard’s co-defendants, proposed raping J.H. Morton stopped the car and he, Heppard, and a third member of the group forced the pregnant girl into the back seat. She begged them not to hurt her and Morton threatened her into silence with a machete; someone in the vehicle told her that if she didn’t shut up, they would kill her unborn baby. > They drove her into the woods where five or six of them raped her orally and vaginally. J.H. was then forced back into the vehicle completely naked and was ordered to direct them to her apartment. Some of the boys threatened that if she failed to do so, they would murder her and dismember her corpse. > Along the way, Heppard orally sodomized the terrified girl a second time and she was raped by two of his co-defendants upon their arrival. As the pregnant 17-year-old girl was being raped for the third time that night, the boys who weren’t otherwise occupied robbed her apartment, stealing a number of items belonging to her and her boyfriend. They then left the apartment, telling her that if she ratted them out, they would come back to kill her. > Once the boys were gone, their victim called a friend, who contacted the police on her behalf. Coincidentally, Heppard and his partners in crime were pulled over by Pierce County police officers at the same time the rape was reported. A call came over their radio regarding the assault, but the cops did not make the connection between the rape and the teenagers they were questioning; however, the rapists heard the report and were now aware that their victim had contacted the police. Heppard and Morton allegedly began plotting to have the girl murdered, but were arrested before she could be silenced. reply kbenson 3 hours agorootparentThat's a horrible story, but pulling it out seems to indicate you didn't understand what the GP was trying to convey at all. The point was that to use a crime from twenty years ago for which a person was convicted and served time for as indicative of problems with the current project he's the head of seems to indicate they could not not find anything relevant to the project itself to criticize. Doubling down on the details of that crime doesn't change that or counter the claim at all. If you want to make a claim that rehabilitation is impossible, or that he's not rehabilitated, then do so. If you want to state that it goes to the point being made in the article, that hiring felons with a violent history is common, then do so. I don't think the article is indefensible, but these additional details without context to make them relevant don't add much. reply tptacek 2 hours agorootparentprevThis would be a more compelling argument if the person you were referring to, who was 16 when this happened, wasn't open about their criminal background (it's literally part of their pitch at this nonprofit, which is significantly about prison reentry) or had in the intervening (checks) thirty years been accused of some other relevant offense. Instead, it's just another instance of exactly the discrediting rhetoric I'm talking about. What makes this worse is, this nonprofit does not look like a compelling use of public dollars, and an effective case could probably be made against it on the merits. Instead, it's just culture war bait. Since we're in a 50/50 culture, that means the entire argument is a push; we'll have to wait for someone else to move the dials on whether we fund organizations like this. reply michtzik 1 hour agorootparentOkay, I'm here to learn. > open about their criminal background (it's literally part of their pitch at this nonprofit, which is significantly about prison reentry) Can you point me to any resource where Heppard expresses remorse about his so-called \"criminal background\"? reply tptacek 33 minutes agorootparentIf you were here to learn and have a curious conversation, I don't think you'd ask me to respond to a rebuttal of an argument I did not make. That's fine, there's no reason we need to engage further on this! Our premises might just be too far apart. reply ta20240528 10 hours agoparentprevYou now know why libel and slander laws in the UK don't allow the factual truth to be an absolute defense. His history with the criminal justice system is a fact, but the intention in the article is to discredit him for a debt to his community that has long been paid. This is just libel. reply perihelions 8 hours agorootparent- \"This is just libel.\" It's not libel; it's just an opinionated argument that you dislike, so much so that you want to stop the other person from speaking. reply CuriouslyC 7 hours agorootparentI don't know. If you were to say the man is a drug dealer, criminal and untrustworthy based on something >20 years in the past, unless your language clearly indicates that stuff was LONG in the past it seems intent on causing material harm to the person by misleading readers which is literally libel. reply perihelions 6 hours agorootparent- \"it seems intent on causing material harm\" It's intent on changing public policy by persuading voters with argumentation. This is the most protected class of speech. That it publishes ugly true facts about individuals (which is also protected!) is an incidental step towards the author trying to paint a persuasive political narrative. (I can't help but want to break this down. You say it's misleading, but the case docket is hyperlinked inline with the text you say is misleading—that's far more than adequate in US jurisprudence. No \"reasonable person\" (a technical standard) is misled on a fact that's plainly disclosed, just one click away. You say the criminality is in the \"distant past\"—but the person's felon status is current, ongoing, and in fact relevant to the author's line of arguing, which is that these nonprofits are a form of \"laundering\" federal government grants to people the federal government otherwise disqualifies. You think (and I'm sympathetic to this) that crimes committed 20+ years ago are not strongly reflective of the current human. But, this is a subjective interpretation—not an objective truth beyond reasonable debate; and the fact that the US government itself disagrees, by permanently disqualifying felons shows how ludicrous it'd be to argue, in a US court, that \"he's a convicted felon\" is speech without social value). reply sotix 6 hours agorootparent> You now know why libel and slander laws in the UK don't allow the factual truth to be an absolute defense. Let’s revisit the original comment. reply perihelions 5 hours agorootparentLet's revisit the original post, which wrote something critical of a $3 million government contract. Should that be legally perilous? Should be people be scared to express opinions online about how their governments award 7-figure contracts, out of fear of the contract recipients hiring law firms and suing them? reply paulmd 57 minutes agorootparentprevWelll yes, and libel law incorporates precisely these sorts of protections! People would in fact get in field and kill each other precisely because “they disliked it so much they wanted to stop the other person from speaking”. Dead people don’t do much speaking, and libel/slander are a social response to the dead people. > 370 Falsely and maliciously accusing another. Sec. 370. Falsely and maliciously accusing another of crime, etc.—Any person who shall falsely and maliciously, by word, writing, sign, or otherwise accuse, attribute, or impute to another the commission of any crime, felony or misdemeanor, or any infamous or degrading act, or impute or attribute to any female a want of chastity, shall be guilty of a misdemeanor. Literally only have to imply the commission of a false or degrading act, or imply a woman is a slut. Why do you think that would be historically if not to keep people from killing each other over saying things they didn’t like. reply pfdietz 8 hours agorootparentprevEvery time I read about that I have a renewed love for the US Constitution's 1st amendment. reply paulmd 56 minutes agorootparentIn the long run, less-free societies using free speech as a Trojan horse to disrupt our social cohesion will probably be the end of the US. It’s not like anyone can feasibly invade or anything, and we’re the world reserve currency so it’s not going to be financial collapse. Meanwhile you see Russia and china very successfully exploiting the infowars/foundations-of-geopolitics tactics etc. Untrammeled free speech is quite likely to literally be the thing that kills the US. reply Intralexical 4 minutes agorootparentMeh. Ten years ago it was going to be the opposite. Unrestricted speech on the Internet was going to be thing took down every autocracy in the world. And for a while, it seemed to be working. You can't blindly extrapolate in a system made of agents that adapt to what happens around them. It's not like this is the first time the US has been in turmoil. reply bradleyjg 7 hours agorootparentprev> a debt to his community that has long been paid I recognize that this is a common, long standing idiom but it doesn’t make any sense. Inasmuch as he had a debt, even abstractly, it was to his victim. And sitting in prison at large net cost to taxpayer isn’t paying anything to anyone. reply flipbrad 6 hours agorootparentprevWhat do you mean? At least in England, there's https://www.legislation.gov.uk/ukpga/2013/26/section/2 reply auggierose 9 hours agorootparentprev> You now know why libel and slander laws in the UK don't allow the factual truth to be an absolute defense. That probably has more to do with Russian oligarchs being enabled to sue critical news organisations into bankruptcy. If factual truth is not enough, you are in deep shit. reply DontchaKnowit 4 hours agorootparentprevYou know what? Fuck that. I dont care if they served their time, I am still going to make judgements about someone for doing something so wicked, until proven otherwise. People can change, yeah, but they usually don't. Seems like he wised up and found a new way to fuck over his fellow man, legally. reply 123yawaworht456 6 hours agorootparentprevof course. malinformation is the biggest threat to a modern \"democracy\". reply scrubs 10 hours agoparentprevWhether viciously written or no, isn't the salient question: why give tax payer dollars to non profits if either (a) no net low cost housing built (b) the customers of non profit are displeased. reply mattmaroon 8 hours agorootparentYes, and think how much more convincing the argument would be if it relied on its merits (which are substantial) rather the what amounts to ad hominems. reply tptacek 4 hours agorootparentYes. This. I think the writing is doubly despicable for discrediting an important argument I agree with. Also I'm irked that the author has so little respect for his readers that he thinks none of them will Google any of the names in his piece. reply mattmaroon 4 hours agorootparentYeah. I am sure that arguments that appeal to people like us might be somewhat different than once that appeal to the general public, but this seems like the kind of argument that is aimed at people like us. I have done a good amount of work with not for profits in the last 10 years and have definitely seen a small piece of the dark side of the industry that he is talking about. reply tptacek 2 hours agorootparentI do a bunch of local politics stuff, and we have problematic nonprofits. You could write a compelling piece about poorly managed nonprofit contracts fucking up their missions, and it wouldn't need to rely on unrelated 20-year-old criminal records. reply darth_avocado 13 hours agoprevThis can all be solved if non profits are forced to transparently show where the money is spent. And before anyone says, they already do, they don’t. With the current rules you’re only required to show the numbers on how much money was “spent” on the cause, but not how. Meaning, for example you could claim 60% of the $100k budget went to providing services to the homeless, but in reality all that money was paid as salary to a person who gave the homeless haircuts, and the value of services received by the homeless could just be worth $2000. Such obfuscation allows charities to waste a lot of money on self enrichment. reply EasyMark 53 minutes agoparentAll nonprofits should have to pay a tax of like 5% on revenue/funding to pay for a government audit system that checks if they're really doing what they're supposed to be doing. Same for any government program/status/aid. There is currently not that much of a feedback system for the performance of just about any government based enterprise other than \"itself\", which always leads to failures. reply michael9423 12 hours agoparentprevYou solve this by dissolving all non-profits and seperating the state and the private sector completely. All tax money must be spent by government organizations or call for bids. Something like this should be illegal: https://www.vox.com/recode/22321861/jeff-bezos-climate-earth... First, Bezos steals money from hard-working employees and tax payers. Then he takes billions and funnels them into climate politics that no tax payer asked for. reply seanmcdirmid 12 hours agorootparentWhy should Bezos spending his own money on climate change be illegal? A non-profit simply is distinguished by the lack of profit. Since the USA only taxes profit for corporations, that means no taxes, but anyone could set one up to varying degrees of success. Employee salaries and any company they invest in that is for profit are still taxed normally. reply michael9423 12 hours agorootparentBecause the people pouring billions into policy changes that should be done transparently by tax-payer money also are the same who make the world an objectively worse place by creating a toxic work culture that leads to mental health decline, suffering, etc. and argue that you can't pay a worker more than a given amount and then as a consequence amazon workers can't even afford health care. But for some reason this system where money is \"so tight\" suddenly leads to billions in profit that can be just thrown away into unrelated activities independent from the company amazon. This is basically circumventing democracy. Money from publicly traded corporations is drained where it would be needed into activities that manipulate the public and don't have political oversight. It's especially ironic, as Bezos says \"We will emphasize social justice, as climate change disproportionately hurts poor and marginalized communities.” Well, how about starting with \"social justice\" in your own company. reply seanmcdirmid 3 hours agorootparent> objectively worse place by creating a toxic work culture that leads to mental health decline, suffering, etc. ... > and then as a consequence amazon workers can't even afford health care. Amazon will eventually solve these issues by completely automating their warehouses so that they no longer need to exploit poor workers anymore (who made more at Amazon than the previous job options they had available). Would that make you happy? Your kind of social justice feels toxic to me, actually. It is basically a \"perfect is the enemy of good\" argument that prevents us from making much progress on the problem at all. Just because Bezos sinks his own money into causes he is interested in does not suck away the capital that government could be investing in the problems instead. reply smeej 6 hours agoprevI had to make a personal rule awhile ago about not supporting nonprofits that are trying to solve an issue that could actually be made worse. The incentives are too misaligned. You can't count on people to do \"the right thing\" on a large scale at the expense of doing \"the right thing\" at the scale of caring for their own/their family's livelihood. One of the few organizations that made the cut for me is Hearts of Joy International. They help arrange lifesaving heart surgery for small children who have Down Syndrome, usually paying for the patient and one parent to travel from Uganda or the Philippines to India for the surgery. There really isn't a way to use policy to cause more children to be born with heart defects and Down Syndrome. Nobody's at fault when that happens, and the young child who needs the surgery is as innocent as it gets. Tangentially, I made another personal rule against supporting nonprofits that do their fundraising by focusing on how \"evil\" their opponents are and how dangerous things would be if the other side \"wins.\" That's meant not supporting certain organizations even when I agree with them that what their opponents are doing is evil. I just don't trust that any organization can overcome evil by focusing on the evil. Too many things can be justified in a \"battle against evil.\" If you want to win, you have to be focused on the good you can do, not the evil you prevent. reply smogcutter 14 hours agoprevMeanwhile in Orange County, County supervisor Andrew Do has apparently stolen millions in Covid relief funds via the “Viet America Society”, a do-nothing nonprofit nominally run by his daughter. https://laist.com/news/politics/orange-county-andrew-do-supe... reply polynomial 13 hours agoparentDidn't Orange County go bankrupt at least once? reply serf 13 hours agorootparentyeah in 94, and the reasons were ridiculous.[0] [0]: https://merage.uci.edu/~jorion/oc/case.html reply throwaway2037 12 hours agorootparentWiki says: > In 1994, an investment fund meltdown led to the criminal prosecution of treasurer Robert Citron. The county lost at least $1.5 billion through high-risk investments in bonds. The loss was blamed on derivatives by some media reports.[46] On December 6, 1994, the County of Orange declared Chapter 9 bankruptcy,[46] from which it emerged on June 12, 1996.[47] The Orange County bankruptcy was at the time the largest municipal bankruptcy in U.S. history.[46] reply 1oooqooq 7 hours agorootparent> orange county > mr citron is Disney writing this? reply fblp 8 hours agorootparentprevThis seems like similar market conditions that led to First Republics collapse (over investment on long term bonds that lose value when interest rates go up) reply tensor 13 hours agoprevIn case anyone is curious. This outfit has a right center bias, but is generally credible. https://mediabiasfactcheck.com/american-affairs-journal/ reply ch33zer 13 hours agoparentOnce you read past the first few sections that becomes exceedingly obvious. It basically turns into a rant. reply twic 4 hours agorootparentFrom a UK perspective, this could also be read as a critique from the left - someone who's an actual socialist, criticising pseudo-socialist progressives, but not so far left that it turns into a gobbledygook screed. I think that sort of person is rare as hen's teeth in the US now, though. reply rob74 11 hours agorootparentprevYup, especially this section sounds very true to brand: > nonprofit organizations hire convicted felons—including murderers, gang leaders, sex offenders, and rapists—who go on to commit more felonies ...because a criminal conviction is always 100% correct and should disqualify the person from ever being given a chance again, right? And of course, all of them will inevitably commit more felonies, after all they are born evil, they can't help it... reply twic 4 hours agorootparentThe whole bit about \"violence interrupters\" being ex-cons, as if that was bad, when in practice that's a required qualification. reply mediumdeviation 12 hours agorootparentprevI don’t think I’ve ever seen a right winger advocate for “increasing state capacity and im­proving the government’s ability to solve problems itself” reply roenxi 14 hours agoprevThe more subtle problem here is actually the idea of governments giving public money to nonprofits at all. The government doesn't need to be involved in this; the non-profit is already providing the organisational oomph to work out what needs to be done where and pooling contributor's funds. But the government's contribution here is organisation and pooling funding, so there is massive redundancy in mission. The only thing the government is providing is compulsorily forcing people to pay into the pool. So the fact that money is being spent on things that people wouldn't willingly commit to is actually the only feature of this system. IE, it is less unexpected than the author may believe. reply _nalply 13 hours agoprevSo some entities call themselves \"nonprofit\" but are shady enterprises. By calling themselves \"nonprofit\" they get some relaxation like tax breaks. They enjoy public trust. After all they are in business for higher purposes. Are they? In Switzerland there's the \"Stiftungsaufsicht\". That's a supervision office for trusts. Does the USA and its states know such institutes? The IRS should be really interested in not granting tax breaks to profit-oriented organizations masquerading as nonprofits. reply wahern 12 hours agoparentIn the US charitable trusts are usually supervised by state attorneys general, roughly similar to how foundations are overseen in Europe, IIUC.[1][2] But the non-profits discussed in this article aren't trusts, but charitable corporations. Similar to this, I believe: https://de.wikipedia.org/wiki/Gemeinn%C3%BCtzige_GmbH Their treatment is much closer to regular business entities in terms of scrutiny of administration and expenditures, so long as expenditures nominally serve the charitable purpose, and it's really only the taxing authority who cares one way or another. [1] I'm relying on https://de.wikipedia.org/wiki/Stiftung [2] In practice oversight of charitable trusts is minimal unless someone complains and there are resources to investigate. As is typical in common law jurisdictions, the rule is ask for forgiveness, not permission. Charitable trusts are typically only scrutinized when a beneficiary complains, AFAIU. Anyhow, at least in the US charitable trusts are basically just a large pile of assets with an administrator who writes checks to charitable corporations, so there's not much to oversee and illicit self-dealing is relatively less common. reply 1oooqooq 7 hours agorootparent1. donate money to widely known corrupt ngo 2. now you're beneficiary, sue. 3. ??? 4. profit reply nsajko 12 hours agoparentprev> some entities call themselves \"nonprofit\" but are shady enterprises Aren't most? reply geysersam 12 hours agorootparentWhy would you think that? reply nsajko 11 hours agorootparentThe very concept is abusable. Tax benefits, funding, etc. attract abuse. reply twic 4 hours agorootparentYou're making a claim about the world. It's better to support such claims with evidence, rather than your feelings. reply unix_fan 13 hours agoprevThis is extremely common in Third World countries, where I come from. In my view, both nonprofits and large religious organizations attract the same kind of people. People who want to create their own little thiefdom, by taking advantage of peoples faith and Good will. reply 1oooqooq 6 hours agoparentexactly. being from the periphery of world economies, this right wing rant reads like \"you can't do in my backward what we do in Africa\". reply mrcartmeneses 8 hours agoprevThe key problem here is that by privatising government services in the form of a non-profit there ceases to be proper democratic oversight of the organisations involved. The possible solutions are therefore very obvious, either bring these services back into direct local government control or in the case of a tenants association require it to be tenant-owned and democratically governed. reply arethuza 8 hours agoparentYou can also have social housing operated separately from central/local government control but have a strong regulator - this is how it works in Scotland. reply wnc3141 14 hours agoprevI think the general rule of thumb is, wherever money moves to, people will absorb it - in the same way bacteria or fungi absorbs biological material. This is not often greed but the result of a chaotic system of (mostly) rational actors trying to eek out a living. This can not only be seen by the masters degrees available - (higher ed. Administration , healthcare administration , etc.) but also ironically by those higher education institutions themselves. As student financing dollars go toward schools, people are there to absorb those dollars through employment in higher education and it's ecosystem. One could argue the merits of having decades of student future salaries garnished to subsidize employment at these institutions, but try explaining that to someone with a career in higher education. This is just one example of the push and pull of providing employment compared to the mandate of efficient production. Of course the stakeholders funding institutions must decide the merits of employment programs (in the form of non profits) vs. the merits of the production of the institution. The answer, being complex may only be born though strong citizen engagement in civic processes where one advocates for and directs the mandate of that institution. reply throwworhtthrow 13 hours agoparentEke, not eek. reply 1oooqooq 6 hours agoparentprevwhy go that far? did you ever sit down an economic course? under or grad, doesn't matter. specially if it is focused on development policies or econometry. its all a joke. they have just enough math that the average person struggles, then they drop the last steps and approximate everything in a way that completely defeats the purpose of using math at all! its all just to hand wave while pretending to be a science. they still base all econometry knowledge on crap like the RAND workforce study (1960) or something. it's all a cult full of useful idiots who think they are making a difference because they laboured up to 80% of a couple derivatives. but on the end, it does work for race to the bottom after short term profits. so... reply pyuser583 14 hours agoprevThis article feels more like a piece of heavy journalism than a political op-ed. The facts it shares are fascinating and damning. I never thought I’d care so much about “Chilean Sea Bass.” reply WesternWind 13 hours agoparentNot badly written but it's poorly supported, and argued mainly from anecdote. There's no engagement with subject matter experts, there's cherry picking of data points, and the suggested solution isn't supported by provided data. It's like if I said, build systems are awful and too much dev time goes to maintaining them, a build system was almost used in a clever hack by a state level actor, Kevin Mitnick, a Convicted Felon, used Make when coding hacking tools he used in his crimes, build systems are a problematic abstraction and we should avoid unnecessary abstractions in coding, and therefore we should call compilers directly from the command line. Maybe we should do public social housing like this article suggests, I actually agree with that, but the above is hardly a measured article in support of that, anymore than I laid out a great case for invoking a compiler directly. reply selimthegrim 8 hours agorootparentThis sounds like a late 80s Pravda article. reply ch33zer 13 hours agoprevThis article decries basically all nonprofits. This is throwing the baby out with the bathwater. There are very effective and important nonprofits out there, just like there are scammy ones. The solution is to set constraints on which non profits the city is allowed to contract with. Limit executive pay to no more than 5% of income, limit overhead to no more than 15%, set contract benchmarks for performance by objective metrics and you can weed out the bad actors. My fear is that, as the article (IMO correctly) points out politicians and the bad actors are making too much money for this system to be implemented. reply yegle 13 hours agoprevI recently learned the concept of B Corp (a type of for-profit corporation ) that is closer to what most people think a nonprofit is: https://en.wikipedia.org/wiki/B_Corporation_%28certification... reply 1oooqooq 6 hours agoparentsame, with one more layer. this is iso9000 but for eco-conscious consumers instead of investors. want to sell the torment Nexus, but need wholefoods to place it next to the organic avocados? bcorp was made exactly for this. just pay certification to this non profit. *shrugemoji reply pkaye 13 hours agoprevI know some people call it the Homeless Industrial Complex here in California. reply TMWNN 13 hours agoparentThere is absolutely a homeless-industrial complex (HIC). The city of San Francisco spends $70,000 annually per homeless person![0] The homeless there are homeless because severe mental and addiction issues cause them to reject help, not because resources aren't available. And no, weather is not the most important reason for why so many homeless come here. If it were, San Diego would have a worse homeless problem than San Francisco. The most important difference is that the city of San Diego spends one third as much as San Francisco per homeless person. ($46.8 million in city spending[1] for an estimated 1900 homeless.[2]) The HIC uses every virtue-signaling, heart-tugging propaganda tool at its disposal to increase the flow of money thrown into its bottomless maw, despite no metric ever improving one bit whatsoever. [0][1][2][3] [3] The count is only of the homeless in downtown San Diego. While presumably most homeless San Diegans are there, just as there are very few San Franciscans living on the sidewalk in Pac Heights, the point is that a larger number would mean that much less spending per homeless person in the city as a whole, and that much more glaring a discrepancy between the two cities. reply EarthAmbassador 13 hours agoprev600,000 homeless times $10,000 per tiny home, clustered where social services are made available, voila, the problem is solved for $6 billion, which is nothing, excluding the cost of those social services. The issue is really there is a lack of genuine desire to solve the problem because the cruelty and baked-in lies within American self-reliance philosophy put such a solution outside the Overton window of what is possible. Instead of a one-time investment, we dump more than $6 billion into the problem but never solve it. reply JohnMakin 13 hours agoparentThrowing money and houses at the problem has failed many, many times before. “just put a bunch of houses where social services are” is profoundly unrealistic. are there any areas that aren’t already developed heavily that would even qualify? social services are already criminally underfunded. Even if you could do this for your extremely underestimated price tag, getting the “chronically” homeless (the people on the street we typically imagine as the homeless) to maintain a property without being a nuisance to neighbors and actually use the social services would require an entirely new social service of its own, with legions case workers being assigned to people, etc. I’m not saying we shouldn’t build more cheap housing near social services, but I think statements like this profoundly underestimate and trivialize a problem that goes very deep - namely, the complete lack of societal safety nets and access to quality healthcare, all of which is exacerbated by a lack of housing. reply lupusreal 2 hours agoparentprevYou make it sound so simple, but the devil is in the details. You've not even gone into the high level details, let alone the low level details. Each level adds more complexity and more cost. At the highest level, obviously the problem can't be solved with a one time purchase of 600k homes. The homes will be worn out and require replacing, some of them very rapidly, and their number will need to grow with time as new people requiring homes come to the city or drop out of regular housing. How many more homes do you need to build each year after the initial batch? Furthermore, this number of homes has an infrastructure cost which isn't captured in your 10k figure. How much does it cost to hook each one into the grid, water and sewage? Without these they will be spoiled immediately. And what of the land cost? Tiny homes aren't usually high rises, so you'll be creating tiny home suburban sprawl. Where do you put it? It should be close to city services, otherwise you might as well make it a camp in the wilderness. But it would be an inefficient use of space close to the city where land is generally valuable, and the cost of the land needs to be factored into it. Furthermore you need to consider the impact to property values this housing project will have. Reduced property values means reduced revenue, from property taxes, from the city. This should be included in the cost of the project when proposing it. reply romafirst3 12 hours agoprevNonprofits in the USA are basically failings in government. Non profits at their best provide services that any half decent government should provide but do it at a fraction of the efficiency that the government could. At their worst they are private individuals spending tax payers money (that’s what tax breaks actually are) on personal causes and self enrichment. That’s why it’s interesting seeing a right wing publication advocating for fewer non profits, I’m all for it. Cut tax breaks for non profits, reduce funding of non profits and fund government to provide the services. reply kayo_20211030 6 hours agoprevI agree in general, but non-profits themselves have very little to do with the argument, which seems to be that the outsourcing of what should be rightly considered government services for the common good results in gross and unpunished waste. Whether the organization receiving the public largesse is for-profit or a non-profit seems beside the point. Government administrations, at all levels, have abdicated their responsibilities to the citizens, by indulging in this form of wasteful theater. A perfect and horrendous confluence of neo-liberal and faux-progressive ideologies has led us to where we now stand. reply 6510 5 hours agoparentI regularly visualize what monumental undertaking it must have been to run a country with a paper administration. Today we have applications a million times more complex running on low end boxes. Small countries don't even compare to the big tech user database. It seems what little arguments the abdication had going for it self require a context before 1990. reply niemandhier 13 hours agoprevI believe that this can be solved least for housing. If you construct a housing cooperative that has the following properties the entity should be incentivized to maximize the number of housing units: 1. Each tenant must own shares of the coop. 2. Shares per member are capped. 3. Profit must be reinvested 4. New shares can only be issued for new units build. Rather than subsidizing rent, the government can subsidize the building of new housing, the corresponding shares can be handed over to new tenants. Since tenants are shareholders, the system stabilizes at a point where rent and living conditions are acceptable. In Germany this type of construction is called a “Wohnungsgenossenschaft”, and receives some tax incentives. Currently about 3 Million people are living in flats provided like this. It’s probably one of the only working pieces of socialism. reply throwaway2037 12 hours agoparentHat tip: Wiki/English: https://en.wikipedia.org/wiki/Housing_cooperative#Germany New York City also has co-ops but for very different reasons: To screen potential owners. reply burnished 14 hours agoprevIf true I think this should lead to charges and jail time, sounds lime fraud and embezzlement on a massive scale. reply blackeyeblitzar 14 hours agoparentWhat do you do when the “embezzlement” is supported by local elected leaders and not prosecuted by sympathetic / politically aligned city attorneys? There’s no real method to create accountability or enforce the law, and everyday people are too busy with work, family, etc to do anything much about it. That is if they even know it’s happening. That’s one problem with the death of local journalism and the general lack of time in everyone’s lives. reply wnc3141 13 hours agoprevJust noting the tilt of the article, \"American Affairs Journal\" was founded explicitly to be a conservative platform. reply brynb 13 hours agoparent“just noting” that this has no relevance to a sober and objective review of the points made within. i sometimes think that we’re lucky we still have two sides of the aisle in this country. i constantly think we ought to aspire to more reply throwaway115 13 hours agoparentprevWhat's your own political tilt, so that I can \"just note\" it? reply wnc3141 13 hours agorootparentIt's important to note the tilt of opinion pieces that color issues of civic concern in any direction. reply throwaway115 13 hours agorootparentCan you not evaluate the contents of an article on their own merit, without first biasing yourself for or against the author? Of everything the article mentions, you seem to think it being on a \"conservative platform\" is the most noteworthy. reply baggy_trough 13 hours agoparentprevThe horror of it all! reply benzible 14 hours agoprevIn case you're wondering where this is coming from, see: https://www.newyorker.com/news/news-desk/a-new-trumpist-maga... (https://archive.is/ymp9I) reply MilStdJunkie 13 hours agoparentAuthor Jon Ireland AKA \"SwannMarcus\" on Xitter, incidentally. As my uncle might have said, \"Whuff. That's a good 'un\". reply j7ake 14 hours agoprevWhat a revealing and detailed article. I’m subscribed reply joemazerino 14 hours agoprevGlad this is being written about. Too often non-profits get a pass on perceived altruism. BLM in particular has been egregious. Not a single positive thing or a dollar has been spent to actually improve black neighborhoods but the leaders have enriched themselves. https://www.independent.co.uk/news/world/americas/black-live... https://nymag.com/intelligencer/2022/04/black-lives-matter-6... Marilyn Mosby, Baltimore’s former top prosecutor and a BLM activist, was convicted of perjury and fraudulently claiming Covid hardship to get tens of thousands of dollars for her vacation homes. https://thepostmillennial.com/no-jail-time-for-former-dem-ba... A BLM activist who was the diversity executive at Facebook and Nike has been sentenced to more than five years in prison for stealing over $5 million. https://thepostmillennial.com/diversity-exec-scams-facebook-... reply superb_dev 14 hours agoparentWere they just “BLM activists” or did they hold actual power in a BLM org? Only the latter seems relevant reply tick_tock_tick 13 hours agorootparentThey are the BLM org now you might not view that org as being legitimately \"in-charge\" of the BLM movement but cities and politicians giving money sure did. reply pseudalopex 6 hours agorootparentsuperb_dev was asking about the 2 people joemazerino called BLM activists probably. Marilyn Mosby is not an organization. I found no connection to any BLM organization. It seems some people call her a BLM activist because she prosecuted the police officers involved in Freddie Gray's death. Barbara Furlow-Smiles is not an organization. I found no connection to any BLM organization. joemazerino copied a sentence Andy Ngo wrote about her. Why Ngo called her a BLM activist was not clear. Black Lives Matter Global Network Foundation is not in charge of the BLM movement regardless of how many politicians or reporters they confused. And The Movement for Black Lives raised more money. reply blackeyeblitzar 14 hours agoprev> Yet nonprofit organizations are frequently the exact opposite of what they appear to be. As a consequence of the benefit of the doubt provided to nonprofits, there is rarely enough oversight to guarantee that they are doing what we pay them to do. This type of unaccountable grift is common in Seattle. During the height of BLM, the city gave lots of money and even public property to random organizations. Taxpayers hard earned money was not just diverted away from core city needs but not really tracked. The same has been true of unaccountable programs to combat homelessness (the “homeless-industrial complex”), with little in terms of metrics to understand what was happening. A lot of this happened because most citizens are too busy to keep track of these things, while activists push for spending on their ideological pet projects, and activist city leaders respond by handing out checks without any competitive process (picking sole winners of contracts) or real public input (hearings are swarmed by activists and held at inaccessible times and the general public isn’t even aware they’re happening). Some example sources to read more: https://www.seattletimes.com/seattle-news/politics/state-aud... https://sccinsight.com/2021/07/29/the-black-brilliance-resea... https://roominate.com/blog/2016/anatomy-of-a-swindle/ reply kreelman 14 hours agoparentThis happens in Australia too. The nursing home industry here seems a little like what the article outlined for the United States. Like anything, there are good and bad examples. Seems inevitable that there is some corruption in the spirit of it. Don't know how you'd police that.... TLDR on the article. I'll markov summarise it and see if it thought of some ways around the bad players. reply hiddencost 14 hours agoparentprevMeh. I don't believe you. I serious doubt a bit of money for black people is breaking the bank for a city the size of Seattle. reply blackeyeblitzar 14 hours agorootparentWell you might view it like that, but to me our taxed money is earned by hard work and seeing it wasted on someone’s political goals or activism or grift is offensive to me. And the city doesn’t have money - it’s about to close like 20 schools because of it. And much of the city is ridden with potholes and missing basics like sidewalks. The police force is less than half the size it is supposed to be. The fire department is heavily staffed by volunteers. https://komonews.com/news/local/seattle-public-schools-sps-e... reply vaidhy 13 hours agorootparentYour disagreement with a policy does not make it corrupt. Being part of a democracy involves compromises and often times, you pick the battle you want to win. This article was about explicit corruption where the public funds are transferred into the hands of a few under dubious reasoning. reply zmgsabst 13 hours agorootparentThey’re closing schools but transferred millions to racist grifters on dubious grounds in violation of public funding laws. And that’s ignoring such race-based policies are explicitly illegal under Washington law. reply zmgsabst 13 hours agorootparentprevThis is taking money from hard working black people and giving it to a handful of racist grifters. Further, it deprives their schools, police, EMTs, etc of tax funding from the wider city to enrich grifters — and these are communities that already struggle with funding. Your comment only makes sense if you view “black people” as an amorphous blob, essentially defined by their ethnicity and not as individually distinct people. Yuck. reply DoreenMichele 14 hours agoprevNon­profits that self-righteously declare themselves providers of homeless services actively lobby to make homelessness worse in order to increase their own funding This is unfortunately all too true. A primary root cause of homelessness is lack of affordable housing. We should be working towards policies and solutions that foster more \"market rate\" affordable housing, not affordable housing via government programs or nonprofit organizations. reply violet13 13 hours agoparentHomelessness, especially in places such as the SF Bay Area, really doesn't boil down to just affordability. Yes, there are some folks who just faced economic headwinds and are living in a car while trying to find a way out. But there is also a huge population of people who couldn't function if given keys to a free apartment. For one, drugs won the war on drugs, addiction is a big part of the problem, and we don't really know how to fix it; harsh punishments don't work, quasi-decriminalization isn't a success, and treatment for people who don't want to be helped is hard. We also don't like to institutionalize people anymore, so folks with severe mental illness often end up on the streets too. reply derefr 12 hours agorootparentI can’t speak for general solutions, but at least one portion of the addicted just have chronic pain; were prescribed extremely strong and addictive painkillers (as the only thing that would work!); and then got cut off from the medical system. These people try and try to solve their debilitating chronic pain problem (which they still have, and likely will always have) through increasingly-desperate and illegal measures; and go through many harrowing things due almost solely of the illegality of acquiring these same drugs outside of medical channels: the difficulties of finding a source and potential for arrest; the income-eating expense (no insurance to cover costs, plus 10x risk markup); the heightened spike-dose addictiveness of street forms of these drugs, that leads to a quick fiending withdrawal and need to redose, leading in turn to loss of employment due to spending all your time on the street hunting for the next dose; and of course, the unpredictable dosing and potential for adulteration, leading into high potential for OD. Most of this particular problem can (and in some trials, has!) been solved just by prescribing these people the drugs they need again. When you go from unpredictably doing random shots of freebase heroin/fentanyl/etc with a dirty needle alone in an alley, back to predictably being able to get precisely-dosed extended-release pills from a pharmacy and take them on a set schedule, a lot of “addict behaviors” for these chronic-pain “addicts” just evaporate. reply tpmoney 6 hours agorootparentThis is definitely a problem. Turns out once you start getting to the \"going to destroy your liver / stomach\" levels of NSAIDs and Tylenol, there really isn't anything other than opioids for pain management. Unfortunately the war on drugs and America's latent puritan streak means that since some people in some places use opioids to get high, then all uses of opioids should be avoided whenever possible, and when they are used, they should only be used grudgingly and with extreme skepticism of the person receiving them. Surely nothing will be better for the health of an individual than barely managing their condition, constantly forcing them to stop effective treatment to make sure they're not growing tolerant of the medication and treating them as only slightly more trustworthy than a criminal conman. Sure, opioids are addictive, and you might find yourself in a situation where you're stuck on them for life and that's not great. And ultimately we have to ask \"so what?\" There are many conditions and medications that are lifelong and we don't treat the patients or their conditions the same way. Imagine telling a person taking SSRIs that they can't have a higher dosage because they're getting \"tolerant\" of the medication. Imagine telling a diabetes patient that you're not going to give them metformin or insulin because they might be on it for life. There are huge amounts of chronic conditions for which the ongoing treatments suck and have a lot of negative side effects, but living with the condition un-treated is worse than the negative side effects. Chronic pain (and the conditions causing that pain) seem to be the only category that we don't accept the possibility of long term negative consequences as the price of dealing with the chronic condition. And the hell of it is, I agree with concerns about pill mills. My family that has had to deal with this has had to deal with pill mills too. And it's bad just having higher and higher doses thrown at you. I agree that treatment needs to include more than just escalating opioid doses. But those same family members that were stuck in a pill mill were there for years longer than they needed to be because finding a way out was nearly impossible. If you want to change pain management doctors, 99% of them will not prescribe you opioids on your first visit. But you likely have a contract with your current pain doctor that says you won't go to other doctors for pain medications, so you can't see a different doctor while getting pain management from your current doctor. Then even if you could get them to prescribe your medications, they almost all want to start you from zero again. Imagine you want to change heart doctors but before they will treat you they want you to stop all your heart medications for a few months so that they can \"get a baseline\" for your condition. That would be an insane thing to ask for any other chronic condition, and yet that's a common thing to be asked of pain patients all the time. Overall as a society we're terrible at dealing with the concept of a chronic condition. We don't really grok the idea that some folks just won't ever \"get better\", and our entire system is set up to assume you will. The sad reality is some people are going to be in constant pain for their entire lives, and there's nothing we can do to stop that from being the case. Restricting these people from being able to safely access treatments to manage that pain because they might get addicted is misguided, cruel and missing the forest for the trees. reply bruceb 12 hours agorootparentprev\"harsh punishments don't work\" They do if they are very harsh, see Singapore, Indonesia etc. America doesn't want to that. reply ceejayoz 7 hours agorootparentIndonesia? The country that has active guerrilla wars that date back to the 1960s? reply mistermann 12 hours agorootparentprev> For one, drugs won the war on drugs What a great line. reply jeffbee 13 hours agorootparentprevWrong on all points. The places with the most addicts (W. Virginia) have the least homelessness (W. Virginia again). There really isn't any more too it than supply and demand. The only reasons that is seems like all homeless are addicts are 1) only a small fraction of homeless are obviously homeless, and 2) addicts are less able to cope with high housing costs. But that's a marginal effect; plenty of drug abuse happens within the community of housed people. reply shuckles 13 hours agorootparent> The only reasons that is seems like all homeless are addicts are... The Point in Time count data indicate the vast majority of homeless people have issues with drug abuse. A substantial majority of those also have dual diagnoses. Maybe you are trying to argue that drug abuse is not sufficient, or something, but you have not brought that nuance. The OP is more correct than you: drug abuse has a large role to play in the street homelessness of San Francisco. (An alternative view: if rents were the primary causal factor, then why is the problem substantially worse than a decade ago when San Francisco rents peaked? Why did the problem get so much worse during COVID when nonpayment evictions were held for many years and low income renters got billions in cash transfers, increasing their aggregate income while rents dropped?) reply DoreenMichele 13 hours agorootparentMaybe you are trying to argue that drug abuse is not sufficient, or something, but you have not brought that nuance. Millionaire rock stars go in and out of rehab repeatedly and don't end up homeless. There is no direct cause-and-effect relationship between drug abuse and homelessness. We only make that connection after the fact. I know of zero credible sources predicting homelessness based on \"He's an addict! So it's inevitable!\" There are lots of problems with the stats we keep on homeless people and such people are by their very nature tough to track. There are political agendas driving how the questions get asked and the data gets framed, all of which contradicts my firsthand experience with homelessness and what I have heard for years from actual homeless people. In a nutshell: California has about 12 percent of the US population, 25 percent of the US homeless population and more than 50 percent of the nation's unsheltered homeless. I do not find it credible to claim these people are all \"locals\" and I firmly believe California is the dumping ground for the nation's homeless problem. reply brynb 12 hours agorootparentgrapevine knowledge, but i have been told (and credibly, i think, given how adaptive humans are, and how they adapt for very specific parameters while devaluing others) that many homeless will go to great lengths to seek out new locales that seem willing to help sustain their needs without requiring wholesale change of their lifestyles. some people do indeed prefer it (or at least fear the alternative, i.e. “proper” integration into society and all that comes with that), as much as some people on this website cannot fathom that reply throwaway2037 12 hours agorootparentprevI've seen you post here many times on this topic. I always enjoy your posts and learn something new. Thank you to share. Your last \"nutshell\" paragraph: I have read similar from other sources. Deeper question: Why? My thoughts: The weather in California makes it possible to be homeless, full time (12 months a year), without shelter -- not great, but not death by freezing. What do you think? I wonder if Hawai'i and Florida also have very high proportion of homeless people for similar reasons. reply DoreenMichele 5 hours agorootparentWeather is absolutely a factor. It's temperate and dry in some parts of California, making it much easier to just camp in a tent than places with freezing temperatures, lots of rain and snow. reply smugma 12 hours agorootparentprevIs this because of our high cost of housing, increased social support infrastructure for the homeless, or the temperate climate? Yes. reply 01100011 11 hours agorootparentprevA lot of folks in Appalachia seem to live in conditions tantamount to a tent encampment. They're more spread out and not in the way of urbanites so we don't talk about it much. reply EasyMark 27 minutes agorootparentIt also helps the cost of living is 1/4 what it is in a place like SF reply zeroCalories 13 hours agorootparentprevit's really hard to afford a sf apartment AND a crippling drug addiction, for several reasond reply chrischen 13 hours agorootparentI’m not sure if getting rid of the addiction would automatically make SF houses affordable. Certainly doesn’t help. But it’s also not SF’s job to house the entire nation’s homeless population. reply waveBidder 13 hours agorootparentit seems like it is if they're former SF residents priced out of their homes. reply seanmcdirmid 13 hours agorootparentHomeless services can’t discriminate based on previous residency, they aren’t even allowed to ask. HUD has some residency requirements, but they are only loosely enforced. A lifelong resident of SF are often competing for the same resources with ex-cons who just got off the bus after being released from prison in Texas given only an open bus ticket. That being said, a resident of SF has many more other ways of avoiding the streets (and still be considered unhoused) vs that ex-con, so the numbers are going to be lopsided if we are just counting the visible homeless problem. reply fragmede 8 hours agorootparentThat's weird. I learned just last week that Palo Alto requires proof that you ever lived in Palo Alto with a piece of mail before they'll let you into their shelter. Also their shelter has bunk cots. not bunk beds, bunk cots, so the bottom person is inches away from the top person. reply seanmcdirmid 4 hours agorootparentAccording to https://www.asaints.org/outreach/hotel-de-zink/, that is Palo Alto’s only homeless shelter and it doesn’t mention a residency requirement. It also doesn’t have the bunks you are referring to. reply shuckles 13 hours agorootparentprevIt's very unlikely that former residents (in the sense of paying for housing for extended periods of time with their own wage income) comprise a significant portion of San Francisco's street homeless. reply doctorpangloss 12 hours agorootparentThis is another one of your misconceptions. > Nearly 8 out of every 10 unhoused people in Oakland were living in Alameda County when they lost their housing. https://www.sfchronicle.com/projects/2021/homeless-project-o... > Primary Cause of Homelessness (Top five responses, Fig. 19) > Family or friends couldn't let me stay or argument with family/friend/roommate: 27% > Eviction/Foreclosure/Rent increase: 25% > Job loss: 22% > Other money issues including medical bills, etc.: 13% > Substance abuse: 13% https://homelessness.acgov.org/homelessness-assets/docs/repo... It's possible that many frequent flyers to emergency rooms have been dumped from other communities. But most homeless people are just that, people who have lost their homes in their community. And anyway, how could it really be any different? > with their own wage income Of course poverty is the number one reason they are becoming homeless. Why are you talking about wage income. They have too little income. Who the hell wants to live on the street! reply shuckles 12 hours agorootparentFirst of all, you cite data about Oakland when I was talking about San Francisco. The cities are different enough that it's worth noting. I will also state that I am well-informed about the matter and have few \"misconceptions\" in the obvious sense. > Nearly 8 out of every 10 unhoused people in Oakland were living in Alameda County when they lost their housing. That's not what the Point in Time count asks or tries to measure. The statistic reported is the location of last known shelter. So, as an example, someone who moves to San Leandro from Fresno to crash on a friend's couch for two weeks and then is asked to find a different place to stay would count as \"living in Alameda County\" for the purposes of the statistic. Another example: a longtime homeless person who has cycled in and out of shelters in the region for decades counts as \"living in Alameda County\" even if they first lost their home in Kern County or out of state. > Primary Cause of Homelessness (Top five responses, Fig. 19) This is silly data to cite. Drug use is correlated with money issues, interpersonal relationship problems, eviction, foreclosure, inability to keep a job, and more. Maybe if the survey had a multiple response design, the distribution would be relevant. > And anyway, how could it really be any different? I can think of dozens. reply seanmcdirmid 12 hours agorootparentprevGreyhound bus stations. If you’ve ever taken a bus across country before, they pick up a lot of people at prisons and they stay on until some west coast city (LA, SF, Portland, or Seattle). That accumulates, and once they stick around for at least a year, they are considered resident (for some definition of housing lost, that means even if they were housed in a hotel once, or couch surfed at the start). Surveys that rely on self reporting are incredibly inaccurate. One was done in Seattle, and found out that 80% of King county’s homeless population was previously housed in pioneer square, an absurdity that put the entire survey in doubt. It really is in the self interest of homeless oriented agencies and NGOs to present the problem as local as possible. If it isn’t local, then giving out housing will only make the local problem worse (people will start arriving for their free housing from other areas of the country), you can judge your success by how much worse the problem gets, which isn’t popular with local voters. Without good information, at any rate, it isn’t weird that we are seeing the problem get worse for every billion we throw at it. Eventually the popular cities will just give up trying very hard because they never had the power to fix it in the first place. reply imtringued 12 hours agorootparentprevIs this some kind of joke? Do you really think people who get evicted for not paying bills automatically get sent to a different city? How do you propose your magic mechanism to send people from SF away the moment they become homeless? Someone who ends up homeless is going to stay in a place that is familiar to them. reply shuckles 12 hours agorootparent> Do you really think people who get evicted for not paying bills automatically get sent to a different city? When did I say that? > How do you propose your magic mechanism to send people from SF away the moment they become homeless? What does this have to do with anything? The vast majority of tenants evicted in San Francisco receive both legal representation and relocation fees starting at ~$7k per person and more if you claim disability, which most do. There are ~80 nonpayment evictions in the city annually, and there were ~0 from 2020-2023. > Someone who ends up homeless is going to stay in a place that is familiar to them. Probably true of the average homeless person, but there are many more homeless people outside San Francisco than in it, so you only need to believe a small percentage of, say, California's homeless population ends up in the city for local homelessness to be dominated by folks who lost their last stable shelter outside the city. reply reducesuffering 13 hours agorootparentprevIt doesn’t just boil down to affordability, but how many people got down in the dumps struggling to afford their expensive apartment, got stressed out and depressed, caused relationship issues and drug use, and led to a downward spiral where they ended up in a place with severe mental illness and couldn’t function with free keys after 5 years of that? reply bjelkeman-again 13 hours agoparentprevFinland seems to have found a method that deals with this. They have nearly halved homelessness in eight years. https://www.weforum.org/agenda/2018/02/how-finland-solved-ho... reply chadcmulligan 6 hours agorootparentThe Finns seem to have a lot of innovative social programs - dementia villages - https://hogeweyk.dementiavillage.com and open prison programs - https://pulitzercenter.org/projects/finlands-open-prisons reply BriggyDwiggs42 12 hours agorootparentprevI feel like this stuff is a bit obvious. Where do you think the economic incentives are to maintain the status quo? reply barry-cotter 10 hours agorootparentIf no housing is built that’s good for landlords and property owners as rents and property prices rise. If no (affordable) housing is built in an area with strong economic growth that’s good for fundraising of some charities as the problem gets worse. reply robertlagrant 8 hours agorootparentEnough housing will make all housing affordable anyway. reply LordShredda 11 hours agorootparentprevNo economic incentive, more of a sense of responsibility to society and some nationalism. look at the UAE where the incentive is more political or for prestige. reply shuckles 13 hours agorootparentprevAcross the entire country, Finland had 3x fewer homeless people at its worst than San Francisco the city does. The problem is of such different magnitudes that it's unlikely Finland has much to teach. reply baq 12 hours agorootparentThey also have less population than Bay Area by a couple million. Of course it’s different but don’t throw out the baby with the bathwater. reply shuckles 12 hours agorootparentAnd Finland is a country so there are less regional differences in policymaking. And Finland doesn't have balkanized healthcare systems. And Finland doesn't have suburban land use that sets a high minimum cost to participating in society. And Finland doesn't have open borders with Republican states which openly advertise that they bus indigent migrants with no work permits to sanctuary cities. Science works best when you have controls that let you measure the consequences of changes precisely. Saying that results from a Nordic social democracy probably don't apply to a city with no administrative capacity and machine politics is not controversial. It's the null hypothesis! reply throwaway2037 12 hours agorootparent> Finland doesn't have suburban land use that sets a high minimum cost to participating in society. What does this mean? reply shuckles 12 hours agorootparentPractically, to earn income in most parts of America, you need to own a car and pay rent on a single family home. That is a $800-$1000 additional expense versus living in a micro-studio and taking transit to work. For someone on the margins, that is a huge and often insurmountable price of entry. Many can make do by, e.g., being housemates in a larger home to reduce the minimum expense. Folks on the margins are often not able to find others willing to live with them. reply throwaway2037 3 hours agorootparentSo... Finland, exactly, right? I assume that you already know that Finland is huge but only has a population of ~5.5M. Except for central Helsinki, the entire country is suburban or rural. I'll offer a counterpoint that people rarely consider: Most of the populated land in Japan in rural. Many people who live more than 50km from a major city drive. Almost all people who live 100km from a major city have 2x cars in their home. This shocks many people. Please stop assuming that the US is special or amazing. It has pockets of very dense cities and the rest is mostly suburban or rural... like other highly advanced countries in Europe or East Asia. reply shuckles 1 hour agorootparentMost people in advanced countries are rich and can choose expensive lifestyles, but that has little to do with the options afforded to those who aren’t. reply patmorgan23 6 hours agorootparentprevSingle Family suburban homes are expensive and require you to buy an expensive car in order to do anything. reply parineum 11 hours agorootparentprevIf you're going to take the moral stand against the way border states handle their illegal migrants publicly and derisively while simultaneously declaring yourself as a safe haven for those migrants, you probably shouldn't be surprised when those states call your bluff. reply shuckles 4 hours agorootparentThat does not really describe what’s happening. The fix for asylum laws being abused is legislative, and in the last year fixes have been scuttled by Republican legislators. Sanctuary city laws are pretty reasonable policy statements about when a city cooperates in deporting illegal immigrants who may otherwise be law abiding. This makes perfect sense as a policy when you operate in an immigration regime that makes legal immigration infeasible for all kinds of necessary labor, from farm workers to domestic help and even high skilled IT. If Texas wanted to call California’s bluff on housing policy, it would also reject the middle and high income households moving to the Triangle from high housing cost California suburbs. But it’s not principled when it comes to that — their money is welcomed! Heck, even the Florida model where they check if businesses are indeed employing legal workers in labor intensive industries like construction would be a more faithful enactment of law and order values with respect to immigration. Anything short of that is its own form of sheltering. reply parineum 4 hours agorootparentI'm not really making a policy criticism of any of it really. I'm just saying, if you're going to publicly antagonize someone about they way they do something and imply your way is the enlightened way to score political points, don't be surprised when those people find a way to call your bluff, publicly. Trucking migrants to sancutary cities has changed people's minds. reply shuckles 4 hours agorootparentNo, forcefully moving people around has not changed any minds and > publicly antagonize someone about they way they do something and imply your way is the enlightened way to score political points, don't be surprised when those people find a way to call your bluff Not sure how you frame a political stunt as a moral comeuppance. reply parineum 4 hours agorootparent> No, forcefully moving people around has not changed any minds I disagree. I've seen poling and heard numerous stories of citizens of sanctuary cites that were not border cites and have a very negligible issue with illegal migration suddenly having to deal with it first that shows that trucking migrants has moved the needle. > Not sure how you frame a political stunt as a moral comeuppance. It's a political comeuppance. reply shuckles 1 hour agorootparentDozens of stories in whatever news media you consume has nothing to do with whether there are serious policy conversations about revisiting sanctuary city laws in those cities. reply ThunderSizzle 8 hours agorootparentprev> Republican states which openly advertise that they bus indigent migrants with no work permits to sanctuary cities. Oh, those stupid Republicans! I can't believe they would ever send illegal immigrants that voluntarily want to go to a sanctuary city that has been proclaiming themselves as a safe harbor for criminals for over a decade, to said sanctuary city! Oh, the horror and atrocity! The only reason the border is wide open is because we don't have sufficient border walls and border enforcement, all of which sanctuary city democrats have opposed and voted against. If your going to go political, you probably should've just pointed to the open border problem, not target Republicans. But it's obvious whose causing an open border right now. reply Calavar 6 hours agorootparent> The only reason the border is wide open is because we don't have sufficient border walls and border enforcement, The bottleneck at the border is not physically policing the border, but the fact that migrants have learned to work the legal system by filing asylum requests. So the solution is legal reform, not building more walls. The 2023 border reform bill would have enforced a mandatory cap on the number of asylum requests processed (not just granted, but processed at all). Any migrant asking for asylum after that cap would have been turned away immediately. > But it's obvious whose causing an open border right now. I wouldn't say it's clear at all. It's true that the Democrats have historically turned a blind eye to the border. But over the past 12 months: First the GOP leadership said they wouldn't pass border reform unless it was bundled with Ukraine/Israel/Taiwan funding, then said they wouldn't pass border reform until it was unbundled, then they brought the Ukraine/Israel/Taiwan funding back to the floor but not the unbundled border reform bill. For the first time in a generation, there were enough Democratic votes to pass significant border reform, and the GOP votes simultaneously vanished into thin air. reply patmorgan23 6 hours agorootparentprevThis just isn't true. The vast majority of illegal immigration happens through legal ports of entry. Usually individuals just over stay a tourist visa. Republicans have rejected the recent bipartisan board deal that would fund hundreds of new boarder patrol agents, and immigration ALJs to expand enforcement and processing capacity at the boarder. All because daddy Trump wants there to be a border crisis because it's a good election issue for him to run on. Republicans don't care about governing, they just care about winning. reply shuckles 5 hours agorootparentprevI made no value judgment about Republicans. That was your projection. The primary cause of border issues is dysfunction in the Senate that prevents asylum laws from being improved. reply ThunderSizzle 2 hours agorootparentYou blamed the blight of sanctuary cities on Republican states. The fact illegal immigration is rampant is the source of the problem, not Republican states. It's a cherry picked statement with an agenda. reply shuckles 1 hour agorootparentNo I pointed out a list of ways Finland was different from San Francisco so we should not naively assume data from there should generalize. It’s not cherry picked since it was one in a long list of ways the two are different. Street homelessness in San Francisco has little to do with immigration, and family homelessness has little to do with illegal immigration (families from South America are largely claiming asylum through the legal process). reply nrclark 6 hours agorootparentprevHN is not the right forum for sports-team red vs blue politics, and that kind of mentality is not good for our nation as a whole. We are all one nation. reply ThunderSizzle 2 hours agorootparentThen the OP shouldn't blame the issues sanctuary cities face on Republican states. If illegal immigration is a problem for sanctuary cities, then that is what should be said. It's not red vs blue to point out an attempt to shift blame on Republican governors. reply shuckles 1 hour agorootparentI was not assigning blame. I was making a factual statement that San Francisco’s recent increase is family homelessness is largely due to bussing policies of Republican run border states. Asylum applicants are in the United States legally while their claims are processed, so the status of sanctuary city has nothing to do with the treatment of asylum applicants. Non-sanctuary cities would treat these applicants in the same way. reply doctorpangloss 13 hours agorootparentprevAh, the old scientific adage, \"if something innovative works at a small scale, that guarantees it will not work at a large scale.\" I mean this guy shared something interesting and thoughtful, more interesting than the article's complaining, and your thought is really this? reply shuckles 12 hours agorootparentThat was not the logic of my comment, and it seems like you didn't try very hard to consider what I meant. For example: the \"interesting and thoughtful\" article linked describes Finland's \"Housing First\" model adopted in 2008. Guess what: Housing First was first introduced as a policy idea in San Francisco in 1994, adopted as de facto city policy about ten years after that, and set as statewide policy in California ten years later. Ten years later, the problem is worse than ever! Perhaps Finland and California are different enough that looking to Finland as a policy playground is a waste of time. This isn't physics; it's political economy. reply resolutebat 11 hours agorootparentAdopting policies is one thing, but is SF actually giving housing first to the homeless? Because my rough understanding is that the answer to that is \"no\". reply shuckles 5 hours agorootparentDepends what you mean. All the subsidized housing available is operated under the housing first model with no sobriety requirement. Is there sufficient housing on offer? Clearly not. But SF expanded subsidized housing inventory substantially during the pandemic and has more such inventory than basically any other west coast city. For political reasons having to do with the NPIC, the city does not invest much in shelter beds. So it’s a $750k-$1m apartment or bust. reply doctorpangloss 12 hours agorootparentprevThe bad faith reasoning in your original comment is what you wrote. You are welcome to write interesting stuff in your comments, you should. reply shuckles 12 hours agorootparentThere was no bad faith reasoning in my original comment. I, pretty evidently, understand the history of Housing First better than you do and am still convinced policy outcomes in Finland aren't very informative to San Francisco because the societies and challenges are very different. reply Log_out_ 12 hours agorootparentUs is deeply anti social at the heart. The original reason it got away with it was the American dream aka upward mobility. That is dead for two generations now, and it has developed a corresponding \"anti-system\" movement the society refused to acknowledge and deal with beyond declaring them \"deplorables\". The day the leader dies, such movements are up for grabs and can spin on the proverbial ideological dime. Maga will go sendero, but there was just nothing to be done. reply shuckles 12 hours agorootparentI don't really buy this doomerism. The USA passed the biggest COVID safety net, in absolute and relative terms. It's still one of the best countries to get ahead. Most of its problems are not framed through grand narratives about decline. They're more pedestrian issues related to mechanisms like common law, a dysfunctional Senate and two party system, and replacement of state capacity with nonprofits (as this article poorly discusses). reply blamestross 8 hours agorootparentprevCalifornia also gets at least one countries worth of homeless shipped to it BECAUSE of the resources it provides (And political spite). This is the bad Nash Equilibrium in action, being punished for improving the situation. reply roshin 12 hours agoparentprevLack of affordable housing may be an issue, but there is a lot of opposition to a very simple solution to that. Assist the homeless in moving to a location with cheap housing. Not everyone has a right to live in the expensive area of San Francisco. If someone can't pay for housing, we don't want to forsake those people. But giving them cheap housing in a very desirable place is hard. Unfortunately, the issue with that is that there will still be people who would prefer to live in the expensive city, even if they don't have a house. reply DoreenMichele 12 hours agorootparentLack of affordable housing is an issue across the US. I'm not personally talking about homelessness in SF per se. reply jl6 11 hours agorootparenthttps://www.theatlantic.com/magazine/archive/2023/01/homeles... https://archive.ph/Sn6dY Housing affordability varies greatly across the country. reply DoreenMichele 5 hours agorootparentThis is true. But no state has enough affordable housing. Some states have a bigger shortage than others, but all states lack sufficient amounts of affordable housing. reply patmorgan23 6 hours agoparentprevWe should make it easy to build large amounts of affordable market rate housing, and build social housing/expand housing vouchers for the bottom quartile who the market may not provide for sufficiently. reply pvdoom 11 hours agoparentprev> A primary root cause of homelessness is lack of affordable housing. Nah, I think that is still a consequence. The root cause is the fundamental assumption that homes should be a commodity that is bought, sold and rented out for profit, rather than something that everyone needs for survival. We should be looking at ways to limit the market here, or finding ways to not treat houses as a commodity, or something for rent-seeking reply koube 11 hours agorootparentI'm not so sure that renting out housing is a bad thing, unless there is somehow too much rental property. Dense rental housing, which is an inherently corporate thing, is key to affordable housing for people without a lot of wealth. My intuitive sense is that the proportion of rented housing probably has a seesaw effect on the price of owned housing vs the price of rented housing. If you want them both to go down it's necessary to build a lot more housing. reply pvdoom 7 hours agorootparentWell, rental as another profit-driven thing, that has multiple consequences - it removes property away from people, it prevents people from getting their own place, it creates concentration of wealth, it drives policies that favour landlords, etc - i.e. big real estate corporations, and landlords generally have a lot more lobbying power than poor renters. reply tpmoney 2 hours agorootparentRental also enables economic and educational mobility, and provides (some) shelter against severe economic losses that are risks when you own property. For my own part, renting enabled me to move across the country as an 18 year old for a higher education. Even if \"affordable\" housing could have been owned at that point in my life, I had no idea if I was going to stay in that area or move. And it turns out I did move, quite a lot. I moved almost every 1-2 years, sometimes chasing lower rent, sometimes to move closer to a new job (allowing me to save money and time on commuting). Having to sell a house or even a condo every 1-2 years would have been a nightmare at best. And that doesn't account for all the economic risk that comes from owning property. When I was renting there was no way I would have been able to afford the tens of thousands of dollars I've had to outlay as a homeowner for everything from water main replacements, roof replacements, plumbing repairs, mold remediation and remodels, HVAC repair / replacement, appliance replacement, tree removal etc. As a renter, all of that is mandated to be repaired by your landlord. Sure, it's possible (and even likely) that your rent will go up next renewal if your landlord is outlaying $20k+ for an emergency roof replacement, but the best part of renting is you can just leave and go somewhere else. Where as if you own property, even if you put the roof repair on credit so that you have a similar \"rent\" increase, you can't just up and leave if you can't afford the rent anymore. That debt stays with you, not the property, and it just got harder to sell if you used the property as collateral for the loan. Oh and that changing rent payment, yeah my mortgage payment is 50% higher than when I started. No it's not an ARM, thats just the increase from property taxes and insurance (see aforementioned emergency roof replacement and tree removals). Don't get me wrong, I absolutely love owning my own home, and there have been times in the last handful of years that being in a stable location without needing to move or (mostly) worry about significant rent changes has helped me through some situations that would have sucked as a renter. But equally having been a home owner, I absolutely look back on my time as a renter and laugh at how naive I was to think that owning would have been a better deal for me. Renting enabled a degree of flexibility I no longer have. I don't necessarily need that flexibility as much as I did, but I also accept a commute that I previously would have considered moving to reduce. reply Symmetry 6 hours agorootparentprevIf houses were just a commodity to be bought and sold we'd have much less of a problem. There were very few homeless a hundred years ago. Efforts to ban residential hotels aka \"flophouses\" were mostly an effort to remove undesirable people from their communities by making it impossible for them to find housing there they could afford. And the fact that many if not most voters don't want housing for what they consider undesirables in their neighborhoods is a huge problem for the construction of homeless shelters and affordable housing to this day. So I don't think that even completely socializing housing would do all that much to help with the problem. reply bell-cot 10 hours agorootparentprevI'd phrase that \"homes should be a great investment for the haves. And who cares about the have-nots, anyway?\". reply nytesky 12 hours agoparentprevWhat percentage of homeless are indigent and dealing with crippling mental health and addiction issues. Maybe they would qualify for SSI but that is a completely untenable income to also pay rent out of. There is really no “affordable” housing that would work for that population. reply Mordisquitos 12 hours agoparentprev> We should be working towards policies and solutions that foster more \"market rate\" affordable housing, not affordable housing via government programs or nonprofit organizations. Why the 'not'? Those options are not mutually exclusive and are in fact complementary. Liberalising overly restrictive zoning laws across the USA can improve supply from all three sectors: for-profit, non-profit and local government, thus encouraging market-rate affordability. reply gigatexal 12 hours agoparentprevSo attack the supply side problem vs paying rent for the homeless or some other demand remediation. Hmm clever. reply Rinzler89 14 hours agoparentprevFor over 20+ years I've been hearing people and politicians talking about \"affordable housing\" and in that time, the gap between housing price increases and wage increases has only gone up, along with wealth inequality, that I'm tired of hearing even more talks, as I already know where this is going: nowhere. Housing will get more expensive and the rich will keep getting richer, as per usual, while the politicians will keep making vague empty political promises about housing affordability when the election time comes. I'm not holding my breath this will be solved ever in my lifetime, unless war or a violent revolution resets the monopoly board. reply klyrs 12 hours agorootparentRight, you can't increase wages, because that would cause inflation! Nevermind the grocers, oil cartels and landlords hiking prices year after year. That's just good business sense, hampering that would destroy prosperity! reply bboygravity 13 hours agorootparentprevIt was already solved though (in Singapore and many other places). Affordable housing is not rocket science nor a novel or hard problem to solve. IMO the root cause is that there's a serious issue with the sustainability of the US political system (and its fiat currency) in general. reply mediumdeviation 12 hours agorootparent> It was already solved though (in Singapore and many other places). I feel like people saying this don’t understand how radical Singapore’s housing solution is. It starts with the government repossessing most of the land in the city to develop housing. I think that’s alone is a non-starter pretty much anywhere in the United States. It’s also far from perfect - I could for example talk about how construction in Singapore is for the most part only viable because of cheap labor from surrounding country, the ticking time bomb that is the 99 year lease or the fact that prices have slowly but surely ticked up in recent years, far in excess of inflation. reply qsi 11 hours agorootparentIn addition I'd add that homelessness still exists in Singapore, albeit at a very low level, estimated at about 1,000 people out of a population of 6 million. ChannelNewsAsia did a feature on homelessness here a while ago: https://www.channelnewsasia.com/watch/homeless-singapore/why... reply fragmede 8 hours agorootparentprevShift the Overton window and propose projects to eminent-domain the shit out of everything everywhere until eminent-domaining just the empty Sears store to put up housing seems more than reasonable. reply Rinzler89 12 hours agorootparentprev>It was already solved though If people can only name Singapore, Tokyo and Vienna as examples where \"housing was fixed\", then it's proof that it hasn't been fixed when there's the other millions of cities worldwide left. reply fragmede 8 hours agorootparentit proves it's a solvable problem, so the other 10,000 or so cities (there aren't millions of them) just aren't properly motivated to solve the problem. The first atomic bomb was hard, because it wasn't known if it was even possible. The second one was still really hard, but once the Americans proved it was possible, everyone else knew it was possible. reply Rinzler89 7 hours agorootparent>so the other 10,000 or so cities (there aren't millions of them) just aren't properly motivated to solve the problem Civilizations, human settlements, people's homes and communities aren't fungible commodity widgets solvable and movable through copy-paste solutions like technical challenges, no matter how much HN insists every world challenge from housing to world peace, is as easily solvable as those at their tech jobs. Just because you can 1:1 reverse engineer a bomb design and have it work the same, doesn't mean you can just take a housing policy that worked in the demographic, geographical, historical and cultural context of one country and just drop it in another country with a completely different context and background, and expect it to just work the same. This is the tech equivalent of: \"well, it was working on my machine\". reply selectodude 6 hours agorootparent“Building enough housing” is actually a concept that we can apply everywhere in the world. My city has a million fewer people than it did 50 years ago and inflation adjusted rents are an order of magnitude higher. It’s not because the ability to build enough housing was lost to the sands of time. reply Rinzler89 5 hours agorootparent>My city has a million fewer people than it did 50 years ago and inflation adjusted rents are an order of magnitude higher. Can I ask which city lost 1 million people in 50 years? reply hgomersall 6 hours agorootparentprevThe only problem with the fiat currency is the total failure to use it effectively due to misguided fears about \"debt\". reply mistermann 12 hours agorootparentprevSolving housing affordability may first require solving culture, and Singapore is a major historical outlier in that regard because of one Human: Lee Kuan Yew https://en.m.wikipedia.org/wiki/Lee_Kuan_Yew reply brynb 12 hours agorootparentprevwhile important, the fiat currency is a second-order issue, imho. the root is self-serving corruption facilitated by an intricate apparatus based on bureaucratic capture. to even appear as a choice on the ballot, one must either be part and parcel of this widespread corruption, or one must take the system by surprise with vast amounts of money and bluster (trump trump something something, fat lot of good that did, though). one of the biggest issues we have is that so much “shadow money” is allowed to influence politics. if the government was originally supposed to be a “check and balance” against accumulation of corporate power against regular people, phenomena like Citizens United et al have guaranteed the undoing of that guardianship. it is a society of grift now, through and through. and of course, being adaptive creatures, we learn from example, and we learn from our enemies, so anyone who once played the “good guy” role is stooping as low as his opponent now. surveying the political landscape, it’s often hard to find anyone spending more time trying to solve problems as opposed to maximizing instagram likes and chances for reelection NB: there’s that whole discourse about “how are you all multi-millionaires when the pay for senator/representative is at most a few hundred thousand a year?” reply zeroCalories 13 hours agorootparentprevYou should migrate to somewhere that does value affordable housing. reply Rinzler89 13 hours agorootparentWhere would that be that's nice to live, safe, good infrastructure, good jobs AND by some miracle still allows you to buy affordable housing? Plus, if you move to a low-CoL area with a high-CoL job then you're not really solving housing, you're just moving your problem elsewhere onto other people making housing unaffordable for them. How many former cheap places to live have been gentrified into oblivion in a short amount of time? The goal is to make housing affordable AND help preserve local communities and culture(even though that might be contradictory to a degree), not to keep gentrifying and uprooting them in a musical chairs style game and then wonder why communities are dying, family units are dying, birth rates plummet, family support networks are dying, mental illnesses are up, loneliness is up, etc. reply baq 12 hours agorootparentYou seem to assume a free market for housing, that would be at least partially incorrect. The free market is a cause of long term (timeframe of decades) housing unaffordabilty because the haves treat housing as investment while the have-nots live under bridges or in cars which puts them in an unemployment/debt spiral. The free market could work if housing was a depreciating asset like cars. Too many people in the west would be pissed off if that happened, though. It will come to some countries with collapsing demographics and that won’t be a pretty sight. reply fragmede 8 hours agorootparentGiven the numerous government regulations, as well as regulatory capture and NIMBYism, housing is not a particularly free market. Zoning and building codes (only some of which are important for not making death traps) as well as low income units lead to prohibited or high costs of construction. reply Rinzler89 11 hours agorootparentprev>It will come to some countries with collapsing demographics and that won’t be a pretty sight. Except that's not how it happens. Demographics have been collapsing in many EU countries for a while now and prices have been going nowhere but up at rates beyond wage growth. Internal migration from rural to urban, and external migration from war torn and impoverished nations also to urban centers, keep pushing demand up regardless of local reproduction numbers. Sure, you can now buy cheap properties in some empty towns in the south of Italy for example, but to what end if there's no jobs there? reply baq 9 hours agorootparentAt some point we run out of people in rural areas to migrate to urban and 30-50 more years later we get Detroit, maybe, except everywhere at once. Watching South Korea very closely... for the next 20 years. reply Rinzler89 9 hours agorootparent>At some point we run out of people in rural areas to migrate to urban But you won't run out of external migrants due to wars, climate change and poverty who want to leave their countries and move to the western developed ones. reply zeroCalories 5 hours agorootparentprevGentrification is based. Unless you want to stop people immigrating you'll never \"preserve the character of your neighborhood\" while keeping prices down. The only solution is to build more housing. reply barfbagginus 13 hours agorootparentprevWork to bring about Revolution, don't just hope for it. reply amanaplanacanal 7 hours agorootparentThe problem with revolution is that when it comes, you won’t be in charge. The person who is willing to be the most violent and ruthless will be. reply Rinzler89 6 hours agorootparentExactly this. It's why the former communist countries had nothing to do with Karl Marx's communism and why people say \"real communism has never been tried\". Those \"communists\" who came to power there didn't distribute the wealth to the people, but came to power to pocket it themselves by replacing the monarchies who had all the wealth and power previously to them, but still kept the folk enslaved at gunpoint. You basically switched one undemocratic oppressive regime with another and the average folk kept getting the short stick both times. So be careful when you wish for major revolutions. Doesn't necessarily mean that with new management, things will always turn out better for you this time. Doesn't mean you should stick to the status quo and never try to change things either. reply komali2 12 hours agoprev> This is the state of affairs in almost every city where “progressives” have a large impact on local politics. Progressives claim to support government spending programs, but also have an anarchistic, anti-governmental attitude that can be seen in their support for policies like police abolition in 2020. Although progressives want the government to fund public programs, their opposition to centralized state power means they often don’t want the government to run the programs being funded. This article is comingling vastly different political ideologies. Very few anarchists would have any interest in engaging the political system to solve problems, since most anarchists believe the political system is one of the systems originating the problems society faces today. There is an alignment between anarchists, socialists, leftists, progressives, and even some liberals (very few!) in police abolitionism, but only because the police are so wildly out of control in the USA that it's easy to align along the general idea of \"starting over.\" And even then, liberals seem to be performative in this support - as soon as students started protesting genocide against Palestinians, liberals predictably started asking why the police aren't out there brutalizing these \"student terrorists.\" Anti-houseless spikes painted with a rainbow flag. > Although progressives want the government to fund public programs, their opposition to centralized state power means they often don’t want the government to run the programs being funded. In my opinion a total mischaracterization. Leftists and DSA types absolutely support centralized state power and socialization / nationalization - go read their takes on public transit and public housing. This whole article is describing neoliberal politicians allowing capitalists to do capitalism however they please, which is in line with neoliberal ideology. San Francisco is the best example of the failure of neoliberal ideology, and just because it's a city full of dirty anarchists doesn't mean said dirty anarchists have any political power - SF is politically neoliberal through and through. > What is taking place in America’s most performatively socialist urban areas is that taxes are constantly raised in order to fund public services I don't understand where the author gets the idea that American urban areas are \"performatively socialist\" from a politician standpoint. American politicians in every city are famously, painfully neoliberal. Eric Adams mischaracterizing and verbally attacking peaceful protesters. London Breed called the cops on a homeless guy she saw laying on a bench. Ted Wheeler pathetically tried to co-opt Black Lives Matter protests before being roundly rejected and then sending the cops to brutalize peaceful protesters. American urban politicians are not socialist nor are they performatively so. > where “socialists” privatize government services at every opportunity; I just don't see this happening. I see neoliberal Democrats doing this, but never avowed socialists. Happy to be corrected. So far as I know even SF has only ever had one avowed socialist politician, Chesa Boudin, and as soon as he was elected every politician in the city banded together to tank his career. Never has such as microscope been pointed at a District Attorney, with predictable results. Everyone was happy to take a potshot and cash in on the socialist under whom violent crime was dropping and who was prosecuting an \"unusually high\" volume of cases. At least the author isn't leaning into the privatization and pointing out the obvious issues this is causing. I would be interested in seeing an analysis on how leftist ideology offering actual solutions is being co-opted by neoliberals serving the same cause as the reactionaries they run against every election. reply mediumdeviation 12 hours agoparent> So far as I know even SF has only ever had one avowed socialist politician Surely you’ve heard of Dean Preston, the DSA member of the Board of Supervisors who owns a $2.5m house in the city and is also the l",
    "originSummary": [
      "Jonathan Ireland's article argues that the term \"nonprofit\" can be misleading, often hiding misconduct and corruption within these organizations.",
      "He provides examples such as San Francisco's TODCO and Seattle's nonprofits, highlighting issues like increased executive pay, lobbying efforts, and hiring practices that lead to reoffending.",
      "Ireland criticizes the inefficiency and high costs of outsourcing government services to nonprofits, suggesting that urban issues like homelessness and public health risks are exacerbated by a lack of effective government intervention and oversight."
    ],
    "commentSummary": [
      "The article critiques certain nonprofits, using Seattle's \"Freedom Project\" as a case study, and explores the debate on whether discrediting individuals for past crimes constitutes libel.",
      "It discusses the origins and impact of libel and slander laws, the role of nonprofits in public policy, and the need for transparency and accountability in nonprofit operations.",
      "The text contrasts Finland's successful \"Housing First\" model with its less effective implementation in San Francisco, and evaluates housing affordability through various models, including socialized housing and zoning reforms."
    ],
    "points": 246,
    "commentCount": 210,
    "retryCount": 0,
    "time": 1716867509
  },
  {
    "id": 40497623,
    "title": "Why Software Estimation Fails: The Log-Normal Reality of Development Tasks",
    "originLink": "https://hiandrewquinn.github.io/til-site/posts/doing-is-normally-distributed-learning-is-log-normal/",
    "originBody": "Home » Posts Doing is normally distributed, learning is log-normal My conspiracy theory on why software estimation is such garbage May 28, 2024 There are few things I think about more than the essays on gwern.net, and there are few with as satisfying a theoretical payout to contemplate in my orb as his essay on “leaky pipelines”, aka log-normal distributions. The skulk: Say you’re working on a Laravel web app. You’re about 90% sure you know how to start the app. You’re 80% sure you know how to handle the infra you’ll need to get it online. And you’re 70% sure you know how to get your first customer. What is your chance of successfully going from zero to first customer? 0.9 * 0.8 * 0.7 = a little over 0.5. That’s … a lot less encouraging than any of the previous numbers, if you buy my multi-step modelling. Software estimation is such a mess in part because it has trouble recognizing that, at least, just-in-time learning is at least non-normally distributed. Everything we know about traditional project management, from Waterfall to Gantt charts to estimation practices, are on some level based around the idea that each individual step in the chain is bell-shaped: A process taking twice as long as it normally does might be 2 standard deviations out, aka in the bottom 2.5% of outcomes. But in a log-normal distribution, processes taking two, three, or five times as long are much more common, and this throws things into disarray. Some things happen much faster than usual - but doing one estimated week-long project in half the time, and another estimated week-long project in twice the time, still leaves you in the red for your time budget. Needless to say, it is almost never the case that in software development you know all or even most of the technical hurdles you will face in the processs of development. An efficient markets hypothesis fanboy might say “If anyone already knew how to do it, it would already be done by now”. I’m not that optimistic, but I do think this leaky-pipeline approach can shed light on some more counterintuitive things about our industry. Such as the relentless emphasis, not only on relevant experience, but on knowing your specific tooling when you apply to a given job. You and I might be hardcore enough computer geeks that we can pick up any language and be productive in it within a few weeks, but if that doesn’t hold for the median software developer, then employers really are justified in deciding that e.g. they only want to hire Java devs with previous experience in… Java. If there’s a 20% chance it takes 1 month, 20% for 2 months, … 20% for 5 months before your non-Java-background Java dev has enough experience to finally start contributing to the Java codebase, then yeah, it makes a lot of sense not to want to gamble on that. Not only do you face the possibility of paying a five- or six-figure sum for someone who’s useless, you have very little ability to estimate as an employer yourself when they will become useful. Now what’s really interesting about this theory is that it suggests that business processes that are normally-distributed are the exception, not the norm, in a sense. Every new process a person has to undertake will have at least some phase which is dominated by learning. That’s as true of web dev as it is of learning to operate the drive-thru at McDonald’s. It takes a special kind of rigor to transform that into an activity routine and repeatable enough that you get a long tail of normally-distributed, profitable activity. Again, this makes sense: I’ve been building Django and Hugo websites and web apps for a few years now, and I’ve been using Python much longer than that, and I sort of have an intuitive sense by now for how long either of them would take me. That’s not because I’m some genius savant - it’s because I have already flowed through the leaky pipeline enough times to know how I like to approach most things. The instant I’m in new territory, we’re back to the leaky pipeline. It may be the case that academic, just-in-case learning is less this way, because the actual things you need to learn is carefully plotted ahead of time for you. This would explain the popularity of tutorials online, as ways to tamp down on the worst-case scenario where that thing you thought would take you a week to do ends up taking you a year. That being said, I definitely had some classes in college that unexpectedly took way less time to me (wireless communication, DSP) and some that unexpectedly took way more (Maxwellian electromagnetism). Communication Econ-Flavored Law-of-Diminishing-Returns Productivity Psych-Flavored The-Medium-Is-the-Message Time-Management Software-Management Software-Dev Next » Trackballs are great for the mostly-mouseless",
    "commentLink": "https://news.ycombinator.com/item?id=40497623",
    "commentBody": "Doing is normally distributed, learning is log-normal (hiandrewquinn.github.io)225 points by hiAndrewQuinn 13 hours agohidepastfavorite69 comments kbrkbr 10 hours ago> You’re about 90% sure you know how to start the app. You’re 80% sure you know how to handle the infra you’ll need to get it online. And you’re 70% sure you know how to get your first customer. What is your chance of successfully going from zero to first customer? 0.9 * 0.8 * 0.7 = a little over 0.5. This would be true if the probabilities were independent [1], which they are probably not, as the infra is tied to the app. The probability can be anything from 40% to 70%. You can see that if you arrange 3 rectangles inside a unit square, one with 90% of the size, one with 80%, one with 70%. If they contain each other, their overlap is 70% of the whole, namely the smallest rectangle. In the worst case it is 40%, when the 80% is 10% out of 90%, giving 70% overlap. And then 70% is 30% out of that overlap, leaving 40%. [1] https://en.m.wikipedia.org/wiki/Independence_(probability_th... reply esperent 8 hours agoparentYou're not wrong, mathematically. But these numbers (90%, 80%, 70%) are made up, so it's kind of irrelevant whether we're doing the maths right or not. reply kqr 2 hours agorootparentQuite the contrary. The only thing that really matters when it comes to probabilistic forecasting is that we do the maths right. The numbers will always be made up estimations, so the only thing that sort of makes sure we converge on similar solutions is getting the maths right. A person is free to make any outlandish estimation of any individual probability, but when they are faced with the task of trying to make it mathematically cohere with other estimations is when they realise how outlandish the initial estimation was. I have tried to write about this before: https://two-wrongs.com/what-is-probability.html reply mjburgess 6 hours agorootparentprevYes, the confidence interval on the model which assigns the probabilities is so large, any confidence on derived conclusions here is pretty meaningless. reply hiAndrewQuinn 6 hours agoparentprevCorrect, it would be better read as P(app start) * P(infra ok|app start) * P(first dollar|infra ok and app start). That's just a mouthful so I elided the \"givens\". reply kqr 2 hours agorootparentThis is usually the easiest way to estimate these probabilities also. Thanks for elaborating! reply kbrkbr 2 hours agorootparentprevYeah, that makes sense! Thanks for pointing out reply weinzierl 12 hours agoprev\"Software estimation is such a mess in part because it has trouble recognizing that, at least, just-in-time learning is at least non-normally distributed. Everything we know about traditional project management, from Waterfall to Gantt charts to estimation practices, are on some level based around the idea that each individual step in the chain is bell-shaped: [..]\" From experience I 100% agree with this, but it always caused a certain cognitive dissonance when thinking about project management approaches. A project is by definition a time bound one time endeavor. It is something where the crucial parts have never been done before. The unknown is the defining characteristic. Learning should be a defining characteristic as well, yet many approaches pretend it's a small part at best. My working theory is that the majority of \"projects\" that are typically used as comparison with IT projects (like construction) are not real projects at all. Real projects are rare (e.g. Burj Khalifa). In IT the ratio is the opposite. The real projects dominate and the \"construction sites\" are still the minority. But we can expect this to change. Thinking this to the end would mean that we could see a resurgence of waterfall in IT - likely under a different name. reply yarbas89 10 hours agoparentThe project is the project side of the construction project and not construction side of the construction project =D Typically, in construction projects, 90%[1] of the challenges are encountered and resolved in the design office by a multi-disciplinary, multi-organisation design team effort across multiple years... [1] - Anecdotal evidence drawn from my experience as a structural engineer. reply weinzierl 9 hours agorootparentMy point is that the \"project\" side is not really a \"temporary endeavor undertaken to create a unique product\" [1] in the majority of cases. Sure the environment, circumstances and result differs for every construction project, but the variance doesn't justify to call it a project in most cases in my opinion. We've built millions of modern bridges that are in operation and we've been doing it for more than 3000 years. The number of environments, configurations and requirements we've not already encountered is very limited and these are the real construction projects that - no doubt - do exist. Now, contrast this with the number of CRUD apps we've built and for how long we've been doing it. There is still a lot to learn there. That's the reason why more bridges are on time and budget than CRUD apps, in my opinion. [1] https://www.pmi.org/about/what-is-a-project reply yarbas89 8 hours agorootparentI sort of understand what you're saying but I'm struggling to agree with your examples in the comparison. Even if you're working on a \"project\" comprising hundreds of near identical houses, there could be massive differences in the project constraints and their solutions for adjacent plots for any given selection of houses. There could for example be a large tree with root protection zones on site where you would have to carefully design the foundations to account for this and their future effects such as heave due to volume change potential of the underlying soil. My point is that there are many \"hidden\" problems solved by the design team during the project / design phase even for seemingly insignificant or simpler \"projects\". In my experience of analysing and designing hundreds of buildings for over a decade every project was unique and as such treated like a \"project\". reply weinzierl 7 hours agorootparentI think there is a spectrum and I think most construction projects are more similar to mass production than real projects. I'm not a civil engineer and maybe I am wrong and it is not the best example. The point I am trying to make is that there are more unknowns in most IT projects than in many other endeavors because it is still a young field. If construction is the best counterexample, I'm not sure. reply 0x000xca0xfe 9 hours agoparentprev\"Construction site\" projects in IT would be Wordpress sites/small business eCommerce etc. and there are millions of successful projects of this type out there. Building them might be messy, but rarely fails from a technical point of view. reply weinzierl 8 hours agorootparent\"there are millions of successful projects of this type out there.\" I wish and believe we are getting there but I don't see it today. Just as an example: I traveled a lot after COVID and almost every tourist attraction has a photographer or photo site and they upsell by offering you pictures. I have yet to encounter two attractions that use the same system, they are all different and terrible in different ways. They all do basically the same thing, so there is no reason not to have a standard solution. I have no doubt one will replace the existing solutions sooner than later. So, millions of them but I would hardly call them successful. Them being messy is the collective learning process that has been mostly completed for real construction projects, but is still in full swing for IT projects. reply 0x000xca0xfe 7 hours agorootparent\"they are all different and terrible in different ways. They all do basically the same thing, so there is no reason not to have a standard solution\" Sounds exactly like small construction projects. I have heard this sentence almost word by word from somebody who owns multiple small rental objects. reply rolisz 7 hours agorootparentprevI think you two agree: there are millions of successful projects out there, but they are not standardized. But the guy who made 1000 of them knows exactly how long it will take him to build the 1001th reply Normal_gaussian 10 hours agoparentprevWhilst waterfall-like may be the approach, at least some (often the largest) steps are tough to estimate. The problem waterfall has is that it attracts deadlines like roadkill attracts flies. Presenting as following \"agile\" gives a strong body of literature to fight this - and actually do something else. We've all seen the criticisms that nobody does agile right - and for many, thats deliberate. reply cameldrv 1 hour agorootparentIMO a waterfall like process can work for a lot of projects, but you need a lot of time for estimation/derisking, and this work is hard, so often developers don't want to do it, and management/customers often don't want to pay for it. Some of the most successful projects I've worked on have started with an estimate that had only a few items with more than 4 hours attached to them. If you get estimates down to that level, that means that most of the significant technical decisions have already been made, and you've determined that your technical approach should work. If you do that, and then add a calibrated amount of padding for mistakes, you can very often hit estimates pretty much dead on (+-10%). How much time it takes to prepare the estimate depends a lot on the nature of the work though. If there's something pretty similar the team has done multiple times, and you have access to estimates/tickets/timesheets, you can often get it done in about 5-10% of the total project time. However, for example, if it's an ML project, there may be a lot of unknowns that have to be clarified first. For example, data quality is almost always not what it is claimed to be at the outset, so this needs to be resolved before the estimate is generated. reply weinzierl 9 hours agorootparentprevDeadlines are good. The world operates on deadlines and IT is the exception. We can build bridges on time and budget most of the time because we've built millions of them in modern times and have been doing it for more than 3000 years. In software we are not there but I have no doubt that we will, and hopefully it won't take us that long. reply tempaccount420 7 hours agorootparentOh man, software development will be so boring when we get to that point. reply danielbln 6 hours agorootparent\"GPT, I need this thing built to spec.\" \"Sure thing boss, come back in 25 minutes.\" reply mif 12 hours agoprevReminds me of [1]. We are good at estimating the median but not the mean. [1] https://erikbern.com/2019/04/15/why-software-projects-take-l... reply baq 9 hours agoparentAlso made this exact association. My takeaway: when estimating unknown quantities in software development you can treat means as infinite thus making any project unestimatable (management hates this reasoning - a friend told me, pinky promise); otherwise assign some non-zero probability than any mean will be 10-100x of the estimate. reply m0rc 9 hours agoprevI think the problem is not just software estimation, but project cost and time estimation is difficult across many domains (construction, transportation, IT, defense, and even the organization of events). The word \"mess\" seems to indicate that the uncertainty is easily fixed, but if it has happened across domains for several millennia, it could indicate that there is something fundamentally challenging. The situation worsens with factors such as project size, requirement changes, methodology (too much or too little of it), technology (specially new technologies), the nature of the delivering institution (public institutions performing worst than private ones), and organizational culture (e.g. waterfall being in many cases detrimental by making adaptations more difficult). There are certainly bad ideas in the field of estimation, like assuming a Gaussian distribution, but the problem is far from trivial. See for example: * Defense: Bolten, Joseph G., et al. Sources of weapon system cost growth: Analysis of 35 major defense acquisition programs. Rand Corporation, 2008. * Public works: Flyvbjerg, Bent, Mette Skamris Holm, and Soren Buhl. \"Underestimating costs in public works projects: Error or lie?.\" Journal of the American planning association 68.3 (2002): 279-295. * Transportation: Cantarelli, Chantal C., et al. \"Cost overruns in large-scale transportation infrastructure projects: Explanations and their theoretical embeddedness.\" arXiv preprint arXiv:1307.2176 (2013). * Olympic Games: Flyvbjerg, Bent, Alexander Budzier, and Daniel Lunn. \"Regression to the tail: Why the Olympics blow up.\" Environment and Planning A: Economy and Space 53.2 (2021): 233-260. reply spion 9 hours agoprevThis is fascinating, but I'm not sure this is the reason (or a good excuse for?) why software estimation is a mess right now. If you look at standard industry practice, the concept of \"probability\" rarely enters the discussion at all when estimating. I've personally had great success with three-point estimation. [1] As far as statistical methods go, its pretty simple - yet it tends to work surprisingly well. To my knowledge however, none of the popular software on the market has any of the tools needed to do even something as basic as this, at least out of the box. There is a whole set of tools out there for probabilistic modeling, and if we're serious about estimation, we should start talking about them. [1]: https://en.wikipedia.org/wiki/Three-point_estimation reply regularfry 7 hours agoparentFrom the trenches, the observable reason that approaches like this aren't taken up is often because of the power dynamics at play. Stakeholders want to treat delivery teams as feature factories, and typically have the organisational power to reject estimates that aren't in the form of \"this will be done tomorrow/next week/in three months\". They choose to reinforce the viewpoint that an inability to provide and keep to hard delivery deadlines is a marker of incompetence, rather than a realistic assessment of the basic physics of the situation. This is - to put it mildly - not helped by situations where hard deadlines are agreed to without speaking to delivery teams at all. There is a lack of maturity in how software delivery is commissioned in the industry, but the fundamental issue is that the political drivers that result where Taylorist management interfaces with unpredictability in delivery reward behaving in a way that doesn't improve that maturity. reply spion 6 hours agorootparent\"this will be done tomorrow/next week/in three months\" This type of answer is also possible if you can agree on the desired confidence. I've found that 95% confidence is sufficient for most cases, but that number will depend on context. Understanding the stakes of the deadline for stakeholders can help adjust this further. I agree that building trust with stakeholders can be tricky, and discussions about risks can be sometimes be interpreted as unwillingness to take responsibility. In that situation, the team can at least get a measure on their level of confidence and give an initial number they're somewhat comfortable with, as well as an estimate of how likely to succeed the agreed upon number is. reply bruce343434 8 hours agoparentprevWhat is your technique for estimating in the case that the tech stack is (at least in part) unfamiliar? Can this technique be applied by junior developers with only a few years experience? reply spion 7 hours agorootparentFor cases where the tech stack is in some ways unfamiliar, we create research stories where the goal is to do the quickest thing possible to answer questions about the tech. This is typically timeboxed by the max time we'd be willing to spend on it, rather than the estimated maximum time. Its still useful to try and put a maximum estimate if possible, because then you can also estimate probability of success for the timebox which makes you better prepared for discussion with business to help them decide if they want to take the risk to invest time in it. Few years of experience should be quite sufficient, although I think overall team culture is probably more important. I've usually done this type of estimation together with teams where there's already good communication, but when that's not the case something like pointing poker to open up safe discussion and uncover uncertainties may make sense as well. reply vslira 5 hours agoprevI think I've said this before on this forum, but it bears repeating: if there's one thing that I'll never forget from my Industrial Engineering degree is that all processes can be designed around long waits, but it's the variance that kills you. reply thesz 6 hours agoprev\"Doing\" is also non-normally distributed, because you cannot do something in a negative amount of time. reply vasco 10 hours agoprevPeople who are super into software estimation can never explain how estimations would go for Ron Jeffries implementing Sudoku in TDD vs Norvig. When you understand that difference you mellow out about estimation. reply krona 11 hours agoprevAt least superficially, doing is not normally distributed either since a task cannot take negative time, but could take an infinite time to complete. reply dxdm 10 hours agoparentWhat's wrong with having a normal distribution of only non-negative values? Or is somebody specifically talking about a _standard_ normal distribution? reply mrks_hy 10 hours agorootparentA normal distribution is infinite so there is always a (small) nonzero expected value for any real number, so technically, even for \"mean\" much larger than zero. However, GP is accidentally reversing the causality, which is incorrect: We say that something follows a normal distribution if the samples we observe fit, e.g. shoe sizes or height, which are clearly non-negative. It doesn't mean that any possible distribution value must occur in the original samples for the population to be normally distributed. reply dxdm 10 hours agorootparentThanks for explaining! This stretching to infinity is interesting, because it assigns a probability to outcomes even if they cannot happen, for example where observed values cannot go negative like here. That shows the difference between the \"model\" and the reality we try to fit it on; and that the model is a tool that just needs to fit well enough to be useful. reply pdpi 9 hours agorootparentYeah, it’s very much a case of “the map is not the territory”. Modelling such processes as normally distributed is objectively wrong, but it’s also not wrong enough to matter. We could, and arguably should, model such processes as truncated normals instead, but normals are just so damn convenient to work with. reply baq 9 hours agorootparentYou say 'convenience' and I don't know if it's a strong enough word - the gap in difficulty between working with normals and anything else is like in the draw the rest of the owl meme - it's so wide nobody ever bothers. reply zmgsabst 10 hours agorootparentprevI think the OPs point is that it’s bounded below at a point not-distant from the mean, and hence you have a one-tailed and not two-tailed distribution. That non-trivially changes the distribution. And then commented why: you have a finite amount you can do a task faster; but an infinite amount you can do it slower. That is in contrast to heights or shoe sizes, which are (effectively) bounded on both sides while having the bounds distant from the mean. reply mrks_hy 10 hours agorootparentThat is just being pedantic. In reality, there are no observable tasks that take infinite time. And the actual statistical process is probably unknowable anyway, we are very likely dealing with a sum of random variables, hence the Central Limit Theorem applies anyway. reply kbrkbr 10 hours agorootparentprevA normal distribution goes to infinity in both directions. Moving the center will not change that. reply krona 10 hours agorootparentprevWhat happens to the distribution as it approaches zero? reply mrks_hy 10 hours agorootparentSee my sibling reply, there is no requirement that you observe negative values for a process to be normally distributed. reply qp11 11 hours agoprevLook at the Explore-Exploit Tradeoff - https://en.wikipedia.org/wiki/Exploration-exploitation_dilem... All Problems are not created the same. And therefore what learning and doing happens depends on the Kind of Problem. Exploring an unknown jungle for gold is a very different problem than Exploiting an existing goldmine. One problem is more biased towards learning, while the other is biased towards doing. The managers and teams you end up producing for each type of problem is very different. The distribution of skills/knowledge/experience/creativity/curiosity etc at the end of solving an explore problem is very different from what you end up with solving an exploit problem. Issues arise when you take a team thats good at Explore and make them work on Exploit. Or vice versa. In our current chaotic ever changing environment most problems have elements of both that keep changing. reply psychoslave 11 hours agoprevThat doesn’t look so specific to software field though. We, mankind, are making buildings since even before we had writing systems. Are they perfect methodologies to make any building in expected time? Sure you can maybe make a bit more of rationalization and replicate a house many time with affordable local material. But you will not replicate the same house everywhere, because they don’t have the same material near them and move it from far might be economically prohibitive and not competitive with local material. Also your house design is probably a poor fit for many environments. And people have different cultural expectation in term of architecture due to historical factors, plus each individual comes with its own whims. Where is the standard scalable (both shrink and expand) omni-environment bridge design, hmm? All bridges have the same basic purpose, and yet I have no doubt building most of them have been a challenging endeavor of its own. reply Sharlin 7 hours agoparentI'd say that's just a function of how much learning you have to do while doing. If you have all the information required to do something, the the time needed probably approaches a normal distribution. But any unknown unknowns that you encounter are vastly more likely to delay than expedite the project, making the right-side tail both fatter and longer. reply yabbs 11 hours agoprevSimilar idea [1]: >Engineering mode shuns uncertainty, because uncertainty may involve risk that corresponds to bad surprises. Discovery mode thrives under uncertainty, especially when a rare but beneficial result leads to finding something new, or a reduction of uncertainty in the face of making strategic decisions. In summary, to understand the distinctions of Discovery and Engineering modes, one needs to have an appreciation for variation and the underlying distribution of outcomes expected while operating in each mode respectively. Without understanding the asymmetry in their outcome distributions, it would be difficult to convey how these work modes are different. [1] https://jabustyermancom.wordpress.com/tag/discovery-vs-engin... reply srg0 9 hours agoprevIf learning durations were log-normally distributed, how would people be able to graduate from universities and finish studies, mostly in time? Or accomplish anything substantial in their limited time span? I agree that distribution of most human tasks' duration is skewed (not necessarily distributed log-normally), but these tasks can still have a reasonable upper bound for completion. The success is not binary. Like in grading, we need to accept that some projects will get an A, and some will get only B or C, and it's still OK. Some may fail. reply baq 9 hours agoparentA curriculum is designed. It's a product. It's optimised for teachers to deliver lessons in such a way that students can learn faster than log-normal. Designing and implementing a curriculum is probably a log-normal effort. reply H8crilA 7 hours agorootparentExactly. To see why notice that in a curriculum you are presented with both a problem and a solution. You are encouraged to find your own solutions to many problems, but regardless of whether you do you are also presented with the correct (optimal) solutions. This removes inaccuracies in your thinking, which would otherwise pile up multiplicatively, yielding a log-normal distribution of the time needed to master some topic. reply mrkeen 9 hours agoparentprev> If learning durations were log-normally distributed, how would people be able to graduate from universities and finish studies, mostly in time? By studying an exponentially smaller domain each time round: * In primary school, study \"The life-cycles of animals\" * In secondary school, study \"mitosis and meiosis\" * As an undergrad, study \"insect reproduction at a cellular level\" * As a postgrad, study \"the effect of a particular molecule on the timing of a mosquito's development into sexual maturity\" reply ahtihn 2 hours agoparentprev> If learning durations were log-normally distributed, how would people be able to graduate from universities and finish studies, mostly in time? If you took a hundred 5 year olds and set the objective to achieve the same PhD in the same field with unlimited time, guess how the time to achieve it would be distributed? I'm sure some won't achieve it in their lifetime, so I disagree that there's a reasonable upper bound. reply mirrorlake 6 hours agoprevWhen simulating or modeling how long something will take, (0 hours, 1 hours, 2 hours...) you never use a normal distribution because of the potential for negative values in the left tail. You would rule that out by default. The title itself implies that the author has this misconception. reply sanxiyn 12 hours agoprevPharmaceutical companies know this by heart. What you are doing is learning about your drug candidates. Learning is not normally distributed. reply richrichie 7 hours agoprevFascinating thing about Central Limit Theorem is that sum of log normals also goes to normal in the limit. reply nabla9 11 hours agoprev [–] Chapter 11: Prepare to throw one away Pilot Plants and Scaling Up Chemical engineers learned long ago that a process that works in the laboratory cannot be implemented in a factory in only one step. An intermediate step called the pilot plant is necessary to give experience in scaling quantities up and in operating in nonprotective environments. For example, a laboratory process for desalting water will be tested in a pilot plant of 10,000 gallon/day capacity before being used for a 2,000,000 gallon/day community water system. Programming system builders have also been exposed to this lesson, but it seems to have not yet been learned. Project after project designs a set of algorithms and then plunges into construction of customer-deliverable software on a schedule that demands delivery of the first thing built. In most projects, the first system built is barely usable. It may be too slow, too big, awkward to use, or all three. There is no alternative but to start again, smarting but smarter, and build a redesigned version in which these problems are solved. The discard and redesign may be done in one lump, or it may be done piece-by-piece. But all large-system experience shows that it will be done.[ Where a new system concept or new technology is used, one has to build a system to throw away, for even the best planning is not so omniscient as to get it right the first time. The management question, therefore, is not whether to build a pilot system and throw it away. You will do that. The only question is whether to plan in advance to build a throwaway, or to promise to deliver the throwaway to customers. Seen this way, the answer is much clearer. Delivering that throwaway to customers buys time, but it does so only at the cost of agony for the user, distraction for the builders while they do the redesign, and a bad reputation for the product that the best redesign will find hard to live down. Hence plan to throw one away; you will, anyhow. You know the source. reply BigglesB 11 hours agoparentHadn’t thought of it in this context before but in game development a “vertical slice” could potentially be thought of as akin to the “pilot plant” in that it’s about proving out the _processes_ for creating game content (and working out how long each piece of content should take to make) before committing to “scaling up” said content production to deliver the full game… reply seventhtiger 7 hours agorootparentGame development is intensely iterative. One way to think of games is software where its only value is its usability. It has has no process workflows or business goals that it ever sacrifices usability for because the user experience is the value proposition itself. It only sacrifices the user experience for other aspects of the user experience. Failed and not fun games do happen, but in general game developers put far more weight to this sort of process. There are no other goals that you can claim were accomplished if the users think the game sucks. So you get these ruthless production cycles and there's an appetite to cut things that don't work. reply roenxi 6 hours agoparentprevIt is an interesting theory and a nice story; but I don't believe it. One of the key issues in programming systems is the profound uncertainty about what the system is meant to be doing. In chemical engineering you know exactly what is supposed to be built and have a clear idea of how the major reactions should work and what the tolerances are. Both situations require design around uncertainty, but the chemical engineer is looking for practical kinks in a roughly-understood system. The programmatic system has no idea what is value add, what will stay or go or anything like that. In the extreme case you get things like Slack or Discord, where the plan wasn't even trying to build communications software to start with. The nature and scale of the uncertainty is different. Software needs to adopt a fail-fast iterative approach with lots of feedback. Chemical engineers are looking for something else. The \"pilot plant\" concept wouldn't carry to software. Although the \"we're going to throw this out\" attitude works wonders (unless you rely on it happening, in which case that code is permanent now for some reason). reply piva00 10 hours agoparentprevFred Brooks as usual stands the test of time. It's incredible how many times I've quoted some part of Mythical Man-Month in my career, it's also part of the recommended literature I give to every junior I've trained in my life. I've noticed lately that it's become less common someone in the room has read it when I mention a passage from it, be it management or other software devs. Not sure why but it used to be much more common that I'd bring the argument against a Second-System and at least one other person would recognise it, or the issues with communication channels between a larger group. For any professional software developer out there who haven't read it, please do, it's a collection of wisdom from 50 years ago that still applies today. reply scubbo 9 hours agorootparent> Hence plan to throw one away; you will, anyhow. > Fred Brooks as usual stands the test of time.[...]I'd bring the argument against a Second-System I'm confused. The GP appears to be saying that one should plan to build two systems - one as a proof-of-concept, and another (which is actually presented to most customers) in which one does things \"the right way\" based on learnings during that process - whereas you (and Fred[0]) seem to allude that the second system tends to be bloated and over-engineered. What am I missing? [0] https://en.wikipedia.org/wiki/Second-system_effect reply nabla9 8 hours agorootparentPrototype is not the first system. reply mpweiher 8 hours agorootparentprev\"If you plan to throw away one, you will throw away two\" -- Programming Pearls https://tildesites.bowdoin.edu/~ltoma/teaching/cs340/spring0... reply piva00 8 hours agorootparentprevSecond-system effect is the replacement of a full production application with another full blown production application where the second-system has to replicate all (or at least most) of the features from the first-system into the second one while also adding new features into the second-system. Even worse are the cases where the first-system still has to coexist and keep being developed while the replacement second-system is in development, that usually leads to a never-ending catch up of the second-system trying to integrate all the changes from the first-system that happened after the start of the second-system project. Have you read the book? I'd recommend it! reply eru 10 hours agorootparentprevIt's always amusing to read Fred Brooks' own lamentations. He's flattered that people still quote him, but also horrified that what he thought was his first, fledgling start in software project management is still quoted as start of the art. We haven't learned anything. reply bckr 6 hours agorootparentSurely we’ve learned some things? After Mythical Man Month there’s been other books (and talks), yes? I’ve read The Phoenix Project (based on The Goal) and its sequels. What should be on every EM’s shelf? reply eru 4 hours agorootparent> Surely we’ve learned some things? I was exaggerating a bit. Brooks lamented that we've learned so little, that his observations stay relevant, instead of being superseded by something more systematic. In contrast, if you were learning how to write a compiler or operating system today, you (hopefully!) wouldn't pick up a book published when the Mythical Man Month came out. reply skydhash 6 hours agorootparentprevPeopleware by Tom Dermaco & Timothy Lister. Making Things Happen by Scott Berkun. Bu I haven’t finished reading it yet. reply worldsayshi 6 hours agoparentprev [–] When building a plant you can only build the same plant once. When building software you can keep iterating on the same software forever. How do you know when to start over and when to keep going? There seems to be no consensus on that? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The essay highlights the challenges of software estimation, noting that traditional project management often incorrectly assumes tasks follow a normal distribution.",
      "In reality, software development tasks often follow a log-normal distribution, where significant deviations are common due to the unpredictable nature of just-in-time learning.",
      "The essay emphasizes the importance of relevant experience and specific tooling knowledge in hiring, as learning new tools can vary widely in time and cost, making accurate timeline estimation difficult."
    ],
    "commentSummary": [
      "The discussion highlights the probabilistic challenges in software project estimation, particularly the differences between normal and log-normal distributions in task execution and learning.",
      "It critiques traditional project management methods, emphasizing the misuse of agile methodologies and the importance of accurate estimation despite uncertainties.",
      "The conversation explores the differences between Discovery and Engineering modes in project management, the necessity of iterative development, and the enduring relevance of Fred Brooks' \"The Mythical Man-Month.\""
    ],
    "points": 225,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1716874349
  },
  {
    "id": 40496150,
    "title": "gh-dash: Customizable GitHub CLI Dashboard for PRs and Issues",
    "originLink": "https://github.com/dlvhdr/gh-dash",
    "originBody": "gh-dash ✨ A GitHub (gh) CLI extension to display a dashboard with pull requests and issues by filters you care about. ✨ Features 🌅 fully configurable - define sections using github filters 🔍 search for both prs and issues 📝 customize columns with hidden, width and grow props ⚡ act on prs and issues with checkout, comment, open, merge, diff, etc... ⌨ set custom actions with new keybindings 💅 use custom themes 🔭 view details about a pr/issue with a detailed sidebar 🪟 write multiple configuration files to easily switch between completely different dashboards ♻ set an interval for auto refreshing the dashboard 📦 Installation Install the gh CLI - see the installation Installation requires a minimum version (2.0.0) of the the GitHub CLI that supports extensions. Install this extension: gh extension install dlvhdr/gh-dash To get the icons to render properly you should download and install a Nerd font from https://www.nerdfonts.com/. Then, select that font as your font for the terminal. Installing Manually How do I get these exact colors and font? ⚡ Usage Run gh dash Then press ? for help. Run gh dash --help for more info: Usage: gh dash [flags] Flags: -c, --config string use this configuration file (default is $GH_DASH_CONFIG, or if not set, $XDG_CONFIG_HOME/gh-dash/config.yml) --debug passing this flag will allow writing debug output to debug.log -h, --help help for gh-dash ⚙ Configuring A section is defined by a: title - shown in the TUI filters - how the repo's PRs should be filtered - these are plain github filters All configuration is provided within a config.yml file under the extension's directory (either $XDG_CONFIG_HOME/gh-dash or ~/.config/gh-dash/ or your OS config dir) or $GH_DASH_CONFIG. An example config.yml file contains: prSections: - title: My Pull Requests filters: is:open author:@me layout: author: hidden: true # width:# grow:this will make the column grow in size - title: Needs My Review filters: is:open review-requested:@me - title: Subscribed filters: is:open -author:@me repo:cli/cli repo:dlvhdr/gh-dash limit: 50 # optional limit of rows fetched for this section issuesSections: - title: Created filters: is:open author:@me - title: Assigned filters: is:open assignee:@me - title: Subscribed filters: is:open -author:@me repo:microsoft/vscode repo:dlvhdr/gh-dash defaults: layout: prs: repo: grow: true, width: 10 hidden: false # issues: same structure as prs prsLimit: 20 # global limit issuesLimit: 20 # global limit preview: open: true # whether to have the preview pane open by default width: 60 # width in columns refetchIntervalMinutes: 30 # will refetch all sections every 30 minutes repoPaths: # configure where to locate repos when checking out PRs :owner/:repo: ~/src/github.com/:owner/:repo # template if you always clone github repos in a consistent location dlvhdr/*: ~/code/repos/dlvhdr/* # will match dlvhdr/repo-name to ~/code/repos/dlvhdr/repo-name dlvhdr/gh-dash: ~/code/gh-dash # will not match wildcard and map to specified path keybindings: # optional, define custom keybindings - see more info below theme: # optional, see more info below pager: diff: less # or delta for example confirmQuit: false # show prompt on quit or not 🗃 Running with a different config file You can run gh dash --configto run gh-dash against another config file. This lets you easily define multiple dashboards with different sections. It can be useful if you want to have a 🧳 work and 👩💻 personal dashboards, or if you want to view multiple dashboards at the same time. ⌨ Keybindings Define your own custom keybindings to run bash commands using Go Templates. This is available for both PRs and Issues. For PRs, the available arguments are: Argument Description RepoName The full name of the repo (e.g. dlvhdr/gh-dash) RepoPath The path to the Repo, using the config.yml repoPaths key to get the mapping PrNumber The PR number HeadRefName The PR's remote branch name BaseRefName The PR's base branch name For Issues, the available arguments are: Argument Description RepoName The full name of the repo (e.g. dlvhdr/gh-dash) RepoPath The path to the Repo, using the config.yml repoPaths key to get the mapping IssueNumber The Issue number Examples To review a PR with either Neovim or VSCode include the following in your config.yml file: repoPaths: dlvhdr/gh-dash: ~/code/gh-dash keybindings: prs: - key: c command: > tmux new-window -c {{.RepoPath}} ' gh pr checkout {{.PrNumber}} && nvim -c \":DiffviewOpen master...{{.HeadRefName}}\" ' - key: v command: > cd {{.RepoPath}} && code . && gh pr checkout {{.PrNumber}} To pin an issue include the following in your config.yml file: keybindings: issues: - key: P command: gh issue pin {{.IssueNumber}} --repo {{.RepoName}} 🚥 Repo Path Matching Repo name to path mappings can be exact match (full name, full path) or wildcard matched using the owner and partial path. An exact match for the full repo name to a full path takes priority over a matching wildcard, and wildcard matches must match to a wildcard path. An :owner/:repo template can be specified as a generic fallback. repoPaths: :owner/:repo: ~/src/github.com/:owner/:repo # template if you always clone github repos in a consistent location dlvhdr/*: ~/code/repos/dlvhdr/* # will match dlvhdr/repo-name to ~/code/repos/dlvhdr/repo-name dlvhdr/gh-dash: ~/code/gh-dash # will not match wildcard and map to specified path The RepoName and RepoPath keybinding arguments are fully expanded when sent to the command. 💅 Custom Themes To override the default set of terminal colors and instead create your own color scheme, you can define one in your config.yml file. If you choose to go this route, you need to specify all of the following keys as colors in hex format (#RRGGBB), otherwise validation will fail. theme: ui: table: showSeparator: true colors: text: primary: \"#E2E1ED\" secondary: \"#666CA6\" inverted: \"#242347\" faint: \"#3E4057\" warning: \"#F23D5C\" success: \"#3DF294\" background: selected: \"#39386B\" border: primary: \"#383B5B\" secondary: \"#39386B\" faint: \"#2B2B40\" 🪟 Layout You can customize each section's layout as well as the global layout. For example, to hide the author column for all PR sections, include the following in your config.yml. defaults: layout: prs: author: hidden: true For prs the column names are: updatedAt, repo, author, title, reviewStatus, state, ci, lines, assignees, base. For issues the column names are: updatedAt, state, repo, title, creator, assignees, comments, reactions. The available properties to control are: grow (false, true), width (number of cells), and hidden (false, true). Author Dolev Hadar dolevc2@gmail.com",
    "commentLink": "https://news.ycombinator.com/item?id=40496150",
    "commentBody": "Gh-dash: A beautiful CLI dashboard for GitHub (github.com/dlvhdr)214 points by robenkleene 18 hours agohidepastfavorite55 comments dcre 5 hours agoThis is really cool! I have something similar just for checking out PRs in a single bash function, powered by gh and fzf: function ghpr() { gh pr list --limit 100 --json number,title,updatedAt,author --template \\ '{{range .}}{{tablerow .number .title .author.name (timeago .updatedAt)}}{{end}}'fzf --height 25% --reversecut -f1 -d ' 'xargs gh pr checkout } Screenshot: https://github.com/oxidecomputer/console/assets/3612203/2805... reply achristmascarl 2 hours agoparentspeaking of fzf, i have an alias for checking out branches using it + git: alias gbs=\"git branch --sort=-committerdatefzfxargs -I{} git checkout {}\" reply danielvaughn 15 hours agoprevI'm really loving this whole trend of well-designed TUI applications that Charm has enabled. reply theshrike79 12 hours agoparentI'm guessing this is the Charm you're referencing: https://charm.sh ? reply renewiltord 2 hours agorootparentThanks for mentioning this, guys. Pretty slick looking toolkit. I always wondered how everyone these days has such fancy looking CLIs. reply danielvaughn 6 hours agorootparentprevCorrect, sorry yeah I should've linked to it. reply oefrha 14 hours agoparentprevFunny that Go is top notch in the TUI space thanks to Charm while the GUI landscape is absolutely abysmal (Wails is the only thing that’s okay). reply endgame 13 hours agorootparentI feel like this \"missing middle\" is common to a lot of language communities these days. Devs writing for themselves are often happy to make cool TUIs, and devs writing for \"end users\" are often writing webapps. reply oooyay 4 hours agorootparentIf you know Typescript and have a component framework you like it's about the same level to write a TUI in Charm as it is to write something in Wails. I kind of alternate between them. reply theshrike79 12 hours agorootparentprevI've heard some good things about Fyne: https://github.com/fyne-io/fyne Doesn't do native-looking software, but the APIs don't look that bad. reply oefrha 10 hours agorootparentI’ve used it twice, both times regretted and rewrote in Wails (that is, back to a web view for UI). Problems not limited to: - (Last I checked, which isWhy? Strongly suggest updating the README to explain why this is useful, or what kind of workflow makes this useful. Just a list of features isn't that compelling to me because I'm not sure I want it. I don't mean to speak for the author, but: The 10 seconds it took for the GitHub web UI to load the homepage of this project is a great justification for its existence! reply peter_l_downs 3 hours agorootparentI hear that, but this is fetching data from the Github graphql API the same way that the website does, so if you're getting slow responses from their servers this won't be any faster. reply inhumantsar 4 hours agoparentprevI'd be keen to check that out! I've been toying around with similar thoughts the last year or so. Metrics and dashboards aren't enough and the GH site doesn't really have an org-level view. reply peter_l_downs 4 hours agorootparentnot sure how to contact you, but if you email me i'll follow up once i have something available for testing! (my email is in my bio) reply autoexecbat 2 hours agoparentprev> Why? Sometimes it's nice to just stay in tmux reply mtmk 4 hours agoprevVery cool. Coincidentally, I was looking into starting to use Midnight Commander (again). When I run it in a repo, I still see my overall PRs, etc. Is there an option to get it to focus on the current repo? reply kreelman 14 hours agoprevHas anyone thought of doing something like this for Gitlab? I'll browse the code and see how readily it could be converted over to working on Gitlab.... also, Gitlab has a GraphQL API available to get at various bits of data surrounding a repo. I've not looked at this code yet, perhaps it does something similar....? reply locusofself 13 hours agoprevI wish I had this for Azure DevOps. Work at MSFT and don't have a choice in the matter. reply alternatex 8 hours agoparentBit misleading. Working at MSFT doesn't mandate to use ADO for hosting code. Plenty of teams host their projects on GitHub. reply DiggyJohnson 1 hour agorootparentIt's not misleading. The obvious assumption is that GP works for a team at Microsoft that uses ADO and not Github. reply jayknight 43 minutes agorootparentTo be fair, it does read like \"Work at MSFT and [therefore] don't have a choice in the matter.\" It's not an institutional policy (MS owns github after all), so it would have been more clear to say \"Work on a team that uses ADO and don't have a choice in the matter.\" reply powerapple 9 hours agoprevLove it, thank you! I couldn't get the help screen by '?' though. I can escape the input box with esc, but then '?' will refocus on the search box again. EDIT: got it now, not sure why it didn't work before reply alchemist1e9 15 hours agoprevLooks very nice! Slightly related question - is there any self-hosted github API compatible solution? I’m pretty sure the answer is no, but figured I ask. reply argulane 11 hours agoparentGitea API is pretty close to GitHub https://docs.gitea.com/ reply alchemist1e9 6 hours agorootparentgh-dash should add Gitea API support! I’ve used gogs a lot in the past and it’s worked very well for my use case which Gitea is a fork of. It would be great to have a fully self-hosted setup with this tui interface. reply pvcnt 11 hours agoparentprevShameless plug: I developed a similar project that is Web-based (source [1], demo [2]) and supports having multiple connections configured. Both github.com and GitHub Enterprise are supported. Other backends (e.g., GitLab, Azure DevOps) could be added in theory. I started this because at work, we use no less than 2 instances of GitHub Enterprise, in addition to github.com. It is a client-side application, meaning that it is very simple to host (no backend required). [1] https://github.com/pvcnt/reviewer [2] https://reviewer.pages.dev reply eyegor 14 hours agoparentprev+1 for gitlab support -- self hosted gitlab (or gitea) is probably the most popular one, since self hosted github is very pricy reply Galanwe 13 hours agoparentprevWould be nice to have it for Github Enterprise for sure! reply rajishx 11 hours agoparentprevgitea, forgejo (but not 100% api compatible) reply bee_rider 12 hours agoprevThere’s something a little funny about a CLI interface to GitHub, a website based on making it easier to use… git, a CLI program. Which isn’t to poo-poo the project, GitHub adds a bunch of extra features so I can see why someone might want to build up on it. It’s just funny, the winding path our files take to get to our eyeballs. reply rejschaap 11 hours agoparentThe funny part is that we decentralized version control with git and then centralized software development in GitHub reply williamdclt 6 hours agorootparentPeople don't care about whether their tools are decentralized or centralized (apart from a few principled individuals). They care about the UX of the tool, and so far centralised options have won on UX. I'm not sure how much of that is due to being centralised, rather than being due to companies investing in UX to gain market share preferring a centralised model. Ofc centralised/decentralised is also part of the UX (what happens when the centralised service goes down), but so far it's not been a net win for decentralised. reply yavor-atanasov 8 hours agorootparentprevWe like creating decentralised technologies to then graviate around centralised services built on top. The Internet was meant to be decentralised (it still is in terms of the protocols that make it work), but then we ended up consuming very much centralised services. Bitcoin's decentralised in terms of technology, but then we are centralising in terms of exchanges, wallets etc. I guess the decentralisation aspects of a given technology just means it's a resilient building block. And since the only way we make sense of things nowadays is through markets, it's inevitable we starting building and consuming centralised services. I'm not saying centralisation is good or bad btw, but I do share the irony in your reply, mostly because I find it funny when a new tech is being sold as new way of doing something in a decentralised way. reply setr 8 hours agorootparentDecentralization mainly grants flexibility/customization/freedom, but generally you don’t really care about this — freedom only really matters when you can’t do the thing you’re trying to do. If everything is already covered without such freedom, or you don’t care about what’s not covered (or failed to conceive it), then you don’t really mind having the freedom or not. It doesn’t change much. Gmail is a good email client — email being decentralized is only relevant to me if I wanted to get off gmail to go to say fastmail. But if I didn’t, what do I care whether the underlying protocol is centralized or not? reply bee_rider 4 hours agorootparent> freedom only really matters when you can’t do the thing you’re trying to do. Disagree, or at least think there’s a need to clarify. This makes it look like a freedom is only occasionally relevant. This kind of freedom derived from decentralization also exists as a persistent threat against bad behavior. If users can leave and bring their stuff with them, that constrains the choices that the platform can even consider. Attempts to centralize should be seen as strategic attempts to change the landscape in a way that makes it easier to exploit users. reply dreamcompiler 8 hours agorootparentprevWe did the same thing with the Web but we prefer Google and Facebook. We did the same thing with email but we prefer Gmail. We did the same thing with Usenet but we prefer Reddit. We did the same thing with IRC but we prefer Slack. We did it with shitcoins but we prefer Coinbase. Turns out people really don't care much for pure decentralization. There's money to be made recentralizing decentralized whatever. reply bee_rider 4 hours agorootparentIt is way more difficult to exploit users of a decentralized service, because they can just go to another host. reply cqqxo4zV46cp 9 hours agorootparentprevNot that funny when “we” means a different thing in each of its uses. reply bathwaterpizza 5 hours agoparentprevThat's the way you see GitHub? for me it's a repo hosting platform, the UI is just a pleasantly reply bee_rider 4 hours agorootparentI don’t use the site much at all, but it isn’t really a matter of how I see the site—they clearly added features like issue tracking, commenting, and CI, and people use those features. reply 3D39739091 3 hours agorootparentThe point is that Github is not just a web-based graphical client for Git. reply burgerrito 12 hours agoprev [–] Possibly an unpopular opinion: TUI application should work out of the box, or in other words, it shouldn't need another font installation (NerdFont in this case) to run and render properly. One of the TUI app that does this well is Helix editor and I love it. reply godelski 12 hours agoparentI really agree here, as someone who uses nerd fonts a lot[0]. There should always be a default without nerd fonts. That said, what does it actually look like without? The docs say \"render properly\". I haven't tried it yet, let alone without nerd fonts. That also said, I'd give this one a bigger break than most. If you're doing development work on a machine you probably got decent control and at least enough space to install fira. [0] I suspect there are two types of people that are terminally terminal: those that are just in their own shells and can always control their full environment and those that work in many machines. I'm the latter, but I tend to notice the latter are also the former, but make different choices because of this, or at least smarter dotfiles that will change things based on environment or (like me) have source install scripts (you can always build a program locally ;) I'm glad people live in the shells, but I hope people making TUIs can realize that there's a bunch of us that have to move machines constantly. reply strathos 12 hours agoparentprev [–] Somewhat related, this is the main reason why I settled on using Wezterm as my terminal. It comes with Nerd Fonts pre-bundled. reply behnamoh 4 hours agorootparent [–] That seems like an unreasonably over the top reason to switch terminals while you literally can brew install any font. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "**gh-dash** is a GitHub CLI extension offering a customizable dashboard for managing pull requests (PRs) and issues.",
      "Key features include configurable sections, search and customization options, themes and layouts, detailed views, multiple configurations, and auto-refresh intervals.",
      "Installation steps: Install GitHub CLI (v2.0.0+), install the extension with `gh extension install dlvhdr/gh-dash`, and install a Nerd font for icon rendering."
    ],
    "commentSummary": [
      "The discussion highlights \"Gh-dash,\" a CLI dashboard for GitHub by dlvhdr, emphasizing the growing trend towards well-designed text user interfaces (TUIs) and the efficiency of command-line interface (CLI) tools over web interfaces.",
      "Users express interest in features such as organization-level views and self-hosted solutions for managing repositories across platforms like GitHub, GitLab, and Azure DevOps.",
      "The conversation also covers the irony of creating a CLI for a web-based tool, the importance of flexibility and decentralization in technology, and debates on the usability of TUIs without additional font installations, including the use of Nerd Fonts and tools like Wezterm."
    ],
    "points": 214,
    "commentCount": 55,
    "retryCount": 0,
    "time": 1716855927
  },
  {
    "id": 40495149,
    "title": "Transformers Show Implicit Reasoning Abilities Through Grokking, Study Reveals",
    "originLink": "https://arxiv.org/abs/2405.15071",
    "originBody": "Computer Science > Computation and Language arXiv:2405.15071 (cs) [Submitted on 23 May 2024 (v1), last revised 27 May 2024 (this version, v2)] Title:Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization Authors:Boshi Wang, Xiang Yue, Yu Su, Huan Sun View PDF HTML (experimental) Abstract:We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting. The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison. We delve into the model's internals throughout training, conducting analytical experiments that reveal: 1) the mechanism behind grokking, such as the formation of the generalizing circuit and its relation to the relative efficiency of generalizing and memorizing circuits, and 2) the connection between systematicity and the configuration of the generalizing circuit. Our findings guide data and training setup to better induce implicit reasoning and suggest potential improvements to the transformer architecture, such as encouraging cross-layer knowledge sharing. Furthermore, we demonstrate that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning. Comments: 22 pages, 16 figures. Code and data: this https URL Subjects: Computation and Language (cs.CL) Cite as: arXiv:2405.15071 [cs.CL](or arXiv:2405.15071v2 [cs.CL] for this version)https://doi.org/10.48550/arXiv.2405.15071 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Boshi Wang [view email] [v1] Thu, 23 May 2024 21:42:19 UTC (7,877 KB) [v2] Mon, 27 May 2024 03:55:35 UTC (7,877 KB) Full-text links: Access Paper: View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CLnewrecent2405 Change to browse by: cs References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=40495149",
    "commentBody": "Grokked Transformers Are Implicit Reasoners (arxiv.org)199 points by jasondavies 21 hours agohidepastfavorite53 comments scarmig 16 hours agoSince I first learned about grokking, I've had a strong suspicion that getting a handle on it and figuring out how to aid it should be the central question in AI. We are currently stuck in a local minimum, where memorizing circuits perform well-enough to handle a whole lot of economically viable use cases. But the profit function has guided us into a valley dominated by a data and compute hungry architecture that isn't ideal for learning generalizing circuits (partially because the memorizing circuits are so effective! We relatively quickly get to a flat loss landscape, after which we blindly jump around for countless epochs in a kind of Brownian motion until we get into an area where regularizers can drive generalization). Research like this paper is incredibly important. I thought this was the most interesting bit from the paper: > Training data distribution, instead of training data size, qualitatively influences generalization behavior. reply aantix 4 hours agoparentIf there are many examples of X on the web, and only a few examples of related Y, would you actively prohibit the additional X samples to be included in the training set to prevent the reinforcement of the common case? reply scarmig 2 hours agorootparentFor the purpose of generalization, I think that's more treating a symptom than the root cause. The better approach IMO would be finding architectures that heavily penalize the formation of memorizing and interpolating circuits. E.g much stronger weight decay than used today. reply taskforcegemini 2 hours agoparentprevI'm sure this is what happened to google search reply scarmig 2 hours agorootparentKind of, but that's not inherent to the over reliance on memorization. I suspect using the top tier models from any of OAI, Anthropic, or Google would have resulted in much less embarrassing results, and I believe they're all primarily memorizers, not generalizers. The search issue happened because Google had to use a really cheap model to power the search results, and a memorizing model that cheap is going to be highly constrained in capabilities (at least right now). reply chaorace 4 minutes agorootparentIt doesn't help that they were caught with their pants down back when GPT-3 first entered the zeitgeist (and thus also the radar of Google's institutional stakeholders). Haste made waste, corners got cut, cargo went overboard. reply nico 3 hours agoprevConceptually, grokking reminds me of the concept presented in the book The Dip Most people, for most tasks, will only learn/train/try to improve, up to where they get to a flat or negative return curve per unit of effort put in But, the people that are the best at a certain task, usually implies they got through The Dip in the curve of return per effort reply PoignardAzur 9 hours agoprevThis paper feels way too abstract, to the point it makes it hard to understand what the team actually did. For instance, the paper claims it beat GPT-4-Turbo and Gemini-Pro-1.5 on certain tasks... but it doesn't include any of the questions they asked GPT4 or Gemini, so it's hard to guess whether these results have any value at all. It's also unclear what they even trained their custom transformer to do. It has a custom tokenizer, but they don't give a list of tokens (aside from a few examples in the diagrams like \"Barrack\", \"Michelle\", \"Trump\"). They talk about in-distribution and out-of-distribution tasks, but they don't give any examples of these tasks and what they look like. This feels like accidental complexity. It wouldn't have been hard to add a few more appendices with eg a list of 20 or so in-distribution sentences they asked the model to complete and 10 out-of-distribution sentences. Instead all they include is diagrams comparing performance for different hyperparameters and stuff, but we don't even know what the models are being tested on. reply julius 6 hours agoparentFeels like science papers need a comment section. Replace peer-review with public-review. A way for authors to interact with the larger (science) community. https://www.papertalk.xyz/ was on HN Frontpage but seems to not have gained any traction (yet). Maybe arxiv should consider implementing it or integrating with some 3rd party? reply perforator 5 hours agorootparentOpenreview is nice. I guess it could integrate with arxiv to allow preprints but someone needs to pay for moderation if we are to keep a high standard of comments. reply nico 3 hours agorootparentprevThat’s a great idea. In a way, HN is that for many papers in topics that the HN community resonates with Are there other communities that also post scientific papers and comment publicly? Even if the community isn’t exclusively about science? reply devnev 5 hours agoparentprevYou got me curious so I unzipped the linked drive files. As a taster, here's a file \"gemini_retrieval_cot_3.txt\" from LLM.zip: Looking through the facts, we find the following: * Mary is older than Kristin. * Kristin is younger than Donya. Since Mary is older than someone who is younger than Donya, we can conclude that Mary is older than Donya. Final Answer: older Some sets of files contain just the answer \"older\" or \"younger\". Other sets of files are as above, a text output with reasoning leading to an older/younger/cannot decide result. Overall it looks like the knowledge graph and reasoning was all using this pattern of age comparison problems. Another result, from \"gpt4turbo_retrieval_cot_88.txt\": To determine the relative ages of Rachel and Andres, we need to find a connection or a common reference point between them through the relationships provided. Let's analyze the information: 1. Rachel is older than Maurice. (Rachel > Maurice) 2. Maurice is older than Josephine. (Maurice > Josephine) 3. Josephine is older than Doreen. (Josephine > Doreen) 4. Doreen is younger than Andres. (Andres > Doreen) From these relationships, we can establish a chain: - Rachel > Maurice > Josephine > Doreen - Andres > Doreen Since both Rachel and Andres are older than Doreen, and Rachel is higher up in the chain above Doreen compared to Andres, we can infer: - Rachel > Andres Final Answer: older EDIT: Found the problem statements. They're too big to paste on in its entirety, but roughly, from \"prompt_cot_3.txt\" used for the first answer above, the first line is \"Hi! I have some facts for you:\", then after a blank there's a single line with thousands (not exaggerated) of age facts, either in the form \"X is older than/younger than/the same age as Y.\" or \"The age of X is N.\", and finally after another blank line, \"Based on these facts, is Mary younger, older or in the same age as Donya? You can think step by step through the problem. Begin your final answer by 'Final Answer: '. Your final answer should be one of ['younger', 'older', 'same age', 'cannot decide'].\" reply PoignardAzur 1 hour agorootparentOh, I didn't notice the \"Code and Data\" link. That's helpful, thanks! reply tczMUFlmoNk 4 hours agorootparentprev> Since Mary is older than someone who is younger than Donya, we can conclude that Mary is older than Donya. Unfortunately, though, this reasoning is just wrong. If Mary is 30, Kristin is 20, and Donya is 40, then Mary is older than Kristin and Kristin is younger than Donya, but Mary is not older than Donya. reply devnev 4 hours agorootparentBoth answers are wrong. I didn't look at many files, and not all of them had the reasoning in them, but it was fairly easy to find examples of wrong answers based on the reasoning in the file. reply vessenes 7 hours agoparentprevI always like example success and failure prompts, too. They do say they generate a random knowledge graph, and then ask for one-hop results from the graph, and they do give two examples: Biden/Trump age comparison, and Barack/Michele wife age. They also say that they fit all (Gemini) or 1/3 (RAG for GPT-4 and Gemini) of all the knowledge graph in the prompt, so to be fair, I wouldn't say they're hiding the ball on the prompts here, but that the prompts are very long, even one would significantly multiply the length of the PDF. Again, I wouldn't mind some excerpts, just like you. reply PoignardAzur 6 hours agorootparent> even one would significantly multiply the length of the PDF. That bit feels like you're playing devil's advocate. Including a prompt wouldn't significantly add to the length of the PDF unless you did it in the most obtuse, malicious-compliance-ish way possible. And when the subject is \"we got X performance on GPT-4\", including (an abridged version of) the prompt isn't just a nice bonus, it's absolutely essential to judge the results. The perf data they give for GPT-4 is worthless without that information. reply tysam_and 6 hours agoprevI sort of wish that we would move on from the \"grokking\" terminology in the way that the field generally uses it (a magical kind of generalization that may-or-may-not-suddenly-happen if you train for a really long time). I generally regard grokking as a failure mode in a lot of cases -- it's oftentimes not really a good thing. It tends to indicate that the combination of your network, task, and data are poorly suited for learning {XYZ} thing. There are emergent traits which I think the network can learn in a healthy manner over training, and I think that tends to fall under the 'generalization' umbrella. Though I'd strongly prefer to call it 'transitive' rather than 'compositional' in terms of generalization, as transitive is the formal term most disciplines use for such things, compositional is a different, more general meaning entirely. Similarly, I'd replace 'parametric' and 'non-parametric' with 'internal' and 'external', etc. Sloughing through the definition salad of words (this paper alone takes up roughly half of the top Kagi hits for 'parametric memory') makes actually interpreting an argument more difficult. One reinterpretation of the problem is -- of course external memory models will have trouble generalizing to certain things like models relying on internal memory do! This is because, in part, models with internal memory will have much more 'experience' integrating the examples that they've seen, whereas, for an external-memory model like a typical RAG setup, anything is possible. But, that being said, I don't think you can necessarily isolate that to the type of memory that the model has alone, i.e., I don't think you can clearly say even in a direct comparison between the two motifs that it's the kind of memory itself (internal vs. external) that is to blame for this. I think that might end up leading down some unfruitful research paths if so. That said, one positive about this paper is the fact that they seem to have found a general circuit that forms for their task, and analyze that, I believe that has value, but (and I know I tend to be harsh on papers generally) the rest of the paper seems to be more of a distraction. Definitional salad buffets and speculation about the 'in' topics are going to be the things that make the headlines, but in order to make real progress, focusing on the fundamentals is really what's necessary here, I think. They may seem 'boring' a lot of the times, but they've certainly helped me quite a bit in my research.leave it training. I’ve often seen people tempted to stop the model training when the validation loss seems to be leveling off. In my experience networks keep training for unintuitively long time. One time I accidentally left a model training during the winter break and when I got back in January it was SOTA (“state of the art”). (This is not the oldest version, and doesn't mention the NN in question, but I believe this was about Neuraltalk, his image captioner.) reply scarmig 4 hours agorootparentprevNot about that quote in particular, but the original grokking paper from Powers et al came about because they accidentally left a training job to run too long (at least as related by gwern). reply gwern 3 hours agorootparentI'm not sure they've written that anywhere else (which is a cautionary lesson for anyone trying to understand how research happens, BTW), but as further evidence besides just 'Ethan told me so on EAI Discord [IIRC]', you can see that in the original Reddit discussion where I mention Caballero's poster conversation, the lead author comments several times and doesn't contradict that anecdote: https://www.reddit.com/r/mlscaling/comments/n78584/grokking_... reply imtringued 2 hours agoprevOne of the biggest bottlenecks of multi layer transformers is that reasoning can only happen in the hidden layers. Past the final layer, the model must generate a token that conforms to the training process. This token can then be fed back into the transformer from the beginning, but since it necessarily must be in natural language, it limits the type of reasoning the model can perform to the \"thoughts\" it has seen in the dataset and is therefore allowed to express. If you could figure out how to have the first layer take both the KV of the first layer and the KV of the final layers in the attention mechanism into account, the model would become capable of infinite length reasoning. reply campers 14 hours agoprevThis is the interesting result where their GPT-2 sized transformer blows away GPT4 and Gemini 1.5 in connecting together facts The difficulty of such a task is two-fold. First, the search space is large. For example, on average, each query entity connects with more than 50 facts, and each bridge entity in the ground truth proof connects with more than 900 facts. Second, there are no surface form clues to exploit and bias the search towards the ground truth proof, unlike most conventional QA benchmarks where the proof steps are transparent from the query. To test LLMs based on non-parametric memory, we translate the facts into natural language by simple templates (Appendix F). Facts/queries for each attribute are grouped/tested separately. We test both the vanilla setup where all facts (28.2K on average) are loaded into the LLM context, and the retrieval-augmented setup (5.4K facts retrieved on average) where the two-hop neighborhoods of the two query entities are retrieved, which includes enough facts to deduce the answer. We also try both standard prompting where the model answers directly, and chain-of-thought (CoT) prompting where the model is prompted to verbalize the reasoning. We test GPT-4-Turbo and Gemini-Pro-1.5, where for GPT-4-Turbo we only test the retrieval-augmented setup due to context length limit. Table 1:Results on the complex reasoning task. Direct/CoT: predict the answer directly/verbalize the reasoning steps. “+R”: retrieval augmentation. GPT-4-Turbo Gemini-Pro-1.5 Grokked Transformer Direct+R CoT+R Direct CoT Direct+R. CoT+R Accuracy (%) 33.3 31.3 28.7 11.3 37.3 12.0 99.3 reply nickpsecurity 3 hours agoparent“ Second, there are no surface form clues to exploit and bias the search towards the ground truth proof” Foundational models are usually trained with a lot of stuff before doing these kinds of tests. Can we know the above statement is true? That something (a) wasn’t in the training data and (b) didn’t have surface-level clues a ML algorithm could spot which the authors didn’t? I felt like asking the latter because both GA’s and NN’s have found simple patterns in problems that humans missed for a long time. They used those patterns to heuristically solve those problems. It might be hard to design tests that eliminate a factor humans can’t see. reply syntaxfree 16 hours agoprev [–] > delve reply bzalasky 16 hours agoparentHad the exact same thought after reading the abstract… FWIW, delve only appears in the abstract. Having not read the rest of the paper yet, I might give the authors the benefit of the doubt that they used an LLM to summarize their findings for the abstract, but didn't abuse an LLM in writing the entire paper. reply bee_rider 15 hours agorootparentPutting aside the possibility that they just happened to use the word “delve,” IMO we still have to figure out the convention for this sort of thing. I don’t particularly value the time scientists spend writing the prose around their ideas, the ideas themselves are the valuable part. One possibility, for example, could be journals allow AI written submissions but also require and distribute the prompts. Then we could just read the prompts and be spared stuff like the passive voice dance. They probably abused a compiler to generate their program instead of writing it in assembly. reply Sysreq2 14 hours agorootparentSoon AI will turn a chickenscrath of notes into a wonderful email. And then turn it back automatically for the end reader. We put to much emphasis on the look rather than the substance. People are afraid to send out an email with 2 words: Meeting Friday and instead pad it out with pleasantry and detail, context and importance, but none of that really matters. reply ericjmorey 12 hours agorootparent'Meeting Friday\" is not enough information to have me attend the meeting. So I'm not sure what this analogy was supposed to illustrate. reply bee_rider 12 hours agorootparentDepends on who it is from I guess. reply idiotsecant 5 hours agorootparentIt's not enough information no matter who it is. If it's someone with enough political, social, or institutional capital you might overlook the annoyance but it still only tells you when. Doesnt say the what the when or the who, all of which have consequences for what I need to do to be prepared. reply manmal 15 hours agorootparentprevA compiler yields deterministic results though. reply bee_rider 2 hours agorootparentRegardless of the nitty-gritty “determinism” questions; why’s this matter? reply kolinko 9 hours agorootparentprevllms are also deterministic reply manmal 6 hours agorootparentNo, in most cases the same input will yield a different output. reply Zambyte 4 hours agorootparentNo, LLMs are deterministic. What you are describing is a randomized seed, which is another input to the LLM. Some interfaces expose this input, and some do not. reply Drakim 4 hours agorootparentprevOnly because most tools provide a randomize seed alongside the input, but you don't have to do that. reply coldtea 6 hours agorootparentprevin a deterministic way based on seeds reply squigz 14 hours agoparentprev? reply zamfi 7 hours agorootparentCommon marker word for LLM-generated text. reply tmalsburg2 3 hours agorootparentA single word is insufficient evidence to conclude that an LLM was used. \"Delve\" may be low frequency in naturalistic text but there are many words in an article and the chance that some of them will be low-frequency is high. I also checked in my bibliography and found that \"delve\" is actually not super rare in academic papers including those written before LLMs. reply ekianjo 6 hours agorootparentprevLLM researchers going full circle reply qazxcvbnm 13 hours agoparentprev [–] With a quick skim, the paper delivers on its promise. It's not a particularly long or difficult paper to follow. > Causal tracing. The transformer could be viewed as a causal graph that propagates information from the input to the output through a grid of intermediate states, which allows for a variety of causal analyses on its internal computation > [...] There are in total three steps: > 1. The normal run records the model’s hidden state activations on a regular input [...] > 2. In the perturbed run, a slightly perturbed input is fed to the model which changes the prediction, where again the hidden state activations are recorded. [...] Specifically, for the hidden state of interest, we replace the input token at the same position as the state to be a random alternative of the same type (e.g., r1 → r′1) that leads to a different target prediction (e.g., t → t′). > 3. Intervention. During the normal run, we intervene the state of interest by replacing its activation with its activation in the perturbed run. We then run the remaining computations and measure if the target state (top-1 token through logit lens) is altered. The ratio of such alterations (between 0 and 1) quantitatively characterizes the causal strength between the state of interest and the target. > The generalizing circuit. [...] The discovered generalizing circuit (i.e., the causal computational pathways after grokking) is illustrated in Figure 4(a). Specifically, we locate a highly interpretable causal graph consisting of states in layer 0, 5, and 8, [...]. Layer 5 splits the circuit into lower and upper layers, where 1) the lower layers retrieve the first-hop fact (h, r1, b) from the input h, r1, store the bridge entity b in S[5, r1], and “delay” the processing of r2 to S[5, r2]; 2) the upper layers retrieve the second-hop fact (b, r2, t) from S[5, r1] and S[5, r2], and store the tail t to the output state S[8, r2]. > What happens during grokking? To understand the underlying mechanism behind grokking, we track the strengths of causal connections and results from logit lens across different model checkpoints during grokking (the “start” of grokking is the point when training performance saturates). We observe two notable amplifications (within the identified graph) that happen during grokking. The first is the causal connection between S[5, r1] and the final prediction t, which is very weak before grokking and grows significantly during grokking. The second is the r2 component of S[5, r2] via logit lens, for which we plot its mean reciprocal rank (MRR). Additionally, we find that the state S[5, r1] has a large component of the bridge entity b throughout grokking. These observations strongly suggest that the model is gradually forming the second hop in the upper layers (5-8) during grokking. This also indicates that, before grokking, the model is very likely mostly memorizing the examples in train_inferred by directly associating (h, r1, r2) with t, without going through the first hop > Why does grokking happen? These observations suggest a natural explanation of why grokking happens through the lens of circuit efficiency. Specifically, as illustrated above, there exist both a memorizing circuit Cmem and a generalizing circuit Cgen that can fit the training data [...] reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper \"Grokked Transformers are Implicit Reasoners\" explores whether transformers can implicitly reason over parametric knowledge, focusing on composition and comparison reasoning types.",
      "Findings indicate that transformers can learn implicit reasoning through extensive training beyond overfitting, known as grokking, but their generalization ability varies: they struggle with composition but succeed with comparison in out-of-distribution examples.",
      "The study suggests improvements in data and training setups, potential architectural enhancements like cross-layer knowledge sharing, and shows that fully grokked transformers outperform models like GPT-4-Turbo and Gemini-1.5-Pro in complex reasoning tasks."
    ],
    "commentSummary": [
      "The discussion emphasizes the importance of AI models that generalize rather than memorize, critiquing the current trend of favoring memorization due to economic incentives.",
      "The paper suggests techniques like stronger weight decay to penalize memorization and highlights the need for detailed prompts in testing complex reasoning in language models.",
      "It also introduces causal tracing as a method to analyze model computations, illustrating how it helps transition models from memorization to generalization."
    ],
    "points": 199,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1716847112
  },
  {
    "id": 40494793,
    "title": "Notepad Tab: Secure Note-Taking Tool Saves Notes in Browser's Address Bar",
    "originLink": "https://notepadtab.com/",
    "originBody": "Notepad TabA simple, secure and private note taking tool {\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"description\":\"Write down anything and have it automatically persisted in the address bar and in the browser’s history. Easily back up or share any note by simply copy and pasting the URL.\",\"headline\":\"Notepad Tab\",\"image\":\"https://notepadtab.com/assets/favicon/android-chrome-512x512.png\",\"name\":\"Notepad Tab\",\"url\":\"https://notepadtab.com/\"}Notepad Tabfunction main() { if (document.readyState !== 'loading') { document.addEventListener('DOMContentLoaded', run); } else { run(); } } function run() { const textarea = document.getElementById('textarea'); loadValue(textarea); addValueListener(textarea); addHashListener(textarea); } function loadValue(textarea) { const value = retrieveValue(); const oldValue = textarea.value; textarea.value = value; if (oldValue === '') { textarea.selectionStart = value.length; } } function addValueListener(textarea) { textarea.addEventListener('input', (event) => { storeValue(event.target.value); }, false); } function addHashListener(textarea) { window.addEventListener('hashchange', () => { loadValue(textarea); }); } function retrieveValue() { const hash = window.location.hash; if (hash === '') { return ''; } return deserialize(hash.substring(1)); } function storeValue(value) { window.location.hash = '#' + serialize(value); } function serialize(value) { if (value === '') { return ''; } const data = new TextEncoder().encode(value); const compressed = pako.deflate(data, { level: 9 }); return Base64.fromUint8Array(compressed, true); } function deserialize(value) { const data = Base64.toUint8Array(value); return pako.inflate(data, { to: 'string' }); } main();",
    "commentLink": "https://news.ycombinator.com/item?id=40494793",
    "commentBody": "Notepad Tab (notepadtab.com)176 points by gamekingro 21 hours agohidepastfavorite134 comments TimTheTinker 19 hours agoThis got me thinking... someone should build a local service that hosts a library of static web apps for use cases like this. Essentially, apps that are something between bookmarklets and Electron apps. Might even make Electron unnecessary for a bunch of use cases. It would be neat to visit /app/notepad, /app/kanban, /app/todo, etc. at a localhost port, with an index of apps at /. There could even be a /get-apps page that connects to an npm-like library of single-HTML-file apps available for download from a github.io static site backed by a public repo that accepts pull requests. I'd love to see all the VC-funded glorified desktop utilities on the web become unnecessary. reply precompute 11 hours agoparentYou're going down the todo-app / \"second brain\" rabbithole, which generates at least a few \"start-ups\" every month. reply rtpg 10 hours agorootparentThe deeper version of this is finding out about smalltalk and realizing that you could make a very configurable system that you could edit on the fly. Salesforce is basically a smalltalk VM and dominates thanks to that reply dmd 7 hours agorootparentIn what way is Salesforce a Smalltalk VM? And if it is, why does it dominate \"thanks to that\"? reply alabhyajindal 5 hours agoparentprevI love this idea and made a little MVP for it: https://github.com/alabhyajindal/local-webapps There's a demo video on the Readme with setup instructions. reply somat 8 hours agoparentprevOn a related note I wish there were an easy way to deelectronfy electron apps. I don't know, perhaps some sort of patch you could apply to get it to act like a normal web app. and a local server to provide the file access. Mainly I am bitter because nobody has ported electron to openbsd yet, cross platform my ass. reply isr 6 hours agoparentprevTiddlywiki - the poster child for a complex app delivered as a single LOCAL web page. reply prmoustache 4 hours agoparentprevWhy would I use that instead of OS native apps? I am failing to see the usecase / appeal. reply TimTheTinker 15 hours agoparentprevThinking about this more - this might be a perfect use case for Redbean. Backend mini-app user data could be stored in SQLite. The server could store the actual apps' static files in its internal zip archive. All of that could easily be managed with a sprinkling of Lua. The mini-apps could store data in localStorage, but that would expose the data to other mini-apps. Maybe build an easy API that functions nearly equivalently against a Lua endpoint. reply kreelman 14 hours agorootparentThis might work.... ...But I've not worked out how to get RedBean serving up files outside of the zip... I'll get there. P'raps LUA could manage the security for different static apps.... as you say. reply TimTheTinker 8 hours agorootparentI'm not saying Redbean should serve files outside of the zip. The mini-apps would all be located inside the zip. When downloading a new mini-app, the Lua script would add it to the zip archive itself -- it can do this as far as I can tell. reply dmazzoni 14 hours agoparentprevDo you actually need a local server? Why not have a real url, but have the site use service workers to persist offline? The only issue I see is if you clear your browser’s cache while offline reply oefrha 14 hours agorootparentThe problem with these supposedly local, private web apps on other people’s domains is that they can start stealing your data any time the page is refreshed. The author can turn malicious (or was malicious to begin with), they can sell the app or let the domain expire, or they can be hacked. The only way to prevent that is to firewall/null route that domain entirely after initial load and audit, and then you can’t ever refresh the page. The web platform simply doesn’t allow you to verify the checksum of an app and reject all further requests. reply oefrha 9 hours agorootparentA late edit for a brain fart: “and then you can’t ever refresh the page” should be removed since it can be served from service worker cache. You can’t refresh the page while allowing a connection to the server. reply doix 11 hours agorootparentprev> The only way to prevent that is to firewall/null route that domain entirely after initial load and audit, and then you can’t ever refresh the page. I mean, at that point, why not just grab the files are host them yourself? reply oefrha 10 hours agorootparentWell that’s exactly the point. Having a directory of these static files you can self host with eyeballs on updates would be great. reply yonatan8070 14 hours agorootparentprevI'm not a web dev so I might be missing something. Theoretically you could just open .html files directly in the browser without any networking being involved, no? reply MzHN 13 hours agorootparentIn the past, yes, and you still could if you use old enough features, but nowadays browsers heavily restrict sites opened as just .html file. For example you could not use modules at all if they're in separate files. That is at least until we get Isolated Web Apps and what ever the other proposals related to it alö called. reply echoangle 12 hours agorootparentIs blocking access to local files a new thing? Considering the threat (you download a html file, open it in your browser and it tries accessing random files and exfiltrates them using JS), I would be surprised if this has worked in the past 10 years reply kwhitefoot 2 hours agorootparentYou should be able to whitelist such files. After all the browser is supposed to be your agent, not your nanny. reply hnthrowaway0328 19 hours agoparentprevHow about a native app collection called \"Sidekick\", which has a Borland bent? reply addandsubtract 10 hours agoparentprev> Essentially, apps that are something between bookmarklets and Electron apps. You're talking about progressive web apps. MacOS / iOS / Android have them, and I started using them more recently. Essentially, they're just webviews wrapped in a native app. reply alternatex 8 hours agorootparentPWAs have nothing to do with web views or native apps. They're just a collection of technologies (service worker, caching API, platform APIs, home screen installation, etc) that makes it possible to have offline available web apps that have a shortcut on your desktop/home screen. There is no web view or native app (outside of your default) browser involved. reply arrakeen 19 hours agoparentprevso... apache? reply TimTheTinker 18 hours agorootparentMaybe a small express service running on nodejs or bun. reply PowerfulWizard 20 hours agoprevHere's what I use (as a bookmark): data:text/html, No save function obviously but this lets me open a new tab and dump some text. reply ASalazarMX 17 hours agoparentSeeing the replies to your comment, I have to ask: Notepad++ persists your unsaved notes, has dark mode and themes, is fast and lightweight... why insist on forcing text-editor-like behavior on the browser? It feels like a solution in need of a problem. reply scubbo 9 hours agorootparent(For myself) because 99% of my time is spent in an IDE or a browser, and there's less mental overhead for me to open a new tab and start typing than for me to open a new app and do so. reply qalmakka 7 hours agorootparentThe IDE is literally a text editor. Why not hit file -> new file and write stuff in there? reply cess11 6 hours agorootparentFor me it's the risk of littering in a project repo. So I use Zim wiki instead: https://zim-wiki.org/ reply jwells89 15 hours agorootparentprevSublime Text fits the “nameless notes” niche for me for similar reasons. It’s super speedy, has plenty of customizability, and has rock solid auto save+restore for unsaved text. reply eviks 13 hours agorootparentCan you restore a closed tab of unsaved text? reply voltaireodactyl 12 hours agorootparentNot out of the box I believe, but I use a package from their directory to do just that. reply nunodonato 8 hours agorootparentprevRight? I use the gnome default editor for this. Also persists unsaved notes, its always available and has some few basic features that sometimes come in handy (regex match, etc) reply Dalewyn 14 hours agorootparentprev>why insist on forcing text-editor-like behavior on the browser? It feels like a solution in need of a problem. Because the browser is the operating system. I might be only half joking. reply 01HNNWZ0MV43FF 14 hours agorootparentI hear people say it a lot and I know what they mean but I just can't agree... to me an OS runs on hardware. (or virtualized hardware) Browsers run on an OS. If you have \"boot to browser\" the OS is still the kernel. Browsers are userspace. It's like that saying \"The difference between a boat and a ship is that a ship can carry a boat, but a boat can't carry a ship.\" And I know there is jslinux but at that point we're in a Turing tarpit where you can say that the Lua VM or wasm is \"an OS\" and the term is just a five-dollar word for \"abstraction layer\". Is a function call an OS? Come on. reply justsomehnguy 15 hours agorootparentprev> persists your unsaved notes Except when you brainfart on the OS shutdown and choose the wrong answer. But yes, I even do the culling every couple of months. reply oefrha 14 hours agorootparentYou should pick a text editor that doesn’t throw up a dialog when quitting then. I use CotEditor on macOS specifically for random notes, everything’s unsaved and some notes have survived dozens of reboots over a number of years. reply justsomehnguy 12 hours agorootparent> doesn’t throw up a dialog when quitting then It doesn't! At least when I Alt+F4 it. > everything’s unsaved and some notes have survived dozens of reboots Yep, this is exactly how I use it. But somehow that one time (note: it was on the shutdown) something went terribly wrong. reply ASalazarMX 1 hour agorootparentI wonder if your notes were borked out of session.xml, but the files were still available at AppData\\Roaming\\Notepad++\\backup. I've changed machines where the user profile was in a different location, copied my AppData, and replacing the old location in Notepad++'s session.xml was enough to restore my unsaved notes. reply justsomehnguy 22 minutes agorootparentNah. Of course I tried everything (except looking in the shadow copy? Don't remember), but in the essence the shutdown triggered Save All workflow (somehow) and I responded with 'No'. *weep* reply robobro 15 hours agorootparentprevyou mean emacs/? reply wesamco 20 hours agoparentprevNice! I bookmarked it and I'm gonna start using it, thank you. For a quick and dirty save, you can press Ctrl+P to open the print window/dialog and select \"Save as PDF\", or you can press Ctrl+S and save as a single HTML file. Edit: to make the text cursor focus automatically when the page loads, you can add the autofocus attribute to the body tag. reply smusamashah 20 hours agorootparentFollowing might work to save as well. data:text/html, reply wesamco 18 hours agorootparentWhile you can't save to localStorage as my sibling commenters have shown, greyface- down below in the thread posted a version that saves to the hash fragment of the URI. Saving to the (Data) URI has a benefit over localStorage of allowing you to save by bookmarking, which also enables you to save many notes, not just one. I code-golfed greyfaces code and made the text cursor autofocus on page load: data:text/html,# reply apatheticonion 19 hours agorootparentprevDug into this for a bit, sadly: > Webstorage is tied to an origin. 'data:' URLs have unique origins in Blink (that is, they match no other origins, not even themselves). even if we decided that 'data:' URLs should be able to access localStorage, the data wouldn't be available next time you visited the URL, as the origins wouldn't match. reply mariocesar 19 hours agorootparentprevIt will need a hostname or a page at least. Failed to read the 'localStorage' property from 'Window': Storage is disabled inside 'data:' URLs. reply FrostKiwi 10 hours agoparentprevAlong the same lines, I created a simple HTML site to interface with Japanese Translation tools: https://blog.frost.kiwi/just-a-text-box/ reply numpad0 19 hours agoparentprevdark mode: data:text/html, reply geor9e 20 hours agoprevI made a very similar notepad tab. Only me and my girlfriend use it (since I don't market it or anything) but we've been using it constantly every day for about a year. I'm surprised it's not a default function of browsers to be honest - new tabs are such wasted space. Basically, every tab is the same document, it saves to the browser's cache locally so you don't lose anything when you close the browser. https://github.com/gth001/George-s-Notes-Tab-Extension reply ASalazarMX 17 hours agoparentI play Simon's Tatham's puzzles from time to time to de-stress. I'm surprised it's not a default function of browsers to be honest - new tabs are such wasted space. How hard would it be to embed it in the new tab so everyone can enjoy some mental recreation? reply nabaraz 20 hours agoprevNeat concept but pollutes the browser history and maximum length of 2048 characters defeats the purpose. Can we use svg instead? reply benrutter 11 hours agoprevThis is really cool- I'm excited to dig through the code for this one! A bunch of the comments talk about using in earnest- at the risk of sounding out of touch, if you want private, offline available notes, what's wrong with text files on a file system? (I think my question looks sarcastic, but I'm genuinely interested!) reply alabhyajindal 8 hours agoparentI find that I have a browser open >90% of the time when I'm at the computer. And it's easier to open a new tab instead of opening a new application. Also I prefer web apps because they are highly customisable. If I don't like something I can modify the source easily. reply jayski 21 hours agoprevThis is basically encoding(base64?) theand putting it in the URL. Its a neat idea, but I think theres a limit to how long URLs can be. reply JTyQZSnP3cQGa8B 20 hours agoparentIIRC in theory it’s 2048 characters, but in practice it’s 2000 (at least in Chrome a few years ago when I toyed with Google Maps for food). reply hobs 20 hours agorootparentIt's actually much longer, I was implementing a javascript bookmarklet and the old published limitations are not respected by much anymore, so you can shove a LOT of data in there. reply erhaetherth 15 hours agoparentprevShould have used base65536. reply sanchez_0_lam 21 hours agoparentprevOuch, that's bad :D Won't store too long texts. reply baliex 20 hours agorootparentIt is at least compressed to make the most of the limit reply erhaetherth 14 hours agorootparentbase64 is the opposite of compressed. Does it actually apply compression before base64-encoding? Doesn't really look like it by watching the URL. reply bozey07 12 hours agorootparentWhat gave you that impression? I tried spamming \"a\" and the URL indeed did not get longer. Reassuringly: function serialize(value) { if (value === '') { return ''; } const data = new TextEncoder().encode(value); const compressed = pako.deflate(data, { level: 9 }); return Base64.fromUint8Array(compressed, true); } reply valbaca 1 hour agorootparentit does get longer. rather than typing in aaaa copy and paste it and copy and paste that to grow exponentially reply skipnup 12 hours agorootparentprevIt’s actually compressed. It uses https://github.com/nodeca/pako and then applies base64. Try entering hundreds of \"a\" and you’ll see that the base64 doesn’t really get much longer. reply zX41ZdbW 20 hours agoprevI did almost the same project, but better, two years ago: https://pastila.nl/ https://github.com/ClickHouse/pastila reply zX41ZdbW 20 hours agoparentIt's better because it minimizes the UI. You get the same experience as in the text editor, such as Notepad or Kate, with no fluff. reply gurjeet 21 hours agoprevNeat idea. But it pollutes the browser history with every keystroke. reply geor9e 20 hours agoparentIt doesn't for me. Looks like they're just changing the html anchor (# …) with a location.replace which isn't a redirect so shouldn't spam history. Edit: Nevermind. It seems only Edge behaves this way. reply ldjb 20 hours agorootparentHappens for me in Firefox. I press 'Back' and it untypes the last character. reply geor9e 19 hours agorootparentOh, you're right, it does spam history in Firefox. Safari and Chrome too. I'm using Edge and it doesn't - theres just one entry in history. reply nunodonato 8 hours agorootparentprevit's not a bug, its a feature! Unlimited undos reply recursive 19 hours agorootparentprevAnd yet it does. Firefox on Windows 11. reply legostormtroopr 19 hours agoparentprev\"pollutes the browser history\" or \"provides infinite undo and redo\"? reply alabhyajindal 15 hours agorootparentYou could do that without polluting the browser history reply pwillia7 6 hours agoprevShameless plug for my bookmarklet that solves the same problem all locally -- This is neat but if I were to use this, I would just use a more full featured site. https://github.com/pwillia7/Text_Bookmarklet reply cal85 7 hours agoprevI love the idea after reading the example use case. Storing the text in a URL fragment is perfect for that use case. I can see myself using it. But it really needs to use `replaceState` instead of adding to the history stack on every character. reply scientific_ass 6 hours agoprevI am from marketing, so maybe I don't really understand the technical value here but how is it different than me opening a new Google doc? I currently have a single doc file that I use like notes, as soon as I type \"note\" in my address bar, my browser automatically fills in the rest of the URL for the file (since I open is frequently). The Doc file allows me to paste images, add links, or anything else essential for note taking. I don't need to save, or find the URL from browser history. Simply type and close. For any new file, I can type Doc.new. It will work the same way. Please explain how is it different? reply phrotoma 6 hours agoparentThe google docs experience is so spririt crushingly bad on an ipad (my preferred casual browsing / reading device) that I immediately bookmarked this site for those moments when I need to write and don't want to fetch my laptop. reply rbits 6 hours agoparentprevGoogle Docs stores things on Google's servers. This site doesn't send anything to anyone, it stays on your device only. reply codazoda 20 hours agoprevI created Ponder, which does something similar. Mine was inspired by the Alpha Smart, so it has 10 “files” and it saves to local storage, so it might not be loss proof, but I use it as a scratch pad and it works quite well. https://github.com/codazoda/ponder reply k310 15 hours agoprevTiddly Wiki? I stumbled upon tiddly desktop, an app that runs tiddly wikis without a browser. reply fmoronzirfas 15 hours agoprevI've built something similar. I use it together with Firefox container tabs for quick notes. It saves to local storage. https://notes.inpyjamas.dev/ Source https://github.com/ff6347/notepad reply cvak 3 hours agoprevVery cool, you are replacing my longest time bookmark bar bookmark of data:text/html,reply aayushdutt 7 hours agoprevhttps://github.com/aayushdutt/notepad This reminds me of a trivial browser notepad I hacked together a while back. Have been using this and has been indispensable - quickly open and jot down some notes. Has syntax highlighting and saves the notes to the browser db. reply ngc6677 13 hours agoprev- https://itty.bitty.site - https://space-element.pages.dev (also can be archived plainly from client side, such as with https://archive.is/uXWBQ and https://archive.is/goog.space) reply behnamoh 21 hours agoprevFrom the website: \"Doesn't use analytics = respects your privacy\" Meanwhile, Brave stopped this tracker: https://static.cloudflareinsights.com/beacon.min.js/vef91dfe02fce4ee0ad053f6de4f175db1715022073587 reply derefr 20 hours agoparentWhile you might assume that Cloudflare \"insights\" is an advertising/analytics system, this is actually part of Cloudflare's anti-DDoS infrastructure. This \"beacon\" gets injected at random on Cloudflare-served HTML pages, to track you throughout your use of all Cloudflare-proxied sites, as an alternative to an evercookie in building a long-term reputation profile of \"human browsing\" for your browser. This reputation profile is then used as part of the heuristic behind CloudFlare Turnstile's \"Are you human?\" checkbox. This is why browsers that have NoScript enabled by default for all sites (e.g. Tor Browser), cause Cloudflare-proxied sites to throw endless security interstitials and never let you through, even when you disable NoScript for the protected website. Without reputation-profile data gathered from other sites, Cloudflare just sees a fresh browser profile making its first connection ever to some obscure site that nobody would ever actually visit as the first thing they do on a new computer. And so it thinks your browser is a (not-very-clever) bot. I don't think it's possible for a site owner to opt out of this reputation-profile data gathering, while still relying on Cloudflare's DDoS protection. However, I also don't believe that the data Cloudflare gathers via this route is sold to third parties. (Someone please correct me if that's wrong.) reply madeofpalk 20 hours agorootparentBuilds a persistent profile of you across the web… this is directly at odds with \"Doesn't use analytics = respects your privacy\" I’m sure the author isn’t aware of it and it’s just an oversight, but still. Why does a single static html file even need a CDN? reply arnorhs 18 hours agorootparent\"need\", probably not. But anything that can be served from a cdn is better off if it fits. From a latency and bandwidth efficiency perspective reply pard68 19 hours agorootparentprevProbably not a CDN, persay, but is using Cloudflare Pages as a host. Hosts static HTML for free on CF's CDNs. I use it for all my sites. reply sph 4 hours agorootparentprevWhy does this static site need DDOS protection is the important question. Hint: it bloody doesn't. reply derefr 3 hours agorootparentIt doesn't matter that this is a static site; it matters what it's hosted on. If this static site is sitting on a CDN or Github Pages or something, then sure, there's no need to mask its IP address. But if this static site is hosted on a cheap VPS or on a home PC with a residential Internet connection — or generally, anything with a monthly bandwidth usage cap — then any teenager who learns its true IP address (and then checks out that IP address's provenance with a whois(1)) could decide to pay $5 to throw a botnet at it for an hour — just because they know they can take it down by spending enough of its bandwidth, and want to try it, to be able to brag to their friends that they took something down. (Yes, teenagers today do that. The most DDoS-ed things in the world today are Minecraft servers — because teens like messing with other teens.) --- Also, half of what makes Cloudflare useful for \"DDoS protection\" isn't actually its \"bot fight\" security system, but rather its caching layer combined with its lack of egress costs (at least until you get forced into their Enterprise billing.) If you are hosting your content on e.g. a public S3 bucket, where you're billed for egress bandwidth, but where S3 also sends sensible long-expiry Cache-Control headers; and you put Cloudflare in front of that S3 bucket (even just Cloudflare's free-tier offering!); then suddenly your S3 bucket will only be serving requests for each resource a few times a day, rather than for every single request. 99.999% of the traffic to your bucket will be a cache hit at the Cloudflare level, and so will be only a conversation between Cloudflare and the customer, not between Cloudflare and S3. So, even in the face of a DDoS, your billing won't explode. reply JonathonW 13 hours agoparentprevAnd the data that's collected here includes the full page URL-- which, in this case, includes the fragment and therefore whatever data is being \"stored\", at the time of capture. This is probably beyond the author's control, but they shouldn't host it somewhere that can inject scripts outside their control (like Cloudflare) and then claim \"privacy\". (The Cloudflare script makes a request to `/cdn-cgi/rum`, with the full page URL in its JSON payload at `timingsV2.name`.) reply Dalewyn 21 hours agoparentprevOn a similar token: >Doesn't use a server = no downtimes Except there is a server, whatever and wherever it is behind notepadtab.com. reply kapep 9 hours agorootparentAlso > - Doesn't need cookies = immune to data loss by accident How is this immune if you have to remember to save it manually? That seems much worse than relying on cookies. Sure you can maybe restore it from the browser history, but if cookies are not considered reliable, then the history even more so. It's easier to delete history than cookies. reply jimbobthrowawy 20 hours agorootparentprevIs there anything in the HTML spec that tells browsers to always show a cached version a page if it can't be loaded the next time you try to access it? I think PWAs might have something like that, but haven't tested it in a normal browser or tried building one. reply etaioinshrdlu 20 hours agorootparentMaybe the entire page could be a self-updating data-url? edit: I tried this and common browser security no longer allows this type of thing. 10 years ago it may have worked. reply greyface- 20 hours agorootparentHere: data:text/html,#SGVsbG8sIHdvcmxkIQ== reply cvak 3 hours agorootparentUnfortunately this doesn't work on FF due to security Uncaught NS_ERROR_FAILURE reply mro_name 9 hours agorootparentprevWonderful! How can I silence the Firefox security error messages? reply TimTheTinker 20 hours agorootparentprevBeautiful. Beat me to it. Much better than a relying on an HTTP response from someone else's computer. reply dntbrsnbl 20 hours agorootparentprevYeah, you can do something like that with a ServiceWorker - it does require some JavaScript though. https://developer.mozilla.org/en-US/docs/Web/Progressive_web... reply Dalewyn 20 hours agorootparentprevHTML dictates how webpages should be structured and then rendered. You're probably asking about HTTPS, in which case: No. The first rule about HTTPS is no caching, because you want to validate that what you see is from the server and you can't prove that with a cache. reply fvilers 13 hours agoprevI wrote TextWrite some years ago with the exact same goal (and to learn to use Tailwind CSS): https://textwrite.vercel.app/ reply gamekingro 12 hours agoprevYou can find the project on github as well: https://github.com/revolter/notepadtab.com reply chongli 20 hours agoprevCool concept but I would never use it. It's an archetypal example of the web being the champion of \"worse is better.\" I am happy with just taking notes in a local text file using a text editor. This is not a complicated problem to solve. Save notes by copying and pasting the URL? Why not just copy and paste the text itself? I don't need formatting in my notes. reply erhaetherth 14 hours agoparentI used to use VSCode for this but then they went and effed it up by adding so much spam to the app. If I happened to have an SSH open connection previously, it just reconnects to that and makes me wait, and then I have to press buttons to get back to a blank local workspace. Or sometimes it like splash up a \"We added a bunch of features you didn't need\" page. Ugh. reply timsneath 21 hours agoprevNice idea. But looks like this is unreliable, with some indeterminate cut-off point after which it stops working. I created 100 paragraphs of Lorem ipsum which I pasted into the textbox. It didn't show any error, but when I pasted the URL into a different window, the textbox just shows 'undefined'. reply erhaetherth 14 hours agoparentprobably the URL got cut off and the base64 got corrupt. reply rlv-dan 13 hours agorootparentYou could read back the url after each write to check the validity. If not valid, assume the limit has been reached and inform the user. reply whatsakandr 19 hours agoprevWhats the use case for this? What computing platform doesn't have a text editor, but does have a webbrowser? Or are we just conditioned to use a web browser for everything? reply asp_hornet 14 hours agoparentMaybe they misunderstood the site and thought hacker news was related to the hacker ethic of building stuff. reply revolter 12 hours agoparentprevMy exact use case was this: I created a tab group in Safari for working on a website, and wanted to jot down some to do items related to that project, and a tab in that tab group made the most sense. I didn't want to write them down in a text editor, because they wouldn't have been bound to my context anymore. Hope my explanation makes sense. Fun fact: I saw people taking notes in translate.google.com for the same reasons that I created notepadtab.com. reply vtrenc 7 hours agoprevStore notes in url and used cloudflare. reply rpastuszak 20 hours agoprevHehe, that's one of the reasons I made write.sonnet.io a few years back. It's a never-ending strip of paper/stream of consciousness. reply seanvelasco 11 hours agoprevat first, i thought, \"if this doesn't use a server, then how...\" - then i started typing and watched the url change. amazing! reply tomr75 19 hours agoprevjust use apple notes or equivalent? reply al_borland 14 hours agoparentOr Text Edit, Notepad, Gedit, vi… the most basic text editor the OS has. This is a solution for a problem that was solved 40 years ago. reply sureIy 7 hours agoprevOr, you know, use an actual notepad that lives on your computer. I don’t understand why people find these tools exciting/useful. I never have to worry about losing data with TextEdit because it autosaves, natively, and is accessible outside the OS that the browser has become. reply mro_name 10 hours agoprevthis could be a static website on a local webserver, right? reply sanchez_0_lam 21 hours agoprevAny thoughts how this works under the hood? Like @gurjeet said, every keystroke is a new url. But then how all this is stored? Will it scale? :D reply skulk 21 hours agoparentIt's stored in your history stack. I spent 15 seconds pressing the back button slowly becoming more and more horrified as I watched my characters disappear one by one. You can see how it works for yourself by opening your browser devtools, opening the JS console, and typing window.location.hash = \"test\" You should see a \"#test\" pop up at the end of your URL. Pressing back will not change the page, but will make that go away. reply ukuina 19 hours agoprevWhy does this even need a server? reply ukuina 19 hours agoparentHere's a version that doesn't need a server. Just save as an HTML file and open it in a browser (either localhost or on a fileserver somewhere).Note Appbody { font-family: Arial, sans-serif; margin: 0; padding: 20px; } textarea { width: 100%; height: 80vh; padding: 10px; font-size: 16px; border: 1px solid #ccc; border-radius: 5px; box-sizing: border-box; } document.addEventListener('DOMContentLoaded', function() { const textarea = document.getElementById('textarea'); const loadValue = () => { const hash = window.location.hash; try { const value = hash ? decodeURIComponent(atob(hash.substring(1))) : ''; textarea.value = value; textarea.selectionStart = value.length; } catch (e) { console.error('Error decoding hash:', e); textarea.value = ''; } }; const storeValue = (value) => window.location.hash = '#' + btoa(encodeURIComponent(value)); textarea.addEventListener('input', () => storeValue(textarea.value), false); window.addEventListener('hashchange', loadValue); loadValue(); }); reply msylvest 7 hours agorootparentNeat! Couldn't but ask myself why the OP should need Ruby... reply makach 6 hours agorootparentprevThis does not scale well. reply CleanCoder 13 hours agoparentprevIt doesn't. Registering a dedicated .com domain is an overkill for this as well. reply sergiotapia 20 hours agoprevit pollutes the back button reply djaouen 18 hours agoprevshould have called it *scratch* imo reply calrain 7 hours agoprev [–] OK, red alert on a cyber security risk with this site. Your URL is logged, therefore everything you type is readable by anyone farming your URLs. What am I missing here, why does this seem like such a security risk. reply helsinkiandrew 7 hours agoparentNothing after the # will be sent over HTTPs but doesn't use of URL means its lost if you loose the URL - wouldn't local web storage be more 'secure' and also possibly syncable to other devices? reply paulnpace 7 hours agorootparent> Nothing after the # will be sent over HTTPs I notice it wants to run Cloudflare insights. Do they track this? reply dontdoxxme 6 hours agorootparentThey do, so in theory everything after the '#' is not sent, however it frequently sends a request to \"https://notepadtab.com/cdn-cgi/rum?\" which is part of Cloudflare Analytics (https://www.cloudflare.com/en-au/web-analytics/). The payload includes \"timingsV2\" data, which leaks the hash part to the server. reply jasperry 7 hours agoparentprev [–] The URL is encrypted in HTTPS/TLS. An eavesdropper can see you making a TCP connection to the server, but everything above that layer, including the HTTP header with the URL, is encrypted payload. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Notepad Tab is a note-taking tool that saves notes directly in the browser's address bar and history, making it simple and private.",
      "Users can back up or share notes by copying and pasting the URL, leveraging JavaScript for efficient note persistence, compression, and decompression.",
      "This tool emphasizes security and privacy, ensuring that notes are stored and retrieved efficiently without external storage."
    ],
    "commentSummary": [
      "A Hacker News discussion examines hosting static web apps locally as an alternative to Electron apps, focusing on security, practicality, and technologies like Redbean, SQLite, and service workers.",
      "Users debate browser-based solutions versus native apps for note-taking and lightweight applications, discussing the evolving role of browsers and modern security limitations.",
      "Concerns include data theft, URL length limits, browser history pollution, and privacy implications, with participants sharing experiences with local storage, bookmarklets, and browser-based notepads."
    ],
    "points": 176,
    "commentCount": 134,
    "retryCount": 0,
    "time": 1716844540
  },
  {
    "id": 40496967,
    "title": "Leaked Google Search API Docs Reveal Hidden Ranking Factors and Data Practices",
    "originLink": "https://sparktoro.com/blog/an-anonymous-source-shared-thousands-of-leaked-google-search-api-documents-with-me-everyone-in-seo-should-see-them/",
    "originBody": "An Anonymous Source Shared Thousands of Leaked Google Search API Documents with Me; Everyone in SEO Should See Them By Rand FishkinMay 27, 2024 On Sunday, May 5th, I received an email from a person claiming to have access to a massive leak of API documentation from inside Google’s Search division. The email further claimed that these leaked documents were confirmed as authentic by ex-Google employees, and that those ex-employees and others had shared additional, private information about Google’s search operations. Many of their claims directly contradict public statements made by Googlers over the years, in particular the company’s repeated denial that click-centric user signals are employed, denial that subdomains are considered separately in rankings, denials of a sandbox for newer websites, denials that a domain’s age is collected or considered, and more. Naturally, I was skeptical. The claims made by this source (who asked to remain anonymous) seemed extraordinary–claims like: In their early years, Google’s search team recognized a need for full clickstream data (every URL visited by a browser) for a large percent of web users to improve their search engine’s result quality. A system called “NavBoost” (cited by VP of Search, Pandu Nayak, in his DOJ case testimony) initially gathered data from Google’s Toolbar PageRank, and desire for more clickstream data served as the key motivation for creation of the Chrome browser (launched in 2008). NavBoost uses the number of searches for a given keyword to identify trending search demand, the number of clicks on a search result (I ran several experiments on this from 2013-2015), and long clicks versus short clicks (which I presented theories about in this 2015 video). Google utilizes cookie history, logged-in Chrome data, and pattern detection (referred to in the leak as “unsquashed” clicks versus “squashed” clicks) as effective means for fighting manual & automated click spam. NavBoost also scores queries for user intent. For example, certain thresholds of attention and clicks on videos or images will trigger video or image features for that query and related, NavBoost-associated queries. Google examines clicks and engagement on searches both during and after the main query (referred to as a “NavBoost query”). For instance, if many users search for “Rand Fishkin,” don’t find SparkToro, and immediately change their query to “SparkToro” and click SparkToro.com in the search result, SparkToro.com (and websites mentioning “SparkToro”) will receive a boost in the search results for the “Rand Fishkin” keyword. NavBoost’s data is used at the host level for evaluating a site’s overall quality (my anonymous source speculated that this could be what Google and SEOs called “Panda”). This evaluation can result in a boost or a demotion. Other minor factors such as penalties for domain names that exactly match unbranded search queries (e.g. mens-luxury-watches.com or milwaukee-homes-for-sale.net), a newer “BabyPanda” score, and spam signals are also considered during the quality evaluation process. NavBoost geo-fences click data, taking into account country and state/province levels, as well as mobile versus desktop usage. However, if Google lacks data for certain regions or user-agents, they may apply the process universally to the query results. During the Covid-19 pandemic, Google employed whitelists for websites that could appear high in the results for Covid-related searches Similarly, during democratic elections, Google employed whitelists for sites that should be shown (or demoted) for election-related information And these are only the tip of the iceberg. Extraordinary claims require extraordinary evidence. And while some of these overlap with information revealed during the Google/DOJ case (some of which you can read about on this thread from 2020), many are novel and suggest insider knowledge. So, this past Friday, May 24th (following several emails), I had a video call with the anonymous source. An anonymized screen capture from Rand’s call with the source Prior to the email and call, I had neither met nor heard of the person who emailed me about this leak. They asked that their identity remain veiled, and that I merely include the quote below: An eagle uses the storm to reach unimaginable heights. – Matshona Dhliwayo After the call I was able to confirm details of their work history, mutual people we both know from the marketing world, and several of their claims about being at particular events with industry insiders (including Googlers), though I cannot confirm details of the meetings nor the content of discussions they claim to have had. During our call, this contact showed me the leak itself: more than 2,500 pages of API documentation containing 14,014 attributes (API features) that appear to come from Google’s internal “Content API Warehouse.” Based on the document’s commit history, this code was uploaded to GitHub on Mar 27, 2024th and not removed until May 7, 2024th. This documentation doesn’t show things like the weight of particular elements in the search ranking algorithm, nor does it prove which elements are used in the ranking systems. But, it does show incredible details about data Google collects. Here’s an example of the document format: Screen capture of leaked data about “good” and “bad” clicks, including length of clicks (i.e. how long a visitor spends on a web page they’ve clicked from Google’s search results before going back to the search results) After walking me through a handful of these API modules, the source explained their motivations (around transparency, holding Google to account, etc.) and their hope: that I would publish an article sharing this leak, revealing some of the many interesting pieces of data it contained, and refuting some “lies” Googlers “had been spreading for years.” A sample of statements from Google representatives (Matt Cutts, Gary Ilyes, and John Mueller) denying the use of click-based user signals in rankings over the years Is this API Leak Authentic? Can We Trust It? A critical next step in the process was verifying the authenticity of the API Content Warehouse documents. So, I reached out to some ex-Googler friends, shared the leaked docs, and asked for their thoughts. Three ex-Googlers wrote back: one said they didn’t feel comfortable looking at or commenting on it. The other two shared the following (off the record and anonymously): “I didn’t have access to this code when I worked there. But this certainly looks legit. “ “It has all the hallmarks of an internal Google API.” “It’s a Java-based API. And someone spent a lot of time adhering to Google’s own internal standards for documentation and naming.” “I’d need more time to be sure, but this matches internal documentation I’m familiar with.” “Nothing I saw in a brief review suggests this is anything but legit.” Next, I needed help analyzing and deciphering the naming conventions and more technical aspects of the documentation. I’ve worked with APIs a bit, but it’s been 20 years since I wrote code and 6 years since I practiced SEO professionally. So, I reached out to one of the world’s foremost technical SEOs: Mike King, founder of iPullRank. During a 40-minute phone call on Friday afternoon, Mike reviewed the leak and confirmed my suspicions: this appears to be a legitimate set of documents from inside Google’s Search division, and contains an extraordinary amount of previously-unconfirmed information about Google’s inner workings. 2,500 technical documents is an unreasonable amount of material to ask one man (a dad, husband, and entrepreneur, no less) to review in a single weekend. But, that didn’t stop Mike from doing his best. He’s put together an exceptionally detailed initial review of the Google API leak here, which I’ll reference more in the findings below. And he’s also agreed to join us at SparkTogether 2024 in Seattle, WA on Oct. 8, where he’ll present the fully transparent story of this leak in far greater detail, and with the benefit of the next few months of analysis. Qualifications and Motivations for this Post Before we go further, a few disclaimers: I no longer work in the SEO field. My knowledge of and experience with SEO is 6+ years out of date. I don’t have the technical expertise or knowledge of Google’s internal operations to analyze an API documentation leak and confirm with certainty whether it’s authentic (hence getting Mike’s help and the input of ex-Googlers). So why publish on this topic? Because when I spoke to the party that sent me this information, I found them credible, thoughtful, and deeply knowledgeable. Despite going into the conversation deeply skeptical, I could identify no red flags, nor any malicious motivation. This person’s sole aim appeared quite aligned with my own: to hold Google accountable for public statements that conflict with private conversations and leaked documentation, and to bring greater transparency to the field of search marketing. And they believed that, despite my years removed from SEO, I was the best person to share this publicly. These are goals I cared about deeply for almost two decades. And while my professional life has moved on (I now run two companies: SparkToro, which makes audience research software and Snackbar Studio, an indie video game developer), my interest in and connections to the world of Search Engine Optimization remain strong. I feel a deep obligation to share information about how the world’s dominant search engine works, especially information Google would prefer to keep quiet. And sadly, I’m not sure where else to send something this potentially groundbreaking. Years ago, before he left journalism to become Google’s Search Liaison, Danny Sullivan, would have been my go-to source for a leak of this magnitude. He had the gravitas, resume, knowledge, and experience to examine a claim like this and present it fairly in the court of public opinion. There have been so many times in the last few years I’ve wished for Danny’s calm, even-handed, tough-but-fair-on-Google approach to newsworthy pieces like this–pieces that could reach as far as the company’s statements on the witness stand (e.g. his eloquent writing on Google’s indefensible privacy claims about organic keyword data). Whatever Google’s paying him, it isn’t nearly enough. Apologies that instead of Danny, dear reader, you’re stuck with me. But since you are, I’m going to assume you may not be familiar with my background or credentials, and briefly share those. I started doing SEO for small businesses in the Seattle area in 2001, and co-founded the SEO consultancy that would become Moz (originally called SEOmoz) in 2003. For the next 15 years, I worked in the search marketing industry and was often recognized as an influential leader in that field. I authored/co-authored Lost and Founder: A Painfully Honest Field Guide to the Startup World, The Art of SEO, and Inbound Marketing and SEO. Publications including the WSJ, Inc, Forbes, and hundreds more have written about and quoted me on the world of SEO and Google search, many of them citing a popular weekly video series I hosted for a decade: Whiteboard Friday. Moz grew to 35,000+ paying customers of its SEO software, revenues of $50M+, and a team of ~200 before being sold to a private equity buyer in 2021. I left in 2018 and started SparkToro, and in 2023, Snackbar Studio. I dropped out of college at the University of Washington in 2001 and do not hold a degree, yet my work on Google and SEO has been cited by the United States Congress, the US Federal Trade Commission, the Wall Street Journal, New York Times, and John Oliver’s Last Week Tonight, among dozens of others. I hold several patents around the design of a web scale link index, and am the creator of numerous link-index metrics, including Domain Authority, a machine-learning based score commonly used in the digital marketing world to assess a website’s capability to rank in Google’s search engine. OK. Back to the Google leak. What is the Google API Content Warehouse? When looking through the massive trove of API documentation, the first reasonable set of questions might be: “What is this? What is it used for? Why does it exist in the first place?” The leak appears to come from GitHub, and the most credible explanation for its exposure matches what my anonymous source told me on our call: these documents were inadvertently and briefly made public (many links in the documentation point to private GitHub repositories and internal pages on Google’s corporate site that require specific, Google-credentialed logins). During this probably-accidental, public period between March and May of 2024, the API documentation was spread to Hexdocs (which indexes public GitHub repos) and found/circulated by other sources (I’m certain that others have a copy, though it’s odd that I could find no public discourse until now). According to my ex-Googler sources, documentation like this exists on almost every Google team, explaining various API attributes and modules to help familiarize those working on a project with the data elements available. This leak matches others in public GitHub repositories and on Google’s Cloud API documentation, using the same notation style, formatting, and even process/module/feature names and references. If that all sounds like a technical mouthful, think of this as instructions for members of Google’s search engine team. It’s like an inventory of books in a library, a card catalogue of sorts, telling those employees who need to know what’s available and how they can get it. But, whereas libraries are public, Google search is one of the most secretive, closely-guarded black boxes in the world. In the last quarter century, no leak of this magnitude or detail has ever been reported from Google’s search division. How certain can we be that Google’s search engine uses everything detailed in these API docs? That’s open to interpretation. Google could have retired some of these, used others exclusively for testing or internal projects, or may even have made API features available that were never employed. However, there are references in the documentation to deprecated features and specific notes on others indicating they should no longer be used. That strongly suggests those not marked with such details were still in active use as of the March, 2024 leak. We also can’t say for certain whether the March leak is of the most recent version of this documentation. The most recent date I can find referenced in the API docs is August of 2023: The relevant text reads: “The domain-level display name of the website, such as “Google” for google.com. See go/site-display-name for more details. As of Aug 2023, this field is being deprecated in favor of info.[AlternativeTitlesResponse].site_display_name_response field, which also contains host-level site display names with additional information.” A reasonable reader would conclude that the documentation was up-to-date as of last summer (references to other changes in 2023 and earlier years, all the way back to 2005, are also present), and possibly even up-to-date as of the March 2024 date of disclosure. Google search obviously changes massively from year to year, and recent introductions like their much-maligned AI Overviews, do not make an appearance in this leak. Which of the items mentioned are actively used today in Google’s ranking systems? That’s open to speculation. This trove contains fascinating references, many that will be entirely new to non-Google-search-engineers. But, I would urge readers not to point to a particular API feature in this leak and say: “SEE! That’s proof Google uses XYZ in their rankings.” It’s not quite proof. It’s a strong indication, stronger than patent applications or public statements from Googlers, but still no guarantee. That said, it’s as close to a smoking gun as anything since Google’s execs testified in the DOJ trial last year. And, speaking of that testimony, much of it is corroborated and expanded on in the document leak, as Mike details in his post. 👀 What can we learn from the Data Warehouse Leak? I expect that interesting and marketing-applicable insights will be mined from this massive file set for years to come. It’s simply too big and too dense to think that a weekend of browsing could unearth a comprehensive set of takeaways, or even come close. However, I will share five of the most interesting, early discoveries in my perusal, some that shed new light on things Google has long been assumed to be doing, and others that suggest the company’s public statements (especially those on what they “collect”) have been erroneous. Because doing so would be tedious and could be perceived as personal grievances (given Google’s historic attacks on my work), I won’t bother showing side-by-sides of what Googlers said vs. what this document insinuates. Besides, Mike did a great job of that in his post. Instead, I’ll focus on interesting and/or useful takeaways, and my conclusions from the whole of the modules I’ve been able to review, Mike’s piece on the leak, and how this combines with other things we know to be true of Google. #1: Navboost and the use of clicks, CTR, long vs. short clicks, and user data A handful of modules in the documentation make reference to features like “goodClicks,” “badClicks,” “lastLongestClicks,” impressions, squashed, unsquashed, and unicorn clicks. These are tied to Navboost and Glue, two words that may be familiar to folks who reviewed Google’s DOJ testimony. Here’s a relevant excerpt from DOJ attorney Kenneth Dintzer’s cross-examination of Pandu Nayak, VP of Search on the Search Quality team: Q. So remind me, is navboost all the way back to 2005? A. It’s somewhere in that range. It might even be before that. Q. And it’s been updated. It’s not the same old navboost that it was back then? A. No. Q. And another one is glue, right? A. Glue is just another name for navboost that includes all of the other features on the page. Q. Right. I was going to get there later, but we can do that now. Navboost does web results, just like we discussed, right? A. Yes. Q. And glue does everything else that’s on the page that’s not web results, right? A. That is correct. Q. Together they help find the stuff and rank the stuff that ultimately shows up on our SERP? A. That is true. They’re both signals into that, yes. A savvy reader of these API documents would find they support Mr. Nayak’s testimony (and align with Google’s patent on site quality): Quality Navboost Data module Geo-segmentation of Navboost Data Clicks Signals in Navboost Data Aging Impressions and clicks Google appears to have ways to filter out clicks they don’t want to count in their ranking systems, and include ones they do. They also seem to measure length of clicks (i.e. pogo-sticking – when a searcher clicks a result and then quickly clicks the back button, unsatisfied by the answer they found) and impressions. Plenty has already been written about Google’s use of click data, so I won’t belabor the point. What matters is that Google has named and described features for that measurement, adding even more evidence to the pile. #2: Use of Chrome browser clickstreams to power Google Search My anonymous source claimed that way back in 2005, Google wanted the full clickstream of billions of Internet users, and with Chrome, they’ve now got it. The API documents suggest Google calculates several types of metrics that can be called using Chrome views related to both individual pages and entire domains. This document, describing the features around how Google creates Sitelinks, is particularly interesting. It showcases a call named topUrl, which is “A list of top urls with highest two_level_score, i.e., chrome_trans_clicks.” My read is that Google likely uses the number of clicks on pages in Chrome browsers and uses that to determine the most popular/important URLs on a site, which go into the calculation of which to include in the sitelinks feature. E.G. In the above screenshot from Google’s results, pages like “Pricing,” the “Blog,” and the “Login” pages are our most-visited, and Google knows this through their tracking of billions of Chrome users’ clickstreams. Quality NSR Data module Video Content Search module Quality Sitemap module #3: Whitelists in Travel, Covid, and Politics A module on “Good Quality Travel Sites” would lead reasonable readers to conclude that a whitelist exists for Google in the travel sector (unclear if this is exclusively for Google’s “Travel” search tab, or web search more broadly). References in several places to flags for “isCovidLocalAuthority” and “isElectionAuthority” further suggests that Google is whitelisting particular domains that are appropriate to show for highly controversial of potentially problematic queries. For example, following the 2020 US Presidential election, one candidate claimed (without evidence) that the election had been stolen, and encouraged their followers to storm the Capital and take potentially violent action against lawmakers, i.e. commit an insurrection. Google would almost certainly be one of the first places people turned to for information about this event, and if their search engine returned propaganda websites that inaccurately portrayed the election evidence, that could directly lead to more contention, violence, or even the end of US democracy. Those of us who want free and fair elections to continue should be very grateful Google’s engineers are employing whitelists in this case. Quality NSR Data Attributes Assistant API Settings for Music Filters Video Content Search Query Features Quality Travel Sites Data module #4: Employing Quality Rater Feedback Google has long had a quality rating platform called EWOK (Cyrus Shepard, a notable leader in the SEO space, spent several years contributing to this and wrote about it here). We now have evidence that some elements from the quality raters are used in the search systems. How influential these rater-based signals are, and what precisely they’re used for is unclear to me in an initial read, but I suspect some thoughtful SEO detectives will dig into the leak, learn, and publish more about it. What I find fascinating is that scores and data generated by EWOK’s quality raters may be directly involved in Google’s search system, rather than simply a training set for experiments. Of course, it’s possible these are “just for testing,” but as you browse through the leaked documents, you’ll find that when that’s true, it’s specifically called out in the notes and module details. This one calls out a “per document relevance rating” sourced from evaluations done via EWOK. There’s no detailed notation, but it’s not much of a logic-leap to imagine how important those human evaluations of websites really are. This one calls out “Human Ratings (e.g. ratings from EWOK)” and notes that they’re “typically only populated in the evaluation pipelines,” which suggests they may be primarily training data in this module (I’d argue that’s still a hugely important role, and marketers shouldn’t dismiss how important it is that quality raters perceive and rate their websites well). Webref Mention Ratings module Webref Task Data module Document Level Relevance module Webref per Doc Relevance Rating module Webref Entity Join #5: Google Uses Click Data to Determine How to Weight Links in Rankings This one’s fascinating, and comes directly from the anonymous source who first shared the leak. In their words: “Google has three buckets/tiers for classifying their link indexes (low, medium, high quality). Click data is used to determine which link graph index tier a document belongs to. See SourceType here, and TotalClicks here.” In summary: If Forbes.com/Cats/ has no clicks it goes into the low-quality index and the link is ignored If Forbes.com/Dogs/ has a high volume of clicks from verifiable devices (all the Chrome-related data discussed previously), it goes into the high-quality index and the link passes ranking signals Once the link becomes “trusted” because it belongs to a higher tier index, it can flow PageRank and anchors, or be filtered/demoted by link spam systems. Links from the low-quality link index won’t hurt a site’s ranking; they are merely ignored. Big Picture Takeaways for Marketers who Care About Organic Search Traffic If you care strategically about the value of organic search traffic, but don’t have much use for the technical details of how Google works, this section’s for you. It’s my attempt to sum up much of Google’s evolution from the period this leak covers: 2005 – 2023, and I won’t limit myself exclusively to confirmed elements of the leak. Brand matters more than anything else Google has numerous ways to identify entities, sort, rank, filter, and employ them. Entities include brands (brand names, their official websites, associated social accounts, etc.), and as we’ve seen in our clickstream research with Datos, they’ve been on an inexorable path toward exclusively ranking and sending traffic to big, powerful brands that dominate the web > small, independent sites and businesses. If there was one universal piece of advice I had for marketers seeking to broadly improve their organic search rankings and traffic, it would be: “Build a notable, popular, well-recognized brand in your space, outside of Google search.” Experience, expertise, authoritativeness, and trustworthiness (“E-E-A-T”) might not matter as directly as some SEOs think. The only mention of topical expertise in the leak we’ve found so far is a brief notation about Google Maps review contributions. The other aspects of E-E-A-T are either buried, indirect, labeled in hard-to-identify ways, or, more likely (in my opinion) correlated with things Google uses and cares about, but not specific elements of the ranking systems. As Mike noted in his article, there is documentation in the leak suggesting Google can identify authors and treats them as entities in the system. Building up one’s influence as an author online may indeed lead to ranking benefits in Google. But what exactly in the ranking systems makes up “E-E-A-T” and how powerful those elements are is an open question. I’m a bit worried that E-E-A-T is 80% propaganda, 20% substance. There are plenty of powerful brands that rank remarkably well in Google and have very little experience, expertise, authoritativeness, or trustworthiness, as HouseFresh’s recent, viral article details in depth. Content and links are secondary when user intention around navigation (and the patterns that intent creates) are present. Let’s say, for example, that many people in the Seattle area search for “Lehman Brothers” and scroll to page 2, 3, or 4 of the search results until they find the theatre listing for the Lehman Brother stage production, then click that result. Fairly quickly, Google will learn that’s what searchers for those words in that area want. Even if the Wikipedia article about Lehman Brothers’ role in the financial crisis of 2008 were to invest heavily in link building and content optimization, it’s unlikely they could outrank the user-intent signals (calculated from queries and clicks) of Seattle’s theatre-goers. Extending this example to the broader web and search as a whole, if you can create demand for your website among enough likely searchers in the regions you’re targeting, you may be able to end-around the need for classic on-and-off-page SEO signals like links, anchor text, optimized content, and the like. The power of Navboost and the intent of users is likely the most powerful ranking factor in Google’s systems. As Google VP Alexander Grushetsky put it in a 2019 email to other Google execs (including Danny Sullivan and Pandu Nayak): “We already know, one signal could be more powerful than the whole big system on a given metric. For example, I’m pretty sure that NavBoost alone was / is more positive on clicks (and likely even on precision / utility metrics) by itself than the rest of ranking (BTW, engineers outside of Navboost team used to be also not happy about the power of Navboost, and the fact it was “stealing wins”)“ Those seeking even more confirmation could review Google engineer Paul Haahr’s detailed resume, which states: “I’m the manager for logs-based ranking projects. The team’s efforts are currently split among four areas: 1) Navboost. This is already one of Google’s strongest ranking signals. Current work is on automation in building new navboost data;” Classic ranking factors: PageRank, anchors (topical PageRank based on the anchor text of the link), and text-matching have been waning in importance for years. But Page Titles are still quite important. This is a finding from Mike’s excellent analysis that I’d be foolish not to call out here. PageRank still appears to have a place in search indexing and rankings, but it’s almost certainly evolved from the original 1998 paper. The document leak insinuates multiple versions of PageRank (rawPagerank, a deprecated PageRank referencing “nearest seeds,” firstCoveragePageRank from when the document was first served, etc.) have been created and discarded over the years. And anchor text links, while present in the leak, don’t seem to be as crucial or omnipresent as I’d have expected from my earlier years in SEO. For most small and medium businesses and newer creators/publishers, SEO is likely to show poor returns until you’ve established credibility, navigational demand, and a strong reputation among a sizable audience. SEO is a big brand, popular domain’s game. As an entrepreneur, I’m not ignoring SEO, but I strongly expect that for the years ahead, until/unless SparkToro becomes a much larger, more popular, more searched-for and clicked-on brand in its industry, this website will continue to be outranked, even for its original content, by aggregators and publishers who’ve existed for 10+ years. This is almost certainly true for other creators, publishers, and SMBs. The content you create is unlikely to perform well in Google if competition from big, popular websites with well-known brands exists. Google no longer rewards scrappy, clever, SEO-savvy operators who know all the right tricks. They reward established brands, search-measurable forms of popularity, and established domains that searchers already know and click. From 1998 – 2018 (or so), one could reasonable start a powerful marketing flywheel with SEO for Google. In 2024, I don’t think that’s realistic, at least, not on the English-language web in competitive sectors. Next Steps for the Search Industry I’m excited to see how practitioners with more recent experience and deeper technical knowledge go about analyzing this leak. I encourage anyone curious to dig into the documentation, attempt to connect it to other public documents, statements, testimony, and ranking experiments, then publish their findings. Historically, some of the search industry’s loudest voices and most prolific publishers have been happy to uncritically repeat Google’s public statements. They write headlines like “Google says XYZ is true,” rather than “Google Claims XYZ; Evidence Suggests Otherwise.” The SEO industry doesn’t benefit from these kinds of headlines Please, do better. If this leak and the DOJ trial can create just one change, I hope this is it. When those new to the field read Search Engine Roundtable, Search Engine Land, SE Journal, and the many agency blogs and websites that cover the SEO field’s news, they don’t necessarily know how seriously to take Google’s statements. Journalists and authors should not presume that readers are savvy enough to know that dozens or hundreds of past public comments by Google’s official representatives were later proven wrong. This obligation isn’t just about helping the search industry—it’s about helping the whole world. Google is one of the most powerful, influential forces for the spread of information and commerce on this planet. Only recently have they been held to some account by governments and reporters. The work of journalists and writers in the search marketing field carries weight in the courts of public opinion, in the halls of elected officials, and in the hearts of Google employees, all of whom have the power to change things for the better or ignore them at our collective peril. Thank you to Mike King for his invaluable help on this document leak story, to Amanda Natividad for editing help, and to the anonymous source who shared this leak with me. I expect that updates to this piece may arrive over the next few days and weeks as it reaches more eyeballs. If you have findings that support or contradict statements I’ve made here, please feel free to share them in the comments below. Subscribe to the SparkToro Blog Get Posts Via Email Email: Subscribe",
    "commentLink": "https://news.ycombinator.com/item?id=40496967",
    "commentBody": "Anonymous Source Shared Leaked Google Search API Documents (sparktoro.com)175 points by andrewfong 16 hours agohidepastfavorite71 comments theolivenbaum 13 hours agoSeems like a lot of it came from them inadvertently posting some internal API to GitHub: https://github.com/googleapis/elixir-google-api/commit/078b4... reply ec109685 12 hours agoparentOops, someone’s script was too greedy when uploading those elixir api documents. reply precompute 11 hours agoprevThis just proves all the \"suspicions\" privacy-conscious users have had about large corporations fingerprinting users, often in very obvious ways. There's often no better place to find ideas for surveillance than the people conscious about being surveilled. reply p3rls 2 hours agoparentMany of the SEO suspicions were confirmed too. I found it VERY amusing if you go to r/SEO just yesterday there were moderators and flaired users (you know, the elites of the SEO community, lol) insisting much of this was \"debunked\" years ago. They of course deleted their posts, but the threads are still up. What a den of scammers over there. https://www.reddit.com/r/SEO/comments/1d1eqjj/comment/l5tvfw... https://www.reddit.com/user/WebLinkr/ I love how reddit is turning into the new SEO scam over night because of this stuff. Great work as always Danny Sullivan! reply p3rls 6 minutes agorootparentIt's just endlessly fascinating to me the grift on rSEO How these types first gain moderator status on a few subs and then the spam begins (picture of spam https://pixeldrain.com/u/a6qUPjTq ) I haven't been able to find a single legitimate expert in the entire sub, and I've checked about every flaired user and moderator. You have lots of people like the above, or https://www.reddit.com/user/jesustellezllc/ that claim to run an agency in Frenso California called Ozelot Media, but when you look him up there's nothing. When you google \"SEO\" + \"Fresno California\", Ozelot media isn't even in the top 100 results. Lol, I thought that was the job of a SEO-type? Why let that stop the grift though? reply precompute 11 hours agoprev> My anonymous source claimed that way back in 2005, Google wanted the full clickstream of billions of Internet users, and with Chrome, they’ve now got it. The API documents suggest Google calculates several types of metrics that can be called using Chrome views related to both individual pages and entire domains. What answer do the engineers at google working on this have for this violation of privacy? reply raxxorraxor 4 hours agoparentThat would be money. If someone has another excuse, they are naive or lying to themselves. It certainly is not \"to improve the net or advertising\" - that would be the lying part. Google has done some good for the net, but the scales of their contributions slowly but steadily move to the negative side. reply danpalmer 10 hours agoparentprevPersonal (not work related opinion): This basically can’t happen with things like DMA and GDPR. DMA in particular means you can’t share data across “products” without explicit consent. So you could for example collect websites that don’t work for the purposes of improving Chrome, but not then share that with the Ads/Search orgs for personalisation or targeting, as far as I understand the legislation. Personal opinion about work at Google (still not googles opinion) I’m consistently impressed with how seriously this stuff is taken and the amount of work that goes into making sure that things like this sharing can’t happen accidentally, and that user choice is respected. The engineers on the ground are absolutely making sure this all works, and most of us care deeply about user privacy. I have personally worked both on implementing new features that significantly push forward privacy, and on implementing privacy controls for regulatory purposes. reply verteu 4 hours agorootparent> I’m consistently impressed with how seriously this stuff is taken and the amount of work that goes into making sure that things like this sharing can’t happen accidentally I believe the law is violated when it's sufficiently profitable -- it just requires VP permission. No public sources for this except Jedi Blue, the old anti-poaching case, etc. reply BrenBarn 9 hours agorootparentprevThe thing is that preventing \"sharing\" isn't sufficient. People who are concerned about privacy don't want any such data collected or stored in the first place, ever. The implicit \"sharing\" of my data with Google (or whatever company) is a problem in itself. Regardless of how \"seriously\" Google (or whatever company) takes it, for a lot of the data I don't want them to ever have it in the first place. reply troyvit 4 hours agorootparent> The thing is that preventing \"sharing\" isn't sufficient. Exactly this. It doesn't matter that google doesn't \"share\" what they gather if they own so many conversion funnels from top to bottom anyway. reply danpalmer 7 hours agorootparentprevThis is a fair position to take, but assuming good faith all round, one that I think will typically be a minority. If you ask a user if they're willing to share crash reports only to improve the reliability of the software, I'd bet most people would be ok with this. In fact it's sufficiently reasonable that I believe GDPR allows this to be opt-out, something I broadly agree with. I do think opt-outs should be available, I do think there should be configuration available for those who do not wish to share anything, but if the laws are being met, in the right spirit, then I would hope it would provide little actual benefit. reply BrenBarn 1 hour agorootparent> assuming good faith all round But why would anyone assume that? I think the position of many privacy advocates is that we're long past the point where it's reasonable to assume Google is acting in good faith in the best interests of its users. (Again, to be fair, this is true of more companies than just Google.) reply thaumasiotes 4 hours agorootparentprevThat analogy doesn't seem like a match. If you ask a user if they're willing to share every action they ever take \"only to improve the reliability of the software\", a lot of them are going to say \"wait, why would you even need that?\" If I'm letting you scrape crash dumps and my browser happens to crash in the request where I send my credit card information to xhamster, that's one thing. Odds are that's never happened to anyone. It's another thing for you to guarantee that you're planning to record that information. reply throwaway743 3 hours agorootparentprev> If you ask a user if they're willing to share crash reports only to improve the reliability of the software, I'd bet most people would be ok with this. You do realize the majority of people are completely oblivious as to why privacy matters as it relates to their data collection. It's not that they're willing to do anything. It's that they're passive/apathetic when faced with vague prompts telling them about a matter they don't have insight on, after being bombarded by terms of service agreements, cookie pop ups, etc for years and years. > This is a fair position to take, but assuming good faith all round, one that I think will typically be a minority. If they were aware of privacy implications / exactly what's being collected on them and how that data is being used, then it's safe to say that they'd be the majority. Can't blame them for not taking the time to read into the matter either, as most outside of tech are wrapped up with a million other hostilities in their daily lives. Defend it all you want, but it's just one more unethical thing screwing people over. reply noprocrasted 5 hours agorootparentprev> This basically can’t happen with things like DMA and GDPR I'm sorry but this is just wishful thinking. It might be what the spirit of the DMA & GDPR want but definitely not the reality thanks to inadequate or outright non-existent enforcement. There are businesses out there whose entire business model and revenue stream are based on violating the GDPR. Not some kind of internal conspiracy or rogue employee, but the entire company is doing it in the open and the result of its doings (targeted ads or spam) are visible out there in the open for all to see. Facebook, credit bureaus, data brokers, \"consent management platforms\", etc. All these companies' business models are big, obvious breaches of the GDPR. Yet, they are... still alive and kicking? There is no chance that a concealed GDPR breach (whether intentional or accidental) will get addressed when the biggest intentional breaches are still allowed to continue out there in the open. I suspect something very similar is going to happen with the DMA - Apple is already acting in bad faith but have yet to see any consequences. reply bdlowery 11 hours agoparentprevHow is it a violation of privacy. Did you read the terms of service? reply precompute 11 hours agorootparentIt's a privacy violation regardless of the ToS. reply y42 11 hours agorootparentprevA tos announcement is not an explicit consent. I doubt that this will help in court, even pre-GDPR. reply HelloNurse 9 hours agorootparentFurther, a TOS announcement can be easily construed as an admission of intent to fuck users. reply xnx 8 hours agoprevI believe these are the leaked docs: https://hexdocs.pm/google_api_content_warehouse/0.4.0/api-re... reply ec109685 12 hours agoprevIf anyone is surprised about chrome sending urls to Google, you can turn the “feature” off by unchecking “Make searches and browsing better” in the sync section of Google chrome settings. Creepy. reply Terr_ 11 hours agoparent\"But what if I don't want my own computer to build and share a detailed profile of everyone I know, everywhere I go, all my preferences, and how to manipulate me?\" \"Well obviously it's your fault for not picking the 'Don't Be Cool' option on subpage 27b-6, duh!\" reply ralfn 10 hours agorootparentYeah. It's victim blaming. Reminds me of \"they should have shouted louder\". The confusing thing is the crime itself is small on an individual level. The question is: does it add up cumulatively if a small crime is committed against many? reply kulshan 30 minutes agorootparentI don't know if it's \"Victim Blaming\"...I teach Digital Literacy courses for seniors new to technology. While I do set them up with Firefox and Ublock, we generally have them use Gmail as they are all Android Devices. Google sends a confirmation email to walk each one of them through their security settings. Of course most users just ignore this email (like I used to have students do) but now we go through it and uncheck this setting in all my courses, and unpersonalize ads as well. Feel like the most basic user who has even the tiniest concern of data privacy should know how to look at their Google Account settings. These are 80 year olds who don't even know what a \"click\" is but they know to be skeptical of using Google. reply HenryBemis 6 hours agoparentprevOr, and hear me out, you never use Chrome again, in any platform.. like ever ever again. reply precompute 11 hours agoparentprevIs that part of Chrome not open-source? reply noman-land 6 hours agoparentprevImagine thinking you can escape your abuser by living in their house and asking them politely to stop. reply llmblockchain 7 hours agoprev> GoogleApi.ContentWarehouse.V1.Model.AppsPeopleOzExternalMergedpeopleapiAboutMeExtendedDataPhotosCompareDataDiffData Java, is that you?! reply deely3 3 hours agoparenthttps://news.ycombinator.com/newsguidelines.html > Omit internet tropes. reply ziddoap 3 hours agorootparentIndeed. Eschew humor. Avoid anything not super serious. Laughs aren't allowed on HN. reply shepherdjerred 2 hours agorootparentThere's a difference between humor and pattern-matching memes like you see in Reddit threads. reply lazide 6 hours agoparentprevMissing the ‘ManagerAgentUtil’ at the end. reply resolutebat 6 hours agorootparentFactoryFactoryImpl reply lazide 4 hours agorootparentBuilder reply lioeters 2 hours agorootparentBean reply cauefcr 23 minutes agorootparentSingleton reply precompute 11 hours agoprevFrom the article: Boosting \"organic traffic\": - Brand matters more than anything else - Experience, expertise, authoritativeness, and trustworthiness (“E-E-A-T”) might not matter as directly as some SEOs think. - Content and links are secondary when user intention around navigation (and the patterns that intent creates) are present. - Classic ranking factors: PageRank, anchors (topical PageRank based on the anchor text of the link), and text-matching have been waning in importance for years. But Page Titles are still quite important. - For most small and medium businesses and newer creators/publishers, SEO is likely to show poor returns until you’ve established credibility, navigational demand, and a strong reputation among a sizable audience. TL;DR: Clickbait + bot farms are the way to go. No wonder the internet is going to shit. reply pembrook 5 hours agoprevWhat I find most interesting about this is that a lot of supposed \"smart\" algorithms of Big Tech are in fact a patchwork of \"dumb\" rules rules and human-picked winners. This would explain why the quality of search results is failing to keep up with developments in LLMs. This also explains why it's impossible for incumbents to unseat the winners in many search categories -- because they've literally been picked as the winners by humans at Google. Looking at my Twitter/X feed, I also see an oddly similar dynamic. Certain accounts appear to have been manually boosted, showing up all the time -- whereas others posting even the same exact content will never appear. Silicon valley will loudly tell you all about how wonderful they are at \"democratizing,\" however, if you look under the surface it appears they're just hand picking the winners. reply trogdor 3 hours agoparent> because they've literally been picked as the winners by humans at Google Is there evidence of that in the leaked documents? reply ilrwbwrkhv 13 hours agoprevAnd that's why if a developer doesn't use Firefox and uses Chrome, they are just helping a monopoly take over everything and make a mess. reply metadigm 3 hours agoparentAs soon as they add the ability to configure shortcuts, I'd more than happy to. After several years of requests, we're finally seeing some movement on their end. reply dgellow 13 hours agoparentprevAny user, not just developers reply olliej 12 hours agorootparentDevelopers just replaced IE as the only thing they develop for with chrome, users then _have_ to use chrome because of web developers who only develop for chrome and consider any behaviour other than \"it works in chrome\" as a bug in other browsers, just as they did with IE. Then there's the relentless parade of \"alternative browsers\" that are just chrome skins - a period IE also went through - that intentionally try to trick people into believing they're not just using chrome but with less security engineering, and more scams. reply dgellow 8 hours agorootparentYou’re conflating lots of unrelated things. IE was a horrible browser to support because Microsoft deliberately implemented their own incompatible version of web standards, or refused to implement modern standards. The push to deprecate IE was because it was creating a massive burden, I personally dealt with IE6 support in corporate world and can attest it’s depreciation was necessary. What you call chrome skins isn’t a thing, people are building softwares on top of Blink, the rendering engine used by Chrome. The issue here is the risk of ending with a single rendering engine for the majority of the browser market, a diversity of engine ensure a good respect of web standards, that has nothing to do with privacy or security. When you say “they just replaced IE”, that was >10 years ago… reply JSDevOps 12 hours agoprevSeriously considering switching back to Firefox after all these years. reply jasonsb 11 hours agoparentWhat's stopping you? I use both browsers and I see no reason why someone would pick Chrome over Firefox at this point in time. reply blitzar 6 hours agorootparent(Some) sites don't work on Firefox. Sure it isn't frequent, but it is frequent enough that once a day or so I have to open chrome to do something. reply elaus 5 hours agorootparentSeriously curious what sites those are, especially if it's not the same page every day. It literally never occurs to me (using Firefox again since 3-4 years) but I mostly browse dev-related websites. reply dmitrygr 2 minutes agorootparentjlcpcb's site is often broken in firefox, sadly. i keep chrome around just for it. redblacktree 4 hours agorootparentprevOne example I often run into: Plaid (the bank-linking company) doesn't work. Just hangs. Though I'll admit it's possible they fixed it. I've been trained to use Chrome when I have to interact with it. reply nonameiguess 2 hours agorootparentprevI have the same problem. I certainly don't use Chrome daily, but do have to keep it around, typically for shopping checkout and a fair number of US government websites don't work on any browser but Chrome. reply ilikehurdles 5 hours agorootparentprevOnce a day? That’s huge. What sites? (I use Firefox daily for about the last year and haven’t had this kind of issue) reply thisisit 4 hours agorootparentprevfor now the seamless extension switching using Extensity. I am yet to find an extension on Firefox which can deliver this functionality. reply metadigm 3 hours agorootparentprevNo shortcut configuration. reply 4gotunameagain 10 hours agorootparentprevWhile the reasons someone would pick Firefox: - Privacy - Tree style tabs reply SushiHippie 4 hours agorootparent- uBo works better in firefox https://github.com/gorhill/uBlock/wiki/uBlock-Origin-works-b... reply mind-blight 4 hours agoparentprevI've been using Firefox since Chrome forced users to sign in to the browser with their Google account, and I'm quite happy. The only time it's a problem is when a site detects Firefox and won't display unlocked your using chrome or IE. I've only seen that a couple of times in the years since I switched back reply Frank2312 4 hours agorootparentEven in that case,there are Firefox extensions to change your user agent. Suddenly the app requesting Chrome/Edge works perfectly, even though we are running in Firefox. reply kernal 4 hours agorootparentprevHow did Chrome force you to log in? I've been using it signed out for the longest time. reply mind-blight 3 hours agorootparentBack in 2018, Chrome released an update that automatically logged users into the browser if they'd logged into a google. It was done silently and automatically, and it was a pain to log out of. They faced a ton of backlash (https://www.pcmag.com/news/google-faces-privacy-backlash-ove...) and rolled the feature back, but that was the tipping point for me. I've been a happy Firefox user since reply Ringz 11 hours agoparentprevI've been using Firefox since the days when it had the other name. Meanwhile, I use Floorp [1], which is based on Firefox, but offers much more possibilities for customization. I am very satisfied, except for the stupid name... [1]: https://floorp.app/en/ reply MrAlex94 41 minutes agorootparentNot to be a stickler, but just a note it no longer counts as FOSS or even open source I believe, with their new licence: https://github.com/Floorp-Projects/Floorp-private-components... It’s left a bad taste in my mouth since they used the work of others to get to where they are, then when others do the same, they don’t like it. reply usui 13 hours agoprev> Prior to the email and call, I had neither met nor heard of the person who emailed me about this leak. They asked that their identity remain veiled And yet the journalist included a screenshot with one of the weakest blurs I've ever seen... Why would you not excise the person's video portion completely? What good does it serve to have it included in the story? Even if that portion is faked, why would you offer potential signals like skin complexion, hair color, background picture, etc.? Why... reply mtlynch 5 hours agoparentThe author is Rand Fishkin, who's not a journalist. He's the founder of SparkToro and Moz, both companies that provide tooling and analytics for SEO. I haven't looked deeply into Fishkin's companies, but I wouldn't expect either to be on the user's side when it comes to privacy. Both companies seem to monetize clickstream data and personal information from users who probably didn't give informed consent. If the source was trying to get this information to a responsible journalist who cares about privacy, I have no idea why they'd approach a company (not even a news organization) who seems to fund the erosion of user privacy. reply txomon 10 hours agoparentprevTo make it worse, he made clear when the call had happened, and you have: 1) Who was in the call 2) When the call happened 3) A blur instead of a complete black out I'm not sure I would feel safe reporting stuff to journalists nowadays. reply mrguyorama 3 hours agorootparentThis person is not a journalist. reply krackers 12 hours agoparentprev>weakest blurs I've ever seen Isn't this the same type of \"swirl\" blur that Interpol was able to reverse even 10 years back? With advancements since then you're basically handing evidence on a silver platter. reply roastedpeacock 10 hours agoparentprevThat also struck me as odd. And seemingly a violation of journalistic best-practices of protecting sources. I sure hope this was done with consent of the anonymous source. reply Control8894 13 hours agoparentprevIt's a fake background. It's also clearly from Google Meet so... yeah. If he was worried about retribution (from Google, anyway) then they probably wouldn't have been using a Google service. reply throwaway743 3 hours agoprev [–] ... why the hell would an anonymous source use google meet to share info on google? ... so much for remaining anonymous :/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Leaked Google Search API documents, verified by ex-Google employees, reveal the use of click-centric user signals, separate subdomain considerations, and a sandbox for new websites, contradicting Google's public statements.",
      "The documents detail a system called \"NavBoost,\" which uses clickstream data from Google Toolbar and Chrome to improve search results, and discuss penalties for exact-match domains, geo-fencing click data, and whitelists during significant events.",
      "The leak, likely from accidental public exposure on GitHub, includes over 2,500 pages of internal documentation, highlighting extensive data collection practices and suggesting Google favors big brands over smaller sites in its rankings."
    ],
    "commentSummary": [
      "Leaked Google Search API documents on GitHub confirm extensive user tracking through Chrome, raising significant privacy concerns.",
      "Discussions on Reddit highlight skepticism towards Google's motivations, suggesting profit-driven actions rather than user benefits, and debate the effectiveness of regulatory frameworks like DMA and GDPR.",
      "Users express mistrust towards tech companies, criticize Chrome's dominance, and show interest in privacy-focused alternatives like Firefox, while also raising concerns about Rand Fishkin's handling of an anonymous source's identity in a leak."
    ],
    "points": 175,
    "commentCount": 71,
    "retryCount": 0,
    "time": 1716865651
  },
  {
    "id": 40501027,
    "title": "Mobifree: Open-Source Mobile Ecosystem Challenging Big Tech Dominance",
    "originLink": "https://f-droid.org/2024/05/24/mobifree.html",
    "originBody": "Finally an alternative to Big Tech, your new open-source mobile ecosystem - Mobifree Posted on May 24, 2024 by HStill Have you ever wondered what it would be like to engage in a mobile ecosystem outside of the watchful eye of the Big Tech giants and gatekeepers? A system that includes everything from operating systems, to app stores, to cloud services, messaging apps, email servers and more? A system that puts your privacy first, believes in a democratic approach and healthy competition, and a system that relies on open-source solutions to drive its software? Welcome to Mobifree, a human-centered, ethical alternative, that champions privacy over profit and believes in collaboration, sustainability and inclusiveness. Everyone is locked into a mobile phone ecosystem where the terms are dictated by a handful of Big Tech companies all located in a single country. From end users looking to download and use their favorite apps, to developers who run into roadblocks when trying to get their solutions published, to governments who are increasingly using apps as a way to provide services to their citizens, we are all impacted by the gatekeeping, data tracking, and railroading Big Tech is imposing on us in the current mobile ecosystem. A new alternative is required to shape a better future. And F-Droid is excited to be a part of creating that new mobile ecosystem, together with our other partners in Mobifree. For an average user, it is difficult to discern how you are being tracked, where your private information is being saved, how it is being used and who it is being sold to. As a long-standing champion of user and developer rights, pushing for privacy over profit, F-Droid has always been committed to upholding high open-source standards in the technology we create. For more than 14 years, F-Droid has been developing solutions which act as pieces of the alternative mobile ecosystem puzzle. So it was a natural fit for F-Droid to become a contributing partner in the broader Mobifree project. Of particular emphasis is the impact this alternative mobile ecosystem will have on the services governments provide their citizens. Governments at all levels are providing services through mobile apps. And in many cases, mobile apps are becoming the preferred way to access important services. These apps are only available via the Apple App Store or Google Play. And installing apps from those stores requires agreeing to their Terms of Services. Both app stores were built on tracking users to sell their data, thereby giving Apple and Google power over how citizens receive services from their own governments. Even the governments themselves are beholden to the Big Tech gatekeepers: citizens and government officials and employees must use apps that are only published on Apple and Google. Austria provides a few specific examples of how governments interact with the current ecosystem at multiple levels. After COVID-19 countermeasures forced schools to adopt online learning, many public schools required education apps for their students, parents and teachers in order to stay connected. Public health insurance providers require a specific app called “Handy Signatur” to be downloaded in order for citizens to access their accounts online. People with the Handy Signatur app can sign petitions, and download vaccination certificates, sometimes required for work or leisure activities. Without this app, it is much more work and effort and is borderline impossible in some cases to engage in certain activities. Governments around the world are taking action to reign in the dominance of Big Tech. South Africa and the UK have changed their competition policy to include user freedom in what they regulate. Japan is working on new laws to open up their mobile markets. And even the US government and many individual US states have sued Big Tech to stop monopolistic behaviors. The EU is also taking a step towards creating distance between its citizens and Big Tech gatekeepers. They passed a landmark law: the Digital Markets Act, representing a whole new approach to tackling gatekeeper companies that aim to keep all sorts of competition out. It builds upon the successes of the EU’s General Data Protection Regulation (GDPR), taking an important step in a more ethical, democratic and citizens-first direction. All of these efforts are helping to open up possibilities for mobile users. And thanks to the Mobifree partnership, funded by an EU Horizon Europe grant, F-Droid can share open-source, privacy-driven solutions with a larger audience. In fact Europe is already seeing significant changes as a result of the Digital Markets Act (DMA). Google has shut out other app stores by ensuring that they have a third-class user experience. The DMA means that Google is now legally forbidden from giving preference to their own app store over alternatives like F-Droid. Additionally, Apple has opened up to external app stores for the first time ever. And while these are great first steps in the right direction, regulations and litigation do not build software. With this in mind, Mobifree is poised to take action on the new opportunities in the market, to build an unprecedented mobile experience for users and developers. One that centers around ethical practices, digital sovereignty, fairness, sustainability and inclusiveness. F-Droid will play a major role in this project, tasked with creating a decentralized distribution system for developers to deliver apps to Android users. The impact will be an opening of the app market for Android, improving honest competition around app development. And a foundational guiding principle is to provide privacy controls to users, without locking anyone out from participating. The app distribution system will focus on a 3-party interaction between app developers, app stores and app users where every party will have freedom of choice at all points of interaction. The system will have no terms of service or even user accounts to sign up for. Developers can publish their own apps, via their own repositories. Any app store can use those repositories to provide users with a method to install those apps. And if the app is open source, it can be included in the main F-Droid.org repository, where it will be reviewed using F-Droid’s proven ethical review process. Users will be given true choice in terms of their apps and app store preferences, additional privacy guardrails and increased transparency into what is happening with their data. F-Droid is one participating organization who has joined forces to help create this new mobile ecosystem. However, additional input, expertise, inspiration and work will be needed in order to break the traditional framework established by Big Tech. From community outreach to legal support, from developers, to researchers and end users, we welcome all forms of support. If you are a curious citizen interested in taking part in the Mobifree movement, we encourage you to reach out to us at f-droid.org and see how you can take part in this exciting mobile alternative. (We will be tracking work under this grant using the Mobifree label.)",
    "commentLink": "https://news.ycombinator.com/item?id=40501027",
    "commentBody": "Mobifree – An open-source mobile ecosystem (f-droid.org)164 points by jrepinc 4 hours agohidepastfavorite105 comments nh2 3 hours agoF-Droid still has some big tasks in order to be useable for the general public, when comparing it with e.g. Google Play Store. For example, if you search it for \"browser\", Firefox will not show up because A) It's called Fennec B) The F-Droid search seems to be exact infix search, and Fennec's title is \"Fennec F-Droid -- Browse the web\", in which \"Browser\" does not appear, only \"browse\". So it doesn't find it. In general, the search has no clue what a \"browser\" is, and cannot use it for ranking, so in practice you need to scroll a lot in F-Droid until you find what you seek. reply SushiHippie 11 minutes agoparentIt's called Fennec because it is not from Mozilla and includes patches to removs propietary bits (which are not allowed on F-Droid). https://gitlab.com/relan/fennecbuild And fennec was the 'internal' name for firefox on android from mozilla. reply janosdebugs 3 hours agoparentprevI think policies[1] are blocking F-Droid more than anything from being a viable alternative. A bank will likely never release their app as open source, nor will any of the big authentication vendors. This means that you cannot completely replace the Google Play Store with F-Droid if you have to use any of these, you can only use it as an additional store, at which point many won't be bothered. [1]: https://f-droid.org/en/docs/Inclusion_Policy/ reply mid-kid 2 hours agorootparentI'm not entirely sure why this is a problem. Nothing's stopping you from using f-droid as an app source as well as some other store - it doesn't need to cover every use case. It's already a viable alternative for apps which can't go on the Play Store due to Google's policies, like AdAway, NewPipe and Termux. The fact there's no freedom respecting bank applications is a problem on its own, though. I will continue avoiding them until there is... reply jraph 2 hours agorootparentprevIt's also its strength and raison d'etre. What use is F-droid if it starts including proprietary software? reply janosdebugs 2 hours agorootparentNo tracking? Providing an alternative? Not strengthening the Google-Apple duopoly on the mobile phone market? I was looking at it as a platform to publish an app on, which the open source requirement stopped dead in its tracks. I didn't have a viable open source business model for it at this time because it was a straight \"customer pays money, gets software\" deal with no tracking or other nonsense. As-is, F-Droid will not be an _alternative_, it can, at most, be an extension to the Play Store. reply jraph 2 hours agorootparentI won't trust no tracking if it's non free. I know I will sound extreme (although I think it shouldn't be the case in among developers) and no offense, but this sounds like a happy outcome to me. I believe proprietary software to be unethical. I'm not interested in being proposed non free software. The software industry should figure it out and start respecting its users. However software funding is a real concern and I do believe there's room for some set of builtin solutions allowing free software funding beyond a donate link. There are many ways to fund free software. I'm not saying it's easy but there are many open source businesses now and as a developer I decided to join one of them. Support, consulting and paid (but still open source) paid features are some options. I don't think anybody would remove a license check in an app sent to f-droid if it's free software. Now, I genuinely thank you for having considered an alternative to Google for distributing your software. I know it's hard to build stuff and bootstrap it. I'm with you on this. In f-droid, you can still add third party repositories and I heard it's getting easier for users. Not sure it is easy enough. Now I'm totally aware only free software doesn't cut it for most people yet, and fdroid will not suffice. Most people will do with the play store, but some will use fdroid as a store of \"trusty\" apps and resort to the play store if really needed. I know people doing this. reply janosdebugs 2 hours agorootparentIn this case the software in question was a game. I have not seen any visble business models for truly open source games under the acceptable licenses for F-Droid. There are source-available games, but that would not be enough in this case. I think the main issue may be how much up-front investment games need that would never be recouped with an open source license, but I'd love to be shown an example to the contrary. reply jraph 1 hour agorootparentGame is its own thing indeed and I'll admit that first, I don't play much, and second, never even started thinking about business models for open source games. Maybe you could license the data for a fee and have the code as open source? Of course this is low effort and you know way better than me. reply janosdebugs 1 hour agorootparentI've done a fair bit of open source work, but the best I could come up with for games is \"source available\" where you can get the code to recompile for yourself or mod it. (My wife and I put up the source code of our first game as a DLC on Steam as an experiment.) Re only code: Game engines often tie code and data together and the code without the data won't even compile in many cases, or you won't be able to produce a new dataset because documenting the required settings is a monumental task. Finally, there is no real benefit to open sourcing a game if you want to make money. There is no complementary product or service you can sell with it. Reviewing and incorporating community fixes requires work and potentially legal review as not everyone in the modding community is also necessarily well versed in IP law. All these things need funding which is not a given if the game is open source in any meaningful way. If a game is truly open source, a less than ethical company with a bunch of cheap labor can go and undercut you, so there is an incentive to make creating an alternative build as hard as possible, defeating the entire idea of open source. I would like to be able to release on an alternative storefront so people who want to degoogle their phones, but F-Droid's policies (most known alternative) make that impossible. (To be clear, it's a perfectly legit choice, I just wish there was a meaningful alternative to Google.) reply jraph 1 hour agorootparent(Not commenting most of your comment, I assumed a clear data and code separation because that's typically what doom does, but that's very old and I'm utterly incompetent in this domain) > Finally, there is no real benefit to open sourcing a game if you want to make money. the same thing can be said for pretty much all software (although it's not always actually true, sometimes open source is a selling point and/or have specific strengths beyond ethics) > a less than ethical company with a bunch of cheap labor can go and undercut you Isn't the data which is the most costly part in developing a game? Of course I realize you stated that separating the data is not practical. > I would like to be able to release on an alternative storefront so people who want to degoogle their phones Still 100% with you on this. Also degoogled with some proprietary software is still a step in the right direction reply janosdebugs 0 minutes agorootparentI wish that transparency and security were a selling point in gaming, but to me it doesn't seem like it. People will install all kinds of stuff and give them administrator permissions without a second thought. Anti-cheat eating itself into the kernel seems to be perfectly normal for most people. It's an entertainment product and people don't want to be inconvenienced for the most part. Regarding the development costs, I'm not exactly an expert and I have never worked in a game studio (apart from the company my wife and I are running), but let's take StarCraft 2 for example. If you were to have the engine, but not the art, you could likely easily develop a very capable multiplayer RTS game. Heroes of the Storm was developed out of a StarCraft 2 mod[1]. As another example, Stormgate[2], made by ex Blizzard devs, is getting a whole lot of press coverage for their netcode. It stands to reason that a good engine and netcode are very real competitive advantages in the RTS space. Other game types, such as a walking sim or an adventure game will have a lot less \"secret sauce\" in the code and more in the art, voice acting, etc. (The Invincible[3] is great in this area) so the code/data split would likely heavily depend on the type of game. (Games can also become very messy between art, visual scripting, engine settings and code, which is what makes releasing the code separately tricky.) My wife and I are (slowly, next to the day job) working on a Python programming/learning game and hopefully we'll manage to make a clean split between the engine and the art because it would be important for modding it. However, I wouldn't feel comfortable releasing it under an open source license because it would cut off a potential source of revenue to license it to educators wanting to make their own challenges and courses. Maybe later we'll figure out that the base game makes enough money and it doesn't matter anymore, but it's really hard to predict success. As a game dev I would really like to make it possible, for example, for game archivists to do their work legally, or for people to legally backup and rebuild their games for newer operating systems, but carving out specific exceptions, especially for unknown future use cases 10-20 years down the line is exceptionally hard and I'm also not a lawyer. [1]: https://starcraft.fandom.com/wiki/Heroes_of_the_Storm [2]: https://www.pcgamer.com/an-upcoming-rts-will-incorporate-the... [3]: https://store.steampowered.com/app/731040/The_Invincible/ xigoi 1 hour agorootparentprevWhy would a game need to be a business? reply jraph 53 minutes agorootparentBecause it takes time and quite some work to build one and there's nothing wrong wanting to live from it. reply xigoi 49 minutes agorootparentSure, but nobody is forcing you to publish on F-Droid. reply mid-kid 2 hours agorootparentprevI mean, f-droid allows adding custom repositories that don't follow the main one's rules. There's a decent amount of apps that do this for one reason or another. Other apps, like for example OSMAnd, have a special f-droid build with google services removed, leading to the lack of some features like android auto support. The client doesn't support any form of purchases and I don't think it ever will. I think you'd be best served using any of the other stores that support this or hosting it on your own website. reply jraph 1 hour agorootparentI don't see why fdroid would never provide a payment feature. At least for ethical reasons. Fdroid focuses on free software (and is also concerned with privacy and other features). Free software does not need to be gratis. As long as the payment code is open source. At worst it would be a NonFreeNet anti feature. Could be mitigated by supporting several payment platform. Of course it could be hard to set up in practice. reply lern_too_spel 2 hours agorootparentprevThat's what this article is about. Building a federated platform for distributing any type of mobile app, open source or not. reply Ajedi32 26 minutes agorootparentprevYeah personally that's what I love most about F-Droid. There's not a very wide selection of applications there, but if you can find an app that does what you want you're almost guaranteed that it'll be open source and free of ads, tracking, and other annoyances. reply ericjmorey 2 hours agorootparentprevAn alternative for app distribution outside of Google Play Store. reply repelsteeltje 1 hour agorootparentprev> A bank will likely never release their app as open source, nor will any of the big authentication vendors. I suppose you're right and I think it's worrying that precisely these kinds of organizations still seem to rely on security through obscurity (to some extent, not solely). reply t0bia_s 1 hour agorootparentprevI'm not using Google play or any other services on android over 6 years now. You don't need banking apps for making transactions. reply janosdebugs 1 hour agorootparentI live in an EU country and I, in fact, cannot use online banking without a Google-enabled Android or an iPhone. There is exactly one bank that offers a desktop authenticator, but we had a really bad experience with them when we tried them for a year. reply t0bia_s 1 hour agorootparentMicroG could work, but I never used banking app, so I cannot verify it. Adopt paying by other, more secure and private friendly ways. reply janosdebugs 29 minutes agorootparentWhat payment method would you recommend for paying mortgages, utility bills, and company expenses? reply izacus 3 hours agoparentprevThat sounds like they don't even deploy something as basic as Solr to drive the search :/ reply squarefoot 1 hour agoparentprevThe search for \"browser\" should have returned Mull, which is a privacy oriented deblobbed fork of Fennec. reply sanroot99 3 hours agoparentprevUse droidify a alternative fork of fdroid reply genpfault 1 hour agorootparent> droidify https://f-droid.org/en/packages/com.looker.droidify/ https://github.com/Droid-ify/client reply rjzzleep 2 hours agorootparentprevDroidify is so vastly superior it’s not even close. Fdroid still has the privileged extension whereas droidify and Neodroid can use Shizuku and other options. I do think that droidify is an unfortunate naming choice though. reply smusamashah 1 hour agorootparentThis still doesn't bring up Fennec if searched for 'browser'. Neo Store does. reply genpfault 1 hour agorootparent> Neo Store https://f-droid.org/en/packages/com.machiav3lli.fdroid/ https://github.com/NeoApplications/Neo-Store reply xnx 3 hours agoparentprevFennec is missing out if it didn't write its store description to include these keywords. reply AndrewKemendo 3 hours agoprevAt this point the access points and the global data and surveillance system is too complicated and interconnected to dis-intermediate the access point and the network itself That is to say you cannot even connect to IP addresses unless you’re already part of the global whitelist for APs Try yourself. Get a VPN and then connect from certain regions and to won’t get very far and you’ll be slooooow You’ll say, oh, that’s because bad actors and are therefore blacklisted - and now you’re back to the same problem: their resource is being limited by some social structure that is by definition constrained You cannot differentiate the broader system from the access point, so long as the system controls which access point access then it doesn’t matter what you do The only solution is to build an entirely new Internet from scratch, not controlled by the people currently google, amazon, cloud flare, respective ISPs, Netflix, etc… The Internet is 99% controlled by a small set of corporations that most people haven’t heard of Just try launching a competitive DNS service and see how far you get reply sneilan1 3 hours agoparent> The Internet is 99% controlled by a small set of corporations that most people haven’t heard of I would love to know more if you have time to elaborate. reply codexb 3 hours agorootparentSee this post that talks about how hosting email yourself doesn't really work anymore, despite it being an open internet standard -- https://news.ycombinator.com/item?id=32715437 Not sure about people never hearing about them, but the internet is no longer a network of sparsely distributed connected machines hosting and consuming content as it was originally designed. The content is all hosted on very small set of networks controlled by just a few companies -- Amazon AWS, Google, MS Azure, etc. The consumers access the internet through clients that are also controlled by these companies, and so it's easy for them to exclude other networks and hosts even though the network specification is technically \"open\". reply spacebanana7 3 hours agoparentprevIt all depends what your privacy threat model is. There are very few organisations capable of identifying the human user behind a Mullvad connection with a VM. If you're worried about local law enforcement or advertisers it's probably enough. reply hughesjj 3 hours agorootparentI'm more worried about future shakeouts and the end of anonymity, one ASN at a time. We're currently relying on AS' who 'play ball' with anonymous users, but that's not a requirement for AS operators by any means. reply sanroot99 3 hours agorootparentprevIsp often sell data, ip can be deanonymized by buying data from different data brokers and triangulating the ip. reply Aurornis 3 hours agoparentprev> That is to say you cannot even connect to IP addresses unless you’re already part of the global whitelist for APs > Try yourself. Get a VPN and then connect from certain regions and to won’t get very far and you’ll be slooooow Connecting to a far away VPN doesn’t prove anything about a “global whitelist” VPN addresses are commonly rate limited and block listed because they’re sources of abusive traffic and therefore trigger all of the common defense mechanisms. Anyone who has run a forum or other service long enough knows that the spammers and the people trying to evade bans love their VPNs. VPN services are also oversubscribed, leading to poor performance. Connecting to far away locations also causes throughput problems, especially if the country has poor internet infrastructure. Your connection has to round trip into and back out of that country to get to many services, meaning performance can be very poor. > Just try launching a competitive DNS service and see how far you get You’ll probably get as far as realizing that it’s an extraordinarily expensive venture to operate with no possibility of income, at which point you’d shut it down unless your hobby is lighting money on fire. There isn’t a conspiracy in this. reply AndrewKemendo 1 hour agorootparentI’m glad we agree so throughly though I’m not sure what you’re referencing with a conspiracy reply c-fe 3 hours agoprevI wish them success ... but with a website that looks like this I doubt their engineering efforts. https://mobifree.org (screenshot: https://imgur.com/a/TITC9bN ) reply ssivark 3 hours agoparentWhat about this website indicates low quality engineering? reply bogwog 1 hour agorootparentHN is mostly web developers I think, so they tend to look at everything through one particular lens reply glenstein 3 hours agoparentprevI fully support the values being expressed put the ratio of talk to action on the website is not great. It looks like a website from 2007 for a custom fork of a Ubuntu distribution. Again, I think the initiative, the funding, the rollout of technologies is the important thing and I'm perfectly fine with there being an aspect of public communication that gives it the kind of visibility that bureaucrats need so that it matches their own personal mental model of what a project is. So I at the end of the day those things, rather than the website, are what's important, but I could have hoped for a better first impression for sure. reply Apocryphon 2 hours agorootparentThere's something about the font that looks awful but having a 2007 look and feel is fine, in my opinion. There's something charmingly retro about the gradients and AJAX era of Web 2.0 before apps ate everything. reply veqq 3 hours agoparentprevI agree. It's a far cry from the greats' HTML 2 comparability. reply Grimeton 4 hours agoprevThey don't explain how they fix even one of the problems they mention at the top. Maybe I'm missing something here but this is just pure marketing bleh. reply afavour 3 hours agoparentIt is marketing, yes. But it’s also announcing a brand new ecosystem that doesn’t exist yet so I’m not too surprised they don’t have in depth implementation details right now. Marketing isn’t automatically evil. If you’re starting an open source project you want people to contribute to you’re going to want to market it. reply glenstein 3 hours agoparentprevThe first paragraph introduces the idea of a comprehensive initiative for an \"all of the above\" mobile ecosystem, listing out the key pieces of that ecosystem. Then there's a lot of mission statement and vision statement type things that appear to be directed at a broad audience, But then specifically they say this: >F-Droid will play a major role in this project, tasked with creating a decentralized distribution system for developers to deliver apps to Android users. It feels like that's kind of what F-droid does already (sans decentralization, unless we regard the capability of custom repos as adequate to the task). So I get the vision, I get the idea of it being under the umbrella of some broader project that's about other aspects of the mobile ecosystem such as operating systems, cloud services, and so on. I'm not sure what's different but I welcome the spirit of the initiative and I suppose that's the important thing. reply criddell 3 hours agoparentprevI believe F-Droid is operating under a series of grants totaling €4.4 million for the next two years. This release is probably directed at the various groups funding them. I hope they succeed but I wish they were using something other than Android. A by-Europeans-for-Europeans project shouldn't be based on an American OS IMHO. https://cordis.europa.eu/project/id/101135795 reply rc_mob 4 hours agoparentprevyes it had all the hallmarks of an seo campaign reply 6510 4 hours agoparentprevI thought the same but eventually I recognized the sound of the government wheelworks of bureaucracy. The article simply explains a single thing elaborately. reply porcoda 3 hours agoprevBasing something around a Google OS isn’t really an alternative to big tech. Given the reaction I’d expect if a Microsoft Open Source project was the basis of an “alternative to big tech”, I don’t see why something with a Google pedigree should be treated differently. reply fisian 3 hours agoparentAnd listing microG as an opensource alternative to Google services is disingenuous. It still uses the same Google servers to make requests, just the client is open. reply zaik 1 hour agorootparentIt seems like the user has a choice to configure microG to use different providers other than Google though. reply meiraleal 2 hours agoparentprevExpecting Microsoft to create an alternative to big tech. Did I miss the /s ? reply izacus 3 hours agoprevI can't figure out what \"it\" actually is. This blog post reads like something AI generated - what exactly are they talking about? reply glenstein 3 hours agoparentDecentralized mobile app distribution for Android. For my part I feel perfectly clear on what the \"it\" is but I'm a little unclear on how it's in a different from what F-Droid already does. reply izacus 3 hours agorootparentBut if you open the linked page, it has bunch of projects that aren't app distribution - /e/OS, some 5G edge VPN thing, etc. reply glenstein 2 hours agorootparentThose are mentioned in the first paragraph. Those are under the umbrella of Mobifree. The part about app distribution is specific to F-Droid. reply bogwog 1 hour agoprevThree chat apps. Why can't Quicksy's contact finding feature be added to Conversations instead of being a whole fork? Delta chat I kind of get why it's a separate thing since it works differently, but it still seems counter productive to have so many chat apps. reply joh6nn 56 minutes agoparentThose aren't meant to be apps representative of the Mobifree \"platform\", those are existing projects that have agreed to partner with Mobifree towards a common goal. They're bulletpoints in thd same list as Murena the E Foundation. As far as answering your actual question, you'd probably need to ask the project devs for Quicksy and Conversations? reply zaik 1 hour agoparentprev> Why can't Quicksy's contact finding feature be added to Conversations instead of being a whole fork? Quickly is designed to have easy phone number registration and contact discovery like the currently popular walled garden chat apps. Conversations works with any XMPP address and you need to ask people for their chat address (like email). reply wkat4242 2 hours agoprevNice! I see MicroG is part of this initiative too: https://mobifree.org/ I hope it becomes a bit more mainstream now. Some custom roms like lineage are very hostile to it because they're afraid Google will do more against them if they include it. You can't even mention it in their community or you will get kicked. It's nothing nasty though, it's just a bare minimum implementation of google play services remade in open source, withholding as much information as possible and giving you the ability to fake things like device model and attestation \"safetynet\". Luckily there's a fork from the microg team https://lineage.microg.org/ . So this is what I normally use. But it's a great middle-ground, you can still use google push and you replace all the other parts like the location DB with privacy-conscious alternatives (though with the abolition of mozilla's wifi location database this has become harder). I'm not a fan of /e/ though, especially because of the name that makes it impossible to google anything. And they are often years behind in android versions. reply udev4096 2 hours agoprevI wouldn't trust anything coming from /e/ because of their horrible track record. For instance, a few years back, their cloud storage had a bug which leaked private files to other storage accounts[0] [0] https://community.e.foundation/t/service-announcement-26-may... reply bogwog 1 hour agoparentThat's terrifying reply butz 2 hours agoprevIt would be great to have some basic, no nonsense Android applications, with least possible permissions possible and no internet connectivity. Some examples: SMS app, tasks, calculator, authenticator (2FA), weather, audio recorder, image/photo viewer, barcode/qr code scanner, calendar, file manager, music player, etc. Small app sizes, no Kotlin or support libraries. With very limited feature set, to keep maintenance efforts low. reply jwells89 2 hours agoparentOne issue I’ve read that devs of good, no-nonsense, telemetry-free offline-only/first apps face on the Play Store is that they get review-bombed and reported by their less scrupulous counterparts to drive more users to their ad-filled, IAP-heavy, or borderline malicious apps. This could be fixed pretty easily if Google took fraudulent reviews more seriously and their handling of reports were less automated and opaque, but they haven’t shown much interest in either. reply lawgimenez 2 hours agoparentprevWhat’s wrong with Kotlin? reply dhosek 3 hours agoprevI’m just thinking that the name is an invitation for Big Entertainment (in particular, LucasFilm, owned by Disney) to sue for trademark infringement since they have a trademark on “droid” (I remember seeing billboards for Android phones sold under the name “droid” back around 2010ish that had footnotes acknowledging the trademark and the licensed use of the term). reply ravenstine 3 hours agoparentDon't jynx it! ;) It would not surprise me if Disney lawyers simply have never heard of F-Droid. Given that they were willing to go after the image of Spider-man on some deceased boy's grave stone, it would not surprise me if they at least demanded some disambiguating language from F-Droid, that is if they don't outright ask them to cease and desist. Hopefully they realize it isn't worth their time. reply alabhyajindal 3 hours agoprevThe Mobifree link in the article: https://mobifree.org Looks sketchy to me. > Our values are different Why? > Big Tech: bad for people and the environment Bad for the environment how? Do they think if they operate at the same scale as big tech, they'll be able to do a better job of managing infrastructure? reply prmoustache 3 hours agoparent> Bad for the environment how? I guess this relate to smartphones being out of support way too early. reply wicket 3 hours agoprevIt seems that Mobifree largely depends on the Android/AOSP ecosystem, which is controlled by Big Tech. reply palata 3 hours agoparentBut being open source means that one could fork AOSP. Also the Google proprietary stuff comes with the Play Services, not AOSP. So AOSP is a lot better than the alternatives, I would say. I have a different feeling about Chromium: it is open source, but through it Google gets to control the future of the Web. But Android is not a standard, it's \"just\" one OS, so changes made by Google in AOSP don't really impact, say, iOS. reply wicket 2 hours agorootparentLet me know when the efforts of Mobifree lead to something that will allow me to install mainline Linux. reply j4hdufd8 1 hour agorootparentThere are already phones like that. Go for it. reply realusername 2 hours agorootparentprev> But Android is not a standard, it's \"just\" one OS, so changes made by Google in AOSP don't really impact, say, iOS. That's interesting because I personally do believe that Android is kind of the mobile standard, it has everything to be one, from the wide variety of devices to the certifications of conformity. reply reify 2 hours agoprevI fully support this work. I actually use both Lineage and E/OS. In my experience of installing E/OS on non tech savvy users phones, it takes just a short while for them to get the hang of it. They of course still want some of the non private data collectors on their phones and thats OK. I for one am convinvced that the more users we can encourage to move over to alternative android OS's the quicker the word will spread. We must not forget that these poor people have been brainwashed into thinking there is only one way to access what they like to use and engage with online. I usually set the E/OS app store to not use open source apps and install droidify to manage those. It is of course very easy for us who have the knowledge, experience and the confidence to install new OS's on android, but we were once beginners, everyone is. Its easy for us to know the difference between Fennec and Firefox. However, Its like me asking you to tell me the difference between the Freudian psychoanalysis and Lacanian psychoanalysis. Its easy for me but not for you. Even the most tech savvy are difficult to convince: One of my friends, who used to work for IBM as an engineer back in the day wants to get a new laptop with windows 11. I said \"what are you doing you muppet install linux\" He is under the impression that linux has changed very little over 25 years and its all terminal, no GUI or support for things like 3D printers even though I tell him otherwise. I personally dont use F-droid anymore I use Droidify which is simpler to use especially adding repositories. It is a pain adding repo's to F-droid. reply flettcher 3 hours agoprev\"This project has been made possible by funding from the European Commission's Next Generation Internet (NGI) initiative: www.ngi.eu\" reply nradov 3 hours agoparentEU funding is the kiss of death for technology initiatives. I can confidently predict that this will go nowhere. reply mitchitized 3 hours agorootparentWhy? Do they mandate process or procedure policies that kill productive organizations, or what? I'm genuinely curious. reply nextos 3 hours agorootparentI've been part of some large consortia. Generally, there is a ton of paperwork. Lots of project partners focus on filling in this paperwork to pretend they are doing something and pocket grant money. But, usually, nothing good is delivered, aside from some expensive DOCX. With that said, things are changing a bit and becoming a bit more efficient and dynamic. reply fabrice_d 2 hours agorootparentThat was the case for big EU funded projects with lots of industrial + accademia participants. I worked at a place where we had people working full time on setting up proposals for these projects. This was a very political game (find partners in countries the EU wants to help this year!), and very low risk too for big partners. We were usually \"selling\" stuff that had been done already, and the new funding was used to work on new stuff. That meant we could always show successful projects, which in turn helped getting new funding - rinse and repeat. NGI is different, they seem to focus more on funding individuals and open source projects with relatively small grants. This has good and bad sides of course - a more level playing field, but makes it still difficult for projects that require long term work. reply rglullis 2 hours agorootparentprevNLNet is an NGO and has been funding a lot of cool stuff. reply nextos 3 hours agorootparentprevYeah, public grants in the EU never work very well. However, some of the consortia members have already delivered good software. Personally, I think someone should have bought Jolla and open-sourced Sailfish and their Android VM. Perhaps turn it into a foundation, like Signal. reply Muromec 1 hour agorootparent>consortia Wait, there is more than one? reply jqpabc123 4 hours agoprevproject works to give European citizens and organizations more choice Ok but why the geographical restriction? reply afavour 3 hours agoparentBecause European governments are passing/have passed regulations forcing big tech companies to open up their platforms. (it’s also taking funding from the EU, doesn’t necessarily mean they’re forced to cater only to EU citizens but the combination of these two factors means it makes sense to) reply rs999gti 3 hours agorootparent> passed regulations forcing big tech companies More Euro governance, bleh. reply palata 3 hours agorootparentHave a look at what the Digital Markets Act is, before you judge. reply afavour 3 hours agorootparentprevThat's a pretty reductive take. Is the European Court of Human Rights \"More Euro governance, bleh\"? reply its_ethan 3 hours agorootparentI'm not gonna claim to know much at all about the ECHR, but this compliance graph from Wikipedia makes it look like a symbolic institution, but one that is largely ignored/ not really respected? Especially given that it's a declining trend, it seems like they may just be doing more and more political theater type stuff instead of \"real\" work. Anyway, it seems like it could somewhat reasonably fall under \"governance bleh\" depending on who you ask. https://en.wikipedia.org/wiki/File:Compliance_with_all_compl... reply _heimdall 3 hours agoparentprevUnless I've missed a very recent change, F-Droid isn't geographically restricted. Much of the blog post focuses on the DMA and its impact on Europe. I think the phrase you keyed in on, and really the bulk of the post, is just a marketing push for the European market and a bit of SEO trying to get keywords and phrases in there. reply permanent 3 hours agoparentprevPerhaps because EU just passed a law for third party app stores? reply wyldfire 3 hours agoparentprevIt's not restricted, it's a reflection of who funded the effort. The world can benefit from EU's investment here. reply jqpabc123 3 hours agorootparentSo instead of \"project works to give European citizens and organizations more choice\" it should more accurately say \"project works to give *the world* more choice\" reply fsflover 2 hours agoprevI prefer these mobile operating systems not dependent on the big tech: https://pine64.org/documentation/PinePhone/Software/Releases... reply talldayo 2 hours agoparentPeople in hell want ice water. reply rvz 3 hours agoprev [–] If this is the best that open source has against the Apple and Google duopoly, then we still have no chance and we’re still decades behind. reply talldayo 2 hours agoparentYou should consider actually trying it, before you confidently write it off. F-Droid is amazing and the single-greatest force keeping me off the official Google Play and YouTube apps. I don't suffer through ad-enabled PDF readers or subscription-service native apps; it's all just free software, and it beats the pants off every other mobile software store I've ever used. It is actually that good. reply boxed 3 hours agoparentprev [–] Isn't this just an android distro though? So just supporting the Google side. reply adamomada 3 hours agorootparent [–] I think it’s just alternate app distribution on existing devices. I get it that you have to choose one or the other, and it does make sense since f-droid has a bunch of open-source apps available already. They just need to work on the hardware and operating system now reply boxed 5 minutes agorootparent [–] I mean... there are open source apps on the iOS app store too. I have several for example: https://kodare.net/apps.html reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mobifree is an open-source mobile ecosystem designed to offer an alternative to major tech companies like Apple and Google, focusing on privacy, democracy, and ethical practices.",
      "The ecosystem includes operating systems, app stores, and cloud services, with F-Droid contributing to a decentralized app distribution system that promotes competition and user choice.",
      "Supported by the EU's Digital Markets Act and Horizon Europe grant, Mobifree aims to create a fair, sustainable, and inclusive mobile experience, encouraging community participation to challenge the dominance of Big Tech."
    ],
    "commentSummary": [
      "F-Droid, an open-source app repository, faces challenges in becoming a mainstream alternative to Google Play Store due to poor search functionality and the exclusion of proprietary apps, limiting its appeal for essential applications like banking.",
      "The discussion covers ethical and funding challenges between proprietary and open-source software, complexities in game development, and difficulties in app distribution outside major platforms.",
      "Despite these issues, F-Droid is valuable for apps restricted by Google's policies, and the conversation explores decentralized app distribution systems, EU regulations on big tech, and environmental concerns of smartphone lifespans."
    ],
    "points": 164,
    "commentCount": 105,
    "retryCount": 0,
    "time": 1716906264
  },
  {
    "id": 40502090,
    "title": "Reproducing GPT-2 Model in 90 Minutes for $20 Using llm.c",
    "originLink": "https://twitter.com/karpathy/status/1795484547267834137",
    "originBody": "# Reproduce GPT-2 (124M) in llm.c in 90 minutes for $20 ✨The GPT-2 (124M) is the smallest model in the GPT-2 series released by OpenAI in 2019, and is actually quite accessible today, even for the GPU poor. For example, with llm.c you can now reproduce this model on one 8X… pic.twitter.com/C9GdaxGPhd— Andrej Karpathy (@karpathy) May 28, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40502090",
    "commentBody": "Reproduce GPT-2 in llm.c (twitter.com/karpathy)161 points by tosh 3 hours agohidepastfavorite36 comments karpathy 2 hours agoHi HN the main (more detailed) article is here https://github.com/karpathy/llm.c/discussions/481 Happy to answer questions! reply lagrange77 1 hour agoparentThank you for the effort you put in your educational work, it helped me and others a lot! In fact, i'm training my nanoGPT version right now. :) > Ultimately my interest in llm.c is to have a nice, clean, minimal, super dependency-light repo in direct C/CUDA implementation, which I find aesthetically pleasing. Also, it's awesome that you spend your time on your passion. Any plans on making a video series on llm.c? :D reply karpathy 1 hour agorootparentYes definitely. Related tweet of mine: https://x.com/karpathy/status/1760388761349927356?lang=en 1. Build the thing 2. Build the ramp Currently on step 1 :). It helps to build it first so you know where you are going, and then you can more easily re-build it when you're vector pointed at the end result. reply lagrange77 50 minutes agorootparentThat's fantastic. My gradient field is pointing towards it. Thank you again! reply htrp 33 minutes agorootparentprevEverytime you take gardening leave, you build something new and interesting! reply ngiyabonga 2 hours agoparentprevHi Andrej! First, thank you for your teaching, it has helped me a lot, didn't think I'd ever have the chance to say thank you, but here you are and I hope this gets to you! Question - what's a relevant (05-2024) baseline to compare the performance of c code to? Back when you made nanoGPT you were seeing \"the file train.py reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training\". So twice the memory on the c node, but unsure of data size /epochs, any other details I may be missing. I.e. what's the net uplift of running c vs \"legacy\" torch code? Thanks again for everything. reply karpathy 1 hour agorootparentThe baseline is definitely PyTorch (or JAX), and indeed something like nanoGPT. I just never got nanoGPT \"past the finish line\" of really crossing the t's and dotting the i's and reproducing the models with as much care as I did now and here in llm.c, and getting to the point where it's a single launch command that just does the thing. I think I'll try to develop the `train_gpt2.py` inside llm.c to be that, so that we have the two implementations exactly side by side, and it's all nice and comparable. The C/CUDA code is currently a little bit faster than PyTorch (last time I measured ~2 weeks ago it was about 6% faster), and I think we can push this further. This is done by manually hard-coding a bunch of fusions/optimizations that are non-trivial for torch.compile to find (e.g. our FusedClassifier). But PyTorch has some pending work/PRs that will also speed up their side a lot. Ultimately my interest in llm.c is to have a nice, clean, minimal, super dependency-light repo in direct C/CUDA implementation, which I find aesthetically pleasing. And on top of that, educational, i.e. using all of the above as an endpoint of an intro LLM course. reply 363849473754 1 hour agoparentprevYou might have covered this topic before, but I'm curious about the main performance differences between nanoGPT and llm.c. I'm planning to take your \"Zero to Hero\" course, and I'd like to know how capable the nanoGPT chatbot you'll build is. Is its quality comparable to GPT-2 when used as a chatbot? reply karpathy 56 minutes agorootparentZero To Hero doesn't make it all the way to a chatbot, it stops at pretraining, and even that at a fairly small scale or character-level transformer on TinyShakespeare. I think it's a good conceptual intro but you don't get too too far as a competent chatbot. I think I should be able to improve on this soon. reply 363849473754 1 minute agorootparentThanks! So, you are considering expanding the Zero to Hero series to include building a basic GPT-2 toy chatbot? I believe you mentioned in one of the early lectures that you planned to include building a toy version of Dalle. Do you still have plans for that as well? reply 1024core 2 hours agoparentprevThank you, from an appreciative reader! reply sturza 1 hour agoparentprevDo you think grokking leads to proper generalized reasoning? https://arxiv.org/abs/2405.15071 reply bilsbie 1 hour agorootparentAny tips on understanding grokking? I’m not following that paper. reply sturza 41 minutes agorootparentGrokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. Overfitting and being cool about it and some new behavior might emerge. reply espadrine 1 hour agoparentprevHow big of a perf improvement would result from using the architectural tweaks that Llama3 and others have put in place since GPT-2? reply karpathy 1 hour agorootparentMy understanding and suspicion is mostly less than you think. Llama 3 architecture has the following changes on GPT-2: 1. delete the absolute positional encoding and replace with RoPE 2. delete all biases in all layers (in LayerNorms, they turn into RMSNorm) 3. GeLU -> SwiGLU non-linearity in the MLP 4. longer context length 5. architecture hyperparameter changes, e.g. slightly different aspect ratios And there was a paper that I can't find the reference to anymore that claimed that if you train long enough, the gap becomes even lower. Possibly because the absolutely positional encoding has enough time to train more fully, where as the RoPE layer benefits from the \"inductive bias\" it adds in the earlier stages of training. But I don't have full confidence on the above claim, maybe someone has tried or has better/concrete reference. reply benterix 2 hours agoprevI just hope than in a couple of years we'll see a submission here titled \"Reproduce GPT-4 on legacy RTX 4090.\" Because currently even with open source (?) models we are still consumers, and the training is still the domain of the rich. reply vineyardmike 1 hour agoparentWe won’t ever get there or need to because GPT-4 wasn’t trained on one GPU it was trained on thousands. The (most likely) biggest meaningful difference between -2 and -4 is the number of parameters and the training data/duration. I don’t think you’d really learn much more. reply ravetcofx 2 hours agoparentprevAccessing the dataset to train from scratch will be the biggest hurdle, now a lot of the pile has had ladder pulled since GPT-4 reply GaggiX 1 hour agorootparenthttps://huggingface.co/datasets/HuggingFaceFW/fineweb has 15T cleaned and deduplicated english web data tokens. reply ravetcofx 37 minutes agorootparentHoly crap, Does huggingface charge for bandwidth if you're downloading 45 terabytes?? reply drexlspivey 17 minutes agorootparentI believe they are hosting it on Cloudflare who doesn’t charge for egress reply fragmede 2 minutes agorootparentMore specifically, Cloudflare R2 doesn't charge for egress, and Cloudflare doesn't charge for egress to members in the Bandwidth Alliance which include Azure, Google Cloud, Oracle, Alibaba Cloud, and others, though critically not AWS. They very much do charge egress fees elsewhere. meiraleal 1 hour agorootparentprevI'm okay with paying for datasets reply CamperBob2 1 hour agorootparentDepends on how the courts rule. If the copyright maximalists prevail, only the wealthiest entities will be able to afford to license a useful data set. Paradoxically enough, this is the outcome that most \"Hacker News\" denizens seem to be rooting for. reply meiraleal 1 hour agorootparentI'd still get most of my dataset from torrent but I could pay for specific things like high quality source code. reply CamperBob2 1 hour agorootparentprevSomeone will come along and say \"Why don't you just mirror Anna's Archive?\" in 3...2...1... reply exe34 1 hour agorootparentprevi suppose you wouldn't be able to use it for external services, but internally, I'm sure you can find some books that fell off the back of a truck... reply HeatrayEnjoyer 1 hour agorootparentNo reason you can't go external. GPT was trained using ebook torrent sites reply artninja1988 32 minutes agorootparentOpenAI has enough money to hire lawyers to defend it until the end of time though reply auspiv 31 minutes agoparentprevConsidering it takes 8x A100 GPUs (80GB VRAM) to train GPT-2, I think it'll take far more than a single 4090. reply bufo 17 minutes agorootparentThe RTX 4090 has about the same BF16 Tensor Core TOPs than the A100, assuming 50% MFU (like the A100 40 GB PCIe) it would take 8x longer on 1 RTX 4090 vs 8x A100 80GB SXM, so 12 hours. Datasheet here for the TOPs https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvid... 50% MFU should be achievable on the 4090. reply anthonix1 13 minutes agoparentprevFWIW, I'm seeing ~318,000 toks/sec throughput on a 4x AMD 7900 XTX machine (less than $4k worth of GPU), using the same settings as in the post (0.5M batch size etc). reply Invictus0 1 hour agoparentprevI'm not saying this to be rude, but I think you have a deep misunderstanding of how AI training works. You cannot just skip the matrix multiplications necessary to train the model, or get current hardware to do it faster. reply notg963 37 minutes agoprevDo you have plans to create videos for the llm.c? reply indigodaddy 2 hours agoprev [–] Looks like this is re: training, but wonder how inference would be on some garbage older machine with no GPU on this model? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Andrej Karpathy tweeted about successfully reproducing the smallest GPT-2 model (124M parameters) using llm.c in just 90 minutes for a cost of $20.",
      "The GPT-2 model, originally released by OpenAI in 2019, is now accessible to individuals with limited GPU resources, making advanced AI more democratized.",
      "This achievement highlights the potential for cost-effective and efficient AI model reproduction, broadening access to powerful AI tools."
    ],
    "commentSummary": [
      "Andrej Karpathy is developing \"llm.c,\" a minimal C/CUDA implementation of GPT-2, focusing on aesthetic and educational value.",
      "The current C/CUDA code is approximately 6% faster than PyTorch, with room for further optimization, aiming to simplify the training process akin to PyTorch's nanoGPT.",
      "Karpathy plans to release a video series on this project, discussing potential architectural improvements and challenges in accessing large datasets, generating significant community interest."
    ],
    "points": 161,
    "commentCount": 36,
    "retryCount": 0,
    "time": 1716911926
  },
  {
    "id": 40501021,
    "title": "Grooved: Free App for Turntable Calibration Using Your Phone's Microphone",
    "originLink": "https://grooved.okat.best/",
    "originBody": "Hey there!I made a little app that lets you to calibrate your turntable by putting on any record and tapping a button. It&#x27;s called Grooved and it uses your phone&#x27;s microphone to see how fast your platter is going, almost like magic.You can see what it looks like in action here: https:&#x2F;&#x2F;twitter.com&#x2F;OKatBest&#x2F;status&#x2F;1795453042994680148The app itself is free without ads, subscriptions, or trackers. It&#x27;s a tool I built for myself, and I just thought someone else might want to use it too. I have never seen this technology being used before, all other apps require you to either print something and use the camera, or to place your phone on the spinning platter and use the accelerometer.You can grab it on the App Store, and I am working on an Android version I hope to release at some point in June.Would love to hear what you think about it!Ivan_",
    "commentLink": "https://news.ycombinator.com/item?id=40501021",
    "commentBody": "I made a free app to calibrate your turntable by simply playing a song (okat.best)159 points by okatbest 4 hours agohidepastfavorite117 comments Hey there! I made a little app that lets you to calibrate your turntable by putting on any record and tapping a button. It's called Grooved and it uses your phone's microphone to see how fast your platter is going, almost like magic. You can see what it looks like in action here: https://twitter.com/OKatBest/status/1795453042994680148 The app itself is free without ads, subscriptions, or trackers. It's a tool I built for myself, and I just thought someone else might want to use it too. I have never seen this technology being used before, all other apps require you to either print something and use the camera, or to place your phone on the spinning platter and use the accelerometer. You can grab it on the App Store, and I am working on an Android version I hope to release at some point in June. Would love to hear what you think about it! Ivan_ nickthegreek 4 hours agoJust put a record on yesterday and my wife said it was playing too fast. I thought she was wrong, then I see this post today! I grabbed the app and gave it a go, 3.5% too fast according to your app. Now I have to figure out how to adjust the damn thing. Really appreciate a free nonsense free app. update: youtube to the rescue. pretty common affordable turntable( Audio-Technica LP60), so I figure I would put the vid link here for others who might want to know - https://www.youtube.com/watch?v=d3PdS2V8Jz0 reply kazinator 3 hours agoparent3.5% is a lot! One semitone is a bit shy of 6 percent.s It's between 59 and 60 cents: more than a quarter semitone. Exact value: 1> (* 1200 (log2 1.035)) 59.5569212695205 So 59.6 to 3 sig figs. Someone with a good pitch memory of a tune (hearing in their head how it should sound) would spot such a difference. Especially if they heard the tune recently. reply criddell 2 hours agorootparentSo many recordings (especially before 2000) seem to be sped up a little. I can’t tell by listening to the song but when you try to play along with a guitar, it sounds terrible. Sometimes it helps to find a live version on YouTube. reply taylodl 1 hour agorootparentCorrect. That was often done when mastering with tape. Record at one speed, playback at another to make it sound more \"lively.\" There are songs that were sped up that way anywhere from 25 to 40 cents. It's another reason why live music so often doesn't sound like the recording. When using digital mastering, you can also adjust the playback speed, but you can correct the shift. You get more \"liveliness\" without the shift change. That's why you're not hearing the pitch shift so much in more recent music. reply okatbest 3 hours agoparentprevHey that's awesome to hear, thanks for trying it out! This is exactly what I hoped for when building this app. Making a dead simple tool for people who want their records to sound better, but may not have the equipment or knowledge necessary to adjust it. reply tiborsaas 2 hours agoparentprev> Now I have to figure out I'm glad it didn't continue with where you'll sleep tonight :) reply FelipeCortez 3 hours agoparentprevhey, I have the same one and didn't know about these controls! I'll definitely try this out now reply crazygringo 3 hours agoprevThis seems really clever -- kudos! I'm curious how it actually works? At first I assumed you were comparing the vinyl track to the reference digital track from some streaming service and either analyzing the frequencies with FFT or the timing of peaks. But I watched your demo video and you don't need to tell it anything about the song. Which makes me think you're rather doing an FFT and simply trying to match how well the frequencies align to the well-tempered scale based on A 440 hZ. Which then leads me to three questions: 1) Obviously this wouldn't work if you were playing a standup comedy album or something? Or the drum solo part of a song? 2) Are essentially all albums perfectly in tune with A 440 hZ? For example with classical music, I understand that 442 is also used, baroque is sometimes at 415, and 432 has been common as well? I don't know about pop but I can't help but wonder if some artists have intentionally chosen something other than 440 over the decades. 3) I assume this won't work if the turntable speed is off by more than 2.8%? Since the distance from A4 (440 hZ) to A-flat-4 (415.30 hZ) is a decrease of 5.6%, and so if the turntable is off by more than half of that you'd be trying to align to the wrong note? reply datpiff 3 hours agoparent> Are essentially all albums perfectly in tune with A 440 hZ? For example with classical music, I understand that 442 is also used, baroque is sometimes at 415, and 432 has been common as well? I don't know about pop but I can't help but wonder if some artists have intentionally chosen something other than 440 over the decades. No they are not all tuned to 440Hz. This is really evident if you play an instrument and want to play along with an album. reply iaresee 2 hours agorootparentEvery. Rolling. Stones. Album. reply okatbest 37 minutes agoparentprevThank you so much! I spoke about why I'm not flat out saying how it works in another comment but let me answer your questions: 1) It actually might! Worth a shot I guess, but I don't have any comedy albums to try it out. I was able to get drum solos working fine 2) Was answered better by somebody else 3) I was able to get it to detect speed issues up to 9% off, beyond that it just stops working completely. Though that was in controlled environment so YMMV. If you see the sample vid I posted above, my player is roughly 4% off which is a lot but I genuinely believe a whole lot of people wouldn't notice that reply jerf 1 hour agoparentprevI would expect: 1. Detect wow: https://www.youtube.com/watch?v=kCwRdrFtJuE 2. Time wow. 3. Round to nearest record speed and report delta. I'd also expect that armed with a normal FFT algorithm that Ye Olde Bash On Algorithme Until Functionale would be fairly likely to work with reasonable effort, without having to get too \"signal processor-y\" on the FFT output. reply mlyle 1 hour agorootparentWow is wavering speed. A constant turntable speed error (that this helps you calibrate out) doesn't produce wow. reply jerf 24 minutes agorootparentAll turntables have wow. Wow is determined by the rotational speed. So if you measure 33.2326 wow oscillations per minute, you can know the turn table is a touch too fast. Wow would not be used to measure the error, it would be used to measure the absolute speed. Getting the error from that point is trivial. reply crazygringo 13 minutes agorootparentI don't think you could use FFT or similar to reliably measure wow to anywhere near the accuracy necessary to produce a reliable amount of correction from just a few seconds of an arbitrary LP. If you were using an LP with a known pure 440 hZ sine wave that you could lock onto exactly, then sure. That's kind of how frequency modulation -- FM radio -- works. But I really don't see how you could do this with songs that are full of frequencies coming in and out and changing all over the place. If you analyzed the full side of the LP, you could probably get enough signal out of that. But trying to measure the precise wavelength of wow when you're only getting a few wavelengths' worth, from a complex signal? I don't see how. reply tibbon 3 hours agoparentprevEven pop albums aren't all 440hz. Def Leopard's Pyromania is about a 1/4 step off standard tuning, and there are many other examples of similar. reply kazinator 3 hours agorootparentPeople sometimes can't even make backing tracks for improvisation that are in tune. reply sambeau 3 hours agoparentprevMy first thought was that it is listening to the scratches rather than the music, but I guess they speed up as the record gets played and the arm moves inwards. So, my second guess is that it's listening to the pitch of the notes being played. Of course, old albums (especially punk albums) are probably tuned to whatever worked on the day—maybe a tuning fork, possibly a piano, maybe just to one of the instruments. reply svantana 3 hours agorootparentUmm, no, scratches meet the needle once per revolution, regardless of the needle's position. So that would definitely be a possible method. But not all records have scratches, or they could be at an angle, which would give the wrong estimate. Also, actual sounds on the record could have scratch-like qualities. reply sambeau 3 hours agorootparent>scratches meet the needle once per revolution, regardless of the needle's position I need to go away and think about that :D reply kevincox 2 hours agorootparentThe needle doesn't pass at a constant \"ground speed\" everywhere on the record. On the outside the needle \"travels faster\" and in theory you have more detail. Near the middle the needle travels over the record surface slower. reply taylodl 1 hour agorootparentThat's one of the reasons you'd put your best song on side A, song 1 and your 2nd best song on side B, song 1 - you got the most detail on the outside track. I remember seeing an interview with Led Zeppelin quite a few years back where they were claiming they knew they'd have a smash hit with Stairway to Heaven. Which was funny, because I remembered then seeing an interview several years before where they said it was a track they never thought was going to go anywhere. Which is it? It's track position - 4th track on the A side, i.e. the last track, tells you how important they thought it was. Which is to say, it was a \"throwaway\" track that made it big. It happens. The other odd thing about that album is \"When the Levee Breaks\" is considered to be the 2nd-most popular song off of the album and it too was the last track on the B side. What this tells me is the album's producer, one Jimmy Page, was out of touch with what Led Zeppelin fans liked most. Based off the sound of the next album, Houses of the Holy, I'd say Mr. Page got the message loud and clear. reply derefr 3 hours agoparentprevYou can extract pitch information and compare it to a set of quantized reference pitches as an instrument tuning app does, sure. But you can also extract BPM info and compare that to a set of quantized reference BPMs that anyone would ever bother to use. And also, since you’re getting multiple pitches from multiple instruments in a time series, if you can isolate particular instruments, you can calculate the key the melody and/or harmony of the song is in. Then, you could either come up with a heuristic, or just train a Bayesian filter, using datasets of “real” and “erroneous” (key, BPM) pairs. reply crazygringo 2 hours agorootparentThere aren't really reference BPM's. Not everything is recorded with a click track, though a lot is. But even so, while it will usually be an integer BPM, it's not always rounded to the nearest 5. 120 might be too fast while 115 is too slow, so you pick 118. For an app that's trying to correct your turntable speed by e.g. 3%, it couldn't even begin to guess the \"correct\" BPM. reply derefr 1 hour agorootparentBut would a large-enough majority of songs be recorded with a multiple-of-5 BPM, that this app could presume the first record you play has a multiple-of-5 BPM, and be right 80+% of the time? If so, then that could still be how it was done here. It wouldn't always work for the first record, but you'd be highly likely to get a \"good reading\" by at least the third one you try. reply crazygringo 48 minutes agorootparentSo I found some data and the answer is -- definitely not: https://assets-global.website-files.com/6188e55dd468b56ab674... from: https://blog.musiio.com/posts/which-musical-tempos-are-peopl... There are plenty of songs ending in every integer. reply fwip 3 hours agorootparentprevAre there really \"standard\" BPM? As far as I know, most music production is just \"whatever tempo the band feels like playing.\" I haven't been in a recording studio, though, so maybe a use of a metronome is more common there? reply derefr 2 hours agorootparentI'm not in the music industry in any capacity myself, so take everything I say here with a grain of salt. And more specifically, to address your question, I don't know enough about the recording part of music production to say whether most performers get a click-track played into their monitors or not. (I know that I've seen at least some performers using click-tracks for studio recording; but I'm not sure if that's only given if they ask for it, or if it's the studio pushing it on performer to make their jobs easier. I imagine click-tracks would always be used for multi-performer async studio recording — as otherwise you'd have performances that have conflicting BPM. But maybe not for single-performer pop/EDM songs, where it's just a vocalist laying down a track, and then everything else is done in software?) The evidence I can cite, is from the outcome of the production process: I have a strong recollection from back in the late 2000s, of using some music library auto-tagging software that would, among other things, analyze the BPM of a song to populate the BPM ID3 field of MP3s. For at least the music I loaded into it, BPM did appear to be (mostly) quantized to nice, round numbers. (And it wasn't just the software being coarse-grained in its estimates; it did give weird numbers for unusually-produced songs.) I do know that for any modern \"produced\" music (i.e. music not recorded as a live ensemble session), even if the performers did lay down their tracks at a weird BPM, the audio engineer is still going to be throwing their performances as separate tracks into a DAW. And one of the things you tell a DAW, when creating a project, is the BPM — which creates a grid of bars that tracks/samples want to snap onto. Even if you're not trimming or speed-adjusting performances so as to snap the ends of each track/sample to the DAW's grid, you're still likely snapping the beginning of each track/sample to the grid — you'd have to fight the DAW to not. Which means that, by default, if the song is \"produced\" enough — tracks cut up and slid around, reused, etc — then the output of this moving-and-snapping process will be a song that reads as the project's BPM. Separately, I know that people learning to play an instrument for classical/orchestra music, often practice their instrument using a metronome (at least at the early levels.) Which perhaps gives at least some of those people the speed equivalent of perfect pitch — the ability to \"lock into\" certain BPMs; and to know if other performers are running fast/slow vs the \"expected\" BPM of the song. I would assume that even for rock performers with no classical training, the early-level \"textbook\" practice for drummers and rhythm guitarists also involves playing with a metronome. It might be a lot more tenable to play a rock song at e.g. 117BPM, if everyone's just playing on the down-beat of the drums; but these performers still don't want to have a sudden, unintended shift in their BPM due to not being able to do a drum fill / complex chord change quickly enough; nor to start rushing as the song amps up, forcing everyone else into increasingly-stressful playing. In doing drills with a metronome to keep their BPM constant, these performers are likely implicitly learning to lock into certain specific BPMs as well. My impression is that the one case where this probably isn't true, is in live jazz performance, and more specifically in live jazz improv \"jam sessions.\" In those, the percussion (if any) isn't driving the performance, but rather is just another component of the harmony, following the (often wavering!) BPM of the lead performer. (Is this why the classic meme of classically-trained performers not getting along with jazz artists? Because the jazz artists won't stay on beat, and this irritates some perfect-pitch-like \"out of tune\" feeling in the classical musician?) reply LeoPanthera 3 hours agoparentprevIt probably doesn't need to do an FFT, it just need to be able to count beats and have a database of BPM values for popular songs. You could use something like AcoustID/Chromaprint to identify the song you're playing. A very simple idea, as all the best ideas are. reply padenot 3 hours agoparentprevShazamKit, from that fetch the BPM, BPM detection from the mic, compare ? reply stavros 2 hours agorootparentIt works offline, though. reply pimlottc 32 minutes agorootparentDoes it? I don't see that claim being made anywhere. They do say that the audio is processed locally, but that does not preclude them from making an API call to find a signature match. > The audio stream is processed locally on your device and never recorded. reply egypturnash 2 hours agoprevAll the turntables in my life as a kid had built-in calibration, in the form of four rows of repeating black/silver squares on the side of the wheel, a light flickering at the mains frequency, and a little dial to adjust the turning speed until the row of squares corresponding to your current combination of 50/60 hz DC and 33/45 RPM appeared to stop moving. It was a very clever trick. reply taylodl 1 hour agoparentYou had access to nicer turntables as a kid! That was a distinguishing feature between the more \"serious\" music hobbyist and the person who just wanted to play some tunes. I haven't messed around with a turntable in over 20 years but these days, only the supposed \"serious\" music hobbyist would be using a turntable and I'd think anyone making a turntable that didn't have this adjustment would be laughed out of the market. What are you going to tell me next - they're also using belt drive, a plastic needle, and a tone arm with a non-adjustable counterweight?! :) reply egypturnash 1 hour agorootparentMy dad was a recording engineer, so he knew where to go to find the sweet spot between \"this sounds great\" and \"this sounds insanely fabulous but requires a second mortgage\". And yeah honestly I'm surprised you can get a turntable that isn't made for the audiophile market any more, but the one shown in the video for this app sure does lack any fine speed adjustment. Just a little button labeled 33/45. reply derefr 55 minutes agorootparent> And yeah honestly I'm surprised you can get a turntable that isn't made for the audiophile market any more It's because of the other re-emerged market for vinyl: the \"not-so-enthusiastic\" collectors, who just want the album \"on vinyl\" to say they have the album \"on vinyl\" or maybe \"in every possible format\"; or because they love the band and the vinyl is just what you happen to get when you buy the special collector's edition box-set; or because they want to display the vinyl album cover somewhere visible in their house as if it were a poster print; or because they consider \"a vinyl collection\" to be an aesthetic decor item, like some people consider a full bookshelf to be an aesthetic decor item. A lot of these are people in their teens/20s who don't actually think that vinyl sounds better, so don't have much enthusiasm for actually listening to the vinyl release (as they're mostly buying new vinyl, and these tend to come with a code for a digital copy as well.) So if they bother to buy a turntable, it's the $30 one they say at Urban Outfitters. They know they're just listening to the vinyl as a novelty experience, something they're only going to be doing a few times — and so they don't want to waste too much money on it. reply c5karl 1 hour agoparentprevI would think that any turntable that allows for adjustment of the speed of the motor would have this already (based on my limited experience with turntables from decades ago). So, nice little app, but if it shows you the error of a non-adjustable turntable, it won't have a lot of practical applications. reply hinkley 1 hour agoparentprevWe have replaced a lot of stepper motors with a more refined version of this using stripes and a photoreceptor. Printers for instance. Surely there must be turntables doing the same thing? reply crazygringo 2 hours agoparentprevI never knew what the heck those squares were for! Just thought they were some funny aesthetic choice from the 70's and 80's. Thanks for giving me the definitive answer for something that has always seemed odd to me. reply chriscjcj 34 minutes agoprevSo is this just a test to see if it's playing records at the right speed? Does it do anything else? Is it comparing it to a known-good recording of the song or is it measuring how many cents the song deviates from a natural key? If it's the latter, I can say that I have many many records don't play back in a natural key. I have several records (45's) in my collection whose hole is not punched perfectly in the center and the pitch wavers up and down as the record rotates. How would this app react to that and what suggestion might it make to mitigate? reply verelo 15 minutes agoprevSo this is super cool but i am questioning its reliability after playing a song from Spotify on my Sonos and then letting the app listen: the result was 0.1% too slow? I guess that’s a very small difference, what’s the degree of error to expect here? reply nickthegreek 3 minutes agoparentI just played Bad Religion - Incomplete on YT Music through Sonos, app said \"That's pretty much perfect\". Played the same track on vinyl, app reported 3.5% too fast. Dev answered elsewhere on the post that .01% is required for the perfect message. reply lukko 3 hours agoprevThis is cool - not sure if it's possible, but it would great if you could determine other things, like is the needle weight correct, or is the cartridge aligned properly (maybe left / right channel would be louder / softer??). I found the copy on the site a bit gross - I'm not sure something like this needs a critique of modern copywriting, and for me it gives that same weird sense of over familiarity ('we get you') that I think it's trying to criticise. reply dmd 2 hours agoprevI've tried this on a dozen or so records and all I get is \"Please made sure your device can hear the music, and that you're using a modern song with a clearly defined beat.\" (There is definitely a very well defined beat.) reply okatbest 1 hour agoparentThat's odd, could you share an example of a record you use? Are those fairly popular/common records? Also, do you see the little disk outline moving during the analysis? This would confirm that your phone can hear the sound coming out of your record player (just to be safe: you need to play the song out loud) reply dmd 1 hour agorootparentI've tried the White Album, Tiffany (I think we're alone now), and a bunch of songs on Cake Comfort Eagle. No, the disk is not moving at all! It's turned up pretty loud. reply okatbest 31 minutes agorootparentThat's super odd. So it's not even rotating? There might be an issue that occurs before the analysis starts. If you don't mind, do you have any special setting on? Lockdown mode or something? Are you able to play the music back if you use the Voice Memo app to record a bit of it? Sorry about all that, hopefully we can figure out what's wrong! reply dmd 19 minutes agorootparentYep, voice memo works fine, not on lockdown (not sure what that is). reply shudza 3 hours agoprevI've used to use an app (there are many) which measures angular velocity using accelerometer and displays RPM (you place the phone on the turntable). I can't see why would anyone need something other than that. reply hobofan 3 hours agoparentComplete noob here, but wouldn't the phones weight (especially with how big phones are nowadays) have a significant impact on the RPM by slowing it down? reply svantana 3 hours agorootparentIt's very possible, increased weight will increase friction in the bearing. However my iphone SE weighs almost the same as a 12-inch record (150g). BUT it is also more difficult to place symmetrically on the turntable, which also could affect friction. reply kazinator 3 hours agoprevI'm skeptical; you have no idea whether a random song from a record player is in tune or not, without a reference copy of that song from digital media. Is this zeroing in on traces of 60 cycle hum in the recording? Or else, won't it adjust an out-of-tune song to be in concert pitch? (Mind you, that would be extremely useful if you're playing along with the track and don't want to adjust your instrument, or cannot do so.) To get it to the correct speed, I would go through my record collection and find a tune with long, sustained notes. Find the same tune on Youtube and get a pitch reference for the note using a digital tuner. Then adjust the record player to match the pitch on that note, down to 1/10 of a Hz precision. (In the first place, why would you have a record player without a built-in strobe light for adjusting the speed in 2024, when this was a common item even in lower income households in 1984.) reply okatbest 25 minutes agoparentUnfortunately a lot of mid-range tables nowadays don't have the strobe (mine included), most likely for aesthetic reasons... Even just looking at online shopping right now I only see a handful that do have it. As for being skeptical, good! That means I'm doing something great according to Clarke's third law. You can see me testing out the app on multiple records here: https://twitter.com/OKatBest/status/1795453042994680148 The results aren't 100% perfect, and they definitely won't be as good as matching specific frequencies, but it'll get you very close to the ideal RPM. reply echoangle 2 hours agoparentprev60 cycle hum probably would not work, europe uses 50hz reply kazinator 2 hours agorootparentThat's a big enough discrepancy that the program could have heuristics to figure it out, particularly if the record player is in the right ballpark. Also west Japan is 50, east 60. The main problem is: (1) there isn't a lot of hum on professionally produced records; (2) but there may be hum from the equpiment itself. reply kernal 4 minutes agoprevMade with Flutter? reply gcp123 3 hours agoprevWhat a perfectly simple app! Going to try this out with my Technics SL1200 mk2 tonight. Thanks for sharing. P.S. Reading comments from a bunch of tech folks trying to wrap their heads around an app for calibrating analogue music playback device is very amusing. :-) reply bastardoperator 2 hours agoparentI assume this is for belt driven turntables, not direct drive. I also have a pair of 1200's. Most mixers these days have BPM built in, and before that you had red micro bpm counters and headphone amps to validate BPM. Never once on my 1200's did it read something outside of the producers BPM count when pitch was locked. reply miah_ 3 hours agoprevThis is a neat idea. I don't know why it needs to be an app on a phone though. I guess you \"know\" a phone has a microphone and the setup is minimal and you have a api for everything? It being a phone app leads me to believe the 'no data collected' bit is temporary as all phone apps eventually gobble data to sell. I have an Android so I can't try this out, as its in 'private beta' on Android currently. Would love to see this as a open source project that I can install on Windows or Linux though. reply okatbest 2 hours agoparentProbably a smart idea to be suspicious of new apps, though I will say I am someone who care deeply about privacy. You can take a look at my previous project Boop ( https://boop.okat.best/ ), which is open source and has been on the App Store for 6 years, yet I still don't have any analytics or data collection in it. I have no clue how many people use it beyond download figures I get from Apple. I have considered making this app open source, and it may still happen in the future, but I do want to release the Android version first. I'm worried that somebody will see my code, repackage it with a new name, beat me with better SEO (which would be easy considering how dumb the copy is on my website) and sell it for profit before I have a chance to reach a wider audience. This is something that did happen with Boop, not a hypothetical. The reality is that this app cannot collect anything valuable either way. It's a simple serving app that only uses your microphone for a couple of seconds at a time, and I assume most people will delete it once it serves its purpose. If I wanted to monetize it, the best way would be to add a \"Pro\" version with a subscription or to add ads, but I have no interest in doing either. This is a fun side project/portfolio piece, and I'd rather collect internet love distributing a ton of free copies, rather than sell a small fraction of that number of paid copies for $0.99 and have to deal with things like marketing, taxes, and customer support. reply dmd 2 hours agoparentprevFunny, I have the exact opposite intuition as you. It being a totally-sandboxed phone app means I can trust it not to collect any data without my knowledge, without me having to spend hours looking through the source code for some trick I might miss. reply gaudystead 4 hours agoprevI'm just here to say that I enjoyed that intro you gave to the website, and although I hate unitaskers in the kitchen, that intro helped to remind me of the magical wonder of them in the early days of \"The Modern Smartphone\". Cool app and thanks for sharing! :D reply virskyfan 1 hour agoprevGreat! In the past I've used the app that measures the turntable speed through its accelerometer...it's somewhat uncomfortable and weird to set the mobile on the turntable, start and stop once and again. Would love to see a comparison of measurements taken with your app and the one using the accelerometer...calibration is a huge thing in engineering. reply okatbest 1 hour agoparentI'm gonna be honest, using the accelerometer will definitely be more accurate because you can take more samples over a longer period of time. That's how this project started, but I also felt uncomfortable putting my brand new phone on a fragile player and even more so asking other people to do the same. Grooved is still very accurate, especially if you run multiple measurements to confirm on multiple songs/records. The threshold for \"perfection\" in the app is 0.01%, which to me anything beyond that is a rounding error and I could not tell the difference in my tests. This is equal to a drift of about a third of a second over an hour of listening. reply strongpigeon 2 hours agoprevNice work! This looks really good. I'm not in the intended audience as I don't have a turntable, but if I had one I feel like I would be much more likely to use this if it was a web app. If I didn't know about this app beforehand, I'm pretty sure I'd be looking on Google rather than the App Store. That being said, you made this app for yourself and it looks great. Congrats on shipping! reply plastic3169 3 hours agoprevLooks amazing and useful for me! The links don’t take me to the app store on my iphone and the app name is not in the heading I need to scroll past the copy to find what to search for. Minor annoyance but for some reason annoyed me a lot. That annoyance led me to hate the copy as I was searching for the app name and tried to skim the text. This is probably the first time I write any feedback to internet. I am usually not one that does that kind of thing haha. Ok, now I will search and install the app. I bet it’s great. reply okatbest 1 hour agoparentHey that's very odd, would you mind telling me which country/region you are located in? I actually released the app yesterday but didn't post about it til today because it wasn't being indexed properly on the App Store yet, it's possible that it's still not live everywhere. I have an image link to the listing in the first section of the web page and if on iPhone it should show you the classic \"this website has an app\" banner at the top. If you're searching for it, \"Grooved\" has many results but if you accept the suggestion of \"Grooved: turntable calibration\" it'll come up. This is a direct link to the store page: https://apps.apple.com/app/grooved-turntable-calibration/id6... Please let me know if it still doesn't work and I can take a look... Thanks! reply plastic3169 1 hour agorootparentI figured that the site breaks with apple lockdown mode on. I think that appstore links usually work still, but might be that that’s one of the lockdown mode protections. The spinning animation also was not there so I didn’t see the app logo there. reply okatbest 48 minutes agorootparentThat's very good to know, I didn't even think to test it with lockdown mode! I'll see if I can add a fallback page if the content is missing. Thanks! reply patrakov 4 hours agoprevDoes it rely on the fact that music has a concept of notes with well-defined frequencies and that there is no such thing as 216 Hz (must be 207.6 or 220 Hz) when the music is played on a properly tuned instrument? Or does it use other heuristics? reply astral303 4 hours agoparentI tried it out and the instructions have tips on what record to pick, they say to pick a well known version of a song (like not a live version etc), and preferably song with a beat, but it says it doesn’t use any 3rd party APIs or libraries, only Apple APIs. So my guess at to what it’s doing is using a ShazamKit recognition behind the scenes and looking at the frequencySkew value of the matched result. It also gives you one answer after listening, instead of a continuous gauge, which seems to corroborate song recognition. It probably won’t work with an obscure record that is not Shazamable. And so I don’t think it can measure wow & flutter as a result. Still pretty cool for those that need to calibrate a turntable, or verify 33 vs 45 PRM for a record. reply pimlottc 35 minutes agorootparentI love that everyone is guessing all these methods of detecting pitches or using the camera to count rotations, and it turns out that they're most likely literally just using a built-in API and displaying a return value [0]. 0: https://developer.apple.com/documentation/shazamkit/shmatche... reply stavros 1 hour agorootparentprevHow can it use ShazamKit if it processes everything on-device? I'm really really curious how this is done now... reply pimlottc 38 minutes agorootparentThey don't claim everything is done on-device, just that the audio stream processing is. > Grooved does not collect any data, whatsoever. The audio stream is processed locally on your device and never recorded. Which is consistent with how ShazamKit works [0]: > Audio is not shared with Apple and audio signatures cannot be inverted, ensuring content remains secure and private. 0: https://developer.apple.com/shazamkit/ reply netsharc 3 hours agorootparentprevI guess a phone's camera can be used to count the RPM, e.g. by filming the label and counting how many milliseconds it needs to do a full rotation (1818ms for a 33 RPM record) reply phreeza 47 minutes agorootparentI tried to build a prototype of something like this with opencv once, didn't get it to work reliably though. I have a feeling there should be a relatively simple signal processing version of this, basically a spatiotemporal Fourier transform, that should solve it. reply franky47 4 hours agoparentprevA \"properly tuned instrument\" still requires a base frequency (usually defined for A4). Whether to use 440Hz or another slightly different frequency is a fun debate to get into. reply datpiff 4 hours agorootparentOlder recordings on analog media also can have timing error at the recording stage or duplication stages. reply thekegsi 3 hours agoprevYou can get apps where you place your phone on the turn table and it directly measures the turntable speed using either gyroscope or compass https://play.google.com/store/apps/details?id=com.vinylrpm&h... reply okatbest 1 hour agoparentThis is how this project started! I thought I was super smart figuring out I could do that, then I realized there are many apps that already do the same thing. Eventually I decided to see if I could find a novel, more user-friendly approach that doesn't require you to put a thousand-dollars phone on a fast moving spinning thingie, and that's how Grooved came to be. reply zamadatix 2 hours agoparentprevHere I was thinking it'd be smart to have the app use the camera to figure out how fast the middle label is spinning around but this seems way simpler now that you mention it. reply shiandow 4 hours agoprevThis just tells you the speed, not the variation in speed over time? reply okatbest 4 hours agoparentThat's correct. The analysis takes a few seconds so even if I take more samples, it all gets averaged over time and won't be able to do instant read. If you're interested in measuring wow and flutter I think you'll need to look at more traditional calibration methods unfortunately. reply gen3 4 hours agoprevNeat POC, are you doing a song fingerprint then comparing speed/stretch? reply echelon 4 hours agoprevI'm not an expert on vinyl. The only record players I've interacted with have only had two speeds. Do most of them do variable spin rates? How frequently do the rates get out of sync? What causes it? Do they vary on an album by album basis? reply okatbest 4 hours agoparentSo you have multiple speed settings (33.3RPM, 45RPM etc.), but most if not all players also have a little adjustment knob to tweak those settings more precisely. Good players will have it on the front, and will have a strobe to help you adjust that (the little dots you see on the outside of the platter on fancy players). However, cheaper players may not have an easy way to adjust, and some (like mine) hide the knob under so it's a pain to adjust. Cheaper players are more likely to come misconfigured from the factory. I asked a lot of friends (who are amateurs, not audiophiles) to test this app out and every single one had a turntable that required adjustment. Mine's running at 34.6 RPM which is just 3% off, but when listening to my favorite albums I can definitely tell that it's too fast and too high pitched. The speed will also sometimes drift slowly over time, which is why it took me a while to notice my table was off. If you have a turntable, you can try listening to the same song from a record and from a digital release, and see if you can hear a difference. If you do, I know just the app for you! reply beAbU 4 hours agoparentprevFancy record players have a trim setting where you can fine adjust the speed up or down a semitone. But all of them have a built in stroboscope/calibrator to help get the speed right. I think the drag of the needle impacts the speed, hence the desire to tune it per record. Audioplebs need not apply. reply CaptainOfCoit 4 hours agoparentprev> I'm not an expert on vinyl. The only record players I've interacted with have only had two speeds. Do most of them do variable spin rates? Lots of turntables have variable speed, DJs can tell you more about it :) > How frequently do the rates get out of sync? What causes it? Do they vary on an album by album basis? When you're DJing, one of the main things you'd deal with, is making things be in sync, one way or another. I'm not sure how much value you'd get from an app like this that seems to be more \"fire and forget\" and not for on-the-fly matching. I'm guessing this app is for people who do their own maintenance, repairs and such of turntables, so they can easily verify their work. reply hnlmorg 4 hours agorootparentI don’t think this would be aimed at DJ turntables. It doesn’t matter if those things drift because you’re never playing anything locked in at a specific RPM. Plus DJ turntables tend to be direct drive, which should be pretty consistent. Where as consumer record players tend to be more belt drives. And I’m assuming those belts can stretch with time? reply CaptainOfCoit 3 hours agorootparent> I don’t think this would be aimed at DJ turntables I agree :) > Plus DJ turntables tend to be direct drive, which should be pretty consistent Indeed, but once you start doing repairs and/or modifications, things can get fuzzy. Maybe the app would help those people? reply LM358 1 hour agorootparentPeople who are serious about turntable repair have calibration records with sine waves for that purpose. reply yial 4 hours agoparentprevSo really short description, you have 33, 45, and 78 rpm. Most record players are belt drive. Anything that is belt drive usually has some slip / variation (if more familiar to you, imagine the belt drive on a lawn tractor). Many have a finer adjustment to tune the spin rate. +- off of the selected rate. (Perhaps you selected 33 rpm but it’s really spinning at 32) I’m sure someone who knows much more then I do, could give a much better explanation along with other types of tuning. (Arm weights, needles, etc etc) reply echoangle 2 hours agorootparentIs the problem really the slip in the belt drive? I thought its just the motor driving it at a slightly wrong speed. reply yial 1 hour agorootparentThat may be the case, my experience is with a limited number of record players. I only commonly use a dual 1219, dual 1229, (both purchased for under $30), and a victor victrola from 1908 so my knowledge is severely limited despite having played with others it’s focused on those units. reply datpiff 4 hours agoparentprevModels targeted at DJs will have variable speed adjustment on the front panel to allow beat-matching. Some turntables for home listening will have 2 or 3 speeds with a fine adjustment for calibration. Rates will be off due to manufacturing tolerance or mechanical wear (i.e. a loose belt slipping on a motor). Most people don't worry about it. It doesn't really vary album-to-album, unless you have some ridiculously heavy records that slow the motor. reply vel0city 4 hours agoparentprevEven a cheaper record player will often have some kind of adjustment on the motor itself. There is sometimes a small screw on the motor that can do some fine adjustments of the speed. Belts age. Friction within the motor and the table's bearing will change over time. It is not necessarily a frequent thing that needs to be done, but usually the speed will change slightly over the life of the table. reply tpurves 4 hours agoparentprevYes, it turns out that it is rare for mass-produced mechanical devices like record players to spin at exactly the rate they are supposed to (eg 33.3 or 45). Or they can drift put of calibration over time. Often they will be a few % off or worse for really cheap players. Usually there is a way to adjust calibration by turning a small screw someplace on the machine. reply rufus_foreman 4 hours agoparentprevDJ turn tables have a pitch fader and lights for calibration: https://www.youtube.com/watch?v=KbdK94vl_Bs. reply empath75 3 hours agorootparentI used to DJ with vinyl at clubs and I don't think I ever knew a DJ that looked at those lights even once. reply arytoriop 4 hours agoprevThis is really cool. reply sambeau 3 hours agoprevVery cool. Great aesthetic and copy. No notes. reply peter_l_downs 3 hours agoprevStrongly recommend updating your website to lead with some variation of \"This is an offline iPhone app to help you calibrate your turntable's speed, all you need to do is play your records and the app will show you the current RPM\" — wayyyyyyy too many words and other nonsense before I could figure out this is what it did. Embed the demo video (the tweet you linked) too! Looks great and if I still used records I would try it out! Congratulations on the launch. EDIT: keep the rest of the writing if you'd like, I have no objection to it and I thought it was funny. But lead with what the app is and what it does! reply okatbest 1 hour agoparentThat is fair criticism, I expected for the website itself to be polarizing. At the end of the day because this is a tiny free app I made for fun, I am willing to throw some people off of the sake of the joke, and judging by a lot of the comments I have succeeded! Overall, my assumption is that by the time you make it to the website, you'll already know what the app does, and most people will link to the App Store directly (which is why the store listing is much clearer on what the app is/does). if not, my second assumption is that people will scroll right past that giant block of text and go to the last line which does explain what the app is in one sentence, because there isn't much more to say about it in my opinion. If I made money off of the app I'd have a widely different approach, but in this case all I get paid with is strangers' attention and it is _very_ funny to me to spend that attention on a useless 2AM irrelevant tirade. reply PascLeRasc 2 minutes agorootparentDon’t throw away the “this is off the record, player” line. That’s gold. reply peter_l_downs 1 hour agorootparentprevFair enough, congratulations on your launch and I hope you see continued success! reply berkes 3 hours agoparentprevMaybe they already changed it, but there's a chapter \"As easy as 1-2-3.\" I'd personally move that way up. I had to scroll across all sorts of fluff and effects. I was sold by this (edit: this chapter, not the fluff) Maybe I'd personally even research how to make the entire website just that. Where maybe step 0 is \"install the app\" :D The (promised) simplicity of the app doesn't show in the communication of the website at least. reply joebergeron 2 hours agoparentprevI thought the copy was fun and don't mind a good soapbox every once in a while, but yeah, wouldn't hurt to slip in a few words about what exactly the product does earlier on. I'll definitely be trying this out on my decks before my next set :) reply peter_l_downs 2 hours agorootparentUpdated my comment for clarification, agreed the extra copy is fun. Also hi joe :^) reply gamegod 3 hours agoparentprevYeah, but he gets great time-on-site engagement numbers by doing that. Somebody give this guy a job in Silicon Valley! reply gchamonlive 2 hours agoparentprev> Let's be real, if this charade of a website was an actual startup, we'd probably wear t-shirts that say... It's intentional. The app is free. There is a statement about this whole corp mission mindset. This is the way it is. If people don't like it or push it away from the product, they are not the target audience. I think this is brilliant. I understood immediately what the app does and kudos for the courage in setting the website the way it is. Was also impressed with the aesthetic of the site. If only I had a turntable... reply peter_l_downs 2 hours agorootparentThis is a Show HN, so I replied with my feedback. The author explicitly invited this, too, by writing: > Would love to hear what you think about it! reply gchamonlive 52 minutes agorootparentAn this is hn, where you post comments and people reply. Doesn't change the fact that I think the original comment misses the purpose of the website. Doesn't matter the downvotes I'm taking, I stand by my comment and I think there are better ways to critique the work other than to bash on an obviously conscious aesthetic choice. reply mk_stjames 3 hours agoprevSo here's another method that doesn't rely on the song being detected or even a song at all: The gyroscope on your phone is accurate enough to measure RPM to about the first decimal place at 33 1/3 - 45 rpm's just by placing your phone on the turntable flat and letting it spin. reply tirthd 4 hours agoprevGreat copywriting. It's very rare that I read an entire paragraph without skipping a word. reply turnsout 3 hours agoparentAgreed—and it does feel like the Apple-inspired copywriting style the author is complaining about is feeling extremely (wait for it) played. Absolutely every site is like: \"Lawn Order. The X900's diamond-tipped rotary blades cut your grass to within 50 microns of your desired length. Overkill? Absolutely. Because it's not just your lawn—it's our passion.\" reply empath75 3 hours agoparentprevI actually disagree. There's a lot of extraneous stuff about app stores that's probably appealing to app developers but are developers the audience or audiophiles? reply KeplerBoy 4 hours agoprev [–] That really is a lot of copywriting for a bunch of FFTs, but nicely done. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ivan has created a free app named Grooved, which enables users to calibrate their turntables using their phone's microphone, eliminating the need for printed materials or placing the phone on the platter.",
      "Grooved is currently available on the App Store, with an Android version anticipated in June.",
      "Ivan is seeking user feedback and has shared a demo link on Twitter."
    ],
    "commentSummary": [
      "A developer released a free app called Grooved, which uses a phone's microphone to calibrate turntable speed without additional materials or placing the phone on the platter.",
      "The app, available on the App Store with an Android version expected, has been praised for its simplicity and effectiveness in detecting and correcting turntable speed issues.",
      "Users appreciate the app's privacy-focused design, though the developer is hesitant to make it open source due to potential misuse."
    ],
    "points": 159,
    "commentCount": 117,
    "retryCount": 0,
    "time": 1716906208
  },
  {
    "id": 40503202,
    "title": "Library of Visual Text Effects Offers Diverse Animation Styles for Canvas Displays",
    "originLink": "https://chrisbuilds.github.io/terminaltexteffects/showroom/",
    "originBody": "Effects Showroom The effects shown below represent the built-in library of effects and their default configuration. Beams Creates beams which travel over the canvas illuminating the characters. Reference Config Beams Command Line Arguments Binarypath Decodes characters into their binary form. Characters travel from outside the canvas towards their input coordinate, moving at right angles. Reference Config Binarypath Command Line Arguments Blackhole Creates a blackhole in a starfield, consumes the stars, explodes the input data back into position. Reference Config Blackhole Command Line Arguments BouncyBalls Characters fall from the top of the canvas as bouncy balls before settling into place. Reference Config Bouncyballs Command Line Arguments Bubbles Forms bubbles with the characters. Bubbles float down and pop. Reference Config Bubbles Command Line Arguments Burn Characters are ignited and burn up the screen. Reference Config Burn Command Line Arguments ColorShift Display a gradient that shifts colors across the terminal. Reference Config ColorShift Command Line Arguments Crumble Characters crumble into dust before being vacuumed up and reformed. Reference Config Crumble Command Line Arguments Decrypt Movie style text decryption effect. Reference Config Decrypt Command Line Arguments ErrorCorrect Swaps characters from an incorrect initial position to the correct position. Reference Config ErrorCorrect Command Line Arguments Expand Characters expand from the center. Reference Config Expand Command Line Arguments Fireworks Launches characters up the screen where they explode like fireworks and fall into place. Reference Config Fireworks Command Line Arguments MiddleOut Text expands in a single row or column in the middle of the canvas then out. Reference Config MiddleOut Command Line Arguments OrbittingVolley Four launchers orbit the canvas firing volleys of characters inward to build the input text from the center out. Reference Config OrbittingVolley Command Line Arguments Overflow Input text overflows ands scrolls the terminal in a random order until eventually appearing ordered. Reference Config Overflow Command Line Arguments Pour Pours the characters back and forth from the top, bottom, left, or right. Reference Config Pour Command Line Arguments Print Prints the input data one line at at time with a carriage return and line feed. Reference Config Print Command Line Arguments Rain Rain characters from the top of the canvas. Reference Config Rain Command Line Arguments RandomSequence Prints the input data in a random sequence, one character at a time. Reference Config RandomSequence Command Line Arguments Rings Characters are dispersed and form into spinning rings. Reference Config Rings Command Line Arguments Scattered Text is scattered across the canvas and moves into position. Reference Config Scattered Command Line Arguments Slice Slices the input in half and slides it into place from opposite directions. Reference Config Slice Command Line Arguments Slide Slide characters into view from outside the terminal. Reference Config Slide Command Line Arguments Spotlights Spotlights search the text area, illuminating characters, before converging in the center and expanding. Reference Config Spotlights Command Line Arguments Spray Sprays the characters from a single point. Reference Config Spray Command Line Arguments Swarm Characters are grouped into swarms and move around the terminal before settling into position. Reference Config Swarm Command Line Arguments SynthGrid Create a grid which fills with characters dissolving into the final text. Reference Config SynthGrid Command Line Arguments Unstable Spawns characters jumbled, explodes them to the edge of the canvas, then reassembles them. Reference Config Unstable Command Line Arguments VHSTape Lines of characters glitch left and right and lose detail like an old VHS tape. Reference Config VHSTape Command Line Arguments Waves Waves travel across the terminal leaving behind the characters. Reference Config Waves Command Line Arguments Wipe Performs a wipe across the terminal to reveal characters. Reference Config Wipe Command Line Arguments",
    "commentLink": "https://news.ycombinator.com/item?id=40503202",
    "commentBody": "TTE: Terminal Text Effects (chrisbuilds.github.io)144 points by makapuf 1 hour agohidepastfavorite16 comments throwanem 10 minutes agoThis is amazing! Please never ever use it in production. reply distortionfield 9 minutes agoparentIdk I wouldn’t mind one or two of these as loading screens or something. reply keploy 19 minutes agoprevThis is cool! we can utilize the 'Burn' or 'ErrorCorrect' effects to highlight warnings or errors in logs dynamically, ensuring critical issues stand out in ongoing terminal output. reply mickeyp 32 minutes agoprevIn a similar vein in Emacs: M-x zone Every time you run it, it triggers a random screen saver effect. Fun when it's on a timer and triggers so your work colleagues wonder wth is going on. reply dmd 8 minutes agoparentI just get a blank emacs buffer when I run that. reply leetrout 12 minutes agoprevI'm glad to see we all take the same approach to these sorts of things ^_^ self.move_cursor_to_top() sys.stdout.write(output_string) sys.stdout.flush() reply benrutter 26 minutes agoprevI freaking love this library! It reminds me of a time that I don't even know happened when computers seemed like a scifi possibility come true. So happy to see it on the front page. One of the coolest things is not just the ability to pipe cat outputs into it, but that it doubles up as a python library- next time you're making a throw away CLI with print outs and prompts, why not make it insanely jazzy? reply filoleg 20 minutes agoprevThis is great, thanks! My favorite is Beams (the one at the very top of the page), reminds me of MGS1 “game over” screen animation a lot. reply terminaltrove 8 minutes agoprevThe effects on this project is incredible, thanks for building this! reply PhilipRoman 34 minutes agoprevGreat work, this will make a lovely splash screen for my tty-only machine. reply jimbobthrowawy 37 minutes agoprevIncredibly goofy, I like it. Especially the demo gifs on the page. Wonder how long until I see it be the default output for a CLI I'm using. Got surprised to see someone using lolcat in the wild before. reply tambourine_man 1 hour agoprevThis is awesome. Thanks reply xoac 22 minutes agoprevSomeone will add this to their javascript framework cli reply devdao 41 minutes agoprevLove it, thanks for posting! reply sciencesama 20 minutes agoprevNow we need this in esp32 !! reply behnamoh 31 minutes agoprev [–] Next time someone donks Python for being incapable of building decent CLI tools, I'll burn their terminal using TTE. I know Go has the crown for creating TUI apps, but Python isn't that awful. If the app doesn't need concurrency (like a terminal file manager [0] does), then Python is fine. [0]: Yazzi (Rust) eats Ranger's (Python) lunch and dinner combined. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The document details a library of built-in visual effects for text display on a canvas, each with a unique animation style.",
      "Examples of these effects include beams of light, binary paths, blackholes, bouncy balls, bubbles, burning text, and more.",
      "Each effect comes with a reference configuration and command line arguments for customization, allowing for tailored visual presentations."
    ],
    "commentSummary": [
      "TTE: Terminal Text Effects is a project providing various text effects for terminal outputs, receiving positive user feedback.",
      "Users appreciate its creativity and nostalgia, seeing potential uses for loading screens or highlighting errors in logs, though some caution against using it in production.",
      "The project is praised for enhancing CLI (Command Line Interface) outputs and its functionality as a Python library, with comparisons to similar tools in Emacs and other programming languages."
    ],
    "points": 145,
    "commentCount": 16,
    "retryCount": 0,
    "time": 1716917508
  },
  {
    "id": 40501739,
    "title": "Leaked Docs Show OpenAI CEO Sam Altman Pressured Ex-Employees with Restrictive Agreements",
    "originLink": "https://futurism.com/sam-altman-silencing-former-employees",
    "originBody": "Egregious and Unusual May 23, 6:00 PM EDT by Maggie Harrison Dupré Egregious and Unusual /Artificial Intelligence Leaked OpenAI Documents Show Sam Altman Was Clearly Aware of Silencing Former Employees \"We want to make sure you understand that if you don't sign, it could impact your equity.\" / Artificial Intelligence/ Ai/ Open AI/ Sam Altman May 23, 6:00 PM EDT by Maggie Harrison Dupré Image by Halil Sagirkaya / Anadolu via Getty / Futurism OpenAI's credibility — and the credibility of its CEO, Sam Altman — is crumbling. Last week, amid a surprise string of high-profile executive and safety team departures, Vox revealed that the ChatGPT creator had pressured employees into signing draconian non-disclosure and non-disparagement agreements by threatening to claw back exiting OpenAI employees' vested equity in the multibillion-dollar AI company. Clawing back vested equity — in short, the amount of company ownership that an employee has gained through their months or years of working there — is a highly unusual practice to begin with. This is especially true in the startup-powered Silicon Valley, where tech workers often forgo high salaries in favor of equity agreements based on the hope that they'll get rich later when a successful startup like OpenAI eventually goes public. For OpenAI to play bizarre contractual take-backsies in exchange for narrative control over former employees would be an awful look for any company — let alone a supposedly \"open\" venture claiming it's the best one to build the imagined all-knowing AI that OpenAI's leaders say will power humanity's future. In response to the Vox report, Altman apologetically took to X-formerly-Twitter to admit that yes, \"there was a provision about potential equity cancellation in our previous exit docs.\" But according to the CEO, though the clause was there, the company never actually clawed anything back. Most importantly, he further claimed that he had no knowledge of the provision. \"This is on me and one of the few times I've been genuinely embarrassed running OpenAI,\" Altman continued in a tone that can only be described as sheepish, \"I did not know this was happening and I should have.\" But according to Vox's latest follow-up, Altman wasn't in the dark about the equity clauses, as he claimed in his tail-between-legs tweet. Documentation reviewed by Vox reveals that several company leaders — including OpenAI chief strategy officer Jason Kwon, who reportedly told staffers following the initial Vox report that OpenAI leadership \"caught\" the provision \"~month ago\" — signed documents that plainly outlined the stifling clawback provision. The list of executives includes Altman, whose signature, according to Vox's reporting, is on \"incorporation documents\" for the holding company that manages OpenAI equity; these documents contain \"multiple passages with language that gives the company near-arbitrary authority to claw back equity from former employees\" or, if employees choose not to actually return the equity, \"block them from selling it\" altogether. In other words, unless someone forged the CEO's signature, he gave express permission for these clauses to exist. Outside of forgery, there are only two plausible reasons for Altman's alleged lack of knowledge: either he didn't fully read the employment contracts he was signing or he was lying. Complicating the denials further is the way employees were reportedly treated on their way out. According to the Vox report, the equity provisions were no secret to OpenAI representatives handling departures, who in some cases gave outgoing employees just seven days to make the incredibly complicated decision about their future — all the while, in instances reviewed by Vox, emphasizing possible clawbacks. \"We want to make sure you understand that if you don't sign, it could impact your equity,\" one rep told an outgoing employee, according to Vox. \"That's true for everyone, and we're just doing things by the book.\" But again, as the report reiterates, this is not \"by-the-book\" behavior. \"For a company to threaten to claw back already-vested equity is egregious and unusual,\" Chambord Benton-Hayes, a California employment law attorney, told Vox. When Vox asked OpenAI to explain how the provisions could have possibly wound up in documents signed by Altman without Altman actually knowing about them, Kwon non-answered that \"we are sorry for the distress this has caused great people who have worked hard for us.\" \"We have been working to fix this as quickly as possible,\" Kwon — who, again, also signed papers delineating this provision — continued in his statement. \"We will work even harder to be better.\" But that's getting harder and harder to believe. Last year, when Altman was briefly forced out of OpenAI in what was pretty much a corporate coup, those who voted to oust the CEO — many of whom departed after losing said coup — claimed that Altman had been \"inconsistently candid\" in his communications with the board. Altman and OpenAI are also caught up in a brewing legal storm with actress Scarlett Johansson, who claims that Altman copycatted her voice for OpenAI's new \"Sky\" AI assistant after she had explicitly turned Altman and OpenAI down. (Altman chalked Johansson's allegations up to simple miscommunication.) Meanwhile, recent departures have ground OpenAI's \"Superalignment\" safety team — the ones tasked with making sure a killer AI doesn't obliterate humankind — into dust. Great stuff. On its website, OpenAI features a \"charter\" declaring that \"OpenAI's mission is to ensure that artificial general intelligence (AGI) — by which we mean highly autonomous systems that outperform humans at most economically valuable work — benefits all of humanity.\" \"We will attempt to directly build safe and beneficial AGI,\" it continues, \"but will also consider our mission fulfilled if our work aids others to achieve this outcome.\" The document then lists a series of \"principles,\" which the company claims it'll follow to achieve this mission. The word \"transparency\" is notably absent. More on OpenAI: Sam Altman Ignoring Scarlett Johansson's Lack of Consent Shows Us Exactly What Type of Person He Really Is Share This Article Read This Next Immune-Boosting Google’s AI Is Churning Out a Deluge of Completely Inaccurate, Totally Confident Garbage Promises vs. Reality While Meta Stuffs AI Into All Its Products, It's Apparently Helpless to Stop Perverts on Instagram From Publicly Lusting Over Sexualized AI-Generated Children Keep Me Posted The Washington Post Tells Staff It’s Pivoting to AI Altman Down Sam Altman Ignoring Scarlett Johansson's Lack of Consent Shows Us Exactly What Type of Person He Really Is War and Palantir Palantir’s Military AI Tech Conference Sounds Absolutely Terrifying",
    "commentLink": "https://news.ycombinator.com/item?id=40501739",
    "commentBody": "[flagged] Leaked OpenAI Docs Show Sam Altman Clearly Aware of Silencing Former Employees (futurism.com)141 points by Paul-Craft 3 hours agohidepastfavorite51 comments neya 2 hours agoI have zero respect for companies with such predatory behaviour. Not a knee jerk reaction, but threatening employees \"diplomatically\" with such draconian agreements and even sometimes non-competes should actually be grounds for mass resignation. I worked in a similar company and till date have traumatic memories from that era. For this and many other reasons, I would never ever use a proprietary AI model in any of my AI projects. \"People first\" should be the guiding principle for developers. reply grokblah 3 hours agoprevOpenAI sounds like a wolf in sheep's clothing reply nickpsecurity 3 hours agoparentThey never were in sheep’s clothing. They took $10 billion from Microsoft. Anyone closely partnering with a top wolf is probably a wolf cub on a journey into the wolf pack. reply tim333 1 hour agorootparentFrom 2015 to 2018 they were non profit but after that it seemed a bit iffy. reply indigodaddy 3 hours agoprevWow, so 5 minutes ago this was #1 on HN. Now it's #347. Hmm.. wonder what might have happened? reply reaperman 2 hours agoparent#413 with 122 upvotes, still just 1 hour after submission. reply apsec112 2 hours agoparentprevIt was probably flagged because it's a dupe (both Vox stories were already covered on HN) reply nicce 2 hours agorootparentBut it discussed both these two stories and added more thoughts. What is the definition of dupe? reply stillold 1 hour agoprevYou might say that Sam Altman is not consistently candid... reply frabjoused 3 hours agoprevNo defense of the act here, but how can someone take a publication seriously that states an opinion in the first sentence, and in the 2nd paragraph uses a clickbait modifier like \"draconian\"? reply throw5345346 3 hours agoparentNot sure how many really better words there are. Onerous? Extreme? Oppressive? Bullying? It does after all refer to a contract people were apparently pressured into signing when quitting (and not when signing up), that apparently offered no new consideration, and required them to never say anything bad about OpenAI, for life, on penalty of losing remuneration another signed contract entitled them to. It's journalism. But even the lawyers are saying \"egregious\". Which is really really not good in this context. \"Draconian\" fits, considering how it is normally used. reply frabjoused 2 hours agorootparentEgregious is a much better word that doesn't intentionally sound alarming. The only new information here is that his signature was on these documents. This article took 18 paragraphs to say that. Real journalism that wasn't influenced by SEO or ad views should probably be one sentence with the new information and a link to a previous article filling in the missing context, for those who happen not to have it. reply throw5345346 2 hours agorootparentHave a look at how \"draconian\" is used. https://www.merriam-webster.com/dictionary/draconian https://en.wikipedia.org/wiki/Draconian https://www.vocabulary.com/dictionary/Draconian Now consider the thing that is happening: someone is being essentially pressured to agree a contract forcing them to stay functionally silent about anything bad they experienced or saw at OpenAI, on pain of losing things they've already earned. For that individual's experience, \"draconian\" -- harsh, unusually severe, repressive -- seems like it would be a good fit, right? And as it is not in the title or the subhead, it's not really being deployed to be clickbait, is it? The title is even more damning (As an aside, why should this article not intentionally sound alarming? This whole situation is a pretty damning indictment of one of the most influential startups on earth right now. IMO it is alarming.) reply throw5345346 1 hour agorootparentprevAlso: > The only new information here is that his signature was on these documents. This is what's called \"confirming the story\". News organisations do this all the time -- corroborate their competitors' stories -- and it is absolutely news. In this case it's a really big bit of news, to people outside the tech world: it means there will likely prove to be concrete, potentially-lawsuit-bound corroboration that Sam Altman is not \"consistently candid\". reply aaomidi 2 hours agorootparentprevAre you basing this off of the assumption that every single person reading this has the same relationship to specific words as you? Egregious and draconian create the same “idea” in my head. There is literally no way to write that takes into account each persons relationship with words. reply throw5345346 1 hour agorootparentMmm, though I think these are two angular perspectives on the same idea, more than they are the same idea. I feel \"egregious\" makes more sense for the idea of these bullshit contracts in the general case -- the idea that a lawyer wrote them or signed off on them. \"Draconian\" is a really good fit for the experience and outcome for the individual, and for the process by which an individual found themselves signing. Either way, I don't think there's any reason to minimise what is happening for the employee, or what it means in terms of characterising the conduct and culture of the ultimate unicorn. If stuff like this is allowed to slide, it's not good. reply RONROC 2 hours agoprevIf you trust Sam I feel bad for you reply breck 3 hours agoprevI love when a website uses the word \"credibility\", and half the website is filled with flashing ads and it has too many trackers and ad scripts to count. reply flutas 3 hours agoprev...and it's gone from the front page reply bun_at_work 3 hours agoprev> \"We want to make sure you understand that if you don't sign, it could impact your equity,\" one rep told an outgoing employee, according to Vox. The article is doing a lot of click-bait stuff here. The quote, without the source, is shown at the top of the article, near Sam Altman's face and a title referencing Sam Altman. However, the quote is not from Sam Altman, which makes its placement a bit disingenuous. I am not invested in defending Altman, but this type of journalism is trash clickbait and a far bigger issue than a tech company threatening employees' vested equity unless they comply in some sort of non-NDA NDA scheme. reply joekrill 3 hours agoparentThat's looking purely at the headline and a single quote that they chose for it. There is a lot more if you read the actual article. They make a case that Altman signed documents where it's quite clear the intention is to be able to claw back vested equity. And that quote in the headline isn't meant to try and attribute it directly to Altman -- but to show just how well-known the policy was. reply bun_at_work 3 hours agorootparentI read the whole article, that's where I found the quote attribution. Like I said, I'm not here to defend Altman, or attack him. It's just disingenuous journalism - or click-bait journalism. reply luxuryballs 3 hours agoprevI must be missing some key detail here because this seems even more over blown than AI itself. They wanted to motivate people to not disclose trade secrets? Oh no. reply jhanschoo 3 hours agoparentYou are making factual errors in your response to the existence of this article. First, this was not primarily about NDAs and trade secrets, even though an NDA was mentioned. This was primarily about non-disparagement and the ability to criticize OpenAI, see the linked article https://www.vox.com/future-perfect/2024/5/17/24158478/openai... Second, this is not even about the existence of the non-disparagement agreement itself, it's about Altman claiming ignorance about its existence (see: the article again) reply digitaltrees 3 hours agoparentprevClawing back vested equity it materially different that a general “incentive”. It’s way outside of standard practice and borderline abuse given the imbalance of negotiating power. Further, if OpenAI or Sam lied publicly by saying they weren’t aware it was going on and did this seems to be the very type of untrustworthy lack of candor that the board identified when they ousted him. I was a vocal defender of Sam in the immediate aftermath but these facts are definitely concerning. OpenAI is moving fast with a critical and dangerous technology. If their culture isn’t profoundly ethical they shouldn’t be trusted with the privilege and responsibility of what they are up to. reply benzible 3 hours agoparentprevThe key details are how completely outside the norms of SV / tech this behavior is; how they, and Sam specifically, clearly lied about it - especially egregious when they want to be seen as the trustworthy steward of \"AGI\"; and as a business story, how this seems to be eroding trust of the company among their current employees and how damaging this will likely be in their efforts to recruit top AI talent. I don't think there will be quite so many heart emoji tweets for Sam in the future. reply m3kw9 3 hours agoprevSmear campaign on going. reply HarHarVeryFunny 3 hours agoparentAltman's got his signature on docs he claims to know nothing about. How it is a \"smear campaign\" to report that? reply throw5345346 3 hours agorootparentOh you. These people have weightier issues like the ethics of the hypothetical technological future on their capacious minds, and mustn't be dragged down to your mucky little level of discussing the ethics of the very real business present. Seriously: don't bother them with these little questions of the rights of actually enumerable humans to whom they have contractual responsibilities! They're busy thinking in great depth about how their product might affect the rights of all the humans they haven't yet sold it to. reply roughly 3 hours agorootparentOne does wonder how the accelerationist ethic conceives of the tradeoff between a hundred future happy fulfilled lives or two hundred permanently indentured serfs. reply ActionHank 3 hours agoparentprevIs it a smear campaign when you did it to yourself? reply nicklecompte 3 hours agorootparent\"Despite the best efforts of his words, his actions continued their relentless smear campaign.\" reply throw5345346 3 hours agorootparent\"My 'we never actually took away people's remuneration using the contracts they signed swearing them to silence' t-shirt has people asking a lot of questions already answered by my shirt\" reply vokadopa 3 hours agoparentprevSam Altman is the new Michael O. Church. Except this time he’s smearing himself. reply segasaturn 3 hours agoparentprevAltman is a billionaire. I'm sure he's got a good crisis management PR team to handle the pickle that he's found himself in. He'll survive. reply m3kw9 2 hours agoparentprevIt’s a smear campaign because of the focus, not if he or he did not do what he said reply ml-anon 3 hours agoparentprevWon’t someone think of the tech CEOs reply runjake 2 hours agorootparentYou registered your account in 2023, so you're relatively new here, but you're in the wrong place with that sort of comment. Or maybe you're not, Hacker News has evolved away from its mission and audience and I'm in the wrong place. reply ml-anon 2 hours agorootparentPretty telling you directed that at my comment rather than the absolute horseshit comment I responded to. reply runjake 2 hours agorootparentHN's intended community wants desperately to be those tech CEOs/tech billionaires. So, your comment is a little ironic. reply nicce 3 hours agorootparentprevStatistically speaking, tech billionaires are rare among humans. We must protect the species on the brink of extinction! reply _cs2017_ 3 hours agoprevNo evidence in the article that Altman knew about this, and intentionally clickbaity and misleading title. Quoting: \"there are only two plausible reasons for Altman's alleged lack of knowledge: either he didn't fully read the employment contracts he was signing or he was lying.\" No evidence was offered that he read the contracts. reply benterix 2 hours agoparentYou are technically true. However, if you sign contracts you haven't read, you would have a hard time defending your innocence in court. This applies even more for higher positions like CEO. reply _cs2017_ 15 minutes agorootparentYes, legally responsible. There's no legal violation here, though. The article made a specific claim: that Altman was aware of something. It provided no evidence that he was. If it accused him of some criminal misdeed, then it wouldn't matter if he was aware of it, as long as he signed the illegal order. That would be a different story, but clearly in this case it doesn't apply. reply DemocracyFTW2 53 minutes agoparentprev> No evidence was offered that he read the contracts This excuse only works for people who do not get paid for doing exactly this: the power to sign contracts, exit papers, stuff like that on behalf of the company, and, therefore, be aware of their contents. \"I didn't go through the contract before agreeing to its content by signing it\" is a damning verdict for someone in this position. You don't have to prove he read it: he signed, therefore he's responsible. reply _cs2017_ 10 minutes agorootparentI didn't say anything about responsibility. The article has a title and it's intentionally misleading. reply jsnell 3 hours agoprev [–] Dupe: https://news.ycombinator.com/item?id=40447431 reply benzible 3 hours agoparent[deleted] reply ChrisArchitect 3 hours agorootparentCan share the link over there. That's where the discussion is, and there's lots of it. reply throw5345346 3 hours agoparentprev [–] They refer to different articles, though. reply jsnell 2 hours agorootparent [–] It doesn't need to be the same article to be a duplicate submission, it just needs to have no significant new information compared to recent submissions that already got a lot of HN attention. This article is literally just summarizing the Vox article with no new information or insight. And the HN submission for the Vox article had 1784 points and 544 comments. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Leaked documents indicate OpenAI CEO Sam Altman pressured departing employees to sign restrictive agreements by threatening to reclaim their vested equity, a rare practice in Silicon Valley.",
      "Despite Altman's public apology and claims of ignorance, evidence shows he signed documents authorizing these provisions, leading to significant executive departures and legal issues.",
      "The controversy, including a dispute with actress Scarlett Johansson over unauthorized use of her voice, undermines OpenAI's credibility and contradicts its mission of transparency and benefiting humanity."
    ],
    "commentSummary": [
      "Leaked documents from OpenAI show CEO Sam Altman's awareness of efforts to silence former employees through stringent agreements, raising ethical concerns.",
      "The term \"draconian\" is debated regarding the pressure to sign contracts under threat of losing benefits, with Altman's signature confirming these practices.",
      "The controversy, including accusations of sensationalism against a Vox article, highlights deviations from Silicon Valley norms, potentially harming trust and future talent recruitment."
    ],
    "points": 141,
    "commentCount": 50,
    "retryCount": 0,
    "time": 1716910389
  },
  {
    "id": 40498092,
    "title": "Gleam 1.2.0 Enhances Fault Tolerance and Developer Experience",
    "originLink": "https://gleam.run/news/fault-tolerant-gleam/",
    "originBody": "Published 27 May, 2024 by Louis Pilfold Gleam is a type safe and scalable language for the Erlang virtual machine and JavaScript runtimes. Today Gleam v1.2.0 has been published, a release that focuses on improving the language server and developer experience. It’s a big one both in terms of size and impact, so let’s take a look as what it includes. Fault tolerant compilation Gleam’s compiler has traditionally halted immediately when a compile time error was encountered, presenting the error to the programmer and requiring them to fix it before compilation can be attempted again. The main advantage of this is that the programmer has just the first and most important error, rather than a mix of the real error and a bunch of red-herring cascading errors. On the other hand, if there’s multiple genuine errors the programmer can only see first the one, which may not be the one they want to work on, or the most understandable. Even worse, this halting behaviour causes major issues for the Gleam Language Server (the engine that text editors like Neovim, Helix, Zed, and VS Code use to get IDE features for Gleam). The language server internally uses the compiler to build and analyse Gleam projects, so when the compiler halts with an error the language server is left without up-to-date information about the code. Without a successful build some language server features may not work, and the longer a project goes without successful compilation (such as during a large refactoring) the more drift there can be between the language server’s understanding of the code and the reality. With this release when the compiler encounters an error during analysis it will move on to the next definition in the module, returning all of the errors along with updated code information once the whole module has been analysed. This has resulted in a dramatic improvement in the experience when using the Gleam language server, it being much more responsive and accurate in its feedback. In future releases we will continue to improve the granularity of the fault tolerant compilation and we will also introduce fault tolerant parsing. Thanks to Ameen Radwan and myself for this feature! Language server imports improvements This is a Gleam import statement: import gleam/option.{type Option, Some, None} // ^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^ // the module unqualified imports The language server could previously autocomplete module names, but now it can also autocomplete types and values when importing them in the unqualified fashion. You can also hover types and values in an import statement to see their documentation, and use go-to definition on them and the module itself to jump to where they defined in your project or its dependencies. Single line pipelines Gleam’s much loved |> pipe operator can be used to pipe the output from one function into the input of another, not dissimilar topipes in bash, etc. Gleam’s code formatter would always put each function of a sequence of pipes on individual lines, but now you can opt to put them all on one line, so long as altogether they are short enough to fit. [1, 2, 3] |> list.map(int.to_string) |> string.join(with: \"\") In addition you can also force the formatter to break a pipe on multiple lines by adding a line break. This: [1, 2, 3] |> list.map(int.to_string) // By putting a newline here I'm telling the formatter to split the pipeline |> string.join(with: \"\") Will turn into this: [1, 2, 3] |> list.map(int.to_string) |> string.join(with: \"\") Thank you to our formatter wizard Giacomo Cavalieri for this feature! Improved error messages for use expressions Gleam’s use expressions are very powerful and can be used to replicate many built-in features of other languages such as Go’s defer, procedural languages’ early returns, Haskell’s do-notation, and more. As an unusual and powerful feature they can be confusing at first, and that was made worse as any type errors from incorrect use expressions would be generic ones rather than being specific to use. These errors have been greatly refined and we hope this will help folks learning the language and debugging their code. Here’s some specific examples: This error message is used when the right-hand-side oftodo } See: https://tour.gleam.run/flow-control/multiple-subjects/ Thank you Giacomo Cavalieri for this feature. Redundant pattern matching auto-fix And further still, if you are using the Gleam language server then a code action is offered to automatically fix redundant tuple wrappers. Place your cursor on the tuple and select this language action and the language server will remove the tuple from the values as well as from all patterns in the case expression. case #(x, y) { #(1, 2) -> 0 #(_, _) -> 1 } Is rewritten to: case x, y { 1, 2 -> 0 _, _ -> 1 } Thank you Nicky Lim for this code action, and for laying the foundation for other code actions like this in future! A helping hand for JavaScript programmers JavaScript programmers sometimes type === by mistake in their Gleam code. We have an error message for that too now: error: Syntax error ┌─ /src/parse/error.gleam:4:37 │ 4 │ [1,2,3] |> list.filter(fn (a) { a === 3 }) │^^^ Did you mean `==`? Gleam uses `==` to check for equality between two values. See: https://tour.gleam.run/basics/equality Thank you Rabin Gaire for this feature! And the rest If you’d like to see all the changes for this release, including all the bug fixes, check out the changelog in the git repository. Thanks Gleam is made possible by the support of all the kind people and companies who have very generously sponsored or contributed to the project. Thank you all! If you like Gleam consider sponsoring or asking your employer to sponsor Gleam development. I work full time on Gleam and your kind sponsorship is how I pay my bills! Alternatively consider using our referral link for CodeCrafters, who have recently launched a course on implementing Redis in Gleam. Use your training budget to learn and to support Gleam too! Aaron Aaron Gunderson Abdulrhman Alkhodiry ad-ops Adam Brodzinski Adam Johnston Adi Iyengar Adi Salimgereyev aelishRollo Aiden Fox Ivey Ajit Krishna Alembic Alex Manning Alexander Koutmos Alexander Stensrud Alexandre Del Vecchio Ameen Radwan AndreHogberg Andy Aylward Anthony Khong Anthony Maxwell Anthony Scotti Arnaud Berthomier Arthur Weagel Austin Daily Barry Moore Bartek Górny Ben Martin Ben Marx Ben Myles Benjamin Peinhardt Benjamin Thomas bgw Bill Nunney brettkolodny Brian Dawn Brian Glusman Bruno B. Bruno Michel bucsi Carlo Munguia Carlos Saltos Chad Selph Charlie Govea Chaz Watkins Chew Choon Keat Chris Donnelly Chris King Chris Lloyd Chris Ohk Chris Rybicki Christopher Dieringer Christopher Keele clangley Clay Cleo CodeCrafters Coder Cole Lawrence Colin Comamoca Constantine Manakov Cristine Guadelupe ctulb Damir Vandic Dan Dresselhaus Daniel Daniel Hayes Danielle Maywood Danny Arnold Danny Martini Darshak Parikh Dave Lucia David Bernheisel David Sancho Denis Dennis Dang dennistruemper dependabot[bot] Dezhi Wu Dillon Mulroy Dima Utkin Dmitry Poroh ds2600 Edgars Burtnieks Edon Gashi Efstathiadis Dimitris Eileen Noonan eli Elie Labeca Elliott Pogue ellipticview Emma EMR Technical Solutions Erik Lilja Erik Terpstra erikareads ErikML Ernesto Malave Felix Mayer Fernando Farias Filip Figiel Fionn Langhans Florian Kraft fly.io Francisco-Montanez Georg H. Ekeberg Giacomo Cavalieri Graeme Coupar grotto Guilherme de Maio Guillaume Hivert Hamir Mahal Hammad Javed Hampus Kraft Hannes Schnaitter Hayes Hundman Hayleigh Thompson Hazel Bachrach Henning Dahlheim Henry Firth Henry Warren Hex human154 Humberto Piaia Ian González Ian M. Jones Igor Rumiha inoas Isaac Isaac Harris-Holt Ismael Abreu Ivar Vong Iván Ovejero J. Rinaldi Jack Malcom Jacob Lamb James Birtles James MacAulay Jan Skriver Sørensen Jean-Luc Geering Jen Stehlik Jenkin Schibel Jeremy Jacob jiangplus Jimpjorps™ Jiri Luzny Joey Kilpatrick Johan Strand John Björk John Gallagher John Pavlick Jonas Dahlbæk Jonas Hartmann Jonas Hedman Engström Jonas Hietala Josef Richter Joshua Steele Julian Schurhammer Kevin Kevin Schweikert Kieran Gill kodumbeats Kramer Hampton Kryštof Řezáč Krzysztof G. Lars Kappert Leandro Ostera Len Blum Leon Qadirie lidashuang LighghtEeloo Loïc Tosser Lucas Pellegrinelli Luci Phillips Lucian Petic Luna Luna Schwalbe mahcodes Manuel Rubio Marcel Marcus André Marcøs Mariano Uvalle Marius Kalvø Mark Holmes Mark Markaryan Mark Spink Martin Janiczek Martin Rechsteiner Mateusz Ledwoń Matt Champagne Matt Savoia Matt Van Horn Matthias Benkort Max Hung Max McDonnell max-tern Michael Duffy Michael Jones Michael Kieran O’Reilly Michael Kumm Mike Mike Roach Mikey J Milco Kats MoeDev Moshe Goldberg MystPi MzRyuKa n8n - Workflow Automation Nashwan Azhari Natanael Sirqueira Nathaniel Knight Nayuki NFIBrokerage Nick Chapman Nick Reynolds Nicklas Sindlev Andersen Nicky Lim NineFX Nino Annighoefer Nomio Ocean Armstrong Lewis OldhamMade Ole Michaelis optizio P. Envall PastMoments Patrick Wheeler Paul Gideon Dann Paul Guse Pawel Biernacki Pete Jodo Peter Rice Peter Saxton PgBiel Philip Giuliani Pi-Cla Piotr Szlachciak qingliangcn Qynn Schwaab Rabin Gaire Race Williams Rahul Butani Ratio PBC Raúl Chouza Redmar Kerkhoff Renovator Richard Viney Rico Leuthold Ripta Pasay Robert Attard Robert Ellen Robert Malko Rodrigo Heinzen de Moraes Roman Wagner Ross Bratton Ross Cousens Ruslan Ustitc Ryan M. Moore Sam Aaron Sam Mercer Sami Fouad Sammy Isseyegh Samu Kumpulainen Santi Lertsumran Saša Jurić Scott Trinh Scott Wey Sean Jensen-Grey Sebastian Porto sekun Seve Salazar Shane Poppleton shayan javani Shuqian Hon silver-shadow Simon Curtis Simone Vittori Siraj Stephen Belanger Szymon Wygnański Sławomir Ehlert TheHiddenLayer Theo Harris Thomas Thomas Ernst thorhj Timo Sulg Tom Schuster Tomasz Kowal tommaisey trag1c Tristan de Cacqueray Tristan Sloughter Vassiliy Kuzenkov Vic Valenzuela Victor Rodrigues Vincent Costa Viv Verner Vladislav Botvin Volker Rabe Weizheng Liu Wesley Moore Willyboar Wilson Silva xhh xxKeefer Yamen Sader Yasuo Higano zahash Zsombor Gasparin ~1847917 Šárka Slavětínská Thanks for reading, I hope you have fun with Gleam! 💜 Try Gleam",
    "commentLink": "https://news.ycombinator.com/item?id=40498092",
    "commentBody": "Gleam 1.2.0 release – Fault tolerant Gleam (gleam.run)139 points by crowdhailer 12 hours agohidepastfavorite48 comments brabel 8 hours agoAmazing how Gleam is moving forward. It seems it's now pretty mature and with great tooling support?! Is it fair to call Gleam an Erlang with modern syntax and a HM type system? It seems it can also compile to JS (not just Erlang's Beam VM) - is the JS target as well supported? Is performance just the same as Erlang (when compiled to Beam) or there's differences? Why would someone use Beam instead of Elixir (they now have optional types - not sure how well those work though)? Is there really demand for more than one modern language on top of Erlang's VM? reply hayleighdotdev 6 hours agoparent> Is it fair to call Gleam an Erlang with modern syntax and a HM type system? Kinda. There's a fair amount similar but things like message passing are implemented as a library in gleam [0] rather than being a part of the core language, so you might say the big things that make erlang _erlang_ are not part of Gleam directly. > is the JS target as well supported? The experience is pretty good, yeah! The compiler can emit typescript declarations, there's a vite plugin for gleam, I maintain a frontend framework written in gleam and it's all grand. > Is performance just the same as Erlang (when compiled to Beam) or there's differences? That's right, perf is the same. > Why would someone use Gleam instead of Elixir (they now have optional types - not sure how well those work though)? Is there really demand for more than one modern language on top of Erlang's VM? Few reasons. Elixir's type system work is very much Work In Progress and could be a long time before we get something \"finished\". Also the semantics and approach have quite different designs: I'd guess folks that like the gradual type experience Elixir is pushing would not like Gleam's type system and vice versa. There's certainly room for us both on the BEAM (and we do interoperate too!) [0]: https://hexdocs.pm/gleam_erlang/ reply spinningslate 6 hours agorootparent> I maintain a frontend framework written in gleam and it's all grand For anyone interested, that framework is Lustre[0]. It looks interesting: An Elm-inspired approach to SPAs on the front end, compiling Gleam to javascript. Back end also in Gleam but compiled to Erlang. Haven't tried it yet but on the list. Kudos, @hayleighdotdev, for putting something that expansive together in a pretty short period of time. [0]: https://hexdocs.pm/lustre/guide/01-quickstart.html reply oDot 8 hours agoprevI rewrote large parts of my webapp, Nestful[0], in Gleam. It's truly a joy to use and the community is very nice. Some features are still missing (e.g, reflection), but I do agree, as a user, with the focus they're putting on the developer experience, as evident is this release. If anyone else has a Vue application they want to dabble with Gleam in, do check out vleam[1]. [0] https://nestful.app [1] https://github.com/vleam/vleam reply cosmic_quanta 6 hours agoprevFault-tolerant compilation is very cool. The Haskell ecosystem is moving towards it as well: https://github.com/haskellfoundation/tech-proposals/pull/63 reply tombert 10 hours agoprevI've never heard of Gleam. Out of curiosity, what does it buy you over something like Elixir? Or rather, why do people prefer it over Elixir (or vanilla Erlang or Joxa or LFE)? ETA: I'm an idiot, I didn't see the \"type safe\" word on there. That is something worth considering. reply madeofpalk 9 hours agoparentThis article does the one great thing - the first setence of this announcement post tells you what Gleam actually is. Other projects should take note. Fly's posts do a great job of this also https://fly.io/blog/skip-the-api/ Edit: lol - I just saw on the homepage that Fly sponsors Gleam. reply weatherlight 8 hours agoparentprevIt's type safe, it's important to note that its type system is a Hindley–Milner type system. Same type system family as OCaml, Haskell, Standard ML etc. Its syntax is heavily inspired by Rust. whereas Elixir's syntax is inspired by Ruby and Erlang's inspired by prolog. https://en.wikipedia.org/wiki/Hindley–Milner_type_system reply abrookewood 9 hours agoparentprevAlso worth remembering that Elixir is now a 'gradually typed system': https://news.ycombinator.com/item?id=38914407 reply williamdclt 7 hours agorootparentI don't know what José meant by \"officially\", but as far as I could see it's still not something that can be actually used. I read it'd be available in 1.17.0 which still isn't out. I found the comms around typing very confusing, it took me an hour to figure out \"is there some damn typing in Elixir or is there not??\" reply andypants 6 hours agorootparentThe RC for 1.17 is out, and the notes will tell you what type checks are included: https://github.com/elixir-lang/elixir/releases/tag/v1.17.0-r... reply gregors 7 hours agorootparentprevCompletely agree. I think they were just excited to make progress but it's not out for anyone to use. I was definitely confused by the comms. reply mtndew4brkfst 6 hours agorootparentprev1.17 is in release candidate as of last week, but keeping your expectations moderate is definitely still advisable. reply guims767 9 hours agoparentprevIt advertises itself as having a \"familiar and modern\" syntax where Elixir has a more ruby-like one. reply brianzelip 8 hours agoprevHere's an informing podcast episode with Gleam's maintainer, https://changelog.com/podcast/588. reply OtomotO 9 hours agoprevI've been experimenting with gleam the last couple of days. Not sure whether I will stick with it or leptos/axum or leptos/salvo.rs but I find it very pleasing to work with and the community is super nice! reply SwiftyBug 7 hours agoprevI'm jumping in the Gleam wagon as soon as something like Elixir's Phoenix LiveView is available. reply hayleighdotdev 7 hours agoparentHey there, I maintain a library a called lustre [0] that does similar things to liveview. The linked docs are the API reference, but there's not currently a guide or a robust example on how to put things together (there's a bare minimum one in the repo you can find) – that'll be changing soon, I was just giving a talk about this at Lambda Days ^.^ In the interest of fairness, there's also sprocket [1] which leans much heavier into thing that you might also what a check out. [0]: https://hexdocs.pm/lustre/lustre/server_component.html [1]: https://sprocket.live reply SwiftyBug 6 hours agorootparentThat's seriously cool! I'm going to try and give this a test this weekend. reply heeton 6 hours agoparentprevI've been thinking of creating an app with a type-safe core in gleam, and then using elixir/phoenix for its more mature web / live interfaces. reply SwiftyBug 6 hours agorootparentCan you run Elixir and Gleam concurrently in the same VM and exchange messages between them? reply lawn 6 hours agorootparentYes, they both run on the Beam VM and you can seamlessly call the code directly or pass messages between processes written in Gleam or Elixir (or Erlang). reply 2wrist 9 hours agoprevClass! It’s a lovely language and a nice community. Love to see it make progress. reply imadj 9 hours agoprevRelated: Gleam: a type safe language on the Erlang VM - https://news.ycombinator.com/item?id=38183454 - Nov 2023 (242 comments) Gleam: A statically typed language for the Erlang VM - https://news.ycombinator.com/item?id=22902462 - Apr 2020 (109 comments) Gleam 0.15 - https://news.ycombinator.com/item?id=27061500 - May 2021 (78 comments) reply gdsdfe 7 hours agoprevThat actually looks like an interesting language, I wonder if it's easy to target wasm with Gleam !? reply ModernMech 6 hours agoprevI thought that since Gleam is build for BEAM, it should be good at concurrency? I thought that was its whole angle, but there's not a mention of concurrency at all in the docs, it's not built into the standard library, and apparently this is all they offer: https://hexdocs.pm/gleam_otp/ If Gleam is good at concurrency why aren't they telling anyone about it, and if it's not good at concurrency what even is the point of it? reply mcintyre1994 5 hours agoparentTheir home page says it is: > Running on the battle-tested Erlang virtual machine that powers planet-scale systems such as WhatsApp and Ericsson, Gleam is ready for workloads of any size. > Thanks to a multi-core actor based concurrency system that can run millions of concurrent tasks, fast immutable data structures, and a concurrent garbage collector that never stops the world, your service can scale and stay lightning fast with ease. It's accompanied by a code example with 200,000 async tasks being spawned. reply ModernMech 5 hours agorootparentIndeed, yes that bit on the homepage is what caused me to write what I did. I ran that code, but it does not run; it tells me that \"task\" is not defined. I read the \"language tour\", but it wasn't covered. So I went to the standard library to find out what \"task\" is, but it's not there. I went to the docs to learn what \"async\" is, but it's not mentioned. I went to the \"gleam for rust users\" doc because I was hoping it would give me a \"gleam vs rust\" concurrency primer, but no luck. Based on this, I feel that concurrency is maybe an afterthought in Gleam, which surprised me, because that's not what I had expected seeing as it runs on BEAM. reply mcintyre1994 3 hours agorootparentGot you. I guess it comes from `gleam/otp/task` in the link you posted, the API matches. But it does seem to be missing from eg the language walkthrough. If I had to guess it'll be because it's a light wrapper around Beam/OTP primitives, and the initial audience for Gleam was probably developers already using the Beam with eg Elixir and familiar with its concurrency already. It definitely looks like it's expanded beyond that though. reply cess11 3 hours agorootparentprevLoad the module: https://hexdocs.pm/gleam_otp/gleam/otp/task.html import gleam/otp/task How modules work is covered by the language tour, and to do something interesting you'll probably want more than just the task module. The concurrency stuff isn't as fleshed out in Gleam as it is in Elixir or Erlang, as I understand it because it's not the project's focus and due to how the type system works. reply lawn 3 hours agorootparentI think I've heard that it's implemented as a library because we're not sure about the best way to implement the concurrency stuff. It's easier to experiment in a library than in the core. reply cess11 3 hours agorootparentRight, yeah, maybe I read that it's something like the typing system putting constraints on direct interop, I'm not sure. I mostly follow from the sidelines, having a deeper buyin with Elixir, but whenever I find the time and a use case I'll build something in Gleam and learn more. reply avsteele 7 hours agoprev [–] Interesting language. Great web page. I find it off-putting to have politics as the third thing on the home page though. Makes me think there is going to be a scissor that cuts the community apart at some point. (\"As a Gleam community, we need to have a position on Israel-Palestine!\") reply brabel 4 hours agoparentWhat I find more offputting is that they aim to be international but mention things that are clearly American-centric (BLM is not an international thing, you know? In most other countries, all people's lives matter and that needs not be said. Saying that in Africa may even be offensive as that kind of implies it wasn't clear that their lives mattered until someone said it?! To me it's almost like a Serbian project mentioning that only people who support Bosnia's independence are welcome - without trying to make any sort of equivalence except for the absurdity of \"local\" issues spilling outside). reply SwiftyBug 7 hours agoparentprevActually I think this is pretty smart. This will deter people who disagree with those values to even participating in the community. reply avsteele 6 hours agorootparentYes, i agree it does that effectively now. But what about future issues? See my post below. reply boxed 7 hours agorootparentprevHow about just getting the work done? reply heeton 6 hours agorootparentIt's somewhat naive to think that politics and life are not intertwined. There is no \"getting the work done\" without a huge amount of hidden politics. Bravo to this community for being explicit about it. reply boxed 6 hours agorootparentCalling your political opponents, or even people on the fence, \"nazis\" is a bit much though. reply Morzaram 1 hour agorootparentSeems like you're reading into this. They literally say \" As a community, we want to be friendly too. People from around the world, of all backgrounds, genders, and experience levels are welcome and respected equally. See our community code of conduct for more. Black lives matter. Trans rights are human rights. No nazi bullsh*t. \" Considering quite a few contributors in the Gleam community are LGBTQ+ it's seems justified that they want to stand for their contributors rights and safety. reply sussmannbaka 7 hours agoparentprev [–] you will find that not allowing Nazis and the likes into your community generally ties it together, rather than cuts it apart. reply avsteele 6 hours agorootparentUntil it doesn't. It isn't the case that even if everyone agrees on the BLM, Trans rights, and the outcome of WWII will agree on future issues of the day. Thier stance sets up an expectation that the language should have an official position on these topics. I even gave an example. A community that fosters professionalism rather than explicit ideology won't have this problem. reply cess11 3 hours agorootparentThe professional thing to do is to not cooperate with people that support obviously criminal states. Don't work with iranians spying on refugees. Don't work with zionists that support the ongoing crimes against the palestinians. Don't work with people that support the russian invasion of Ukraine. It's pretty much the same as don't enter into professional relations with the local mafia, it's mostly a matter of scale and available weaponry. If I recruit you into my organisation I trust that you will not bring in criminals into our enterprise. I would not hire you if I thought that it could bring attention from the fuzz or expose us to extortion. reply SSLy 5 hours agorootparentprevI'll bite, does \"the likes\" include tankies? reply boxed 7 hours agorootparentprev [–] \"And the like\" including people who see that BLM has accomplished not even nothing, but destroyed the small chance of taking the issue of police brutality seriously. It's not a black vs white thing. It's a police vs civilian thing, and a problem of police being under trained. People who want to stop people from dying by taking the problem seriously: \"nazis\". :( reply boxed 6 hours agorootparentInstead of down voting, do watch some videos from Gracie Breakdown: https://www.youtube.com/watch?v=itYfWN65t2A reply patmorgan23 6 hours agorootparentprev [–] Wow you read a whole lot of your own biases into that. Says more about you than the project. reply boxed 6 hours agorootparent [–] \"nazis and the like\" implies the other things listed in the same list I think. If the poster didn't mean that then sure. But I think he did. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Gleam, a type-safe and scalable language for the Erlang VM and JavaScript runtimes, has released version 1.2.0, focusing on enhancing the language server and developer experience.",
      "Key updates include fault-tolerant compilation, better import statement autocompletion, single-line pipelines in the code formatter, refined error messages, and an auto-fix for redundant pattern matching.",
      "A new error message helps JavaScript programmers correct the use of `===` to `==`, and the release acknowledges contributions from various developers, encouraging sponsorship for ongoing development."
    ],
    "commentSummary": [
      "The Gleam 1.2.0 release improves fault tolerance and developer experience, featuring modern syntax and a robust Hindley-Milner type system.",
      "Gleam compiles to both Erlang's Beam VM and JavaScript, offering good performance and is compared to Elixir, which is moving towards gradual typing.",
      "The active community supports projects like the Lustre frontend framework and the Nestful web app, though some users find the documentation lacking in type safety and concurrency areas."
    ],
    "points": 140,
    "commentCount": 48,
    "retryCount": 0,
    "time": 1716879335
  },
  {
    "id": 40502956,
    "title": "Openkoda: Open-Source Platform for Rapid Business App Development",
    "originLink": "https://github.com/openkoda/openkoda",
    "originBody": "Ready-to-use development platform that accelerates the process of building business applications and internal tools. Reduce development time and effort. Use pre-built functionalities and out-of-the-box features. Adopt a flexible and scalable approach. Build applications with dynamic entities. Choose from multiple multi-tenancy models. Use technology you already know: Java, Spring Boot, JavaScript, HTML, Hibernate, PostgreSQL Extend as you wish. Openkoda offers unlimited customization and integration options. 📌Contents 🧩 Integrations 🚀 How to start ✅ Out-of-the-box features 👨💻 Tech stack 💡 Sample applications 💡 Application screenshots 💙 Contribution 📜 Release notes 🤝 Partners 🧩 Integrations Enhance your application by adding integrations. Open Source Enterprise 👨💻 Tech stack Java (17+) Spring Boot 3.x Hibernate PostgreSQL GraalVM 🚀 Getting started Installation There are two installation options to start application development with Openkoda: Building from sources Running as a Docker container Option #1: Build from Source Prerequisites: Git, Java 17+, Maven 3.8+, PostgreSQL 14+ Create an empty database Clone or download this Git repository Build application with maven: mvn -f openkoda/pom.xml clean install spring-boot:repackage -DskipTests Initialize the database in a first run: java -Dloader.path=/BOOT-INF/classes -Dspring.profiles.active=openkoda,drop_and_init_database -jar openkoda.jar --server.port= Run Openkoda java -Dloader.path=/BOOT-INF/classes -Dsecure.cookie=false -jar openkoda.jar --spring.profiles.active=openkoda --server.port= Detailed instructions can be found in the Installation manual. Option #2: Run as a Docker Container Docker images are available at Docker Hub : https://hub.docker.com/r/openkoda/openkoda It can be launched via simple: docker pull openkoda/openkoda:latest Please note that in that case Postgres needs to be already in place and SPRING_DATASOURCE_URL, SPRING_DATASOURCE_USERNAME, SPRING_DATASOURCE_PASSWORD env variables needs to be adjusted when running docker (see Docker Hub for detailed options) Docker compose A simpler option may be to use the Docker Compose scripts located in the: ./docker/docker-compose.yaml and ./docker/docker-compose-no-db.yaml - depending on your preference, with or without Postgres as a part of the docker service. Here is a useful one-liner : curl https://raw.githubusercontent.com/openkoda/openkoda/main/docker/docker-compose.yamldocker compose -f - up ✅ Out-of-the-box features To significantly reduce development time and effort, Openkoda offers pre-built functionality and out-of-the-box features. 🔀 Dynamic entities: Create database table, CRUD functionality, form, and overview with no need of re-compilation 🛠 Application admin panel: App Configurations: Manage email settings, roles, privileges, and HTML templates. Audit Screen: Track changes to data for accountability. System Logs: Review logs for activity insights and troubleshooting. System Health: Get a quick overview of system performance and status. 👤User Management Invite users to the organization Set roles globally and within the organization context Access user profile settings Spoof user (available in admin mode) 🔑 Roles and Privileges Create global or organization-specific roles Assign privileges from a list to each role 🏢 Organization management Separate organization data Implement security rules for data access Customize your own dashboard Assign organizational roles, such as member or admin, to users. 📝 CMS Modify HTML templates Edit draft versions of resources Introduce new public resources 🗂 Resource Management Manage file overview Resize images Set files to public access 🔊 Event Listeners: Respond to application events (e.g., user creation, login, application start) with built-in Openkoda handlers (e.g., messaging, push notifications). 💾 Backups: Embedded database backup routines 📥 Import and export: Export components from current app and easily import them into another Openkoda Core instance 🗄 Multiple Multi-tenancy models: Openkoda supports the following multi-tenancy setups: Single Database / Single Schema Single Database / Many Schemas Multiple Databases / Many Schemas See multitenancy setup for more details 🔄 Job Requests Schedule jobs to be performed in time intervals Process jobs with event listeners 🔔 Notifications Synchronize your application with notifications channels: Email Slack Jira GitHub Trello Basecamp ✉ Email Sender Customize email templates via CMS Schedule emails 💡 Sample applications Openkoda Application Templates are sample applications built with Openkoda. They represent a standard set of functions for a traditional web application provided by Openkoda Core, as well as business functionalities created specifically for these examples. Application Templates can be easily extended, taking into account both the data storage schema and any custom functionality. Learn more in our 5-minute guide. Timelog Timelog is a time tracking solution for companies of all sizes. It allows employees to record hours spent on specific tasks, while managers generate monthly performance reports. Learn more. Insurance Policy Management Insurance Policy Management is a dynamic policy data storage tool with a variety of embeddable widgets for personalized customer and policy dashboards. Widgets include: message senders, email schedulers, attachment and task lists, notes, and detailed customer/policy information to improve operational efficiency and customer engagement. Learn more. Weather App A sample application that provides weather forecast for selected vacation spots. Watch the short video to see the building process: 💡 Application screenshots CMS Organization Settings Job Request Event Litener Forgot Password 💙 Contribution Openkoda is an open source project under MIT license. It’s built by developers for developers. If you have ideas for improvement, contribute and let's innovate together. How to contribute: Create a fork Create a feature branch from main branch Push Create a Pull Request to an upstream main branch Detailed contribution rules 📢 Follow, learn, and spread the word Openkoda Community: Become a part of Openkoda YouTube: Learn how to use Openkoda LinkedIn: Stay up to date About us: Let us introduce ourselves 🗃 Release notes Openkoda is constantly evolving. Check out the changelog: Openkoda 1.5. 🚀 Dynamic Entities: Now create database tables, perform full CRUD operations and generate forms. New Dashboard UI: Enhanced for better readability and smoother navigation flow. Files Assignment: Support for dynamically registered entities. Organization-Level Email Configuration: Customize email settings at the organization level. Bug Fixes: Various fixes for improved app stability and performance. Openkoda 1.4.3. Page Builder: Introducing a tool for creating custom dashboards. Web Forms Assistance: Streamlined web form creation based on your data model definitions. YAML Components Import/Export: Easily manage components such as web forms, endpoints, server code, event listeners, schedulers, and frontend resources. Dashboard UI: Upgrades for an improved dashboard interface. Updates & Security: Minor adjustments and security fixes. 🤝 Partners Openkoda source code is completely free and is available under the MIT license. Join us as a partner in transforming the software development market by delivering maximum value to your clients using Openkoda. The goal is to simplify the process of building enterprise applications, allowing developers to focus on core business logic. Learn more about Openkoda Partner Program. ☁ Managed Cloud Our enterprise managed cloud allows for easy deployment and scaling of your Openkoda applications. Contact us for more information.",
    "commentLink": "https://news.ycombinator.com/item?id=40502956",
    "commentBody": "Openkoda – Open–source, private, Salesforce alternative (github.com/openkoda)139 points by mgl 1 hour agohidepastfavorite53 comments righthand 1 hour agoSo a CRM that replicates Salesforce APIs? Sales people don’t want a Salesforce FOSS version, that’s why they green light salesforce. It’s about trend not practical software. Only dev teams that have been implementing Salesforce for 5 years want a FOSS version. reply mgl 1 hour agoparentSalesforce is a generic application platform today, and this is how see it. Openkoda is not a drop-in replacement for Salesforce CRM, it is a useful replacement when you want to build your core business application a) retaining full source code ownership and ability to get any Java/JS team to work on it and run anywhere you want, b) without becoming dependent on technology and commercial limitations of working with big S. reply megadal 1 hour agoparentprevWell, there is Odoo. Which is pretty much exactly what OpenKoda is (FOSS ERP). Odoo is doing quite well. It made Fabien Panckaers the youngest billionaire in Belgium. reply tomrod 1 hour agorootparentOdoo's quest for monetization from open source has been a bit off-putting. I stopped using it a few quarters back due to that. Community and Enterprise are becoming too disjointed. reply megadal 58 minutes agorootparentThanks for the input. I've never used Odoo or OpenKoda, just read a lot about both and was impressed. Sad to hear about Odoo's disjointed approach to enterprise monetization. Hopefully OpenKoda takes note. reply mgl 53 minutes agorootparentPoint taken. reply mgl 1 hour agorootparentprevYes, I think you could compare Openkoda with Odoo, but well... we are nowhere near being billionaires ;) reply megadal 1 hour agorootparentAssuming you are apart of the team, I'm sure you know Odoo is 20 years old. So, I can't see why in time there couldn't be real competition. OpenKoda and Odoo actually have sparked interesting questions for me about what an Open source ERP market would look like. One conclusion I came to is as opposed to vendor lock-in as most ERP/CRM products try to enforce, it would actually be better to go the opposite direction (high compatibility with existing alternatives). That said, have you guys considered allowing imports from Odoo into OpenKoda or other deep integrations? In theory, I feel like you can run Odoo apps in OpenKoda, or even vice versa. The experience would be suboptimal but being split between two ERP systems is too. reply abraae 2 minutes agoprevWhere can one find the breakdown of features on the free vs enterprise versions? reply creaktive 1 hour agoprevI was just complaining to my partner about the OG Salesforce… I feel extra inspired to check this out! reply mgl 1 hour agoparentThanks, I really appreciate your words! When talking to our users (and clients - as we customize Openkoda for enterprise companies building their bespoke applications as well) so many of them are tired of Salesforce being: a) slow, b) limited, c) expensive (probably in this order). reply llsf 40 minutes agoprevConsidering the tag line \"Ready-to-use development platform that accelerates the process of building business applications and internal tools\", I would think OpenKoda would compete with Retool more than Salesforce, no ? reply grepLeigh 1 hour agoprevSalesforce's biggest value proposition is the partner ecosystem, which Salesforce has cultivated for like 20+ years. Some platform companies (Shopify comes to mind) cannibalize their plugin ecosystem by adding similar functionality to the core platform, but Salesforce deliberately avoids competing with software/services in the partner ecosystem. It's such a safe bet to build on top of Salesforce, because there's basically 0 platform risk and the technology is ubiquitous. What's your approach to plugins, add-ons, and service partners? reply finnh 1 hour agoparentI am not sure I'd agree with that - at one point Salesforce tried to buy my company, which was a best-of-the-AppExchange partner, and their justification for their (very) low offer was \"that's what it would cost us to copy you\". We sold the company 2 years later for 10x their offer, without a ton in the way of new revenue (our AppExchange ranking mysteriously tanked after we turned down their offer). reply htrp 39 minutes agorootparent> AppExchange ranking mysteriously tanked after we turned down their offer This alone would be worth a blog post (assuming you don't have any non disparagements with CRM) reply finnh 15 minutes agorootparentThis was over a decade ago, so happily not something I'm feeling too sharply nowadays. It all worked out better in the end. reply mgl 1 hour agoparentprevWe actively look for service/technology partners, so if you want to build an application or extension on top of Openkoda for you or your clients we would be more than happy. Openkoda Core is released under MIT license, so we have no means to stop you! reply cooperadymas 58 minutes agoparentprev> but Salesforce deliberately avoids competing with software/services in the partner ecosystem It helps that Salesforce takes a huge slice from sales in the AppExchange. This isn't entirely true, though. Salesforce does, at times, position itself against its own ecosystem. Their recent attempt at devops, no matter how feeble it is, directly competes against major players like Copado and Gearset. Mulesoft and some other ventures over the years should have theoretically relegated a huge multitude of sync apps to irrelevancy, if Salesforce had been able to execute better on them. They launched a payment system a couple of years ago that competes with some other top marketplace options. There are dozens of examples like this. reply echelon 1 hour agoparentprev> Some platform companies (Shopify comes to mind) cannibalize their plugin ecosystem by adding similar functionality to the core platform, but Salesforce deliberately avoids competing with software/services in the partner ecosystem When do you compete, and when do you cultivate? Are some business sectors better at one versus the other? reply eddythompson80 1 hour agorootparentAlso insert the standard HN comment of “I can’t believe it’s [CURRENT_YEAR] and this functionality isn’t part of the core platform itself” reply 999900000999 1 hour agoprevLooks cool, but honestly what companies need a Salesforce like too but can't afford Salesforce force ? I'm still have to host this thing somewhere, if I'm bootstraping a startup I might as well rely on Excel until I can afford Salesforce. I like the project though. Best of luck. reply mgl 58 minutes agoparentThat's a great question and we have great answers from actual clients - the repeating pattern goes as follows: \"We invested three years building this app, it's a booming success, but their user-based pricing is killing us\" \"We can't run complex queries\" \"It's too slow\" \"We feel our data is stuck there\" \"I want to own my code and my application\" \"Why do I need to use APEX where we use Java and JS everywhere else in the company\" reply 999900000999 33 minutes agorootparentOkay so this isn't really for end users, it's not for Billy Bob's shop which needs to use Salesforce to track cupcake orders, but rather a vendor who would sell Salesforce like software to Billy Bob with some light customization on top. Understood, thank you for your answer and I wish you the best of luck! reply mgl 26 minutes agorootparentHonestly, I don't think there are many (revenue speaking) Billy Bob's shops using Salesforce to track their cupcake orders out there. reply vantubbe 1 hour agoprevHow would this compare with something like goHighLevel? reply mgl 1 hour agoparentThanks for this question. I understand that goHighLevel is a closed marketing application platform, whilst Openkoda is an open-source enterprise application platform with pre-built application templates for a quick start in specific industries (now: insurance and realestate, more to come). reply tootie 1 hour agoprevIs this meant to be a drop-in replacement for Salesforce or just another CRM? There's a built-in advantage to choosing the tool that everybody uses and that's compatibility. If you look at 1000 data integration or marketing automation tools on the market, 1000 of them will have OOTB integration with Salesforce as a selling feature. Just to be realistic, building a better CRM won't be enough to replace Salesforce. reply mgl 1 hour agoparentSalesforce nowadays is more a universal, business application platform. Closed, proprietary, with user-based pricing model. reply mateus1 1 hour agoparentprevNothing can be a drop-in replacement for Salesforce as it is designed almost exclusively to lock-in customers. reply llmblockchain 1 hour agoprevIn true enterprise fashion, they went with Java. Enterprise, confirmed. reply manuelabeledo 1 hour agoparentHonest question, what's wrong with Java? It's performant, has mature libraries, good documentation, and supported pretty much everywhere. reply moooo99 8 minutes agorootparentThere is nothing inherently wrong with Java, people just love to joke about it. To some extent, justifiably so. Not necessarily because of the language, more because of the common patterns in established there reply promptingmetosi 1 hour agorootparentprevThere's plenty I'd like to improve but it's honestly in a very good shape. There's a reason enterprise picks Java - it's battle tested and there are enough quality developers around. Frameworks like Spring for webdev are top notch. Unless you've got a very specific reason to pick something else (python or rust or whatever) Java is the safe bet (along with c# but Java is an order of magnitude bigger I think). It's just people taking memes seriously. Java is great and continuously improving. reply xuancanh 25 minutes agorootparentIt's not just a meme. The developer experience in Java is worse compared to some other popular programming languages like JavaScript, Go, Python, etc. The language is a bit verbose, and the compiling speed is slow, hence the development speed is slower. Java developers tend to overly abstract things, so the code tends to be unnecessarily complicated. The JVM also has a high memory footprint, the startup speed is slow, and it requires warming up the JIT. Some popular libraries and frameworks overuse reflection and annotation, they are nice to use but are nightmares to debug when issues happen. This is why GraalVM and Kotlin have been gaining popularity recently, as they aim to address several issues with the JVM and Java. The biggest strengths of Java are its ecosystem and community. reply neonsunset 2 minutes agorootparentprevShould have been C# with ASP.NET Core and EF Core instead. Hibernate is extremely dreadful to work with and no one in their sane mind should even have the gall to ask users to touch it when better options exist. Spring Boot also performs very poorly against ASP.NET Core. reply llmblockchain 29 minutes agorootparentprevThere's nothing wrong with Java. It's old and boring, but most good tech is. reply candiddevmike 1 hour agoprevLast commit is 2 months ago? Are you still building it? reply mgl 1 hour agoparentYes, absolutely - the next release is scheduled for June, our current development cycle (which may not be ideal) is based on internal builds and testing before public release. The main reason is that there are e.g. insurance companies out there and whilst well, it's still open-source, we would not like to break anything for them. reply ActionHank 1 hour agorootparentCurious what testing is covered that you don't have automated as part of the CI? reply mgl 1 hour agorootparentThese are mainly either integration tests with hard-to-automate third-party platforms and tests in more complex multi-tenant deployment scenarios. There is also the Enterprise version which is developed in parallel with the open-source Core edition. Being a small team we just found it was easier to phase the release schedule. reply hellcow 1 hour agorootparentprevCould you merge consistently and just cut stable releases on a scheduled cadence? reply mgl 1 hour agorootparentYes, that's a good point - actually we were discussing this option (again!) just last week as we are cautious of not-that-frequent public releases. reply Timshel 1 hour agorootparentWithout it any contribution will probably be a pain to handle (and it's usually already not trivial to handle correctly ;). reply tcdent 1 hour agoprevAnybody have recommendations for similar projects written in Python? Curious to know the motivations for choosing Java in this case. reply bfmalky 1 hour agoparentI would guess java was chosen for its high quality libraries, efficient runtime and its static typing, which enables easy and safe refactoring. reply mgl 1 hour agorootparentExactly this. We find modern Java to be ubiquitous, fast, and (still) super popular in enterprise environments. reply creaktive 1 hour agoparentprevSalesforce is customized in something called Apex, which is a dialect of Java. Would make migration/integration more convenient reply jrklabs_com 1 hour agoparentprevYes, I haven't used this myself yet but I have been keeping my eye on this project for a while now, https://erpnext.com/ The Frappe framework that is the foundation of ERP Next can be used to extend the ERP platform or build standalone apps. reply likeafox 1 hour agoparentprev[Odoo is Python based](https://github.com/odoo/odoo) and the 'core' modules are open - Not providing any endorsement of the software nor can I say how the Odoo CRM/CMS compares to Salesforce. I've tried migrating a non Salesforce business to Odoo twice and haven't had success yet. reply fire_lake 1 hour agoparentprevPython is significantly more resource heavy than Java. This is not a notebook or quick script. What would Python offer here? reply evantbyrne 1 hour agorootparentObjectively, very little probably. Non-java shops tend to avoid it because java codebases have a reputation for being relatively complicated. Java is a great language though and it may very well be the best cultural fit for the type of org that _actually likes_ salesforce. They also seem to be publishing a docker image, so it shouldn't be difficult for non-java orgs to integrate. reply OutOfHere 46 minutes agorootparentprevPython is more resource heavy than Java for CPU, but not for memory. I am pretty sure that Java is more resource heavy for memory. Also, Java programmers have the habit of using excessive abstractions that don't solve real problems. reply snagglemouth 1 hour agoprev [–] You already posted a Show HN like 10 days ago. Why are you posting again? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Openkoda is a development platform aimed at speeding up the creation of business applications and internal tools by providing pre-built functionalities and out-of-the-box features.",
      "It supports dynamic entities, multiple multi-tenancy models, and uses familiar technologies such as Java, Spring Boot, JavaScript, HTML, Hibernate, and PostgreSQL, allowing for extensive customization and integration.",
      "Openkoda is open-source under the MIT license, includes features like user management, CMS, job scheduling, and offers managed cloud services for easy deployment and scaling."
    ],
    "commentSummary": [
      "Openkoda is an open-source platform for building core business applications, offering full source code ownership and avoiding commercial limitations, unlike Salesforce.",
      "It is compared to other FOSS (Free and Open Source Software) ERPs like Odoo, but with fewer concerns about monetization, and is seen as a cost-effective alternative to Salesforce.",
      "Openkoda uses Java for its performance and mature ecosystem, despite debates about its verbosity, and provides Docker images to ease integration for non-Java organizations."
    ],
    "points": 138,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1716916316
  }
]
