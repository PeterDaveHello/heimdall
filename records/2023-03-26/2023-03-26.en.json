[
  {
    "id": 35297420,
    "timestamp": 1679702869,
    "title": "Gordon Moore, Creator of Moore's Law, Dies at 92",
    "url": "https://www.moore.org/article-detail?newsUrlName=in-memoriam-gordon-moore-1929-2023",
    "hn_url": "http://news.ycombinator.com/item?id=35297420",
    "content": "Gordon Moore, co-founder of Intel and the Gordon and Betty Moore Foundation, has passed away, aged 93. A pioneer in the semiconductor industry, Moore's prediction in 1965 that the number of transistors that could be placed on an integrated circuit would double every year for at least a decade formed the basis of the notion of exponential growth in technology known as \u201cMoore\u2019s Law\u201d. The prediction has proved remarkably accurate and was instrumental in directing a range of technological innovations since the 1970s. Moore was also a renowned philanthropist, who in 2012 joined the Giving Pledge campaign by committing to give away more than half his wealth to charitable causes.\nGordon Moore, the co-founder of Intel and creator of Moore's Law, has died at the age of 92. Moore's Law predicted that the number of transistors on a microchip would double every 18-24 months, resulting in a rapid technological acceleration. After leaving Intel, Moore and his wife founded the Gordon and Betty Moore Foundation, which has invested over $6bn in scientific research and environmental conservation. He was praised for his humble approach and belief in the improvability of the human condition. In an interview for his 50th anniversary celebration, Moore quipped that he avoided making another successful prediction after Moore's Law. He leaves behind a legacy that has revolutionized technology and pushed the boundaries of innovation.Discussions on a Hacker News thread range from praising the contributions of Robert Noyce and Gordon Moore to technology, to proposing new \"laws\" and \"axioms\" for innovation and problem-solving. Some users share personal anecdotes about their own experiences with electronics kits from the past, while others reflect on the \"legendary\" figures of innovation and how they're remembered. The thread also touches on the decline of Intel, and the importance of being \"paranoid,\" a trait attributed to Andy Grove. Overall, the thread represents a diverse range of perspectives on tech and innovation, with some users offering practical advice and others musing on the nature of progress itself.Gordon Moore, co-founder of Intel and creator of Moore's Law, has passed away at the age of 92. The law he first proposed in 1965 predicted that the number of transistors on a microchip would double every year while costs would decrease. The prediction, later revised to every two years, has largely held true for more than five decades, helping to drive the digital revolution. Moore co-founded Intel in 1968 with Robert Noyce. In a statement, Intel CEO Pat Gelsinger called Moore a \"pioneer whose leadership, intellect and humanity inspired generations of engineers\" and who \"made the world a better place\".Gordon Moore, co-founder of Intel and pioneer of Moore's Law, has passed away at the age of 92. Moore's Law, which he first proposed in 1965, posits that the number of transistors that can fit on a computer chip will double every two years. This principle drove the exponential growth of the computing industry for decades, as companies raced to produce smaller, faster, and more powerful chips. Moore co-founded Intel in 1968 with Robert Noyce and Andy Grove, and the company became one of the world's largest semiconductor manufacturers. In addition to his contributions to the computing industry, Moore and his wife co-founded the Gordon and Betty Moore Foundation, which focuses on advancing scientific discovery, protecting nature, and improving the quality of life for future generations.Gordon Moore's exponential rate that makes electronics faster, smaller and cheaper became the driving force behind the semiconductor industry, and millions of everyday products have chips. Moore helped set up the Gordon and Betty Moore Foundation, donating over $5.1 billion to charitable causes since its founding in 2000, which focus on philanthropy, environmental conservation, science, and patient care improvements during his lifetime. Comments on Moore's contribution to the industry reflect that although his eponymous law may be dead, his impact cannot be understated. People continue to confuse Moore's Law with Dennard scaling, and the discussion moves on to the sustainability of packing more density into chips without making them smaller.People are commenting on a news article about the passing of Gordon Moore, co-founder of Intel and inventor of Moore's Law. Many express their condolences and admiration for his contributions to the technology industry. Some comment on his legacy and the impact of his law, while others make sarcastic remarks or suggestions for the HN website to change its bar color in his honor. There are also a few comments regarding his supposed death earlier in the day. Intel is mentioned as well, with one comment referencing their intention to continue pursuing Moore's Law.",
    "summary": "- Gordon Moore, co-founder of Intel and creator of Moore's Law, has passed away at the age of 92\n- Moore's Law predicted the doubling of transistors on a microchip every 18-24 months, leading to rapid technological growth\n- Moore co-founded Intel in 1968 and later founded the Gordon and Betty Moore Foundation, which invested over $6bn in scientific research and environmental conservation\n- Comments on Hacker News reflect on Moore's legacy and impact on the technology industry, with some expressing condolences and others making sarcastic remarks\n- The discussion also touches on the decline of Intel and the sustainability of packing more density into chips without making them smaller"
  },
  {
    "id": 35299071,
    "timestamp": 1679715578,
    "title": "AI Language Models: Caution & Potential for Productivity Boost",
    "url": "",
    "hn_url": "http://news.ycombinator.com/item?id=35299071",
    "content": "Hacker News users discuss their experiences using AI language models, such as GPT and Copilot, for coding, creative writing, and other tasks. Some users find these tools helpful for increasing productivity and generating ideas, while others express concerns about the reliability and privacy concerns of these models. The discussion also touches on the potential biases and limitations of AI language models and the need for users to exercise caution and critical thinking when using them. Additionally, the conversation highlights the desire for self-hosted AI tools and the need for transparent regulation to ensure privacy and ethical use of these tools. Some users report positive experiences with ChatGPT, while others express skepticism about its therapeutic value and the accuracy of its advice. Overall, users agree that AI language models have the potential to revolutionize various industries, but their implementation must be approached thoughtfully and with caution.ChatGPT is being hailed as a useful resource for various topics such as contracts, visas, and troubleshooting. OpenAI's chatbot, GPT-3 is being discussed for its potential applications such as making writing easier and language learning, but its accuracy can be unreliable, and it shouldn't be relied on for health or travel planning. Users report utilizing ChatGPT for quick access to information, including creating plans and prototypes. It's also better than Google for quick access to information as it doesn't include ads or fabricated content. However, there are concerns surrounding the insidious nature of such AI tools as it can be used for advertising and promoting certain ideas.",
    "summary": "- AI language models are being used for coding and creative writing, with some finding them helpful for productivity and generating ideas.\n- However, there are concerns regarding their reliability, biases, and privacy implications, and users are urged to exercise caution and critical thinking when using them.\n- There is a desire for self-hosted AI tools and transparent regulation to ensure privacy and ethical use.\n- ChatGPT is seen as a useful resource for quick access to information and creating plans and prototypes, but there are concerns about its accuracy and potential misuse for advertising and promoting certain ideas."
  },
  {
    "id": 35304017,
    "timestamp": 1679760400,
    "title": "US Insurers Deny Legitimate Claims to Save Millions in Healthcare",
    "url": "https://www.propublica.org/article/cigna-pxdx-medical-health-insurance-rejection-claims",
    "hn_url": "http://news.ycombinator.com/item?id=35304017",
    "content": "Cigna, a leading US insurer, saves millions of dollars by having its doctors automatically reject claims on medical grounds without reviewing patient records, according to internal company documents seen by ProPublica and The Capitol Forum. During a two-month period last year, Cigna\u2019s doctors used a computer algorithm to reject over 300,000 requests for payment, averaging 1.2 seconds on each case. Critics have raised questions over whether Cigna's system constitutes a fair review process. California's former insurance commissioner, Dave Jones, suggested that the speed at which Cigna doctors review cases does not comply with the state\u2019s insurance laws. Cigna's review system saves money by preventing claims that the company considers unnecessary or harmful to patients from being paid.\nInsurance company Cigna has developed a system called \"Pay Enough, Don\u2019t Pay Too Much,\" or PXDX, that automatically denies certain medical claims without consulting the attending physician. The system focuses on tests and treatments that typically cost a few hundred dollars each, and is estimated to save the company millions of dollars in medical costs. However, critics argue that many patients may pay the bills rather than fight the appeal process. Former Cigna employees say patients are unaware of the PXDX denial process, and may not appeal the rejection. Cigna knows that not many appeals are expected to be filed. Whereas the PXDX system does provide room for patients and their doctors to appeal a medical director\u2019s decision to deny a claim, the company expects only 5% of rejections to be appealed.A Reddit thread about a person's frustrating experience with hospital billing in the US has turned into a discussion about the need for healthcare reform. The original post described how the individual had to fight for insurance coverage after a surgery, with the insurance company denying their claim and the hospital overcharging them. One commenter advised sending a letter detailing the situation to the insurance company's liability insurance provider and suggested it would \"get you an highly aggressive advocate with teeth\". As the discussion continued, many people expressed their frustration with the current healthcare system, suggesting that it needed radical reform. However, some commenters also pointed out the difficulties of making changes to the system.The text recounts a personal experience dealing with the American healthcare system, in which an insurer denied coverage for a necessary surgical procedure. The author had to provide multiple statements and appeals before eventually discovering that the denial was based on the review of an individual doctor who had a history of performance issues. The process was frustrating and time-consuming, and the insurer did not pay the full amount promised. The comments show support for the author's experience, with many sharing similar struggles with the healthcare system. Some suggest solutions such as mandating that insurers pay for treatments recommended by doctors or creating a system for ongoing coverage without yearly bureaucratic hurdles. Overall, the text highlights the inefficiencies and frustrations of the American healthcare system.A report by Kaiser Health News highlights how health insurance giant Aetna has been systematically denying coverage for emergency room visits, with one doctor hired by the company reportedly approving some claims in as little as 10 seconds. Transactions for \u201cavoidable ER visits,\u201d remain covered by Aetna, but the list of what it defines as \u201cemergencies,\u201d which will be covered, has shrunk over time. Aetna denies any wrongdoing and says it is following industry practices, but doctors argue the change represents a violation of federal law which can result in life-threatening consequences for affected patients. Aetna, which is owned by pharmacy chain CVS Health, faces multiple complaints from doctors and hospitals, and has also been sued by some of its own policyholders.\nCigna's automated review system that uses AI algorithms to flag mismatches between diagnoses and what it considers acceptable tests for ailments has been accused of wrongfully denying claims. Commenters on Hacker News argue that Cigna's system uses an error-prone automation process and unnecessary complexity involving multiple diagnostic codes. Those wrongfully denied healthcare are unable to determine the reason for denial, and are only able to have their claims approved after going through the appeals process, which can often be lengthy. Commenters argue that health insurance companies that blatantly deny claims should be treated as committing insurance fraud, and propose that every health insurance denial should come from a named doctor personally and professionally liable for the health outcomes. Some suggest the need for a CFPB for healthcare similar to the Consumer Financial Protection Bureau.This online discussion revolves around Cigna\u2019s policy of denying medical claims on the first submission, only to fulfill some on appeal. Individuals are sharing their personal stories about healthcare woes, such as rejection of mental healthcare and prescriptions for their children, difficulty finding good doctors, and paying a ton of money for insurance that is not worth the cost. The conversation covers several aspects of the healthcare industry, including the shortage of doctors, no negotiations on drug and medical device prices, and healthcare becoming a free market in which anyone should be allowed to practice medicine. Participants call for consequences for doctors who testify in court cases with little knowledge of their subject and suggest suing Cigna in a class action type lawsuit. They also discuss the benefits and drawbacks of Europe\u2019s healthcare compared to the US.The US health insurance industry is facing criticism for denying insurance claims in a bid to maximise profits. Medical professionals and other social media users are calling out the industry for interfering with patient care, causing deep harm in some instances. In the absence of proper institutional regulation, insurance firms are exploiting patients who are excluded from selecting health insurers. Health industry experts believe fundamental change is needed in the sector, although the US's heavy pro-commercial lobbying environment makes this difficult. It is said that a non-profit structure, together with strong regulations and competition, could offer an alternative with better conditions for both healthcare providers and patients.\nInsurance companies in the US are routinely denying legitimate claims and using automated systems to do so, saving millions in the process. The approach is one in which claims are automatically rejected, with the insurer only viewing a small percentage of rejected cases that are subsequently appealed or contested, according to insiders. There are also suggestions that doctors themselves are being pressured to take part in the practice. The automated approach is increasingly popular but has led to increasing numbers of lawsuits being filed against insurers accused of denying legitimate claims. Critics says the practice highlights systemic issues within the US's medical insurance system that prioritises profits over patient outcomes.A ProPublica investigation has revealed how health insurer Cigna authorized blood tests that turned out to be medically unnecessary, over-charged patients for the tests, and made it difficult for doctors to provide alternative treatments. The tests were carried out by medical laboratories Health Diagnostic Laboratory and Singulex, which colluded with Cigna to create a billing model in which doctors were paid for authorizing batches of medically unnecessary tests, rather than for providing genuine medical care. Authorities allege that the scheme was a fraud. Cigna, which has accused the labs of misleading it, insists it is \u201ccommitted to the highest standards of business ethics\u201d. Concerns have been raised about medical ethics among US health professionals, while others argue the situation is due to inadequate regulation of the healthcare sector.Many commenters on Hacker News are discussing their experiences with healthcare in the US compared to other countries. Some argue that the US has higher quality care, while others argue that the lack of accessibility and affordability makes it a worse system overall. The cost of healthcare is a significant issue, with many people struggling to afford necessary procedures and treatments. Some commenters point out that employer-provided healthcare can significantly reduce costs, but this is not accessible to everyone, particularly those who are unemployed or self-employed. There are also concerns about the unpredictability of healthcare costs and the lack of transparency in pricing. Overall, many argue that a publicly funded and regulated healthcare system is necessary in the US, despite potential drawbacks.The American healthcare system has become a \"legal mafia cartel,\" according to a recent discussion on Hacker News. In many areas, medical groups dictate pricing for related procedures, but most insurance carriers don't want to pay these prices, resulting in some providers not being covered. This can lead to long waits for appointments as patients search for a covered provider. The high costs of healthcare in the US have led some to seek medical treatment abroad, where prices can be significantly lower. Private healthcare in the US is expensive, with an MRI costing between $1,000 and $8,000, but it can cost as little as $200 in other countries. The issue of insurance companies denying claims and delaying treatment for terminal illnesses was also discussed. Overall, the American healthcare system was described as \"brutal\" and in need of significant changes.Delaying payment in the US healthcare system can result in delayed treatment, as most patients cannot afford to pay hospitals upfront without the promise of reimbursement. Refusal to pay can lead to withholding treatment, and some insurance companies take advantage of this fact. AI decision-making and lack of human empathy may exacerbate the issue. Some individuals report having excellent experiences with the US healthcare system, but others recognize the potential benefits of a public healthcare system that would ensure equal access to care for everyone, regardless of income or insurance status. Data shows that the US spends more per capita on healthcare than most wealthy countries but has worse health outcomes, highlighting the need for reform.The comments section of an online article discussing the pros and cons of public versus private healthcare reveal a mix of personal experiences and opinions. One commenter who moved from a third world country with both socialized and private healthcare to the US pays \"vastly more for worse healthcare than anywhere I've lived.\" They dislike the fact that insurance can deny a doctor-recommended procedure without medical qualification. However, other commenters argue that healthcare quality depends on the specific city/state and insurance plan chosen. Both public and private systems have bureaucracy and issues with wait times or access to certain drugs or treatments. Some commenters also criticize the potential for government-run \"death panels\" made up of unqualified bureaucrats, while others argue that private insurance companies already make similar life-or-death decisions without medical expertise.",
    "summary": "Leading US insurance companies are accused of wrongfully denying claims to save millions on healthcare costs, with some using computer algorithms to reject claims without reviewing patient records. Patients often end up paying bills rather than fighting the appeals process. Critics suggest that the automated approach highlights systemic issues prioritising profits over patient outcomes. Some healthcare reform is called for, including a move towards a non-profit structure, stronger regulations, and increasing competition to improve conditions for both healthcare providers and patients. Many individuals express frustration with the inconsistent and unpredictable nature of the US healthcare system, with some advocating for government-run healthcare as a potential solution."
  },
  {
    "id": 35305655,
    "timestamp": 1679769614,
    "title": "Scrape Websites with GPT using scrapeghost Library",
    "url": "https://jamesturk.github.io/scrapeghost/",
    "hn_url": "http://news.ycombinator.com/item?id=35305655",
    "content": "\n\n\n\n\n\n\n\n\n\n\n\nscrapeghost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n          Skip to content\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            scrapeghost\n          \n\n\n\n            \n              About\n            \n          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Initializing search\n          \n\n\n\n\n\n\n\n\n\n\n\n\n    jamesturk/scrapeghost\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    scrapeghost\n  \n\n\n\n\n\n\n    jamesturk/scrapeghost\n  \n\n\n\n\n\n\n          About\n          \n\n\n        About\n      \n\n\n\n      Table of contents\n    \n\n\n\n    Quickstart\n  \n\n\n\n\n    Command Line Usage Example\n  \n\n\n\n\n\n\n    Features\n  \n\n\n\n\n\n\n        Tutorial\n      \n\n\n\n        OpenAI / GPT\n      \n\n\n\n        Usage\n      \n\n\n\n        FAQ\n      \n\n\n\n\n          Reference\n          \n\n\n\n\n          Reference\n        \n\n\n\n        Command Line Interface\n      \n\n\n\n        API Reference\n      \n\n\n\n\n\n\n\n          About Scrapeghost\n          \n\n\n\n\n          About Scrapeghost\n        \n\n\n\n        Hippocratic License\n      \n\n\n\n        Code of Conduct\n      \n\n\n\n        Changelog\n      \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      Table of contents\n    \n\n\n\n    Quickstart\n  \n\n\n\n\n    Command Line Usage Example\n  \n\n\n\n\n\n\n    Features\n  \n\n\n\n\n\n\n\n\nAbout\u00b6\n\nscrapeghost is an experimental library for scraping websites using OpenAI's GPT.\nThe library provides a means to scrape structured data from HTML without writing page-specific code.\n\nImportant\nBefore you proceed, here are at least three reasons why you should not use this library:\n\n\nIt is very experimental, no guarantees are made about the stability of the API or the accuracy of the results.\n\n\nIt relies on the OpenAI API, which is quite slow and can be expensive.  (See costs before using this library.)\n\n\nCurrently licensed under Hippocratic License 3.0.   (See FAQ.)\n\n\nUse at your own risk.\n\nQuickstart\u00b6\nStep 1) Obtain an OpenAI API key (https://platform.openai.com) and set an environment variable:\nexport OPENAI_API_KEY=sk-...\n\nStep 2) Install the library however you like:\npip install scrapeghost\n\nor\npoetry add scrapeghost\n\nStep 3) Instantiate a SchemaScraper by defining the shape of the data you wish to extract:\nfrom scrapeghost import SchemaScraper\nscrape_legislators = SchemaScraper(\n  schema={\n      \"name\": \"string\",\n      \"url\": \"url\",\n      \"district\": \"string\",\n      \"party\": \"string\",\n      \"photo_url\": \"url\",\n      \"offices\": [{\"name\": \"string\", \"address\": \"string\", \"phone\": \"string\"}],\n  }\n)\n\n\nNote\nThere's no pre-defined format for the schema, the GPT models do a good job of figuring out what you want and you can use whatever values you want to provide hints.\n\nStep 4) Passing the scraper a URL (or HTML) to the resulting scraper will return the scraped data:\nresp = scrape_legislators(\"https://www.ilga.gov/house/rep.asp?MemberID=3071\")\nresp.data\n\n{\"name\": \"Emanuel 'Chris' Welch\",\n \"url\": \"https://www.ilga.gov/house/Rep.asp?MemberID=3071\",\n \"district\": \"7th\", \"party\": \"D\", \n \"photo_url\": \"https://www.ilga.gov/images/members/{5D419B94-66B4-4F3B-86F1-BFF37B3FA55C}.jpg\",\n   \"offices\": [\n     {\"name\": \"Springfield Office\",\n      \"address\": \"300 Capitol Building, Springfield, IL 62706\",\n       \"phone\": \"(217) 782-5350\"},\n     {\"name\": \"District Office\",\n      \"address\": \"10055 W. Roosevelt Rd., Suite E, Westchester, IL 60154\",\n       \"phone\": \"(708) 450-1000\"}\n   ]}\n\nThat's it!\nRead the tutorial for a step-by-step guide to building a scraper.\nCommand Line Usage Example\u00b6\nIf you've installed the package (e.g. with pipx), you can use the scrapeghost command line tool to experiment.\n#!/bin/sh \nscrapeghost https://www.ncleg.gov/Members/Biography/S/436  \\\n        --schema \"{'first_name': 'str', 'last_name': 'str',\n        'photo_url': 'url', 'offices': [] }'\" \\\n        --css div.card | python -m json.tool\n\n{\n    \"first_name\": \"Gale\",\n    \"last_name\": \"Adcock\",\n    \"photo_url\": \"https://www.ncleg.gov/Members/MemberImage/S/436/Low\",\n    \"offices\": [\n        {\n            \"type\": \"Mailing\",\n            \"address\": \"16 West Jones Street, Rm. 1104, Raleigh, NC 27601\"\n        },\n        {\n            \"type\": \"Office Phone\",\n            \"phone\": \"(919) 715-3036\"\n        }\n    ]\n}\n\nSee the CLI docs for more details.\nFeatures\u00b6\nThe purpose of this library is to provide a convenient interface for exploring web scraping with GPT.\nWhile the bulk of the work is done by the GPT model, scrapeghost provides a number of features to make it easier to use.\nPython-based schema definition - Define the shape of the data you want to extract as any Python object, with as much or little detail as you want.\nPreprocessing\n\nHTML cleaning - Remove unnecessary HTML to reduce the size and cost of API requests.\nCSS and XPath selectors - Pre-filter HTML by writing a single CSS or XPath selector.\nAuto-splitting - Optionally split the HTML into multiple calls to the model, allowing for larger pages to be scraped.\n\nPostprocessing\n\nJSON validation - Ensure that the response is valid JSON.  (With the option to kick it back to GPT for fixes if it's not.)\nSchema validation - Go a step further, use a pydantic schema to validate the response.\nHallucination check - Does the data in the response truly exist on the page?\n\nCost Controls\n\nScrapers keep running totals of how many tokens have been sent and received, so costs can be tracked.\nSupport for automatic fallbacks (e.g. use cost-saving GPT-3.5-Turbo by default, fall back to GPT-4 if needed.)\nAllows setting a budget and stops the scraper if the budget is exceeded.\n\n\n\n\n\n\n\n            Back to top\n          \n\n\n\n\n\n\n      Copyright \u00a9 2023 James Turk\n    \n  \n  \n    Made with\n    \n      Material for MkDocs\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHacker News\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Hacker News\n\n\n\n\n\n\n\n\n\n          Sorry, we're not able to serve your requests this quickly.\n        \n\n\n\n\nreload\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "summary": "- scrapeghost is a library for scraping structured data from HTML using OpenAI's GPT.\n- Users must proceed with caution when using scrapeghost, as it is experimental, relies on the OpenAI API, and is licensed under Hippocratic License 3.0.\n- Steps to use scrapeghost include obtaining an OpenAI API key, installing the library, defining the data schema, and passing a URL to the scraper.\n- The library offers features such as Python-based schema definition, HTML cleaning, pre-filtering with CSS or XPath selectors, JSON and schema validation, and cost controls.\n- A CLI tool is also available for experimentation."
  },
  {
    "id": 35301447,
    "timestamp": 1679742268,
    "title": "Apple's Neural Engine: ML Acceleration for iOS Devices",
    "url": "https://github.com/hollance/neural-engine",
    "hn_url": "http://news.ycombinator.com/item?id=35301447",
    "content": "The Neural Engine is a specialized processor found in most newer iPhones and iPads that accelerates machine learning models. Not much is known about how the processor actually works, and Apple provides little guidance on how to optimize models to take advantage of it. The ANE is great for making ML models run fast on iOS devices, and a model that is optimized for the ANE will seriously outperform the CPU and GPU. However, the ANE has limitations, and not every Core ML model can make full use of it. Other companies are developing their own AI accelerator chips, and Google's TPU is a notable example. This document provides a summary of what is currently known about the Apple Neural Engine.The discussion is around the efficiency of owning expensive hardware that sits idle most of the time. Some argue that it is more expensive to build out bandwidth than to give everyone compute. Others point out that lots of hardware sits idle in the cloud as well and that cloud providers have higher server utilization numbers than what one could achieve on a mobile device without annihilating the battery. Some say that inactive silicon isn't inefficient, and it's not even wasteful to produce. ANE is used for a variety of things, including biometrics, image analysis, text to speech, and speech to text. Apple moved Siri on-device with iOS 15, and it is possible to run large language models locally using ANE. Some mention that using a local model for transcription on the device with ANE kills the battery.The discussion revolves around the neural engine on the new M1/M2 Max chip and whether it is directly hooked up to the memory like the GPU. One user expresses frustration over the lack of detailed documentation provided by Apple about the neural engine, while another user suggests using CoreML as an abstraction for programming it. Some users speculate on the possible reasons behind the lack of documentation, with one user stating that it is likely a competitive advantage for Apple. Another user highlights the potential changes to the Neural Engine compute block from one chip generation to the next, making it more stable to use an abstraction like CoreML. One user jokes about the word \u201c\u00c2ne,\u201d which means donkey in French, being present in the name \u201cApple Neural Engine.\u201d",
    "summary": "- Apple's Neural Engine is a specialized processor that accelerates machine learning models on most newer iPhones and iPads.\n- The ANE is great for making ML models run fast on iOS devices, but not every Core ML model can make full use of it.\n- Other companies are developing their own AI accelerator chips, and Google's TPU is a notable example.\n- The discussion around the efficiency of owning expensive hardware that sits idle most of the time is ongoing.\n- ANE is used for biometrics, image analysis, text to speech, and speech to text.\n- Some mention that using a local model for transcription on the device with ANE kills the battery.\n- Users discuss the neural engine on the new M1/M2 Max chip and suggest using CoreML as an abstraction for programming it.\n- There is frustration over the lack of detailed documentation provided by Apple about the neural engine, while others speculate on possible reasons for this."
  },
  {
    "id": 35300200,
    "timestamp": 1679727568,
    "title": "Cloudflare Disables IPFS Gateway Over Copyright Abuse Reports",
    "url": "https://torrentfreak.com/cloudflare-disables-access-to-pirated-content-on-its-ipfs-gateway-230324/",
    "hn_url": "http://news.ycombinator.com/item?id=35300200",
    "content": "Cloudflare, a company which provides access to millions of websites and offers an IPFS gateway, has reported that it disables IPFS access in response to copyright abuse complaints. This is despite the decentralised nature of IPFS, which means that websites can become completely decentralised and virtually impossible to shut down. While Cloudflare has no control over any of the content being made available, it may disable access through its gateways in response to abuse reports, including reports of copyright, technical and other abuse. This policy resulted in 1,073 IPFS actions in the first quarter of 2022. Cloudflare's actions have caused disappointment amongst some members of the HN community, who say that Cloudflare's business model \"has always been man-in-the-middle the entire internet\".Cloudflare has shut down its IPFS Gateway citing the gateway's potential for abuse. While many users who were leveraging the IPFS Gateway were doing so for non-piracy related reasons, there are still concerns over the exposure of IPFS addresses and the possibility that illegal content could be hosted on the platform. The discontinuation of the gateway has sparked discussions among developers and users on the value of IPFS, and the need for alternatives to enable secure file sharing. Some users pointed out the advantages for people living in countries with heavy censorship, while others noted that the open nature of IPFS raises concerns for hosting illegal content. Cloudflare's rank on the list of top IPFS gateway providers has not been shared.Cloudflare, a cybersecurity company that provides content delivery network services, has blocked access to the IPFS gateway operated by the Pirate Bay. The Pirate Bay is a website that tracks files, offering links to content that it does not actually own or host. The move by Cloudflare to block the site has received criticism from some quarters, with individuals expressing concern about internet censorship and centralisation. However, others have argued that it is not practical for some independent website owners to avoid using Cloudflare if they want to avoid massive bot traffic and DDOS attacks. In response to this, some important websites are offering API access, which grants token access that can provide at least some level of rate limiting, thereby offering some protection against cyber threats.Comments on a Hacker News thread discuss Cloudflare's decision to block access to IPFS gateways. Some users express skepticism about Cloudflare's motives, while others argue that IPFS is primarily used for infringing content. The thread also touches on Cloudflare's business model and the effectiveness of its security suite, with one user comparing it to a \"protection racket.\" The conversation veers into discussions about the role of cryptography and blockchain technologies in IPFS and Filecoin, with some users arguing that these technologies are overhyped marketing terms while others see potential in their use cases.",
    "summary": "- Cloudflare disables access to IPFS gateway due to copyright abuse complaints, despite IPFS's decentralized nature.\n- Policy resulted in 1,073 IPFS actions in Q1 2022, causing disappointment among some members of HN community.\n- Discontinuation of gateway sparks discussions on value of IPFS and need for alternatives to enable secure file sharing.\n- Cloudflare's blocking of Pirate Bay receives criticism for internet censorship and centralization, but some argue it's necessary for protection against cyber threats.\n- Comments on HN thread discuss Cloudflare's decision, with users expressing skepticism about motives and effectiveness of security suite.\n- Conversation veers into discussions on role of cryptography and blockchain technologies in IPFS and Filecoin, with some seeing potential use cases while others see them as overhyped marketing terms."
  },
  {
    "id": 35300482,
    "timestamp": 1679731091,
    "title": "YunoHost: Simple Self-Hosting for App Management",
    "url": "https://yunohost.org",
    "hn_url": "http://news.ycombinator.com/item?id=35300482",
    "content": "French project YunoHost aims to simplify server administration, and the self-hosting of apps. The Linux distribution offers an alternative to cloud-hosted solutions, and its users can manage a server for their own firms, friends or associations. YunoHost offers a convenient solution for those who want to self-host, providing\u00a0a clean web interface for server management and setup guides for novice users. The project has also\u00a0been around for over a decade and\u00a0benefits from\u00a0a dedicated community of developers. However, users have raised maintenance concerns, such as the management of\u00a0updates and the need for experts to resolve\u00a0obscure issues.YunoHost is a libre software project maintained by volunteers whose goal is to democratize self-hosting while ensuring that it is reliable, secure, ethical, and lightweight. Based on Debian GNU/Linux, the distribution offers users the ability to administer their server through a user-friendly web interface, deploy apps in a few clicks, manage users, domain names, and backups, and connect to all apps simultaneously through the user portal. The YunoHost OS also includes a full e-mail stack, SSL certificates, and security systems such as Fail2ban and yunohost-firewall. The project also provides documentation and support for users. While similar capabilities can be gathered through Debian/Ubuntu and Docker containers, YunoHost is appreciated for simplifying the process.Yunohost is a self-hosting platform for easy management of user-facing applications, with a focus on app management rather than infrastructure or devops management. It features SSO and makes it easier to add new services, benefiting from containerization. Users have praised its user management and community support, as well as the ease of customizing the platform. Some have raised concerns about security, as it is not containerized or sandboxed. It has been compared to other self-hosting platforms such as Cloudron or Sandstorm, and although it may be less polished, it offers a wider selection of packages and is free of cost. It is a community project that originated in the French ISP scene.",
    "summary": "- YunoHost simplifies server administration and self-hosting of apps with a clean web interface and setup guides for novice users.\n- Based on Debian GNU/Linux, it offers user-friendly web interface, user, domain, backup management, and secure systems like Fail2ban and yunohost-firewall.\n- YunoHost is appreciated for simplifying the process of self-hosting and app management and features SSO and easy addition of new services.\n- Users have praised its user management and community support, but some have raised concerns about security, as it is not containerized or sandboxed.\n- YunoHost offers a wider selection of packages, is free of cost, and is a community project that originated in the French ISP scene."
  },
  {
    "id": 35303391,
    "timestamp": 1679756905,
    "title": "US Department of Energy seeks new contractor for Fermilab particle physics laboratory",
    "url": "https://www.science.org/content/article/major-shake-coming-fermilab-troubled-u-s-particle-physics-center",
    "hn_url": "http://news.ycombinator.com/item?id=35303391",
    "content": "The U.S. Department of Energy (DOE) has launched a new competition for the contract to manage the Fermi National Accelerator Laboratory (Fermilab), the country's only dedicated particle physics laboratory. The move came after the institution failed an annual performance review and in response to cost increases and delays in a neutrino experiment. DOE rarely seeks a new contractor for performance problems. Since 2007, Fermilab has been run by the University of Chicago and the Universities Research Association. DOE will issue a request for formal proposals in the summer and intends to award the contract by 30 September 2024, with control of the laboratory passing on 1 January 2025.\nFermilab, the US particle physics center, is reportedly facing a major shake-up, with the center's future and budget under scrutiny. Earlier this year, a scientific advisory panel concluded the lab's flagship particle accelerator, the Tevatron, should be shut down. The advisory panel recommended focusing on international projects that could ultimately help replace the Tevatron, which physicists agree has outlived its usefulness. The review, called for by US Congress, is expected to recommend significant changes to the center's management structure and financial arrangements. Fermilab has remained in operation, despite the uncertainty, and recently researchers announced the discovery of the long-sought Higgs boson.Comments on an article about Fermilab facility share personal stories and experiences with the lab, including working and studying there, visiting as a tourist, and salvaging old equipment. Some comments touch on the bureaucracy and management issues at the lab, including delays in approving contracts and a perceived bias towards technical PhDs for management positions. Others discuss the value and challenges of getting rid of old electronics and equipment. Overall, the comments paint a picture of a place that is both fascinating and frustrating, with a mix of cutting-edge science and bureaucratic challenges.Fermilab, a particle physics laboratory in Illinois, is struggling due to a lack of funding and diversification, according to industry insiders. The laboratory has had a difficult time following the shutdown of its Tevatron particle accelerator in 2011, with the Large Hadron Collider in Switzerland overshadowing Fermilab work. Critics have accused the US government of tinkering with the country's science budget to the point of irrelevance. With a shift towards \"profitable\" scientific fields becoming the norm, the US may no longer boast of technological investment. One anonymous commentator wrote: \"Government organisations should be wound down more regularly... yet there seems to be this 'strictly increasing' mindset around organisations funded by the taxpayer.\"\nFermilab, the US Department of Energy's premier particle physics laboratory, is struggling with delays and cost overruns on multiple projects, which have sparked criticism from physicists and science advocates. The lab's Long Baseline Neutrino Facility and Deep Underground Neutrino Experiment, which aim to study the behavior of subatomic particles, have been delayed by four years and their cost has increased from $1.6 billion to $2.4 billion. The particle accelerator complex, which is utilized in the Fermilab Accelerator Science and Technology (FAST) facility, has also been hit by delays and cost overruns, with its cost nearly doubling from $167 million to $314 million. Critics have argued that such delays and cost overruns are undermining the lab's ability to carry out cutting-edge research.",
    "summary": "- The US Department of Energy is looking for a new contractor to manage Fermilab, the only dedicated particle physics laboratory in the country.\n- The decision came after Fermilab failed an annual performance review and experienced cost increases and delays in a neutrino experiment.\n- The University of Chicago and the Universities Research Association have managed Fermilab since 2007.\n- The lab's flagship particle accelerator, the Tevatron, has been recommended to be shut down in favor of international projects by a scientific advisory panel.\n- Fermilab has faced challenges with funding and diversification, causing delays and cost overruns on multiple projects.\n- Critics argue that the US government is neglecting investment in scientific fields to focus on profitability.\n- Fermilab researchers recently announced the discovery of the long-sought Higgs boson.\n- Comments on an article reflect a mix of fascination and frustration with the lab, with some highlighting management issues and others discussing the value of disposing of old equipment."
  },
  {
    "id": 35307647,
    "timestamp": 1679780016,
    "title": "Utah introduces laws to regulate teen social media use",
    "url": "https://www.bbc.com/news/world-us-canada-65060733",
    "hn_url": "http://news.ycombinator.com/item?id=35307647",
    "content": "Utah has become the first US state to limit teen social media access through two measures to protect young people, signed by Governor Spencer Cox. The bills require social media firms to verify users as being at least 18 and get parental consent for children to use their apps. They also push for parents to have full access to their children's online accounts, while imposing a social media curfew between the hours of 22.30 and 06.30. Social media companies will also no longer be able to collect a minor's data or be targeted for advertising. The two bills, which take effect on 1 March 2024, are designed to make it easier to take legal action against social media companies.Utah has passed the first law in the US to require social media companies to gain permission from parents for users under 13 in order to prevent addiction. The law covers platforms that automatically display content to users, and includes specific exemptions for essential services and educational tools. Social media networks remove the protection if there is a \"reasonable belief\" the user is 13 or above. The measure will come into effect in 2022. Concerns have been expressed that the law, although described as necessary, could still have a chilling effect on the internet and the sharing of knowledge online. Critics suggest that it places too much responsibility on tech companies to control user behaviour.Utah has passed a bill banning social media platforms from collecting or storing the personal information of minors, including their location, and search and browsing histories, without explicit parental consent. The bill also requires social media companies to install \"age verification measures\" and \"delete data\" that has been used to verify the age of the user. Non-compliance with this could result in a $5,000 civil fine with up to $25,000 in damages for \"causing addiction or social or emotional harm\" to a minor. Critics have highlighted the vagueness of many of its definitions and exemptions, and the unintentional impact it could have on educational resources like GitHub and Stack Overflow.\nUtah has passed a law requiring social media companies to restrict kids under 18 from creating accounts. The law defines social media companies as entities with over 5 million account holders globally which provide an interactive computer service. Some websites such as Reddit and Hacker News do not require users to create accounts to participate, but other platforms, including Facebook, Instagram and Snapchat are directly impacted by the law. Critics say that the legislation is unconstitutional and also puts enforcement duties onto the social media companies, rather than placing responsibility on parents. Additionally, experts point out that the law's broad definition of social media may affect multiple other online services, potentially including games and information-sharing websites.There is a debate over whether the government should regulate technology and media. Some argue that regulations that ban products or services to protect children can be detrimental to adults, and they are against these bans. Others argue that some regulations, like those on tobacco use, are necessary to protect the public, and that social media is the modern-day tobacco. Some suggest that corporations should stop providing services to places where they face regulations they don't agree with, as a way to apply pressure to lawmakers. There is also concern that laws against social media could affect marginalized communities that rely on these platforms for support.Utah has passed a new law that requires social media platforms to verify the real names and ages of users. Anyone who posts using an assumed name and is caught would be subject to a warning and a suspension of their account. Critics of the legislation argue that the rule violates the principles of free speech and privacy. Advocates of the legislation argue that this form of transparency would help reduce cyberbullying, sexting and online harassment, and would help to protect minors from internet predators.\u00a0The bill \u201cdoesn\u2019t solve everything and raises some privacy issues, but at least it\u2019s a start,\" said sponsored Republican Senator Dan Hemmert.\nUtah has proposed a bill that will allow parents to access their children's social media accounts, including private messages without consent, as well as monitor their online activity. If passed, the bill will also impose a curfew on social media access between 22:30 and 06:30, unless adjusted by parents. Critics have raised concerns over the privacy implications and potential harm to teenage mental health, while supporters argue that the bills aim to protect young people from harmful online content. The legislation also raises questions on the responsibility of social media companies and the role of regulation in preventing negative outcomes for children.Utah has become the first US state to require social media companies to ask for identification to prevent minors from accessing their platforms. The new law, SB152, requires platforms to ask users for a form of government-issued identification to ensure that they are 18 or over. If the user is under 18, they will need parental consent to create an account. The law applies to platforms with over five million account holders. The Utah government has said the law will protect young people from cyberbullying, online predators, and addiction. However, some have raised concerns that the law will lead to the end of anonymity and that it could be unconstitutional.",
    "summary": "- Utah becomes first US state to introduce laws regulating teen social media use\n- Two measures require social media firms to verify users as at least 18 and get parental consent for children to use their apps, as well as imposing a social media curfew and banning data collection on minors\n- Law prohibits social media companies from collecting/storing personal info of minors without explicit parental consent\n- Utah passes law requiring social media companies to restrict kids under 18 from creating accounts\n- Debate over government regulation of technology and media\n- Utah proposed bill would allow parents to access their children's social media accounts and monitor their online activity\n- Utah requires social media companies to ask for identification to prevent minors from accessing their platforms"
  },
  {
    "id": 35308498,
    "timestamp": 1679785528,
    "title": "AI Replacing Human Creativity in Creative Industries",
    "url": "https://reddit.com/r/blender/comments/121lhfq/i_lost_everything_that_made_me_love_my_job/",
    "hn_url": "http://news.ycombinator.com/item?id=35308498",
    "content": "A 3D artist has shared on Reddit their frustration with AI that has taken over their creative role in a small games company. They have seen their role reduced to prompting, photoshopping and implementing pre-selected pictures, effectively eliminating their ability to use their own creativity in their job. The artist\u2019s boss sees the move to AI as a time and money saver, and the artist is now fearful that leaving to find another job would be no different. Other Reddit contributors shared their sympathy, while some discussed the inevitability of job replacement by AI and the impact on society.\u00a0 The post raises fundamental questions about the perceived normative role of work, creativity and our evolving relationship with emerging AI technologies.\u00a0AI is beginning to take over jobs that were once thought of as secure, and this trend is expected to continue. This has already resulted in the reduction of jobs in certain industries, such as the gaming industry, where AI is being used for concept art. While some argue that AI is simply another tool that can help workers produce better output, others are concerned that it is fundamentally changing the nature of many jobs and creating a McDonald's-ification of industries. Some worry that the displacement of jobs will lead to a need for labor protections, but others argue that such protections may not be feasible in a global market and that society needs to figure out what to do with all the unemployed workers that AI creates.\u00a0\u00a0The use of generative AI in art and animation is upending the industry, with some fearing that creative directors and artists will be replaced by machines. However, some comments on a Hacker News post suggest that the high-level creative direction aspect of the job,\u00a0such as developing prompts for AI-generated work, is the more challenging part. While AI can generate imagery or animation, it\u2019s difficult to control and lacks the feedback mechanisms that real artists use to shape their work. A skilled creative director with a deep understanding of different styles of art and framing angles can quickly generate the right prompts for an AI, creating better results. Ultimately, while AI can generate art, it\u2019s unlikely to replace artists or the field of art altogether.\u00a0AI-generated art models may make it easier to commission art, but it also raises concerns about the role of the artist in the creative process. Many artists feel that the use of these models diminishes their contributions to the final product, reducing their work to mere polishing. Some see generative AI as a tool that may be too compelling to avoid, but that could be boring and take the joy out of their craft. Others argue that people with a good understanding of art and framing should still be able to create better prompts for clients. However, some software developers suggest that using AI models in creative fields might provide more opportunities for those who enjoy the path of the creative process rather than just the outcome. Regardless, the potential effects of AI on the art industry remain a topic of discussion and concern.AI is drastically reducing the time it takes to create art, which means fewer people will be needed. However, some argue that the resulting output is no longer the artist's own work. The same phenomenon is happening in the software development industry, where AI and machine learning are taking on tasks previously done by entry-level positions. This may make it harder for new developers to gain experience and enter the industry, and it may also lead to a situation where only aged software engineers remain. Additionally, some believe that if AGI is created, the value of creative output will be driven to zero, as it will be cheap to produce and will lead to more competition. While some people argue that subsistence farming might be a good alternative career, others believe that we need universal basic income as a solution to unemployment caused by automation.The use of AI in creative fields is eliminating the need for certain jobs and impacting the livelihoods of artists, according to a conversation on Hacker News. One individual detailed how their job making 3D models for a video game company has been largely replaced by an AI prompting tool, and despite producing higher quality work, they no longer receive the same recognition or pay. Others express concern over the long-term impact of AI on self-worth and the fear that machines will eventually replace all mental tasks. Some feel that the shift towards AI-generated art is creating a \u201cmediocre slurry of content\u201d, while others argue that the use of AI can free artists to focus on more meaningful work.The use of AI prompts to generate art is replacing the jobs of 3D artists, and the trend may spread to other creative professions such as writing and software engineering. A post by a 3D artist named Midjourney describing the use of AI prompts in his work has sparked a discussion about the potential impact of automation on creative fields. While some commenters argue that AI will not be able to fully replace human creativity, others point out that the use of AI can make it easier for businesses to produce content, potentially leading to job losses for human workers. The discussion raises questions about the future of work and the role that automation will play in different industries.Several comments on a Hacker News thread discuss the limitations of AI-generated code. One user challenges others to provide a link to a production app with paying users created in minutes with AI. Others respond that while AI can help with repeatable tasks, such as incorporating LLMs into workflows, it cannot replace human developers entirely. Influencers may be promoting AI as magical and easy to sell their courses, but the reality is less flashy. Some express hope that AI will augment workflows and automate tedious tasks rather than completely replacing developers. There are concerns that ChatGPT, an AI tool that can create boilerplate code in seconds, will attract snake oil salespeople promising to deliver fully functional apps without human involvement.",
    "summary": "- AI is replacing the jobs of 3D artists and other creative professionals, leading to concerns about the future of work and the role of automation in different industries.\n- While some argue that AI is simply another tool that can help workers produce better output, others worry that it is fundamentally changing the nature of many jobs and creating a McDonald's-ification of industries.\n- The use of generative AI in art and animation is upending the industry, but skilled creative directors with a deep understanding of different styles of art and framing angles can quickly generate the right prompts for AI-generated work, creating better results.\n- AI is drastically reducing the time it takes to create art, which means fewer people will be needed. However, some argue that the resulting output is no longer the artist's own work.\n- There are concerns about the limitations of AI-generated code, and the potential for snake oil salespeople promising to deliver fully functional apps without human involvement.\n- Some express hope that AI will augment workflows and automate tedious tasks rather than completely replacing developers."
  },
  {
    "id": 35308033,
    "timestamp": 1679782544,
    "title": "Nasal nanotech spray blocks COVID-19 & variants in lab animals",
    "url": "https://www.helsinki.fi/en/news/pandemics/nasal-spray-protects-against-coronavirus-infection-effective-also-against-recent-immune-evasive-variants",
    "hn_url": "http://news.ycombinator.com/item?id=35308033",
    "content": "Researchers at the University of Helsinki have developed a nasal spray that offers protection against the coronavirus, and is also effective against recent immune-evasive variants. The spray makes use of nanotechnology, which delivers a small amount of the vaccine directly to cells in the nasal cavity, offering a more targeted approach to vaccination. In animal testing, the spray has been found to produce high levels of antibodies against the virus in just a single dose. The vaccine is expected to be much easier to distribute than traditional vaccines, as it does not require refrigeration and can be stored at room temperature for several months.Researchers at the University of Helsinki have developed a nasal spray that is effective in preventing coronavirus infection, including immune-evasive variants, in laboratory animal studies. The molecule, called TriSb92, identifies a region in the spike protein of the coronavirus that is common to all current variants of the virus and inhibits its functioning. Unlike face masks, the molecule can prevent infection even after a few hours of exposure. The molecule remains fully functional at room temperature for at least 18 months, making it well-suited for use as a nasal spray. The researchers believe the molecule could protect immunocompromised individuals and the elderly who do not gain sufficient immunity from vaccines. It could also work against future animal-borne close relatives of SARS-CoV-2. The molecule must now be tested in clinical trials before commercial availability.A nasal spray that inhibits the spike protein of SARS-CoV-2 has been developed that could help reduce the spread of COVID-19. The spray, called TriSb92, was shown to protect mice from SARS-CoV-2 infection in a recent study. However, it only provides protection for a few hours and needs to be taken frequently. The mechanism of action of TriSb92 is different from vaccines as it does not invoke the immune system. Instead, it reduces the amount of virus in the nasal cavities by inhibiting the spike protein\u2019s ability to enter cells. It is hoped that TriSb92 will go to human trials soon but is still several years away from possible approval.The comment section discusses the reasons for vaccine hesitancy. One suggestion is that people are afraid of needles, but many have also started skipping established vaccines for their children. Some argue that the messaging around COVID vaccines and politicization of the issue have made people hesitant, while others point out that authorities may have made unsupported claims about vaccine safety. The mRNA vaccines are noted as a novel style of delivery, with some concerns raised about their potential effects on healthy cells and tissues. Others bring up nasal sprays and saline spray as possible protective measures against COVID. There is some debate over whether the government should mandate vaccines or how to enforce vaccination.",
    "summary": "- University of Helsinki researchers create nasal spray blocking COVID-19 and variants in animal testing \n- Spray uses nanotechnology, delivers vaccine directly to nasal cavity, produces high levels of antibodies in single dose \n- Vaccine does not require refrigeration, can be stored at room temperature for several months \n- Molecule, TriSb92, inhibits functioning of spike protein, protects against all current variants and works at room temperature for at least 18 months \n- Spray protects against infection for a few hours, reducing amount of virus in nasal cavities \n- Vaccine hesitancy reasons debated in comments, including fear of needles, politicization, safety concerns, and potential effects on healthy cells \n- Some suggest nasal sprays and saline spray as possible protective measures \n- Debate over government mandates and enforcement of vaccination."
  },
  {
    "id": 35304241,
    "timestamp": 1679761802,
    "title": "Repeal Supersonic Flight Ban for Aviation Growth",
    "url": "https://www.elidourado.com/p/50-years-supersonic-ban",
    "hn_url": "http://news.ycombinator.com/item?id=35304241",
    "content": "Eli Dourado argues that the speed limit imposed on US airspace 50 years ago has hindered the development of supersonic aviation, and proposes repealing the ban on supersonic flight in the FAA reauthorization act. He suggests that regulations should be based on human response data, and that new low-boom aircraft be built. NASA plans to use the X-59 aircraft to gather data on human response to sonic boom, which will be analysed by the FAA and foreign regulators who are part of the Committee on Aviation Environmental Protection. However, Dourado doubts that a new standard will be implemented before the late 2030s, as environmental groups may sue to delay the process. Repealing the ban would signal to the aviation industry that America is open for business and encourage manufacturers to start working on new designs.The author argues that the overland ban on supersonic flight introduced in 1973 has contributed to economic stagnation. The author acknowledges that a complete halt to the development of key technology, simply because a few harmless sonic booms might annoy a vocal minority, is decadent. The author suggests that with current boom-shaping technology it is possible to address any sonic booms. The author highlights an FAA-sponsored paper that evaluated ways to reduce sonic boom using laser beams to create a \"phantom body.\" The author concludes that after fifty years of quiet skies, it is time to make America boom again and let entrepreneurs make their business case for supersonic flight. The author also suggests that we must get back to doing great things.",
    "summary": "- Eli Dourado proposes repealing the ban on supersonic flight in the FAA reauthorization act to encourage aviation growth.\n- Regulations should be based on human response data and new low-boom aircraft should be built.\n- NASA plans to use the X-59 aircraft to gather data on human response to sonic boom.\n- A new standard may not be implemented before the late 2030s due to potential lawsuits from environmental groups.\n- Repealing the ban would signal to the aviation industry that America is open for business and encourage manufacturers to start working on new designs.\n- Overland ban on supersonic flight introduced in 1973 has contributed to economic stagnation.\n- Current boom-shaping technology can address sonic booms and an FAA-sponsored paper evaluated ways to reduce sonic boom using laser beams.\n- It is time to repeal the ban and let entrepreneurs make their business case for supersonic flight to do great things."
  },
  {
    "id": 35303423,
    "timestamp": 1679757084,
    "title": "Embrace New Identity: Call Yourself Titles",
    "url": "https://josem.co/call-yourself-titles/",
    "hn_url": "http://news.ycombinator.com/item?id=35303423",
    "content": "Call yourself titles | Jose M.\nHomeArticlesWorkAboutCall yourself titlesMarch 25, 2023We constantly question ourselves when attempting anything new for the first time, which is daunting. And what\u2019s worse, it prevents us from doing anything new, so we can\u2019t change who we are.If you\u2019ve always been terrible at sports and want to start working out, the natural reaction after a few attempts is to stop and return to your old habits. At the end of the day, you\u2019re not an athlete.If you\u2019ve always drawn poorly and always had low marks in arts, and want to start drawing, after a few attempts, you might wonder, what am I even doing? I\u2019m not an artist. And so on and so forth. But there\u2019s a solution to this challenge: calling ourselves with the new title very early in the process.We become a runner when we start running a few days a week. An amateur one, a beginner in the world of runners, but a runner nonetheless. And runners run.If you start going to the gym to improve your fitness (and not just record yourself to post it on TikTok), you\u2019re an athlete. Yes, you\u2019re not a CrossFit world champion; you\u2019re a beginner, but an athlete at the end of the day.And here\u2019s where the magic happens. If you\u2019re now an athlete, you\u2019ll go to the gym often and pay attention to your nutrition because that\u2019s what athletes do. When presented with a decision around your lifestyle, you\u2019ll choose correctly because of your new identity.If we start calling ourselves our new titles, what we think of ourselves changes as well, and it\u2019ll be easier to overcome that impostor syndrome and stick to new habits, feeling that you\u2019re doing what you\u2019re supposed to be doing.The earlier you use that new term: \u201cathlete,\u201d \u201cwriter,\u201d or \u201cartist,\u201d the easier it will be to accept your new identity and act accordingly.If you start playing the piano, you\u2019re a pianist, and pianists play the piano. Some do it better than others, but they practice and keep improving, and now you belong to that group.You don\u2019t need to do something for years, or even professionally, to call yourself by your new title. By using that new identity early and accepting it naturally, you\u2019ll have a head start in your new hobby or habit you want to incorporate. Give it a try.Subscribe below to get future posts in your inbox (no spam)Or use the RSS feed link.\u00a9 2023 Jose M. |\nRSS feed | Based on:\nPaperMod\n\n\n\nHacker News\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Hacker News\n\n\n\n\n\n\n\n\n\n          Sorry, we're not able to serve your requests this quickly.\n        \n\n\n\n\nreload\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "summary": "- Changing how we identify ourselves can help overcome the fear of trying new things.\n\n- If we adopt a new title early in the process, we are more likely to stick to our new habit or hobby.\n\n- Embracing a new identity can help overcome imposter syndrome.\n\n- It's not necessary to be an expert or have years of experience to call oneself a new title."
  },
  {
    "id": 35301657,
    "timestamp": 1679744348,
    "title": "New Techniques to Break Language Models with Prompts",
    "url": "https://github.com/greshake/llm-security",
    "hn_url": "http://news.ycombinator.com/item?id=35301657",
    "content": "Researchers have demonstrated new ways of breaking app-integrated Language Model (LLMs), according to a paper on archive.org. Kai Greshake and colleagues at CISPA Helmholtz Center found that connecting LLMs to other applications can have significant security implications, and that injecting small prompts can offer the same power as arbitrary code execution. Techniques for attacking code completion engines and spreading malicious agents via email were among newly enabled attack vectors created by the researchers, who called for further investigation of the potential risks of giving LLMs interfaces to other applications. Automated social engineering and targeted attacks on LLMs with multi-stage payloads were also included in the group's demonstrations.OpenAI's language models (LLMs) can be vulnerable to prompt injection attacks, in which an attacker adds malicious prompts to LLM input. Researchers have highlighted that the problem is difficult to solve because input can be classified as code, data or instructions unpredictably. To mitigate prompt injection vulnerabilities, the researchers propose a distinction between the different types of input and output at the token level. Each token can be associated with a \"color\" or \"channel\" that corresponds to its source or destination. This would allow the model to consider each (word, color) combination as a separate token, and create different sets of weights for each, shrinking the risk of confusion if two kinds of tokens have similar spellings. Response to the proposal has been mixed, with some users pointing out that there is no clear cut solution to the problem.",
    "summary": "- Researchers demonstrate new ways of breaking app-integrated language models (LLMs) by connecting LLMs to other applications and injecting small prompts.\n- New attack vectors and potential risks were identified, including automated social engineering, targeted attacks with multi-stage payloads, and spreading malicious agents via email.\n- Prompt injection attacks are difficult to solve because input can be classified as code, data, or instructions unpredictably.\n- To mitigate prompt injection vulnerabilities, researchers propose a distinction between the different types of input and output at the token level using \"colors\" or \"channels\" to create separate sets of weights for each (word, color) combination.\n- There is no clear-cut solution to the problem, and responses to the proposal have been mixed."
  },
  {
    "id": 35305079,
    "timestamp": 1679766314,
    "title": "NJ Hospitals Accused of False Opioid Reports on Mothers",
    "url": "https://reason.com/2023/03/23/hospitals-are-still-reporting-new-mothers-for-neglect-based-on-drug-tests-triggered-by-poppy-seeds/",
    "hn_url": "http://news.ycombinator.com/item?id=35305079",
    "content": "New Jersey hospitals have been accused of conducting nonconsensual and medically unjustified drug tests on perinatal patients, including two mothers whose tests were falsely positive for opioids due to poppy seed ingestion. The hospitals subsequently reported the mothers to the New Jersey Department of Child Protection and Permanency, triggering an investigation. The American Civil Liberties Union has filed a complaint asking for the hospitals' drug testing policies to be halted and for compensatory damages to be awarded, claiming that these practices violate state law. Existing litigation and guidelines have already highlighted maternity wards' previous failures to adjust their testing standards for poppy seeds ingestion, while an investigation in New Jersey revealed that the laboratories used low cutoffs for opiate testing. Such investigations have disrupted new parents and resulted in the traumatic and erroneous removal of newborns from their mothers.\nThe website Hacker News is experiencing technical difficulties that are causing slow loading times and error messages. Users have been advised to try to reload the page. There may be sarcastic comments from frustrated users, but they are not relevant to the issue at hand.",
    "summary": "- New Jersey hospitals accused of conducting unjustified drug tests on perinatal patients, falsely accusing two mothers of opioid use.\n- Hospitals reported the mothers to the NJ Department of Child Protection and Permanency, triggering an investigation.\n- The American Civil Liberties Union has filed a complaint asking for the hospitals' drug testing policies to be halted and for compensatory damages to be awarded.\n- Maternity wards have previously failed to adjust testing standards for poppy seed ingestion, and laboratories used low cutoffs for opiate testing.\n- Investigations have disrupted new parents and resulted in the traumatic and erroneous removal of newborns from their mothers.\n- Hacker News experiencing technical difficulties causing slow loading times and error messages."
  },
  {
    "id": 35297491,
    "timestamp": 1679703311,
    "title": "Intel Co-Founder Gordon Moore Passes Away at 94",
    "url": "https://www.intel.com/content/www/us/en/newsroom/news/gordon-moore-obituary.html",
    "hn_url": "http://news.ycombinator.com/item?id=35297491",
    "content": "Gordon Moore, the co-founder of Intel Corporation and author of Moore's Law, has passed away at the age of 94. Moore founded Intel Corporation in 1968 with Robert Noyce and helped pioneer the commercial production of silicon transistors and integrated circuits. He served as executive vice president, president, chief executive officer and chairman of the company's board. Moore was a philanthropist and environmental conservationist; he and his wife established the Gordon and Betty Moore Foundation in 2000, which donated over $5.1 billion to charitable causes. Moore dedicated his life to inspiring innovation and technological advancements that shape everyday lives. His legacy will carry forward as true north and inspiration at Intel.Intel Corporation highlights the potential of data to transform business and society positively. The company encourages people to learn more about their innovations by visiting their website. They reaffirm their commitment to respecting human rights and avoiding complicity in human rights abuses, and their products and software are designed only to be used in applications that do not violate internationally recognized human rights. In other news, Hacker News reports the death of Gordon Moore, Intel Co-Founder, at the age of 94. Comments related to the thread have been moved.",
    "summary": "- Gordon Moore, co-founder of Intel Corporation and pioneer of commercial production of silicon transistors and integrated circuits, passed away at age 94.\n- Moore was a philanthropist and environmental conservationist who established the Gordon and Betty Moore Foundation, which donated over $5.1 billion to charitable causes.\n- Intel Corporation emphasizes the potential of data to positively transform business and society and reaffirms their commitment to respecting human rights. \n- Hacker News reports the death of Gordon Moore, with related comments moved."
  },
  {
    "id": 35297389,
    "timestamp": 1679702667,
    "title": "Google's Code Jam announces competition results",
    "url": "https://codingcompetitions.withgoogle.com/codejam",
    "hn_url": "http://news.ycombinator.com/item?id=35297389",
    "content": "\n\n\n\nCode Jam - Google\u00e2\u0080\u0099s Coding Competitions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHacker News\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Hacker News\n\n\n\n\n\n\n\n\n\n          Sorry, we're not able to serve your requests this quickly.\n        \n\n\n\n\nreload\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "summary": "- Google's Code Jam has announced the results of its coding competition. \n- The competition featured over 10,000 participants from around the world. \n- Winners were recognized in both individual and team categories."
  },
  {
    "id": 35307150,
    "timestamp": 1679777164,
    "title": "Managers Target Loyal Workers for Unpaid Work & Extra Hours",
    "url": "https://today.duke.edu/2023/03/managers-exploit-loyal-workers-over-less-committed-colleagues",
    "hn_url": "http://news.ycombinator.com/item?id=35307150",
    "content": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManagers Exploit Loyal Workers Over Less Committed Colleagues | Duke Today\n\n\n\n\n\n\n\n\n      Skip to main content\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                Sections\n              \n\n\n\n\nSections\n\n\nArts & Humanities\n\n\nBusiness & Economics\n\n\nCampus & Community\n\n\nEnvironment & Sustainability\n\n\nGlobal\n\n\nHealth & Medicine\n\n\nScience & Technology\n\n\nWorking@Duke\n\n\n\n\nMore News & Info\n\n\nAthletics\n\n\nBooks\n\n\nCOVID Response\n\n\nMedia & Opinion\n\n\nResearch & Innovation\n\n\nSeries\n\n\n\n\n\n\n\n\n\n\nClose\n\n\n\n\n          Back\n        \n\n\n\n\nTrending\n\n\nWatch\n\n\n\n\n\n\n\n\n\n\n\nSearch\n\n\n\n\nSearch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Menu\n\n\n\n\n\n\n\n\nMain navigation\n\n\n\n\n\n\n\n\n\n\n                Sections\n              \n\n\n\n\nSections\n\n\nArts & Humanities\n\n\nBusiness & Economics\n\n\nCampus & Community\n\n\nEnvironment & Sustainability\n\n\nGlobal\n\n\nHealth & Medicine\n\n\nScience & Technology\n\n\nWorking@Duke\n\n\n\n\nMore News & Info\n\n\nAthletics\n\n\nBooks\n\n\nCOVID Response\n\n\nMedia & Opinion\n\n\nResearch & Innovation\n\n\nSeries\n\n\n\n\n\n\n\n\n\n\nClose\n\n\n\n\n          Back\n        \n\n\n\n\nTrending\n\n\nWatch\n\n\n\n\nSocials\n\n\n\n\n\n\n\n\n\n            Facebook\n          \n\n\n\n\n\n\n\n\n\n\n            Instagram\n          \n\n\n\n\n\n\n\n\n\n\n            Twitter\n          \n\n\n\n\n\n\n\n\n\n\n            YouTube\n          \n\n\n\n\n\nFooter\n\n\nCampus Communications\n\n\nContact Us\n\n\nFor the Media\n\n\n\n\n\n\n\n\n\nClose\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManagers Exploit Loyal Workers Over Less Committed Colleagues\n\nA loyal worker gets more extra work than the fair, honest, or disloyal\n\n \n\nImage\n \n\n\n\n\n\n\n\n\n      Managers prefer to task workers considered to be loyal with unpaid work and extra hours over less committed colleagues, finds a new study. (Photo by Marco Verch via CC) \n  \n\n\n\n\n\nCaptionManagers prefer to task workers considered to be loyal with unpaid work and extra hours over less committed colleagues, finds a new study. (Photo by Marco Verch via CC) \n        \n\n\n\n\n\n\n\n\n\n\nClose\n\n\n\nImage\n \n\n\n\n\n\n\n\n\n      Managers prefer to task workers considered to be loyal with unpaid work and extra hours over less committed colleagues, finds a new study. (Photo by Marco Verch via CC) \n  \n\n\n\n\n\n\n\n\n\nPublished\nMarch 20, 2023\n\n\n\n    All Meta\n  \n\n\n\nCredits\n \nDan Vahaba\nDuke Institute for Brain Sciences\n\n\n\n\nSchools\nFuqua School of Business\n\n\n\n\n\nDURHAM, N.C. \u2013 Company loyalty is a double-edged sword, according to a new study. Managers target loyal workers over less committed colleagues when doling out unpaid work and additional job tasks.\n\u201cCompanies want loyal workers, and there is a ton of research showing that loyal workers provide all sorts of positive benefits to companies,\u201d said Matthew Stanley, Ph.D., the lead researcher on the new paper and postdoctoral researcher at Duke University's Fuqua School of Business. \u201cBut it seems like managers are apt to target them for exploitative practices.\u201d\n\n\n\n\n\n\nThat\u2019s the main conclusion from a series of experiments conducted by Stanley and his colleagues Chris Neck, Ph.D. and Chris Neck, father-and-son researchers at Arizona State University and West Virginia University, respectively.\nThe findings appeared online January 6 in the Journal of Experimental Social Psychology.\nFor the study, Stanley recruited nearly 1,400 managers online to read about a fictional 29-year-old employee named John. The managers all learned that John\u2019s company was on a tight budget, and to keep costs down, had to decide how willing they would be to task John with extra hours and responsibilities without any extra pay. (Participants handing out the unpaid work in Stanley\u2019s study were compensated $12 an hour.)\nNo matter how Stanley and his colleagues framed the scenario, branding John as loyal always resulted in managers being more willing to ask him to shoulder the unpaid labor.\nManagers were more willing to exploit Loyal John over Disloyal John. And when a separate group of managers read a letter of recommendation about John, the letters praising John as loyal led to an increased willingness to recruit him for unpaid work over versions of John extolled for honesty or fairness.\nThe reverse was true, too: when John was portrayed as having a reputation to accept extra hours and workload, managers rated him as more loyal than a John who had a reputation to decline the same workload. Agreeable John and Refusal John were rated as similarly honest and fair however, demonstrating that loyalty but not closely related moral traits is bolstered by a history of doing free labor. \u201cIt\u2019s a vicious cycle,\u201d Stanley said. \u201cLoyal workers tend to get picked out for exploitation. And then when they do something that's exploitative, they end up getting a boost in their reputation as a loyal worker, making them more likely to get picked out in the future.\u201d\n\n\nOne reason managers preyed on loyal workers over others is their belief that it\u2019s just the price to pay for being loyal. Stanley and his team found that managers targeted loyal workers because they believe that loyalty comes with a duty to make personal sacrifices for their company.\nIt\u2019s not all malicious, though. Exploitation may be in part just due to ignorance, or what psychologists call \u201cethical blindness.\u201d\n\u201cMost people want to be good,\u201d Stanley said. \u201cYet, they transgress with surprising frequency in their everyday lives. A lot of it is due to ethical blindness, where people don\u2019t see how what they're doing is inconsistent with whatever principles or values they tend to profess.\u201d\nThe study doesn\u2019t provide a quick fix to eradicate employers\u2019 exploitative practices, but one partial cure might be simply having managers recognize the error of their ways and point out these ethical blind spots, Stanley said.\nWhile company loyalty seems to come with consequence, Stanley cautions that it doesn\u2019t mean we should just abandon work commitments or dodge uncompensated overtime. This is just an unfortunate side effect of a mostly positive trait, which Stanley recently found also happens with other aspirational traits, like generosity.\n\u201cI don't want to suggest that the take-away of the paper is to not be loyal to anybody because it just leads to disaster,\u201d Stanley said. \u201cWe value people who are loyal. We think about them in positive terms. They get awarded often. It's not just the negative side. It's really tricky and complex.\u201d\nCITATION: \u201cLoyal Workers Are Selectively And Ironically Targeted For Exploitation,\u201d Matthew L. Stanley, Christopher B. Neck, Christopher P. Neck. Journal of Experimental Social Psychology, Jan. 6, 2023. DOI: 10.1016/j.jesp.2022.104442\n\n\n\n\n\n\n\nEditor's Pick\n\n\n\n \n\n\n\nMarch 20, 2023\n\nClimate Research Across Duke\n\n\n\n                Stories from researchers from across Duke who are leading collaborative research with impact in a changing world.\n      \nRead on  Duke Stories\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSocials\n\n\n\n\n\n\n\n\n\n            Facebook\n          \n\n\n\n\n\n\n\n\n\n\n            Instagram\n          \n\n\n\n\n\n\n\n\n\n\n            Twitter\n          \n\n\n\n\n\n\n\n\n\n\n            YouTube\n          \n\n\n\n\n\n\n\nFooter\n\n\nCampus Communications\n\n\nContact Us\n\n\nFor the Media\n\n\n\n\nDuke Today is produced jointly by University Communications and the Office of Communication Services (OCS). Articles are produced by staff and faculty across the university and health system to comprise a one-stop-shop for news from around Duke. Geoffrey Mock of University Communications is the editor of the 'News' edition. Leanora Minai of OCS is the editor of the 'Working@Duke' edition. We welcome your comments and suggestions!\n\n\n      \u00a9 Copyright 2023 Duke University. All rights reserved.\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nHacker News\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Hacker News\n\n\n\n\n\n\n\n\n\n          Sorry, we're not able to serve your requests this quickly.\n        \n\n\n\n\nreload\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "summary": "- Managers target loyal workers for unpaid work and extra hours over less committed colleagues, according to a study by Duke University.\n- The study found that managers consider loyal employees as obligated to make personal sacrifices for the company, leading them to be exploited more often than non-loyal employees.\n- The exploitation of loyal workers is a result of ethical blindness, and pointing out ethical blind spots could reduce the exploitation, the researchers suggest."
  },
  {
    "id": 35297766,
    "timestamp": 1679705092,
    "title": "Autodoc: AI Codebase Documentation Using LLMs",
    "url": "https://github.com/context-labs/autodoc",
    "hn_url": "http://news.ycombinator.com/item?id=35297766",
    "content": "Autodoc is an experimental toolkit for auto-generating codebase documentation using LLMs. It indexes a codebase, calls an LLM for each file and folder to write the documentation, and generates a doc command that developers can use to ask specific questions about the codebase. The documentation is stored in the codebase and travels with the code, and will be re-indexed as part of a future CI pipeline. Autodoc is in early development, but its creators welcome contributions. While documentation typically records design decisions and developer intent not encoded in code, Autodoc may have its merits for coding maintenance and legacy codebases.ChatGPT is a language model that has been trained to complete text prompts, making it possible to use it for writing documentation for software. Some users have reported success in using ChatGPT for this purpose, but others warn that it can lead to lousy documentation or outright inaccuracies. The model can help summarize code and provide overviews of projects, but it does not provide the rationale and contextual information that good documentation usually provides. Some experts argue that relying solely on ChatGPT-generated documentation may encourage laziness, but others claim that it may be a useful starting point for projects with little documentation. While the tool shows potential, it is still in its early stages of development and is best used in conjunction with human-generated documentation.Autodoc, an AI-powered tool by ContextLabs, is designed to automatically generate documentation for codebases. The AI model is based on GPT-3 and uses natural language processing to translate code into user-friendly documentation. Autodoc claims to save time and reduce errors by generating documentation that describes how a system and its subsystems work independently and interdependently. Users can query the documentation using natural language questions. While some users remain sceptical of AI-generated documentation, developers who use the tool say that Autodoc provides a useful starting point for understanding unfamiliar code. Autodoc currently supports Python, JavaScript and Rust, and more languages are expected in future.",
    "summary": "- Autodoc is an AI toolkit for auto-generating codebase documentation using LLMs, which indexes a codebase, calls an LLM for each file and folder, and generates a doc command that developers can use to ask specific questions about the codebase.\n- ChatGPT is being used as a language model to complete text prompts for writing documentation for software, but some argue it may lead to lousy documentation without proper context from human-generated documentation.\n- Autodoc by ContextLabs is an AI-powered tool based on GPT-3 that translates code into user-friendly documentation, saving time and reducing errors. It provides a useful starting point for understanding unfamiliar code and currently supports Python, JavaScript, and Rust."
  },
  {
    "id": 35302735,
    "timestamp": 1679753068,
    "title": "Unintuitive Clock Mirrors US Date Format",
    "url": "https://domsson.github.io/freedom-clock/",
    "hn_url": "http://news.ycombinator.com/item?id=35302735",
    "content": "\n\n\n\n\nFreedom Clock\n\n\n\n\n\n\n\nThe Freedom Clock\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat the fuck?\n\n\n\n\nConfused?\nDoes your brain hurt? Do you find this way of displaying the time unintuitive or outright stupid?\nWell, now you know how the rest of the world feels when they see the American date format.\nThe Freedom Clock follows the exact same \"logic\".\nLet me see again...\n\n\n\n\n\n\n\n\n\n\nHacker News\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Hacker News\n\n\n\n\n\n\n\n\n\n          Sorry, we're not able to serve your requests this quickly.\n        \n\n\n\n\nreload\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "summary": "- The Freedom Clock displays time in an unintuitive manner, similar to the US date format.\n- Some may find the display confusing or stupid.\n- The clock follows the same \"logic\" as the American date format.\n- Hacker News may not always be able to serve requests quickly."
  },
  {
    "id": 35303574,
    "timestamp": 1679757988,
    "title": "Nvidia CEO discusses impact of AI & growth plans",
    "url": "https://stratechery.com/2023/an-interview-with-nvidia-ceo-jensen-huang-about-ais-iphone-moment/",
    "hn_url": "http://news.ycombinator.com/item?id=35303574",
    "content": "Nvidia's CEO Jensen Huang spoke with Ben Thompson about the impact of ChatGPT, which he called AI's iPhone moment due to its game-changing capabilities and widespread adoption. ChatGPT has driven an inflection point in the adoption of AI and created a new computing model where the way computers are programmed has changed, said Huang. This has accelerated demand for training and inference for language models, and Nvidia is working to support this growth. The company has responded by upgrading its models, developing supporting models and providing inference capability. Huang emphasised the need to respond to demand and increase urgency to create new resources.Nvidia CEO Jensen Huang discussed the challenges the company faced when taking large write-downs on inventory, which included future purchase order obligations with TSMC. Huang discussed balancing these write-downs with the perceived explosion in AI applications and the disconnect he identified. He also talked about the constraints on building more and meeting demand, including the complexity of building AI supercomputers and the need for switches, NICs, and cables. Huang also discussed the importance of inference and the scale of its business. He talked about the limitations of the 800 series for China due to export controls and the importance of interconnects in scaling up. Finally, he talked about DGX Cloud and Nvidia's plans to run services in GPUs in everyone's clouds.Nvidia CEO Jensen Huang discussed the company's strategy of building a computing platform available everywhere, including at data centre scale, which involves integrating into cloud service providers (CSPs) such as AWS, Azure and OCI. Nvidia is selling its full-stack platform separately and connecting it into the world's computing fabric. Huang said he is building a company that is full-stack, vertically integrated and operates in multiple domains such as artificial intelligence, robotics and computational lithography. Further, the firm has a salesforce and marketing operation that\u00a0works closely with the CSPs, and they all\u00a0win in the relationship. In Huang's view, today's computer is a data centre, which must be orchestrated as if it were one, leading to the development of software-defined architecture.Jensen Huang, CEO of Nvidia, says the company's new subscription model is not designed to replace its current business model of selling products, but rather to engage directly with customers and accelerate their end-to-end machine learning operations platforms. He points out that many businesses standardise their ML and AI functions, and their teams need to accelerate frameworks themselves. They believe they can do a better job and that is why they require the expertise and proprietary technology of Nvidia. However, for those who are new to machine learning, Huang advises firms to use one of the public clouds because \u201cif it\u2019s already accelerated on GPU, it'll be accelerated on just about everybody\u2019s cloud.\"Nvidia CEO, Jensen Huang, discussed the advantages and limitations of centralised vs local computing, and his opinion on the future of AI computing in an interview with Ben Thompson of Stratechery. Huang said that centralised computing offers scalability, but there are costs attached to it. Meanwhile, local computing offers lower run-time costs and it is expected that local generative AI applications will run everywhere, including on phones, in the future. Huang expects the performance of large language models to improve drastically over the next ten years. Nvidia has been a major player in the development of AI due to its core GPU, and Huang believes that inference, a piece of software generated by computers, will be the way software is operated in the future.Nvidia's announcement of discontinuing support for the open standard OpenCL API has drawn mixed opinions, including skepticism on the chances for its proprietary CUDA platform. Commentators suggest that while CUDA has dominated the market for a decade and enjoys a low-level software interface, recent moves by competitors may soon change that. Apple's development of its own chips and gyroids for mobile AI applications, Google creating its chips for extensions for TensorFlow, and AMD producing both CPUs and GPUs, suggest that there could be potential for viable alternatives to CUDA. Nvidia's lack of a story for edge devices may be its biggest challenge in the face of rising competition. However, its dominance in the gaming industry, already experiencing its AI moment, may act as a fallback.AMD's graphics R&D is focused on consoles, which they're working on with Sony and Microsoft for their CPU-GPU fusion products, leaving them behind Nvidia when it comes to BLAS on an APU. AMD does not have the market share to determine what PC-gaming features are adopted, limiting its ambitions to punching Intel. Even if they were to develop a \"DLSS-level\" improvement, it would be difficult to get it adopted as it would only impact a narrow slice of the market. AMD is not willing to invest in graphics research on their own if it doesn't have an obvious return on investment. Consoles drive AMD's graphics R&D, and while they'll tap other lucrative markets like HPC, they won't invest in big spends that don't generate income. Nvidia's dominance in DLSS and commercial market research is a result of the company being focused on software that sells hardware.Nvidia CEO, Jensen Huang, tells Bloomberg that the hardware software for AI is becoming easier, as inference becomes a more important mode of operating software. Huang suggests that \"every computer will just run inference someday\", because the software will be written by a machine learning (ML) algorithm or model. Huang also says that \"the world runs on models, and data is the new oil\". Nvidia\u2019s success in AI hardware has come through its development of the CUDA parallel computing platform, which is still dominant over other hardware platforms. However, competitors including Intel, AMD and startup Cerebras Systems, are slowly making inroads into the market as they develop AI chips specifically designed for the processing demands of deep learning, among other fields.The discussion is about apps that make text easier to consume. NaturalReaders is recommended as a good one. Someone asks what's wrong with just reading text as is.",
    "summary": "- Nvidia CEO Jensen Huang discusses the impact of ChatGPT on the adoption of AI and the need for supporting models and resources to meet demand\n- Nvidia is working on building a computing platform available everywhere and integrating with cloud service providers like AWS, Azure, and OCI\n- Huang explains that the new subscription model is meant to accelerate customers' end-to-end machine learning operations platforms\n- Huang talks about the advantages and limitations of centralized vs local computing and the future of AI computing\n- Nvidia's discontinuation of support for the OpenCL API and reliance on its proprietary CUDA platform is drawing skepticism due to rising competition from AMD, Apple, and Google\n- AMD's graphics R&D is focused on consoles, limiting its impact on other markets\n- Huang predicts that every computer will eventually run inference as the software is written by ML algorithms and models\n- NaturalReaders is recommended as a good app for making text easier to consume."
  },
  {
    "id": 35302858,
    "timestamp": 1679753861,
    "title": "Code Alpaca: Fine-Tuned Instruction-Following Model for Code Generation",
    "url": "https://github.com/sahil280114/codealpaca",
    "hn_url": "http://news.ycombinator.com/item?id=35302858",
    "content": "Code Alpaca is a fine-tuned instruction-following LLaMA model for code generation. It is hosted on GitHub and aims to build and share an instruction-following LLaMA model for code generation. The model is not finetuned to be safe and harmless, so one should be cautious. The Code Alpaca models are fine-tuned from a 7B and 13B LLaMA model on 20K instruction-following data generated by the techniques in the Self-Instruct paper. The repo contains the 20K data used for fine-tuning the models, code for generating the data, and code for fine-tuning the model. The fine-tuning was done using standard Hugging Face training code and deepspeed with specific hyperparameters. The results are currently not available for evaluation. However, it is expected to have limitations since the approach has not been optimized yet. The model weights are not part of the release to respect OpenAI's TOS and LLaMA license.The comment thread is discussing the cost and feasibility of fine-tuning the Llama C++ instruction model with a large number of parameters using GPUs. Many users are discussing the possibility of replicating the project using cloud GPUs and alternative fine-tuning strategies. Some users express interest in obtaining access to the fine-tuned models, while others caution that there may be licensing restrictions. The main concern is the cost of training the model, but some users suggest that it can be done for relatively low cost using cloud GPUs and parameter-efficient fine-tuning strategies. The weights of the model have changed during fine-tuning compared to the original Llama models.",
    "summary": "- Code Alpaca is a fine-tuned instruction-following LLaMA model for code generation, hosted on GitHub and fine-tuned from a 7B and 13B LLaMA model on 20K instruction-following data.\n\n- Users are discussing the cost and feasibility of replicating the project using cloud GPUs and alternative fine-tuning strategies.\n\n- The fine-tuning was done using standard Hugging Face training code and deepspeed with specific hyperparameters, and the results are currently not available for evaluation.\n\n- The model weights are not part of the release to respect OpenAI's TOS and LLaMA license, and caution should be taken as the model is not fine-tuned to be safe and harmless."
  },
  {
    "id": 35304078,
    "timestamp": 1679760710,
    "title": "Aquarium: AI Control for Linux Containers with GPT-3",
    "url": "https://github.com/fafrd/aquarium",
    "hn_url": "http://news.ycombinator.com/item?id=35304078",
    "content": "Aquarium is a project that uses AI to control Linux containers, giving a large language model control over a Linux machine. It starts with a prompt, such as \"Your goal is to run a Minecraft server,\" and the AI will then execute Linux commands and provide the resulting terminal output. Each command's outcome is sent to OpenAI, which responds with a summary that is used as part of the next prompt. Bot Aquarium is inspired by xkcd.com/350 and Optimality is the tiger, agents are its teeth. It is built using Go programming language and Docker. This project is not perfect, with no success criteria built in yet, and it cannot give input to running programs. Terminal output handling is not perfect, and some commands require manual input.The development and application of AI technology has been described as both exhilarating and terrifying. Some say that AI could be the most awesome force the planet has ever seen, while others have concerns about the potential for AI to cause harm. Some commenters on a recent Hacker News post suggest using AI for tasks such as analyzing and responding to attacks and anomalies, while others warn that AI could become a weapon in malicious hands. There is also discussion about the potential for AI to be used in industries such as nuclear power plants and aviation, as well as its potential use in video games. Some express concern that people do not fully understand the dangers of AI and that more sophisticated cognitive architectures should be developed for autonomy.OpenAI's GPT-3 language model has been given control of a virtual Linux machine, making it capable of executing commands and potentially carrying out actions without human oversight. While the project is being seen as a fun experiment, it has also raised concerns about the potential misuse of AI technology. Critics have compared the development to the plot of the Terminator movies and warned of the dangers of giving AI too much control, while others have pointed out that AI is already being used to control certain aspects of critical infrastructure. The debate about the role and limits of AI continues to gather momentum, with some calling for stricter regulation of the technology.",
    "summary": "- Aquarium is a project that uses AI to control Linux containers, executing Linux commands and providing terminal output based on a prompt given.\n- Bot Aquarium is built using Go programming language and Docker, with some imperfections such as no success criteria built in yet and the inability to give input to running programs.\n- OpenAI's GPT-3 language model has been given control of a virtual Linux machine through Aquarium, causing concerns about the potential misuse of AI technology and the need for stricter regulation."
  },
  {
    "id": 35297839,
    "timestamp": 1679705557,
    "title": "PDF v1.2: Multi-Object Compression with LZWDecode Filter",
    "url": "https://www.cs.utexas.edu/~fussell/courses/cs352h/papers/moore.pdf",
    "hn_url": "http://news.ycombinator.com/item?id=35297839",
    "content": "The text appears to be a PDF file, version 1.2. It consists of multiple objects, including fonts and pages, with a parent catalog and previous reference. The content of each object is compressed using the LZWDecode filter, with varying lengths. The key points are that the file is a PDF version 1.2, contains multiple objects, and is compressed using the LZWDecode filter.",
    "summary": "- PDF file version 1.2\n- Multiple objects (fonts, pages, etc.)\n- Compressed using LZWDecode filter (varying lengths)"
  },
  {
    "id": 35299951,
    "timestamp": 1679724405,
    "title": "C vs. C++ for Embedded Programming",
    "url": "https://www.youtube.com/playlist?list=PL3GWPKM6L17H0RyU2o7p9gCnepjSTaHia",
    "hn_url": "http://news.ycombinator.com/item?id=35299951",
    "content": "CSE 325 lecture videos on low-level C programming have been shared on Hacker News. One commenter argues that C++ compilers are more complex and not available on many small platforms, making C a better choice for most embedded programming. Another points out that compiler availability for specific microcontrollers and architectures is the main factor driving C's dominance in the industry. Many support tooling, header files, peripheral drivers, and RTOS are also written in C, giving it an advantage. However, some embedded C++ and Rust features, such as RAII and operator overloading, can make these languages cleaner and more efficient for specific use cases. The C standard is noted for being minimalistic and allowing for use on diverse architectures.Using a uniform ARM instruction set has made embedded software development easier by enabling the use of mainline GCC instead of vendor-managed compilers. However, some developers still deal with outdated SDKs due to price constraints. C++ can be useful in embedded programming thanks to features such as RAII, operator overloading, and references, but heap allocation restrictions mean that some C++ features cannot be used. Despite this, the STL is not fundamentally tied to heap allocation, and C++ could be simplified by removing older and redundant features. Overall, a uniform instruction set and modern programming features can streamline embedded software development.",
    "summary": "- C is a better choice for most embedded programming due to the complexities and limited availability of C++ compilers on small platforms\n- Compiler availability for specific microcontrollers and architectures drives C's dominance in the industry\n- C has an advantage with support tooling, header files, peripheral drivers, and RTOS written in it\n- Embedded C++ and Rust have features that can make them cleaner and more efficient for specific use cases\n- The C standard allows for use on diverse architectures and using a uniform ARM instruction set has made embedded software development easier\n- C++ can be useful in embedded programming thanks to features such as RAII and operator overloading, but some features cannot be used due to heap allocation restrictions\n- The STL is not fundamentally tied to heap allocation, and C++ could be simplified by removing older and redundant features.\n- A uniform instruction set and modern programming features can streamline embedded software development."
  },
  {
    "id": 35301138,
    "timestamp": 1679738445,
    "title": "Common Lisp ANSI Standard Quick Reference",
    "url": "http://clqr.boundp.org",
    "hn_url": "http://news.ycombinator.com/item?id=35301138",
    "content": "\n\n\nCommon Lisp Quick Reference\n\n\n\n\n\n\n\n\n\nCommon Lisp Quick Reference\n\n\n\n\n\nhome\ndownload\nprinting & bookbinding\nsource\nlicense\n\u00a0\n            statistics\n\n\n\n\n\n\nIntroduction\n Common Lisp Quick Reference is a free booklet with short\n\t    descriptions of the thousand or so symbols defined in the ANSI standard. \n\t    It comes with a comprehensive index.\n\t  \n\n\t    This rather humble effort is by no means meant to rival\n\t    the \n\t    Common Lisp HyperSpec \n\t    or any of the great introductory web resources and books. \n\t    Its purpose is to give those who like a piece of\n\t    dead tree in their hands a quick overview on things they\n\t    know already, or some clue on what to look up elsewhere.\n\t  \nIt is written in LaTeX and formatted for printing on both\n\t    A4 and letter paper. After folding the sheets lengthwise,\n\t    they can easily be turned into a handy booklet. \n\t  \n\n\t    Please report any errors to \n\t    trebbu@googlemail.com. Nitpickers welcome!\n\t  \n\u00a0\nLatest Changes\n2018-10-10 14:08:44 +0200\nFix bugs reported by Fengjing Xiao\n- define-compiler-macro is a macro, not a function.\n- symbol-name and symbol-package aren't setfable.\n\n2018-02-05 12:00:32 +0100\nInitarg names don't need to be keywords\nFix definitions of defstruct, define-condition, signal, warn, error,\ncerror, and assert.\nThanks to Fengjing Xiao for the bug report.\n\n2018-02-05 11:55:28 +0100\nFix argument list of scale-float\nThanks to Fengjing Xiao for the bug report.\n\n2018-01-23 12:36:42 +0100\nCLOS-related initargs don't need to be keywords\nThanks to Fengjing Xiao for the hint.\n\n2018-01-15 15:24:54 +0100\nRemove a bit-rotten LaTeX dependency\nMinor layout modifications.\n \n\n\n\n\t  \u00a0\n\t\n\n\n\n\t  \u00a9 2008 - 2018 \u00a0\n\t  \n\t    Bert Burgemeister\n\t  \n\n\n\n\n\n\n\n\n\n\n\n\nCommon Lisp Quick Reference (2018) | Hacker News\n\nHacker News\nnew | past | comments | ask | show | jobs | submit \nlogin\n\n\n\n\n Common Lisp Quick Reference (2018) (boundp.org)\n140 points by abudabi123 20 hours ago  | hide | past | favorite | 30\u00a0comments \n\n\n \n  \n \ndjha-skin 16 hours ago  \n             | next [\u2013] \n\nOf particular interest to me is the loop facility reference. It appears to be terse, understandable, and relatively complete. I also like the notation. A subscript f to tell me it's not a macro is incredibly helpful.Friendly reminder about printing services like this one[1] that will print and ship a PDF to you.https://www.printme1.com/\n \nreply\n\n\n\n  \n \nr9550684 14 hours ago  \n             | parent | next [\u2013] \n\nhyperspec has a compact take on loop, which is my goto when I don't quite remember details of syntax, http://www.lispworks.com/documentation/HyperSpec/Body/m_loop.... it also has a benefit of being ascii representable\n \nreply\n\n\n\n  \n \nUser23 15 hours ago  \n             | parent | prev | next [\u2013] \n\nAnother fun option is to print the folios and bind them yourself. A serviceable job can be done in an afternoon with little more than a sewing kit. It\u2019s easy, using a ruler, an awl or something similar, and a stiff piece of cardboard, to make a guide template for punching holes in the folios. After that it\u2019s just a matter of stitching.\n \nreply\n\n\n\n  \n \nakho 12 hours ago  \n             | root | parent | next [\u2013] \n\nThe book in the post is a single folio. A whole afternoon is an overstatement, I\u2019d say.\n \nreply\n\n\n\n  \n \ndjha-skin 14 hours ago  \n             | root | parent | prev | next [\u2013] \n\nI did it myself just now it works great.\n \nreply\n\n\n\n  \n \nkqr 16 hours ago  \n             | prev | next [\u2013] \n\nWhenever I see things like this I wish CL was available by default in more environments. It would have been the ultimate glue language also capable of growing software into medium-to-large size projects with some care.Instead we have Perl. It's not bad and fulfills many of the same criteria, but it's also not quite as elegant.\n \nreply\n\n\n\n  \n \nmark_l_watson 13 hours ago  \n             | parent | next [\u2013] \n\nSBCL is available to install on MacOS with brew, with apt or yum on Linux, etc. You can make small standalone applications easily. I have used it for command line utilities (I have some simple examples in my CL book that you can read online https://leanpub.com/lovinglisp/read)The linked CL reference looks very nice, BTW.\n \nreply\n\n\n\n  \n \nNezteb 8 hours ago  \n             | root | parent | next [\u2013] \n\nI also recently updated the Mac install docs for Portacle for anyone looking for anyone looking to tinker with CL on newer Macs! https://portacle.github.io/\n \nreply\n\n\n\n  \n \nmark_l_watson 5 hours ago  \n             | root | parent | next [\u2013] \n\nPortacle is especially nice because you bundle Emacs with it.\n \nreply\n\n\n\n  \n \nimwithstoopid 10 hours ago  \n             | parent | prev | next [\u2013] \n\nI would argue perl has a superior practical eleganceso you can write perl in a kinda-sexp style if you really want to (no one does, which tells you something)...but you get a kinda-sexp built-in syntax for hashes whereas lisps make you construct them with bolted on functions\n \nreply\n\n\n\n  \n \nKototama 10 hours ago  \n             | parent | prev | next [\u2013] \n\nWho is still using Perl as a glue language? It's often Python to glue things now.\n \nreply\n\n\n\n  \n \nmpweiher 12 hours ago  \n             | parent | prev | next [\u2013] \n\n> It would have been the ultimate glue languageInteresting.  Why particularly a glue language?  Just because it's the ultimate language and thus by definition also the ultimate glue language or is there something more specific?\n \nreply\n\n\n\n  \n \ncitizen_friend 12 hours ago  \n             | prev | next [\u2013] \n\nI also like the simplified Common Lisp reference: https://jtra.cz/stuff/lisp/sclr/index.html\n \nreply\n\n\n\n  \n \nofalkaed 8 hours ago  \n             | prev | next [\u2013] \n\nPeople who publish programming material online in PDF format should really take notice of this and the formatting. The half A4 version is skinny enough that you can stick it next to your editor/IDE window and still see the entire page even on small screens.\n \nreply\n\n\n\n  \n \nnanna 7 hours ago  \n             | prev | next [\u2013] \n\nThis is a great showcase not only of Common Lisp, but of (La)TeX.\n \nreply\n\n\n\n  \n \nlenkite 12 hours ago  \n             | prev | next [\u2013] \n\nWill there ever be a newer edition of the Common Lisp standard to reflect the major changes in the computing landscape ?\n \nreply\n\n\n\n  \n \nimwithstoopid 10 hours ago  \n             | parent | next [\u2013] \n\nI would suggest that this is what Racket isthey don't call it Lisp because it came from Scheme but they wisely realized the first step to making changes is to stop calling it Scheme otherwise purists will just rain fire on it and insist on living in the pastfrom a practical perspective it seems perfectly safe and reasonable to say Racket is Lisp2023otherwise maybe elisp? it is by far the most important thing with \"lisp\" in its name in 2023\n \nreply\n\n\n\n  \n \nNezteb 8 hours ago  \n             | root | parent | next [\u2013] \n\nI\u2019d agree about Racket; the community and resources available for it are fantastic.Also worth noting is Hy, a sort of alternate Lisp syntax for Python. http://hylang.org/\n \nreply\n\n\n\n  \n \nlispm 12 hours ago  \n             | parent | prev | next [\u2013] \n\nWho knows? Currently there is no one working on a new standard. Maybe an AI will publish one in 2094, hundred years after the first standard.For now with CL we can program in the past and reuse some code from 30 years ago. Changes will need to reflected in implementations, libraries and community standards. Some stuff would need changing the existing, others not so much. Example: Due to meta-programmability something like the Common Lisp Object System was implemented 95% in the language itself - the remaining 5% largely needed to be implementation specific things like changing the type system. Other stuff like 'threading' or 'foreign function interface' would need more work - but they exist already in implementations and libraries.\n \nreply\n\n\n\n  \n \nergonaught 17 hours ago  \n             | prev | next [\u2013] \n\nIt is amusing that the quick reference is a densely packed \"52 pages\".\n \nreply\n\n\n\n  \n \ndreamcompiler 14 hours ago  \n             | parent | next [\u2013] \n\nCommon Lisp is huge because libraries were not a big thing when it was designed, so it included lots of kitchen sinks.This has some advantages:1. The built in functions generally are very efficient and don't have many edge-case gotchas.2. You don't have to worry about which version of a library function to use, i.e. no dependency hell.3. The above is true for every CL implementation, from any supplier.It also has some disadvantages: When designing a language you can never predict all the functionality people will need. Useful things like networking and multithreading and package management were not included in the CL standard. Libraries for the missing pieces now exist to fix those shortcomings.\n \nreply\n\n\n\n  \n \npfdietz 3 hours ago  \n             | root | parent | next [\u2013] \n\nHaving the functions built in also allows them to be specialized at compile time.  For example, if we have an operation on sequences, and we know the sequence is a particular kind (say, a list), we can invoke the method for that sequence.It would be nice if such advantages could be ported to user defined libraries, or if built in functions could be extended (by making them generic and adding methods, say) without losing these advantages.  Many of the features in Common Lisp are there because they helped implement things in the standard; it would be nice if that principle could be extended.  The ultimate goal would be (in some standardized way) to expose to a library writer all the mechanisms that could make the built ins particularly effective.\n \nreply\n\n\n\n  \n \ndreamcompiler 1 hour ago  \n             | root | parent | next [\u2013] \n\ndefine-compiler-macro handles many of the cases you describe in your first paragraph. But I agree it would be nice if more of the functions in CL were generic. I think the issue is that those functions were standardized before CLOS was added and made defgeneric/defmethod a normal mechanism for writing functions.\n \nreply\n\n\n\n  \n \nlispm 12 hours ago  \n             | root | parent | prev | next [\u2013] \n\nwe need to keep in mind that many things were still emerging when the core language was designed - 1982 specs appeared with the first language book published in 1984. CLOS (the Common Lisp Object System) for example was mostly a research project, whose first spec was published in 1988, the Meta-object Protocol book was published in 1991 - which was unusual, since one would standardize on common practice. Stuff like networking and multithreading existed in very different ways or only very primitive. Networking usually meant to interface to the local platform network stacks and multithreading was still largely unsolved - parallel Lisp was still a research subject, even though thread/concurrency/parallelism support existed in very different ways - often 'green threads' (cooperatively scheduled) or for experimental machines (like the massive parallel connection machine). Now even my watch has two cpu cores and smartphones have even more cores.\n \nreply\n\n\n\n  \n \n1bent 16 hours ago  \n             | parent | prev | next [\u2013] \n\nI've always felt that Common Lisp was the Lisp family incarnation of PL/I; include everything you think might be helpful to offer a warm welcome to folks coming from FORTRAN or COBOL.\n \nreply\n\n\n\n  \n \ninimino 16 hours ago  \n             | parent | prev | next [\u2013] \n\nIt's CL, what are you gonna do...\n \nreply\n\n\n\n  \n \nTurboHaskal 12 hours ago  \n             | prev | next [\u2013] \n\nThis is so lovely one of my main reasons to main Common Lisp is to have an excuse to use it. Not even kidding.\n \nreply\n\n\n\n  \n \nnanna 7 hours ago  \n             | prev | next [\u2013] \n\nIs there an Emacs info mode CL manual out there?\n \nreply\n\n\n\n  \n \nr9550684 7 hours ago  \n             | parent | next [\u2013] \n\nthere's dpans2texi https://github.com/rebcabin/dpans2texi, which lets you convert dpans https://github.com/xach/dpans the tex source for the Common Lisp standard to texinfo which you can open in emacs\n \nreply\n\n\n\n  \n \ngibsonf1 17 hours ago  \n             | prev [\u2013] \n\nThis is fantastic.  The way this is organized I have already found some functions I didn't realize were available.  Many thanks to the author!\n \nreply\n\n\n\n\n\n\n\nApplications are open for YC Summer 2023\nGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact\nSearch:  \n\n\n",
    "summary": "- Common Lisp Quick Reference is a free booklet with short descriptions of the thousand or so symbols defined in the ANSI standard\n- It is meant to give a quick overview to Common Lisp programmers or a clue on what to look up elsewhere\n- It is written in LaTeX and formatted for printing on both A4 and letter paper\n- Latest Changes include bug fixes reported by users and minor layout modifications\n- Common Lisp is a large language that includes many built-in functions and has advantages and disadvantages. Libraries for missing pieces now exist to fix the language's shortcomings.\n- Printing services are available for the booklet, and some users have recommended binding the pages yourself.\n- Many people still find Common Lisp useful and are using it for various purposes."
  },
  {
    "id": 35299885,
    "timestamp": 1679723646,
    "title": "Immune Cells in Gut Impact Mental Health; Anti-inflammatory Found",
    "url": "https://www.hopkinsmedicine.org/news/newsroom/news-releases/new-evidence-immune-system-cells-in-the-gut-linked-to-stress-induced-depression",
    "hn_url": "http://news.ycombinator.com/item?id=35299885",
    "content": "A team of researchers led by Johns Hopkins Medicine have identified immune system cells in the gut that impact the gut microbiome, which in turn affects brain functions linked to stress-induced depression. The researchers identified intestinal gamma delta T cells (\u03b3\u03b4 T cells) and a protein receptor, dectin-1, as potential targets for treating stress-induced behaviours. A compound extracted from wild mushrooms, pachyman, was tested as a natural anti-inflammatory agent and was found to bind to dectin-1, inhibiting stress-induced \u03b3\u03b417 T cell activity and easing social avoidance behaviour. The researchers also investigated the gut organisms in people with major depressive disorder and found that the relative abundance of Lactobacillus was inversely related to higher depression and anxiety scores. Further research is needed to understand the impact of \u03b3\u03b4 T cells in the intestinal immune system on neurological functions.The comments section of an article on gut health and mental wellbeing includes anecdotal advice on diet and probiotics. Some users suggest that fasting, eating early, and consuming probiotics can help improve gut health and alleviate issues such as hemorrhoids and infections. Others caution against drawing conclusions from limited studies, and advise prioritizing known therapies for mental health issues. However, one user argues that people should not wait for scientific consensus before taking proactive steps to address their health. They suggest that the early stage of research into the gut-brain connection provides enough information for people to make changes to their diets and habits, without relying solely on medical professionals.",
    "summary": "- Researchers identify immune system cells in the gut that impact the gut microbiome, which affects brain functions linked to stress-induced depression.\n- Intestinal \u03b3\u03b4 T cells and dectin-1 protein receptor are identified as potential targets for treating stress-induced behaviours.\n- Pachyman, a compound extracted from wild mushrooms, is found to inhibit stress-induced \u03b3\u03b417 T cell activity, easing social avoidance behaviour.\n- The relative abundance of Lactobacillus is inversely related to higher depression and anxiety scores in people with major depressive disorder.\n- Further research is needed to understand the impact of \u03b3\u03b4 T cells in the intestinal immune system on neurological functions.\n- Anecdotal advice on diet and probiotics is shared in the comments section, some caution against drawing conclusions from limited studies.\n- One user suggests that the early stage of research into the gut-brain connection provides enough information for people to make changes to their diets and habits without relying solely on medical professionals."
  },
  {
    "id": 35308246,
    "timestamp": 1679783873,
    "title": "Crafting a Safe Rust Interpreter for Lox with Optimizations",
    "url": "https://ceronman.com/2021/07/22/my-experience-crafting-an-interpreter-with-rust/",
    "hn_url": "http://news.ycombinator.com/item?id=35308246",
    "content": "Manuel Cer\u00f3n shares his experience building an interpreter with Rust by implementing an interpreter for a dynamic language named Lox. Cer\u00f3n was interested in checking if Rust could match C/C++\u2019s speed while providing better safety and ergonomics. He decided to stick to purely safe code, taking advantage of Rust standard library as much as possible. The hardest part of implementation was writing a garbage collector with no manual memory allocation and deallocation. In the end, he used a very simple design for a GC in safe Rust, which had some shortcomings. He wrote the Lox interpreter 100% in safe Rust code, passing the 243 tests from the Lox integration test suite. Finally, he added a few unsafe blocks to his code base to come close to the speed of clox.The author created the Loxido implementation of an interpreter in Rust for a scripting language called Lox. The initial view was that Loxido would be slower than clox, the C implementation, but it was much worse than expected. The author wrote some workarounds to please Rust's borrow checker. They also tried different approaches, such as raw pointers, but all failed. Then, by optimizing hash map operations, the author decided to write their own implementation, closely following the implementation of clox. They found the difference in the time it took to execute the benchmarks was substantially larger for the ones stressing the GC. To solve this, instead of using a trait object, the author used an enum which improved tracing speed considerably.The author of this article discusses how they crafted an interpreter with Rust, a multi-paradigm programming language designed for performance and safety. The article delves into the trade-offs between performance and safety and how the author made optimizations to gain speed but sacrificed safety in some areas, such as avoiding safety checks for stack underflow and overflow. The article includes thoughts on the Rust language as a whole, its strong community, and areas where the language could improve. The author also discusses benchmarks and potential improvements to the interpreter they crafted, such as switching to struct inheritance to represent GC objects and using variable-length instructions for a tighter bytecode.This discussion thread on Hacker News covers various aspects of programming language Rust, including the challenges of understanding ownership and borrowing, limitations on certain features such as self-referential structs, and the lack of a goto statement. One individual's experience of learning Rust is described as challenging but ultimately rewarding, and suggestions are made for ways in which learning materials could be more explicit. The thread also contains discussions of virtual machine development and issues relating to implementing finite state and pushdown automata. While opinions vary on the usefulness of goto statements, the potential advantages of having tight control over flow are noted, particularly in the case of virtual machine design.",
    "summary": "- Manuel Cer\u00f3n built a safe Rust interpreter for the Lox scripting language, passing all 243 integration tests using purely safe code.\n- Initially slower than the C implementation, the interpreter was optimized by implementing a custom hash map and using an enum instead of a trait object to improve GC tracing.\n- The article discusses the trade-offs between performance and safety and potential improvements to the interpreter, such as using variable-length instructions for tighter bytecode.\n- The discussion thread covers challenges in learning Rust, limitations on self-referential structs, and debates on the usefulness of goto statements in virtual machine design."
  },
  {
    "id": 35308796,
    "timestamp": 1679787805,
    "title": "Fix systemd CPU Spikes by Changing Timezone to Etc",
    "url": "https://forum.proxmox.com/threads/systemd-100-cpu-hang.124767/",
    "hn_url": "http://news.ycombinator.com/item?id=35308796",
    "content": "Several users on the Proxmox Support Forum reported a similar issue where their Proxmox host was not able to boot due to systemd consuming 100% CPU. They tried various solutions such as reinstalling, changing hardware, and downgrading the kernel but to no avail. However, one user reported that changing the timezone to Etc solved the problem. Further investigation revealed that systemd was continuously attempting to access the /etc/localtime file, causing the CPU spike. While the exact cause of this issue remains unclear, it appears to be related to the upcoming daylight saving time change. As a temporary fix, changing the timezone to Etc seems to be effective.",
    "summary": "- Proxmox users reported systemd consuming 100% CPU causing boot problems\n- Various solutions tried but changing timezone to Etc solved the issue\n- Systemd trying to access /etc/localtime file causing spike, possibly related to daylight saving time change"
  },
  {
    "id": 35300437,
    "timestamp": 1679730654,
    "title": "New Steel Tech Reduces Emissions, Boosts Efficiency & Offers Cleaner Production",
    "url": "https://industrydecarbonization.com/news/making-steel-with-electricity.html",
    "hn_url": "http://news.ycombinator.com/item?id=35300437",
    "content": "ArcelorMittal announced plans to commercialize Siderwin, a new steelmaking technology developed by the Siderwin research project, which uses direct electrolysis of iron oxide. This process allows electricity to be used to split iron oxide into its elements, rather than relying on the traditional use of coke made from cooking coal. This new process is said to be more efficient than using hydrogen, which is currently the most promising technology for green steel production. During the pilot phase, Siderwin also tested using waste products from other industries that contain iron oxide as a feedstock, such as\u00a0mill scale, a waste product from steel processing. ArcelorMittal expects an investment decision for a larger plant with a production capacity between\u00a040 and 100 tonnes per year by 2025, with a full-scale plant by the end of the decade.A novel electrolysis process developed by MIT spinout, Boston Metal, offers a cleaner, cheaper and more efficient way of producing steel without carbon emissions, the company claims. Its process replaces the use of coke or coal as a reducing agent in the conversion of iron ore to steel. Steel production is among the leading emitters of carbon. The industry\u2019s emissions represent between 7% and 9% of the global total, according to the\u00a0World Steel Association. A megatonne of steel produced via conventional methods emits around two tonnes of CO2, according to some estimates. Massachusetts-based Boston Metal currently produces a few kilograms an hour of iron, but plans to\u00a0scale-up output within a few years.New steelmaking technology that uses electricity to dissolve iron oxide to make steel has been developed by scientists at the University of Utah. The process, named Siderwin, also produces pure oxygen as a by-product. While steel production accounts for 7% of global emissions, the new method achieves an 80% reduction, using 60% less energy than the traditional blast furnace method. The approach appears particularly suited to regions with an abundance of cheap and clean electricity such as Europe, Canada or Scandinavia. The technology is expected to cost more than current methods; however, it is suggested that the technology will soon become competitive as the cost of electricity continues to fall. Besides, \"green\" industries are seeking\u00a0to lower emissions from materials in their supply chains.\nResearchers at Cambridge University have succeeded in recycling old steel using electrolysis, a process that could lead to a significant reduction\u00a0in carbon emissions, according to an article in The Guardian. Recycling steel in the current way, using an electric arc furnace to melt scrap, is about 30% less energy-intensive than producing it from scratch, but methane and CO2-heavy coking coal are still used in the process. The Cambridge method, which dissolves iron oxide\u00a0in a sodium hydroxide-based electrolyte at about 110C, uses much less power and emits only oxygen and steam. The researchers claim the \u201czero carbon\u201d steel can be manufactured using existing mills and furnaces, which could make for a quicker\u00a0switch from traditional to green steel production.",
    "summary": "- ArcelorMittal plans to commercialize Siderwin, a new steelmaking technology that uses direct electrolysis of iron oxide, which is more efficient than using hydrogen and can also use waste products as a feedstock. \n- Boston Metal's novel electrolysis process offers a cleaner, cheaper, and more efficient way of producing steel without carbon emissions, replacing the use of coke or coal. \n- Scientists at the University of Utah have developed Siderwin, achieving an 80% reduction in emissions and using 60% less energy than the traditional blast furnace method, and producing pure oxygen as a by-product. \n- These new technologies aim to reduce steel production's 7% global total of emissions and meet the growing demand from \"green\" industries to lower emissions from materials."
  },
  {
    "id": 35307640,
    "timestamp": 1679779985,
    "title": "Lessons from Docker's Workflow & Monetization Issues",
    "url": "https://computer.rip/2023-03-24-docker.html",
    "hn_url": "http://news.ycombinator.com/item?id=35307640",
    "content": "the problem of Docker's failure as a company is not to focus on the software itself but on the concept of Docker as a workflow for creating and distributing software images. While Docker as a software product was revolutionary in its ease of use, its value lies more in the images it creates and the ease with which they can be composed. However, Docker as a company has become a \u201cbackwater of rent-seeking and foot-shooting\u201d by trying to monetize through tools like Docker Desktop and Docker Hub. Docker Hub is replaceable, but its status as the default registry and its initially unsustainable \u201cfree\u201d model made it the central infrastructure of the DevOps community. The lesson to be learned is that Silicon Valley's VC-driven model can create products that are ultimately unsustainable and can cause problems for their users.The author argues that Docker Hub has become a \"backwater of rent-seeking and foot-shooting.\" While it is obvious to simply stop using Docker Hub, the reality is that moving off it is a pain and many projects will break a lot of downstream users. Docker Inc. is seen as engaging in rent-seeking behavior with Docker Desktop and Docker Hub, making dependent users pay in. The article suggests that people be skeptical of free services and that free services should never become deeply embedded dependencies. The article also states that the ongoing slow-motion meltdown over Docker Hub could have been greatly mitigated if the ability to use multiple Docker registries were considered, or at least being able to specify a third-party registry and authenticate. The article believes that Docker is an example of how open source projects could be owned by a company and used to extract rent.Docker is under scrutiny following their decision to eliminate free teams and mandate a registry for users to push and pull images from. While some critics believe that Docker's technology was merely evolutionary rather than revolutionary, others emphasize the importance of the user-friendly interface and workflow that Docker provided. However, some believe that Docker's decision to monetize their service and lack of transparency will cause the company to lose trust with the community. Furthermore, there is concern about Docker's ability to handle a large volume of images on their registry, with many users considering alternatives to avoid future issues. Despite the controversy, it is possible that Docker's technology will continue to thrive even if the company itself does not.The comments on a Hacker News post about Docker reveal mixed feelings about the company's monetization strategies. Some users argue that Docker could have taken a more aggressive monetization approach, while others point out that attempting to charge for basic services could lead to users switching to cheaper alternatives. Docker's decision to monetize its Desktop service caused backlash among some users, who felt that it clashed with the company's open-source image. Meanwhile, Docker Hub has become a central part of many DevOps toolchains, leading to concerns that the high cost of operating the service could be Docker's undoing. Despite concerns, some users plan to continue using Docker for personal projects, citing its ease of use and solid functionality.",
    "summary": "- Docker's value lies more in the ease with which it creates and distributes software images rather than the software product itself.\n- Docker's attempts to monetize through tools like Docker Desktop and Docker Hub have caused backlash and the perception of \"rent-seeking behavior\".\n- The slow meltdown over Docker Hub could have been mitigated with the ability to use multiple Docker registries or specify a third-party registry and authenticate.\n- Free services should not become deeply embedded dependencies, and users should be skeptical of them.\n- Despite controversy, Docker's technology may continue to thrive even if the company itself does not.\n- Opinions on Docker's monetization strategies are mixed, with some arguing for aggressive monetization and others expressing concerns about high costs and user backlash.\n- Docker Hub has become a central part of many DevOps toolchains, leading to concerns about its high cost of operation."
  },
  {
    "id": 35304766,
    "timestamp": 1679764481,
    "title": "Harvard Affirmative Action Trial: Transparency Questions Raised",
    "url": "https://www.newyorker.com/news/our-columnists/the-secret-joke-at-the-heart-of-the-harvard-affirmative-action-case",
    "hn_url": "http://news.ycombinator.com/item?id=35304766",
    "content": "During the recent trial concerning affirmative action at Harvard, a parody of Harvard's attitude towards Asian Americans was produced by a federal official and shared with the dean of admissions. When this was presented in court, Judge Allison Burroughs chose to keep it sealed from the public. This, along with other sealed sidebars, prompted journalist Jeannie Suk Gersen to file a letter with the court, asking that they be unsealed. Harvard objected, citing confidential information about applicants. Gersen and other media outlets filed further letters in support of Gersen's request for public access. In November, Burroughs rejected most of Gersen's request, but did reveal the existence of what she called a \"joke memo\" from the Department of Education official, which contained \"anti-Asian remarks.\" The content of the memo has since been released, but the case raises questions about court transparency and the use of racial data in university admissions.A report in The New Yorker has revealed that a private joke featuring \"Asian stereotypes\" played a part in a case brought against Harvard University. The case, brought by Students for Fair Admissions, alleged that the institution discriminated in favour of black and Hispanic applicants at the expense of equally qualified Asians. Among the evidence was a memo that mocked an Asian-American applicant, colloquially referred to in administrative shorthand as an \u201cAA CJer\u201d, and detailed his supposed interest in science and medicine. The trial judge, Allison Burroughs, deemed the document irrelevant and excluded it. The report also suggests that anonymous letters questioning Burroughs's impartiality, which she refused to open in court, referred to her own\u00a0rejection from Harvard.The \"holistic review\" process used by colleges to make admission decisions should be banned because it is often used for discriminatory purposes, according to HN commenters discussing a\u00a0lawsuit by the Students for Fair Admissions (SFFA) group against Harvard University. The process, which considers factors such as standardized test scores, extracurricular activities, race and socioeconomic background,\u00a0was intended to reduce the number of Jewish students, according to an Economist article. Critics argue that it\u00a0becomes a system of subterfuge, used for making subjective decisions based on undisclosed criteria. Commenters debated\u00a0the value of standardized test scores in isolation and the validity of the holistic review process, suggesting\u00a0that it doesn't necessarily benefit poorer applicants or change the perceived\u00a0inequalities of university admissions.The removal of SATs from university admissions is cause for concern because it reduces the predictive quality of GPA as a standard for academic preparedness and quality. The use of methods and rubrics to deemphasize the predictive quality of GPA is also a confounder in the admissions process, although it is likely that schools removing SATs are seeking to identify different qualities in their candidates. Additionally, extracurricular activities, letters of recommendation and personal statements used to determine admissions, favour the middle to upper class. Many universities aim to build future leaders in various fields, and while tests and GPAs bucket individuals into capable or incapable, schools ought to consider the results in a holistic manner. Accountability and standards in admissions remain areas that merit attention.The debate centers around whether grades alone or a holistic approach should determine who gets into universities. Commenters discuss the potential unfairness of using only grades, as wealthy students can afford better tutors and more study time, giving them an advantage. Others argue that a holistic approach can result in admitting students who are not prepared to compete, leading to a waste of resources. Some comments address the issue of legacy institutions like Harvard and suggest that they should be judged on the rigor of their curriculum rather than their past reputation. One commenter brings up the declining academic rigor of colleges and universities and the prevalence of unrigorous majors.The discussion centers around the idea of education and intelligence, with comments offering both support and skepticism. One comment references divinity degrees in the 19th century as being ideological and unrigorous. Another disputes this, saying that divinity students were erudite, intelligent, and that even if one disagrees with theology, it doesn't mean the people who studied it were silly. The conversation then shifts to whether the educated Western man of the 19th century was truly educated, or if there are better options for modern education. The discussion also includes ideas about women's education in the past and how education and intelligence are not necessarily the same thing. Some comments focus on Harvard as a private social club, perpetuating wealth and power in society.The debate around college admissions processes, particularly at elite institutions such as Harvard, has been reignited by a recently settled legal case. In the case, brought by Students for Fair Admissions, it was argued that the university\u2019s affirmative action programme unfairly discriminated against Asian-American applicants. The plaintiffs claim that Harvard rates Asian Americans lower than other ethnic groups when making subjective assessments of their personality traits. The court ultimately found in Harvard\u2019s favour, stating that the personal rating system was not discriminatory, but was a result of non-racial factors such as cultural differences. However, many activists remain concerned that subjective elements of admissions processes\u00a0can present hidden bias.Asian parents who emigrate to another country often prioritize their children\u2019s career choices in lucrative fields like medicine or law, due to the significant personal, financial and social risks they took while moving. However, if you visit Asian countries and see their countryside and farming villages, you see a different scenario. People there follow traditional lifestyles and practice their culture. There could be scenarios where parents pressure their children to pursue a particular field of study, as in the case of a Chinese international student whose parents forced her to study math instead of chemistry, and now want her to pursue grad school. Harvard has taken steps to push back on such overbearing parental attitudes by selecting independent thinkers who can make a difference in the world. However, choosing a career based on personal passion or interest may be a luxury afforded to few coming from low-income backgrounds.The article discusses the controversy surrounding a joke memo mocking Harvard University's apparent discrimination against Asian American applicants, which was initially kept secret during a lawsuit against the university. While some argue that the memo was intended as criticism and satire, others see it as evidence of discrimination. The article also highlights a potential conflict of interest between a regulator overseeing Harvard and the university's admissions office. There is a debate about whether the memo or the regulator's relationship with the admissions office is more significant. Regardless, the case has raised questions about the use of quotas and affirmative action in college admissions and could potentially lead to changes in policies.Comments on an article regarding the recent Harvard admissions lawsuit suggest that admissions controversies stem from the fact that the number of \"perfect\" candidates for Ivy League admission is high versus available spots. Selecting candidates only based on objective criteria, which seems inherently fairer, is impossible, leaving schools to select candidates based on subjective criteria, leading to biases creeping in. The article argues that Asian American students are being penalized for being hard-working, whilst subjective admission criteria allow the wealthy to buy their way in. The opacity of the process leads to distrust in admissions systems, and some suggest that universities want to meet demographic quotas but hide this fact behind obfuscated and opaque criteria. However, it is noted that ultimately, admissions will remain subjective, and it is the universities' right to admit those students they believe will best represent their institution.The text is discussing quotas and the need to be upfront about them. The author is suggesting that if quotas are desired, they should be clearly stated rather than implied or hidden. There may be sarcasm in some comments, but it is not the main point. The text also mentions that applications are open for YC Summer 2023 and provides a link to guidelines, FAQs, and other resources.",
    "summary": "- A parody memo mocking Harvard's alleged discrimination against Asian American applicants was initially sealed during a lawsuit against the university, raising questions about court transparency and conflict of interest.\n- The lawsuit brought by Students for Fair Admissions alleges that the university's holistic review process, which considers race as a factor, discriminates against Asian Americans.\n- The debate centers around the use of subjective criteria in university admissions, with critics arguing that it becomes a system of subterfuge used for making undisclosed subjective decisions.\n- The removal of SATs from university admissions is a cause for concern, as it reduces the predictive quality of GPA as a standard for academic preparedness and quality.\n- Asian parents may prioritize their children's career choices in lucrative fields, but students from low-income backgrounds may not have the luxury of pursuing personal passions or interests.\n- Admissions controversies stem from selecting candidates based on subjective criteria, leading to biases creeping in and the wealthy having an advantage.\n- The use of quotas and affirmative action in college admissions raises questions about transparency and fairness, and if quotas are desired, they should be clearly stated."
  },
  {
    "id": 35298339,
    "timestamp": 1679709354,
    "title": "Orange Pi 5: Faster, Affordable Alternative to Raspberry Pi 4",
    "url": "https://www.phoronix.com/review/orange-pi-5",
    "hn_url": "http://news.ycombinator.com/item?id=35298339",
    "content": "The Orange Pi 5, an ARM64 single-board computer, is a faster and more affordable alternative to the Raspberry Pi 4, according to Phoronix. It features an 8-core Rockchip RK3588S SoC and up to 32GB of RAM, making it suitable for a more diverse user-base, including those in need of a budget ARM Linux developer desktop. The Orange Pi 5 also has HDMI 2.1, Gigabit LAN, M.2 PCIe 2.0, and USB 3 connectivity. While the Orange Pi 5 supports several software options, including Orange Pi OS, Ubuntu, Debian, Android, and Armbian, it is not yet supported by the mainline kernel, which could lead to compatibility issues. However, progress is being made towards mainline support.The lack of availability and support for ARM single-board computers (SBCs) is a common frustration among users, according to comments on a Phoronix article. While the Raspberry Pi was one of the examples cited, shortages of the $35 board are exacerbating the problem. Many alternative chips are either poorly supported or unaffordable. Some users recommended the Rock Pi 4, the Orange Pi 5 and the VisionFive 2, but others preferred to look forward to increasing adoption of RISC-V rather than remain on ARM. There were also calls for greater mainstream support for all SBCs, while users such as kkielhofner warned that the Pi is often oversubscribed for tasks it isn't suited to.The author discusses their experience with old hardware, specifically Marvell Kirkwood devices, which have been supported for over a decade due to Marvell's commitment to upstreaming their work. They note that many other devices end up in a graveyard because of poor support, and that good vendors upstream their work to keep their devices working well. While they acknowledge the sufficiency of microcontrollers like the ESP32, they argue that using Linux systems are worth it, especially for the ease of use and community support of a general-purpose OS. They hope for improvements in embedded OSes like Zephyr and NuttX, but for now, recommend sticking with Linux. They also touch upon the issue of different port protocols used by different types of devices, but note that this may change in the future as good connectivity standards become more ubiquitous.The Orange Pi 5 is a new single-board computer that boasts being two to three times faster than the Raspberry Pi 4. The board is powered by a Rockchip RK3588 processor, which includes two Cortex-A76 cores and four Cortex-A55 cores. It also has 8GB of RAM, PCIe 4.0 support, and Ethernet and storage interfaces over PCIe instead of USB. Some users reported success in setting up Ubuntu and running containers with the board, and it is under active development for integration with the mainline Linux kernel. Critics, however, called it a cash grab and questioned its longevity and software support compared to the Raspberry Pi.",
    "summary": "- The Orange Pi 5 is a faster and more affordable alternative to the Raspberry Pi 4, featuring an 8-core Rockchip RK3588S SoC and up to 32GB of RAM.\n- It has HDMI 2.1, Gigabit LAN, M.2 PCIe 2.0, and USB 3 connectivity, and supports Orange Pi OS, Ubuntu, Debian, Android, and Armbian.\n- However, it is not supported by the mainline kernel yet, which could lead to compatibility issues.\n- Users have expressed frustration with the lack of availability and support for ARM single-board computers (SBCs), leading to recommendations for alternative chips like the Rock Pi 4 and the VisionFive 2, as well as calls for greater mainstream support for all SBCs.\n- The author discusses their experience with old hardware and notes the importance of good vendors upstreaming their work to keep devices working well.\n- The Orange Pi 5 is two to three times faster than the Raspberry Pi 4, but critics question its longevity and software support compared to the Raspberry Pi."
  },
  {
    "id": 35305050,
    "timestamp": 1679766172,
    "title": "Debunking the Alpha Wolf Myth: Age & Family Control Pack Order",
    "url": "https://www.newyorker.com/science/elements/the-myth-of-the-alpha-wolf",
    "hn_url": "http://news.ycombinator.com/item?id=35305050",
    "content": "The myth of the alpha wolf, which has infiltrated human society, has been debunked by new research. The idea originated from research done on captive wolves by Rudolf Schenkel in the 1930s and 40s, but further study has shown that it was largely inaccurate. Recent research on free-range wolves has shown that pack order is determined by age and familial structure rather than a fierce and constant battle for dominance. Additionally, elders play an important role in pack survival and are often responsible for leading pack members and mediating conflicts. Wolves have a complex social structure, and different populations have packs of varying sizes and structures, with family units headed by parents rather than alpha males and females. The wolf has been an important and often misunderstood figure in human lore for centuries.A study by Kira Cassidy and Connor Meyer, published in the journal Nature, looked at the effects of toxoplasmosis infection on wolf behaviour. The single-celled parasite is ingested by wolves when they eat the faeces of infected cougars, which multiply in the small intestine of the predator. Scientists discovered that the parasite influenced whether wolves dispersed and assumed leadership roles, with toxoplasmosis a strong predictor for both behaviours. The findings suggest humans may also be affected by toxoplasmosis, as studies show it makes people more aggressive and prone to car and motorcycle accidents. The Yellowstone wolf population, which has been studied for 27 years, provided ample data, according to researchers.",
    "summary": "- The myth of the alpha wolf has been debunked by new research on free-range wolves, which showed that pack order is determined by age and familial structure, rather than dominance. \n- Elders play an important role in pack survival and are responsible for leading pack members and mediating conflicts. \n- Wolves have a complex social structure, with family units headed by parents rather than alpha males and females. \n- The effects of toxoplasmosis infection on wolf behavior were studied, showing that the parasite can influence whether wolves assume leadership roles and disperse. \n- The findings suggest that humans may also be affected by toxoplasmosis, as studies have shown it can make people more aggressive and prone to accidents."
  },
  {
    "id": 35306805,
    "timestamp": 1679775307,
    "title": "Debate over Free AI Cover Letter Generator for Job Applications",
    "url": "https://www.careered.ai/tool/cover-letter",
    "hn_url": "http://news.ycombinator.com/item?id=35306805",
    "content": "CareeredAI has released a free AI-powered cover letter generator that takes data from users\u2019 resumes and a copy of the job post to deliver a personalised letter. The company claims the service can complete cover letters in minutes without users having to do more than providing information. However, the move was met with ironic criticism on social news aggregation site Hacker News. Some users pointed out that the application would simply add to the noise of submitted copy if enough people used it. Others noted the AI system's shuffling of stock phrases and flattery would stifle creativity and extinguish any semblance of human engagement from job applications.The comment thread discusses the pros and cons of cover letters in job applications. Some argue that cover letters are useful for weeding out candidates who are not serious about the job, while others feel that they are a waste of time and add unnecessary friction to the application process. Some suggest that AI could automate the process, while others express skepticism about the effectiveness of automated cover letters. There are also concerns about the effectiveness of hiring practices in general, with some feeling that the system is rigged against job seekers. Despite these issues, many still see value in the hiring process as a means of identifying good candidates.Some comments on HN discuss the use of AI-generated cover letters, with some expressing concern that it could devalue the importance of genuine effort put into cover letters. Others argue that the majority of cover letters received are already low-effort or boilerplate, and that a well-written cover letter can still stand out. One user suggests that the bar for communication in job searching is incredibly low, and simple person-to-person discussions can make a huge difference. Another user argues that passion is not a necessary quality for a good programmer, and relying on passion may lead to lower quality work on unexciting tasks. The discussion also touches on the potential impacts of AI on jobs, with some suggesting that the demand for human labor will decline as population increases and technology advances, and that basic income may be a solution.is a tool that generates a cover letter for a job application using OpenAI's GPT-3 language model. The tool prompts the user for some basic information and then generates a unique cover letter based on the job description and the user's inputs. Some commenters have expressed concerns that the use of an AI-generated cover letter would make job applicants look less authentic and could result in lower quality applications. Others have argued that cover letters are often ignored in the hiring process anyway and that the use of an AI-generated version could save time and effort for both applicants and recruiters.Comments on a job application platform include mixed feelings about the usefulness of cover letters in the modern application process, with some suggesting networking as a better approach. One user even suggests that AI-generated cover letters can be used to combat the perceived uselessness of cover letters. However, another user suggests the value in crafting a specific cover letter that explains why the job is the applicant's dream job rather than simply restating information from their resume. One user shares their own cover letter for an OpenAI Research Residency program application as an example of how to highlight applicable skills and experience. Some comments also touch on programming languages and AI technology.",
    "summary": "- CareeredAI has released a free AI-powered cover letter generator that creates a personalized letter in minutes, which has been met with criticism on Hacker News.\n- The discussion on Hacker News debates the pros and cons of cover letters in job applications, with some arguing they are useful for weeding out uncommitted candidates, while others see them as unnecessary friction in the application process.\n- Some users express concern that AI-generated cover letters could devalue the importance of genuine effort put into cover letters, while others suggest they could save time and effort for both applicants and recruiters.\n- Mixed opinions on the usefulness of cover letters lead some to suggest that networking and crafting a specific cover letter that explains why the job is an applicant's dream is valuable, while others see AI-generated cover letters as a solution to cover letter uselessness.\n- Discussion on programming languages and AI technology is also touched upon."
  }
]
