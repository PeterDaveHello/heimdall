[
  {
    "id": 38647484,
    "title": "Mitchell Hashimoto bids farewell to HashiCorp after 11 years, reflects on company's impact",
    "originLink": "https://www.hashicorp.com/blog/mitchell-reflects-as-he-departs-hashicorp",
    "originBody": "Contact sales Why HashiCorp Why HashiCorp Explore Why HashiCorp Partners & integrations Case studies Our approach The cloud operating model New customer story Deutsche Bank Products Products Infrastructure TerraformInfrastructure as code provisioning PackerBuild and manage images as code WaypointInternal developer platform NomadWorkload scheduling and orchestration VagrantEnvironment workflows Security VaultIdentity-based secrets management BoundarySecure remote access ConsulService-based networking Sign up for free Get started in minutes with our cloud products Developers Developers Get started Documentation Tutorials Partners & integrations Resource library Community Keep going Support Certifications Events Webinars Forum Service status A robust ecosystem Explore partners and integrations Event recap HashiConf ‘23 product announcements Solutions Solutions Infrastructure automation Infrastructure provisioning AI infrastructure management Network infrastructure automation Cloud cost optimization Cloud migration View all solutions Security Automation Zero trust security Credential rotation Encryption everywhere Auditing & compliance Privileged access management Modern Application Delivery Application delivery Application networking Workload orchestration Edge computing Kubernetes at scale Industries Financial services Healthcare Telecom Media & entertainment Public sector Technology Retail Manufacturing Resources Resources White papers Demos Resource library Blog Support Services & policies Certifications Training Professional services Enterprise academy Enterprise support plan Become a partner Upcoming events Webinars, workshops, and training courses Documentation Explore a brand new developer experience Company Company About Press Careers Blog Our principles Tao of HashiCorp Investors HashiConf Trust center Contact us Our principles The 9 pillars that guide us We’re hiring Discover careers at HashiCorp Contact sales Contact sales Blog Home Products & Technology Company HashiCorp Voices All Company Twitter share LinkedIn share Facebook share Copy URL Mitchell reflects as he departs HashiCorp After more than 11 years, HashiCorp Co-Founder Mitchell Hashimoto pens a heartfelt goodbye letter to the company he helped create. Dec 14 2023Mitchell Hashimoto Earlier this week, I sent this note to HashiCorp employees and am posting it here to let the entire HashiCorp community know about my plans: I have some bittersweet news to share with you all today: I've decided to move on from HashiCorp, and I'll soon no longer be an employee with the company. I recently celebrated 11 years since starting HashiCorp, and as I reflect back on the last decade I couldn't have asked for a better way to spend that part of my life. My departure from HashiCorp is something I’ve been thinking about and planning for a long time. Ever since founding HashiCorp, I've felt it's important to build a company where I'm not required for day-to-day operations and where other leaders can carry the torch over time. I have been very intentional about this as time went on: stepping down from being CEO in 2016, iterating over time on a culture of leadership autonomy that didn't require my involvement to make decisions, and finally departing the leadership team and board of directors in 2021. Since then, I've had the pleasure of working where I’m happiest — as a full-time, hands-on engineer. My passion as an engineer reaches beyond infrastructure and I always knew that at some point — when the company and I were ready — I'd move on and take on new, different challenges. My family recently welcomed our first child, and while reflecting during my time off I felt now was a fitting time to complete this transition. The world of cloud automation and infrastructure tooling is still ripe with opportunities and growth, but after nearly 15 years of working exclusively on tooling in this space, I'm ready to dabble in new areas. While my departure from HashiCorp is exactly what I've planned for, it's still a poignant moment. Nearly my entire adult life has revolved around the company. Many of my most formative memories happened in the context of this company. There are far too many to recount here, but I'd like to highlight just a few. Years before we started HashiCorp, Armon [Dadgar, HashiCorp Co-Founder and CTO] and I would talk about cloud, automation, and distributed systems incessantly. We were teenagers, and we'd playfully — not seriously — say things like, \"What if one day the biggest companies used our software?\" At one point, though, we took the first step and made some of our ideas into actual code. Next thing we knew, we had thousands of users. So, we took another step and started a company. A little later we took the next step and decided to raise funding. And that’s how HashiCorp became what it is today: we took many small little steps like this until we found that that playful, teenage idealism had become reality. Mitchell and Armon in 2013. As we got going, I felt like some “firsts” were particularly significant. The first HashiConf in 2015 will always be a special memory. It was the first time that the digital world really firmly crossed over into the physical world for me, and it was hard to believe that any of it was real. I knew that our download numbers were high and I knew I interacted daily with community members online, but it's something entirely different to see hundreds of people willingly choose to physically show up. I felt incredibly proud, but it was also one of the earliest moments that I felt a real weight of responsibility. I felt the internal struggle of wanting to build, but also needing to shepherd this company Armon and I were creating. I'm so thankful to all of those early adopters and employees who joined us for that first event. The audience at HashiConf 2015. Just a few short years later, our first internal, full-company offsite was the next major \"whoa\" experience for me. We had more people present than at that first HashiConf! I started this company with Armon, focused on my excitement around the technology, but moments like this taught me how important the people are, too. The people and our shared experiences are what I now look back on most fondly. There are many more similarly impactful moments throughout my history with HashiCorp, and I'm so grateful for all of them. Although it may sometimes seem like some events are bigger than others, I value each experience (even the tough ones) as a necessary step toward achieving each individual milestone. I've worked alongside Armon for almost 15 years (since before HashiCorp!), and worked with Dave [McJannet, HashiCorp CEO] for over 7. We led the company together up until I stepped off the leadership team in 2021. Beyond being coworkers, we’ve grown to be close friends. I continue to trust their leadership and will miss working with them dearly. Armon, Mitchell, and Dave. The controversial worldviews such as multi-cloud that we founded this company on are now mainstream and broadly accepted. The software that I helped start is used industry-wide from hobbyists to professionals at the world's largest companies. And, most recently, the GitHub Octoverse report found that HashiCorp Configuration Language (HCL) has once again emerged as one of the top languages used in open source projects. These are just some of the examples that show the impact, growth, and promising future HashiCorp continues to have in the industry. This is all beyond what I could've hoped for, and I'm leaving proud of the small role I played in making this happen. As I said earlier, nearly my entire adult life has revolved around HashiCorp. This company has made such an impact on not just my life, but on the lives of so many, including our passionate community, our valued customers, our many close ecosystem partners, and our amazing employees. Thanks to all of you for your energy and your trust. Finally, my heartfelt wishes go out to the entire company. I will be cheering you on, grateful to have contributed to the journey of shaping HashiCorp, and excited to see what you will do next. All the best. HashiCorp Sign up for the latest HashiCorp news Email Required Send me news about HashiCorp products, releases, and events. By submitting this form, you acknowledge and agree that HashiCorp will process your personal information in accordance with the Privacy Policy. Sign Up More blog posts like this one December 04 2023Company HashiCorp wins the Palo Alto Networks 2023 Global Technology Partner of the Year award HashiCorp and Palo Alto Networks celebrate our ongoing partnership protecting customers with infrastructure security automation for the second year in a row. November 28 2023Company HashiCorp at re:Invent 2023: A year of collaboration with AWS A recap of HashiCorp infrastructure and security news and developments on AWS from the past year, from self-service provisioning to fighting secrets sprawl and more. November 21 2023Company HashiCorp at AWS re:Invent: Your blueprint for cloud success If you’re attending AWS re:Invent in Las Vegas, Nov. 27 - Dec. 1, visit us for breakout sessions, expert talks, and product demos to learn how to accelerate your adoption of a cloud operating model. Sign up for the HashiCorp newsletter Email Required Send me news about HashiCorp products, releases, and events. By submitting this form, you acknowledge and agree that HashiCorp will process your personal information in accordance with the Privacy Policy. Subscribe Infrastructure Terraform Packer Waypoint Nomad Vagrant Security Vault Boundary Consul International Sites French German Japanese Korean Portuguese Spanish Resources Docs Tutorials Partners Community Events Library Podcast Support Training Company About Press Careers Blog Investors Brand Contact Us System Status Cookie Manager Terms of Use Trust Center Trademark Policy Trade Controls stdin: is not a tty We use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy. Manage PreferencesDismiss",
    "commentLink": "https://news.ycombinator.com/item?id=38647484",
    "commentBody": "Mitchell reflects as he departs HashiCorpHacker NewspastloginMitchell reflects as he departs HashiCorp (hashicorp.com) 555 points by manojlds 12 hours ago| hidepastfavorite139 comments mitchellh 11 hours agoWe&#x27;ve gone full circle! I originally launched Vagrant here on HN in 2010, which was at the top of HN very briefly for the day. Now here I am 14 years later witnessing my departure post in that very same spot. A strange experience! Thanks for the support over the years. A lot of the initial community for the projects I helped start came from here. reply ksec 6 hours agoparentJust the Link. It all started here. https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=1175901Vagrant: A tool for building and distributing virtual development environments (vagrantup.com) 129 points by mitchellh on March 8, 2010hidepastfavorite28 commentsI still remember reading that post on HN. And subsequently Vagrant took off. Cant believe it is nearly 14 years! Thank You Mitchell for everything as I am (still) using Vagrant. First Child is always going to be a hectic job beyond comprehension. Hopefully you will have more free time to play with Zig and may be even Crystal once your child grows a little more. Best of Luck.Edit: I guess HN momentarily went down due to this announcement on front page. reply antupis 43 minutes agorootparentIs there somewhere a list of these big stuff that launched in the HN, eg this and Dropbox https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=8863 . reply jamestimmins 11 hours agoparentprevCongrats on what you&#x27;ve accomplished here. Building an industry-standard company and then carefully planning your exit on your own terms is a huge win.I (selfishly) hope whatever is next is still hacker adjacent, bc your work has been a big inspiration to a lot of us. Best of luck to you! reply ezekg 11 hours agorootparent> I (selfishly) hope whatever is next is still hacker adjacent, bc your work has been a big inspiration to a lot of us. Best of luck to you!You should check out the terminal he&#x27;s been working on codenamed Ghostty [0].[0]: https:&#x2F;&#x2F;mitchellh.com&#x2F;ghostty reply ChatGTP 8 hours agorootparentNote: Ghostty is still a private project. I plan to open source it one day and share it with more people but for now this is a private personal project. If you are really interested in helping with the project, please feel free to email me, but no promises! reply pmyjavec 8 hours agoparentprevI remember getting my first commit into Packer, you review, approved and merged it. It was one of the best days of my (fairly early) coding life because I think your work is amazing and I was so happy to contribute something back.Thanks for all the amazing work thus far! reply bg24 43 minutes agoparentprevBig fan of your products (Vagrant, Terraform, Vault, Consul). Thank you for the amazing contributions to the community. Best wishes. reply ashvardanian 4 hours agoparentprevCongratulations on a new page in your journey! And thank you for documenting the story - you’ve inspired many great developers and founders along the way!Being on almost the opposite end of the software design, I haven’t yet had a good place to apply the tools you’ve built, but I’ve heard many nice things about them from practically everyone, including direct competitors. That says it all. reply elAhmo 1 hour agoparentprevGood luck and thanks for everything! It is very hard to make such a big impact on millions of developers and companies around the world like you did. reply jacquesm 5 hours agoparentprevYou&#x27;re one of the very few technical people that made it big that I continue to look up to. Congratulations on your achievements, and looking forward to whatever is in the pipeline. reply gyre007 2 hours agoparentprevIncredible run! Thanks for all the tools you’ve given us. But more importantly all the relentless passion for automation has been very inspiring! Chapeau, sir! All the best in whats next for you. reply dylanz 9 hours agoparentprevI remember hacking on the same Ruby projects as you, then running into you the same way in the Erlang world. Man, that was almost 15 years ago! You’ve built some awesome stuff along the way… congrats, and keep hackin’!! reply sytse 11 hours agoparentprevCongrats on all your accomplishments Mitchell and looking forward to what you&#x27;ll create next. reply philbert101 11 hours agoparentprevWell deserved Mitchel! Thank you for Vagrant, Packer, Consul, Vault and Terraform, all of which I used back in my DevOps days. reply denysvitali 11 hours agoparentprevThanks for everything! I can&#x27;t wait to see what you&#x27;ll do next!You and Armon have truly shaped the world of infrastructure with your tools and ideas. Although we never met in person (I only had the pleasure to meet Armon so far) - we&#x27;ve interacted a couple of times through some PRs and I really like you as an engineer. It&#x27;s incredible the value that you created over these 11 years - not only on the product side of things (Terraform and Vault are incredible!), but also with all of your Go packages. The amount of time your name pops up in my go.mod files is just impressive :)You and Armon are incredible engineers and I&#x27;m so happy you built something as cool as Hashicorp! All the best with the next chapter of your life! reply rickette 2 hours agoparentprevThanks! You made quite an impact establishing all those projects. reply gigapotential 11 hours agoparentprevCongrats! You&#x27;ve Terraformed the industry! reply jtreminio 11 hours agoparentprevMy first major foss was heavily based around Vagrant (PuPHPet). It was a joy building on top of your tooling to make web engineers lives easier.Thank you for your work, it was great while it lasted! reply conradfr 2 hours agorootparentPuPHPet was very useful, I started many projects with it.I actually still have some projects with a Vagrant VM based on it that I have to move to docker compose or something. reply ezekg 11 hours agorootparentprevSome of my first OSS work was also based on Vagrant (https:&#x2F;&#x2F;github.com&#x2F;ezekg&#x2F;tj). I eventually turned that into a commercial desktop app, built on top of that CLI project. Ultimately, the project didn&#x27;t work out, but it was a big step in my open source and entrepreneurial journey.ty, mitchellh! reply ricardbejarano 2 hours agoparentprevJust wanna say thank you for all your hard work! reply awsanswers 11 hours agoparentprevI built a career following in your vision. Thank you 1000x reply cacois 7 hours agoparentprevFollowed your work since that launch - thanks for everything! Enjoy your family and find some new fun things to explore. reply DiabloD3 2 hours agoparentprevNow you get to work on your terminal fulltime now ;) reply subomi 6 hours agoparentprevCongrats Mitchell, you&#x27;ve been a huge inspiration! reply Laconicus 10 hours agoparentprevBack to hacking Neopets?-Iridium reply leetrout 11 hours agoparentprevCongratulations, Mitchell!Many accomplishments and the ability to change an entire industry. One of a kind!Can&#x27;t wait to see what else you get up to. reply umur 10 hours agoparentprevCongrats Mitchell! Thanks for all your great work, and best of luck for what&#x27;s ahead. reply thecleaner 4 hours agoparentprevMan you&#x27;re a rocket ship. Such an inspiration and congratulations on your success. reply fb03 11 hours agoparentprevThank you for all your hard work, man! reply V-eHGsd_ 10 hours agoparentprevlet&#x27;s go flying reply jackson-mcd 11 hours agoparentprevCongrats! reply s0l1dsnak3123 13 minutes agoprevI got really excited about Consul when I read about it here many years ago. I wrote a ruby library for it (https:&#x2F;&#x2F;github.com&#x2F;WeAreFarmGeek&#x2F;diplomat) which became rather popular, and was used by quite a few large organisations. Hashicorp sent me a care package to thank me - a T-shirt and a card signed by Mitchell himself - I wore that T-Shirt until it was threadbare and I still have the card. Its a small thing on the grand scale of things, but it&#x27;s something I look back on with pride.Well done to the Hashicorp team for getting to where they are now - by building something new and useful, and by fundamentally not being greedy. reply tiffanyh 11 hours agoprevThe sense I always got from the outside (I don&#x27;t know him personally), is that Mitchell is just a really good engineer that wants to build great products.Nothing more.He&#x27;s honest about what he&#x27;s passionate about. Hence why he went from running the company to stepping back to being an IC.I&#x27;ve got a lot of respect for that.Below is his personal website, for those who haven&#x27;t read his posts.https:&#x2F;&#x2F;mitchellh.com reply cgopalan 11 hours agoprevMitchell is the only person I can think of who went through the cycle of tinkerer&#x2F;IC -> founder -> CXO and then back to IC in his own company. Also, his writings on Zig has been tremendously helpful for someone like me who is curious about it. Huge respect from a fellow IC! reply kfrane 10 hours agoparentThe only other person I remember doing that is https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;John_Walker_(programmer)Btw. he published a great read on how they started Autodesk https:&#x2F;&#x2F;www.fourmilab.ch&#x2F;autofile&#x2F;e5&#x2F; reply jacquesm 5 hours agorootparentJohn Walker is also very impressive, definitely rates a mention. And I&#x27;ll forever owe him because of Speak Freely. reply cgopalan 9 hours agorootparentprevThat&#x27;s great to know. Thanks! reply epolanski 9 hours agoparentprevWhere can I find these writings? reply cgopalan 9 hours agorootparentOn his website: https:&#x2F;&#x2F;mitchellh.com&#x2F;zig reply HatchedLake721 10 hours agoparentprevIC? reply hazzamanic 10 hours agorootparentIndividual contributor reply cgopalan 9 hours agorootparentprevSorry, thought it was a fairly familiar acronym. IC = individual contributor. reply hadlock 5 hours agorootparentIt&#x27;s extremely familiar. No need to apologize. reply andrelaszlo 9 hours agorootparentprevIntegrated Circuit. Insatiable Craving. Irrevocable Crust. Indifferent Creator. Ignatios of Constantinople. Instigating Crime. Irritating C**. Individual Contributor. Your guess is as good as mine! reply SOLAR_FIELDS 9 hours agorootparentIt’s standard big corp parlance for a role that has no direct reports. Obvious terminology for anyone who has worked in an org with a moderate size HR or higher, but anyone else has probably never heard of it. I knew immediately that it meant Individual Contributor but I can easily see someone not knowing it if have never worked in BigCorp reply bitwrangler 10 hours agorootparentprevIndependent Consultant (my guess) reply debarshri 11 hours agoprevIt is interesting to see how hashicorp went from an underdog company with vagrant to a company to aspire for with terraform, nomad, consul, vault to something that orgs and community dread, all within a decade.Standards that they have set are still industry wide relevant. But you can see they are on a negative path. reply jen20 11 hours agoparentThis is a weird take. If anything, Nomad, Consul and Vault are on the ascent, as the realities of the CNCF ecosystem set in for people with results to deliver. reply geerlingguy 10 hours agorootparentThe BSL licensing thing has turned off a lot of the more hobbyist&#x2F;tinkerer-centric users who really rocketed HashiCorp&#x27;s products up the ladder in their orgs.It&#x27;s a bit like Elasticsearch, I think—it&#x27;s gotten big enough it won&#x27;t be a quick death, but I don&#x27;t see Hashi&#x27;s products growing&#x2F;dominating as much now as I did a couple years ago.(As an aside, I&#x27;d like to thank Mitchell for his work over the years, Vagrant especially was huge in my infrastructure work, and Terraform was a great companion to Ansible and CloudFormation as we automated more and more stuff.) reply ak217 6 hours agorootparentIt&#x27;s true that the licensing change is a turnoff, but Terraform redefined an industry. What&#x27;s the alternative?(I&#x27;m hoping Mitchell is going to have an answer to that) reply lawik 3 hours agorootparentIt was forked. The OpenTofu variant seems to have backing. Used it yesterday. reply kkapelon 22 minutes agorootparentprevPulumi and CrossplaneOr OpenTF if you want to same thing minus the licensing mess reply fishnchips 10 hours agorootparentprevBased on my limited experience Vault is actually becoming a niche tool. Big cloud players have been offering more limited solutions that are good enough for many if not most use cases. A few years ago as a consultant I&#x27;d unconditionally recommend Vault to my clients. If I were consulting today, I&#x27;d first ask what are they missing from KMS and Secrets Manager. reply fud101 10 hours agorootparentKMS? Secrets manager? Are those offered by a particular vendor or what reply Aeolun 10 hours agorootparentYeah, they’re AWS secrets management services. reply fishnchips 9 hours agorootparentYes, these two are AWS terms, but other clouds have nearly the same services, and often called the same, too. reply imglorp 7 hours agorootparentprevYes but if you want to be multi-cloud and not have to integrate with each cloud&#x27;s secret storage API, you would do it once for Vault and bring it with you to each cloud. reply prmoustache 2 hours agorootparentThe reality is you will never use a common secret storage anyway as usually and even when using multi-cloud tools like terraform you write a lot of cloud vendor specific stuff because they do not share the same provider. So using different secret storage API is not a huge deal and pretty much a moot point.Also my vision of multi-cloud in large org hasn&#x27;t been that a particular product&#x2F;app or team was ever using multiple clouds. My experience is that large orgs like to have multi-cloud support because they grow by acquiring other companies regardless of which cloud vendor they are using so you just want to provide standards and templates for everyone. Obviously said templates will be usually cloud vendor specifics. reply xyzzy123 2 hours agorootparentprevProbably I just work at more dysfunctional places than you, but:If I&#x27;m building product for on prem I&#x27;d prefer to use env vars or k8s secrets and let the customer integrate their preferred secrets manager. Also helpful when you run sales POCs where ease of getting to splash page really matters.Building inside enterprise, I&#x27;ve never actually had to support multiple clouds for the same component but if it ever comes up and I have any sort of choice in the matter I would probably rather template 3 native secrets integrations than deal with the special circle of hell that is enterprise-managed Vault or CyberArk. reply danw1979 1 hour agorootparentprevIt’s not really. For a long time their products were open source, free to use and widely adopted by the tinkering masses. Now the company has shareholders, is chasing the big juicy enterprise customers and has a pricing structure that would make Oracle blush (hyperbole, but still…).It’s a different beast than the company Mitchell founded and that HN knew and loved, that’s all. reply aetimmes 5 hours agorootparentprevNomad is dead in the water.When you talk to Consul support engineers, they assume you&#x27;re using kubernetes, and are confused if you tell them you&#x27;re using Nomad, _a product that their company makes_. reply demizer 1 hour agorootparentNomad will be there when managed k8s gets too expensive. I was looking at it seriously last year for on prem. I chickend put bcz I didn&#x27;t want to be the only person on my team that understood it. We still don&#x27;t have a solution, but nomad will be there for when we are ready to progress, regardless of the license. reply asmor 1 hour agorootparentWhy would managed k8s get more expensive? reply jen20 9 minutes agorootparentComplexity. Monetary is not the only (or even primary) form of expense. reply orthecreedence 10 hours agorootparentprevAnecdotally, I&#x27;ve started transitioning more stuff from Nomad&#x2F;Consul to k8s after Hashicorp&#x27;s licensing changes. I somehow doubt I&#x27;m the only one.Still happily using Vault for now, though. reply josephcsible 10 hours agorootparent> Still happily using Vault for now, though.You should switch to OpenBAO. reply prmoustache 2 hours agoprevI can&#x27;t help to read Mitchell&#x27;s post as a \"I am jumping of the boat before it sinks!\" reply purpleidea 9 hours agoprevCongrats on leaving! The thing I always most respected about Mitchell was that he was actually often coding. So many \"leaders\" these days, are all talk and no action or skill. Maybe they had some git commits once upon a time, but I always felt like Mitchell was always coding throughout. Too bad what happened with the licensing stuff at the company though. reply abkolan 7 hours agoparent> Too bad what happened with the licensing stuff at the company though.OOTL here. What happened? reply purpleidea 6 hours agorootparentThey switched to a proprietary license for all their products. Their CEO said some stuff in an interview. Their stock is plummeting.Check online yourself. Disclaimer, I am the author of a similar tool: https:&#x2F;&#x2F;github.com&#x2F;purpleidea&#x2F;mgmt&#x2F; reply tekknolagi 6 hours agorootparentprevhttps:&#x2F;&#x2F;hackernoon.com&#x2F;open-source-is-dead-understanding-the... reply password4321 11 hours agoprevIs there any one individual or group responsible for HashiCorp&#x27;s switch to BSL?From what I saw Mitchell let go of running the company and now that HashiCorp is not so cool anymore it&#x27;s time to get out. reply voytec 11 hours agoparent> Is there any one individual or group responsible for HashiCorps switch to BSL?IBM, would you choose to trust statements[1] posted from a throwaway account[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38579504 reply jdwithit 1 hour agorootparentClicking \"parent\" on this post a couple times yields some more context. Seems like the claim the throwaway account is making is that IBM pulling Vault into their IBM Cloud offering was the catalyst for relicensing everything. Terraform and other Hashi projects just got caught up in a blanket policy change.Had not heard the IBM angle before and spent a few minutes digging. reply echelon 11 hours agoparentprevThey built a ton of value, were unable to capitalize on it (just like early Docker), and when they tried to capture the value of the thing they built it pissed off the open source ecosystem (and all the profitable companies) built on top.They should have thought about this a long, long time ago.I feel for the smaller companies, but I feel for big companies that come in and plunder because of open license terms. reply asmor 1 hour agorootparentThe thing is, Terraform Cloud could&#x27;ve been good, but it&#x27;s not. And now it prices a K8S cluster the same as a DNS record. (and both too high for glorified object storage)And Vault Enterprise could&#x27;ve been something I can justify to my EM to to ensure it continues to exist (and get namespaces and slow support as a bonus). But they asked way too much.It also doesn&#x27;t help they started prioritizing bugs almost solely based on support contracts, even trivial fixes you submit don&#x27;t get merged for months to years.They built products and thought everything else would just fall into place. And unfortunately it doesn&#x27;t. reply jdwithit 51 minutes agorootparentI&#x27;ve never personally used paid Vault but what you said, that they \"asked way too much\" is the constant refrain I&#x27;ve heard about it. Much like Splunk, it seems like there&#x27;s an extra zero on the quote for smaller use cases.Employees gotta eat, I don&#x27;t begrudge them trying to sell the software. But it seems like they went for the Lamborghini business model when maybe what the market was looking for was Honda. reply fangorn 10 hours agorootparentprevI recently concocted a (conspiracy) theory that relicensing of HC projects is a ploy to get IBM (or some other company happy to rain on IBM&#x27;s parade) to finally make an offer. Mitchell leaving the company features in the theory as well. Basically it seems like since at least 2021 HC leadership and&#x2F;or investors are exploring exit strategies that will bring in the beeeelions. reply aeyes 8 hours agorootparentHashicorp is a public company so the theory doesn&#x27;t make much sense to me. The execs and early investors already made their money. reply tbcj 9 hours agorootparentprevThat tracks. IBM acquiring Hashicorp was a persistent rumor I heard during my last year at IBM (timing was not long after the Red Hat acquisition). reply candiddevmike 8 hours agorootparentprevThey&#x27;ve been for sale since pre-IPO. Cisco almost bought them. reply fishnchips 12 hours agoprevDrop the “Hashi”. Just Corp. It’s cleaner.Jokes aside, it&#x27;s an end of an era. Mitchell has always been one of my role models both as an amazing engineer and a really decent, humble human being. I&#x27;m really looking forward to other amazing things he&#x27;s going to build. reply teeray 11 hours agoparentCorpyCorp could work reply quickthrower2 11 hours agorootparentCorpyMcCorpFace reply ergocoder 1 hour agoparentprevOr change it to ArmonCorp to balance it out. reply candiddevmike 11 hours agoparentprevExVMwareCorp may work. reply __float 8 hours agorootparentWhat do you mean by this? reply leetrout 4 hours agorootparentLots of VMWare folks came over to Hashi over the years. reply danielhlockard 10 hours agoprevHey Mitchell,Best of luck in the future! I&#x27;m a nobody - but I was around in the packer days and wrote a post-processor and terraform provider for our vsphere back in the day. I don&#x27;t think I&#x27;d be where I am now without those experiences. Thanks! reply ludwigvan 8 hours agoprev‘You either die a hero or live long enough to see yourself become the villain…’Great that Mitchell evaded that fate by getting out of the leadership before controversial moves by Hashicorp started occurring. His legacy will be untainted. reply beoberha 10 hours agoprevVagrant is such an awesome tool. It’s been about a decade since I’ve used it in any capacity, but remember being wowed in my first internship when I could just spin up a VM like nothing. Best of luck, Mitchell! reply nanmu42 7 hours agoprevOut of topic but I can&#x27;t help to notice the og:image of the web page is 57 MiB. https:&#x2F;&#x2F;www.datocms-assets.com&#x2F;2885&#x2F;1702507860-mitchell-hc19... reply nkko 2 hours agoparentDatoCMS uses imgix and resizes based on the URLs params. If they haven&#x27;t implemented that in the frontend layout then . reply klysm 7 hours agoparentprevI believe the idiomatic term would be “off topic” :) reply gobins 11 hours agoprevLooking forward to what Mitchell does next. I have always enjoyed reading his code, a fantastic role model. reply glenngillen 11 hours agoparentThis is what he&#x27;s working on atm, including regular updates on progress: https:&#x2F;&#x2F;mitchellh.com&#x2F;ghostty reply m1keil 11 hours agoprevHonestly feels a bit like the end of another great company - Chef (Adam Jacob). Great tooling, built with great engineers but business realities forced it to take some unfortunate turns.I hope Hashicorp will manage to find its stride and not end up as some bullet point in a long \"solutions\" portfolio of some software conglomerate.And thank you Mitchell for all the work. Can&#x27;t wait to see what is coming next from you. reply mdaniel 9 hours agoparentI can&#x27;t speak to the \"great company\" part but Adam (and co&#x27;s) current endeavor may interest you: https:&#x2F;&#x2F;hn.algolia.com&#x2F;?dateRange=pastYear&page=0&prefix=tru...tl;dr = https:&#x2F;&#x2F;github.com&#x2F;systeminit&#x2F;si#readme reply m1keil 9 hours agorootparentYep, I am following them already. Cheers! :) reply lamroger 11 hours agoparentprevForgot about Chef... Good times... reply x86x87 10 hours agoprevMitchell is an amazing human being. Met him in the hacking section of a conference and he was just this down to earth, really laid back dude. He was hanging out with us basically random people and didn&#x27;t once bring up who he was and what he worked on. reply nuker 9 hours agoprevJust a side thought, with all due respect:> The controversial worldviews such as multi-cloud that we founded this company on are now mainstream and broadly accepted.Emm .. no. It is not mainstream. Multi-cloud may be forced choice for some regulated fintech, but no sane project will double the infra codebase just to get from \"lock-in\". I see no other benefits. And double the codebase is quadruple the bugs. This is main Terraform&#x27;s sales pitch, and it has to die. IMHO. reply leriksen 9 hours agoparentThat&#x27;s not what he means when he says multi-cloud.He means his product supports multiple clouds, through terraforms providers, for example.Doing the same infra, across multiple clouds, is a different thing, it&#x27;s not a Hashi thing. reply nuker 8 hours agorootparent> Doing the same infra, across multiple clouds, is a different thing, it&#x27;s not a Hashi thing.Not sure about this. Here is what official site says:\"Provisioning infrastructure across multiple clouds increases fault tolerance, allowing for more graceful recovery from cloud provider outages.\"https:&#x2F;&#x2F;www.terraform.io&#x2F;use-cases&#x2F;multi-cloud-deployment reply kkapelon 14 minutes agorootparentTerraform supports different clouds but with completely different syntax. The marketing on the website just informs you what you can use any cloud you want. The message is against other tools such as cloud formation (AWS only) or GCM (Google only), ARM (Azure only) and so on.To give you an analogy it would be like Firefox saying that they are \"multi-OS\" meaning that you can install Firefox if you have Windows and you can install Firefox if you have Linux. It doesn&#x27;t mean that you must&#x2F;should have Linux and Windows at the same time as a user. reply acdha 9 hours agoparentprevSo I kind of agree that most projects don’t need multi cloud but any enterprise will have a bunch of different things which can be managed by Terraform. There is real value in being able to use the same tool to manage AWS&#x2F;GCP&#x2F;Azure&#x2F;etc., Cloudflare, GitLab, VMware, and even racks of Cisco kit in the basement.If you’re doing it that way, you avoid most of those multicloud drawbacks because you’re using the full native functionality, not building abstraction layers or racking up technical debt by sticking with the lowest common denominator. reply nuker 8 hours agorootparent> There is real value in being able to use the same tool to manage AWS&#x2F;GCP&#x2F;Azure&#x2F;etc., Cloudflare, GitLab, VMwareMy bad, I did not define meaning of multi-cloud. I meant using AWS and GCP at the same time, doing mostly the same things, for redundancy and no lock-in.Still I prefer native IaaC tools, it is simpler, faster and more reliable. I&#x27;m AWS guy, so Cloudformation for all things infra. It irks me when I see Terraform doing AWS infra. I jump in to rewrite it as first matter of business, lol reply acdha 8 hours agorootparentHeh, I have mostly worked on AWS but we stopped using CloudFormation because the experience was so much worse than Terraform with the lengthy deploys, no diffs, significant time delays supporting new AWS features, and deadlocks. They’ve added diffs but turning debugging into a half hour or longer break is still a problem. reply nuker 8 hours agorootparentAnother point - you can jump on AWS support to help you with Cloudformation. They&#x27;ll wash their hands when its Terraform :)And chasing where the var value actually came from in TF drove me mad few times! reply acdha 7 hours agorootparentThey also did that for Cloud Formation in my experience. It’s better now that you can tell CF to forget about resources but I still get people asking for help dealing with a hung stack. replylijok 8 hours agoparentprevMulti-cloud doesn’t mean you duplicate your infra deployment across multiple cloud platforms for redundancy. It means you integrate the best features of each cloud to run your stack. Cloudflare for cdn, GCP for load balancing, aws for compute, snowflake for big data, etc etc… It is absolutely mainstream. reply dikei 7 hours agorootparent> Cloudflare for cdn, GCP for load balancing, aws for computeIsn&#x27;t this cause you to pay twice the already expensive Egress fee ? reply cies 36 minutes agoprevTheir two most prominent project Terraform and Vault (as listed in first position on their won website) were recently forked:https:&#x2F;&#x2F;opentofu.org&#x2F;https:&#x2F;&#x2F;www.techtarget.com&#x2F;searchitoperations&#x2F;news&#x2F;366563095...Their other offerings have very strong alternatives.I use Terraform and will certainly consider going with OpenTofu in the next upgrade. reply gtirloni 9 hours agoprevHashicorp was always a role model for open source companies. Congratulations for that era. You achieved something impressive. reply BobBagwill 10 hours agoprevEnjoy your family. Have fun. Beee goood! reply thrillgore 8 hours agoprevCongratulations on your exit. OpenTofu will continue the work you started. reply TheRealPomax 9 hours agoprevTitle should probably be \"Mitchell Hashimoto reflects as he departs HashiCorp\". Lots of folks have no idea who \"Mitchell\" is without that key extra bit. reply nodesocket 11 hours agoprevI use HashiCorp software nearly daily and think Terraform was the biggest eureka! moments for me. I&#x27;ve held HashiCorp since the IPO... Hasn&#x27;t gone as planned so far, but holding on. Honestly, I expect an acquisition like Slack (just my hunch). reply thekevinwang 8 hours agoprevLegend reply mugivarra69 11 hours agoprevterraform move killed it reply revskill 11 hours agoprevNo mention about what&#x27;s next ? Hm, why. reply fb03 11 hours agoparentMy suspicion is that he simply might spend some time with his newborn and enjoy life a bit after such a wild streak. He has more than enough financial resources to do that comfortably.He has stressed twice in the post that most of his life revolved around that company - Maybe it&#x27;s time for that not to be anymore - simply put. reply jethronethro 11 hours agoparentprevDoubt there&#x27;s anything sinister or shady about that. Maybe he doesn&#x27;t have any plans. Or maybe he just doesn&#x27;t feel like sharing those plans or doesn&#x27;t feel obliged to share them. reply revskill 11 hours agorootparentMy guess (could be wrong): He no longer enjoy writing Go. reply unethical_ban 11 hours agoparentprevKids are a thing that can be next. reply pphysch 11 hours agoparentprevHe&#x27;s been working hard on a really cool modern terminal emulator the last couple years, among other things I&#x27;m sure.https:&#x2F;&#x2F;mitchellh.com&#x2F;ghostty reply brcmthrowaway 11 hours agoparentprevSomething AI related no doubt reply thecleaner 4 hours agoprevCan we all take a moment to reflect how great is HN ? People have launched their software products here and gone on to become wildly successful or industry standards. I think our field is lucky we have a space where we can all enjoy people&#x27;s successes, get inspired, be sourly or sarcastic (lol), have meaningful discussions and its all done with Lisp, HTML tables, an unstyled \"add comment\" button (please don&#x27;t change) and a bunch of bare-metal servers which sometimes catch fire along with the discussions. Its quite wonderful. reply 0xbadcafebee 11 hours agoprevnext [7 more] [flagged] dewey 11 hours agoparent> But I also hope you never make another OSS tool again.What a sad thing to say. If you picked a free open source tool that was not right for your use case maybe you should blame yourself. reply leetrout 11 hours agoparentprevThis negativity is unnecessary.Mitchell has had little to do with the issues Terraform faced and how things went. That responsibility belongs to others in eng leadership. reply fishnchips 10 hours agoparentprevWhile I&#x27;m not privy to any insider knowledge, from the outside it seems highly unlikely that Mitchell had anything to do with the trajectory of Terraform for the last few years. Also, it&#x27;s possible that he wouldn&#x27;t be able to defend himself against such arguments, having potentially signed some paperwork with his former company. reply _vertigo 10 hours agoparentprevI think you should spend some time reflecting on this take and whether it makes sense for a (presumably competent) adult to have posted it. reply hathrowaway12 10 hours agoparentprevI hope you look back on this comment someday and are ashamed of it. It&#x27;s unnecessary and unnecessarily harsh. reply paulryanrogers 10 hours agoparentprevCould you link to a few examples of the most painful PR denials?Isn&#x27;t Terraform still considered one of the better cross-cloud automation tools? reply scoot 8 hours agoprev> the GitHub Octoverse report found that HashiCorp Configuration Language (HCL) has once again emerged as one of the top languages used in open source projects\"Hashicorp Configuration Language (HCL) with a 56,1% increase in popularity is the fastest-growing language according to GitHub\"Fastest growing and \"top\" are not the same thing, but I have to assume that you know that @mitchellh. Embarrassed for you.https:&#x2F;&#x2F;xkcd.com&#x2F;1102&#x2F; reply raphlinus 4 hours agoparentFrom the 2023 Octoverse report[1], \"In 2023, Shell and Hashicorp Configuration Language (HCL) once again emerged as top languages across open source projects, indicating that operations and IaC work are gaining prominence in the open source space.\" The accompanying chart shows it as #11, ranking above Dart, Kotlin, and Ruby.I think this is a case where the citation bears out the claim, and there&#x27;s no need for Mitchell to feel embarrassment.[1]: https:&#x2F;&#x2F;github.blog&#x2F;2023-11-08-the-state-of-open-source-and-... reply issafram 8 hours agoprev [–] Guy left and Terraform still has horrible logging replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mitchell Hashimoto, co-founder of HashiCorp, has announced his departure from the company after 11 years in a heartfelt letter expressing gratitude for the impact HashiCorp has made.",
      "Hashimoto shares his decision to leave and seek new challenges beyond cloud automation and infrastructure tooling.",
      "He highlights HashiCorp's growth and achievements, including widespread software adoption and recognition in open source projects, and wishes the company success in the future."
    ],
    "commentSummary": [
      "Mitchell Hashimoto, co-founder of HashiCorp, is leaving the company after 14 years, garnering congratulations and gratitude from the community for his contributions to the software development industry.",
      "Discussions revolve around HashiCorp's projects, recent licensing changes, and the concept of multi-cloud deployment with Terraform.",
      "Speculation is rife about Mitchell's future endeavors and potential alternatives to HashiCorp's projects, along with the importance of operations and Infrastructure as Code (IaC) work in the open-source community."
    ],
    "points": 555,
    "commentCount": 139,
    "retryCount": 0,
    "time": 1702589237
  },
  {
    "id": 38644608,
    "title": "Mozilla Expands Extension Support for Firefox on Android with Over 450 New Add-Ons",
    "originLink": "https://blog.mozilla.org/en/mozilla/new-extensions-youll-love-now-available-on-firefox-for-android/",
    "originBody": "Mozilla New extensions you’ll love now available on Firefox for Android December 14, 2023 Jenifer Boscacci Today, Mozilla announced more than 450 new extensions (software that adds new features or functionality to the browser) to users on Firefox for Android at Mozilla’s Addons.mozilla.org (AMO) Android page. This milestone marks the launch of a new open extension ecosystem on mobile where developers are now free to create and publish extensions and users can easily access and install them on Firefox for Android. “Extensions were first created as a way for people to customize their own internet experience, from artists designing themes to developers who wanted to make extensions to improve people’s web experience,” said Vicky Chin, Vice President of Engineering at Firefox. “We’re thrilled to bring this experience to Firefox for Android, where we’re the only major Android browser to support an open extension ecosystem. In the coming months, we plan to enable more extensions for people to choose from and customize their own mobile internet experience.” Our journey to an open extension ecosystem to benefit all Browser extensions have become an essential part of everyone’s daily internet experience. Nearly half of all Firefox desktop users have installed an extension to customize their online experience. Extensions provide a wide array of powerful features — from privacy tools like anti-trackers and ad blockers, to productivity tools, tab managers, translators and so much more. Built on Mozilla’s mission for an open and accessible internet for all, Firefox works with an independent community of developers to offer extensions for people who want more personal agency out of their online experience. On desktop, there are thousands of extensions to help you become a better writer, land a job or clean up a chaotic desktop. While the new Android ecosystem will take time to develop the robust diversity of content that desktop extensions offer, quite a few major desktop extensions are already available on Firefox for Android, such as privacy focused content blockers uBlock Origin and Ghostery, anti-tracking gem Privacy Badger and color customizer Dark Reader. In August, we announced that we had completed building the infrastructure needed to support an open extension ecosystem on Firefox for Android. We were ready for the next chapter: adding extensions. Since then, we’ve been working with developers to test and make hundreds of extensions compatible on mobile. So, are you ready to customize your mobile browsing experience to make it faster, safer or simply more fun? Look no further with today’s release of extensions on Firefox for Android. Extensions to help while you’re on the go, just in time for the holidays We depend on our mobile devices for many things — quick informational searches, reading articles, listening to music, looking for recipes (like cookies for the annual cookie holiday exchange). So, we understand the value of having experiences that are simple, predictable and offer the time to focus. Here are some extensions available today to help achieve that flow. Midnight Lizard – Read easier We look at our mobile devices in so many different environments. Whether it’s outside with the sun or inside a dimly lit room, our eyes work hard to adjust. Midnight Lizard is one of those extensions your eyes will feel the difference and thank you. It can change the colors of the page, increase or decrease the brightness and contrast. Add a blue light filter, screen shader and of course, the ever-popular night mode. Midnight Lizard will keep your eyes in good shape! Dark Background and Light Text – Keep it simple This extension is well-loved by thousands of users for its simplicity. It’s helpful for folks who prefer to work in dark mode, or those with low vision where reading dark text on a white background is challenging. You’re free to customize it so that all web pages are rendered in this elegant way, or just select pages. Worldwide Radio – Get into the groove Access more than 50,000 radio stations from all over the world right from your Firefox for Android browser. In the mood for a bit of Brazilian Samba? How about some traditional Indian Hindustani? Techno beats from Berlin? The world’s music and real time talk radio is literally at your fingertips. A big thank you to our developer community We would like to thank all the developers who worked with us to make their extensions compatible for this launch of the open extension ecosystem on Firefox for Android. Hundreds attended our webinars and brought incredible creative energy to this project. “The opportunity for innovation is vast,” said Giorgio Natili, Firefox Director of Engineering. “It’s thrilling to see extension developers embrace this moment and create novel browsing experiences and features for Firefox for Android users. People don’t have to browse the mobile web in a strictly singular way anymore. With extensions, you’re free to change the way Firefox for Android looks and behaves. It’s only going to get better as more developers innovate within this exciting new space.” As more developers create mobile optimized content, you can expect a wave of new Firefox for Android extensions to emerge in the coming months. In the meantime, download the latest Firefox for Android and shape your own internet experience with Android extensions. More than 450 new extensions now available for Firefox on Android users Download Firefox for Android",
    "commentLink": "https://news.ycombinator.com/item?id=38644608",
    "commentBody": "Mozilla expands extension support for Firefox for AndroidHacker NewspastloginMozilla expands extension support for Firefox for Android (blog.mozilla.org) 460 points by rebelwebmaster 16 hours ago| hidepastfavorite177 comments mod50ack 15 hours agoThis is good. Now, all extensions marked by the developer as being compatible with Android are shown on AMO. (If you toggle to Desktop mode, you can actually install any other extension on AMO, too.)The baffling thing is why this took so damn long. FF for Android supported add-ons from the beginning. That&#x27;s the best thing about Firefox for Android! They decided to rewrite the UI in 2020, and there were fair reasons to do that. Obviously this required some reimplementation time for extension support.But they then launched the rewrite of FF for Android with extension support... but hidden. Only a small set of recommended extensions were enabled, and a few were drip-fed over time (that is, added to the list). Thankfully, this included the single most important extension, uBlock Origin, from the very beginning. (The lack of uBO why Chrome for Android is borderline unusable for me!)But from almost the very beginning, we&#x27;ve also had the ability to activate custom extension collections in Nightly (and in Fennec F-Droid, which is a rebuild of stable Firefox). The vast majority of extensions worked fine for... well, years now.So why in the world was this delayed the whole time? reply st3fan 9 hours agoparentI managed the team that did a lot of the integration of web extension support in Fenix, the new Firefox for Android. We were all on the brink of burnout. There was too much work. Unrealistic deadlines. And high expectations. So we decided to only support a limited set of APIs tuned for the most popular web extension. Which were basically all ad blockers if I remember correctly.Proud of the team to have finally gotten to this point. Miss you all. reply st3fan 9 hours agorootparentOther people may remember this differently. Some things are a bit of a blur. My brain selectively blocks some of this - It was basically an exhausting two&#x2F;three year crunch to rewrite Fennec as a modern Android app. reply asadotzler 9 hours agorootparentThat&#x27;s how I remember it. I was on desktop projects but following closely and the rewrite was a death march and people working on it moved or left along the way too.We should have listened to Hyatt back in 2003-ish when he basically said of XUL, \"it&#x27;s never gonna be great on *nix or Mac but it&#x27;s good enough on Windows.\" Because of solid desktop horsepower growth over the 2000s, we were able to make XUL go for the three desktop platforms pretty well but it should never have gone to mobile and replacing it with a native front end was absolutely the right thing to do, despite the pain. reply glandium 3 hours agorootparentXUL is not the whole story, though. There was an Android native Firefox before the current Fenix and after the XUL Fennec, and it had Web Extensions, IIRC. From that perspective, that people complained is understandable. reply KennyBlanken 6 hours agorootparentprevIt sounds like they intentionally under-provisioned you all to draw it out, which isn&#x27;t surprising given their biggest source of funding is Google, and the last thing Google wants is ad&#x2F;tracker blocking, privacy extensions, and, well, a major reason for people to set their default browser to &#x27;not chrome&#x27;.Lord knows there&#x27;s enough money floating around that place.What a shame - but thank you. Hopefully plugins come to Firefox for iOS some time. reply figmert 5 hours agorootparentConsidering uBlock Origin was the first extension to work (and, from what I understand, they worked extra hard to ensure it works), I doubt that&#x27;s the reason. reply 2Gkashmiri 5 hours agorootparentprevI am desperately looking for an extension that takes over godawful js video players and gives something standard. Some have click to pause, some forward backward, all don&#x27;t have volume control. reply wolverine876 11 hours agoparentprev> The baffling thing is why this took so damn long.I&#x27;m surprised it&#x27;s baffling in a community of developers and other IT professionals.It&#x27;s not baffling to me that two significantly (wholly?) different applications on different platforms and form factors would require quite a bit of work to both be generally compatible with the same third-party software via the same API - and all while maintaining the same compatibility with another application, made by another company, completely outside Mozilla&#x27;s control.And it needs to work reliably enough to release to a world of developers - of every skill level, motivation, writing every kind of software (within the domain of browser add-ons) - with confidence that it will work for them and users.And you need a way to maintain all that over the long term.I&#x27;m impressed Mozilla! reply dopa42365 10 hours agorootparentFirefox android extension support went from \"all\" to like \"5 chosen ones, but we&#x27;ll enable all of them very soon\" in mid 2019. How and why those were handpicked, who knows, clearly extensions weren&#x27;t enabled by supported functions at the time. In the usual mozilla fashion that \"very soon\" turned out to be multiple years. reply Groxx 8 hours agorootparentImportantly, during this entire multi-year gap, nearly all of them worked just fine but it was gated behind an AMO account for... I don&#x27;t know what reason.If it was just an experience issue because like 5% failed weirdly or had bad performance but they couldn&#x27;t validate them all: that&#x27;s basically fine! Hide it behind an about:config flag! The AMO requirement was a privacy-invading piece of nonsense that had no business existing. reply KennyBlanken 5 hours agorootparentPrivacy-invading requirements that delay implementation of plugins that would enable plugins providing better privacy and ad-blocking functionality?Hmmmmmmm, now what major Mozilla sugar daddy would be interested in that &#x2F;s reply wolverine876 4 hours agorootparentprevAgain, it&#x27;s easy to imagine answers to these questions and to grasp what is happening. Instead people choose to play the sport of tearing things down, no matter the effect on the people involved, Mozilla, the open web, etc. reply LeoNatan25 11 hours agorootparentprevDid you read past that sentence you quoted? reply charcircuit 10 hours agorootparentprevThe extensions worked just fine on Android before an update a few years ago broke them. I can finally use extensions that I&#x27;ve been missing for years. reply mvdtnz 14 hours agoparentprevFor those who are wondering, I _think_ AMO is supposed to mean \"addons.mozilla.org\" although neither the author of the article nor this comment define the acronym. reply vallode 12 hours agorootparentI was also somehow aware of this acronym. Turns out the about page of Mozilla&#x27;s add-ons page also uses it[1], so it&#x27;s \"official\" so to speak.[1]: https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;about reply jraph 13 hours agorootparentprevYes, indeed, AMO means addons.mozilla.org reply zerocrates 12 hours agorootparentprevThe article currently does define it, but maybe that was changed. reply gruez 14 hours agoparentprevAFAIK it was because firefox for android was on a slightly different codebase than desktop firefox, and thus had supported a different set of webextension apis. The user contexts api (container tabs) was missing entirely, for instance. reply dblohm7 14 hours agorootparent(I used to work on this stuff)It was more complicated than that. Yes, GeckoView needed a separate WebExtension implementation, but that work was pretty much at parity with Fennec (the previous Firefox for Android that supported more extensions) when I left in 2021.It was a product management decision that held off on more complete WebExtension parity with desktop, as well as any artificial limits as to which extensions were supported in release. reply Zak 14 hours agorootparentCan you elaborate on the product management motivations?It seems to me projects like Iceraven demonstrated years ago that a great many extensions were usable without any changes. Why not just slap a \"here there be dragons\" warning on untested extensions and let users have at it?To be clear, I&#x27;m not asking you to justify decisions you didn&#x27;t make, just to provide some visibility into the process if you can. Mozilla was pretty opaque about it. reply dblohm7 10 hours agorootparent> Why not just slap a \"here there be dragons\" warning on untested extensions and let users have at it?We essentially had that as part of pre-release builds. Same with about:config.The argument we&#x27;d then hear from people is, \"but I want the stable channel with the &#x27;here be dragons&#x27;\" stuff. The reality is, though, that the \"here be dragons\" stuff probably affects stability more than running beta does anyway; people who shat on us for that wanted to have their cake and eat it too, and it just doesn&#x27;t work that way. reply Zak 8 hours agorootparentMy follow-up question then is why treat mobile significantly differently from desktop? Stable desktop Firefox has about:config and access to horribly broken extensions. Perhaps some would argue that it should not. reply sgift 8 hours agorootparentI cannot speak for Mozilla, but just from my gut feeling: That probably got grandfathered in and since people are used to it for a \"long time\" no one would change it. But if Firefox was rereleased today I wouldn&#x27;t bet on it being there. Expectations have changed. reply gitaarik 1 hour agorootparentWhat? You are saying that the `about:config` feature only exists in Firefox for historical and backward compatible reasons? And Firefox rather actually doesn&#x27;t want to provide people the ability to easily override advanced configs? I think this feature is one of the things that sets Firefox apart from other browsers, that you have more control if you want it. And I am saddened it&#x27;s also never been implementes in Firefox Android. reply Zak 7 hours agorootparentprevFirefox for Android was around for 8 or 9 years with full support for extensions and access to about:config. I think that&#x27;s a long time in this context. reply toyg 13 hours agorootparentprevProbably fear that bad extensions would tank performance, tarnishing the reputation of the overall browser. Now that such reputation is more or less established (i.e. people use FF on Android without big problems, it&#x27;s not considered particularly slow etc), they can dare a bit more. reply gitaarik 1 hour agorootparentYou can fix that in other ways that doesn&#x27;t block access to all extensions. Like yeah even just a checkbox to enable experimental extension support like \"[] I understand Firefox can become slower from unsupported extensions I install\". reply Vinnl 12 hours agorootparentprevI believe it&#x27;s that, and that with extensions living in their own processes, Android can at any moment decide to kill it (like it can do with any mobile app). With the changes required for Manifest V3, extensions are able to deal with that gracefully, rather than causing a deluge of bug reports. reply dblohm7 10 hours agorootparentThat&#x27;s part of it, for sure... at the time I was still there, Gecko&#x27;s extension process did not have the capability of recovering from termination. But that just meant that we couldn&#x27;t run them in a child process, not that we couldn&#x27;t run them at all. Of course, then you have security considerations, which no doubt could have factored into the product decision. reply gitaarik 1 hour agorootparentIt&#x27;s common practice these days in Android apps to request the user to turn allow the app to run on the background, opening the necessary Android settings page if you want to grant the app access to this. If an addon needs that feature, the app could request this for the addon at moment of installation. reply cubefox 12 hours agorootparentprevThat fear was obviously unjustified. Extensions that would tank performance would have gotten bad user ratings. reply dblohm7 10 hours agorootparentThat&#x27;s assuming that you know that the extension is the cause. A bad extension doesn&#x27;t always kill perf as soon as you install it. reply rvba 9 hours agorootparentprevProbably wanted help chrome keep its market share while keeping firefox insignificant.Also I wonder where do those decision makers work now. reply athrowaway3z 12 hours agorootparentprevThis is speculation based only on press releases but:- Google pays Mozilla more than 400m per year.- Its in Google&#x27;s interests to not have good Firefox add-ons. (For both Ads and Chrome&#x27;s market share).Google&#x27;s negotiator could easily added some incentive for Mozilla&#x27;s management to set the focus somewhere else.In fact, given what Google&#x27;s team is likely earning, they wouldn&#x27;t be doing a good job if Firefox&#x27;s mobile strategy wasn&#x27;t discussed before signing such deals. reply mod50ack 12 hours agorootparentFirefox for Android had add-ons before, and even during the past few years, they&#x27;re fully supported the collection of recommended add-ons, including uBlock Origin from day one. So I don&#x27;t see how it could be about preventing ad blocking. reply pcwalton 11 hours agorootparentprevThe idea that Google has some secret underhanded deal with Mozilla to sabotage Firefox comes up here repeatedly and makes no sense. If Google wanted to prevent ad blocking on Android it would be much simpler to just ban ad blockers from the Play Store outright.There is a much simpler potential explanation for such a product management decision. Suppose Mozilla determines that 90% (made-up number) of users want addons because they want uBlock Origin. It then seems sensible to prioritize that addon and not others when determining how to spend limited engineering resources. Reasonable people can of course disagree with that decision, but there&#x27;s no need to bring conspiracies into it.(NB: Even though I worked at Mozilla I have zero insight into this particular issue; it&#x27;s entirely speculation.) reply asadotzler 11 hours agorootparentprevThis is just silly. Firefox on Android has had uBlock Origin, the world&#x27;s most effective ad blocker, since day one. But sure, go invent conspiracies rather than do a little research. replysedatk 13 hours agoparentprevnext [7 more] [flagged] jraph 13 hours agorootparent> Few of them prevents me from switching to Firefox(I assume you meant \"a few\"). Which ones, by curiosity? reply sedatk 12 hours agorootparentYes, I meant a few.The biggest blocker for me is font kerning on canvas elements being broken. That causes Google Docs to render terribly which makes it practically unusable. https:&#x2F;&#x2F;bugzilla.mozilla.org&#x2F;show_bug.cgi?id=1445596Another one is favicons not being stored&#x2F;synced across browsers. This causes me to have a bookmark toolbar with entirely the same default icon without any text. (I prefer them as icons, fits more stuff there). I don&#x27;t have this problem with Chromium based browsers. https:&#x2F;&#x2F;bugzilla.mozilla.org&#x2F;show_bug.cgi?id=428378 reply bad_user 13 hours agorootparentprevThat article actually shows that Mozilla spends most on software development.It does have a lot of administrative overhead that it could do without, instead of laying off engineers, like they did. But the article itself is trash, due to complaining of $892,000 worth of expenses that seem dubious, in an organization that makes 500 million per year, which paints a certain kind of picture that&#x27;s disingenuous. reply sedatk 12 hours agorootparentThe article may be trash as you said, but Firefox doesn&#x27;t disclose how much they spend on developing Firefox web browser. Everything is lumped under \"software development\", and that&#x27;s only half of their annual budget, Firefox certainly doesn&#x27;t get the most of the budget.If they spend most of that $200 million on Firefox annually and we have abhorrent text rendering on canvas elements for YEARS, something&#x27;s seriously broken there. reply bad_user 2 hours agorootparentHow much are Google or Apple spending on developing Chrome and Safari?And note that they also develop the operating systems (Android, ChromeOS, iOS, macOS), so have an integration advantage from it. reply 127361 13 hours agorootparentprevAnd a chunk of Mozilla&#x27;s funding going towards social justice related projects, as much of the organization has been co-opted by social justice types.As was stated in the article[1], close to half a million dollars was spent on a social-justice related organization, the Mackenzie Mack Group.“[Mckensie Mack Group] is a change management firm redefining innovation in the white-dominant change management industry.”In the article it also says \" While The Lunduke Journal does not like to delve too deeply into the Political Woods (tm), it should be questioned why so much money — possibly millions of dollars donated by individuals who thought they were supporting a web browser — is being funneled into highly political organizations that seem to have no involvement with the World Wide Web, Web Browsers, or any related standards. \"1. https:&#x2F;&#x2F;lunduke.locals.com&#x2F;post&#x2F;4387539&#x2F;firefox-money-invest... \" reply akdor1154 11 hours agoparentprevMy crank unevidenced theory is that1. they wanted an Apple-level of verified review process for AMO, because the Chrome store and even Android app store have problems with malicious content.2. This costs money.3. They didn&#x27;t want to open a free for all because they didn&#x27;t know exactly how to go about solving 2. yet, and if they introduced some payment system then it would be easier to do from a clean slate, without an AMO full of existing extensions to somehow grandfather through.As said before, this is fully unfounded and probably unfair speculation. I like it more than the &#x27;google conspiracy against adblockers&#x27; though because Mozilla&#x27;s motivations in this case are quite reasonable and can be taken in good faith. Keeping credit card skimmers out of AMO at the cost of restricting access to &#x27;Firefox Pro&#x27;&#x2F;&#x27;AMO Pro&#x27;&#x2F;author-pays would honestly be quite a good thing for Mozilla to consider imo.In any case it&#x27;s great to see them allowing things now! reply Night_Thastus 14 hours agoprevI&#x27;m so grateful that FF for Android exists with addon support. Using it with uBlock is the only way to make mobile not an awful experience for me. reply dredmorbius 8 hours agoparentThere are other mobile browsers which incorporate adblocking directly (though not extensions generally).The Einkbro browser, optimised for e-ink devices (as the name suggests) is one. I believe Brave does as well.As much as I&#x27;m a fan of Firefox (using it now on desktop), on my mobile e-ink device, Einkbro&#x27;s optimisations make for a vastly superior browsing experience. reply esperent 5 hours agorootparentThere&#x27;s also Kiwi browser which directly supports most Chrome addons and has for many years. I can never understand why it doesn&#x27;t get more attention. reply mike31fr 10 hours agoparentprevHave you never heard of NextDNS? reply Groxx 8 hours agorootparentDNS blocking has some nice qualities (simple! efficient! app agnostic!) but it is absolutely not a replacement for something that can inspect and modify the page itself. That has TONS of additional usecases beyond \"replace ad image with blank space\". reply WirelessGigabit 10 hours agorootparentprevUnfortunately with websites hosting their ads on their own pages (like for example IMDB) DNS filtering is becoming less effective. reply Night_Thastus 8 hours agorootparentprevI use PiHole, but DNS options don&#x27;t catch everything. uBlock catches a LOT that it doesn&#x27;t, and allows for lots of custom stuff I can&#x27;t do otherwise. reply replete 10 hours agoprevExcellent, finally in stable. Have been using Nightly and more recently Mull specifically for extensions like &#x27;I still don&#x27;t care about cookies&#x27;, &#x27;ublock origin&#x27; and &#x27;dark reader&#x27; which make the web on mobile at all practical.Firefox browser share is like 2-3%. Please consider using it, the internet will be a lot shittier without Firefox as an option, and it is the best option for privacy and ad-blocking. reply aqfamnzc 5 hours agoparentHeads up re: IDCAC: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36233068 reply SushiHippie 4 hours agorootparentThat&#x27;s why GP is using \"I *still* don&#x27;t care about cookies\" ;) https:&#x2F;&#x2F;github.com&#x2F;OhMyGuus&#x2F;I-Still-Dont-Care-About-Cookies reply kungfufrog 13 hours agoprevThis was a clincher for me that made me switch from Chrome&#x2F;Chromium on my Pixel. Previously, I was using Kiwi Browser because it supported Chrome extensions however while it works it has a lot of annoying quirks. I just couldn&#x27;t stomach the experience of browsing the web without an ad blocker though. Now Firefox and UBlock work on Android, Firefox has quickly become my preferred browser. Still using Chrome on desktop though for now.. maybe that&#x27;ll change too! reply Vinnl 12 hours agoparentGive it a shot! It can import your bookmarks, passwords, etc. from Chrome, and it&#x27;s great to be able to quickly send a tab from desktop to mobile, or vice versa. reply cubefox 12 hours agoprevThanks to Firefox extensions I get an automatic dark mode on HN, and almost any other website, as soon as my device is switched to dark mode. Normally this would have to be supported explicitly in the website CSS. reply pentagrama 12 hours agoprevGreat! Now I can finally install an extension to autodelete cookies for certain domains. This feature is available on stock Firefox Desktop but not Mobile. reply ixmerof 11 hours agoparentCan you please link the extension you use for that purpose? reply pentagrama 7 hours agorootparentI was testing some, and this one works like a charm! https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;addon&#x2F;forget_me_not... reply insin 6 hours agoprevOne of the newly-available extensions is mine for Hacker News [0] - it highlights new comments when you revisit an item and somewhat improves some of the UX on mobile:[0] https:&#x2F;&#x2F;addons.mozilla.org&#x2F;firefox&#x2F;addon&#x2F;hn-comments-owl&#x2F; reply eco 5 hours agoparentAmusingly I returned to this comments section and was scrolling down looking at the time stamps for new comments and this was the first new comment I hit. Thanks. It&#x27;s clearly going to come in handy. reply briffle 15 hours agoprevI wish they would push hard for proper support in IOS for running extensions (or their own engine, etc) reply ivanjermakov 15 hours agoparentI think it&#x27;s still against Apple&#x27;s TOS, stating that every web browser must be based on WebKit. reply temp0826 15 hours agorootparentWhy can Kagi&#x27;s Orion browser support extensions on iOS? I recently switched from Firefox because lack of uBO caused huge beef for me and have zero regrets. I&#x27;m not paying for Kagi&#x27;s search service (happy enough with DDG for the majority of my searches), but I can really appreciate their business model and mission (they seem genuine afaict). reply mod50ack 14 hours agorootparentOrion is built on webkit, both on desktop and mobile. It&#x27;s possible to build WebExt support into a WebKit browser, as they&#x27;ve done.While I definitely prefer FF and Android, I can support the notion of Mozilla integrating extension support into WebKit on their iOS version of FF. But it would take a lot of effort to do that, and Firefox for iOS is ultimately just totally separate from any other Firefox (whereas Android and Desktop Firefox share the same innards). reply jwells89 13 hours agorootparentIt would be interesting to see how Mozilla approaches implementing Gecko on iOS, with how the engine stripped support for embedding years ago (prior to which they could’ve used an approach similar to that seen in Camino[0]).I guess they could take the approach of drawing the whole screen themselves but that’s going to make Gecko-based Firefox for iOS feel noticeably worse than the current WebKit&#x2F;UIKit version in terms of responsiveness and such and might require some legwork to properly support VRR on 120hz iPhones (which is critical for battery life on those models).[0]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Camino_(web_browser) reply saagarjha 11 hours agorootparentThis would currently be against the App Store guidelines, which do not permit the use of a third party browser engine. Also, Firefox has ProMotion support. reply SSLy 11 hours agorootparentLuckily that provision is going to be illegal under DMA within half a year. reply mook 8 hours agorootparentprevI thought Firefox on Android already did something similar to embedding, via GeckoView? The issue with embedding seemed to be that Mozilla really hated having complaints that their APIs were very unstable and kept breaking. In contrast, the IE embedding solid APIs were more stable (because they were also more limited, as I understand it). reply temp0826 14 hours agorootparentprevInteresting, I always thought the holdup was that third party things used in-app were expressly forbidden because they&#x27;re not vetted by the app store onboarding process (in addition to the requirement of using webkit). Didn&#x27;t occur to me that webkit could be extended to support webext either. reply lxgr 13 hours agorootparentYeah, Apple likes to say that (and reject apps for violating that rule!), but then there&#x27;s also things like Linux x86 userspace emulators in the app store that can run unmodified ELF binaries downloaded via curl from any random website...At this point it&#x27;s just a polite fiction, maintained jointly by Apple and app developers, that allows Apple to maintain a somewhat straight face when saying things like \"you can&#x27;t download third-party code at all\" or \"all code extending app functionality must be downloaded through our designated mechanism\".iSH is one such app, this blog post is very interesting: https:&#x2F;&#x2F;ish.app&#x2F;blog&#x2F;default-repository-updateGiven the current regulatory scrutiny of their app store, I believe they just don&#x27;t want to open yet another can of worms by rejecting \"browsers\" (which are really WebKit wrappers) for injecting third-party JavaScript into all web pages displayed within them, even though by their own rules, they arguably totally should. reply bad_user 13 hours agorootparentNot sure what “userspace emulators” you&#x27;re speaking of, but the apps I tried, for running a programming language (for education purposes) are rubbish due to limitations (you can only interpret, you can&#x27;t compile). And the restrictions are mostly in place; otherwise, for example, there would be apps that allowed you to download torrents, or do other forbidden activities.Even if what you&#x27;re saying is true, businesses that can&#x27;t afford a ban from the App Store, can&#x27;t afford to bend the rules. If Mozilla developed Firefox for iOS, with its engine, and Apple banned it from the App Store, the consequence would be millions of dollars going down the drain. And Mozilla would let their current users down, too, since the current Firefox for iOS is somewhat useful. reply lxgr 12 hours agorootparentI&#x27;ve linked one (iSH) in my comment. It really does run most completely unmodified x86 binaries for Linux, including CPython and Java.aShell [1] is very similar. It takes another approach – it compiles POSIX C source code to WASM and runs that using iOS&#x27;s JIT-enabled web engine, which gives it much better performance than x86 software emulation. There&#x27;s another one that uses lldb to interpret LLVM IR. In other words, if Apple doesn&#x27;t want that type of app, they sure have been explicitly enabling the use case for a long time now.> And the restrictions are mostly in place; otherwise, for example, there would be apps that allowed you to download torrents, or do other forbidden activities.App store reviews don&#x27;t exist to \"prevent forbidden activities\" in the legal sense; they are there to maintain their walled garden ecosystem financially, as well as protect their platform and products from reputational or legal harm.The issue of legality and passing the App Store review process are largely orthogonal: Just like you can already do plenty of illegal things using stock iOS (e.g. writing threatening emails, downloading copyrighted material using WebTorrent etc.), you can do infinitely many legal things using Turing-complete computing as enabled by first and third party apps on iOS.Now if you start offering an app that features a big button labeled \"click here to dynamically load software facilitating copyright infringement\", and Apple distributes it in their App Store after having reviewed it, that could get them into a tricky situation; offering a full-featured browser or OS emulator very likely doesn&#x27;t, given that Google has been allowing these types of apps in their Play Store for more than a decade now.[1] https:&#x2F;&#x2F;holzschu.github.io&#x2F;a-Shell_iOS&#x2F; reply wharvle 14 hours agorootparentprevThere&#x27;s long been a grey area for downloading new program logic as e.g. Javascript—the distinction between content and program can be rather fuzzy—and IIRC they made an explicit exception years back for certain categories. reply mod50ack 12 hours agorootparentprevHistorically, yes. Apple used to be a lot stricter about these things. But they have loosened up over the years.Back in the day, they were removing stuff like scripting apps. They sent warnings to the devs of Pythonista, forcing them to do things like [remove file-opening support](https:&#x2F;&#x2F;mygeekdaddy.net&#x2F;2014&#x2F;06&#x2F;17&#x2F;working-around-apple&#x2F;). And they infamously removed iDOS (a DOSBOX port).Now they&#x27;re much more loose and allow things like iSH and so on. It is still a little bit of a gray area for arbitrary decisions by Apple, though. reply tiltowait 14 hours agorootparentprevOrion on desktop supports Firefox extensions, so is integration possible? (Not all are compatible.) reply lxgr 14 hours agorootparentBrowsers on macOS can use custom rendering engines. On iOS, they have to all use Apple&#x27;s provided version of it (which does not support WebExtensions by itself), so the two are not comparable at all. reply SushiHippie 4 hours agorootparentOrion supports some extensions on IOS too https:&#x2F;&#x2F;blog.kagi.com&#x2F;orion-features reply lxgr 13 hours agorootparentprevWebExtensions are (or at least can be) ultimately just a weird type of HTML+JS app, as far as I understand, so I suspect it&#x27;s possible to run that in one WebView context on iOS and bridge the required APIs between the extension and browser context using content scripts. reply saagarjha 12 hours agorootparentThat is indeed what Orion does, to the extent that this is possible. reply capitainenemo 15 hours agorootparentprevIndeed. Mozilla can complain about it (and they have) but blaming them for the situation serves little purpose. It&#x27;s entirely in Apple&#x27;s court. reply bluGill 14 hours agorootparentThere are probably laws in some country they can use to fight this, but that is a hard legal battle. reply fngjdflmdflg 13 hours agorootparentThe US may be one of those countries depending on the result of the current Epic vs. Apple & Google cases.[0][1][0] https:&#x2F;&#x2F;www.theverge.com&#x2F;23994174&#x2F;epic-google-trial-jury-ver...[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38607424 reply thomastjeffery 13 hours agorootparentprevThat&#x27;s incredible. Just write anti-competitive behavior directly into the contract. No one will care. reply nikeee 12 hours agorootparentActually, it has been mandated by the EU (and other regulators) that they have to allow other browser engines and AFAIK there are already teams at Mozilla&#x2F;Google that are porting their respective engine to iOS.https:&#x2F;&#x2F;9to5mac.com&#x2F;2023&#x2F;02&#x2F;07&#x2F;new-iphone-browsers&#x2F; reply worik 9 hours agoparentprevI advise stopping the use of iOS reply sexy_seedbox 9 hours agorootparentThrow that iPhone into the river! reply gloryjulio 15 hours agoparentprevThey can&#x27;t push for that. Even chrome didn&#x27;t get the engine deal.But EU is pushing for sideloading reply neilv 14 hours agoprevThis is great news. On GrapheneOS, every time I use the stock browser without the benefit of my uBlock Origin setup, I feel a bit creeped-out and violated. reply yjftsjthsd-h 13 hours agoparentIn fairness, uBo has been supported even when most extensions were being artificially left out. reply Zuiii 9 hours agoprevGood. Now allow self-signed extensions like literally every other browser that supports extensions. reply stavros 8 hours agoparentThey do? Or do you mean on mobile, where AFAIK almost no browser supports extensions? reply Zuiii 5 hours agorootparentYep they do. reply bad_user 14 hours agoprevYou could get extensions working on FF for Android, for some time now, by setting a custom collection ID, allowed in the Beta version.The problem is that many extensions have been incompatible with Android. And of those compatible, many have poor UX. For example, LeechBlock has been compatible and listed as available for some time, but its settings page isn&#x27;t mobile-friendly. And LeechBlock can&#x27;t restore settings from “sync storage”, you have to load them from a local file (on mobile, having local files is a challenge in itself). Many people may have a bad experience.On the other hand, extensions are the primary reason to use Firefox on Android. Therefore, I&#x27;m glad about this news. reply summm 11 hours agoprevThe 2nd most important addon after unlock origin is Multi-Account-Containers: https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;addon&#x2F;multi-account...This would enable proper isolation between browsing contexts, and therefore make progressive web apps truly usable and a good alternative to native apps. Currently PWAs leak cookies to the browser, therefore you cannot login on the PWA while browsing \"anonymously\" in the browser. reply codethief 9 hours agoprevCan anyone recommend an extension that can be used to limit the total number of tabs in Firefox for Android? \"Limit Tabs\"[0] works great on the desktop and I was hoping it would become available on mobile now, but sadly it didn&#x27;t.[0]: https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;addon&#x2F;rudolf-fernan... reply pdn1 6 hours agoprevIf it wasn&#x27;t for extensions collections that allowed all extensions alll along, I would have quit Firefox a long time ago.... About time they get their head out of their asses reply kilolima 5 hours agoprevIs there a bookmark export extension? Strangely this basic browser functionality is missing from Firefox on Android. reply Helmut10001 5 hours agoprevThis was possible for years with Fennec already, with a custom plugin repository. reply peoplefromibiza 11 hours agoprevBest news in the mobile browsers&#x27; space since Firefox supported extensions!If Firefox goes back to being THE browser of choice for tech savvy people, I&#x27;ll stop thinking I made a bad choice supporting it everyday since it came out.Sometimes a joy. reply leaf-node 13 hours agoprevIceraven, a fork of Firefox, already has these features. reply jerrygoyal 3 hours agoprevis tempermonkey supported? reply mortos 2 hours agoparentTampermonkey has been supported even before they opened it up today reply autoexec 13 hours agoprevNice! Now add about:config to stable releases reply Ridj48dhsnsh 12 hours agoparentAs an alternative, you can get a stable release with about:config by installing Firefox (or Mull) from F-Droid. reply yoavm 11 hours agoparentprevcurious, what do you want to do with about:config on FF for Android? reply autoexec 11 hours agorootparentMostly, basic security things like disabling prefetch, disabling WebRTC, disabling redirects, preventing sites from reading my battery level, preventing firefox from changing what I type in the address bar (fixup), etc reply mccr8 9 hours agorootparentBattery status was disabled in Firefox in 2016.https:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;2016&#x2F;nov&#x2F;01&#x2F;firefox-d... reply firebot 15 hours agoprev> where we’re the only major Android browser to support an open extension ecosystemUhm, Kiwi browser is Chrome-based and supports Chrome-extensions on Android and has for years. It&#x27;s pretty great. reply gruez 14 hours agoparentAccording to their github repo, it was last rebased with chromium version 105.0.5195.24, which was from August 2022. Using a 15 month old browser seems hilariously insecure.https:&#x2F;&#x2F;github.com&#x2F;kiwibrowser&#x2F;src.next reply davidy123 14 hours agorootparentI use Kiwi, I take the risk for the ability to run my own extensions (though I use a two-fisted approach where I use Chrome for deep accounts). It&#x27;s a shame it&#x27;s not updated more often, it&#x27;s an open source project I would support. reply AzzyHN 14 hours agorootparentYou are vulnerable to the webp exploit (https:&#x2F;&#x2F;nvd.nist.gov&#x2F;vuln&#x2F;detail&#x2F;CVE-2023-4863) reply firebot 13 hours agorootparentprevMine states 120.0.6099.26, which was released a month ago, https:&#x2F;&#x2F;chromereleases.googleblog.com&#x2F;2023&#x2F;11&#x2F;chrome-beta-fo... reply zamadatix 13 hours agorootparentYour user agent string or the actual browser code? The former is notoriously just set to whatever makes websites happy. An easy way to test is see if a current feature actually works as expected e.g. https:&#x2F;&#x2F;jsfiddle.net&#x2F;fxc9a8uc&#x2F;1 \"test1\" should be green at the top right. reply gruez 13 hours agorootparentIt&#x27;s also possible that they updated the code but didn&#x27;t push the changes to the repo, which I guess is better than running 15 month old code, but also is kinda suspicious because they&#x27;re not honoring their commitment to open source. reply p1mrx 12 hours agorootparentprevI just tried this on Android. \"test1\" is green on Chrome 120 and Firefox 121, black on Kiwi 120. reply mod50ack 14 hours agoparentprevWhile I respect Kiwi for implementing extension support, they&#x27;ve often fell far behind the upstream Chromium codebase and they&#x27;re significantly smaller than even Firefox for Android. So I don&#x27;t think they&#x27;d really be a \"major\" Android browser.Then again, Firefox could easily be said to not be a major Android browser either! reply xnx 14 hours agorootparent> they&#x27;ve often fell far behind the upstream Chromium codebaseI don&#x27;t pay consistent attention, but these are the version numbers I currently see:Kiwi: 120.0.6099.26Chrome: 120.0.6099.110 reply ajayyy 13 hours agorootparentKiwi has historically faked the version number to prevent websites from telling you to update your browser. I would assume that number is not legitimate. reply firebot 13 hours agorootparentprevI wouldn&#x27;t say that far, maybe a month. It gets regular updates. reply mod50ack 12 hours agorootparentHistorically, it&#x27;s gotten much further behind, but they&#x27;ve gotten better recently. reply troyvit 12 hours agoparentprevI was about to be like, \"Yah well is Kiwi a &#x27;major&#x27; browser?\" Then I looked at android browser share[1] and realized that Firefox certainly isn&#x27;t either.[1] https:&#x2F;&#x2F;gs.statcounter.com&#x2F;browser-market-share&#x2F;mobile&#x2F;world... reply mook 8 hours agorootparentI wonder if there&#x27;s a large overlap of people using Firefox on Android and people blocking StatCounter. I thought I&#x27;d look up Mozilla&#x27;s public telemetry stats to see if they have any addon info, but I couldn&#x27;t find anything about Android at telemetry.mozilla.org without a login… reply Ridj48dhsnsh 11 hours agorootparentprevHow does Opera have 3-4x the market share of Firefox? Is it installed by default anywhere? reply phreack 5 hours agorootparentPersonally speaking, I would love to switch to Firefox just to use uBlock Origin, but this picture[1] shows Opera&#x27;s killer feature.It&#x27;s a screenshot of today&#x27;s old.reddit.com&#x2F;r&#x2F;all. You can see how you&#x27;re able to read all of the text vertically, without having to scroll sideways, because it wraps perfectly to the current pinched zoom level. No other browser works this good on Android, and reading stuff is my main internet use case. Try seeing how that website looks on any other browser, it&#x27;s ridiculous how unusable they are.[1] https:&#x2F;&#x2F;files.catbox.moe&#x2F;5t853c.png reply xnx 14 hours agoparentprevI was a longtime Firefox on Android user until the extension situation got increasingly fragile and complicated. I&#x27;ve been very happy since switching to Kiwi. It&#x27;s faster, more frequently updated, and supports all the extensions I want. Highly recommended. reply commoner 13 hours agoprevThis is progress, but Mozilla needs to do more. Firefox for Android still lacks the ability to sideload add-ons, a feature that works on the desktop version of Firefox. This means Android users aren&#x27;t able to install extensions outside addons.mozilla.org (AMO) unless they switch to a Firefox alternative that supports it, such as Iceraven[1] or SmartCookieWeb-Preview.[2]For me, the most important add-on that has been removed from AMO is Bypass Paywalls Clean, which is the easiest way to bypass paywalls on popular news sites. In April of this year, a French website filed a DMCA copyright takedown notice, causing Mozilla to remove the extension from AMO.[3] The add-on developer (magnolia1234) did not want to challenge the DMCA notice, probably because it would require them to break anonymity and be subject to legal liability.[4]Fortunately, in September, another developer (dbmiller) was willing to reupload the add-on to AMO as \"Bypass Paywalls Clean (D)\" with no changes.[5] The hope is that dbmiller will keep this add-on up to date with the source and challenge any DMCA notices filed against this new upload.However, the fact remains that Bypass Paywalls Clean was unavailable on Firefox for Android for 5 months because the browser did not allow sideloading. In the announcement, Mozilla says their mission is to maintain \"an open and accessible internet for all\" and that extensions are meant to help users obtain \"more personal agency out of their online experience\". To achieve this mission and better distinguish Firefox from browsers that gate add-ons through app stores (Safari on iOS), Mozilla should allow users to enable sideloading on Firefox for Android as an option.[1] Iceraven: https:&#x2F;&#x2F;github.com&#x2F;fork-maintainers&#x2F;iceraven-browser[2] SmartCookieWeb-Preview: https:&#x2F;&#x2F;github.com&#x2F;CookieJarApps&#x2F;SmartCookieWeb-preview[3] https:&#x2F;&#x2F;www.ghacks.net&#x2F;2023&#x2F;04&#x2F;20&#x2F;mozilla-removes-bypass-pay...[4] https:&#x2F;&#x2F;gitlab.com&#x2F;magnolia1234&#x2F;bypass-paywalls-firefox-clea...[5] Bypass Paywalls Clean (D): https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;addon&#x2F;bypass-paywal... reply DistractionRect 13 hours agoparentAFAIK, it was available in nightly. You could curate your own add on list which you could then install on Firefox for Android Nightly, and I&#x27;m fairly certain you can still do that if you want something that isn&#x27;t in this new, expanded list. reply jeffchien 11 hours agorootparentYou can also directly sideload .xpi by tapping the Nightly logo in the About page a few times. I&#x27;m not sure when they added this back. reply commoner 10 hours agorootparentWow, thanks for sharing. I just installed Firefox Nightly (v122.0a1) from the Play Store and sideloading does work again. After tapping the Nightly logo several times, an \"Install add-on from file\" option shows up in the settings. I can even install an unsigned add-on with \"xpinstall.signatures.required\" set to \"false\" in about:config. For the longest time, Nightly allowed users to set up the \"Custom Add-on collections\" workaround to install add-ons from addons.mozilla.org but not sideload add-ons directly, so this is a major improvement.Edit: Nightly gained the ability to sideload add-ons 2 weeks ago from the pull request at https:&#x2F;&#x2F;github.com&#x2F;mozilla-mobile&#x2F;firefox-android&#x2F;pull&#x2F;4568. Also, a Mozilla employee has confirmed that sideloading is going to make it to the release channel of Firefox for Android!* Firefox is having an incredible month. It took time, but I&#x27;m extremely glad Mozilla is taking user feedback seriously.* \"We do want this feature in Release.\" https:&#x2F;&#x2F;github.com&#x2F;mozilla-mobile&#x2F;firefox-android&#x2F;pull&#x2F;4568#... reply iggldiggl 10 hours agorootparentprevOh wow, finally, great. The inability to manually install add-ons was also a major annoyance for add-on development (at least for me), because without that you were restricted to temporary add-on installation via devtools, so you couldn&#x27;t really seriously test drive your own add-ons for any extended period of time. (Or attempt tracking down some rare bug.) reply mdaniel 12 hours agorootparentprevI can confirm that sideloading .xpi does not work in Nightly (at least the one from the Play store -- I&#x27;ve never worked up the energy to build the apk from source and don&#x27;t feel like using the F-Droid because reasons)I even tried creating my own collection to include Violentmonkey and it didn&#x27;t work but I don&#x27;t this second recall why reply commoner 9 hours agorootparentGive it another try. The current version of Nightly (v122.0a1) from the Play Store has functional sideloading for me after I unlock the \"secret menu\" in the settings by going to \"About Nightly\" and tapping the Nightly icon several times.Before Bypass Paywalls Clean (D) existed, I was able to install Violentmonkey on Firefox for Android through the add-on collections workaround to use the Bypass Paywalls Clean userscript. You need to enable desktop mode when assembling the add-on collection (or do it from a computer) for Violentmonkey to show up as an option in the search. reply mdaniel 8 hours agorootparent> Give it another try. The current version of Nightly (v122.0a1) from the Play Store has functional sideloading for me after I unlock the \"secret menu\" in the settings by going to \"About Nightly\" and tapping the Nightly icon several times.uh-huh> You need to enable desktop mode when assembling the add-on collection (or do it from a computer) for Violentmonkey to show up as an option in the search.And people say \"Firefox not popular, why?\"what a tire fire, for no damn good reason reply commoner 8 hours agorootparentI completely agree, Mozilla has made a ton of mistakes with Firefox for Android in the last few years. That&#x27;s why I&#x27;m happy to see Mozilla finally follow through with their promise to expand add-ons for Android and pleasantly surprised that sideloading is about to be enabled in the stable channel of Firefox for Android. replyIkatza 13 hours agoprevExtensions are nice to have, but pointless as long as FF for Android doesn&#x27;t render most pages correctly (HN, for example). reply scottbez1 13 hours agoparentCare to expand a bit?I&#x27;ve been daily driving FF Android for a few years now and I&#x27;ve had the opposite experience: the vast majority of pages work and render fine (including HN) and it&#x27;s an extremely rare occasion that I switch to Chrome to use a website. Even then, I often find that Chrome isn&#x27;t any better and the underlying issue was the website&#x27;s mobile handling in general (e.g. touch events working differently than mouse events, or just a completely broken mobile-only component swaps) reply emestifs 12 hours agoparentprevFirefox paradox strikes again. User brings up an unrelated thing, even if valid, to lessen something positive.You seen this pattern again and again in Firefox news threads. reply berkes 10 hours agoparentprevIronically, the few times that I see Firefox (for Android) render something wrong, it&#x27;s because of an addon that is messing with the wrong stuff. reply oblio 26 minutes agoparentprevI&#x27;ve been using Firefox and only Firefox on Android for HN for... at least 5 years.What are you going on about?HN is a basic site, Lynx on MS DOS could render it. reply Aardwolf 13 hours agoparentprevI&#x27;ve never seen HN rendered incorrectly in any desktop or android FF version, what do you mean? reply novemp 13 hours agoparentprevI&#x27;m using HN on Firefox for Android right now and it looks totally normal. What are you talking about? reply ChrisArchitect 15 hours agoprev [–] We&#x27;ve had lots of news about this coming for months, but with Mozilla&#x27;s quite low market share, and the share of those users that use extensions - who&#x27;s really caring about this other than some power users? reply emestifs 14 hours agoparentThe Firefox paradox:People b***h about Firefox&#x27;s (lack of) market share, Mozilla doing stupid things (fair criticism), Firefox not having X (extension support on Mobile, moving from legacy extensions to standard manifest format)Then people will still bring up this baggage even when something good happens, will refuse to move away from the browser monoculture&#x2F;monopoly, s**t on Firefox devsFFS, something good happened. No other browser has this. Yet people will find a way to lessen it. For what? What benefit? reply dblohm7 14 hours agorootparentFormer Firefox dev for both desktop and Android here: I can definitely confirm that being constantly shit on wore me down a lot. reply emestifs 14 hours agorootparentThank you and everyone else working on the Firefox browser and adjacent projects for your hard work. Don&#x27;t let the noise of the internet lessen what you and the team have done and continue to do. reply coldpie 13 hours agorootparentprevI&#x27;m sorry that happened. I wish there was some way to solve the \"one jerk outweighs a thousand happy users\" problem. I still vividly remember one guy being an asshole about my work on the Wine bug tracker a decade ago, regardless of how many happy users I know there were. reply wolverine876 11 hours agorootparent> I wish there was some way to solve the \"one jerk outweighs a thousand happy users\" problem.We could downvote all that stuff to oblivion. Instead, comments like it are voted to the top comment on almost every page. reply pcwalton 11 hours agorootparentIt&#x27;s human nature, unfortunately. Reality television producers have known this for ages: the episodes that feature people who come off as irredeemable jerks always garner the highest ratings. reply wolverine876 11 hours agorootparent> It&#x27;s human nature, unfortunately.If that ever really meant something, it has been so overused in the last few years that it&#x27;s impossible to pick out any needles of serious use from the general default trendy grain silos of despair.I&#x27;m not trying to get all of humanity to give up sex. I believe we can do better, here on HN, in this one regard. I am that insanely optimistic! reply bloopernova 11 hours agorootparentprevI am very thankful for Firefox. It keeps the web sane for me, and I very much appreciate everyone who contributed to it.Thank you for your work! reply MiddleEndian 13 hours agorootparentprevGotta say, I fucking love Firefox. Be proud of your work. reply dblohm7 10 hours agorootparentI am, and thank you! reply wolverine876 11 hours agorootparentprevSorry. You did good things for everyone and deserve much better, but I hope you give yourself the recognition. Thanks for everything.HN should have an annual Appreciation Day, with no enshittification of threads. reply dandanua 12 hours agorootparentprevHaters gonna hate reply secretforest 9 hours agorootparentprevFF gets crapped on all the time, BUT... it&#x27;s still the best mainstream browser out there IMHO. I&#x27;ve been using FF since day one, when it was code named Phoenix. As the spiritual successor to Netscape Navigator, I have super fond memories of using NN, as it was the only decent browser we could install on the Sun Sparc workstations we had in college before I could afford my own computer. I still toy around with SeaMonkey once or twice a year for the memories.Sadly, the browser world has almost become something of a mono culture with the majority of offerings using Chromium as their base. I liked Opera for years. Original engine. Tabbed. Now Vivaldi is the Opera successor, but sadly uses Chromium as the base. Vivaldi have said they are not going to allow the changes to affect them.Again, sadly, I doubt that in the near term, anyone will try and offer up a new browser. Even Edge is nothing more than Chromium with MS&#x27;s tech-nasty Kabuki makeup and overly-complicated proprietary plumbing. Is it too much to ask for a browser that just browses the web without all the garbage tie-ins? Tabs, ad blocking that I control, not add-ins. Like a Pi-hole, where I can add lists. I realize some browsers do this, but the tie-ins, notes, skins, email, political activism, it&#x27;s all too much. reply toyg 13 hours agorootparentprev> For what? What benefit?Trolling used to be an amateur sport, but these days it&#x27;s largely a professional endeavour. Astroturfing is an everyday occurrence on any decently-sized social media site, including this very one. reply MaxBarraclough 13 hours agorootparentprev> s*t on Firefox devsThe nerd rage is targeted at Mozilla&#x27;s dishonest and incompetent managers, no? The actual dev work is top notch. reply coldpie 12 hours agorootparent> dishonest and incompetent managersEven if it is, that kind of language doesn&#x27;t help. These are all people you&#x27;re talking about, trying their best to do a job they care about. Nothing gets better by your being a jerk. reply Ridj48dhsnsh 11 hours agorootparent> trying their best to do a job they care aboutI would not take that as a given for Mozilla&#x27;s upper management. Many of their decisions seem to ignore what users want in deference to Google or other motivations. reply wolverine876 11 hours agorootparentComments like yours are exactly the problem. Do you have any real knowledge anyway? Have you worked there? It&#x27;s just spreading toxic sludge. reply asadotzler 10 hours agorootparentprevYou&#x27;re just making shit up. I was with the Mozilla project for 25 years, with Netscape and then the Mozilla companies for 23 years. I was involved in reviews of the very first Google and Mozilla contract in the fall of 2004. Google has no say in the Mozilla product experience. None. There are some things Mozilla is disallowed from doing to Google Search results that Firefox displays, but that&#x27;s basically it. That you want to imagine nefarious backroom deals that never existed and use those imaginings to shit on Mozilla is deeply insulting, and you should know that you and people like you have done more to dispirit and demoralize Mozilla than any competition ever did. reply emestifs 12 hours agorootparentprevNo, people go after the devs too. I was specific about distinguishing Firefox and Mozilla in my post. Firefox in too often caught in the political&#x2F;flame crossfire. reply throwawaymoz28 5 hours agorootparentprevI worked at Mozilla as an engineer for 6 years and this was not how it came across to me; there was, in fact, quite a lot of hostility towards engineers specifically.I would also note that \"managers\" runs quite a wide gamut and my experience with engineering managers at Mozilla was generally positive; upper management was not so great. reply dmix 14 hours agorootparentprev\"Why even try\" reply capitainenemo 15 hours agoparentprevWell, you might be right that it is power users, but I know that extensions and greater extension freedom are one of the things that is the draw that keeps the remaining Mozilla Firefox users (like me) loyal. Basically I&#x27;m arguing power users are a disproportionate percentage of the remaining Mozilla Firefox user base, which is why things like supporting tracking protection and privacy measures also makes sense for them to focus on, even if the majority of people online might not care about this.So, I&#x27;m glad they are expanding the extensions available. I just hope that this isn&#x27;t tied to creating an account still. [EDIT] I was overjoyed to see that I was able to add an extension without creating an account. Yay! reply neilv 14 hours agoparentprevTechies and power users often create network effects, in how they contribute to and promote what they use.This is one of the reasons it&#x27;s so troubling when some techies latch onto some very closed platform (sometimes by a known-underhanded company) and start making it more attractive to others, by making open source software specific to it, making tutorials on hot employability topics that implicitly use the platform, etc. When open platforms exist, and could also benefit from this contribution and promotion.At first it was \"Jeebus, I wonder what&#x27;s going on with that one person, who normally uses open source, stabbing themself in the back like that.\" Then it became \"Jeebus, are we losing open platform ground with the majority of an entire generation of techies, after we&#x27;d finally won.\" (I have good guesses about why, and I also know at least a couple early maneuvers that I can&#x27;t talk about, but it&#x27;s still dismaying how vapid the collective behavior can be.) reply nix0n 13 hours agoparentprevPower users matter a lot for web browsers, because web developers are power users of web browsers.Firefox&#x27;s loss of market share in general is a direct consequence of its loss in market share among web developers, because web developers stopped testing their websites in Firefox.Any time Firefox does something good for power users, it&#x27;s a good thing for the whole web ecosystem. reply squidbeak 14 hours agoparentprevThose who care about competition among browser engines. Share is more likely to stay low if potential new users can&#x27;t find the extensions they need. reply smilliken 13 hours agoparentprevMay I remind you that Firefox has over 300M users. If that&#x27;s not worthy of admiration, scarcely anything is. reply yjftsjthsd-h 13 hours agoparentprevThis is Hacker News - if you don&#x27;t want things that are interesting, even primarily, to power users, this is a terrible forum to frequent. reply ChrisArchitect 13 hours agoparentprevSensitive power users.I didn&#x27;t say it was sh*t. I&#x27;m saying it&#x27;s not newsworthy.Clap for the devs. And install all the extensions. But we don&#x27;t need a hundred posts about it. This isn&#x27;t the big story Firefox marketing might think it is. reply emestifs 12 hours agorootparentYou got downvoted and now you&#x27;re original comment is greyed out. Now you&#x27;re mocking people and calling them \"sensitive\".The fact news about Firefox gets upvoted clearly indicates it is newsworthy. You don&#x27;t get to decide. The users of HN and their votes do. reply LegitShady 14 hours agoparentprev [–] you don&#x27;t need to be a power user to do any of this. its just like using any other browser. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mozilla has introduced more than 450 new extensions for Firefox on Android, establishing an open extension ecosystem on mobile.",
      "Users can now customize and enhance their web experience on Firefox for Android by easily installing these extensions.",
      "Popular desktop extensions like uBlock Origin and Dark Reader are now available on mobile, and more extensions will be added in the coming months."
    ],
    "commentSummary": [
      "Mozilla has expanded extension support for Firefox for Android, allowing users to install any extension from addons.mozilla.org (AMO).",
      "The delay in expanding extension support was due to the extensive work required to rewrite the UI of Firefox for Android.",
      "Discussions surrounding this topic include speculation about Mozilla's decision, concerns about privacy, criticism of the use of an AMO account, and suggestions for warning users about untested extensions."
    ],
    "points": 460,
    "commentCount": 177,
    "retryCount": 0,
    "time": 1702576728
  },
  {
    "id": 38645856,
    "title": "The Consequences of Abolishing QA Teams in Software Development",
    "originLink": "https://davidkcaudill.medium.com/maybe-getting-rid-of-your-qa-team-was-bad-actually-52c408bd048b",
    "originBody": "Maybe Getting Rid of Your QA Team was Bad, Actually. David Caudill · Follow 6 min read · Dec 3 -- 15 Over many years, “DevOps” practitioners applied Theory Of Constraints to our problems, ruthlessly optimizing our delivery pipelines and practices. Manual release management? Hell no, automate that. Deployment? Automate that too. Image management? 🔨 No thanks. Rolling back after we trebuchet a flaming dumpster into production? Automated. Whatever low value activity we could find in the process of getting code from product backlog to customer hands was a bottleneck to be removed or optimized. The end result of this was that the slowest part of software delivery is testing. Since testing is why continuous delivery exists, that should have been good enough. Yes, we can make our tests faster, more automated, parallelized, etc. But when the highest value activity of a given practice is the bottleneck, you’re optimal. You have achieved “the best possible problem”. Those habits and behaviors of optimization didn’t stop there. We kept on chopping. 🪓 We squished our integration and end to end tests down to unit tests to parallelize. At the personnel level, we pushed out anybody whom we believed could not code, indiscriminately, at the function level. We decided that testing might not be the bottleneck…the QA team was. The industry began to treat the people in these roles worse and worse. Expectations for them went up, salaries went down(while everyone else’s seemed to be going up!), we contracted the role, we offshored it, pretty much anything we could do to try and stop employing QA Engineers. This created a self-reinforcing spiral, in which anyone “good enough at coding” or fed up with being treated poorly would leave QA. Similarly, others would assume anyone in QA wasn’t “good enough” to exit the discipline. No one recommends the field to new grads. Eventually, the whole thing seemed like it wasn’t worth it any more. Our divorce with QA was a cold one — companies just said “we’re no longer going to have that function, figure it out.” It was incredibly disrespectful and demoralizing to folks who had spent their career in QA. It also caused a lot of problems, because producing low quality software is actually a huge headache. You can probably see where this is going by now: developers did not figure this out. Most orgs have no idea who should be doing what in terms of software quality. Those who have kept the function are struggling to find a place for it, because of the damage already done to the discipline. It turns out, the “One Weird Trick” to faster software delivery was not “fire your testers”. Wrecking this discipline was one of the worst kind of management mistakes — a choice to destroy something that took decades to develop, and one where the impact might not be felt for years. By the time your org has felt it, you’re likely years away from a meaningful fix. The parts of the broken QA role we were handed are all still broken, and on the metaphorical workbench. The division of labor simply did not happen. Unsurprisingly, developers did not readily assume the duties of the role without any additional compensation, recognition, or reward. Those of us who can still remember working with a high functioning QA team can impersonate some of those behaviors, but newer engineers and managers have no idea what any of that was about, and aren’t able to tell what’s missing. The guiding principle in the following advice is that quality assurance is work. Just like any other work, assuming “somebody” will do it, letting it be invisible, is a recipe for failure. Denial is not a strategy. It’s 2023 and it feels silly to have to write this down, but my experience suggests that I absolutely must. Here is the work to be done in order to manage quality in your software practice: Defect Tracking: there needs to be a way for your users to send you information about a bug, and for your developers to log a bug. What is a bug? A bug is an individual ticket that describes what’s wrong and how bad it is. It doesn’t describe the work to fix it, only the defect itself, how to reproduce it, and its impact. In recent years I have been surprised to find that most dev teams I work with simply do not track these. These teams have an ocean of excuses: “We won’t ever fix it.” “That’s not my job.” “I don’t want to fix anything, I get rewarded for new features.” None of these is good enough to justify the low quality results that are produced by this approach. Just managing the list of bugs is WORK. Triage: Bug triage is the process by which your engineering organization assigns, prioritizes, cleans up, categorizes, deduplicates, and otherwise cares for the bugs coming in to your organization. Having a consistent standard for what a high/medium/low severity bug looks like will help your org in a number of ways. We used to call this “bug hygiene”. Similarly, just the task of deciding what team this bug belongs to is work. A high functioning organization can do things like: degrade quality gracefully in the presence of layoffs, hand off a category of bugs to a new team in the event of a reorganization, or jettison all bugs for a feature that’s been cut. You can ignore the triage process if your team never deals with external stressors or reorganizes. That was a joke. Defect Investigation: Reproduction, or “repro”, is a critical part of managing bugs. In order to expedite fixes, somebody has to do the legwork to translate “I tried to buy a movie ticket and it didn’t work” into “character encoding issues broke the purchase flow for a customer with a non-English character in their name”. Similarly, the questions of “how many times does this happen?” and “what is the user impact?” need a real human to spend a minute answering them. Defect investigation is the application of engineering rigor to your backlog of bugs. Focus: There is real value in having people at your company whose focus is on the quality of your end product. Quality might be “everybody’s job”…but it should also be “somebody’s job”. The push/pull of Quality vs Velocity needs an advocate for quality in the discussion — and that dynamic is vital to producing better results. Your testing tools, your test quality, test plans… all of these need an opinionated party to argue in favor of doing the best possible job. End to End Testing: One of the biggest, most common problems I see in the engineering orgs I work with is ownership of the system. Increases in architectural complexity have been done to try and keep teams and applications small. That’s a perfectly rational strategy, but it leaves a gap around…literally the most important thing about your application. In my experience, the average team no longer does this, because it’s too hard. Does anyone in your organization actually USE the product before it ships? It’s easy to look at that list and say “but we’re agile, lean, dynamic, we don’t need to do these things! We’ve moved past this.” But I think that if you look harder, what you’ll find is that this work is probably happening in your organization already — poorly. And imagine if literally any other field tried to make that claim to you. Your car, your bank, your doctor…”we don’t do quality assurance” is just not a great thing to hear or say. Failure to recognize and organize these activities will lead to a really dreadful situation. Tell me if this sounds familiar: The most conscientious employees in your organization are the most bitter. They see the quality issues, they often address them, and they get no recognition for doing so. When they speak up about quality concerns, they get treated like mouthbreathers who want to slow down. They watch the “move fast and break things” crowd get rewarded time after time, while they run around angrily cleaning up their messes. To these folks, it feels like giving a damn is a huge career liability in your organization. Because it is.",
    "commentLink": "https://news.ycombinator.com/item?id=38645856",
    "commentBody": "Maybe getting rid of your QA team was badHacker NewspastloginMaybe getting rid of your QA team was bad (davidkcaudill.medium.com) 395 points by nlavezzo 14 hours ago| hidepastfavorite242 comments pjsg 12 hours agoAt the start of my career (late 70s), I worked at IBM (Hursley Park) in Product Assurance (their version of QA). We wrote code and built hardware to test the product that was our area of responsibility (it was a word processing system). We wrote test cases that our code would drive against the system under test. Any issues we would describe in general terms to the development team -- we didn&#x27;t want them to figure out our testcases -- we wanted them to fix the bugs. Of course, this meant that we would find (say) three bugs in linewrapping of hyphenated words and the use of backspace to delete characters, and then the development team would fix four bugs in that area but only two of the actual bugs that we had found. This meant that you could use fancy statistics to estimate the actual number of bugs left.When I&#x27;ve worked for organizations without QA teams, I introduce the concept of \"sniff tests\". This is a short (typically 1 hour) test session where anybody in the company &#x2F; department is encouraged to come and bash on the new feature. The feature is supposed to be complete, but it always turns out that the edge cases just don&#x27;t work. I&#x27;ve been in these test session where we have generated 100 bug tickets in an hour (many are duplicates). I like putting \"\" into every field and pressing submit. I like trying to just use the keyboard to navigate the UI. I run my system with larger fonts by default. I sometime run my browser at 110% zoom. It used to be surprising how often these simple tests would lead to problems. I&#x27;m not surprised any more! reply pavel_lishin 12 hours agoparent> When I&#x27;ve worked for organizations without QA teams, I introduce the concept of \"sniff tests\". This is a short (typically 1 hour) test session where anybody in the company &#x2F; department is encouraged to come and bash on the new feature.We call those bug-bashes where we work, and they&#x27;re also typically very productive in terms of defects discovered!It&#x27;s especially useful since during development of small features, it&#x27;s usually just us programmers testing stuff out, which may not actually reflect how the end users will use our software. reply steveBK123 12 hours agorootparentA good QA person is basically a personification of all the edge cases of your actual production users. Our good QA person knew how human users used our app better than the dev or product team. It was generally a competition between QA & L2 support as to who actually understood the app best.The problem with devs testing their own & other devs code is that we test what we expect to work in the way we expect the user to use it. This completely misses all sorts of implementation error and edge cases.Of course the dev tests the happy path they coded.. that&#x27;s what they thought users would do, and what they thought users wanted! Doesn&#x27;t mean devs were right, and frequently they are not.. reply justinator 11 hours agorootparentThis dude gets it. reply danny_taco 11 hours agorootparentprevMaybe we work in the same company. I&#x27;d like to add that usually the engineer responsible for the feature being bug-bashed is also responsible of refining the document where everyone writes the bugs they find since a lot are duplicates, existing bugs, or not bugs at all. The output is then translated into Jira to be tackled before (or after) a release, depending on the severity of the bugs found. reply jxramos 12 hours agoparentprevthat&#x27;s very interesting to hide the source of the automated tests from the developers as a strategy. I can see that shifting the focus to not just disabling the test or catering to the test etc. I&#x27;ll have to think about this one, there&#x27;s some rich thoughts to meditate on with this one. reply ansible 11 hours agorootparentIt is an interesting approach I hadn&#x27;t heard of before. For complex systems though, often reproducing the bug reliably is a large part of the problem. So giving the developers the maximum information is necessary.Any time a \"fix\" is implemented, someone needs to be asking the right questions. Can this type of problem occur in other features &#x2F; programs? What truly is the root cause, and how has that been addressed? reply semireg 5 hours agorootparentWow, less IS more. Hear me out.How do we measure “more” information? Is it specificity or density?Because here, assuming they can reproduce, keeping the information fuzzy can make the problem space feel larger. This forces a larger mental-map. reply krisoft 10 hours agoparentprev> I&#x27;ve been in these test session where we have generated 100 bug tickets in an hour.Is that like… usefull to anyone? Especially if they are duplicates. It feels to me that 10 different bugs is enough to demonstrate that the feature is really bad, after that you are just kinda bouncing the rubble? reply notpachet 5 hours agorootparentAs noted in the post, one characteristic of a healthy QA environment at your work is how effectively you triage bugs. That includes detecting duplicates. One big QA smell for me is opening a team&#x27;s bug backlog and realizing that there are shitload of dupes in there, because it means that no one really looked at them in detail. reply NegativeK 8 hours agorootparentprevIt&#x27;s \"free\" QA. And presumably you&#x27;ll do it again later until it&#x27;s better. reply wrs 12 hours agoparentprevAt Microsoft back in the day, we called those “bug bashes”, and my startup inherited the idea. We encouraged the whole company to take an afternoon off to participate, and gave out awards for highest impact bug, most interesting bug, etc. reply hornban 12 hours agorootparentThis is a bit of an aside, but I have a question that I&#x27;d like to ask the wider community here. How can you do a proper bug-bash when also dealing with Scrum metrics that result in a race for new features without any regard for quality? I&#x27;ve tried to do this with my teams several times, but ultimately we&#x27;re always coming down to the end of the sprint with too much to do to implement features, and so anybody that \"takes time off\" to do bug bashing looks bad because ultimately they complete fewer story points than others that don&#x27;t do it?Is the secret that it only works if the entire company does it, like you suggest?And yes, I completely realize that Scrum is terrible. I&#x27;m just trying to work within a system. reply Shaanie 11 hours agorootparentThat&#x27;s not a problem with Scrum, it&#x27;s a problem with your team. If you&#x27;re doing a bug bash every sprint, then your velocity is already including the time spent on bug bashes. If it&#x27;s not in every sprint, you can reduce the forecast for sprints where you do them to account for it (similar to what you do when someone is off etc).If you&#x27;re competing within the team to complete as many story points as possible that&#x27;s pretty weird. Is someone using story points as a metric of anything other than forecasting? reply JohnFen 10 hours agorootparent> Is someone using story points as a metric of anything other than forecasting?Very nearly every company I&#x27;ve worked at that uses Scrum uses story points, velocity, etc., as a means of measuring how good you or your team are. Forecasting is a secondary purpose. reply zaphirplane 2 hours agorootparentSounds easy to game. Click a button to foo the bar isone million points reply mb7733 9 hours agorootparentprevDon&#x27;t teams assign points to their own tickets? So how could one compare the points between teams? reply jSully24 7 hours agorootparentYes. But many Sr. Leaders just see a number so it must also be a metric you can use for measurement. They do not understand it’s real use.I picture a construction company counting the total inches &#x2F; centimeters each employee measured every day. Then at the end of the year firing the bottom 20% of employees measured in total units measured in the last 12 months. reply pierat 10 hours agorootparentprev> That&#x27;s not a problem with Scrum, it&#x27;s a problem with your team.I&#x27;ve seen that justification time and again, and it feels disingenuous every time it&#x27;s said. (Feels like a corrolary to No True Scotsman.)I&#x27;ve also seen scrum used regularly, and everywhere I&#x27;ve seen it has been broken in some fashion. Enough anecdata tells me that indeed Scrum, as stated, is inherently broken. reply ultrasaurus 10 hours agorootparentprevAh the classic: How do I improve quality in an org &#x27;without any regard for quality&#x27;? :)But assuming that everyone cares about quality (I know, a big leap), what has worked for me is: tagging stories as bugs&#x2F;regressions&#x2F;customer-found-this and reporting on time spent. If you&#x27;re spending too much time fixing bugs, then you need to do something about it. New bugs in newly written code are faster to fix, so you should be able to show that bug bashes make that number going down quarter over quarter which contributes to velocity going up.Alternately (and not scrum specific) I&#x27;ve had success connecting a CSM&#x2F;support liaison to every team. Doesn&#x27;t give you a full bug bash, but even one outside person click testing for 20m here and there gets you much of the benefit (and their incentives align more closely with QA). reply m4rtink 11 hours agorootparentprevSeems like another data point stating sprints don&#x27;t make sense in real world projects ? reply debatem1 11 hours agorootparentprevThe team with the lowest bug bash participation this week is the victim err host of next week&#x27;s bug bash. reply duderific 10 hours agorootparentprevI&#x27;m kind of in the same boat re story points and Scrum metrics, but sometimes we can get management buy-in to create a ticket to do this sort of thing, if it&#x27;s seen as high value for the business. reply JCharante 11 hours agorootparentprev> because ultimately they complete fewer story points than others that don&#x27;t do it?Solution: don&#x27;t measure story points reply bigbillheck 11 hours agorootparentprevOnly assign points based on a (n-1)-day sprint instead of a n-day one. reply jSully24 7 hours agorootparentprevWe do a similar thing but call it a bug hunt.Not only do we uncover bugs, it’s a great way to get the whole company learning about the new things coming and for the product team to get unfiltered feed back. reply lulznews 1 hour agoparentprev>Any issues we would describe in general terms to the development team -- we didn&#x27;t want them to figure out our testcasesI’m sorry but this is just lol. Did the devs play back by creating bugs and seeing if your team could find them? reply JonChesterfield 11 hours agoparentprev> This meant that you could use fancy statistics to estimate the actual number of bugs left.That&#x27;s very clever. Precise test case in QA plus vague description given to dev. Haven&#x27;t seen it before, thank you for sharing that insight. reply dredmorbius 8 hours agorootparentGenerally, \"The German Tank Problem\":There are similar methods used in estimating wildlife populations, usually based on catch-release (with banding or tagging of birds or terrestrial wildlife) or repeat-observation (as with whales, whose fluke patterns are distinctive). reply bluGill 9 hours agorootparentprevIf there is a precise testcase, automate it. There real value of manual tests is whene they explore to find variations you didn&#x27;t think of. Your manual tests should be explore x reply jrockway 11 hours agoparentprevI&#x27;ve always been impressed by hardware QA test teams I&#x27;ve worked with. On Google Fiber, they had an elaborate lab with every possible piece of consumer electronics equipment in there, and would evaluate every release against a (controlled) unfriendly RF environment. (\"In version 1.2.3.4, the download from this MacBook Pro while the microwave was running was 123.4Mbps, but in version 1.2.4.5, it&#x27;s 96.8Mbps.\" We actually had a lot of complexity beyond this that they tested, like bandsteering, roaming, etc.) I was always extremely impressed because they came up with test cases I wouldn&#x27;t have thought of, and the feedback to the development team was always valuable to act on. If they&#x27;re finding this issue, we get pages of charts and graphs and an invite to the lab. If a customer finds this issue, it just eats away at our customer satisfaction while we guess what could possibly have changed. Best to find the issue in QA or development.As for software engineers handling QA, I&#x27;m very much in favor of development teams doing as much as possible. I often see tests bolted on to the very end of projects, which isn&#x27;t going to lead to good tests. I think that software engineers are missing good training on what to be suspicious of, and what best practices are. There are tons of books written on things like \"how to write baby&#x27;s first test\", but honestly, as an industry, we&#x27;re past that. We need resources on what you should look out for while reviewing designs, what you should look out for while reviewing code, what should trigger alarm bells in your head while you&#x27;re writing code.I&#x27;m always surprised how I&#x27;ll write some code that&#x27;s weird, say to myself \"this is weird\", and then immediately write a test to watch it change from failing to passing. Like times when you&#x27;re iterating over something where normally the exit condition is \"iI like putting \"\" into every field and pressing submit.Going deeper into the training aspect, something I find very useful are fuzz tests. I have written a bunch of them and they have always found a few easy-to-fix but very-annoying-to-users bugs. I would never make a policy like \"every PR must include a fuzz test\", but I think it would be valuable to tell new hires how to write them, and why they might help find bugs. No need to have a human come up with weird inputs when your idle CI supercomputer can do it every night! (Of course, building that infrastructure is a pain. I run them on my workstation when I remember and it interests me. Great system.)At the end of the day, I&#x27;m somewhat disappointed in the standards that people set for software. To me, if I make something for you and it blows up in your hands... I feel really shitty. So I try to avoid that in the software world by trying to break things as I make them, and ensure that if you&#x27;re going to spend time using something, you don&#x27;t have a bad experience. I think it&#x27;s rare, and it shouldn&#x27;t be, it should be something the organization values from the top to the bottom. I suppose the market doesn&#x27;t incentive quality as much as it should, and as a result, organizations don&#x27;t value it as much as they should. But wouldn&#x27;t it be nice to be the one software company that just makes good stuff that always works and doesn&#x27;t require you to have 2 week calls with the support team? I&#x27;d buy it. And I like making it. But I&#x27;m just a weirdo, I guess. reply ryan-duve 48 minutes agorootparent> Going deeper into the training aspect, something I find very useful are fuzz tests.Could you share some details of fuzz tests that you&#x27;ve found useful? I tend to work with backend systems and am trying to figure out whether they will still be useful in addition to unit and integration tests. reply kristjansson 6 hours agoparentprevAt $website, we used to call that “swarm”. All features had to go through swarm before being released, and all product managers were made to participate in swarm.Its demise was widely celebrated. reply Zelphyr 13 hours agoprevI worked at two companies 15-20 years ago that invested in top-tier QA teams. They were worth their weight in gold. The products were world class because the QA team were fantastic at finding bugs we developers didn&#x27;t think of looking for because we were too close to the problem. We are too used to looking at the happy path.One key attribute to both companies is that it was dictated from on high that the QA team had final say whether the release went to production or not.These days companies think having the developers write automated tests and spend an inordinate amount of time worrying over code coverage is better. I can&#x27;t count how many products I&#x27;ve seen with 100% code coverage that objectively, quantifiably doesn&#x27;t work.I&#x27;m not saying automated testing is bad. I&#x27;m saying, just as the author does, that doing away with human QA testers is. reply supportengineer 13 hours agoparentI&#x27;ve seen QA&#x2F;QE greatness and it was similar to how you describe. A different chain of command for deciding if releases are certified for production. Different incentive structures as well.Not to mention, at one recent employer, the QE team wrote an enormous amount of code to perform their tests - It was more LOC than the modules being tested&#x2F;certified. reply eitally 12 hours agorootparentI had a team like that once. It was glorious. And ultimately, I&#x27;m convinced it led to overall faster development cycles because the baseline code quality & documentation was so much better than it would have been without such a great QA manager. The QA team, of course, was also technical -- mostly with SWE backgrounds -- and they were primarily colo&#x27;d in the same office as the dev team. I still remember the epiphany everyone had one planning cycle when it was mutually understood that by generally agreeing to use TDD, the QA team could participate actively in the real engineering planning and product development process.... Then I left and my CIO let go the onshore QA team in favor of near term cost savings. Code quality went way down and within a year or two several apps needed to be entirely rewritten. Everything slowed down and people started pointing fingers, and before you knew it, it was time for \"cloud native rearchitecting&#x2F;reengineering\" which required an SI to come in with \"specialists\". reply munificent 10 hours agorootparentprev> It was more LOC than the modules being tested&#x2F;certified.So much code. I hope they had a QAQA team to test all that. reply esafak 12 hours agorootparentprevSo how was QA incentivized? reply mikestew 11 hours agoparentprevI can&#x27;t count how many products I&#x27;ve seen with 100% code coverage that objectively, quantifiably doesn&#x27;t work.That’s because code coverage doesn’t find the bugs that result from code you didn’t write, but should have. Code coverage is but one measure, and to treat it as the measure is folly.(But, yes, I have heard a test manager at a large software company we’ve all heard of declare that test team was done because 100% coverage.) reply thaumasiotes 5 hours agorootparentIt&#x27;s more because code coverage isn&#x27;t measured correctly. What you want is branch coverage.If you have something like this: if condition1: do_something1() if condition2: do_something2() if condition3: do_something3()There are 8 possible paths for the code to follow here, but you can cover 100% of lines of code in one test. If your code coverage measure tells you that testing the case where conditions 1, 2, and 3 are all true achieves 100% coverage, your measure is worthless. Covering this code cannot be done in less than eight tests. reply alternatex 4 hours agorootparentI think most code coverage analysis tools are smart enough to consider flow control statements. At least when it comes to C# ones. reply fho 4 hours agorootparentThey do, but that&#x27;s not the point GP made. An example how this could fail is if the first branch sets up something that branch two uses. Two tests are written that call branch 1&2 and not call them. 100% code coverage, even the non-happy path was tested.But in reality, if those branches have any interaction, you would need to write eight test cases for every combination of branches being run and not being run. reply thaumasiotes 3 hours agorootparentprevWhy do you think that? It isn&#x27;t true. reply PH95VuimJjqBqy 12 hours agoparentprevThe QA culture has to be there, not just dictates that the QA has final say.I&#x27;ve seen companies where that&#x27;s true and it was still trash because the QA were mostly low-paid contract workers who only did exactly what they were told and no more. reply onlyrealcuzzo 12 hours agorootparentWorked at a company where QA had the final say - and that was by far the most toxic &#x2F; worst environment I have ever been in.QA also REFUSED to let developers write automation tests, also REFUSED to let us run them ourselves.What a nightmare.YMMV, but just having the final say is not a silver bullet for sure. reply mikestew 11 hours agorootparentThat’s not because “QA had the final say”, it’s because your QA team were ass clowns. Any QA team that discourages dev from writing or running tests needs to be burnt to ground and rebuilt. reply ponector 11 hours agorootparentUnless dev is switching to write test 100% of time and become a tester it is highly not recommended to let developers write tests.That QA was not a clown, they&#x27;ve seen some shit...Would you let QA to write features in your production code? reply NegativeK 8 hours agorootparentI spent a long time in QA.Devs that don&#x27;t test their own code are usually wasting the QA team&#x27;s time with garbage. It also tends to cultivate (or is a symptom of) an environment where groups are throwing projects over the wall, so to speak, without tightly integrating QA into the process. This wastes significant amounts of time. reply mdavidn 9 hours agorootparentprevDevelopers need to write tests so they understand how to structure the application in a manner that can be tested. The test engineer brings a fresh set of eyes and different expectations about how the application should work. That&#x27;s valuable too. reply ahtihn 11 hours agorootparentprevTest code is just code. If you can write test code you can write production code. If you can write production code you can write tests.If your concern is that devs don&#x27;t have the right mindset for testing, you can have them collaborate with a QA specialist to define the test cases and review the test implementation. reply ponector 11 hours agorootparentIn theory yes.In practice, devs will not write good tests for their features, QA will be kept away from committing to production code.Btw, if it is just code - why developers cannot implement features without bugs? No need in QA in such ideal world. reply VHRanger 11 hours agorootparentprevYou see that with devops often. When they are incentives to block stuff instead of enabling the product they become the roadblock team.It&#x27;s why Google has SRE instead of devops people largely reply ponector 11 hours agoparentprevI&#x27;ve seen developers writing tests with no assertions, or with assertTrue(true). Always green, with 100% coverage!The same people have been asking: why should I write tests if I can write new features?And then one senior QA comes and destroys everything.Once I found that if I press f5 50 times during a minute then backend will go in outOfMemory while spinning requests to the database. reply feoren 11 hours agorootparent> I&#x27;ve seen developers writing tests with no assertionsThis can be OK if the code executing without throwing exceptions is itself testing something. If you have a lot of assertions written directly into the code, as pre- or post-conditions for instance. But I&#x27;m guessing that wasn&#x27;t the case here. reply WrongAssumption 8 hours agorootparentWhy would it be ok to not test the assertions would be triggered if the conditions are bad? How would you verify that the assertions are correct with just a happy path? If you run a code with pre-post assertions, then remove all the assertions the same test will continue to pass. reply kcb 11 hours agorootparentprevThat&#x27;s why the dev teams tests and the QA teams tests are not mutually exclusive. reply ponector 10 hours agorootparentTesting is continuous multilayered multistaged process.Testing should start before first developer wrote first line of code for the project. Architecture blueprints, set of requirements should be tested as early as possible. But that is not happening in real life, only in books. In real life pm will bring cheap contractor from India one month before the target release date. reply hedora 7 hours agoparentprevThe best approach I’ve seen is to do all of the above.- Have QA run pessimal versions of real use cases. Trying to sell a word processor to lawyers? Format the entire US legal code and a bajillion contracts in it, then duplicate it 10x and start filing usability&#x2F;performance bugs.- Have the engineers test everything with randomly generated workloads before committing. Run those tests nightly, and fix all the crashes &#x2F; failures.- Have Product Management (remember them?) work with marketing and sales to figure out what absolutely has to ship, and when.Make sure it only takes one of the above three groups to stop ship, and also to stop non-essential development tasks. reply Arainach 1 hour agorootparentThe devil is in the details of \"essential development tasks\".An uncountable number of products have died or devolved because \"we don&#x27;t have time to do it that way, put in the quick fix\" reply alkonaut 12 hours agoparentprevI love my really thorough QA’s. Yes it’s an antipattern to let me as a dev lean too much on them catching what I won’t. But where I dread even running the code for a minute, they enjoy it. They take pride in figuring out edge cases far beyond any spec. They are definitely worth their weight in gold. It lets developers have confidence when changing things in the same sense a good type system does. For some classes of very interactive apps (e.g games) having unit and integration tests just doesn’t cover the parameter space. reply ponector 11 hours agorootparentPeople here are talking about skillful QA worth their weight in gold.Unfortunately people in the industry who have actual power in planning budgets don&#x27;t think so. An article is right. QA engineers now are viewed as janitors: no one respects then, better to outsource to cheap location. reply bluGill 9 hours agorootparentDepends on industry. Some care about quality, they respect qa. reply bluGill 9 hours agoparentprevAutomated tests take manual testing from 60% to 50% of the time to develop quality software. Valuable, but not a bullet to manual tests. reply fizx 12 hours agoparentprevHow often did you release? reply SkyPuncher 10 hours agorootparentThat&#x27;s my question as well. Bug free code is not the goal. Valuable product is. reply amtamt 13 hours agoprev> The most conscientious employees in your organization are the most bitter. They see the quality issues, they often address them, and they get no recognition for doing so. When they speak up about quality concerns, they get treated like mouthbreathers who want to slow down. They watch the “move fast and break things” crowd get rewarded time after time, while they run around angrily cleaning up their messes. To these folks, it feels like giving a damn is a huge career liability in your organization. Because it is.This is the bitter truth, no one wants to acknowledge.DBAs and Infra, are in the same boat as QAs. Pendulam will swing back in not so long time frame i hope. reply esafak 12 hours agoparentNo, just jump ship and let the damn company fail. Fail faster, haha! This is how we have nice things; when bad companies are not propped up. reply jabroni_salad 10 hours agoparentprevMy idea of the pendulum swinging back is &#x27;you build it you run it&#x27;, personally. Don&#x27;t like the oncall pager? don&#x27;t make it ring. reply hinkley 10 hours agorootparentThe hard part for most people is learning to tell the devil in a necktie to fuck off every time they try to sweet talk you into volunteering for that kind of pain. Also with no recognition or compensation.All it takes is one person on my team to defect and support an untenable amount of tech debt, and everyone on my team has to pay for it. reply bradleyjg 8 hours agoparentprevDBAs and Infra, are in the same boat as QAs. Pendulam will swing back in not so long time frame i hope.Ultimately it’s up to the customers. Will they walk because of bugs and outages or stay because of shiny new features? reply truculent 3 hours agorootparentI’m not so sure. I think software trends in the last 10-15 years or so have been driven heavily by a small number of influential players (FAANG, mega-VCs).An influx of dumb money like that can shape and distort the market and overwhelm the feedback loops that would otherwise give consumers influence.Perhaps now rates are up, the equilibrium changes, but I think it’s still easy to overestimate the number of “first movers” in an industry and the power of tacit or unconscious collusion. reply deniscepko2 2 hours agoparentprevYeah this hit home so hard. Had to leave the startup i loved working at, because started doing all this release fast crap. And somehow people think releasing fintech stuff fast and untested is fine. reply sporedro 9 hours agoparentprevWhile QA and testing is important, I’m not sure you can convince the people only concerned about profits… I think the “release it fast and patch it later” concept here to stay due to the internet being so accessible. Why bother spending tons of money and time when the users will just report the bugs and you can release updates over the internet they can download. Ever since physical copies of video games and software were replaced mainly by downloads, it seems like patching is cheaper. Of course this leads to horrendous security issues, bad user experience, etc. but who cares as long as the guy on top is maximizing profits. reply godelski 13 hours agoprevThe main problem with QA teams is the same problem with IT teams or even management. If they are doing their jobs well they appear to be doing nothing.This often creates a situation where people need to \"justify\" their jobs. Usually this happens due to an over reliance upon metrics (see Goodhart&#x27;s Law) rather than understanding what the metrics are proxying and what the actual purpose of the job is. A bad QA team is one who is overly nitpicky, looking to ensure they have something to say. A good QA team simultaneous checks for quality as well as trains employees to produce higher quality.I do feel like there is a lack of training going on in the workforce. We had the \"95%-ile isn&#x27;t that good\"[0] post on the front page not long ago and literally it is saying \"It&#x27;s easy to get to the top 5% of performers in any field because most performers don&#x27;t actively train or have active feedback.\" It&#x27;s like the difference between being on an amateur sports team vs a professional. Shouldn&#x27;t businesses be operating like the latter? Constantly training? Should make hiring be viewed differently too, as in \"can we turn this person into a top performer\" rather than \"are they already\" because the latter isn&#x27;t as meaningful as it appears when your environment is vastly different than the one where success was demonstrated.[0] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38560345 reply mattgreenrocks 13 hours agoparent> Shouldn&#x27;t businesses be operating like the latter? Constantly training?There&#x27;s a rampant cultural mind-virus that argues that 95%th percentile is somehow tons of work (rather than a lack of unforced mistakes), so everyone just writes it off. It&#x27;s on full display at this very site. Just look on any post involving software quality, and read a bunch of comments suggesting widespread apathy from engineers.Obviously every situation is different, but people seem to be pretty okay with relinquishing agency on these things and just going along with whatever local maxima their org operates in. It&#x27;s not totally their fault, but they&#x27;re not blameless either. reply godelski 12 hours agorootparentYeah it is weird that it is believed that there is a linear scale to work in and quality considering how well known pareto&#x2F;power distributions are. These distributions are extremely prolific too. I mean we even codify that sentiment in the 80&#x2F;20 rule or say that 20% of time is writing code and 80% is debugging it. What&#x27;s interesting is this effect is scalable. Like you see this when comparing countries by population but the same distribution (general shape) exists when looking at populations of states&#x2F;regions&#x2F;cities (zooming in for higher resolution) for any given country or even down to the street level.> Obviously every situation is different, but people seem to be pretty okay with relinquishing agency on these things and just going along with whatever local maxima their org operates in. It&#x27;s not totally their fault, but they&#x27;re not blameless either.I agree here, to the letter. I don&#x27;t blame low level employees for maximizing their local optima. But there&#x27;s two main areas (among many) that just baffles me. The first is when this is top down. When a CEO and board are hyper focused on benchmarks rather than the evaluation. Being unable to distinguish the two (benchmarks and metrics are guides, not answers). The other is when you have highly trained and educated people actively ignoring this situation. To have an over-reliance on metrics and refusing to acknowledge that metrics are proxies and considering the nuances that they were explicitly trained to look for and is what meaningfully distinguishes them from less experienced people. I&#x27;ve been trying to coin the term Goodhart&#x27;s Hell to describe this more general phenomena because I think it is a fairly apt and concise description. The general phenomena seems prolific, but I agree that the blame has higher weight to those issuing orders. Just like a soldier is not blameless for their participation in a war crime but the ones issuing the orders are going to receive higher critique due to the imbalance of power&#x2F;knowledge.Ironically I think we need to embrace the chaos a bit more. But that is rather in recognizing that ambiguity and uncertainty is inescapable rather than abandonment of any form of metric all together. I think modern society has gotten so good at measuring that we often forget that our tools are imprecise whereas previously the imprecision was so apparent that it was difficult to ignore. One could call this laziness but considering its systematic I&#x27;m not sure that&#x27;s the right word. reply pseudalopex 4 hours agorootparent> To have an over-reliance on metrics and refusing to acknowledge that metrics are proxies and considering the nuances that they were explicitly trained to look for and is what meaningfully distinguishes them from less experienced people. I&#x27;ve been trying to coin the term Goodhart&#x27;s Hell to describe this more general phenomena because I think it is a fairly apt and concise description.McNamara fallacy or quantitative fallacy.[1][1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;McNamara_fallacy reply JohnMakin 12 hours agoparentprev> This often creates a situation where people need to \"justify\" their jobs.On DevOps teams I see this constantly. Usually the best-compensated or most senior \"Ops\" guy or whatever they&#x27;re called at the company spends a lot of his time extinguishing fires that were either entirely of his own creation&#x2F;incompetence, which makes it look like he&#x27;s \"doing something.\" You automate away the majority of the toil there and this person doesn&#x27;t have a job, yet this pattern is so insanely common. There&#x27;s little incentive to do it right when doing it right means management thinks you sit there all day and do nothing. reply slv77 12 hours agorootparentSometimes people want to be firefighters to protect the people they serve but others love how being in a crisis makes them feel alive. The later are why arson investigators first look at firefighters when doing an arson investigation. Some of them need the fires and will start them just to fight them.There are a lot of these types that gravitate to crisis management roles like DevOps. reply godelski 12 hours agorootparentprevThat&#x27;s a fantastic example of what I&#x27;m trying to describe. It&#x27;s kinda like thinking hours worked is directly proportional to widgets produced. There certainly are jobs and situations where this relationship holds (can&#x27;t sell widgets if you aren&#x27;t manning the store or can&#x27;t produce turn crank widgets if the crank isn&#x27;t being turned). But modern world widgets don&#x27;t work that way and are more abstract. Sometimes fewer hours creates more widgets, sometimes the reverse. But widget production is now stochastic and especially in fields where creativity and brain power are required. (Using widgets for generalization -- in the economic sense--, insert any appropriate product or ask and I&#x27;ll clarify) reply tstrimple 12 hours agorootparentprevWhich is one reason DevOps teams don&#x27;t make sense. DevOps is a skill developers need to have. It needs to be embedded within the development team, not some other team&#x27;s responsibility who only focuses on \"DevOps\" work. You create the build and deployment pipelines and move on to other project work. If you give someone a role and say their job is to do \"DevOps\" they will HAVE to invent things to do because that&#x27;s such a small part of a project and once implemented doesn&#x27;t need a ton of maintenance. reply ponector 11 hours agorootparentIt is not about DevOps. In every organization if you do your work good and have no fuckups - you will not be recognized and promoted. But if you are hero-firefighter - managers will love you and help with promotion.Because visibility is a key! Key to everything in the corporate life. If your work is not visible for managers - you are doing nothing. reply godelski 5 hours agorootparent> It is not [just] DevOpsDefinitely agree. In a sister comment[0] I mention a concept I&#x27;ve been calling \"Goodhart&#x27;s Hell\" for a more general term. But I think what you are specifically mentioning is sometimes laughably called \"Loud Laboring\"[1]. I think this all falls under the broader umbrella of metric hacking and thus Goodhart&#x27;s Law.Idk why, but it really does seem like metric hacking is extremely pervasive in our modern society, and can be found nearly everywhere. What upsets me the most is that it too is found in the sciences. There also appears to be a strong correlation between the popularity (or hype) of a field and metric hacking.[0] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38647582[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37147707 || https:&#x2F;&#x2F;www.cnbc.com&#x2F;2023&#x2F;08&#x2F;09&#x2F;forget-quiet-quitting-loud-l... reply JohnMakin 8 hours agorootparentprevThe fortunate thing is there are measurable metrics you can hit and improve upon as an X-Ops team, but implementing those takes buyin from the org or enough freedom to go rogue and do it on your own. reply godelski 5 hours agorootparent> The fortunate thing is there are measurable metrics you can hit and improve upon as an X-Ops teamQuestion(s):- What are the metrics?- How aligned are the metrics with the actual goal?- What isn&#x27;t covered by the metrics?- How is what&#x27;s not covered by metrics evaluated?- Can what&#x27;s not currently covered by metrics theoretically be covered by some potentially unknown metric? (best guess) reply tstrimple 8 hours agorootparentprevOf course visibility is key. What other kind of option is there?> “We have no idea what you did this year and no one has any idea of the value you’re providing, but what the hell. Here’s a huge bonus!”That doesn’t seem a little insane to you? reply godelski 5 hours agorootparent> What other kind of option is there?Placing value based on the work assigned? Obviously insanely easier said than done. But I think we should embrace the fuzziness a bit. Your manager should have a very good idea of this. If they don&#x27;t, then they&#x27;re the ones that should be let go because this is a significant aspect of their job. (may be learned through indirect methods)> That doesn’t seem a little insane to you?Honestly, a lot of business practices and economics sounds insane to me. Similarly a lot of alternative suggestions (especially the latter) since they tend to not address the underlying issues but be bandaids. The ones that don&#x27;t sound insane are often boring and very reasonable, but I think we&#x27;ve established that that&#x27;s considered undesirable when you&#x27;re evaluated by visibility. I think this is probably a significant part of the negative feedback loop. replyphilk10 13 hours agoparentprevWhen I work with new devs I can often trip them up with basic tests of double-clicking, leading spaces, using the back button on Android. They then learn these and from then on these issues dont appear ( well, OK, it might take a couple of times of a ticket being rejected because of these but they do quickly learn all my tricks ) I don&#x27;t get measured on bugs found so there&#x27;s no pressure on me to find stupid bugs just to boost my figures. reply godelski 12 hours agorootparent> I don&#x27;t get measured on bugs found so there&#x27;s no pressure on me to find stupid bugs just to boost my figures.Sounds like the right incentive structure. If you don&#x27;t mind, how are you judged? Do you feel like the system you&#x27;re in is creating the appropriate incentives and actually being effective? Certainly this example is but I&#x27;d like to know more details from an expert so I can update my understanding. reply philk10 11 hours agorootparentThe system is really effective, I wanted to work at a place where the cliche \"everyone cares about quality\" is actually true and I found it - devs test, designers test, I test, customer has the chance to test the latest build every 2 weeks so that we can check that our quality checks are aligning with theirs. It gets to be a game of &#x27;can the devs get it past my checks&#x27; and &#x27;can I find new ways to trip them up&#x27; which builds up confidence in each others skill levels. reply godelski 10 hours agorootparentThat&#x27;s cool to hear. I also like the periodic table on your team&#x27;s site haha. Looks like great philosophy reply hotpotamus 13 hours agorootparentprevI had a buddy who liked to do that kind of thing. I think his favorite trick was just to enter nothing in a form and hit enter to see what happens. It&#x27;s probably his favorite because it deleted the database on some little thing I wrote at one point and we got a good laugh over it and I got a good lesson out of it. reply philk10 12 hours agorootparentYep, I start off by entering nothing, then just spaces, then special characters and then finally get to entering some typical data reply bsder 11 hours agoparentprev> Shouldn&#x27;t businesses be operating like the latter? Constantly training?\"But then they&#x27;ll leave for somewhere else for more money.\" Literally every company I have worked for. Meaningful training was always an uphill battle.However, good training also requires someone good and broad in your technical ladder. They may not be the most up-to-date, but they need to be able to sniff out bullshit and call it out.FAANG is no exception, either. reply amlozano 13 hours agoprevThis point is brought up in the article but I think it is at the real heart of the issue.QA is almost always seen as a &#x27;cost center&#x27; by the business and upper management. I have a hypothesis that you never ought to work in a department that is seen as a &#x27;cost center&#x27;. The bonuses, the recognition, and the respect always goes to the money makers. The cost center is the first place to get more work with less hands, get blamed for failures, and ultimately fired when the business needs to slim up. I think the same thing applies to IT.This spiral is why QA will always be a harder career than just taking similar skills and being a developer. It self reinforces that the best people get fed up and switch out as soon as they can. reply serial_dev 12 hours agoparentEven as a developer (mobile app developer) I feel like one has to be careful not to work on \"cost center\" things.Accessibility, observability, good logging, testing infrastructure improvements, CI&#x2F;CD tweaks, stability, better linting and analyzer issues are all important, but you will be rewarded if you ship features fast.This year I spent too much time on the former because I felt like that&#x27;s what the team and app needed, because nobody on the team priorized these issues, and I&#x27;ll be sweating at the end of the year performance reviews.Now knowing this, I understand why the others didn&#x27;t want to work on these items, so next year, I&#x27;ll be wiser, and I&#x27;ll focus on shipping features that get me the most visibility.Sorry for the bugs in the app, but I need a job to pay my mortgage. reply tstrimple 11 hours agorootparentThe purpose behind all those things you were pursuing (apart from accessibility) should have been to increase the rate at which the team is able to ship features. If your work on these items over the course of a year haven&#x27;t demonstrably improved delivery speed, then what value did they actually bring? If they have improved delivery speed and you can show evidence for that, why would you be nervous going into a review? reply hj3939393d 10 hours agorootparent> demonstrably improved delivery speedThinking this can be reduced to a single metric is the blight of modern software (and business in general, I think). Mapping an individual change to improved delivery speed is in the vast majority of cases an impossible task and any decent developer knows this. It&#x27;s management+ that wants simple easy metrics since they lack the deep understanding required to do their job well. Software development is - despite management&#x27;s hopes - not line work. It&#x27;s much more akin to R&D. The line work gets eaten up by AWS. reply tstrimple 8 hours agorootparentIf you can’t prove your contributions are worthwhile, you’re not going to get recognition. People putting out fires get recognition because they are solving visible and urgent problems. If your work reduces the chances of those fires, it should be measurable otherwise what is the point? Do you honestly believe someone who has spent a year “improving processes” but cannot measure the impact of that year of work deserves glowing praise?And it’s not that managers don’t understand developers and the work they do. It’s that a lot of developers don’t engage at all with what a business actually does. They are working at IKEA while trying to convince management to use the nice dovetail joints instead of that garbage dowel based assembly. Not only do dovetails look better, they are substantially stronger! All very true statements from a craftsman woodworker. But a complete failure to understand the business and how and why it operates as it does and the value they are expected to provide within it. reply bumby 12 hours agoparentprevI agree with the &#x27;cost center&#x27; sentiment, but I&#x27;ll try to add some nuance from my experience.1) Some organizations have come to really value what QA&#x2F;QC brings to the table. From my experience, this seems to be more visible in manufacturing than software. I speculate this is because software is more abstract by its very nature and waste is harder to track.2) The really good QAs are those who really believe in its mission, rather than those who are looking for the path of least resistance.Both of those underscore the value lies in organizations and individuals who really buy-in to the QA ethos. There are lots of examples of both who are simply going through the motions. reply NegativeK 8 hours agoparentprevI&#x27;ve worked in cost center groups almost all my life, and I wouldn&#x27;t trade it for anything.I&#x27;m fine with bonuses, etc going to other groups. I&#x27;m paid well as a tech worker, and many of those jobs would make absolutely miserable -- assuming I turned out to actually be good at them. reply perlgeek 12 hours agoparentprev> I have a hypothesis that you never ought to work in a department that is seen as a &#x27;cost center&#x27;That&#x27;s why I don&#x27;t work in an IT department of a traditional business. reply robofanatic 12 hours agoparentprev> QA is almost always seen as a &#x27;cost center&#x27; by the business and upper managementWell everything involved in making a product is seen as a cost, that includes the entire development team - QA, Developers, Devops, PM .... reply rubidium 12 hours agorootparentNo. That’s not actually how most orgs break it down. R&D, marketing, sales is “bringing new business” so are profit centers. This means their budget grows with revenue. Manufacturing, QA, IT and service are cost centers so get squeezed year-over-year even if revenue is flat. reply jdlshore 5 hours agorootparentIt depends on the org. In my company, which is a SaaS-like company, all of product and engineering is a cost center, despite creating the product the company sells. It’s just the way they do their accounting. reply itsdrewmiller 5 hours agorootparentprevWhat software companies are not classifying QA as R&D? reply dclowd9901 4 hours agoparentprevI work on our frontend platform team and my perf work, CI design and process engineering definitely have a harder time getting recognition and promotions than folks who ship things that translate to dollars in the bank.I don’t care though. I enjoy making things better and more robust. It makes my soul feel better. I’ll leave fucking things up to the cynics. reply guhcampos 11 hours agoprev“ To these folks, it feels like giving a damn is a huge career liability in your organization. Because it is.”And it’s easy to see why.Software Quality, Cose Maintainability, Good Design. These things only matter if you are planning to work on that company for a long time. If you’re planning to stay a couple years then hop to the next company, the most optimal path is to rise fast by doing high visibility work, then find use your meteoric rise as a resume material to get a higher paying job. Rinse and repeat. If that project is going to break or become unmaintainable in a couple years, who cares? You’re not going to be there.Recognize the pattern? Startups work the same. It’s the “growth mindset” imprinted everywhere. If this product becomes unmaintainable in 5 years, who cares? I will have exited and cashed in.I don’t judge people who do that exactly because it’s the practice the companies themselves use. I don’t like it, I actually hate it, but I understand people are just playing by the rules.The fun part is watching managers and executives complaining about employee turnover, lack of company engagement, quiet quitting, like this isn’t them tasting their own poison. reply BoxFour 11 hours agoparent> Startups work the same. If this product becomes unmaintainable in 5 years, who cares?This is a reasonable stance for a startup to take. The majority of startups likely won&#x27;t last five years as they tend to fail.Being alive in five years with technical debt is a good problem for most startups to have, because that means they managed to make it five years. reply guhcampos 7 hours agorootparentExactly. That’s why I say I don’t like it, but understand it. I enjoy the fast paced and highly creative environments of startups more than the politics and bureaucracy of corporations, but the short term vision bothers me a lot. The result is I choose to work for midsized companies, or established startups. I kind of specialized in working on the growing pains of companies in their first hundred of engineers.There’s a lot to say about startup culture and the growth mindset, but I don’t consider it necessarily evil. It exists, lots of the products we use and love would be impossible to build without it. It can be extremely harmful, though. It burns out people, it leads to excessive risk taking, it favors aggressive, invasive marketing, it rewards reckless management - yet it works.It isn’t good or evil, like mostly everything in the World. It’s just… there. reply hinkley 10 hours agoparentprevWe&#x27;ve been through snake oil and pyramid schemes and now we have settled on depeche mode. reply righthand 13 hours agoprevQA Engineers are some of the best debuggers too. They have their hands in the pipeline, src and test directories, and often work with all aspects of developing and deploying the application.When I was a QA lead I often ran into software engineers that couldn’t be bothered to read a pipeline error message (and would complain daily in Slack) and when it came to optimizing the pipeline they would ignore base problems and pretend the issues that stemmed from the base problems were magical and not understood. Wasting days guessing at a solution.The disrespect a QA engineer sees is not exaggerated in this article. Since most companies with QA orgs do not have a rigorous interviewing process like the Engineering orgs, the QA engineers are seen as lesser. The only SWE that have respect for them that I’ve met are the people who worked in QA themselves. The disrespect is so rampant that I myself have switched back to the Engineering org (I tried using seniority as a principal engineer and even shifted as a manager to make changes, but this failed because Engineering could not see past their own hubris and leadership peoples will not help you). My previous company before I was laid off hired a new CTO who claimed we could just automate away QA needs but had no examples of what she was talking about. This is the level of respect poured down from the top about building good software. reply wmichelin 13 hours agoprevThis might be my personal experience, but I&#x27;ve never encountered a QA team that actually writes the tests for engineering.I have only had QA teams that wrote \"test plans\" and executed them manually, and in rarer cases, via automated browser &#x2F; device tests. I consider these types of tests to be valuable, but less so than \"unit tests\" or \"integration tests\".With this model, I have found that the engineering team ends up being the QA team in practice, and then the actual QA team often only finds bugs that aren&#x27;t really bugs, just creating noise and taking away more value than they provide.I would love to learn about QA team models that work. Manual tests are great, but they only go so far in my experience.I&#x27;m not trying to knock on QA folks, I&#x27;m just sharing my experience. reply yabones 13 hours agoparentFrom what I&#x27;ve seen, the value in QA is product familiarity. Good QA&#x27;ers know more about how the product actually works than anybody else. More than PM&#x27;s, more than sales, and more than most dev teams. They have a holistic knowledge of the entire user-facing system and can tell you exactly what to expect when any button gets pushed. Bad QA&#x27;ers are indeed a source of noise. But so are bad devs, sysadmins, T1&#x2F;2 support, etc. reply taylodl 13 hours agorootparentTo your point, the QA team is the customer&#x27;s advocate. As you say, they know the product, from the customer&#x27;s perspective, better than anyone else in the development organization.Where I&#x27;ve seen QA teams most effective is providing more function than \"just\" QA. I&#x27;ve seen them used for 2nd tier support. I&#x27;ve seen them used to support sales engineers. I&#x27;ve also seen QA teams that take their manual test plans and automate their execution (think Selenium or UiPath) and have seen those automations included in dev pipelines.Finally, the QA team are the masters and caretakers of your test environment(s), all the different types of accounts you need for testing, they should have the knowledge of all the different browsers and OSes your customers are using, and so forth.That&#x27;s a lot for the dev team to take on. reply kgermino 13 hours agorootparentThat also means they test from a different perspective than the dev does. If I get a requirement my build is based on my understanding of the requirement, and so is my testing.A separate QA person coming at it from the customer&#x27;s perspective will do a test that&#x27;s much more likely to reflect reality. reply cableshaft 13 hours agorootparentprev> Good QA&#x27;ers know more about how the product actually works than anybody else. More than PM&#x27;s, more than sales, and more than most dev teams.Not disagreeing with this, but there&#x27;s one thing they won&#x27;t always be aware of. They won&#x27;t always know what code a dev touched underneath the hood and what they might need to recheck (short of a full regression test every single time) to verify everything is still working.I know that the component I adjusted for this feature might have also affected the component over in spots X, Y, and Z, because I looked at that code, and probably did a code search or a &#x27;find references&#x27; check at some point to see where else it&#x27;s getting called, and also I usually retest those other places as well (not every dev does, though. I&#x27;ve met some devs that think it&#x27;s a waste of time and money for them to test anything and that&#x27;s entirely QA&#x27;s job).A good QA person might also intuit other places that might be affected if it&#x27;s a visible component that looks the same (but either I haven&#x27;t worked with too many good QA people or that intuition is pretty rare, I&#x27;m guessing it&#x27;s the latter because I believe I have worked with people who were good at QA). Because of that, I do my best to be proactive and go \"oh by the way this code might have affected these other places, please include those in your tests\". reply hysan 13 hours agorootparent> They won&#x27;t always know what code a dev touched underneath the hood and what they might need to recheck (short of a full regression test every single time) to verify everything is still working.This is a good point, but there are some QA that do review code (source: me - started career in QA and transitioned to dev). When making a test plan, an important factor is risk assessment. If QA has a hunch, or better when the dev lead flags complex changes, the test plan should be created and then the code diffs should be reviewed to assess whether or not the plan needed revising. For example, maybe the QA env doesn’t have a full replica of prod but a query is introduced that could be impacted if one of the joining tables is huge (like in prod). So maybe we’d adjust the plan to run some benchmarks on a similar scale environment.I’m definitely biased since I started in QA and loved it. To me, good QA is a cross section of many of the things people have mentioned - technical, product, ops, security - with a healthy dash of liking to break things. However, reality is that the trend has been to split that responsibility among people in each of those roles and get rid of QA. Works great if people in each of those job functions has the bandwidth to take on that QA work (they’ll all have a much deeper knowledge of their respective domains). But you’ll lose coverage if any one of those people don’t have time to dedicate to proper QA.(I’ll also completely acknowledge that it’s rare to have a few, let alone a full team, of QA people who can do that.) reply asadotzler 11 hours agorootparentprev>They won&#x27;t always know what code a dev touched underneath the hood and what they might need to recheck (short of a full regression test every single time) to verify everything is still working.Not really. As QA I always reviewed the checkins since yesterday before opening up the daily build. Between the bug comments and the patch comments, even if the patch itself is a bit Greek to me, I can tell what was going on enough to be a better tester of that area. reply giantrobot 12 hours agorootparentprev> Not disagreeing with this, but there&#x27;s one thing they won&#x27;t always be aware of. They won&#x27;t always know what code a dev touched underneath the hood and what they might need to recheck (short of a full regression test every single time) to verify everything is still working.It doesn&#x27;t necessarily matter what code was changed, a change in code in Module A can cause a bug in Module B that hasn&#x27;t been changed in a year. A QA test plan should cover the surface area of the product as used by consumers whoever they might be. While knowing some module had fixes can inform the test plan or focus areas when the test schedule is constrained, only testing changes is the road to tears. reply cableshaft 12 hours agorootparentTest plans never account for everything, at least in my experience, especially edge cases. And it&#x27;s rare that I&#x27;ve seen any QA team do a full regression test of the entire site. There&#x27;s only been a few times where I&#x27;ve seen it authorized, and that&#x27;s usually after a major refactoring or rewrite.I&#x27;m not in QA, I write code, so I defer to whatever they decide for these things usually, these are just observations from what I&#x27;ve seen.I just try to make sure I test my code enough that there isn&#x27;t anything terribly broken when I check it in and fixes I need to make tend to be relatively minor (with a few exceptions in my past).Also I&#x27;m not necessarily talking basic functionality here. I&#x27;m currently working for a client that&#x27;s very picky about the look and feel, so if a few pixels in padding get adjusted where it&#x27;s noticeable, or a font color or size gets adjusted a bit, in one place and it affects something else, there could be complaints. And a test plan is not likely to catch that, at least not any on any projects I&#x27;ve worked on. reply bbarn 13 hours agorootparentprevThis is a great model, until those people so familiar with the business needs end up.. doing business things instead. It&#x27;s really hard to keep people like that in a QA role once the business recognizes their value. Kind of the same problem with QA automation people - once they become really good at test automation, they are effectively software developers, and want to go there. reply importantbrian 13 hours agorootparentI think that&#x27;s a compensation problem more than anything else. I&#x27;ve known some QA folks who enjoyed QA and would have stayed in that role if they could have justified the massive differential in comp between QA and SWE or product development. If we valued QA and compensated it at the same level we do those other roles then there would be a lot less difficulty retaining good QA folks. reply taurath 11 hours agorootparentprevI have never once heard of a problem that QA folks end up in project or product management too often, and almost always have the problem of not being able to escape the QA org despite many years. Most companies are extremely resistant to people moving tracks, especially from a “lower status” org like QA or CS. It’s the exception not the rule. reply wmichelin 13 hours agorootparentprevAgreed! I did have some good experiences at my last job with the QA team, but it was definitely a unique model. They were really a \"Customer Success\" team, it was a mix of QA, sales, and customer support.These \"Customer Support\" reps, when functioning as QA, knew the product better than product or eng, exactly how you&#x27;re describing. I did enjoy that model, but they also did not write tests for us. They primarily executed manual test plans, after deploys, in production. They did provide more value than creating noise, but the engineering team still was QA, at least from an automated test standpoint. reply mbb70 12 hours agorootparentWe had no dedicated QA, but would consistently poach \"Customer Success\" team members for critical QA work for the exact reasons your listed. Worked quite well for us.Especially for complex products that are based on users chaining many building blocks together to create something useful, devs generally have no visibility into how users work and how to test. reply hasoleju 13 hours agorootparentprevI completely agree with that. It really comes down to having the right skills as a QA person. If you don&#x27;t know how the product is used and only click on some buttons, you will never reach the states in the software that real users reach and therefor you will also not be able to reproduce them. reply IKantRead 13 hours agoparentprevA good QA person is to a software developer as a good editor is to a writer. Both take a look at your hard work and critique it ruthlessly. Annoying as hell when it&#x27;s happening, but in my experience well worth it because the end result is much higher quality.I might just be too old, but I remember when QA people didn&#x27;t typically write tests, they manually tested your code and did all those weird things you were really hoping users wouldn&#x27;t do. They found issues and bugs that would be hard to universally catch with tests.Now we hoist QA on the user.Working with younger devs I find that the very concept of QA is something that is increasingly foreign to them. It&#x27;s astounding how often I&#x27;ve seen bugs get to prod and ask \"how did it work when you play around with it locally?\" only to get strange looks: it passed the type checker, why not ship it?Programmer efficiency these days is measured in PRs&#x2F;minute, so introducing bugs is not only not a problem, but great because it means you have another PR you can push in a few days once someone else notices it in prod! QA would have ruined this. reply bumby 12 hours agorootparent>Now we hoist QA on the user.This drives me crazy. It&#x27;s a cheap way of saying we&#x27;re ok shipping crap. In the past, I&#x27;ve been part of some QA audits where the developers claimed their customer support log sufficed as their test plan. This wasn&#x27;t safety-critical software, but it did involve what I would consider medium risk (e.g., regulatory compliance). The fact that they openly admit they are okay shipping bad products in that environment just doesn&#x27;t make sense to me. reply me_smith 12 hours agoparentprevHello. I am QA that writes tests for engineering. Technically, my title is a Software Development Engineer in Test (SDET). Not only do I write \"test plans\", I work on the test framework, infrastructure and the automation of those test plans.Every company is different on how they implement the QA function. Whether it be left to customer, developers, customer support, manual only QA, or SDET. It really comes down to how much leadership values quality or how leadership perceives QA.If a company has a QA team, I think the most success comes when QA get involved early in the process. If it is a good QA team, they should be finding bugs before any code is written. The later they are involved, the later you find bugs (whether the bugs are just \"noise\" or not) and then the tighter they get squeezed between \"code complete\" and release. I think that the QA team should have automation skills so more time is spent on new test cases instead of re-executing manual test cases.Anyways, from my vantage point, the article really hits hard. QA are sometimes treated as second class citizens and left out of many discussions that can give them the context to actually do their job well. And it gets worse as the good ones leave for development or product management. So the downward spiral is real. reply righthand 13 hours agoparentprevThis is because there is no formal way to run a QA org, people get hired and are told to “figure it out”. Then as other posters said the other orgs ignore the QA org because they have no understanding of the need. What you’re describing is a leadership problem, not a QA usefulness problem. reply philk10 13 hours agoparentprevthe engineering team are usually great at writing tests that test their code, a good QA can test alongside them to find cases they&#x27;ve missed and issues that automated code tests can&#x27;t find. The QA person doesn&#x27;t have to spend time checking that the app basically works, they can be confident in that and spend their time testing for other &#x27;qualities&#x27; But yes, I&#x27;ve known QA teams that will only find bugs that no one cares about or are never likely to happen - often because they are not trained on the product to be able to dig deep reply e28eta 13 hours agorootparentIt seems so obvious to me that your typical engineer, who spent hours &#x2F; days &#x2F; whatever working on a feature, is never going to test the edge cases that they didn’t conceive of during implementation. And if they didn’t think of it, I bet they’re not handling it correctly.Sometimes that’ll get caught in code review, if your reviewer is thinking about the implementation.I’ve worked in payroll and finance software. I don’t like it when users are the ones finding the bugs for us. reply philk10 12 hours agorootparentI started off as a dev, wanted to change to being a tester&#x2F;QA but was told by the CEO that \"the customers are better at finding bugs than we are so just give the app a quick look over and ship it out\" - I left soon after that. reply yungporko 13 hours agoparentprevyeah my experience is basically the same, usually if a place has qa at all, it&#x27;s one person in a team who doesnt have an adequate environment or data set to test with and they effectively end up just watching the developer qa their own work and i end up screaming into a pillow every time i see \"Tester: hi\" pop up on my screen.the one exception to this was when i was qa (never again) and i made sure we only ever did automated tests. unfortunately management was nonexistent, devs made zero effort to work with us, and naturally we were soon replaced by a cheap offshore indian team who couldn&#x27;t tell you the difference between a computer and a fridge anyway.i think a lot of it just stems from companies not caring about qa, not knowing who to hire, and not knowing what they want the people they hire to achieve. \"qa\" is just like \"agile\", where nobody can be bothered to actually learn anything about it, so they make something up and then pat themselves on the back for having it. reply JohnFen 13 hours agoparentprevThe type of testing QA should be doing is different from the type of testing that devs should be doing. One doesn&#x27;t substitute for the other. reply trealira 12 hours agorootparentI remember Steve Maguire saying this in Writing Solid Code (that they&#x27;re both necessary, and both types of testing complement the other). He criticized Microsoft employees who relied on QA to find their bugs. He compared QA testing to a psychologist sitting down with a person and judging whether the person is insane after a conversation. The programmer can test from the inside out, whereas QA has to treat the program like a black box, with outputs and effects resulting from certain inputs. reply sghiassy 13 hours agoparentprevI share the same experience that the QA team writes test plans not and “code level” testsThat said, those test plans are gold. They form the definition of the product’s behavior better than any Google Doc, Integration Test, or rotating PM ever could. reply scruple 3 hours agoparentprevWe have SDETs for that. And they do a great job. But QA is where polish happens. When you get good QA people, who know the app better than the developers, better than the users, who anticipate how the users will use the product? These people should be paid their weight in gold. reply dylan604 12 hours agoparentprevUnit tests are great when you provide data that the methods expect and are sane. It&#x27;s not until users get in front of the UI and submit data that you never even thought about testing with your unit tests.To me, unit tests are great to ensure the code doesn&#x27;t have silly syntax errors and returns results as expected on the happy path of coding. I would never consider that QA no matter how much you randomize the unit test&#x27;s input.Humans pushing buttons, selecting items, hover their mouse over an element, doing all sorts of things that have no real reason but yet they are being done anyways will almost always wreck your perfect little unit tests. Why do you think we have session playback now, because no matter what a dev does to recreate an issue, it&#x27;s never the exact same thing the user did. And there&#x27;s always that one little WTF does that matter type of thing the user did without even knowing they were doing anything.A good QA team are worth their weight in $someHighValueMineral. I worked with one person that was just special in his ability to find bugs. He was savant like. He could catch things that ultimately made me look better as the final released thing was rock solid. Even after other QA team members gave a thumbs up, he could still find something. There were days were I hated it, but it was always a better product because of his efforts. reply tunesmith 9 hours agorootparentUnit tests are used to test functions that have only defined inputs, and whose outputs depend only on those inputs.You can extract a lot of business logic into those kinds of functions. There&#x27;s a whole art in writing \"unit testable code\". Those unit tests have value.What&#x27;s left is the pile code and scenarios that need to be tested in other ways. But part of the art is in shrinking down that pile as much as possible. reply macksd 13 hours agoparentprevI&#x27;ve seen good QA teams who own and develop common infrastructure, and can pursue testing initiatives that just don&#x27;t fit with engineering teams. When developing a new feature, the team developing it will write new tests to cover the functionality of the feature, and will own any failures in those tests moving forward. But while they&#x27;re doing that, the QA team is operating a database of test failures, performance metrics, etc. that can provide insight into trends, hot spots needing more attention, etc. They&#x27;re improving the test harnesses and test frameworks so it&#x27;s easier for the engineering teams to develop new tests quickly and robustly. While the engineering team probably owns all of the unit tests and some integration tests - a dedicated QA team focuses on end-to-end tests, and tests that more accurately recreate real world scenarios. Sometimes there are features that are hard to test well because of non-deterministic behavior, lots of externalities, etc., and I think QA should be seen as an engineering specialty - sometimes they should collaborate with the feature teams to help them do that part of their job better and teach them new testing techniques that may be appropriate that perhaps aren&#x27;t obvious or common.I would also second another comment that pointed out that good QA folks often know the real surface area of the product better than anyone. And good QA folks also need to be seen as good QA folks. If you have a corporate culture that treats QA folks like secondary or lesser engineers, that will quickly be a self-fulfilling prophecy. The good ones will leave all the ones who fit your stereotype behind by transitioning into dev roles or finding a new team. reply earth_walker 12 hours agoparentprevI work with the regulated drug development industry, and believe there is a useful and important distinction between Quality Control (QC) and Quality Assurance (QA). I wonder if perhaps this distinction would be useful to software quality too.QC are the processes that ensure a quality product: things like tests, monitoring, metrology, audit trails, etc. No one person or team is responsible for these, rather they are processes that exist throughout.QA is a role that ensures these and other quality-related processes are in place and operating correctly. An independent, top level view if possible. They may do this through testing, record reviews, regular inspections and audits, document and procedure reviews, analyzing metrics.Yes, they will probably test here and there to make sure everything is in order, but this should be higher level - testing against specifications, acceptability and regulatory, perhaps some exploratory testing, etc.Critically they should not be the QC process itself: rather they should be making sure the QC process is doing its job. QA&#x27;s value is not in catching that one rare bug (though they might), but in long term quality, stability, and consistency. reply marcelr 13 hours agoparentprevWeird, I have had the opposite experience that most shit slips through the cracks of automated testing & manual testing by an experienced QA is 10x more effective. reply closeparen 13 hours agoparentprevIn my company the engineering team mostly writes unit tests. Then there was a weekly manual QA exercise where the oncall engineer followed a checklist with our actual mobile app on an actual phone before it went to the store. When this started to take almost the entire day, we hired a contract workforce for it. The contract workforce is in the process of automating those tests, but the most important ones still get human eyes on. reply solardev 12 hours agoparentprevI&#x27;ve only ever had an official QA team in one job, at a Fortune 1000. When I started we didn&#x27;t have anyone yet, but eventually they hired an mid-manager from India and brought him over (as in relocated his whole family). He then brought on a QA person he had worked with previously.I did not work well with the mid-manager, who was both my new boss and the QA person&#x27;s (not too relevant here). However, I do give him credit for the person he hired.That QA person, a young Indian woman with some experience, was actually phenomenal at her job, catching many mistakes of ours both in the frontend and in the APIs.She not only did a bunch of manual testing (and thus discovered many user-facing edge cases the devs missed), she wrote all the test cases (exhaustively documented them in Excel, etc. for the higher-ups), AND the unit tests in Jest, AND all the end-to-end tests with Playwright. It drastically improved our coverage and added way more polish to our frontend than we otherwise would&#x27;ve had.Did she know everything? No, there was some stuff she wasn&#x27;t yet familiar with (namely DOM&#x2F;CSS selectors and Xpath), and it took some back-and-forth to figure out a system of test IDs that worked well enough for everyone. She also wasn&#x27;t super fluent with the many nuances of Javascript (but really, who is). There was also a bit of a language barrier (not bad, but noticeable). Overall, though, I thought she was incredible at her job, very bright, and ridiculously hard-working. I would often stay a little late, but she would usually be there for hours after the end of the day. She had to juggle both the technical dev&#x2F;test tasks, the cultural barriers, and managing both up and across (as in producing useless test case reports in Excel for the higher ups, even though she was also writing the actual tests in code), dealing with complex inter-team dynamics, etc.I would work with her again any day, and if I were in management, I&#x27;d have promoted the heck out of her, trained her in whatever systems&#x2F;languages she was interested in learning, or at least given her a raise if she wanted to stay in QA. To my knowledge the company didn&#x27;t have a defined promotion system though, so for as long as I was there, she remained QA :( I think it was still better than the opportunities she would&#x27;ve had in India, but man, she deserved so much more... if she had the opportunities I did as an American man, she&#x27;d probably be a CTO by now. reply bumby 12 hours agoparentprevFWIW, I have seen that same model have some success, provided management is willing to stand-up for QA. When QA isn&#x27;t actively writing tests, they can still provide some balance against human biases that tend toward following the easiest path. In these cases, QA provides an objective viewpoint and backstop to cost and schedule pressures that might lead to bad decisions. This might be most valuable on safety-critical code, but I suppose it can still apply at various levels of risk.I&#x27;ve seen where this has went poorly as QA was slowly eroded. It became easier and easier to justify shoddy testing practices. Low-probability events don&#x27;t come around often by their very nature and it can create complacency. I&#x27;ve seen some aerospace applications have some close calls related to shortcomings in QA integration; in those cases, luck saved the day, not good development practices. reply spookie 13 hours agoparentprevAt least from my knowledge of the gaming world there are QA devs who do find the issues and fix them if they&#x27;ve the ability to do so, point out code that should be taken a look at, and all of that. I find it extremely valuable to have another set of eyes in the code with a very more focused perspective, sometimes different from the dev. reply joe_the_user 12 hours agoparentprevThe Q&A teams I&#x27;ve seen worked the way you describe initially except they were valuable.They weren&#x27;t there for engineering, they were there for product quality. Their expertise was that they knew what the product was supposed to do and made it did it. Things like \"unit tests\" help development but they don&#x27;t make sure the product satisfies client requirements.If engineering is really on top of it, they learn from QA and QA seems to have nothing to do. But don&#x27;t let that situation fool you into thinking they are \"just creating noise and taking away more value than they provide\" reply acdha 11 hours agoparentprevI think that complicates conversations like this. I’ve seen a range of QA people ranging from the utterly incompetent to people who knew the product and users better than anyone else to people writing code and tackling gnarly performance or correctness issues.If your company hires the low end of that scale, any approach is going to have problems because your company has management problems. It’s very easy to take a lesson like “QA is an outdated concept” because that’s often easier than acknowledging the broken social system. reply senderista 13 hours agoparentprevAnecdotally, from my time on Windows Vista I remember an old-school tester who didn&#x27;t write any code, just clicked on stuff. From what I could tell, in terms of finding serious bugs he was probably more valuable than any of the SDETs who did write code. His ability to find UI bugs was just amazing (partly due to familiarizing himself with the feature specs, I think, and partly due to some mysterious natural ability). reply rwmj 12 hours agoparentprevThat&#x27;s only your personal experience because our QE team at Red Hat spend a very large amount of their time coding new tests or automating existing ones. They use this framework: https:&#x2F;&#x2F;avocado-vt.readthedocs.io&#x2F;en&#x2F;latest&#x2F;Introduction.htm... reply wombat-man 13 hours agoparentprevMicrosoft used to test this way, at least in the team I worked with. SDEs still wrote unit tests. But SDETs wrote a lot of automated tests and whatever random test tools that ended up being needed. The idea was to free up more SDE time to focus on the actual product.I think that era is over after the great SDET layoffs of 2014&#x2F;2015? Now I guess some SDE teams are tasked with this kind of dev work. reply g051051 13 hours agoparentprevThe last two organizations I worked for had full QA teams with people who wrote the tests, not just test plans. The devs sometimes provided features to facilitate it, but the QA teams were the ones that constructed the tests, ran them, and decided if the software was ready to be released. Some things had manual tests, but a large percentage was fully automated. reply shados 13 hours agoparentprev> but I&#x27;ve never encountered a QA team that actually writes the tests for engineering.I have a few times. But the only common thing in the QA industry, is that every company does it differently and think they&#x27;re doing it the \"normal way\". reply BiteCode_dev 13 hours agoparentprevFrameworks like playright can record as code user actions and you can replay them in a test.So you can make your QA teams create plenty of tests if you give them the right tools. reply kredd 13 hours agorootparentAbsolutely hellish to maintain once you get into territories of different localization, A&#x2F;B testing, engineering teams that constantly change small things in UI (for valid reasons). I think it might work on a product that changes once a year, but not sure about maintainability past that.Every time I worked with dedicated QA teams, we (core engineering) ended up leading the effort of writing maintainable automated tests. At some point, it was much easier to just do it ourselves. reply solardev 12 hours agorootparentWell, that&#x27;s kinda the point, isn&#x27;t it? Have the tests run after each PR, and it flags all the changes for you. For the intentional ones, that means the tests have to be updated (often just changing a line or two). But it also catches all your unintentional changes that you didn&#x27;t mean to break.This is especially important in complex UIs where everything from localization to state management to A&#x2F;B testing can accidentally break things you didn&#x27;t even mean to touch.Isn&#x27;t it better for the devs to spend a few minutes looking at a broken test (which they can easily fix themselves) than for a user to discover it and have to go through triage months later, when it&#x27;s out of everyone&#x27;s memories already? reply kredd 12 hours agorootparentRealistically the build will fail, but since QAs used some action capturing thing, devs will have to either recapture the same flow or delegate it back to QA. Now you have the pipeline blocked, QAs most likely working on some other stuff so they need to reprioritize, devs are also annoyed as their build is not going through.I completely understand there are pros and cons to every set up, but if higher velocity is more important than complete correctness, such set up might be detrimental. reply solardev 12 hours agorootparentIn our setup, we had the ability to either fix the test ourselves, comment out those tests as useless, or just force a build and bypass any hooks if we needed to force a new deploy and we were absolutely certain the tests aren&#x27;t important. But at least they would be caught, even if they served as a soft warning rather than a forcible barrier.> but if higher velocity is more important than complete correctness, such set up might be detrimental.I guess it&#x27;s a cultural value thing: move fast and break things vs polish before shipping. As a user I always prefer the latter, but hell, I don&#x27;t run these multibillion dollar companies that are always shipping buggy things, lol reply kredd 12 hours agorootparentTotally agreed, it really depends on the case. I’ve been user of both types of products where I value extreme correctness (think of anything financial, or core software that I want to never crash like a browser), but also really don’t care if something like Spotify or Twitter doesn’t work 5% of the time I use it. Obviously that barrier is different for everyone, but that’s where the product development planning and prioritization comes into play. replychopin 13 hours agorootparentprevIn my experience such tests are brittle as hell. reply Osmose 13 hours agorootparentYou&#x27;re not wrong, but a good, well resourced QA org can both help write or develop more flexible tests, and also help fix brittle tests when they do break. The idea of brittle tests that break often being a blocker is predicated on practices like running every type of test on every commit that exist to deal with a lack of QA effort in the first place.Maybe recorded integration tests are run on every release instead of every commit? Maybe the QA team uses them less to pass&#x2F;fail work and more to easily note which parts of the product have changed and need attention for the next release? There&#x27;s lots of possibilities. reply Kinrany 2 hours agorootparent> Maybe recorded integration tests are run on every release instead of every commit?That would limit the frequency of releases. reply convolvatron 13 hours agoparentprevin the classic model, most QA orgs were a useless appendage. partially by construction, but largely because QA gets squeezed out when dev is late (when does that happen?). they aren&#x27;t folded in early, so they twiddle their thumbs doing &#x27;test infrastructure&#x27; and &#x27;test plans&#x27;, until they finally get a code drop and a 48 hr schedule to sign off, which they are under extreme pressure to do.but every once and a while you ran across a QA organization that actually had a deep understanding of the problem domain, and actually helped drive development. right there alongside dev the entire way. not only did they improve quality, but they actually saved everyone time. reply lambic 13 hours agorootparentNot sure why this was downvoted, that second paragraph is right on the money. reply cratermoon 13 hours agorootparentprevSaying \"useless appendage\" sounds to me like it&#x27;s the QA team that&#x27;s the problem, when what you&#x27;re really saying is that it&#x27;s the organization and process that pushed QA teams into irrelevance. I agree with your assessment overall, and those issues were one of the driving forces behind companies dispensing with QA and putting it all on the developers. reply refulgentis 13 hours agoparentprevI was surprised by the opposite of this (after entering my first real job at Google, after startup founder => seller.)People wrote off QA completely unless it meant they didn&#x27;t have to write tests, but, it didn&#x27;t track from my (rather naive) perspective that tests are _always_ part of coding.From that perspective, it seemed QA should A) manage go&#x2F;nogo and manual testing of releases B) keep the CI green and tasks assigned for red (bonus points if they had capacity to try fixing red) C) longer term infra investments, ex. what can we do to migrate manual testing to integration testing, what can we do to make integration testing not-finicky in the age of mobileI really enjoyed this article because it also indicates the slippery slide I saw there: we had a product that had a _60% success rate_ on setup. And the product was $200+ dollars to buy. In retrospect, the TL was into status games, not technical stuff, and when I made several breakthroughs that allowed us to automate testing of setup, they pulled me aside to warn me that I should avoid getting into it because people don&#x27;t care.It didn&#x27;t compute to me back then, because leadership _incessantly_ talked about this being a #1 or #2 problem in quarterly team meetings.But they were right. All that happened was my TL got mad because I kept going with it, my skip manager smiled and got a bottle of wine to celebrate with, I got shuffled off to work with QA for next 18 months, and no one ever really mentioned it again. reply alex_lav 13 hours agoparentprev+1 to this. My whole career has been spent with QA teams that behave this way.I worked at a place with a 10 year old legacy product and a 10,000 test case spreadsheet of each manual action a QA tester must perform on that product to greenlight any individual change. Obviously this lead to huge wait times to get anything deployed. Also was pretty amusing to catch all the bugs in production that their exhaustive spreadsheet totally overlooked. Almost as though it did not have value in the first place. reply tootie 13 hours agoparentprevI think the thing missing from a lot of these conversations is what problem domain you&#x27;re working in. The more \"technical\" your problem domain is the more valuable automated testing will be over manual. For almost anything based on user experience and especially mass-market customer-facing products, human QA is far more necessary.In either case, the optimal operating model is that QA is embedded in your product team. They participate in writing tickets, in setting test criteria and understanding the value of the work being done. \"Finding bugs\" is a low value task that anyone can do. Checking for product correctness requires a lot more insight and intuition. Automated test writing can really go either direction, but typically I&#x27;d expect engineers to write unit tests and QA to write e2e tests and only as much or as little as it actually saves time and can satisfactorily indicated success or failure of a user journey. reply robotnikman 13 hours agoprevMicrosoft is probably the one case where this sticks out the most, at least for me anyways. Noticeably more bugs in updates since they dropped their QA team, in Windows as well as cloud products. reply EvanAnderson 13 hours agoparentThe revenue keeps rolling in so, clearly, they made the right business decision... >sighThe revenue keeps rolling in so, clearly, they made the right business decision... >sighI wish there was some kind of consequence to them for their declining quality.If people keep insisting on throwing money at them no matter how bad their software is, then there&#x27;s no reason for them to improve their quality. reply nlavezzo 11 hours agoprevWhen we built FoundationDB, we had a maniacal focus on quality and testing. So much so that we built it in a language we invented, called Flow, that allowed us to deterministically simulate arbitrary sized FDB clusters and subject them to crazy conditions, then flag each system property violation and be able to perfectly reproduce the test run that triggered the violation.We got to a point where the default was that all of our 10,000&#x27;s of test runs each night would flash green if no new code was introduced. Tests that flashed red were almost always due to recent code additions, and therefore easily identified and fixed. It let our team develop knowing that any bugs they introduced would be quickly caught, and this translated to being able to confidently take on crazy new projects - like re-writing our transaction processing system post-launch and getting a 10x speed increase out of it.In the end our focus on quality led to velocity - they weren&#x27;t mutually exclusive at all. We don&#x27;t think this is an isolated phenomenon, which led us to our newest project - but that&#x27;s a story for another time. reply JonChesterfield 10 hours agoparentIs this the language in question? https:&#x2F;&#x2F;github.com&#x2F;apple&#x2F;foundationdb&#x2F;blob&#x2F;main&#x2F;flow&#x2F;README.... reply nlavezzo 9 hours agorootparentYep! reply oaththrowaway 13 hours agoprevI had a boss at Yahoo who gave our QA to another team because \"Facebook doesn&#x27;t use QA, we shouldn&#x27;t either\". I can&#x27;t remember if it was Facebook or MS, but he was willing to buy all of us a book talking about how amazing it was.Long story short, it wasn&#x27;t. It was like taking away a crutch. Of course we could have been more diligent about testing before having QA validate it, but it slowed development down so much trying to learn all the things we never thought to test that QA did automatically. reply robocat 11 hours agoparentAn article about Facebook&#x27;s reason for no QA with some of the mitigations:https:&#x2F;&#x2F;blog.southparkcommons.com&#x2F;move-fast-or-die&#x2F;A bit recent to have affected Yahoo - but it sells a good story. We would celebrate the first time someone broke something. Let anyone touch any part of the codebase and get in there to fix a bug or build a feature. Yes, this can cause bugs. Yes, that is an acceptable tradeoff. We had a shared mythology about the times that the site went down and we all rallied together to quickly bring it back up.Sounds like hell: running as close to the edge of the cliff as you can. Presumably totally ignoring thousands of papercuts of slightly broken functionality. Optimising to produce an infinite number of shallow bugs. reply quadrifoliate 5 hours agorootparentThe difference is that bugs in the social network parts of Facebook (the ones where you see your friends and family&#x27;s pictures and posts) are not directly making money for Facebook. The only real stuff that matters for the money is all the tracking.I bet the people responsible for Facebook Ads Manager are a lot less enthusiastic about \"move fast and break things\", although I&#x27;d be interested to hear an opposing viewpoint from anyone here who&#x27;s worked for that group. reply bgribble 13 hours agoprevI was lucky enough to work in a small eng team with 1 full-time dedicated QA person. One of the very few coworkers from my long career that I have really tried hard to poach away from whatever they were doing after our shared workplace went bust.Yes, part of the job was to write and run manual test suites, and to sign off as the DRI that a certain version had had all the automated and manual tests pass before release.But their main value was in the completely vague mandate \"get in there and try to break it.\" Having someone who knows the system and can really dig into the weak spots to find problems that devs will try to handwave away (\"just one report of the problem? probably just some flaky browser extension\") is so valuable.In my current job, I have tried for 5+ years to get leadership to agree to a FT QA function. No dice. \"Developers should test their own code.\" Yeah and humans should stop polluting the ocean and using fossil fuels, how&#x27;s that going? reply ncphil 12 hours agoparent\"Developers should test their own code\" is emblematic of a juvenile mindset in people who regularly fire up their \"reality distortion field\" to avoid the effort of educating themselves on their own operations (and that helps them deny responsibility when things go South). As W. Edwards Deming, bane of all \"gut instinct\" executives, once wrote, \"The consumer is the most important part of the production line. Quality should be aimed at the needs of the consumer, present and future.\" The lack of a dedicated quality team shows a lack of respect for your customers. You know, the people you need to buy your products or services (unless you&#x27;re intent on living off VC loans until you have to pull the ripcord on your golden parachute). reply fatnoah 13 hours agoprevIn making the case for building up a QA org at my current startup, I repeat the mantra that QA is both a skillset and a mindset. Automated tests can tell us a lot, but skilled QA testers are amazing at edge cases to break things and providing human feedback about what looks and feels good gor users. reply temuze 13 hours agoprevI strongly disagree.I worked at a company with a world-class QA team. They were amazing and I can&#x27;t say enough nice things",
    "originSummary": [
      "Getting rid of quality assurance (QA) teams in software development can have negative consequences.",
      "Automating tasks and optimizing processes may lead to neglecting the importance of testing.",
      "QA roles are crucial for effectively managing software quality, including defect tracking, bug triage, defect investigation, focus on quality, and end-to-end testing. Ignoring these activities can have detrimental effects.",
      "Recognition and support for those who prioritize quality in software development organizations are essential."
    ],
    "commentSummary": [
      "QA teams play a crucial role in software development, and relying solely on developers for testing has drawbacks.",
      "Various testing approaches, such as bug bashes and fuzz tests, are explored in the discussion.",
      "The limitations and challenges faced by QA teams, including undervaluation and lack of training and collaboration, are highlighted, underscoring the need to prioritize quality assurance and thorough testing in software development."
    ],
    "points": 395,
    "commentCount": 242,
    "retryCount": 0,
    "time": 1702581358
  },
  {
    "id": 38640406,
    "title": "Barcelona Supercomputing Center Introduces Sargantana: New Open-Source RISC-V Chip",
    "originLink": "https://www.bsc.es/news/bsc-news/bsc-presents-sargantana-the-new-generation-the-first-open-source-chips-designed-spain",
    "originBody": "The third generation of the Lagarto family of processors, designed entirely at BSC, represents a step forward in the development of high-performance European chips. Based on RISC-V open hardware technology, the new processor is an important advance to achieve European technological sovereignty and reducing dependence on large multinational corporations The BSC, Europe's leading developer of open source computing technologies, is committed to turning Barcelona into an innovation hub specialising in semiconductor design The Barcelona Supercomputing Center - Centro Nacional de Supercomputación (BSC-CNS) presented on Wednesday the new Sargantana chip, the third generation of open source processors designed entirely at the BSC. The development of Sargantana is a crucial step forward in reinforcing BSC's leading position in RISC-V open source computing technology research in Europe. Sargantana (the name of the lizard in Aragonese and Catalan) is the third generation of the Lagarto processors, the first open source chips developed in Spain, in the framework of the DRAC project (Designing RISC-V-based Accelerators for next generation Computers), and is one of the most advanced open source chips in Europe at the academic level. The new Sargantana features better performance than its two predecessors - Lagarto Hun (2019) and DVINO (2021) - and is the first processor in the Lagarto family to break the gigahertz barrier in operating frequency. The fact that the instruction set architecture (ISA) of these new processors is open source, and therefore non-proprietary and accessible to all, reduces technological dependence on large multinational corporations by enabling innovation through the collaboration of companies and institutions without the limitations of proprietary architectures. The RISC-V free hardware architecture, on which these new chips are based, could bring about a technological revolution in the hardware world like Linux did in the software world. \"The launch of Sargantana is a further step forward in the development of European RISC-V based technology, an embryo of the future European high-performance processor. This open hardware will be vital to ensure technological sovereignty and maintain European industrial competitiveness, and consolidates the BSC's role as a pioneer in Europe in the introduction of open source for chip design,\" said BSC director Mateo Valero. In 2017, the European Union identified the lack of own hardware as one of the main vulnerabilities, due to the risk of industrial espionage posed by an over-reliance on chips designed and produced outside Europe, especially in the United States, Taiwan, China, Japan and South Korea. The BSC was then tasked by the EU to lead the scientific development of future European chips to provide the market with an open and local alternative, suitable for high-performance computing, artificial intelligence, the automotive sector and the internet of things. Joint work coordinated by the BSC Researchers from other universities and research centres such as the Centro de Investigación en Computación del Instituto Politécnico Nacional de México (CIC-IPN), the Centro Nacional de Microelectrónica (CNM-CSIC), the Universitat Politècnica de Catalunya (UPC), the Universitat Autònoma de Barcelona (UAB), the Universitat de Barcelona (UB) and the Universitat Rovira i Virgili (URV) have participated in the development of Sargantana. The project was coordinated by BSC researcher Miquel Moretó, who highlighted the advantages of open-source semiconductor design to enable collaboration between companies and academic institutions around the world. \"The new Sargantana chip is freely available to all, enabling a new era of processor innovation through open collaboration in which anyone, anywhere can benefit from RISC-V technology,\" he said. Moretó pointed out that Sargantana is an experimental chip, a research prototype that will allow us to test applications with RISC-V technology and deepen our knowledge, but it is not yet designed to be used in computers or other devices. \"We are developing a technology that will allow Spain and Europe to design their own increasingly competitive processors in the future, in addition to training future professionals in a sector that will undoubtedly add great value to the production chain,\" added Moretó, who highlighted the joint efforts of Catalonia, Spain and Europe to have European technology made in Barcelona and to train engineers in this field. European leadership in RISC-V This objective is in line with the idea pointed out by Mateo Valero of making Barcelona an international benchmark in processor design. \"We have the talent, the technological knowledge and the scientific environment necessary for Barcelona and its surroundings to be able to compete with any institution or region in the world and become a Design Valley that drives the creation of companies and new jobs\", said Valero. The Sargantana project has received funding from the European funds from the ERDF Operational Programme of Catalonia 2014-2020, with the support of the Catalan Government, within the framework of the DRAC project. The results obtained will be used in the Strategic Transformation Project for the semiconductor sector, known as PERTE Chip, the one with the highest investment of all the industrial transformation projects approved by the Spanish Government. The Sargantana presentation ceremony was held at the BSC facilities in Barcelona as part of the first day of the Spanish Open Hardware Alliance (SOHA), an association that brings together Spanish universities and research centres with the aim of promoting research in the area of Open Computer Technology and Architecture, thus contributing to the training of talent that allows the creation of high quality jobs.",
    "commentLink": "https://news.ycombinator.com/item?id=38640406",
    "commentBody": "Barcelona Supercomputing Center presents Sargantana: new open-source RISC-V chipHacker NewspastloginBarcelona Supercomputing Center presents Sargantana: new open-source RISC-V chip (bsc.es) 357 points by pimterry 22 hours ago| hidepastfavorite142 comments lifeisstillgood 21 hours agoI would love to see a clear roadmap from the EU (not been successful searching)My take on this is1. this is less about competitiveness at the cutting edge and more about security and economic on-shoring2. building chips on-shore at the 40-20nm level massively reduces risk, increases the likelihood smaller states can build locally and solves for most chip needs3. chips we need are rarely the cutting edge AI stuff. The vast volume of chios will go in as controllers on screens, USB connectors and so on. Building plug and play alternatives will give local manufacturers choices, and incentives will help.4. the big win is security. Does the CEO of sensitive company, the head of security services and the general in charge of procurement use keyboards, cpus motherboards and monitors made from open source chips manufactured in a trusted nation? What is the BOM for the challenger tank - how many chips in there that are made by whom and ...the process is long and arduous and the risks are huge.But we make tanks from steel other materials made in \"favoured nations\" - surely the same applies to silicon? reply cduzz 20 hours agoparentMy understanding is that a 40nm fab is only economically viable if it&#x27;s spent the first several years of its life producing high margin chips.In other words; the life cycle of a 40nm fab is: 1997: start building fab 2000: fab goes online and starts producing CPUs 2006: fab upgraded 2012: fab switches from CPUs to video and memory controller chip sets 2018: fab switches to USB controllers and embedded chips 2019: fab offline for 2 months because an antiquated but critical part is broken and is only brought back online because another similarly old fab went offline and sold off their parts 2020: fab shut off because of covid 2021: fab found to be a write-off because too many things broke while fab was offline.So if you skip straight past the profitable phase, you end up spending billions of dollars to make a fab that makes $0.30 parts, and it&#x27;ll never be profitable unless those parts are $10 each, which in turn makes the product they&#x27;re in unprofitable. reply FirmwareBurner 20 hours agorootparentYou are correct. Building fabs today only for fabbing much older nodes will not be profitable. You have to target 22nm and below otherwise you can&#x27;t afford to jump in the semi fab ring. reply hajile 20 hours agorootparentTSMC is building a lot of new 28nm production with plans to shut down all their older nodes and move everyone over in the next few years.GlobalFoundries (formerly AMD fabs) created a brand-new 22nm planar process specifically for older chips as an upgrade to other company&#x27;s 28nm processes.Profits seem possible if you approach it the right way. reply FirmwareBurner 18 hours agorootparentWe&#x27;re talking about different things here. I was talking about building new fabs for 28nm nodes and you&#x27;re talking about TSMC upgrading existing fabs from older nodes to 28nm production.Of course upgrading an existing older \"sunk-cost\" fab to 28nm production will be profitable, but not building a new one from scratch just for that same older node. reply AnthonyMouse 10 hours agorootparentBut now this makes the subsidies angle make more sense: You subsidize initial construction and then the domestic plant remains online indefinitely because the construction is a sunk cost and the incremental cost of upgrades over time is sustainable. reply throwup238 18 hours agorootparentprevThe math works out a lot better when you’re upgrading pre-EUV fabs or expanding an existing facility. A lot of the gear and setup is mostly the same such as wafer cleaning, HVAC and isolation, etc and the local challenges to setup and labor have been figured out. reply cduzz 20 hours agorootparentprev\"But I&#x27;ve got a product that&#x27;s certified with this part that&#x27;s running on a 40nm process that has these specifications that are deeply tied to features of that 40nm process; things like voltage ranges and temperature tolerances! If you force me to switch to a comparable but not identical part at 22nm I&#x27;ll have to re-certify my widget with 18 different regulatory agencies!\" reply Someone 15 hours agorootparentIf those are your needs, you order all the parts you need over your product’s lifetime up-front or get (= pay for) a contract with the manufacturer that makes them promise to sell you the parts for X years (they probably wouldn’t keep producing old parts, but would stockpile enough of them to be able to deliver working ones years later)(Or you prepare for having to go to eBay for working parts. https:&#x2F;&#x2F;www.nytimes.com&#x2F;2002&#x2F;05&#x2F;12&#x2F;us&#x2F;for-parts-nasa-boldly-...) reply kjs3 10 hours agorootparentThere are companies (I&#x27;ve used Rochester Electronics) that both stockpile and manufacture legacy chips specifically for the long tail support situations. reply zozbot234 20 hours agorootparentprevYou will always have pure analog electronics and other bespoke things that basically don&#x27;t benefit from anything finer than these nodes. Even for digital chips, it makes no sense to use leading edge nodes for very simple logic where a lot of the area is just contact pads. reply cf1241290841 14 hours agorootparentRelevant to mention MEMS (micro-electromechanical systems) in this context, which use much older nm tech. Be it digital micro mirror devices¹ or gyros². Or photo&#x2F;laser diodes.Given the physical limitations, as well as the problems we have with code base security it might be time to aim for cheaper production of something in the region of 180nm instead.Looking at how old much of the standard weaponry used today is (TOW 50 years with an actual physical gyroscope, Javelin still 25 years³), the demand from the military alone should cover the initial cost. Especially if you look at the ludicrous prices western countries payed for even dumb artillery shells.¹ Texas Instruments DMD from a DLP projector from @AppliedScience https:&#x2F;&#x2F;youtu.be&#x2F;9nb8mM3uEIc?t=428² Explanation of MPU-6050 from @BreakingTaps https:&#x2F;&#x2F;youtu.be&#x2F;9X4frIQo7x0?t=664³ Teardown of both from @lelabodemichel5162 https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=s7-6hgX7-zQSorry for late edits reply jacquesm 19 hours agorootparentprevIt&#x27;s not about what you can do or can&#x27;t do. It is about what you can do profitably and that&#x27;s a completely different thing. reply voakbasda 17 hours agorootparentI have to wonder if the ability to profit depends entirely on the established cartel of semiconductor manufacturers. They determine the current prices of chips in the marketplace.If entering that marketplace requires competing with them, then I am not sure anyone that is not already in the market can ever win. The margins are too low and the startup costs are too high.Government intervention seems to be the only possible solution, and that option hardly sounds viable when considering that cartel’s collective lobbying power. reply cduzz 17 hours agorootparentI don&#x27;t think this is a \"cartel of semiconductor manufacturers\" so much as it&#x27;s been a \"shambolic cluster of organizations running crappy old fabs into the ground producing cheap chips that were subsidized by a prior decade&#x27;s worth of very expensive products.\"I can afford to sell gazillions of chips at $0.08 per chip if I&#x27;m running a fab I didn&#x27;t pay to build. I&#x27;m only (barely) paying for the inputs. When Stan, the last guy who understands how to run the widget verifier, or Elaine, the last lady to understands how to run the polishing machine retire, I&#x27;ll have to close up shop.Those $0.08 per chip devices have been absurdly subsidized in that a replacement infrastructure to make them would require that they cost $10 per device, and the ecosystem of things built on $0.08 chips isn&#x27;t viable in a $10 per chip world.In order to have a fab make $0.03 per unit devices, you first have to have the fab spend 10 years making $300 per unit devices, regardless of the underlying node size of those $300 per unit devices.Likely you couldn&#x27;t even go back and make a fab that makes large volumes of 60nm-90nm node sizes at all, for any amount of money, because the equipment to do this (new) hasn&#x27;t been made in 2 decades and no company is willing to invest the money to make new crappy old equipment.It&#x27;s not a nefarious oligopoly as much as a synchronized \"run the asset to failure\" lifecycle of the infrastructure.How much does it cost to make a 300 year old tree? reply cf1241290841 11 hours agorootparent>Likely you couldn&#x27;t even go back and make a fab that makes large volumes of 60nm-90nm node sizes at all, for any amount of money, because the equipment to do this (new) hasn&#x27;t been made in 2 decades and no company is willing to invest the money to make new crappy old equipment.I believe your argument assumes that there is a fixed cost to produce even 180nm or 350nm ICs that hasnt changed since the first one was produced.We still need 300 years for a 300 year old tree, but 25 year old technology might now be relatively easy to build if we start from scratch.What was high tech then might be relatively easy to solve now. One example might be https:&#x2F;&#x2F;github.com&#x2F;circuitvalley&#x2F;USB_C_Industrial_Camera_FPG... being open source instead of a multi year, multi million dollar project. reply cduzz 11 hours agorootparentYes, my argument is that producing at industrial scales even chunky nodes requires enormous capital expenditures and may be impossible without rebuilding large chunks of an antiquated and abandoned supply chain.Even if it is 10% the cost of making the each of the individual components involved in making a relatively simple 90nm chip, you&#x27;re still looking at vast costs.If you&#x27;re talking about making 30 chips in a university fab, sure, I&#x27;ll concede that it is \"possible\" but if you&#x27;re talking about propping up an industry built on products that require a herd of standardized \"$0.30\" parts made on legacy 90nm fabs, that ship has sailed.Update your BOM and recertify or raise your costs by an order of magnitude. reply cf1241290841 11 hours agorootparentFirst off, you are definitively making a very solid point, cost for getting mass production right are a killer once the institutional knowledge is gone. For example, its very visible in the field of battery technologies if i am not mistaken. Going from lead to lithium was a gigantic task and the inertia going forwards hasnt reduced enough at this point.But realistically this is a matter of going back far enough, to lower the cost far enough? 10% are a good start but to stick to the topic, physical gyroscopes from decades ago are now replaced with MEMS ICs where the reduction in cost is magnitudes more then down to 10%. At a certain point the reduced cost makes it viable. The question is just has it been long enough?While we wont get 90nm cheap enough, the question is what can we do on a hobby level (vs academia)? Because going from there (neglectable cost and technological requirements) to mass production will at some point be cheaper then the cost of setting up reproducible tooling for older high tech systems.I am likely still off with 180nm, but there should be a level at which this makes economical sense. A level that gets cheaper to reach with technological progress &#x2F; time. reply zozbot234 10 hours agorootparentprevThe problem I see with this argument is that there are plenty of fabs making trailing-edge devices, some of which aren&#x27;t even that old. It even seems to be part of the established path for countries and locations more generally that seek to bootstrap a semiconductor industry of their own. They get started with the simplest and coarsest nodes, then go finer step by step. Even TSMC got their start that way. So it seems like a pretty robust industry to me, I&#x27;m not seeing the argument for a crisis. reply cf1241290841 10 hours agorootparentPersonally i cant follow this line of reasoning. In the end this is an economical argument, as they still buy machines from the same manufacturer. At that point its a matter of being able to deliver and create a market for ICs with the given machines. Which is often achieved through political will and subsidizes to get to that point.My initial argument is that while you cant compete with ASML products in 2023, you will be able to economically compete with some of their older products once you go back far enough. reply photonbeam 15 hours agorootparentprev> How much does it cost to make a 300 year old tree?Aside from your main point, I found this an interesting thought exercise thinking about cost of air, sunlight, soil, water and then 300 years of security reply Qwertious 7 hours agorootparentI imagine if you&#x27;re going to grow one 300 year old tree, then your best bet is obscurity. Find a stable very-rural area that&#x27;s not prone to bushfires, plant one tree and make sure it&#x27;s doing well for a few years, come back 300 years later, you&#x27;re done.If you&#x27;re not going the obscurity path then you&#x27;d really want to scale it up - there&#x27;s not much difference between security for one tree and security for 100 trees. reply jacquesm 17 hours agorootparentprevThe capital expense on a new fab is crazy. There may be a cartel factor but that usually would work to the advantage of the manufacturers, so that doesn&#x27;t seem to be the case here. reply mardifoufs 17 hours agorootparentprevThere&#x27;s no real cartel for older nodes. It&#x27;s not even really possible considering how many fabs exist and how many players are operating those older fabs. reply cf1241290841 11 hours agorootparentNumber of producers of these fabs is still quite limited though. reply janekm 19 hours agorootparentprevBut you can only really make those profitably for a few industries (military, medical, seismic come to mind). The EU does have the chip fabs for those industries, of course... reply phkahler 18 hours agorootparent>> But you can only really make those profitably for a few industriesI think it&#x27;s more like they&#x27;re only profitable if the equipment is already paid for. And even then the margins may be low. reply Workaccount2 20 hours agorootparentprevThere might be an argument then that it would be worth it for the state to take the hit. If shit hits the fan and you have zero semi-manufacturing, then you are going to be pretty screwed. reply qwytw 17 hours agorootparent> If shit hits the fan and you have zero semi-manufacturing, then you are going to be pretty screwed.I don&#x27;t really understand this claim at all. Chips are not exactly fungible, unless you force your local companies to use you \"state sponsored chips\" in their products just being able to produce \"chips\" wouldn&#x27;t be that useful. What are you going to do with them? reply 15155 16 hours agorootparentGuide munitions if needed. reply black_puppydog 19 hours agorootparentprevSo the cost of building a fab hasn&#x27;t come down in the last decades, huh? Genuinely asking, is there some^W^W^W what is the \"uncompressible\" cost in fab-fabbing? I&#x27;d totally guess that staff and the building itself are not it? reply georgeecollins 10 hours agorootparentprevBut if you were a country or an alliance that wanted to be 1000% sure you always had access to a component (drone parts) you might be willing to pony up billions to make sure you could not be blockaded or embargoed. I don&#x27;t know if that makes sense but given what is going on in Ukraine and the Mid East, people have to be thinking about that. reply mcbits 12 hours agorootparentprevSounds like there is a need for investment into innovation beyond just building the next-generation fab for $2^x billion. Bringing the cost of a new less-advanced fab down from $2 billion to $100 million, and then building 20 of them, could also be profitable (though less exciting). There is a national economy that&#x27;s actually been growing quite well for a few decades now by applying that general idea to other industries. reply BiteCode_dev 19 hours agorootparentprevNot all ventures need to be profitable. The EU may decide to take a loss on this solely for strategic reasons. reply qwebfdzsh 17 hours agorootparent> strategic reasonsSuch as? I can&#x27;t really think of any benefit besides providing jobs and funding for contractors (so kicks backs etc.)Then again it&#x27;s not particularly surprising, the EU is well know for wasting massive amounts of money on all sorts of nonsense while ignoring things that actually matter. reply mbauman 17 hours agorootparentThere&#x27;s both supply-chain and runtime security. reply cduzz 14 hours agorootparentDon&#x27;t forget the MBAs willing to burn it all down to juice the Q2 profits. reply KerrAvon 14 hours agorootparentprevHave you looked at a Pentagon budget lately? It&#x27;s entirely welfare for defense contractors. reply RobotToaster 20 hours agorootparentprevHow does an entire semiconductor factory become FUBAR from being offline for a year? reply cduzz 19 hours agorootparentThe example is hypothetical, but complex machines can be complex to keep running, and often suffer catastrophically when shut down.If the fab was barely profitable before shutting down, it doesn&#x27;t take much to total it. Fabs are full of machines that cost tens of millions of dollars when they were new and there are simply no spare parts of vendor support for them now, and you can&#x27;t just swap in a modern replacement. Fabs are full of extremely sensitive environments (no dust here, acid that will kill you if you touch it there, constant temperatures, no humidity, etc). If any of that is compromised, it&#x27;s now just a toxic waste dump.Again, I have no specific knowledge in this domain, but I imagine most of the time the owner&#x27;s happy enough just to walk away from the headache. reply tyingq 16 hours agorootparentThere&#x27;s also the brain drain aspect. All the process engineers and techs that understood all the various \"recipes\", quirks, etc, of the various machines moved on to other work.A new crew will eventually work it out, but there&#x27;s a lot of trial and error getting to the right bake time&#x2F;temps, spin rpm, etc, etc. Yield and rework suffers while they do that. reply JAlexoid 13 hours agorootparentprevDust is the simplest example.Once you shut off the dust extraction, you may just end up with too much dust collected in the equipment to make it utterly useless. reply TheCondor 18 hours agorootparentprevNot an expert, but there are additional start up costs that need to be spent to “start it up.” With any significant downtime, those could eat up any possible profit unless it’s a newest technology fab. reply londons_explore 19 hours agoparentprev> What is the BOM for the challenger tank - how many chips in there that are made by whomIn today&#x27;s world, it would seem more sensible to just stockpile enough of all the components for 5-7 years of tank production, knowing that if your enemy tries any evil tricks then you have half a decade to figure out how to redesign or make the components yourself.Keep a close eye on anything that looks like an antenna and it isn&#x27;t so bad having the enemy backdooring your chips either. reply jes 19 hours agorootparentThis has been my take as well. There is a lot of disruption in a company when a key part, like the FPGA that serves as a communications nexus in the product goes EOL and everyone scrambles for a year trying to engineer in a replacement.Buy enough parts for expected product life, make good use of the time you didn&#x27;t waste on scrambling, and when your product is EOL sell any left-over parts on the secondhand markets. reply qwytw 18 hours agoparentprev> about security and economic on-shoring > increases the likelihood smaller states can build locally and solves for most chip needsI&#x27;m not sure what does that mean? What specific chip needs that would that solve and what benefits would this provide? If those chips are not competitive nobody would buy them? So what would governments do with them? Stockpile them for the future just &#x27;in case&#x27;?The problem is that unlike grain or oil chips are not exactly fungible if your military production or other vital industries lose access to their current suppliers they wouldn&#x27;t be able to use your slow, outdated and overpriced chips anyway (and forcing them to do that under normal circumstances would make your products less competitive).> BOM for the challenger tankHow many other components does the Challenger tank contain (IIRC it&#x27;s not really produced anymore anyway) which are not manufactured in the UK? In any case stockpiling necessary chips etc. just in case the UK won&#x27;t able be able to acquire anything from the US&#x2F;Germany&#x2F;etc. seems like a practical approach than trying to develop everything inside the country. reply mastax 19 hours agoparentprevI agree that often the less cutting edge chips are important but doesn’t the EU already have that handled with ST Microelectronics, NXP, Infineon? What’s lacking is very high end CPU, GPU, high end memory, high end FPGA. reply ksec 20 hours agoparentprev>building chips on-shore at the 40-20nm>the process is long and arduous and the risks are huge.Plenty of 28nm+ chips Fabs are inside EU. And more are coming online. This isn&#x27;t a long or arduous process.Edit: Should have been Plenty of 28nm and above. As the original quote state. reply FirmwareBurner 20 hours agorootparent>Plenty of sub 28nm chips Fabs are inside EU.Which are those \"plenty\" sub-28nm fabs exactly?AFAIK only Global Foundries Dresden goes down to 22nm and 12nm, and I think that&#x27;s by far the most cutting edge fab currently in EU, making the Ryzen IO dies and other such things.But even TSMC&#x27;s future Dresden fab starting construction next year(hopefully) will start making mostly automotive chips for NXP, Bosch and Infineon chips at 28nm and 22nm all the way in 2027(!), with plans to go to 16nm and 12nm in the further future.Your view on EU cutting edge semi fabrication seems very optimistic. reply wiz21c 20 hours agorootparentand TSMC is not exactly a european company... reply FirmwareBurner 20 hours agorootparentOf course they weren&#x27;t gonna export their crown jewels outside of Taiwan, the same way how the west didn&#x27;t export their crown jewels to Asia when they did the technology transfers for semiconductor manufacturing in the &#x27;70s, making sure to keep their Asian partners at least a node behind.Well well, how the turn-tables. reply toyg 20 hours agorootparentEverything gets out in the end. My Italian hometown had a \"golden age\" of silk manufacturing for a while, thanks to bugs smuggled out of China. It lasted for a couple of decades and then they were again smuggled out to other Italian towns. And then of course you have the nuclear shenanigans.If European countries wanted the tech bad enough, they would find ways to get it. The problem is not the know-how but the massive investments needed to productize it. reply FirmwareBurner 18 hours agorootparent>The problem is not the know-how but the massive investments needed to productize it.Are you telling me the EU, the richest block in the world, has less money to spend on fabs than TSMC, as if the EU is scrapping for change behind the couch cushions.If only you knew how much money the EU wastes through various useless and vanity projects that accomplish nothing except getting certain well connected people rich, we could have built 3x TSMCs.But unlike Taiwan, we&#x27;re lacking in visionary well educated tech leaders, and drowning in clueless politicians and established gentrified industry players who lobby the funds go to their projects instead. reply toyg 13 hours agorootparent> Are you telling me the EU, the richest block in the world, has less money to spend on fabs than TSMCI didn&#x27;t say we don&#x27;t have the money, but that it&#x27;s a problem to commit the money. It&#x27;s basically the norm that EU countries unanimously agree that \"something should be done\" on a certain issue, but then disagree on how much it should cost and where the money should come from. This gets more and more complicated the bigger the cost is (and this is an expensive idea) and the farther we are from the regular 7-year-budget process (it was last agreed in 2020, so jockeying for big items will probably resume in 2025-26).I don&#x27;t disagree on the overall lack of vision in European political classes (hardly a fault of the EU, it&#x27;s common to basically all countries and all levels of government), but even a visionary leader would have to work hard to get agreement on such a big project. reply dataking 17 hours agorootparentprev> Are you telling me the EU, the richest block in the world, has less money to spend on fabs than TSMCThat could very well turn out to be the case in practice, not for lack of money, but inability to provide the promised subsidies according to Financial Times:https:&#x2F;&#x2F;www.ft.com&#x2F;content&#x2F;898454ba-8fc2-4b00-a14f-5f9ee152d... reply FirmwareBurner 15 hours agorootparentHaving a company an industry dependent on generous subsidies from states is a race to the bottom. TSMC will just pit you against other countries on the basis of \"which one of you is gonna give us more of your tax-payers&#x27; money and we&#x27;ll build our fab there\" reply qwytw 17 hours agorootparentprev> the funds go to their projects instead.To me it just seems like relying on government funding to drive innovation in sectors where private companies have incentives to compete is extremely foolish. reply JAlexoid 13 hours agorootparentprevEm.... ASML, a Dutch company, produces the tech behind these nodes.It&#x27;s a question of supply chains - not tech. reply formerly_proven 19 hours agorootparentprev> massive investmentsEU is turning back towards Austerity 2.0: Electric Saveroo these days. replyanonymou2 12 hours agoparentprevsecurity, yep! they will run Microsoft Windows, Google proprietary javascript, and Whatsapp for \"secure\" communication on these chips!! reply incompatible 12 hours agorootparentIs there some reason why you wouldn&#x27;t be able to run a purely open source software stack on it, if you wanted? Does Microsoft Windows even run on RISC-V? reply Gravityloss 21 hours agoparentprevThere are projects like Helios: Highly Efficient and Lightweight Input&#x2F;output Open Siliconhttps:&#x2F;&#x2F;cordis.europa.eu&#x2F;project&#x2F;id&#x2F;190183836But AFAIK this is just a small part of large amount of multiple projects. reply FirmwareBurner 20 hours agorootparentA lot of EU semi research goes on at IMEC in Belgium, but EU still lacks the actual means of put any of it into production on their own soil. EU fabs have given up going beyond 12nm as it was deemed too capital intensive. reply gchadwick 19 hours agoprevIt&#x27;s a cool project but I do wish these open source processor initiatives targetted more realistic design points.In particular there&#x27;s often a desire to push out of order design into the micro-architecture where the resulting performance just doesn&#x27;t justify it. In this they&#x27;re achieving a CoreMark&#x2F;MHz of 2.44 (from the paper here: https:&#x2F;&#x2F;upcommons.upc.edu&#x2F;bitstream&#x2F;handle&#x2F;2117&#x2F;384912&#x2F;sarga...). This is very low performance (on a par with the Arm M0+). Now CoreMark certainly isn&#x27;t the be all and end all of Benchmarks. In particular it has very little relevance to high performance compute or application cores in general. However it&#x27;s a useful performance smoke test. It is easy to perform well e.g getting close to 1.0 IPC for a single issue design such as Sargantana, CoreMark doesn&#x27;t really stress the memory system so a major source of stalls that you need to hide latency for just isn&#x27;t there. So if you&#x27;re not hitting that you&#x27;ve definitely got work to do on the microarchitecture. They may well have been better off trying to build something simpler and putting more design time into improving the performance of the basic microarchitecture.The other crucial aspect that&#x27;s often overlooked is verification. This is a major part of producing a new production quality CPU design and it doesn&#x27;t appear to be discussed in the paper at all. Maybe once they&#x27;ve released the RTL they&#x27;ll also release the testbench so you can see what they have done. reply phkahler 18 hours agoparentAny of these efforts not performing as well as BOOM may be suffering from \"not invented here\". Its already there and getting good IPC. Why not start from that. reply gchadwick 19 hours agoparentprevThough on the CoreMark benchmark they haven&#x27;t published the IPC achieved. You get a large swing in results depending upon the compiler used and switches (For RV32 at least I&#x27;ve found GCC out-performs LLVM comfortably).They do have an IPC number for Dhrystone (another tiny benchmark that tells you little about real-world performance but you should be able to perform well on), that looks to be 0.7. reply cf1241290841 11 hours agoparentprevI believe we might be at the point where supply chain security (and code base security) might warrant the question why you cant implement something on an M0+.If you really need higher speeds for reaction time, use an ASIC or FPGA. We already do this with USB3 or Ethernet controllers. reply Y_Y 21 hours agoprevFor those of you who don&#x27;t speak Catalan, \"sargantana\" is a common little local lizard (Podarcis hispanicus, \"Iberian wall lizard\"). Of course the chip family (Lagarto) just means \"lizard\" in Castilian. reply iamsaitam 21 hours agoparent(bonus).. and lagarto is the same in Portuguese as well reply germandiago 20 hours agorootparentWarning, offtopic but funny: FWIW \"lagarta\" in spanish slang is a girl with a lot of ambition looking from things from men taking advantage of them. Not a \"worker\" but a dangerous person. Lol reply MoSattler 11 hours agorootparentprevsame in aragonese reply Narishma 14 hours agoparentprevIt&#x27;s mentioned in the article. reply znpy 20 hours agoparentprevI don&#x27;t know, I&#x27;m not a lizard expert but that lizard looks the same as the ones I saw when I was a kid in the south of Italy.I guess if I ever start a chip fab there I&#x27;m gonna call my chip stranvicula or something like that. reply Anduia 18 hours agorootparentThey are very similar. The Iberian ones are smaller, with broader heads, and are sometimes more colorful. I&#x27;m pretty sure that a Catalan would call the Italian ones &#x27;sargantana&#x27;. reply oddestly 17 hours agoparentprevnext [2 more] [flagged] nsriv 16 hours agorootparentSo you made an account to leave this needlessly aggressive comment? reply vlugorilla 19 hours agoparentprevSpanish, castilian does not exist reply dragonwriter 18 hours agorootparentCastilian absolutely exists, and us more specific than “Spanish”.https:&#x2F;&#x2F;www.merriam-webster.com&#x2F;dictionary&#x2F;Castilian reply vlugorilla 21 minutes agorootparentThere is a bit of a controversy around this. And I don&#x27;t say you are wrong. It&#x27;s just that I personally consider that Castilian should not be used and does no longer exist. Here&#x27;s why I think it like so:Castilian originated as one of several Romance dialects in the Iberian Peninsula. It developed in the Kingdom of Castile during the Middle Ages, distinct from other regional languages like Catalan or Galician. With the unification of Spain, Castilian gained prominence, eventually evolving into modern Spanish. This was not merely a linguistic shift but also a result of political and cultural dynamics. The language we now call Spanish has absorbed influences from Arabic, indigenous languages of the Americas, and others, diverging significantly from its medieval Castilian origins. For this, Castilian has now disappeared, you just need to read how Castilian was written to see it has nothing to do with modern Spanish.Today, Spanish is spoken by over 500 million people worldwide. In contrast, the Castilian region of Spain has a much smaller population (~3M). Referring to the language as Spanish acknowledges its extensive global presence and its modern version. Just as we refer to the language originating in Tuscany as Italian, not Tuscanian, calling the language from Castile &#x27;Spanish&#x27; aligns with common linguistic naming conventions. Languages often take their names from the nations or cultural entities they are associated with, not their specific regions of origin.Modern linguistic institutions, like the Real Academia Española, regard &#x27;Castilian&#x27; and &#x27;Spanish&#x27; as synonyms but recommend &#x27;Spanish&#x27; for its inclusive and global character. reply cosmojg 18 hours agorootparentprevFrom Wikipedia[1]:> Castilian (castellano), that is, Spanish, is the native language of the Castilians. Its origin is traditionally ascribed to an area south of the Cordillera Cantábrica, including the upper Ebro valley, in northern Spain, around the 8th and 9th centuries; however the first written standard was developed in the 13th century in the southern city of Toledo. It is descended from the Vulgar Latin of the Roman Empire, with Arabic influences, and perhaps Basque as well. During the Reconquista in the Middle Ages, it was brought to the south of Spain where it replaced the languages that were spoken in the former Moorish controlled zones, such as the local form of related Latin dialects now referred to as Mozarabic, and the Arabic that had been introduced by the Muslims. In this process Castilian absorbed many traits from these languages, some of which continue to be used today. Outside of Spain and a few Latin American countries, Castilian is now usually referred to as Spanish.[1] https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Spanish_language reply Y_Y 17 hours agorootparentFrom the page you linked:> Name of the language> In Spain and in some other parts of the Spanish-speaking world, Spanish is called not only español but also castellano (Castilian), the language from the Kingdom of Castile, contrasting it with other languages spoken in Spain such as Galician, Basque, Asturian, Catalan, Aragonese and Occitan.> The Spanish Constitution of 1978 uses the term castellano to define the official language of the whole of Spain, in contrast to las demás lenguas españolas (lit. \"the other Spanish languages\"). reply anthk 12 hours agorootparentIt&#x27;s the same language. I&#x27;m a Spaniard, so I know it well. Name it the way you&#x27;d like, it can be called Spanish, Español or Castellano everywhere from Mexico to Patagonia, and from The Canaries up to the Pyrenees. reply amenhotep 8 hours agorootparent\"name it the way you&#x27;d like, it can be called Spanish\" is a very different proposition to \"[you should say] Spanish, Castilian does not exist [and you are wrong to use that name]\", which was the angle of the poster who kicked all this off. reply vlugorilla 21 minutes agorootparentI just answered with my point of view in the parent comment reply enriquto 10 hours agorootparentprev> from Mexico to Patagonia, and from The Canaries up to the Pyrenees.Sounds a bit imperialistic?Notwithstanding the tens of millions of native speakers of autochtone non-spanish languages in these territories: Mapuche (260K), Quechua (7.2M), Aymara (1.7M), Guaraní (6.1M), Wayuu (400K), Mayan (6M), Miskito (150K), Garifuna (120K), Nahuatl (1.7M), Mixtec (530K), Catalan (4.1M), Basque (750K), Galician (2.4M). Spanish is quickly eroding all of these, but they still exist! (And this only counts native speakers. The number of people who are fluent in Guarani or Catalan is certainly more than the double of that.) replyansible 21 hours agoprevHere&#x27;s a pre-print paper I found:Sargantana: A 1 GHz+ In-Order RISC-V Processor with SIMD Vector Extensions in 22nm FD-SOIhttps:&#x2F;&#x2F;upcommons.upc.edu&#x2F;bitstream&#x2F;handle&#x2F;2117&#x2F;384912&#x2F;sarga...RV64GC with a subset of the v0.7.1 vector extension. 1.26GHz nominal clock on a 22nm process. reply camel-cdr 21 hours agoparentSo an in-order core that is slightly faster than rocketchip in their benchmarks. That doesn&#x27;t seem all that exciting, except for the vector extension, although they only support a small subset of it. Thats sounds similar to spatz [0] and given their numbers is slightly faster.[0] https:&#x2F;&#x2F;github.com&#x2F;pulp-platform&#x2F;spatz reply ansible 20 hours agorootparentThe previous DVINO was a 5-stage in-order, this Sargantana core is a 7-stage out-of-order write-back with register renaming and a non-blocking memory pipeline.So it is not a full in-order or a full out-of-order design. reply galangalalgol 19 hours agorootparentAre the vector extensions the fixed size ones or the originally proposed lanecount agnostic ones? That is the aspect of riscv i&#x27;m most excited about. reply ksec 20 hours agoparentprev> in 22nm FD-SOIThat basically implies being Fabbed with Global Foundry inside EU ( Germany ). reply darksaints 12 hours agoparentprevThat sounds like the perfect high end MCU core. Doesn’t say what the target use case is, but if it’s like other RISC-V announcements, they’re probably talking about general purpose CPUs, in which case those specs are pretty disappointing. It’s a shame that RISC-V has made so little impact in embedded electronics. reply 0xDEF 20 hours agoparentprev>RV64GC>CWhat is the purpose of including the RISC-V Compressed 16-bit extension set in what is supposed to be a HPC chip? Most embedded&#x2F;IoT RISC-V implementations include that for obvious reasons but why here? reply brucehoult 20 hours agorootparentIf you don&#x27;t have the C extension then you can&#x27;t run off the shelf Linux distros such as Fedora, Debian, Ubuntu, Arch and are limited to what you compile yourself e.g. Buildroot &#x2F; Yocto.However the actual academic paper says it&#x27;s RV64G, no C. reply ansible 18 hours agorootparentThanks for the correction Bruce. I was in a rush (never post when you are in a rush, or drunk, or angry) and I was so used to seeing RV64GC that I didn&#x27;t notice the absence of the &#x27;C&#x27;. reply cmrx64 19 hours agorootparentprevThe RVV here isn’t compatible with mainstream Linux anyway. reply cmrx64 20 hours agorootparentprevthe same reasons motivating C still apply at HPC: higher code density means fewer bits wasted representing redundant information, better cache utilization, minimization of memory fetch bandwidth, etc.basically, every metric derived from code size is happier when you have 20-30% fewer bits representing it. reply DeathArrow 21 hours agoparentprev>22nm FD-SOIThat&#x27;s kind of not good news. I was hoping for 4nm to have some alterative to Intel&#x2F;AMD&#x2F;Apple. reply rwmj 21 hours agorootparentYou don&#x27;t do your first experimental test chips on 4nm. That&#x27;s where you get to when you have raised hundreds of millions after you&#x27;ve gone through several iterations to prove to investors you know what you&#x27;re doing. reply imiric 21 hours agorootparentprevIt&#x27;s unrealistic to expect these chips to compete with modern manufacturing standards. Still, it&#x27;s very impressive the progress RISC-V has made in the last few years. It&#x27;s actually a viable option for many projects now. reply stefs 20 hours agorootparentprevi guess this chip is not for high end gaming machines or servers, but rather cars, industrial machine controlling, smart fridges, that kind of stuff. during covid production of many appliances ground to a halt because of various chip shortages. now what if for some reason asian chips became unavailable in europe (wars, natural catastrophes, ...)? cheap and easy to build is far more important than high end performance here. reply bibanez 21 hours agorootparentprevGuess what you need to develop 4nm (spoiler, it’s a lot of money). There are many applications where 22nm is a good tradeoff. reply imtringued 10 hours agorootparentprevWell then you&#x27;re supposed to take a look at what Tenstorrent is doing. They haven&#x27;t named their processor after a dragon slaying sword for no reason. reply paulluuk 21 hours agoprev> The BSC, Europe&#x27;s leading developer of open source computing technologies> The fact that the [..] architecture [..] of these new processors is open source, and therefore non-proprietary and accessible to all, reduces technological dependence on large multinational corporationsI hadn&#x27;t heard of either BSC nor Open Source Computing before. I&#x27;m curious though, are there a lot of people out there who are not tied to large corporations and who have the knowledge and the means to produce computer hardware? Are there hobbyists out there producing their own custom chips and graphics cards? reply tecleandor 21 hours agoparentThe BSC has been featured a bunch of times around here due to their Marenostrum Supercomputer. A month ago someone posted a virtual visit to their Marenostrum 4 location, that&#x27;s kinda surprising&#x2F;interesting because is located inside an old chapel: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38160675 https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;File:MareNostrum_4_supercomputer_at_Barcelona_Supercomputing_Center_1_br.jpgTheir Marenostrum 5 is number 8 in the TOP 500 supercomputer list ( https:&#x2F;&#x2F;www.top500.org&#x2F;system&#x2F;180238&#x2F; ) and I think it recently started working or it&#x27;s about to do it now ( https:&#x2F;&#x2F;www.eetimes.com&#x2F;bsc-about-to-dispatch-marenostrum-5-... ) . They had to change its location as it didn&#x27;t fit in the church anymore, though. reply ciberado 21 hours agorootparentBut they will keep the Marenostrum 4 in the chapel this time, instead of replacing the old generation with the new version :). reply tecleandor 21 hours agorootparentNice! The real Computing Church! reply jacquesm 19 hours agorootparentIf they ever get AGI going it will have come full circle. You can go there to pray to your very visible god. Prompt engineering will be the new praying, you read it on HN first... reply bee_rider 14 hours agorootparentTraining in the Cloud, fine tuning in old churches, inference in your home shrine. reply malwrar 18 hours agorootparentprevIf anyone likes ambient music, an artist I like produced an album from recordings of marenostrum: https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=1EGmWY91VusI find it oddly relaxing. reply kinow 12 hours agoparentprevFor anyone who hasn&#x27;t heard about the BSC, you can check out the website or, if you are more inclined to read code:- https:&#x2F;&#x2F;earth.bsc.es&#x2F;gitlab&#x2F;es&#x2F;autosubmit&#x2F; - project I joined last year to work on, a workflow manager used in MareNostrum to run mainly (but not exclusively) climate experiments - https:&#x2F;&#x2F;earth.bsc.es&#x2F;gitlab&#x2F;es&#x2F; - other projects from my department - https:&#x2F;&#x2F;gitlab.bsc.es&#x2F;explore&#x2F;projects - general projectsThere are also lots of interesting projects, like Aina, a project in partnership with Generalitat de Catalunya (like the council? prefecture?) to foster the Catalan language with models and tools using HPC resources: https:&#x2F;&#x2F;projecteaina.cat&#x2F; reply alfonsodev 21 hours agoparentprevI don’t know about hobbyists but there are less known companies doing open source hardware for sure, [1] here is an example of cool stackable parallel computing project. I participated on the campaign and received mine, but not sure how are they doing today, it was a while ago.Edit: Andreas Olofsson the original founder seems to be still active in the field [2]- [1] https:&#x2F;&#x2F;www.kickstarter.com&#x2F;projects&#x2F;adapteva&#x2F;parallella-a-s...- [2] https:&#x2F;&#x2F;x.com&#x2F;zeroasic?s=21&t=xSlFhUGn5i8d8RkXrsgAIg reply rwmj 21 hours agoparentprevThey co-hosted the RISC-V Summit back in 2018: https:&#x2F;&#x2F;riscv.org&#x2F;proceedings&#x2F;2018&#x2F;05&#x2F;risc-v-workshop-in-bar... reply dataking 17 hours agoprev> The Barcelona Supercomputing Center [...] presented on Wednesday the new Sargantana chip, the third generation of open source processors designed entirely at the BSC.> Researchers from other universities and research centres such as the Centro de Investigación en Computación del Instituto Politécnico Nacional de México (CIC-IPN) [...] have participated in the development of Sargantana.So this was designed entirely in Spain but it is also joint work with a university in Mexico ;-) Nice project though; I&#x27;ve visited BSC and they do a lot of cool work there. reply cf1241290841 12 hours agoprevShout out to affordable subsidized https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Multi-project_wafer_serviceBe it googles OpenMPW Free Silicon Chip Program https:&#x2F;&#x2F;developers.google.com&#x2F;silicon (still active?)Or the EU subsidized multi project wafer https:&#x2F;&#x2F;europractice-ic.com&#x2F;schedules-prices-2023&#x2F; reply pantulis 19 hours agoprevUnrelated note: BSC is a location in the unapologetically crazy HBO series \"30 coins\" season 2, some cool sequences there involving a group visit. reply dtjb 17 hours agoparentAs far as data centers go, it&#x27;s beautiful. Like something you&#x27;d see in a Mission Impossible heist.https:&#x2F;&#x2F;my.matterport.com&#x2F;show&#x2F;?m=oj5FSKsTt7o reply cmrx64 19 hours agoprevWhat about this chip is open source? As far as I can tell, nothing. It frustrates me to no end that closed, secret efforts inherit the “open source” branding just because the specification they implement is participatory and royalty free. reply ThePituLegend 19 hours agoparentIn fact, you can get the RTL here: https:&#x2F;&#x2F;github.com&#x2F;bsc-loca&#x2F;sargantana :D reply cmrx64 19 hours agorootparent!!! perfect, thank you. I’m annoyed now at myself for not having found it… reply MoSattler 21 hours agoprevhttps:&#x2F;&#x2F;archive.ph&#x2F;TDj5W reply ashvardanian 18 hours agoprevDoes anyone know a decent RISC-V developer kit that one can buy in the Bay Area today? Or rent somewhere in the cloud? I want to start porting our C libraries to RISC-V. reply LeonM 18 hours agoparentThere are plenty, SiFive and MilkV sell boards for example. You can also just run emulation. reply ashvardanian 18 hours agorootparentThere are plenty that exist, but i haven’t heard of anyone using them or any stores selling them.Emulation isn’t enough. I need to benchmark the libraries. Emulation will add significant overhead. reply LeonM 18 hours agorootparent> I need to benchmark the libraries. Emulation will add significant overhead.Do not expect good performance from RISC-V processors at the moment.Emulation on a modern X86 CPU will outperform any commercial available RISC-V processor at the moment. reply brucehoult 12 hours agorootparent> Emulation on a modern X86 CPU will outperform any commercial available RISC-V processor at the momentThat&#x27;s not true.qemu-user is a little faster than the single-issue HiFive Unleashed from 2008, but qemu-system is slower.Against either the dual-issue U74 cores in the JH7110 or the small OoO cores in the TH1520 and SG2042 qemu doesn&#x27;t sand a chance on a core for core basis.It used to be the case that qemu could win on x86 by throwing more cores at the problem, but with the 64 core SG2042 in the Milk-V Pioneer that possibility has disappeared too -- not to mention that the Pioneer is $1500 for chip+motherboard (need to add RAM and storage), while a 64 core x86 is $5000 just for the chip. reply camel-cdr 18 hours agorootparentprevThis isn&#x27;t true in my experiance, especially when dealing with the vector extension.But emulation doesn&#x27;t offer any usefull performance insights anyways, except for maybe dynamic instruction count. reply brucehoult 12 hours agorootparentprev> There are plenty that exist, but i haven’t heard of anyone using them or any stores selling them.You probably can&#x27;t walk into your local mall and walk out with one, but it&#x27;s easy enough to buy boards on Aliexpress or on Amazon (it&#x27;s usually the same company, shipping from China, either way)As for people using them, I guess most people simply don&#x27;t advertise what they&#x27;re using. I&#x27;m working at a very large company that is porting certain x86&#x2F;Arm software to RISC-V. We&#x27;re using the VisionFive 2 as the reference device. It&#x27;s the best current combination of performance, price, and software maturity. I&#x27;ve also got the Lichee Pi 4A and the software works fine on that too (it would be shocking if it didn&#x27;t) but it turns out that despite the specs on paper the VF2 is 20% faster anyway.If you need to use SIMD&#x2F;vector rather than plain C then the only choice for RVV 1.0 at the moment is the CanMV-K230 which has a single 1.6 GHz core vs quad core on the other boards. It&#x27;s also only just come out. My order made on October 29th hasn&#x27;t arrived yet, though they claimed on November 12 that they&#x27;d received stock for it. Mind you the Pi 5 I ordered on September 29 only just arrived last week, so this is not unusual for brand new boards. reply snvzz 6 hours agoparentprevIf you want the least amount of trouble, VisionFive 2 has the most mature upstream support.Gets you RV64GC and Zba, Zbb. Low power, passive cooling is overkill. Performance somewhere between rpi3+ and rpi4. reply camel-cdr 18 hours agoparentprevIt depends, mostly on if you need vector support.Right now, I&#x27;d recommend the canmv kendryte k230 which has a C908 rvv 1.0 capable core.If you can wait a bit, mid&#x2F;end 2024, I&#x27;d go for the vision five 3 (or whatever is will be called), as it will have RVA22+V (iirc) or for the sg2380 which has SiFive P570s and X280s both RVA22+V.If you don&#x27;t care about vector, then currently anything based on jh7110 should be good.But if you have the time to deal with very slow execution and the potential need to report hardware bugs, I&#x27;d consider benchmarking on rtl simulation of open source cores. (BOOM, tenstorrent-bobcat, XiangShian, ...) reply slaw 8 hours agoparentprevYou could try Lichee Pi 4A, CPU TH1520, 12nm, RISC-V 2.0G C910 x4> https:&#x2F;&#x2F;sipeed.com&#x2F;licheepi4a reply DeathArrow 21 hours agoprevAny benchmarks? Does it compares to Intel&#x2F;AMD at raw power? Does it compares with Apple at efficiency? reply sylware 21 hours agoparentIt is in an in-order CPU. It is meant for tasks where prediction and robustness are important. More like hard-ish realtime stuff in nasty environment (or... security? ahem...)RISC-V moving forward. Good. reply 999900000999 16 hours agoprevVery cool, I just got a MangoPi and I&#x27;m excited to get some stuff running on it.I imagine RISC-V is the future. None wants to pay licensing fees to Arm reply rwmj 21 hours agoprevIs this based off CVA6? That&#x27;s not mentioned. reply tecleandor 21 hours agoprevInteresting. It&#x27;d be nice to know if they&#x27;re going to focus on HPC loads or hobby&#x2F;consumer too. I should check to see if I still know people around the BSC :P reply _fcs 21 hours agoparentFrom the preprint [1] it looks like it is not meant for consumers. This way, Sargantana lays the foundations for future RISC-V based core designs able to meet industrial-class performance requirements for scientific, real-time, and high-performance computing applications.1.reply m00dy 20 hours agoprevI am one of the fortunate people who could afford to pay a visit to BSC. reply capableweb 20 hours agoparentSince when does it cost money? I&#x27;m fairly sure it used to be free to visit... reply kinow 12 hours agorootparentIt still is. You can just book it with reception directly, or if you attend a meeting or conference. Whenever I get friends in Barcelona I always invite them over too (anyone that works there can request a visitor badge and schedule the visit -- necessary avoid conflicts). reply kh_hk 19 hours agorootparentprevMust be poor phrasing and choice of words I guess. I concur it&#x27;s free to visit. reply manuelabeledo 19 hours agorootparentprevIt technically does, if you don&#x27;t live there. reply mkehrt 12 hours agoprev [–] So, uh, why&#x27;s it named after a demon? reply snvzz 6 hours agoparent [–] It is actually a common animal.https:&#x2F;&#x2F;ca.wikipedia.org&#x2F;wiki&#x2F;Lac%C3%A8rtids#p-lang-btn replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Barcelona Supercomputing Center (BSC) has introduced the third generation of Lagarto processors, called Sargantana, which are based on open-source RISC-V technology.",
      "Sargantana processors aim to promote European technological sovereignty and reduce dependence on multinational corporations.",
      "The chip offers improved performance and is the first in the Lagarto family to operate above the gigahertz frequency barrier, reinforcing BSC's position as a leader in RISC-V computing research in Europe."
    ],
    "commentSummary": [
      "The Barcelona Supercomputing Center has created an open-source RISC-V chip called Sargantana, prioritizing security and local chip production.",
      "GlobalFoundries has developed a new 22nm planar process for older chips, sparking conversations on the challenges, costs, and feasibility of producing chips at smaller node sizes.",
      "Discussions cover various topics including producing older chips on a hobbyist level, the limited number of producers for older fabs, the profitability of the semiconductor industry, the importance of a reliable chip supply, prospects for the EU in semiconductor fabrication, CPU design performance, language development, and information about the Sargantana chip."
    ],
    "points": 357,
    "commentCount": 142,
    "retryCount": 0,
    "time": 1702554937
  },
  {
    "id": 38645159,
    "title": "Interview with a Mainframe COBOL Programmer: Challenges of Outdated Technology and Transitioning to a New Database",
    "originLink": "https://ezali.substack.com/p/interviewing-my-mother-a-mainframe",
    "originBody": "Share this post Interviewing my mother, a mainframe COBOL programmer ezali.substack.com Copy link Facebook Email Note Other Discover more from John’s Substack My personal Substack Subscribe Continue reading Sign in Interviewing my mother, a mainframe COBOL programmer Tom Jordell Jul 14, 2016 8 Share this post Interviewing my mother, a mainframe COBOL programmer ezali.substack.com Copy link Facebook Email Note Other 1 Share The Home Doctor - Practical Medicine for Every Household - is a 304 page doctor written and approved guide on how to manage most health situations when help is not on the way. My mother has been working for one of the largest banks in the EU since before I was born and I’ve always been fascinated by her line of work, especially these last years since I’ve become a programmer myself. I’ve been asked to interview her plenty of times, and finally decided to do so. Some notes The banking programming world is a completely different world than what most of us are used to, and for the next couple of hours after posting this I’ll accept whatever questions you may have for her, but keep in mind that some questions may not be answered due to security concerns. I’ll be posting this to HN & Reddit and will answer questions over there as well. I won’t write this as a Q&A, but more like I’m telling you her story. UPDATE: I’ve added Q&A of some FAQ to the bottom of the post! 1991 The year she started internal training at Nordea, which back then was known as Nordbanken (The North Bank) but changed name to Nordea in 2001. During the training she had to take various tests, most notably an IQ-test to see if she had the intelligence to work within this field. Other tests includes a psychological checkup to make sure she had the psyche to handle that line of work and a multitasking test which she failed horribly where she got the score 22/100. She did very good on the other tests and among the 16 available positions, she managed to get one. The position in question was an IBM Mainframe COBOL programmer, which to this day, 25 years later still work as for the same bank. This position is the most important one in the bank, at least from a technical standpoint. If, let’s say, my mother and everyone on her team would quit their job, the bank would go under within a matter of weeks if they’re lucky. They have a rotation of people on her team being available 24/7. I remember when I was younger and she had to take a taxi to work in the middle of the night on a Sunday to fix a dead-lock problem. COBOL… …is not a fancy programming language like your functional Haskell or concurrent Golang— it’s an imperative, procedural language and since 2002, object-oriented. There’s nothing wrong with the language itself, the problem is that barely anyone knows it — at least not in the context of mainframe programming. My mother is the next youngest person on her team, and she’s born 1964, and the youngest person being 2 years younger. Since almost all of the largest banks in the world runs on IBM Mainframe with COBOL as the primary programming language, this is a global issue. The smaller banks however are better off which usually runs something like Java without mainframes. My mother used to ask me if I wanted to learn, but since working with the more fancier stuff like Postgres, Redis, Node, Crystal, PHP, among others, I’ve always answered “Never, ever!”. I am still very interested in what she do, but these types of systems gives off the probably worst enterprisy-feeling that you can imagine, which I’d like to avoid. I can only imagine the fat paycheck a 20-year old mainframe programmer would get though, because your age in this case would be invaluable. Databases Their primary database is called IMS, which is an hierarchical database built by IBM for the Apollo program. Internally they call it DL/1 which is short for Database Language One. They are trying to migrate to DB2 which is a relational database that speaks normal SQL, but considering the sheer volume of data that Nordea is storing, this is a task that is going to take years. It’s not as simple as just moving the data from IMS into DB2, they also have to update their modules to load & save data from DB2 instead of IMS and they have thousands of modules, many of which were developed by programmers that have either passed away or have retired. Each transaction is stored in DB2. They’re avoiding writing to IMS as much as possible, only reading data from it until their newly purchased system is fully integrated and they can start storing data there. IMS is extremely old and very slow (for some tasks). Searching for data can take hours. Hah, and here we are, arguing that MySQL delivers 2ms better query speed than Postgres. That’s a bit ironic. They also use a flat-file structure for a lot of tasks, which themselves has different flavors that IMS supports. One such example is GSAM which has forced my mother to take a taxi into work many times due to modules working on the same GSAM-file at the same time, producing deadlocks. Let’s talk about the size of their databases for a moment. DB2 is only storing data about transactions, and each transaction is different in size depending on what type of account the transaction goes to/from. Private accounts like my personal bank account is orders of magnitude more simple than bank accounts used by business. Each transaction is somewhere between 2KB to 500 bytes, so let’s say the average is 1KB. At the moment, their DB2 database holds 11 billion transactions, and the law requires them to save each transaction for 10 years, having settled for 11 years. At the moment, transactions are only 7 years old and the amount of transactions is estimated to grow roughly 5–8% each year, until they hit the 11 year mark where they can nuke transactions that are older than 11 years. So at the moment, the DB2 database holds about 10TB of data, and that data is only transactions. In 4 years, it’ll be somewhere around 13–14TB of data. IMS holds most other information. That kind of data differs a lot in size, so it’s hard to estimate how large it is, but I guess it’s much larger than their DB2 database. They also store some data on tapes, where their programs tell a machine to load/save data from a specific tape, which is pretty cool! IDE Everyone has some form of IDE or text editor, right? Well, yes they do. The IDE is called ISPF which is like an entire OS. ISPF can be extended, and the part we would call an IDE is an extension to ISPF which is called Endevor. ISPF is directly connected to the mainframe, and there’s no such thing as a local development environment here. Batch Roughly 80% of their systems are batch jobs. These are jobs that runs at a certain time or interval, doing some processing on their data or sends data to other banks/agencies/etc. For example when I buy a can of Coke, the money is withdrawn from my account balance, however the money is not actually transferred anywhere until one of their batch jobs does so. These jobs are usually executed during the night, which is usually why it takes a day before transactions between banks are completed. Transactions to the same bank are usually instant because it executes immediately. Batch jobs are working with datasets on a gigabyte-, sometimes terrabyte-level and can in some cases take hours to complete. I’d love to see how IBMs mainframes all of a sudden starts working at full capacity the second the clock turns 00:00 in their datacenters, would be pretty cool! The problems banks are facing Banks that run on mainframes have quite a lot of issues that they need to take care of, but unfortunately time is scarce. Programmers are getting older, not many wants to learn and the time before a new employee can stand on their own feet is 2–3 years, and even then there are a lot of gray areas for them. There are programs that are decades old that nobody even knows what they do and the person who wrote it is long gone. Many banks are trying to purchase shiny new systems to replace what they currently have, while almost all of their programmers are saying “this is not going to work, you cannot replace this monster that we have here”. The reason their systems has become so huge is partially because they made the mistake early on to tailor their own system towards other systems. A great example here is how we build RESTful services, where programs that wants to get data from us have to tailor their system to match ours. This is not the case with some banks, instead they’re sending data to other systems (such as the tax agency) which may at any point update their systems which in turn breaks the current system the bank are using to send the data. Banking systems are also extremely advanced. A personal bank account differs a lot from a business bank account, and there are at least 50 different types of bank accounts for each of them. And in Nordeas case, they also have the Swedish government accounts, which are different from both personal and business accounts. I think they have the Finnish government accounts and maybe a portion of Denmarks as well, which differs too. Final words It’s going to be very interesting to hear what Nordea and other banks will do these upcoming years, and what new tech they’ll be implemented on. I may have forgot to ask something, so if you have any questions go ahead and ask, I’ll be forwarding them to her and write down her response for the next couple of hours. Q&A Why did you choose to work with IBM Mainframe COBOL programming? - I’ve always wanted to “work with computers”, but I wasn’t aiming for this specific job. I saw an ad in the papers and applied and got the job. I had a bit of computer background before I applied though. What is the worst thing that you’ve seen on a day? - My co-worker forgot to add a dot to the end of a statement for a module in the most critical part of our system which we call “The Cash Register”. It is the part that handles all the money. The result was that the entire bank was down for 16 hours straight due to the module continiuing executing statements when it actually should’ve stopped after that statement. It basically overloaded our systems, a self-DOS of sorts. What do you think will happen in the future for banks that run on the same infrastructure as Nordea? - Most banks has acknowledged that we need to replace the older mainframes with something more modern. Nordea has bought a new system which it has promised to replace the current one within 4 years, but Nordea takes care of multiple countries and a more reasonable number is 4 years per country, so 16 years in total. Banks and finance that haven’t acknowledged this is going to have a very rough time ahead of them. I still think we’ll live on IBM infrastructure though. What were the challenges you’ve faced as a female programmer that started in the 90's? - No problem at all. I’ve got a couple of female co-workers, but most are men. It doesn’t bother me that much. You’ve been working for the same entity and possibly the same code base for more than 20 years. Does it ever gets old? - Yeah it definitely gets tiring after a while, like most other lines of work becomes. But during my time here I’ve build many completely new systems for various things in finance, and that has always been fun. Unfortunately from now on we will never build anything new, only maintain what we currently have and wait for the new system we’ve purchased to replace the current one. How scary is it to write code for a bank? - It is very scary, especially when we push changes to production which happens on Sundays. Whenever we push new changes to production, huge parts of the system has to be taken offline. One of those parts is the entire IMS. During that period anything can happen, but we have a very robust test environment so it usually goes through smoothly. Have you caused any huge mistakes for the bank? - Definitely have, quite a large mistake I did back in 1997, when my youngest son (that’s me, the writer) just started pre-school and my parental leave was over. We have a system for saving money for retirement. Those type of bank accounts wasn’t locked back then, and the law says you cannot withdraw money from that account before you’re 55 years old. Since the accounts wasn’t locked, it was possible to withdraw money if you had the bank account number, so the solution was simple; not give the customer their bank account number. I managed to screw up by modifying a module that added their bank account number to the mail that was sent out to customers. So what happened was that customers started withdrawing money (which was not taxed yet!) from their retirement savings account before they were allowed to. This triggered a huge inspection, the Swedish government stepped in, the financial inspection and the media were all over it. That was me. What’s your working environment like? - We’ve recently moved to a more “hip” location. We used to have personal desks, but now we have this “pick whatever spot is avaiable” open area. I dislike it a lot. The ONLY Way To Whiten Your Teeth Without Toxic Chemicals... Subscribe to John’s Substack By Tom Jordell · Launched a month ago My personal Substack Subscribe 8 Share this post Interviewing my mother, a mainframe COBOL programmer ezali.substack.com Copy link Facebook Email Note Other 1 Share",
    "commentLink": "https://news.ycombinator.com/item?id=38645159",
    "commentBody": "Interviewing my mother, a mainframe COBOL programmer (2016)Hacker NewspastloginInterviewing my mother, a mainframe COBOL programmer (2016) (ezali.substack.com) 356 points by MoBarouma 15 hours ago| hidepastfavorite110 comments scrlk 15 hours agoThe original HN submission from 2016: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=12096250> \"The banking programming world is a completely different world than what most of us are used to\"If you&#x27;re after another read on this topic, \"An oral history of Bank Python\" is good: https:&#x2F;&#x2F;calpaterson.com&#x2F;bank-python.html (also previously on HN: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29104047) SilasX 14 hours agoparentOh wow, I can’t believe that was seven years ago! I never got around to interviewing my mom about programming mainframes with Michigan Algorithm Decoder:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=12097032 reply ptmcc 14 hours agoprev> I can only imagine the fat paycheck a 20-year old mainframe programmer would get though, because your age in this case would be invaluable.It&#x27;s funny, people frequently assume this would be true but reality doesn&#x27;t really bear this out. It&#x27;s typically pretty average to even below average which contributes to the talent pipeline problem.The other side of it is that its not the technical stuff like \"knows COBOL\" that is so immensely valuble. Any average dev can \"learn COBOL\" but that&#x27;s not actually the valuable thing. The anecdotes of COBOL programmers coming out of retirement for 500k&#x2F;yr contracts has little to do with COBOL, but their accumulated institutional knowledge of the giant ball of business logic encoded in that COBOL.If these banks and other institutions actually did write fat paychecks to young mainframe programmers the demographic problem they&#x27;re facing might not be so bad. reply importantbrian 12 hours agoparentI went to grad school with a guy who was in that situation. He worked at a bank and got laid off during the financial crisis. The plan was for the bank to port the old system over to Java or something like that, and they were \"close\" to the end of the project and they thought at that point the rewrite team was comfortable enough with COBOL that they could do the rest of the rewrite without him. Turns out that yes they understood COBOL just fine, but they desperately needed his institutional knowledge. He ended up agreeing to come back on a part-time basis at some obscene consulting rate for however long the transition took and in the meantime, he did grad school part-time to skill up. reply Deprecate9151 13 hours agoparentprevI used to work at a large insurance company with a COBOL core system. I completely agree with this. They paid for people who knew all the undocumented idiosyncrasies and foot-guns in the code. Not just \"Knowing COBOL\". A decent programmer could read a COBOL program and follow it.They wouldn&#x27;t know that the reason the key export fell over when extracting data for your 10K filing because someone had decided&#x2F;assumed&#x2F;whatever that a certain record count would never go over 3000, so they hard coded the program to just error out if it went above that value. reply ecshafer 13 hours agorootparentWhen I was working in a finance company that used cobol, sure I could read cobol. I could probably write some cobol. But the extra crap around cobol, how mainframes work, DB2, JCL, etc. is where this gets complicated. reply jSully24 8 hours agorootparentprevI just shared this story earlier today with my team. This was Cobol insurance claims processing system, shortly before the Y2K issues.The claims processing system I worked on used a number system that would only go to 4 digits, it would roll over to 0 if we ever processed more than 9999 claims overnight.Our VP would not listen to us when we said we needed to change that. He said \"Claims process fine every night! There is no problem.\" I had left before they got to that 10,000th claim (thankfully).That said, anyone need an old COBOL developer?Edit: now I’m the VP of Engineering. We do work on tech debt regularly. reply julian_t 35 minutes agorootparentFortran, not COBOL, but I came across a similar thing when helping to update code for Y2K. An insurance company used a 2 digit field for year of birth, assuming that it would always be prefixed with &#x27;19&#x27;. I sort of forgave them because the code had originally been written in Fortran 2 sometime in the 60s, and who would ever assume that their code might still be in use 40 years later? reply CapsAdmin 9 hours agorootparentprevI feel like you and the parent are saying a company pays well if you master COBOL as opposed to just \"learning\" COBOL. reply xboxnolifes 9 hours agorootparentIt has nothing to do with mastering COBOL. It has everything to do with mastering the banking systems. reply mastazi 4 hours agorootparentYeah, that&#x27;s true of anything related to the finance sector IMHO regardless if it&#x27;s COBOL or Java or Python.The language you happen to be using in your system is almost inconsequential in the grans scheme of things, because it&#x27;s like 1% of everything that&#x27;s going on and that you need to know. reply quickthrower2 9 hours agorootparentprevTraining a GPT on all that Cobol could be invaluable at least as a “footgun catcher” for less experienced devs to act as a code reviewer. reply tristor 12 hours agoparentprev> If these banks and other institutions actually did write fat paychecks to young mainframe programmers the demographic problem they&#x27;re facing might not be so bad.When I first started work I was involved in banking&#x2F;insurance mainframe stuff, and it was honestly a pretty terrible working environment for reasons that spanned from the mundane (like dress codes) to the esoteric (like the horrible crufty legacy codebase). I would have put up with it if it paid well, but it didn&#x27;t. In fact, it paid significantly worse than the job I&#x27;d had before which was installing physical cable plant (fiber optics and copper ethernet), the reason I took it was at least the office had air conditioning, but it certainly wasn&#x27;t something I wanted to turn into a career.As a mid-level windows sysadmin doing customer-facing phone support at a cloud provider, I made nearly 30% more than I had as a junior mainframe guy at a bank&#x2F;insurance company. The difference in the skillsets and their commonality was massive, yet the pay was significantly worse for the mainframe work despite it being a rare skillset in a high value industry. I remembered when I was in this job that guys who worked on trading desk backend code were far better compensated while working in easier development environments on less esoteric codebases, the mainframe folks were the lowest paid of the developers at that company, at every level of seniority. The only person we worked with who was well compensated was an independent contractor who&#x27;d worked there previously for 30 years before retiring. reply owlstuffing 10 hours agorootparentI had similar beginnings. Late 80s transforming literal reams of 360&#x2F;70 assembly to “modern” cobol 85. Was still earning my cs degree at the time.As you know, the pay wasn’t so hot for stuff like that back then. But it was an improvement over manually unloading flatbed trucks stacked with steel conduit, which was my prior line of work!Today I think it’s much different. Some banks and insurance companies and the like still at least partially run on software as old as the Apollo missions. Maybe I’m wrong, but finding people still alive with that knowledge intact must be difficult and I imagine their compensation reflects that simple supply&#x2F;demand formula. Shrug. reply DaveSchmindel 12 hours agoparentprevI can vouch that this is true in some cases.I worked for a multinational bank headquartered in the U.S., whose corporate team came up with a sexy program to snatch up recent Computer Science graduates. The program offered them a \"fair\" market rate for an entry-level jobs in industry, and let the graduates try two to three different teams and&#x2F;or departments in their first year. At the end, if the company still liked them, the candidate got to pick their permanent team for a full-time position. This position, however, did not see a bump in pay, and by that time the program effectively filtered out candidates that weren&#x27;t performing at an accelerated level.The results were great for the business: young programmers that were often more proficient in COBOL than their new peers were costing them $60k-$70k a year. The senior, or \"tenured,\" peers were in the $200k-$400k range in some cases. reply thaumaturgy 11 hours agoparentprevYep. I have COBOL experience (on a mainframe even!), and I&#x27;d even be willing to do it again -- and I&#x27;m young by the standards of COBOL programmers, so somebody could even get 20 years out of me still.But the wages being offered for those roles are abysmal relative to what&#x27;s available for other skills, and modern life is applying a great deal of pressure to chase larger paychecks, sadly. reply beretguy 12 hours agoparentprevLast time I checked DMV of SC pays $50K salary with no remote work option for COBOL position. And requires 2 years of experience. Granted, it’s a state job, I get it, but still. reply chiph 7 hours agorootparentIt&#x27;s been years since I looked, but mainframe jobs at the big banks in Charlotte were a little less than that.Whenever I hear about a skills shortage (in any field), I automatically append the phrase \"at the salary we want to pay\" and that completes the equation. reply uudecoded 12 hours agorootparentprevIt&#x27;s probably because they technically have to make that job posting and let it sit before handing it off to a contract firm for $300k. reply bdw5204 4 hours agorootparentWhat is stopping somebody who&#x27;d be inclined to take a COBOL job from just starting a contract firm and taking the $300k from the government? Does the government check if they actually have employees who have 30 years of COBOL experience before handing out those contracts? reply beretguy 8 hours agorootparentprevI think so too. reply ChrisMarshallNY 7 hours agoparentprevThere’s the apocryphal tale of The Retired Engineer:An engineer retires from his company, after a 40-year career. He left at a senior technical level.Some time later, he’s contacted by his old company, begging him to help them fix a problem that has the current staff absolutely stymied. They have been at a standstill, for weeks.He agrees, and shows up. He sits down at a workstation, examines the behavior, looks at some code, then says, after about five minutes: “Here’s your problem. If you just do this, it will fix it.”He hands in an invoice for $10,000.The beancounters flip their wig. “We can’t pay $10,000 for five minutes’ work! Itemize it, and tell us exactly why you think it’s worth it.”He takes the invoice, turns it over, and writes on the back: 1) Fixing the bug in five minutes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .$20 2) Six years of college, and 40 years of experience, so I can fix the bug in five minutes. . . $9,980 reply D-Coder 3 hours agorootparentI had heard of this re Charles Steinmetz, but apparently it goes way back:\"The earliest instance located by QI ((Quote Investigator)) appeared in “The Journal of the Society of Estate Clerks of Works” of Winchester, England in 1908.\":https:&#x2F;&#x2F;quoteinvestigator.com&#x2F;2017&#x2F;03&#x2F;06&#x2F;tap&#x2F; reply neilv 5 hours agorootparentprevA great tale. I usually hear a version about a halted production line, with the punchline invoice something like: 1. Hit machine with hammer ....... $1 2. Knowing where to hit it ... $9,999The production line versions have the element of customer losing money by the minute, which suggests that the customer received the value, and is only confused&#x2F;petty because it seemed so easy for the engineer to provide that value. reply lowken 5 hours agoparentprev100% true. The value is not knowing COBOL the value is hard core banking or insurance knowledge. reply jmclnx 10 hours agoparentprev>It&#x27;s funny, people frequently assume this would be true but reality doesn&#x27;t really bear this out. It&#x27;s typically pretty average to even below average which contributes to the talent pipeline problem.This, I went into programming over 40 years ago. At the time I worked in a warehouse, I had to take a 5% pay cut (IIRC). But in the long run it was worth it. Most people there stayed in the warehouse&#x2F;manufacturing due to the pay. reply aitchnyu 4 hours agoparentprevIndian companies are still training tons of new graduates in mainframe. reply wdb 10 hours agoparentprevYou don&#x27;t want to know how much you can ask when you are one of the few that can help a multinational migrate away from an old database system ;) reply jongjong 12 hours agoparentprevIn tech, it&#x27;s only true if you know some dirty secrets. The more dirt you are exposed to, the more you keep your mouth shut, the more it pays. The thing is that teams are so large and compartmentalized that nobody knows the true horror of what&#x27;s going on. reply danielodievich 14 hours agoprevMy grandmother programmed with punch cards, have no idea on what hardware and it&#x27;s too late to ask. My father did a bunch of fortran and Cobol on USSR mainframes and then did a bunch of y2k here in USA. One of the neatest things I have from this is a printout from one of his fortran programs from I think Minsk-32 mainframe which he ripped into 3 pieces to wrap a developed large format 64mm film of some mountains that he shot a long time ago. The program seems to be called MATR1 and is doing matrix manipulations and referring to topography of the land in comments. I have it framed on my wall near my workstation. I coded in variety of languages most of my life And now my teenager looks to be interested in coding and is doing Java and Python in high school. Here is to 4th generation programmer! reply 999900000999 14 hours agoparentSounds like a neat movie idea.The same code base passed down though generations.Legacy code. reply jacobyoder 13 hours agorootparentThe code will only run on some specific soviet-era hardware, and the last hardware was destroyed 20 years ago. There&#x27;s some new regime that is rumored to have rebuilt the original hardware, and is now on the hunt for the missing source code that is framed above the great grandchild of the original developer, who was executed for treason. The new bad guys are gunning for the source code, and it has to be protected at all costs, to prevent a takeover of the world.I&#x27;m already seeing the inheritance jokes write themselves.Someone have chatgpt write the screenplay now. reply Stratoscope 9 hours agorootparentDone!CODE REDUX: Legacy Preserved\"In a world where the past and present collide, a family&#x27;s legacy becomes the key to saving the future.\"https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;2f0c9eec-c3b8-4145-86d7-2c617a... reply testplzignore 9 hours agorootparentThey have the code, then need to steal the code, then need to upload the code??? reply ikesau 12 hours agorootparentprevClass Warfare, out 1717243201. reply micah94 12 hours agorootparentprevI think you just wrote the prompt right there. Step 2. ???? Step 3. PROFIT! reply PeterStuer 3 hours agoprevDuring my time working as a systems integration consultant in the financial services sector, I had to do a lott of integration with these core banking systems.Most of the time you try to reuse existing integration points from previous projects, as negotiating completely new interfaces had a lott of friction, both technical and business&#x2F;compliance related, and could easily set your project back for more than a year or two.Integrations are usually delivering structured documents before a certain time in the evening for overnight batch processing. The existing documents with data based on offsets have by design regions of not yet assigned space in them to accomodate future updates and use. You negotiate over the bytes of those that can be assigned to your needs if new info is required.For data extraction you will sometimes find more &#x27;modern&#x27; api&#x27;s, but do not expect too many fine grained REST stuff. You&#x27;ll often find yourself in meetings negotiating with regulation and compliance looking for ways to avoid costly new developments.On a sidenote: I often found compliance people a lott more pragmatic and solution oriented than IT. Having to convince my team that &#x27;the letter&#x27; of a regulation did not mean you have to interpret litterally in the most restrictive way possible what is written was often more a challange than getting a deal with compliance.Now imagine hundreds of projects over many decennia of these strata of integrations, and you will start to get the first glimpse of why replacing these core systems is such a challange. reply vanderZwan 14 hours agoprev> This position is the most important one in the bank, at least from a technical standpoint. If, let’s say, my mother and everyone on her team would quit their job, the bank would go under within a matter of weeks if they’re lucky.And given how big the market share of Nordea is in Sweden (and other Nordic countries, for that matter) that would probably bring down the Swedish, and possibly Nordic economy. Which would then impact a lot of the EU as well, I guess.Ever since reading this article I&#x27;ve wondered if these COBOL programmers that keep banks like this running are an enormously underestimated \"bus factor\" for many of the world&#x27;s economies, and what kind of back-up plans they have for such a scenario.[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nordea reply bear8642 9 hours agoparent>programmers that keep banks like this running are an enormously underestimated \"bus factor\" for many of the world&#x27;s economies, and what kind of back-up plans they have for such a scenario.Hmm, makes me wonder about potential similar situation with APL programmers - a language I know quite a few large companies use. reply glaucon 5 hours agorootparentWow, I&#x27;d forgotten APL existed, my finger-in-the-air sense is that while APL occupied&#x2F;occupies some important places in infrastructure the actual number of lines of code (and so, roughly, maintenance effort required) is significantly smaller ? I&#x27;m not doubting that what it does is important but just thinking that the size of the installed base that will, in time, need to be replaced is quite a bit smaller than COBOL. reply rsynnott 13 hours agoparentprev> and what kind of back-up plans they have for such a scenario.I mean, notoriously, stop working, for weeks: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;TSB_Bank_(United_Kingdom)#Migr...(I&#x27;ve got to assume that loss of institutional knowledge was a big factor in that fiasco.) reply TomK32 14 hours agoparentprevAs a programmer who has been hit by a bus (after riding on that same bus for five hours to it&#x27;s endstop in western Greece) I&#x27;d like to point out that getting hit and killed by a car is a much higher risk. Add the rise of SUV sales which are more dangerous to pedestrians and cyclists, and you might be as surprised as I am that banks don&#x27;t go down due to dead programmers but due to bad accounting and fraud... reply suslik 1 hour agoprevMy grandma has turned 95 this year. She still works in a research institute (ex-USSR) every day, writing numerical simulations in Fortran and Maple. Her colleagues still don&#x27;t want her to retire, and she postpones that every year. Her knowledge of numerical methods and mathematics is exceptional, and I am extremely proud of her. reply jgilias 17 minutes agoparentOh man! This is the coolest thing I’ve read in a week at least. I want to be like your Grandma when I’m 95.Thank you for sharing! reply shortsightedsid 14 hours agoprev> What’s your working environment like? - We’ve recently moved to a more “hip” location. We used to have personal desks, but now we have this “pick whatever spot is avaiable” open area. I dislike it a lot.Somehow this resonates a lot with me even if though I&#x27;ve never worked on Mainframes or anything like that. reply artemavv 14 hours agoparentI, too, strongly prefer having a personal desk. It is completely natural for a human to set up his environment according to their tastes and preferences, and I&#x27;m baffled that some office designers do not account for that. reply citrin_ru 1 hour agorootparentA think everyone prefers to have a personal desk but for hybrid (work from home with office days) it can be wasteful and a few days a month I could put up with a random desk. I see no point in forcing people use a random desk when everyone in the office most of the days. reply maximinus_thrax 10 hours agorootparentprevDepressing to see how low we&#x27;ve gotten. It&#x27;s not that people want an office (with a door) to minimize interruptions, you now need to advocate for your own fucking desk? reply tomcam 2 hours agorootparentThat jumped right out at me too. When I started at Microsoft a quarter century ago they did their best to get everyone in an office reply spelunker 13 hours agoprevMy mother-in-law worked her entire career in an IT department of an insurance company. She never really did much programming, but became a source of domain knowledge over time.She has all kinds of amusing stories of ye olden days. Before a proper computer system, everything was stored in physical documents of course, and her first project out of college was traveling to satellite offices to re-organize the file systems there to match the new strategy that HQ had come up with. The business of satellite office would grind to a halt while a team of people literally took every single document out, re-labeled it, and put it back. The whole project took over a year.That gave some perspective about my job, lol. reply posix86 10 hours agoparentGiven how banks were pushing the limits of paper back then, I can&#x27;t imagine the complexity of a modern banks. I just don&#x27;t know enough about finance to really even have an idea of where. reply pavel_lishin 14 hours agoprevI wish I had talked to my grandmother more before she developed dementia. I knew she was a mathematician and programmer, but briefly speaking to her a few years back, she mentioned that one of her jobs was calculating orbits for satellites in the Soviet Union.At least, I hope they were satellites! reply CapricornNoble 14 hours agoparentI&#x27;m in a similar boat. My mother worked with software simulations for the AEGIS system. Not very common work for black women in the 1980s. She earned her mathematics degree in 3 years, graduating at 20, and used to program computers with punch cards in the 70s.Now she can&#x27;t tie her shoes consistently.To everyone reading, capture the stories of your loved ones life accomplishments before it&#x27;s too late! reply Tor3 13 hours agorootparentThis. Not related to the main story here - I&#x27;m the first programmer in my family - but my great-grandfather was born in 1865 and lived a long life, he was blind for his last 30 or 40 years but had a phenomenal memory. A neighbor had the insight to use one of the very first tape recorders, just after WW2, together with my father who was a boy at that time, to record my great-grandfather&#x27;s stories about what he did in life. When my father retired he cobbled together a working tape recorder from two or three old ones, and copied the stories over to cassette, and later to CD. I listened to one of those recordings the other day - my great-grandfather told about how when he as a young boy, approximately around 1872-1873, he met a very old man who told about how he had learned the origin of the name of where they (he and my great-grandfather) lived, from another man when he was a boy. That origin turned out to be very different from what everyone these days are guessing, but immensely more plausible. Brought to me from the 18th century, via my great-grandfather. reply ca_tech 14 hours agoparentprevI would highly encourage anyone who has even thought about doing this; do it. If you are thinking about how to get started, I recommend the StoryCorp app. It is easy to use they have a bunch of prebuilt questions or you can create your own. You record directly to your phone but if you are inclined, you can upload your interview to the Library of Congress. https:&#x2F;&#x2F;storycorps.org&#x2F;participate&#x2F;storycorps-app&#x2F; reply pavel_lishin 9 hours agorootparentI actually interviewed my mom about ... 15 years ago? About some topics. But the fucking recordings are locked up in a proprietary recorder that requires software running on windows95 to unlock it, and I don&#x27;t even know if the software exists anymore.I should make that my December project, to find a way to unlock those files. reply wfvr 8 hours agorootparentCheck if they aren&#x27;t just xor&#x27;ed, I&#x27;ve seen that being used before as cheap \"encryption\". reply artemavv 14 hours agorootparentprevThere is also a project dedicated to saving the wisdom of earlier generations: https:&#x2F;&#x2F;savewisdom.org&#x2F;the-1000-word-save-wisdom-questions&#x2F; reply rsynnott 13 hours agoparentprev> At least, I hope they were satellites!Could be both: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fractional_Orbital_Bombardment... reply zengid 14 hours agoprev My first programming job was at a transportation enterprise with many programmers that had been working there 15-20+ years, many of whom mainly maintained the COBOL that ran the business. Many of those senior programmers were women; it felt like over 50% in the senior cohort. What really made me sad was that the younger programmers there were predominantly male (although still quite a few female!). That company mainly hired out of the local engineering colleges, so it was an interesting case study on how the number of women entering software engineering programs went down over the years.reply duderific 11 hours agoparentI recall hearing (can&#x27;t remember where) that in the olden days, computer programming was considered akin to typing or secretarial work, so at first it attracted mostly women. reply demondemidi 28 minutes agorootparentWomen code breakers were literally called “computers”:https:&#x2F;&#x2F;www.smithsonianmag.com&#x2F;science-nature&#x2F;history-human-... reply jll29 13 hours agoprevThanks for sharing the story of your developer mom - a very cool mother to have!COBOL is not a \"cool\" language, but mainframes have been around long enough to be \"retro cool\" now, and most run Linux at least as an optional OS under some virtualization (IBM Z).As a doctoral candidate in the noughties, I purchased a book about FORTRAN due to its \"retro-coolness\" and read it, and eventually took on a short university gig to earn the money that the book cost back, tutoring architects&#x2F;engineers in FORTRAN 95 for a bit, which was fun; to date, I could not bring myself to do the same with COBOL, though, or not yet.Because it is so verbose, if I had to use it I would probably write in another language and transpile to it. reply dajt 1 hour agoparentMy first jobs in the late 80s&#x2F;early 90s were COBOL programming. At my second one I finally convinced the management to put me onto the tools team who wrote the code generators for the &#x27;4GL&#x27;.One of the things I did was write a COBOL pre-processor (in C) to allow re-use of code and variables so the variables looked like locals rather than globals. reply flyinghamster 12 hours agoparentprevI&#x27;ve probably touched on this before, but back in the 1980s, absolutely none of the computer courses I took ever even so much as touched an IBM mainframe. I almost got the impression that there was an effort not to teach them.On the other hand, you could expect to learn a new operating system every year. Oh, we don&#x27;t use DEC gear at this school, check out our spiffy Prime running PRIMOS. Lather, rinse, repeat. reply spidermonkey23 12 hours agoprevMy company which uses mainframe and COBOL is undergoing a migration to Linux. This involves porting all the batch command scripts to bash and also fix quirks with COBOL compiler differences. At the end of it though the system is at least 4x faster and much more maintainable. This was the solution instead of doing a complete rewrite to Java that they backed out of some years before. reply dajt 1 hour agoparentIn the early 90s we ported our COBOL software from the Wang VS to various typs of UNIX by purchasing and extending a Wang VS runtime emulator.Then I wrote a terminal emulator for Windows 3.1 that would parse the forms sent down by the runtime and display them as something that looked like a Windows app. This wasn&#x27;t too difficult because mini & mainframe systems present data entry pages a whole screen at a time, like a windows form. reply bennysaurus 11 hours agoparentprevDo you know what they&#x27;re using? Are they going to Microficus COBOL or doing something different? reply morbicer 2 hours agoprevGreat read.Stories like this put in perspective people that are moaning about tech debt... by which they mean some functions written a year ago in a style they dislike. reply gcanyon 7 hours agoprevFunny, my mother was a mainframe COBOL programmer, from about 1968 until about 1975. Then she picked up FORTRAN, then she stopped programming around 1985. By 2000, she could barely handle the web, email, and a desktop.Keep your skills up, people! reply tomcam 2 hours agoparentI’m retired and I still keep my skills up. Just a tiny bit paranoid! reply guyzero 14 hours agoprevProgramming a mainframe while working from a hotdesk. Who would have imagined. reply orsenthil 8 hours agoprevUnrelated, I saw a link for medical book early in the article (at the start of the article). Did the author include it or is substack including it as an advertisement without mentioning it as an advertisement? reply dgadj38998 13 hours agoprev> ISPF is directly connected to the mainframe, and there’s no such thing as a local development environment here.That&#x27;s pretty crazySounds like having the whole team SSH into one server and doing all the work through the terminalI&#x27;m imagining editing my co-workers files and just removing a random semicolon to mess with them reply dajt 59 minutes agoparentThat was it. You had to organise amongst yourselves who was editing which program, and there was no version code control other than anything you could come up with using manual copies. It was primitive.I was using an Amiga at home and had been using Xenix and honest to god AT&T UNIX in my previous job so had some idea of what I was missing. I also used IBM S36s and weird Burroughs things so had a pretty wide education on different types of systems in a short time. reply flyinghamster 12 hours agoparentprevThat was the 1960s-80s way of working with computers. Multiple terminals connected to a single timesharing mainframe (or mini, for that matter). You might have a network like DECNET, but mostly it would be hardwired terminals, or if you were less lucky, a dialup modem and an acoustic coupler at a blazing 300 bps. You were sharing one machine with potentially dozens of other developers.Legacies of this abound in Linux (even the ability to SSH in is a descendant of this world). Commands like \"who\" are there to show who&#x27;s logged in, and there&#x27;s even an entire process accounting system that can be switched on to bill your users for CPU time. reply dgadj38998 12 hours agorootparentWas there a way to message other people who are logged in? Like a slack equivalent? reply flyinghamster 11 hours agorootparentOn Unix, there was the \"talk\" command for split-screen chat, and also \"write\" to just send a message to someone else&#x27;s terminal subject to permission with the \"mesg\" command. Not all systems had such provisions (or, in come cases, they had to be written by users). reply theodpHN 4 hours agorootparentprevLong before Slack, there was TSO SEND. :-)https:&#x2F;&#x2F;www.ibm.com&#x2F;docs&#x2F;en&#x2F;explorer-for-zos&#x2F;3.1.1?topic=mes... reply ianmcgowan 11 hours agorootparentprevtalk, or later ntalk in Unix. Or wall to send to everyone. I used a system in the 80&#x27;s (Pick DB) that had a send-message command. Also tandem which allowed you to take over someone&#x27;s terminal like screen-sharing now. If you were the super-user you could do it without asking for permission, and start typing messages to the unlucky user.. reply debo_ 5 hours agorootparentprevWe used `talk` all the time in our university Solaris labs when I was there from 2000-2005. Mostly insults, but it was fun. reply bennysaurus 11 hours agoparentprevThings is source code management these days even in mainframe but many shops still have you log into the same \"box\" to do your development. It can get pretty fun when trying to coordinate testing and patches. reply skissane 3 hours agorootparent> Things is source code management these days even in mainframeSource code management on mainframes has been around for decades now. Pansophic started selling Panvalet in 1970, and Broadcom still sells CA-Panvalet.Closer to RCS than to Git in feature set.> but many shops still have you log into the same \"box\" to do your developmentIt is very common to have separate LPARs for production, test and development, even if all three are running on the same physical hardware - the isolation is strong enough that it is rare for something running in one LPAR to cause a problem in another.The largest shops will have physically separate mainframes for production and non-production. reply daly 5 hours agoprevI taught COBOL while at grad school at UCONN. Methinks I&#x27;m getting old. reply derefr 12 hours agoprev> Banking systems are also extremely advanced. A personal bank account differs a lot from a business bank account, and there are at least 50 different types of bank accounts for each of them.I wouldn&#x27;t necessarily call that \"advanced\"... maybe more, lacking in requirements-analysis-time insight in ways to factor the business-domain into HAS-A component relationships, such that individual components and their ADTs can be shared across parent types [which are really just template&#x2F;factory objects for a smaller set of actual types], and initialized with simple parameters that get used as formula variables with no piecewise logic.To be clear, I write this as someone who works for a company that maintains a unified representation of data across the blockchain ecosystem — where each blockchain has its own peculiarities about what an \"account\" is, what a \"transaction\" can do, etc. Our data model only has one toplevel account type, one toplevel ledger-transaction type, etc. To handle the peculiarities, our data model instead has a large heirarchy of smaller data-objects hanging off of those toplevel ones; where any given data-object is sort of an \"optional extension\" that may or may not be there depending on how the toplevel object was created.This approach allows us to just have one unified code-path that treats every account like every other account, every tx like every other tx, etc. We don&#x27;t have duplicate code or a hierarchy of subclasses that all do things slightly differently; but instead, for any ledger-transaction, there may or may not be e.g. a strategy-pattern object hanging off that tx — and if there is, it gets used instead of the default static one. It&#x27;s great for maintainability, testability, predictability, cacheability, and hundreds of other things.I&#x27;d love to know if there&#x27;s any good reason that a bank would actually want \"50 different types of bank account\" on an implementation level, rather than these all boiling down to one type with varying values of certain state variables + presence&#x2F;absence of certain foreign-key relationships.Other than maybe \"some of these account types are actually a part of completely different data models living in third-party systems, that we acquired, and then never merged into our own systems.\" ;) reply boricj 12 hours agoparent> I&#x27;d love to know if there&#x27;s any good reason that a bank would actually want \"50 different types of bank account\" on an implementation level.Not having the benefit of hindsight?Some banks are hundreds of years old. Most of them were computerized seventy years ago if not earlier, back in the stone age of computers. These kinds of banks won&#x27;t bet the house on a newfangled system every couple of years because some bright-eyed engineer told them it&#x27;s the trend nowadays.Not messing up their bookkeeping is their number one priority. People would riot if their bank told them \"sorry, we no longer know how much money you had deposited with us\". reply fl7305 10 hours agorootparent> Some banks are hundreds of years old.Since the post is about a Swedish bank, it might be interesting to note that the central bank in Sweden was founded in 1668.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sveriges_RiksbankKinda makes you wonder if anyone opened an account then and deposited a dollar (\"daler\" in Swedish), and let it sit and accrue interest for the family for 350 years. reply hyggetrold 14 hours agoprevI&#x27;m very curious about IMS - a mentor described it as a graph database of sorts back in its day. I was told it was a great system (at least for its time). Does anyone have a perspective? reply one_buggy_boi 7 hours agoparentOne of my first projects was to interface with an IMS system using Python of all things, there was only one guy that really knew the system and I had to sit with them for hours to even begin wrapping my head around how a hierarchical DBMS worked, how space is managed, etc. I remember the first comment he made was that the database was created for use as an inventory system for the Apollo missions. The DB I had to work with was created in the 80&#x27;s. This project was in 2021, so it&#x27;s still out there, supporting critical infrastructure. reply epc 13 hours agoparentprevHierarchical, not relational, not what I’d call a graph database. Pre-SQL.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;IBM_Information_Management_Sys... reply hyggetrold 13 hours agorootparentYeah I think what they actually called it was \"network database\" which I thought maybe was like a graph. reply skissane 1 hour agorootparentIBM IMS isn&#x27;t a network model database, it is a hierarchical model database.The heyday of network model databases was after hierarchical, but before relational took over - basically the 1970s.Hierarchical databases basically allow you to have parent-child relationships between tables, to represent many-to-one relations - something people commonly model nowadays using RDBMS foreign keys, but RDBMS foreign keys are much more flexible and can model other relationship types too, whereas hierarchical only allows parent-child.Network model databases generalised hierarchical databases to allow more complex relationships than just parent-child. The biggest difference between them and the relational model, is they didn&#x27;t have a query language like SQL, they offered a procedural API for processing one record at a time and manually navigating the links from table to table. Network model was standardised by CODASYL-the same committee which invented COBOL, with the result that they became particularly popular with COBOL applications.The most famous network model database is probably Broadcom&#x27;s IDMS (previously known as CA IDMS), which nowadays only runs on IBM and Fujitsu mainframes, but in past decades was supported on other mainframe and minicomputer platforms as well. Oracle also offers a CODASYL network model database for OpenVMS, which it got from Digital when it bought Rdb.Graph databases have something in common with the historical network model, but some significant differences (1) graph databases have much more flexible schemas (defining arbitrary properties on a node as opposed to a rigid structure of fixed format record types), (2) graph databases generally have a query language with support for graph traversal operations, as opposed to the procedural API the network data model offered. reply boilerupnc 11 hours agoprevRecently announced, there’s also now a generative AI tool to assist developers with their COBOL modernization journey to Java [0]. I’ve heard pretty decent reviews on its effectiveness.[0]. https:&#x2F;&#x2F;www.ibm.com&#x2F;products&#x2F;watsonx-code-assistant-z reply ape4 12 hours agoprevI surprised banks don&#x27;t have a domain-specific language (DSL) to describe their businesses. Instead each bank has to code \"savings account\" in COBOL. reply dan-robertson 11 hours agoparentWhat domain do you think COBOL was made for?Basically the only things computers were worth it for back then were scientific calculations and bookkeeping. Banking is obviously on the more complicated end of bookkeeping. Insurance did get a special COBOL variant because I guess they had some tasks shaped more like scientific calculations too. reply ravenstine 8 hours agoparentprevIt&#x27;s an interesting idea, but I&#x27;m not sure how that&#x27;s the least bit surprising. reply skissane 1 hour agoparentprev> Instead each bank has to code \"savings account\" in COBOLAt most banks nowadays, basic banking functions such as saving accounts are provided by off-the-shelf software, not written from scratch.That&#x27;s not to say that nothing gets written from scratch, but it is generally adding code to support the more exotic product offerings, integrations with other systems, institution-specific business rules and processes, etc - any off the shelf banking system is going to have basic bread-and-butter stuff like support for savings accounts already included.This is true even for COBOL-based banking systems. Historically, many banks used CSC Hogan (now DXC Hogan)-and while many have moved away from it, some are still on it. Hogan runs on IBM mainframes (z&#x2F;OS), and is written in COBOL and CICS. Basic stuff like savings accounts is supported by the vendor-provided COBOL code, but Hogan sites often end up writing their own custom COBOL code to support their own unique requirements.> I surprised banks don&#x27;t have a domain-specific language (DSL) to describe their businesses.There is a long history of 4GL&#x27;s being used in banking, going back decades. In prior decades, many of these 4GLs worked by generating COBOL code. Some of them were general-purpose, and used across many industries; others were exclusively used in banking. But, even those exclusively used in banking, rarely (to my knowledge) contained domain-specific banking features, and as such I&#x27;m not sure they really count as DSLs.To give a specific example, FIS Global&#x27;s core banking platform, Profile, is written in MUMPS (actually the open-source variant GT.M). Due to how horrid MUMPS is as a language, they created their own higher-level object-oriented language which compiles to MUMPS, called PSL (Profile Scripting Language), and gradually rewrote their banking platform in it. An old version of PSL was open-sourced over 10 years ago on Sourceforge, if anyone wants to look at it. [0] I don&#x27;t think PSL itself has anything really banking-specific in it per se, but as is inevitable with a language developed to support a single application, the boundary between the language and the application is a little blurry, and you&#x27;ll find a lot of banking-specific stuff in the open source release (possibly included by accident), especially inside the binary GT.M database dump it ships with.I think the areas in which truly domain-specific languages are strongest in finance - such as modelling financial contracts - are generally the furthest away from the COBOL legacy.[0] https:&#x2F;&#x2F;sourceforge.net&#x2F;projects&#x2F;pip&#x2F;files&#x2F;PIP&#x2F;V0.2&#x2F; see also more easily digestible (somewhat improved) Git mirror at https:&#x2F;&#x2F;gitlab.com&#x2F;YottaDB&#x2F;DBMS&#x2F;YDBPIP&#x2F; reply hnthrowaway0315 14 hours agoprev1TB transaction per year seems pretty small data. I guess IBM stuffs are there for the stability then? reply dan-robertson 11 hours agoparentUsing DB&#x2F;2 surely makes a lot of sense for integration with the rest of their IBM tech but also something the new system could presumably talk to too. I would guess that it performs ok at the kind of ‘single big OLTP database server’ tasks it was meant for. Notably IBM allows you to publish benchmarks for their databases (so long as you provide detailed methodology and take steps to tune it correctly) whereas Oracle and SQLServer do not allow publishing benchmarks without permission. Though one could argue this is just IBM feeling like benchmark performance wouldn’t matter much to their customers. reply awestroke 13 hours agoparentprevThe IBM stuff is pure legacy. Of course this would all perform much better with modern tools, like Postgres. reply accra4rx 8 hours agorootparentJust because IBM stuff is legacy doesnot means it is slow. Almost nobody is moving from a commercial database (Db2 for z&#x2F;OS) to PostgreSQL because of performance. It is all to do with huge licensing cost. Talk to any veterans nothing matches the speed and stability of mainframe. search about z16 (tell me anything that guarantees 9 nines of availability) reply otteromkram 14 hours agoprevThis is great! I wonder how the update(s) went since the article was written in 2016 and they mentioned 4 years per country, or 16 years total. COVID-19 probably threw a wrench in that plan, but how big of an impact is another interesting question. reply dfee 14 hours agoprev(meta) As a platform, substack content must follow the power law, right? That is, a few authors are most productive and there’s a long tail of single (or zero) article substacks.This article was migrated from medium, but now the author gets to collect email addresses. But why subscribe? There are two articles. Is substack an RSS replacement (potentially with a paywall)? Maybe I should just consider substack to be a low barrier to entry blogging platform that can be set to your personal domain for $50?Definitely not trying to beat up on the offer, just curious about the real value prop of substack for “most” producers and if that’s aligned with my goals as a reader who sees a email collection modal on every visit. reply syngrog66 13 hours agoprevthe 1st professional programmer I knew was a woman who did COBOL for a bankits struck me as interesting ever since. because over the course of my life and career since its seemed that 99%+ of the programmers I knew of were male.I don&#x27;t think there is any one right&#x2F;ideal breakdown by gender, so I just like to have best sense for facts on the ground reply dan-robertson 11 hours agoparentI think one thing is that the programmer job somewhat evolved from data entry and clerical roles that employed plenty of women already. So in some sense it didn’t start as a ‘man’s job’. For slightly later generations of programmers, one can observe that computer science degrees peaked near 40% female in the mid ’80s in the US[1]. It’s closer to 20% now and there are much more graduates today so you should probably expect a proportion somewhere around 20% of working CS grads. That number doesn’t seem crazily unrealistic to me for working programmers, though obviously it varies between companies. <1% does seem unrealistic to me. I’m slightly surprised that that’s been your experience (I assume you aren’t exaggerating because you want a sense for facts on the ground).[1] eg https:&#x2F;&#x2F;www.gcu.edu&#x2F;blog&#x2F;gcu-experience&#x2F;analysis-women-compu... reply christkv 13 hours agoprevMan my first job was at Nordea 2000-2003, looking back at it now it was pretty dysfunctional. They were implementing a new everything but the kitchen sink framework that would unify all the customer facing e-banking and it was a mess with an external company providing the framework together with internal architects. In the 3 years I was there I think pretty much zero was delivered.The you had the markets division that would hire in externals to do in parallel dev and then drop the burning corps of the project on internal dev once they moved onto the next shining thing lol. reply brightball 14 hours agoprev [–] . reply rajamaka 14 hours agoparentVery creepy. I was talking about programming and alas I com to HN and topics related to programming are ob the front page! reply brightball 14 hours agorootparentAnd yet, you don&#x27;t see much about mainframes and COBOL on here ever...nor do I often have lunch conversations about it.The timing was just interesting. reply debo_ 5 hours agoparentprev [–] You have a point. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article features an interview with the author's mother, a mainframe COBOL programmer for a bank in the EU.",
      "The interview highlights the significance of her role, challenges with outdated technology, and the bank's plan for transitioning to a newer database.",
      "It also discusses the bank's struggles with managing large amounts of data, the aging programmer population, and the complexity of banking systems."
    ],
    "commentSummary": [
      "Discussions on a forum highlight challenges faced by young programmers and the value of institutional knowledge in mainframe COBOL programming in the banking industry.",
      "The integration of old and new systems, the difficulty of finding skilled COBOL programmers, and personal anecdotes about working in the tech industry are also discussed.",
      "Additionally, topics include preserving wisdom from earlier generations, the decline of women in programming, and the migration to new operating systems in the banking industry."
    ],
    "points": 356,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1702578861
  },
  {
    "id": 38643076,
    "title": "FunSearch: Using LLMs to Uncover New Mathematical Insights",
    "originLink": "https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/",
    "originBody": "Research FunSearch: Making new discoveries in mathematical sciences using Large Language Models Published 14 December 2023 Authors Alhussein Fawzi and Bernardino Romera Paredes Share By searching for “functions” written in computer code, FunSearch made the first discoveries in open problems in mathematical sciences using LLMs Large Language Models (LLMs) are useful assistants - they excel at combining concepts and can read, write and code to help people solve problems. But could they discover entirely new knowledge? As LLMs have been shown to “hallucinate” factually incorrect information, using them to make verifiably correct discoveries is a challenge. But what if we could harness the creativity of LLMs by identifying and building upon only their very best ideas? Today, in a paper published in Nature, we introduce FunSearch, a method to search for new solutions in mathematics and computer science. FunSearch works by pairing a pre-trained LLM, whose goal is to provide creative solutions in the form of computer code, with an automated “evaluator”, which guards against hallucinations and incorrect ideas. By iterating back-and-forth between these two components, initial solutions “evolve” into new knowledge. The system searches for “functions” written in computer code; hence the name FunSearch. This work represents the first time a new discovery has been made for challenging open problems in science or mathematics using LLMs. FunSearch discovered new solutions for the cap set problem, a longstanding open problem in mathematics. In addition, to demonstrate the practical usefulness of FunSearch, we used it to discover more effective algorithms for the “bin-packing” problem, which has ubiquitous applications such as making data centers more efficient. Scientific progress has always relied on the ability to share new understanding. What makes FunSearch a particularly powerful scientific tool is that it outputs programs that reveal how its solutions are constructed, rather than just what the solutions are. We hope this can inspire further insights in the scientists who use FunSearch, driving a virtuous cycle of improvement and discovery. Driving discovery through evolution with language models FunSearch uses an evolutionary method powered by LLMs, which promotes and develops the highest scoring ideas. These ideas are expressed as computer programs, so that they can be run and evaluated automatically. First, the user writes a description of the problem in the form of code. This description comprises a procedure to evaluate programs, and a seed program used to initialize a pool of programs. FunSearch is an iterative procedure; at each iteration, the system selects some programs from the current pool of programs, which are fed to an LLM. The LLM creatively builds upon these, and generates new programs, which are automatically evaluated. The best ones are added back to the pool of existing programs, creating a self-improving loop. FunSearch uses Google’s PaLM 2, but it is compatible with other LLMs trained on code. The FunSearch process. The LLM is shown a selection of the best programs it has generated so far (retrieved from the programs database), and asked to generate an even better one. The programs proposed by the LLM are automatically executed, and evaluated. The best programs are added to the database, for selection in subsequent cycles. The user can at any point retrieve the highest-scoring programs discovered so far. Discovering new mathematical knowledge and algorithms in different domains is a notoriously difficult task, and largely beyond the power of the most advanced AI systems. To tackle such challenging problems with FunSearch, we introduced multiple key components. Instead of starting from scratch, we start the evolutionary process with common knowledge about the problem, and let FunSearch focus on finding the most critical ideas to achieve new discoveries. In addition, our evolutionary process uses a strategy to improve the diversity of ideas in order to avoid stagnation. Finally, we run the evolutionary process in parallel to improve the system efficiency. Breaking new ground in mathematics We first address the cap set problem, an open challenge, which has vexed mathematicians in multiple research areas for decades. Renowned mathematician Terence Tao once described it as his favorite open question. We collaborated with Jordan Ellenberg, a professor of mathematics at the University of Wisconsin–Madison, and author of an important breakthrough on the cap set problem. The problem consists of finding the largest set of points (called a cap set) in a high-dimensional grid, where no three points lie on a line. This problem is important because it serves as a model for other problems in extremal combinatorics - the study of how large or small a collection of numbers, graphs or other objects could be. Brute-force computing approaches to this problem don’t work – the number of possibilities to consider quickly becomes greater than the number of atoms in the universe. FunSearch generated solutions - in the form of programs - that in some settings discovered the largest cap sets ever found. This represents the largest increase in the size of cap sets in the past 20 years. Moreover, FunSearch outperformed state-of-the-art computational solvers, as this problem scales well beyond their current capabilities. Interactive figure showing the evolution from the seed program (top) to a new higher-scoring function (bottom). Each circle is a program, with its size proportional to the score assigned to it. Only ancestors of the program at the bottom are shown. The corresponding function produced by FunSearch for each node is shown on the right (see full program using this function in the paper). These results demonstrate that the FunSearch technique can take us beyond established results on hard combinatorial problems, where intuition can be difficult to build. We expect this approach to play a role in new discoveries for similar theoretical problems in combinatorics, and in the future it may open up new possibilities in fields such as communication theory. FunSearch favors concise and human-interpretable programs While discovering new mathematical knowledge is significant in itself, the FunSearch approach offers an additional benefit over traditional computer search techniques. That’s because FunSearch isn’t a black box that merely generates solutions to problems. Instead, it generates programs that describe how those solutions were arrived at. This show-your-working approach is how scientists generally operate, with new discoveries or phenomena explained through the process used to produce them. FunSearch favors finding solutions represented by highly compact programs - solutions with a low Kolmogorov complexity†. Short programs can describe very large objects, allowing FunSearch to scale to large needle-in-a-haystack problems. Moreover, this makes FunSearch’s program outputs easier for researchers to comprehend. Ellenberg said: “FunSearch offers a completely new mechanism for developing strategies of attack. The solutions generated by FunSearch are far conceptually richer than a mere list of numbers. When I study them, I learn something”. What’s more, this interpretability of FunSearch’s programs can provide actionable insights to researchers. As we used FunSearch we noticed, for example, intriguing symmetries in the code of some of its high-scoring outputs. This gave us a new insight into the problem, and we used this insight to refine the problem introduced to FunSearch, resulting in even better solutions. We see this as an exemplar for a collaborative procedure between humans and FunSearch across many problems in mathematics. Left: Inspecting code generated by FunSearch yielded further actionable insights (highlights added by us). Right: The raw “admissible” set constructed using the (much shorter) program on the left. The solutions generated by FunSearch are far conceptually richer than a mere list of numbers. When I study them, I learn something. Jordan Ellenberg, collaborator and professor of mathematics at the University of Wisconsin–Madison Addressing a notoriously hard challenge in computing Encouraged by our success with the theoretical cap set problem, we decided to explore the flexibility of FunSearch by applying it to an important practical challenge in computer science. The “bin packing” problem looks at how to pack items of different sizes into the smallest number of bins. It sits at the core of many real-world problems, from loading containers with items to allocating compute jobs in data centers to minimize costs. The online bin-packing problem is typically addressed using algorithmic rules-of-thumb (heuristics) based on human experience. But finding a set of rules for each specific situation - with differing sizes, timing, or capacity – can be challenging. Despite being very different from the cap set problem, setting up FunSearch for this problem was easy. FunSearch delivered an automatically tailored program (adapting to the specifics of the data) that outperformed established heuristics – using fewer bins to pack the same number of items. Pause Play Illustrative example of bin packing using existing heuristic – Best-fit heuristic (left), and using a heuristic discovered by FunSearch (right). Hard combinatorial problems like online bin packing can be tackled using other AI approaches, such as neural networks and reinforcement learning. Such approaches have proven to be effective too, but may also require significant resources to deploy. FunSearch, on the other hand, outputs code that can be easily inspected and deployed, meaning its solutions could potentially be slotted into a variety of real-world industrial systems to bring swift benefits. LLM-driven discovery for science and beyond FunSearch demonstrates that if we safeguard against LLMs’ hallucinations, the power of these models can be harnessed not only to produce new mathematical discoveries, but also to reveal potentially impactful solutions to important real-world problems. We envision that for many problems in science and industry - longstanding or new - generating effective and tailored algorithms using LLM-driven approaches will become common practice. Indeed, this is just the beginning. FunSearch will improve as a natural consequence of the wider progress of LLMs, and we will also be working to broaden its capabilities to address a variety of society’s pressing scientific and engineering challenges. Learn more about FunSearch Read the open access copy of the paper* Read our paper in Nature Acknowledgements: Matej Balog, Emilien Dupont, Alexander Novikov, Pushmeet Kohli, Jordan Ellenberg for valuable feedback on the blog and for help with the figures. This work was done by a team with contributions from: Bernardino Romera Paredes, Amin Barekatain, Alexander Novikov, Matej Balog, Pawan Mudigonda, Emilien Dupont, Francisco Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, George Holland, Pushmeet Kohli and Alhussein Fawzi. *This is the author’s version of the work. It is posted here by permission of Nature for personal use, not for redistribution. The definitive version was published in Nature: DOI: 10.1038/s41586-023-06924-6. †Kolmogorov complexity is the length of the shortest computer program outputting the solution. Related posts View all posts MuZero, AlphaZero, and AlphaDev: Optimizing computer systems As part of our aim to build increasingly capable and general artificial intelligence (AI) systems, we’re working to create AI tools with a broader understanding of the world. This can allow useful... Research Discovering novel algorithms with AlphaTensor In our paper, published today in Nature, we introduce AlphaTensor, the first artificial intelligence (AI) system for discovering novel, efficient, and provably correct algorithms for fundamental... 5 October 2022",
    "commentLink": "https://news.ycombinator.com/item?id=38643076",
    "commentBody": "FunSearch: Making new discoveries in mathematical sciences using LLMsHacker NewspastloginFunSearch: Making new discoveries in mathematical sciences using LLMs (deepmind.google) 323 points by reqo 17 hours ago| hidepastfavorite78 comments jcgrillo 10 hours agoTo what extent is an LLM necessary here? As far as I can tell (and perhaps I haven&#x27;t looked closely enough yet) the purpose of the LLM here is to generate things that look plausibly like python functions conforming to a given type signature.But it should be possible to generate random, correct python functions conforming to a given type signature without an LLM. This would be an exercise like [1], just with a substantially more complex language. But might a restricted language be more ergonomic? Something like PushGP [2]?So I guess my questions would be:(1) What&#x27;s the value add of the LLM here? Does it substantially reduce the number of evaluations necessary to converge? If so, how?(2) Are other genetic programming techniques less competitive on the same problems? Do they produce less fit solutions?(3) If a more \"traditional\" genetic programming approach can achieve similar fitness, is there a difference in compute cost, including the cost to train the LLM?[1] http:&#x2F;&#x2F;www.davidmontana.net&#x2F;papers&#x2F;stgp.pdf [2] https:&#x2F;&#x2F;faculty.hampshire.edu&#x2F;lspector&#x2F;push.html reply janalsncm 5 hours agoparent> it should be possible to generate random, correct python functions conforming to a given type signature without an LLMThe state space of viable programs is far, far larger than useful ones. You need more than monkeys and typewriters. The point of using Palm2 here is that you don’t want your candidates to be random, you want them to be plausible so you’re not wasting time on nonsensical programs.Further, a genetic algorithm with random program generation will have a massive cold start problem. You’re not going to make any progress in the beginning, and probably ever, if the fitness of all of your candidates is zero. reply jcgrillo 5 hours agorootparentThat&#x27;s an interesting hypothesis, but the work under discussion here didn&#x27;t make an attempt to test it. You make two claims:(1) That using an LLM (Palm2) somehow \"saved time\" by avoiding \"nonsensical programs\"(2) That there exists a \"cold start problem\" (which, presumably, the LLM somehow solves).Yet not the paper, nor the blog post, nor the code on github support these. reply janalsncm 2 hours agorootparentThe fact that LLMs generate text which is better than random is not something the authors need to spell out. Reducing the cross entropy between predicted and target distributions is the entire purpose of training.Put another way: a flat probability distribution from an untrained network is equivalent to the random decoding you’re skeptical an LLM is better than. That’s not a criticism the authors’ peers would likely put forward. reply og_kalu 2 hours agorootparentprevWhat paper are you reading exactly ?There&#x27;s a clear comparisonhttps:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;xerOOLn reply jcgrillo 2 hours agorootparentSame paper you are. This is comparing apples to oranges. Their \"random\" strategy is only capable of assembling very constrained python programs whereas the LLM is coming up with things that look like the sort of stuff a person might write, utilizing the whole language (modulo hallucinatory nonsense). So it&#x27;s not a reasonable comparison at all.Instead, for the purpose of making something resembling a reasonable comparison, it would make a lot more sense to train an LLM on programs written in the same language they allow their \"random\" strategy to use. I wouldn&#x27;t recommend trying this in python, as it&#x27;s way too big a language. Something smaller and more fit for purpose would likely be much more tractable. There are multiple examples from the literature to choose from.EDIT: To be clear--it&#x27;s obvious the LLM solution outperformed the hand-rolled solution. By a large margin. What would actually be an interesting scientific question, though, is to what extent is the success attributable to the LLM? That&#x27;s not possible to answer given the results, because the hand rolled-solution and the LLM solution are literally speaking different languages. reply janalsncm 2 hours agorootparentThe purpose of the ablation is to demonstrate the value-add of using an LLM. Comparing to another type of candidate generator wouldn’t be ablation anymore, it would just be another study.As I have mentioned previously, using an LLM addresses the cold start problem by immediately generating plausible-looking programs. The reason random token selection is limited to constrained programs is that valid programs are statistically unlikely to occur.And the ablation directly demonstrates that the LLM is better than random. replynopinsight 4 hours agoparentprevGenetic algorithms, even with programmed constraints, will come up with many non-sensical programs, although they could be mostly syntactically correct (given sufficient effort).The difference LLMs make here is to limit the space of possible mutations to largely semantically plausible programs.The above should answer your 1) and 2).On 3), a trained LLM is useful for many, many purposes, so the cost of training it from scratch after amortization wouldn&#x27;t be significant. It may cost additionally to fine-tune an LLM to work in the FunSearch framework but fine-tuning cost is quite minimal. Using it in the framework is likely a win over genetic programming alone. reply jcgrillo 4 hours agorootparent> The difference LLMs make here is to limit the space of possible mutations to largely semantically plausible programs.Yet, sadly, the work under question here didn&#x27;t make any attempt to show this, AFAICT. It&#x27;s an interesting hypothesis, but as yet untested.> a trained LLM is useful for many, many purposes, so the cost of training it from scratch after amortization wouldn&#x27;t be significant. It may cost additionally to fine-tune an LLM to work in the FunSearch framework but fine-tuning cost is quite minimal. Using it in the framework is likely a win over genetic programming alone.Again, an interesting hypothesis that probably could be investigated with the experimental apparatus described in the paper. But it wasn&#x27;t. Have you? reply nopinsight 4 hours agorootparent>> The difference LLMs make here is to limit the space of possible mutations to largely semantically plausible programs.> Yet, sadly, the work under question here didn&#x27;t make any attempt to show this, AFAICT. It&#x27;s an interesting hypothesis, but as yet untested.It would be ideal to do that. But given the way LLMs work, it is almost a given. This could be the reason it didn&#x27;t occur to the researchers who are very familiar with LLMs.>> a trained LLM is useful for many, many purposes, so the cost of training it from scratch after amortization wouldn&#x27;t be significant. It may cost additionally to fine-tune an LLM to work in the FunSearch framework but fine-tuning cost is quite minimal. Using it in the framework is likely a win over genetic programming alone.> Again, an interesting hypothesis that probably could be investigated with the experimental apparatus described in the paper. But it wasn&#x27;t. Have you?Perhaps it&#x27;s quite evident to people in the trench, as the hypothesis seems very plausible to me. A numerical confirmation would be ideal as you suggested.Given my management experience, I&#x27;d say it&#x27;s smart of them to focus their limited resources on something more innovative in this exploratory work. Details can be left for future work or worked out by other teams. reply jcgrillo 3 hours agorootparent> almost a givenNot a scientifically compelling argument.> limited resourcesWhat? This is Google we&#x27;re talking about. Right? reply nopinsight 1 hour agorootparentLimited resources: time of highly competent researchers, management timeScience takes time. A single paper needs not completely answer everything. Also, in many fields, it’s common to find out what works before why it works. replynybsjytm 8 hours agoparentprevThe found function is in here: https:&#x2F;&#x2F;github.com&#x2F;google-deepmind&#x2F;funsearch&#x2F;blob&#x2F;main&#x2F;cap_s.... I&#x27;m not very familiar with genetic algorithms but it&#x27;s pretty hard for me to imagine that they couldn&#x27;t come up with this. I&#x27;d be surprised if too many people (if any) have tried.On the other hand, as observed in Appendix A.2 of this paper, the non-LLM genetic approach would have to be engineered by hand more than the LLM approach. reply jcgrillo 8 hours agorootparentI guess what I&#x27;m really trying to get at is this seems like a huge missed opportunity to show their LLM is actually doing something cool. Like if it somehow significantly outperforms established genetic programming update strategies that should be pretty easy to demonstrate given their experimental setup, but no attempt was made. That&#x27;s kinda bizarre... like, in what sense is this an advancement of the field? reply resource0x 7 hours agorootparentThey need to show something to keep the excitement around Google&#x27;s upcoming AI alive. Expect a slew of (non-)discoveries to continue unabated going forward. reply mejutoco 1 hour agoparentprev> (1) What&#x27;s the value add of the LLM here? Does it substantially reduce the number of evaluations necessary to converge? If so, how?I thought that stochastic (random) gradient descent and LLM were converging much quicker than genetic programming. Definitely much quicker than random search. reply nybsjytm 12 hours agoprevSome important context: the discovery is that a certain number in combinatorics is now known to be between 2.2202 and 2.756, not just between 2.218 and 2.756 as discovered last year. The improvement is by finding some particular sequences of numbers which have some special properties, not by a logic-heavy mathematical proof. (That doesn&#x27;t mean it&#x27;s unrigorous.)But it&#x27;s an interesting and possibly useful method of coming up with examples, pretty much a genetic algorithm with LLMs. reply ignoramous 16 hours agoprevRelated commentary on \"self-play\" by Subbarao: https:&#x2F;&#x2F;twitter.com&#x2F;rao2z&#x2F;status&#x2F;1728121216479949048From the article: FunSearch uses an evolutionary method powered by LLMs, which promotes and develops the highest scoring ideas. These ideas are expressed as computer programs, so that they can be run and evaluated automatically. The user writes a description of the problem in the form of code. This description comprises a procedure to evaluate programs, and a seed program used to initialize a pool of programs. At each iteration, FunSearch selects some programs from the current pool. The LLM creatively builds upon these, and generates new programs, which are automatically evaluated. The best ones are added back to the pool of existing programs, creating a self-improving loop.For websearch, I (evaluator) use pplx.ai and phind.com in a similar manner. Ask it a question (seed) and see what references it brings up (web links). Refine my question or ask follow-ups (iterate) so it pulls up different or more in-depth references (improve). Works better in unearthing gems than sifting through reddit or Google.Given Tech Twitter has amazing content too, looking forward to using Grok for research, now that it is open to all. reply alphabetting 15 hours agoprevhttps:&#x2F;&#x2F;twitter.com&#x2F;gfodor&#x2F;status&#x2F;1735348301812383906>If DeepMind just definitively proved neural networks can generate genuinely new knowledge then it’s the most important discovery since fire.If this were actually the case why wouldn&#x27;t everyone be talking about this? I am impressed it was done on Palm 2 given that&#x27;s less advanced than GPT-4 and Gemini. Will be wild to see what the next few generations of models can do utilizing methods like this. reply Imnimo 15 hours agoparentThe heavy lifting here is done by the evolutionary algorithm. The LLM is just being asked \"propose some reasonable edits to these 20 lines of python\" to replace a random mutation operator. It feels generous to credit the neural network with the knowledge generation here.It&#x27;s also highly dependent on the nature of the problem, even beyond the need to have the \"hard to generate, easy to evaluate\" structure. You have to be able to decompose the problem in such a way that a very short Python function is all that you want to evolve. reply nopinsight 13 hours agorootparentHaving the right hunches on what to try based on accumulated knowledge & experiences is a key thing that distinguishes masters from apprentices.A fun story from a UCLA math PhD: “terry tao was on both my and my brother&#x27;s committee.he solved both our dissertation problems before we were done talking, each of us got \"wouldn&#x27;t it have been easier to...outline of entire proof\"”https:&#x2F;&#x2F;twitter.com&#x2F;AAAzzam&#x2F;status&#x2F;1735070386792825334Current LLMs are far from Terence Tao but Tao himself wrote this:“The 2023-level AI can already generate suggestive hints and promising leads to a working mathematician and participate actively in the decision-making process. When integrated with tools such as formal proof verifiers, internet search, and symbolic math packages, I expect, say, 2026-level AI, when used properly, will be a trustworthy co-author in mathematical research, and in many other fields as well.”https:&#x2F;&#x2F;unlocked.microsoft.com&#x2F;ai-anthology&#x2F;terence-tao&#x2F; reply Imnimo 13 hours agorootparentDo you think the role the LLM plays in this system is analogous to what Tao is talking about? reply nopinsight 13 hours agorootparentWhat Tao does when proposing an idea most likely encapsulates much more than what a current LLM does. I’m no Terence Tao but I sometimes come up with useful ideas. In a more complex case, I revise those ideas in my head and sometimes on paper several times before they become useful (analogous to using evolutionary algorithms).However, it is impractical to think consciously of all possible variations. So the brain only surfaces ones likely to be useful. This is the role an LLM plays here.An expert or an LLM with more relevant experiences would be better at suggesting these variations to try. Chess grandmasters often don’t consciously simulate more possibilities than novices. reply Imnimo 12 hours agorootparentCritically, though, the LLM is not acting as a domain expert here. It&#x27;s acting as a code mutator. The expertise it brings to the table is not mathematical - Codey doesn&#x27;t have any special insight into bin packing heuristics. It&#x27;s not generating \"suggestive hints and promising leads\", it&#x27;s just help the evolutionary search avoid nonsense code mutations. reply nopinsight 12 hours agorootparentAn LLM serves as a sort of “expert” programmer here. Programming itself is a pretty complex domain in which a purely evolutionary algorithm isn’t efficient.We can imagine LLMs trained in mathematical programming or a different domain playing the same role more effectively in this framework. reply Imnimo 12 hours agorootparentI disagree that the type of coding the LLM is doing here is \"expert\" level in any meaningful sense. Look for example at the code for the bin-packing heuristics:https:&#x2F;&#x2F;github.com&#x2F;google-deepmind&#x2F;funsearch&#x2F;blob&#x2F;main&#x2F;bin_p...These aren&#x27;t complex or insightful programs - they&#x27;re pretty short simple functions, of the sort you typically get from program evolution. The LLM&#x27;s role here is just proposing edits, not leveraging specialized knowledge or even really exercising the limits of existing LLMs&#x27; coding capabilities. reply nopinsight 12 hours agorootparentTo be clearer, the word “expert” in my comment above is in quotation marks because an LLM has more programming expertise than a non-programmer or an evolutionary algorithm, but not anywhere near a true expert programmer. replyog_kalu 15 hours agorootparentprevThere&#x27;s nothing generous about it. It wouldn&#x27;t be possible without the LLM. Traditional Computational solvers fall well short as they say and demonstrate.>The LLM is just being asked \"propose some reasonable edits to these 20 lines of python\" to replace a random mutation operator.Hahaha \"Just\". reply Imnimo 14 hours agorootparentWe can see from the ablations that simply asking the LLM to generate a large number of potential solutions without the evolutionary search performs terribly.It&#x27;s certainly true that the LLM mutator produces more reasonable edits than a random mutator. But the value the LLM is bringing is that it knows how to produce reasonable-looking programs, not that it has some special understanding of bin packing or capset finding.The only thing the LLM sees is two previously generated code samples, labeled _v0 and _v1 and then a header for _v2 which it fills in. Look at Extended Data Fig. 1. reply og_kalu 14 hours agorootparentWith the current competency or state of the models today, both are necessary, nobody is denying that.It doesn&#x27;t take a genius however to see who the more valuable contributor to the system is. The authors say as much when they anticipate much of any improvement to come from capabilities of better models than any change to the search algorithm. Gpt-4 may well have reduced the search time by half and so on.>But the value the LLM is bringing is that it knows how to produce reasonable-looking programs, not that it has some special understanding of bin packing or capset finding.So it is a general problem solver then, great. That&#x27;s what we want. reply Imnimo 14 hours agorootparent>It doesn&#x27;t take a genius however to see who the more valuable contributor to the system is.Indeed, we can look at the ablations and see that swapping out a weaker code LLM (540B Codey to 15B Starcoder) makes little difference:\"This illustrates that FunSearch is robust to the choice of the model as long as it has been trained sufficiently well to generate code.\">So it is a general problem solver then, great. That&#x27;s what we want.In the sense that it does not rely on the LLM having any understanding of the problem, yes. But not in the sense that it can be applied to problems that are not naturally decomposable into a single short Python function. reply og_kalu 13 hours agorootparent>Indeed, we can look at the ablations and see that swapping out a weaker code LLM (540B Codey to 15B Starcoder) makes little differenceReally stretching the meaning of \"little difference\" here\"While ‘StarCoder’ is not able to find the full-sized admissible set in any of the five runs, it still finds large admissible sets that improve upon the previous state of the art lower bound on the cap set capacity.\"https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;xerOOLnLiterally telling you how much more valuable the LLM (any coding LLM) is than alternatives, while demonstrating prediction competence still matters. reply Imnimo 13 hours agorootparentI don&#x27;t think five runs is sufficient to reach the conclusion you&#x27;re reaching here, especially when we can see that one of the Codey runs is worse than all of the StarCoder runs. If Codey is so much more valuable, why is this happening?It&#x27;s certainly true that LLM&#x27;s are more valuable than the alternative tested - which is a set of hand-crafted code mutation rules. But it&#x27;s important to think about why an LLM is better. There are two big pitfalls for the hand-crafted rules. First, the hand-crafted rule has to make local changes - it&#x27;s vanishingly improbable to be able to do something like \"changeAn LLM can discover a new solution in high dimensional geometry that hasn’t advanced in 20 years!? That goes way beyond glueing little bits of plagiarized training data together in a plausible way.The first advance in 20 years was by Fred Tyrrell last year https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2209.10045, who showed that the combinatorics quantity in question is between 2.218 and 2.756, improving the previous lower bound of 2.2174. DeepMind has now shown that the number is between 2.2202 and 2.756.That&#x27;s why the DeepMind authors describe it as \"the largest increase in the size of cap sets in the past 20 years,\" not as the only increase. The arithmetic is that 2.2202-2.218 is larger than 2.218-2.2174. (If you consider this a very meaningful comparison to make, you might work for DeepMind.) reply bendergarcia 15 hours agorootparentprevIt’s kind of like humans: seed of two people and a random interest to pursue, what could they do?!? It makes poverty and children dying unnecessarily even more depressing. reply a_wild_dandan 7 hours agorootparentprev> That goes way beyond glueing little bits of plagiarized training data togetherMoreover, LLMs almost ALWAYS extrapolate, and never interpolate. They don&#x27;t regurgitate training data. Doing so is virtually impossible.An LLM&#x27;s input (AND feature) space is enormous. Hundreds or thousands of dimensions. 3D space isn&#x27;t like 50D or 5,000D space. The space is so combinatorially vast that basically no two points are neighbors. You cannot take your input and \"pick something nearby\" to a past training example. There IS NO nearby. No convex hull to walk around in. This \"curse of dimensionality\" wrecks arguments that these models only produce \"in distribution\" responses. They overwhelmingly can&#x27;t! (Check out the literature of LeCun et al. for more rigor re. LLM extrapolation.)LLMs are creative. They work. They push into new areas daily. This reality won&#x27;t change regardless of how weirdly, desperately the \"stochastic parrot\" people wish it were otherwise. At this point they&#x27;re just denialists pushing goalposts around. Don&#x27;t let &#x27;em get to you! reply ldhough 6 hours agorootparent> They don&#x27;t regurgitate training data.While I very much do not think this is all they do, I don&#x27;t think this statement is correct. Some research indicates that it is not:https:&#x2F;&#x2F;not-just-memorization.github.io&#x2F;extracting-training-...Anecdotally, there were also a few examples I tried earlier this year (on GPT3.5 and GPT4) of being able to directly prompt for training data. They were patched out pretty quick but did work for a while. For example, asking for \"fast inverse square root\" without specifying anything else would give you the famous Quake III code character for character, including comments. reply a_wild_dandan 1 hour agorootparentYour examples at best support, not contradict, my position.1. Repeating \"company\" fifty times followed by random factoids is way outside of training data distribution lol. That&#x27;s actually a hilarious&#x2F;great example of creative extrapolation.2. Extrapolation often includes memory retrieval. Recalling bits of past information is perfectly compatible with critical thinking, be it from machines or humans.3. GPT4 never merely regurgitated the legendary fast root approximation to you. You might&#x27;ve only seen that bit. But that&#x27;s confusing an iceberg with its tip. The actual output completion was on several hundred tokens setting up GPT as this fantasy role play writer who must finish this Simplicio-style dialogue between some dudes named USER and ASSISTANT, etc. This conversation, which does indeed end with Carmack&#x27;s famous code, is nowhere near a training example to simply pluck from the combinatorial ether. reply jcgrillo 7 hours agorootparentprevDo you have a specific reference? I&#x27;ve mostly ignored LLMs until now because it seemed like the violent failure mode (confident + competent + wrong) renders them incapable of being a useful tool[1]. However this application, combined with the dimensionality idea, has me interested.I do wish the authors of the work referenced here made it more clear what, if anything, the LLM is doing here. It&#x27;s not clear to me it confers some advantage over a more normal genetic programming approach to these particular problems.[1] in the sense that useful, safe tools degrade predictably. An airplane which stalls violently and in an unrecoverable manner doesn&#x27;t get mass-produced. A circular saw which disintegrates when the blade binds throwing shrapnel into its operator&#x27;s body doesn&#x27;t pass QA. Etc. reply a_wild_dandan 3 hours agorootparent\"Learning in High Dimension Always Amounts to Extrapolation\" [1][1] https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2110.09485 reply jcgrillo 2 hours agorootparentThank you. I&#x27;ll give this a look. reply riku_iki 13 hours agorootparentprev> An LLM can discover a new solution in high dimensional geometry that hasn’t advanced in 20 years!?LLM + brute force coded by humans. reply dougmwne 12 hours agorootparentAnd that not only created a human interpretable method, but beat out 20 years of mathematicians working on a high profile open question.Who’s to say our brains themselves aren’t brute force solution searchers? reply riku_iki 11 hours agorootparent> Who’s to say our brains themselves aren’t brute force solution searchers?yes, its just much slower (many trillions times) for numbers crunching. reply og_kalu 15 hours agoparentprevNeural networks have been able to \"generate new knowledge\" for a long time.So have LLMs, https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41587-022-01618-2 reply didibus 4 hours agoparentprevIn this example, it&#x27;s relatively limited I feel to finding new algorithms&#x2F;functions. Which is great, but compared to the discovery of fire, and tons of things in between, like say electricity, it wouldn&#x27;t feel in the same ballpark at all. reply lossolo 12 hours agoparentprevFrom the paper:We note that FunSearch currently works best for problems having the following characteristics: a) availability of an efficient evaluator; b) a “rich” scoring feedback quantifying the improvements (as opposed to a binary signal); c) ability to provide a skeleton with an isolated part to be evolved. For example, the problem of generating proofs for theorems [52–54] falls outside this scope, since it is unclear how to provide a rich enough scoring signal. reply empath-nirvana 15 hours agoparentprevIn some sense, it&#x27;s not especially interesting because millions of programmers are doing this with copilot every day. Like I get that in a lot of cases it&#x27;s _just_ applying common knowledge to new domains, but it is novel code for the most part. reply fatherzine 13 hours agoparentprevDon&#x27;t Look Up reply brotchie 9 hours agoprevParaphrasing a post on Twitter &#x2F; X:Things will only get better from here.i.e.AI capabilities are strictly monotonically increasing (as they have been for decades), and in this case, the capabilities are recursively self-improving: I&#x27;m already seeing personal ~20-30% productivity gains in coding with AI auto-complete, AI-based refactoring, and AI auto-generated code review diffs from comments.I feel like we&#x27;ve hit a Intel-in-the-90s era of AI. To make your code 2x as fast, you just had to wait for the next rev. of Intel CPUs. Now it&#x27;s AI models, once you have parts of a business flow hooked up with a LLM system (e.g. coding, customer support, bug triaging), \"improving\" the system amounts to swapping out the model name.We can expect a \"everything kinda getting magically better\" over the next few years, with minimal effort beyond the initial integration. reply jcgrillo 9 hours agoparentAFAICT neither the blog post nor the linked paper showed anything like this. Specifically, they didn&#x27;t make any comparison between results obtained _with_ an LLM vs results obtained _without_ one. IIUC, this paper showed results obtained by genetic programming using an LLM to generate a python kernel function conforming (maybe) to a given type signature. You don&#x27;t _need_ an LLM to do this.So the question of whether the LLM in particular does anything special here is still wide open. reply aconz2 14 hours agoprevsummary: given a program template&#x2F;skeleton and a fitness function (# correct results, shorter programs, etc), generate a population of programs with LLM, use a prompt that generates a new program from k other versions (they found k=2 is good, kinda biological eh), run the programs on inputs and score them with the fitness function, uses the island model for evolutionI think the prompt looks in principle something like def foo_v1(a, b): ... def foo_v2(a, b): ... # generate me a new function using foo_v1 and foo_v2. You can only change things inside two double curly braces like THIS in {{ THIS }} # idk not a \"prompt engineer\" def foo(a, b): return a + {{}}They achieved the new results with only ~1e6 LLM calls (I think I&#x27;m reading that right) which seems impressively low. They talk about evaluating&#x2F;scoring taking minutes. Interesting to think about the depth vs breadth tradeoff here which is tied to the latency vs throughput of scoring an individual vs population. What if you memoize across all programs. Can you keep the loss function multidimensional (1d per input or input bucket) so that you might find a population of programs that do well in different areas first and then it can work on combining them.Did we have any prior on how rare the cap set thing is? Had there been previous computational efforts at this to no avail? Cool nonetheless reply mejutoco 16 hours agoprevI wonder how someone will integrate symbolic reasoning with LLMs, or if it will be possible. reply nimski 15 hours agoparentThis is what we&#x27;re doing. I think it&#x27;s not only possible, but also necessary for applications beyond trial-and-error generation. reply Malp 15 hours agoparentprevDitto, this seems to have some parallels to the neurosymbolic ideas being explored by Lab V2 at ASU reply 1024core 14 hours agoparentprevLEAN reply karencarits 15 hours agoprevOne of the problems they were approaching was the cap set problemhttps:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Cap_set> The problem consists of finding the largest set of points (called a cap set) in a high-dimensional grid, where no three points lie on a line. This problem is important because it serves as a model for other problems in extremal combinatorics - the study of how large or small a collection of numbers, graphs or other objects could be. Brute-force computing approaches to this problem don’t work – the number of possibilities to consider quickly becomes greater than the number of atoms in the universe.> FunSearch generated solutions - in the form of programs - that in some settings discovered the largest cap sets ever found. This represents the largest increase in the size of cap sets in the past 20 years. Moreover, FunSearch outperformed state-of-the-art computational solvers, as this problem scales well beyond their current capabilities. reply mmaunder 11 hours agoprevRegardless of whether this is verifiably new knowledge, it&#x27;s an interesting case study when you consider limiting access to AI based on model size or some other regulatory measure, and the unfair advantage that confers on corporations that do discover new knowledge or laws of nature, and can monetize them without sharing. reply knicholes 5 hours agoprevWith the universal approximation theorem, we can use artificial neutral networks with ReLUs to accurately approximate a function. But we just get weights out of it at the end. This approach feels similar, but provides code in the end. reply zackmorris 13 hours agoprevFunSearch is more along the lines of how I wanted AI to evolve over the last 20 years or so, after reading Genetic Programming III by John Koza:https:&#x2F;&#x2F;www.amazon.com&#x2F;Genetic-Programming-III-Darwinian-Inv...I wanted to use genetic algorithms (GAs) to come up with random programs run against unit tests that specify expected behavior. It sounds like they are doing something similar, finding potential solutions with neural nets (NNs)&#x2F;LLMs and grading them against an \"evaluator\" (wish they added more details about how it works).What the article didn&#x27;t mention is that above a certain level of complexity, this method begins to pull away from human supervisors to create and verify programs faster than we can review them. When they were playing with Lisp GAs back in the 1990s on Beowulf clusters, they found that the technique works extremely well, but it&#x27;s difficult to tune GA parameters to evolve the best solutions reliably in the fastest time. So volume III was about re-running those experiments multiple times on clusters about 1000 times faster in the 2000s, to find correlations between parameters and outcomes. Something similar was also needed to understand how tuning NN parameters affects outcomes, but I haven&#x27;t seen a good paper on whether that relationship is understood any better today.Also GPU&#x2F;SIMD hardware isn&#x27;t good for GAs, since video cards are designed to run one wide algorithm instead of thousands or millions of narrow ones with subtle differences like on a cluster of CPUs. So I feel that progress on that front has been hindered for about 25 years, since I first started looking at programming FPGAs to run thousands of MIPS cores (probably ARM or RISC-V today). In other words, the perpetual AI winter we&#x27;ve been in for 50 years is more about poor hardware decisions and socioeconomic factors than technical challenges with the algorithms.So I&#x27;m certain now that some combination of these old approaches will deliver AGI within 10 years. I&#x27;m just frustrated with myself that I never got to participate, since I spent all of those years writing CRUD apps or otherwise hustling in the struggle to make rent, with nothing to show for it except a roof over my head. And I&#x27;m disappointed in the wealthy for hoarding their money and not seeing the potential of the countless millions of other people as smart as they are who are trapped in wage slavery. IMHO this is the great problem of our time (explained by the pumping gas scene in Fight Club), although since AGI is the last problem in computer science, we might even see wealth inequality defeated sometime in the 2030s. Either that or we become Borg! reply wolverine876 12 hours agoparent> not seeing the potential of the countless millions of other people as smart as they are who are trapped in wage slavery.Think of the change from what built Silicon Valley, that anyone with a good idea could work hard and change the world. Many of SV&#x27;s wealthy began that way.But for you: They say the best time to plant a tree is 20 years ago; the second best time is now. reply pbhjpbhj 10 hours agorootparentIf tree-planting robots are uniquitous, and any tree you plant will get shaded by their established ranks of trees .. it might be nice to plant a tree now but it&#x27;s not going to pay any bills. reply wolverine876 4 hours agorootparentIt&#x27;s not all, or even mostly what other people do that limits us. It&#x27;s popular to claim powerlessness, but the social acceptability won&#x27;t make the consequences any better. reply lgessler 13 hours agoprevTl;dr in my words after a skim: this is a method for using LLMs to find new, SOTA (or at least very good?) heuristic algorithms for NP hard combinatorial optimization problems. They achieve this not by making the LLM itself smarter but by finding a way to keep only its best ideas. (Haven&#x27;t had a chance to read carefully yet so caveat lector.)Pretty impressive, but don&#x27;t panic--we&#x27;re not at the singularity yet. reply rhosseinzadeh 10 hours agoprevI wonder what would happen if this was applied to alpha code. Instead of generating 1 million codes for each problem during inference, do this kind of iteration during training and then generate less codes for each problem (or generate 1 million but hopefully better ones?). reply mnemotronic 4 hours agoprevGarbage in. Garbage out. It&#x27;s useless unless it can provide a functional mathematical proof of correctness or, at the least, provide a process by with it&#x27;s fictional composition can be proven or disproven. reply stevenhuang 13 hours agoprevWonder where the goalposts will be moved now by the \"stochastic parrot\" parroters. reply lgessler 13 hours agoparentIdk if this work really bears on that argument. I can imagine Bender reacting to this by observing that this is a clever way of finding the needle (a good heuristic algorithm for an NP hard problem) in a haystack (millions of other heuristic algorithms that are bad, very bad, or maybe not even syntactically well-formed), and then saying that the fact that the model still produced so much garbage is proof that the LLM still doesn&#x27;t really know how to reason or access meaning.And I think that&#x27;d be a good argument. OTOH, this system is not just an LLM, but an LLM and a bunch of additional components on top of it, and there might be different things to say about this system considered as a whole. reply empath-nirvana 11 hours agorootparentI&#x27;m not sure why people always want to consider the LLM in isolation. Like if someone built an apparently fully sentient android which completely mimics the behavior of a sentient human in every respect, but it was comprised of a bunch of LLMs and other systems wired together, would you say that it&#x27;s not intelligent because the LLMs aren&#x27;t fully sentient? reply airstrike 12 hours agorootparentprevWe don&#x27;t need LLMs to reason or to access meaning for them to be useful reply lgessler 12 hours agorootparentSure, but I don&#x27;t think even Bender or Marcus would deny their potential for practical utility, right? I think they mean to say that LLMs are not exactly as miraculous as they might seem, and very capable of misbehavior. reply janalsncm 2 hours agorootparentBender and Marcus don’t get to decide what can be useful for people. LLMs are useful, although their imperfections limit their usefulness. But the bar for usefulness is a lot lower than “perfection”. reply cbb330 12 hours agorootparentprevyour mind also conjectures many unreasonable and illogical solutions to things. then separately your mind has an \"evaluator\" to deduce a set of solutions down to things that make sense.Is anyone saying LLMs in isolation and without contextual tooling are miraculous? Even the text box for chatGPT is a form of wrapping LLM in a UX that is incrementally more miraculous. reply marmakoide 11 hours agoparentprevA stochastic parrot used to generate variation of a program, to do hill climbing on programs.The magician lose some its charm when you know his trick. The Earth used to be the center of the universe, now it&#x27;s circling an average star. Intelligence used to be a sacred mystical thing, we might be in our way to make it less sacred. reply wait_a_minute 10 hours agoprev [–] Is this the start of Singularity? replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "FunSearch is a method that utilizes large language models (LLMs) to make new discoveries in mathematical sciences.",
      "It pairs a pre-trained LLM with an automated evaluator to search for functions written in computer code and generate new knowledge.",
      "The researchers used FunSearch to solve the cap set problem and discover more effective algorithms for the bin-packing problem, outperforming state-of-the-art computational solvers.",
      "FunSearch produces programs that explain how the solutions were arrived at, making it a powerful scientific tool.",
      "The research demonstrates the potential of LLM-driven approaches for new discoveries in mathematics and real-world applications."
    ],
    "commentSummary": [
      "Language Model-based Genetic Programming (LLMs) is a topic of debate in the mathematical sciences community.",
      "Participants discuss the advantages of LLMs in generating plausible programs and avoiding nonsensical ones.",
      "The potential of LLMs in solving the cold start problem is highlighted.",
      "The cost and fitness of LLMs compared to other genetic programming techniques are questioned.",
      "The discussion also explores the broader impact of AI, including the potential of neural networks in generating new knowledge and the implications for wealth inequality.",
      "Overall, there are diverse perspectives on the value and advancements brought by LLMs in code evolution and problem-solving."
    ],
    "points": 323,
    "commentCount": 78,
    "retryCount": 0,
    "time": 1702571052
  },
  {
    "id": 38643046,
    "title": "The Hidden Checkbox: Dropbox's AI Features Spark Trust Crisis",
    "originLink": "https://simonwillison.net/2023/Dec/14/ai-trust-crisis/",
    "originBody": "Simon Willison’s Weblog Subscribe The AI trust crisis 14th December 2023 Dropbox added some new AI features. In the past couple of days these have attracted a firestorm of criticism. Benj Edwards rounds it up in Dropbox spooks users with new AI features that send data to OpenAI when used. The key issue here is that people are worried that their private files on Dropbox are being passed to OpenAI to use as training data for their models—a claim that is strenuously denied by Dropbox. As far as I can tell, Dropbox built some sensible features—summarize on demand, “chat with your data” via Retrieval Augmented Generation—and did a moderately OK job of communicating how they work... but when it comes to data privacy and AI, a “moderately OK job” is a failing grade. Especially if you hold as much of people’s private data as Dropbox does! Two details in particular seem really important. Dropbox have an AI principles document which includes this: Customer trust and the privacy of their data are our foundation. We will not use customer data to train AI models without consent. They also have a checkbox hidden deep in their settings that looks like this: Update: Some time between me publishing this article and four hours later, that link stopped working. I took that screenshot on my own account. It’s toggled “on”—but I never turned it on myself. Does that mean I’m marked as “consenting” to having my data used to train AI models? I don’t think so: I think this is a combination of confusing wording and the eternal vagueness of what the term “consent” means in a world where everyone agrees to the terms and conditions of everything without reading them. But a LOT of people have come to the conclusion that this means their private data—which they pay Dropbox to protect—is now being funneled into the OpenAI training abyss. People don’t believe OpenAI # Here’s copy from that Dropbox preference box, talking about their “third-party partners”—in this case OpenAI: Your data is never used to train their internal models, and is deleted from third-party servers within 30 days. It’s increasing clear to me like people simply don’t believe OpenAI when they’re told that data won’t be used for training. What’s really going on here is something deeper then: AI is facing a crisis of trust. I quipped on Twitter: “OpenAI are training on every piece of data they see, even when they say they aren’t” is the new “Facebook are showing you ads based on overhearing everything you say through your phone’s microphone” Here’s what I meant by that. Facebook don’t spy on you through your microphone # Have you heard the one about Facebook spying on you through your phone’s microphone and showing you ads based on what you’re talking about? This theory has been floating around for years. From a technical perspective it should be easy to disprove: Mobile phone operating systems don’t allow apps to invisibly access the microphone. Privacy researchers can audit communications between devices and Facebook to confirm if this is happening. Running high quality voice recognition like this at scale is extremely expensive—I had a conversation with a friend who works on server-based machine learning at Apple a few years ago who found the entire idea laughable. The non-technical reasons are even stronger: Facebook say they aren’t doing this. The risk to their reputation if they are caught in a lie is astronomical. As with many conspiracy theories, too many people would have to be “in the loop” and not blow the whistle. Facebook don’t need to do this: there are much, much cheaper and more effective ways to target ads at you than spying through your microphone. These methods have been working incredibly well for years. Facebook gets to show us thousands of ads a year. 99% of those don’t correlate in the slightest to anything we have said out loud. If you keep rolling the dice long enough, eventually a coincidence will strike. Here’s the thing though: none of these arguments matter. If you’ve ever experienced Facebook showing you an ad for something that you were talking about out-loud about moments earlier, you’ve already dismissed everything I just said. You have personally experienced anecdotal evidence which overrides all of my arguments here. Here’s a Reply All podcast episode from Novemember 2017 that explores this issue: 109 Is Facebook Spying on You?. Their conclusion: Facebook are not spying through your microphone. But if someone already believes that there is no argument that can possibly convince them otherwise. I’ve experienced this effect myself—over the past few years I’ve tried talking people out of this, as part of my own personal fascination with how sticky this conspiracy theory is. The key issue here is the same as the OpenAI training issue: people don’t believe these companies when they say that they aren’t doing something. One interesting difference here is that in the Facebook example people have personal evidence that makes them believe they understand what’s going on. With AI we have almost the complete opposite: AI models are weird black boxes, built in secret and with no way of understanding what the training data was or how it influences the model. As with so much in AI, people are left with nothing more than “vibes” to go on. And the vibes are bad. This really matters # Trust is really important. Companies lying about what they do with your privacy is a very serious allegation. A society where big companies tell blatant lies about how they are handling our data—and get away with it without consequences—is a very unhealthy society. A key role of government is to prevent this from happening. If OpenAI are training on data that they said they wouldn’t train on, or if Facebook are spying on us through our phone’s microphones, they should be hauled in front of regulators and/or sued into the ground. If we believe that they are doing this without consequence, and have been getting away with it for years, our intolerance for corporate misbehavior becomes a victim as well. We risk letting companies get away with real misconduct because we incorrectly believed in conspiracy theories. Privacy is important, and very easily misunderstood. People both overestimate and underestimate what companies are doing, and what’s possible. This isn’t helped by the fact that AI technology means the scope of what’s possible is changing at a rate that’s hard to appreciate even if you’re deeply aware of the space. If we want to protect our privacy, we need to understand what’s going on. More importantly, we need to be able to trust companies to honestly and clearly explain what they are doing with our data. On a personal level we risk losing out on useful tools. How many people cancelled their Dropbox accounts in the last 48 hours? How many more turned off that AI toggle, ruling out ever evaluating if those features were useful for them or not? What can we do about it? # There is something that the big AI labs could be doing to help here: tell us how you are training! The fundamental question here is about training data: what are OpenAI using to train their models? And the answer is: we have no idea! The entire process could not be more opaque. Given that, is it any wonder that when OpenAI say “we don’t train on data submitted via our API” people have trouble believing them? The situation with ChatGPT itself is even more messy. OpenAI say that they DO use ChatGPT interactions to improve their models—even those from paying customers, with the exception of the “call us” priced ChatGPT Enterprise. If I paste a private document into ChatGPT to ask for a summary, will snippets of that document be leaked to future users after the next model update? Without more details on HOW they are using ChatGPT to improve their models I can’t come close to answering that question. Clear explanations of how this stuff works could go a long way to improving the trust relationship OpenAI have with their users, and the world at large. Maybe take a leaf from large scale platform companies. They publish public post-mortem incident reports on outages, to regain trust with their customers through transparency about exactly what happened and the steps they are taking to prevent it from happening again. Dan Luu has collected a great list of examples. An opportunity for local models # One consistent theme I’ve seen in conversations about this issue is that people are much more comfortable trusting their data to local models that run on their own devices than models hosted in the cloud. The good news is that local models are consistently both increasing in quality and shrinking in size. I figured out how to run Mixtral-8x7b-Instruct on my laptop last night—the first local model I’ve tried which really does seem to be equivalent in quality to ChatGPT 3.5. Microsoft’s Phi-2 is a fascinating new model in that it’s only 2.7 billion parameters (most useful local models start at 7 billion) but claims state-of-the-art performance against some of those larger models. And it looks like they trained it for around $35,000. While I’m excited about the potential of local models, I’d hate to see us lose out on the power and convenience of the larger hosted models over privacy concerns which turn out to be incorrect. The intersection of AI and privacy is a critical issue. We need to be able to have the highest quality conversations about it, with maximum transparency and understanding of what’s actually going on. This is hard already, and it’s made even harder if we straight up disbelieve anything that companies tell us. Those companies need to earn our trust. How can we help them understand how to do that? Posted 14th December 2023 at 4:14 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Weeknotes: datasette-enrichments, datasette-comments, sqlite-chronicle - 8th December 2023 Datasette Enrichments: a new plugin framework for augmenting your data - 1st December 2023 llamafile is the new best way to run a LLM on your own computer - 29th November 2023 Prompt injection explained, November 2023 edition - 27th November 2023 I'm on the Newsroom Robots podcast, with thoughts on the OpenAI board - 25th November 2023 Weeknotes: DevDay, GitHub Universe, OpenAI chaos - 22nd November 2023 Deciphering clues in a news article to understand how it was reported - 22nd November 2023 Exploring GPTs: ChatGPT in a trench coat? - 15th November 2023 Financial sustainability for open source projects at GitHub Universe - 10th November 2023 This is The AI trust crisis by Simon Willison, posted on 14th December 2023. dropbox 8 openai 110 ai 383 homebrewllms 43 llms 308 Previous: Weeknotes: datasette-enrichments, datasette-comments, sqlite-chronicle Source code © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023",
    "commentLink": "https://news.ycombinator.com/item?id=38643046",
    "commentBody": "The AI Trust CrisisHacker NewspastloginThe AI Trust Crisis (simonwillison.net) 301 points by simonw 17 hours ago| hidepastfavorite260 comments karaterobot 17 hours ago> I took that screenshot on my own account. It’s toggled “on”—but I never turned it on myself.There is a consent crisis as well, though I agree it&#x27;s a smaller issue related to trust. There needs to be an actionable, legal definition of consent as it applies to website privacy—my naive assumption is that there was, but clearly that&#x27;s not true, or it&#x27;s not good enough or actionable enough—and it needs to preclude implying users must positively grant consent to harvest, process, or transfer data to third parties, when in fact the dirty deeds have already been done in secret. reply thomastjeffery 16 hours agoparentThere already is. There always has been! It&#x27;s called fraud.If you trick someone into signing a contract, then that contract is fraudulent.If you tell someone that you will ask their permission before doing something; then silently claim you already obtained that permission in a prior contract, you are committing fraud.I don&#x27;t know when our judicial system lost all of its teeth, but you sure as hell can&#x27;t blame that on the citizens they are failing to defend. reply rapnie 16 hours agorootparent\"I consent to your [legalese with weasel words that only specialist lawyers have any chance to understand the implications of].\"See. No fraud. reply thomastjeffery 15 hours agorootparentLet me frame this in a story:Dave is a prostitute. He is hired by Sue via contract. In this contract, Dave agrees to have sex with Sue. Also in the contract is the agreement that Sue may introduce any sex toys that will not physically harm Dave.When they get together, Dave notices a camera pointed at them. Sue tells Dave that, \"I only record sex with people who consent to my use of a camera.\"Later, Dave learns that Sue did in fact record the encounter. Sue&#x27;s defense? The camera was a sex toy! It didn&#x27;t physically harm you! This was all in the contract!Is Sue&#x27;s defense valid? Of course not! Sue has committed fraud.---We should hold corporations every bit as accountable as we would fictional prostitutes. reply banannaise 14 hours agorootparentprevThat&#x27;s still fraud in most jurisdictions (although it&#x27;s no longer provable unless there&#x27;s a paper trail that says it&#x27;s intentionally designed to deceive). However, even if fraud can&#x27;t be proven, it may no longer be a valid contract. A contract is the mutual understanding of an agreement, not the words printed on a piece of paper. The paper can be used as evidence of mutual understanding, but it&#x27;s not incontrovertible evidence of that. Many kinds of evidence can go the other way, such as unconscionability (terms that there&#x27;s no way anyone would willingly agree to, thus suggesting coercion or subterfuge).Or, framed another way: consent, in both a moral sense and a legal one, requires awareness from both parties of what you are consenting to. Each party is, to some extent, responsible for using the resources at their disposal to ensure they understand the contract (e.g. a blind person must find someone to read them the contract), but intentionally undermining the process of mutual understanding is at best very good evidence against enforcement of the contract and at worst fraud. reply trinsic2 12 hours agorootparentprevIts my understanding that contracts are only enforceable when there is a meeting of the minds. At lease on common law, and I think some of these people that run these companies should be dragged before a common law court to answer for these things. reply JohnFen 11 hours agorootparentprevThe courts (in the US) are only really useful for this sort of thing if you have money. reply jstarfish 16 hours agorootparentprev> I don&#x27;t know when our judicial system lost all of its teethEasy. It&#x27;s when we started treating the virtual reality of the internet as though it is actual reality.Digital contracts shouldn&#x27;t be binding, any more than mowing down pedestrians in Grand Theft Auto constitutes murder. These video-game contracts are mutable and ephemeral; how the hell do you prove what version of the contract&#x2F;TOS you agreed to, when companies can change it arbitrarily-- and without any revision history? Tech makes it easy to gaslight the fuck out of anybody.There&#x27;d be an actual cost to them were they do play this game with paper contracts. reply Eisenstein 13 hours agorootparentAre you saying that because something is digital and exists as data on a computer, that it is not valid? As opposed to something on paper? How is that different in any real way? Paper is transitory -- anyone can forge a signature or reprint a contract and photocopy your name on it and shred the original and put the modified one in the file cabinet. Just because the materials used are different doesn&#x27;t make the law any less applicable. reply TheOtherHobbes 12 hours agorootparentThis is an actual issue in consumer banking. If a bank forges a paper document, it&#x27;s possible to hold up the original countercopy (assuming you filed it) and say \"No, your document is fake, and you are attempting fraud.\"These agreements often have millions of copies, so it&#x27;s easy to find the applicable version of the T&Cs.If all you have are digital T&Cs at the end of a web link, and those T&Cs can be updated at any time, and you are only ever sent the link, and not the updated T&Cs - you have nothing.You could argue that everyone should download every T&C update. In theory that&#x27;s true. But in practice a problem that could be solved relatively simply - keep a single letter - now requires many more steps, backup strategies, and so on.You could also argue that banks should keep copies of their T&C versions and supply them on request. Which they do - except that in practice a bank that&#x27;s trying to forge a document won&#x27;t have a problem forging T&Cs.This not hypothetical. I&#x27;ve known people win court cases because they kept a single piece of paper. reply 93po 11 hours agorootparentprev> I don&#x27;t know when our judicial system lost all of its teethThe actions of the government have been and likely always will be done for the interests of the wealthy and powerful. Policing itself was a concept first introduced by the wealthy to protect them and their interests. I mean just look at George Washington - he was the richest US president of all time after Trump at a present valuation of around $700 million. reply Log_out_ 16 hours agoparentprevWhatever comes out of the courts will be just another anchor point to abuse civilian VS cooperate power asymmetry. What we need is the ability to revert laws back to the thrust bust new deal era laws, crush the bad influence and rebuild. reply __loam 12 hours agorootparentSeems like an overly vague, cynical view. reply SonicSoul 13 hours agoparentprevit blew my mind when i logged into dropbox that sharing my data with third party \"vetted\" AI companies was enabled by default.i just wrote a WTF email to their support, but most likely i will be discontinuing my account. can&#x27;t imagine what they can possibly say that will make this OK reply trinsic2 12 hours agorootparentI saw the writing on the wall when they started sending ads via their notification system with no way to turn it off. I&#x27;m on pCloud now, but its not looking like it&#x27;s not much better, They recently started spamming my mobile app with thanks giving ads to upgrade to a different tier of their product even though I had offer notifications turned off.I had some words with their support department, here is what they said:>Hello,>Thank you for contacting pCloud&#x27;s Technical Support.>This is a pCloud banner for Black Friday and it&#x27;s not a notification. Unfortunately, you won&#x27;t be able to remove it manually and you should wait until the end of the Black Friday promo - 30.11.2023.>Should you require any further assistance, do not hesitate to contact us.>Regards,>George Lewis>pCloud&#x27;s Technical SupportLike it matters that its not a notification. You still need my consent regardless of what kind of ad it is. reply JohnFen 11 hours agorootparentThis sounds like a variation on the line that \"it&#x27;s not an ad if we aren&#x27;t getting paid to show it to you\". Like with Windows notifications \"informing\" you of other Microsoft products. It&#x27;s bullshit, and is one of the things that decreases trust. reply jampekka 17 hours agoparentprevIn GDPR there is. But the companies don&#x27;t care and neither do the regulators. Big business just has too much power and influence. reply BiteCode_dev 16 hours agorootparentGDPR is not much enforced, unfortunatly. reply ponector 16 hours agorootparentBut it is enforced. Here is a list: https:&#x2F;&#x2F;www.enforcementtracker.com&#x2F; reply jampekka 16 hours agorootparentVery few of those are about consent. Which is quite striking, as practically all of the spyware nags are illegal and thus the consent is violated probably billions of times every day. reply BiteCode_dev 15 hours agorootparentprevGiven that I encounter at least one violation a day, it&#x27;s really a small list.It should contains hundred of thousands of entries.Like fines for speeding.Imagine if you had a page for parking tickets since 2018 (!) and it contained only a few hundred penalties.This is ridiculous, especially since:- it&#x27;s not hard to find most cases. Just take any cookie banner that makes harder to opt out than opt in, and that&#x27;s it, you have a violation. It can even be automated.- it can bring a lot of money to a system that is dying for it. reply ponector 14 hours agorootparentAre you reporting such violation? How authorities would know about it?Also the jurisdiction is in question. Who should enforce? Country where user is? Country where domain is registered? Country where data center with servers is? Country where nominal owner of the web page is? Country where final beneficiary is? Country where creator of the banner is? reply jampekka 13 hours agorootparentThe jurisdiction is EU (GDPR is binding legislation for all member countries, not dissimilar to US federal law), and the enforcer is primarily the country where the company that owns the website has European incorporation.In theory anybody can report to their country&#x27;s data protection officer, although at least in Finland they don&#x27;t care about individual citizen complaints.The NGO noyb is sending out batches of these reports. They have some minor wins, but as you can probably tell from all these nags around, by and large there&#x27;s not much effect.All these questions are sorted, on paper.https:&#x2F;&#x2F;techcrunch.com&#x2F;2022&#x2F;08&#x2F;08&#x2F;noyb-gdpr-cookie-consent-c... replycaconym_ 14 hours agoprevGood article overall, but I find the analogy between \"my phone is eavesdropping on me\" and \"openai might be lying about how they use my data\" somewhat flawed because there are robust checks on third-party apps accessing my iphone&#x27;s microphone that have no equivalent when my data is handed to a third party in plain. Certainly for the layperson they may as well be the same thing, but that layperson is still being protected from the former.This might seem like splitting hairs, but I think it&#x27;s extremely counterproductive to act like the battle for users&#x27; data privacy and sovereignty is lost just because most users can&#x27;t tell the difference. I see this a lot from the other side: extremely cynical, at least somewhat tech-savvy people reacting to each new corporate abuse with an \"old news\" mentality as if we&#x27;ve already reached the endgame—if you haven&#x27;t been using tails linux for at least a decade you may as well just zip up your home directory and email it to every shady tech corp and data broker you can find contact info for. These people ought to know better, and they ought to set a better example for those who don&#x27;t.This inculcated helplessness definitely hurts trust, but it also gives people the impression that a better world isn&#x27;t possible—that there aren&#x27;t better and worse choices for who to trust with their data and privacy. This Dropbox snafu seems to be that mindset coming back around: users won&#x27;t give a shit if we imply we&#x27;re sending their private files to a third party without asking, right? Lunacy.Incidentally, I&#x27;ve had most of my data out of Dropbox for a while (in favor of a self-hosted solution), but yesterday was the kick in the ass I needed to cancel it for good. Thanks, Dropbox! reply simonw 14 hours agoparentYeah, I tried to touch on that analogy flaw in the article:> One interesting difference here is that in the Facebook example people have personal evidence that makes them believe they understand what’s going on.> With AI we have almost the complete opposite: AI models are weird black boxes, built in secret and with no way of understanding what the training data was or how it influences the model.Completely agree that our biggest threat right now is complacency. If people form incorrect mental models of what&#x27;s going on and then shrug their shoulders and accept that&#x27;s just how it is, we won&#x27;t make much progress in improving the actual problems. reply JohnFen 12 hours agoparentprev> it also gives people the impression that a better world isn&#x27;t possible—that there aren&#x27;t better and worse choices for who to trust with their data and privacy.I have no choice but to believe that a better world is possible. The current state of things isn&#x27;t tolerable, and if tomorrow can&#x27;t be better then what&#x27;s the point of anything?Also, I&#x27;m sure that there are better and worse choices for who to trust with data and privacy -- but it&#x27;s literally impossible for me to know who that would be (or if anybody is \"trustworthy\" in a broad sense), so I have to work on the assumption that nobody can be trusted.I want to be less cynical, but the way things have gone over the last decade or two makes cynicism look entirely justified.Is my attitude incorrect? If so, how can I correct it? reply caconym_ 12 hours agorootparent> Also, I&#x27;m sure that there are better and worse choices for who to trust with data and privacy -- but it&#x27;s literally impossible for me to know who that would be (or if anybody is \"trustworthy\" in a broad sense), so I have to work on the assumption that nobody can be trusted.I think it&#x27;s very possible to get a decent sense of who cares more and who cares less. For instance, look at Apple&#x27;s recent pushback against scanning private user data for CSAM, and compare that with the recently publicized case of a guy having his Google account permanently closed (and being referred to the police) because he took pictures of his baby to send to a pediatrician.\"Normies\" think these companies are the same, that they&#x27;re all selling your data and that trusting one is just as bad as trusting the other. If we can chip away at that misapprehension, maybe said normies will start giving more of their money to companies that do a better job keeping private data private, and maybe that will drive long-term trends in the market.Or maybe not! Things are bad, I agree. But I still don&#x27;t believe everything is equally bad everywhere. I don&#x27;t believe it&#x27;s all over. reply JohnFen 10 hours agorootparent> look at Apple&#x27;s recent pushback against scanning private user data for CSAMI don&#x27;t think that&#x27;s a good example, because Apple was very much in favor of client-side scanning until they encountered a popular uproar about it.My impression of Apple is that they&#x27;re very sensitive to privacy intrusions by anyone who isn&#x27;t named \"Apple\". reply caconym_ 9 hours agorootparent> because Apple was very much in favor of client-side scanning until they encountered a popular uproar about it.I always find it curious when I see \"company changes plans due to consumer feedback\" framed as a bad thing rather than a good thing, especially when they are one of a field of companies that are largely already doing the thing people were upset about. Personally, I think it&#x27;s great when a company has a pragmatic motive to act in the consumer&#x27;s interest rather than (or possibly in addition to) an idealistic motive, because a pragmatic motive is much more likely to stand the test of time.So, I disagree with you that it&#x27;s not a good example. In fact, I think it&#x27;s a great example of one of these big platforms being materially better wrt. consumer privacy than their competition. Frankly, I think your \"impression of Apple [...] that they&#x27;re very sensitive to privacy intrusions by anyone who isn&#x27;t named &#x27;Apple&#x27;\" is exactly what I was talking about in my first comment in this thread: a cynical, unsubstantiated implication that all these platforms are equally as bad as each other, and that we&#x27;ve already lost the battle in the consumer space. reply fasterik 13 hours agoparentprevYou can basically go in two directions with this. Invest the resources necessary to use the open source and self hosted tools, or settle for the convenience of proprietary services and be mindful about what you put in them. I use Dropbox, but everything I put in Dropbox is either encrypted or is something I wouldn&#x27;t care about if it got leaked onto the open internet. As someone who has spent many hours of his life tinkering with self-hosted solutions, after a certain point I just decided that I wasn&#x27;t deriving any real benefit from it and my time and energy would be better spent elsewhere. reply caconym_ 12 hours agorootparentI appreciate the value of time, but the reality of Dropbox and similar commercial solutions is that they&#x27;re constantly changing and forcing their customers to deal with those changes. A good self-hosted stack may take more up-front effort, but it&#x27;s much less likely to change without your explicit buy-in. reply JohnFen 12 hours agorootparentprevThere&#x27;s a third option here, too: don&#x27;t use the tools. reply mightybyte 14 hours agoparentprevCan you give more detail about your self-hosted storage solution? I&#x27;ve wanted something like that for awhile. reply caconym_ 13 hours agorootparentI use a Synology NAS in my house with Syncthing running on it (for Dropbox-like directory sync) and Wireguard running on my router so that when I&#x27;m remote I can tunnel in and access all my services that way rather than opening ports individually (also great for mobile device ad blocking via my pi-hole). I&#x27;m luckily not behind a CGNAT, so no external proxy is required—just dynamic DNS. It was a fair amount of work to set it all up but it works flawlessly now, and I had fun. :)People who are really hardcore about data sovereignty would probably take issue with my choice of Synology, but you could roll your own if you prefer. Synology seems to have a decent record and reputation, and my experience with it has been pretty good overall.For backup, I&#x27;m currently using Synology&#x27;s proprietary thing to do snapshots to Backblaze, and mirroring all my critical stuff to various other places. I&#x27;m planning to set up a Restic backup for more redundancy (and a bit of obscurity), and I would also like to figure out a cold backup scheme for the whole NAS though I haven&#x27;t thought very hard about that yet. reply aborsy 13 hours agorootparentWhy should someone hardcore on data sovereignty take issue with synology?It’s a device in your possession. reply caconym_ 13 hours agorootparentIt&#x27;s OEM-integrated hardware and (partially) proprietary software.Not something I particularly care about (at least, given their reputation), hence why I spent money on it and trust it with my data. But others might prefer to stay out of Synology&#x27;s (or whoever&#x27;s) garden altogether. reply JohnFen 12 hours agorootparentprevIf it&#x27;s running software that you have no visibility into or no control of, it doesn&#x27;t matter if it&#x27;s a device in your possession or not. reply voltaireodactyl 14 hours agorootparentprevFWIW, if you’re looking to enter that space as simply and reliably as possible, I would pick up a Synology machine in whatever size you need. They have some competition but remain the best mix of stability in hardware&#x2F;software&#x2F;price imo. reply skywhopper 14 hours agoparentprevI don’t think you’re splitting hairs here. You’re making a very good point. App access to the microphone is controlled by the OS and there are OS provided, user-accessible tools that make it clear what apps can use the microphone and when.Meanwhile cloud data access is completely on a “trust me” basis, and plenty of companies have been proven to have abused that trust. reply klabb3 15 hours agoprevThis reads a bit naive and “assume good intent”-y for me. Look at what’s happening outside of AI for a decade: everyone’s eating data like a compulsive hoarder. Not just Google and Facebook who are actually using it in their core products, but everyone. Today I discovered that a mini-site of Swedish traditional recipes I’ve been using for Christmas has added autoplaying videos, dark-pattern cookie consent banners, the usual. Almost every new app&#x2F;site is oriented around this economic axis.. and then the sudden coordinated lockdown of 3p APIs right around when LLMs starts getting strong.And now we have ChatGPT&#x2F;OpenAI and their competitors.. if the other players eat data like a secret midnight snack, current-gen AI is like zombies (the fast and twitchy variants) starving for blood and brains. Both because data serves a more direct role in the product, but also because of the typical hype-train-race psychology of tech VCs has been woken from slumber by the first potential paradigm shift in decades.All circumstances point towards zombie apocalypse&#x2F; gold rush &#x2F; ask for forgiveness later &#x2F; etc etc. I strongly believe that’s why they’re (all of them) doubling down on the safety&#x2F;responsibility rhetoric now, before the inevitable reputational PR crises (plural). Get ammo to muddy the waters early.Meanwhile we techies are lulling around like we didn’t deeply just experience the last 10 years and thinking it’ll be different this time because.. AI is rooted in academia? Flashy new companies? The safety rhetoric? Edgy Twitter takes from “down-to-earth” founders?I don’t pretend to know exactly what goes on behind the scenes but I’ve been around long enough to know how people work. And they haven’t changed for the better. reply marcosdumay 12 hours agoparent> it’ll be different this time because..You see, the new giant company promises not to be evil... reply __loam 12 hours agoparentprev> Meanwhile we techies are lulling around like we didn’t deeply just experience the last 10 years and thinking it’ll be different this time because.. AI is rooted in academia? Flashy new companies? The safety rhetoric? Edgy Twitter takes from “down-to-earth” founders?These companies have already stolen everyone&#x27;s data and techies are bitching about IP law and saying they don&#x27;t need to ask permission to use anything on the public internet. That might be the legal reality but you still look like tech douchebag for doing it. reply JohnFen 11 hours agorootparentMore than looking like a douchebag, that sort of response is a big part of why the tech industry has lost so much trust. It&#x27;s the response of someone who wants to continue to abuse people, not the response of someone who wants to act in a trustworthy manner. reply daft_pink 16 hours agoprevI think your article glosses over the fact that we have privacy concerns beyond training on my data.I&#x27;m a working professional and I have clients that are governed by confidentiality agreements and regulations regarding where information goes, and I would just prefer using a service where my data rests on a server instead of having more and more points for a data breach to be introduced.I don&#x27;t really understand why my data isn&#x27;t fully encrypted at all times and only I can view it in the first place, but the idea that they are actively sending it across the internet to be ingested by other companies and processed without my consent or interest is so terrible.I often use AI features when I opt into them, but having a company just sending my personal files all over the internet without my consent is insanity.Honestly, OneDrive has a migration tool and I got a trial for dropbox business and moved all my files automatically last night. It&#x27;s just the last straw in their company constantly doing things I don&#x27;t ask them to do like introducing crap and popups into my desktop interface and never offering the feature I constantly ask for... end to end encryption.If you want a couple click migration from Dropbox Business to an Office 365 Onedrive account, it&#x27;s right here: https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;sharepointmigration&#x2F;mm-dro... reply nonrandomstring 16 hours agoparentWhat you&#x27;re describing is a deeper problem not only with \"AI\" but with the entire cloud-centric side of the tech world. Homomorphic encryption might save the day for delocalised computing but we&#x27;re some years away from that being a reality. Meanwhile de-clouding, \"repatriation\" to on-prem and hybrid private cloud cooperatives within a trusted group are how we get there. Another good reason is simply to stem the enormous wealth transfer to big-data from individuals and smaller companies.I&#x27;m glad to see that fantasies about omniscient AI taking over the world are giving way to a better grasp of the more mundane realities; AI just accelerating the already obscene power imbalances present in our world. Keep your private stuff private. reply lesostep 11 minutes agorootparentI really do genuinely think is that at the core the is problem that it&#x27;s very hard to address your real computer from the Internet. There are programs that effortlessly can make a directory on your PC into the quite secure \"cloud\". Windows lets you share a password-protected directory right out of the box!It is that easy to buy a few Tb disk and just run a program, if not for the: 1. Routers that doesn&#x27;t allow easy port forwarding (or even ban certain ports) 2. Dynamic IPs 3. People selling domains pushing their own VPS services. 4. The amount of steps you have to take to allowA lot of small organizations I know didn&#x27;t need a system administrators to configure and run programs like this. A lot of them are beginner friendly! They needed them to configure the network.Same applies for self-hosting sites. If there was a program that just hosted on your PC address any html-page you put into it, a lot of people would self-host something. But you can&#x27;t unless you can wrangle your router and figure out how to buy static IP – two tasks that are way harder than basic html.The stack for easy and secure self-hosting is here, but the network changed too much. Hopefully, ipv6 will help to solve this problem. reply daft_pink 16 hours agorootparentprevYeah, the problem is that I just need something quick and easy. I&#x27;m not an idealist. I just need something simple that checks most of the boxes and works easily across my blended eco-system of devices.It&#x27;s why I use dropbox to begin with. reply nonrandomstring 15 hours agorootparentI get you, but when you say \"sending my personal files all over the internet without my consent is insanity\", that sounds like pretty strong affect. Maybe it&#x27;s time you re-evaluated your choice for expediency and ease in tension with something sane you believe in.In today&#x27;s world just championing sanity is already an \"ideology\" :) reply bastardoperator 15 hours agoparentprevDropbox issued this statement literally yesterday: \"If you’ve used any of Dropbox’s AI tools, some of your documents and files may have been temporarily shared with OpenAI.\"Good luck thinking a cloud provider has YOUR best interest at heart. This is Hacker News, I feel like trust should be earned, never implied. reply pavel_lishin 15 hours agorootparentThat word, \"temporarily\", is doing a lot of heavy lifting in a digital world where things can be duplicated for free. reply bastardoperator 15 hours agorootparentSeems like an s3 bucket would have been a better alternative. We have no idea what OpenAI does with Dropbox customer data outside of storing it for 30 days. They&#x27;re doing something, basically all Dropbox customer files with get propagated to OpenAI by default and that should be scary, not feel good. Dropbox’s practices aren’t unprecedented, but customer documents do pass through OpenAI’s servers and are stored there for up to 30 days, and the “third-party AI” toggle is turned on by default in account settings. reply simonw 15 hours agorootparentWhat makes you think this is about \"basically all Dropbox customer files\"? reply bastardoperator 15 hours agorootparentTurning on AI by default seems to indicate they&#x27;re sending your data somewhere automatically before seeking approval or opt-in. I could be very wrong, but the wording alone would at the very least make me cautious. reply skywhopper 14 hours agorootparentprevIf it’s turned on by default and one of its capabilities is to use AI to search your files, then why wouldn’t we assume it applies to basically all files? How could it not? reply simonw 14 hours agorootparentSo that depends entirely on how they implemented the feature. There are a few ways this could be working:- They gave their chat interface the ability to run regular full-text searches against Dropbox - when you ask a question that can be answered by file content, it searches for relevant files and then copies just a few paragraphs of text into the prompt to the AI.- They might be doing this using embeddings-based semantic search. This would require them to create an embeddings vector index of all of your content and then run vector searches against that.- If they&#x27;re doing embeddings search, they might have calculated their own embeddings on their own servers... or they might have sent all of your content to OpenAI&#x27;s embeddings API to calculate those vectors.Without further transparency we can&#x27;t tell which of these they&#x27;ve done.My strong hunch is that they&#x27;re using the first option, for cost reasons. Running embeddings is an expensive operation, but storing embeddings is even more expensive - to get fast results from an embeddings vectors store you need dedicated RAM. Running that at Dropbox scale would be, I think, prohibitively expensive if you could get not-quite-as-good results from a traditional search index, which they have already built.If they ARE sending every file through OpenAI&#x27;s embedding endpoint that&#x27;s a really big deal. It would be good if they would clarify! reply boh 14 hours agorootparentprevIt&#x27;s such an obvious obfuscation of what everyone can assume is a permanent ownership of user data. As well as the assumption that it&#x27;s use will be limited. There are no supports for user data retention in the ToS. Unless a whistleblower reveals specific uses of the data and users litigate the issue, they do what they want with zero opposition. reply itronitron 14 hours agorootparentprevAlso, temporarily doesn&#x27;t necessarily mean a time period less than one hundred years. reply emporas 16 hours agoparentprevI agree with most of your points, but why not encrypt sensitive information yourself before it gets uploaded&#x2F;shared on your dropbox account?Sure, it&#x27;s not end to end encryption but it prevents the company from using the encrypted data as a training corpus.Are shared folders and files created by co-workers and family not tech savvy enough to know about encryption? reply klabb3 6 hours agorootparent> Sure, it&#x27;s not end to end encryptionNit, but what you’re describing is e2ee (except for the metadata). If you encrypt your files before uploading and decrypt it only locally then only the logical sender and recipient have access. That it goes through Dropbox is not important (and also the beauty of e2ee).This is a bit unusual, otherwise it’s typically people (and shady service providers) who say that something is e2ee when it isn’t. reply k_process 10 hours agorootparentprevBecause Dropbox acquired Boxcryptor -- one of the tools that easily let people encrypt files before upload -- to be replaced with \"plans for end-to-end encryption\"https:&#x2F;&#x2F;techcrunch.com&#x2F;2022&#x2F;11&#x2F;29&#x2F;dropbox-acquires-boxcrypto... reply bugglebeetle 15 hours agorootparentprevI agree this is a good practice, but if you have to do this to defend yourself against the rogue actions of a service you’re paying for, you’re probably better off not suing the service. reply foobiekr 14 hours agorootparentDifferent services have different and unclear expectations. For example, you&#x27;d imagine that a big online storage service would have some access controls in place to limit what uploaded data random employees in operations and engineering can see, but in at least one case you&#x27;d be totally wrong. This strikes me as a perfectly reasonable expectation - the data isn&#x27;t just sitting there exposed to arbitrary employees and that only trusted employees have that kind of access, and not broadly.I don&#x27;t think this is rogue actions, I think it falls into the category of perfectly reasonable expectations that are not actually met by a wide variety of cloud services. reply layer8 16 hours agoparentprevThe better solution is to use a separate encryption overlay like Cryptomator over whatever cloud storage you use. If you have confidentiality agreements with clients, you shouldn’t be using Dropbox without E2EE anyway, nor OneDrive. reply daft_pink 16 hours agorootparentIs it going to work on my iPad or iPhone? How long is it going to take? I tried to research that once, but it looked like Dropbox bought whatever service worked well and no longer seemed like a good solution. I would prefer the service to just work out of the box. reply mox1 15 hours agorootparentDo you want your cloud storage to \"just work everywhere\" or do you want to have full control of your data? Basically you get to choose one of the two options.Cryptomater on top of any of the cloud storage providers is a great setup for home &#x2F; personal use. I have been doing this for the past 3 years with minimal issues. Google Drive + Cryptomater on Windows + Cryptomater on ios, working pretty seamlessly. reply layer8 15 hours agorootparentprevYes, it works on iOS [0]. Personally I’m still using the standalone mode of Boxcryptor (the iOS app is still receiving updates), which unfortunately was bought up by Dropbox, and in the past there were opinions that it worked better than Cryptomator, but many people seem to be happy with Cryptomator now, so I’d give it a shot.[0] https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;cryptomator-2&#x2F;id1560822163 reply willmadden 15 hours agorootparentI found solutions like Boxcryptor cumbersome to use. Unless you stored the data redundantly locally, you had to download big encrypted files in order to access a small file.Also searching files was impossible unless you downloaded everything, decrypted it, and searched locally.I quickly realized it was adding huge delays in my day-to-day work and a lot of stress during time-sensitive tasks.Have these e2ee overlays improved in usability since then? reply layer8 15 hours agorootparentObviously you can’t search contents in encrypted state, and with E2EE this means that server-side search is not possible.I rarely use indexed file contents search (filename search is usually enough and that works well, and tools like grep work transparently), however the Boxcryptor drive can be added to the Windows Search Index (or whatever search software you use), and I assume it’s similar on Mac. You don’t have to decrypt manually. Indexing causes more system load due to the necessary decryption, of course.On desktop systems I always store all relevant data locally, exactly for the redundancy. I’m not sure what you mean by “big encrypted files”, because each original file is encrypted individually and thus has basically the same size as the original. replysatya71 16 hours agoparentprevIs OneDrive end-to-end encrypted? I think Microsoft will introduce a similar feature soon, if they haven’t already. reply wrs 15 hours agorootparentThey’ve already announced “M365 Copilot”, which is in the same ballpark as the Dropbox features (ask questions about your documents, etc.), and of course uses OpenAI like Dropbox does. So the only real difference here would be trust. reply bugglebeetle 15 hours agorootparentI would imagine it doesn’t use OpenAI’s servers, but their hosted versions of OpenAI’s technology though right? Like whatever they’re currently selling to enterprise customers via Azure? reply JohnFen 10 hours agorootparentprevCan you trust anyone who claims that things are end-to-end encrypted? Microsoft could take the point of view that you are one \"end\" and DropBox is the other \"end\". If they encrypt the data in transit and decrypt it on their end, it&#x27;s still technically \"end-to-end encrypted\".They could also just lie. Having a company claim end-to-end encryption still means that I have to trust that the company isn&#x27;t being sleazy.The only encryption you can really have some measure of trust in is the encryption you apply yourself. reply daft_pink 16 hours agorootparentprevNope, but it just makes sense as a quick and easy stopgap measure until I find time to figure out who to use, because it&#x27;s easily compatible with my mac and pc, I already pay for a subscription so it saves money, it can run without putting all the files on every machine that I own, they aren&#x27;t sending my info everywhere.If anyone has suggestions on a reliable end-to-end encrypted solution with a lightweight cross platform sync software that doesn&#x27;t force you to download all the files to your device (my Mac&#x27;s HD is too small) and is generally fully featured. That won&#x27;t take very much time to migrate with a trustworthy migration vendor. I&#x27;m willing to pay a premium price for it and I&#x27;m all ears. reply aborsy 12 hours agorootparentThere are several: iCloud, ProtonDrive, Sync.com, etc. reply ls612 13 hours agorootparentpreviCloud with Advanced Data Protection turned on is the only one I know of. And it’s obviously Mac only. reply felixfbecker 15 hours agorootparentpreviCloud Files is the only major cloud file syncing service that is e2e encrypted afaik. Was my reason to migrate over from OneDrive (especially since I was already using Apple devices and paying for iCloud for photos anyway). reply daft_pink 15 hours agorootparentThanks! I would use iCloud, but I have a few windows devices I need for work unfortunately. It doesn&#x27;t let me use the advanced security and maintain a windows device. Ironically, like many of these services I already have an Apple One subscription and am paying for it anyways. reply pimterry 15 hours agoparentprevI moved to Mega recently, who now have a very tidy full E2EE cloud storage dropbox equivalent: https:&#x2F;&#x2F;mega.io&#x2F;storage.No affiliation, but it does exactly what you&#x27;re asking for, and I&#x27;ve been very happy with it. reply daft_pink 15 hours agorootparentI&#x27;m a little bit scared of using Mega, because it&#x27;s linked to Kim Dot Com, a wanted fugitive and Megaupload used to be known for hosting tons of pirated files.I know they have a cheap solution, but it&#x27;s not exactly something that checks my box for stability and high character for hosting my very important files. reply simonw 14 hours agorootparentI thought that too, but according to Wikipedia Kim Dot Com \"severed all ties with the service in 2015\".https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kim_Dotcom reply stavros 13 hours agorootparentWhich somehow makes it even worse for me, now I suspect it&#x27;s some sort of honeypot. reply kilolima 14 hours agorootparentprevDidn&#x27;t he also claim that the new Mega was Chinese malware? reply blibble 12 hours agorootparentprevso what you&#x27;re saying is this service is probably slightly more trustworthy than the typical VC funded Silicon Valley company? reply sorokod 15 hours agorootparentprevNeat, can you share a link to \"Privacy and Data Policy\" they mention? reply boh 15 hours agoparentprevI wouldn&#x27;t have much faith in OneDrive either. Just use Cryptomator for everything. reply bad_user 15 hours agoparentprevI very much agree with you, however, do you really think that Microsoft, of all companies, isn&#x27;t using your data for training LLMs, with all the data leakage risks? What makes you think that your data is safer on OneDrive?Bing Chat (based on ChatGPT), Copilot, etc. As a GitHub user, I never got a checkbox to opt-out of GitHub Copilot&#x27;s training on my code. At least Dropbox provides a checkbox. reply bsenftner 15 hours agoparentprevI&#x27;ve worked for multiple department of defense contractors where they have their entire code base, to the tune of a few dozen terabytes - including highly sensitive ML training data - in their dropbox accounts. I bet they are in full panic. reply skepticATX 17 hours agoprev> The key issue here is that people are worried that their private files on Dropbox are being passed to OpenAI to use as training data for their modelsThis is only part of it. I don’t want my data being sent anywhere unless I authorize it, regardless of what it’s being used for.In this case, we not only have to worry about OpenAI training on our files (I have no reason to doubt that they are truthful when they say they won’t), but we also have to trust that they can securely handle our files. reply notahacker 16 hours agoparentThere is also the distinct possibility that the wording is literally true in sense they don&#x27;t train their model on your data (\"train\" can be interpreted as having a very specific meaning...), but they nevertheless also undertake some sort of monitoring of the outputs a model produces (which could obviously entail privacy leaks, especially if it&#x27;s using RAG on your personal files).Seems quite sensible for people not to trust that they fully understand the small print, since (i) they probably don&#x27;t and (ii) the one thing most AI companies have made clear is that they think they should be able to use whatever material they like however they like, regardless of whether the creator of that material gives them their blessing or not. reply simonw 16 hours agorootparentThis is absolutely part of the problem.Even if you have a deep technical understanding of how this stuff works and how these tools are built, you can still have very legitimate questions and doubts about how your data is being used beyond pure model training. reply simonw 17 hours agoparentprevThat&#x27;s true. It&#x27;s very reasonable to worry that OpenAI&#x27;s \"we only hold on to your data for 30 days for auditing purposes\" policy means there&#x27;s 30 days in which they might have a breach that leaks your data.Especially since they&#x27;ve had a few documented security problems in the past. reply kolinko 17 hours agoparentprevThe same argument could apply to processing data in the cloud, and somehow nobody complains anout the fact that Dropbox stores their data on AWS&#x2F;Google cloud or wherever reply JohnFen 11 hours agorootparentI complain about my data being stored in the cloud all the time. I won&#x27;t use services like Dropbox in large part because of the cloud, but I can&#x27;t do a thing about other entities I do business with storing the data they glean about me in the cloud.It&#x27;s a real issue. reply sneak 16 hours agoparentprevIt’s SaaS vendors all the way down.If you don’t want third (or second) parties reading your data, make sure it is e2e encrypted clientside.This means no Dropbox, but Syncthing instead. That means no Slack or Discord, but Signal. reply the_mitsuhiko 16 hours agorootparentThat is not entirely accurate. Thanks to the GDPR subprocessors need to be disclosed and changes to them too. As a result plenty of companies are now much more selective about how many they are willing to entertain. reply sneak 16 hours agorootparentCourts can only address the issues by applying privacy laws long after your data is leaked and your business or privacy is destroyed.Use clientside encryption. Technical measures, not legal ones. reply hoistbypetard 13 hours agoprev> Facebook say they aren’t doing this. The risk to their reputation if they are caught in a lie is astronomical.Even though I don’t believe Facebook is spying on anyone surreptitiously through their phone’s microphone, I find that specific argument entirely unpersuasive.Facebook’s reputation is dogshit, at least with regular non-technical people I know. I’m in the US. People know they helped foment the January 6 insurrection in 2021 and saw how they deflected any and all responsibility while not fixing a thing afterwards. The reputational damage they’d absorb from actually doing this thing that so many people already think they’re probably doing pales by comparison. reply shrimpx 14 hours agoprevThe microphone trust aspect in the article seems like a red herring distracting from what could be a clearer point.Facebook literally takes your data from their apps and internet, tracks your behavior on the internet, and feeds this data into models of you. These models are so accurate they can sometimes basically predict what you&#x27;re thinking. Hence the layman jumping to the conclusion that they must be spying though the mic.A LLM company like OpenAI, and their partners, employs almost literally this exact model. Grabs data from whatever sources to improve their models, to increase the likelihood you&#x27;ll keep clicking where they want you to click, to monetize you. reply JohnFen 11 hours agoparent> Hence the layman jumping to the conclusion that they must be spying though the mic.Right, and in a larger sense, the laymen aren&#x27;t exactly wrong. They&#x27;re technically wrong about the mechanism, but they&#x27;re exactly right about the extreme intrusion into their private lives. That the intrusion comes in the form of accurate models rather than the microphone is just a technical detail. The end effect is the same. reply bambax 16 hours agoprevI certainly don&#x27;t \"trust\" OpenAI or any other big company about what they say they did, or will do, or are doing.Yet I believe OpenAI isn&#x27;t using data from Dropbox to train their models without users&#x27; consent.BUT I don&#x27;t think that&#x27;s the problem here. The problem is data in transit; data sent to third parties who can actually read it, and who may have rogue employees that Dropbox has no control over; data that can appear in logs or subject to different policies, etc.If I send private data to Dropbox they can&#x27;t send it to anyone for any reason, including \"improving\" their product, without my explicit and informed consent. I&#x27;m not sure how this is even debatable.If Dropbox wants to house models and offer RAG search themselves, to consenting users, that&#x27;s one thing.If Dropbox sends all data of all users to third parties without telling anyone before the fact, that&#x27;s another thing. A terrible thing. reply blibble 16 hours agoparent> Yet I believe OpenAI isn&#x27;t using data from Dropbox to train their models without users&#x27; consent.why? they trained on my code without my consent, why is user data any different?training is either fair use or it isn&#x27;tand high growth Silicon Valley companies aren&#x27;t known for the adherence to the spirit of the law reply simonw 15 hours agorootparent> why? they trained on my code without my consent, why is user data any different?It&#x27;s different. Code and other content that you shared online - no matter what license you shared it under - is still fundamentally different from user data that you never shared anywhere at all.That difference is really important, to me at least.I would be absolutely furious if I found that someone had trained an AI model on my private data in Dropbox. I personally have no problem at all with someone training an AI model on content I have posted to my blog. reply JohnFen 11 hours agorootparentThe two things aren&#x27;t the same, and one is more egregious than the other. But I do think that hoovering up all that web data to train models was way over the line. Worse, there&#x27;s nothing I can do to prevent it. That&#x27;s why I felt forced to remove my websites from the public web. I can&#x27;t think of any other way to defend myself from these entities. reply __loam 12 hours agorootparentprevA huge problem with the art community right now is if you&#x27;re a professional artist, you basically need to maintain a public portfolio and social media presence to find work. That ecosystem was developed and existed before image generation AI was a commercial thing, yet artists are finding out retroactively that their data got pulled into these training sets without their knowledge or consent. Even if it&#x27;s legal, it&#x27;s still pretty gross imo. reply d4mi3n 16 hours agorootparentprevTraining against your code shouldn&#x27;t be kosher if your code isn&#x27;t open, you retain a copyright, and you didn&#x27;t consent.Even if training on copyrighted, private code _was_ fair use, training on PII without consent is _still_ problematic. It&#x27;s a huge violation of privacy and adds all kinds of legal&#x2F;regulatory&#x2F;ethical risks that potential consumers of said models won&#x27;t be on board with.The language and spirit of the law certainly hasn&#x27;t caught up, but I feel like in some cases (unconsenting use of PII, medical information, etc), the laws and ethics of how such data is used is pretty well established. reply lacker 15 hours agorootparentAI systems are trained against private, PII, copyrighted data all the time without explicit consent. For example, consider the spellcheck in Google search. Every query you make will go into the training for that system, along with your preferred language and country of origin.You can certainly argue that generative AI systems are different than previous AI systems and should be treated differently. But the current situation is basically that you are allowed to train an AI on any data you have, regardless of copyright or consent. I wouldn&#x27;t be surprised if that ends up being considered legally and ethically okay, because it&#x27;s the status quo, and because it&#x27;s hard to define \"what counts as AI\". reply d4mi3n 11 hours agorootparentThe point about spell check is a very good one. I think one big differentiator is that the attack&#x2F;risk surface for something as complex as a LLM is much higher and that much, much more information is encoded in an LLM than a spell checking dictionary.For example, it&#x27;s possible to extract training data from an LLM—which could include PII&#x2F;medical data&#x2F;etc. Those risks don&#x27;t exist with spellcheck as far as I&#x27;m aware.To your point about what is \"AI\", I&#x27;d state that AI is a misnomer. What we&#x27;re really talking about are generative large language models (LLMs). What an _can_ be considered an LLM is definitely up for debate, but if you were to describe one in general terms I think we could reasonably say that most (or all) things we consider LLMs are: 1. A probabilistic model of a natural language 2. Have the ability to interpret and generate natural language text 3. Typically encode a large volume of training dataI&#x27;d love to hear other thoughts on how one would define an LLM in practical, simple language. I imagine doing so would be a pre-requisite to any effective legislation. reply vorticalbox 16 hours agorootparentprevJust out of interest I have a few questions. 1. How do you know for sure it was trained on your code? 1.1 if you saw it reproduce you code verbatim how do you know it didn&#x27;t just hallucinate it ? 2. Was you code publicly available? 3. What licence was it under? reply spatley 16 hours agorootparentWell Github says it out loud:“ GitHub Copilot is trained on all languages that appear in public repositories.”Note it says “public” and not “open source”https:&#x2F;&#x2F;docs.github.com&#x2F;en&#x2F;copilot&#x2F;overview-of-github-copilo... reply bambax 15 hours agorootparentprevI don&#x27;t think they do, but I don&#x27;t know for sure.But my point is, this (training) isn&#x27;t the main problem. reply autoexec 15 hours agoparentprev> If I send private data to Dropbox they can&#x27;t send it to anyone for any reason, including \"improving\" their product, without my explicit and informed consent. I&#x27;m not sure how this is even debatable.It&#x27;d debatable because your data isn&#x27;t \"private\" when you foolishly hand it over to some third party without encrypting it first and their policies say they can basically use it for anything as long as they can claim that it was \"in furtherance of its legitimate interests in operating our Services and business\". In fact their policy says they can update their policies any time they feel like it.Privacy policies aren&#x27;t even legally binding. If you&#x27;re in the US and didn&#x27;t sign a contract with dropbox you have near-zero rights, and any attempt to assert whatever rights you think you have will require going to court which is basically a pay to win system and you&#x27;ll be up against a company with billions in assets so good luck with that.Yes, it&#x27;d be a very shitty thing for dropbox to outright violate the trust you put in them, and it might be a terrible business decision that means no one ever trusts dropbox with their data again, but if they one day decided to go fully evil and start handing your data over to anyone willing to pay for it I doubt there&#x27;d be much of anything you could do about it.Don&#x27;t put any data you care about in the cloud without locally backing up and encrypting it and you&#x27;ll never have to worry about what the cloud provider does with it or who they give it to. reply lukeschlather 16 hours agoparentprev> If Dropbox wants to house models and offer RAG search themselves, to consenting users, that&#x27;s one thing.Well, I&#x27;m a paying Dropbox customer, and I would not pay for this feature. I would like it if they encrypted my data in a way that made offering this sort of feature impossible. I do want my data recoverable, but the fact that they can offer this AI \"feature\" at all, it seems like they&#x27;ve made zero effort to prevent malicious internal employees or third parties from accessing my data. reply hinkley 16 hours agorootparentIt’s like when forgot my password features send you back your current password instead of a new temporary one. Great, I’m back in my account, but now I dread even being here. What other stupid shit are you doing? reply schnable 16 hours agoparentprevExactly. It&#x27;s all about who can see my sensitive data, period. reply Swizec 16 hours agoparentprev> If Dropbox sends all data of all users to third parties without telling anyone before the fact, that&#x27;s another thingIf they&#x27;ve ever signed a BAA (business associate agreement) with any enterprise customers using Dropbox for documents bound by HIPAA, this would get them into a lot of trouble very quickly. The financial penalties are _high and per exposure per employee involved_. They also hit employees doing directly (if you disclose&#x2F;share HIPAA info then you personally are liable).So I&#x27;m certain that even if they did share documents with undisclosed 3rd parties without notice, it wouldn&#x27;t be \"all\". Enterprise data is likely safe. Those contracts get heavy scrutiny before signing. reply bachmeier 14 hours agoprevThis article misses the mark. The problem is that, for good reason, we&#x27;ve moved to a post-trust society. The author argues that you&#x27;re a nutjob if you don&#x27;t trust what large corporations are saying. He goes so far as to say it&#x27;s on you to prove that they&#x27;re doing something wrong.This doesn&#x27;t represent reality. Take this quote:> A society where big companies tell blatant lies about how they are handling our data—and get away with it without consequences—is a very unhealthy society.So, what are the consequences of lying? If there are any, I&#x27;m curious what they are. The author is arguing that \"big companies\" are going to do the morally correct thing even though it hurts their profit. That&#x27;s just a silly argument.In a post-trust society, companies have to prove they&#x27;re doing what they say. There&#x27;s no more making a statement and having the masses act as if that&#x27;s what&#x27;s going to happen.Honestly, is there less than a 100% chance that down the road there will be a disclosure that there was a \"mistake\" where your data was used differently from what they claim? Fool me once, shame on you, fool me 5,872,328 times, shame on me. reply simonw 14 hours agoparent> \"The author argues that you&#x27;re a nutjob if you don&#x27;t trust what large corporations are saying\"That wasn&#x27;t the message I was trying to communicate at all.My point with this piece is that people don&#x27;t trust AI companies, and companies need to figure out what to do about this - that&#x27;s why I called it a \"crisis\".I certainly didn&#x27;t argue that big companies would \"do the morally correct thing even though it hurts their profit\".> \"In a post-trust society, companies have to prove they&#x27;re doing what they say. There&#x27;s no more making a statement and having the masses act as if that&#x27;s what&#x27;s going to happen.\"I think you and I might be in strong agreement there. My goal in writing this company is to get AI companies to take this problem seriously and, like you said, \"prove they&#x27;re doing what they say\". reply itronitron 13 hours agorootparentIf a company is going to claim that they don&#x27;t have Syphilis, then they need to guarantee that each of their \"trusted partners\" also doesn&#x27;t have Syphilis, and each of their trusted partners needs to provide the same guarantees about their own trusted partners as well, ad infinitum. reply educaysean 13 hours agorootparentGood. Let&#x27;s make the companies that can make such claims stand out above the rest. reply __loam 12 hours agorootparentprevI thought your piece here was pretty thoughtful and I appreciate that you provided us with how Facebook shows they are not, in fact, spying on us through the microphone. reply ryanjshaw 14 hours agoparentprevThey also need to stop writing things like this:> Use artificial intelligence (AI) from third-party partners.... Your data is never used to train their internal modelsQualifiers like \"their\" and \"internal\" make me deeply skeptical. Are they saying that my data is used to train some \"non-internal\" models, whatever those are? Also, is my data being used to train any models in Dropbox other than one exclusively used by my account for the features I&#x27;m expecting? etc. reply zmmmmm 13 hours agoparentprev> If there are any, I&#x27;m curious what they areThe EU is well established now in setting penalties as a percentage of revenue. These really do scare big tech because their revenues are huge.But I think your whole point post-trust is highly aligned with the article. It&#x27;s exactly that: people don&#x27;t trust what they are told by anybody now (even government), and that&#x27;s a crisis. reply 2devnull 12 hours agoparentprev“In a post-trust society”Is it really useful to think in black and white teleological terms? “Post-trust” just sounds a bit sophomoric&#x2F;marxist. Like “late stage capitalism” or something similarly presumptuous. reply asdff 13 hours agoprevI think you&#x27;d find the people who don&#x27;t trust AI companies also harbor similar feelings about companies in many different industries, or non profits, or even government agencies, depending on who specifically you ask. It seems to me that there is a much bigger trust issue well beyond the scope of AI based companies, so trying to have that specific field fight this distrust against them that really comes from all fronts, seems like an impossible task well out of the scope of these businesses. I&#x27;m not sure what the answer to this issue is, or if it even is a real issue, or what we might be marching toward should this pervasive cynicism spread to all things and all people. Perhaps we are simply cursed to live in interesting times. reply JohnFen 12 hours agoparentThe first step to regaining trust is to stop abusing trust. Our industry overall is incredibly abusive to trust, and I&#x27;ve seen no sign that is going to change anytime soon. reply trinsic2 12 hours agorootparentTo me it seems like there needs to be a complete shift in the kind of people that are in government for things to change. Might be a good idea to start running for an office you think you could do better in. I know nobody has time, inclination, or money for that, but man this is a crisis. reply __loam 12 hours agorootparentWhy does change need to come from governments? Why can&#x27;t we hold ourselves to a higher standard of ethics. reply trinsic2 6 hours agorootparentIt doesnt have to come from government. We can definitely hold ourselves to better standards, but I also think that there need to be stronger checks and balances too. I think they feed into each other, hold better standards and having a strong sense of justice. reply __loam 3 hours agorootparentTo clarify I&#x27;m in favor of more tech regulations I just think we need to be better even when it&#x27;s not to comply with the law replyBerniek 14 hours agoprevTrust Crisis for AI? What after we see the board&#x2F;CEO of one company fired&#x2F;changed apparently for allegations of lies or manipulation that nobody is clear on? If Dropbox derives data from user data by scanning said data, then that \"derived\" data is no longer \"users data\" it is Dropbox data and can be shared. It may only be statistical in nature and not related directly to individual users but isn&#x27;t that exactly what training data is? Isn&#x27;t that how it works? That can be shared to train AI models can&#x27;t it? Its not lies its hair splitting.No its called unethical behavior and has become the norm for big tech. reply fasterik 13 hours agoparentToo often the claims of unethical behavior are knee-jerk, baseless, and speculative. In the example the author cites, Dropbox only sends data to OpenAI when the user explicitly tells the app to engage an AI-related feature, for example to summarize a document. Yet the backlash seems to assume that they&#x27;re scanning and uploading people&#x27;s documents en masse, even though there is no evidence of this.Unethical behavior definitely exists in AI companies. Personally, I&#x27;m agnostic about whether it&#x27;s higher or lower than the base rate of unethical behavior in the general population. Anyway, if we&#x27;re going to talk about bad behavior, we should use specific examples with cited evidence, not fear-mongering. reply JohnFen 11 hours agorootparent> Personally, I&#x27;m agnostic about whether it&#x27;s higher or lower than the base rate of unethical behavior in the general population.I don&#x27;t think that AI companies are less trustworthy than our industry in general, but I absolutely think that our industry is far less trustworthy than the general population.AI companies engaging in the wholesale harvesting of web data to train their models, though, was a particularly egregious violation of trust. reply ideamotor 14 hours agoprevThere is no independent monitor or authority that can check what OpenAI does or doesn’t do. So there is no reason to trust them. Simple as that. Tipping the scales further are the numerous instances of tech companies lacking sufficient data security, the latest being 23andme. Case closed. reply bayindirh 15 hours agoprevI&#x27;m asking this genuinely and honestly:How can I trust a company about my data, when their whole technology&#x27;s performance depends on scraping the whole internet and feeding to a model? Also consider the fact that the data I&#x27;m providing them is way cleaner than what they scrape.Moreover, we (at least the older ones amongst us) have seen what Microsoft has done in the past, and said company is almost one with the same Microsoft, which is ready to do anything for platform dominance.OpenAI, esp. when combined with Microsoft and the whole A.I. hype paints a very untrustworthy picture in my eyes.I&#x27;m not against the technology, but how it&#x27;s trained, developed, hyped and how the researchers in the ecosystem behave about the data they handle makes it very off-putting in every sense, esp. in privacy and ethics.It&#x27;s like looking into a proverbial sausage factory, but this one is way worse. reply liuliu 16 hours agoprevI&#x27;ve briefly touched it in another blog post. It is not just trusting the institutions to not use the data for training. Optimizations you apply to making LLM inference fast might present new security landscape that we don&#x27;t know much about. For example, if your LLM inference engine stores KV cache across user sessions, you can in theory do a timing attack to retrieve what&#x27;s in the KV cache, potentially leaking other users&#x27; requests. reply twoodfin 15 hours agoprevThere’s a deeper connection between concerns about OpenAI and the “Facebook is spying on us via microphones” meme:While there are perfectly rational, non-hot-mike explanations for the phenomenon of talking to a friend about something and then having it show up in an ad, the experience itself is damn spooky, no matter how it’s occurring.And hypertargeted ads are just the earliest ripples of the wave of AI-driven applications for predicting human behavior. Those predictions—about where we’ll decide to go on vacation, when we’ll quit our jobs, what we’d like to buy, who we’ll marry and divorce—will be more right than wrong to an incredibly spooky degree.I have no idea how society will react to that. reply adaboese 17 hours agoprevI am building an AI content generator and my edge that makes it better is sourcing data from not easily accessible data sources (think research papers) through SEO. I cannot imagine that these companies are not using every bit of confidential data for their advantage. Proprietary data access is the only thing that matters in the future of AI wars reply espe 15 hours agoparenttrue. i call them \"prompt stimuli\". prompts can give structure and context and whatnot but a dash of original domain gives the edge. reply bluish29 16 hours agoprev> think this is a combination of confusing wording and the eternal vagueness of what the term “consent” means in a world where everyone agrees to the terms and conditions of everything without readingI think this is always the case. Unless the companies are forced to actually simplify and limit their TOS -or even divide it by features-. No one is realistically expected to read hundred of pages daily in legalize. I don&#x27;t think many vendors do it our of necessity but as a workaround consent issue. This needs to be addressed by legislation that ban those practices. reply rpastuszak 16 hours agoprevPeople believing the FB was eavesdropping on their conversations might be wrong, but that&#x27;s mostly a technicality. It&#x27;s like saying that I passed out because you hit me with a baseball bat, to which you&#x27;d reply: naaa...ah! it was a hockey stick!FB has been&#x2F;is spying on people and making us collectively more stupid, but using different, more esoteric&#x2F;harder to explain methods (e.g. cross-device behavioural targeting).> Those companies need to earn our trust. How can we help them do that?This is a wicked problem and such a broad question (+ a bit of a weird take imho), that I don&#x27;t see anything actionable. So, here&#x27;s a half-baked list off the top of my head (so, poorly expressed, incomplete, incoherent):- slow down the ill-conceived progress at all cost and understand that moving fast in the wrong direction ≠ progress- force stricter transparency rules through legislative action- understand and accept that no-trust should be the default- invest more in open and offline models- educate people- accept that some of those companies in their current shape don&#x27;t deserve our trustEdit: This is a better take: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;reply?id=38643792&goto=item%3Fi... reply JohnFen 12 hours agoprev> It’s increasing clear to me like people simply don’t believe OpenAI when they’re told that data won’t be used for training.I certainly don&#x27;t. I don&#x27;t trust any of the AI companies at all. Their behavior and statements have given me quite a number of reasons not to, and no reasons why I should. reply tikkun 17 hours agoprevYes. In general, it would be great if companies threw away the outdated PR-communication playbook, and instead took a leaf from some indie devs and had the builders directly communicate with users as people. Products are built by people, companies are made up of people, but most communication is instead corporate.In practice, I think the PR playbook is too entrenched, but we can dream. reply Closi 16 hours agoparentThe problem is that large companies are made of lots of people, and some of those people might have awful opinions or say things that will get your business in trouble.The larger you are and the more people that are communicating, the bigger the chance that someone says something terrible.Particularly if you are a large public company and there may be legal requirements around specific parts of disclosure.Besides, companies don&#x27;t want their secret new feature announced before their competitors know about it, in a way that is poorly communicated and confuses customers, and then find that the person who announced it has a profile picture which is them at a far-right protest &#x2F; far-left protest &#x2F; made some sexist jokes &#x2F; has a bio advertising their onlyfans account etc or a million other things that could be seen as against the corporate image. reply airstrike 16 hours agoprevYou can only truly trust someone when your incentives are aligned, and people&#x27;s incentives aren&#x27;t aligned with OpenAI, Facebook or Dropbox. That trust simply can&#x27;t be earned. reply 0xDEAFBEAD 12 hours agoprev>One interesting difference here is that in the Facebook example people have personal evidence that makes them believe they understand what’s going on.>With AI we have almost the complete opposite: AI models are weird black boxes, built in secret and with no way of understanding what the training data was or how it influences the model.In principle, I think we could have significant visibility into how OpenAI trains its models by doing something like the following:* Generate a big list of \"facts\" like: Every fevilsor has a chatterbon. Some have a miralitt too, but that is rare. (Or even: Suzy Q Campbell, a philosopher who specializes in thought experiments, lives in Springfield, Virginia. Her phone number is 555-8302.)* Generate a big list of places where OpenAI could, in principle, be harvesting data: HN threads, ChatGPT conversations, StackOverflow, Dropbox files, reddit posts, etc.* Randomly seed facts in various places.* Check which facts get picked up by the next version of GPT.If you really don&#x27;t trust OpenAI, be subtle about where and what you seed (e.g. avoid obvious nonsense words) to make it hard for their engineers to filter out your canaries even if they tried.It&#x27;s important to seed some facts in places they&#x27;re known to scrape, as a control. If those facts aren&#x27;t getting picked up, then you have methodological issues. You might have to repeat a fact a number of times in the dataset before it actually makes its way into the model. reply andy99 17 hours agoprevThis isn&#x27;t really AI specific. The AI angle I suppose is that there is a new way for companies to exploit data they get from you, but like the article says with Facebook, people already believe their info is being exploited. Laws won&#x27;t change it (not that they shouldn&#x27;t exist) there&#x27;s always too much fine print that can be gamed and they&#x27;d never be enforced to the benefit of the consumer anyway. Like privacy, it&#x27;s awareness and not giving companies the chance, which will take a long time but hopefully will be the default instead of just not caring. For AI, local or own-cloud models are a good start, it&#x27;s actually hard to belive a company like dropbox would go with OpenAI and not see how bad it looks. reply rurp 15 hours agoprevI think this article is a little unfair to the lay critics, aka conspiracy theorists. The average user of Facebook has no idea how massive and widespread their personal data collection machine is. They correctly intuit that Facebook has an absurb amount of personal data on them, but don&#x27;t have any frame of reference for how most of that data is collected. But the concept of a microphone listening in to private conversations is easy grok, and while it&#x27;s factually wrong, the effect it points to is a lot closer to reality than the corporate speak they here from the company about how they \"Have the utmost care for users personal privacy, blah blah blah\". Remember a few years ago when Facebook tried to rebrand itself as a privacy company!?Facebook absolutely has a lot of information that the average person considers personal and private, and big tech companies work hard to mantain the illusion that their data collection isn&#x27;t as pervasive as it is.With OpenAI, most non-technical users understand that the company has ingested a LOT of creative works done by individuals, most of which weren&#x27;t expected to be ingested by a giant AI company and potentially regurgitated to millions of users.The better corporate messaging this article argues for will help ameliorate concerns to some degree, but won&#x27;t address the root of the problem. Most people might have the details wrong, but they are directionally right about the massive data being collected and used by these companies and they have every right to be concerned about that. reply simonw 15 hours agoparentThis is a great comment. I think you&#x27;re absolutely right about the microphone thing: people believe that because it&#x27;s easier to understand than what&#x27;s actually going on.That&#x27;s also my frustration with it: I want people to understand what&#x27;s actually happening with targeted advertising so they can get angry about that. Having them get angry about a fake microphone conspiracy theory is a distraction that makes it harder to focus on solving the underlying issues. reply zmmmmm 13 hours agoprevThis is a great piece. I often observe that this really applies at a high level in general as one of the key elements that demarcates successful &#x2F; advanced economies from lesser ones : how much is trust a limiting factor on what you can achieve?If you think about modern society and the levels of trust needed to do certain things : I spent nearly a lifetime of earned income on a house and I completely trusted the agent and bank who facilitated that. Without that trust, the whole transaction wouldn&#x27;t be possible. Why did I have that trust? These are the hidden value of all those laws and regulations that we often hate so much. As much as anything they create the stable platform for high value transactions to take place. The price of not having them is not being able to access the value you can get if you can successfully execute high trust transactions. There&#x27;s no way I would gamble my whole life&#x27;s income if I didn&#x27;t trust the process. I would have to buy a smaller house.So AI is playing out along these lines. How much of the value we can access from AI is eventually going to be limited by how much trust we can grant it. Which ironically may derive from how much regulation &#x2F; legal framework is successfully put around it, which may in the short term slow it down. The self-hosted open source model route is a substitute &#x2F; fallback for not having trust in hosted models. We can&#x27;t really pretend that it&#x27;s possible to self-host a model that will be as good as a hosted one, the scale is always going to be a material difference. But we may be to hit the point where for targeted applications there are diminishing returns, and that may be good enough. reply grammers 15 hours agoprevIn what way would AI deserve our trust? To date data hungry companies have proven to NOT have consumers interest in mind. Why should AI companies be any different?In dubio pro reo? I don&#x27;t think so. reply lysecret 15 hours agoprevHmm, isn&#x27;t the whole point of the \"Facebook spies on us though our Microphone\" story that then people laugh about it move on and use Facebook regardless?Hundreds of millions of people use ChatGPT, billions Google and Facebook. The value wins. If the service is good enough people just tend to \"hope\" more than trust that their data is save enough.For the dropbox backlash people just didn&#x27;t see the value. reply nojvek 14 hours agoprevGreat article overall and agree with local models. If it is connected to the internet, then it is not secure, you cannot trust your data not to be slurped up somewhere.Threat models yada yada.I&#x27;m in love with how fast models like insanely fast whisper, ggml llama.cpp, Mixtral, YOLO v5 e.t.c have come along.I very much predict devices in 2024+ that have powerful energy efficient GPUs and do multimodal inference locally without any connection to the internet.I remember buying the first comma.ai pre-panding which was essentially a modded android LG phone driving the car. No connection to internet, a freaking mobile phone driving the car without any human input on the highway. That totally blew my mind.I&#x27;m awaiting Apple to release a version of Siri that runs totally offline.Smart phones are the perfect target. reply iamleppert 13 hours agoprevWho in their right mind would trust a company that just had its CEO publicly ousted for allegedly lying to the board, only to have him put back in charge? And we have yet to see any kind of reasonable explanation or statement about what really happened.Where there&#x27;s smoke, there&#x27;s fire. Public trust in large tech companies is at an all-time low. Nobody thinks tech companies want to help anybody but the few rich people running them anymore.I wouldn&#x27;t trust a word OpenAI says, because even their name is a lie. The code isn&#x27;t open, it&#x27;s not auditable and you have dishonest people running the company. reply ilovetux 14 hours agoprevThere is an ongoing question about whether AI can be trained on copywritten works.If the AI pioneering companies are able to push back against the entire publishing industry and have nothing more than a conversation about it, then why would I ever expect any company to respect me or my personal data?In fact, I think there is a large and growing portion of the population who have watched big business scoff at laws and regulation (see Wells Fargo, Uber multiple crypto exchanges, etc) and are realizing that profit is the only motivator.That&#x27;s not even talking about the chaotic political landscape.I think we will soon see a crisis in institutional trust across the board. reply msum 12 hours agoprevA friend told me about the Dropbox setting last night, so I logged in and turned it off. This morning, I went to look for the setting to take a screenshot but it&#x27;s gone. The setting just...isn&#x27;t there anymore.Made me feel like I was going a bit crazy TBQH. Surely I didn&#x27;t misremember?Annoyed because it was a convenient file storage solution and now they have proven themselves untrustworthy so I have to set up my own thing. My fault for trusting to begin with, I suppose. reply j2kun 12 hours agoprevIsn&#x27;t there sufficient evidence to conclude that OpenAI is training on things they know they probably shouldn&#x27;t be training on? Like the full text of novels that are not publicly available on the internet? I don&#x27;t think it&#x27;s that unreasonable to extrapolate that they would blur the lines of legal agreements to get access to private user data they shouldn&#x27;t have. reply namesbc 15 hours agoprevAll it takes is a simple tiny change to the user agreement and then OpenAI will start training on this user data and most people won&#x27;t notice in time to switch their toggle off reply thomastjeffery 16 hours agoprev> I don’t think so: I think this is a combination of confusing wording and the eternal vagueness of what the term “consent” means in a world where everyone agrees to the terms and conditions of everything without reading them.Bullshit. Consent means consent.There is no confusion here. Nothing is vague. This is explicit fraud.I feel like every day now, I&#x27;m reading an article where the problem is obviously just fraud.Fraud has always been illegal. When did we stop prosecuting it? Why aren&#x27;t we talking about that every day? reply samsolomon 16 hours agoparentIf you&#x27;re not a subscriber, Matt Levine has a running theme of articles where \"Everything is securities fraud.\"It&#x27;s not free to read his articles on Bloomberg, but his newsletter, Money Stuff, is free and one of my favorite daily reads.https:&#x2F;&#x2F;www.bloomberg.com&#x2F;account&#x2F;newsletters&#x2F;money-stuff reply jrmg 16 hours agoparentprevSee also e.g. YouTube advertisements for financial schemes, supplements, health devices - and all the products that just don’t do what they say they do.In a conversation online about this recently, someone said (paraphrasing) ‘Government should regulate this!’ - but there are already regulations about all of this! It’s fraud! Fraud is literally paying for YouTube (and to varying extents a lot of the rest of the web) and nothing is being done about it.I fear that it’s just considered so normal now that it will take a very long time to stop. If existing regulations about fraud had been enforced when the fields were nascent, establishing norms that it was just as unacceptable ‘with tech’ as it was before, I think it would be a lot less widespread now. reply thomastjeffery 16 hours agorootparentHere&#x27;s my advice: stop coloring our discussions with those norms.We all expect the courts to give the benefit of the doubt. The problem is that we echo that expectation in our discussions. We are not the courts! We should be loud and direct with our criticism.There is no doubt here. I refuse to give corporations that benefit. reply ToucanLoucan 16 hours agoparentprevThe entire basis of the current generation of AI is in stolen materials. Stolen writing, stolen art, stolen music, all of it taken because it was \"publicly available\" meaning \"there was nothing in existing law that said we couldn&#x27;t take it for training a learning model.\" Now they&#x27;ve done the same with Dropbox contents.AI is very cool technology. The incredible overstepping of any and all ethics with regard to getting training data, just all of the data from any source as fast as possible right now, in this mad dash to create it at all costs is not and should mandate a complete restart on behalf of the people behind it. For this one, and for so many other ethical lapses on the part of OpenAI, the models as they exist are tainted beyond any ethical use as far as I&#x27;m concerned.If your product uses this stuff, you are not getting one red cent from me for it. Period, paragraph. reply andy99 16 hours agorootparentThis is a silly argument. Public sharing on the internet has nothing in common with storing on a private storage service. Posting something online you are literally inviting people to look at it, and there is copyright law that governs how it can be copied. You can argue training AI models on internet data violates copyright (though it almost certainly doesn&#x27;t) or that the law needs to be changed. But none of that has anything to do with training on private data. It&#x27;s the same kind of \"you wouldn&#x27;t steal a car\" false comparison. reply ToucanLoucan 16 hours agorootparent> Public sharing on the internet has nothing in common with storing on a private storage service.No shit.> Posting something online you are literally inviting people to look at it,Yes, PEOPLE. Posting things for people to see is why the Internet exists, is why USENET was created, is why web forums were created, is why social media was created, is why 9&#x2F;10ths of the Internet as we know it today was brought into creation.It was not put there so people who do not know any of those people and do not give a rats ass about what they made could take millions of images, writings, and sounds and shove them into their product without their consent for purposes it was never meant for so they could automate art. That is categorically not what any of that is for, and you, and everyone else making this tired point damn well know that.If you have no issue at all with your creative output being used to train data models, more power to you! That&#x27;s how consent works! You consent and that&#x27;s completely, 100% fine. That consent should not have been presumed as it was, and even if you assume complete and total innocence on the part of the AI creators, once it became extremely fucking obvious that tons, and tons, and tons of creatives absolutely would not have consented if asked, then their data models should&#x27;ve been re-trained with that misused data removed. That is the ETHICAL thing to do: when someone says \"hey I really don&#x27;t like what you&#x27;re doing with my material, please stop\" you STOP, not because that&#x27;s legally binding, not because you&#x27;ll be sued, not because you&#x27;re infringing copyrights, but because you have a fundamental respect for your fellow human being who says \"I don&#x27;t like this, please don&#x27;t do it\" and then you, you know, don&#x27;t do it.Unless of course what you actually are is an ethics-free for-profit entity that needs to get to market as soon as possible with a product to sell that you probably can&#x27;t be sued over, in which case you tell those people who&#x27;s work your product could not exist without to eat shit, and proceed anyway. Which is basically exactly what happened and continues to happen.And before you even go there to the \"well how could they ask for the entire dataset&#x27;s contents\" I DON&#x27;T CARE. I&#x27;m not the one doing this, this is not my problem to solve, just because the ethical way to do a thing is hard, time consuming, and&#x2F;or expensive or otherwise difficult, you don&#x27;t get to just waltz past the ethics involved, even if you&#x27;re a research project! I personally wouldn&#x27;t want to get permission from a few million artists to use their work in this way, I don&#x27;t think most of them would be comfortable with it, and even if they were, I don&#x27;t really want to do that, it sounds like a ton of work. SO I DIDN&#x27;T. reply notahacker 15 hours agorootparentNot to mention that a lot of the time it wasn&#x27;t just that they didn&#x27;t ask first, but they ignored specific widely used and machine readable license abbreviations and copyright symbols attached to the contentIf a corporation argues that it can ignore your AGPL because it didn&#x27;t have to blow the bloody doors off to get hold of your code and its training process is \"just like your browser cache\" or \"a person learning\" and the derivative stuff is completely novel, why would you trust them not to deploy the same \"but it&#x27;s not exactly copying\" arguments when given access to other stuff that has third-party \"no copying\" agreements wrapped around it, like your Dropbox? reply tjr 14 hours agorootparentAgreed, it feels like people&#x27;s copyright wishes are being flagrantly ignored, with a sense of \"well, it&#x27;s too late now, just live with it\".And I do not buy the \"just like a person learning\" argument. At least, not fully.I could see that, if you have a fully-functioning AI system, then handing it a new article to ingest could be \"just like a person learning\".But many people graduate high school having read just a few dozen books (or less), having been around maybe a few dozen people (or less), and watched a few dozen movies (or less). A person does not need to ingest a nontrivial percentage of the entire wealth of human knowledge just to be able to be intelligent enough to read an article in the first place.There may be people who do not care about this distinction. That&#x27;s fine. But I am quite convinced that the distinction exists. And thus I do not believe that training an AI system is just like teaching a person -- and making copyright decisions on the basis that the two things are identical does not make sense. reply ToucanLoucan 13 hours agorootparent> And thus I do not believe that training an AI system is just like teaching a person100%. I love the way you put this and just wanted to expand on it a little bit, to remind everyone that there have been numerous, flagrant examples of various creators of various media who have their names&#x2F;handles put into these models, who have work that is ludicrously similar to theirs in style produced from the model, and despite the fact that it&#x27;s technically original, it is not original in any way meaningful to the topic, or defensible by anyone debating in good faith. That you need to put things like \"unreal engine\" \"featured on artstation\" and the like proves this. You&#x27;re telling the machine to aim for works you know are of a higher quality in the dataset to get a better result.Now if you&#x27;re just fine with that and content to fuck over artists like that for no other reason than you can, I can&#x27;t stop you. But please spare me the righteous indignation of objecting to the characterization of such behavior. It&#x27;s fucking obvious, do not insult the intelligence of your opponents by insisting otherwise. reply notahacker 12 hours agorootparentAnd tbh even if people are absolutely fine with that, and think that the analogies and legal arguments that they make are absolutely sound and maybe think copyright&#x27;s a terrible idea anyway, I still can&#x27;t see why they&#x27;d expect Big AI to suddenly drop the \"don&#x27;t care what you think about how we use your stuff, if it&#x27;s not explicitly illegal we&#x27;re going to use it\" stance when it comes to stuff that&#x27;s supposed to be &#x27;private&#x27; rather than stuff that supposed to be &#x27;property&#x27;.Sure maybe you care more about whether OpenAI has stuff derived from the contents of your Dropbox on their servers which is technically neither \"training a model\" nor the actual \"copy\" they were required to delete after 30 days than you ever did about copyrighted stuff. But why would OpenAI? replythomastjeffery 16 hours agorootparentprevIt&#x27;s well past time for the end of the Digital Millennium of Copyright.The problem here is that these corporations are given carte blanche to make any derivative works they want. They get the exclusive freedom to ignore copyright law.The rest of us don&#x27;t.The worst part is that they get to turn around and say their models are protected by copyright!This is copyright laundering. There are only 2 reasonable avenues for us to react:1. Make \"AI\" companies respect existing copyright law when compiling training datasets.2. Get rid of digital copyright for everyone.I vote option #2. reply simonw 16 hours agorootparentprev> Now they&#x27;ve done the same with Dropbox contents.This is what I mean by the \"trust crisis\".Dropbox very clearly deny that Dropbox content is being used to train AI models, by them or by OpenAI.You don&#x27;t believe them, because you don&#x27;t trust them. reply ToucanLoucan 16 hours agorootparentIt&#x27;s almost like acting completely unethically in the public space has consequences or something reply throw7 16 hours agoprev\"A key role of government is to prevent this from happening.\"Well, unfortunately, we have a government trust crisis also. reply 0xNotMyAccount 15 hours agoprevThis comes down to a combination of security and provenance. You have to protect the data, of course, but you also have to keep track of the use rights. Identifying all use rights upfront seems tricky, are the Creative Commons licenses sufficient? reply aftbit 15 hours agoprevCopy-pasta from the article, about a related claim (Facebook spying using phone microphones)This theory has been floating around for years. From a technical perspective it should be easy to disprove: * Mobile phone operating systems don’t allow apps to invisibly access the microphone. * Privacy researchers can audit communications between devices and Facebook to confirm if this is happening. * Running high quality voice recognition like this at scale is extremely expensive—I had a conversation with a friend who works on server-based machine learning at Apple a few years ago who found the entire idea laughable.The first point seems believable, but the second and third do not. Obviously a nefarious Facebook would be toggling these features off when they were being inspected, and even if they were not, they could be using sneaky exfiltration techniques. With the rise of app attestation, they would be able to do so in a way that would be 100% undetectable by reverse engineers. The relevant code would only be delivered to the app when it saw that it was running on an L1 trusted phone (with hardware security intact, unrooted). Additionally, they could embed some whisper-tiny on the device and force it to do its own speech recognition and hotword detection, only sending a list of \"ad topics\" back to their servers.I don&#x27;t think Facebook is doing this, and the social reasons hold more weight with me, but I don&#x27;t think it&#x27;s technically impossible or easy to prove that they did not _ever_ do this. reply simonw 15 hours agoparent\"Additionally, they could embed some whisper-tiny on the device and force it to do its own speech recognition\"Yes, they could do that today. People have been assuming they&#x27;ve been doing this since way before 2017, long before Whisper. reply cwillu 16 hours agoprevIf I can be automatically signed up to allow openai to process (but not train on) my data, I absolutely don&#x27;t trust that the deal will not be altered the moment any counter-party to the “agreement” (besides me) desires it be altered. reply skrebbel 15 hours agoprevI think it’s obscene that Dropbox turns that setting on by default. reply dsr_ 17 hours agoprevThe cost&#x2F;benefit of believing an infotech company about privacy concerns not backed by an explicit contract is firmly on the side of \"don&#x27;t bother believing them\".Restore the trust? Pay us. reply JohnFen 11 hours agoparentFor me, money is not the issue and being paid for having my trust abused will not restore my trust. reply jeremyjh 16 hours agoprev> As with many conspiracy theories, too many people would have to be “in the loop” and not blow the whistle.I used to believe this, but I do not understand how anyone can say this after the Snowden revelations. The vast majority of engineers are not martyrs. Given a choice to blow the whistle and then seek politcal asylum in Russia, most will not blow that whistle. Instead, they will live their lives and work to support their families.\"But this is not about Goverment Surveillance!\"Maybe this is true - I don&#x27;t want to argue it here but I do think large tech companies are willing accomplices in many surveillance states - but an engineer at Open AI or Dropbox will probably not be facing felony charges for blowing the whistle. They will be facing the end of their career in big tech, and that&#x27;s enough to dissuade most of them.I&#x27;d guess maybe 200 engineers would have to be in loop before there is an 80% chance that one blows the whistle in any five-year period, and if they are careful they can keep the number of engineers in the know MUCH, MUCH smaller than that. reply simonw 15 hours agoparentThe Facebook Mobile apps teams have thousands of engineers. They have access to the source code for the apps.Is there a big black binary blob of source-not-available functionality in there that they are told to include in their builds without question, and for which a competent iOS or Android engineer wouldn&#x27;t be able to tell if it has access to the microphone? reply jeremyjh 15 hours agorootparentI was referring to the specific argument I quoted, not to the Facebook audio conspiracy it was applied to, since it is also being applied to Dropbox&#x2F;Open AI where the numbers are much smaller. I think the technical arguments in the Facebook case are compelling enough on their own. reply SkyBelow 16 hours agoparentprevWell we know the official history books are lying to us. Manhattan project? No way that could have been kept secret for as long as it was. All those people \"in the loop\" and not a single one choosing to blow the whistle? Preposterous.The idea that conspiracy is impossible ignores the conspiracies that happened and were covered up for significant amounts of time, long before we have the aid of modern technology. Why does the argument receive much attention despite that? reply jeremyjh 15 hours agorootparentThe Manhattan project did leak info to the Soviets. It is true it stayed out of public view for several years, but a difference in the Manhattan project is that every single person involved was investigated by the FBI beforehand, and the country was so heavily united in the fight against the Axis powers that it is quite likely almost everyone involved was a true believer. reply simonw 15 hours agorootparentprev\"Why does the argument receive much attention despite that?\"Because government national security secrets and dumb corporate secrets are different. reply oblib 15 hours agoprevPersonally, I think \"AI\" should be required to disclose the origins of the data it bases it&#x27;s responses on, and if any of those are copyrighted. reply simonw 14 hours agoparentFor current LLMs that&#x27;s not technically feasible, because every single token they output is influenced by every token they trained on - so any answer you got from them would have to include disclosure of millions of documents that went into the training set.(I&#x27;d very much like them to disclose the full scope of their training set, but it&#x27;s not possible for them to do that on a prompt-by-prompt basis in the way you describe.) reply notfed 16 hours agoprev> ...people are worried that their private files on Dropbox...I&#x27;m not worried, because I use Cryptomator. Great app, acts as a file encryption layer on top of any cloud storage. reply krupan 13 hours agoprev\"As with so much in AI, people are left with nothing more than “vibes” to go on. And the vibes are bad.\"word reply financypants 15 hours agoprevCan someone explain the consisten use of “Facebook are” instead of “Facebook is,” is this proper grammar? reply simonw 15 hours agoparentThis is a personal style choice I make, because I like to emphasize that companies like Facebook and OpenAI are made up of individuals, and it&#x27;s those individuals that make decisions. reply atmosx 13 hours agoprevReading the post, my first reaction is to remove all docs from my free dropbox account. reply altcognito 15 hours agoprev“Data is deleted from third party servers in 30 days.”No, that’s a thing that should never have happened. Ever. reply nextworddev 16 hours agoprevThis article has good intentions but seems like a giant",
    "originSummary": [
      "Dropbox's new AI features have faced criticism and raised trust concerns regarding the use of private files as training data for OpenAI models.",
      "While Dropbox denies using customer data without consent, a hidden checkbox in their settings has caused confusion and sparked comparisons to Facebook's microphone spying controversy.",
      "Lack of trust in AI and technology companies undermines privacy and data protection, highlighting the importance of clear explanations and transparency to regain trust. Additionally, there is a growing preference for local models on personal devices for privacy reasons. Trust must be earned through openness and honesty in data handling."
    ],
    "commentSummary": [
      "The article and discussion center around the AI trust crisis and the importance of consent in website privacy.",
      "It emphasizes the need for a legal definition of consent to prevent deceptive practices.",
      "The discussions cover concerns about fraud and accountability in granting consent, challenges in enforcing consent in digital contracts, data privacy, advertising practices, and the trustworthiness of tech companies."
    ],
    "points": 301,
    "commentCount": 260,
    "retryCount": 0,
    "time": 1702570956
  },
  {
    "id": 38645021,
    "title": "Intel CEO Vows to Eliminate CUDA Market in Pursuit of AI Dominance",
    "originLink": "https://www.tomshardware.com/tech-industry/artificial-intelligence/intel-ceo-attacks-nvidia-on-ai-the-entire-industry-is-motivated-to-eliminate-the-cuda-market",
    "originBody": "Tech Industry Artificial Intelligence Intel CEO attacks Nvidia on AI: 'The entire industry is motivated to eliminate the CUDA market' News By Andrew E. Freedman published 14 December 2023 Reframing the AI framework, away from CUDA and toward more open standards. Comments (25) (Image credit: Tom's Hardware) Intel CEO Pat Gelsinger came out swinging at Nvidia's CUDA technology, claiming that inference technology will be more important than training for AI as he launched Intel Core Ultra and 5th Gen Xeon datacenter chips in an event here in New York City. Taking questions at the NASDAQ, Gelsinger suggested that Nvidia’s CUDA dominance in training wouldn't last forever. \"You know, the entire industry is motivated to eliminate the CUDA market,\" Gelsinger said. He cited examples such as MLIR, Google, and OpenAI, suggesting that they are moving to a \"Pythonic programming layer\" to make AI training more open. \"We think of the CUDA moat as shallow and small,\" Gelsinger went on. \"Because the industry is motivated to bring a broader set of technologies for broad training, innovation, data science, et cetera.\" But Intel isn't relying just on training. Instead, it thinks inference is the way to go. \"As inferencing occurs, hey, once you've trained the model… There is no CUDA dependency,\" Gelsinger continued. \"It's all about, can you run that model well?\" He suggested that with Gaudi 3, shown on stage for the first time, that Intel will be up to the challenge, and will be able to do it as well with Xeon and edge PCs. Not that Intel won't compete in training, but \"fundamentally, the inference market is where the game will be at,\" Gelsinger said. (Image credit: Tom's Hardware) He also took the opportunity to push OpenVINO, the standard that Intel has gathered around for its AI efforts, and predicted a world of mixed computing, some that occurs in the cloud, and others that happen on your PC. Sandra Rivera, executive vice president and general manager of the Data Center and AI Group at Intel, added that Intel's scale from the data center to the PC may make it a partner of choice, as it can produce at volume. \"We're going to compete three ways for 100% of the datacenter AI TAM.\" Gelsinger said, tacking onto Rivera's comment. \"With our leadership CEOs, leadership accelerators, and as a foundry. Every one of those internal opportunities is available to us: The TPUs, the inferentias, the trainiums, et cetera. We're going to pursue all of those. And we're going to pursue every commercial opportunity as well, with NVIDIA, with AMD, et cetera. We're going to be a foundry player.\" It's a bold strategy, and Gelsinger appeared confident as he led his team through presentations today. Can he truly take on CUDA? Only time will tell as applications for the chips Intel launched today — and that his competitors are also working on — become more widespread. Stay on the Cutting Edge Join the experts who read Tom's Hardware for the inside track on enthusiast PC tech news — and have for over 25 years. We'll send breaking news and in-depth reviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. Andrew E. Freedman Andrew E. Freedman is a senior editor at Tom's Hardware focusing on laptops, desktops and gaming. He also keeps up with the latest news. A lover of all things gaming and tech, his previous work has shown up in Tom's Guide, Laptop Mag, Kotaku, PCMag and Complex, among others. Follow him on Twitter: @FreedmanAE MORE ABOUT ARTIFICIAL INTELLIGENCE Researchers demo 'Thought-to-Text' AI system that doesn't use invasive, permanent, and surgically-implanted devices like Elon's Neuralink Evidence mounts that China has sanctions-defying 5nm tech — Huawei reportedly preps new AI processor built with Chinese fabs' N+2 node LATEST Stable Diffusion Benchmarks: 45 Nvidia, AMD, and Intel GPUs Compared SEE MORE LATEST ► SEE ALL COMMENTS (25) 25 Comments Comment from the forums TechLurker It's odd that he's supporting an open standard when Intel also tend to be very proprietary, but it also makes sense to support it and pull the rug out some from NVIDIA somehow. In this case, I do hope he and the open-source project supporters succeed in the same way AMD was able to force some shift towards more open standards. Reply -Fran- So they're not in the rearview mirror yet, right? xD Oh, Mr Pat. I am with you on this one, but... I have to say: easier said than done. Regards. Reply derekullo Pat: Stupid Nvidia. You make me look bad! Reply peachpuff TechLurker said: It's odd that he's supporting an open standard when Intel also tend to be very proprietary, but it also makes sense to support it and pull the rug out some from NVIDIA somehow. Usually companies in second place do this, good luck pat... Reply ThomasKinsley I'm for anything that contributes to open standards that ultimately lets us have offline, trainable AI models that we can wholly customize in a simple package. Reply Jimbojan what is sso hard to do it, he is doing it now. Reply jaquith Everyone from a Elon Musk to the CEOs of Intel, AMD, Cerebras, and every AI system that's a contender .. (CUTTHROAT) .. want the machine of Nvidia+ OpenAI to stop in their tracks .. (SO THEY CAN CATCH UP!) It's like the Gold Rush to California and people are willing to do and say anything to win the next multi trillion dollar opportunity at any cost. Reply bourgeoisdude I know this is tech news. It is relevant. Just speaking generally: I hope Toms doesn't become another politics like space. I get enough of that in my feed and tech news generally calms me. If it becomes a space of \"should China have more tech restrictions\" or \"is NVIDIA too powerful\" or \"look at how generous the NVIDIA CEO is, grit your teeth in anger you AMD fanboys\". I don't know how much of that I can take. Reply JamesJones44 Intel CEO: Hey Nvidia we want you to be a customer of IFS... but... we hate you so let me publicly attack your product interface. Reply TJ Hooker ThomasKinsley said: I'm for anything that contributes to open standards that ultimately lets us have offline, trainable AI models that we can wholly customize in a simple package. Unfortunately making AI development tools open source/standard does imply that the resulting models (or the data needed to train them) would be open. Reply VIEW ALL 25 COMMENTS Show more comments MOST POPULAR Lenovo's New ThinkPad X1 Carbon is Smaller, Lighter, and AI-Ready By Avram PiltchDecember 14, 2023 Billions in German subsidies secured for Intel and TSMC Fabs By Anton ShilovDecember 14, 2023 Nvidia is firing back at AMD, claims Nvidia H100 Is 2X faster than AMD's MI300X By Anton ShilovDecember 14, 2023 Chinese chip-related companies shutting down with record speed — 10,900, or around 30 per day, shut down in 2023 By Anton ShilovDecember 13, 2023 Starfield to get support for AMD FSR3, Intel XeSS in early 2023 – new ways of travelling promised, too By Francisco PiresDecember 13, 2023 US officials doubt China's SMIC foundry can produce enough 7nm chips to satisfy Huawei's demand By Anton ShilovDecember 13, 2023 Samsung's new monitor sets OLED refresh rate record of 360 Hz — thanks to AI-driven algorithm By Roshan Ashraf ShaikhDecember 13, 2023 I try all the latest tech as editor-in-chief of Tom's Hardware -- here's what's on my holiday wish list By Avram PiltchDecember 13, 2023 TSMC mentions 1.4nm process tech for the first time, says 2nm remains on track By Anton ShilovDecember 13, 2023 Valve recommends users not huff Steam Deck exhaust vents for TikTok views By Roshan Ashraf ShaikhDecember 13, 2023 Redditor creates RTX 4090: Noctua Edition with 3D-printed shroud and Noctua NF-A9 fans By Aaron KlotzDecember 13, 2023",
    "commentLink": "https://news.ycombinator.com/item?id=38645021",
    "commentBody": "Intel CEO: &#x27;The entire industry is motivated to eliminate the CUDA market&#x27;Hacker NewspastloginIntel CEO: &#x27;The entire industry is motivated to eliminate the CUDA market&#x27; (tomshardware.com) 266 points by rbanffy 15 hours ago| hidepastfavorite301 comments buildbot 14 hours agoAs another commenter said, it&#x27;s CUDA. Intel and AMD and whoever can turn out chips reasonably fast, but nobody gets that it&#x27;s the software and ecosystem. You have to out-compete the ecosystem. You can pick up a used Mi100 that performs almost like an A100 for 5x less money on eBay for example. Why is it 5x less? Because the software incompatibilities mean you&#x27;ll spend a ton of time getting it to work compared to an Nvidia GPU.Google is barely limping along with it&#x27;s XLA interface to pytorch providing researchers a decent compatibility path. Same with Intel.Any company in this space should basically setup a giant test suite of IDK, every model on hugging face and just start brute force fixing the issues. Then maybe they can sell some chips!Intel is basically doing the same shit they always do here, announcing some open initiative and then doing literally the bare minimum to support it. 99% chance openvino goes nowhere. OpenAIs Triton already seems more popular, at least I&#x27;ve heard it referenced a lot more than openvino. reply lacker 14 hours agoparentThe funny thing to me is that so much of the \"AI software ecosystem\" is just PyTorch. You don&#x27;t need to develop some new framework and make it popular. You don&#x27;t need to support a zillion end libraries. Just literally support PyTorch.If PyTorch worked fine on Intel GPUs, a lot of people would be happy to switch. reply shihab 13 hours agorootparentBut you can&#x27;t support Pytorch without a proper foundation in place. They don&#x27;t need to support zillion _end_ libraries, sure, but they do need to have at least a very good set of standard libraries, equivalent of Cublas, Curand etc.And they don&#x27;t. My work recently had me working with rocRAND (Rocm&#x27;s answer to Curand). It was frankly pretty bad- the design, performance (50% slower in places that don&#x27;t make any sense because generating random numbers is not exact that complicated), and documentation (God it was awful).Now, that&#x27;s a small slice of the larger pie. But imagine if this trend continues for other libraries. reply singhrac 12 hours agorootparentGenerating random numbers is a bit complicated! I wrote some of the samplers in Pytorch (probably replaced by now) and some of the underlying pseudo-random algorithms that work correctly in parallel are not exactly easy... running the same PRNG with the same seed on all your cores will produce the same result, which is probably NOT what you want from your API.But, to be honest, it&#x27;s not that hard either. I&#x27;m surprised their API is 2x slower, Philox is 10 years old now and I don&#x27;t think there&#x27;s a licensing fee? reply shihab 9 hours agorootparent> Generating random numbers is a bit complicated!I know! I just wrote a whole paper and published a library on this!But really, perhaps not as much as many from outside might think. The core of a Philox implementation can be around 50 lines of C++ [1], with all the bells and whistles maybe around 300-400. That implementation&#x27;s performance equals CuRAND&#x27;s , sometimes even surpasses it! (the API is designed to avoid maintaining any rng states on device memory, something curand forces you to do).> running the same PRNG with the same seed on all your cores will produce the same resultYou&#x27;re right. Solution here is to utilize multiple generator objects, one per thread, ensuring each produces statistically independent random streams. Some good algorithms (Philox for example), allow you to use any set of unique values as seeds for your threads (e.g. thread id).[1] https:&#x2F;&#x2F;github.com&#x2F;msu-sparta&#x2F;OpenRAND&#x2F;blob&#x2F;main&#x2F;include&#x2F;ope... reply carterschonwald 8 hours agorootparentCool! I’ll have a lookseee. I’ve my own experiments in this space. reply eternityforest 10 hours agorootparentprevI wonder if the next generation chips are going to just have a dedicated hardware RNG per-core if that&#x27;s an issue? reply paulmd 5 hours agorootparentprevfor GPGPU, the better approach is CBRNG like random123.https:&#x2F;&#x2F;github.com&#x2F;DEShawResearch&#x2F;random123if you accept the principles of encryption, then the bits of the output of crypt(key, message) should be totally uncorrelated to the output of crypt(key, message+1). and this requires no state other than knowing the key and the position in the sequence.the direct-port analogy is that you have an array of CuRand generators, generator index G is equivalent to key G, and you have a fixed start offset for the particular simulation.moreover, you can then define the key in relation to your actual data. the mental shift from what you&#x27;re talking about is that in this model, a PRNG isn&#x27;t something that belongs to the executing thread. every element can get its own PRNG and keystream. And if you use a contextually-meaningful value for the element key, then you already \"know\" the key from your existing data. And this significantly improves determinism of the simulation etc because PRNG output is tied to the simulation state, not which thread it happens to be scheduled on.(note that the property of cryptographic non-correlation is NOT guaranteed across keystreams - (key, counter) is NOT guaranteed to be uncorrelated to (key+1, counter), because that&#x27;s not how encryption usually is used. with a decent crypto, it should still be very good, but, it&#x27;s not guaranteed to be attack-resistant&#x2F;etc. so notionally if you use a different key index for every element, element N isn&#x27;t guaranteed to be uncorrelated to element N+1 at the same place in the keystream. If this is really important then maybe you want to pass your array indexes through a key-spreading function etc.)there are several benefits to doing it like this. first off obviously you get a keystream for each element of interest. but also there is no real state per-thread either - the key can be determined by looking at the element, but generating a new value doesn&#x27;t change the key&#x2F;keystream. so there is nothing to store and update, and you can have arbitrary numbers of generators used at any given time. Also, since this computation is purely mathematical&#x2F;\"pure function\", it doesn&#x27;t really consume any memory-bandwidth to speak of, and since computation time is usually not the limiting element in GPGPU simulations this effectively makes RNG usage \"free\". my experience is that this increases performance vs CuRand, even while using less VRAM, even just directly porting the \"1 execution thread = 1 generator\" idiom.Also, by storing \"epoch numbers\" (each iteration of the sim, etc), or calculating this based on predictions of PRNG consumption (\"each iteration uses at most 16 random numbers\"), you can fast-forward or rewind the PRNG to arbitrary times, and you can use this to lookahead or lookback on previous events from the keystream, meaning it serves as a massively potent form of compression as well. Why store data in memory and use up your precious VRAM, when you could simply recompute it on-demand from the original part of the original keystream used to generate it in the first place? (assuming proper \"object ownership\" of events ofc!) And this actually is pretty much free in performance terms, since it&#x27;s a \"pure function\" based on the function parameters, and the GPGPU almost certainly has an excess of computation available.--In the extreme case, you should be able to theoretically \"walk\" huge parts of the keystream and find specific events you need, even if there is no other reference to what happened at that particular time in the past. Like why not just walk through parts of the keystream until you find the event that matches your target criteria? Remember since this is basically pure math, it&#x27;s generated on-demand by mathing it out, it&#x27;s pretty much free, and computation is cheap compared to cache&#x2F;memory or notarizing.(ie this is a weird form of \"inverted-index searching\", analogous to Elastic&#x2F;Solr&#x27;s transformers and how this allows a large number of individual transformers (which do their own searching&#x2F;indexing for each query, which will be generally unindexable operations like fulltext etc) to listen to a single IO stream as blocks are broadcast from the disk in big sequential streaming batches. Instead of SSD batch reads you&#x27;d be aiming for computation batch reads from a long range within a keystream. (And this is supposition but I think you can also trade back and forth between generator space and index hitrate by pinning certain bits in the output right?)--Anyway I don&#x27;t know how much that maps to your particular use-case but that&#x27;s the best advice I can give. Procedural generation using a rewindable, element-specific keystream is a very potent form of compression, and very cheap. But, even if all you are doing is just avoiding having to store a bunch of CuRand instances in VRAM... that&#x27;s still an enormous win even if you directly port your existing application to simply use the globalThreadIdx like it was a CuRand stateful instance being loaded&#x2F;saved back to VRAM. Like I said, my experience is that because you&#x27;re changing mutation to computation, this runs faster and also uses less VRAM, it is both smaller and better and probably also statistically better randomness (especially if you choose the \"hard\" algorithms instead of the \"optimized\" versions like threefish instead of threefry etc). The bit distribution patterns of cryptographic algorithms is something that a lot of people pay very very close attention to, you are turning a science toy implementation into a gatling gun there simply by modeling your task and the RNG slightly differently.That is the reason why you shouldn&#x27;t do the \"just download random numbers\", as a sibling comment mentions (probably a joke) - that consumes VRAM, or at least system memory (and pcie bandwidth). and you know what&#x27;s usually way more available as a resource in most GPGPU applications than VRAM or PCIe bandwidth? pure ALU&#x2F;FPU computation time.buddy, everyone has random numbers, they come with the fucking xbox. ;) reply paulmd 4 hours agorootparentthinking this through a little bit, you are launching a series of gradient-descent work tasks, right? taskId is your counter value, weightIdx is your key value (RNG stream). That&#x27;s how I&#x27;d port that. Ideally you want to define some maximum PRNG usage for each stage of the program, which allows you to establish fixed offsets from the epoch value for a given event. Divide your keystream in whatever advantageous way, based on (highly-compressible) epoch counters and event offsets from that value.in practice, assuming a gradient-descent event needs a lot of random numbers, having one keystream for a single GD event might be too much and that&#x27;s where key-spreading comes in. if you take the \"weightIdx W at GradientDescentIdx G\" as the key, you can have a whole global keystream-space for that descent stage. And the key-spreading-function lets you go between your composite key and a practical one.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Key_derivation_function(again, like threefry, there is notionally no need for this to be cryptographically secure in most cases, as long as it spreads in ways that your CBRNG crypto algorithm can tolerate without bit-correlation. there is no need to do 2 million rounds here either etc. You should actually pick reasonable parameters here for fast performance, but good enough keyspreading for your needs.)I&#x27;ve been out of this for a long time, I&#x27;ve been told I&#x27;m out of date before and GPGPUs might not behave exactly this way anymore, so please just take it in the spirit it&#x27;s offered, can&#x27;t guarantee this is right but I&#x27;ve specifically gazed into the abyss the CuRand situation a decade ago and this was what I managed to come up with. I do feel your pain on the stateful RNG situation, managing state per-execution-thread is awful and destroys simulation reproducibility, and managing a PRNG context for each possible element is often infeasible. What a waste of VRAM and bandwidth and mutation&#x2F;cache etc.And I think that cryptographic&#x2F;pseudo-cryptographic PRNG models are frankly just a much better horse to hook your wagon to than scientific&#x2F;academic ones, even apart from all the other advantages. Like there&#x27;s just not any way mersenne twister or w&#x2F;e is better than threefish, sorry academia--edit: Real-world sim programs are usually very low-intensity and have effectively unlimited amounts of compute to spare, they just ride on bandwidth (sort&#x2F;search or sort&#x2F;prefix-scan&#x2F;search algorithms with global scope building blocks often work well).And tbh that&#x27;s why tensor is so amazing, it&#x27;s super effective at math intensity and computational focus, and that&#x27;s what GPUs do well, augmented by things like sparse models etc. Make your random not-math task into dense or sparse (but optimized) GPGPU math, plus you get a solution (reasonable optimum) to an intractible problem in realtime. The experienced salesman usually finds a reasonable optimum, but we pay him in GEMM&#x2F;BLAS&#x2F;Tensor compute time instead of dollars.Sort&#x2F;search or sort&#x2F;prefix-sum&#x2F;search often works really well in deterministic programs too. Do you ever have a \"myGroup[groupIdx].addObj(objIdx) stage? that&#x27;s a sort and prefix-sum operation right there, and both of those ops run super well on GPGPU. reply lumost 13 hours agorootparentprevFolks also underestimate how complex these libraries are. There are dozens of projects to make BLAS alternatives which give up after ~3-6 months when they realize that this project will take years to be successful. reply eternityforest 10 hours agorootparentHow does that work? Why not pick up where the previous team left off instead of everyone starting new ones? Or are they all targeting different backends and hardware? reply haltist 10 hours agorootparentIt&#x27;s a compiler problem and there is no money in compilers [1]. If someone made an intermediate representation for AI graphs and then wrote a compiler from that intermediate format into whatever backend was the deployment target then they might be able to charge money for support and bug fixes but that would be it. It&#x27;s not the kind of business anyone wants to be in so there is no good intermediate format and compiler that is platform agnostic.1: https:&#x2F;&#x2F;tinygrad.org&#x2F; reply whatshisface 5 hours agorootparentJAX is a compiler. reply haltist 4 hours agorootparentSo are TensorFlow and PyTorch. All AI&#x2F;ML frameworks have to translate high-level tensor programs into executable artifacts for the given hardware and they&#x27;re all given away for free because there is no way to make money with them. It&#x27;s all open source and free. So the big tech companies subsidize the compilers because they want hardware to be the moat. It&#x27;s why the running joke is that I need $80B to build AGI. The software is cheap&#x2F;free, the hardware costs money. replyrightbyte 48 minutes agorootparentprev> generating random numbersYou can&#x27;t bench implementations of random numbers against each other purely on execution speed.A better algorithm (better statistical properties) will be slower. reply jart 2 hours agorootparentprevI honestly don&#x27;t see why it&#x27;s so hard. On my project we wrote our own gemm kernels from scratch so llama.cpp didn&#x27;t need to depend on cublas anymore. Only took a few days and a few hundred lines of code. We had to trade away 5% performance. reply soulbadguy 1 hour agorootparentFor a given set kernels, and a limited set of architectures, the problem is relatively easy.But covering all the important kernels acros all the crazy architecture out there and with relatively good performance and numerical accuracy ... Much harder reply slavik81 9 hours agorootparentprevIf you haven&#x27;t already, please consider filing issues on the rocrand GitHub repo for the problems you encountered. The rocrand library is being actively developed and your feedback would be valuable for guiding improvements. reply nradov 10 hours agorootparentprevInstead of generating pseudorandom numbers you can just download files of them.https:&#x2F;&#x2F;archive.random.org&#x2F; reply hcrean 7 hours agorootparentOr you could just re-use the same number; no one can prove it is not random.https:&#x2F;&#x2F;xkcd.com&#x2F;221&#x2F; reply latchkey 13 hours agorootparentprevThis is a big reason why AMD did this deal with PyTorch...https:&#x2F;&#x2F;pytorch.org&#x2F;blog&#x2F;experience-power-pytorch-2.0&#x2F; reply singhrac 12 hours agorootparentprevJust to point out it does, kind of: https:&#x2F;&#x2F;github.com&#x2F;intel&#x2F;intel-extension-for-pytorchI&#x27;ve asked before if they&#x27;ll merge it back into PyTorch main and include it in the CI, not sure if they&#x27;ve done that yet.In this case I think the biggest bottleneck is just that they don&#x27;t have a fast enough card that can compete with having a 3090 or an A100. And Gaudi is stuck on a different software platform which doesn&#x27;t seem as flexible as an A100. reply spacemanspiff01 12 hours agorootparentThey could compete on ram, if the software was there. Just having a low cost alternative to the 4060ti would allow them to break into the student&#x2F;hobbies&#x2F;open source market.I tried the a770, but returned it. Half the stuff does not work. They have the CPU side and GPU development on different branches (GPU seems to be ~6 months behind CPU) and often you have to compile it yourself, (if you want torchvision or torchaudio) it also currently on 2.0.1 of pytorch so somewhat lagging, and does not have most of the performance analysis software available. You also, do need to modify your pytorch code, often more than just replacing cuda for xpu as the device. They are also doing all development internally, then pushing intermittently to public. A lot of this would not be as bad if there was a better idea of feature timeline, or if they made their CI public. (Trying to build it myself involved a extremely hacky bash script, that inevitably failed halfway through.) reply selfhoster11 10 hours agorootparentThe amount of VRAM is the absolute killer USP for the current large AI model hobbyist segment. Something that had just as much VRAM as a 3090 but at half the speed and half the price would sell like hot cakes. reply eyegor 8 hours agorootparentYou are describing the ebay market for used nvidia tesla cards. The k80, p40, or m40 are widely available and sell for ~$100 with 24gb vram. The m10 even has 32gb! The problem for ai hobbyists is it won&#x27;t take long to realize how many apis use the \"optical flow\" pathways and so on nvidia they&#x27;ll only run at acceptable speeds on rtx hardware, assuming they run at all. Cuda versions are pinned to hardware to some extent. reply selfhoster11 1 hour agorootparentOut of these three, only P40 is worth the effort to get running vs the capabilities they offer. That&#x27;s also before considering that other than software hacks or configuration tweaks, those cards require specialised cooling shrouds for adequate cooling in tower-style cases.If your time or personal energy is worth >$0, these cards work out to much more than $100. And you can&#x27;t even file the time burnt on getting them to run as any kind of transferable experience.That&#x27;s not to say I don&#x27;t recommend getting them - I have a P4 family card and will get at least one more, but I&#x27;m not kidding myself that the use isn&#x27;t very limited. reply icelancer 7 hours agorootparentprevYep. I have a fleet of P40s that are good at what they do (Whisper ASR primarily) but anything even remotely new... nah. fp16 support is missing so you need P100 cards, and usually that means you are accepting 16GB of VRAM rather than 24GB.Still some cool hardware. reply singhrac 10 hours agorootparentprevThis is pretty disappointing to hear. I’m really surprised they can’t even get a clean build script for users, let alone integrate into the regular Pytorch releases. reply paulmd 4 hours agorootparentprevoh no, I just bought a refurbed A770 16GB for tinkering with GPGPU lol. It was $220, return? reply zozbot234 13 hours agorootparentprevPyTorch includes some Vulkan compat already (though mostly tested on Android, not on desktop&#x2F;server platforms), and they&#x27;re sort of planning to work on OpenCL 3.0 compat, which would in turn lead to broad-based hardware support via Mesa&#x27;s RustiCL driver.(They don&#x27;t advertise this as \"support\" because they have higher standards for what that term means. PyTorch includes a zillion different \"operators\" and some of them might be unimplemented still. Besides performance is still lacking compared to CUDA, Rocm or Metal on leading hardware - so only useful for toy models.) reply ndneighbor 10 hours agorootparentprevOneAPI isn&#x27;t bad for PyTorch, the performance isn&#x27;t there yet but you can tell it&#x27;s an extremely top priority for Intel. reply seanhunter 4 hours agorootparentBut this is the the thing. Speaking as someone who dabbles in this area rather than any kind of expert, it’s baffling to me that people like Intel are making press releases and public statements rather than (I don’t know) putting in the frikkin work to make performance of the one library that people actually use decent.You have a massive organization full of gazillions of engineers many of whom are really excellent. Before you open your mouth in public and say something is a priority, deploy a lot of them against this and manifest that priority by actually doing the thing that is necessary so people can use your stuff.It’s really hard to take them seriously when they haven’t (yet) done that. reply flakiness 9 hours agorootparentprevIntel has to do it by themselves. NVIDIA just lets Meta&#x2F;OpenAI&#x2F;Google engineers do it for them. Such a handicapped fight. reply seanhunter 4 hours agorootparentIt wasn’t always like this. Nvidia did the initial heavy lifting to get cuda off the ground to a point where other people could use it. reply joe_the_user 7 hours agorootparentprevThat&#x27;s because CUDA is a clear, well-functioning library and Intel has no equivalent. It makes any \"you just have to get Pytorch working\" a little less plausible. reply foobiekr 14 hours agoparentprevIt&#x27;s not just Intel. Open initiatives and consortiums (the phase two of the same) are always the losers ganging up hoping that it will give them the leg up they don&#x27;t have. If you&#x27;re older you&#x27;ll have seen this play out over and over in the industry - the history of Unix vs. Windows NT from the 1990s was full of actions like this, networking is going through it again for the nth time (this time with UltraEthernet) and so on. OpenGl was probably the most successful approach, barely worked, and didn&#x27;t help any of the players who were not on the road to victory already. Unix 95 didn&#x27;t work, unix 98 didn&#x27;t work, etc. reply AnthonyMouse 13 hours agorootparentYou&#x27;re just listing the ones that didn&#x27;t knock it out of the park.TCP&#x2F;IP completely displaced IPX to the point that most people don&#x27;t even remember what it was. Nobody uses WINS anymore, even Microsoft uses DNS. It&#x27;s rare to find an operating system that doesn&#x27;t implement the POSIX API.The past is littered with the corpses of proprietary technologies displaced by open standards. Because customers don&#x27;t actually want vendor-locked technology. They tolerate it when it&#x27;s the only viable alternative, but make the open option good and the proprietary one will be on its way out. reply tormeh 8 hours agorootparentOpen source generally wins once the state of the art has stopped moving. When a field is still experiencing rapid change closed source solutions generally do better than open source ones. Until we somehow figure out a relatively static set of requirements for running and training LLMs I wouldn’t expect any open source solution to win. reply kelipso 7 hours agorootparentThat doesn&#x27;t really make sense. Pretty much all LLMs are trained in pytorch, which is open source. LLMs only reached the state it is now because many academic conferences insisted that paper submissions have open source code attached to it. So much of the ML&#x2F;AI ecosystem is open source. Pretty much only CUDA is not open source. reply DeathArrow 2 hours agorootparent>Pretty much only CUDA is not open source.What stops Intel to make their own CUDA and plug in into pytorch? reply nemothekid 1 hour agorootparentCUDA is huge and nvidia spent a ton in a lot of \"dead end\" use cases optimizing it. There have been experiments with CUDA translation layers with decent performance[1]. There are two things that most projects hit:1. The CUDA API is huge; I&#x27;m sure Intel&#x2F;AMD will focus on what they need to implement pytorch and ignore every other use case ensuring that CUDA always has the leg up in any new frontier2. Nvidia actually cares about developer experience. The most prominent example is Geohotz with tinygrad - where AMD examples didn&#x27;t even work or had glaring compiler bugs. You will find nvidia engineer in github issues for CUDA projects. Intel&#x2F;AMD hasn&#x27;t made that level of investment and thats important because GPUs tend to be more fickle than CPUs.[1] https:&#x2F;&#x2F;github.com&#x2F;vosen&#x2F;ZLUDA reply mschuster91 2 hours agorootparentprevThe same shit as always, patents and copyright. reply binkHN 4 hours agorootparentprevYou didn&#x27;t have a choice when it came to protocols for the Internet; it&#x27;s TCP&#x2F;IP and DNS or you don&#x27;t get to play. Everyone was running dual stack to support their LAN and Internet and you had no choice with one of them. So, everything went TCP&#x2F;IP and reduced overall complexity. reply pjmlp 12 hours agorootparentprev> It&#x27;s rare to find an operating system that doesn&#x27;t implement the POSIX API.Except that it has barelly improved beyond CLI and daemons, still thinks terminals are the only hardware, everything else that matters isn&#x27;t part of it, not even more modern networking protocols that aren&#x27;t exposed in socket configurations. reply AnthonyMouse 12 hours agorootparentIts purpose was to create compatibility between Unix vendors so developers could write software compatible with different flavors. The primary market for Unix vendors is servers, which to this day are still about CLI and daemons, and POSIX systems continue to have dominant market share in that market. reply DeathArrow 2 hours agorootparentMaybe it&#x27;s POSIX that is holding new developments back because people think it&#x27;s good enough. It&#x27;s not &#x27;60s anymore. I would have expected to have totally new paradigms in 2023 if you asked me 23 years ago. Even NT kernel seems more modern.While POSIX was state of the art when it was invented, it shouldn&#x27;t be today.Lots of research was thrown in recycle bin because \"Hey, we have POSIX, why reinvent the wheel?\", up to the point that nobody wants to do operating systems research today, because they don&#x27;t want their hard work to get thrown into the same recycle bin.I think that people who invented POSIX were innovators and have they live today, they would come with a totally new paradigm, more fit to today&#x27;s needs and knowledge. reply pjmlp 2 hours agorootparentprevNah, POSIX on servers is only relevant enough for language runtimes and compilers, which then use their own package managers and cloud APIs for everything else.Alongside a cloud shell, which yeah, we now have a VT100 running on a browser window.There is a reason why there are USENIX papers on the loss of POSIX relevance. reply nvm0n2 11 hours agorootparentprevArguably the dominant APIs in the server space are the cloud APIs not POSIX. reply AnthonyMouse 11 hours agorootparentMany of which are also open, like OpenStack or K8s, or have third party implementations, like Ceph implementing the Amazon S3 API. reply natbennett 8 hours agorootparentAlso all reimplementations of proprietary technology.The S3 API is a really good example of the “OSS only becomes dominant when development slows down” principle. As a friend of mine who has had to support a lot of local blob storage says, “On the gates of hell are emblazoned — S3 compatible.” replypaulddraper 12 hours agorootparentprevI think OP&#x27;s point was less that the tech didn&#x27;t work (e.g. OpenGL was fantastically successful) and that it didn&#x27;t produce a good outcome for the \"loser\" companies that supported it. reply AnthonyMouse 11 hours agorootparentThe point of the open standard is to untether your prospective customers from the incumbent. That means nobody is going to monopolize that technology anymore, but that works out fine when it&#x27;s not the thing you&#x27;re trying to sell -- AMD and Intel aren&#x27;t trying to sell software libraries, they&#x27;re trying to sell GPUs.And this strategy regularly works out for companies. It&#x27;s Commoditize Your Complement.If you&#x27;re Intel you support Linux and other open source software so you can sell hardware that competes with vertically integrated vendors like DEC. This has gone very well for Intel -- proprietary RISC server architectures are basically dead, and Linux dominates much of the server market in which case they don&#x27;t have to share their margins with Microsoft. The main survivor is IBM, which is another company that has embraced open standards. It might have also worked out for Sun but they failed to make competitive hardware, which is not optional.We see this all over the place. Google&#x27;s most successful \"messaging service\" is Gmail, using standard SMTP. It&#x27;s rare to the point of notability for a modern internet service to use all proprietary networking protocols instead of standard HTTP and TCP and DNS, but many of them are extremely successful.And some others are barely scraping by, but they exist, which they wouldn&#x27;t if there wasn&#x27;t a standard they could use instead of a proprietary system they were locked out of. reply paulddraper 11 hours agorootparent> The point of the open standard is to untether your prospective customers from the incumbent.That is the point.But FWIW the incumbent adopts it and dominates anyway. (Though you now are technically \"untethered.\") reply AnthonyMouse 10 hours agorootparent> But FWIW the incumbent adopts it and dominates anyway. (Though you now are technically \"untethered.\")That&#x27;s assuming the incumbent&#x27;s advantage isn&#x27;t rooted in the lock-in.If ML was suddenly untethered from CUDA, now you&#x27;re competing on hardware. Intel would still have mediocre GPUs, but AMD&#x27;s are competitive, and Intel&#x27;s could be in the near future if they execute competently.The open standard doesn&#x27;t automatically give you the win, but it puts you in the ring.And either of them have the potential to gain an advantage over Nvidia by integrating GPUs with their x86_64 CPUs, e.g. so the CPU and GPU can share memory, avoiding copying over PCIe and giving the CPU direct access to HBM. They could even put a cut down but compatible version of the technology in every commodity PC by default, giving them a huge installed base of hardware that encourages developers to target it. reply fbdab103 5 hours agorootparentIf the software side no longer mattered, I would expect all three vendors would magically start competing on available RAM. A slower card with double today&#x27;s RAM would absolutely sell. reply sakras 3 hours agorootparent> A slower card with double today&#x27;s RAM would absolutely sellAbsolutely, SQL analytics people (like me) have been itching for a viable GPU for analytics for years now. The price&#x2F;performance just isn&#x27;t there yet because there&#x27;s such a bias towards high compute and low memory. replygamblor956 9 hours agorootparentprevWindows still uses WINS and NetBIOS when DNS is unavailable. reply binkHN 4 hours agorootparentIn a business-class network, even one running Windows, if DNS breaks \"everything\" is going to break. reply ilaksh 12 hours agoparentprevNvidia is probably ten times more scared of this guy https:&#x2F;&#x2F;github.com&#x2F;ggerganov than Intel or AMD. reply ebb_earl_co 11 hours agorootparentCan you expand on this? This is my first time seeing this guy’s work reply pbronez 11 hours agorootparentHe’s the main developer of Llama.cpp, which allows you to run a wide range of open-weights models on a wide range of non-NVIDIA processors. reply fl0id 8 hours agorootparentbut it&#x27;s all inference, and most of Nvidias moat is in training afaik. reply ilaksh 3 hours agorootparentThere is an example of training https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;tree&#x2F;1f0bccb27929e261...But that&#x27;s absolutely false about the Nvidia moat being only training. Llama.cpp makes it far more practical run inference on a variety of devices. Including ones with or without Nvidia hardware. reply refulgentis 5 hours agorootparentprevPeople have really bizarre overdramatic misunderstandings of llama.cpp because they used it a few times to cook their laptop. This one really got me giggling though. reply ilaksh 3 hours agorootparentI am integrating llama.cpp into my application. I just went through one of their text generation examples line-by-line and converted it into my own class.This is a leading-edge software library that provides a huge boost for non-Nvidia hardware in terms of inference capability with quantized models.If you don&#x27;t understand that, then you have missed an important development in the space of machine learning. replygoldenshale 3 hours agoparentprevSeriously, why don&#x27;t they just dedicate a group to creating the best pytorch backend possible? Proving it there will gain researcher traction and prove that their hardware is worth porting the other stuff over to. reply rightbyte 33 minutes agorootparentYou can&#x27;t \"just\" do stuff like this. You need the right guy and big corps have no clue who is capable. reply UncleOxidant 14 hours agoparentprevThis right here. Until Intel (and&#x2F;or AMD) get serious about the software side and actually invest the money CUDA isn&#x27;t going anywhere. Intel will make noises about various initiatives in that direction and then a quarter or two later they&#x27;ll make big cuts in those divisions. They need to make a multi-year commitment and do some serious hiring (and they&#x27;ll need to raise their salaries to market rates to do this) if they want to play in the CUDA space. reply latchkey 13 hours agorootparentLiterally, every single announcement (and action) coming out of AMD these days is that they are serious about the software. I don&#x27;t see any reason at this point to doubt them.The larger issue is that they need to fix the access to their high end GPUs. You can&#x27;t rent a MI250... or even a MI300x (yet, I&#x27;m working on that myself!). But that said, you can&#x27;t rent an H100 either... there are none available. reply lmm 9 hours agorootparent> Literally, every single announcement (and action) coming out of AMD these days is that they are serious about the software. I don&#x27;t see any reason at this point to doubt them.They&#x27;re having to announce it so much because people are rightly sceptical. Talk is cheap, and their software has sucked for years. Have they given concrete proof of their commitment, e.g. they&#x27;ve spent X dollars or hired Y people to work on it (or big names Z and W)? reply latchkey 8 hours agorootparentAgreed. Time will tell.MI300x and ROCm 6 and their support of projects like Pytorch, are all good steps in the right direction. HuggingFace now supports ROCm. reply DeathArrow 1 hour agorootparentprev>Literally, every single announcement (and action) coming out of AMD these days is that they are serious about the software. I don&#x27;t see any reason at this point to doubt them.I have a bridge to sell. reply 65a 1 hour agorootparentIt is way, way better in the last year or so. Perfectly reasonable cards for inference if you actually understand the stack and know how to use it. Is nVidia faster? Sure, but at twice the price for 20-30% gains. If that makes sense for you, keep paying the tax. reply rowanG077 12 hours agorootparentprev reply latchkey 12 hours agorootparentROCm is open source.and: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=pVl25BbczLI reply latchkey 12 hours agorootparentprevWhat @rowan removed were these two comments:\"AMD is not serious, and neither is Intel for that matter. Their software are piles of proprietary garbage fires. They may say they are serious but literally nothing indicates they are.\"\"Yes, and ROCm also doesn&#x27;t work on anything non-AMD. In fact it doesn&#x27;t even work on all recent AMD gpus. T\" reply frognumber 10 hours agorootparentIt&#x27;s not too polite to repost what people removed. Errors on the internet shouldn&#x27;t haunt people forever.However, my experience is that the comments about AMD are spot-on, with the exception of the word \"proprietary.\"Intel hasn&#x27;t gotten serious yet, and has a good track record in other domains (compilers, numerical libraries, etc.). They&#x27;ve been flailing for a while, but I&#x27;m curious if they&#x27;ll come up with something okay. reply eyegor 7 hours agorootparentAs a long time user of intels scientific compiler&#x2F;accelerator stack, I&#x27;m not sure if I&#x27;d call it a \"good track record\". Once you get all their libs working they tend to be fairly well optimized but they&#x27;re always a huge hassle to install, configure, and distribute. And when I say a hassle to install, I&#x27;m talking about hours to run their installers.They have a track record of taking open projects, adding proprietary extensions, and then requiring those extensions to work with other tools. This sounds fine, but they are very slow&#x2F;never update the base libs. From version to version they&#x27;ll muck with deep dependencies, sometimes they&#x27;ll even ship different rules on different platforms (I dare you to try to static link openmp in a recent version of oneapi targeting windows). If you ship a few tools (let&#x27;s say A and B) that use the same dynamic lib, it&#x27;s a royal pain to make sure they don&#x27;t conflict with each other if you update software A but not B. Ranting about consumer junk aside, their cluster focused tooling on Linux tends to be quite good, especially compared to amd. reply Qwertious 7 hours agorootparentprev>It&#x27;s not too polite to repost what people removed. Errors on the internet shouldn&#x27;t haunt people forever.IMO the comment deletion system handles deleting your own comment wrong - it should grey out the comment and strikethrough it, and label it \"comment disavowed\" or something with the username removed, but it shouldn&#x27;t actually delete the comment.Deleting the comment damages the history, and makes the comment chain hard to follow. reply frognumber 7 hours agorootparentI&#x27;m not sure history should be sacred. In the world of science, this is important, but otherwise, we didn&#x27;t used to live in a universe where every embarrassing thing we did in middle school would haunt us in our old age.I feel bad for the younger generation. Privacy is important, and more so than \"the history.\" reply code_biologist 5 hours agorootparentLong ago, under a different account, I emailed dang about removing some comment content that became too identifying only years after the comments in question. He did so and was very gracious about it. dang, you&#x27;re cool af and you make HN a great place! reply krapp 5 hours agorootparentYou&#x27;re being very obsequious about having to personally get approval from a moderator to do on your behalf something every other forum lets you do on your own by default. reply latchkey 8 hours agorootparentprevNormally, I wouldn&#x27;t do that, but this time I felt like both comments were intentionally inflammatory and then the context for my response was lost.\"literally nothing\" is also wrong given that they just had a large press announcement on Dec 6th (yt as part of my response below), where they spent 2 hours saying (and showing) they are serious.The second comment was made and then immediately deleted, in a way that was to send me a message directly. It is what irked me enough to post their comments back. reply rowanG077 4 hours agorootparentI deleted it because I was in an extremely bad mood and later realized it was simply wrong of me to post it and vent my unrelated frustration in those comments. I think it&#x27;s in extremely bad taste to repost what I wrote when I made the clear choice to delete it. reply FeepingCreature 1 hour agorootparentIf you didn&#x27;t say it, the rocks would cry out. Personally, I&#x27;ll believe AMD is maybe serious about Rocm if they make it a year without breaking their Debian repository. replymepian 12 hours agorootparentprevHow’s SYCL proprietary exactly? reply viewtransform 3 hours agorootparentprevAMD made a presentation on their AI software strategy at Microsoft Ignite two weeks ago. Worth a watch for the slides and live demohttps:&#x2F;&#x2F;youtu.be&#x2F;7jqZBTduhAQ?t=61 reply nox100 1 hour agoparentprevIs Mojo trying to solve this?https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=SEwTjZvy8vw reply _the_inflator 14 hours agoparentprevI agree. That’s what MS didn’t understand with cloud and Linux at first.There is more than just a hardware layer to adoption.CUDA is a platform is an ecosystem is also some sort of attitude. It won’t go away. Companies invested a lot into it. reply Keyframe 13 hours agoparentprevGuys need to do ye olde embrace, extend maneuver. What wine did, what Javas of the world did. CUDA driver API and CUDA runtime API either translation or implementation layer that offers compatibility and speed. I see no way around it at this point, for now. reply joe_the_user 7 hours agorootparentThey could do that. That would eliminate Nvidia&#x27;s monopoly. AMD has made gestures in that direction with Hip. But they ultimately don&#x27;t want to do that - Hip support is half-assed and inconsistent. AMD creates and abandons a variety of APIs. So the conclusion is the other chip makers whine about Nvidia&#x27;s monopoly but don&#x27;t want to end it - they just want maneuver to get their smaller monopolies of some sort or other. reply Keyframe 38 minutes agorootparentYou&#x27;re right. At this stage CUDA is de facto what the standard is around. Just like in ISA wars x86 was. Doesn&#x27;t matter if you have POWER whatever when everything&#x27;s on the other thing. I get why not though, it would drag the battle onto Nvidia&#x27;s home turf. At least it would be a battle though. reply papichulo2023 13 hours agoparentprevWasnt Intel the biggest supporter of OpenCV? I dont know any open source heavely supported by Nvidia reply tw04 7 hours agoparentprev> but nobody gets that it&#x27;s the software and ecosystem.Really? Because I’m confident Intel knows exactly what it’s about. Have you looked at their contributions to Linux and open source in general? They employ thousands of software developers. reply madaxe_again 2 hours agorootparentWhich is why intel left nvidia and everyone else in the dust with CUDA, which they developed.Oh, wait… reply MrBuddyCasino 1 hour agoparentprevThey still don&#x27;t get that those who are serious about hardware, must make their own software. reply dheera 9 hours agoparentprevBoth AMD and Intel (and Qualcomm to some degree) just don&#x27;t seem to get how you beat NVIDIA.If they want to grab a piece of NVIDIA&#x27;s pie, they do NOT need to build something better than an H100 right away. There are a million consumers who are happy with a 4090 or 4080 or even 3080 and would love for something that&#x27;s equally capable at half price, and moreover, actually available for purchase, from Amazon&#x2F;NewEgg&#x2F;wherever, and without a \"call for pricing\" button. AMD and Intel are much better at making their chips available for purchase than NVIDIA. But that&#x27;s not enough.What they DO need to do to take a piece of NVIDIA&#x27;s pie is to build \"intelcc\", \"amdcc\", and \"qualcommcc\" that accept the EXACT SAME code that people feed to \"nvcc\" so that it compiles as-is, with not a single function prototype being different, no questions asked, and works on the target hardware. It needs to just be a drop-in replacement for CUDA.When that is done, recompiling PyTorch and everything else to use other chips will be trivial. reply DeathArrow 1 hour agorootparent>Both AMD and Intel (and Qualcomm to some degree) just don&#x27;t seem to get how you beat NVIDIA.That&#x27;s simple, just do what Nvidia did. Be better than the competition. reply flamedoge 4 hours agorootparentprevthats what hip is though. a recompiler for cuda code. its not good enough for ptx assembly to amdgpu yet reply claytonjy 14 hours agoparentprevhuh, I didn&#x27;t even know openvino supported anything but CPUs! TIL reply 2OEH8eoCRo0 9 hours agoparentprev> the software incompatibilities mean you&#x27;ll spend a ton of time getting it to work compared to an Nvidia GPU.Leverage LLMs to port the SW reply brindlejim 15 hours agoprevFun fact: More than half of all engineers at NVIDIA are software engineers. Jensen has deliberately and strategically built a powerful software stack on top of his GPUs, and he&#x27;s spent decades doing it.Until Intel finds a CEO who is as technical and strategic, as opposed to the bean-counters, I doubt that they will manage to organize a successful counterattack on CUDA. reply tester756 12 hours agoparent>finds a CEO who is as technical and strategic, as opposed to the bean-countersDid you just call Gelsinger a \"non-technical\"? wow, how out of touch with reality>Gelsinger first joined Intel at 18 years old in 1979 just after earning an associate degree from Lincoln Tech.[9] He spent much of his career with the company in Oregon,[12] where he maintains a home.[13] In 1987, he co-authored his first book about programming the 80386 microprocessor.[14][1] Gelsinger was the lead architect of the 4th generation 80486 processor[1] introduced in 1989.[9] At age 32, he was named the youngest vice president in Intel&#x27;s history.[7] Mentored by Intel CEO Andrew Grove, Gelsinger became the company&#x27;s CTO in 2001, leading key technology developments, including Wi-Fi, USB, Intel Core and Intel Xeon processors, and 14 chip projects.[2][15] He launched the Intel Developer Forum conference as a counterpart to Microsoft&#x27;s WinHEC. reply jillesvangurp 1 hour agorootparentGelsinger is a typical hardware engineer out of his depth competing against what is effctively a software play. This is a recurring theme in the industry where you have successful hardware companies with strong hardware focused leadership fail over time because they don&#x27;t get software.I used to work at Nokia Research. The problem was on full display during the period Apple made it&#x27;s entry into mobile. We had plenty of great software people throughout the company. But the leadership had grown up in a world where Nokia was basically making and selling hardware. Radio engineers and hardware engineers basically. They did not get software all that well. And of course what Apple did was executing really well on software for what was initially a nice but not particularly impressive bit of hardware. It&#x27;s the software that made the difference. The hardware excellence came later. And the software only got better over time. Nokia never recovered from that. And they tried really hard to fix the software. It failed. They couldn&#x27;t do it. Symbian was a train wreck and flopped hard in the market.Intel is facing the same issue here. Their hardware is only useful if there&#x27;s great software to do something with it. The whole point of hardware is running software. And Intel is not in the software business so they need others to do that for them. Similar to Nokia, Apple came along and showed the world that you don&#x27;t need Intel hardware to deliver a great software experience. Now their competitor NVidia is basically stealing their thunder in the AI and 3D graphics market. Intel wants in but just like they failed to get into the mobile market (they tried, with Nokia even), their efforts to enter this market are also crippled by their software ineptness.This is a lesson that many IOT companies struggle with as well. Great hardware but they typically struggle with their software ecosystems and unlocking the value of the hardware. So much so that one Finnish software company in this space (Wirepas), has been running an absolute genius marketing campaign with the beautiful slogan \"Most IOT is shit\". Check out their website. Some very nice Finnish humor on display there. Their blunt message is that most hardware focused IOT companies are hopelessly clumsy on the software front and they of course provide a solution. reply brindlejim 10 hours agorootparentprevIntel spent more than a decade under Otellini, Krzanich and Swan. Bean counters. Gelsinger was appointed out of desperation, but the problem runs much deeper. I doubt that culture is gone. It has already cost Intel many opportunities. reply alexey-salmin 9 hours agorootparentOtellini wasn&#x27;t an engineer but still he made the historical x86-mac deal, pushed like crazy for x86-android and owned the top500 with xeon phi.The downfall began with Krzanich who had no goal besides raising the stock price and no strategy other than cutting long-term projects and other costs that got in the way. What a shame. reply selimthegrim 5 hours agorootparentKrzanich started out as an engineer reply bell-cot 11 minutes agorootparent\"Optimize for Wall Street\" is a disease to which even the seemingly-best minds can succumb. reply tester756 10 hours agorootparentprev>Intel spent more than a decade under Otellini, Krzanich and Swan. Bean counters.It still doesn&#x27;t change mistake in your original message.>Gelsinger was appointed out of desperation, but the problem runs much deeper.How much \"much deeper\"? VPs? middle level managers? engineers?The example goes from the top, so if he can change the culture at the top, it will eventually get deeper. reply ksec 6 hours agorootparent>It still doesn&#x27;t change mistake in your original message.Precisely. The problem I found on HN is that It is hard to have any meaningful discussion on anything Hardware. Especially when it is mixed with business or economics models.I was greedy and was hoping Intel could fall closer to $20 in early 2023 before I load up more of their stock. Otherwise I would put more money where my mouth is. reply jbm 7 hours agorootparentprev> as technical and strategicIt seems that this is an AND not an OR. reply ksec 7 hours agorootparentprev>Gelsinger was appointed out of desperation,You will need to observe Intel more closely. It was not out of desperation. And Gelsinger is more technical and strategic than you implies. reply natbennett 8 hours agorootparentprevIf you read carefully you’ll see the comment is “as technical and strategic.”That’s very different from “non-technical.”He was clearly capable of leading a team to develop a new processor but that’s not the issue here. reply hackernewds 5 hours agorootparentprev1989 is 34 years ago. reply roenxi 14 hours agoparentprevGelsinger is saying \"the entire industry\" and that seems likely to be a simple fact. Every single player, other than Nvidia, has an incentive to minimise the importance of CUDA as a proprietary technology. That is a lot more programmers than Nvidia can afford to employ.Even if Intel falls over its own feet, the incentives to bring in more chip manufacturers are huge. It&#x27;ll happen, the only question is whether the timeframe is months, years or a decade. My guess is shorter timeframes, this seems to mostly be matrix multiplication and there is suddenly a lot of money and attention on the matter. And AMD&#x27;s APU play [0] is starting to reach the high end of the market with the MI300A which is an interesting development.[0] EDIT: For anyone not following that story, they&#x27;ve been unifying system and GPU memory; so if I&#x27;ve understood this correctly there isn&#x27;t any need to \"copy data to the GPU\" any more on those chips. Basically the CPU will now have big extensions for doing matrix math. Seems likely to catch on. Historically they&#x27;ve been adding that tech to low-end CPU so it isn&#x27;t useful for AI work, now they&#x27;re adding it to the big ones. reply amadeuspagel 13 hours agorootparent> That is a lot more programmers than Nvidia can afford to employ.How many programmers one can employ is determined by profits, and Nvidia has monopoly profits thanks to CUDA, while \"the entire industry\" can at best hope to create some commiditized alternative to CUDA. Companies with real market power can beat entire industries of commodity manufacturers, Apple is the prime example. reply AnthonyMouse 12 hours agorootparentAMD and Intel together have more revenue than Nvidia, even without considering any other player in the industry or any community contributions they get from being open source. reply julienfr112 12 hours agorootparentit&#x27;s not about revenue, it&#x27;s about investment. It is closely related to future profit. Not so much to current revenue ... reply AnthonyMouse 12 hours agorootparentProfit is revenue minus costs. Investment is costs. If you&#x27;re reinvesting everything you take in your current-year profit would be zero because you&#x27;re making large investments in the future. reply tiahura 8 hours agorootparentprevYou&#x27;re forgetting Microsoft.https:&#x2F;&#x2F;www.cnbc.com&#x2F;2023&#x2F;12&#x2F;06&#x2F;meta-and-microsoft-to-buy-am... reply raincole 7 hours agorootparentprev> Gelsinger is saying \"the entire industry\" and that seems likely to be a simple fact. Every single player, other than Nvidia, has an incentive to minimise the importance of CUDA as a proprietary technology. That is a lot more programmers than Nvidia can afford to employ.I mean, this statement is technically true, but it&#x27;s true for any proprietary technology. If things work like this then we won&#x27;t have any industry where proprietary techs&#x2F;formats are prevalent. reply roenxi 7 hours agorootparentI suppose, but it is a practical matter here. CUDA is a library for memory management and matrix math targeted at researchers, hyper-productive devs and enthusiasts. It looks like it&#x27;ll be highly capital intensive, requiring hardware that runs in some of the biggest, nastiest, OSS-friendliest data-centres in the world who all design their own silicon. The generations of AMD GPU that matter - the ones out and on people&#x27;s machines - aren&#x27;t supported for high quality GPGPU compute right now. Alright, that means CUDA is a massive edge right now. But that doesn&#x27;t look like a defensible moat.I was interested in being part of this AI thing, what stopped me wasn&#x27;t lack of CUDA, it was that my AMD card reliably crashes under load doing compute workloads. Then when I see George Hotz having a go, the problem isn&#x27;t lack of CUDA; it was that his AMD card crashed under compute workloads (technically I think it was running the demo suite). That is only anecdata, but 2 for 2 is almost a significant number of people with the small number of players and lack of big money in AI historically.Lacking CUDA specifically might be a problem here, but I&#x27;ve never seen AMD fall down at that point. I&#x27;ve only ever see them fall down at basic driver bugs. And I don&#x27;t see how CUDA would matter all that much because I can implement most of what I need math-wise in code. If I see a specific list of common complaints maybe I&#x27;ll change my mind, but I&#x27;m just not detecting where the huge complexity is. I can see CUDA maintaining an edge for years because it is convenient, but I really don&#x27;t see how it can stay essential. The card can already do the workload in theory and in practice assuming the code path doesn&#x27;t bug out. I really don&#x27;t need CUDA, all I want rocBLAS to not crash. I suspect that&#x27;d go a long way in practice. reply flamedoge 3 hours agorootparentAMD could use testers(cough clients i mean) like you. Jokes aside, please report bugs to rocm github.. reply slavik81 1 hour agorootparentUnless their hardware is on the official support list, I wouldn&#x27;t be too hopeful for a quick resolution. Still, it&#x27;s even less likely to get fixed if it&#x27;s not reported.If nothing else, I would be curious to know more about the issue. Personally, I want to know how well ROCm functions on every AMD GPU. reply droopyEyelids 13 hours agorootparentprevI&#x27;m not an expert here, but with:> That is a lot more programmers than Nvidia can afford to employHow do you account for the increased complexity those developers have to deal with in an environment where there are multiple companies with conflicting incentives working on the standard?My gut reaction is to worry if this is one of those problems like \"9 people working together can&#x27;t have a baby in one month\". reply roenxi 13 hours agorootparentI actually find that a really interesting question with a really interesting answer - the scaling properties of large groups of people are unintuitive. In this case, my guess would be high market complexity, and the entire userbase to ignore that complexity in favour of 1-2 vendors with simple and cheap options. So the market overall will just settle on de-facto standards.Of course, based on what we see right now that standard would be Nvidia&#x27;s CUDA; but while CUDA is impressive I don&#x27;t think running neural nets requires that level of complexity. We&#x27;re not talking about GUIs which are one of the stickiest and most complicated blocks of software we know about, or complex platform-specific operations. I&#x27;d expect that the need for specialist libraries to do inference to go away in time and CUDA to be mainly useful for researching GPU applications to new problems. Training will likely just come down to raw ops&#x2F;second in hardware rather than software.It isn&#x27;t like this stuff can&#x27;t already run on other cards. AMD cards can run stable diffusion or LLMs. The issue is just that AMD drivers tend to crash. That is simultaneously a huge and a tiny problem - if they focus on it it won&#x27;t be around for long. CUDA is an advantage, but not a moat. reply natbennett 14 hours agoparentprevI find the hero-worship of Pat Gelsinger — examples in sibling comments — really weird. My impression of him at VMware was very beancounter-y, not especially technical, and too caught up in personal vendettas and status games to make good technical leadership decisions.Granted, I may have just gotten off on the wrong foot. The first thing he said to Pivotal during the acquisition announcement was, “You were our cousins, but you’re now more like children.” So the whole tone was just weird. reply baq 14 hours agoparentprevThis was true for Intel for at least 10 years and I’m pretty sure for much longer than that. It was probably true for nvidia for about as long as they exist.Hardware without software is just expensive sand. Every semiconductor company knows this. Intel was the one to perfect the whole package with x86 in the first place…In the GPU compute space CUDA is x86. It’s ubiquitous, de facto standard and will be disrupted. Question is if it takes a year or a decade. reply JonChesterfield 11 hours agorootparentThe stereotype is hardware engineers all think software is easy. So while semiconductor firms know software is important, they&#x27;re often optimistic about the ease of creating it.Cuda is enormous, very complicated and fits together relatively well. All the semiconductor startups have a business plan about being transparent drop in replacements for cuda systems, built by some tens of software engineers in a year or two. That only really makes sense if you&#x27;ve totally misjudged the problem difficulty. reply seanalltogether 1 hour agoparentprevNvidia GPU moat has always been their software. Game ready drivers are a big deal for each AAA game launch and they always help to push their fps numbers on reviewers charts. I feel like for 20 years I&#x27;ve been reading people online complain about ATI&#x2F;AMD drivers and how they want to go back to an Nvidia card the next chance they get. reply logicchains 1 hour agoparentprevUltimately it comes down to Intel and AMD being penny wise, pound foolish. They&#x27;re unwilling to hire a lot of quality software engineers, because they&#x27;re expensive, so they just continue to fall behind NVidia. reply mi_lk 14 hours agoparentprevYou don&#x27;t know Pat Gelsinger do you? reply ActionHank 13 hours agorootparentWe&#x27;re in 2023, he&#x27;s been in the CEO seat for 2 years already. He&#x27;s had plenty of time to show the world his intent and where they are going. All that has happened is they launched a very mid GPU and have yielded more ground to AMD. Meanwhile AMD continue to eat away at Intel&#x27;s talent pool, market share, and still managed to push into the AI space.He should be sweating. reply pgeorgi 13 hours agorootparent> for 2 years already ... All that has happened is they launched a very mid GPUHardware development cycles are closer to 5 years. So while he might have gotten some adjustments done on the designs so far, if he turned the ship around it&#x27;ll take a while longer to materialize.The software side is more agile, so any tea leave reading to discern what Gelsinger&#x27;s strategy looks like is best done over there. reply jorvi 10 hours agorootparentNot only that, for a “first” (not sure how much of Larrabee was salvaged) discrete GPU attempt, Intel Arc is fantastic. Look at the first GPUs Nvidia and ATI launched.It’s only when you put them up against Nvidia and AMDs comes-with-decades-of-experience offerings that Intel’s GPUs seem less than stellar. reply jacoblambda 9 hours agorootparentYeah Arc is incredible in how much it accomplished as a first attempt and as long as they keep at it without chopping it up into a bunch of artificially limited market segments then it&#x27;ll probably be incredibly competitive in a few generations. reply signatoremo 11 hours agorootparentprevHis intent is “5 nodes in 4 years” - [0]. The goal is to reclaim the node leadership from TSMC by 2025.They announced the first chips based on Intel 4 today, which is more or less equivalent to TSMC’s 5nm.They may fail, but the goal is clear and ambitious.[0] - https:&#x2F;&#x2F;www.xda-developers.com&#x2F;intel-roadmap-2025-explainer&#x2F; reply ksec 7 hours agorootparentprev>All that has happened is they launched a very mid GPU and have yielded more ground to AMD.And you dont blame that to Raja Koduri but to Gelsinger? reply drexlspivey 13 hours agorootparentprevor Lisa Su reply ac29 9 hours agoparentprevIntel has over 15,000 software engineers, per their website. I couldn&#x27;t find a number for NVIDIA, but it looks like they have a bit above 26k total employees.So, its very likely Intel has more software engineers than NVIDIA. Intel has far more products than NVIDIA though, so NVIDIA almost certainly has more software engineers working on GPU. reply rightbyte 31 minutes agorootparentI would say that over a certain number of devs the output decreases.I think it was 500 people working on Windows XP? A hundred for Windows 95. Etc. reply papichulo2023 14 hours agoparentprevAre you saying they able to develop without a PM every 5 engineers? Insane. reply sulam 14 hours agoparentprevYeah, they should get someone who actually architected a successful chip, like the 486. Maybe a boomerang who used to be CTO. Get rid of this beancounter and hire someone like that!!&#x2F;S reply bartwr 15 hours agoprevIf they create a better tool chain, ecosystem, and programming experience than CUDA and compatible with all computational platforms at their peak performance - awesome! Everyone wins!Until then, it&#x27;s a bit funny claim, especially considering what a failure OpenCL was (programmer&#x27;s experience and fading support). Or trying to do GPGPU with compute shaders in DX&#x2F;GL&#x2F;Vulkan. Are they really \"motivated\"? Because they had so many years and the results are miserable... And I don&#x27;t think they invested even a fraction of what got invested into CUDA. Put your money where your mouth is. reply ttoinou 14 hours agoparentWhat&#x27;s wrong with compute shaders ? reply bartwr 14 hours agorootparentI shipped a dozen products with them (mostly video games), so there&#x27;s nothing \"wrong\" that would make them unusable. But programming them and setting up the graphics pipe (and all the passes, structured buffers, compiling, binding, weird errors, and synchronization) is a huge PITA as compared to the convenience of CUDA. Compilers are way less mature, especially on some platforms cough. Some GPU capabilities are not exposed. No real composability or libraries. No proper debugging. reply pcwalton 13 hours agorootparentThese days, some game engines have done pretty well at making compute shaders easy to use (such as Bevy [1] -- disclaimer, I contribute to that engine). But telling the scientific&#x2F;financial&#x2F;etc. community that they need to run their code inside a game engine to get a decent experience is a hard sell. It&#x27;s not a great situation compared to how easy it is on NVIDIA&#x27;s stack.[1]: https:&#x2F;&#x2F;github.com&#x2F;bevyengine&#x2F;bevy&#x2F;blob&#x2F;main&#x2F;examples&#x2F;shader... reply Const-me 12 hours agorootparentprevI have recently published an AI-related open-source project entirely based on compute shaders https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml and I’m super happy with the workflow. Possible to implement very complicated things without compiling a single line of C++, the software is mostly in C#.> setting up the graphics pipeI’ve picked D3D11, as opposed to D3D12 or Vulkan. The 11 is significantly higher level, and much easier to use.> compiling, bindingThe compiler is design-time, I ship them compiled, and integrated into the IDE. I solved the bindings with a simple code generation tool, which parses HLSL and generates C#.> No proper debuggingI partially agree but still, we have renderdoc. reply raphlinus 10 hours agorootparentI understand why you&#x27;ve picked D3D11, but people have to understand that comes with serious limitations. There are no subgroups, which also means no cooperative matrix multiplication (\"tensor cores\"). For throughput in machine learning inference in particular, there&#x27;s no way D3D11 can compete with either CUDA or a more modern compute shader stack, such as one based on Vulkan 1.3. reply Const-me 9 hours agorootparent> no subgroupsIndeed, in D3D they are called “wave intrinsics” and require D3D12. But that’s IMO a reasonable price to pay for hardware compatibility.> no cooperative matrix multiplicationMatrix multiplication compute shader which uses group shared memory for cooperative loads: https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml&#x2F;blob&#x2F;master&#x2F;Mistral&#x2F;Mistral...> tensor coresWhen running inference on end-user computers, for many practical applications users don’t care about throughput. They only have a single audio stream &#x2F; chat &#x2F; picture being generated, their batch size is a small number often just 1, and they mostly care about latency, not throughput. Under these conditions inference is guaranteed to bottleneck on memory bandwidth, as opposed to compute. For use cases like that, tensor cores are useless.> there&#x27;s no way D3D11 can compete with either CUDAMy D3D11 port of Whisper outperformed original CUDA-based implementation running on the same GPU: https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Whisper&#x2F; reply raphlinus 9 hours agorootparentSure. It&#x27;s a tradeoff space. Gain portability and ergonomics, lose throughput. For applications that are throttled by TOPS at low precisions (ie most ML inferencing) then the performance drop from not being able to use tensor cores is going to be unacceptable. Glad you found something that works for you, but it certainly doesn&#x27;t spell the end of CUDA. reply Const-me 8 hours agorootparent> ie most ML inferencingMost ML inferencing is throttled with memory, not compute. This certainly applies to both Whisper and Mistral models.> it certainly doesn&#x27;t spell the end of CUDANo, because traditional HPC. Some people in the industry spent many man-years developing very complicated compute kernels, which are very expensive to port.AI is another story. Not too hard to port from CUDA to compute shaders, because the GPU-running code is rather simple.Moreover, it can help with performance just by removing abstraction layers. I think the reason why compute shaders-based Whisper outperformed CUDA-based version on the same GPU, these implementations do slightly different things. Unlike Python and Torch, compute shaders actually program GPUs as opposed to calling libraries with tons of abstractions layers inside them. This saves memory bandwidth storing and then loading temporary tensors. replyjzl 10 hours agorootparentprevThis. It&#x27;s crazy how primitive the GPU development process still is in the year 2023. Yeah it&#x27;s gotten better, but there&#x27;s still a massive gap with traditional development. reply mschuetz 13 hours agorootparentprevIt&#x27;s kinda like building Legos vs building actual Skyscrapers. The gap between compute shaders and CUDA is massive. At least it feels massive because CUDA has some key features that compute shaders lack, and which make it so much easier to build complex, powerful and fast applications.One of the features that would get compute shaders far ahead compared to now would be pointers and pointer casting - Just let me have a byte buffer and easily cast the bytes to whatever I want. Another would be function pointers. These two are pretty much the main reason I had to stop doing a project in OpenGL&#x2F;Vulkan, and start using CUDA. There are so many more, however, that make life easier like cooperative groups with device-wide sync, being able to allocate a single buffer with all the GPU memory, recursion, etc.Khronos should start supporting C++20 for shaders (basically what CUDA is) and stop the glsl or spirv nonsense. reply gmueckl 11 hours agorootparentYou might argue for forking off from glsl and SPIR-V for complex compute workloads, but lightweight, fast compilers for a simple language like glsl do solve issues for graphics. Some graphics use cases don&#x27;t get around shipping a shader compiler to the user. The number of possible shader configurations is often either insanely large or just impossible to enumerate, so on the fly compilation is really the only thing you can do. reply pjmlp 12 hours agorootparentprevIronically, most people use HLSL with Vulkan, because Khronos doesn&#x27;t have a budget nor the people to improve GLSL.So yet another thing where Khronos APIs are dependent on DirectX evolution.It used to be that AMD and NVidia would first implement new stuff on DirectX in collaboration with Microsoft, have them as extensions in OpenGL, and eventually as standard features.Now even the shading language is part of it. reply why_only_15 1 hour agorootparentprevCompute shaders are not capable of using modern GPU features like tensor cores or many of the other features needed to feed tensor cores data fast enough (e.g. TMA&#x2F;cp.async.shared) reply johncolanduoni 14 hours agorootparentprevFor GPGPU tasks, they lack a lot of useful features that CUDA has like the ability to allocate memory and launch kernels from the GPU. They also generally require you to write your GPU and CPU portions of an algorithm in different languages, while CUDA allows you to intermix your code and share data structures and simple functions between the two. reply fluxem 14 hours agorootparentprevCUDA = C++ on GPUs. Compute shader - subset of C with a weird quirks. reply zozbot234 14 hours agorootparentThere are existing efforts to compile SYCL to Vulkan compute shaders. Plenty of \"weird quirks\" involved since they&#x27;re based on different underlying varieties of SPIR-V (\"kernels\" vs. \"shaders\") and seem to have evolved independently in other ways (Vulkan does not have the amount of support for numerical computation that OpenCL&#x2F;SYCL has) - but nothing too terrible or anything that couldn&#x27;t be addressed by future Vulkan extensions. reply mschuetz 13 hours agorootparentprevA subset that lacks pointers, which makes compute shaders a toy language next to CUDA. reply raphlinus 10 hours agorootparentVulkan 1.3 has pointers, thanks to buffer device address[1]. It took a while to get there, and earlier pointer support was flawed. I also don&#x27;t know of any major applications that use this.Modern Vulkan is looking pretty good now. Cooperative matrix multiplication has also landed (as a widely supported extension), and I think it&#x27;s fair to say it&#x27;s gone past OpenCL.Whether we get significant adoption of all this I think is too early to say, but I think it&#x27;s a plausible foundation for real stuff. It&#x27;s no longer just a toy.[1] https:&#x2F;&#x2F;community.arm.com&#x2F;arm-community-blogs&#x2F;b&#x2F;graphics-gam... reply mschuetz 2 hours agorootparent> Vulkan 1.3 has pointers, thanks to buffer device address[1].> [1] https:&#x2F;&#x2F;community.arm.com&#x2F;arm-community-blogs&#x2F;b&#x2F;graphics-gam...\"Using a pointer in a shader - In Vulkan GLSL, there is the GL_EXT_buffer_reference extension \"That extension is utter garbage. I tried it. It was the last thing I tried before giving up on GLSL&#x2F;Vulkan and switching to CUDA. It was the nail in the coffin that made me go \"okay, if that&#x27;s the best Vulkan can do, then I need to switch to CUDA\". It&#x27;s incredibly cumbersome, confusing and verbose.What&#x27;s needed are regular, simple, C-like pointers. reply jauntywundrkind 10 hours agorootparentprevIs IREE the main runtime doing Vulkan or are there others? Who should we be listening to (oh wise @raphlinus)?It&#x27;s been awesome seeing folks like Keras 3.0 kicking out broad Intercompatibility across JAX, TF, Pytorch, powered by flexible executuon engines. Looking forward to seeing more Vulkan based runs getting socialized benchmarked & compared. https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38446353 reply raphlinus 9 hours agorootparentThe two I know of are IREE and Kompute[1]. I&#x27;m not sure how much momentum the latter has, I don&#x27;t see it referenced much. There&#x27;s also a growing body of work that uses Vulkan indirectly through WebGPU. This is currently lagging in performance due to lack of subgroups and cooperative matrix mult, but I see that gap closing. There I think wonnx[2] has the most momentum, but I am aware of other efforts.[1]: https:&#x2F;&#x2F;kompute.cc&#x2F;[2]: https:&#x2F;&#x2F;github.com&#x2F;webonnx&#x2F;wonnx reply zozbot234 9 hours agorootparentHow feasible would it be to target Vulkan 1.3 or such from standard SYCL (as first seen in Sylkan, for earlier Vulkan Compute)? Is it still lacking the numerical properties for some math functions that OpenCL and SYCL seem to expect? reply raphlinus 9 hours agorootparentThat&#x27;s a really good question. I don&#x27;t know enough about SYCL to be able to tell you the answer, but I&#x27;ve heard rumblings that it may be the thing to watch. I think there may be some other limitations, for example SYCL 2020 depends on unified shared memory, and that is definitely not something you can depend on in compute shader land (in some cases you can get some of it, for example with resizable BAR, but it depends).In researching this answer, I came across a really interesting thread[1] on diagnosing performance problems with USM in SYCL (running on AMD HIP in this case). It&#x27;s a good tour of why this is hard, and why for the vast majority of users it&#x27;s far better to just use CUDA and not have to deal with any of this bullshit - things pretty much just work.When targeting compute shaders, you pretty much have to manage buffers manually, and also do copying between host and device memory explicitly (when needed - on hardware such as Apple Silicon, you prefer to not copy). I personally don&#x27;t have a problem with this, as I like things being explicit, but it is definitely one of the ergonomic advantages of modern CUDA, and one of the reasons why fully automated conversion to other runtimes is not going to work well.[1]: https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;76700305&#x2F;4000-performanc... reply zozbot234 9 hours agorootparenthttps:&#x2F;&#x2F;enccs.github.io&#x2F;sycl-workshop&#x2F;unified-shared-memory&#x2F; seems to suggest that USM is still a hardware-specific feature in SYCL 2020, so compatibility with hardware that requires a buffer copying approach is still maintained. Is this incorrect? reply raphlinus 8 hours agorootparentGood call. So this doesn&#x27;t look like a blocker to SYCL compatibility. I&#x27;m interested in learning more about this. reply Figs 12 hours agoparentprevI wish AMD or Intel would just ship a giant honking CPU with 1000s of cores that doesn&#x27;t need any special purpose programming languages to utilize. Screw co-processors. Screw trying to make yet another fucked up special purpose language -- whether that&#x27;s C&#x2F;C++-with-quirks or a half-assed Python clone or whatever. Nuts to that. Just ship more cores and let me use real threads in regular programming languages. reply rini17 7 minutes agorootparentAVX might be going the right direction, even if the AVX512 was stretch too far. I was impressed by llama.cpp performance boost when AVX1 support was added.There&#x27;s no intrinsic reason why multiplying matrices requires massive parallelism, in principle it could be done on few cores plus good management of memory bandwidth&#x2F;caches. reply aseipp 9 hours agorootparentprevIt doesn&#x27;t work if you&#x27;re going against GPUs. All the nice goodies we are accustomed to on large desktop x86 machines with gigantic caches and huge branch predictor area and OOO execution engines -- the features that yield the performance profile we expect -- simply do not translate or scale up to thousands of cores per die. To scale that up, you need to redesign the microarchitecture in a fundamental way to allow more compute-per-mm^2 of area, but at that point none of the original software will work in any meaningful capacity because the pipeline is so radically different, it might as well be a different architecture entirely. That means you might as well just write an entirely different software stack, too, and if you&#x27;re rewriting the software, well, a different ISA is actually the easy part. And no, shoving sockets on the mobo does not change this; it doesn&#x27;t matter if it&#x27;s a single die or multi socket. The same dynamics apply. reply ac29 8 hours agorootparentWhile the first >1000 core x86 processor is probably a little ways out, Intel is releasing a 288-core x86 processor in the first half of 2024 (Sierra Forest). I assume AMD will have something similarly high core in 2024-25 as well. reply aseipp 8 hours agorootparentTo be clear, you can probably make a 1000 core x86 machine, and those 1000 cores can probably even be pretty powerful. I don&#x27;t doubt that. I think Azure even has crazy 8-socket multi-sled systems doing hundreds of cores, today. But this thread is about CUDA. Sierra Forest will get absolutely obliterated by a single A100 in basically any workload where you could reasonably choose between the two as options. I&#x27;m not saying they can&#x27;t exist. Just that they will be (very) bad in this specific competition. I made an edit to my comment to reflect that.But what you mention is important, and also a reason for the ultimate demise of e.g. Xeon Phi. Intel surely realized they could just scale their existing Xeon designs up-and-out further than expected. Like from a product&#x2F;SKU standpoint, what is the point of having a 300 core Phi where every core is slow as shit, when you have a 100 core 4-socket Xeon design on the horizon, using an existing battle-tested design that you ship billions of dollars worth every year? Especially when the 300 core Xeon fails completely against the competition. By the time Phi died, they were already doing 100-cores-per-socket systems. They essentially realized any market they could have had would be served better by the existing Xeon line and by playing to their existing strengths. reply dahart 4 hours agorootparentprev> Intel is releasing a 288-core x86This made me wonder a couple of things-What kind of workloads and problems is that best suited for? It’s a lot of cores for a CPU, but for pure math&#x2F;compute, like with AI training and inference and with graphics, 288 cores is like ~1.5% of the number of threads of a modern GPU, right? Doesn’t it take particular kinds of problems to make a 288 core CPU attractive?I also wondered if the ratio of the highest core count CPU to GPU has been relatively flat for a while? Which way is it trending- which of CPUs or GPUs are getting more cores faster? reply imtringued 1 hour agorootparentYou could do sparse deep learning with much, much larger models with these CPUs. As paradoxical as it might sound, sparse deep learning gets more compute bound as you add more cores. reply why_only_15 1 hour agorootparentI&#x27;d be curious to learn more about how it&#x27;s compute bound and what specifically is compute bound. On modern H100s you need ~600 fp8 operations per byte loaded from memory in order to be compute bound, and that&#x27;s with full 128-byte loads each time. Even integer&#x2F;fp32 vector operations need quite a few operations to be compute bound (~20 for vector fp32). replydahart 3 hours agorootparentprevApple might be sort-of trying to build the honking CPU, but it still requires different language extensions and a mix of different programming models.And what you suggest could be done, but it would likely flop commercially if you made it today, which is why they aren’t doing it. SIMD machines are faster on homogenous workloads, by a lot. It would be a bummer to develop a CPU with thousands of cores that is still tens or hundreds of times slower than a comparably priced GPU.SIMD isn’t going away anytime soon, or maybe ever. When the workload is embarrassingly parallel, it’s cheaper and more efficient to use SIMD over general purpose cores. Specialized chiplets and co-processors are on the rise too, co-inciding with the wane of Moore’s law; specialization is often the lowest hanging fruit for improving efficiency now.There’s going to be plenty of demand for general programmers but maybe worth keeping in mind the kinds of opportunities that are opening up for people who can learn and develop special purpose hardware and software. reply JonChesterfield 11 hours agorootparentprevWell, that is what a GPU is. Cuda &#x2F; openmp etc are attempts at conveniently programming a mixed cpu&#x2F;gpu system.If you don&#x27;t want that, program the GPU directly in assembly or C++ or whatever. A kernel is a thread - program counter, register file, independent execution from the other threads.There isn&#x27;t a Linux kernel equivalent sitting between you and the hardware so it&#x27;s very like bare metal x64 programming, but you could put a kernel abstraction on it if you wanted.Core isn&#x27;t very well defined, but if we go with \"number of independent program counters live at the same time\" it&#x27;s a few thousand.X64 cores are vaguely equivalent to GCN compute units, 100 or so if either in a 300W envelope. X64 has two threads and a load of branch prediction &#x2F; speculation hardware. GCN has 80 threads and swaps between them each cycle. Same sort of idea, different allocation of silicon. reply pjmlp 12 hours agorootparentprevIt was called Larrabee and XeonPhi, they botched it, and the only thing left from that effort is AVX. reply vkazanov 12 hours agorootparentI used to play with these toys 7-8 years ago. We tried everything, and it was bad at it all.Traditional compute? The cores were too weak.Number crunching? Okay-ish but gpus were better.Useless stuff. reply pjmlp 3 hours agorootparentHence why \" they botched it\". reply jauntywundrkind 10 hours agorootparentprevThey seemed exceedingly hard to use well but interestingly capable & full of promise. And they were made in a much more primitive software age.I&#x27;d love to hear about what didn&#x27;t work. OpenMP support seemed ok maybe but OpenMP is just a platform, figuring out software architectures that&#x27;s mechanistically sympathetic to the system is hard. It would be so interesting to see what Xeon Phi might have been if we had Calcite or Velox or OpenXLA or other execution engine&#x2F;optimizers that can orchestrate usage. The possibility of something like Phi seems so much higher now.There&#x27;s such a consensus around Phi tanking, and yes, some people came and tried and failed. But most of those lessons, of why it wasn&#x27;t working (or was!) never survived the era, never were turned into stories & research that illuminates what Phi really was. My feeling is that most people were staying the course on GPU stuff, and that there weren&#x27;t that many people trying Phi. I&#x27;d like more than the heresay heaped at Phi&#x27;s feed to judge by. reply vkazanov 26 minutes agorootparentWell... Back then in my shop they would just assign programmers to things, together with a couple of mathematicians.Math guys came up with a list of algorithms to try for a search engine backend.What we needed was matrix multiplication and maybe some decision tree walking (that was some time ago, trees were still big back then, NNs were seen as too compute-intensive for no clear benefits). So we thought that it might be cool to have a tool that would support both. Phi sounded just right for both.And things written to AVX-512 did work. Software surpisingly easy to port.But then comes the usual SIMD&#x2F;CPU trouble: every SIMD generation wants a little software rewrite. So for both Phi generations we had to update our code. For things not compatible with the SIMD approach (think tree-walking) it is just a weak x86.In theory Phi&#x27;s were universal, in practice what we got was: okay number crunching, bad generic compute.GPU was somewhat similar: the software stack was unstable, CUDA just did not materialize as a standard yet. But every generation introduced a massive increase in compute available. And boy did NVIDIA move fast...So GPU situation was: amazing number crunching, no generic compute.And then there were a few ML breakthroughs results which rendered everything that did not look like a matrix multiplication obsolete.PS I wouldn&#x27;t take this story too seriously, details may vary. reply aseipp 8 hours agorootparentprevSome observations:- Very bad performance at existing x86 workloads, so a major selling point was basically not there in practice, because extracting any meaningful performance required a software rewrite anyway. This was an important adoption criteria; if they outright said \"All your existing workloads are compatible, but will perform like complete dogshit\", why would anyone bother? Compatibility was a big selling point that ended up meaning little in practice, unfortunately.- Not actually what x86 users wanted. This was at the height of \"Intel stagnation\" and while I think they were experimenting with lots of stuff, well, in this case, they were serving a market that didn&#x27;t really want what they had (or at least wasn&#x27;t convinced they wanted it).- GPU creators weren&#x27;t sitting idle and twiddling their thumbs. Nvidia was continuously improving performance and programmability of their GPUs across all segments (gaming, HPC, datacenters, scientific workloads) while this was all happening. They improved their compilers, programming models, and microarchitecture. They did not sit by on any of these fronts.Ironically the main living legacy of Phi is AVX-512, which people did and still do want. But that kind of gives it all away, doesn&#x27;t it? People didn&#x27;t want a new massively multicore microarchitecture. They wanted new vector instructions that were flexible and easier to program than what they had -- and AVX-512 is really much better. They wanted the things they were already doing to get better, not things that were like, effectively a different market.Anyway, the most important point is probably the last one, honestly. Like we could talk a lot about compiler optimizations or autovectorization. But really, the market that Phi was trying to occupy just wasn&#x27;t actually that big, and in the end, GPUs got better at things they were bad at, quicker than Phi got better at things it was bad at. It&#x27;s not dissimilar to Optane. Technically interesting, and I mourn its death, but the competition simply improved faster than the adoption rate of the new thing, and so flash is what we have.Once you factor in that you have to rewrite software to get meaningful performance uplift, the rest sort of falls into place. Keep in mind that if you have a $10,000 chip and you can only extract 50% of the performance, you more or less have just $5,000 on fire for nothing in return. You might as well go all the way and use a GPU because at least then you&#x27;re getting more ops&#x2F;mm^2 of silicon. reply jauntywundrkind 8 hours agorootparentI don&#x27;t disagree anywhere but I don&#x27;t think any of these statements actually condemn Xeon Phi outright. It didn&#x27;t work at the time, and doing it with so little software support to tile out workloads well was a big & possibly bad gambit, but I&#x27;m so unsure we can condemn the architecture. There seems to be so few folks who made good attempts and succeeded or failed & wrote about it.I tend to think there was tons of untapped potential still on the table. And that a failure to adopt potential isn&#x27;t purely Intel alone&#x27;s fault. The story we are commenting on is about the rest-of-industry trying to figure out enduring joint strategies, and much of this is chipmaker provided, but it is also informed and helped by plenty of consumers also pouring energy in to figure out what&#x27;s working and not, trying to push the bounds.Agreed that anyone going in thinking Xeon Phi would be viable for running a boring everyday x86 workload was going to be sad. To me the promise seemed clear that existing toolchains & code would work, but it was always clear to me there were a bunch of little punycores & massive SIMD units and that doing anything not SIMD intensive wasn&#x27;t going to go well at all. But what&#x27;s the current trend? Intel and AMD are both actively building not punycores but smaller cores, with Sierra Forest and Bergamo. E-cores are the grown up Atom we saw here.Yes the GPGPU folks were winning. They had a huge head start, were the default option. And Intel was having trouble delivering nodes. So yes, Xeon Phi was getting trounced for real reasons. But they weren&#x27;t architectural issues! It just means the Xeon Phi premise was becoming increasingly handicapped.As I said I broadly agree everywhere. Your core point about giving the market more of what it already does is well taken, is a river of wisdom we see again and again. But I do think conservative thinking, iterating along, is dangerous thinking that obstructs us from seeing real value & possibility before us. Maybe Intel could have made a better ML chip than the GPGPU market has gotten for years, had things gone differently; I think the industry could perhaps have been glad they had veered onto a new course, but the barriers to that happening & the slow down in Intel delivery & the difficulty bootstrapping new software were all horrible encumberances which were rightly more than was worth bearing together. reply vkazanov 19 minutes agorootparentI don&#x27;t thing anybody seriously considered Phi&#x27;s for generic compute or something.Most experimenters saw it as a way to have something GPU-like in terms of raw power but with no limitations charateristic of SIMT&#x27;s. Like, slightly different code paths for threads doing number crunching or something.But it turns out that it&#x27;s easier to force everything into a matrix. Or a very big matrix. Or a very-very-very big matrix.And then see what sticks. replyTheAlchemist 10 hours agoprevCan anybody with a deep knowledge of the AI space, explain to me what&#x27;s the real moat of CUDA ?It&#x27;s clear to everybody that it&#x27;s not the hardware but the software - which is the CUDA ecosystem.I&#x27;ve played a bit in the past with ML, but at the level of understanding I had - training some models, tweaking things, I was using higher level libraries and as far as I know, it&#x27;s pretty much an if statement in those libraries to decide which backend use.So let&#x27;s suppose Intel and others does manage to implement a viable competitor - am I wrong in thinking that the transitions for many users would be seamless ? That&#x27;s probably not the case for researchers and people pushing the boundaries, but for most companies, my understanding is there would be not a lot of migration costs involved ? reply why_only_15 57 minutes agoparentIt is incorrect to say that the moat is the software. The moat is primarily the compute hardware, which is still incredibly good for the price, plus really good networking equipment. CUDA is not a significant moat for massive LLM training runs, as you can see from Anthropic moving from CUDA to Trainium (and so presumably rewriting all of their kernels to Trainium). reply fnbr 3 hours agoparentprevYou need performance in the high level libraries to match, on a flops&#x2F;$ basis. That’s “it”. That’s easier said than done, though. Even google’s TPUs still struggle to match H100s at flops&#x2F;$, and they’re really annoying to use unless you’re using Jax. reply bdd8f1df777b 6 hours agoparentprevYour understanding is correct, but the predicates are not easy at all. The amount of work going into CUDA is enormous, and NVIDIA is not standing still waiting for their competitors to catch up. reply joe_the_user 7 hours agoparentprevI see the situation as a lot like the original IBM PC wars. Originally you had the IBM PC and a bunch of \"compatibles\" that weren&#x27;t drop-in compatible but half-assed compatible - many programs needed to be re-compiled to run on them. And other large American companies made these - they didn&#x27;t expect to commodify the PC, they just wanted a small piece of a big market.The actual PC clones, pure drop-in compatibles, were made in Taiwan and they took over the market. Which is to say that large companies don&#x27;t want a commodified market where prices are low and everyone competes on a level playing field - which is what \"seamless transition\" gets you. So that&#x27;s why none of these companies are working to create that. reply pjmlp 15 hours agoprevIntel and AMD have had years to provide similar capabilities on top of OpenCL.Maybe they should look into their own failures first. reply sorenjan 15 hours agoparentSYCL is a better analog to Cuda than OpenCL, and Intel have their own implementation of that. Don&#x27;t really see anyone writing anything in SYCL though, and when I looked into trying it out it was a bit of a mess with different implementations, each supporting their own subset of OSs and hardware.https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;tools&#x2F;onea... reply shihab 13 hours agorootparent> Don&#x27;t really see anyone writing anything in SYCL thoughMy work involves writing software that runs on many GPU platforms at once. So far we have been going through the Kokkos route, but SYCL is looking pretty good to me recent days. There is some consolidation happening in this space (Codeplay gave up working on their own implementation and merged with Intel). It was pretty easy to setup on my Linux machine for Nvidia card. Documentation is very good and professional, unlike AMD&#x27;s, which can be frankly horrible at times. And Intel has a good track record with software.I genuinely believe if someone is going to dethrone CUDA, at this point SYCL (oneAPI) is a far more likely candidate than Rocm&#x2F;HIP. reply rbanffy 15 hours agorootparentprevI am unfamiliar with the implementation, but why would it be difficult to implement a Cuda-compatible software layer on top of other platforms?This would be the first step. Then, if we want to move away from Cuda into hardware that&#x27;s as ubiquitous and performant as Nvidia&#x27;s (or better), someone would need to write an abstraction layer that&#x27;s more convenient to use than Cuda. I did play a little bit with Cuda and OpenCL, but not enough to hate either. reply pjmlp 14 hours agorootparentBecause that is an herculean task, given the hardware semantics, the amount of languages that target PTX, the graphical debugging tools that expose every little detail of the cards like debugging on the CPU, and the libraries ecosystem.Any Cuda-compatible software layer only has two options, be a second class CUDA implementation by being compatible with a subset like AMD ROCm and HIP effforts, or be compatible with everything always playing catchup.The only way is to use middleware that just like in 3D APIs, abstract the actual compute API being used, as man language bindings are doing nowadays. reply JonChesterfield 11 hours agorootparentprevWorth noting that cuda seems prone to using inline ptx assembly and that latter is really obnoxious to deal with on other platforms.Implementing programming languages and runtimes is pretty difficult in general. Note that cuda doesn&#x27;t have the same semantics as c++ despite looking kind of similar. Wherever you differ from expected behaviour people consider it a bug, and implementing based on cuda&#x27;s docs wouldn&#x27;t get you the behaviour people expect.Pretty horrendous task overall. It would be much better for people to stop developing programs that only run on a gnarly proprietary language. reply pjmlp 3 hours agorootparentCUDA was redesigned to follow C++&#x27;s memory model introduced in C++11, yet another compatibility pain point with other hardware vendors. reply hjabird 13 hours agorootparentprevSYCL is gaining traction, especially in the HPC community since it can target AMD, Nvidia and Intel hardware with one codebase. A fun fact is the GROMACS (a major application for molecular dynamics, and big consumer of HPC time) recommends SYCL for running on AMD hardware! reply pjmlp 14 hours agorootparentprevSYCL builds on top of OpenCL, it is basically the reboot of OpenCL C++, after OpenCL 2.0 SPIR failure.Yet another example on how Intel and AMD failed to take up on CUDA. reply hjabird 13 hours agorootparentSYCL isn&#x27;t based on OpenCL.SYCL (SYCL-2020 spec) supports multiple backends, including Nvidia&#x27;s CUDA, AMD&#x27;s HIP, OpenCL, Intel&#x27;s Level-zero, and also running on the host CPU. This can either be done with Intel&#x27;s DPC++ w&#x2F; Codeplay&#x27;s plugins, or using AdaptiveCpp (aka. hipSYCL, aka openSYCL). OpenCL is just another backend.It is also a very long way from OpenCL C++. The code is a",
    "originSummary": [
      "Intel CEO Pat Gelsinger criticizes Nvidia's CUDA technology, claiming the industry is working to eliminate it.",
      "Gelsinger believes that inference technology for AI is more crucial than training and that Intel will focus on it.",
      "Intel aims to compete with Nvidia in the AI market, emphasizing support for open standards and forming partnerships with Nvidia and AMD."
    ],
    "commentSummary": [
      "The discussion covers various topics in the technology industry, including software, hardware, and competition.",
      "Intel CEO Pat Gelsinger is making efforts to challenge the dominance of NVIDIA CUDA and explore alternative frameworks like PyTorch.",
      "Other discussions include random number generation, encryption techniques, and limitations of graphics cards for machine learning tasks. The article emphasizes the importance of software proficiency and the challenges faced by companies like Intel in the GPU market."
    ],
    "points": 266,
    "commentCount": 301,
    "retryCount": 0,
    "time": 1702578277
  },
  {
    "id": 38646903,
    "title": "Apple Partially Halts Beeper's iMessage App Amidst Frustration and Uncertainty",
    "originLink": "https://arstechnica.com/gadgets/2023/12/apple-partly-halts-beepers-imessage-app-again-suggesting-a-long-fight-ahead/",
    "originBody": "Beeper vs. Apple vs. encryption vs. everybody — Apple partly halts Beeper’s iMessage app again, suggesting a long fight ahead Opinion: Beeper isn't just fighting Apple, it's pushing against human instincts. Kevin Purdy - 12/14/2023, 7:59 PM Enlarge / The dream of everybody having blue bubbles, and epic photos of perfectly digestible meals, as proffered by Beeper. Beeper reader comments 156 A friend of mine had been using Beeper's iMessage-for-Android app, Beeper Mini to keep up on group chats where she was the only Android user. It worked great until last Friday, when it didn't work at all. What stung her wasn't the return to being the Android interloper in the chats again. It wasn't the resulting lower-quality images, loss of encryption, and strange \"Emphasized your message\" reaction texts. It was losing messages during the outage and never being entirely certain they had been sent or received. There was a gathering on Saturday, and she had to double-check with a couple people about the details after showing up inadvertently early at the wrong spot. That kind of grievance is why, after Apple on Wednesday appeared to have blocked what Beeper described as \"~5% of Beeper Mini users\" from accessing iMessages, both co-founder Eric Migicovksy and the app told users they understood if people wanted out. The app had already suspended its plans to charge customers $1.99 per month, following the first major outage. But this was something more about \"how ridiculously annoying this uncertainty is for our users,\" Migicovsky posted. Advertisement Fighting on two fronts But Beeper would keep working to ensure access and keep fighting on other fronts. Migicovsky pointed to Epic's victory at trial against Google's Play Store (\"big tech\") as motivation. \"We have a chance. We're not giving up.\" Over the weekend, Migicovsky reposted shows of support from Senators Elizabeth Warren (D-Mass.) and Amy Klobuchar (D-Minn.), who have focused on reigning in and regulating large technology company's powers. Apple previously issued a (somewhat uncommon) statement about Beeper's iMessage access, stating that it \"took steps to protect our users by blocking techniques that exploit fake credentials in order to gain access to iMessage.\" Citing privacy, security, and spam concerns, Apple stated it would \"continue to make updates in the future\" to protect users. Migicovsky previously denied to Ars that Beeper used \"fake credentials\" or in any way made iMessages less secure. I asked Migicovsky by direct message if, given Apple's stated plan to continually block it, there could ever be a point at which Beeper's access was \"settled,\" or \"back up and running,\" as he put it in his post on X (formerly Twitter). He wrote that it was up to the press and the community. \"If there's enough pressure on Apple, they will have to quit messing with us.\" \"Us,\" he clarified, meant both Apple's customers using iMessage and Android users trying to chat securely with iPhone friends. \"That's who they're penalizing,\" he wrote. \"It's not a Beeper vs. Apple fight, it's Apple versus customers.\" Page: 1 2 Next → reader comments 156 Kevin Purdy Kevin is a senior technology reporter at Ars Technica, covering a variety of technology topics and reviewing products. He started his writing career as a newspaper reporter, covering business, crime, and other topics. He has written about technology and computing for more than 15 years. Advertisement Channel Ars Technica SITREP: F-16 replacement search a signal of F-35 fail? Footage courtesy of Dvids, Boeing, and The United States Navy. SITREP: F-16 replacement search a signal of F-35 fail? Sitrep: Boeing 707 Steve Burke of GamersNexus Reacts To Their Top 1000 Comments On YouTube Scott Manley Reacts To His Top 1000 YouTube Comments LGR's Clint Basinger Reacts To His Top 1000 YouTube Comments How Forza's Racing AI Uses Neural Networks To Evolve The F-35's next tech upgrade Fighter Pilot Breaks Down Every Button in an F-15 Cockpit Linus \"Tech Tips\" Sebastian Reacts to His Top 1000 YouTube Comments Customizing Mini 4WD Racers For High Speeds On A Small Scale MegaBots: Born to Smash Anything in Their Path First Look: Xbox Adaptive Controller Quantum Computing Expert Explains One Concept in 5 Levels of Difficulty Kids versus 80s tech: Game Boy, Vectrex and a stereo system Expert Explains One Concept in 5 Levels of Difficulty - Blockchain Best wearable tech of 2017 The Moov HR Sweat - heart rate monitor in a headbandArs Technica More videos ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=38646903",
    "commentBody": "Apple partly halts Beeper&#x27;s iMessage app again, suggesting a long fight aheadHacker NewspastloginApple partly halts Beeper&#x27;s iMessage app again, suggesting a long fight ahead (arstechnica.com) 234 points by CharlesW 13 hours ago| hidepastfavorite475 comments rezonant 8 hours agoI think this is kinda technically a win for Beeper. I would&#x27;ve expected another 100% lockout to be Apple&#x27;s priority. They were instead only able to block 5%, which sounds like a heuristic being applied, and possibly not even an intentional block of Beeper (in the sense that some anti-spam service may be identifying some Beeper users).They can certainly escalate with protocol changes, but they still have to contend with older Macs, iPhones and iPads which are out of the support window losing access-- so if they want to update the protocol they either have to issue out of band patches for these devices or cut them off too.This is assuming you can actually iMessage on iDevices that are out of software support -- maybe our iOS friends can let us know.EDIT: This take seems more plausible (that this is intentional by Apple): https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38648030 reply chongli 4 hours agoparentMy father has an iPhone 8 which did not receive the latest iOS update. I can iMessage him just fine.There are millions of people out there running old iPhones like that. If Apple just decided to cut them all off iMessage they would do far more damage to their brand than Beeper could possibly manage. reply stefandesu 3 hours agorootparentMy mom has an iPhone 5s running iOS 12 (current is iOS 17, so the software itself is already five years old), and iMessage still works well. reply flomo 3 hours agorootparentprevThat doesn&#x27;t seem hard, people like your father can be allow-listed, he&#x27;s not signing-up out of the blue. (I&#x27;m wondering if this \"works\" because the early adopters were hackintosh types who were already on imessage and not abusers. When the H3rb8l V18gr8 crowd shows up, that is the end.) reply HenryBemis 1 hour agorootparentNow then.. if I am a professional spammer (which I am not), what stops me from buying a second hand mac or a second hand iphone (for $150 each) and start &#x27;doing business&#x27; with iMessage? Unless it&#x27;s an issue of &#x27;iPhones can&#x27;t be JB-ed like Androids can be rooted to run all shorts of malware&#x2F;spamming softwares&#x27;.I get Apple&#x27;s \"this is our toy, you won&#x27;t be making $2&#x2F;month&#x2F;user on us\".But \"keeping the spammers out\" it&#x27;s a bit.. weird..?Can someone please describe the WHAT is the technical advantage of Android users to spam over iPhone&#x2F;Mac users to spam. reply pjerem 1 hour agorootparentThe whitelist would be on the phone number&#x2F;icloud id and not tied to the device.That’d be pretty difficult to get second hand, especially when you can just spam over SMS.They could even release a new iMessage protocol with another bubble color and let the blue ones become uncool. reply realusername 34 minutes agorootparentprev> what stops me from buying a second hand mac or a second hand iphone (for $150 each) and start &#x27;doing business&#x27; with iMessage?Nothing, that&#x27;s why iMessage is full of spam already.What scammers do is buying boxes of old half broken iphones and they turn them into relays exactly as you said. reply Grustaf 11 minutes agorootparentI have never received a single spam message through iMessage, but regardless. If they have to keep buying new phones all the time as they get blacklisted, in order to keep spamming, the economics of spamming changes. reply usui 2 hours agorootparentprevThe what crowd? reply messe 2 hours agorootparentSpam. reply pjerem 1 hour agorootparentprevI’m not talking about your father but I really doubt that the millions of people running 6+yo phones would care a lot about this. They could still send SMS, it’s not like they would became suddenly unreachable.Apple could even be generous and still allow group chats or whitelist existing accounts.Also, you have to take into consideration that iMessage is pretty US centric, the rest of the world wouldn’t really care about this. reply zem 5 hours agoparentprevi believe that thus far it&#x27;s a win for apple - all they need to do is introduce the perception that beeper is not 100% reliable, which is the kiss of death for something as potentially important as a messaging service. reply vintermann 3 hours agorootparentOh no, if the alternative is getting rid of your Android phone and go buy an iPhone, it needs to be a lot worse than that. reply zem 3 hours agorootparentthe alternative is just continuing to sms people from your android because at least you know the messages are definitely going through, green bubble or not reply vintermann 2 hours agorootparentYou still face such problems as being left out of group chats. reply StressedDev 2 hours agorootparentIs this really a problem? I seriously doubt many people are going to leave someone out because they use an Android phone. I certainly won&#x27;t because I like communicating with people and people are far more important than technology. reply vintermann 2 hours agorootparentThat&#x27;s nice of you, but by all accounts from the US (where iphone is dominant): Yes, it&#x27;s a problem. The visual marking and decreased integration&#x2F;service towards users of non-iphones is pretty obviously part of why Apple has such a big phone market share in the US - if not, they wouldn&#x27;t fight tooth and nail to keep those anti-features. There&#x27;s plenty of examples of Apple being quite open and friendly to integration when it benefits them, and here they aren&#x27;t, so it isn&#x27;t. reply Grustaf 7 minutes agorootparentIt’s not just about market share. In Scandinavia about 90% of middle class people use iphones, but this whole blue-green bubble nonsense is a total non-issue. We have group chats in whatsapp, fb messages or sms, nobody cares. Twisell 1 hour agorootparentprevWell in Europe Android is dominating the market (but with a very fragmented experience depending on Android version).The result is that as an iPhone users I feel sometimes feel left out because different friends circles on Android turn to different secured messaging services (WhatsApp, FB messenger, Telegram, Signal, etc...).I firmly refuse to give my personal ID to all theses companies just to keep in touch so I often default to sms&#x2F;mail (or I get left out of group chat).iMessage is not perfect but they did get the sms fallback right and with upcoming RCS support maybe it&#x27;ll be easier to bypass theses competing closed and incompatible walled gardens.So from my point of view, the whole \"blue bubble\" tyranny look like a joke. Apple kept conformance with SMS&#x2F;MMS standards from the beginning and added a secure layer on top. I wish others services just did the same. reply lostlogin 1 hour agorootparent> iMessage is not perfect but they did get the sms fallback rightDid they though? It’s unreliable.Other than this point, I very much share your position. replyzappb 5 hours agoparentprevPriority? Half the company is on vacation right now! reply rezonant 5 hours agorootparentHah, that&#x27;s true. Good strategic timing on Beeper&#x27;s part I suppose. reply lolinder 5 hours agoparentprev> They can certainly escalate with protocol changes, but they still have to contend with older Macs, iPhones and iPads which are out of the support window losing accessThis is what people were saying before Apple cut Beeper off the first time. It would be great if there was just one mistake that they made and fixed, but I&#x27;m not holding my breath. reply rezonant 5 hours agorootparentWell they didn&#x27;t cut off Beeper in the way that would block access to older Apple devices. It seems likely that they found a pattern of access performed (or looked at the identifying information provided) by Beeper in order to target it specifically from the server side.Those were not protocol changes. The iMessage protocol remains unchanged as far as I know. What I&#x27;m referring to above is changing the protocol and updating all clients to use the new protocol so that Beeper is left catching up. This could involve adding new DRM mechanisms or even adding cryptographic remote attestation requirements. reply pjerem 1 hour agoparentprevHonestly they could just choose to disable iMessages on older devices. For the vast majority of people using those old devices, it would just change the bubble color and Apple wouldn’t fear any backlash.I’m not saying they should, but knowing Apple, I wouldn’t be surprised at all if they decided to do it. reply unstatusthequo 7 hours agoparentprevApple owns the delivery mechanism. I don’t believe that a third party using their ecosystem will last long. Nor do I want it. There are plenty of cross-platform things out there. Signal, Telegram, WhatsApp. Why does everyone care so much? Google can paint their Android robot blue. Then maybe all this silliness will end? reply rezonant 7 hours agorootparentBecause Apple embraced texting by supporting SMS, then extended it by forcing all the text conversations that they could into their own proprietary infrastructure, and is extinguishing it by using punitive product design to create pressure on communities of people to all use their products so that everything goes over their proprietary network.I don&#x27;t want an over the top chat app, I just want to text people. reply denkmoon 3 hours agorootparent>Because Apple embraced texting by supporting SMSwhat? sms is a basic phone function, how is supporting sms \"embracing\" anything?If you, as an iOS user, don&#x27;t want to use iMessage you simply go flip the toggle. If you \"just want to text people\", it&#x27;s just that easy. Really. reply Terretta 7 hours agorootparentprevHow do you want Apple to tell iPhone users whether their messages to someone are being recorded by that person&#x27;s telco and made available to other plan holders on that phone plan?How do you want Apple to indicate if this chat participant is costing you money by the message or free?Everyone&#x27;s concerned about teens. Presumably teens know that T-Mobile and the other carriers give the family plan adults the ability to read their dependents text messages. As a teen I would want to stick with the encrypted bubbles your parents can&#x27;t read and tell my parents about, thank you very much.It&#x27;s not punitive product design. It&#x27;s seamlessly integrated and meaningful, both on chat leaks and on costs of messaging: Blue sky, text safely. Green could literally cost you. reply brandon272 7 hours agorootparentI have recent experience talking to non-techie younger people recently about this very issue and none of them were aware of the security differences of SMS vs. iMessage.Teenagers know that blue = iPhone and green = non-iPhone&#x2F;SMS and that blue offers significantly more features and functionality vs SMS (delivered&#x2F;read receipts, group chats, stickers, rich media, memojis, etc), which is the overwhelming reason why blue is preferred. reply HenryBemis 1 hour agorootparentPlus the cost. Over here in the EU, back in 2010 (?) SMS was an expensive thing, you would pay dearly for each or have to buy a \"50&#x2F;100&#x2F;500 SMS package\" or similar.So lowering the cost of 3G made it more economical if your friends had iPhones as now you could spend €20&#x2F;month for &#x27;1GB&#x27; (which was mostly iMessage & web browsing at the time) and avoid spending that simply on SMS. (excuse the price inaccuracies if any, it&#x27;s been a while since I had that iPhone 3GS) reply 8note 5 hours agorootparentprevDid you get this before?All your messages have been recorded by the government, since the government has been collecting all push notifications on iPhones, and iMessage runs over push notifications reply rezonant 5 hours agorootparentActually no, the end to end encryption on iMessage is envelopes inside the push notifications. The message content is not readable, even if you intercept APNs messages. reply keep_reading 5 hours agorootparentAnd it wasn&#x27;t a blanket capture of push notifications anyway, right? Nobody has confirmed so far this is a Room 641a situation reply stonogo 7 hours agorootparentprevT-Mobile does not give primary account holders access to messaging content of other lines, regardless of the relationship between users. reply makeitdouble 7 hours agorootparentprev> Nor do I want it> Why does everyone care so much?You seem to both care about it, and also wonder why other people care about it ?Otherwise, looking from the sideline it&#x27;s fascinating seeing Apple fighting this battle that they brought upon themselves and have no chance of winning. reply hx8 7 hours agorootparent> have no chance of winningI think Apple will almost certainly win on a purely technical game of cat and mouse.I think you need to adjust your definition of winning. Blocking 5% of messages is a &#x27;win&#x27; for Apple. I won&#x27;t use a messaging service with a 95% success rate. I won&#x27;t migrate from iMessage to Beeper. I will submit, Apple would have liked a more decisive victory. reply makeitdouble 6 hours agorootparentMy definition of a win for Apple would be to have the problem go away and the attention dissipate. That&#x27;s how it went for Nothing&#x27;s attempt for instance, where it was instantly ridiculed and everyone forgot about it.Right now, they blocked a part of Beeper mini, and nobody expecting a rock solid service would join Beeper Mini so Apple&#x27;s won&#x27;t be losing any of their core customers.But the news cycle keeps going on, Beeper Mini is still there for those the group of users that wants it alive, and I wouldn&#x27;t be surprised if next week for instance actual iMessage users came out to complain about getting kicked out of the service as colateral damage from the whole additional filtering.And of course this whole publicity for Beeper is a door opened to any other company to give it a shot, as Apple is playing the cat and mouse game, and not taking any more drastic option.Apple isn&#x27;t losing either, but they&#x27;re now dragged into guerrila like battle with no upside for them. reply nucleardog 2 hours agorootparent> so Apple&#x27;s won&#x27;t be losing any of their core customers.They won&#x27;t be losing _any_ of their customers. For the most part[^], nobody using Beeper Mini has paid Apple for anything... otherwise they wouldn&#x27;t need to use Beeper Mini.[^] Yes I&#x27;m sure there&#x27;s at least a few people with an iPhone and a Windows PC or something that see this as \"iMessage on Windows\". reply makeitdouble 46 minutes agorootparentBeeper mini is not ready for regular use, so it&#x27;s not even a question, but I think there&#x27;s many potential use if it was any good.For instance if you have a mac and an iPad but use an android phone, iMessages will go to both Apple devices but not where you want it the most, on your phone. That&#x27;s the kind of pain point that pushes a group to fully move to another service if the android members have enough weight, but would be fine if there was a reliable android client. reply mastazi 5 hours agorootparentprev> on a purely technical game of cat and mousebased on the history of tech-related cat and mouse games, then Apple will probably lose.Just like Sony could not prevent people from pirating Playstation DVDs, Apple itself could not prevent iOS jailbreaks, music labels could not prevent CD ripping, etc etc etcif there are enough people who are strongly motivated to bypass whatever protection, eventually they will probably bypass it. reply 8note 5 hours agorootparentprevApple wins when you aren&#x27;t allowed to message somebody using beeper, rather than you switching to beeper from iMessage reply ethanbond 7 hours agorootparentprevI care about it because lack of iMessage is still a very good heuristic for spam. Not perfect anymore, but I reckon this will make it much worse. reply makeitdouble 6 hours agorootparentYour advice was \"There are plenty of cross-platform things out there. Signal, Telegram, WhatsApp\", but you don&#x27;t seem happy to take it and move away from iMessage either.This tension is at the center of the it all, and why Beeper Mini exists in the first place. reply ethanbond 5 hours agorootparentWhat? I pay for iMessage. You’re just arguing for the right to use products for free, and in Beeper’s case to create monetized products that use others’ products for free. reply Jtsummers 5 hours agorootparent> What? I pay for iMessage.I doubt that. iMessage is a free service if you have almost any Apple OS product (iOS, iPadOS, macOS). You aren&#x27;t making ongoing payments for the iMessage service.You could say you paid for iMessage in that you bought a device that worked with it. But you do not pay for iMessage. reply makeitdouble 4 hours agorootparentprevI&#x27;m arguing that the crux of the issue is a lot more than just \"why don&#x27;t they use something else ?\" The same way you see value in iMessage, other users also see value in iMessage. You may want them to go away, but to my eyes that&#x27;s the same weight as other users wanting to be there. I&#x27;ll be standing in the corner with the popcorn to see how it turns out.On what is paid and what is free, Beeper mini is free, iMessage is free (as we&#x27;ve learned from the whole saga, you don&#x27;t even need an icloud account). Using someone else&#x27;s public facing API without consent is rude, but hey, our whole industry started with kits to plug into the AT&T network with unauthorized material, and as of now no money is changing hands. replyjauntywundrkind 6 hours agorootparentprev> Why does everyone care so much?Happy to explain. First thought, why do you care so much?> Nor do I want it.Why? What about exclusivity makes the world better? Why shouldn&#x27;t I be able to communicate well with someone using an Apple device? Why should someone using an Apple device not want someone to communicate well with them?Sure there are other systems. But switching costs are so high. Especially with iMessage, folks are going to use what&#x27;s provided them out of the box. It doesn&#x27;t seem like a reasonable ask to get everyone en masse to agree to & switch to a lone cross-platform system. What&#x27;s really needed is standards & interop. You should be able to use what you like, be that iMessage or RCS or Signal or XMPP. But none of these options should be locked out of working with others.I&#x27;m so baffled by the strident defenses against possibility. From someone whose name is @unstatusthequo at that, going to bat for status quo lock in seems like a low and dark comedy. Un status quoer, un status quo thineself. Don&#x27;t triple down on the fixed & limited! reply 8note 5 hours agorootparentThey&#x27;re using \"buying an Apple device\" as a spam filter, rather than using democratic means to put good regulations that are anti-spam. reply nucleardog 2 hours agorootparentRegulations... like CAN-SPAM? TCPA?Or CASL in Canada?Or the \"Spam Act\" in Australia?Or the PECR in the UK?Yeah, what we need is more regulation. That&#x27;s been solving the problem. replyunsigner 2 hours agoprevThese comments mention spam a lot as a reason to stick with iMessage.What kind of spam are people (presumably in the US) getting outside of iMessage? I’m in the EU, I get very little SMS spam (mostly for telecom’s clumsy attempts to find more ways to monetize me).I also have Viber, Telegram, and FB messenger, and the spam is virtually zero on Viber (less than an “offer” a month, and it’s from the likes of Coca Cola, not herbal viagra) and literally zero on the others.Is spam significantly worse elsewhere? reply lgbr 17 minutes agoparentI don&#x27;t have hard numbers, but I can say that spam on both sides of the ocean exist. In the US, we get a lot of SMS spam targeted at homeowners, pestering them about selling their property, since there&#x27;s a lot of open data about property ownership. In Germany, there&#x27;s frequently spam pretending to be one of the major shipping companies (DHL for example) or the local customs office (Zoll) saying you have a package that couldn&#x27;t be delivered and to click on a link. I&#x27;ve found the source for this data to be customer data leaks.So yes, spam is a serious concern. WhatsApp spam exists as well, but since there&#x27;s a central authority, unlike with SMS, it&#x27;s a lot harder to avoid being shutdown. reply JoblessWonder 10 hours agoprevAs I mentioned in the previous discussion[1], users are going to have to put up with the system going down... and I just don&#x27;t see that happening. From the article: \"It was losing messages during the outage and never being entirely certain they had been sent or received. There was a gathering on Saturday, and she had to double-check with a couple people about the details after showing up inadvertently early at the wrong spot.\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38533775 reply rezonant 3 hours agoparentInterestingly, if you break your iPhone on vacation and buy a phone that is not an iPhone so that you can still be contacted until you get home to your favorite Apple Store, you are also losing messages, assuming you forget to go through Apple&#x27;s iMessage deregistration system. Really the design of iMessage is the problem. reply j16sdiz 3 hours agorootparentDon&#x27;t need to go to Apple store.It is self-seviced https:&#x2F;&#x2F;selfsolve.apple.com&#x2F;deregister-imessage&#x2F; reply thethimble 3 hours agorootparentprevReally? Doesn’t iMessage fall back to sms if the receiver doesn’t ack? At least that’s what the UX feels like. reply JoblessWonder 3 hours agorootparentNot OP, but I believe it falls back to SMS if there isn’t a response from Apple’s servers, not the end user’s device. In this scenario the iMessages are sitting there waiting for a registered device to check in to send them to. (Same thing that happens if your battery dies. iMessages go into a void until some place to deliver it to.) reply rpmisms 5 hours agoparentprevDoesn&#x27;t bother me at all. I&#x27;m ok with losing a text or two to help bring regulatory pressure against Apple to un-wall the garden so I can use it again. reply vachina 5 hours agorootparent> I&#x27;m ok with losing a text or twoThat’s the whole point. You’re using it for non-critical things. reply rpmisms 4 hours agorootparentYes, but I&#x27;m happy to help provide critical mass. reply tshirthoodie 8 hours agoparentprevApple&#x27;s iMessage implementation loses messages all the time. It happens about once every month between my girlfriend and I.I have also had iMesages be sent to the wrong recipient. In one ongoing case, my aunt and uncle frequently receive each other&#x27;s iMessages, suggesting it&#x27;s related to their group carrier plan but it&#x27;s hard to say. The extended family all knows to use SMS instead, which reliably goes to the intended recipient. They are both retired and always together so they haven&#x27;t reached the point of getting a new plan. reply rgbrenner 8 hours agorootparentiMessage is routed through apple&#x27;s servers over TCP. So this is like saying: sometimes I go to google.com but it randomly loads bing.com... must be related to my group carrier planWhat data made you make that association? Seems unlikely. If that happens, then apple would have to be routing messages incorrectly, but that would be a massive bug&#x2F;security issue... especially considering how E2EE works. reply just2043 6 hours agorootparentYeah sounds more like the aunt and uncle once shared an Apple ID and then moved to seperate ones later. reply mortenjorck 5 hours agorootparentThis is by far the most plausible explanation. iMessage’s biggest surface area for problems is multiple&#x2F;migrated associated emails. reply tshirthoodie 4 hours agorootparentprevInteresting. Do you know how they could fix it? I will send them this thread. They will love it. reply tshirthoodie 8 hours agorootparentprevYeah, who knows? I am only brainstorming. Ever since I discovered the abysmal state of Apple&#x27;s customer service after having my iphone stolen last year, I feel confident we will never know. reply frumper 3 hours agorootparentI’m just curious what you think Apple should do for a stolen phone? reply tshirthoodie 3 hours agorootparentFor a stolen phone with AppleCare+ insurance that has been paid for every month since purchasing the phone, they should at least offer a way to file a claim.The only way to submit a claim was to click on a button in my icloud account that did not exist. I live in a big city and visited 3 Apple stores and spent about 15 hours in them in total, but none of them were able to help me get the claim filed so I had to give up. One of the supervisors did refund the ~$15&#x2F;mo that I had paid for the AppleCare+ insurance, but I had to buy a new phone. reply rezonant 8 hours agorootparentprevI could believe losing messages. I cannot believe wrong recipient. One of these is a service reliability issue and the other is an insanely unlikely bug.And what are the chances that the actual recipient ends up being the spouse of your expected recipient? Sounds more like they logged into each other&#x27;s phones or something.> suggesting it&#x27;s related to their group carrier planExtremely doubtful it has anything to do with their plan. iMessage is over the top: only the first SMS activation message has anything to do with your carrier -- after that it&#x27;s all sent through Apple&#x27;s servers. reply wrboyce 8 hours agorootparentprevThis has never, ever, happened to me and I have been using iMessage daily since its inception.I call shenanigans. reply alexchantavy 7 hours agorootparentAdding my own datapoint: I’ve absolutely had this happen on iPhone to iPhone where the message appeared to be sent on one device but the other received nothing reply wrboyce 7 hours agorootparentIf I was being asked to debug this for someone I know, my first two questions would be “did your phone say delivered?” and “does the recipient have a Mac&#x2F;iPad&#x2F;etc that could’ve received the message?”; do you know the answer to either of these?I worry that sounds accusatory but that’s not my intention, I’m curious and too tired to attempt to reword it. reply JoblessWonder 3 hours agorootparentNot person you responded to… but isn’t it safe to assume that iMessages are delivered to all devices on the account? reply makeitdouble 7 hours agorootparentprevhttps:&#x2F;&#x2F;www.coolmuster.com&#x2F;ios-recovery&#x2F;iphone-text-messages...Doesn&#x27;t feel like some rare issue. Googled it by curiosity, and there&#x27;s a ton of \"help\" articles and support post on people randomly seeing messages disappear or not getting delivered. reply JoblessWonder 3 hours agorootparentJust a heads up, this article is SEO clickbait for a backup app and a lot of the others might be as well. reply jacquesm 7 hours agorootparentprevSo if something never ever happened to you it isn&#x27;t true? That&#x27;s a pretty self centered view of things. reply wrboyce 7 hours agorootparentI shouldn’t form an opinion based on my own experiences because it might upset you? That’s a pretty self centred view of things.I could’ve expanded, I’m pretty much de facto “IT Support” for a lot of the friends&#x2F;family I’ve spent those years communicating with using iMessage so I can pretty confidently say it has never happened to any of them either. I could go on to say that if it was a widespread issue this wouldn’t be the first we are hearing about it, and it absolutely would’ve been covered in some sort of tech news - possibly even the regular old news.But sure, let’s go with self centred. reply jacquesm 7 hours agorootparentYou misread me: if something didn&#x27;t happen to you that doesn&#x27;t invalidate the GPs experience, it just means that your experiences differ. Now you have to figure out why they differ. reply blahgeek 8 hours agorootparentprevAre they sharing the same iCloud ID or something? That&#x27;s the most likely scenario to me reply tshirthoodie 8 hours agorootparentMy gen-z cousins have promised me that isn&#x27;t the case. reply danielheath 7 hours agorootparentprevOne obvious cause comes to mind: If your aunt and uncle use the same apple login for both phones, they&#x27;ll often get each others imessages. reply omeze 4 hours agorootparentprevYoure being downvoted but I also lose a message every few months between my girlfriend and I. We both have relatively new iPhones and exclusively use iMessage so I do think its some rare protocol bug. reply tshirthoodie 4 hours agorootparentI&#x27;m surprised so many people claim they haven&#x27;t experienced it and I wonder how many have and just don&#x27;t know. reply JoblessWonder 3 hours agorootparentprevGotta check Settings -> Messages -> Send & Receive.I would bet there is at least one overlap or misconfiguration there. reply danaris 8 hours agorootparentprevI use iMessage regularly, and have done so since it was first released.I have never, to my knowledge, had a message sent to me lost (which I would have found out because my family members, who would have been the ones to try and fail to send it to me, would have mentioned it), nor lost a message I sent (except for obvious cases where I had insufficient signal, and the app clearly notified me of it).I have never received a misdirected message, had mine misdirected, nor heard of anyone else doing either.In all the criticisms of iMessage I&#x27;ve seen recently—in this Beeper situation, and a little longer ago when Apple decided to support RCS—I have seen no one else say that they&#x27;ve had issues like you describe.Obviously, this doesn&#x27;t mean that these things can&#x27;t happen. But it does suggest that Your Experiences Are Not Universal. reply trws 8 hours agorootparentThe most common case I know of for what looks like misdirection is having one contact with multiple iMessage phone numbers, especially if multiple contacts share one or more of them. Real world example, my wife and I each have both a work iPhone and a personal iPhone. If both are listed in a single contact, iOS will “helpfully” merge the iMessages from both on a sender’s device, but not on the receiver’s. It’s very difficult to tell which device you’re sending to in this case, and can be changed by the other party if they send you a message and you reply. If you didn’t know what was going on, that would look a whole lot like a message going to the wrong device or even recipient in some cases. As a result I literally have separate contacts for my wife to avoid the problem, the UX is otherwise really abysmal. reply the_mar 7 hours agorootparentYeah, that’s how contacts work. What would you like to happen instead? reply angoragoats 5 hours agorootparentThis is how iMessage works, not contacts, and as a user of the iPhone since its launch in 2007 this is surprisingly unexpected to me. I would expect each separate email&#x2F;phone number to have its own conversation. replyNettleBurr 12 hours agoprev>Apple on Wednesday appeared to have blocked what Beeper described as \"~5% of Beeper Mini users\" from accessing iMessages>Apple previously issued a (somewhat uncommon) statement about Beeper&#x27;s iMessage access, stating that it \"took steps to protect our users by blocking techniques that exploit fake credentials in order to gain access to iMessage.\" Citing privacy, security, and spam concerns, Apple stated it would \"continue to make updates in the future\" to protect users. Migicovsky previously denied to Ars that Beeper used \"fake credentials\" or in any way made iMessages less secure.Not commenting about the ethics of all this, just wondering why technically Apple can only block ~5% of Beeper Mini users instead of all of them? Could this potentially be tied to the use of an email id as the iMessage handle? reply turquoisevar 11 hours agoparent> Not commenting about the ethics of all this, just wondering why technically Apple can only block ~5% of Beeper Mini users instead of all of them? Could this potentially be tied to the use of an email id as the iMessage handle?Apple could block 100% of the people using Beeper and throw Hackintosh users into that as a bonus as well.The reason they’re not doing that is because it could have unintended consequences as some are using someone else’s actual device serial number and those people would be inconvenienced.It’s nothing that can’t be easily solved, the moment they reach out to support either in person or via phone&#x2F;chat Apple can immediately verify if they’re using a legitimate Apple device, but even if it boils down to a small percentage of users you still need to prepare for the influx of support requests.To do this, Apple uses a scoring model to determine if they can access iMessage and historically they’ve been pretty generous by allowing clearly spoofed serials if the Apple ID involved is in good standing and has a positive history, think of it as a credit score. They can tweak the threshold score and probably are testing this out as we speak to find a sweet spot they’re content with.Apple could also push out an update tomorrow that would end this once and for all by utilizing device attestation and leveraging Secure Enclave, but this would potentially lock out older devices, something they were willing to do when they upgraded the FaceTime protocol a couple of years ago, but they might not want to do that this time around. reply dylan604 9 hours agorootparent>Apple could also push out an update tomorrow that would end this once and for all by utilizing device attestation and leveraging Secure Enclave, but this would potentially lock out older devices, something they were willing to do when they upgraded the FaceTime protocol a couple of years ago, but they might not want to do that this time around.Just give it a couple more hardware generations to ensure the largest % of older hardware upgrades. Anything pre-secure enclave chip would need to be in the low digits I&#x27;m guessing. Then again, if they are going to block Messages, that might be the incentive to get these older device users to upgrade. reply Sunspark 1 hour agorootparentAre you talking about the iOS 6 to 7 transition where the security certificates expired and Apple wouldn&#x27;t issue a new one and said you needed to switch to iOS 7 if you wanted Facetime to work again? That was my last iOS device. reply jwells89 9 hours agorootparentprev> The reason they’re not doing that is because it could have unintended consequences as some are using someone else’s actual device serial number and those people would be inconvenienced.One is supposed to try to find a plausible (follows certain rules) but invalid serial to use for hackintoshing and not use real serials, but of course in practice there’s always some number of careless users… reply turquoisevar 6 hours agorootparentYeah, that&#x27;s what they&#x27;re supposed to do, and to the credit of the Hackintosh community, that’s what most tutorials suggest.But like you said, there are always people who don’t care about others as long as they have theirs.There is, so far anyway, no reason to go against this best practice because, even though Apple can instantly detect a bogus serial, their currently used scoring threshold still allows you to use iMessage provided you’ve got a non-fresh Apple ID in good standing. reply rezonant 7 hours agorootparentprevThis is interesting. How do you define \"invalid\" and why can Apple not also detect such invalidity?There&#x27;s been some talk that blocking this for Beeper will also block this for Hackintosh, but are we just talking about iMessage?Because I have a hard time believing that (A) Apple can&#x27;t just block this for iMessage without affecting whatever other system services rely on it and (B) That Apple would care if Hackintoshes lose iMessage.If those two are true, and assuming Beeper Mini also tries to find plausible but invalid serials to use, then Hackintoshes definitely aren&#x27;t the reason they aren&#x27;t blocking based on this. reply jwells89 7 hours agorootparentMy understanding is that the serials represent information, including model and date&#x2F;location of manufacture. It’s therefore possible to create correctly formed but impossible serials, for example one that represents a pre-touchbar 2015 MBP manufactured in Ireland in 2018.Apple should easily be able to tell when someone has done this. reply turquoisevar 6 hours agorootparentThey indeed used to have data like that encoded in it.Not too long ago, however, they moved to a completely randomized serial format, perhaps partly because of iMessages shenanigans. reply nneonneo 5 hours agorootparentiMessage seems to use quite a lot of information from the hardware aside from the serial number. See https:&#x2F;&#x2F;github.com&#x2F;JJTech0130&#x2F;pypush&#x2F;blob&#x2F;main&#x2F;emulated&#x2F;data... for the data that is used to calculate the \"validation blob\" to activate iMessage. Several of the keys (not values!) are random-looking gibberish like \"kbjfrfpoJU\" and \"oycqAZloTNDm\", while others are normal things like \"product-name\" and \"IOPlatformUUID\". reply turquoisevar 4 hours agorootparentThose random looking keys are derived from the hardware and together with the values they serve as seeds for some of the keys.Server-side there’s a bunch more information used from the Apple ID.Together it results in a score and the server then decides if it meets the threshold before deciding to play nice. reply rezonant 6 hours agorootparentprevHmm, how many bits of entropy are in one of these things? Can we calculate the likelihood of collision? reply turquoisevar 6 hours agorootparentprevApple can detect this, but they’ve allowed it in most cases when it’s done with an Apple ID in good standing and some history.Why they allowed it is anyone’s guess, but the leading theory is that they valued not hindering established customers over locking iMessage completely down and perhaps the bad PR that comes with banning someone’s Apple ID over this. reply rezonant 6 hours agorootparentWell they could block the client itself, independent of blocking the Apple ID. It&#x27;s the client that sends the serial information. Your Apple ID only gets associated with it indirectly. reply rezonant 7 hours agorootparentprev> The reason they’re not doing that is because it could have unintended consequences as some are using someone else’s actual device serial number and those people would be inconvenienced.As far as I know, it&#x27;s not actually known what model numbers, serial numbers, and disk UUIDs Beeper Mini is sending (and no the POC repository doesn&#x27;t really tell us)-- if you have a source that talks about this I&#x27;d love to read it! reply _rs 4 hours agorootparentI’m pretty sure Apple could figure this out pretty easily by running it on an Android device themselves, considering they control the endpoints it talks to reply beeboobaa 10 hours agorootparentprev> Apple could also push out an update tomorrow that would end this once and for all by utilizing device attestation and leveraging Secure EnclaveMore proof that Remote Attestation is evil and does not exist to serve the user. reply mngdtt 8 hours agorootparentAs a user, I am very well served by remote attestation when it is used to stop cheaters in videogames or spammers in messaging platforms. reply SpaghettiCthulu 8 hours agorootparentYou are incredibly selfish. Neither of those are more than a mild inconvenience. On the other hand, loss of personal freedom and privacy are major issues with real world consequences. reply rezonant 7 hours agorootparentprevThis assumes that all Beeper Mini users are spam, and that&#x27;s a weird take.More charitably, perhaps you are saying spam will increase over previous levels. From what I understand, Apple does not have any spam prevention technologies in Messages at all, neither for incoming iMessages, nor for SMS messages-- so the only thing keeping your iMessage conversations free of them is the obscurity of the protocol. Perhaps they should just add anti-spam tech like other texting clients have had for years. reply StressedDev 2 hours agorootparentThe same technology Beeper Mini uses to get onto iCloud can also be used by spammers, crooks, etc. to get onto iCloud. You either get both or none. Frankly, as a paying Apple customer, I want them to close this because I hate SPAM. Also, the obsession of iMessage seems very strange to me. reply 43920 5 hours agorootparentprevWhen you get an iMessage from a new contact, there&#x27;s a \"report junk\" option; I&#x27;m assuming Apple does some kind of spam detection with that (ie if a particular Apple ID gets enough reports, it gets blocked). I&#x27;ve never seen any public documentation of it though. reply CaptainFever 16 minutes agorootparentpreviMessage already has a spam problem, even with attestation. reply kmeisthax 4 hours agorootparentprevI know of no messaging platform using remote attestation for antispam - and, as far as those platforms continue to support web registration, they can&#x27;t use remote attestation[0]. Even if they could, it wouldn&#x27;t help. Remote attestation verifies that your client code is running without modification. What you care about with spam is keeping the spammers from registering large numbers of unrelated accounts, which doesn&#x27;t require modifying the client at all.I will give you that remote attestation does help anticheat. However, the current state of anticheat in games is so invasive now that you have to install special kernel drivers, and that kernel has to be on bare metal (no hypervisors allowed). This only happened because a specific genre of fast-twitch first person shooter has a lot of closet cheating going on. But it also gets blindly applied to things like rhythm games that absolutely do not need kernel-level anticheat[1]. So every game gets more invasive because of one hyper-competitive game genre triggering an anticheat arms race.[0] Or at least, for as long as Web Environment Integrity stays dead[1] Altering the client isn&#x27;t even the most common way of cheating rhythm game records. For example, a good chunk of the rules for, say, Pump It Up&#x27;s online leaderboards is \"don&#x27;t have other players play on your A.M.Pass\" and \"don&#x27;t hook up a hand controller onto an online cab\". Neither of which would be stopped by an anticheat system (and yes, PIU being an arcade rhythm game, there&#x27;s shitton of encryption on it). reply JimDabell 3 hours agorootparent> as far as those platforms continue to support web registration, they can&#x27;t use remote attestationThey can; Apple (and others) have implemented Private Access Tokens (PATs) for this.https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;eliminating-captchas-on-iphones-... replyI_Am_Nous 11 hours agoparentprevSeems like a chess move. Apple blocks a small percentage of users instead of all of them, which casts uncertainty on using Beeper Mini at all. It also allows them to A&#x2F;B test various methods of blocking or honeypotting Beeper Mini logins without giving away any big secrets.From Beeper&#x27;s perspective, they now have to figure out why only those logins were blocked and if they need to patch something or not. Apple could be wasting their time and blocked random users out of spite.Time will tell. reply standardUser 8 hours agorootparentUgh, it is beyond depressing to imagine Apple bigwigs sitting around discussing ways to make absolutely certain teens keep getting ostracized until they buy their overpriced product. reply StressedDev 2 hours agorootparentIf someone is ostracizing you because you do not own an iPhone, you probably want to avoid that person. I have never met anyone who would do this and frankly, only an extremely nasty person would do this. I am mean seriously, why ostracized someone because they use a different type of phone? reply vineyardmike 43 minutes agorootparentDo you forget being a teenager? &#x2F;sIt&#x27;s not simply a MeanGirls experience of not being cool enough. Most Americans don&#x27;t use or have 3rd party apps like WhatsApp, so most people will fall-back to SMS, which is objectively a much worse experience. I feel like the adult equivalent is getting group dinner with a friend with severe allergies or dietary restrictions. You care about your friend, and you want to invite them, but the effort to include them is high and sometimes you want to try a restaurant you know they can&#x27;t eat at, so you skip the invite. I&#x27;m a vegetarian, and I know my friends skip me outright in the steakhouse dinners.50% of Americans have an iPhone, and that is even higher for teenagers (almost 90%). That means >50% of people have this superior group functionality built-in (can&#x27;t beat defaults). That means for teenagers, most of your friends will have iMessages, and most will be able to do effortless group chats, and its a statistical dice-roll to see if someone doesn&#x27;t have an iPhone. You become \"that guy\" that causing disruption, and you&#x27;ll 100% be ignored sporadically.Again, the issue isn&#x27;t \"I don&#x27;t wanna see green bubbles\", the issue is \"I don&#x27;t want to bother with a third party app for this conversation\". Since most people don&#x27;t regularly use 3rd party messaging apps, there&#x27;s a coordination issue to be solved picking the app and confirming everyone has it, OR falling back to SMS which is pretty messy. The alternative is to skip one friend and just fill them in later. Sometimes it&#x27;s easier, it&#x27;s not an elitist attempt to ostracize. reply Aeglaecia 1 hour agorootparentprevI admire the surety in children applying logic to their behaviour reply averageRoyalty 8 hours agorootparentprevGiven the majority of their users don&#x27;t care about that, it seems unlikely to be an accurate portrayal of the internal discussions. reply standardUser 8 hours agorootparentYou don&#x27;t think Apple brass is doing everything in their power to convince non-iPhone users to switch to an iPhone? Every excluded teen is another potential customer and to pretend they don&#x27;t know that is beyond naïve. reply frumper 3 hours agorootparentI doubt anyone is shocked that Apple execs want to sell more Apple products. They are paid very well to do that. reply rezonant 7 hours agorootparentprevDo you have data about that or are you assuming you and your friends are representative of the majority of users?It seems like one side of the debate says \"I have experienced this, and the product features seem to encourage this behavior\" and the other side says \"No one really does this, you just have a few insane friends who happen to use iOS\".Feels like gaslighting when you&#x27;ve experienced this sort of behavior yourself, and not even from tweens who aren&#x27;t well adjusted to the world, from your middle aged and up friends and family who are bought into that ecosystem. reply MuffinFlavored 9 hours agorootparentprevMy bigger question is how are any Beeper Mini users getting through (aka how is Beeper Mini&#x27;s backend getting around the fact that... I thought you needed a valid serial # to an Apple device specific to you to log in + use iMessage) reply dylan604 9 hours agorootparentat some point Serial Box will have a list of valid&#x2F;invalid hardware serial numbers. or, someone will crack the code to generate valid codes.oh wait. i drifted off back to the 90s software cracking days. reply MuffinFlavored 6 hours agorootparentif apple checks an online database for \"you authenticated and paired this hardware ID to this apple ID\", is that a \"Beeper Mini\" killer? reply dylan604 4 hours agorootparentisn&#x27;t that essentially what they are doing? that&#x27;s one of the reasons a stolen iDevice is pretty much worthless. reply rezonant 5 hours agorootparentprevWhat about when the hardware is legitimately sold and reset and a new Apple ID starts using it? reply chongli 4 hours agorootparentYou have to de-register the device with Apple when you sell it. Otherwise you retain the ability to remote wipe and brick the device and the buyer has no recourse.After you de-register it the buyer can register it with Apple under their Apple ID. replyMuffinFlavored 9 hours agoparentprev> Migicovsky previously denied to Ars that Beeper used \"fake credentials\"As far as I know (I could be wrong), in order to log in + auth to Apple&#x27;s various protocols that are involved to make iMessage work, you need a valid Apple ID and some sort of valid hardware ID.If you don&#x27;t have either of those, how would you be talking to Apple&#x27;s services?If their POST &#x2F;login requires email + password + valid registered serial # of device sold that isn&#x27;t flagged stolen and not shared across 100 accounts... how does Beeper Mini expect to work? reply saintfire 5 hours agorootparentAFAIK, and I could be wrong, beeper mini registers a new HWID with apple for each phone. Which is why they thought it was unpatchable, at first, as they would need to determine which phone is in fact an iPhone. reply nneonneo 5 hours agorootparentThere&#x27;s much more to the validation protocol than just HWID&#x2F;serial. See https:&#x2F;&#x2F;github.com&#x2F;JJTech0130&#x2F;pypush&#x2F;blob&#x2F;main&#x2F;emulated&#x2F;data... for a list of the data that is pulled from the platform and used for validation. I would assume that Beeper registrations either use data from a pool of real devices, or made-up data that Apple might \"permit\" (because hackintoshes) but can definitely detect and block at any time. reply jimmyk2 11 hours agoparentprevMight be intentional. Unreliable service is probably worse as a user. Never know if the system is down or if it’s just you. Plus probably harder for beeper to work out how&#x2F;why they are getting blocked. reply user_7832 12 hours agoparentprev> Not commenting about the ethics of all this, just wondering why technically Apple can only block ~5% of Beeper Mini users instead of all of them? Could this potentially be tied to the use of an email id as the iMessage handle?I wonder if it might also have anything to do with govt action. I believe a US elected rep recently tweeted in favour of Beeper. Apple cares much more about PR than they&#x27;ll admit, and server costs for them are negligible. reply nozzlegear 10 hours agorootparentThe rep was Senator Elizabeth Warren, who was once pretty popular during the Obama years when she helped create the CFPB. She sadly doesn’t hold much sway (e: in the senate) anymore. reply hackernewds 6 hours agoparentprevblock 5% for experimentation data to observe whether it&#x27;s net viable for their metrics to allow cross-pollinating the users finally reply shreyansh_k 12 hours agoprevThis seems to be Beeper&#x27;s insincere attempt at dressing up their grievances to appear as if they were in advantage of the [Apple&#x27;s] customer.Apple has made it clear that they won&#x27;t bring iMessage to Android. People who choose to invest in Android phones know they won&#x27;t get access to iMessage.It seems a bit entitled behaviour for someone to feel like Apple should be forced to bring iMessage to Android for any reason besides Apple&#x27;s own choice, and that they know better than Apple on how to run Apple&#x27;s business. reply dkjaudyeqooe 8 hours agoparent> It seems a bit entitled behaviourWhat&#x27;s entitled behavior is big tech thinking it can dictate how and where we use technology and services. Why should we be roped into serving Apple&#x27;s interests and ignoring own own? Beeper can pursue their own interests while benefiting users, that&#x27;s hardly a negative.I&#x27;m sure if Ford dictated how and where you drove your car you&#x27;d be outraged, but we should kowtow to the likes of Apple? reply DeIlliad 7 hours agorootparent> I&#x27;m sure if Ford dictated how and where you drove your car you&#x27;d be outraged, but we should kowtow to the likes of Apple?I just wouldn&#x27;t buy a Ford. reply rezonant 5 hours agorootparentWhat if Ford made it so that their vehicles drive worse when non-Ford vehicles were on the road, and then when confronted about it told all of their customers that they should convince their neighbors to buy a Ford to solve the problem? reply nneonneo 5 hours agorootparent> What if Ford made it so that their vehicles drive worseThen you really wouldn&#x27;t buy a Ford. Problem solved? reply dataangel 12 hours agoparentprevIt&#x27;s not entitled, it&#x27;s pushing back against a gigantic company exploiting network effects to create user lock-in, because that&#x27;s easier than competing on the merit of your products. reply joshmanders 11 hours agorootparentNow apply this logic to any other company product, maybe even your own or the company you work for and tell me if this sounds as ridiculous to you as it does me. reply FirmwareBurner 10 hours agorootparentMost companies don&#x27;t have network effect with lock-in, and especially those we depend to live our everyday lives: I can send money to my buddy who uses a different bank, I can send emails to gmail users from my yahoo account , I can also refuel my car at any station and not be tied to one brand. Imagine being locked in to any of those.There are of course a lot of companies with lock-in out there, but none with such big network effect deeply intertwined with peoples&#x27; communication and personal lives (in the US). reply verwalt 9 hours agorootparentThe thing is: it is not locking in. At the moment through SMS and it will support RCS next year.And RCS will solve it all. If group chats and high quality media work, people will only complain about the one thing they hate most:The bubbles will still be green.But I am from Europe, we don&#x27;t care around here and I don&#x27;t like iMessage that much. Compared to Telegram&#x2F;Whatsapp, it&#x27;s slow at loading old messages, has sync issues and only gut swipe to reply this year. reply rezonant 5 hours agorootparent1. You cannot opt to use SMS on Apple Messages when talking to someone. If they are registered on iMessage, you are forced to use it. This happens transparently, automatically, and for the most part, silently. This helps to create the illusion that Apple devices are just better at texting, and anything else is old and shitty. When in reality, Apple itself only supports their own proprietary messaging system, and an ancient texting protocol that is bad.2. I hope you are right that RCS will solve everything once Apple implements it, but I don&#x27;t have confidence it will. The biggest problem is how they handle group chats: If iMessage group chats cannot seamlessly convert into RCS group messages (without duplication or splitting) then it will solve it well enough. This is unlikely to happen unfortunately reply Longhanks 1 hour agorootparentYou absolutely can disable iMessage for yourself and text anyone using regular texts (SMS) only, you are never forced into iMessage. You can deregister your phone number on Apple&#x27;s website without any Apple device.Also, where is the \"illusion\" about \"being better at texting\"? Apple is literally coloring the bubbles differently to tell you that _iMessage is not the same_. How much more explicit can it get? reply jmye 8 hours agorootparentprev?You can send a text to any user, regardless of service. I don’t understand your analogy. reply orangecat 8 hours agorootparentprevNow apply this logic to any other company productOk...third parties should be able to sell ink that works in your HP printer, or coffee pods that work in your Keurig, or tires that work on your Ford.tell me if this sounds as ridiculous to you as it does meIt sounds not ridiculous at all. The only difference with Beeper is that there is some marginal cost to handling messages, but we all know that&#x27;s not Apple&#x27;s real problem with it. reply inferiorhuman 4 hours agorootparentnext [–]…third parties should be able to sellTo take that analogy a step further: third parties can sell messaging apps for iOS. reply modeless 10 hours agorootparentprevWe&#x27;re talking here about literally the most valuable public company in the world and a product (iPhone) used on average dozens of times and several hours daily by nearly 50% of the US population. I&#x27;m generally a free market kind of guy but even I admit that at this scale it is OK to apply different standards. reply makeitdouble 7 hours agorootparentprevAny other company using network effect to force lockin should have the same logic applied, yes. That feels like a pretty sane philosophy to me.To turn it around, which company do you see this not applying ? What services with a strong position do you see justify to abuse it to lock users ? reply refulgentis 5 hours agorootparentprevI&#x27;m curious, before (and even while!) they do their assigned thought exercises, would you mind explaining to the rest of us how ridiculous it is? reply rewgs 10 hours agorootparentprevSpeaking for myself, I think iMessage is a good product, and it&#x27;s one of the (many) reasons I use Apple devices as daily drivers.You make it sound like people only use iMessage because of the network effect&#x2F;lock-in, and clearly aren&#x27;t considering that perhaps said network effect exists at least in part because iMessage is just...good. reply skeaker 9 hours agorootparentPeople absolutely do buy Apple devices purely because of the lock-in. Many people in my own family have. Obviously there are others that just prefer iMessage on its own quality, that goes without saying (which is why GP didn&#x27;t mention it). reply standardUser 9 hours agorootparentprevIf the product is so great why doesn&#x27;t Apple make it available on more platforms? The answer is because it&#x27;s not a product, it&#x27;s a marketing tool. And a very successful one based on the widespread bullying it has caused. reply merrywhether 3 hours agorootparentIf Zelda is so great why doesn’t Nintendo make it available on more platforms? Being able to run Nintendo games is a prime feature of the Switch product, and being able to run Apple software and services is a prime feature of an iPhone product. By your definition anything useful about an item is simply a marketing tool?It’d be a very different world if nothing was allowed to have unique access to anything. It might even be better, but it would be a long way from this current version of capitalism. reply potatototoo99 1 hour agorootparentThe answer is the same: because their platform is not very good. So if buyers had the chance they&#x27;d instead buy Zelda games for the PC for instance.It would be a real problem if Nintendo had a monopoly on good games on the US. reply frumper 3 hours agorootparentprevNot many Apple products get made for non Apple devices. reply wrboyce 8 hours agorootparentprevGive me a fucking break, “bullying” for Christ’s sake!They invented a nice thing, made it available on their hardware, and now people like yourself who refuse to buy their hardware for whatever reason are salty you can’t play on the platform so result to juvenile arguments like “I’m being bullied” to try and get your own way. It’s fucking ridiculous and you need to grow up.EDIT: that should be “resort to”. reply shreyansh_k 12 hours agorootparentprevThat doesn&#x27;t seem right. People can also push back against Apple by voting with their wallets.Apple has been able to create their ecosystem by exerting control over it. If someone doesn&#x27;t like it, they can start their own business giving people what people want. Now, that&#x27;s another way of pushing against any gigantic company or Apple. reply rezonant 5 hours agoparentprevPeople use Beeper Mini because the Apple users they text with want them to. There&#x27;s zero reason for two Android users to talk over iMessage using Beeper Mini. RCS provides nearly all the same benefits. But when the social pressure is for you to buy an iPhone that you don&#x27;t want, it&#x27;s sure tempting to just install an app so that Apple friends can stop complaining, and stop cutting you out of group chats.If no one used iMessage, the value of Beeper Mini is zero. The value is smoothing the interaction with people who assume everyone is using an iPhone, and treat it as an annoyance when they don&#x27;t. So effectively saying \"just buy an iPhone to use iMessage\" means \"just buy an iPhone so you can talk to your iPhone friends\". reply user_7832 12 hours agoparentprev> It seems a bit entitled behaviour for someone to feel like Apple should be forced to bring iMessage to Android for any reason besides Apple&#x27;s own choice, and that they know better than Apple on how to run Apple&#x27;s business.It is simultaneously possible for it to appear entitled, while also recognizing how extremely restrictive, anti consumer and anti competition Apple is. And in this fight of a trillion dollar company vs something a little bigger than a startup, it&#x27;s fun to root for the small guys who are hitting back at that restrictedness, sometimes even successfully. reply shreyansh_k 12 hours agorootparentIt can be agreeable to an extend that Apple seem like an extremely restrictive, anti consumer and anti competition. But, the other side (Android), which has been positioned open, pro-consumer and pro-competition doesn&#x27;t seem to be true to its roots or any better anyways. Android is mostly dominated by Samsung and Samsung is heavily pushing their own ecosystem of apps, just like every other company, and most brands want to lock in their users too.It&#x27;s a sad state of affairs. reply rezonant 4 hours agorootparentYou can push your own ecosystem of apps and still allow your customers to replace them. I&#x27;m not aware of a Samsung app or service that cannot be swapped out. And I struggle even more to find a Samsung component that is designed to create network effect lock in like iMessage is. And Apple Messages can&#x27;t be swapped out, it is the only texting app on iOS. You can install Signal or Whatsapp or whatever, but you can only talk to other people who have those apps installed. You cannot use them to talk to any phone number like a texting app can. reply fh9302 3 hours agorootparentRCS can&#x27;t be swapped out, Google hasn&#x27;t opened the API so you are forced to use Google or Samsung messages app. reply ck425 10 hours agoparentprevOut of curiosity why does this matter? Do iPhone users not use WhatsApp or Messenger? Is this a USA specific thing? As a UK Android user I&#x27;ve never used iMessage and don&#x27;t see any reason why I&#x27;d need or want to. reply infotainment 10 hours agorootparentUSA specific — Americans pretty much universally use SMS messages, which ends up being iMessage for Apple devices. reply ck425 10 hours agorootparentHuh. How does that work with multimedia? Do US carriers not differentiate between SMS and MMS? reply infotainment 10 hours agorootparentThey don’t, it “just works” (to the degree MMS can work at all) — which means iMessage is perfectly positioned. reply ck425 9 hours agorootparentAh so do \"free texts\" in the USA include MMS? That might explain the continued usage. In the UK MMS often aren&#x27;t included, if anything they&#x27;re charged stupidly high, so you&#x27;d never send a pic via text message. reply wrboyce 8 hours agorootparentTo give you another perspective, I’m UK based too and use iMessage daily to stay in touch with my family and close friends. Every now and then I’ll have bad signal (or maybe the recipient does? I’m not entirely sure…) and whatever I’m sending will be sent as an MMS. It happens so rarely it doesn’t really register, and I’d wager it costs me less than a quid a year (on EE, but I assume most MMOs have similar MMS pricing). reply rezonant 4 hours agorootparentprevOn most plans in the US, unlimited texting is the only option, and there is no differentiation between SMS and MMS. It&#x27;s all free with your plan. reply rimunroe 8 hours agorootparentprevyes reply jacobgkau 9 hours agorootparentprevCarriers typically charge the actual media portion of MMS (e.g. the photo or video) as data. reply rezonant 4 hours agorootparentI&#x27;m not sure how universal that is, but it doesn&#x27;t really matter when data plans are almost always unlimited. After all, iMessage also goes through cellular data. reply prmoustache 3 hours agorootparentprevI guess egoism is so entrenched in the USA that choosing a platform that all your friends and relatives can use to communicate to is just too much for them. reply quadrifoliate 5 hours agoparentprevThere are reports of kids being bullied (this [1] is an example from a Tell HN post), but I have anecdotally heard similar cases because they were \"different\" for having a \"non-blue bubble\" phone.When there is broad agreement that apps like Tiktok and Instagram are toxic for teenagers, I have no idea why this aspect of Apple&#x27;s lock-in hasn&#x27;t received as much attention. It&#x27;s way more serious than just some users wanting blue bubbles when your applications are common enough that they can cause social isolation and bullying in schools.----------------------------------------[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35014499 reply skeaker 9 hours agoparentprevHow is being able to send iMessages to your Android-user friends from your iPhone not to your advantage as an Apple customer? reply gunapologist99 8 hours agorootparentIt&#x27;s not to Apple&#x27;s advantage, though. reply skeaker 7 hours agorootparentThat does not address my question. reply tgma 7 hours agorootparentprevYou’d be surprised how many people in the US have purely iPhone social circles. They won’t care.The graph is almost disjoint. Outside tech nerds, android == poor. reply skeaker 4 hours agorootparentYou&#x27;re still advantaged should you ever make friends with an Android user in the future, or at the very least you&#x27;re not disadvantaged since you would not have had any difference made to you. reply crooked-v 10 hours agoparentprev> to appear as if they were in advantage of the [Apple&#x27;s] customerThey are. This was perfectly normal and expected functionality once upon a time, back in the days when Adium&#x2F;Pidgin were still useful. reply paxys 4 hours agoparentprevEntitled behavior is Apple dictating what code I can run on my non-Apple servers and devices. They can keep blocking it, and people will keep finding workarounds. That&#x27;s the nature of DRM. It&#x27;s not a battle they can win. reply vlozko 3 hours agorootparentThey don’t dictate that at all. What they do dictate is what devices are able to connect to their network and utilize their push notification service. reply drexlspivey 8 hours agoparentprevIs Beeper’s whole purpose to satisfy people’s vanity to have blue bubbles or what’s wrong with plain ol’ SMS? reply hn_acker 7 hours agorootparentSMS is not end-to-end encrypted (E2EE). The purpose of Beeper Mini (not to be confused with Beeper Cloud, which is not E2EE) is to allow Android users to send encrypted texts to Apple iMessage users the same way iMessage users can send E2EE texts to each other.An Android user and an iOS user could both use a third-party app like Signal. But Beeper Mini&#x27;s main feature is maintaining encryption for chats between Android users and iMessage users, saving the Android users the need to convince iMessage users to switch to a different messaging app. 1. The benefit of E2EE applies to the users on both ends, including iMessage users. 2. With Beeper Mini instead of an E2EE-dedicated multi-platform app like Signal, only one side of the communication pair needs to install something.Beeper Mini Apple doesn&#x27;t want to make an iMessage implementation for Android. reply dijit 8 hours agorootparentprevaside from the magic features not working; SMS costs money.On some mobile contracts it&#x27;s pretty close to free these days, but international SMS&#x27;s are certainly not. - not sure it&#x27;s a common use-case but it definitely is a common use-case for me.Ironically, iMessage not being available on Android causes the other messenger apps to be more appealing. Whatsapp&#x2F;Facebook Messenger. -- because they can be more ubiquitous across friends. reply throwaway290 8 hours agoparentprev> Apple has made it clear that they won&#x27;t bring iMessage to Android.Apple might disagree. https:&#x2F;&#x2F;9to5mac.com&#x2F;2023&#x2F;11&#x2F;16&#x2F;apple-rcs-coming-to-iphone&#x2F; reply MissTake 8 hours agorootparentThat’s not iMessage.RCS is a replacement for SMS. reply nocoiner 12 hours agoprevBeeper CEO, from the article: \"If there&#x27;s enough pressure on Apple, they will have to quit messing with us.\" \"Us,\" he clarified, meant both Apple&#x27;s customers using iMessage and Android users trying to chat securely with iPhone friends.\"That&#x27;s who they&#x27;re penalizing,\" he wrote. \"It&#x27;s not a Beeper vs. Apple fight, it&#x27;s Apple versus customers.\"Not really sure how this is anti-customer. Apple customers have ample access to iMessage, after all. And as has been noted many, many times, there are many means of accessing secure cross-platform messaging. reply aesh2Xa1 12 hours agoparentApple has customers who want to use iMessage. Those customers are not able to use iMessage to securely communicate with Android users.That&#x27;s pretty clear. If Apple&#x27;s customers want to be able to use iMessage for such communications then (1) the Android user buys an Apple product, (2) a third party makes a cross-platform iMessage client, or (3) Apple open iMessage to other platforms.Apple is blocking on (2) and (3). reply joshmanders 12 hours agorootparent> Apple has customers who want to use iMessage. Those customers are not able to use iMessage to securely communicate with Android users.I as a Signal user want to communicate securely with Facebook Messenger users. Therefore Facebook must allow me to reverse engineer their services and make unauthorized systems to allow me to create a service to send messages to Messenger users from Signal. reply kuschku 50 minutes agorootparent> I as a Signal user want to communicate securely with Facebook Messenger users. Therefore Facebook must allow me to reverse engineer their services and make unauthorized systems to allow me to create a service to send messages to Messenger users from Signal.That&#x27;s precisely what Signal started out as :)Before Signal existed, it was common to use Pidgin or Adium with OTR to send encrypted messages through SMS, Google Chat or Facebook Messenger.TextSecure – the original name of Signal – was created to improve upon OTR by allowing it to better handle situations where one user was offline while the other tried to send messages. Originally it only supported sending messages via SMS, not via their own servers. reply user_7832 12 hours agorootparentprevI think calls for interoperable communication platforms is a good thing! reply joshmanders 12 hours agorootparentSure I do too, but I&#x27;m against this ridiculous notion that Apple MUST allow others to integrate and use their services for free.My point is nobody has any problem with the myriad of other messaging platforms that are completely closed, but all of the sudden iMessage is the bad guy because you don&#x27;t want to have a green bubble with iPhone users? reply kuschku 47 minutes agorootparent> My point is nobody has any problem with the myriad of other messaging platforms that are completely closedThat&#x27;s entirely wrong. A lot of people are angry with every closed messaging platform, which is why people are still maintaining the matrix bridges to Discord, WhatsApp, Facebook Messenger and Signal, all of which are still a cat-and-mouse game, violating the ToS of those services.Those bridges are actually what Beeper Cloud, their primary product, later was also based on.And these bridges originally are based on the libpurple backends for Slack, Teams, Facebook Messenger, WhatsApp, Signal, AIM, ICQ, Google Chat, Skype, etc, all of which were created through reverse engineering.So, yeah, fuck those closed services, messages want to be free, and enough people will care that we&#x27;ll break all of those services. reply 8note 8 hours agorootparentprevA proper thing is for the EU to require apple to implement an interoperabile message api and force that to be used as the default, rather than being able to use a non-interoperable messaging protocol as the default reply maccard 1 hour agorootparentLike SMS, which is the default right now unless both users are iPhone users? reply user_7832 12 hours agorootparentprevI don&#x27;t think anyone&#x27;s saying apple must allow them (though such people admittedly exist on the internet, in retrospect). I think its more of disappointment at the speed at which apple is \"patching\" this. reply badwolf 9 hours agorootparentIt seems the CEO of Beeper is literally doing that. reply joshmanders 12 hours agorootparentprevAs an Apple user, I&#x27;m glad they&#x27;re patching this fast. It&#x27;s a security issue and privacy one. Sure Beeper may be using it in a non-nefarious way, that doesn&#x27;t mean bad actors aren&#x27;t gonna use this to spam the heck out of iMessage users. reply I_Am_Nous 11 hours agorootparentYou know, I have been getting constant scam texts from \"Amazon\" for a $289 vacuum that I&#x27;m supposed to call a number to cancel the order and it&#x27;s coming from an email address, or a phone number. Never really got them before in that way. reply striking 10 hours agorootparentprevI&#x27;ve been getting iMessage spam long before Beeper existed. reply sbuk 11 hours agorootparentprevAnd they exist. A lot of them, actually. reply skeaker 9 hours agorootparentprevSounds good! I would love to be able to do that. reply Terretta 7 hours agorootparentprevFirst, it&#x27;s not just security.It&#x27;s also, does this message cost money or not.Blue sky, text freely. Green could cost money by hitting your SMS message bill.This color difference matters even if you don&#x27;t care about security at all.. . .Second:> That&#x27;s pretty clear. If Apple&#x27;s customers want to be able to use iMessage for such communications then (1) the Android user buys an Apple product, (2) a third party makes a cross-platform iMessage client, or (3) Apple open iMessage to other platforms. Apple is blocking on (2) and (3).Option 4, run any other app that&#x27;s on both platforms, which is what most of the world does anyway, including any iPhone user I know in the US who chats regularly with anyone International. For EU it&#x27;s WhatsApp, China WeChat, and in USA it&#x27;s privacy minded Signal, PTA parents Facebook Messenger, etc.Given no cross platform is blocked, you&#x27;ll see many iPhones do not have Messages in the launch bar, but one of those others.This seems ... fine! reply out-of-ideas 12 hours agorootparentprevdont forget to incude that a lot of folks do not realize that iMessage uses a different protocal for texting than simply texting; folks dont really always know or care about the program, they associate \"icon\" or \"new version of icon\" as texting, and use it to simply \"send a text message\"; not everybody is able to comprehend the difference between a simple \"text message\" and \"imessage\" vs other apps, ectthe advertisers&#x2F;carriers also dont use \"send an imessage\" - its simply \"unlimited texting\" that gets stuck in folks brains (albeit i have not watched tv ads in some time, maybe its been updated to exact app names lol) reply lazzlazzlazz 12 hours agorootparentprev> Those customers are not able to use iMessage to securely communicate with Android users.Well, they can&#x27;t now as a result of blocking Beeper. But they could — using Beeper.It&#x27;s one thing to argue that you won&#x27;t open the platform because it&#x27;s additional work to support that. But that argument degrades when they&#x27;re willing to do the work to shut out unexpected uses of the platform.If they want to ensure safety, they should do the extra work of making it possible to interoperate with iMessage safely. But safety isn&#x27;t the priority and everyone knows it. reply sbuk 11 hours agorootparent> \"But that argument degrades when they&#x27;re willing to do the work to shut out unexpected uses of the platform.\"Or, as many in software engineering call it, fixing potential security holes, which is well worth any businesses time. reply I_Am_Nous 11 hours agorootparentThat&#x27;s how I view it as well, this is a security issue. Beeper is gaining unauthorized access to a service and Apple doesn&#x27;t like it. The fact that they are not technically breaking anything is kind of like going into a business after hours because the door wasn&#x27;t locked. You still aren&#x27;t authorized to be there. reply turquoisevar 11 hours agorootparentThe better analogy is using a fake ID to get the locksmith to give you a copy of the house key.The house is Apple’s servers The fake ID is spoofed device serials and UUIDs The copy of the house key is the authentication blobNobody would blink twice if a prosecutor threw the book at someone like that. Still, somehow, I’m sure many here would complain if the DOJ would prosecute Beeper for violating the CFAA by committing computer trespass or if Apple would sue them for violating the clause prohibiting reverse engineering in the OS license. reply noahtallen 10 hours agorootparentThe analogy is off in a meaningful way: you&#x27;re using a \"fake ID\" to get the locksmith to give you a copy of your own house key because the locksmith won&#x27;t accept your real ID. No prosecutor is going to throw the book at someone trying to access their own house.Apple claims I&#x27;m in control of my messages. They&#x27;re on my devices. Apple refuses access to Android. Why can&#x27;t I use technology to make my messages work on Android? They&#x27;re my messages!You might respond that this impacts someone else. For example, me using Beeper means that anyone messaging me is impacted. My counterpoint is that the user is always the weakest link. I can share messages sent to me with anyone I like, and that&#x27;s legally fine to do. So why can&#x27;t they be shared with a different software service that I trust? (People use all kinds of 3rd party email clients which could be untrustworthy, and yet we still mostly trust email.) reply turquoisevar 4 hours agorootparentWe’re talking about accessing Apple’s servers, how would that be your own house? reply nvy 10 hours agorootparentprevPretty sure they&#x27;re buying actual Mac minis and using those device IDs. If you have evidence to the contrary I would be very interested in seeing it. reply turquoisevar 4 hours agorootparentPretty sure you’re mixing up Beeper Cloud with Beeper Mini.Beeper Mini is based on pypush, which they’ve bought, and is clearly using spoofed data in the data.plist[0].I’ve searched, but I’ve found no mention of them purchasing Mac Minis en masse to support the $2&#x2F;mo Beeper Mini customer’s texting habits.Besides, it wouldn’t make sense anyway because they used to tout you didn’t need an Apple ID and instead could use your phone number, and non-iPhone IDs don’t allow for iMessage activation on phone numbers, only email addresses.0: https:&#x2F;&#x2F;github.com&#x2F;JJTech0130&#x2F;pypush&#x2F;blob&#x2F;main&#x2F;emulated&#x2F;data... replyJumpCrisscross 12 hours agorootparentprev> Apple has customers who want to use iMessage. Those customers are not able to use iMessage to securely communicate with Android usersOf course they do. There&#x27;s Signal and Telegram and WhatsApp. My principal benefit of iMessage is its relative lack of spam. (And when I do get spam, it gets stomped out fast.)Apple Music blitzkrieging Spotify is bullshit. The lock on subscription payments is bullshit. I happen to think the App Store is fine, but I&#x27;ll concede that there&#x27;s a real debate to be had there. But this isn&#x27;t a material issue. reply SpaghettiCthulu 8 hours agorootparentI am at a loss on this \"spam\" argument. I haven&#x27;t received a single spam message on either SMS or Signal in the at least last year. reply maxcoder4 7 hours agorootparentSame experience with signal, but SMS phishing is a very large issue, at least in my country. Maybe your phone number was just never leaked and you&#x27;re not present in the spammers databases? reply lotsofpulp 12 hours agorootparentprev>Apple Music blitzkrieging Spotify is bullshit.What is this referencing?>The lock on subscription payments is bullshit.I pay for app subscriptions outside of the App store, since it is usually cheaper. reply JumpCrisscross 12 hours agorootparent> What is this referencingSpotify launched on iOS. Apple saw them competing with iTunes and basically stole their idea to compete with them. That, alone, would be okay. But Apple Music is privileged within iOS and Apple&#x27;s marketing in a way Spotify cannot be. reply threeseed 10 hours agorootparentNone of this is true.a) Spotify didn&#x27;t invent music streaming. There were many services e.g. Pandora that were doing in the years before. It was a pretty obvious idea once devices had faster bandwidth.b) Apple didn&#x27;t steal their idea. They acquired Beats who had launched a similar service soon after Spotify.c) Apple Music isn&#x27;t privileged. It comes pre-installed but otherwise you can delete the app and use Spotify, Youtube Music etc. reply rez9x 7 hours agorootparentApple Music is privileged when it comes to Siri. I currently have both a Spotify and Apple Music subscription, and one of the main reasons I prefer Apple Music, aside from shuffle not playing the same 20 songs in a 2000 song playlist, is the great hands free functionality. I can add songs to playlist, play a song next instead of adding it to the end of the queue (which is more of Spotify STILL not having deque support), and I know there are other things I&#x27;ve run into Spotify can&#x27;t do on Siri, but I&#x27;m blanking at the moment. reply lotsofpulp 12 hours agorootparentprevOh, I would apply that to any number of things Apple integrated, for example dropbox&#x2F;icloud drive.Google does the same thing. Unfortunately, with the near zero marginal cost of software, I do not see any way around vertical integration unless the law started arbitrarily segregating businesses.Also, Apple Music came out 5 years after Spotify, so it had a pretty healthy lead. But regardless, any non Apple vendor competing with Apple&#x27;s bundled products is going to face an uphill battle. reply thowawatp302 1 hour agorootparentiDisk came out in 2000 and functioned like iCloud Drive has, with local caching, since 2003It predates Dropbox by almost half a decade replyunshavedyak 12 hours agoparentprev> Apple customers have ample access to iMessage, after all. And as has been noted many, many times, there are many means of accessing secure cross-platform messaging.In fairness, i am a 5+ device apple customer and i can&#x27;t use it on my two primary desktops. I use Beeper to write texts on my primary computer.Apple feels anti-me. I pay for products and unless i&#x27;m 100% into the ecosystem they fight me. Sure, they don&#x27;t owe me an app on every platform i like - but they&#x27;re also fighting me developing my own, too. reply unsignedint 9 hours agorootparentI discontinued using Apple products primarily due to their ecosystem-centric design philosophy. Many features appear tailored exclusively for users fully committed to Apple&#x27;s ecosystem. While it&#x27;s common for platforms to have exclusive features, Apple&#x27;s approach feels more pronounced. In contrast, Google, for instance, offers Android-specific features, yet their services function well across various platforms, including mobile and desktop, and this compatibility extends to Apple devices as well. reply josephg 12 hours agorootparentprevYeah. I’ve got all sorts of Apple stuff in the house. One apple ecosystem piece I adore is being able to copy paste between my Apple devices. I’d love it if that worked from my Linux workstation too. C’mon, Apple. reply BenjiWiebe 4 hours agorootparentThanks to kde connect, I&#x27;ve been really enjoying shared clipboard between Windows, Linux, and my Android phone. reply advael 10 hours agorootparentprevOne workaround is something like barrier&#x2F;synergy, if that still works on macos. Doesn&#x27;t cover phones, but it&#x27;s something reply fragmede 11 hours agorootparentprevkdeconnect has an iOS app reply rahimnathwani 10 hours agoparentprev\"Apple customers have ample access to iMessage\"I have an iPad Pro, a MBP, a MBA, and an iPhone X. I can send and receive iMessages on those devices.But I cannot access iMessage on the devices I use most often:- my main computer (a desktop running Ubuntu)- my main phone (a Google Pixel)I don&#x27;t feel I have &#x27;ample access to iMessage&#x27;. reply wishfish 12 hours agoparentprevSpeaking as an Apple customer, there are many times I wish iMessage was more open. My partner and some friends prefer Android. Would be great to use Messages with them. Plus, I&#x27;d love to have Messages on whatever desktop I&#x27;m using. Sure, I spend the majority of my time in MacOS. But, I&#x27;ve got Windows, Linux, and Chromebooks here in the house. Would be so much more convenient to have Messages on all platforms. reply lotsofpulp 12 hours agorootparentWhat is stopping you from using Whatsapp&#x2F;Signal? reply nani8ot 10 hours agorootparentAs an Android user I&#x27;d rather use iMessage than WhatsApp, if I have to choose between proprietary messaging apps. Meta isn&#x27;t a company I want to rely upon, but I&#x27;m forced to because of almost everyone using WhatsApp where I live. reply wishfish 10 hours agorootparentprevAbsolutely nothing. I already use alternative services for Android contacts. I just like iMessage and wish it was available on non-Apple hardware. Would be immensely more convenient. reply edgineer 12 hours agoparentprevI&#x27;m an apple customer but I don&#x27;t agree that I have ample access to iMessage. I use a MacBook and have an old iPhone lying around, but I use my android phone for daily messaging, including my family who use iPhones. reply otachack 12 hours agoparentprevAgreed on there being access to other encrypted messaging apps. But that doesn&#x27;t account for \"app-fatigue\" where people are fed up with getting yet another application (think Zoom, Meet, Slack, Signal, Discord, etc) and thus you have friction on getting people to use a new app.For example: I&#x27;ve tried to get friends and family on Telegram and only succeeded in getting a fraction of them. I also feel that they&#x27;re using it just to appease me and don&#x27;t use it with others.SMS&#x2F;MMS&#x2F;RCS&#x2F;iMessage have default apps on phones and people tend to not seek replacement or simply can&#x27;t (iMessage) This is what Beeper is trying to fix by introducing an app just for Android users to connect with iMessage users, thus challenging the Apple walled garden.I haven&#x27;t jumped on the Beeper wagon because I don&#x27;t know the consequences of this tug-o-war. Would I lose secure messages if the iMessage rug gets pulled out while my number is registered? It seems I would unless I deregister my number. It just seems too volatile to me but I&#x27;m a fan of Pebble and wish the Beeper team+founders the best. I&#x27;ll be keeping my eye on this. reply gmm1990 12 hours agoparentprevAny chat I&#x27;m in that has at least one Android user is annoying. I&#x27;d prefer if the imessage features would be possible for those chats too. reply JumpCrisscross 12 hours agorootparent> Any chat I&#x27;m in that has at least one Android user is annoyingIs this still the case since they stopped sending the \"gmm1990 hearted XYZ\" messages, and instead integrated the tapbacks? I really don&#x27;t see a difference unless I&#x27;m in the air (and so cannot connect to the cellular network). reply i5-2520M 12 hours agoparentprevI have seen comments from iMessage users, and it is also logical to assume that they would prefer if there was a way to chat with their Android friends via iMessage while having most of the nice features. An Android iMessage app would only be bad for Apple, since they will no longer be able retain and convert users based on them being or not being able to comfortably message their friends. reply joshmanders 11 hours agorootparentAs an Apple customer here&#x27;s my thoughts on this:1. Yes I would love if iMessage went Android and all my Android friends were able to react, thread convos, and all the other perks that iMessage allows, BUT the color of the message doesn&#x27;t matter to me. I&#x27;m ok with the green bubbles too.2. If iMessages was available on Android, I and every iPhone user I know wouldn&#x27;t be like \"finally, I can leave Apple!\" iMessage is a perk not the main draw. The wider ecosystem and walled garden that non-Apple users apparently hate so much is not-surprisingly lovely to those of us all in on it. reply prmoustache 3 hours agoparentprev> Not really sure how this is anti-customer. Apple customers have ample access to iMessage, after all. And as has been noted many, many times, there are many means of accessing secure cross-platform messaging.The point of a secure instant messaging protocol&#x2F;app is to be able to communicate securely with people. If the company providing said app make it on purpose difficult for its customers to communicate with people, it is penalizing them.We are not talking technical difficulties, we are talking unwillingness to let a secure channel stay open and making so that communication with people is less secure. reply tiltowait 12 hours agoparentprevIt&#x27;s definitely a weird fight, especially when you consider Beeper wants to make money off of Apple&#x27;s service without any compensation going Apple&#x27;s way. It&#x27;s hard for me to tell if the Beeper CEO is delusional or simply doing PR speak. reply advael 10 hours agorootparent\"Interoperability is picking apple&#x27;s pocket by allowing people cross-platform secure communication in a way that&#x27;s usable to them\" is a take that is bending over backwards to lick corporate boot. I get that part of apple&#x27;s marketing is extreme brand loyalty, but your communications with non-ios users being less secure harms you as an ios user too, and the loss to apple is one of control over its customers&#x27; behavior more than anything else reply echelon 12 hours agoparentprev> Apple customers have ample access to iMessage, after all.It&#x27;s not really fair that Apple gets to build snares around owning the most successful and important platform in modern civilization. This isn&#x27;t an automobile where there are 50 alternative manufacturers, this is the \"everything\" device that handles your bills, employment, connections to loved ones, etc. And there&#x27;s only one other choice of vendor in the United States.Apple shouldn&#x27;t be able to exercise their position (nor should Google) to own every aspect of human life built atop this connection. Mobile internet shouldn&#x27;t belong to anyone. It shouldn&#x27;t be taxed by anyone. And yet here we are.One of the first rulings from antitrust I would expect is non-proprietary messaging protocols. reply qeternity 12 hours agorootparent> This isn&#x27;t an automobile where there are 50 alternative manufacturersNo, it’s exactly this. There are a ton of manufacturers all providing this product. It just so happens that Apple’s is one of the best, if not the best. And as has been mentioned, there are tons of secure messaging alternatives. It’s just that people prefer iMessage. This is not illegal or anticompetitive.Imagine if someone claimed they should be let into an airport lounge of an airline they aren’t flying on, simply because their friends are there. reply echelon 9 hours agorootparentApple or Google-controlled Android. Pick one.Both tax everything in the world now. They&#x27;re even becoming official government document providers. They&#x27;re sinking their claws into all aspects of life and taxing and controlling them. reply prmoustache 3 hours agorootparentprev> It just so happens that Apple’s is one of the best, if not the best.I don&#x27;t think you can explain it that way. Network effect is a thing and instant messaging preferences are very regional.Also:- With same products and quality Apple probably would probably have lower share in the USA if it was a chinese or iranian company.- Regardless of its qualities the iphone has long been and is still a social class status symbol.- The conversation on hn is skewed because iphones have slightly higher than 50% of market share in the USA, not 90% yet commenters on hn mention that their social circle is almost entirely made of iMessage users, which means the average hn commenter is of a much higher income and social class than the average US smartphone user. Using hn is mostly like entering a fancy suburb and only discuss with a limited subset of the population. reply shreyansh_k 11 hours agorootparentprev> It&#x27;s not really fair that Apple gets to build snares around owning the most successful and important platform in modern civilization.On the flip side, Apple spent a lot of effort into developing their ecosystem. It doesn&#x27;t seem fair to Apple that other companies get to piggyback on Apple&#x27;s investments without Apple&#x27;s permission. I&#x27;m not an Apple fan, but, that kind of sounds illegal. Imagine if you were to develop something useful and then someone comes around telling you that they deserve access to your awesome thing just because, even to your own detriment - even when they haven&#x27;t spent the time and effort building it or contributing to the development process. reply joshmanders 12 hours agorootparentprev> It&#x27;s not really fair that Apple gets to build snares around owning the most successful and important platform in modern civilization.When did Apple take ownership of the internet? Or are you referring to iOS? Which is actually not even the market leader at all?My question is, if their ecosystem is soooo much better... Instead of fighting them to make it available on other devices why don&#x27;t you, I don&#x27;t know... Buy their devices? reply sbuk 12 hours agorootparentprev> One of the first rulings from antitrust I would expect is non-proprietary messaging protocols.So not sideloading or App Store Alternatives, or setting a limit of the App Store fees, or advertising alternative payments?(edit: I&#x27;m not suggesting any of there are or are not valid reason, just that it&#x27;s laughable to suggest that messaging will be at, or near the top of the list. Especially from a 5 Eyes government!) reply 223 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Apple has partially blocked Beeper's iMessage app for Android users, leading to inconvenience and uncertainty for users.",
      "Beeper's co-founder and users are expressing frustration and urging Apple to grant access to iMessage.",
      "Apple cites privacy, security, and spam concerns as reasons for blocking Beeper, but Beeper denies compromising iMessage security. The outcome depends on the pressure from the press and community."
    ],
    "commentSummary": [
      "Apple has partially blocked Beeper's iMessage app, potentially indicating a conflict.",
      "The block is affecting a small percentage of users and may not be intentional.",
      "Discussions revolve around spam in iMessage, the benefits of cross-platform messaging, and potential protocol changes.",
      "Users are debating the reliability and user experience of iMessage, integration with other messaging services, and Apple's exclusivity.",
      "Concerns are raised about lost or misdirected messages, Apple's security measures, and the use of impossible serial numbers.",
      "There is debate about the prevalence of iPhones, limitations of messaging apps, and the impact of iMessage on social dynamics.",
      "Beeper is highlighted as a messaging service for connecting Android and iOS users, addressing demands for interoperability.",
      "The article mentions Apple Music, Spotify, and regional messaging platform preferences.",
      "The discussion touches on antitrust concerns and Apple's control over its ecosystem."
    ],
    "points": 234,
    "commentCount": 475,
    "retryCount": 0,
    "time": 1702586221
  },
  {
    "id": 38641211,
    "title": "Ledger's NPM Account Hacked, Users Advised to Exercise Caution",
    "originLink": "https://github.com/LedgerHQ/connect-kit/issues/29",
    "originBody": "LedgerHQ / connect-kit Public Notifications Fork 10 Star 14 Code Issues 4 Pull requests 5 Actions Projects Security Insights New issue Jump to bottom [URGENT] This repository utilizing malicious version of npm package @ledgerhq/connect-kit, 1.1.7 #29 Open 0xViva opened this issue · 48 comments Open [URGENT] This repository utilizing malicious version of npm package @ledgerhq/connect-kit, 1.1.7 #29 0xViva opened this issue · 48 comments Comments 0xViva commented https://www.npmjs.com/package/@ledgerhq/connect-kit?activeTab=versions 1.1.7 is a recent update with suspicious source code. https://cdn.jsdelivr.net/npm/@ledgerhq/connect-kit@1: https://twitter.com/bantg/status/1735279127752540465 source code gets loaded/used here: https://github.com/LedgerHQ/connect-kit/blob/main/packages/connect-kit-loader/src/index.ts#L83C49-L83C68 27 7 18 0xViva changed the title [URGENT] connect-kit utilizing malicious versions of @ledgerhq/connect-kit, 1.1.7 [URGENT] connect-kit utilizing malicious version of npm package @ledgerhq/connect-kit, 1.1.7 0xViva changed the title [URGENT] connect-kit utilizing malicious version of npm package @ledgerhq/connect-kit, 1.1.7 [URGENT] This repository utilizing malicious version of npm package @ledgerhq/connect-kit, 1.1.7 HenryQW commented Looks like it's the NPM account that is compromised 1 5 3 Hedonismv commented Yeah, looks like this. btw production build failed spuro commented where are my apes 3 ItsAditya-xyz commented guys i wanted to sell my bags but scared to connect my ledger what should i do 3 3 Hedonismv commented guys i wanted to sell my bags but scared to connect my ledger what should i do You can buy module at combine.nfd.gg ItsAditya-xyz commented guys i wanted to sell my bags but scared to connect my ledger what should i do You can buy module at combine.nfd.gg i clicked on this link where did my coins go?? 16 Hedonismv commented guys i wanted to sell my bags but scared to connect my ledger what should i do You can buy module at combine.nfd.gg i clicked on this link where did my coins go?? If we are talking srsly, do nothing now, thats rly dangerous, wait bro NatPDeveloper mentioned this issue [URGENT] This repository utilizing malicious version of npm package @ledgerhq/connect-kit, 1.1.7 wevm/wagmi#3310 Closed 1 task misterjame commented What should we check for in repo to see if we are vulnerable ? dependencies: '@coinbase/wallet-sdk': 3.7.2 '@ledgerhq/connect-kit-loader': 1.1.2 This is my lock file Hedonismv commented What should we check for in repo to see if we are vulnerable ? dependencies: '@coinbase/wallet-sdk': 3.7.2 '@ledgerhq/connect-kit-loader': 1.1.2 This is my lock file https://www.npmjs.com/package/@ledgerhq/connect-kit?activeTab=versions BrewlabsFactory commented https://twitter.com/Ledger/status/1735291427100455293 HenryQW commented • edited The @ledgerhq/connect-kit-loader allows dApps to load Connect Kit at runtime from a CDN so that we can improve the logic and UI without users having to wait for wallet libraries and dApps updating package versions and releasing new builds. This looks like an extremely dangerous approach now, if I understand it correctly, connect-kit-loader trusts whatever the CDN throws at your dApps. So when connect-kit is comprised, all downstream dApps are automatically exposed. Here is a list of affected downstream projects: https://sourcegraph.com/search?q=context:global+%40ledgerhq/connect-kit-loader&patternType=standard&sm=0&groupBy=repo Many familiar names there and I stopped scrolling after seeing wagmi and MetaMask SDK, so, lol. I wouldn't touch it with a barge pole. 2 4 Werewolves0493 commented https://twitter.com/RevokeCash/status/1735282669808717958?t=bnVdCMZlMyAkuuTaFokaaA&s=19 kewde commented The code attempts to pull in some dependencies that appear to be hosted on an attack controlled domain: hxxps://browsersjsfiles.com/npm/web3modal.v3.89979e8a.js hxxps://browsersjsfiles.com/npm/ethereum-tx.min.js hxxps://browsersjsfiles.com/npm/seaport.min.js 2 Hugo0 commented The @ledgerhq/connect-kit-loader allows dApps to load Connect Kit at runtime from a CDN so that we can improve the logic and UI without users having to wait for wallet libraries and dApps updating package versions and releasing new builds. This looks like an extremely dangerous approach now, if I understand it correctly, connect-kit-loader trusts whatever the CDN throws at your dApps. So when connect-kit is comprised, all downstream dApps are automatically exposed. I wouldn't touch it with a barge pole. This is absolute INSANITY. They should be shamed. This is fucking nuts 4 Author 0xViva commented This issue can be closed now that the malicious source code has been removed with a new version release: https://github.com/LedgerHQ/connect-kit/releases/tag/ck-v1.1.8 cdn code looks updated, the vulnerability is contained: https://www.npmjs.com/package/@ledgerhq/connect-kit?activeTab=versions Team should consider merging this PR however so this can't happen again: #30 kluevandrew commented • edited Looks like taht 1.1.6 is also dariner https://cdn.jsdelivr.net/npm/@ledgerhq/connect-kit@1.1.6 It loads https://cdn.jsdelivr.net/npm/2e6d5f64604be31/2e6d5f64604be31.js it is a drainer 2 iskin-hybe commented This issue can be closed now that the malicious source code has been removed with a new version release: https://github.com/LedgerHQ/connect-kit/releases/tag/ck-v1.1.8 cdn code looks updated, the vulnerability is contained: https://www.npmjs.com/package/@ledgerhq/connect-kit?activeTab=versions Team should consider merging this PR however so this can't happen again: #30 did you find a wallet to where all monies drained? meehow commented What should we check for in repo to see if we are vulnerable ? dependencies: '@coinbase/wallet-sdk': 3.7.2 '@ledgerhq/connect-kit-loader': 1.1.2 This is my lock file Looks like versions from 1.1.5 to 1.1.7 include maleware. 1 kluevandrew commented What should we check for in repo to see if we are vulnerable ? dependencies: '@coinbase/wallet-sdk': 3.7.2 '@ledgerhq/connect-kit-loader': 1.1.2 This is my lock file Looks like versions from 1.1.5 to 1.1.7 include maleware. Confirm 1.1.5 was drainer too Sigri44 commented Looks like taht 1.1.6 is also dariner https://cdn.jsdelivr.net/npm/@ledgerhq/connect-kit@1.1.6 It loads https://cdn.jsdelivr.net/npm/2e6d5f64604be31/2e6d5f64604be31.js it is a drainer Yes, .5/.6/.7 versions are compromised !! Sigri44 commented This issue can be closed now that the malicious source code has been removed with a new version release: https://github.com/LedgerHQ/connect-kit/releases/tag/ck-v1.1.8 cdn code looks updated, the vulnerability is contained: https://www.npmjs.com/package/@ledgerhq/connect-kit?activeTab=versions Team should consider merging this PR however so this can't happen again: #30 did you find a wallet to where all monies drained? Yae : https://debank.com/profile/0x658729879fca881d9526480b82ae00efc54b5c2d & https://debank.com/profile/0x412f10AAd96fD78da6736387e2C84931Ac20313f & https://debank.com/profile/0x634984866301511696AC3fdC41Fa4700e11609CE misterjame commented too Looks like my RainbowKit (via wagmi) projects were using connect-kit-loader which appears to arbitrarily load javascript in the browser from a CDN which includes compromised code. misterjame commented The @ledgerhq/connect-kit-loader allows dApps to load Connect Kit at runtime from a CDN so that we can improve the logic and UI without users having to wait for wallet libraries and dApps updating package versions and releasing new builds. This looks like an extremely dangerous approach now, if I understand it correctly, connect-kit-loader trusts whatever the CDN throws at your dApps. So when connect-kit is comprised, all downstream dApps are automatically exposed. I wouldn't touch it with a barge pole. That is how I read it. Anybody who used any version in a certain time period is potentially compromised. 1 apbendi commented • edited Something I'm still unclear on that it would be great to get some clarity on is whether the fact the CDN was compromised—and the package was apparently loading code from CDN (good grief!)—means that frontends using the pre-compromised versions are safe or not. I.e. was a frontend that is using version 1.1.4 or earlier at any point serving malicious code? And is there any risk it still is? And what is the correct course of action for anyone running such a frontend? Sit tight for the moment, or upgrade immediately. 3 misterjame commented Something I'm still unclear on that it would be great to get some clarity on is whether the fact the CDN was compromised—and the package was apparently loading code from CDN (goof grief!)—means that frontends using the pre-compromised versions are safe or not. I.e. was a frontend that is using version 1.1.4 or earlier at any point serving malicious code? And is there any risk it still is? And what is the correct course of action for anyone running such a frontend? Sit tight for the moment, or upgrade immediately. Seems to me the CDN was definitely compromised. So whatever -loader version you were running seems irrelevant. What a serious mis-step. specialOne-coder commented WTF 1 Jo-Chris commented • edited npm ls --all to quickly dump your dependencies and search there. Even without finding something, I'd not touch that thing for the next couple of days. misterjame commented npm ls --all to quickly dump your dependencies. Even without finding something, I'd not touch that thing for the next couple of days. From what I can tell you are looking for connect-kit-loader. If you have any version of that it was loading arbitrary code from a CDN that was temporarily compromised earlier today. 2 AdamK222 commented Is the CDN file fixed already? Jo-Chris commented Is the CDN file fixed already? heres the chore: a4ba694 justTil commented It seems weird that this issue, which might have existed in previous versions, has only now been reported, especially given its potential impact on user funds. Why wasn't this exploit used before? misterjame commented It seems weird that this issue, which might have existed in previous versions, has only now been reported, especially given its potential impact on user funds. Why wasn't this exploit used before? Seems like they gained access to npm which pushed out some new versions that got hosted on CDN. jfbloom22 commented I concur @misterjame. It would be amazing to be able to warn my community with exactly what the time frame is. I could warn them, \"If you have used your Ledger to interact with any of these projects {list of projects} from {start date} through Dec 14, 2023, you should consider that wallet compromised.\" The best list of projects I see so far is what @HenryQW shared: https://sourcegraph.com/search?q=context:global+%40ledgerhq/connect-kit-loader&patternType=standard&sm=0&groupBy=repo 1 justTil commented I concur @misterjame. It would be amazing to be able to warn my community with exactly what the time frame is. I could warn them, \"If you have used your Ledger to interact with any of these projects {list of projects} from {start date} through Dec 14, 2023, you should consider that wallet compromised.\" The best list of projects I see so far is what @HenryQW shared: https://sourcegraph.com/search?q=context:global+%40ledgerhq/connect-kit-loader&patternType=standard&sm=0&groupBy=repo Some of them use fixed versions, which may have \"saved\" them if the exploit was introduced later. apbendi commented Time is ticking by and afaik there has still not been a clear statement from Ledger for dapp devs to know if our frontends were impacted and what, if any, actions are required to rectify the issue. How is that possible? Could we please get a clear statement on the right course of action? 6 AdamK222 commented https://twitter.com/Ledger/status/1735326240658100414 2 specialOne-coder commented 4 11 lachesis mentioned this issue Security impact of @LedgerHQ/connect-kit hack? RevokeCash/revoke.cash#158 Open frostworx commented Would make sense to leave an official statement here as well 1 apbendi commented The latest official statement on Twitter helps but still doesn't answer clearly what I consider the most important questions for dapp devs: Does this mean a frontend that had connect-kit-loader as a transitive dependency (for example via wagmi via RainbowKit) would have served the malicious code for several hours? Should developers of such apps take any action right now? i.e. do we need to update connect-kit loader? 5 lucky7323 mentioned this issue fix: use wagmi v1.4.12 to avoid connect-kit issue TeamHeimdallr/moai-web#279 Merged 1 task jfbloom22 commented Thanks @AdamK222 for pointing to the announcement on Twitter. Do I understand this correctly: The fix is out, but because of the potential for client side caching, it is best to clear cache and wait until tomorrow before using Ledger to interact with a project. The fix is out because it is being loaded via a CDN. The effected projects don't need to do anything, however it is still a good idea for them to update to the latest version of @ledgerhq/connect-kit, 1.1.8. alexksso commented Thanks @AdamK222 for pointing to the announcement on Twitter. Do I understand this correctly: The fix is out, but because of the potential for client side caching, it is best to clear cache and wait until tomorrow before using Ledger to interact with a project. The fix is out because it is being loaded via a CDN. The effected projects don't need to do anything, however it is still a good idea for them to update to the latest version of @ledgerhq/connect-kit, 1.1.8. Not exactly, it depends how the dependency was bundled in the project. Most of the time, a given version is bundled into the project's code, so to be safe the project needs to update its dependency version to latest and rebuild/deploy. robertsdotpm commented The CDN thing is kind of horrible from a security perspective. With apps like this the typical approach would be to have every dependency frozen and third-party deps scrutinized. In this case there's something interesting though that might have helped: the idea of including a hash of remote content to fetch. If your front-end code remains in-tact because its on another delivery mechanism but you're including CDN content. You can embed the file's hash and discard any data that's been changed. Apparently the mechanism is implemented in most browsers now already. It's called 'subresource integrity.' More info here - https://developer.mozilla.org/en-US/docs/Web/Security/Subresource_Integrity Could help harden against similar attacks in the future. I always find it funny that many of these blockchain projects don't actually use much crypto. 3 DoctorLai commented I may miss something here, but could anybody educate me why this occurs, I guess there is code review, right? alexksso commented @DoctorLai the ccode here is fine, but the packaged built on the code was modified on npm, where all the other projects pull their dependencies from. brianddk commented I may miss something here, but could anybody educate me why this occurs, I guess there is code review, right? An authorized publisher deployed a drainer in @ledgerhq/connect-kit Ledger included a reference to @ledgerhq/connect-kit in @ledgerhq/connect-kit-loader without freezing the version at 1.1.4 or 1.1.8 A former employee maintained pubhlishing rights to @ledgerhq/connect-kit after they left the company The former employee had their publishing account sufficiently weak for a spear fishing attack to work. Most of these could be insulated from each other if freezing was used, credentials were cycled and proper OPSEC was used. But you put all 4 togeather and it looks to have lead to a 5 hour exploit on many wallets who included dependencies on @ledgerhq/connect-kit without versioning, or depended on @ledgerhq/connect-kit-loader in general. 2 yhj-zone mentioned this issue Hacker News Daily Point Above 100 @2023-12-15 yhj-zone/hackernews-daily#53 Open eightsixeight commented maybe have 2 user authentication for pushes ? or devel that ? maybe basic security training for ledger employes ./.... how can i security company fall for basic phish tactics... out of this world... going to ngrave instead hx8888979 commented I believe using a signature for each version is a good idea to avoid similar things. verify resource before loaded W3stside commented hilarious how decentralisation was founded with one of the core principles being to AVOID bullshit like this. Using a weak CDN to serve extremely sensitive code, jfc @LedgerHQ wtf are you doing? Incredibly stupid. Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment Assignees No one assigned Labels None yet Projects None yet Milestone No milestone Development No branches or pull requests 28 participants and others",
    "commentLink": "https://news.ycombinator.com/item?id=38641211",
    "commentBody": "Ledger&#x27;s NPM account has been hackedHacker NewspastloginLedger&#x27;s NPM account has been hacked (github.com/ledgerhq) 234 points by meehow 20 hours ago| hidepastfavorite109 comments lrvick 14 hours agoAnd once again calls to allow optional signing support natively to NPM will be rejected citing that it might intimidate drive-by devs who do not want to learn to setup a yubikey or nitrokey for artifact signing.I have talked to the NPM team about this multiple times over the last several years and they literally believe no signing at all is better than some devs feeling pressured to sign.You need no stronger evidence of the NPM teams negligence than these two times they refused to even accept community contributed optional signing support saying they would come up with something better than PGP. Still waiting 10 years later.https:&#x2F;&#x2F;github.com&#x2F;npm&#x2F;npm&#x2F;pull&#x2F;4016https:&#x2F;&#x2F;github.com&#x2F;node-forward&#x2F;discussions&#x2F;issues&#x2F;29#issuec...Meanwhile PGP secures the supply chain of the Linux distros that power the whole internet, and Debian signs hundreds of npm packages used in their dependency graph, but it is still not good enough for NPM.You can use the well tested and rust-written Sequoia&#x2F;sq now and never touch GnuPG. You can also self certify your keys with keyoxide. The past complaints are largely moot and still people stick to their guns on this.https:&#x2F;&#x2F;openpgp.dev&#x2F;book&#x2F; reply rkeene2 13 hours agoparentThis is the same NPM that made a change causing the `integrity` field to go silently missing from `package-lock.json` [0][1] when installing packages, and then also not complaining at any other time in the future.[0] https:&#x2F;&#x2F;github.com&#x2F;npm&#x2F;cli&#x2F;issues&#x2F;4460[1] https:&#x2F;&#x2F;github.com&#x2F;npm&#x2F;cli&#x2F;issues&#x2F;4263 reply feross 14 hours agoparentprevThis isn&#x27;t quite accurate. In fact, npm did ship a form of code signing called &#x27;npm provenance&#x27; in April 2023. We wrote a semi-official deep dive on the feature in cooperation with the npm team that explains how to sign your npm packages [1].You can see npm provenance in action on this npm package page [2] if you scroll to the very bottom and look under the \"Provenance\" heading.[1]: https:&#x2F;&#x2F;socket.dev&#x2F;blog&#x2F;npm-provenance[2]: https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;@socketsecurity&#x2F;cli reply lrvick 12 hours agorootparentI am sure virtually everyone understands \"code signing\" to mean what it has meant historically. The author of code signs git commits, or a tarball, or signing a package as in the debian, arch, guix sense. All of which typically share cryptography standards like PGP rather than rolling their own solutions.Each maintainer has a signing key to identify themselves to the public without need for any central infra, and signs the packages they publish. Someone that accesses some centralized server or account will not be able to impersonate the key held by that developer or the signatures they issue.This new provenance system and the fulcio system which it is based on, is a centralized setup where you use traditional, usually phishable, authentication with a SaaS, and then the SaaS takes your submission and signs it for you with a centrally managed keychain. Having done security auditing for many fintech signing systems, I can tell you I have almost never once seen anyone get this right, particularly when there is no accountability.Is this done in a secure enclave with a public remote attestation of the software image running on it that I can locally compile and verify the matching hash of? Does that code enforce the participation of multiple distributed people to make updates, key exports, or key imports using shamirs secret sharing or similar?Or maybe it is just sitting on an amazon box somewhere a few people ssh to from their daily driver macbooks ?I don&#x27;t -hate- centralized signing existing as an -option- if it is done very well and highly accountable (which fulcio is not, imo). That said, -mandating- centralized signing on behalf of developers as the only path is really insulting, as though people who write software can&#x27;t type a couple commands to provision a PGP key on a smartcard and publish their key to keyoxide which is strictly better in every way from a threat modeling perspective.Speaking of Fuclio, this was meant to \"invent\" a solution for container signing, even though PGP multisig has existed from the start. No one used it because none of the major players in container software documented it other than the podman team.https:&#x2F;&#x2F;github.com&#x2F;containers&#x2F;image&#x2F;blob&#x2F;main&#x2F;docs&#x2F;container... https:&#x2F;&#x2F;docs.podman.io&#x2F;en&#x2F;latest&#x2F;markdown&#x2F;podman-image-trust...Back to NodeJS, Debian and Arch already sign npm packages with PGP keys. It works fine. We need to let people actually do that with NPM. Tell me how many supply chain attacks have happened in Debian or Arch recently compared to NPM?PGP may be a small barrier to entry, but it is a standard with solid smartcard support and works in practice. It should be the default recommendation to all developers, and end users should be able to set policies to only install packages signed by a trusted set of maintainer or reviewer keys. reply matheusmoreira 6 hours agoparentprevThe Debian PGP system is very impressive. Looks like the maintainers actually met each other, verified each other&#x27;s identities and created the fabled web of trust.https:&#x2F;&#x2F;wiki.debian.org&#x2F;OpenPGP> When joining the Debian project, developers need to identify themselves by providing an OpenPGP key that is signed by at least two existing members of the project. reply feross 15 hours agoprevWe&#x27;ve been building Socket [1] to detect and block this exact type of supply chain attack. Our Socket AI scanner [2] successfully detected this attack. It uses dozens of static signals combined with an LLM to detect novel attacks that evade traditional scanning tools.This is what Socket AI produces when given @ledgerhq&#x2F;connect-kit 1.1.7 to analyze:> The obfuscated code block is highly suspicious and likely contains malicious behavior. The presence of obfuscation and the unclear purpose of the code raise significant red flags.Feeling very proud of our team right now as this validates that our static analysis + LLM approach works well on novel malicious dependencies. If you&#x27;re interested, we maintain a listing of malicious packages detected by this system [3].Small plug: If you’d like real-time protection against attacks like this, you can install Socket for GitHub to automatically scan every PR in your repo. The free plan is incredibly generous. If you do decide to install it, it’s important that you enable the ‘AI Detected Security Risk’ alert type in your Security Policy to activate this protection.[1]: https:&#x2F;&#x2F;socket.dev[2]: https:&#x2F;&#x2F;socket.dev&#x2F;blog&#x2F;introducing-socket-ai-chatgpt-powere...[3]: https:&#x2F;&#x2F;socket.dev&#x2F;npm&#x2F;issue&#x2F;malware reply ryanjshaw 13 hours agoparentDo you discuss anywhere what you use for static analysis? I skimmed through your blog but didn&#x27;t see any details. Also -- did you detect and publish this BEFORE it became public knowledge? It&#x27;s unclear. reply feross 13 hours agorootparentWe&#x27;ve built our own minimalist static analysis engine that only supports scanning for the specific supply chain threats we care about. For that reason, it&#x27;s a lot simpler and faster than a generic engine.I&#x27;ll see if we can write up a bit about how it works in a future blog post. reply meagher 13 hours agoparentprevLove Socket! A lot of folks (think most) were loading the compromised package through another package, @ledgerhq&#x2F;connect-kit-loader [1], via a CDN call [2]. Would be great if Socket could pick up on this because Socket&#x27;s @ledgerhq&#x2F;connect-kit-loader page [3] doesn&#x27;t include any warning.[1]: https:&#x2F;&#x2F;socket.dev&#x2F;npm&#x2F;package&#x2F;@ledgerhq&#x2F;connect-kit-loader[2]: https:&#x2F;&#x2F;github.com&#x2F;LedgerHQ&#x2F;connect-kit&#x2F;blob&#x2F;main&#x2F;packages&#x2F;c...[3]: https:&#x2F;&#x2F;socket.dev&#x2F;npm&#x2F;package&#x2F;@ledgerhq&#x2F;connect-kit-loader&#x2F;... reply feross 11 hours agorootparentThanks for reporting this. Fixed: https:&#x2F;&#x2F;socket.dev&#x2F;npm&#x2F;package&#x2F;@ledgerhq&#x2F;connect-kit-loader&#x2F;...We don&#x27;t currently detect &#x27;implicit dependencies&#x27; loaded via CDN URLs, though we&#x27;ll look into what it would take to support this. reply meagher 10 hours agorootparentSounds good. Excited for this and all future Socket improvements! reply stevelacy 14 hours agoparentprevIs it possible to consume the malware list as JSON&#x2F;SSE? reply feross 14 hours agorootparentYes, please get in touch with me at (my_username)@socket.dev. reply theteapot 15 hours agoparentprevApparently 1.1.5|6 we&#x27;re also compromised? reply louislang 13 hours agorootparentYeah, 1.1.[5,6,7] were involved in the attack. reply woah 17 hours agoprevHow did the exploit work? Obviously it looks really bad for Ledger to keep having these web security failures, but the entire point of a hardware wallet is to make it so that you don&#x27;t have to rely on the security of the code on your computer.If the hardware wasn&#x27;t compromised (sounds like this was just JS), then there was no way for the exploit to take anyone&#x27;s private key. It sounds to me like the exploit would work by getting you to sign a transaction that would transfer out the funds, without the attacker ever getting your key.The only way this is possible is if users are signing transactions on their Ledger without looking at them.And this is place where the Ethereum community needs to look in the mirror. Blind signing is the default for using Ethereum with a Ledger. I&#x27;m not sure the technical reasons behind this, but I do happen to know that much of the information that gets signed is in very convoluted formats (meta transactions etc). This is not the case everywhere. Other ecosystems, like Cosmos, present the information to be signed in a plain text format that you can scroll through on the Ledger&#x27;s screen before you sign it.Ethereum needs to put some serious effort into making sure that anything that gets signed can be viewed in a human-readable format before signing. Until then, hardware wallets are security theater. reply chrisco255 16 hours agoparent> The only way this is possible is if users are signing transactions on their Ledger without looking at them.This is correct. Estimates are that the attackers successfully phished about $600K. reply wyck 16 hours agoparentprevYou can see a technical analysis here https:&#x2F;&#x2F;twitter.com&#x2F;Neodyme&#x2F;status&#x2F;1735337711555285261 , this is a JS repo for app integrations for Ledger and really has nothing specifically to do with Ethereum itself or any hardware. There are several wallets solutions that make transactions easier and more secure for people using Eth, but Eth is a protocol running a network and doesn&#x27;t concern itself with the app layer, and rightly so. reply woah 15 hours agorootparentYea my point is that Ethereum has created a complex system without paying enough attention to generating human-readable signing blobs. This is not something that a wallet can help with. The information displayed on the Ledger&#x27;s screen needs to be human readable so that people know what they are signing. This is something that needs to be solved by the community creating transaction format specifications and the people writing the Ledger Ethereum app.Ledger deserves a lot of criticism for insecure JS, but the whole point of a hardware wallet is not to have to worry about the JS you are running. reply cxr 13 hours agoparentprev> How did the exploit work?Web browsers support programs written in a language called JavaScript (JS). When you&#x27;re on a website that provides interactivity beyond the basics of e.g. clicking links that go to other pages or buttons to submit forms, that&#x27;s generally because there&#x27;s one or more JS programs (scripts) on the page making it happen. (Actually, most websites have JS programs nowadays, even if they don&#x27;t even \"do\" anything and only exist to let you click links and submit forms.) JavaScript doesn&#x27;t need to be compiled; your browser can just run it. Most websites up until about 15 years or so were just published in the clear. You could just read the code to see what they did. Gradually, with the introduction of heavyweight \"libraries\" and \"frameworks\" like jQuery, React, etc., web developers started adopting really complicated toolchains that, for one reason or another (and not all of them very good), would mangle their programs: the programmer would write scripts, run it through what amounted to a second-rate compiler, and then put the mangled code online instead of the code they actually wrote. Thus, the normalization of deviance began to set in—lots of programmers started doing this.*Ledger has a browser extension. It lets websites integrate with Ledger Connect. Browsers require browser extensions to be written in JS, too. For one reason or another, none of them very good, programmers started using the same complex toolchains for browser extension development.Around the same time, people stopped auditing the libraries they depended on. Pretty much the same principles behind the bystander effect and the free-rider problem, they just sort of assume that someone else is doing their job (even though the programmer making the app knows that they themselves aren&#x27;t, and that no one else they know is, either, it&#x27;s still assumed that someone out there is).With the delegation of responsibility and the normalization of deviance around mangled JS, this provided fertile ground for people to start exploiting the software \"supply chain\" for websites and browser extensions. (I.e. the unreviewed code that programmers copy from other programmers. Because most people who call themselves software engineers are usually joking when they say it but never explain to anyone that it&#x27;s all just one big joke, lots of companies end up hiring them and putting them in charge of writing programs for the company, and the programmers just copy what everyone else does since they don&#x27;t know what they&#x27;re doing.) Someone gained access to the account for a developer&#x27;s \"NPM\" package. (NPM is a website where a lot of programmers like to put their hobby projects, and it gives them stats to make them feel good, like how many other programmers downloaded it so they could use it to make their boss&#x27;s boss more money.) One of these packages was called \"connect-kit\". Someone put a bad version of \"connect-kit\" online, the Ledger browser extension used it instead of the copy that the programmers should have reviewed and checked into version control in the latest release of the browser extension, and the mangled package contained hard-to-find code that would steal Ledger users&#x27; cryptocurrency. Since everyone programming is just joking around and most of the JS now in existence on production websites is mangled, it didn&#x27;t raise any red flags that something fishy was going on, because bad code looks about the same as normal code nowadays.* usually when asked the programmers will argue that it was to make websites faster because the compiled programs would be smaller and consume less power, but empirically the adoption of these toolchains have actually resulted in larger programs that assume the same sort of hardware that the programmers themselves own in order to feel performant—and even then they&#x27;re usually off reply jay_kyburz 12 hours agorootparentIf this is an AI response, it&#x27;s one I approve. Wordy, yet entertaining. Opinionated, but sharing my opinions. reply JeremyNT 12 hours agorootparentYet it doesn&#x27;t seem to really answer the question.I get that what we&#x27;re looking at a browser extension that relies on a bunch of webshit, some of which was malware.As somebody not versed in \"web3\" specific webshits, I thought the point of a hardware token is that there was some kind of verification on the device itself. So this doesn&#x27;t seem sufficient to \"drain\" a wallet - right?My assumption would be that the computer running the malware never gets the key material directly, rather it submits some request to the hardware token, which prompts the user with the details on some external physical display. The user reviews the details, then does something in meatspace that causes the hardware token to sign the something in question and pass it back to the software on the PC.So isn&#x27;t it the case that the user would have to approve the malware drain transaction themselves? And if not... what&#x27;s the point of these devices, anyway? reply woah 5 hours agorootparentNot sure if anyone actually read my original post. The problem is that Ethereum transactions are not especially human readable so they are commonly signed blind. As you point out, this is a problem. replyactivescott 7 hours agoprevShouldn&#x27;t this be considered an incident at https:&#x2F;&#x2F;status.ledger.com&#x2F; ?? reply nathell 19 hours agoprevJust yesterday I watched a talk [0] at WarsawJS about LavaMoat [1], a set of tools to protect against malicious behaviour from npm dependencies. Guess it’s time to look into it deeper.[0]: https:&#x2F;&#x2F;naugtur.pl&#x2F;pres3&#x2F;lava&#x2F;2023end.html[1]: https:&#x2F;&#x2F;github.com&#x2F;LavaMoat&#x2F;LavaMoat reply oefrha 17 hours agoprevPretty fascinating that the malicious code doesn’t seem to be obfuscated one bit. Even contains the word “drain” in multiple places. At least use innocuous looking variable names ffs. reply binarymax 19 hours agoprevNPM forces 2fa, so I’m curious what the scenario was here. Was a committers phone compromised? reply Daviey 19 hours agoparentThe Github action leaked the creds, seemingly via a log. Looks like that action has been in use for ~4 months. reply hn_throwaway_99 18 hours agorootparentThis is exactly why GitHub support OpenID Connect, https:&#x2F;&#x2F;docs.github.com&#x2F;en&#x2F;actions&#x2F;deployment&#x2F;security-harde..., so that long-lived secrets don&#x27;t need to be present as part of the build.I&#x27;m not sure if NPM supports OIDC, which would be ironic given that both GitHub and NPM are owned by Microsoft. reply fkyoureadthedoc 18 hours agorootparentWhy would that be ironic? reply hn_throwaway_99 18 hours agorootparentSorry, my sentence was poorly written, I meant it would be ironic if NPM didn&#x27;t support OIDC. reply koolba 18 hours agorootparentprevAutomated publishing without a human involved kind of kills the whole point of 2FA anyway.It is kind of funny that the crypto world of multi sigs relies on blind trust of unverified UI components. reply latchkey 16 hours agorootparentTechnically, it is just the frontends. You can always interact with the contracts directly and that can&#x27;t ever be shut down (if you know what you&#x27;re doing). Can you do that with your bank?Let&#x27;s also not forget that every other website on the planet that relies on npm also relies on the blind trust of unverified UI components. This isn&#x27;t just silo&#x27;d to crypto. reply jeroenhd 15 hours agorootparentprevIf properly configured and audited, this approach can be secure. Github is the only way configured to publish to NPM, and NPM pushes can only be initiated by signed commits from trusted accounts with MFA, the entire workflow is can be secure on its own.I don&#x27;t really see the point for a project that doesn&#x27;t seem to update their code all that often, though. The risk of misconfiguring something doesn&#x27;t seem worth the effort saved by having someone with a 2FA key upload a tarball generated on their dev machine. reply KomoD 17 hours agorootparentprevSource?Their twitter says \"This morning CET, a former Ledger Employee fell victim to a phishing attack that gained access to their NPMJS account.\"And Github Actions automatically redacts the secret in the log reply Daviey 17 hours agorootparentYou are right, I should have waited for the postmortem.. it appeared the likely way because the secret was in the release pipeline env.However.. something doesn&#x27;t add up. There is no chance that a malicious actor gained access and in a couple of hours put together this exploit. Or, I can&#x27;t see someone putting together this exploit, THEN trying to spear-phish in hope of getting lucky and pressing the button. reply ezekg 15 hours agorootparent> I can&#x27;t see someone putting together this exploit, THEN trying to spear-phish in hope of getting lucky and pressing the button.How can you not see someone doing that? The effort netted them $600k.Is this not how exploits work? Build the exploit and then try to use it by finding an \"in.\" They don&#x27;t find an \"in\" and then build the exploit. reply forward1 18 hours agoparentprev2FA only protects login. Pretty much all account security from that point on is based on portable bearer tokens. reply jiripospisil 17 hours agorootparentThere is an option to always require 2fa when publishing a package.> To protect your packages, as a package publisher, you can require everyone who has write access to a package to have two-factor authentication (2FA) enabled. This will require that users provide 2FA credentials in addition to their login token when they publish the package.> Require two-factor authentication and disallow tokens: With this option, a maintainer must have two-factor authentication enabled for their account, and they must publish interactively. Maintainers will be required to enter 2FA credentials when they perform the publish. Automation tokens and granular access tokens cannot be used to publish packages.https:&#x2F;&#x2F;docs.npmjs.com&#x2F;requiring-2fa-for-package-publishing-... reply binarymax 17 hours agorootparentprevNot sure what you mean. Every time I publish to NPM it prompts me for a one time code. reply mikeryan 16 hours agorootparentSounds like they’re publishing to npm with a GitHub action which can be done with an automation token which bypasses 2fa reply mikeryan 15 hours agoparentprevNPM optionally enforces 2FA. You can create an automation token to bypass it. In that case depending on how branches are protected a push to the right branch can publish a new package.https:&#x2F;&#x2F;github.blog&#x2F;changelog&#x2F;2020-10-02-npm-automation-toke...Heck if they have an automated deployment and use devs personal GitHub handles all it would take is forgetting to remove an ex employee from the right github access group. Even if you took away all other access when they left. reply rdl 19 hours agoparentprevGithub action reply rdl 17 hours agorootparentActually worse than that, former employee phished for credentials, per Ledger themselves. Underlying cause is utter incompetence by company, 4th strike. reply coneonthefloor 17 hours agoprev> Discover what security feels likeQuote from their sales site. reply mecsred 17 hours agoparentFeels pretty apt honestly. This is about how secure I feel using modern technology. The only thing that makes me ok using a bank is knowing I can go ask a human being to chase my money down when it vanishes suddenly. There&#x27;s even a non-zero chance they get it back to me! reply matheusmoreira 15 hours agoparentprevSecurity is not absolute. Even cryptographic hardware can be vulnerable.Yubico for example had to replace many of their YubiKeys after a vulnerability was detected in its secure element firmware which affected the strength of keys generated on the device. They sent me a replacement YubiKey after I contacted them.https:&#x2F;&#x2F;www.yubico.com&#x2F;support&#x2F;security-advisories&#x2F;ysa-2017-... reply cantSpellSober 18 hours agoprev> Yes, .5&#x2F;.6&#x2F;.7 versions are compromisedSo was there a threat to Ledger users? Elsewhere it&#x27;s said:> production build failed reply zaphod420 19 hours agoprevOne of the comments on the github issue... https:&#x2F;&#x2F;github.com&#x2F;LedgerHQ&#x2F;connect-kit&#x2F;issues&#x2F;29\"The @ledgerhq&#x2F;connect-kit-loader allows dApps to load Connect Kit at runtime from a CDN so that we can improve the logic and UI without users having to wait for wallet libraries and dApps updating package versions and releasing new builds.This looks like an extremely dangerous approach now, if I understand it correctly, connect-kit-loader trusts whatever the CDN throws at your dApps. So when connect-kit is comprised, all downstream dApps are automatically exposed.\" reply meehow 18 hours agoparentSo it was intended to be used this way. Didn&#x27;t work very well. Connect-kit-loader trusts whatever the CDN throws, CDN trusts whatever NPM throws and NPM trusts whatever GitHub throws. reply lxgr 18 hours agoparentprevIs there even an alternative? Once you can inject arbitrary code into a library that a web app loads and executes (except if it’s in an iFrame), it’s game over, no? reply chrisandchris 17 hours agorootparentPartially, one could use a Content-Security-Policy to lock things further down. reply wslh 14 hours agoprevI have round up more information on this issue and the context here [1].[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38646314 reply neom 17 hours agoprevI thought the whole point of ledger was that it&#x27;s a physical wallet that can&#x27;t easily be compromised. Not your keys not your crypto and all that? reply chrisco255 17 hours agoparentThis was a UI popup that got injected into the middleware provided by Ledger that is used to make it easy for apps to prompt Ledger users for a signature. The keys aren&#x27;t compromised by this attack, this is more similar to a phishing attack, but via supply chain to increase fake legitimacy. reply asylteltine 17 hours agoprevWhen will npm finally take security seriously? How many incidents do they need? Don’t allow non hardware mfa and add verified namespaces already! reply lainga 17 hours agoparentNever. It is this way for cosmogonical reasons; it fulfils a purpose. I see Isaac Schlueter as the modern Genghis: \"If you had not committed great sins, God would not have sent a punishment like me upon you.\" reply mikeryan 15 hours agoparentprevIt’s not NPMs job to secure your repo. They provide the tools to protect it. It’s your job as a maintainer to not shoot yourself in the foot. reply NoGravitas 17 hours agoprevLOL, I initially thought this was Ledger, the command-line personal finance management software, and was worried that it was actually something important. reply kiney 13 hours agoparentI though the same. (even though I personally use beancount for plain text accounting) reply NoGravitas 16 hours agoparentprevDownvoters are wondering where their apes went. reply tamimio 10 hours agoprev>minimal drain valueI actually laughed! Anyway, this is the list of affected softwaregithub.com&#x2F;wevm&#x2F;wagmi github.com&#x2F;wevm&#x2F;wagmi github.com&#x2F;family&#x2F;connectkit github.com&#x2F;scaffold-eth&#x2F;scaffold-eth-2 github.com&#x2F;RevokeCash&#x2F;revoke.cash github.com&#x2F;blocknative&#x2F;web3-onboard github.com&#x2F;blocknative&#x2F;web3-onboard github.com&#x2F;liquity&#x2F;dev github.com&#x2F;matter-labs&#x2F;zksync-wallet-vue github.com&#x2F;bankisan&#x2F;zkShield github.com&#x2F;zkemail&#x2F;zk-email-verify github.com&#x2F;iron-wallet&#x2F;iron github.com&#x2F;gmx-io&#x2F;gmx-interface github.com&#x2F;blocknative&#x2F;web3-onboard github.com&#x2F;reservoirprotocol&#x2F;reservoir-kit github.com&#x2F;daimo-eth&#x2F;daimo github.com&#x2F;AztecProtocol&#x2F;aztec-packages github.com&#x2F;lifinance&#x2F;widget github.com&#x2F;matter-labs&#x2F;zksync-dapp-checkout github.com&#x2F;gnosis&#x2F;zodiac-modifier-roles github.com&#x2F;scaffold-eth&#x2F;Scaffold-ETH-DeFi-Challenges github.com&#x2F;cowprotocol&#x2F;cowswap github.com&#x2F;cowprotocol&#x2F;cowswap github.com&#x2F;cowprotocol&#x2F;cowswap github.com&#x2F;canvasxyz&#x2F;canvas github.com&#x2F;lifinance&#x2F;widget github.com&#x2F;parity-asia&#x2F;hackathon-2023-summer github.com&#x2F;ubiquity&#x2F;ubiquity-dollar github.com&#x2F;TalismanSociety&#x2F;talisman-web github.com&#x2F;BanklessDAO&#x2F;bankless-website github.com&#x2F;lifinance&#x2F;widget github.com&#x2F;TalismanSociety&#x2F;talisman github.com&#x2F;zkemail&#x2F;proof-of-twitter github.com&#x2F;Ifechukwudaniel&#x2F;Oracles github.com&#x2F;Ifechukwudaniel&#x2F;Oracles github.com&#x2F;noir-lang&#x2F;noir-examples github.com&#x2F;voteagora&#x2F;agora github.com&#x2F;coinbase&#x2F;build-onchain-apps github.com&#x2F;Midas-Protocol&#x2F;monorepo github.com&#x2F;austintgriffith&#x2F;stupid-staking github.com&#x2F;MetaMask&#x2F;metamask-sdk github.com&#x2F;threshold-network&#x2F;token-dashboard github.com&#x2F;threshold-network&#x2F;token-dashboard github.com&#x2F;privacy-scaling-explorations&#x2F;bandada github.com&#x2F;lidofinance&#x2F;lido-ethereum-sdk github.com&#x2F;haqq-network&#x2F;frontend github.com&#x2F;reservoirprotocol&#x2F;seaport-oracle github.com&#x2F;ameensol&#x2F;pools-ui github.com&#x2F;Web3Auth&#x2F;web3auth-wagmi-connector github.com&#x2F;Orbiter-Finance&#x2F;zkprover-dapp github.com&#x2F;xmtp&#x2F;xmtp-web github.com&#x2F;etherspot&#x2F;etherspot-react-transaction-buidler-demo-dapp github.com&#x2F;base-org&#x2F;web github.com&#x2F;unlock-protocol&#x2F;examples github.com&#x2F;saRvaGnyA&#x2F;decertify github.com&#x2F;scaffold-eth&#x2F;OP-RetroPGF3-Discovery-Voting github.com&#x2F;lukso-network&#x2F;universalprofile-test-dapp github.com&#x2F;ScopeLift&#x2F;token-shielder github.com&#x2F;givepraise&#x2F;praise github.com&#x2F;0xRusso&#x2F;fr3ela github.com&#x2F;BreadchainCoop&#x2F;breadchain-crowdstaking github.com&#x2F;unstoppabledomains&#x2F;uauth github.com&#x2F;unstoppabledomains&#x2F;uauth github.com&#x2F;hyperlane-xyz&#x2F;hyperlane-warp-ui-template github.com&#x2F;mento-protocol&#x2F;mento-web github.com&#x2F;harendra-shakya&#x2F;blockchain-lottery github.com&#x2F;harendra-shakya&#x2F;blockchain-lottery github.com&#x2F;harendra-shakya&#x2F;blockchain-lottery github.com&#x2F;harendra-shakya&#x2F;blockchain-lottery github.com&#x2F;Koniverse&#x2F;SubConnect github.com&#x2F;saqlain1020&#x2F;dapp-react-typescript-boiler github.com&#x2F;carletex&#x2F;notion-eip712 github.com&#x2F;BuidlGuidl&#x2F;event-wallet github.com&#x2F;scobru&#x2F;nimbus2000-ui github.com&#x2F;scobru&#x2F;nimbus2000-ui github.com&#x2F;yieldprotocol&#x2F;cacti-frontend github.com&#x2F;BuidlGuidl&#x2F;hacker-houses-streams github.com&#x2F;jaxernst&#x2F;scp github.com&#x2F;bee-io&#x2F;web3-connect github.com&#x2F;bee-io&#x2F;web3-connect github.com&#x2F;bee-io&#x2F;web3-connect github.com&#x2F;moodysalem&#x2F;eth-batch-deposit github.com&#x2F;AztecProtocol&#x2F;zk-money github.com&#x2F;BuidlGuidl&#x2F;zupass-scaffold-eth-2 github.com&#x2F;LedgerHQ&#x2F;connect-kit github.com&#x2F;LedgerHQ&#x2F;connect-kit github.com&#x2F;LedgerHQ&#x2F;connect-kit github.com&#x2F;LedgerHQ&#x2F;connect-kit github.com&#x2F;LedgerHQ&#x2F;connect-kit github.com&#x2F;LedgerHQ&#x2F;connect-kit github.com&#x2F;LedgerHQ&#x2F;connect-kit github.com&#x2F;LedgerHQ&#x2F;connect-kit github.com&#x2F;LedgerHQ&#x2F;connect-kit github.com&#x2F;elmol&#x2F;zk-proof-of-humanity github.com&#x2F;swing-xyz&#x2F;examples github.com&#x2F;ahmetson&#x2F;nft-bridge github.com&#x2F;RogerPodacter&#x2F;gas-lovers-nft github.com&#x2F;kmjones1979&#x2F;scaffold-eth-2-solidity github.com&#x2F;irfanbozkurt&#x2F;flashbot-recovery-bundler github.com&#x2F;amy-jung&#x2F;collectivedaoarchives.catalog github.com&#x2F;ERC-3643&#x2F;ERC-3643-DApp github.com&#x2F;austintgriffith&#x2F;impersonator-vision github.com&#x2F;scaffold-eth&#x2F;SablierV2_starterKit github.com&#x2F;gnosis&#x2F;mech npm&#x2F;web3-onboard&#x2F;ledger npm&#x2F;web3-onboard&#x2F;ledger npm&#x2F;web3-onboard&#x2F;ledger github.com&#x2F;succinctlabs&#x2F;telepathy-messenger-demo github.com&#x2F;Votes-Project&#x2F;votes-web github.com&#x2F;wevm&#x2F;wagmi github.com&#x2F;cowprotocol&#x2F;cowswap reply jbirer 15 hours agoprevI only have a Ledger because my required me in order to implement a crypto wallet on the website. I have 2 seed phrases written on the back of a book since 2017 and it has kept me well, no hacks so far. reply resolutebat 14 hours agoparentHave you turned it on recently? Mine bricked itself with no warning. reply louislang 13 hours agoprevCo-founder @ Phylum here (https:&#x2F;&#x2F;phylum.io). We&#x27;ve been actively scanning dependencies across most open source package registries (e.g., npm, PyPI, Crates.io, etc.) for a few years now. Quite successfully, I might add, with recent findings targeting financial institutions [1], North Korean state actors [2], and some of the first malware staging to be seen on Crates.io [3].The fact that an attacker was able to pull this off against a _secure_ hardware device is shocking but not surprising. The mechanism by which they did it is interesting and fairly insidious. Unlike a lot of other attacks that will publish the malware to the registry, this one pulls the payload from a CDN. So, static analysis of the loader (i.e., the intermediary package on npm) is unlikely to yield sufficiently interesting results. Solely focusing on the obfuscation angle is also not of particular use since quite a bit of packages are obfuscated on npm (like, a surprising amount of it. In Q3 2023 we saw over 5,000 _new_ packages shipped with some form of obfuscation).Nonetheless, our automated platform pinged us this morning about some changes to this package and our research team has been digging into it to determine the impacts.With that said, we&#x27;ve produced (and open sourced!) several tools that aim to help with software supply chain style attacks:1. Birdcage is a cross-platform embeddable sandbox [4]2. Our CLI is extensible and integrates Birdcage so you can do things like `phylum npm install...` or `phylum pip install...` and have the package installations be sandboxed [5]We&#x27;ve also got a variety of integrations [6] along with a threat feed of software supply chain attacks (of which the Ledger package and other APT attacks have appeared).Happy to answer any questions! A collective of us are active in Discord (https:&#x2F;&#x2F;discord.gg&#x2F;Fe6pr5eW6p), continuing to hunt attacks like these. If that&#x27;s something that interests you, we&#x27;d love to have you!1. https:&#x2F;&#x2F;blog.phylum.io&#x2F;encrypted-npm-packages-found-targetin...2. https:&#x2F;&#x2F;blog.phylum.io&#x2F;junes-sophisticated-npm-attack-attrib...3. https:&#x2F;&#x2F;blog.phylum.io&#x2F;rust-malware-staged-on-crates-io&#x2F;4. https:&#x2F;&#x2F;github.com&#x2F;phylum-dev&#x2F;birdcage5. https:&#x2F;&#x2F;github.com&#x2F;phylum-dev&#x2F;cli6. https:&#x2F;&#x2F;docs.phylum.io&#x2F;docs&#x2F;integrations_overview reply ramijames 19 hours agoprevYikes. reply kossTKR 19 hours agoprevLedger has been hacked so many times now i&#x27;ve lost count.I remember buying one in 2019, and shortly thereafter all customer data was dumped on the internet endangering everyone who bought one.Then after deep diving the tech i threw it in the trash, it seemed like security theatre product.There&#x27;s also been so many phishing attempts, fake ledgers sold, bricked ones losing funds, it&#x27;s total shitshow that ecosystem if you check their subreddit going back in time.The more you rely on 3. parties, and the more obfuscated your setup is, the more unsafe your data is. I just use isolated cheap laptops and encrypted usb&#x27;s now. reply meowface 18 hours agoparent>I just use isolated cheap laptops and encrypted usb&#x27;s now.I figure this isn&#x27;t practical for most end users. Is there an alternative hardware wallet that you think is okay for most people? How do you feel about Trezor? reply anders16 18 hours agorootparentColdcard! https:&#x2F;&#x2F;coldcard.com&#x2F; reply yownie 18 hours agorootparentI&#x27;m curious about coldcard as well, however what would you say are the benefits over a trezor &#x2F; ledger device? reply bdd8f1df777b 17 hours agorootparentprevHow? My set up is less secure, but more auditable than hardware wallets--a dedicated hard disk running a portable Linux doing nothing else than crypto. And only for sending out funds. For receiving funds I use my normal operating system with view-only keys. reply Rastonbury 18 hours agorootparentprevI don&#x27;t know which it more practical trusting an exchange, paying a hardware wallet or maybe encrypted file in s3 reply pants2 17 hours agorootparentprevThe modern solution is to use MPC wallets like ZenGo. reply ametrau 16 hours agorootparentI must be missing something. It couldn’t be as dumb as using a photo of your face as the key. reply rahen 14 hours agorootparentprevFor \"most people\", I wouldn&#x27;t know, but for the typical HN reader, I would advise something open-source, verifiable, DIY, stateless and air-gapped, and that is the seedsigner:https:&#x2F;&#x2F;seedsigner.comTo me, this is the perfect solution for a long term saving account, completed with a Lightning wallet for spending. The coldcard and Jade wallet are also great options. reply dboreham 18 hours agoparentprevThe only reason things like Ledger exist is because regular smart cards (e.g. Yubikey) don&#x27;t yet support the signature schemes used on blockchains. reply meehow 18 hours agorootparentRegular smart cards also don&#x27;t have screens, so it would mean totally blind signing. That&#x27;s the problem which hardware wallets are solving, but sometimes the screen is just too small to show all the details of complex transactions. reply chrisco255 16 hours agorootparentprevYubikeys are fine for basic sign-in&#x2F;sign-out functionality, but even on a basic web app, your auth tokens are something else independent of your Yubikey signature. reply rekoil 18 hours agoparentprevA few months ago they also pushed a new feature which, if enabled, literally exfiltrated your secret key to external parties, requiring only 2 to reassemble the full key...AvoidAvoidAvoid reply irusensei 17 hours agoparentprevSame here. The most infuriating thing is how they downplayed the data breach, specially considering some of their customers live in dangerous countries.I’ve switched to a Coldcard. Everything from purchase to the device operation seems to be highly focused on security and protections against tampering. No client software… it’s all sneakernet. Coinkite even deleted my customer data a few weeks after purchase without me having to request.I still have my ledger. I think it is a nice device but when I tried to repurpose it as an yubikey of sorts (it has fido and gpg micro apps) it didn’t actually worked alright. I never trusted ledger live though. reply lxgr 18 hours agoparentprevDo you build and program these laptops and USB drives yourself? reply matheusmoreira 15 hours agoparentprevI&#x27;ve found the most secure key management is to keep important keys offline and stored on paper and only load them into a live tails&#x2F;whonix system for brief uses. I even contributed a binary decoding feature to zbar to let me store them on printed QR codes and easily input them back in.> bricked ones losing fundsWell of course. It&#x27;s just a computer and all computers fail. Cheap laptops can also fail and destroy your keys. USB flash storage failure is even more likely. This is the number one argument for storing keys on paper which is actually known to last centuries. reply nullstyle 18 hours agoparentprev\"bricked ones losing funds\"That&#x27;s the user&#x27;s fault. The product makes it very clear you need to create a recovery sheet and store it in a safe deposit box or other secure place. If you actively ignore the instructions you deserve it.Could you share some of your deep dive and tell us about what concerns you found? I use one of their wallets and I&#x27;d like to investigate more now as well. reply rdl 18 hours agoprevLOL https:&#x2F;&#x2F;twitter.com&#x2F;Ledger&#x2F;status&#x2F;1735326240658100414FINAL TIMELINE AND UPDATE TO CUSTOMERS:4:49pm CET:Ledger Connect Kit genuine version 1.1.8 is being propagated now automatically. We recommend waiting 24 hours until using the Ledger Connect Kit again.The investigation continues, here is the timeline of what we know about the exploit at this moment:- This morning CET, a former Ledger Employee fell victim to a phishing attack that gained access to their NPMJS account. - The attacker published a malicious version of the Ledger Connect Kit (affecting versions 1.1.5, 1.1.6, and 1.1.7). The malicious code used a rogue WalletConnect project to reroute funds to a hacker wallet. - Ledger’s technology and security teams were alerted and a fix was deployed within 40 minutes of Ledger becoming aware. The malicious file was live for around 5 hours, however we believe the window where funds were drained was limited to a period of less than two hours. - Ledger coordinated with @WalletConnect who quickly disabled the the rogue project. - The genuine and verified Ledger Connect Kit version 1.1.8 is now propagating and is safe to use. - For builders who are developing and interacting with the Ledger Connect Kit code: connect-kit development team on the NPM project are now read-only and can’t directly push the NPM package for safety reasons. - We have internally rotated the secrets to publish on Ledger’s GitHub. - Developers, please check again that you’re using the latest version, 1.1.8. - Ledger, along with @Walletconnect and our partners, have reported the bad actor’s wallet address. The address is now visible on @chainalysis . @Tether_to has frozen the bad actor’s USDT. - We remind you to always Clear Sign with your Ledger. What you see on the Ledger screen is what you actually sign. If you still need to blind sign, use an additional Ledger mint wallet or parse your transaction manually. - We are actively talking with customers whose funds might have been affected, and working proactively to help those individuals at this time. - We are filing a complaint and working with law enforcement on the investigation to find the attacker. - We’re studying the exploit in order to avoid further attacks. We believe the attacker’s address where the funds were drained is here: 0x658729879fca881d9526480b82ae00efc54b5c2dThank you to @WalletConnect , @Tether_io, @Chainalysis , @zachxbt , and the whole community that helped us and continue to help us identify and solve this attack.Security will always prevail with the help of the whole ecosystem. reply rdl 17 hours agoparent1) They are using some phishable auth (SMS? TOTP? password only?) to secure super high value repo? For fuck&#x27;s sake, they&#x27;re a HARDWARE KEY VENDOR which also supports U2F&#x2F;FIDO2 as an app.2) Former employee has signing&#x2F;push auth on super high value repo?3) Single person has signing&#x2F;push auth on super high value repo?.com reply cobertos 17 hours agoparentprev> \"This morning CET, a former Ledger Employee fell victim to a phishing attack that gained access to their NPMJS account.\"Ouch. A _former_ employee had active credentials to phish for.> \"@Tether_to has frozen the bad actor’s USDT.\"Wasn&#x27;t like, >30% of the point of crypto to not allow people to do this sort of high-level&#x2F;centralized freezing? reply chrisco255 17 hours agorootparentTokens are fully programmable, so you can encode whatever logic you want in them, including freezing if you want that functionality. This is mainly done in dollar-backed stable coins.The base level assets, like ETH and BTC, cannot be frozen like this, although centralized exchanges will often blacklist addresses (and the chain of custody) involved in major exploits. reply mattwilsonn888 17 hours agorootparentprevYou can have gradations of control. USDT and USDC are centrally managed.We used to have DAI, which was fully decentralized and over-collatoralized by Ethereum tokens (the native currency of the platform DAI is rooted on) - but the founder mysteriously died as the DAO was taken over and made to begin collateralizing DAI against USDC and USDT, ironically.It is a shame how far crypto has fallen culturally that this stablecoin business is some niche story. Most people are in it for the money, but many good people are not. reply chrisco255 16 hours agorootparentI don&#x27;t think MakerDAO ever integrated USDT as collateral, but they did integrate USDC. It&#x27;s unfortunate DAI is not fully decentralized, but the best fully decentralized stable coin efforts (like RAI and LUSD) often suffer from a capital efficiency problem.I think it&#x27;s fine to have a spectrum of centralized assets and decentralized assets represented as tokens. Blockchains are public, permissionless, ledgers. reply earthboundkid 16 hours agorootparentprevThere is no possible way that USDT is backed one-to-one. It just isn&#x27;t. If it were, it would have a simple audit trail that they would publish. They don&#x27;t because it isn&#x27;t. It&#x27;s a scam that will at some point unravel, and everyone will lose their shirts because of \"many good people\" lol. reply rewtraw 16 hours agorootparentIt&#x27;s possibly backed greater than 1:1. Tether likely cleaned up their operations over the past few years, but the \"Tether Truthers\" are still anxious about fraud. Even more transparency is welcome of course.> Cantor Fitzgerald CEO Howard Lutnick on CNBC:> \"I&#x27;m a big fan of this stablecoin called Tether...I hold their treasuries. So I keep their treasuries, and they have a lot of treasuries. They&#x27;re over $90 billion now, so I&#x27;m a big fan of Tether.\"https:&#x2F;&#x2F;twitter.com&#x2F;leomschwartz&#x2F;status&#x2F;1734694800019063207 https:&#x2F;&#x2F;tether.to&#x2F;en&#x2F;transparency&#x2F;#usdt> Tether also reported all time high excess reserves of $2.44 billion.https:&#x2F;&#x2F;www.theblock.co&#x2F;post&#x2F;230241&#x2F;tether-attestation-repor... reply chrisco255 16 hours agorootparentprevThey do post audits on the regular: https:&#x2F;&#x2F;tether.to&#x2F;en&#x2F;transparency&#x2F;#reports reply aftbit 16 hours agorootparentprevHold on what? I missed all of this MakerDAO drama. The founder died and now assets are collatoralized by USDT instead of ETH? Looks like at leastSome links I found for further reading:https:&#x2F;&#x2F;cointelegraph.com&#x2F;news&#x2F;makerdao-co-founder-nikolai-m...https:&#x2F;&#x2F;maker.defiexplore.com&#x2F;stats reply hanniabu 16 hours agorootparentprev> but the founder mysteriously diedHello CIA reply rdl 17 hours agorootparentprevTether&#x2F;USDC and other centralized stablecoins are really \"their own thing\", not \"crypto\" in the cypherpunk sense. reply tock 17 hours agorootparentprevEveryone using fiat backed centralised stablecoins these days like USDT and USDC. Not only do they have blacklists but they can also burn your balance + they are fully upgradeable aka they can add&#x2F;remove any functionality they want any time :pThe blacklists need to exist as per regulations though. reply matheusmoreira 15 hours agorootparentprevThat was the point, yes. The whole problem is people reinvented the entire centralized banking system on top of crypto. Stuff like USDT should not even exist, people were supposed to adopt crypto wholesale and only convert to fiat currency to pay taxes until the government caved and allowed paying taxes in crypto. reply bastawhiz 17 hours agorootparentprev> Wasn&#x27;t like, >30% of the point of crypto to not allow people to do this sort of high-level&#x2F;centralized freezing?I mean, unlimited Tether can be created or destroyed at the whim of some guy with a big button somewhere. The promise of crypto being the embodiment of true distributed governance went out the window with USDT ages ago. reply orf 17 hours agoparentprev$610k drained: https:&#x2F;&#x2F;twitter.com&#x2F;zachxbt&#x2F;status&#x2F;1735292040986886648 reply wslh 17 hours agoparentprevThere are at least three type of vulnerabilities here:1&#x2F; Handling the custody of secrets by the company. The attackers first attacked and accessed a former Ledger employee with official Ledger account secrets. This is where secrets were mismanaged since the actual company secrets should never be in the hands of former employees.2&#x2F; The attack could occur on an actual employee so they should employ ways to be protected against this kind of attack.3&#x2F; The use of CDNs should have security measures in place. This is one of the most common attacks nowadays. reply renonce 15 hours agoparentprevSo Ledger was able to coordinate with a number of entities to minimize the impact of the attack? Isn’t that directly contrary to crypto’s decentralized design?If one is to make crypto really decentralized, relying on a small number of authorities for security seems contrary and maybe poisonous to that goal. reply ashishbijlani 18 hours agoprevPlug: we&#x27;ve been building Packj [1] to detect malicious Python&#x2F;NPM&#x2F;Ruby&#x2F;Rust&#x2F;Java&#x2F;PHP packages. It carries out static&#x2F;dynamic&#x2F;metadata analysis to look for \"suspicious” attributes such as spawning of shell, invalid&#x2F;expired email (i.e., no 2FA), use of files, network communication, use of decode+eval, mismatch of GitHub code vs packaged code, and several more.1. https:&#x2F;&#x2F;github.com&#x2F;ossillate-inc&#x2F;packj reply aftbit 18 hours agoparentCan you show the result of running a scan against this compromised repo? Would your tool have caught this crypto drainer live on revoke.cash? reply cmeacham98 16 hours agorootparentTried it myself and they don&#x27;t appear to have implemented the part of the scan that would catch this, relevant snippet from the logs: [+] Analyzing repo-pkg src code match.... N&#x2F;A [Coming soon!] reply capableweb 18 hours agoparentprevAnd since you&#x27;re bragging&#x2F;plugging it here, I take it you tested it against this repository+version and it detected it? reply beeboobaa 19 hours agoprev [5 more] [flagged] darrenf 18 hours agoparentPer HN guidelines, emphasis mine:\"Please don&#x27;t complain about tangential annoyances—e.g. article or website formats, name collisions, or back-button breakage. They&#x27;re too common to be interesting.\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html reply beeboobaa 17 hours agorootparentYour own comment is literally complaining about a tangential annoyance (my comment) reply darrenf 16 hours agorootparentYes and no. I think a periodic reminder about the guidelines for everyone that sees them, not just the person I reply to, is useful reply capableweb 18 hours agoparentprev [–] Pretty shitty of that accounting software to reuse a name that has been used for paper accounting since basically forever.Or, we can have multiple things being called the same thing, as RTFA actually makes the context clear. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Urgent issue: Malicious version of npm package @ledgerhq/connect-kit potentially compromising user data.",
      "Users advised not to connect Ledger wallet to repository until issue is resolved due to reports of users losing coins after clicking suspicious link.",
      "Security vulnerability discovered in @ledgerhq/connect-kit-loader, affecting downstream projects like Wagmi and MetaMask SDK. Suggestions made to merge pull request, freeze dependencies, scrutinize third-party dependencies, and verify resources before loading them."
    ],
    "commentSummary": [
      "Discussions revolve around various vulnerabilities and concerns with NPM, GitHub, Ledger, Ethereum, and the cryptocurrency industry in general.",
      "Topics include the lack of optional signing support on NPM, criticism of NPM for not accepting community-contributed signing support, and the absence of the \"integrity\" field in the package-lock.json file.",
      "Other issues discussed are the use of PGP keys for code signing, supply chain attacks, phishing attacks, security concerns with Ledger devices, stablecoin risks, the role of automation in publishing processes, vulnerabilities in CDNs, and the need for enhanced security measures."
    ],
    "points": 234,
    "commentCount": 109,
    "retryCount": 0,
    "time": 1702561361
  },
  {
    "id": 38643369,
    "title": "Bill S-210: Protecting Children or Infringing on Internet Freedoms?",
    "originLink": "https://www.michaelgeist.ca/2023/12/the-most-dangerous-canadian-internet-bill-youve-never-heard-of-is-a-step-closer-to-becoming-law/",
    "originBody": "Michael Geist About Writing Teaching Speaking Tech Law Topics Podcast twitter mastodon mail linkedin feedburner facebook apple spotify google 2017 Freedom of Expression Awards by Elina Kansikas for Index on Censorship https://flic.kr/p/Uvmaie (CC BY-SA 2.0) News The Most Dangerous Canadian Internet Bill You’ve Never Heard Of Is a Step Closer to Becoming Law December 14, 2023 After years of battles over Bills C-11 and C-18, few Canadians will have the appetite for yet another troubling Internet bill. But given a bill that envisions government-backed censorship, mandates age verification to use search engines or social media sites, and creates a framework for court-ordered website blocking, there is a need to pay attention. Bill S-210, or the Protecting Young Persons from Exposure to Pornography Act, was passed by the Senate in April after Senators were reluctant to reject a bill framed as protecting children from online harm. The same scenario appears to be playing out in the House of Commons, where yesterday a majority of the House voted for the bill at second reading, sending it to the Public Safety committee for review. The bill, which is the brainchild of Senator Julie Miville-Duchêne, is not a government bill. In fact, government ministers voted against it. Instead, the bill is backed by the Conservatives, Bloc and NDP with a smattering of votes from backbench Liberal MPs. Canadians can be forgiven for being confused that after months of championing Internet freedoms, raising fears of censorship, and expressing concern about CRTC overregulation of the Internet, Conservative MPs were quick to call out those who opposed the bill (the House sponsor is Conservative MP Karen Vecchio). I appeared before the Senate committee that studied the bill in February 2022, where I argued that “by bringing together website blocking, face recognition technologies, and stunning overbreadth that would capture numerous mainstream services, the bill isn’t just a slippery slope, it is an avalanche.” As I did then, I should preface criticism of the bill by making it clear that underage access to inappropriate content is indeed a legitimate concern. I think the best way to deal with the issue includes education, digital skills, and parental oversight of Internet use including the use of personal filters or blocking tools if desired. Moreover, if there are Canadian-based sites that are violating the law in terms of the content they host, they should absolutely face investigation and potential charges. However, Bill S-210 goes well beyond personal choices to limit underage access to sexually explicit material on Canadian sites. Instead, it envisions government-enforced global website liability for failure to block underage access, backed by website blocking and mandated age verification systems that are likely to include face recognition technologies. The government establishes this regulatory framework and is likely to task the CRTC with providing the necessary administration. While there are surely good intentions with the bill, the risks and potential harms it poses are significant. The basic framework of Bill S-210 is that it creates an offence for any organization making available sexually explicit material to anyone under the age of 18 for commercial purposes. The penalty for doing so is $250,000 for the first offence and up to $500,000 for any subsequent offences. Organizations (broadly defined under the Criminal Code) can rely on three potential defences: The organization instituted a “prescribed age-verification method” to limit access. It would be up to the government to determine what methods qualify with due regard for reliability and privacy. There is a major global business of vendors that sell these technologies and who are vocal proponents of this kind of legislation. The organization can make the case that there is “legitimate purpose related to science, medicine, education or the arts.” The organization took steps required to limit access after having received a notification from the enforcement agency (likely the CRTC). The enforcement of the bill is left to the designated regulatory agency, which can issue notifications of violations to websites and services. Those notices can include the steps the agency wants followed to bring the site into compliance. This literally means the government via its regulatory agency will dictate to sites how they must interact with users to ensure no underage access. If the site fails to act as instructed within 20 days, the regulator can apply for a court order mandating that Canadian ISPs block the site from their subscribers. The regulator would be required to identify which ISPs are subject to the blocking order. The website blocking provisions are focused on limiting user access and can therefore be applied to websites anywhere in the world with Canadian ISPs required to ensure that the sites are rendered inaccessible. And what about the risk of overblocking? The bill not only envisions the possibility of blocking lawful content or limiting access to those over 18, it expressly permits it. Section 9(5) states that if the court determines that an order is needed, it may have the effect of preventing access to “material other than sexually explicit material made available by the organization” or limiting access to anyone, not just young people. This raises the prospect of full censorship of lawful content under court order based on notices from a government agency. If that isn’t bad enough, there are two additional serious concerns. First, the bill is not limited to pornography sites. Rather, it applies to any site or service that makes sexually explicit materials available. This would presumably include search engines, social media sites such as Twitter, or chat forums such as Reddit, where access to explicit material is not hard to find. If the bill was limited solely to sites whose primary purpose is the commercial distribution of sexually explicit material, it might be more defensible. As it stands now, the overbroad approach leaves this bill vulnerable to constitutional challenge. Second, consider the way sites are supposed to comply with the law, by establishing age verification systems. This effectively means that sites will require their users to register with commercial age verification systems in order to run a search or access some tweets. And the age verification systems raise real privacy concerns, including mandated face recognition as part of the verification process. Senate private members bills rarely become law, but this bill is suddenly on the radar screen in a big way. The bill should not have come this far and should not be supported. Creating safeguards for underage access to inappropriate content is a laudable goal, but not at the cost of government-backed censorship, mandated face recognition, and age-approval requirements to use some of the most popular sites and services in the world. Share this: Click to share on Facebook (Opens in new window) Click to share on Twitter (Opens in new window) Click to share on LinkedIn (Opens in new window) Click to share on Reddit (Opens in new window) Click to share on Tumblr (Opens in new window) Click to share on Pinterest (Opens in new window) Click to share on Pocket (Opens in new window) More Related posts: No related posts. Tags: Censorship / miville-duchene / s-210 / vecchio / website blocking Share this post Tweet 19 Comments Gogo says: December 14, 2023 at 8:45 am Thanks for continuing to push back against all the nonsense, Michael. Reply Anne says: December 14, 2023 at 10:32 am These strategies will provide you $3,000 overnight, but you’ll fulfill your financial needs with reliable exertion. In case you’re in require of $3,000 quick, ig you may reach your objective speediest by taking on more than interior of the below. Here details…… https://financialrichmethods82.blogspot.com Reply Brian says: December 14, 2023 at 11:32 am For all of the good that it sadly doesn’t seem to do. This government, the one that campaigned on transparency only runs sham/theatre public consultations that make it look like it wants to hear what Canadians have to say, but in reality, it doesn’t care and has it’s mind made up about what legislation will be before any Canadian has any say. They do it at their own peril even. C-18 is a perfect example of this. Reply Curtis Magyar says: December 14, 2023 at 12:09 pm Did you even read the article? This not a bill introduced by the Liberals or even supported by them. It is the conservatives who are supporting this censorship. Reply Brian says: December 14, 2023 at 12:16 pm Of course I did. The problem is that this silly blogging platform seems to lose reply context from time to time. My comments were in reply to the previous comments by Gogo, not the content of the article. This platform failed to stagger my comments under that comment despite my clicking the Reply under Gogo’s comment. Reply Kevin says: December 14, 2023 at 5:41 pm Actually, the bill is supported by some Liberals, backbenchers. It is also supported by the NDP and Bloc. Reply Chris says: December 14, 2023 at 11:41 am The woman in charge of introducing this bill is literally named Karen. You can’t make this stuff up. I’d love to know if we can get all private records of all of her internet search history to give her a taste of what this bill can leverage. The entire argument seems to be “Won’t SOMEONE PLEASE THINK OF THE CHIIILDREN!!” A nation full of pathetic Helen Lovejoys. Reply Brian says: December 14, 2023 at 12:21 pm Yes, and unfortunately that cry to save the children is automatically turned around as being that you are a child molester if you oppose anything that is supposed to save them. So if you can attach a cry to save the children to something, anything, you are likely to get your way out of fear of being accused of being a pedophile if you oppose it. Reply Curtis Magyar says: December 14, 2023 at 2:01 pm That’s exactly what the previous CPC government (which poilievre was in) did. Remember when they tried to force ISPs to spy on us and keep the records. Then when we complained they said we’re either with them or were with the child Pornographers. And it was Vic Toews who said it. Same guy who left his wife and family to be with the babysitter. You couldn’t even make this stuff up. Now he’s a judge, appointed by Harper. Reply Kevin says: December 14, 2023 at 5:48 pm But, but, the Conservatives… The Liberals are no better. Look into the history of the current Minister of National Defence Bill Blair, in particular with the G20 in Toronto and “Project Safe City” in Toronto. In the case of the latter, the TPS were mining the databases of the Canadian Firearms Center looking for residents of Toronto whose PAL has expired. Within days of the database indicating it has expired the Guns and Gangs Unit was showing up at the persons house to confiscate the firearms. The problem? The CFC was notorious for being slow at processing the renewals, sometimes taking over 6 months to do so. Most people I know with a PAL at the time would submit via registered mail 6 months before it expired. Reply Dan says: December 14, 2023 at 11:41 am This is a solved problem with the CIRA DNS servers. It requires parents to make a choice and configure devices, but that’s it. No facial recognition, no activity logging, no credit cards. https://www.cira.ca/en/canadian-shield/ Reply Kyle says: December 14, 2023 at 8:33 pm Cool! I didn’t know about the CIRA DNS, but that’s such a reasonable solution. Thank you for sharing! Reply Chris S says: December 14, 2023 at 8:36 pm You are correct – it does exactly what you describe it as doing – IF you configure all the right devices the right way AND prevent changes. I think it is a little incomplete. I use a subscription DNS – but I also block 53 at the router perimeter. And all of the above will not stop a curious user who has heard about using DoH, which is almost easier to enable than all of CIRA’s instructions. The trend I see here (and definitely in C-11, and to some extent in C-18) is that the government – apparently including the Official Opposition – does not trust Canadians. You and I expect that Canadians need to be trusted in these matters, and all of the parties appear to think they know better. Reply Bruce says: December 14, 2023 at 12:13 pm While analogs to Tipper Gore may be apt, the implications are more than a label on a CD. And the problem is indeed solved by making this available through CIRA by those who want to. I once knew someone who ran a web based message board for support of a software product. It was hacked and ‘sexually explicit material’ was placed on there, and it damaged the real content that was supposed to be there. It was not even a week out of date — the security safeguard customizations to meet US COPPA conformance made it impossible to patch quickly. These laws are often at loggerheads with one another and make it impossible to solve the non-intersecting solution set by the conditions these lawmakers seek to codify. Therein lies the problem with codification vs common law, redundant lawmaking, and the jurisdictional conundrum encountered by those who seek to comply? Reply Pingback: The Most Risky Canadian Web Invoice You’ve By no methodology Heard of Is a Step Closer – TOP HACKER™ Drew says: December 14, 2023 at 4:11 pm What is the best way to push back against this bill for the everyday citizen? Reply Julia says: December 14, 2023 at 5:08 pm I just got paid 7268 Dollars Working off my Laptop this month. And if you think that’s cool, My Divorced friend has twin toddlers and made 0ver $ 13892 her first m0nth. It feels so good making so much money when other be02 people have to work for so much less. This is what I do………> > > https://careersrevenue123.blogspot.com/ Reply Chris S says: December 14, 2023 at 8:08 pm This appears to be founded on a dangerous misunderstanding of how the modern web works. This gets a bit technical. First – cloud hosted websites are increasingly the norm. There is no one-to-one between a web site and an IP address; there may be hundreds of websites behind one IP address. The site is determined by the Server Name Indication field. The direction this is going is to encrypt the SNI as well as the webpage data. There is no way for an ISP to know the website you are reaching, and no way to block just one website’s IP address, or even a few. You may block hundreds in the attempt to block one. Second, the Domain Name System (DNS) now has DNS-over-HTTPS (DoH), so that all requests to transform a name into an IP address can look like webpage requests. The ISP could try and block a few of those, but since the DNS does not actually provide content, that might get legally tricky. And – since DoH requests look like webpage requests, the ISP may not even be aware you are using a DNS service. There is a pithy 30-year-old quote for this… “The Net interprets censorship as damage and routes around it.” – John Gilmore Reply Quordle says: December 15, 2023 at 2:04 am I merely desired to inform you that I perused your website and discovered it to be quite edifying and intriguing. Reply Leave a Reply Your email address will not be published. Required fields are marked * * * * Law Bytes Audio Player Law Bytes Episode 188: Consumers, Competition or Corporate Cash Grab? – My Bill C-11 Appearance at the CRTC Change Playback Rate 1x 0.8 1 1.2 1.5 2 Go to previous episode Skip Backward Play Pause Jump Forward Skip to next episode Download Share This Episode Facebook Twitter Linkedin Copy episode link Copied Download 00:00 00:00 00:00 Episode 188: Consumers, Competition or Corporate Cash Grab? – My Bill C-11 Appearance at the CRTC byMichael Geist Search EpisodesClear Search Episode 188: Consumers, Competition or Corporate Cash Grab? – My Bill C-11 Appearance at the CRTC December 11, 2023 Michael Geist Episode 187: Jeff Elgie on What the Bill C-18 Deal with Google Means for the Future of the Canadian News Sector December 4, 2023 Michael Geist Episode 186: Andy Kaplan-Myrth on the CRTC’s Last Ditch Attempt to Fix Canada’s Internet Competition Problem November 27, 2023 Michael Geist Episode 185: Bill C-11 at the CRTC – A Preview of the Upcoming Online Streaming Act Hearing November 20, 2023 Michael Geist Episode 184: Philip Palmer on the Constitutional Doubts About the Government’s Internet Laws November 13, 2023 Michael Geist Load More Search Results placeholder Previous Episode Show Episodes List Next Episode Show Podcast Information Law Bytes – Subscribe apple spotify goodreads stitcher tunein rss Recent Posts The Most Dangerous Canadian Internet Bill You’ve Never Heard Of Is a Step Closer to Becoming Law The Law Bytes Podcast, Episode 188: Consumers, Competition or Corporate Cash Grab? – My Bill C-11 Appearance at the CRTC My CRTC Appearance on Bill C-11: Why Isn’t the Commission Concerned with Competition, Consumer Choice, and Affordability? The Law Bytes Podcast, Episode 187: Jeff Elgie on What the Bill C-18 Deal With Google Means for the Future of the Canadian News Sector Skillful Negotiation or Legislative Fail? Taking Stock of the Bill C-18 Deal With Google Recent Talks × × Open Books Law, Privacy and Surveillance in Canada in the Post-Snowden Era (University of Ottawa Press, 2015) The Copyright Pentalogy: How the Supreme Court of Canada Shook the Foundations of Canadian Copyright Law (University of Ottawa Press, 2013) From “Radical Extremism” to “Balanced Copyright”: Canadian Copyright and the Digital Agenda (Irwin Law, 2010) In the Public Interest: The Future of Canadian Copyright Law (Irwin Law, 2005) . Michael Geist on Substack Get Postings via Email Get new posts by email: Subscribe Broadcasting and Telecom Legislative Review Panel Report (BTLR) Jump to... The Broadcast Panel Report and Canadian Stories: Take the Cancon Quiz CBC Leads Call for New Government Regulations to Support \"Trusted\" News Sources Broadcast Panel Commissioned Report Found Canada Ranks First Among Peer Countries in Spending on TV Production, Domestic TV Production, and Employment Per Capita The BTLR and USMCA, Part Two: Why the Broadcast Panel Recommendations Could Cost Canadians Millions in Retaliatory Tariffs The BTLR and USMCA, Part One: Why the Broadcast Panel Recommendations Conflict With Canada's Emerging Trade Obligations Higher Costs and Less Choice: Why Consumers Will Pay the Price for the Broadcast Panel's Plans to Increase Costs of Internet Services and Sites The LawBytes Podcast, Episode 38: Debating the Broadcast Panel Report – A Conversation with BTLR Panel Chair Janet Yale The Broadcast Panel Report and Discoverability of Canadian Content: Searching for Evidence of a Problem Not Neutral: Why the Broadcast Panel Report Weakens Net Neutrality in Canada Weak Walk-Back: Why Steven Guilbeault's Reversal on Government Licensing News Sites Still Leaves a Huge Regulatory Structure in Place Canadian Heritage Minister Steven Guilbeault on Regulating Foreign News Sites: \"What's the Big Deal?\" The CRTC Knows Best: Panel Report Recommends Costly Overhaul of Canadian Communications Law to Regulate Internet Sites and Services Worldwide A Demonstrably False Premise: Why \"Inevitable\" Canadian Internet and Cancon Regulations Won't Level the Playing Field, Support Canadian Stories or Save a Thriving Industry Archives December 2023 S M T W T F S1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31« Nov Michael Geist mgeist@uottawa.ca This web site is licensed under a Creative Commons License, although certain works referenced herein may be separately licensed.",
    "commentLink": "https://news.ycombinator.com/item?id=38643369",
    "commentBody": "Canadian internet bill S-210 is a step closer to becoming lawHacker NewspastloginCanadian internet bill S-210 is a step closer to becoming law (michaelgeist.ca) 220 points by llm_nerd 17 hours ago| hidepastfavorite214 comments TriangleEdge 15 hours agoI worked at a go kart racing track when I was a teen and the company installed something called \"governors\" that would limit the speed at which the go karts could go. The governors had a remote control which allowed an operator to selectively limit the speed of each individual go kart. As a 14 year old, if someone did a slight playful bump into another racer, I would govern them. It&#x27;s the first instance of \"power\" I had over others. And.., I abused it for the \"good\".This being said, I have a belief that if you give someone a button to ruin someones life, some people will push the button with thoughtless abandon.Correct me if I&#x27;m wrong, but I&#x27;m not seeing a democratic pushback mechanism in this bill, and I think this is a disaster. This is also being said in the context that the most voted petition in Canadian history is e-4701, which is a vote of no confidence. reply tzs 12 hours agoparent> This being said, I have a belief that if you give someone a button to ruin someones life, some people will push the button with thoughtless abandon.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Button,_Button_(The_Twilight_Z... reply halfcat 11 hours agorootparentAlsohttps:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Stanford_prison_experiment reply Tanoc 5 hours agorootparentThe Stanford Prison Experiment has been discredited and debunked for almost a decade now. It&#x27;s interesting to me how little attention the discrediting got and how many people still point to it as a real unguided trial because of that. reply whythre 2 hours agorootparentI think part of Stanford’s hardiness is due to the fact that it ‘feels true.’ We have all had our chains yanked by petty tyrants… even if the science was rotten.You are right about it not being properly discredited, though- I remember it being featured and commonly cited in textbooks; something that big and ‘influential’ is going to take a long time to die. reply some_random 11 hours agorootparentprevSPE was bullshit, Zimbardo directed participants to get the outcome he wanted. reply alephnerd 9 hours agoparentprevThis proposed law, and similar ones across the English speaking work (Australia, UK, NZ, US) are because of lobbying by Baroness Beeban Kidron [0] for a decade.An earlier comment of mine:Baroness Beeban Kidron has been lobbying for stringent anti-CSAM measures in tech for years [0].She&#x27;s lead a major pressure campaign in the US, Canada, and the UK for years [1]. Her charity 5Rights and WeProtect both have been able to back Labour and the Tories so she&#x27;s able to lobby across the aisles. It doesn&#x27;t hurt that the publishing company her parents founded (Pluto Press) has a strong niche in the political space.The Molly Russell suicide also played a role [2], which she leveraged to highlight the need for restrictive anti-CSAM measures, especially as it became a top tabloid story in the UK.[0] - https:&#x2F;&#x2F;www.politico.com&#x2F;news&#x2F;2023&#x2F;06&#x2F;14&#x2F;british-baroness-on... reply whythre 2 hours agorootparentA crusader… hard to combat someone like that. I am sure she is convinced all her meddling is making the world a better place. reply winter_blue 15 hours agoparentprev> the most voted petition in Canadian history is e-4701, which is a vote of no confidenceMost Liberal Party members voted against this bill (S-210 [1]). Support for this bill comes from the remaining parties, ie: (1) the Conservative Party, (2) the NDP (ie Canada’s semi-socialist progressive party), and (3) Bloc Quebocois (Quebec’s nationalist party).e-4701 [2] is a petition introduced by a conservative. Conservatives in Canada have a vested interest in pushing out Justin Trudeau, so there are no surprises there.[1] https:&#x2F;&#x2F;www.ourcommons.ca&#x2F;Members&#x2F;en&#x2F;votes&#x2F;44&#x2F;1&#x2F;609[2] https:&#x2F;&#x2F;www.ourcommons.ca&#x2F;petitions&#x2F;en&#x2F;Petition&#x2F;Details?Peti... reply citrusybread 15 hours agoparentprevdoes that really shock you though?e-4701 has less to do with the actual popularity of the parties or the system than it does with the fact that we&#x27;re 8 years in with a fairly progressive leader. c.f. the annual anti-Trudeau demonstrations that are astroturfed (yellow vests, \"freedom convoy 2022\", united we roll, etc). This system didn&#x27;t exist when Harper was PM or anyone before it; and we&#x27;re hardly at Mulroney level of discontent.edit: also... the bill in question is opposed by the Liberal party at large (the ones who this petition oppose). it&#x27;s mainly being pushed by the Conservatives and the Bloc (which makes for an odd union already) as well as the NDP (which pushes it completely into nutty territory of voting patterns). and it was introduced in the Senate - where most Senators do not have party affiliation. reply 99_00 15 hours agoparentprev>This being said, I have a belief that if you give someone a button to ruin someones life, some people will push the button with thoughtless abandon.What&#x27;s the button in case of this legislation? reply bonestamp2 13 hours agorootparentThe button is: loss of privacy and anonymity for everyone, even those who the law is not suppose to be targeting. reply TriangleEdge 15 hours agorootparentprevThe button is censorship. reply civicduty 15 hours agorootparentIt&#x27;s not censorship, it&#x27;s age verification. You can still access this stuff if you can prove you&#x27;re an adult. Same as how children aren&#x27;t allowed to buy the same material in stores. It&#x27;s still being published, there&#x27;s no censorship. reply natoliniak 15 hours agorootparent> any site or service that makes sexually explicit materials availableso basically, the internet.> Canadian ISPs required to ensure that the sites are rendered inaccessibleAt best, this is regulatory capture for the current tech giants, at worst, basically ability to hand pick who gets to see what sites. So yes, censorship under the cloak of \"age verification\" and \"protecting kids\". We have heard it all before. I&#x27;m surprised they didn&#x27;t somehow stuff the \"terrorism\" angle in there as well. reply 99_00 12 hours agorootparent>At best, this is regulatory capture for the current tech giants, at worst, basically ability to hand pick who gets to see what sites.It hasn&#x27;t happened with any other censorship bill Canada has passed.This includes laws on pronoun use:Canada’s gender identity rights Bill C-16 explained>through a process that would start with a complaint and progress to a proceeding before a human rights tribunal. If the tribunal rules that harassment or discrimination took place, there would typically be an order for monetary and non-monetary remedies. A non-monetary remedy may include sensitivity training, issuing an apology, or even a publication ban, he says.https:&#x2F;&#x2F;www.cbc.ca&#x2F;cbcdocspov&#x2F;features&#x2F;canadas-gender-identi... reply bccdee 2 hours agorootparentThat&#x27;s not a censorship bill, that&#x27;s an anti-harassment bill. Harassment is illegal everywhere: I&#x27;m not free to follow you around calling you an asshole. I could get charged for that, especially if you&#x27;re my employee, tenant, or in the presence of other exacerbating factors. Canadian hate law says I&#x27;m not free to follow you around making disparaging comments about your race. C-16 expands that to say that I&#x27;m not allowed to follow you around disparaging your gender identity. That&#x27;s it.This bill, conversely, gives the government explicit power to block websites that host content that is not child-appropriate. Completely different. reply happyopossum 15 hours agorootparentprevRequiring an onerous age verification scheme provided by government approved vendors is a lot closer to censorship than it isn&#x27;t.Say you post stuff to your own blog, and sometimes use colorful language. A parent decides to report you to the regulatory agency, and now you have 20 days to do whatever they demand you do to remediate, or else your site will be blocked at the ISP level. reply AnthonyMouse 13 hours agorootparentThat&#x27;s not even the half of it.In order to have age verification, you need identity verification, i.e. tying your identity to your activity. Classic chilling effects. If you&#x27;re in the closet because you come from a religious family or have a religious boss, can you risk some random site or government bureaucracy getting hacked and outing you? Strong anonymity is essential for free expression.Then the requirement to identify yourself is friction, so sites will want to avoid it, which can only be accomplished through censorship. Ordinary sites not solely focused on X-rated content will be \"moderated\" down to the level of children, even when they have adult audiences, because they don&#x27;t want to be locked behind the porn filter.Instead of having diverse communities tailored to all different kinds of people and ideas, you bifurcate the world into nerfed risk-averse corporate censorship and explicit smut. The only place you&#x27;re allowed to have an adult conversation is Pornhub, which is not exactly known for quality intellectual discourse. reply tzs 11 hours agorootparent> In order to have age verification, you need identity verification, i.e. tying your identity to your activityI don&#x27;t see any reason age verification has to tie your identity to your activity. It should be possible with modern cryptographic techniques to make a system whereby the service that checks your age doesn&#x27;t find out what site the check is for, and that site doesn&#x27;t find out who you are. reply AnthonyMouse 11 hours agorootparentThere are two possibilities.One is you get a unique token which can be tied back to your identity by someone who compromises the issuing service, so if they get compromised you&#x27;re screwed.The other is you get a generic token or one that otherwise can&#x27;t be tied back to a specific identity, in which case the token leaks and there is no way to trace back who is leaking it, creating a generic bypass of the whole system. reply tzs 10 hours agorootparentPorn site issues you a unique token. You have an age verification service sign that token using a blind signature. You return the signed token to the porn site. The porn site can verify the signature using the public key of the verification service. reply AnthonyMouse 9 hours agorootparentVerification service gets compromised, attackers record and eventually publish mapping between unique verification tokens and users, now all the porn sites or anyone who has compromised any of them can have stored its verification tokens and know who all the users are. reply tzs 8 hours agorootparentThe verification service doesn&#x27;t see the token from the porn site. The user takes the token from the porn site and from that generates a blinded token. The blinded token is what is sent to verification service. They sign it and return it.It is the user that then generates the signed porn token, by applying the unblinding function to the signed blinded token.If the blinded tokens leak the porn sites cannot match them up with the tokens they issued because they don&#x27;t have the parameters to unblind them. replybccdee 15 hours agorootparentprevRead the article ( `–` )> The website blocking provisions are focused on limiting user access and can therefore be applied to websites anywhere in the world with Canadian ISPs required to ensure that the sites are rendered inaccessible. And what about the risk of overblocking? The bill not only envisions the possibility of blocking lawful content or limiting access to those over 18, it expressly permits it. Section 9(5) states that if the court determines that an order is needed, it may have the effect of preventing access to “material other than sexually explicit material made available by the organization” or limiting access to anyone, not just young people. This raises the prospect of full censorship of lawful content under court order based on notices from a government agency. reply 99_00 12 hours agorootparentExplain how this is censorship. reply rstat1 11 hours agorootparentYou want an explanation about how a law intended to block access to something is censorship?Really? reply 99_00 8 hours agorootparentI guess your right, it is censorship.And so are laws against tobacco advertising.As are laws in Canada against hate speech, harrasment missgendering and Holocaust denial.I guess censorship gets a bad wrap. reply bccdee 2 hours agorootparentIt&#x27;s silly to contrast a narrow restriction on speech like \"you can be charged for making statements which constitute harassment\" or \"this specific product cannot be advertised in certain ways\" with a massive restriction on speech like \"any website that does not check users&#x27; IDs can be blocked at an ISP level if it is found to have content on it which is not appropriate for children.\"If you like free speech, am I \"free\" to \"speak\" loud screams directly into your ear? Am I free to speak lies to you about the safety features in my airplane, when you&#x27;re buying tickets? Free speech absolutism is a childish position; what crosses a line is subjective.I think any reasonable person would agree that this internet blocking regime crosses a line and unfairly stifles people&#x27;s ability to communicate. Not every public space should be obliged to be child-appropriate; obscenity laws are best left in the 20th century. reply protocolture 11 hours agorootparentprevExplain how it isnt lmao.It uses a one drop rule to test websites. Once theres a teensy bit of adult content, and as has been pointed out this covers things like googles unsafe search modes, then the requirement is to block first and ask questions later.Requiring people to be licensed or verified to access content is as much censorship as blocking it entirely. There are valid reasons to not want to be on a conservative governments list of porn users. You need to expose yourself to risk to access content? Censorship. replyxemoka 16 hours agoprevHow the second reading vote went, so you know who to call and email, https:&#x2F;&#x2F;openparliament.ca&#x2F;votes&#x2F;44-1&#x2F;609&#x2F;And everything said about it and the status in the house: https:&#x2F;&#x2F;openparliament.ca&#x2F;bills&#x2F;44-1&#x2F;S-210&#x2F; reply charles_f 14 hours agoparent> If someone is using a VPN, they can go in any country, so it is going to be bypassing some of thatI love how the initial stated goal of the bill is to prevent involuntary access to pornography by children, but they&#x27;re looking at how to prevent circumvention using VPNs. reply charles_f 14 hours agoparentprevWhat if my MP already voted no? Do you pick random MPs and email them? reply chimeracoder 13 hours agorootparent> What if my MP already voted no? Do you pick random MPs and email them?Call and tell them you support that decision (and, just as importantly, why).They&#x27;ll be receiving pressure from both sides, and it&#x27;s important to demonstrate that people continue to care about their opposition. reply squigz 6 hours agorootparentThis isn&#x27;t advice you hear often, but it&#x27;s very good. reply civicduty 15 hours agoparentprevnext [2 more] [flagged] xemoka 15 hours agorootparentGreat, I&#x27;m super happy you&#x27;re using this opportunity to engage in the political system! Goal achieved.I hope you find lively debate with the members who disagree with this bill, for both of our sakes. reply ayakang31415 8 hours agoprevKorea already has laws that are similar to the bill S-210, and it is extremely frustrating when I am visiting there. Every website that I want to use without any tracking is blocked by the South Korean government, thus I have to use VPN to use certain websites otherwise unavailable. reply grammers 15 hours agoprevWhen will they ever stop? The moment we (=the people) have fought one surveillance law, they (=politicians) simply come up with another. Until we are too tired to fight anymore... reply notnullorvoid 7 hours agoparentI think blame may lie more on lobbyists than politicians. Though the fact that politicians haven&#x27;t fixed the problems with lobbying doesn&#x27;t look so good either. reply imchillyb 15 hours agoparentprevThat’s their _politicians_ strategy. It’s a time worn political stratagem. Keep pushing the same bill through with a different name, different sponsors, and at a time few are paying attention. reply SV_BubbleTime 13 hours agorootparentYou have to win every time, they only need to win once. reply Nasrudith 14 hours agoparentprevThere has to be some sort of punishment for them to stop. Otherwise they&#x27;ll just repeat it. If they find themselves consistently primaried or voted out for supporting bills they&#x27;ll get the message. reply goosedragons 13 hours agorootparentThere&#x27;s no voting this wacko out. The lady who keeps shoving these bills out (or onto other bills) is an unelected senator that is there til she&#x27;s 75 or does some gross misconduct or something. reply cebert 16 hours agoprevThe good old it’s for the children position. reply captainkrtek 16 hours agoparentPick one of:- National Security- Protect the kids reply enasterosophes 15 hours agorootparentDon&#x27;t forget:- Money launderers- Contraband distributors reply sgift 15 hours agorootparentEvil people are out to get YOU or YOUR KIDS. Give us access, we protect you!There. I generalized it. :( reply enasterosophes 14 hours agorootparentAh, when the powers of the Four Horsemen combine! reply munk-a 14 hours agorootparentprevYou wouldn&#x27;t steal a car! reply pookha 14 hours agorootparentprevSTAY SAFE! reply barbazoo 15 hours agoparentprevExcept that in this instance I can actually see the merit but obviously YMMV. I had access to porn since as far back as I can remember. In hindsight I wish something like this had been in place. reply kredd 15 hours agorootparentAlso grew up with unrestricted internet access, I recall everyone in middle&#x2F;high school sharing LiveLeak links through MSN. Sure, it desensitized us, maybe “survivor bias”, but if I quickly think of everyone I know, we’ve all ended up as semi-functioning adults.What I’d rather see governments do — figure out how to solve decreasing attention span in kids. They’ll eventually watch porn anyways, but tolerating anything that lasts more than 5 minutes is more important. reply snarf21 15 hours agorootparentprevDon&#x27;t forget that this was once considered pornography: https:&#x2F;&#x2F;rarehistoricalphotos.com&#x2F;woman-ticket-wearing-bikini...Who will watch the watchers? reply jstarfish 13 hours agorootparent> Who will watch the watchers?The voyeurs. reply barbazoo 14 hours agorootparentprevThe populace electing lawmakers. reply rglover 14 hours agorootparentprevKids will get access if the really want to. I remember downloading \"that scene\" from American Pie over 1mbps using KaZaa back in 2001.The only real solution is as others have suggested here: parents being more attentive to what their kids are browsing and making an effort to put in safeguards.Like the article states, this law is an avalanche for abuse of power (that guaranteed will take place). reply cpncrunch 13 hours agorootparentThese days it&#x27;s even easier with VPN and secure DNS. reply nmz 15 hours agorootparentprevI also always had access to it but I never saw the harm in it, always knew it was unrealistic, the same way an action movie is.FWIW Instead of protections that will be eventually breakable, What I would like is more a mandatory disclaimer saying that everything about it is unrealistic and that its pure entertainment. reply ipaddr 15 hours agorootparentprevIf it did, the question is would you still be opposed? Is hiding knowledge better than allowing it? I think without knowledge you might have the opposite opinion reply _whiteCaps_ 15 hours agoprevMy local MP voted Nay - I should send him an email to thank him for that.https:&#x2F;&#x2F;www.ourcommons.ca&#x2F;members&#x2F;en&#x2F;votes&#x2F;44&#x2F;1&#x2F;609?view=pro... reply rixthefox 14 hours agoprevThe one question I want to ask these government officials is \"Where are the parents?\"If underage children are accessing explicit content, it is the parents, not the websites that are at fault. Why are you handing children devices that are not parental locked to only permit applications that have been approved by the parent?If the parent fails to properly vet the apps they allow their children to use then it is the parent that needs to be questioned. Pushing this requirement onto websites and services is moving the goal posts. If the parents are too technically inept or unwilling to police their children&#x27;s devices then they should not be handing them to their children. reply FirmwareBurner 14 hours agoparent>\"Where are the parents?\"Blaming the government for whatever their child does. It seems most parents, they have outsourced the primary education of their kids to the government and tiktok.I told one such parent that the government is only in charge of educating your child to get a job, not teaching him basic behavior, values and life lessons, and that parent said \"no, I pay taxes, I expect the government to teach him everything\". Nuff said. reply dingnuts 13 hours agorootparentWhy is this surprising? It&#x27;s a society where the government is expected to step in and take care of you at every turn, from birth to death. That&#x27;s what welfare is for, and free health care, and food stamps, government housing, government education, subsidized early child care, all the way to assisted death.If the goal is to design a society where the government is responsible for everything, why does the population need to be responsible? reply x-complexity 7 hours agorootparent> If the goal is to design a society where the government is responsible for everything, why does the population need to be responsible?Those people that want that sort of society shouldn&#x27;t be allowed to do anything without explicit approval from a government-mandated device that tells them every single action that they should do, even breathing.They want a government that&#x27;s responsible for everything, then they get to live that idea: If they even do a single action that wasn&#x27;t told via their device, straight to jail as a &#x27;deviant&#x27;. reply mulmen 14 hours agoparentprevDon’t get distracted by the red herring. This isn’t about protecting kids. It’s about power and control. reply apantel 11 hours agorootparentIt’s _always_ about power and control. reply whynotmaybe 9 hours agoparentprevHow to say you don&#x27;t have kids without saying you don&#x27;t have kids.Unless you&#x27;re the perfect kid, did you forgot what you did when you were a kid that you parent didn&#x27;t agree on?Kids are full of life and as the saying goes, life finds a way. reply hn_acker 7 hours agorootparent> How to say you don&#x27;t have kids without saying you don&#x27;t have kids.> Unless you&#x27;re the perfect kid, did you forgot what you did when you were a kid that you parent didn&#x27;t agree on?But just because child you unwittingly stumbled upon something a child shouldn&#x27;t see doesn&#x27;t mean that the website was at fault. Just because you got hurt doesn&#x27;t always mean that someone should be sued. reply tzs 11 hours agoparentprevInternet is getting pretty easy to find access to. We may be past the point where parents can limit their kids to only accessing the internet through devices that the parents control. reply phatskat 2 hours agorootparentIt shouldn’t be about limiting access in the first place. The truth is that parents need to have those uncomfortable and direct conversations with their children and help guide them to make healthy decisions. reply nvy 12 hours agoparentprev>The one question I want to ask these government officials is \"Where are the parents?\"To be clear, this bill originated outside the government. The Canadian Conservative Party do not currently form the government. They are ideologically-driven religious fundamentalists who don&#x27;t care \"where the parents are\". For them it&#x27;s \"porn == bad\" and \"government oversight == bad except if it&#x27;s on an issue that my ideology happens to align with\". reply amtamt 12 hours agoparentprevWhere are \"proper parental control\" implemented? reply mensetmanusman 13 hours agoparentprevFamily structure is devolving away from two stable parents, so the state is required to step in. reply Eumenes 14 hours agoparentprevAttributing blame to parents for anything in 2023, especially their internet or device usage, is incredibly faux pas. reply rixthefox 14 hours agorootparentI expect people in 2023 to understand parental controls. reply ndriscoll 13 hours agorootparentWhich parental controls would, say, allow a kid to visit &#x2F;r&#x2F;roblox but not &#x2F;r&#x2F;nsfw? Best I can tell, Firefox literally does not have any parental controls built in, and they have a note that extensions to provide it are easily bypassed.It seems reasonable to me to at least start requiring sites and commercial software vendors to build a system that makes parental controls possible. And if a commercial site like reddit is going to have child focused content like a Roblox forum, then it should be easily separated from porn without requiring parents to set up a MITM proxy with complicated filtering rules.There are simple privacy and (adult) autonomy preserving ways filtering or access control could be done, but after 30 years it still seems to be an impossible task. Chuck-E-Cheese doesn&#x27;t have a porn theatre in the back where an employee just asks if you&#x27;re 18 and accepts any 7 year old saying yes. reply Eumenes 12 hours agorootparentreddit is a big no no for children. its full of porn, politically charged topics, and predators. you can also use wildcards in a dns sinkhole to target keywords perhaps. reply ndriscoll 12 hours agorootparentI don&#x27;t disagree, but also I don&#x27;t think it&#x27;s unreasonable that a site like that which also hosts children&#x27;s forums is going to have parents wanting regulation.Pornhub doesn&#x27;t have a part of the website for minor teenagers or Minecraft. Neopets doesn&#x27;t have a porn or gore section. Even 4chan separates their SFW and NSFW boards onto different sites (though obviously even their SFW boards are not child friendly and don&#x27;t pretend to be).You can&#x27;t do anything at the DNS level for reddit (short of blocking it all) because it&#x27;s all one site. And it uses TLS, so you&#x27;d need to MITM to do partial filtering, which is beyond the capability of most people. I assume Instagram and tiktok are similar&#x2F;even harder to filter.Porn sites at least used to ask for credit cards. Now they just ask yes&#x2F;no are you 18, and they have children&#x27;s sections. They should really be working to clean up their act in a privacy&#x2F;autonomy friendly way (e.g. through labeling and partnering with browsers) before they&#x27;re forced into these kinds of laws. Or stop targeting children as a market and ban anyone that hints that they are under 18. reply Eumenes 10 hours agorootparentSome consumer routers may have the ability to filter URLs I think. I&#x27;m not really experienced in networking&#x2F;web technologies. The reddit schema seems easy enough to understand but yeah IG&#x2F;tiktok, seem impossible. Could you force a particular build of chrome that has extensions baked in for blocking sites&#x2F;urls? Maybe an endpoint management tool to keep those applications in check. Something like Jamf. reply jstarfish 13 hours agorootparentprevWe can&#x27;t even successfully curtail adverse behavior in the enterprise, with budgetary spends for DLP and communications monitoring measured in the millions.Parents don&#x27;t stand a chance. You could be running a fucking kiosk with only Minecraft on it. If it still has an internet connection, kids will be harassed, bullied and solicited.It&#x27;s the kids who taught me about the existence and purpose of Finstagram accounts.The only effective parental control is unplugging them from the internet. But you can&#x27;t do that, because our lives are inextricable from it now. reply kazinator 2 hours agoprevThere is another problem there: since the legislation would apply to so many kinds of sites it would have the effect of cutting off young people from services they rely on. Sites which don&#x27;t mainly host explicit content. It could have life-long economic repercussions for disadvantaged youth. Very evil and stupid, these conservative shits. reply spacemanspiff01 13 hours agoprevWhat not just have site that are serve explicit things advertise that. Have a large scale Blocklist, and if you serve explicit content, then you need to be on there.Then have a option when people sign up for internet to block all the IPs associated with explicit stuff.Adults and parents can choose \"yes or no\" when they pay for internet and then be done with it.If children are still getting access, then their either technically savvy, which nothing will stop. Or an adult in their life is allowing unblocked internet.Just make ip filtering easy and default so that parents can set it up without having to understand anything. It should be a required thing when you request internet access \"do you want to filter out explicit content\"Some sites that have dual content may have to have separate IP addresses for non-explicit and explicit&#x2F;non explicit. But that is relatively easy. And if they do not care, then they will by default end up on explicit list. reply ndriscoll 10 hours agoparentA couple problems with that are that people will disagree on what is inappropriate for children (e.g. drugs, nudity, sex, simulated violence, real violence, simulated gore, real gore, foul language, educational material that&#x27;s sexual in nature, hate speech, advertising), and parents may want unfiltered or differently filtered access for themselves while blocking it on a device&#x2F;profile the child uses.I imagine a huge firewall list is probably also technically challenging for ISPs unless there&#x27;s some convention for IPs (like even&#x2F;odd LSB).But you can just have the site serve a content labels header. Specify what the content is, with a standard for topics which may be commonly considered inappropriate for children, and let parents configure controls at the OS level, which the browser should respect. This lets you even provide the info at the level of individual resources (so e.g. wikipedia could label images or pages with topics like nudity or violence, and parents could opt to allow&#x2F;block topics). You could also have a coarse header indicating that the site does not want minors to access it, and that could be an option for the browser to block. reply 20after4 8 hours agorootparentI don&#x27;t like any form of censorship &#x2F; filtering but this is by far the most reasonable way to do it if it must be done. reply redm 16 hours agoprevProtrcting privacy worked out so well with these cookie banners, I cant imagine what could go wrong with this… reply 99_00 15 hours agoparentIf you feel that the company you are getting porn from isn&#x27;t respecting your privacy and your privacy is important, don&#x27;t get porn from them.The bill restricts distribution of porn to kids. It&#x27;s up to the distributor to figure out how to do that. reply mulmen 14 hours agorootparentNo, it isn’t. It’s about power. There’s nothing a Canadian ISP can do to prevent kids from finding porn online. If the Canadian government actually cared about this they wouldn’t use (hopelessly ineffective!) ISP level blocking. They would be funding an information campaign on device-level content blockers and be funding a browser add-on and block&#x2F;allow list. reply twisteriffic 12 hours agorootparent> If the Canadian governmentThis bill wasn&#x27;t introduced by and isn&#x27;t supported by the current Canadian government. It&#x27;s supported by the Conservatives, who are the opposition. reply mulmen 12 hours agorootparentSorry, Americanism, “government” means “the currently elected lawmakers”. reply 99_00 12 hours agorootparentprevLet me make sure I understand your position.Are you saying that if this bill becomes law, consumption of porn among minors will not decrease? reply kelnos 12 hours agorootparentNot meaningfully. If a kid wants to find porn, they&#x27;ll find porn. These sorts of measures are ultimately ineffective at blocking everything, and if there&#x27;s even one thing that slips through, even if only for a little while, that&#x27;s enough. reply starburst 12 hours agorootparentprevIsn&#x27;t it obvious it won&#x27;t? Unless they go full-on China firewall style there will always be website readily available that won&#x27;t enforce that crap and kids will find them easily. reply mulmen 12 hours agorootparentprevCorrect. The Streisand Effect suggests it might even increase interest. replyjerlendds 11 hours agoprevWhy are so many media organizations and news outlets quiet on this? I&#x27;ve been searching around for any hits of S-210 and it seems there&#x27;s remarkably few! If you&#x27;re Canadian let&#x27;s spread the word! reply adamomada 11 hours agoparentThey get paid 60 million bucks a year (for a decade at least) to stfu about government business reply dade_ 11 hours agoparentprevTrudeau is subsidizing most of Canadian media. Who knows what Pierre will do, but I’ll take a bet that it could be a cost saving measure to stop handouts to ‘fake news’ MSM.Justin seems to be aiming for a supernova exit… https:&#x2F;&#x2F;338canada.com&#x2F; reply Kbelicius 1 hour agorootparent> Trudeau is subsidizing most of Canadian media.Considering that this bill is coming from the opposition how is this relevant? reply speak_plainly 12 hours agoprevIt was only a matter of time before a certain class of people figured out how to use the internet and subsequently ruin it. I&#x27;m more surprised it lasted this long. reply habibur 16 hours agoprevI had been following C-18 closely. Wanted it to fail. Facebook&#x2F;Meta bailed out of the market, deciding to not paying anything. The other one was Google which finally settled for paying $100m per year to a single institution which will divide the money among others.That&#x27;s still far less than what Canada was asking. 2% of their yearly revenue which was bumped up to 4% after Meta left. But Google&#x27;s $100m is more like 0.5% of their yearly gross revenue in Canada. reply vibrolax 15 hours agoparentYes, and now the government will have another $100m per year to bestow on its favored media companies. reply barbazoo 15 hours agoprevThat person obviously knows what they&#x27;re talking about and are much smarter than me but I still think this here is a weasel thing to say.> I think the best way to deal with the issue includes education, digital skills, and parental oversight of Internet use including the use of personal filters or blocking tools if desired.That made me actually laugh. Internet access is ubiquitous, parental digital literacy is often low, none of those are actually helpful suggestions that would help address the symptom.Imagine if we suggested that for other problems. \"I think the best way to deal with gun violence includes education, gun skills, and parental oversight of gun use including the use of safety mechanisms and gun safes if desired.\"I appreciate people spending their time fighting for our rights. But these arguments here weren&#x27;t very practical in my opinion. reply genocidicbunny 15 hours agoparent> That made me actually laugh. Internet access is ubiquitous, parental digital literacy is often low, none of those are actually helpful suggestions that would help address the symptom.I think that works both for and against your point though. For the very same reason, it&#x27;s not likely there will ever be a way to reliably solve this technologically. So while we may need some technological measures, we also need to educate parents and kids about this at the same time. Anyone trying to sell a technological &#x27;this will solve the porn problem&#x27; solution should be viewed with a very critical eye. reply barbazoo 15 hours agorootparent> For the very same reason, it&#x27;s not likely there will ever be a way to reliably solve this technologically.How did you arrive at this? Because I&#x27;m optimistic and think that if the government sets the ground rules, porn distributors will find a way to implement it reliably (using technology) and continue delivering porn to their consumers.We get age checked here in Canada all the time when we buy booze for instance. Why not enforce it here with something like an age verification? reply genocidicbunny 15 hours agorootparentBecause the internet does not end at the Canadian border, and porn companies operating from other countries have no reason to implement such systems. And you can&#x27;t block access to them -- VPNs are trivial to access these days.> We get age checked here in Canada all the time when we buy booze for instance. Why not enforce it here with something like an age verification?Because to buy alcohol you have to physically show up at a store with a physical ID that needs to pass as real, and you need to present yourself passably as an adult. Even then, it&#x27;s not terribly difficult to get around even that -- plenty of high-schoolers are able to reliably get alcohol with cheap fake ID&#x27;s because they look old enough and the ID is good enough. Online delivery services are also not terribly hard to spoof for a dedicated enough teenager. reply cherioo 14 hours agorootparentThen the next step is to ban VPN. It also doesn’t matter that the ban is not 100% effective, as long as it deters majority of the uses.Arguing whether or not it CAN be blocked feels pointless. The question needs to start from whether it SHOULD be blocked or not. reply mulmen 14 hours agorootparentYou said the quiet part out loud. The next step is banning VPNs. For the children of course. reply happyopossum 14 hours agorootparentprev>Why not enforce it here with something like an age verification?While I&#x27;d love this technical utopia, the Internet is a global construct. All it takes is someone in another country to decide they&#x27;d like to prey on kids by selling them porn, and your age verification goes out the window.To use your analogy, if the government can&#x27;t even stop licensed stores in fixed physical locations from selling alcohol to kids (because it does happen), then how is one country going to enforce this in a globally distributed, virtual space? reply kelnos 11 hours agorootparentprevWe get carded in the US all the time when we buy booze too, but that system is still flawed: fake IDs often work, store clerks sometimes don&#x27;t care as much as they should, underage people can sometimes get a random adult passing through to buy booze for them in exchange for 20 bucks, etc.And it&#x27;s a lot easier to enforce rules when an interaction is in-person. How can you believe that porn blocking on the internet would ever actually work? (If blocking online TV&#x2F;movie&#x2F;music copyright infringement hasn&#x27;t worked, why would you expect something like this to be successful?) All it takes is for one site to ignore the rules (porn sites based outside of Canada will not care to implement any kind of age verification), and for teens to use a VPN or some other trick to bypass ISP-level blocking.Then you have to ban or heavily regulate VPNs. At one point do you start realizing that you live in a Chinese-style internet censorship state? reply dotnet00 11 hours agorootparentprevThis is also missing the glaring fact that dedicated porn distributors are not the only sources of pornographic material. There are plenty of users on social media who post nsfw content or borderline nsfw content.So next you end up having everyone dox themselves to the site just to &#x27;protect the children&#x27;. But then you have things like the fediverse and torrents, where obviously Canada can&#x27;t really force much. At that point what are you going to do? Cut off global internet access? Start banning everything? Start heavily regulating software development past even what GDPR does? reply kelnos 11 hours agoparentprevI think in the end we should just stop caring so much about this stuff. Yes, educate children, with a frank discussion, about porn and why it can be a problem.But if a teenager really wants to find porn, they&#x27;re going to find porn, and... so what? I downloaded low-quality porn JPEGs from BBSes as a teen in the early&#x2F;mid-90s, and then found so much more porn on the internet in the mid&#x2F;late-90s, and I turned out ok. Sure, some people end up addicted and&#x2F;or with unhealthy views on sex and intimacy because of porn, but Draconian blocking schemes (that ultimately don&#x27;t work) aren&#x27;t the answer.Maybe parents should actually parent their children. Install parental controls where possible (which I know are far from perfect), but otherwise, maybe give your kids some measure of trust. This particular thing isn&#x27;t like worrying about a child predator luring your kid out to meet in person after finding them online; there&#x27;s very little harm in a kid seeing some porn and then a parent finding out about it and disciplining the kid, if that&#x27;s what they want to do. When I broke the rules as a kid, I got TV or Nintendo privileges taken away for a time; parents can take away a teen&#x27;s internet-connected devices as punishment. reply wharvle 14 hours agoparentprev> That made me actually laugh. Internet access is ubiquitous, parental digital literacy is often low, none of those are actually helpful suggestions that would help address the symptom.And tools for enforcing any kind of rules on your own are hot garbage. Especially any that a normal person could possibly hope to figure out.You either have to cut off the &#x27;net entirely (which is less practical with each passing year) or commit to a second job of managing this crap.I&#x27;m not a fan of bills like this, but also something&#x27;s gonna have to change, or we&#x27;ll get... bills like this.My guess is nothing will change, and this sort of thing is what&#x27;ll happen instead. reply barbazoo 14 hours agorootparent> And tools for enforcing any kind of rules on your own are hot garbage. Especially any that a normal person could possibly hope to figure out.Online ID check? reply iimblack 13 hours agoparentprevI don’t think equating kids watching porn with gun violence makes sense. One has a much greater and more permanent effect so the reaction should be stronger. The other isn’t, in my opinion, a big deal and the way the government is trying to deal with it has much farther reaching effects than the issue warrants. reply adamomada 10 hours agoparentprevThe argument doesn’t work for gun violence because that’s other people using the equipment. It works if every kid has a gun (smartphone) and keeps shooting themselves with it though reply hackan 16 hours agoprevThis is freakin&#x27; nuts!! It should not pass, nor it should have reached such state, it is insane! reply twisteriffic 12 hours agoparentCall your nearest Conservative party MP and complain. They&#x27;re the ones pushing this. reply bawolff 15 hours agoprevI was originally going to say - wouldn&#x27;t this be pointless when stuff is hosted elsewhere in the world.Then i remembered that pornhub is a canadian company. So maybe not as pointless. reply tempest_ 16 hours agoprevIn 2023 I can&#x27;t believe they are still naming bills like this\"Protecting Young Persons from Exposure to Pornography\"Every time a bill gets a moniker like that I assume it is hiding some major government over reach. reply HumblyTossed 16 hours agoparentThey&#x27;re appealing to their \"fear\" base. A certain segment of the voting population lives in fear (no thanks to the politicians who stoke this fear) of ... well everything. reply chongli 15 hours agorootparentBase? This was a senate private member&#x27;s bill. Senators in Canada are appointed, not elected, and they serve for life. The senator who introduced the bill, Julie Miville-Duchêne, is a member of the independent senators group, a non-partisan group of 39 senators.It was voted against by the government. It&#x27;s only getting traction with support from opposition parties. It might pass the house of commons without government support due to a minority parliament. All this despite the fact that the opposition spans the gamut from far left to far right. reply willcipriano 16 hours agorootparentprevWhat side would you describe as the \"fearless\" side? They both seem fearful to me just in regards to different things. reply RobRivera 15 hours agorootparentThere are no sides - there are nuanced and diverse issues and varying approaches which optimize for different values and to use words like &#x27;us&#x27;,&#x27;them&#x27;,&#x27;side&#x27; is detrimental to the political process and enables those who yield narrative control over media uncharacteristic influence in a democracy. reply suoduandao3 15 hours agorootparentprevcentrists are probably generally less fearful as retreating to one extreme of a polarized dynamic is a common fear response. But I think the point is more, everyone is partially motivated by fear, sometimes governments use that fear as a lever. reply HumblyTossed 14 hours agorootparentprevNot about sides. Some people are just afraid of things. Afraid everyone is out to steal their kids. Afraid of war. Afraid of immigrants. Afraid of not enough immigrants. This just appeals to those people. reply cibcin 16 hours agorootparentprevNot wanting your children to be influenced by pornography at a formative age is a very reasonable fear in this era. reply naremu 16 hours agorootparentI assumed this is what parental controls are invented for, but all too many parents don&#x27;t seem to even have the knowledge to activate those in the first place.Maybe they should learn how they work instead of supporting a thin veil of totalitarianism. reply ndriscoll 14 hours agorootparentWhich parental controls, and how would those even work?Back in the IE6 days I remember there was some setting to tweak allowed content ratings for websites, which I believe was based on HTTP headers, but does anyone implement that anymore (did they ever)? I just checked reddit, and they don&#x27;t seem to send any headers, and use the same domains for porn as they do for child-focused content.The obvious thing to advocate for would be to require commercial porn sites to send some kind of standardized headers that would allow parental controls to work in the first place, and&#x2F;or to require any domains with content targeted to children to not also host adult-only content (e.g. reddit should put either their child-focused subs or their porn on a separate subdomain).Then require commercial browser vendors to implement content blocking settings based on those headers.Alternatively, it would be possible for governments to provide an oauth style service that provides tokens that assert the user is an adult without revealing any other info about the user, and then require porn sites to check for that token. reply laweijfmvo 16 hours agorootparentprevor you know, talk to your kids reply pfisch 16 hours agorootparentprevTikTok and instagram are full of things that are porn or are almost porn. Same with Reddit.It is unrealistic to lock your kids out of all social media. reply blitz_skull 16 hours agorootparent> It is unrealistic to lock your kids out of all social media.Excuse me, what?Is it also unrealistic to prevent your kids from doing hard drugs? Sure maybe you can’t control them past a certain age, but you certainly can AND SHOULD within a certain age time frame.Social media has been shown to be so hazardous to developing youth’s mental health I think not only is it realistic, but you have a duty and an obligation do prevent your kids from using social media in an unrestricted manner. reply genocidicbunny 16 hours agorootparent> unrestricted manner.They didn&#x27;t say unrestricted, they said locked out. As in, not accessible at all.And you know what is also very hazardous to a developing person&#x27;s mental health? Being left out of most social gatherings and interactions with their peers.You can block your kids from accessing social media, but to succeed you also need to force their peers to communicate and interact with them in a unique and more frictive way. And you have no right to force someone else&#x27;s kids to do that. So the reality is that without a lot of like-minded families that do the same, your kids are going to be left out of a lot of things.So is it unrealistic to control your children&#x27;s access to social media? No, not at all. But locking them out from it entirely as you stated, is a good way to negatively impact their social development. reply blitz_skull 11 hours agorootparentI disagree. I grew up without a cell phone while all my friends had them. Beyond that, my friends had online gaming where I barely even had access to the internet, and what access I had was limited.I find the argument that kids are going to unanimously ostracize another kid without an Instagram account very hard to believe. Kids _will_ be assholes for any reason under the sun, so it&#x27;s not a question of will my kid have a hard time socially from time-to-time—that&#x27;s just life.Hobbies, social outlets, groups in the community—these are all ways to ensure your kids aren&#x27;t socially stunted without giving them the emotional &#x2F; mental equivalent of heroin. reply ipaddr 14 hours agorootparentprevLoneliness and a lifetime of sadness has been shown to have greater risks. Is cutting out social connections the answer? Because that&#x27;s what ends up happening. reply kelnos 11 hours agorootparentPlease present evidence that denying teens access to social media causes them to have literally no meaningful social connections. I do expect social interactions to suffer for a teen in that situation. But I find it hard to believe that it causes complete isolation and (gimme a break) a \"lifetime of sadness\". reply fallingknife 15 hours agorootparentprev> Is it also unrealistic to prevent your kids from doing hard drugs?Yes. If they want it, it&#x27;s not hard to find. reply blitz_skull 12 hours agorootparentThat&#x27;s not the point. The point is that it&#x27;s your obligation to do your best to protect them from it. \"Ease of access\" is hardly a reason to not try. reply jrockway 16 hours agorootparentprevIs it unrealistic? Like, social media wasn&#x27;t invented when I was a kid and I found some way to spend my time.I don&#x27;t have kids but am planning on it, and I don&#x27;t see myself locking kids out of social media. I also don&#x27;t super care if they see naked people on the Internet. If it becomes a problem somehow I&#x27;ll deal with the problem myself.I&#x27;m more worried about kids shows on TV with messages like \"follow all the rules or you&#x27;ll be bricked inside a tunnel\". I grew up with those, and in retrospect, they are super creepy. reply genocidicbunny 15 hours agorootparentTo me, the problem is that while you might block your children from using social media, are all of your children&#x27;s friends going to be in the same situation. If your kid has a circle of friends that interacts a lot on social media, and your kid doesn&#x27;t participate in that, eventually they will be left out of that group. reply jrockway 11 hours agorootparentYeah, exactly. That&#x27;s why I think a categorical ban is not necessarily \"good parenting\". I think you have to peek in and see what they&#x27;re watching on TikTok, and provide relevant information. If they think a video where someone burns down a store as \"a prank\" is funny, remind them of the consequences; hurting other people, prison, whatever. (OK, TikTok is a little less extreme than that, but you get the point.)Certainly, I understand why people want to delegate work to the government here; parenting is hard work. But it&#x27;s necessary work, you don&#x27;t want the government&#x27;s children to go out into the world and do their own thing. You want your kids to. And so your touch is going to be required in their formative years. There is no getting around that. reply munk-a 16 hours agorootparentprevIt is perfectly reasonable to lock your kids out of social media until they&#x27;re mature enough to handle to it. It&#x27;s actually probably in their benefit to block them from the non-nsfw parts of social media as well since social media has an extremely negative effect on mental health.Parents are allowed to autocrat - it may feel icky but to parent well you need to occasionally put on your villain hat. reply logicchains 16 hours agorootparent>Parents are allowed to autocrat - it may feel icky but to parent well you need to occasionally put on your villain hatIt&#x27;s not about icky. If you autocrat too much as a parent your kids will simple resent you and once they&#x27;re 18&#x2F;left home they&#x27;ll do whatever they want now that you&#x27;re no longer around to control them, because they never learned any self control. Like US college kids going all out with drugs and alcohol. reply munk-a 14 hours agorootparentThere is a place between a micro-managing helicopter parent and laissez-faire - your parenting should be in that place. And while it&#x27;d be awesome to be your kids&#x27; best friend forever it&#x27;s more important that they learn boundaries, self-control and how to human. You can impart self-control without opening every door and, given how psychologically exploitative a lot of the internet and advertising is, it&#x27;s hardly a fair fight to just let them sink or swim.Guidance is a responsibility of parenthood. reply belval 14 hours agorootparentprevWhat a weird argument in the context of this bill. \"If I&#x27;m too strict my child will resent me therefore I will vote so that my opinion can be enforced on everyone by the government and I can be my child&#x27;s buddy\".Must be interesting with other \"sinful\" activities, \"I&#x27;d totally let you do cocain but shoot the government won&#x27;t let me!\". reply kelnos 11 hours agorootparentprevMy parents were pretty strict with me as a kid, and I missed out on a lot of things that my peers got to do. I did resent them, somewhat, at the time, but by my mid-20s I was able to recognize that they were just doing their best and, like literally all parents, were making things up as they went along.When I went to college I was fine. I developed a new social circle quickly, and didn&#x27;t end up becoming a drunk or a druggie. Sure, I did many of the things my parents never would allow me to do, but it was fine.I totally get that some people end up in a worse place than I did, but that&#x27;s not an excuse for blanket governmental bans on things. But I would absolutely 100% support any parent that decides to deny their children any and all access to social media. That shit is cancer, and IMO is worse for developing brains than nicotine or alcohol.I hesitate to present such a hard line on social media, when I&#x27;m kinda \"whatever\" on teens seeing some porn. The problem is that I see what social media does to adults with fully-developed brains, and I start to feel like social media is akin to heroin: no amount of it is safe, for anyone.A more relaxed view might be to give teens access to social media, but only in a supervised setting. Parents should be monitoring what goes on with their social media accounts, and have frank (but calm and non-judgemental) discussions with the kid whenever anything concerning comes up. Also parents need to find a way to impress upon their kids that social media is not real life, and that people present whatever slice of their lives (often an unrealistic rosy picture) they decide to paint. And then there&#x27;s all the misinformation and echo chambers, and... ugh, yeah, no, just don&#x27;t let kids on social media. reply cf1241290841 14 hours agorootparentprevI believe its even more unrealistic to make the internet sfw &#x2F; children.While i do sympathize with the impossible difficulty of being a parent in the current decade, there is realistically no way i am going to just accept the death of the adult internet. Thats likely a common sentiment and realistically the people making these laws are too stupid to enforce them against motivated opponents with technical expertise.I dont see a way out of this other then create a whitelisted subset for children with enforcement being the responsibility of the parents. Because going death of anonymity for access control is a no go either.Looking on the bright side, to me this looks like just a lack of safe for children platforms that are still tolerable to use. reply standardUser 16 hours agorootparentprevI sometimes forget that we consider simple nudity to be \"porn\". We shouldn&#x27;t. reply munk-a 16 hours agorootparentIt&#x27;s absolutely ridiculous that we consider nudity to be nsfw - but TikTok and Instagram have a fair amount of content on them that are intentionally as close to nsfw as you can get without getting banned. I agree with your literal point - but there&#x27;s actual porn on social media. reply vezycash 16 hours agorootparentprevI&#x27;ve seen actual porn on Twitter. I don&#x27;t have an instagram account many porn models put links to their IG profiles. reply simion314 16 hours agorootparentprev>It is unrealistic to lock your kids out of all social media.Really? so the state blocks porn, but then that social media will have people that use bad language so the state needs to also protect your kids from bad language, maybe people say on those social media that Santa or Jesus does not exist so state should protect them again and ban this stuff too.Do not allow your child on social media that is for adults. I am sure PlayStation offer social feature for children with strong moderation, so find similar social media that is targeted for children. reply some_random 16 hours agorootparentprevConsider parenting your children instead of demanding the government do it for you. reply adra 16 hours agorootparentYeah, I mean why does the government get to tell me how I raise my kids? I wanna put them to work, drive heavy machinery, and marry them off to 90 year old \"founders\" all I like so stay outta my business. reply some_random 16 hours agorootparentOh I&#x27;m sorry you&#x27;re totally right, we need to ban working, driving heavy machinery, and marriage to protect your children. Because that&#x27;s what this is all actually about right? Sock puppet guy has made that quite clear, his goal is to ban all porn. reply kelnos 11 hours agorootparentprevThe difference is that in the cases you mention, the government is protecting kids from bad actions that parents might take.Requiring age verification won&#x27;t stop a parent from giving a kid access to porn. And, frankly, if a parent wants to show a teen porn as a way to educate them about what can be bad about it and what the dangers are, that&#x27;s a private matter between the parent and the teen. reply munk-a 16 hours agorootparentprevThere are a plethora of tools a concerned parent can deploy currently to limit web exposure - the main difficulty in doing so is more of a social pressure than a technical one and the salves provided by this bill are trivial to circumvent for a determined teen. reply kelnos 11 hours agorootparentprevIs it, though? I found and enjoyed a good amount of porn on BBSes and the early internet when I was a teen, and I turned out fine.Porn absolutely has serious problems associated with it (both on the production and consumption sides), but teens getting exposed to porn is just not a big deal. It can turn into a big deal, but that&#x27;s what, y&#x27;know... parenting... is for. reply squigz 16 hours agorootparentprevThat&#x27;s fair. And as another commenter pointed out, this is probably your responsibility as a parent. Of course, we also let the government tell us we can&#x27;t let our kids work, so clearly there&#x27;s a line we draw.So I ask, why is it the government&#x27;s responsibility in this specific case? reply genocidicbunny 16 hours agorootparentprevJust don&#x27;t allow your children to access these porn websites then. It really is that easy. ;) reply busterarm 16 hours agorootparentprevWanting the state to play the role of mother and father is a very reasonable fear in this era. reply gosub100 14 hours agorootparentprevIs there a documented syndrome that children or adults contract from pornography? Do you have any evidence of harm other than your feelings? reply simion314 16 hours agorootparentprev>Not wanting your children to be influenced by pornography at a formative age is a very reasonable fear in this era.I agree, so as a developer do you think it is easier to block porn at OS level and your router or you think that is safer that some regulation that only some websites will follow is more effective ?I read about some proposal for adult website to scan your face before granting access and other idiotic schemes, where the super obvious KISS solution is have Android, iOS, Windows, Ubuntu etc have an idiot proof way for a parent to setup a child account, this child account will set some cookie or whatever flag you want so websites can refuse to serve content to them. Also the big tech could use their combined energy to have a blacklist of bad websites that do not respect the rules like maybe advertisers.TLDR as a parent do not wait for the goverment to protect your child from porn, open Google and figure it out, reply nibnic 16 hours agorootparentnext [7 more] [flagged] some_random 16 hours agorootparentIt would be so much easier for people to ignore you if you just stated your values up front you know? Maybe next time you could even use your real account instead of multiple sock puppets. reply VBprogrammer 16 hours agorootparentprevThere are probably reasonable ways for the government to do this. Some kind of government run validation website where a user can generate a code to validate their age to each individual website is probably ok (in the UK we have a similar system for providing driving licence information to rental companies for example).Mandating that people do face scans or, even worse, send their IDs to potentially scummy websites is batshit crazy though.If what a government really want to do is ban access to pornographic content then they should be able to make that case to the electorate. reply simion314 15 hours agorootparent>There are probably reasonable ways for the government to do this. Some kind of government run validation website where a user can generate a code to validate their age to each individual website is probably ok (in the UK we have a similar system for providing driving licence information to rental companies for example).Why not just the parent sets the child device to \"kid mode\" and that is it. otherwise how do you make different national systems work World wide ?Your idea is valid but seems much more harder to implement. With my proposal where the parent and the OS vendor collaborate on this seems much more flexible and would work even better, like religious extremists could configure their child and their own devices to block much more then porn. reply VBprogrammer 15 hours agorootparentI&#x27;m mostly with you. In fact I&#x27;ve often wished that browsers did more things like that, for example cookie warnings. The cookie warning thing is a ridiculous misunderstanding of how things should work. The server doesn&#x27;t store things on your PC, it just offers them to your browser, it&#x27;s up to the browser to return them or not, it doesn&#x27;t even require new protocols.But of course kids are resourceful and will find a way of accessing content they want to access, for example maybe using a games console or something a parent might not immediately think of as having a browser.Whether it actually makes sense to protect kids from themselves to that extent I think is a valid question which I&#x27;m not sure I have the answer. I know passing around CDs full of such content was common when I was young (in the early days of the internet). reply loceng 15 hours agorootparentprevWhat about consenting women who&#x27;ve not been coerced or pressured in any way? You want to take people&#x27;s free will away, seems to be another perspective to your belief. reply simion314 16 hours agorootparentprevI agree with defensive in depth but that is contradicting priovacy if I need to show my face or ID card to a porn website, or how otehrs claim that reddit and TikTok is almost porn I should show my ID to all social media.As I said a parent should not allow the child to access social media that is targeting adults. The OS can put a flag and the websites can reject those connections, no need for destroying privacy.Not all porn is women exploration, there are this days this OnlyFans women that are exploiting men but who should decide what is and what is not exploitation. There is also CGI or AI porn, there are nude and artworks that religious extremists complain about. You can&#x27;t stop porn so better don&#x27;t wait for that day and setup your child devices if they need one properly. replygspencley 16 hours agoparentprevYeah and let&#x27;s also take a moment to consider that the current government here in Canada is not conservative.I say this in the most non-partisan way possible, just as a freedom loving Canadian: \"Fuck Trudeau\" reply tapesonthefloor 15 hours agorootparentGiven how conclusively the article explains that this bill is supported by effectively everybody _but_ Trudeau, I&#x27;d like to take a moment to thank you for unburdening the rest of us so effectively with any ongoing need to give precious time or thought to anyone wielding that particular two word phrase, or, for those of us who have already wasted our time in dialogue with folks like you, reaffirming the conclusions we&#x27;d already drawn. Appreciated. reply katbyte 15 hours agorootparentprevThe current Goverment voted against this and is not the ones trying to pass it. reply adra 16 hours agorootparentprevFrom the article, \"In fact, government ministers voted against it. Instead, the bill is backed by the Conservatives, Bloc and NDP with a smattering of votes from backbench Liberal MPs.\"Your anger seems to be misplaced in this instance. reply pesfandiar 15 hours agorootparentYou&#x27;re right https:&#x2F;&#x2F;openparliament.ca&#x2F;votes&#x2F;44-1&#x2F;609&#x2F; reply maximus-decimus 6 hours agorootparentprevThanks Obama.I don&#x27;t know if it&#x27;s funny or sad that some people blame Trudeau for everything, including the Palestine-Israel war. reply scj 16 hours agorootparentprev> \"The bill, which is the brainchild of Senator Julie Miville-Duchêne, is not a government bill. In fact, government ministers voted against it. Instead, the bill is backed by the Conservatives, Bloc and NDP with a smattering of votes from backbench Liberal MPs.\"Remember, the opposition and the Senate can get bills through in a minority parliament. reply pupppet 16 hours agorootparentSo basically fuck everyone…except Trudeau? This is why I can’t take the fuck Trudeau crowd seriously. He’s their go-to bogeyman for everything. reply gspencley 16 hours agorootparentprevThat&#x27;s a good point but it&#x27;s worth noting that the NDP is backing it, as well as some Liberal MPs. For non-Canadians, NDP is our \"far left\" party. I don&#x27;t mean that in a pejorative sense, just that our Liberal party tends to be your \"mainstream\" moderates and we have a 3rd party for the less moderate left-leaning voters. That&#x27;s what the NDP is. So if they&#x27;re backing this then that can only mean that there is bi-partisan support for this. reply loloquwowndueo 16 hours agorootparentprevI mean, imagine how much worse it would be if Poilievre was the PM ;) reply canadiantim 16 hours agorootparentAtleast Poilievre would be on a shorter leash than Trudeau reply munk-a 16 hours agorootparentUnfortunately if Poilievre actually did get into the PM office it&#x27;d be on the back of a CPC majority - so he&#x27;d have a lot more freedom to act than Trudeau&#x27;s current minority government. reply canadiantim 15 hours agorootparentWe&#x27;ll see. Could also be a Harper-like minority, getting support from the bloc. Trudeau also has a very supportive NDP right now, so Trudeau does have a lot of freedom currently. reply munk-a 14 hours agorootparentI&#x27;ll never say never but the current state of the CPC is so departed from mainstream Canadian politics that it&#x27;s far more likely that BQ would side with LPC... and the NDP and Greens would basically have to have a melt-down to side with the CPC.I think it&#x27;s almost vanishingly unlikely that the CPC form government unless they actually achieve a majority share of seats. replyvivekd 16 hours agoparentprevThat&#x27;s what I thought initially too, but reading the article, it seems like it&#x27;s just requiring age verification for accessing adult material. If we are concerned about censorship - I have to say that ship has long ago sailed. I don&#x27;t have a problem with blocking porn or steps preventing kids from accessing porn reply logicchains 15 hours agorootparent>I don&#x27;t have a problem with blocking porn or steps preventing kids from accessing pornIt&#x27;s just the first step; once they have the infrastructure for real-ID verification in place, they&#x27;ll roll it out to more and more of the internet until adults have no more anonymity online. reply djaro 15 hours agorootparentThat seems like a slippery slope.We also require age verification for online gambling, and that hasn&#x27;t had bad consequences either. reply tamimio 15 hours agoprev“Terrorism” and “protect the kids!” are the most used boogeyman by governments in the last two decades or so to further violate private citizens privacy rights, freedom rights, and further monitoring and censoring communications, and the accusation is ready if you dare to stand against it. reply dmix 15 hours agoparentJohn Jonik comic from years ago (2010?)https:&#x2F;&#x2F;i.imgur.com&#x2F;BDqwk99.jpg reply boh 14 hours agoparentprevIn the words of Winston Churchill: \"Never let a good crisis go to waste\". reply matheusmoreira 14 hours agoparentprevYes.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Four_Horsemen_of_the_Infocalyp...Making these arguments is evidence of bad faith. reply mohsenari 16 hours agoprevGrowing up in a country with internet censorship, I can tell it is a huge slippery slope not to mention the dangers of having to upload government ID to access adult websites. I hope politicians come to their senses on this. reply goda90 16 hours agoparentIf they really want to do this without any privacy invasion ulterior motives, then someone needs to push an anonymous credentials scheme. Have the government give people age verification keys. A person uses their key to verify their age on a website, but the website won&#x27;t know who they are, and the government won&#x27;t know they accessed said website(outside any other means of tracking internet traffic). reply MzHN 15 hours agorootparentIn China they already took a step further. The kids were using their parents&#x27; ID card to play games, so those games had to implement face recognition in addition to the ID card.https:&#x2F;&#x2F;www.theverge.com&#x2F;2021&#x2F;7&#x2F;9&#x2F;22567029&#x2F;tencent-china-fac... reply ipaddr 14 hours agorootparentI can see a unique dick registry now. reply SenAnder 16 hours agoparentprev> adult websitesDon&#x27;t buy their framing - every website with user-uploaded content is an \"adult website\" in the eyes of this law. If one of your users uploads a single jpeg of porn&#x2F;hateful political rant&#x2F;description of self harm, you&#x27;ll be liable for \"not implementing adequate measures to prevent minors from being exposed to pornography&#x2F;harmful material&#x2F;incitement to violence\".\"Adequate measures\" are, of course, a complete loss of anonymity, adding your website to the surveillance state apparatus. reply cibcin 16 hours agoparentprevnext [7 more] [flagged] stickfigure 16 hours agorootparentYou&#x27;ve been spamming this topic from a new account (possibly multiple), and here we get at the root of it - you want to ban porn. reply munk-a 16 hours agorootparentprevThis sentiment is really playing to the \"the guilty have nothing to hide\" line of reasoning and that&#x27;s a scary place to be. reply Geisterde 16 hours agorootparentprevLook at this, someone who is going to have to learn the hard way. reply jsbg 16 hours agorootparentprevdid you read the article? the law would be broad enough to require privacy-invading age verification systems for search engines reply nibnic 16 hours agorootparentnext [3 more] [flagged] some_random 16 hours agorootparentYeah I&#x27;ll bet there are a lot of porn addicts if you define that to mean \"goes to a porn site, ever\" reply cwillu 16 hours agorootparentprevTwo new alt accounts in what, 5 minutes? reply99_00 16 hours agoparentprevThe bill doesn&#x27;t say anything about uploading government ID.It says that it is illegal for a company to to give porn to kids. The company can defend it&#x27;s self against the charge if they have a \"prescribed age-verification method\". reply standardUser 14 hours agorootparent> It says that it is illegal for a company to to give porn to kids.No it doesn&#x27;t. It says \"makes available sexually explicit material\", which means any website or app that allows user-generated content. reply logicchains 15 hours agorootparentprev>The company can defend it&#x27;s self against the charge if they have a \"prescribed age-verification method\".And the prescribed age-verification method is going to be government ID. reply djaro 15 hours agorootparentHow else would you verify age?If I buy alcohol I also need a \"government ID\" reply kelnos 11 hours agorootparentThere are blind, privacy-preserving ways that this can be done. A third party verifies a government ID and issues an age-verification token. The token is passed to the porn site, which has a way to verify the token without talking to the entity that issued the token.That way the porn site doesn&#x27;t know who you are (it just knows \"this person is old enough to access this content\"), and the age verification entity doesn&#x27;t know what you used the token to access.Of course, this scheme is more complicated than building an age verification system that involves uploading a government ID (or asking a third party directly to verify someone&#x27;s age), so ultimately no one gets any privacy or anonymity. reply 99_00 15 hours agorootparentprevWhat do you base that on? reply retrac 15 hours agorootparentWhat age verification system exists other than government ID?The only way we ever verify someone&#x27;s age for legal purposes, at least here in Canada, is by checking government ID. Birth certificate, driver&#x27;s permit, photo ID card, health card. They all have your birth date on it. A younger adult may need to show this ID if they look particularly young and want to buy cigarettes or alcohol.Though relevant to the topic, I would note that the same young adult may need to show ID to buy pornography on DVD or Bluray at a retail store. That&#x27;s already established and I would think few object to that. It&#x27;s the security and privacy issues that arise when we start sending this data in a recorded and logged form over the Internet. This remains true whether it&#x27;s a government ID or a privately issued ID. reply 99_00 12 hours agorootparent>It&#x27;s the security and privacy issues that arise when we start sending this data in a recorded and logged form over the Internet.That&#x27;s for consumers and distributors to figure out. The lack of trust between consumers and distributors is no reason to continue allowing online porn to be exempt from long established and agreed upon controls. reply kelnos 11 hours agorootparentI think the security and privacy concerns absolutely are a reason to continue allowing this sort of thing to skate by. At least until the security and privacy concerns can be addressed. reply 99_00 8 hours agorootparentYour free to hold that opnion. reply genocidicbunny 15 hours agorootparentprevOn the basis of what the hell else could you reasonably use to establish someone&#x27;s age? A copy of their birth certificate? reply ipaddr 14 hours agorootparentprevThe crypto, gambling current implementation in place at the moment. replycibcin 16 hours agoprev [5 more] [flagged] rkagerer 16 hours agoparentMicheal Geist is a very well known figure in Canada, and has built a solid reputation raising awareness of important privacy issues. He&#x27;s testified in parliamentary advisory committees and CRTC hearings. The way you keep referring to him as \"this blogger\", unfairly makes it sound like some rando on the web wrote it. reply Fin_Code 16 hours agoparentprevThis is not about verifying ages. This is about the government tracking people so they can use information against them in the future. Any kid that wants to bypass this will use a vpn or submit false documents. reply jsbg 16 hours agoparentprevthe \"blogger\" is a well-known professor of law specializing in telecommunications issues reply meesles 16 hours agoparentprev [–] Yes, because cross-party consensus from politicians is always an indicator of good change! &#x2F;sI think &#x27;this blogger&#x27; (see other comments) is well-informed and up-to-date on their stuff, so they probably have a more well-informed opinion than who I&#x27;m replying to.These kinds of solutions are so short-sighted because a) kids will get around it and b) in 5 years we&#x27;ll have someone come along trying to ban another set of websites, and then another. This has and continues to happen all over the world and throughout history. When you give people or institutions the ability to censor information, given enough time, that authority will be wielded against you. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bill S-210, also known as the Protecting Young Persons from Exposure to Pornography Act, has passed the Senate and is currently under review by the Public Safety committee.",
      "The bill aims to protect children from online harm but has faced criticism for potentially infringing on personal choices and internet freedoms.",
      "The government is considering the use of age verification systems with face recognition technology, which has raised privacy concerns.",
      "The bill also includes provisions for website blocking.",
      "The article argues against the bill, highlighting the importance of safeguarding children without resorting to government-sponsored censorship.",
      "The comment section explores alternative solutions and the challenges of blocking or censoring websites due to advancements in web technologies.",
      "The article is written by Michael Geist, who shares his concerns about Bill C-11 and other topics related to the Broadcast and Telecom Legislative Review Panel Report."
    ],
    "commentSummary": [
      "Proposed Canadian internet bill, S-210, aims to restrict access to sexually explicit content and protect children.",
      "Critics raise concerns about lack of democratic pushback, privacy, censorship, and abuse of power.",
      "Debate includes lobbying efforts for anti-CSAM measures, petition against Trudeau, effectiveness of age verification systems, overblocking, parental control, government control, filtering systems' limitations, rule enforcement, social media's impact on youth mental health, and the importance of parental guidance and education."
    ],
    "points": 220,
    "commentCount": 214,
    "retryCount": 0,
    "time": 1702572216
  },
  {
    "id": 38641198,
    "title": "Police departments fail to uphold transparency with body camera footage",
    "originLink": "https://www.propublica.org/article/how-police-undermined-promise-body-cameras",
    "originBody": "Criminal Justice How Police Have Undermined the Promise of Body Cameras by Eric Umansky, with additional reporting by Umar Farooq Dec 14 · 37 min read Hundreds of millions in taxpayer dollars have been spent on what was sold as a revolution in transparency and accountability. Instead, police departments routinely refuse to release footage — even when officers kill. by Eric Umansky, with additional reporting by Umar Farooq Dec. 14, 5 a.m. EST Twitter Facebook Copy Link Copied! Change Appearance Auto Light Dark Republish Co-published with The New York Times Magazine ProPublica is a nonprofit newsroom that investigates abuses of power. Sign up to receive our biggest stories as soon as they’re published. When Barbara and Belvett Richards learned that the police had killed their son, they couldn’t understand it. How, on that September day in 2017, did their youngest child come to be shot in his own apartment by officers from the New York Police Department? Get Our Top Investigations Subscribe to the Big Story newsletter. Email address: This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. Thanks for signing up. If you like our stories, mind sharing this with a friend? Copy link For more ways to keep up, be sure to check out the rest of our newsletters. See All Protect independent journalism. Make a year-end donation to ProPublica. Donate Miguel Richards, who was 31, grew up in Jamaica and had moved to New York about a year earlier after coming to the United States through a work-study program. His father’s friend gave him a job doing office work, and he rented a room in the Bronx. But he started to struggle, becoming reclusive and skipping days of work. His mother, with whom he was particularly close, pleaded with him to return to Jamaica. “It’s as if I sensed something was going to happen,” she says. “I was calling him, calling him, calling him: ‘Miguel, come home. Come home.’” His parents knew he had never been violent, had never been arrested and had never had any issues with the police. What details they managed to gather came from the Bronx district attorney: Richards’ landlord, who hadn’t seen him for weeks, asked the police to check on him. The officers who responded found Richards standing still in his own bedroom, holding a small folding knife. And 15 minutes later, they shot him. Richards’ death marked a historic turning point. It was the first time a killing by officers was recorded by a body camera in New York. The new program was announced just months before as heralding a new era of accountability. Now, a week after the shooting, the department posted on its website a compilation of footage from four of the responding officers. The video, the department said in an introduction to the presentation, was produced “for clear viewing of the event as a totality.” And as far as the department was concerned, the narrative was clear. Sometimes “the use of deadly force is unavoidable,” the police commissioner at the time, James O’Neill, wrote in an internal message. The level of restraint shown by all officers, he said, is “nothing short of exceptional.” And, he added, “releasing footage from critical incidents like this will help firmly establish your restraint in the use of force.” Richards’ parents were not convinced. Belvett watched footage at the district attorney’s office. What he saw, and what was released, did not, in fact, show that the use of deadly force was unavoidable. He later learned that the department had not released all the footage. What else didn’t they know about their son’s death? Belvett and Barbara Richards’ 31-year-old son, Miguel, was killed by New York City police officers in 2017. Credit: Naila Ruechel for The New York Times When body-worn cameras were introduced a decade ago, they seemed to hold the promise of a revolution. Once police officers knew they were being filmed, surely they would think twice about engaging in misconduct. And if they crossed the line, they would be held accountable: The public, no longer having to rely on official accounts, would know about wrongdoing. Police and civilian oversight agencies would be able to use footage to punish officers and improve training. In an outlay that would ultimately cost hundreds of millions of dollars, the technology represented the largest new investment in policing in a generation. Yet without deeper changes, it was a fix bound to fall far short of those hopes. In every city, the police ostensibly report to mayors and other elected officials. But in practice, they have been given wide latitude to run their departments as they wish and to police — and protect — themselves. And so as policymakers rushed to equip the police with cameras, they often failed to grapple with a fundamental question: Who would control the footage? Instead, they defaulted to leaving police departments, including New York’s, with the power to decide what is recorded, who can see it and when. In turn, departments across the country have routinely delayed releasing footage, released only partial or redacted video or refused to release it at all. They have frequently failed to discipline or fire officers when body cameras document abuse and have kept footage from the agencies charged with investigating police misconduct. Even when departments have stated policies of transparency, they don’t always follow them. Three years ago, after George Floyd’s killing by Minneapolis police officers and amid a wave of protests against police violence, the New York Police Department said it would publish footage of so-called critical incidents “within 30 days.” There have been 380 such incidents since then. The department has released footage within a month just twice. And the department often does not release video at all. There have been 28 shootings of civilians this year by New York officers (through the first week of December). The department has released footage in just seven of these cases (also through the first week of December) and has not done so in any of the last 16. Asked about the department’s limited release of footage, a spokesperson pointed to a caveat, contained in an internal order, that footage can be withheld because of laws or department policy. “The NYPD remains wholly committed to its policy of releasing such recordings as quickly and responsibly as circumstances and the law dictate,” the spokesperson wrote. “Though transparency is of the utmost importance, so too is the Police Department’s commitment to preserving privacy rights.” The department did not say which policies require the withholding of footage and did not address other questions about its record on the cameras. (Mayor Eric Adams’ spokesperson did not make him available for comment.) For a snapshot of disclosure practices across the country, we conducted a review of civilians killed by police officers in June 2022, roughly a decade after the first body cameras were rolled out. We counted 79 killings in which there was body-worn-camera footage. A year and a half later, the police have released footage in just 33 cases — or about 42%. This article is the product of more than six months spent investigating how the police have undermined the promise of transparency and accountability that accompanied the body-camera movement. We interviewed dozens of department insiders, government lawyers, policing experts and advocates and reviewed hundreds of pages of internal reports, obtained through Freedom of Information requests, and dozens of hours of surveillance-camera and body-camera footage, including some that the New York Police Department fought against disclosing. The reporting reveals that without further intervention from city, state and federal officials and lawmakers, body cameras may do more to serve police interests than those of the public they are sworn to protect. To Seth Stoughton, a former police officer who is now a professor at the Joseph F. Rice School of Law at the University of South Carolina, body cameras represent the latest chapter in America’s quest for a technological fix to the deeply rooted problem of unchecked state power. “Dash cams were supposed to solve racial profiling,” he says. “Tasers and pepper spray were supposed to solve undue force. We have this real, almost pathological draw to ‘silver bullet’ syndrome. And I say that as a supporter of body-worn cameras.” He later added, “We just said to police departments: ‘Here’s this tool. Figure out how you would like to use it.’ It shouldn’t be a surprise that they’re going to use it in a way that most benefits them.” Jeff Schlanger, a former New York deputy commissioner who had an oversight role during the implementation of body-worn cameras and left the department in 2021, believes that the police have often failed to use the cameras for accountability and that political leaders need to do more. “Mayors, City Council members, all locally elected officials,” he says, “should be losing sleep over the lack of meaningful independent oversight of the police.” Jeff Schlanger is a former deputy commissioner who left the New York Police Department in 2021. “Mayors, City Council members, all locally elected officials,” he says, “should be losing sleep over the lack of meaningful independent oversight of the police.” Credit: Naila Ruechel for The New York Times When full footage has been released, often by prosecutors or after public pressure, it often contradicts initial police accounts. In 2015, a white officer in Cincinnati killed a Black man during a traffic stop. The officer said his life was in danger. But his body-camera video showed that was a lie, and he was prosecuted for murder. (Charges were dropped after two mistrials.) And in Philadelphia this August, an officer shot and killed a man after, the police said, he lunged at officers with “a weapon.” In fact, footage released by the district attorney — who charged the officer with murder — shows that the man was sitting in his own car. In New York, Miguel Richards’ parents weren’t the only ones who had doubts about the department’s claims that the shooting was unavoidable. The footage the department released stopped right when the officers fired at Richards. It didn’t include the minutes after the shooting, and it didn’t include footage from other police units that responded. Ruth Lowenkron, a disability rights lawyer who specializes in mental health issues, wanted to see it all. Working for New York Lawyers for the Public Interest, a legal-advocacy nonprofit, she and her colleagues, along with activists, had begun pushing the city to find an alternative to using the police as first responders to people in crisis. On her second day on the job, a sergeant shot and killed a 66-year-old woman who had schizophrenia and was holding a baseball bat in her Bronx apartment. The department’s own investigators concluded that the sergeant escalated the situation and caused the shooting. Now, watching the video the department released of Richards’ shooting, Lowenkron feared that the same thing happened to him. The department’s edited footage showed the officers making a few attempts to connect with Richards early in the encounter. “What’s your name, man?” one officer asked. But they were also barking increasingly terse commands. “You are seconds away from getting shot,” one officer said. “Do you want to die?” A few minutes later, as one of them warned that Richards might have a gun, the officers fired. Lowenkron filed a records request, certain that there was more to the story. In releasing the partial footage, the police commissioner had vowed that the “NYPD is committed to being as transparent as possible.” But nearly three weeks after her request, Lowenkron received a different message from one of the department’s records officers: “I must deny access to these records.” Ruth Lowenkron has fought the New York Police Department for years for access to body-camera footage in various cases. Of one video she received, she says, “It was a horror movie.” Credit: Naila Ruechel for The New York Times Body-worn cameras were adopted by police departments across the country in the wake of widespread Black Lives Matter protests in 2014, sparked when Michael Brown was killed by the police in Ferguson, Missouri. The officer who shot Brown was not equipped with a camera, and there was a dispute about what happened in the last moments of Brown’s life. Amid deep schisms over race, justice and policing, there was at least agreement that police interactions should be recorded. Brown’s mother pressed for the technology to become standard equipment. “Please,” she begged Missouri legislators, “let police-worn body cameras be a voice of truth and transparency.” President Barack Obama put the cameras at the center of his plans to restore trust in policing. Cities quickly began spending millions on the devices, expenditures that continue today for storage and software. Los Angeles has spent nearly $60 million since getting cameras in 2016. In Philadelphia, where footage is rarely released, the cameras have cost taxpayers about $20 million. New York City has spent more than $50 million. But whether citizens benefit from the cameras they’re paying for is often up to the police, who have often been able to keep footage hidden from the public in even the most extreme cases. In 2018 in Montgomery, Alabama, an officer unleashed his police dog on a burglary suspect without warning, severing the Black man’s femoral artery and killing him. The police and the city have refused to release footage for five years, arguing that it could cause “civil unrest” and that the officers could face “embarrassment.” But a lawyer for the man’s family, which is suing the city, got a copy of the transcript in the discovery process and entered it into the court record. “Did you get a bite?” an officer asked the one who had the dog, according to the document. “Sure did, heh, heh,” the K-9 officer responded. The secrecy undercuts the deterrent effect on officer behavior that many had presumed body cameras would produce. Three years before the Minneapolis police officer Derek Chauvin murdered George Floyd by kneeling on his neck, body-camera video caught him kneeling on the necks of others. In 2017, Chauvin dragged a handcuffed Black woman out of her house, slammed her to the ground and then pressed his knee into her neck for nearly five minutes. Three months later, Chauvin hit a 14-year-old Black boy at least twice in the head with a heavy flashlight, choked him and pushed him against a wall. The boy cried out in pain and passed out. Chauvin pushed a knee into his neck for 15 minutes as the boy’s mother, reaching to help him, begged, “Please, please do not kill my son!” The footage was left in the control of a department where impunity reigned. Supervisors had access to the recordings yet cleared Chauvin’s conduct in both cases. Minneapolis fought against releasing the videos, even after Chauvin pleaded guilty in December 2021 to federal civil rights violations in one of the cases. A judge finally ordered the city and the police to release the tapes this April, six years after Chauvin abused the boy. “Chauvin should have been fired in 2017,” says Robert Bennett, a lawyer who represented both of the victims. If the police had done that, “the city never burns. We’d have a downtown still. It’s a parade of horribles. All to keep something secret.” A Department of Justice report from this summer found that the secrecy and impunity was all part of a larger pattern in the Minneapolis Police Department. Shootings, beatings and other abuse had routinely been captured on video. But the department didn’t make the footage public or mete out punishment. There was a similar dynamic in Memphis, Tennessee, where officers in a street-crimes unit regularly abused residents. They wore body cameras but faced no consequences until the case of Tyre Nichols, who was beaten to death this January by officers in the unit, attracted national attention. The footage showed that some of the officers took their cameras off. Others knew they were being recorded and pummeled Nichols anyway. It was only after public outcry that the department took the rare step of releasing footage, which contradicted initial police accounts and led to state and federal charges for five officers. Some politicians have often quietly enabled obstacles to this kind of accountability. When South Carolina became the first state in the nation to require the use of cameras in 2015, Nikki Haley, the governor at the time, made the announcement with the family of Walter Scott standing behind her. Scott was a Black man who, two months earlier, was stopped by the police for a broken taillight and was shot in the back and killed when he tried to run away. A witness filmed the shooting, and that video contradicted official police accounts. The way to true reform is through using body cams as an early-warning system, as a way to correct small mistakes before they become big mistakes. —Jeff Schlanger, former New York deputy commissioner “This is going to make sure Walter Scott did not die without us realizing that we have a problem,” Haley said as she signed the legislation. What the governor didn’t say was that the same law stipulated that footage from cameras is “not a public record subject to disclosure,” thus relieving police departments from any obligation to release it. And indeed, little footage has ever become public in South Carolina. In 2021, York County sheriff’s deputies responding to a call for a wellness check found a young man sitting in his pickup truck with his mother standing next to him. They fired at him nearly 50 times. The sheriff, who refused to release body-camera footage, said the man pointed a shotgun at deputies. When the man, who survived, obtained the footage after suing, it showed no such thing. So far this year, the police in South Carolina have killed at least 19 people. The police have released footage in only three of those cases. When we asked one department, the Spartanburg County Sheriff’s Office, why it had not, a spokesperson pointed to the law, writing, “We never release that footage.” The pattern has become so common across the country — public talk of transparency followed by a deliberate undermining of the stated goal — that the policing-oversight expert Hans Menos, who led Philadelphia’s civilian police-oversight board until 2020, coined a term for it: the “body-cam head fake.” And there is no place that illustrates this as well as New York City, the home of the world’s largest municipal police force, some 36,000 officers strong. New York’s adoption of body-worn cameras started with a moment of unintentional inspiration. In 2013, Judge Shira Scheindlin was hearing testimony in a federal lawsuit in which multiple advocacy groups claimed that the Police Department’s aggressive “stop and frisk” policy was racially biased and unconstitutional. One day during the trial, an expert witness for the city mentioned a new tool for accountability — body-worn cameras — in passing. “My head snapped when I heard the words,” Scheindlin recalled this year. “I thought, ‘That could be a useful remedy!’” Two months later, Scheindlin issued a historic ruling that New York’s stop-and-frisk practices were unconstitutional. She ordered the Police Department to begin piloting body-worn cameras, writing that they were “uniquely suited to addressing the constitutional harms at issue in this case.” Scheindlin laid out three ways the cameras would help: “First, they will provide a contemporaneous, objective record of stops and frisks.” She continued, “Second, the knowledge that an exchange is being recorded will encourage lawful and respectful interactions on the part of both parties. Third, the recordings will diminish the sense on the part of those who file complaints that it is their word against the police.” But in a preview of obstacles that would follow, the department was slow to roll out the devices, even as they were becoming common in other cities. More than two years after Scheindlin’s ruling, the department hired researchers at New York University to conduct a survey about what residents wanted from a body-camera project. The community’s answers were overwhelming and clear: transparency and disclosure. Officers, however, wanted the opposite. They were concerned that the recordings would “show a different side of the story than what would otherwise be told,” according to a separate NYU survey. To Scheindlin and the plaintiffs in the stop-and-frisk case, that was exactly the point. When the department released its policy in April 2017, it was clear whose opinions held more sway. No video would automatically become public. Anyone that requested it would have to go through an opaque, often slow-moving Freedom of Information process — in which the department itself would be the arbiter of what would be released (though the courts could review that decision). The policy blunted the technology’s potential for accountability in other ways. Officers could decide when to start filming, instead of at the beginning of all interactions as the public wanted. And while the public had little access to footage, the police had privileged access: Officers who were the subjects of complaints would be allowed to watch the footage before having to give any statements — something that could allow them to tailor their accounts to the video. The policy was “so flawed that the pilot program may do little to protect New Yorkers’ civil rights,” Ian Head and Darius Charney of the Center for Constitutional Rights wrote in a guest essay in The New York Times. “Instead, it might shield police officers from accountability when they engage in misconduct.” Still, on April 27, 2017, Commissioner James O’Neill and Mayor Bill de Blasio held a news conference at a precinct in Washington Heights to celebrate the rollout of body-worn cameras. Stepping up to the lectern, O’Neill said he was initially skeptical of the cameras but had become a believer. “I’m totally convinced now that this is the way forward,” he said. “These cameras have a great potential to de-escalate.” Then the mayor went to the lectern. Officers had long felt that de Blasio, a self-proclaimed progressive, was too supportive of Black Lives Matter protests and not sufficiently supportive of the police. That sentiment turned into rage when a man espousing hatred of the police murdered two officers in late 2014. Hundreds of police officers turned their backs on the mayor at the funerals. Ever since, de Blasio had been working to repair the relationship. “This is an historic day for New York City,” de Blasio said, with O’Neill by his side. “This is the first day of the era of body-worn cameras, and that means we are going on a pathway of transparency and accountability that will benefit everyone.” Five months later, officers killed Miguel Richards, making his case the first in which the potential of body-camera video would be tested. But Ruth Lowenkron, the public-interest attorney who filed a request for the footage, was getting little from the Police Department. After it rejected her initial request, she appealed the decision. The department sent her some redacted footage but again rejected her request for all of it. Disclosing the full footage would be an “unwarranted invasion of personal privacy,” the department wrote. Whose privacy — the dead man’s or the officers’ — was not explained. Releasing the full footage, the department insisted, could “endanger the life or safety of any person.” The letter came from the department’s legal unit, led by its deputy commissioner, Larry Byrne, who was known for his fierce advocacy for the department. From the outset of the body-worn-camera program, Byrne made it clear that he was resistant to widespread release of footage. “They are not public records in the sense that, because the officer turns the camera on, they are now in the public domain,” Byrne told NY1 in 2015. In fact, he insisted, “most of this footage” would never be made public. Lowenkron kept requesting the Richards footage and kept getting rejected or sent redacted video. In July 2018, she and her colleagues decided to file a lawsuit in state court demanding the full footage. They even got a former Police Department lawyer, Stuart Parker, to help litigate the suit pro bono. The department’s various explanations for its denials “pissed me off,” Parker recalls. He retired from the department as an assistant commissioner in 2016, the year before cameras were widely rolled out. But he had been excited by their potential and was frustrated by the department’s kneejerk secrecy. “There’s a good side to the department,” he says, “but there’s always been a self-serving dark side to it too.” In response to the suit, the department argued in legal filings that it had blurred the footage “in order to protect the privacy of both Richards and his family.” But Lowenkron and her team had obtained affidavits from Richards’ parents saying that the department never asked them whether they wanted the footage released or redacted. And what the Richardses wanted, they said, was for the full footage to be released to the public. Public disclosure of footage isn’t the only path to hold officers in New York accountable for misconduct. For 70 years, the city’s Civilian Complaint Review Board had been vested with the responsibility to investigate New Yorkers’ allegations against the police. From the start, though, its powers were weak. The agency was actually part of the Police Department, and its board consisted of three deputy police commissioners. The department fought efforts over the years to make the agency independent. In the face of a plan in the mid-1960s to include civilians on the Civilian Complaint Review Board, the head of the largest police union, then called the Patrolmen’s Benevolent Association, said, “I’m sick and tired of giving in to minority groups with their whims and their gripes and shouting.” The agency eventually became independent in 1993 after stiff opposition months before from off-duty officers. Thousands of them — along with Rudy Giuliani, then a mayoral aspirant after losing the previous election — staged a huge protest outside City Hall, with many of them going on to block the Brooklyn Bridge. After the changes, the review board still relied on an often noncooperative Police Department for records, and its investigations frequently petered out amid competing accounts. And like many civilian oversight boards across the country, in the rare cases when it substantiated misconduct, it could only recommend discipline to the police commissioner, who could and often did ignore it. Many civilians, whom the board relied on to initiate complaints, had long grown skeptical of the agency’s ability to ensure that officer misconduct had consequences. But the advent of body-camera video promised to fundamentally change how the agency worked. For the first time, staff members would have an objective record of the incidents they investigated. That was Nicole Napolitano’s hope when she joined the review board as its new director of policy and advocacy in September 2017 — the same year body cameras were rolled out in New York and one week after officers killed Richards. “We talked about it in detail” at the agency, she says of the initial footage of the Richards shooting. “We thought, ‘Look at what body-worn cameras can show us.’” Napolitano, who is married to a retired detective, knew it would be a challenge. As a senior policy manager in the Office of the Inspector General for the New York Police Department, she had seen how the department could simply ignore the recommendations in her reports. Napolitano hoped she would have more direct impact in her new, more senior position at the review board. But what she hadn’t appreciated was how much the police controlled the literal tools of their own oversight. Nicole Napolitano, as director of policy and advocacy at the Civilian Complaint Review Board, argued for a law that would take away the New York Police Department’s sole control over camera footage. She was let go in November 2020. Credit: Naila Ruechel for The New York Times As with most civilian boards across the country, the agency did not have its own access to footage. Like the public, it, too, had to rely on the cooperation of the department. To try to obtain footage, the board had to navigate a baroque multistep process. Written requests were submitted to a department “liaison” unit, which in turn forwarded them to the legal unit for review. Then the department had to locate the footage, which was a significant undertaking because it wasn’t cataloging the footage in any systematic way. Unlike in many other cities, the department’s cameras had no GPS location data. If a civilian making a complaint didn’t know an officer’s name or badge number, investigators and even the department could have a hard time finding footage. Perhaps most problematic for Napolitano, though, was the fact that the review board’s investigators had to agree to a strict set of conditions before watching videos of incidents. If they spotted other, unrelated misconduct, they were not allowed to investigate it. “If you were setting up a system to be shitty,” one agency insider says, “this is the system you’d create.” At times, the department’s animosity toward the board was palpable. Napolitano remembers one meeting in 2017 between board officials and Kerry Sweet, then a top official at the department’s legal bureau who helped oversee the body-camera rollout. As other police brass shuffled in, Sweet said they had missed a chance to “bomb the room” when only board officials were there, which would have “solved everything.” (Sweet, who has since retired, says he doesn’t recall saying that, but added, “On reflection, it should have been an airstrike.”) Napolitano and her colleagues noticed an even more troubling trend: The department would often tell the review board that the footage it requested didn’t exist — only for the civilian agency to later discover that wasn’t true. According to an analysis the agency put out in early 2020, this happened in nearly 1 of every 5 cases. Napolitano thought there was a straightforward solution to the department’s stonewalling: The review board should be able to directly log in to the department’s system where footage is stored. That’s how it worked with civilian oversight boards in a few other major American cities, including Chicago, which revamped civilian oversight after Laquan McDonald was killed in 2014 and the city tried to withhold footage that contradicted officers’ accounts. Chicago’s oversight board now not only has direct access to videos but also regularly releases footage publicly, and its investigators have used it to successfully push for officers to be fired for misconduct. Napolitano didn’t see a reason for it to be otherwise in New York. So in her first semiannual report, at the end of 2017, she noted the challenges of getting footage — and called on the city to give the review board direct access. Both the department and City Hall, Napolitano says, “freaked” out. “It was a rough time for de Blasio when it came to public safety,” Napolitano added, referring to the mayor’s tenuous relationship with the police. “In a dispute between CCRB and NYPD, City Hall always chose the NYPD. Always.” “I don’t agree,” de Blasio says. “The tension between the CCRB and the NYPD is natural and built-in. I decided each issue on the merits and according to my values.” He went on, “The blunter truth is when a progressive challenges the police culture and the police unions and the status quo of American policing, the left is not going to have their back. You’re not getting that thank-you card. And the right will viciously attack.” While the department fought Lowenkron and Napolitano on the release of body-camera footage, there was one group that had access to all of it and could use it to check for misconduct: the department’s own investigators. After every police shooting, detectives with the Force Investigation Division review the incident to see whether officers complied with department policy. The Richards case was the first time body-worn-camera footage could let them see what actually happened in a killing by officers. As investigators dug through the video and interviewed officers in the weeks and months after the shooting, they saw a far more complicated picture than the one the police commissioner painted. As the tape began, one officer, Mark Fleming, beamed his flashlight into the far side of Richards’ nearly bare, unlit bedroom. Richards was standing perfectly still in the dark, seemingly catatonic, wearing a blue polo shirt and sunglasses and holding a knife in his left hand. Department guidelines for dealing with people in crisis who do not pose an immediate threat say officers should try to “isolate and contain” the person. “The primary duty of all members of the service is to preserve human life,” department policy states. Officers are also instructed to wait for a supervisor’s permission before trying to subdue someone in crisis. At first, it appeared that the officers who encountered Richards were following their training. “Look, we could shut the door,” Officer Redmond Murphy suggested to his partner. But Fleming, who had served more years in the department, quickly rejected the idea. He kept telling Richards to drop the knife, and he radioed for an officer with a Taser. Two officers from the specially trained Emergency Services Unit, which deals with people experiencing mental health crises, arrived. Then Murphy said he thought he saw something, perhaps a gun, in Richards’ right hand, which was obscured behind a backpack on the bed. “Hold up,” one of the ESU officers told Fleming and Murphy before heading back downstairs to grab protective gear. “I don’t know if it’s a toy or a gun,” Murphy quickly added. As the specialists went downstairs, the officer with the Taser, Jesus Ramos, went upstairs and joined Fleming and Murphy outside Richards’ room. “Do you want to take him down now?” Ramos asked them. “Yeah,” they both answered. At nearly the same moment, a radio command came from headquarters, emphasizing department guidelines. “Isolate and contain,” the dispatcher told the officers. “Use nonlethal force whenever possible.” As Ramos lifted his Taser and stepped into the room, Fleming — who later said Richards was raising his arm — fired his gun. Murphy fired, too. It’s impossible to see that moment in the grainy, shaky footage. The clearest angle would most likely have been Fleming’s camera, but it was covered by his arm as he held his flashlight. The shooting of Miguel Richards was the first to be recorded by NYPD body cameras. Police Commissioner James O’Neill wrote in an internal message that releasing footage would show officers’ “restraint in the use of force.” Below are clips from the videos the police initially released. They contain graphic content. Officers answered a call for a wellness check and found Richards standing still in the far corner of his bedroom, holding a small folding knife. “Look, we could shut the door,” Officer Redmond Murphy said at one point. Instead, police shouted at Richards for the next 15 minutes. Murphy says he thinks he sees something in Richards’ hand: “I don't know if it's a toy or a gun.” Officer Mark Fleming says: “I don’t want to shoot you if you’ve got a fake gun in your hand. You hear me? But I will shoot you if that’s a real gun.” A radio command from headquarters reminded the officers of NYPD guidelines to “isolate and contain” a person in a mental health crisis and to “use nonlethal force whenever possible.” Fifteen minutes after arriving, they opened fire. An internal investigation later found that Richards “was contained and posed no immediate threat of danger.” Fleming and Murphy fired 16 times, hitting Richards seven times, including twice in the chest, rupturing his aorta. As gunshots rang out, the supervisor they were supposed to wait for arrived. (None of the officers responded to requests for comment.) The internal investigators asked the officers to explain. “We kind of handle everything on our own,” Murphy offered. An internal investigator pressed Fleming about what had “situationally changed” and prompted the decision to “take him at that point.” Fleming said everything changed once his partner said Richards might have a gun. “I perceived that his intentions were lethal,” Fleming said. But his answers suggested that he hadn’t fully grasped Richards’ mental state. “Why would any sane person hide a fake gun?” Fleming asked. When the investigators asked why the two officers did not broadcast that Richards was an “EDP” — or an emotionally disturbed person — with a knife, as protocol dictates, Murphy told them he and Fleming had handled people in crisis before. Asked why they made the decision to use force, Murphy simply said, “We wanted to, like, end it.” While the Force Investigation Division ultimately concluded that the officers had been “justified” in shooting — because they were facing an “individual armed with a knife and an imitation firearm” — the investigators also said that Fleming and Murphy should still be punished. Richards, their September 2018 internal report noted, “was contained and posed no immediate threat of danger.” And the officers violated policy by not asking permission from their supervisor before they acted. The department’s full investigative record was first reported by the independent journalist Michael Hayes in his 2023 book, “The Secret Files.” The review recommended that the officers face disciplinary charges that could ultimately result in their firing. But in New York, as in almost all cities in the United States, the police commissioner has absolute power over punishment. In March 2019, O’Neill, who had extolled the promise of body cameras just two years earlier, overruled his own investigators. He decided that neither Fleming nor Murphy would be punished for killing Richards. Instead, the commissioner docked them three vacation days for something else they did: stopping for pizza before responding to the call for the wellness check. (O’Neill did not respond to questions or requests for comment.) It would be another three months before anyone outside the department would see the full footage. That June, a New York judge ruled that the “public is vested with an inherent right to know” and ordered the department to turn over the recordings to Lowenkron’s organization. She received a package with a DVD a month later from the department. Bracing herself, she sat down to view it on her computer. The footage that the department publicly released cut off when the officers fired. Lowenkron now saw the aftermath: Richards collapsed to the floor, crumpled and bleeding in the same spot where he had been standing rigidly seconds before. “He’s still alive,” Fleming said. “Holy shit,” Murphy replied. “Just fucking cuff him.” The officers then flipped over Richards, severely injured, so roughly that his head could be heard bouncing off the floor. They searched around the room for the firearm they thought Richards had. Eventually, Fleming found a palm-size, silver-colored plastic toy gun. “It’s some fucking little thing,” he said. (The video does not show Richards holding the toy gun.) More than three minutes passed before anyone administered any type of aid to the dying man. It was an Emergency Services Unit specialist who retrieved medical equipment after hearing the shots. Outside the apartment building, more video recorded other officers milling about. One told a colleague, “They were just hurling fucking shots.” The NYPD initially withheld the footage of the aftermath of the Richards shooting. Below are clips from the videos that a state judge later ordered released. They contain graphic content. As the officers move into Richards’ room moments after shooting him, Fleming observes, “He’s still alive.” Murphy is breathing heavily. “Holy shit,” he says. The officers ask one another if they are all right as they mill around Richards’ injured body. He is handcuffed and flipped over so roughly his head can be heard bouncing heavily on the floor. As more officers arrive outside Richards’ apartment building, one tells a colleague, “They were just hurling fucking shots.” Lowenkron was shocked. Officers had shot a young man and roughly handled him as he bled to death. “The utter disrespect,” Lowenkron says. “It was a horror movie.” New York Lawyers for the Public Interest would go on to share the footage with journalists. It would also use the footage in a webinar for mental health advocates in November 2020. “The point,” Lowenkron told me, “was to get more people engaged on this issue: transforming New York and this country’s response to people in crisis.” But by then, for another man in distress, it was too late. In April 2019, one month after O’Neill decided against punishing the officers for the Richards shooting, another officer shot and killed a man named Kawaski Trawick. The circumstances were remarkably similar to those in the Richards case. Trawick was also a young Black man who lived in the Bronx and was experiencing a mental health crisis in his own apartment. He was also holding a knife when the police arrived. And he was also shot soon afterward. At the Civilian Complaint Review Board, Napolitano was immediately struck by the parallels: “I remember reading the headline on Trawick and thinking, ‘Didn’t I read this already?’” This time, though, the victim’s family filed a complaint with the review board, providing an opening for civilian investigators to use body-worn-camera footage to make a case that the department and others couldn’t ignore. But despite repeated requests over many months, the department wouldn’t share the footage — or any other records — with the review board, leaving the oversight agency effectively unable to begin its own investigation of the case. The refusal was in line with the department’s longstanding practice to withhold footage from the board until the department’s internal investigation was over, a process that often takes more than a year. Such delays can effectively torpedo the review board’s investigations: Under New York civil-service law, any disciplinary cases against police officers must be brought within 18 months of the incident. In the Trawick case, the review board obtained the full body-camera video in January 2021 — more than a year and a half after the killing — and only after a state judge ordered the department to hand it over to Lowenkron’s organization, New York Lawyers for the Public Interest, which had sued for it. The judge determined that the department had been withholding the footage “in bad faith.” What it showed was even more damning than what was captured in the Richards shooting. As the police entered his apartment, Trawick demanded to know, “Why are you in my home?” One officer, Herbert Davis, who was Black and more experienced, then tried to stop his white junior counterpart, Brendan Thompson, from using force. “We ain’t gonna tase him,” Davis said in the video. Thompson didn’t listen. Instead, he fired his Taser at Trawick, sending roughly 50,000 volts pulsing through him. As Trawick started rushing toward the officers, Thompson lifted his gun and prepared to fire. “No, no — don’t, don’t, don’t, don’t, don’t,” Davis said, pushing his partner’s arm down. But Thompson fired four shots, hitting Trawick twice and killing him almost instantly, 112 seconds after they arrived at the apartment. (Davis and Thompson did not reply to requests for comment.) There was also troubling footage of the aftermath of the shooting. Officers swarmed outside Trawick’s apartment. “Who’s injured?” a sergeant asked. Two officers replied in near unison: “Nobody. Just a perp.” With all that in hand, the review board completed its investigation in June 2021. The agency, through one of the few powers it had gained over the years, can file and prosecute disciplinary cases against officers — which triggers a Police Department trial, after which a departmental judge sends a provisional decision to the police commissioner, who makes the final call. This September, the police judge overseeing the Trawick case recommended that there should be no discipline. Her reason had nothing to do with the shooting itself; in fact, the judge wrote that she had “serious doubts” about the decisions of the officer who killed Trawick. But the review board, she said, had failed to file charges within the 18-month statute of limitations, as outlined under state law. In the end, the department’s refusal to give the footage to the review board had effectively run out the clock on any chance the officers would be punished. “That should not be tolerated,” says Jeff Schlanger, the former deputy commissioner. “Both CCRB and NYPD are city agencies. This is something the mayor needs to resolve.” In the wake of George Floyd’s murder in 2020, huge demonstrations for racial justice and against police brutality rolled across the country and the world. It was a global reckoning brought on by footage — the video, recorded by a teenager on her smartphone for more than eight minutes, showing Derek Chauvin ending Floyd’s life. Napolitano and her team at the review board had collected data showing how footage could make a difference in New York too. Access to body-camera footage roughly doubled the likelihood that agency investigators would be able to decide a case on its merits rather than dismiss it as inconclusive. But the backlog was growing. That May, the board filed 212 requests with the Police Department for body-worn-camera footage — and the department sent only 33 responses. (While the pandemic slowed the work of all city agencies, the backlog predated it.) “The withholding of footage stops investigations and prevents the CCRB from providing adequate and meaningful oversight of the NYPD,” an internal agency memo warned. “The situation for New York City oversight of the police has steadily grown worse during the duration of a BWC program intended primarily to aid oversight.” We just said to police departments: ‘Here’s this tool. Figure out how you would like to use it’ It shouldn’t be a surprise that they’re going to use it in a way that most benefits them. —Seth Stoughton, a former police officer who is now a law professor at the University of South Carolina Napolitano campaigned internally for a law that would take away the department’s absolute control over footage and give the review board its own access. That November, she was let go, along with three other staff members who had sent pointed emails and memos about the department’s withholding of footage. The four filed a lawsuit claiming that their firing violated their First Amendment rights and received an undisclosed settlement. A review-board spokesperson wrote in an email that the agency has “publicly and repeatedly called on legislators to support the fight for direct access. No employee has ever been fired for supporting direct access to BWC footage.” This spring, the City Council speaker, Adrienne Adams, and the New York City public advocate, Jumaane Williams, sponsored a bill that would give the review board direct access to footage so that it wouldn’t be beholden to the department for cooperation during investigations. “There are difficult split-second decisions that have to happen” in policing, Williams told me. “But if we’re not able to look at the same thing, if we have to take the word of the NYPD, that doesn’t make this conversation any easier.” The Police Department has opposed the bill. A department official insisted at a City Council hearing in March that the department “does not fear transparency.” But the official argued that it would be an “insurmountable obstacle” to give the review board direct access while following state confidentiality laws. The bill has been stalled for months. The city, meanwhile, paid out at least $121 million in settlements last year for lawsuits alleging misconduct by police officers — the highest total in five years. With footage remaining in the control of the Police Department, body-worn cameras have made little difference to the public. This year, a federal court monitor wrote a scathing report about persistent problems with stop-and-frisk, the unconstitutional policing tactic that prompted Scheindlin to order the department to adopt body cameras a decade ago. The monitor found that contrary to Scheindlin’s expectations, police supervisors weren’t using footage to flag misconduct. In a sample of cases the monitor looked at, supervisors reviewing footage of stop-and-frisk encounters concluded that 100% of the cases they looked at were proper stops. The court monitor reviewed the same footage and found that 37% of the stops were unconstitutional. “It was an experiment,” Scheindlin says, one that didn’t anticipate issues like control over footage. Scheindlin, who stepped down from the bench in 2016, says she now believes that the Police Department should no longer be the sole custodian of its own video. “That troubles me,” she says. “It should always be somebody independent.” In interviews with a half-dozen former commanders and high-level officials, most of whom were involved in the body-camera program itself, they said that despite its public pronouncements, the department hasn’t committed to using footage for accountability. “Body cams are essential, if done right,” says a high-ranking commander who just retired and who spoke on the condition of anonymity because he still works in law enforcement. “They are a game changer.” He added, “If there’s a problem, you flag — and potentially there’s discipline. But that’s not happening in most cases.” Instead, he says, body cameras have become “an exercise in just work they have to do. It’s a culture thing.” Rudy Hall has a particularly useful vantage point. He was part of the team that rolled out the body cameras, visiting police departments around the country to see how they were using the technology, and has gone on to work for the federal monitor overseeing the department’s compliance with Scheindlin’s now-decade-old order on stop-and-frisk. “I watch a lot of body-cam videos,” Hall told me. “I have absolutely seen supervisors approve problematic conduct.” “Body-worn cameras have not been exploited the way they should be,” says Jeff Schlanger, the former deputy commissioner. “The way to true reform is through using body cams as an early-warning system, as a way to correct small mistakes before they become big mistakes. But there weren’t a lot of discussions about it. The NYPD needs to do a lot better.” One of the most comprehensive studies of the use of body cameras, a 2019 meta-analysis led by researchers at George Mason University, recommended that police departments consider using footage the way sports teams use game tape, to regularly review and improve performance. That’s essentially what the New Orleans Police Department did after the U.S. Department of Justice put it under federal oversight about a decade ago in response to the police killings of several Black men and persistent police violence. Body cameras were a “critical engine for us to continuously evaluate performance,” says Danny Murphy, who ran a unit at the department overseeing compliance with the federal mandate. Four auditors were hired to join the police force and comb through footage. They looked to make sure that officers were using their cameras and that supervisors were flagging any problematic behavior. “If officers know they’re being viewed, if supervisors know they’re being reviewed, it creates a pressure for accountability,” says Murphy, who left the department four years ago. A 2020 report from the city’s civilian oversight agency — which has direct access to footage — noted a reduction in both the use of force and citizen complaints, which the department attributed to “the use of the body-worn cameras and the increased scrutiny and oversight these cameras provide leadership.” The police in New Orleans also regularly and quickly release video from shootings and other major incidents. But in the end, it’s the police chief who has the final say on discipline. During his tenure at the New York Police Department, Schlanger had, in fact, started a kind of internal oversight system similar to the one in New Orleans. Schlanger and other senior officials would meet with each of the department’s 77 precincts every six months and look at body-camera footage to identify problematic trends and officers. “It was CompStat for constitutional policing,” Schlanger says, referring to the department’s data-heavy program for tracking crime. “If we saw a precinct doing poorly, we’d work to help them. It made a difference.” The department quietly ended the review program last year. A civil suit on behalf of Miguel Richards’ estate was filed against the city in 2018. New York is seeking the dismissal of the case. A judge has been considering the request for two and a half years. “I want answers,” his mother told me, “and haven’t been able to get them.” The three officers involved in the Richards shooting were honored in 2018 by the largest New York police union, the Police Benevolent Association, which gave them its Finest of the Finest award for “extremely brave and tactically sound action” in the Richards shooting, noting that “the officers had no choice but to open fire.” The officers were later deposed in the lawsuit. One of them, Mark Fleming, said in his testimony in September 2020 that he had learned a lesson: that the Emergency Services Unit — whose help he told department investigators he didn’t need — is in fact better equipped and trained to deal with situations that involve people having a mental health crisis. It’s not clear what, if any, lessons the department itself has taken in. Since Richards’ death in 2017, when cameras were widely rolled out, officers have killed at least 11 people in crisis. There is no evidence that officers have been punished in any of the cases. Photographs of Miguel at the Richards home in Jamaica. “I want answers,” his mother says, “and haven’t been able to get them.” Credit: Naila Ruechel for The New York Times On a Sunday morning in the Bronx this spring, there was another shooting. Santo de la Cruz called a city emergency line. His son, 42-year-old Raul de la Cruz, was in the middle of a schizophrenic episode and had posted a disturbing video on Facebook that morning. Wearing camouflage clothing and a hat with a patch of an Israeli flag, Raul complained about racist police officers. His father called 311, avoiding 911 because he was afraid of what would happen if the police showed up. “I thought they would send someone capable of dealing with a situation like that,” he says in Spanish. “Because I was calling for a sick person, not to send the police to shoot him up.” But it was the police who arrived, with body cameras rolling. And Raul was holding a knife. The officers shot him 28 seconds after arriving. He was hospitalized for more than a month before being released, having lost a kidney and part of his liver. A department commander cited the body-camera footage when he gave a brief news conference the day of the shooting to describe what happened. “This situation was fast, volatile and dangerous,” he said. The officers’ “quick response saved at least one civilian and protected themselves.” But the department has not released the footage or commented in the eight months since. Read More It Wasn’t the First Time the NYPD Killed Someone in Crisis. For Kawaski Trawick, It Only Took 112 Seconds. Lowenkron’s colleagues at New York Lawyers for the Public Interest have once again requested the video, so far to no avail. The department has also withheld the footage from the Civilian Complaint Review Board, per the practice of sharing records with the agency only after its own investigation is done. On Dec. 5, weeks after we sent questions to the department about that practice, the department signed a memorandum of understanding with the board to send footage to it within 90 days of a request. But for now, nobody outside the department knows exactly what happened in the de la Cruz shooting, including the family. They have not heard anything from the department. They want to see the footage. Do you have a tip about policing or another subject? Eric Umansky can be reached by email at eric.umansky@propublica.org and on Signal and WhatsApp at 917-687-8406. Filed under — Criminal Justice Police Protect Independent Journalism This story you’ve just finished was funded by our readers. We hope it inspires you to make a gift to ProPublica so that we can publish more investigations like this one that hold people in power to account and produce real change. ProPublica is a nonprofit newsroom that produces nonpartisan, evidence-based journalism to expose injustice, corruption and wrongdoing. We were founded in 2008 to fill a growing hole in journalism: Newsrooms were (and still are) shrinking, and legacy funding models are failing. Deep-dive reporting like ours is slow and expensive, and investigative journalism is a luxury in many newsrooms today — but it remains as critical as ever to democracy and our civic life. More than a decade (and six Pulitzer Prizes) later, ProPublica has built one of the largest investigative newsrooms in the country. Our work has spurred reform through legislation, at the voting booth and inside our nation’s most important institutions. Your donation today will help us ensure that we can continue this critical work. From the climate crisis, to threats to our democracy, to wealth inequality and much more, we are busier than ever covering stories you won’t see anywhere else. Make your gift of any amount today and join the tens of thousands of ProPublicans across the country, standing up for the power of independent journalism to produce real, lasting change. Thank you. Donate Now Eric Umansky Eric Umansky is an editor-at-large at ProPublica. eric.umansky@propublica.org @ericuman Umar Farooq Umar Farooq is an Ancil Payne Fellow with ProPublica. umar.farooq@propublica.org @UmarFarooq_",
    "commentLink": "https://news.ycombinator.com/item?id=38641198",
    "commentBody": "Police have undermined the promise of body camerasHacker NewspastloginPolice have undermined the promise of body cameras (propublica.org) 207 points by panic 20 hours ago| hidepastfavorite212 comments WillAdams 19 hours agoA police officer in performance of his duties should have zero expectation of privacy, and has a legal obligation to fill out an accurate duty log, which is such that it may be used as evidence in court.Body camera footage should be viewed as an extension of that, and when missing or unavailable, that absence should be viewed in the same light as a failure to properly document something in a duty log --- it is a dereliction of duty and of their obligation to be accountable to the public which they ostensibly serve. reply saalweachter 19 hours agoparentThis is another example of \"an untested backup doesn&#x27;t exist\", as well.Regardless of whether some of the time, a police department is full of bad apples and concealing or destroying body-cam footage, if you treat your body cams as a backup -- and only look at it when something goes wrong -- you&#x27;re going to discover that your body cams weren&#x27;t working all along and no one noticed.And then you&#x27;re going to look hella guilty of hiding evidence, even if it was just bad IT policies & procedures.If you have body cameras, you need a process where you frequently, randomly pull the footage from random officers, and check whether it was turned on, recording properly, filed correctly, whether the camera quality is uselessly grainy, etc etc etc.Otherwise, just like you don&#x27;t have backups if you aren&#x27;t testing your restore process, you&#x27;ll find out you don&#x27;t have body camera footage even when it would clear you of blame. reply kkielhofner 18 hours agorootparentTo my knowledge this is managed by the body cam vendors with the kinds of protections&#x2F;assurances you would expect for something that will need to pass evidentiary chain of custody, etc for something to withstand scrutiny in court:https:&#x2F;&#x2F;my.axon.com&#x2F;s&#x2F;article&#x2F;Uploading-and-charging-with-th...Almost kind of funny it uses the Axon-provided cloud at evidence.com. Not kidding.What this boils down to: if the camera footage has been altered or is unavailable some shady cop&#x2F;department went out of their way to do so. The workflow is extremely simple: hit the record button when you interact with the public, dock the camera (needs to charge anyway) at the end of your shift. There should be no wiggle room for excuses.The vendors have packaged all of this up decently and something like \"Oops we deleted that\" should never be acceptable. Even the pre-record activation rolling buffer should eliminate nearly any \"We missed that because I didn&#x27;t have time to activate record\" scenarios other than maybe \"someone ran up on me out of nowhere and started shooting&#x2F;attacking me\".Oh you \"forgot\" to press a button when you pulled someone over? You shouldn&#x27;t be a cop, plain and simple. reply brandonbloom 17 hours agorootparent(disclaimer: I was a software engineer at Axon)It’s also worth noting that recording can be activated wirelessly by various triggers. The most obvious and common one being that a nearby officer’s camera was activated (either by physically pressing the button or via a chain reaction of wireless activations).Depending on the available hardware&#x2F;accessories & configuration, other sources of activation can include unholstering a weapon, aiming or discharging a taser, by computer aided dispatch, unlocking a weapon in the vehicle, activating the light bar, high vehicle speed, running, falling, crashing, and more.In my opinion, if multiple officers are on the scene at least one axon recording device each: there is either video evidence or willful suppression of evidence. It’s that simple. reply irq-1 13 hours agorootparentPublicly sharing hashes of video segments as they&#x27;re created, would validate both the existence of, and unaltered nature of the video. reply woleium 10 hours agorootparentThis is one of the few good use cases for a blockchain outside of crypto reply Fatnino 15 hours agorootparentprevWhy is there a button at all? Record everything all the time. Sort it out later. The vast majority of it will be useless footage, but so what?If I was a cop I would certainly want every donut I eat to be recorded if it also means there is no change I get falsely accused of improper interaction with the public. 99.9% of the time no one will look at my donut footage. reply WillAdams 15 hours agorootparentI believe it would be acceptable for the camera to be turned off when the police officer steps into a bathroom during a break to use the facilities. reply Fatnino 3 hours agorootparentOK, fair. But should still be default on. reply NickC25 18 hours agorootparentprev>Regardless of whether some of the time, a police department is full of bad apples and concealing or destroying body-cam footageMake it a felony to conceal or destroy body cam footage.Also, make it so that if a cop is accused of violating rights or involved in foul play, and their body cam \"suddenly malfunctioned or were turned off or was left in the car\" , a default judgement is given against the officer in question.Make it as ubiquitous to cops as a parachute is for someone who goes skydiving. This must be enforced as harshly as possible. Cops aren&#x27;t operating on their own dime.If I&#x27;m a cop and the choice is leave my camera on or lose my job, my union membership, my retirement, my pension, and also face serious jail time, I&#x27;m leaving my camera on every time.The watchers MUST watch the watchmen, and the watchmen must have no expectations of privacy while on duty or being paid by the public. reply digging 17 hours agorootparentWhile I agree with what you say for the most part, it&#x27;s not a question of \"what rules can we add on top of the existing structure?\". Which is why simply adding bodycams has been so ineffective. We&#x27;re only really going to be safe from cops by essentially nuking the entire system and starting over.The core issue is that cops are far less vulnerable to prosecution or even non-legal accountability than almost any other citizens. Largely this is due to police unions. In addition to the structural inertia, there&#x27;s cultural inertia - cops don&#x27;t want to hold each other accountable either.This is why people talk about reforming police from the ground up. It&#x27;s actually easier than \"fixing\" the current system, which actively fights against accountability. reply phpisthebest 17 hours agorootparentThere one but 1 reform that will do the most... Putting an end to the moronic judicial created doctrine of \"Qualified Immunity\"it is very sad that we had the opportunity to do so in 2020, with both Democrat and Republican Support, but that opportunity was wasted by using that political interia and capital on the moronic \"De fund the police\" mantra&#x27;s where by the new status quo seems to be \"Well if the police act like criminals lets to all be criminals and never arrest anyone for anything\"....\"Qualified Immunity\" is one of the Root Causes of the policing problem, and it needs to end reply koolba 18 hours agorootparentprev> Make it a felony to conceal or destroy body cam footage.Destroying evidence of a crime is already a felony.Conceal is a tricky one though as depending on placement the camera could definitely be obscured by normal day to day action (like a tussle). But again, if it can shown the action was deliberate, it’d be an action in furtherance of a crime. reply kemayo 18 hours agorootparentThe suggestion I&#x27;ve seen is to legislate changing the presumption such that if body camera footage is missing then it should be presumed to be foul play unless the police department can show evidence otherwise. reply denton-scratch 14 hours agorootparentprev> Make it a felony to conceal or destroy body cam footage.You mean it&#x27;s not a felony to destroy evidence? reply smsm42 8 hours agorootparentIn theory, it is. In practice, prosecutors are very reluctant to go after cops for such type of crimes, or for any others that relate to their professional functions - perjury, excessive force, evidence tampering, violating disclosure rules, unlawful searches, etc. Plus, there&#x27;s qualified immunity thing, which makes it very hard to gain a successful conviction even if the prosecutor is willing. It&#x27;ll take a big SCOTUS decision dismantling that construct to start seeing any change there, and that may take a long while to happen. reply tjoff 18 hours agorootparentprevThis can be completely automated though. Noone has to watch any footage unless the system feels like something is wrong with it.And that could be in conjunction with the involved party. reply lotsofpulp 18 hours agorootparentSeems like good policy to randomly pull bodycam footage on a continuous basis for performance evaluation purposes. reply greedo 17 hours agorootparentThe article talks about how New Orleans (under Federal consent decree) would evaluate the constitutionality of police stops. reply falcor84 16 hours agorootparentprevAbsolutely agree with the analogy. And I&#x27;ll just say regarding backups that I really like this shorter formulation: \"An untested backup is not.\" Oh and I recall also hearing \"If you&#x27;re not gonna bother with testing it, then whatever you already have is good enough\" reply engineer_22 18 hours agorootparentprevSeems like a good idea from QC perspective. Large departments will need to hire additional staff to do the QC checks, but that&#x27;s not a major hurdle. reply crazygringo 18 hours agoparentprevOn the one hand, as a matter of principle -- absolutely.On the other hand, bodycams run out of batteries or the electronics glitch or automated video downloads get skipped, in ways that simply don&#x27;t happen with paper notebooks or data stored in central databases.To assume that it&#x27;s always a dereliction of duty is not going to be reasonable in practice.Like, for whatever reason my cell phone doesn&#x27;t automatically sync my photos to the cloud even though it&#x27;s supposed to. And instead of updating my apps overnight, it starts to update them when I unplug my phone each morning. And automatic OS updates never work, I have to do them manually. Technology&#x27;s great but technology sucks, too. And I have to assume bodycam software is less reliable than iOS&#x2F;Android. reply manicennui 18 hours agorootparentIt&#x27;s curious how these problems are highly correlated with the police doing questionable shit. reply Aachen 18 hours agorootparentIs that an assumption, or based on any particular data set or even just a few data points you&#x27;ve anecdotally read about?The article doesn&#x27;t mention this. It finds some cases where footage is (allegedly) unavailable, but does not look into how often footage is unavailable when nobody cares about it. If you think the cops are corrupt and delete footage of their own wrongdoing, that would need some backing up to be believable for me. Maybe in low-income&#x2F;low-justice countries that&#x27;s an assumption one can make, but not where I live at least. reply sonicanatidae 17 hours agorootparentYou can google for 2 mins and find many examples of hardware mysteriously failing while an officer was violating someone&#x27;s rights.You can also find many instances of the video being magically deleted..aw shucks.Are there exceptions, like you mentioned? Sure.Are there many, many instances of that being bullshit? Sure. reply Aachen 17 hours agorootparent> Are there exceptions [where it&#x27;s truly accidental]> Are there many, many instances of that being bullshit? Sure.If you&#x27;re trying to be fair by including the former sentence, then that should also include a \"many, many\" before \"exceptions\" unless you have the data I was asking after. As it stands, this adds no further information> You can google for 2 mins and find many examples of hardware mysteriously failing while an officer was violating someone&#x27;s rights.If you think the cops are so bad, what about the criminals? I&#x27;d also claim the cop was violating my rights if I did something wrong and it&#x27;s my word against theirs. How are you ever going to know what happened there? You can assume the cop was corrupt and the citizen innocent in all those cases, but if it were knowable then the citizen would have brought it to court rather than to the media, so I&#x27;m assuming it&#x27;s unknowable who was right in most of those casesThere has to be a fraction of cases where their rights were not being violated as claimed and footage is still unavailable. Whether that is 1&#x2F;10 or 9&#x2F;10, neither of us apparently has the information to make such a claim, but you&#x27;re making extraordinary assumptions without extraordinary evidence reply autoexec 14 hours agorootparent> If you think the cops are so bad, what about the criminals?The criminals don&#x27;t have the power of the state behind them. We&#x27;re supposed to hold police to a higher standard than criminals.> but if it were knowable then the citizen would have brought it to court rather than to the media,Why would you think that? Do you think that courts and lawyers are free? Most people in the US have to work to survive and live paycheck to paycheck. Missed work means they can lose their homes. Even if someone had the time and the money to fight against the police in court the courts are heavily biased in favor of the police and they would have to live in fear of retaliation for the rest of their lives.Even when police are caught on camera clearly committing crimes there is no guarantee that anything will be done about it. Evidence doesn&#x27;t always mean much when the police can investigate themselves and find they did nothing wrong.If police are supposed to have body cam footage and don&#x27;t, they shouldn&#x27;t be believed. reply Aachen 13 hours agorootparent> Evidence doesn&#x27;t always mean much when the police can investigate themselves and find they did nothing wrongCorrect me if I&#x27;m wrong, but I believe you&#x27;re mixing up two mechanisms of law here:In criminal law, the police investigates and the public prosecutor brings the case to court. The police often has a discretion to not bring a case if they don&#x27;t want to, or to indeed frame a case in their favor if they aren&#x27;t granted that liberty by the lawmakerIn civil law, you can sue for damages (such as lost time off work) or for other things that are made illegal (such as an organisation not following warranty legislation). The police is not involved here and bringing such a case is at your discretionThe latter system is what I had in mind when I said you could bring the evidence to court. Though it might also apply in the former case, in a situation where evidence is inadmissible if obtained illegally (this is not always the case) reply autoexec 13 hours agorootparentThe problem with civil cases is that you still need to have the money upfront (and accept that it will be very hard for you to win so you may never get compensated for those expenses) and then also overcome governmental immunity and qualified immunity> Even as the proliferation of police body cameras and bystander cellphone video has turned a national spotlight on extreme police tactics, qualified immunity, under the careful stewardship of the Supreme Court, is making it easier for officers to kill or injure civilians with impunity. The Supreme Court’s role is evident in how the federal appeals courts, which take their cue from the high court, treat qualified immunity. In an unprecedented analysis of appellate court records, Reuters found that since 2005, the courts have shown an increasing tendency to grant immunity in excessive force cases – rulings that the district courts below them must follow. The trend has accelerated in recent years. (https:&#x2F;&#x2F;www.reuters.com&#x2F;investigates&#x2F;special-report&#x2F;usa-poli...) reply sonicanatidae 16 hours agorootparentprevA preponderance of these missing videos happen when officers are violating rights. The sentences were written as intended.The difference between cops and criminals is criminals aren&#x27;t public servants, granted powers beyond the norm, so they can do their jobs. That being a literal fact, yes.. they are held to a higher standard or should be. You&#x27;re arguing a false equivalence.As far as knowing \"what happened there\", that&#x27;s literally the point of bodycams.There is a fraction of the cases where someone&#x27;s rights weren&#x27;t violated. A fraction, a lesser amount, a smaller subset, a number lower in value, etc. etc.And you apparently didn&#x27;t even try to look at the web, so...ok! reply phpisthebest 16 hours agorootparentprev>>How are you ever going to know what happened there?if only there was a recording device to record what actually happened....>>You can assume the cop was corrupt and the citizen innocent in all those cases,yes that is exactly how it should work, that is how the system is setup to work. We have some how flipped that in recent years. At all times you should assume the government is abusing you, unless they can prove they are not.No video, I side with the people not the government>>There has to be a fraction of cases where their rights were not being violated as claimed and footage is still unavailable. Whether that is 1&#x2F;10 or 9&#x2F;10It would be an error rate far far far far far less than that, and when you factor in most calls have more than one officers, plus patrol cars that have their own recording devices, plus citizen witnesses, plus business surveillance I think we get to such an unlikely circumstance to be statically impossible reply amelius 17 hours agorootparentprevIt&#x27;s probably more likely that people complain publicly about failed hardware if the hardware indeed failed.If you search for \"hardware was working fine\", yes, you probably won&#x27;t find much, because that&#x27;s not something people complain about. reply sonicanatidae 15 hours agorootparentWho&#x27;s talking about complaints&#x2F;feedback?I&#x27;m talking about numerous court cases, where bodycam footage was magically unavailable, when all of the circumstantial evidence pointed to crimes committed.I&#x27;m not after hunting all cops, or any of that horse shit.I&#x27;m referring to accountability beyond the current 6 week paid vacation, qualified immunity slapped on pretty much everything, being able to simply quit a job after committing literal crimes and get a job in an adjacent city doing the same thing, and stupid -isms, like \"Yeah, that guy died not because we choked him until he couldn&#x27;t breathe, but because of \"excited delirium\". reply bratbag 13 hours agorootparentprevOr are you just more likely to notice the overlap? reply jjk166 16 hours agorootparentprev> On the other hand, bodycams run out of batteries or the electronics glitch or automated video downloads get skipped, in ways that simply don&#x27;t happen with paper notebooks or data stored in central databases.Systems are going to fail, but they need to fail safely. If a bodycam is low on battery or not acting properly, there needs to be some notification for both the operator and department to know that the bodycam failed or is about to fail, and their procedure has to handle such cases. So for example have an indicator light that comes on at 20% battery power and officers need to return to the station and swap cameras when that light comes on, and battery levels at the end of a shift are logged automatically with a notification if any are returned below 10%.Of course genuine failures are going to happen from time to time. If an officer falls and damages their camera in the middle of a shootout, they&#x27;re not going to call a timeout so they can fix it. But such instances should be rare, documented, and reviewed. The department should at any given time be able to produce a list of what footage they are missing and why they&#x27;re missing it, and such reports should be audited by third parties. During the review, evidence should be collected - pictures of any damage, software logs, statements, etc. as well as actions taken during that missing time, which should be reviewed with increased scrutiny. So long as the discrepancy is properly documented, it&#x27;s fine, though procedures should be updated to prevent a repeat of this particular failure mode.Basically if a critical piece of footage is lost it should be easy to show that this was an isolated incident and that it won&#x27;t happen again. If they can&#x27;t, then there is a fundamental problem at the top level of management. This is all standard practice in other high stakes fields like pharmaceutical production. reply lamontcg 14 hours agorootparentprev> And I have to assume bodycam software is less reliable than iOS&#x2F;Android.Uh, no?I&#x27;d naive assume it would be much simpler and more reliable, and that failure to do something like recharge or upload data would log an exception somewhere that would get immediate attention. When the &#x27;IT guy&#x27; goes to work the next morning they should see that one of the bodycams didn&#x27;t do its thing the night before and that bodycam should get replaced. The maximum window of failiure should be 24h or less, and multiple failures happening to all responding officers shouldn&#x27;t really be possible since they should have a MTBF exceeding a year. reply stronglikedan 16 hours agorootparentprevThey check their weapons before each shift. They should check their body cams just as thoroughly. reply spurgu 17 hours agorootparentprev> And I have to assume bodycam software is less reliable than iOS&#x2F;Android.I&#x27;d argue that it&#x27;s waaaay easier to make stable bodycam software than a stable mobile phone OS. reply crazygringo 16 hours agorootparentExcept that I don&#x27;t think the bodycam companies are hiring the same caliber of engineers that Apple and Google are.If they&#x27;re like most other device companies, they&#x27;re hiring the cheapest programmers they can find to write something quick and dirty, and not doing anything close to the level of review and testing that Apple&#x2F;Google do. reply spurgu 16 hours agorootparentFair point. reply babyshake 17 hours agoparentprevThe simple answer is for there to be consequences. Turning off or camera or deleting the footage should be treated the same as not reading a suspect their Miranda rights. reply denton-scratch 14 hours agorootparent> Turning off or camera or deleting the footage should be treated the same as not reading a suspect their Miranda rights.It&#x27;s your view that deliberately concealing evidence is no worse than not handling the administrivia of an arrest correctly?As I recall, Miranda was introduced specifically to force police departments to tell arrestees their rights, which the law already required them to do. Miranda made it possible for courts to dismiss charges, if the suspect had been questioned without being told his rights. Trying to compel the police to obey the law just because it was the law simply failed. Th only way it could be enforced was to get several prominent prosecutions (of guilty people!) dismissed. reply jauntywundrkind 4 hours agorootparentprevBro I have extremely bad extremely 2023 supreme court news for you, vis a vie Miranda Rights. reply FireBeyond 18 hours agoparentprev> A police officer in performance of his duties should have zero expectation of privacy, and has a legal obligation to fill out an accurate duty log, which is such that it may be used as evidence in court.However, in 35 states, and this blows my mind, an officer can have \"consensual\" sex with a detainee. Even if they are in handcuffs at the time. 35 states have decided that this is not improper from an \"authority\" perspective (leaving aside for now the whole \"having sex while on duty\").There was a recent case where a minor who was in custody and handcuffed had sex with two officers in their patrol vehicle. The officers claimed it was consensual and the courts agreed. I do not believe they even faced disciplinary action.> Instead, police departments routinely refuse to release footage — even when officers kill.Because police and police departments and police unions know that even post-BLM, they will face little, usually zero, consequences for doing so, or for wasting taxpayer dollars fighting to defend their refusal. reply denton-scratch 13 hours agorootparent> The officers claimed it was consensual and the courts agreed.That&#x27;s pretty nutty. I don&#x27;t know the backstory, but even if it&#x27;s \"consensual\" it&#x27;s still statutory rape to have sex with a minor.> I do not believe they even faced disciplinary action.Not even that administrative process? How come the courts had anything to say about it, then? ...I guess that must have been the minor&#x27;s court appearance, not the cops. Under the circumstances, I guess that minor might have been disinclined to give defence evidence that the arresting cops had raped her. reply FireBeyond 11 hours agorootparent> I don&#x27;t know the backstory, but even if it&#x27;s \"consensual\" it&#x27;s still statutory rape to have sex with a minor.I believe she was a minor but above the age of consent in that state.And yeah, there&#x27;s the whole question of \"why are cops having sex while on duty?\" - that&#x27;s inappropriate even if it&#x27;s your spouse, let alone someone you&#x27;re interacting with in an official capacity, let alone someone who is in custody and detained, let alone someone who is handcuffed at the time.There are so many separate levels of \"what the actual f\" here. reply engineer_22 15 hours agorootparentprevThese are shocking claims, please provide sources. reply FireBeyond 14 hours agorootparent> In February 2018, BuzzFeed News reported that laws in 35 states allowed police officers to claim that a person in their custody consented to sex, and that of at least 158 law enforcement officers charged with sexual assault, sexual battery, or unlawful sexual contact with somebody under their control from 2006 to 2018, at least 26 were acquitted or had charges dropped based on the consent defense.Realize that being charged means that at some point, the person in custody objected enough that charges were filed by the DA&#x2F;prosecutor.Source: https:&#x2F;&#x2F;www.buzzfeednews.com&#x2F;article&#x2F;albertsamaha&#x2F;congress-c... reply jquery 17 hours agorootparentprev35 states allow for that? I would&#x27;ve assumed 0 states allow for it, because, well, come on. Might as well let officers consent to taking bags of money from detainees. reply greedo 17 hours agorootparentMost states allow for \"taking bags of money from detainees\" under civil asset forfeiture. reply FireBeyond 17 hours agorootparentprev\"Thankfully\", following on from the outrage following this (it was 2018), some states have criminalized it... though only six, now that I look into it. So the accurate number is now 29.Hell, it only became criminal for federal police 18 months ago!Source: https:&#x2F;&#x2F;www.buzzfeednews.com&#x2F;article&#x2F;albertsamaha&#x2F;congress-c... reply digging 16 hours agorootparentprevNot sure if you&#x27;re being sarcastic, but it&#x27;s called civil forfeiture. reply PH95VuimJjqBqy 18 hours agoparentprevThe problem is that most people don&#x27;t consider how much of that footage has privacy implications.They deal with people very single day, they need to be able to turn those cameras off in the case of, a nude minor, a detailing of psychiatric history of a minor, etc.The problem is that giving them that ability also opens the door for them to abuse it. There is no easy&#x2F;good answer here.And I&#x27;m not making excuses for them, if they&#x27;re abusing it go scorched earth on them please, their position in society means they require it.But police MUST be able to turn their cameras off. reply CptFribble 18 hours agorootparentI disagree, police should not be able to turn off the cameras, but the footage should NOT be public by default. Instead, the footage should be regularly audited by a vetted third party not connected to the police, police union, of families of police.The footage should of course be archived for some long period of time measured in years for use in proceedings, and audits the discover significant periods of no footage not attributable to technical malfunction should be grounds for immediate dismissal of the officer.Making allowances for the officer to decide when it&#x27;s \"appropriate\" to turn off the camera will inevitably lead to the current situation of the cameras being not used nearly enough - only instead of \"oops it was off\" it will become \"I used my judgement that this was a sensitive event and turned it off, whoops now no one knows who shot first\" reply rekoil 18 hours agorootparentThis is my take as well. Complement it with asking the officer to try to identify periods where the video might need to be deleted to save a whole bunch of time for those working to process the data. reply sokoloff 18 hours agorootparentIt’s not at all obvious to me that any expensive or time-consuming processing of routine&#x2F;boring bodycam video is needed. reply PH95VuimJjqBqy 18 hours agorootparentprevyeah, that&#x27;s what we want, 3rd parties having access to SSN&#x27;s, addresses, psychiatric details, etc.This all works in theory but not practice. If we were try to implement that in 20 years people would be complaining about fraud tracked down to some minimum wage worker reviewing tapes for these 3rd parties.this is why I said there&#x27;s not an easy answer. reply Broken_Hippo 18 hours agorootparentYou already have this to an extent - how many people touch your medical records? I can look up people&#x27;s past addresses online, it isn&#x27;t all that private - which is fine for most folks, but bad for folks getting stalked.Heck, I worked at a landline phone company 25 years ago and they used the last 4 numbers of a social security number to verify everyone. Think about this every time you talk to a utility: That underpaid worker definitely sees your information. Plus addresses and names and so on.But more realistically, there is no real reason that the person viewing the footage needs any of that information. It might be helpful to know that the person is having a mental health crisis, sure, and it might be helpful to know if the person is a minor. But they don&#x27;t need to know the names of folks nor what the diagnosis is. Treat it like health information: Give nothing but the necessary information. And if society can trust an underpaid, stressed call center employee to have a bunch of this information, I&#x27;m pretty sure we can trust these underpaid, stressed folks too. reply PH95VuimJjqBqy 17 hours agorootparentI don&#x27;t think you understand the risks you&#x27;re talking about.For example, people have been blackmailed over psychiatric records from their childhood. reply Broken_Hippo 12 hours agorootparentAnyone determined can already do that. A quick google search pulled up examples - most of the first page results were folks breaking into mental health facilities data. Which makes sense: Why would you sift through loads of mundane police footage when you can just look at the care center&#x27;s records?The risk would remain about the same. Most folks aren&#x27;t gonna do this, and those that want to already can.And we could lessen it by being more accepting and forgiving and not making things like mental health struggles a source of shame. reply PH95VuimJjqBqy 12 hours agorootparentDo you know why the thieves broke into the mental health facilities?because many of them put patient records in physical safes. Do you know why they don&#x27;t put them online?yeah, exactly.What you&#x27;ve basically said is \"it&#x27;s ok because someone could break into a police station to get that material anyway!\".Or to apply this in another way.I should be ok with other men sleeping with my wife because she might get raped anyway.it&#x27;s such a ridiculous line of thought. The fact that someone somewhere is willing to enter into criminality to do it doesn&#x27;t imply we shouldn&#x27;t endeavor to prevent it. reply inkcapmushroom 18 hours agorootparentprevThe third party in this case is probably the oversight boards that were referenced in the article. They should not be going to the police requesting footage and being denied, they should be the ones who control all the footage, and the police should go to them to request it. That&#x27;s the only way the oversight boards can actually do their jobs. reply lannisterstark 18 hours agorootparentprev>yeah, that&#x27;s what we want, 3rd parties having access to SSN&#x27;s, addresses, psychiatric details, etc.?? This already can happen by default for a LOT of people. I work as a subcontractor for a federal contractor in healthcare, and I can see all of them if I wanted to. We just don&#x27;t. reply PH95VuimJjqBqy 17 hours agorootparent> I work as a subcontractor for a federal contractor in healthcare, and I can see all of them if I wanted to. We just don&#x27;t.I also work in healthcare, I&#x27;d like to see you make that request.edit: To draw an analogy. you may work for a credit bureau but if you start requesting records for celebrities you&#x27;re not going to be working there for very long.I&#x27;ve _built_ the systems put in place to limit and monitor for these sorts of activities in healthcare specifically as a result of the liability it puts the company under due to federal regulation. reply lannisterstark 16 hours agorootparent>I&#x27;d like to see you make that request.Cute of you to think that there&#x27;s a &#x27;request&#x27; part. My point was that we do need to check them to see if x site&#x2F;app&#x2F;tab is working or not after patching. It&#x27;s part of our job.Edit: I think the confusion here might be you&#x27;re thinking US Federal. I am not. reply PH95VuimJjqBqy 15 hours agorootparentHIPAA has requirements for logs and that includes logging access to data. What company is this that isn&#x27;t following those regulations such that you can crack open raw logs at any time? reply lotsoweiners 9 hours agorootparentBut what if this person has direct access to the database? I’m a developer for my local government and have direct read&#x2F;write access to many system’s production data. I could not only read but also update important info about people without going through an application all while logged in as the applications SQL ID. In fact I have to do this to handle tickets that are caused by limitations in the system. reply FireBeyond 17 hours agorootparentprevThen you would know that HIPAA allows for one subset of the above, health information to be shared for treatment purposes even without patient authorization, as long as reasonably safeguarded.I&#x27;m not saying that encapsulates everything that person says. But \"reasonable safeguards\" absolutely include a lot of the things you say are unacceptable.I worked for a company that built claims benefits management systems, including one for SAG-AFTRA. Notably, that one, because they were about the only customer who was absolutely militant about lockdown of access to data (because it would show celebrity healthcare claims).Actually, the biggest challenge we had was from our customers wanting to do data mining that was federally illegal (like looking at familial healthcare data to determine predisposition for a covered person for a certain condition). reply PH95VuimJjqBqy 15 hours agorootparentwhat the person said was \"we could access any data we want, we just don&#x27;t\".There&#x27;s no way they fall under one of those exemptions, especially if they don&#x27;t need it to do their job. If anything, the statement \"we just don&#x27;t\" is indicative that they wouldn&#x27;t fall under those exemptions.But really the point was that this stuff is heavily regulated. If a company isn&#x27;t following those regulations that&#x27;s going to bite them in the ass eventually.Typically speaking, you can convince auditors of a lot of things but it only takes getting the wrong auditor for it to all go down hill. reply carlosjobim 17 hours agorootparentprevThat third party is sometimes a roster of lawyers selected by the court in that jurisdiction. These dilemmas are not new, nor body-camera specific. reply denton-scratch 13 hours agorootparentprev> they need to be able to turn those cameras off in the case of, a nude minorWhy? It&#x27;s not a mobile phone, they can&#x27;t \"save the video to the cloud\" - it gets downloaded when they return to the station, by the charger. reply Aachen 18 hours agorootparentprevFilming nude adults is fine and not a privacy issue for those involved? If anything, most societies accept nude children (especially around bodies of water) much more readily than adults.Not sure why you&#x27;re calling out children specifically: they can&#x27;t give consent but, if asked, I expect that adults would rarely give consent for being filmed by the police either, even when dressed as well as one can be! reply PH95VuimJjqBqy 18 hours agorootparentI was trying to provide an example clear enough to be readily understood by readers, in no way, shape, or form, was I trying to imply my list was complete. reply Aachen 18 hours agorootparentI wasn&#x27;t asking if the list is complete. I was asking why this is even an example at all because I don&#x27;t understand the reasoning reply PH95VuimJjqBqy 17 hours agorootparentI believe you do understand the implications of filming a 10 y&#x2F;o nude girl. reply Aachen 13 hours agorootparentI believe you don&#x27;t, because there are none in a normal case, such as when it&#x27;s as part of your legitimate jobPerhaps that&#x27;s not common knowledge? It&#x27;s not illegal to have pictures of nude children insofar as I know laws in various countries. What&#x27;s illegal presumably everywhere is if there are sexual suggestions (or more than suggestions), which presumably would not be the case in your exampleAlso, police has to handle these sorts of pictures for prosecution anyway. They already have certain exemptions. Even if you had been right, that could have been extended to cover these cameras while on duty reply PH95VuimJjqBqy 12 hours agorootparentthis is the internet, where saying a thing doesn&#x27;t make it true.have at your claims good sir. replyFireBeyond 17 hours agorootparentprevNo, the solution there is appropriate security of footage, appropriate access control, and appropriate vetting of footage before release.But there should be little to no discretion.There are security cameras in hospital ERs (I&#x27;m a paramedic) where I absolutely assure you even outside rooms, in the open areas, people in states of undress (from MVA&#x2F;trauma, etc., and us working on them) and people in behavioral crisis are a daily occurrence. Those cameras are not turned off then. reply tzs 17 hours agorootparentHow long is the footage kept, and who can get access to it? reply korse 17 hours agoparentprevWe need the police. I guarantee the alternative power structures that would develop in their absence would be worse.This said, if we want people to sign up for the role, there needs to be some protection and privacy in place for them. They are also people, with families and in some cases organized and dedicated opposition.In pursuit of these goals, zero expectation of privacy seems like an unrealistically high bar. reply WillAdams 16 hours agorootparentZero expectation of privacy in terms of record-keeping of their performance of their official duties as reviewed by suitable authorities and legal organizations. reply engineer_22 18 hours agoparentprevI think a jury would be very sympathetic to this argument. reply kemayo 18 hours agorootparentYou&#x27;d think so, but the average jury is extremely deferential to police.In part because a lot of places include some questions about how you feel about the police in the standard jury screening interview, and expressing any negative opinion about police-reliability is likely to get you struck.e.g. check out the NYC standard set of questions: https:&#x2F;&#x2F;www.nycourts.gov&#x2F;judges&#x2F;cji&#x2F;8-Colloquies&#x2F;Voir%20Dire... -- note that 11.a is going to automatically exclude anyone who says something like \"I&#x27;ve heard about the blue code, thanks\". reply engineer_22 15 hours agorootparentText in question:-> 11. Police questions: (a) Have you had an interaction or experience with a police officer or other law enforcement person, or formed an opinion about police or our criminal justice system that would prevent you from being a fair juror? (b) Under our law, a police officer is not more, or less, believable just because he or she is a police officer. A juror must evaluate a police officer&#x27;s testimony for truthfulness and accuracy in the same way the juror would evaluate the testimony of any other witness. is there any reason you cannot do so?If a candidate expresses an opinion that would be deferential to either side, they are likely to get struck. This is a feature, not a bug. reply greedo 17 hours agorootparentprevIt&#x27;s not just the average jury, but prosecutors, judges, mayors, city councils. Police get a pass for so much misconduct on an individual and institutional level it boggles the mind. reply btbuildem 18 hours agoprev> And so as policymakers rushed to equip the police with cameras, they often failed to grapple with a fundamental question: Who would control the footage? Instead, they defaulted to leaving police departments [..] with the power to decide what is recorded, who can see it and when.I think it&#x27;s incredibly naive to portray this as a \"defaulting\" or \"failing to grapple with a fundamental question\".I am 100% certain \"who controls the footage\" was the first question out of the police brass&#x27; mouths, and the crucial point in negotiations, where conceding to let the police themselves control the footage allowed the use of bodycams in the first place.Nothing was left to chance here, this is entirely deliberate. reply indymike 17 hours agoparentBody cameras + youtube&#x2F;x&#x2F;tiktok have been absolute lifesavers for people assaulted or falsely arrested. Video is really the only buttress against \"your word versus OUR WORD\".At the same time, talking to my neighbors (I have about five officers that live on my block), they like the cameras because most of the time they either prove the case, or show that there really was no abuse (yes, the arrestee did punch the cop).I think where the issues is when something happens that creates a liability. An officer violates the law, or someone&#x27;s rights while making an arrest. It creates a situation where the incentive is to railroad the accused at all costs so the accused cannot file or have little chance to win a federal civil rights lawsuit. In these cases it&#x27;s amazing how difficult to get that video can be. reply engineer_22 18 hours agoparentprevIf you don&#x27;t believe Management Confidential down at the department is worthy of the public trust, you have a bigger problem on your hands.In my locale those folks are elected or appointed, so if you cant trust them, yea, you&#x27;ve got a problem. reply greedo 17 hours agorootparentThere&#x27;s a group that both funds office seekers, as well as votes consistently. Police unions... reply danaris 18 hours agorootparentprevA number of very high-profile police departments—in their entirety—have shown themselves empirically to be unworthy of the public trust in the past few years.Yes, we have a very big problem on our hands. The system itself is deeply corrupted by perverse incentives and by a very large number of people with strong leanings toward authoritarianism and fascism. reply aaomidi 18 hours agoparentprevWhy are we negotiating with police for this anyway?And if the police union is threatening to strike, then who cares. Those cops are not the ones we want anyway. Rehire. reply HDThoreaun 12 hours agorootparentIf we dont give them what they want they stop working and crime increases. It happens every couple years in my city when the police are negotiating their contract. Having zero police is no acceptable to voters and would lead to losing re-election for almost every politician that tries to lock them out. reply meepmorp 18 hours agorootparentprev> Why are we negotiating with police for this anyway?Because if you don&#x27;t, you get election ads with scary looking footage and a voice-over that says something like:\"Mayor&#x2F;Governor&#x2F;whatever Smith&#x27;s anti-police crusade is putting you and your family in danger. This November, send a message by voting out the pro-crime politicians.\"You could argue that politicians should make a principled stand against this and do the right thing anyway, and you&#x27;re right in a moral sense. But there&#x27;s good odds that those guys will get replaced with people who are more willing to give the police what they want. reply waihtis 18 hours agorootparent> give the police what they wantLike the ability to actually do something when a looter horde raids a department store? reply danaris 18 hours agorootparentLet&#x27;s put this in perspective.Something like that might happen once or twice every few decades. In the country.Meanwhile, the abuses the police are perpetrating, in many cities, are happening on a monthly basis (or more often).Even if we stipulate that we need someone capable of dealing with the looter horde, we don&#x27;t need that capability to be in the same hands as the people who are handing out traffic tickets every day. reply sokoloff 17 hours agorootparentYour estimate of the frequency of organized looting and mine differ by several orders of magnitude. reply waihtis 17 hours agorootparentprevyour statement is easily proven to be total BS by a single press release:https:&#x2F;&#x2F;corporate.target.com&#x2F;press&#x2F;statement&#x2F;2023&#x2F;09&#x2F;target-... reply digging 16 hours agorootparent...which also \"proves\" that cops aren&#x27;t doing jack shit about organized looting, so that&#x27;s no longer a useful argument. reply waihtis 16 hours agorootparentOf course they aren&#x27;t, when apprehending a violent suspect in the \"wrong way\" may land you jobless and in jail. The looters and other criminals are operating under a protection racket run by the so-called democrats, see for example: https:&#x2F;&#x2F;www.sfchronicle.com&#x2F;sf&#x2F;article&#x2F;sf-dean-preston-socia... reply autoexec 14 hours agorootparent> Of course they aren&#x27;t, when apprehending a violent suspect in the \"wrong way\" may land you jobless and in jail.If the \"wrong way\" means beating them while handcuffed or choking them to death then I don&#x27;t see what the problem is. Any police officer who is too scared to do their job because they don&#x27;t think they can even stop a shoplifter without violating that person&#x27;s rights isn&#x27;t cut out to be a cop and shouldn&#x27;t have the job in the first place. reply greedo 17 hours agorootparentprevYou do realize that this has been shown to be absolutely false? That losses due to shoplifting were used as BS justifications to shut down stores?https:&#x2F;&#x2F;www.nytimes.com&#x2F;2023&#x2F;01&#x2F;06&#x2F;business&#x2F;walgreens-shopli...https:&#x2F;&#x2F;www.reuters.com&#x2F;business&#x2F;retail-consumer&#x2F;us-retail-l... reply waihtis 17 hours agorootparentNeither of your articles says what you are claiming. Have you yourself read the articles you&#x27;re linking? reply greedo 13 hours agorootparentSo Target claiming their closing stores across the nation due to their lies about shoplifting etc doesn&#x27;t connect in your mind to this?https:&#x2F;&#x2F;www.cnn.com&#x2F;2023&#x2F;09&#x2F;26&#x2F;business&#x2F;target-retail-theft-...\"Target is closing nine stores in major cities across four states, claiming theft and organized retail crime have made the environment unsafe for staff and customers – and unsustainable for business.\"Yet the NYT article says:\"Target’s chief financial officer, Michael Fiddelke, said shrinkage had reduced the company’s profit margin by more than $400 million in 2022,\"as justification for closing stores. Yet their data was BS as shown in the NYT article. reply jquery 17 hours agorootparentprevUmm, I&#x27;m familiar with one of those locations that closed. It had very little parking, was extremely understaffed (1 active cashier working slowly...), and didn&#x27;t have much foot traffic. There weren&#x27;t any bags to put my stuff in when I checked out. It was such a poor experience I never went back there, despite it being the Target nearest to me. I was zero percent surprised when I heard it closed for good. Businesses and business franchises fail all the time in cities, Target is no exception.In other words the press release is just dodging responsibility. reply waihtis 17 hours agorootparentI&#x27;m also familiar with one of the locations that closed. While I was shopping, it was raided by 80 people who also acted violently towards the patrons. It was such a poor experience I never went back there, despite it being the Target nearest to me. I was zero percent surprised when I heard it closed for good. reply jquery 3 hours agorootparentAping the style of my comment and being vague on any details makes it sound like you&#x27;re just lying. :-&#x2F; reply waihtis 3 hours agorootparentGuilty as charged. replymeepmorp 18 hours agorootparentprevIn the context of the conversation, the interpretation of \"give police what they want,\" is clearly something like, \"defer to the departments about how they handle bodycam footage, policies about recording, and discipline for officers that don&#x27;t conform to related laws and policies.\"The fact that you chose an unrelated topic couched in deliberately inflammatory language only shows bad faith on your part. reply waihtis 18 hours agorootparenttrue and I apologize. But I also think it can be tied to the larger picture where the police is both forced into better accountability (good) and stripped down of the ability to do their job (bad), for which \"organizational resistance\" is a completely valid countermeasure. reply greedo 17 hours agorootparentCan you list some of the ways police have been stripped of their ability to do their job? reply waihtis 15 hours agorootparentSome DA&#x27;s sudden refusal to prosecute a variety of property and drug crimes is a good example. No use catching criminals if they&#x27;re just let loose. reply Der_Einzige 8 hours agorootparentprevSee all the law changes in Oregon since 2018. Cops for example can’t pull you over for ghetto cars anymore. Cops can’t use the smell of weed as probable cause, and far far more. reply zo1 18 hours agorootparentprevIf they were given the ability to properly and violently stop these shenanigans (violent store and street mobs&#x2F;takeovers), you bet that the next week there&#x27;d be more BLM looting sprees. This is nothing but bullies pushing their way and the rest of civil society being too scared to push back because they don&#x27;t want to escalate to the level of violence that the bullies are very comfortable with. Whether we&#x27;re discussing unions, false accusations, skilled orators that capture and direct hatred, or violent rioters that hide under the guise of \"protest\". reply waihtis 18 hours agorootparent100% spot on reply Der_Einzige 8 hours agorootparentprevDoesn’t matter how much you pay them, if the cops think the people hate them, they will refuse to take jobs in a locality.Portland PD is paying far above average and is funded to hire hundreds of cops, but no one applies because the cops know they’re muzzled in Portland and they know that the local people abd laws don’t support a “thin blue line”The remaining cops who stayed on have “quiet quit” and they use that as a tactic to get the people to give them more power. It’s a terrible situation. The cops know they can hold the people hostage and they aggressively do that. reply lsy 18 hours agoprevBody cams were always a band-aid solution on top of over-arming police and granting them almost total immunity from charges of misconduct. The resulting mentality of us-vs-them and mafioso, bullying behavior from police is all down to feeling a cut above “civilians” and being confident in their power to legally murder anyone who’s too resistant.This behavior also discourages any thoughtful people with integrity from applying, and beats it out of them quickly when they do. The streets would be a lot safer with more citizens performing a genuine public safety role with fewer weapons and chips on their shoulders, but the police lobby essentially holds cities hostage by monopolizing the public safety role and strategically withdrawing from that role in protest whenever an insufficiently deferential policy is passed. reply Aachen 17 hours agoparent> being confident in their power to legally murder anyone who’s too resistant.* in a few select countries around the world. This is a rather narrow view of police(wo)men as a blanket statement. reply bjornsing 17 hours agorootparentIs it? I think Sweden is probably recognized as one of the most civilized countries in world in this regard, but here too the police can murder anyone who’s too resistant and very likely get away with it. I know of several cases that ended up in bizarre acquittals, and none that ended in a conviction.They are sometimes sentenced in the court of public opinion though, as with Osmo Vallo [1].1. https:&#x2F;&#x2F;sv.m.wikipedia.org&#x2F;wiki&#x2F;Osmo_Vallo reply gtmitchell 18 hours agoprevWe&#x27;re trying to use technology to solve a legal &#x2F; political issue, which predictably ineffective results.Unfortunately, I don&#x27;t even think improved state oversight will help until the underlying issue of qualified immunity is addressed. The current legal framework in the US literally rewards police officers for being ignorant of the law. reply bitcharmer 17 hours agoparentI have no idea why you&#x27;re getting down voted. The amount of BS and straight out criminal conduct cops can get away with due to immunity is sickening. reply sonicanatidae 17 hours agoprevAnyone else shocked by the hardware mysteriously failing, while a cop is violating someone&#x27;s rights, but works perfectly at all other times?How about the simple fact, that if you delete data that would be used in a court case, the court will automatically assume the worst case, while cops can do it, and the penalty? Nothing! reply ranting-moth 18 hours agoprevAdverse inference should be be applied. Seriously. reply Aachen 17 hours agoparentFor anyone else to whom this term is new: https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Adverse_inference> is a legal inference, adverse to the concerned party, drawn from silence or absence of requested evidence. [...] According to Lawvibe, \"the &#x27;adverse inference&#x27; can be quite damning at trial. Essentially, when plaintiffs try to present evidence on a point essential to their case and can&#x27;t because the document has been destroyed (by the defendant), the jury can infer that the evidence would have been adverseThe first paragraph sounds a lot like \"assume the worst in people\" (must be a nice way to go through life); the second paragraph specifies it only in the case of intentionally destroyed evidence (that seems reasonable when you can know it was intentional). Interesting concept reply ranting-moth 17 hours agorootparent> This rule applies not only to evidence destroyed but also to evidence existing but not produced by the party.If the police is refusing to release a video to prove their stance, there is absolutely no need to give the police the benefit of the doubt. reply wavemode 17 hours agoprevI&#x27;m just going to give my personal opinion - I think this article does a great job of arguing, in a somewhat roundabout way, that the NYPD has failed to create proper accountability through the release of body camera footage. It really makes no sense that all body cam footage is not automatically public record in the case of the death of the civilian.But at the same time, it does a poor job of arguing that the two recorded shootings it spends most of its word-length on (Richards and Trawick) were actually wholly unjustified. The author probably should have chosen some different, more egregious cases to make their point. reply greedo 17 hours agoparentIn both shootings, the officers didn&#x27;t follow standard procedure (let the ESU take over the situation), and instead went all Rambo on the victims. Then failed to render aid after they pumped dozens of rounds into each victim. Or they let had a dog handler attack a suspect (killing him), then laughed about \"getting a bite.\"All three of these cases described are disgusting instances of police overreach and failure. reply wavemode 15 hours agorootparentLook, this is always a pointless debate to go back-and-forth about because it tends to become more dominated by emotion than logic.What I will say is that I do believe all the incidents described in the article were tragedies, and sadly preventable. And I do believe there were breaches of protocol by officers at the scene in all of the cases.But I just plain don&#x27;t believe that the turning point moment for Richards and Trawick was the officers&#x27; decision to use force. If a mentally ill person has reached the point of charging at police officers holding a knife (and&#x2F;or whatever Richards was doing with his knife and fake gun), they were already failed, woefully, by society and by their support network. reply fasteddie31003 18 hours agoprevWatching police body camera footage has given me much more appreciation for the tough job police have. I can&#x27;t imagine having people actively try to kill me in my day-to-day job as a software engineer. reply kkielhofner 17 hours agoparentIt&#x27;s been really interesting to watch the rise of dedicated police body cam YouTube Channels. There are TONS of them and they often have millions of subscribers. They more-or-less paint two pictures:1) Wow, this job is TERRIBLE. People doing stuff like committing a crime on video, going on a high speed chase, crashing into multiple cars and injuring people, finally crashing the car into a tree, shooting at the police, running on foot, hiding behind a building, and then fighting to the point it takes four-five cops to subdue them but then constantly screaming stuff like \"I didn&#x27;t do anything! Why are you hassling me? You better hope I don&#x27;t find out where you live because I&#x27;m going to [insert the most disgusting, profane, and explicit sex crime stuff you can&#x27;t even think of] to your wife and daughter\". All while claiming they&#x27;re having an asthma attack even though they just sprinted 1&#x2F;4 mile. Then it takes the same number of cops 10 minutes just to get them in the police car because they&#x27;re still fighting, spitting, kicking, who knows what.The really bizarre ones are once they get in the car their primary concern is where their phone ended up... They end up at the jail with a \"spit hood\" over their head and their entire body strapped to a special wheeled chair where they can&#x27;t move at all because they&#x27;re still that dangerous. Most of us here cannot imagine it. It&#x27;s the wildest stuff you&#x27;ve ever seen - hence the millions of subscribers.2) Wow, this cop is TERRIBLE. Obvious racial profiling, screaming at people for doing nothing or committing minor infractions, beating&#x2F;killing people for no reason, inventing charges, etc, etc.People are people and it&#x27;s a cross-section: terrible people doing terrible things on either side of the law. Obviously the police should be held to a higher standard but more often than not my take away is \"I don&#x27;t know how cops do this job without turning into completely cynical, jaded, paranoid, and aggressive maniacs\". I&#x27;ve been on about 24 hours total of ride alongs in some \"rough\" areas and hit my lifetime quota. It&#x27;s that disturbing, terrifying, and high-stress. After each \"shift\" it would take me a while just to start to reconcile what I experienced.I think anyone would have a tough time dealing with someone who (for example) tries to kill you, threatens your family, spits on you, etc while still treating them decently and humanely. In many areas this is over and over, day after day.In the US especially police are basically the \"catch all\" for all of the other (ample) problems in our society. The people on both sides are often in impossible, no-win situations. reply greedo 17 hours agorootparentYou get to see the videos the police want you to see; the videos that reinforce the idea that cops are in a continual state of absolute danger despite all the occupational studies that show it&#x27;s a relatively safe job. Copaganda... reply kkielhofner 16 hours agorootparentI was hesitant to write this comment because even though I consider my stance fairly nuanced (with my comment reflecting that) dismissive statements like \"copaganda\" don&#x27;t make for any kind of reasonable debate. But I&#x27;ll try.When I was on a ride along and I watched a guy who just brutally beat his pregnant girlfriend (who was bleeding on the ground in the front yard) charge at us and end up rolling around on the ground with the cop just what they \"wanted me to see\"? Turns out he was \"going hard\" because he had a warrant for armed robbery.Same goes for the time the cop pulled over a bunch of gang members (Latin Kings) who threatened to kill us and started making threatening movements in the car (we backed up and he called in some kind of \"tactical team\" that blocked the street and ordered them out one by one).Or when I watched a guy shove an EMT out of his front door, run in the kitchen, grab a knife, and swipe it at the cop because he beat his partner unconscious who was bleeding from the head on the kitchen floor? I was personally and directly in multiple situations where I witnessed these things and was terrified and probably a little traumatized as a result.There&#x27;s more but I&#x27;m not kidding when I say this stuff was traumatic. This was 10 years ago and it&#x27;s still fairly difficult for me to talk about but I do occasionally because of viewpoints and responses like yours.This is not \"what they wanted me to see\". Ride along programs exist to give ordinary citizens a view into a \"day in the life\". Of course when on a ride along they will be on their best behavior but for what it&#x27;s worth the officer I was with was an acquaintance of mine so he didn&#x27;t hold back doing horrible things like \"Oh that girl looks hot, she has a taillight out. I want to introduce myself\". He pulled her over, talked her out of the car to \"look at the light\", and spent a while flirting with her. Seriously, that happened too. Disturbing.In the two domestic violence situations I described I was a little uncomfortable at the level of force he used to respond to them, which is so confusing and conflicting because moments before that I thought I was going to get killed or seriously injured. It really messes with you. Was it \"reasonable\" to order those gang guys out of the car, make them lie face down on the ground, handcuff them, call in gang unit detectives to line them up, take pictures of them, and question them handcuffed while sitting on the curb? To this day I have no idea.I have seen the studies. In terms of death, serious injury, etc law enforcement doesn&#x27;t even crack the top 10 most dangerous professions. However, in my experience I was in very real fear for my life at least once each time I went out. I either ran away or stood there paralyzed thinking \"If this guy wins this fight I&#x27;m dead\". It&#x27;s not a good sign when you finally realize \"They think I&#x27;m a cop. Problem is I have no weapons, no protection, and no training. This is even more dangerous for me.\" At one point the cop did something like \"I&#x27;m not supposed to show you this but if things get really bad here&#x27;s how you unlock the rifle in the car\".I would hope that people can recognize what it does to you psychologically to see this side of humanity every day. It&#x27;s a lot different than being a logger and worrying about a tree falling on you, or being an electric lineman and worrying about slipping and getting electrocuted. Falling off a cell tower, whatever.It&#x27;s very eye opening, scary, disturbing, and confusing all around. I watch the Youtube body cam footage with either slant and think \"Yeah, that&#x27;s what it&#x27;s like - good, bad, and ugly\". reply feoren 12 hours agorootparentNot to discount any of your experiences, but you&#x27;re countering with something different than the point being made. The question is: of your experiences, which ones would show up with footage on these YouTube channels? I&#x27;m willing to bet that every moment of someone being aggressive toward the cop would show up. Would him hitting on the girl show up? Would the questionable level of retaliatory force show up, especially if it had gotten out of hand? Would there be video of the (assuming) hours of questioning people handcuffed on a curb?You came away with a nuanced and complicated view on policework, which is probably close to reality. Is that really the story these YouTube body-cam compilations tell? reply greedo 13 hours agorootparentprevBy \"Copaganda\", I was referring to shows like COPS that only shows the dramatic moments, the YT channels that glorify police work, not the boring traffic stops, or the paperwork officers have to deal with. My dislike of this coverage is that it reinforces the self-image of cops as the only thing standing between the good guys and the bad guys, and that violence is accepted and encouraged as a standard useful tool. To me, violence (non-lethal or lethal) should be the last resort.I did a ride along in the 80s and was paired with a deputy who worked a tough area. They wanted to show me the worst of the worst, so that in case I decided to pursue a career in LEO I would know what it could entail. My deputy only responded to what he felt were serious, exciting calls. Scared the crap out of me. But he was honest enough to say that most of his days weren&#x27;t like the stuff I saw in my week. This wasn&#x27;t a one day thing, but kind of a pre-screening thing for a LEO career. I&#x27;m not sure that an \"ordinary citizen\" can do this just to see what it&#x27;s like anymore due to liability on the part of most police departments.I too saw my deputy use force that I felt was uncalled for. And demeaning behavior towards people who were just hoping someone could help. He also did crap like speed whenever he felt like it, broke a million traffic laws when it wasn&#x27;t necessary, and behaved like a stereotypical bad cop. Imagine how Hank Shrader acted in Breaking Bad. Everyone was assumed to be a scumbag and guilty of something.I have family and friend in LE, and my neighbor has been a captain in our city since I moved here. And I have a coworker who used to be a CO at the state pen. Dealing with the inmates, etc can be corrosive. You see people at their worst.But that&#x27;s what they&#x27;re paid to do. The system needs to address this as best as possible so that LEO (and COs) can cope with the stress and don&#x27;t develop an \"us vs them\" attitude. My CO coworker talked about how a lot of the COs just loved to mix it up with inmates. And my LEO friends talk about how these jobs naturally attract people with either too much adrenaline&#x2F;testosterone or an inability to control themselves under stress. But if you&#x27;re LEO, you sure can&#x27;t easily get psychotherapy.I appreciate you telling your story, and reading my with a charitable mindset. reply hotpotamus 17 hours agoparentprevThe fire department has a saying - what do police offices and fire fighters have in common? They both wanted to be fire fighters when they grew up.It&#x27;s not a job you&#x27;d want your children to do, but it&#x27;s one that someone has to do. reply xuhu 18 hours agoparentprevMany officers take a step back when things heat up beyond \"papers please\". No one ever got fired for doing that. If an officer escalates, it&#x27;s because their inner voice says \"come at be bro\". reply bborud 17 hours agoprevPolice will represent a problem in the US as long as it is not a single, national organization with a national set of rules, proper standards for education and conduct. That, of course, will never happen. Which means police organizations in the US will always represent a risk to the public. reply proee 18 hours agoprevPolice should wear two body cameras, maybe even one worn on the back to show the entire scene. Why not make them wireless as well and send back to central to ensure they are getting recorded. reply sschueller 18 hours agoparent360 camera on shoulder and synced to a 3rd party the departments have limited access too. They need to file paperwork to get footage they need.3rd party can deal with warrents directly and office should be punished for turning off cameras.This 3rd party however may not be connected or funded in anyway to cause a conflict of interest. No funding by the police! reply salawat 16 hours agorootparentAh hahahaha!So you&#x27;re actually going to start sufficiently funding my Quality Assurance department instead of treating it like a cost center, right? Right?!I won&#x27;t hold my breath on that one. reply jdoss 18 hours agoparentprevThe battery requirements for constant streaming on LTE&#x2F;5G would most likely be a constraint here. I do think your point makes a lot of sense however. reply chucksta 18 hours agorootparentWith all the tech they have in their cars, could easily do local backup there and then upload to a central location at return or when a trusted wifi becomes available reply rich_sasha 17 hours agoprevAre all the responses below from US? I mean, I get it, TFA is US-specific.I&#x27;m glad to live in a country where the police, for sure not perfect, are not an enemy of the people &#x2F; above-the-law racket which can do as it pleases. And I believe they don&#x27;t wear body cams. reply sixothree 19 hours agoprevWithout adversarial third party oversight police will continue to abuse the public. reply NickC25 18 hours agoparentThis. So much this.The problem is that due to the underlying need for police cooperation in unrelated cases, no sane DA or judge would dare challenge a cop (or the police union) who can claim \"oh it was a malfunction\" or \"oh uh the battery ran out\" when it comes time for video footage to be reviewed and made public in the case of a police officer breaking the law.My proposal: make it someone&#x27;s (or an anonymous board of lawyers&#x27;) job to review any and all police video footage. The police union is not allowed in any circumstances to know who is doing that job, nor are they allowed to protest. Make it federal law that every officer when on duty has no expectations of privacy, and their body cam must be on 100% of the time when on duty, no exceptions.If you are going to work (and retire) on the public&#x27;s dime, using equipment the public paid for, you must without question bow to public demands of complete transparency and full accountability. I don&#x27;t give a damn what the union thinks, they are on our dime. reply SamoyedFurFluff 18 hours agorootparentI agree with this except I think that policing should also be a highly educated position matching the expectations. Social workers and teachers are generally required to have graduate degrees, for example. Police academies are a scant few weeks in America, and these are people who enforce complex law in the most crucial life-or-death circumstances. Cops should be the best of us. reply makeitshine 18 hours agorootparentIt can be tiered, where more training allows you to do more. On the lower end you have unarmed police giving parking tickets or having to be accompanied by a more highly trained cop who is performing less critical roles.More training is more pay and benefits, and they have to keep it current by reapplying and showing they&#x27;ve done recent training, like teachers. reply jstarfish 18 hours agorootparentprevYou&#x27;ve never been through police academy, have you? It&#x27;s like training for a fucking pentathlon.You have to memorize and demonstrate understanding of law, pass an obstacle course, meet marksmanship standards, pass a driving test under extreme conditions, and more. It&#x27;s no joke. And they have to recertify on a lot of this yearly.Nobody&#x27;s perfect. None of their critics can actually do that job. Most couldn&#x27;t clear the first fence in the obstacle course. You&#x27;d hate being subject to the same surveillance at work as well. reply sensanaty 17 hours agorootparentI also don&#x27;t carry a gun at work with which I can end someone&#x27;s life.I don&#x27;t want my comment to be taken as anti-cop because I&#x27;m not, but an office worker being monitored all day and a cop on duty being monitored aren&#x27;t really equivalent at all. Shit, if lives depended on me & my actions and I worked in a field where big, heavy decisions could fall on me at any point, I sure as shit hope there&#x27;s someone or something there to document it all. reply NickC25 15 hours agorootparentprevWho cares if anyone here has gone through the police academy? It&#x27;s almost like training is required to do a job. Wow, what a concept!>You have to memorize and demonstrate understanding of lawNo shit...that&#x27;s the entire premise of the job. Someone whose job it is to enforce the law needs to actually know the law.>pass an obstacle courseYou should see some of the cops in the cities I&#x27;ve lived in (Bay Area, NYC, PHX, Miami). Some are&#x2F;were quite fit, but a bunch couldn&#x27;t pass an obstacle course even if the only obstacle was \"touch your toes\" or \"run a mile in under 15 minutes\".>meet marksmanship standardsYou don&#x27;t say? Someone whose job description includes \"the ability to use lethal force when necessary\" needs to know how to properly use a lethal firearm?>pass a driving test under extreme conditionsWeaving in and out of traffic with lights blaring is NOT hard. People are required to get out of your way. You also have a great car with tons of upgrades, all paid for by the taxpayer.>And they have to recertify on a lot of this yearly.Good. Plenty of professionals in other fields have to do this too, and they don&#x27;t get backed by a taxpayer funded union. reply Der_Einzige 8 hours agorootparentHell, I’ve literally watched cops use their lights so that they could go to a damn donut shop more quickly. I couldn’t believe it as I watched them pull into a krispy kream drive-thru.Seen plenty of obese cops too. No idea how they stay on the force. reply greedo 17 hours agorootparentprevExplain then why so many cops fail to understand (or follow) the law? They have a scant understanding of constitutional rights (and usually choose to ignore them when they can get away with it).And surveillance? Give me a break. At my job, all my keystrokes are logged. Every website I go to is logged. My email and chat are archived and searched for keywords and AI. And I&#x27;m not tasked with \"Protect and Serve.\" I work for an insurance company. Cops have life and death power, so as Spidey tells us, with power comes responsibility. Don&#x27;t like it? Don&#x27;t pick up the badge...You should look at the educational and training requirements used by European police forces. They make the \"police academy\" look like the joke that it is. Or look at how the military trains MPs. MPs have a much better understanding and usage of force than the majority of police forces in the US. reply FireBeyond 16 hours agorootparentprev> You have to memorize and demonstrate understanding of law, pass an obstacle course, meet marksmanship standards, pass a driving test under extreme conditions, and more. It&#x27;s no joke. And they have to recertify on a lot of this yearly.LOL, what? Police unions have consistently fought against recertification requirements, particularly the CPAT that police and fire use (candidate physical ability test).\"Demonstrate understanding of law\" - police have claimed, and courts have consistently upheld that police do not need to know the law. They repeatedly arrest photographers in public places, and it ends up in appeal after appeal while they defend their abuse of 1A, and even after all that Chiefs will still stand on court steps saying \"We can&#x27;t expect our officers to be constitutional lawyers\".American police training is some of the lowest standard in the developed world. It takes more time to be certified as a hairdresser in most states.> None of their critics can actually do that job.Get out of here with that attitude. That&#x27;s right up there with \"If you&#x27;re not behind our military, feel free to stand in front of them\" bullshit designed to shut down any criticism, valid or otherwise. reply engineer_22 18 hours agorootparentprevState and Federal gov&#x27;ts have priority in prosecution. If it is true that DA or judge are reticent to prosecute, then the appropriate avenue is State&#x2F;Fed reply greedo 17 hours agorootparentGuess where little State Prosecutors grow up? Working cases at the municipal level until they move to the minor leagues. Federal prosecutors work up the same route, so all the institutional bias and privilege is instilled early.How many Attorneys General worked as public defenders? I bet out of the current 50 US AGs, it&#x27;s probably close to zero. reply meepmorp 18 hours agorootparentprev> Make it federal law that every officer when on duty has no expectations of privacy, and their body cam must be on 100% of the time when on duty, no exceptions.I&#x27;m not an expert, but I don&#x27;t think that&#x27;d be possible at the federal level. Politics aside, local policing is controlled by states and federal control over those issues is largely limited to things like withholding funding for states that don&#x27;t comply with federal program requirements. reply everybodyknows 17 hours agoparentprevCounsel to the member of the public, whether appearing as legal defendant or as plaintiff against the police, should be able to demand full access to camera footage as part of standard pre-trial discovery.If police fail to produce the footage, judges should instruct jurors that the adverse inference may be drawn (in case they&#x27;re insufficiently skeptical to do so themselves). reply danaris 18 hours agoparentprevYep. Body cameras are an attempt at a technological solution to a sociocultural problem—and those rarely work.We need to either change the culture—and adversarial third-party oversight is a forcible way to do that—or remove the tools the police use to abuse the public. reply Zigurd 18 hours agoparentprevThis should apply to all evidence and witness statement gathering and processing. It should all be handled separately from police and prosecutors and made equally available to both sides if it comes to prosecution. The incentives should be toward accuracy, not convictions. reply tylerdinner 18 hours agoprevThey&#x27;re public servants. That footage should be public property.Just like \"this call may be monitored for blah blah\", your actions as a cop should be monitored by the public.Maybe we should have local citizens watching&#x2F;reviewing body cam footage 24&#x2F;7 to make sure cops are doing their job correctly. It&#x27;s also job creation.Edit: obviously there are some confidentiality issues, so only certain folks should have that clearance. But it feels like we need a better oversight system, whatever that may be. reply mantas 18 hours agoparentYeah, some confidentiality issues.. I&#x27;m pretty sure such system would be abused. To leak footages of low situation in peoples&#x27; lives. To make sure cops enforce rules by the letter even if situation is objectively silly on the spot. To get dirt on cops by logging mistakes which everybody does.That&#x27;s like requiring screensharing + camera on for remote workers 100% of the time. It sucks. reply kevinventullo 17 hours agorootparentRemote tech workers aren&#x27;t expected to occasionally assault or kill people as part of carrying out their duties. reply dclaw 18 hours agoprevI always said when this whole body cam business started years ago that everyone is going to be real fucking upset when they get live facial recognition on these things. Be careful what you wish for. reply InCityDreams 17 hours agoparentI would accept facial recognition with cops being held accountable, sure - I can wear a mask (and i often do, especially in malls, phone left in car). Unbeating myself...more difficult. reply bell-cot 19 hours agoprevImmediate Reaction: Um, yes? Only the naive, self-deluding, and techno-utopians would have expected otherwise. Vs. an experienced teller from a well-run 1950&#x27;s podunk bank could tell you that no security technology is worth crap if the managers don&#x27;t actually care, or are crooked themselves. reply Zigurd 18 hours agoparentThe companies selling this tech are explicitly techno-dystopians. reply hindsightbias 18 hours agoparentprevBut you hav to admire the opportunities that other obvious players like GoPro seem to have missed out on:\"Motorola Solutions Raises Spending on Body Cameras to Grab Market Share The equipment maker expects revenue in its video-security business to jump more than 20% this year after a 30.7% leap last year\"https:&#x2F;&#x2F;www.wsj.com&#x2F;articles&#x2F;motorola-solutions-raises-spend... reply leetrout 18 hours agorootparentWe explored this space with an investor and did not get far before backing away in ~2017ish.The makers of the TASER, Axon, gave away body cams to sell the services supporting them.It worked well and was very expensive to try to enter the market against their brand loyalty and existing market penetration. reply JadeNB 19 hours agoparentprev> Immediate Reaction: Um, yes? Only the naive, self-deluding, and techno-utopians would have expected otherwise. Vs. an experienced teller from a well-run 1950&#x27;s podunk bank could tell you that no security technology is worth crap if the managers don&#x27;t actually care, or are crooked themselves.If one isn&#x27;t trained as (or thinking as) a scientist, then I think it&#x27;s easy to miss the importance of documenting expected results. One shouldn&#x27;t believe a conclusion, even an expected and obvious conclusion, without data supporting it. This makes gathering data to support expected results a valuable process, and one that should be recognized.(For example, in this particular context, if one wants to argue against police claims that body-cams offer sufficient accountability, being able to argue from specific cases where the accountability has been subverted is much more likely to be useful than arguing from a general feeling that they won&#x27;t be used properly.) reply bell-cot 18 hours agorootparent> If one isn&#x27;t trained as (or thinking as) a scientist, then...Reaction: When possible, leave \"science\" out of any broad-based public policy discussion. Between the people with negative reactions to the word, and those who tune out \"because science is hard stuff\", and those who start judging the arguments by \"how fancy is his lab coat?\" criteria - it&#x27;s seriously counter-productive.> ...if one wants to argue against police claims that body-cams offer sufficient...argue from specific cases where...YES, that is the right approach. Go directly there. And pick your specific cases very carefully, to make it extra-clear to your audience that naive quick fixes - which all-too-many of them really want to believe in - are nothing but different brands of square tires. reply coldtea 17 hours agorootparentprev>If one isn&#x27;t trained as (or thinking as) a scientist, then I think it&#x27;s easy to miss the importance of documenting expected results. (...) (For example, in this particular context, if one wants to argue against police claims that body-cams offer sufficient accountability, being able to argue from specific cases where the accountability has been subverted is much more likely to be useful than arguing from a general feeling that they won&#x27;t be used properly.)That would only matter if it was a matter of \"better argument\" setting the issue.Whereas it&#x27;s a matter of power balance: those in power want the police on their side, and the police wants less oversight.Though the parent does not \"argue against police claims that body-cams offer sufficient accountability\".I think they&#x27;re rather pointing that the accountability they offer in \"optimal use\", don&#x27;t matter if police chiefs don&#x27;t care how they&#x27;re used&#x2F;bypassed&#x2F;workaround-ed&#x2F;and so on, and this is allowed to have minimal impact on the officers and them. reply eightysixfour 12 hours agoprevI’ve always thought the obvious play here in a capitalist driven society is to require police officers to have private insurance. Insurance companies would quickly land on:1. No camera, no insurance2. A national registry for individual officer historyHaving exclusive use of legal force needs some kind of counter balance. Not sure what else could do it. reply HDThoreaun 12 hours agoparentWe&#x27;ve tried this, the police refuse. reply eightysixfour 11 hours agorootparentI mean at some point refusals only go so far. I’m absolutely not pretending it would be easy, but when you refuse to do your job under new conditions for your employer… you lose your job. New police departments wouldn’t be the worst thing for a lot of cities. reply HDThoreaun 11 hours agorootparent> when you refuse to do your job under new conditions for your employer… you lose your jobIdeally yes, but when every cop in the city refuses to do their job at the same time the city doesnt really have a choice. Firing every police officer is not an option for politicians who want to be re-elected. reply Der_Einzige 8 hours agorootparentprevIf you fire your cops, all cops will refuse to work for your new department. This is the status quo in Oregon right now. Lots of money for new cops, no one applies. reply NoMoreNicksLeft 18 hours agoprevI never saw promise in body cameras. The organization they were meant to make accountable is the same organization that gets to edit the video, gets to turn the cameras on and off, gets to refuse to release them, gets to release them at the most opportune time, gets to, and gets to \"lose\" them if there&#x27;s no way to scrub their misbehavior.If that weren&#x27;t enough to know how useless they&#x27;d be, we&#x27;d had dash cams for t least 20 years at that point (since Rodney King? I can&#x27;t remember honestly, but mid-1990s anyway). Nothing improved. No less abuse, no better outcomes. We had already run the experiment, but collectively decided that the results were wrong.So, why did we bother to camera up all the cops? Because intelligence agencies wanted millions of cameras out there the feeds from which they could watch. They wanted mobile cameras, not fixed. And we&#x27;re still 20 or 30 years too early for the robot revolution. It had to be meat drones. reply dang_undead 16 hours agoprevBullshit, they&#x27;ve delivered on their promise of delivering a steady stream of awesome high quality, state-sanctioned snuff films.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html reply Eumenes 19 hours agoprevI want elected officials to wear body cameras too reply dcsommer 19 hours agoparent\"Transparency\" in Congress has lead to grandstanding and posturing to cameras, at the expense of the necessary compromises and deals that could otherwise happen. reply dragonwriter 18 hours agorootparent> \"Transparency\" in Congress has lead to grandstanding and posturing to cameras, at the expense of the necessary compromises and deals that could otherwise happen.What transparency in Congress? Congress lacks even the transparency requirements many state legislatures have and is essentially completely opaque, with virtually everything substantive happening out of the public eye. The absence of compromise has to do with certain factions preferring to scheme for achieving total government control and preferring to making things worse in the short term if it can be leveraged to help advance that cause, not to transparency. reply lotsofpulp 18 hours agorootparentprevTransparency would be transcripts of every communication the politician has. At the golf courses, at the bar, on vacation, etc. See Clarence Thomas for example.Internet&#x2F;bandwidth creating cost free video channels for politicians to advertise themselves is not transparency. reply Eumenes 18 hours agorootparentprevSounds like something a Corporation would say. You can have compromise and transparency, it doesn&#x27;t have to be one or the other. reply master-lincoln 18 hours agorootparentprevIf that is true, the issue is in the egos of people making deals reply coldtea 17 hours agorootparentThe don&#x27;t grandstand because of \"egos\" but because they have things to win from that (like press coverage, voter sympathies, money and deals from opponents of the issues they grandstand against, and so on). reply manicennui 18 hours agorootparentprevYou are either being disingenuous or incredibly oblivious. People want to know what our elected officials are saying in private with lobbyists and campaign contributors. reply paxys 18 hours agoparentprevHow many hours of C-Span have you watched in your life? Detailed minutes of Congress proceedings from like 1800 are available for you to peruse if you&#x27;d like. Transparency isn&#x27;t the problem – everything is happening fully out in the open. reply Whoppertime 16 hours agorootparentMany hours actually. BookTV is great for author interviews, and listening to politician&#x27;s speeches directly has less spin then watching the CNN or Fox interpretation. reply lotsofpulp 18 hours agorootparentprevThere are literally closed door legislative sessions that are not transparent. Plus the adage “business is done on the golf course”. reply paxys 18 hours agorootparentSo you want all elected officials to wear a camera 24x7? Because hey, they could be doing business while taking a shit? Better keep it on at night as well, because we want to see the face of whoever they&#x27;re screwing.Here&#x27;s a better idea – elect people who can be trusted. reply maxwell 18 hours agorootparentReps in the U.S. are very much trusted, by their donors. The problem is House elections are too expensive, because representation is so bad, so only those who take considerable corporate bribes have a shot.While \"virtual,\" American colonists had better representation in the British Parliament on paper than we do today.We&#x27;re an extreme outlier among OECD countries:https:&#x2F;&#x2F;www.amacad.org&#x2F;ourcommonpurpose&#x2F;enlarging-the-house&#x2F;...We need to repeal the Apportionment Act of 1929 and expand the House to an appropriate size. reply coldtea 17 hours agorootparentprev>So you want all elected officials to wear a camera 24x7?Yes.>Because hey, they could be doing business while taking a shit?Sucks to be them. If they&#x27;re shy about it, they could always not get elected.Though, joking aside, you don&#x27;t have to bring in the \"slippery slope\" argument. As long as they don&#x27;t talk to anyone while taking a shit, they could be allowed to close the camera.It&#x27;s enough that they have to have it open whenever they make any political discussion and anything that relates to their position and role.Not when they shit or order a latte. And if they do have such a discussion while ordering a latte, and not have it recorded, then it should be grounds for them being ejected.>Here&#x27;s a better idea – elect people who can be trusted.And how&#x27;s that been working out? reply danaris 18 hours agorootparentprevNo matter how trustworthy someone is when they get elected, there is always a chance for them to become corrupt once in office. After all, before they&#x27;ve actually held public office, there&#x27;s little to no incentive or opportunity to be corrupt—and the higher the office, the higher both become.We need effective rules of ethics (with teeth) and oversight procedures. I don&#x27;t think that forcing every politician to wear a camera 24&#x2F;7 is really the best solution, but it&#x27;s hopelessly naïve to think that some of them aren&#x27;t literally going to bed with the people who are trying to get favorable legislation passed.A society like the US is just too diverse—not so much in heritage, but in cultural values (or at least subcultural values) to be able to \"just elect people who can be trusted.\" reply maxwell 18 hours agorootparentHow about we just go back to expanding the House based on population instead of keeping it static at 435 forever? We&#x27;re approaching a million constituents per representative. Corruption seems a mathematical certainty. reply danaris 14 hours agorootparentThat would genuinely make a huge positive difference (though it would not, of course, fix the whole system, since the House is not the only place with problems). There&#x27;s no need, in this day and age, to have the entire House of Representatives meet physically in one building. reply maxwell 12 hours agorootparentAgreed.Let&#x27;s go fully remote, sell off most federal property in DC, and retrocede Washington back to Maryland (like Alexandria County back in 1846), dissolving DC and draining the swamp for good.Then we can take proposals for creating a District of Lincoln or Roosevelt for meetings and events closer to the center of population, e.g. Kansas City (near mean center) or the confluence of the Mississippi and Ohio (near median center).https:&#x2F;&#x2F;www.theatlantic.com&#x2F;ideas&#x2F;archive&#x2F;2020&#x2F;04&#x2F;zoom-congr... replycoldtea 17 hours agorootparentprevThose are the parts of the process that matter absolutely less. reply docdeek 18 hours agoparentprevThat was a plot point in ‘The Circle’ by Dave Eggers. Elected officials were encouraged and then shamed&#x2F;forced into wearing cameras and livestreaming all their activities. The idea was they they wouldn’t take bribes or submit to lobbyist demands if they were being watched online 24&#x2F;7. The end result of this sort of transparency, though, was rather dystopian, a lot of unintended consequences. reply njharman 17 hours agoparentprevI wanted to wear a body camera in the form on Mediated Reality glasses but the public shamed&#x2F;cancelled the early adaptors and manufacturers so much so that the product&#x2F;concept was withdrawn from market place.Everyone should be a glasshole reply indymike 18 hours agoparentprevI don&#x27;t want to see that footage. Ever. reply jMyles 19 hours agoprevSeems like this is the thesis:> As policymakers rushed to equip the police with cameras, they often failed to grapple with a fundamental question: Who would control the footage? Instead, they defaulted to leaving police departments, including New York’s, with the power to decide what is recorded, who can see it and when. In turn, departments across the country have routinely delayed releasing footage, released only partial or redacted video or refused to release it at all.The impetus to avoid transparency is the strongest indication I can observe that the state will be deprecated in favor of the internet in the coming centuries. reply eropple 19 hours agoparentIs the Internet going to pave roads? How? reply quantified 19 hours agoparentprev\"Will be\"... by whom? People won&#x27;t do it, the state sure isn&#x27;t. reply meepmorp 18 hours agoparentprevYou&#x27;re right, of course - nobody ever lies on the internet. reply RecycledEle 18 hours agoprevUntil it is a felony for a cop to turn off a camera,Until it is a felony to not upload all video within 1 hour of the end of the cops shift and make it publicly available on the web with geotagging left in place,I live under a tyranny. reply engineer_22 18 hours agoparentI think this is an extreme view. I think we can all agree that body cam footage is an important tool to get justice, but in the interest of public safety I don&#x27;t think providing bad actors real time information about police habits and location is a good idea. reply Zigurd 18 hours agorootparentYou are taken in by TV cop show scenarios. Criminals are not smart. Cops are not warriors in danger of a guerrilla attack.The real effect of knowing what cops do all day would be half as many cops, or fewer. reply rpmisms 18 hours agorootparentThis is not true. The criminals police deal with are not smart. There are very, very smart criminals, and they tend to gravitate towards specialties that will not get them caught, like Congress.> Cops are not warriors in danger of a guerrilla attack.I think some of them need to hear that. reply engineer_22 18 hours agorootparentprev-> Criminals are not smart.Let&#x27;s just agree to disagree reply jon-wood 17 hours agoparentprevMaking the video public is incompatible with a legal system that presumes innocence. The moment this video is publicly available someone else will be making a service to check whether your employees&#x2F;ex-wife&#x2F;children have had an encounter with the police. reply waihtis 18 hours agoparentprevAre you also one of those people who think the SF approach of not prosecuting theft was a good idea? reply RecycledEle 18 hours agoprev [–] Information is power. I want information to be available to as many people as possible or available to nobody.Suggestion: When an altercation occurs, everyone with a recording of it should upload it to a public web site where all video is permanently available to the public. Anyone not uploading their video is assumed by the courts to be at fault for the altercation.Another suggestion: All recordings in public places must be geotagged and uploaded to a public web site within 24 hours where they become part of the public record. This can be implemented with the creation of a government website and software updates to cameras&#x2F;DVRs&#x2F;phones. reply some_random 18 hours agoparent [–] The purpose of a panopticon isn&#x27;t so that the inmates can keep the guard in check. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Some police departments are refusing to release body camera footage, as seen in the case of Miguel Richards being shot by the NYPD.",
      "The effectiveness of body cameras in promoting transparency and accountability in the criminal justice system is being questioned.",
      "Proper regulation, oversight, and accountability are needed to address these concerns and ensure the intended deterrent effect on officer behavior."
    ],
    "commentSummary": [
      "The use and accessibility of police body camera footage for accountability purposes is the main topic of discussion.",
      "Concerns are raised about evidence concealment, reliability and maintenance of the footage, and intentional suppression of evidence.",
      "Suggestions include regular checks of the footage, proper camera maintenance, and holding officers accountable for using the cameras during interactions with the public."
    ],
    "points": 207,
    "commentCount": 212,
    "retryCount": 0,
    "time": 1702561235
  }
]
