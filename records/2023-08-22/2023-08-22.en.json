[
  {
    "id": 37215557,
    "title": "uBlock Origin Lite now available on Firefox",
    "originLink": "https://addons.mozilla.org/en-US/firefox/addon/ublock-origin-lite/",
    "originBody": "Firefox Browser Add-ons Extensions Themes More… Log in Search Search uBlock Origin Lite by Raymond Hill A permission-less content blocker. Blocks ads, trackers, miners, and more immediately upon installation. You'll need Firefox to use this extension Download Firefox and get the extension Download file Extension Metadata Used by 100 Users 9 Reviews Rated 5 out of 5 5 Stars 5 9 4 0 3 0 2 0 1 0 Screenshots About this extension uBO Lite (uBOL) is a *permission-less* MV3-based content blocker. The default ruleset corresponds to uBlock Origin's default filterset: - uBlock Origin's built-in filter lists - EasyList - EasyPrivacy - Peter Lowe’s Ad and tracking server list You can add more rulesets by visiting the options page -- click the _Cogs_ icon in the popup panel. uBOL is entirely declarative, meaning there is no need for a permanent uBOL process for the filtering to occur, and CSS/JS injection-based content filtering is performed reliably by the browser itself rather than by the extension. This means that uBOL itself does not consume CPU/memory resources while content blocking is ongoing -- uBOL's service worker process is required _only_ when you interact with the popup panel or the option pages. uBOL does not require broad \"read and modify data\" permission at install time, hence its limited capabilities out of the box compared to uBlock Origin or other content blockers requiring broad \"read and modify data\" permissions at install time. However, uBOL allows you to *explicitly* grant extended permissions on specific sites of your choice so that it can better filter on those sites using cosmetic filtering and scriptlet injections. To grant extended permissions on a given site, open the popup panel and pick a higher filtering mode such as Optimal or Complete. The browser will then warn you about the effects of granting the additional permissions requested by the extension on the current site, and you will have to tell the browser whether you accept or decline the request. If you accept uBOL's request for additional permissions on the current site, it will be able to better filter content for the current site. You can set the default filtering mode from uBOL's options page. If you pick the Optimal or Complete mode as the default one, you will need to grant uBOL the permission to read and modify data on all websites. Keep in mind this is still a work in progress, with these end goals: - No broad host permissions at install time -- extended permissions are granted explicitly by the user on a per-site basis. - Entirely declarative for reliability and CPU/memory efficiency. Other popular extensions uBlock Origin Recommended 6,171,541 users New Tab Override Recommended 62,282 users Search by Image Recommended 257,425 users Enhancer for YouTube™ Recommended 707,468 users Rate your experience How are you enjoying uBlock Origin Lite? Log in to rate this extension There are no ratings yet Report this add-on for abuse Read all 9 reviews Permissions Learn more This add-on needs to: Block content on any page This add-on may also ask to: Access your data for all websites More information Add-on Links Homepage Support site Version 1.0.23.8155 Size 7.5 MB Last updated 7 days ago (Aug 15, 2023) Related Categories Privacy & Security License GNU General Public License v3.0 Privacy Policy Read the privacy policy for this add-on Version History See all versions Tags ad blockeranti trackercontent blockerprivacysecurity Add to collection Select a collection… Create new collection Release notes for 1.0.23.8155 Properly serialize CSS combinators according to position in selector Updated filter lists More extensions by Raymond Hill CCaptioner 587 users uBlock Origin Recommended 6,171,541 users uMatrix 21,524 users uBO-Scope 811 users Go to Mozilla's homepage Add-ons About Firefox Add-ons Blog Extension Workshop Developer Hub Developer Policies Community Blog Forum Report a bug Review Guide Browsers Desktop Mobile Enterprise Products Browsers VPN Relay Monitor Pocket Twitter (@firefox) Instagram (Firefox) YouTube (firefoxchannel) PrivacyCookiesLegal Except where otherwise noted, content on this site is licensed under the Creative Commons Attribution Share-Alike License v3.0 or any later version. Change language Afrikaans عربي Asturianu Azərbaycanca Български বাংলা Bosanski Català Maya Kaqchikel Čeština Dansk Deutsch Dolnoserbšćina Ελληνικά English (Canadian) English (British) English (US) Español Eesti keel Euskara فارسی suomi Français Frysk Gaeilge עברית Hrvatski Hornjoserbsce magyar Interlingua Bahasa Indonesia íslenska Italiano 日本語 ქართული Taqbaylit 한국어 lietuvių kalba Latviešu Македонски Монгол Melayu Malti Norsk bokmål Nederlands Norsk nynorsk ਪੰਜਾਬੀ (ਭਾਰਤ) Polski Português (do Brasil) Português (Europeu) Română Русский සිංහල slovenčina Slovenščina Shqip Svenska తెలుగు ไทย Türkçe Українська اُردو Tiếng Việt 中文 (简体) 正體中文 (繁體)",
    "commentLink": "https://news.ycombinator.com/item?id=37215557",
    "commentBody": "uBlock Origin Lite now available on FirefoxHacker NewspastloginuBlock Origin Lite now available on Firefox (addons.mozilla.org) 566 points by tech234a 12 hours ago| hidepastfavorite276 comments FiloSottile 11 hours agoThis is the uBlock Origin edition based on the much-maligned WebExtensions Manifest V3, which implements blocking declaratively instead of allowing&#x2F;requiring live request interception.Firefox—my daily driver—still supports the \"main\" uBlock Origin (and I&#x27;m a somewhat heavy user of features unavailable in Lite like custom filters), but I had been waiting for Lite to be available and immediately went ahead and replaced uBlock Origin with uBlock Origin Lite.The security win can&#x27;t be understated: with its permission-less design (enabled by MV3) I am down to zero third-party developers that can get compromised and silently push an update that compromises all my web sessions. Sure, attackers could still get into Mozilla, Apple (as I run macOS), or cause a backdoored update to be pushed via Homebrew (how I install unsandboxed applications when no web app is available, which thanks to the likes of WebUSB is getting less common), but unsandboxed browser extensions were clearly the lowest hanging fruit, so this update (and MV3) significantly raised my security posture (and transitively that of projects I have access to, and that of their users). reply e2le 11 hours agoparent>I am down to zero third-party developers that can get compromised and silently push an update that compromises all my web sessions.It&#x27;s my understanding that because uBlock Origin is a \"recommended extension\", it must undergo a formal code review each time a new update is published. A malicious update would not face zero obstacles.https:&#x2F;&#x2F;support.mozilla.org&#x2F;en-US&#x2F;kb&#x2F;recommended-extensions-... reply Timshel 11 hours agorootparentThe switch from full acces to white-listing for full blocking is just awesome imo.You can just decide for each case the tradeoff between advanced blocking and security. reply yellowapple 5 hours agorootparentThat tradeoff is already possible with the normal uBlock Origin; it just has a different (preferable IMO) default.The only apparent upside to this version is if you don&#x27;t trust Raymond Hill to refrain from spying on your browsing sessions via his add-on. By every other metric this seems like a downgrade. reply nottorp 2 hours agorootparentprevWhy do you think so?Say I open HN on my morning coffee and open 5 links in new tabs. They don&#x27;t have to be to sites I&#x27;ve opened before. I will be tracked until i go to each tab and add it to uBlock Origin.How is that an improvement? reply e2le 10 hours agorootparentprevI&#x27;m unsure of how it exactly differs and whether there are features missing. I will admit that if I were to install uBOL today, I would be worried that it would be less capable and my browsing experience less-safe. reply odo1242 9 hours agorootparentalso:- one thing that is much less powerful is cosmetic filters, which means ads may be replaced with gray squares if uBOL can&#x27;t remove them entirely- less filters overall, because the limits on how many filters are possible are pretty strict- extension updates will be both larger and much more frequent because filter lists can no longer be updated separately from the extension reply flangola7 7 hours agorootparentWhy is there a filter limit at all? Why did Firefox add that? Can config change it? reply bornfreddy 5 hours agorootparent> Why did Firefox add that?They didn&#x27;t, Google did. It is part of MV3 specs, afaik. reply z0ccc 3 hours agorootparentBut Firefox still didn&#x27;t have to implement the limit. reply postalrat 6 hours agorootparentprevFilters aren&#x27;t free reply JacobSeated 3 hours agorootparentFilters take a minuscule amount of resources. E.g. Even if you had to loop over a list with thousands of entries this would be unlikely to matter, and can in fact even be optimized quite significantly with various algorithms. However, because a database is most likely used, this is not even an issue, and the resource use will in fact be truly minimal.I can not give you an exact number, but it is massively bigger than the number of trackers &#x2F; elements that any usable web page can realistically implement. Probably you can do several thousands database calls in less than a second due to indexing; and that can be further optimized by doing it in batches, bringing it up to tens of thousands, if not in the hundreds on modern PCs. It is literally not an issue. reply wtallis 5 hours agorootparentprevThey are quite often better than free, blocking unwanted content that would consume more memory and CPU time than the filter itself. reply eipi10_hn 9 hours agorootparentprevThere are many features missing, more prone to anti-adblock&#x2F;ads-reinsertion (problems with `redirect-rule` and unable to fast updates) and ads&#x2F;trackers&#x2F;popups can slip through if cannot be caught by regex filters. reply winter_blue 4 hours agorootparentprevHmm, it looks like the uBO \"Enter element picker mode\" feature is not available. I&#x27;ve found that pretty useful occasionally in the past on websites where uBO doesn&#x27;t catch an ad. reply mmis1000 6 hours agorootparentprevActually every extension on AMO must go through manual inspection to push as auto update. You may push new version to market without manual inspection. But it won&#x27;t auto update to users&#x27; computers until then.So human review really isn&#x27;t the real difference on firefox&#x27;s side. Because it is required since day one reply Semaphor 5 hours agorootparent> When a developer submits an extension to addons.mozilla.org, it’s scanned for a set of common issues. It may also be subject to human review.-- https:&#x2F;&#x2F;support.mozilla.org&#x2F;en-US&#x2F;kb&#x2F;tips-assessing-safety-e...*May* is important here. That is also what I remembered changing years ago. reply Semaphor 6 hours agorootparentprevDo you have a source for this? I always thought they long ago stopped manual inspections for most extensions. reply mmis1000 6 hours agorootparentI literally wrote one. And it had been taken down due to reviewer unable to reproduce the achieve I uploaded (it turns out to be a \\r\\n \\n line ending issue. Thanks windows and git). reply Semaphor 5 hours agorootparentI maintain one as well, and had it approved despite the source having a bug, so that doesn’t sound like much of a source. reply mmis1000 5 hours agorootparentI don&#x27;t think they ever care about whether your extension has bug or not. They probably only review whether your extension has weird minified code or dependencies. reply zagrebian 6 hours agorootparentprevOk, when adding a new extension to the store. But “recommended extensions” get manually reviewed whenever they update. replyjosephcsible 8 hours agoparentprevI&#x27;m pretty sure that once you factor in the security reduction from ad blocking being less effective, switching from uBO to uBOL is actually a net worsening of security posture. reply xvector 8 hours agorootparentIf you&#x27;re getting targeted with major browser zero-days, ads are the least of your concern. reply vGPU 6 hours agorootparentThen you should be running more powerful tools like noscript and the full version of this, not a pared down version. Or a significantly more locked down version of Firefox on qubesOS.Manifestv3 will have negligible improvements on potential security risks and will significantly decrease overall security. reply bboygravity 7 hours agorootparentprevWhy do you assume you need to be targetted to be a victim of being pwned by ads? reply paulryanrogers 6 hours agorootparentBecause exploits that can break out via ads are not usually worth burning on randos? reply saagarjha 3 hours agorootparentTargeted ads let you make sure they don’t get deployed on randos. reply flexagoon 3 hours agorootparentprevIn what way does ad blocking improve your security? It significantly improves your user experience and slightly improves privacy, but it doesn&#x27;t have anything to do with security, unless you click on random \"download\" links, which I assume people on HN don&#x27;t do reply josefx 3 hours agorootparentYou assume that modern day browsers are even remotely secure. They still suffer from significant security bugs every now and then. Reducing the amount of third party java script you run is a security improvement. reply raxxorraxor 1 hour agorootparentprevExfiltration of data is a security issue and ads tend to grab any info they can, even if it just loading their ressources. Not a security issue many large tech companies like to focus on for obvious reasons. reply sph 2 hours agorootparentprevFewer requests = fewer potential attack vectors, especially when most of the blocked requests are executable JS code. reply saagarjha 3 hours agorootparentprevAds can be a malware delivery vehicle. reply the_gipsy 2 hours agoparentprevYou could just disable automatic updates on extensions. uBlock origin is a featured extension, so it&#x27;s already audited.MV3 is safer, but so is running no adblocker at all. There is a tradeoff. I get much more ads on Safari+AdGuard (iPhone) which uses MV3 or some similar declarative approach, than on Firefox+uBlock-Origin where I get basically none.I still prefer to trust one extension like uBlock Origin, just like I trust other software packages on my system, and to really fend off all the web tracking nonsense. reply MasterYoda 2 hours agoparentprev> The security win can&#x27;t be understated: with its permission-less design (enabled by MV3) I am down to zero third-party developers that can get compromised and silently push an update that compromises all my web sessions.Can you or someone else elaborate way it would be more secure? I dont quite follow or see the benefit. reply raxxorraxor 1 hour agorootparentLook at the security on mobile OS. It is perfectly secure for Apple and Google. But seriously, the benefit is theoretical and only with the assumption that you believe Apple and Google to treat your data better than a third party. Brave assumption in my opinion. reply 4bpp 10 hours agoparentprevIt seems like this issue could also be sidestepped by simply not silently pulling updates, especially in the case of something like browser extensions where the extension is sandboxed (so the potential negative impact of not immediately getting out a \"critical security update\" is bounded) but the developer is not fully trusted. Have we normalised micromanagement of the user by software vendors so far that this is no longer a default that anyone would consider? reply chii 9 hours agorootparent> not silently pulling updatesa regular user would not have the capability to audit an update. A power user, with entirely too much time on their hands, could of course, but one should not be designing systems based on such niche scenarios. reply Jach 8 hours agorootparentThe scenario is: don&#x27;t enable automatic updates. How long will a compromise exist before someone notices? Often, not very long at all. It&#x27;s entirely common to avoid an issue because you haven&#x27;t bothered to update. It&#x27;s also part of why corpos have their own repository mirrors too since dev supply chain attacks have gotten more common and no one&#x27;s going to audit their dozens&#x2F;hundreds of NPM dependencies every update. reply arsome 3 hours agorootparentprevYou don&#x27;t need to audit the update, simply don&#x27;t update until there&#x27;s some actual benefit to it. reply generic92034 2 hours agorootparentThe counter move is announcing some \"security fixes\" (of course without any further detail) in each and every update. Now you do not know if you are increasing security by applying the update or if you are decreasing it... reply sanitycheck 1 hour agorootparent\"Security fixes\" just means the developer wanted to list what actually changed (which would be \"added more telemetry, fixed analytics client ID persistence, made sidebar blue match logo\" or something) but the PM insists on using the same generic message each time. Safe to ignore those updates! reply eviks 7 hours agorootparentprevBut one should be designing systems, unlike today where there is bad design of forced autoupdates For example, they could be a system of distributed code reviews where an update is offered to the users only after some review reply Dalewyn 8 hours agorootparentprevA regular user would also not not care about \"web sessions\", \"permissions\", \"silent updates\", and all the other techno-mumbo-jumbo they will file away in their folder of Do Not Care.Essentially, there&#x27;s an issue of hypocrisy in the threat model and type of user proposed. reply Timshel 11 hours agoparentprevThe issue with v3 is when it&#x27;s the only solution. Which is not the case here :> However, uBOL allows you to explicitly grant extended permissions on specific sites of your choice so that it can better filter on those sites using cosmetic filtering and scriptlet injections.Which I would expect allow it to work as well as uBO. reply zamalek 10 hours agorootparent> Which I would expect allow it to work as well as uBO.Note that there are still some adblocker workarounds that will foil MV3, such as CNAMEs. uBO will always be more effective than MV3, unless some substantial improvements are made to MV3. reply cookiengineer 4 hours agorootparentNote that CNAMEs is literally caused by GDPR, and the pathway every single ad or tracking company seems to go sooner or later.For people not understanding how it works: you can set a CNAME entry on your tracker.domain.tld to bypass all Browser&#x27;s third-party tracking preventions, and make it look like it&#x27;s a normal subdomain of your website.You need to make a CNAME tracker database manually by resolving the reverse entries for known IPs. Usually there is hundreds or thousands of CNAME entries pointing to the same IP address.The AdGuard team also made a database for this, in case anyone needs it for UBOL [1]Most, if not all of those trackers use assets that they serve from there (like a tracking pixel gif socket), so I highly doubt that uBOL will catch those; because the cat and mouse game is now in the ad tracker&#x27;s favor and it is impossible to keep up now. And that was the intended purpose. We now have to play our hand with marked cards.[1] https:&#x2F;&#x2F;github.com&#x2F;AdguardTeam&#x2F;cname-trackers reply warpspin 2 hours agorootparent> Note that CNAMEs is literally caused by GDPRNo. The migration to CNAMEs has to do with the phasing out of third party cookies. The GDPR literally does not care about how the technology works. reply eurg 4 hours agorootparentprev> Note that CNAMEs is literally caused by GDPRHow so? reply cookiengineer 4 hours agorootparent> How so?Because it shifts the contractual obligations and the \"legitimate interest\" of data to a seemingly first party, which all companies seem to think they can get away with.Well, until the tracked subjects do a reverse DNS lookup anyways. reply Sander_Marechal 2 hours agorootparentAdding a CNAME does not make tracking first-party. You can simply report them to the relevant DPO. replyeipi10_hn 9 hours agorootparentprevNo, it won&#x27;t work as well as uBO. Many features from uBO are missing in uBOL even in full mode, more prone to anti-adblock&#x2F;ads-reinsertion (problems with `redirect-rule` and unable to fast updates) and ads&#x2F;trackers&#x2F;popups can slip through if cannot be caught by regex filters. reply MC68328 10 hours agoparentprev> I am down to zero third-party developers that can get compromised and silently push an update that compromises all my web sessionsYeah, but is this really a risk for anyone who isn&#x27;t the sort to have installed Bonzi Buddy back in the day?That attack surface, compared to that of brew, npm, pip, gem, etc., is miniscule. And browser plugins don&#x27;t yank in obscure dependencies at install time.I only run uBlock, and I suspect I&#x27;m in the majority here, and my choice of browser is predicated on the availability of a non-crippled ad blocker, because malicious ads are the primary threat. reply anadem 9 hours agorootparent> I only run uBlock,(as noted by fsckboy): uBlock was the original name for the add-on that subsequently was ethically compromised&#x2F;\"sold out to\" advertisersuBlock Origin is the 2nd version written by the original author (gorhill) and is not compromised. reply raxxorraxor 1 hour agoparentprevFor me this security scenario isn&#x27;t relevant at all. It reminds me of the dysfunctional situation on mobile OS. Sure, theoretically a plugin could get compromised and an update would be malicious. That is true for any software I run on my machine.But it also comes with costs. The browser is less customizable and further locked down. That reduces possibilities without netting advantages for me. Overall this is security FUD in my opinion. And the negatives can be observed in mobile OS. reply wredue 6 hours agoparentprevAnd the security problems of malicious ads slipping through at higher rates aren’t an issue? reply boomer_joe 9 hours agoparentprev>as I run macOSHow is the FDE story on macOS? Isn&#x27;t it closed source - how can you tolerate that as a cryptographer? (Not saying Linux is perfect, cryptsetup doesn&#x27;t have a secure AEAD mode) reply zahllos 2 hours agorootparentAn AEAD mode on a physical disk doesn&#x27;t make a lot of sense. You are mapping disk blocks to disk blocks (in the case of cryptsetup, literally via devicemapper) and so you have two choices: a) alter the sector size to something weird so you can fit in tags per sector, likely breaking a lot of code that can&#x27;t cope with this or b) just use XTS and accept that you can&#x27;t have AEAD.It isn&#x27;t like the average hard disk permits padding oracles and chosen plaintext&#x2F;ciphertext attacks to be mounted easily, except of course if you are storing disk images in the cloud, but then you&#x27;re using the wrong tool anyhow - do crypto at the file level where you aren&#x27;t constrained by sector sizes. reply saagarjha 3 hours agorootparentprevApple’s crypto implementation is. reply boomer_joe 1 hour agorootparentAnd what&#x27;s the use if you don&#x27;t know they&#x27;re not compiling against a backdoored version under the hood? reply pipes 1 hour agoparentprevIs there any benefit to ublock origin full fat Vs lite? I&#x27;ve been using it for years on Firefox and android but it sounds like I should switch? reply sebzim4500 11 hours agoparentprevHqng on, MV3 still lets extensions read web traffic, right? It just can&#x27;t block it. reply FiloSottile 11 hours agorootparentFirefox&#x27;s implementation of MV3 allows both async permission-less blocking (declarativeNetRequest API) and permissioned synchronous blocking (webRequest API). uBO Lite uses the former to provide an ad-blocker without read&#x2F;write permissions.You can still write a unsandboxed extension with MV3 (and in Firefox it will still be able to intercept requests, while in Chrome it will not be on the network hot path) but the point is that you can also write a permission-less ad-blocker now, which is what I want. reply minedwiz 11 hours agorootparentprevdeclarativeNetRequest (https:&#x2F;&#x2F;developer.chrome.com&#x2F;docs&#x2F;extensions&#x2F;reference&#x2F;decla...) involves loading a ruleset into the browser, which then does the blocking itself inside the network process. reply npace12 11 hours agorootparentprevYou need the webRequest API (that uBO Full is using) from manifest v2 to be able to read the traffic. Without it, you can just block&#x2F;allow based on rules.Chrome is deprecating it with v3, Firefox supposedly no. reply sebzim4500 10 hours agorootparentUnless I&#x27;m misunderstanding the docs, the webRequest permission isn&#x27;t going anywhere, just the webRequestBlocking one. So it doesn&#x27;t sound like there has been any security win here. reply npace12 9 hours agorootparentYeah, I think you&#x27;re correct. The security win is that you can block without needing the permissions for webRequest which are \"can read and modify everything you do\" reply eviks 7 hours agoparentprevIf you sideload an extension, you can achieve your 0 third-party silent autoupdate goal without compromising on any functionality (though this misfeature should be a per extension toggle at the browser level) reply justinclift 7 hours agorootparentIn Firefox, you can disable automatic updates per-extension. So you don&#x27;t need to sideload to achieve this. reply eviks 5 hours agorootparentEven better, thanks for the correction reply Dalewyn 8 hours agoparentprev>I am down to zero third-party developers that can get compromised and silently push an update that compromises all my web sessions.Why would you even have autoupdates in the first place if that is your threat model? reply downWidOutaFite 7 hours agoparentprevI don&#x27;t like the goal of giving less power to extensions. Extensions have traditionally generated independent innovation, when they&#x27;re allowed to. They&#x27;re an escape hatch. reply vGPU 6 hours agoparentprevLack of custom filters is an immediate no-go. reply sneak 11 hours agoparentprevYou could also just turn off extension autoupdate. reply FiloSottile 11 hours agorootparentI considered that a few times, but eventually complex things like modern ad-blockers rot, so I would be forced to update every once in a while, and let&#x27;s be honest: I am neither qualified nor prepared to audit the diff.I guess deferring updates would give me lead time to let others get targeted &#x2F; detect an issue before it&#x27;s likely I would get the update. Still, installing the permission-less version is so much simpler and reassuring. reply sneak 3 hours agorootparentI rely on the latter. I am much more concerned about supply chain attacks of mass exploitation than I am about 0day in my Signal client or my browser extensions.If there is something big enough to warrant quick update, my HN addiction will make sure I find out about it before it is a 1day.There really isn&#x27;t a great configuration for browser security rn, is there? The gold standard I think is Qubes, which afaict is not practical. reply captn3m0 11 hours agorootparentprevThat only makes a difference if you’re auditing each extension update. Switching to extensions with per-site permissions reduces the attack surface drastically and you don’t have to worry about auditing or disabling updates. reply huydotnet 11 hours agorootparentprevTurn off extension autoupdate sounds like a bad choice, not all updates are mallware injected, many of them may contains security updates anyway reply vachina 8 hours agoparentprevIf you don’t trust the OG ublock what makes you think you can trust the Lite ublock? reply Barrin92 8 hours agorootparentthe fact that it cannot do a lot of untrustworthy things under the new v3 policy, like remote execute code, that is literally the point. reply postalrat 6 hours agorootparentThe point is to neuter extensions to the point where they can&#x27;t effectively block ads. reply pritambaral 4 hours agorootparentprevCan uBOL be autoupdated to a v2 extension? That would negate this point. reply tech234a 4 hours agorootparentIt would only partially negate the point. Any new permissions would trigger a prompt for the user to accept the additional permissions before installing the update. Also there is some aspect of human review for updates to extensions on the Mozilla Addons site. reply pgeorgi 4 hours agorootparentprevAt that point it would request the global \"read and modify all sites\" permission, which makes it kinda obvious. reply predictabl3 11 hours agoparentprevSo can you tell Firefox to only allow MV3 (or MV3+sandboxed, I guess) extensions then? Or have you manually audited your list of extensions?I was sort of aware but your post clearly reminds me that Firefox extensions are probably my single biggest point of general vulnerability on my phone and computer, given how much is done in browser.Appreciate your original thoughts either way. reply dealuromanet 11 hours agoparentprevWhat do you think about using Brave on Apple with its built-in ad-blocking? reply nvy 7 hours agorootparentBrave just shows a different set of ads. reply anderber 6 hours agorootparentThat&#x27;s only if you opt-in to the Brave ads. reply stefan_ 11 hours agoparentprev> attackers could still get into Mozilla, Apple (as I run macOS), or cause a backdoored update to be pushed via Homebrew [..] but unsandboxed browser extensions were clearly the lowest hanging fruitThis is a total non-sequitur. The source of all malicious browser extensions is Google, Apple and Mozilla, and none of them have demonstrated any willingness whatsoever to fix the problem, even when a mere grep across their distributed extension base can trivially identify all the various openly advertised trojan SDKs that cause millions of users to be tracked or have their internet connection reused for various shady proxy websites. reply kccqzy 9 hours agorootparentYou have a different definition of \"malicious\" than the general public. In fact most of us on HN do. That shouldn&#x27;t be dictating what browser vendors think of as malicious extensions. Consider an extension that tracks your browsing in exchange for giving you promo codes to get 5% off on some purchase. Plenty of users have considered this kind of trade off and decided that the 5% discount is worth the privacy impact. Most HNers would consider it malicious. But if browser vendors start to block these extensions we would sooner hear news reports of tech companies being overly paternalistic.You are not speaking for all users and you know it. reply flangola7 7 hours agorootparentNo informed user would consent to a Trojan however. reply beebeepka 3 hours agorootparentJust like nobody informed is using a smartphone or cloud anything reply dancemethis 8 hours agoparentprevYou use Mac. You are already being attacked by Apple. Both on the permissions to run the computing you want, and your data being harvested by them.Good on you nonetheless to check one less, but the one still open is much larger, so the fight goes on. reply botanical 5 hours agoprevOne thing I&#x27;ve noticed is that for years uBlock used to say 7% of all data requests was blocked; in this past year it&#x27;s climbed to 8%. So almost 10% of data transferred is useless to me as it consists of ads, trackers and annoyances.I wonder in my lifetime how much bandwidth and energy I&#x27;ve saved if a blocker has blocked around 10% of all data requests.I&#x27;ll stick with the full version on Firefox. reply Pannoniae 52 minutes agoparentMy one says 26% was blocked. It&#x27;s shocking. reply abwizz 1 hour agoparentprevtrue, the ammount of data, energy, attention and time safed by adblockers cannot be understated reply guerrilla 2 hours agoparentprevI&#x27;m surprised to hear it&#x27;s so little. reply nottorp 2 hours agorootparentIt sounds like 7-8% of requests. If that&#x27;s video ads it can be 50% of the actual data transferred? reply maratc 1 hour agorootparentBut then, if you mostly watch videos (or take a video call in your browser), the ads — both video and non-video — can fall to 1% of the actual data transferred.As each person&#x27;s internet usage is different, the percentage of requests blocked seems to me a better measurement than the percentage of the actual data transferred. reply nottorp 38 minutes agorootparentIt is, i was just nitpicking on mixing # of requests and data volume. reply abwizz 1 hour agorootparentprevnot entirely sure if that was a question because it is formed like a statement, but, yes. reply npace12 11 hours agoprevOne interesting thing I noticed while trying to port little-rat to FF, using the same declarativeNetRequest API as uBOL last week:In Chrom*, extensions can intercept calls from other extensions, while in Firefox, they can&#x27;t. If anyone happens to have any insight, please let me know.EDIT: removed links as I&#x27;m being downvoted, not trying to promote, just would love to make it work in FF. reply rc_kas 11 hours agoprevHow is this different that the uBlock Origin addon for Firefox that I have been running for the last 5 years? reply captn3m0 11 hours agoparentIt uses browser provided APIs for filtering, instead of running script injection on every page. This improves security, and performance at the cost of some capability. The reduction in capability comes from the inability to do all kinds of cosmetic filtering, but it lets you enable this on a per site basis.Check the details on the extension page for more information. reply throwawaylolx 1 hour agorootparent\"Cosmetic filtering\" sounds unimportant, but it&#x27;s what most people expect from ad blockers. It&#x27;s a bizarre term. reply afterburner 11 hours agorootparentprev> The reduction in capability comes from the inability to do all kinds of cosmetic filteringOh, that&#x27;s too bad. The cosmetic filtering is incredible. I wonder how much I would be impacted by switching to Lite. Guess I&#x27;ll try one day. reply eclipticplane 10 hours agorootparentIt&#x27;s definitely jankier without cosmetic filters. You end up with content holes and weird layouts when trackers or ads don&#x27;t load -- much like browsing on bad wifi. You still avoid (most) ads.The new method will almost certainly allow site&#x2F;ad network operators to work around the block filters more easily than they could uBlock Origin. reply mission_failed 10 hours agorootparentprevAre there stats on performance differences?Anecdotally, I find that blocking ads and associated garbage massively improves page loading, and FF performs fine even with hundreds of tabs open reply captn3m0 10 hours agorootparentWould be nice to see benchmarks, I agree. Will take a stab at it, if I remember #ideas. reply 38 11 hours agorootparentprev> improves security, and performance> reduction in performancehuh? reply captn3m0 11 hours agorootparentTypo, reduction in capability. Corrected above. reply tech234a 10 hours agoparentprevThe biggest loss is the ability to add custom Block Element rules, and currently filter lists must be selected from a pre-set list. reply eipi10_hn 9 hours agorootparentActually as a volunteer for the project, I personally consider the lack of regex filters, `redirect-rule` and unable to fast updates are more severe than \"hiding elements\" ability. reply kccqzy 4 hours agoparentprevI&#x27;m going to use it and not the original uBlock Origin for a week and see how it goes. Only sure way to tell. reply syntaxing 11 hours agoprevThree questions, is this less resource intensive and does it still block YouTube ads?Also, since it uses manifest v3, how slim are the chances it’ll be ported to safari? reply thangalin 10 hours agoparentCan you run both versions? Here&#x27;s my config for uBlock Origin: ||accounts.google.com&#x2F;gsi&#x2F;*$xhr,script,3p ##.ytp-endscreen-content youtube.com##.ytp-scroll-min.ytp-pause-overlay youtube.com##.ytp-ce-covering-shadow-top youtube.com##.ytp-pause-overlay youtube.com##.ytp-ce-covering-overlay youtube.com##.ytp-ce-element ! 2021-06-10 https:&#x2F;&#x2F;www.statista.com statista.com##.vertical-align-content.default.otCenterRounded instagram.com##.RnEpo instagram.com##body:style(overflow: auto !important) ! 2023-07-08 https:&#x2F;&#x2F;www.roadandtrack.com www.roadandtrack.com##journey-modal-meterYouTube ads are blocked, as are those pervasive sign-in requests from Google. reply ripdog 10 hours agorootparentThis new addon can&#x27;t do any cosmetic filtering. It doesn&#x27;t run any code on the page, only declares filters for network requests. reply raxxorraxor 1 hour agorootparentI guess a year late Google might release sone ad CDN on their domain that embedds third party ads and then you are out of luck. Same as Microsoft already did that with news. They try to embed themselves as gatekeepers here too. reply palmer_fox 10 hours agoparentprevSafari still has a limited implementation of Manifest v3, so that might affect the timeline. E.g. declarativeNetRequest API, which all adblockers use heavily, is missing very important functionality like redirects.If this extension doesn&#x27;t use the missing features then porting is as simple as running a single command to generate an Xcode project and then building the extension executable. reply SpacePortKnight 10 hours agoparentprevSafari already has a long list of content blockers which blocks ads by supplying a list of urls to the browser. I use Ka Block! for iOS and it works well enough. reply Not_Real 7 hours agoprevuBlock origin is probably the best project to be created in the last decade. The amount of websites that are unusable with ads is crazy. reply EMM_386 7 hours agoprevI was a Firefox user since Phoenix&#x2F;Firebird and only recently switched to Brave for performance (although I think I&#x27;m going back, given the recent performance gains).I have also been using uBlock Origin heavily since the start.I&#x27;m not sure I fully understand the purpose of this. If this is a Manifest V3 thing, I thought Mozilla wasn&#x27;t adopting it ... so why would uBlock need to adopt it on Firefox?I&#x27;m clearly missing something. reply donatzsky 1 hour agoparentFirefox is also moving to Manifest V3, but a more \"relaxed\" version that still allows a lot of what is being removed in Chrome.What seems to have happened here, is that uBO decided that, since they now have a declarative version for Chrome, they may as well release it for FF also (but with a few improvements, apparently). reply cosmojg 6 hours agoparentprevI believe the plan is to support it without forcing it, unlike Chrome which plans to force it. reply cubefox 10 hours agoprevUnpopular take.Wouldn&#x27;t it be more ethical to not visit ad supported websites in the first place? Instead of removing the source of their income while still consuming their content?Someone should make an extension \"SiteBlock Origin\": Everytime it detects the presence of an ad, the whole website gets blocked, not just the ad. That would be ethically consistent. reply eipi10_hn 9 hours agoparentNo.The ethical principles written clearly by World Wide Web Consortium are for users, NOT for websites:> 2.12 People should be able to render web content as they want> People must be able to change web pages according to their needs. For example, people should be able to install style sheets, assistive browser extensions, and blockers of unwanted content or scripts or auto-played videos. We will build features and write specifications that respect peoples&#x27; agency, and will create user agents to represent those preferences on the web user&#x27;s behalf.https:&#x2F;&#x2F;www.w3.org&#x2F;TR&#x2F;ethical-web-principles&#x2F;#render---> Everytime it detects the presence of an ad, the whole website gets blocked, not just the ad. That would be ethically consistent.By the time the extension knows if there&#x27;s ads or nots, the trackers&#x2F;fingerprinting connections are already loaded to users&#x27; machines.Written in literature, it sounds awesome. In practice with real programming, it&#x27;s awful. reply nsonha 7 hours agorootparentthey&#x27;re talking about ethics, and you&#x27;re talking about authority aka what the w3c recommends reply eipi10_hn 1 hour agorootparentAnd if rendering contents as client users want is unethical, that authority section won&#x27;t ever need to exist. reply cubefox 47 minutes agorootparentThis is not always unethical. We are only talking about ad blocking here. reply nhinck2 9 hours agorootparentprevDespite being called an ethical web principle, that really doesn&#x27;t make it ethical. reply eipi10_hn 8 hours agorootparentDon&#x27;t know why there&#x27;s no reply button under your other reply. (Ah ok, I see it now, looks like HN needs to wait a bit before that button appears).For me, it&#x27;s ethical. Loading trackers&#x2F;malicious connections&#x2F;contents on my own machine is unethical. That&#x27;s it. I don&#x27;t run those on your server, why do you run them on my devices?What I said is simple: there&#x27;s ethical pricinples standing by users&#x27; sides, and nothing for websites.If you think it&#x27;s unethical, you do you. I won&#x27;t participate in arguing about your personal preferences. reply nhinck2 8 hours agorootparentYou can continue to reply through your profile.Loading malicious content certainly is unethical. I&#x27;m not disputing that, I run an ad-blocker and I advocate to everyone that they should.However, that isn&#x27;t what we&#x27;re talking about, we&#x27;re talking about blocking ads as a concept. It is pretty indisputably unethical as it breaks the social contract of the service delivery.The fact that is easy to do, has no punishment, and is incredibly low stakes doesn&#x27;t make it ethical. reply eipi10_hn 8 hours agorootparent> blocking ads as a conceptProblem is, ads now are trackers. Of course, there are few ads that are not. I also won&#x27;t mind if the ads are static images (that are not generated from&#x2F;linked to 3rd-party&#x2F;trackers) and unable to click on. Thing is, those are just rare, and in practice blockers can&#x27;t block them by default, because they are not distinguishable with other contents. So in general, those are not blocked, and blocking \"ads\" (the ads that are trackers) is still ethical to me.And just FYI, blockers have the rule that don&#x27;t block self-promotions (self-advertisements) by default. reply barbariangrunge 8 hours agorootparentprevAds aren’t neutral, informative pieces of information. Most are there to manipulate you, often subconsciously. Eg, all the product placement in tv and movies is subliminal advertising, or the car ads meant to make you think something is high status without ever using a logical argument. Looks what ads have done to our culture over the last 30 years, and the environmental and financial consequences connected to it reply insonable 7 hours agorootparentprevi think eipi10_hn&#x27;s point is that from the very beginning of designing&#x2F;imagining the web, those involved wanted to make it a user-controlled experience. so the disconnect here is between two views: a) there is an obligation to support sites by watching ads or b) content providers should know defining principles of this medium dictate that users can block&#x2F;change&#x2F;etc so they support content with blockable ads at their own risk. Under b) users blocking ads is ethical whereas sites trying to circumvent ad-blocking are acting unethically. Both stances have merits it seems to me. reply ndriscoll 7 hours agorootparentprev> It is pretty indisputably unethical as it breaks the social contract of the service delivery.I would dispute that there is such a social contract, any more than there is a social contract that if you download a patch to fix DRM, you are implicitly agreeing to install the virus it comes with.Ad-funded businesses are engaging in market dumping, subsidizing their offerings by poisoning the minds of billions of people, and creating anxiety, insecurity, and dissatisfaction in the process. If someone gives you something for free covered in lead dust, and you accept it but clean the dust off first before touching it, I don&#x27;t see the ethical quandary. Particularly when you know their widget cost them a fraction of a penny, and they were being paid to give you the poison.Like Bill Hicks said, these people are Satan&#x27;s little helpers. Engaging with Satan and undermining him may be unwise, but it&#x27;s not unethical.As others have pointed out, these people also have a level of stalking going on that I don&#x27;t think the average person (or even a relatively informed person) can grasp, and so there&#x27;s no possibility for a social contract to exist there. reply nhinck2 7 hours agorootparentSince you like analogies...There is a supermarket that at the checkout has a bowl of candy that operates on the honour system.If you push a button next to the bowl of candy an ad will play and you can take a piece of candy. The candy itself costs a fraction of a cent to the business and the business doesn&#x27;t care to put anyone in place to monitor compliance with button pushing.This system is known by everyone and operating in this way for decades so there is no deception towards the person at the supermarket.Is it ethical to take a piece of candy without pushing the button? reply ndriscoll 7 hours agorootparentIn the supermarket analogy, it&#x27;s more like saying you forgot your shoppers card and having the checkout person scan one for you, or using 867-5309 as your phone number. And no, it&#x27;s still not unethical. The unethical actor here is Kroger buying every major grocery chain, and adding 20% to your bill if you don&#x27;t agree to be tracked. Normal humans in the loop, employees included, will happily support you undermining their system. reply nhinck2 6 hours agorootparentThat&#x27;s not what I asked, is it unethical to take the candy? reply ndriscoll 6 hours agorootparentNo, it isn&#x27;t. In practice no human will care whether you push the button. The social understanding is that it&#x27;s fine to just take the free candy. In fact, the employees probably don&#x27;t want to hear the ad again, so it is an ethical imperative to not push the button and subject them to that. reply kirenida 4 hours agorootparentThere is probably some human who sold the ad space to somebody, and who is monitoring how many button presses there are. And they will probably put pressure on the supermarket to make sure customers are reminded that they have to push the button if they want candy. Sure, the employees are probably sick of the ad, but the people who don&#x27;t have to hear the ad don&#x27;t care about them :) reply austinjp 1 hour agorootparentprevI&#x27;d take the candy without watching the ad, for the same reason I refuse to use loyalty cards. Both the ads and loyalty cards are worth more to the supermarket than they are to me. They&#x27;re basically ripping me off while pretending to give me something gratis.(Actually, in reality I&#x27;d ignore the candy since I don&#x27;t need more sugar.) replyeipi10_hn 9 hours agorootparentprevWhat is \"it\" here? reply nhinck2 8 hours agorootparentThat blocking ads is ethical. reply sensanaty 7 minutes agoparentprevI don&#x27;t care about the ethics here because the ad companies, the parasites they are, don&#x27;t give a shit about ethics. They track every single possible thing there is to track about a person and sell that information to anyone with a couple of bucks to spend.Funnily enough, of all websites out there one of the best is still 4chan when it comes to ads. They have 2 banners, one at the top of the page, one at the very bottom of the page. These are static banners, at most a gif, with no tracking pixels or fingerprinting capabilities or any other similar form of horrid, unethical behavior. No embedded ads masquerading as regular content, nothing that blocks interaction on the page, just simple banners that target the site&#x27;s particular niche like anime or cheap junk from Japan.But as long as websites aren&#x27;t using this model of ads and are instead opting for something disgusting like https:&#x2F;&#x2F;fingerprint.com then you won&#x27;t see an iota of sympathy or care for \"ethical\" behavior from me. reply kbrosnan 10 hours agoparentprevI ran without an adblocker for a long time with a similar sort of reasoning. What got me to finally install an adblocker is an increase in malvertising. Going to legitimate sites with third party ads resulted in drive by downloads, fake update warnings, fake AV warnings, attempts to get you to install shady extensions, etc. I disable the adblocker for websites that use better ad sourcing methods.Ex. https:&#x2F;&#x2F;support.mozilla.org&#x2F;en-US&#x2F;kb&#x2F;i-found-fake-firefox-up... reply qrio2 10 hours agorootparentI think this is a key to the argument for ad-block. If it was literally just banner ads without tracking, sure, go right ahead. Modern web advertising is so much more than that (aggressive tracking, data collection without consent, or worse).I miss getting those banner ads for decreasing my mortgage rates as a 14 year old who doesn&#x27;t even pay rent yet reply lobsterthief 8 hours agorootparentprevUnfortunately “better ad sourcing methods” require a lot of human capital to support (direct-sold ads, constant monitoring of inventory, being able to afford higher bid floors, etc.) or ultimately access to better advertisers by having a large amount of traffic.All of these are features of larger publishers, unfortunately, which means that smaller publishers suffer more malvertising. So you’re basically just supporting large publishers. Which is definitely better than supporting none, so I still commend you :) reply wredue 6 hours agorootparentYeah. Why would we ever want people to have jobs when the executive branch can just pocket those wages for themselves instead. reply kerkeslager 5 hours agoparentprev> Wouldn&#x27;t it be more ethical to not visit ad supported websites in the first place? Instead of removing the source of their income while still consuming their content?That&#x27;s fundamentally not how the web works. If you want me to pay for content, you need to get me to agree to pay for content. Just requesting a page, which I have no way beforehand of knowing contains ads, is not me agreeing to pay for the content. If you didn&#x27;t want me to view the content without paying for it, why did you send me the content?This is morally equivalent to the fake monk scam[1] in NYC where a fake Buddhist monk gives you a prayer bracelet and then demands that you pay them for it. You don&#x27;t get to give people things and then demand that they pay for it when that was never agreed upon. Even if the payment is with their attention.This is all setting aside the ethical blight that advertising, by its very nature, poses in the first place. Advertising is just lying--either literally, or by omission through presenting a one-sided view of products. There is never a case where advertising is ethical.Note that the NYT has mostly stopped serving up content to people who haven&#x27;t agreed to pay for it, and they&#x27;re doing quite well financially lately.[1] https:&#x2F;&#x2F;tricycle.org&#x2F;article&#x2F;monk-scam&#x2F; reply Prickle 9 hours agoparentprevI don&#x27;t think it is that unpopular of a take. Generally speaking, Ads and subscriptions pay for the website.The issue I personally have is:1) When the Ads themselves contain malware.2) Eat up all your bandwidth&#x2F;mobile data.2.1) Have auto-playing videos &#x2F; popups.1) is somewhat rare. But it is something that has happened multiple times with major websites and services.If I remember correctly, the Washington Post and Yahoo have previously had this issue. Google&#x27;s Advertisement platform has repeatedly allowed malware to spread via their advertisement system. (Both on Mobile devices, and desktop devices, but usually more focused on mobile devices.)2) is something I have to deal with everyday on the phone. When on a train filled to the brim, a lot of times the connection speed drops precipitously. In short, I don&#x27;t have bandwidth to spend on an Ad, especially a video Ad. So I block them all, and usually try not to browse any image or video heavy sites.2.1) is really just a quality of life thing. reply klabb3 8 hours agoparentprevIt’s an arms race. The utopian hyper-civilized ethics are replaced when your adversary are doing everything possible to turn you into a product. Tracking, fingerprinting, creating shadow profiles for you, etc etc, etc without any meaningful consent.If the adversaries followed idealized ethics, they would respect DNT header, for one.That said, actively avoiding those actors who are unethical is commendable. It’s just very difficult to do in practice, since basic communication with eg neighbors, parents, friends are mainly through these channels. reply akincisor 10 hours agoparentprevThe contract of the web is:- I ask for a resource - you give it to me - any linked resources (stylesheets, scripts, images etc) are up to me to requestTherefore there is no \"ethical\" conundrum in blocking ads. The ad industry brought this on themselves by trying to push malware, spam and actively trying to make the web worse. reply HeckFeck 6 hours agorootparentAgreed. Advert blocking wasn’t a necessity until adverts became intrusive, tracking and targeting became pervasive, and every site flooded with cookie banners.I remember when AdWords was just a humble bar of contextual text links, absolutely manageable. Not so much the case now. reply qrio2 10 hours agoparentprevThis is an interesting argument. I own my computer and network, should I not be allowed to control what content is or is not allowed in my network? I guess the corollary that would follow from MY argument is that they should be permitted to block me from accessing their site if they see I&#x27;m not permitting ads reply eipi10_hn 9 hours agorootparentExactly, users are the ones who should allow and block what contents to be served to their devices, NOT the websites.> 2.12 People should be able to render web content as they want> People must be able to change web pages according to their needs. For example, people should be able to install style sheets, assistive browser extensions, and blockers of unwanted content or scripts or auto-played videos. We will build features and write specifications that respect peoples&#x27; agency, and will create user agents to represent those preferences on the web user&#x27;s behalf.https:&#x2F;&#x2F;www.w3.org&#x2F;TR&#x2F;ethical-web-principles&#x2F;#renderDon&#x27;t fall for what ads companies&#x2F;corporations are trying to shape users&#x27; thoughts. reply johnnyanmac 9 hours agorootparentprev>I guess the corollary that would follow from MY argument is that they should be permitted to block me from accessing their site if they see I&#x27;m not permitting adsThat&#x27;s pretty much what Medium and many general news sites are doing. I haven&#x27;t paid for one yet, but I can respect the move if it means they don&#x27;t need to rely on clickbait to build a customer base. reply mcfedr 4 hours agorootparentJudging by all the titles on medium, click bait is still an important part of revenue, it&#x27;s just not driving ads consumption but subscriptions reply zarzavat 7 hours agoparentprevPeople don’t block ads because they want to deprive websites of income. They block ads because they have been driven to it, by the ads themselves.This could be avoided if websites served ads responsibly: no JS, no animations, no video, no audio, no tracking, no scam merchants, no tricks, no manipulation, no unskippable ads, no dishonesty.Almost no websites do this, so I have no ethical qualms giving of the ads the banhammer.Once websites start respecting their users, then we can have this conversation about ethics, but not a second before. reply oneshtein 3 hours agorootparentWhy some sites are blocked from net due to having just a link to a «bad» website, while many other sites and ad networks receive zero punishes for their active attempts to scam victims or harm victim computers? reply eipi10_hn 1 hour agorootparentExamples? reply johnnyanmac 9 hours agoparentprevFair point and I do pay for ad free browsing in a few sites. But consider1. sites that don&#x27;t have any other model. e.g. my favorite game news website is Gematsu, but holy heck is the ads crazy intrusive. On mobile we are talking full screen video ads that have a tiny X to remove... for maybe 1 minute. I&#x27;ve expressed interest multiple times to donate or otherwise do something to directly fund the site but nothing has come up. And even if I did move on to make a point, this model isn&#x27;t something that has spread to many, if any, modern gaming news site (and I&#x27;ve long since left Reddit, a topic in and of itself). Do I just give up on gaming news and let clickbait Youtubers inform me instead of written articles?2. Exploratory purposes. I&#x27;m not going to know which and what websites do or do not have ad support, and most of my browsing when searching is very casual. I wouldn&#x27;t feel too compelled to neither turn off my ad block nor pay a sub for some place I googled up once 3 months ago for a quick answer. I don&#x27;t quite have an answer for this one.3. ublock isn&#x27;t simply blocking ads. trackers, certain cookies, overly large media elements, java script, remote fonts, even individual pieces of HTML elements you specify in a CSS manner. It&#x27;s so much more powerful and privacy-oriented than a simple ad blocker. If it closed off any site with any of these issues there simply wouldn&#x27;t be an interet to browse.It&#x27;s a compromise at the end of the day, and I can only look out for myself at some point. I&#x27;m not necessarily trying to teach websites a lesson per se. reply kerkeslager 5 hours agorootparent> If it closed off any site with any of these issues there simply wouldn&#x27;t be an internet to browse.That&#x27;s actually not true. It&#x27;s just that it&#x27;s much harder to find that internet, because search engines are controlled by advertisers. reply johnnyanmac 3 hours agorootparentWell sure, it&#x27;s not going to literally block every single website (to my surprise, HN on fact seems to lack all of the above factors. Or at least UBlock cannot trace them. Kudos). But so much of the internet is closed down that I essentially cannot rely on anything that isn&#x27;t a small personal blog (that is NOT hosted by any of the major web deployment platforms. e.g. WordPress).I&#x27;d need to roll my own email provider (and deal with that fallout since I&#x27;m now \"spam\". Ironic), cannot apply to 99% of job portals (employer nor job boards), cannot use most of my productivity apps on the web, and I still can&#x27;t access most major news, subscription or not.If I was still in school there&#x27;s a non-zero chance I can access my class portals.I can replace most of these, but not all. reply rodric 5 hours agoparentprevThe web is, in theory, an open venue, and somebody publishing on the web is not unlike somebody performing in the street. It is not your duty, as the consumer, to ensure the producer’s income—particularly not at the expense of your privacy. The producer has something to say, and you the consumer are willing to hear it: that may just as well be the extent of your relationship. How, or even whether, the producer monetizes this state of affairs is not the consumer’s responsibility, though some consumers (who can) may choose to patronize the producer. reply meristohm 10 hours agoparentprevI&#x27;d use that. As it is, I often back out from sites that ask me to disable my adblocker, and often do the same when the cookie-choice pop-up is present; it&#x27;s a helpful check on how I&#x27;m spending my time. I&#x27;m absolutely spoiled for choice there, and as with a meal of mostly minimally-processed plants, I feel best after reading a book. Which is not to say I never eat&#x2F;read the snack&#x2F;article that is quickly but momentarily diverting. reply Benjamin_Dobell 10 hours agoparentprevIf the site offers an ad-free paid subscription model, that&#x27;s reasonable. I mean, it&#x27;d be much better just to redirect to the sign-up page. However, if the site is so user hostile that they think bombarding users with invasive ads is the only way to monetise, well that&#x27;s on them. reply L3viathan 1 hour agorootparentEven better: if it offers a way to pay a few cents to read _this one article_. I don&#x27;t want to subscribe to hundreds of websites for reading a single article every so often. reply barnabee 1 hour agoparentprevIMO showing advertising itself is unethical and there’s no right to force anyone to see an advertisement, no matter how much some companies would like there to be.Any content you make available publicly is fair game to be remixed, reformatted, summarised, and yes, ad-blocked.It’s not the user’s job to make someone’s business model work. reply KptMarchewa 10 hours agoparentprevRunning ublock is like not looking at ad. Would you support TV that requires you to focus on ad while it&#x27;s running? reply johnnyanmac 9 hours agorootparentI don&#x27;t think it&#x27;s quite the same because ads don&#x27;t care if you are \"focusing on the ad\". Well, they kinda do, but not by any useful metric (idling on a computer =&#x2F;= engaging with the ad).It&#x27;s more equivalent to changing the channel during a commercial, which seems to be what the GP is implying as an action. reply SpaghettiCthulu 9 hours agorootparentWhich works great until most channels conveniently have commercials running at the same times reply bobro 6 hours agoparentprevIn the end I just dont care about being ethical to these companies. It’s like screwing over my drug dealer. reply tekeous 9 hours agoparentprevNot visiting cost them nothing. No serve, no cost.Viewing the ad made them money.Visiting and not viewing the ad lost them money. They paid for the server but made no money.Only one of these three options is painful enough for them to get the point. It’s harsh i know, writers need to eat, but they need to understand I won’t “pay” them with my eyeballs unless the site is usable in return at a bare minimum. reply josefx 9 hours agoparentprev> Wouldn&#x27;t it be more ethical to not visit ad supported websites in the first place?There is a negative feedback loop where most third party content is only published on the most popular sites, so it becomes impossible to entirely avoid these sites even if the companies behind them are cancer.> That would be ethically consistent.Don&#x27;t drag ethics into a mud fight with billion dollar companies. I lived through ads that faked download buttons, faked virus alerts, provided links to fake \"official\" download sites with malware or directly tried to infect your computer. The only ethical thing you can do with the ad industry is rob those rotten sociopaths blind. reply jokoon 3 hours agoparentprevThat&#x27;s already what I am somehow doing. I always think twice before opening a link.Reddit mastodon lemmy etc already sanitize websites I read, and often I just read the comments, not the article.Often people quote important parts of the article in the comments.Ad blockers is like an antibiotic, I use them but I also try to not expose myself to germs. reply mr-pink 5 hours agoparentprevno, the web wasn&#x27;t intended to be such a commercial hellscape. if you want to make money ethically you should come up with your own way to reach people. reply nottorp 2 hours agoparentprevI&#x27;ll be ethical about ads when the ads are ethical about me :) reply flangola7 7 hours agoparentprevAs my hair grays I have reached the determination that for-profit advertising itself is systematically unethical. Maybe it was ethical many, many decades ago; here today any moral values it once had are long gone.To that end any mechanism that reduces the presence and effect of advertising is a moral imperative. reply richard_cory 5 hours agorootparent> for-profit advertising itself is systematically unethicalVery interesting. Can you please expand a bit more on why do you think this is the case? reply magic_hamster 4 hours agorootparentNot oc, but I share some of this sentiment. Modern advertising is heavily based on behavioral science, psychological and especially emotional manipulation. This is on top of extreme methods to hijack your attention at all cost. It might sound like hyperbole but if you read marketing case studies you realize this isn&#x27;t only the norm, it&#x27;s something they take pride in, especially when it appears to work (which it does).In my view, blocking this isn&#x27;t just morally just, it&#x27;s absolutely necessary. I deliberately choose not to partake in this and not be a target for manipulation to the best of my ability.Maybe there was a time when advertising was more about creating awareness instead of feeling and making you want the product, but advertising changed dramatically over the 20th century. There&#x27;s quite a lot of reading material out there if you&#x27;re interested. reply barnabee 1 hour agorootparentprevAlso not oc, but…Without advertising, “content marketing”, and paid placements&#x2F;reviews people would buy things when they desire or need them.They’d ask friends, compare specs, and read&#x2F;watch reviews before determining what to buy.That is: without ads, people would gravitate towards buying what fits their needs best. They would make generally rational choices given the information available.Advertising’s job is to subvert those rational choices and make people buy something, whether it’s the best option or not. In fact, even when they don’t actually want or need anything at all.It causes people exposed to it to spend money unnecessarily, and on the wrong products and downright bad products. Some are more susceptible than others, but in the end it’s an illegitimate tax levied every time you buy something. Even if you didn’t respond to advertising when making a purchase, advertising is so ubiquitous and necessary in most markets that the price you paid probably contributed to the advertising the manufacturer had to deploy to keep up with the arms race.There’s nothing ethical or necessary about any of this.Ideally there would be legislation that would force business models to change, but while there is not, ad blocking is absolutely an imperative. reply jayd16 10 hours agoparentprevOh no. He caught us.Oh well... reply vanous 6 hours agoprevTIL it can be installed into Thunderbird [1], awesome. Is this thanks to the MV3?https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;addon&#x2F;ublock-origin... reply hiisukun 6 hours agoprevIs there any upside to mv3 in terms of ublock&#x2F;adblocker usage detection?I understand the significant security implications (whether or not you agree with mv3), but found myself wondering if the permissionless model might make it harder to detect a user adjusting the page at a different point in the request&#x2F;response&#x2F;draw cycle. reply kwstas 12 hours agoprevCool to have this available in FF too. Other than less permissions I can also compare the behaviour directly with chrome now. reply jessekv 3 hours agoprevMy favorite use of uBlock Origin is opting-in to allow javascript per site. I miss this functionality on moble. reply foepys 3 hours agoparentFirefox for Android can install uBlock Origin and do this. reply lfmunoz4 5 hours agoprevwhy don&#x27;t we block ads at the operating system level instead at the browser level? If we are talking about having more security and performance wouldn&#x27;t it more sense? Not sure I understand how ublock origin works anyone have a summary on this? reply pythux 1 hour agoparentThe operating system level does not have as much information as the browser so it would be much less effective. You could do domain-level blocking (mostly) but miss lots of granularity on URLs, types of requests, contexts of request (what&#x27;s the URL of the parent frame?), etc.Also it would be impossible to perform cosmetics injections (scriptlets, alterations of the DOM, etc.)It might already provide a good base-line but definitely not enough for all cases (e.g. YouTube ads, etc.) reply vitorgrs 3 hours agoparentprevWhy operating system level and not network level? See Pi-Hole (or NextDNS, Adguard DNS) reply abwizz 1 hour agoparentprevbecause the operating system can&#x27;t see what&#x27;s going on inside the connection or even know about the semantics of individual elements, the user-agent does that.the value of dns&#x2F;ip level blocking is diminishing as resource density per host increases due to ipv4 shortage and should be completely inpractical with ipv6 due to adress space size. reply hjek 3 hours agoparentprev> why don&#x27;t we block ads at the operating system level instead at the browser level?sure, we do that, too: https:&#x2F;&#x2F;github.com&#x2F;StevenBlack&#x2F;hosts reply orthecreedence 2 hours agoparentprevBut this would make Windows 10+ unable to boot =[ reply downWidOutaFite 5 hours agoparentprevMost of the network streams are encrypted at the os level. reply Jiahang 6 hours agoprevI&#x27;m looking forward to using it on safari reply nuker 8 hours agoprevNow its time to revive uBO for Safari, same model imho. reply jxy 12 hours agoprevWould this be ported to Safari? reply yankput 12 hours agoparentThe safari rules are even less capable that that, last time I checked. reply tech234a 11 hours agorootparentOngoing discussion: https:&#x2F;&#x2F;github.com&#x2F;uBlockOrigin&#x2F;uBOL-home&#x2F;issues&#x2F;52 reply midoridensha 8 hours agoparentprevIf you&#x27;re worried about blocking ads, Safari is not the browser to use. Just use something better. reply n42 5 hours agorootparentI use Safari every day and don’t see any ads. reply mcfedr 4 hours agorootparentDo you visit websites with it? reply slater 10 hours agoprevSomeone should make a hardware equivalent, something like a Rpi that sits between your wifi router and modem, that just blocks any advertising JS.Yes, pi-hole exists, but i get the feeling that&#x27;s considered \"too nerdy\". I mean something that&#x27;s about the size of a rubik&#x27;s cube, ethernet in&#x2F;out, power, and works out of the box with zero configuration needed. reply tech234a 10 hours agoparentThat would require decrypting connections to HTTPS websites and would require deploying a root certificate to all client devices on the network, which is probably more complicated than installing a browser extension. reply BenjiWiebe 10 hours agoparentprevAt minimum you&#x27;d need to install another SSL certificate in your browser to allow it to intercept HTTPS. reply AA-BA-94-2A-56 5 hours agoparentprevNextDNS is what I use after running into various networking issues with RPi. reply mtzaldo 12 hours agoprevI dont see it in firefox for android :( reply greazy 12 hours agoparentI believe Firefox curates the installable extensions on android&#x2F;mobile. reply eco 11 hours agorootparentThey are going to finally crack open the full add-on library soon. Sometime after September if I remember correctly. reply cpeterso 4 hours agorootparentYep!> In the coming months Mozilla will launch support for an open ecosystem of extensions on Firefox for Android on addons.mozilla.org (AMO). We’ll announce a definite launch date in early September, but it’s safe to expect a roll-out before the year’s end.https:&#x2F;&#x2F;blog.mozilla.org&#x2F;addons&#x2F;2023&#x2F;08&#x2F;10&#x2F;prepare-your-fire... reply throwthat1 10 hours agorootparentprevYou can already install any addon by creating a collection but its a pain in the ass... reply midoridensha 8 hours agoparentprevDon&#x27;t use Firefox on Android. Use Firefox Nightly instead. reply hendersoon 11 hours agoprevKind of a brilliant compromise, actually. By default it&#x27;s a declarative content-blocker, but if you run into a specific site that shows ads you can enable the full-fat uBlock Origin featureset there. reply eipi10_hn 9 hours agoparentNo, it&#x27;s not full-fat uBO. It&#x27;s more prone to anti-adblock&#x2F;ads-reinsertion (problems with `redirect-rule` and unable to fast updates) and ads&#x2F;trackers&#x2F;popups can slip through if cannot be caught by regex filters. reply thisisthenewme 12 hours agoprevFWIW, Firefox and uBlock on my Android phone will always keep me on that ecosystem. My desire to go into the Apple ecosystem (because of supposed privacy protections) faded as soon as I learned I can&#x27;t really have a good ad blocking solution there. reply fsckboy 12 hours agoparent> FWIW, Firefox and uBlock on my Android phone will alwaysuBlock was the original name for the add-on that subsequently was ethically compromised&#x2F;\"sold out to\" advertisersuBlock Origin is the 2nd version written by the original author (gorhill) and is not compromised.Just wasn&#x27;t sure which you are talking about reply lolinder 11 hours agorootparentI appreciate what you&#x27;re trying to do here, but when I search for \"uBlock\" on Firefox&#x27;s Add-ons, only uBlock Origin comes up in the first 6 pages. It looks like it&#x27;s still available (and even \"Featured\") in the Chrome ecosystem, but in the context of Firefox it&#x27;s no longer ambiguous which one they&#x27;re referring to.https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;search&#x2F;?page=1&q=ub... reply fsckboy 11 hours agorootparentIt&#x27;s good to know people are unlikely to get the wrong one.It&#x27;s still called uBlock Origin, and in general I don&#x27;t think keeping track contextually of when you can get away with a name collision is a great way to do things, and this is an area of privacy concern so I think many of the people interested in the space would like to remain educated about it. reply dharmab 11 hours agoparentprevI&#x27;m using Orion on iOS which has native ad blocking and supports a good number of Chrome and Firefox extensions. Even without uBO I have a virtually ad-free experience.https:&#x2F;&#x2F;browser.kagi.com&#x2F;faq.html#safari reply meruem 11 hours agorootparentI’m super impressed with orion as well. I use an iPad and Orion provides a decent support (still a WIP though) for Firefox&#x2F;chrome desktop extensions to run in iOS. After Reddit axed third party support, I almost stopped browsing Reddit until I found out I can run RES with old.reddit inside Orion. This has been an absolute game changer for me. reply nixass 11 hours agorootparentprevEvery browser on iOS pretending not to be Safari is also huge no. reply dharmab 11 hours agorootparentOrion on iOS is not a Safari reskin. It uses WebKit, but the similarities end there. reply bandergirl 11 hours agorootparentAgain: every browser on iOS is a Safari reskin because it cannot be otherwise. Safari and WebKit are essentially the same thing (download WebKit on Mac to find out)Only “remote browsers” like Opera Mini can currently use something other than the system’s webview. reply NotYourLawyer 11 hours agorootparentprevIf it uses webkit, then it 100% is a Safari reskin. reply eviks 7 hours agorootparentHow is adding ad-blocking a reskin? reply wahnfrieden 11 hours agorootparentprevMore specifically it uses WKWebView. You can’t compile WebKit yourself to include in an app, which means less flexibility than non-iOS WebKit apps and Chromium forks. Their complaint is valid (“reskinned safari” is just a casual way of saying this) reply hnarn 12 hours agoparentprevI’m not saying that it’s as good technically, but I use AdGuard for Safari together with NextDNS and it seems to do the trick. Probably just using NextDNS would go a long way. reply Scoundreller 12 hours agorootparentI use these:https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;Adblock&#x2F;comments&#x2F;koowte&#x2F;encrypted_d...I like how I don’t need a separate app (just install the profile) but I do wonder if I need to implicitly trust the website that has the profiles for download.So far so good though.I use the mullvad ones. Sometimes it breaks public wifi signins, so I switch to a less restrictive one in those situations (usually CIRA, which is the Canadian domain registrar)The really nice thing about DNS profiles is that they’re system wide, so it works against in-app ads too. reply metadat 12 hours agorootparentprevIs AdGuard a proprietary product? I recall looking into it and being a bit turned off once I learned it&#x27;s not FOSS. reply Nextgrid 12 hours agorootparentMost repos here show GPLv3 as the license: https:&#x2F;&#x2F;github.com&#x2F;AdguardTeam reply metadat 11 hours agorootparentI&#x27;d be delighted to be mistaken, because Safari on iPhone sucks with all the ads. reply SSLy 11 hours agorootparentprevthe iOS one is closed. Linux and browser exts are open. reply soraminazuki 9 hours agorootparentCould you possibly be referring to the AdguardForiOS code with the GPLv3 license?https:&#x2F;&#x2F;github.com&#x2F;AdguardTeam&#x2F;AdguardForiOS&#x2F;blob&#x2F;master&#x2F;COP... reply isykt 12 hours agoparentprevWhat Adblock features are missing on iOS? reply Nextgrid 12 hours agorootparentiOS (and macOS Safari) only has the stupid \"declarative blocking\" functionality which is trivial for ads to bypass. In addition, it often breaks websites because it can&#x27;t inject runtime code (like uBlock filters can) to substitute malicious JS payloads with neutered versions that still expose the same API so the rest of the JS doesn&#x27;t error out. reply bandergirl 11 hours agorootparentThat’s false. iOS has had full-fledged extensions for years now. Nothing stops uBO from existing on Safari other than stubbornness.Most serious iOS content blockers ship both a native list (or multiple) and an active counterpart, usually focusing on YouTube ads.However I am aware that adblocking is still poor on Safari, maybe nobody just can match uBO reply Nextgrid 11 hours agorootparentYou are mistaken. Safari removed the APIs necessary for an uBlock port (there used to be one), see https:&#x2F;&#x2F;github.com&#x2F;el1t&#x2F;uBlock-Safari&#x2F;issues&#x2F;158.Injecting code via Web Extensions is too late for reliable blocking - by then, either the malicious JS you are trying to defuse has already ran (if it wasn&#x27;t blocked declaratively), or if not then the rest of the page&#x27;s JS depending on it has already exploded and \"fixing\" it after the fact (by substituting a neutered shim via Web Extensions) doesn&#x27;t fix the rest of the page. reply veave 11 hours agorootparentprevIn theory you are right, in practice it works just as well. reply luuurker 11 hours agorootparentThat depends a lot on the site. It works well on some, but on others it&#x27;s just not enough.Safari&#x2F;iOS blocking is closer to uBlock Origin than to DNS blocking, but is not as powerful as uBO and some sites \"exploit\" those limitations. reply vorpalhex 11 hours agorootparentprevNo, it really does not. My iPad with safari and safari filters next to my android with firefox + ublock is nowhere near as comprehensive. Even news websites sneak ads into safari. reply CraigJPerry 11 hours agorootparentGot any example urls handy? I’m using AdGuard and i just don’t recall getting ads anywhere i visit. I’m interested to see if any slip through.The only exception i can recall right now was youtube but SponsorBlock does great there in Safari. reply Zak 12 hours agorootparentprevBrowser extensions, which can block HTML elements based on arbitrary selectors rather than just origin domain. reply Nextgrid 12 hours agorootparentSafari does actually support CSS selectors in its content blocking API. However, see my other comment on this very subthread, it&#x27;s nowhere near enough and is trivial for ads to bypass. reply lnxg33k1 11 hours agoparentprevMh yeah I am on iOS and at home I have pihole and on the road I have mullvad with ad&#x2F;tracking&#x2F;etc. blocking, and can&#x27;t complain, I never see ads, I think right now all use the same adblock lists more or less so staying in a ecosystem for that seems, I mean everyone do their choices, but there are harder things to overcome reply kmlx 12 hours agoparentprevthere are many good ad blocking solutions on desktop and mobile safari. reply Nextgrid 12 hours agorootparentThey are equivalent to \"Manifest V3\" blockers (like this one). It&#x27;s nowhere as good as original uBlock Origin. reply dharmab 11 hours agorootparentNo, there are full ad blocker solutions on iOS: https:&#x2F;&#x2F;browser.kagi.com&#x2F;faq.html reply blibble 12 hours agoparentprevbrave supports the ublock filters reply leokennis 12 hours agoparentprevSo you’re trading in (supposed) privacy protection for a couple less ad impressions or broken site visits?I mean, to each their own principles but… reply 26 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "uBlock Origin Lite is a content blocker extension for Firefox that blocks ads, trackers, and miners.",
      "The extension uses a declarative approach for efficient filtering and does not require broad permissions at installation.",
      "Users have the option to grant extended permissions on specific sites for more precise filtering."
    ],
    "commentSummary": [
      "Discussions are centered around the use and ethics of ad-blocking extensions, with a focus on uBlock Origin and its various versions.",
      "The security benefits and limitations of these extensions are being debated, including their ability to reduce potential attack vectors and concerns about compromised updates.",
      "The ethical implications of ad-blocking, the invasive nature of online advertising, and the impact of browser functionality changes on ad-blockers are also being discussed."
    ],
    "points": 566,
    "commentCount": 276,
    "retryCount": 0,
    "time": 1692653031
  },
  {
    "id": 37208083,
    "title": "GNU Parallel, where have you been all my life?",
    "originLink": "https://alexplescan.com/posts/2023/08/20/gnu-parallel/",
    "originBody": "Alex Plescan Blog Projects Twitter Email GNU Parallel, where have you been all my life? 20 August 2023Permalink I was recently trying to figure out how likely a bunch of end-to-end tests were to be flaky, and wanted to gather some stats about their pass/fail rates on my local machine before including them in a broader test suite. These tests run for a long time, as they execute extensive scenarios against a live service over HTTP. In this post I’ll share the approach I ended up with using GNU Parallel. A quick aside: If you wanna follow along and run the upcoming examples in your own terminal, use this command to generate some test files. They’ll emulate a flaky test by sleeping between 5-15 seconds, then randomly exiting with a failure (exit code 1) or success (exit code 0): parallel \"echo 'sleep \\$((\\$RANDOM%10+5)) && [ \\$((\\$RANDOM%2)) = 1 ] \\ && printf PASS \\ || (printf FAIL && exit 1)'\" \\ '>' potentially_flaky_{1}.sh ::: {1..5} Typically to gather flakiness stats I’d use a couple of nested loops, one for each test I want to run, and another for each attempt. I like doing this kind of stuff in bash for its simplicity/portability: tests=( potentially_flaky_1.sh potentially_flaky_2.sh potentially_flaky_3.sh potentially_flaky_4.sh potentially_flaky_5.sh ) # For each test for test in \"${tests[@]}\"; do # For each attempt of that test (1 through to 10) for attempt in $(seq 1 10); do # Capture timestamp for when the test started start=$(date \"+%s\") # Run the test and capture its exit code bash \"$test\" > /dev/null status=$? # Calculate duration of the test run end=$(date \"+%s\") duration=$((end - start)) # Print results printf \"$test attempt $attempt took ${duration}s \" if [[ status -eq 0 ]]; then printf \"PASS\\n\" else printf \"FAIL\\n\" fi done done This approach ended up being tediously slow though… since the tests take a while to execute, running them sequentially wasn’t gonna cut it. I knew about GNU Parallel, but had never used it before. $ man parallel and 15 minutes later, I was “living life in the parallel lane” (as the GNU Parallel book encourages you to do!) Rewriting the above to work using parallel ended up looking like: tests=( potentially_flaky_1.sh potentially_flaky_2.sh potentially_flaky_3.sh potentially_flaky_4.sh potentially_flaky_5.sh ) parallel --progress --jobs 5 --delay 2 --timeout 3600 --shuf --results out.csv \\ bash {1} ::: ${tests[@]} ::: {1..10} The joy of finding the right tool for the job can’t be beat - more performance and functionality, with less code! Let’s go into a bit more detail… Passing inputs In GNU Parallel, you specify a command that is to be executed in parallel. In the example provided, the command is bash {1}. The {1} is a placeholder that gets replaced with each input value (if you have more than one input you can use {2}, {3} etc). The inputs to the command are specified after the ::: operator. In this case, the inputs are the array of test scripts (${tests[@]}) and a sequence of numbers from 1 to 10 ({1..10}). These inputs are provided to the command in all possible combinations. So in this case, we have 5 test scripts and we want to run each one 10 times, parallel will execute each command 50 times in total. Controlling concurrency parallel provides a number of options that can be used to avoid resource contention, here are a few that I found useful for my tests: --jobs 5: Caps the number of concurrent jobs to 5 (by default parallel will try to execute as many jobs as you have CPU cores). --delay 2: Ensures each job waits for 2 seconds before starting, preventing a thundering herd problem. --timeout 3600: Terminates any jobs that have been running for over an hour. --shuf: Runs the jobs in a shuffled order. Capturing output By default the output of your command will be printed to your terminal, however in this case since I wanted to capture stats - using parallel’s capability to output a CSV file instead was very helpful: --results out.csv: Outputs job completion results to the given file which includes duration, exit codes, and captured stdout/stderr. --progress prints live progress as the jobs are executing. The CSV file ends up looking like this (only including the first lines for brevity): Seq,Host,Starttime,JobRuntime,Send,Receive,Exitval,Signal,Command,V1,V2,Stdout,Stderr 4,:,1692491267.732,6.025,0,4,1,0,\"bash potentially_flaky_5.sh\",potentially_flaky_5.sh,9,FAIL, 2,:,1692491263.646,12.025,0,4,0,0,\"bash potentially_flaky_3.sh\",potentially_flaky_3.sh,3,PASS, 1,:,1692491261.604,14.067,0,4,1,0,\"bash potentially_flaky_5.sh\",potentially_flaky_5.sh,2,FAIL, 5,:,1692491269.779,6.023,0,4,1,0,\"bash potentially_flaky_1.sh\",potentially_flaky_1.sh,3,FAIL, 3,:,1692491265.686,11.055,0,4,1,0,\"bash potentially_flaky_4.sh\",potentially_flaky_4.sh,8,FAIL, It’d be trivial to use this output to aggregate/chart stats. Exploring further… This is barely scratching the surface of what parallel can do. I strongly recommend the excellent, free, and funny book by Parallel’s author Ole Tange. The first chapter takes 15 minutes to get through and covers 80% of what you’re likely going to use. The book covers things such as: Distributing jobs across different hosts using SSH, a powerful feature for leveraging multiple machines. Monitoring the mean time for job completion and setting timeouts for jobs based on a percentage of the mean, providing more control over long-running tasks. Retrying if jobs are known to be failure prone. Resuming jobs if parallel execution stops midway, ensuring you don’t lose progress. Limiting amount of jobs that can run based on CPU utilization (or other signals). Limiting concurrency with a semaphore (and an excellent analogy about toilets). Happy paralleling! Want to get in touch? Send me an email, a tweet, or check out my GitHub. This website is open source, and built using Jekyll. Photos are © Alex Plescan (2023).",
    "commentLink": "https://news.ycombinator.com/item?id=37208083",
    "commentBody": "GNU Parallel, where have you been all my life?Hacker NewspastloginGNU Parallel, where have you been all my life? (alexplescan.com) 432 points by alexpls 22 hours ago| hidepastfavorite249 comments BoppreH 20 hours agoIt&#x27;s a nice tool, but it also shows the shortcomings of shell commands.In a proper programming language, we&#x27;d have something like parallel [1..5], i => { sleep random()*10+5; possibly_flaky i } &#x2F;&#x2F; [{\"Seq\": 4, \"Host\": \":\", \"Starttime\": 1692491267...And `parallel` would only have to worry about parallelization.Instead, the shell environment forces programs to invent their own parameter separator (:::), a templating format ({1}), and a way to output a list of structures (CSV-like). You can see the same issues in `find`, where the exec separator is `\\;`, the template is `{}`, and the output is delimited by \\n or \\0. And `xargs` does it in yet another different way.It&#x27;s very hard to acquire and retain mastery over a toolbox where every tool reinvents the basics. If you ever found yourself searching \"find exec syntax\" multiple times in a week, it&#x27;s not your fault.As for alternatives, I&#x27;m a fan of YSH[1] (Javascript-like), Nushell[2] (reinvented from first-principles for simplicity and safety) and Fish[3] (bash-like but without the footguns). Nushell is probably my favorite from the bunch, here&#x27;s a parallel example: lswhere type == dirpar-each { |it| { name: $it.name, len: (ls $it.namelength) } }[1] https:&#x2F;&#x2F;www.oilshell.org&#x2F;release&#x2F;latest&#x2F;doc&#x2F;ysh-tour.html[2] https:&#x2F;&#x2F;github.com&#x2F;nushell&#x2F;nushell[3] https:&#x2F;&#x2F;fishshell.com&#x2F; reply JNRowe 18 hours agoparent[I&#x27;m not recommending this, but maybe… No, no. I&#x27;m not sure…]It isn&#x27;t even just the newer shells that have solved this, zsh also has a solution out of the box¹. The extensive globbing support in zsh can largely replace `find`, and things like zargs allow you to reuse your common knowledge throughout the shell.For example, performing your first example with zargs would use regular option separators(`--`), regular expansion(`{1..5}`), and standard shell constructs for the commands to execute.I&#x27;ll contrive up an example based around your file counter, but slightly different to show some other functionality. f() { fs=($1&#x2F;*(.)); jo $1=$#fs } zargs -P 32 -n1 -- **&#x2F;*(&#x2F;) -- fThat should recursively list directories, counting only the files within each, and output² jsonl that can be further mangled within the shell². You could just as easily populate an associative array for further work, or $whatever. Unlike bash, zsh has reasonable behaviour around quoting and whitespace too.Edit to add: I&#x27;m not suggesting zargs is a replacement for parallel, but if you&#x27;re only using a small subset of its functionality then it may be able to replace that.¹ https:&#x2F;&#x2F;zsh.sourceforge.io&#x2F;Doc&#x2F;Release&#x2F;User-Contributions.ht...² https:&#x2F;&#x2F;github.com&#x2F;jpmens&#x2F;jo³ https:&#x2F;&#x2F;github.com&#x2F;stedolan&#x2F;jq reply chasil 18 hours agoparentprevGNU Parallel is also based on perl, so the footprint is quite large.GNU xargs implements limited parallelization, and is compiled C. This functionality is present within busybox, including the Windows version.https:&#x2F;&#x2F;www.linuxjournal.com&#x2F;content&#x2F;parallel-shells-xargs-u...GNU Parallel will have much greater functionality, but it will not reach as far as xargs. reply reddit_clone 15 hours agorootparent> GNU Parallel is also based on perlTime to rewrite it in Rust &#x2F;s:p reply steveklabnik 14 hours agorootparentThere have been multiple ports already, I believe. reply mistrial9 15 hours agorootparentprevmeanwhile, python DASK is very well funded to be cloud-native, and also local.. however it relies on a python runtime, so you know .. also not sure about the DASK license terms reply coliveira 19 hours agoparentprevWhat you mention is the main reason why shell script is not a decent language to write long programs. It is full of inconsistencies, and since it depends on other commands, you have to learn the quirks of each command you use. Moreover, good luck if you need to debug this. Shell should only be used for small scripts that are easy to debug. reply runeks 18 hours agorootparentIf doing even simple things requires looking up documentation, why does it matter whether the shell script is long or short?Spending extra time doing simple things — because you need to Google e.g. \"how to pass multiple space-separated arguments from a string to a command\" — is also a waste of time. reply lysium 19 hours agorootparentprevDo you recommend any good alternative when your shell program gets too large?Honest question, as I’m struggling to leave the shell environment once the program gets too large. I could use Perl, but $? and the likes get quickly out of hand. Python’s support for pipes was difficult last time I used it, but that may have changed. What would you recommend? reply fho 3 minutes agorootparentUnpopular opinion, but I used Haskell \"scripts\" with relative success for a while. Stack has a nice script interpreter mode that is runnable in the familiar #! way.Even allows to add dependencies and if necessary compile the script on the fly. reply HeckFeck 18 hours agorootparentprevYou&#x27;ve some hesitation with Perl, but if you stick at it, you&#x27;ll find what you seek. It feels very &#x27;unixy&#x27; and can achieve much the same as shell while being more consistent in its syntax. Its portability means it will work the same across environments. Plus the newest editions have niceties like modern classes and try&#x2F;catch as inbuilt language features.Sharing this because its the route I went, anything I&#x27;d have written in Bash I&#x27;d now do in Perl. reply lysium 13 hours agorootparentThank you for encouraging me to use Perl. After Perl 6 came out I got confused at what and how to use and hence I’ve abandoned that path. I’ll try once more now. reply phone8675309 17 hours agorootparentprevThe tooling around Perl has also gotten better over the last decade or so while also allowing you to pack everything to run on even ancient machines running old Perl 5. reply BoppreH 18 hours agorootparentprevIf it&#x27;s too large, then just write normal Python code. It&#x27;ll be a lot longer than the equivalent shell-like script, but you&#x27;ll gain it back in maintenance effort, debugabillity, and robustness. reply tesseract 14 hours agorootparentprevGood ergonomics for Perl-style quick and dirty text processing were part of the original design goals for Ruby. Those parts of the language are still there. You can write code that feels more concise than Python yet, IMO, tends to be more readable&#x2F;maintainable than Perl can stereotypically be. Modern style guides, however, de-emphasize that style of Ruby since it might not be the most appropriate in the context of say a large Rails project. reply mplewis 18 hours agorootparentprevI use Go. You can run scripts with go run directly, and this package makes shell tasks easy: https:&#x2F;&#x2F;github.com&#x2F;bitfield&#x2F;script reply lysium 13 hours agorootparentScript looks promising, thank you! I’ll give it a try, as some sister comment also suggests Go. reply cb321 14 hours agorootparentprevOn Unix, you might try Nim (https:&#x2F;&#x2F;nim-lang.org) with https:&#x2F;&#x2F;github.com&#x2F;Vindaar&#x2F;shell and there are a slew of pipeline-y&#x2F;Unix-y utilities over at https:&#x2F;&#x2F;github.com&#x2F;c-blake&#x2F;buNim is statically typed and (generally) native-compiled, but it has very low ceremony ergonomics and a powerful compile-time macro&#x2F;template system as well as user-defined operators (e.g., you can use `+-` to make a constructor for uncertain values so that `9 +- 2` builds a typed object as in https:&#x2F;&#x2F;github.com&#x2F;SciNim&#x2F;Measuremancer . reply lysium 13 hours agorootparentThanks for pointing me to nim, it looks promising. I‘ll try to use https:&#x2F;&#x2F;nim-lang.org&#x2F;docs&#x2F;osproc.html to pipe programs.My use case is approx. like this: I can get 80% what I want with ls …sed …grep -v … but then it gets complicated in the script and I’d like to replace the sed or grep part with some program. reply cb321 13 hours agorootparentThis sounds like a job for what standard C calls \"popen\". You can do import posix; for line in popen(\"ls\", \"r\").lines: echo linein Nim, though you obviously need to replace `echo line` with other desired processing and learn how to do that.You might also want to consider `rp` which is a program generator-compiler-runner along the lines of `awk` but with all the code just Nim snippets interpolated into a program template: https:&#x2F;&#x2F;github.com&#x2F;c-blake&#x2F;bu&#x2F;blob&#x2F;main&#x2F;doc&#x2F;rp.md . E.g.: $ ls -lrp -pimport\\ stats -bvar\\ r:RunningStat -wnf\\>4 r.push\\ 4.f -eecho\\ r RunningStat( number of probes: 26 max: 31303.0 min: 23.0 sum: 84738.0 mean: 3259.153846153846 std deviation: 6393.116633069013 ) reply lysium 42 minutes agorootparentThank you! Popen looks like what I was looking for! reply colejohnson66 17 hours agorootparentprevPowershell uses proper objects instead of stringly nonsense. reply reddit_clone 15 hours agorootparentprevPerl6&#x2F;Raku is my personal choice (when I can).Python to me, is too far away from shell&#x2F;unix. It is a programming language for writing applications. For the use case of writing shell scripts but in a more powerful language, perl is still the king here (or it should be. Sadly it doesn&#x27;t appear to be the case. No one is using it except for die hard gray beards.)Raku is a modern (still a big) language with kitchen sink. Again doesn&#x27;t appear to be much uptake. reply paulddraper 14 hours agorootparentprevPython.Just the inclusion of argparse alone is worth it IMO.> Python’s support for pipes was difficultWell, the idea would be to replace a lot of your pipe usage.Off the wall, but Scala has a concise syntax for process operations, but startup time is likely prohibitive. reply paddim8 12 hours agorootparentprevElk is a Shell language with syntax similar to Python. https:&#x2F;&#x2F;elk.strct.net reply mongol 17 hours agorootparentprevI once read a HN thread that recommended Go for this, and it made me interested. I think it was a useful suggestion, it made me learn Go, and I kind of agree with it, 5+ years after. It is not a shell, but it is simple and fast and useful. reply salawat 2 hours agoparentprevYour find exec problem can be trivially solved with either - exec &#x2F;bin&#x2F;bash -c \"script\" or you can spend a little extra time figuring out how to properly structure your scripts in such a way where the incocations just flow with little more than an invocation +getoptsIf you feel like the answer is rewriting the shell, the answer is practically never rewriting the shell. It&#x27;s learning to use it. reply zackmorris 18 hours agoprevSince nobody asked, I&#x27;m reiterating my position that computers to effectively utilize parallel functionality simply aren&#x27;t available today. I&#x27;ve always wanted a computer with at least 256 cores and local content-addressable memories beside each core to send data where it&#x27;s needed. By Moore&#x27;s Law, we could have had MIPS machines with 1000 cores around 2010, and 100,000 to 1 million cores today, for under $1000.Contrast that with GPU shaders where one C-style loop operates on buffers separate from system memory, and can&#x27;t access system services like network sockets or files. GPUs have around 32 or 64 physical cores, so theoretically that many shaders could run simultaneously, although we rarely see that in practice. And we&#x27;d need bare-metal drivers to access the GPU cores directly, does anyone know of any?The closest thing now is Apple&#x27;s M1 line, but it has specialized NN and GPU cores, so missed out on the potential of true symmetric multiprocessing.The reason I care about this so much is that with this amount of computing power, kids could run genetic algorithms and other \"embarrassingly parallel\" code that solves problems about as well as NNs in many cases. Instead we&#x27;re going to end up with yet another billion dollar bubble that locks us into whatever AI status quo that the tech industry manages to come up with. And everyone seems to love it. It reminds me of the scene in Star Wars III when Padme notes how liberty dies with thunderous applause. reply pradn 16 hours agoparent1) Amdahl&#x27;s law means it&#x27;s not useful to have hundreds of cores for general purpose computing. There&#x27;s not that much parallel work to do in typical applications. Increasing the proportion of work that&#x27;s parallelizable for a given application pays dividends when you have more cores - that&#x27;s why Servo is so exciting. In some cases, picking an O(n2) algorithm that&#x27;s easy to parallelize will be faster than a less parallizable O(nlog(n)) algorithm - this is true for problems like Single-Source Shortest Paths (SSSP).2) Shared resources (in-memory mutable data, hardware devices) mean the ratio of contention to CPU work goes up when you have more cores.3) Cores on a single die need to share the same constraints - thermal limits and transistor count. So you&#x27;re best off having enough powerful cores to get you to a sweet spot of single-core performance vs multi-core parallelism.4) It&#x27;s hard to provide a performant and useful many-core machine model. Cache coherence makes it easier to program a many-core machine, but limits performance. Without it, you&#x27;re stuck with distributed systems-style problems. reply JonChesterfield 17 hours agoparentprevThis exists now. Some AI accelerators are a grid of independent compute units with their own memory, message passing between them. Graphcore&#x27;s IPU is an instance.An AMD GPU is a grid of independent compute units on a memory hierarchy. At the fine grain, it&#x27;s a scalar integer unit (branches, arithmetic) and a predicated vector unit, with an instruction pointer. Ballpark of 80 of those can be on a given compute unit at the same time, executed in some order and partially simultaneously by the scheduler. GPU has order of 100 compute units, so that&#x27;s ~8k completely independent programs running at the same time.You&#x27;ve got a variety of programming languages available to work with that. There&#x27;s a shared address space with other GPUs and the system processors, direct access to system and GPU local memory. Also some other memory you can use for fast coordination between small numbers of programs.There&#x27;s a bit of a disconnect between graphics shaders, the ROCm compute stack and what you can build on the hardware if so inclined. The future you want is here today, it just has a different name to what you expected. reply MawKKe 17 hours agoparentprev1000 cores?? I don&#x27;t have 100 cores! What do you even need 10 cores for? Well, here&#x27;s 4 cores. Give 2 to your brother. Don&#x27;t go wasting all those hyper threads all at once!Intel ca. 2010, probably reply giantrobot 17 hours agorootparentAlso Intel: ECC memory support? In this economy? reply FuckButtons 16 hours agoparentprevArguing about where we should be based on a projection of an empirical exponential curve seems pretty irrational. Nothing in reality is exponential forever. reply dragontamer 16 hours agoparentprevTypical GPUs are easily 6000+ shaders (aka kinda-sorta like cores) on the more expensive end.At least, 6000+ 32-bit multiplies per clock tick on ~2GHz+ clocks. Even cheap GPUs easily are 2000+ shaders.> GPUs have around 32 or 64 physical coresNVidia SMs and AMD WGPs are not \"cores\", they are... weird things. They have many shaders inside of them and have huge amounts of parallelism.As far as grunt-work goes, a \"multiplier unit\" (literally A x B) is perhaps the most accurate count to compare CPU cores vs GPU \"cores\", because the concept of CPU-core vs GPU WGP &#x2F; SM is too weird and different to directly compare.Split up that WGP &#x2F; SM into individual multipliers... and also split up the ~3 64-bit multipliers or ~48 CPU SIMD multipliers per core (3x 512-bit on Intel AVX512 cores), and its perhaps a more fair comparison point.---------Back 20 years ago, you&#x27;d only have 1x multiplier on a CPU core like a Pentium 4, maybe as many as 4x with the 128-bit SSE instructions.But today, even 1x core from Intel (3x 512-bit SIMD) or 1x core from AMD (4x 256-bit SIMD) has many, many, many more parallel elements compared to a 2004-era CPU core. reply imtringued 45 minutes agorootparent>NVidia SMs and AMD WGPs are not \"cores\", they are... weird things. They have many shaders inside of them and have huge amounts of parallelism.They aren&#x27;t weird things. They are the equivalent of CPU cores. By your logic CPU cores aren&#x27;t CPU cores, \"they are... weird things\" because of SMT. reply imtringued 56 minutes agoparentprevSorry but we do have computers with 256 cores. I used to have this excuse back when processors only had 4 cores. When you consider that processors lower their turbo boost frequency as you use more cores and there is overhead from synchronization, your 4 core processor may only give you a 2x performance benefit at the expense of your code becoming difficult to reason about (depending on the problem at hand). Nowadays 8 core processors are quite cheap, below 200€. At 4x performance boost and easily 12x more if you are willing to spend the money, it is definitively worth it. The caveat of course is that there aren&#x27;t actually that many programs that need the full power of your processor. The most common exception is a video game that was developed for a limited number of players or even single player but then the multiplayer version of the game becomes extremely popular and you get servers with 60 or even a hundred players, way beyond what the developers planned to support. Supporting multiple cores was not a priority and then very suddenly it becomes the biggest bottleneck.The real problem we are facing is that our programming models aren&#x27;t parallel by default.>By Moore&#x27;s Law, we could have had MIPS machines with 1000 cores around 2010, and 100,000 to 1 million cores today, for under $1000.https:&#x2F;&#x2F;corescore.store&#x2F;You can have 10000 RISC-V cores on an FPGA but nobody cares. Why? Because even a bit serial processor (that means it processes one bit per clock cycle, or 32 clock cycles for a 32 bit addition) runs into memory bandwidth limitations very quickly if you have enough of them. Main memory is very slow compared to registers and caches. The only way to utilize this many cores is by having a workload that is entirely latency bound. Your memory access pattern is perfectly unpredictable. The moment you add caching, the number of cores you can have shrinks dramatically and companies like AMD are not slimming down their CPUs, they are adding more and more cache. Their highest end processors have almost a gigabyte of cache. reply gumby 16 hours agoparentprevHave you considered finding a Connection Machine? reply _a_a_a_ 16 hours agoparentprevRead a lot of this kind of post. Years ago I recall someone bleating for 8 cores when 1 or 2 was the norm. Now you want 256. Next generation will ask for thousands. All for nothing because you have no idea what to do with it except give the handwaviest justifications. A computer&#x27;s a tool to do an actual job. You can and probably do have more computing power on your desktop than all the world&#x27;s supercomputers put together from the 1970&#x27;s.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cray_X-MP Price US$7.9 million in 1977 (equivalent to $38.2 million in 2022) Weight 5.5 tons (Cray-1A) Power 115 kW @ 208 V 400 Hz[1] CPU 64-bit processor @ 80 MHz[1] Memory 8.39 Megabytes (up to 1 048 576 words)[1] Storage 303 Megabytes (DD19 Unit)[1] FLOPS 160 MFLOPSIn 2070 it still won&#x27;t be enough for you. It never will be enough. reply ketanmaheshwari 22 hours agoprevGNU Parallel has been one of my go to tool to accomplish more on the terminal. Generate test data, transferring data from one node to another using rsync, run many-task, embarrassingly parallel jobs on HPC, pipelines with simple data dependencies but run over hundreds or files are some of the places where I use GNU Parallel.Many thanks to Ole Tange for developing the wonderful tool and helping the users on Stack Overflow sites to this day.Shameless plug, I am developing a tutorial on GNU Parallel to be presented at eScience conference in Cyprus this year: https:&#x2F;&#x2F;www.escience-conference.org&#x2F;2023&#x2F;tutorials&#x2F;gnu_paral... reply juujian 21 hours agoparentI&#x27;m surprised the CPU would in any way be the bottleneck for transferring data. Is it really faster to parallelize that? reply Ultimatt 21 hours agorootparentIt&#x27;s more GNU Parallel has host groups in a config so you can send files for a job to the right one where its going to execute and bring things back. Essentially it can turn a local xargs type job into any kind of remote task execution including dealing with files locally needing to be remote. reply Aissen 21 hours agoprevGNU parallel is great for the kind of tasks highlighted in the post. Note that being written in Perl, it&#x27;s slower than its simpler C counterpart moreutils parallel. And that in many uses cases xargs --max-procs=$(nproc) can replace it. reply cstrahan 17 hours agoparentI also recommend checking out `xe`: https:&#x2F;&#x2F;github.com&#x2F;leahneukirchen&#x2F;xeIt’s like xargs with sane defaults and a couple tricks of its own. reply astrodust 19 hours agoparentprev`xargs` has you covered in more cases than most realize. reply cb321 17 hours agorootparentThis really is true and you may be understating with \"most\". Here are a couple: mkdir &#x2F;tmp&#x2F;g seq 1 10tr \\\\n \\\\0xargs -0n2 -P4 bash -c &#x27;t=$EPOCHREALTIME; sleep $((RANDOM%5)); echo \"$@\" >&#x2F;tmp&#x2F;g&#x2F;$t&#x27; d0 cat &#x2F;tmp&#x2F;g&#x2F;*Another one is xargs -P \"$(nproc)\" --process-slot-var=s sh -c &#x27;grep X \"$@\" >>&#x2F;tmp&#x2F;g.$s&#x27; d0 cat &#x2F;tmp&#x2F;g.*You can also cobble together that second style with a custom config setup wherein a command is given $s and responds with some host names and there might be an `ssh` in front of the `grep`, for example. That `d0` argument (for $0) is a bit janky and there can be shell quoting issues, of course. But then again, you may not have hostile filenames&#x2F;whatever. Remote loadavg adaptation might be nice, but then again, maybe you control all the remotes. Similarly, I could not get back-to-back executions of the EPOCHREALTIME thing closer than 250 microseconds. So, collision basically will not happen even though it probably could in theory. reply green-orca 21 hours agoprevI&#x27;m using task spooler a lot for parallel background processing. What I like the most it the ability to add further tasks to the queue after processing has already started.https:&#x2F;&#x2F;manpages.ubuntu.com&#x2F;manpages&#x2F;xenial&#x2F;man1&#x2F;tsp.1.html reply pdimitar 21 hours agoparentNever knew about this, thanks! I&#x27;ll definitely try it because `parallel` has bitten me before in a few more advanced cases. It has rough edges here and there. reply NelsonMinar 15 hours agoparentprevWow this tool is fantastic, thank you! The UI is very nice and simple. How has this not existed in Unix for 30+ years?https:&#x2F;&#x2F;github.com&#x2F;justanhduc&#x2F;task-spooler reply gjvc 21 hours agoparentprevfrom that man page, there is a name clash with \"ts\" from moreutils reply codetrotter 20 hours agorootparentI installed task-spooler just now, because I’ve been wanting something like this for a long time.It looks like the actual name of the task-spooler command on Debian after install is “tsp”, not “ts”. So no collision :)Now it just remains to be seen if the package by default allows the tasks to continue to run after I log out, or if systemd will annoyingly kill the tasks after I disconnect from ssh the same way systemd annoyingly kills my “screen” sessions when I disconnect ssh, and there is some cumbersome thing you have to do on each of your systems to have systemd not kill “screen” :( reply chriswarbo 20 hours agorootparentprevSome distros rename the binary to &#x27;tsp&#x27; (I think Debian does that) reply twic 17 hours agorootparentI really want there to be a database-backed version for larger tasks called tbsp. reply aftbit 19 hours agorootparentprevmoreutils also clashes with parallel, does it not? i remember installing some package for chronic and thus breaking GNU parallel, at least back in the late 2010s. reply gjvc 18 hours agorootparentyes it does, good point reply throwaway277432 21 hours agoprevIs the author still adding the \"cite me or pay 10000€\" notice to the output? And calling that GPL?And still answering every xargs Stackoverflow question with \"you should use GNU Parallel\" instead of answering the question? That really gets old quickly when googling for xarg answers.These are just some of the reasons I&#x27;ll never use parallel. xargs is perfectly fine for most usecases, and it can do everything I need it to. reply dahart 20 hours agoparent> Is the author still adding the \"cite me or pay 10000€\" notice to the output? And calling that GPL?IIRC the citation notice was cleared by Stallman as GPL compatible. I’d be surprised if anyone’s paid, I assumed that’s rhetoric to imply the value of a citation, or lack of citation, for anyone publishing scientific works.> These are just some of the reasons I’ll never use parallel.Hey I’ve actually ranted on HN before about the citation notice (e.g. https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=15319715) - in part because I find the language of the notice a little misleading; it’s not tradition to write citations for tools used to conduct research, and it’s a requirement (not just tradition) to cite academic sources. If I used parallel to speed up some calculations, that doesn’t justify an academic citation. I don’t cite bash or python or C++ when I write papers either. On the other hand, if I’m writing a computer science paper about how to parallelize code, and especially if I compare it to GNU Parallel, then a citation isn’t optional, and I don’t need a guilt trip to add one, it’ll get requested in review, and rejected without. Is there even a journal publication to cite? (Edit: found it - the request is to cite an article in USENIX magazine.) So I find the notice a little irritating and I’m not sure who it’s aimed at exactly, or what the history of Ole feeling snubbed by scientists really is. Maybe some people were trying to compete with GNU Parallel and failing to cite it? Maybe Ole is paid by an organization that appreciates citations and will continue to fund development on Parallel if there’s evidence of it’s use in academia?That said, GNU Parallel really is totally awesome, the documentation is amazing, and the citation notice is a one-time thing you can silence permanently. I don’t think the notice is a good reason to never use Parallel, and I do think Parallel is worth using, FWIW. reply thomasahle 20 hours agorootparent> it’s not tradition to write citations for tools used to conduct research.Thia is true, but it also makes it very hard for academics and PhD students who mainly write software over papers. They get no citations and eventually have to leave academia.If we had a better practice of citing central software we use - at least the academic software that wants to be cited - we could have a more flourishing ecosystem of such software funded by the universities. reply dahart 20 hours agorootparentI can understand that, and I can understand Ole’s request for citations - Googling him it looks like he is (or has been) employed by a university.The good news is that the new ‘tradition’ these days for academic software is to open-source all the software written for a paper or academic project, so practically everything done is visible on github & arxiv. reply gorjusborg 19 hours agorootparentprev> They get no citations and eventually have to leave academia.You&#x27;re welcome?Seriously though, adding the citation nag to software is two wrongs not making a right.As a software user, it isn&#x27;t my fault academia hasn&#x27;t figured out how to reward software contribution. If they can&#x27;t figure it out, finding a greener pasture makes a lot of sense. reply thomasahle 15 hours agorootparent> As a software user, it isn&#x27;t my fault academia hasn&#x27;t figured out how to reward software contribution.If you&#x27;re not writing papers, the citation notification isn&#x27;t for you. Can&#x27;t you just mute it and continue using the software without worries? reply gorjusborg 13 hours agorootparent> Can&#x27;t you just mute it and continue using the software without worries?It _seems_ like a reasonable thing to ask, it&#x27;s a minor inconvenience, really, so what&#x27;s the big deal?The big deal is that the behavior doesn&#x27;t fit the unix philosophy. Tools are meant to do one thing, and do it well. They get composed in pipelines to get jobs done. In these pipelines, the communication medium is text, via stdin&#x2F;stdout&#x2F;stderr. If a tool is unpredictable in what it puts out via text, it can make the whole pipeline unpredictable, or at least more complicated.If it _was_ okay, we should welcome everyone putting nag features in these simple cli tools, right? Well, I&#x27;d be on board with that as long as I can blanket disable all of them. If not, let&#x27;s just leave our political&#x2F;professional&#x2F;begging messaging outside our computing tools. Okay? reply specialist 16 hours agorootparentprevLike a colophon for research? reply rlpb 19 hours agorootparentprev> and the citation notice is a one-time thing you can silence permanentlyThis doesn&#x27;t scale. Imagine if all the software you used nagged you and had their own individual methods to silence them. I don&#x27;t think this would be reasonable.What makes this particular software so special? reply repiret 19 hours agorootparentLots of software nags with something when it first starts up. It’s mostly annoying, but doesn’t seem to have a scaling problem.zsh gives you a config wizard, sudo admonishes you to use it responsibly, just about every iOS app and an increasing number of desktop apps gives you a few pages of “what’s new” every time they’re upgraded. Desktop apps have given tips-on-startup since the 90s. reply dahart 19 hours agorootparentprevI agree, and I think I might have even used this argument before. ;)It does scale solely for GNU Parallel though for now, and very few other people have taken the same tack as GNU Parallel’s citation notice. Despite the potential for a slippery slope, it doesn’t seem to be happening. I’d speculate that if it did start to happen, then GNU would change their stance on what’s allowed by the license, perhaps. reply shakow 17 hours agorootparentprev> This doesn&#x27;t scale.--will-cite reply imtringued 38 minutes agorootparentThere is a difference between \"will cite\" and being in a situation that warrants citing. reply paulmd 15 hours agorootparentprevsure, but the other guy wants me to submit a picture in a funny hat, not a citation. and the third guy wants me to add some additional legal provisions and disclaimers to the GPL license.you need a more generalized --clickwrap-consent parameter really. One that just says \"whatever it is, I accept and I&#x27;ll do it\".And that&#x27;s exactly the thing GPL was supposedly founded to get away from. Restrictions on user freedoms. Especially violations so routine and tedious that we open-palm-slam \"accept\" without reading them.You could absolutely write this to not look like a clickwrap agreement and lean on users. \"please cite me, I&#x27;m an academic and impact matters\" in the manfile or --help is not something anyone would ever get upset about or probably even patch to remove.The only reason it&#x27;s OK is because basically everyone knows it&#x27;s not enforceable because of the severability part of the GPL. But it&#x27;s blatantly designed to look like a serious and enforceable notice to users who don&#x27;t know that, and require affirmative action from the user to \"consent\" and bypass the screen. And clickwrap agreements of this type are generally enforceable if there is not something like the GPL that allows you to ignore it.like I flatly do not get why this is even debatable or questionable, the dude is trying to pull a fast one on users with a scary-sounding legal notice that implies that you need to accept this clickwrap agreement. and it&#x27;s not entirely clear that he cannot actually burden you with this in all jurisdictions, since it&#x27;s an agreement between you and the author that exists outside the actual source code&#x2F;distribution. You can end up paying for free stuff in lots of places in life, if you&#x27;re not aware about what \"should\" be free, and those agreements stand and are enforceable even though the thing was supposed to be free. You agreed to it. You don&#x27;t have to, the GPL says that, you can edit the software to remove it without consenting, but you did accept it.Letting the camel&#x27;s nose under the tent on clickwrap agreements on GPL&#x27;d software is such an incredibly bad idea legally and morally, and this dude has been an utter dick about anyone who questions that. Sure, \"he&#x27;s willing to do it and nobody else is stepping up\" but on the other hand he&#x27;s also going off and attacking other maintainers doing their jobs, too. But that&#x27;s not Stallman&#x27;s problem I guess. That&#x27;s another problem that only works with N=1 jerk, if that was normalized we&#x27;d have a problem.I do not get why this guy is getting this special blessing or dispensation from FSF. Like it&#x27;s not just that he&#x27;s a random weirdo releasing under GPL and then trying to add additional terms (lol get stuffed), this is all occurring with the FSF&#x27;s blessing, Stallman&#x27;s signoff, and in the GNU distribution. Official GNU clickwrap license I guess.At the end of the day - if the guy can&#x27;t be satisfied with a polite request in the manfile, wow that sucks. But the GPL isn&#x27;t about you, it&#x27;s about the end user. There are explicitly licenses like BSD that require acknowledgement if that&#x27;s your thing! reply Eiim 19 hours agorootparentprevCiting tools is maybe not tradition, but it&#x27;s gaining popularity. R, for example, has 320,000 cites. (https:&#x2F;&#x2F;scholar.google.com&#x2F;scholar?cites=1605575084878280829...) reply bluenose69 18 hours agorootparentIt seems to me that citing R (or some other software tool) makes sense when it spares the author the task of providing detailed explanations.Administrators who gauge work quality by counting citations are not helping the world much. Maybe it&#x27;s time we started citing administrators who help us in our work ... so that their administrators can get rid of them if they are not helping. But of course I&#x27;m dreaming in technicolour -- administrators are never really subject to review, it seems. reply shakow 17 hours agorootparentprev> R, for example, has 320,000Which is, to be frank, ridiculous compared to the number of papers it enabled. reply 542458 17 hours agorootparentprev> IIRC the citation notice was cleared by Stallman as GPL compatibleDo you have a source for this? Im confused by this, as the GPL section 7 is pretty clear that additional restrictions are effectively void. I suppose it’s technically not contrary to the GPL to idly state those restrictions, but it is contrary to the GPL to expect them to do anything. If the author is deliberately including an impotent clause in the hope that people will follow it anyways, I feel that trying to confuse or scare people into doing something the GPL gives them explicit permission to do is contrary to the spirit of the GPL.Furthermore, trying to retaliate against people who (as permitted by the GPL) remove the citation notice, as the author here has done, seems very contrary to the spirit of the GPL. reply dahart 17 hours agorootparentHere’s one source, there are several other places Ole has talked about it: https:&#x2F;&#x2F;git.savannah.gnu.org&#x2F;cgit&#x2F;parallel.git&#x2F;tree&#x2F;doc&#x2F;cita...I think the confusing issue here is that the notice is not a license requirement, it does not add additional licensing restrictions. It’s an honor-system agreement between the user and Ole, and does not involve the GPL. It does seem to be walking a very fine line, and it’s easy for users to not understand the distinction, but I believe the notice does adhere to the GPL’s rules, even if it doesn’t initially appear to for us non-lawyers. reply 542458 16 hours agorootparentYes, to me it looks like he’s adding an official license-like note, but then declaring that he’s still GPL compliant because although his note is easily confused with a license it’s not actually a license. He then gets cranky if people remove his not-a-license note or don’t act like it’s a license. Feels very much to me like he’d be better served with something other than the GPL if he doesn’t want people using his software in GPL-permitted ways. reply flanked-evergl 20 hours agorootparentprev> IIRC the citation notice was cleared by Stallman as GPL.I really hope that whomever adjudicate these disputes regarding licence agreements doesn&#x27;t care what a random person says about it. reply DiggyJohnson 20 hours agorootparentIt&#x27;s disingenuous to say that Richard Stallman is a \"random person\" regarding the GPL. reply speedgoose 19 hours agorootparentI hope that Stallman’s future opinions have no impact of the GPL licensed software. He is the main author of the license but I wouldn’t bet that what he says years later have to be considered. reply grotorea 19 hours agorootparentprevBut legally, does intention matter in either copyright or contract law? And specially someone who is neither licensee or licensor? reply dahart 18 hours agorootparentI’m not sure I understand what you’re asking. Why is the intention of the author of the license in question?In this case, Stallman simply clarified that Parallel’s notice did not count as a legal requirement and does not conflict with the GPL. His opinion wasn’t necessary, but since he wrote the license, it is authoritative. In this case, the question wasn’t brought to court, it was simply a clarifying discussion, and thus his intention did affect how things go in practice.> And specially someone who is neither licensee or licensor?Also wasn’t Stallman effectively the licensor or representing the licensor at the time, as president of FSF, head of the GNU project, and author of the GPL? reply memefrog 18 hours agorootparent>His opinion wasn’t necessary, but since he wrote the license, it is authoritative.No it isn&#x27;t. Licences, like most legal documents, are construed objectively. The subjective intention of the author is totally irrelevant to the meaning. reply dahart 17 hours agorootparentYou might have misunderstood what I said. It’s not up for debate whether RMS’s opinions or intent on the GPL have affected industry practice; that’s a fact of history. His statements on the GPL are authoritative in the sense that they may have prevented the courts from examining this question. reply memefrog 14 hours agorootparent\"Authoritative\" has a particular meaning. You might have intended to say \"influential\". You didn&#x27;t. I can only reply based on what you said. reply dahart 11 hours agorootparentWhat meaning are you thinking of, exactly? I looked up the definition and it matches what I intended to say in every dictionary I checked (Merriam Webster, Oxford, Cambridge, Dictionary.com…) Some of the definitions seem more or less synonymous with “influential”, maybe you’re making some incorrect assumptions? replyhnfong 18 hours agorootparentprevRMS&#x27; comments and interpretations of the GPL have generally influenced how the industry deals with the GPL, and to the extent it has become de facto standard practice, the courts might take notice and take it into consideration.Generally though, the contents of the text and (if applicable) case law surrounding its interpretation is more important. reply ruined 18 hours agorootparentprevthe difference between law and code is the interpreter: law is interpreted by humans, significantly based on perceived intent, and code is interpreted by computers, which are expected to only act literally and deterministically. reply tshaddox 19 hours agorootparentprev“Arbitrary” is probably a better description than “random” here. reply DiggyJohnson 18 hours agorootparentI still disagree with arbitrary, just less strongly.Whether his opinion on GPL is relevant, or if it is, how important it is, is up for debate. But I still don’t think it’s “based on a random choice or personal whim, rather than any reason or system”. reply dahart 19 hours agorootparentprevWhat do you mean? Stallman adjudicates the disputes. reply dec0dedab0de 19 hours agorootparentI think disputes are handled between the author of the code and the user.Stallman is rightfully the most prominent voice to comment on the spirit of the GPL&#x2F;CopyLeft&#x2F;Free Software. reply dahart 19 hours agorootparentYou’re referring to a different kind of dispute than I (and parent) was talking about. In this case, Stallman did handle the dispute about whether GNU Parallel’s citation notice conflicted with the terms of the GPL. reply dec0dedab0de 19 hours agorootparentOkay that makes sense. You&#x27;re saying that since the GPL itself is not open, that it needs Stallman&#x27;s approval for modifications that are not explicitly allowed. And I was saying that it does not necessarily mean those modifications are enforceable between two parties in a random jurisdiction, which comes down to courts and whatnot. reply dahart 19 hours agorootparentYes kind-of… in this case Parallel’s notice is not a modification of the license at all, and Stallman is the person who ruled on this question and confirmed this to be true. The GPL doesn’t prevent authors from including a notice, and having a notice doesn’t conflict with the terms of the GPL.I feel like the whole problem here is that the legality of Parallel’s notice, and the separation of the notice from the GPL, is not at all clear. The language is confusing to users. People who take the license seriously are staying away from Parallel because of the fear of accidentally breaking the license terms. reply memefrog 18 hours agorootparent>in this case Parallel’s notice is not a modification of the license at allThis is a question of law that only a court can answer. reply dahart 18 hours agorootparentThat’s not true. The language of Parallel’s citation notice, while confusing to some users, does not impose any legal requirements and is not part of the license. Neither the notice nor the license claim otherwise. RMS, and more importantly, Ole Tange, agree that Parallel’s notice is not legally binding, and intended to write it that way, and there is a publicly visible history of this intention and agreement. reply memefrog 14 hours agorootparentRMS, not being a judge, is incapable of \"authoritatively\" or otherwise determining whether this notice is legally binding.If it is something that needs to be \"confirmed\" by someone \"authoritatively\" then you should ask a lawyer for advice. You should not ask a programmer for a \"ruling\".What RMS might be saying is \"we won&#x27;t seek to enforce it\". That is completely different. reply dahart 9 hours agorootparent> What RMS might be saying is “we won’t seek to enforce it”. That is completely different.If you review the thread from the top, you might find the primary question we were discussing from the start before you jumped in is whether the Parallel notice is GPL compliant. Whether Parallel’s notice is definitively and absolutely legally binding on its own and away from the GPL is a nuance you introduced, but it has been answered for all practical purposes by both Ole and RMS. It will probably never go to court or be tested by a judge, partially as a result of what Ole and RMS have said: that the notice is not a license and is not contractual.There is no dispute about this, and because there is no dispute and because it’s not going to court, the statements by Ole and RMS are the most definitive answer we’ve got, and to date is what people are using when making and acting on decisions about Parallel usage. Both of them have said the Parallel notice complies with the GPL because the notice is not legally binding, so Ole & RMS both were saying more than GNU won’t seek to enforce Parallel’s notice. “Academic tradition” is not legally binding law, and the notice doesn’t reference any other relevant law. The notice is full of legal holes, if you insist on interpreting it as a legal contract. It was written by Ole (not a lawyer) and doesn’t define what research usage would constitute a mandatory citation, nor what happens if the user doesn’t see the notice, or if a citation is inappropriate, or if the citation is rejected by reviewers, among many other possibilities. It doesn’t take a lawyer or judge to see that the Parallel notice is not legally enforceable, and it doesn’t take a legal education to see that it’s not Ole’s intent to enforce it as a contract. He is just asking for citations, in slightly confrontational language.It would be fair to say that a judge or court, if this issue was ever tested in court, might overrule some aspect of Ole’s or RMS’s stated intent because their language was imprecise and effectively said something different than they meant. Then again, another judge can override the first judge. There’s nothing definitive or absolute or permanent in law, regardless of whether a judges rules on it, and intent does matter in practice. Before this ever goes to court (probably never), all questions on this topic can be (and already are!) answered by non-judges, which is why it’s demonstrably not true to claim this question can only be answered in court or by a judge.> You should not ask a programmer for a “ruling”.RMS wasn’t acting as a programmer when he wrote the GPL, btw, nor when he opined on whether Parallel’s notice complies, so in that sense your framing is veering into the hyperbolic. reply paulmd 15 hours agorootparentprevI don&#x27;t think anyone would make a deal with oracle on a \"don&#x27;t worry this isn&#x27;t legally binding, you&#x27;re just stating your intent to comply\" basis. reply dahart 14 hours agorootparentIndeed, and people are choosing not to use Parallel for the same reason. The notice would be much better IMO from the user perspective if it was more clear. I guess that’s maybe the point, to leave people with the mistaken impression that this is a binding agreement. reply foldr 20 hours agorootparentprev> it’s not tradition to write citations for tools used to conduct researchAcademics seem to have a very blinkered attitude to this. I wrote some software that was popular for a while in a niche field, and people were forever asking me to waste my time by &#x27;publishing&#x27; the manual in some pointless journal so that they could cite something and give me credit. Writing useful software counts for less in that world than publishing another pointless paper that no-one will read. reply Symbiote 19 hours agorootparentA more useful option is to use Zenodo to provide a DOI for a GitHub repository.https:&#x2F;&#x2F;docs.github.com&#x2F;en&#x2F;repositories&#x2F;archiving-a-github-r... reply foldr 19 hours agorootparentThat doesn&#x27;t really help. People already know how to paste the URL for a piece of software into a paper. It&#x27;s more that it doesn&#x27;t count for anything (because it&#x27;s a piece of software and not a paper). reply Symbiote 13 hours agorootparentIt does help, as the DOI system provides the tracking needed to count citations and measure the \"effectiveness\" of the researcher. reply foldr 13 hours agorootparentSuch citations are not valued by $FIELD (for most values of FIELD). Only citations of published papers count. reply bee_rider 18 hours agorootparentprevThat is a weird request, a source doesn’t need to be published in a journal to be cite-able. On the other hand, if you put a bibtex snippet on your site that indicates how you’d like to be cited, that is super helpful. reply dahart 17 hours agorootparentOle did provide a BibTeX entry to a USENIX magazine article about Parallel, which is fine, though I was always taught that non-journal references generally belong in footnotes or an appendix and not the bibliography, especially for something you’re not referencing for research purposes. Not sure if footnote or appendix or open-source usage citations count for what Ole needs; I’d guess he wants citations you can easily index using Google Scholar or other citation indexes, i.e. it should count toward Parallel’s H-index (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;H-index) reply foldr 17 hours agorootparentprevI wasn&#x27;t very clear in my comment. I think the idea was that if the thing they were citing was a journal article, then the citations would actually mean something, as widely-cited journal articles are one of the currencies of most academic fields. While one certainly can cite an unpublished document, the author doesn&#x27;t necessarily gain much from it in terms of their academic CV.If someone writes a piece of software specifically for the purposes of doing certain types of scientific research, and then other scientists use this software to help conduct published experiments, then IMO it really ought to be possible to give that person meaningful credit for their work. It&#x27;s a perfectly legitimate way to contribute to a field, even if it does not take the form of a paper. But with the system as it stands, the only way to get meaningful credit is to publish a pointless paper saying, in effect, \"Hey! I wrote some software!\">if you put a bibtex snippet on your site that indicates how you’d like to be cited, that is super helpful.I should probably have done that, but from my point of view it didn&#x27;t really matter. I have a name, and the software had a website. I didn&#x27;t really mind exactly how individual people chose to cite it. The absence of a ready-baked bibtex snippet would never be accepted as an excuse for failing to cite any other kind of source. reply bee_rider 16 hours agorootparentI agree on all points, just trying to look at it from the other party’s point of view. But yeah, the situation with this sort of stuff is a pain.There should be a high-prestige “journal of READMEs and User Handbooks,” haha. reply5e92cb50239222b 21 hours agoparentprevI wanted to say &#x27;not anymore&#x27;, but it turns out that some distributions remove that message.https:&#x2F;&#x2F;gitlab.archlinux.org&#x2F;archlinux&#x2F;packaging&#x2F;packages&#x2F;pa...Debian too (thanks to iib for pointing this out)https:&#x2F;&#x2F;salsa.debian.org&#x2F;med-team&#x2F;parallel&#x2F;-&#x2F;tree&#x2F;master&#x2F;deb...And looks like the author is aware of both:https:&#x2F;&#x2F;gitlab.archlinux.org&#x2F;archlinux&#x2F;packaging&#x2F;packages&#x2F;pa... reply ilyt 20 hours agorootparentI kinda want to submit patch to GNU Parallel that changes \"Hall of Shame\" to \"Hall of Heroes\".But yeah, if guy wants to have the name of the app mentioned there is a BSD license for that I thin... reply iib 21 hours agorootparentprevI think Debian also does this, I didn&#x27;t see it when using the latest version reply raverbashing 19 hours agorootparentprevnext [–]- # *YOU* will be harming free software by removing the notice. You - # accept to be added to a public hall of shame by removing the - # line. That includes you, George and Andreas.The Open source way of buzzing a contestant reply dsissitka 21 hours agoparentprevIt&#x27;s in current Fedora&#x27;s: [david@pc ~]$ echo fooparallel echo Academic tradition requires you to cite works you base your article on. If you use programs that use GNU Parallel to process data for an article in a scientific publication, please cite: Tange, O. (2023, July 22). GNU Parallel 20230722 (&#x27;Приго́жин&#x27;). Zenodo. https:&#x2F;&#x2F;doi.org&#x2F;10.5281&#x2F;zenodo.8175685 This helps funding further development; AND IT WON&#x27;T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. More about funding GNU Parallel and the citation notice: https:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;parallel&#x2F;parallel_design.html#citation-notice To silence this citation notice: run &#x27;parallel --citation&#x27; once. foo [david@pc ~]$ reply 5e92cb50239222b 20 hours agorootparentnext [–]> Tange, O. (2023, July 22). GNU Parallel 20230722 (&#x27;Приго́жин&#x27;).Looks like the latest release is named after Prigozhin. Yeah, probably that one, although I couldn&#x27;t find anything in the mailing list to confirm it.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Yevgeny_Prigozhinedit: all releases are named after current political events:https:&#x2F;&#x2F;git.savannah.gnu.org&#x2F;cgit&#x2F;parallel.git&#x2F;refs&#x2F; reply jnsaff2 20 hours agorootparentLooking at the release history it may be that the releases are named after some front page daily news headline or something.Even so, knowingly naming a release after a war criminal is very off-putting. reply folmar 19 hours agorootparentIt shall not be. It is a notability indication, not endorsement, just take a look at other release names from the time of the war. Or compare to Time&#x27;s \"Person of the year\". reply jnsaff2 18 hours agorootparentI get the system. It&#x27;s just a bad system.The author is having musk-ish type of fun and that&#x27;s their freedom. My freedom is to feel disgust by seeing mass murderers, even if they are treated equal to not controversial topics. reply cnr 20 hours agorootparentprevDoes &#x27;Приго́жин&#x27; relate somehow to infamous &#x27;Евгений Пригожин&#x27;. Or am I just biased? reply pneumic 19 hours agorootparentPrigogine&#x2F;Prigozhin is a not-uncommon Russian-Jewish name: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Prigozhin reply folmar 10 hours agorootparentprevYes it does. See sibling comment by 5e92cb50239222b:> all releases are named after current political events reply computerfriend 19 hours agorootparentprevYeah, I think it does. reply Matl 20 hours agoparentprev> And calling that GPL?You can add any message you want into your GPL program. Also, a GPL program does not have to be free.This has nothing to do with the GPL. You can say in your program that &#x27;by using this software you agree that you&#x27;re a a cat&#x27; and license it under the GPL.That does not mean the GPL relates to cats in any way. reply 542458 20 hours agorootparentYou can add all the extra restrictions you want, but they effectively won’t do anything. Expecting both the GPL and the additional restrictions to apply is a violation of section 7 of the GPL.> All other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term.Debian explains this further in their patch file. reply akerl_ 20 hours agorootparentprevCan you back that up?If I put the GPL in my software and add a file next to it that says \"Also you can&#x27;t use this software if you make more than $100k&#x2F;year\", I&#x27;ve pretty clearly added an additional clause that&#x27;s incompatible with the GPL. reply aidenn0 18 hours agorootparentIf I say \"Please don&#x27;t use this software if you make more than $100k&#x2F;year\" I haven&#x27;t added an additional clause, just communicated a desire. I&#x27;m as annoyed by parallel&#x27;s citation nag (particularly since I don&#x27;t plan on ever publishing a scientific paper), but it does not impose extra requirements. reply akerl_ 18 hours agorootparentI think I agree with you: if you put in requests that have no mandate or attachment to \"or this changes your rights under the license\", there&#x27;s no issue.But I was responding to the comment upthread \"You can add any message you want into your GPL program\". If you add a message that says a user of the software must do something &#x2F; must agree to additional terms &#x2F; etc, that additional text is not compatible with the GPL. I&#x27;m not a lawyer, so I have no idea whether the result would be that the restriction doesn&#x27;t count and the software is GPL&#x27;d, or that the software isn&#x27;t viably GPL&#x27;d because the GPL+clause isn&#x27;t a valid license for somebody to use. reply oneshtein 19 hours agorootparentprevAuthor of software package can revoke his license. Author can provide same package under multiple licenses, for example: GPL and proprietary license, like QT. Author may say that user A may use GPL license only, while user B may use proprietary license only. reply akerl_ 19 hours agorootparentEh, this is kind of right but also not really responsive to the thread.For one thing, if the author provides the source with a GPL license to user A, and user A sends it to user B, user B has the software under a GPL&#x27;d license. The normal reason for dual licensing GPL&#x2F;proprietary is so that user B can pay money to bundle the software in a non-GPL-compliant way. The author can stop licensing future releases under GPL, but they can&#x27;t revoke the GPL on already-distributed software.For another, this isn&#x27;t what&#x27;s happening here. GNU Parallel is released under the GPL, and the author is affixing what is debatably an additional term to the GPL&#x27;d release, under the claim that it doesn&#x27;t count as an additional restriction because it&#x27;s \"academic tradition\". By the same token, I can add a clause to my software saying that rich people can&#x27;t use it, because it&#x27;s \"hippie tradition\" to stick it to The Man. reply Matl 15 hours agorootparentprevIt basically means that clause doesn&#x27;t do anything. If it is phrased as a request, rather than a requirement then it doesn&#x27;t violate anything for sure.The author of Notepad++ for example is famous for adding all kinds of statements associated with the software and in no way is that part of the license.On the other hand, if your license.txt states to i.e. not use the software for evil aka JSON famously did then yes, it is part of the license. reply Bjartr 20 hours agorootparentprevThe message isn&#x27;t part of the license, and it&#x27;s phrased in a way that wouldn&#x27;t be binding if it were.It says \"please cite\" and \"feel free to not cite if you pay\".It doesn&#x27;t say \"must cite\" or \"you may only not cite if you pay\".IANAL, but it doesn&#x27;t seem like it would interact with the GPL at all. So the worst that could be said is that the implementation is annoying or in poor taste. reply paulmd 18 hours agorootparentSoftware cannot be distributed with a clickwrap agreement under the GPL. Requiring the user to affirmatively agree to a contract is a clickwrap agreement even if the terms are non-monetary. The old “you are making a second agreement, not the one the software is distributed under” approach.Notionally the GPL allows you to disregard this but it may or may not be binding depending on your jurisdiction, and it’s certainly distasteful and against the absolute spirit and most likely the text of the GPL. This is an incompatible term being forced on the end user and the entire license might well be void. reply aidenn0 18 hours agorootparentIt&#x27;s not a clickwrap agreement, since the user doesn&#x27;t agree to anything: To silence this citation notice: run &#x27;parallel --citation&#x27; once. reply paulmd 15 hours agorootparentdid you ignore all the stuff before that about the implication of using that option and what the user agrees to by using it?this is like saying that a user doesn&#x27;t actually agree to anything just because they clicked \"accept\" in a EULA. you&#x27;re just clicking buttons in software, it doesn&#x27;t obligate you to anything!!! but actually yes that is most likely fairly binding in a lot of jurisdictions.that is, again, literally the definition of a clickwrap licensing agreement and you cannot do that in GPL software, even if it&#x27;s non-monetary. Requiring the user to submit a selfie in a funny hat would not be permissible under the GPL either. You can&#x27;t limit what the user does with the software and how, or else it&#x27;s not GPL.it&#x27;s open and shut, clickwrap agreements completely subvert and nullifies the moral stand the FSF is trying to make. And it doesn&#x27;t matter how innocuous it seems, it undermines the whole point of the exercise.fortunately the GPL includes a \"severability\" clause that basically allows you to ignore this and grants you a license regardless. but it is not a good look, it is not good behavior, and if every GPL&#x27;d package started adding random clickwrap agreements with big \"IM A DOODOO HEAD IF I IGNORE THIS\" parameters the whole ecosystem would degrade.Arch and others are not only allowed but actually morally and practically in the right for stripping these messages, and it doesn&#x27;t reflect well on Ole at all that he then goes on and throws more tantrums because he doesn&#x27;t like the consequence of the license he chose.If he wants to go proprietary, or BSD (which requires acknowledgement!), that&#x27;s fine, but he&#x27;s being a child and the terms he are adding are utterly uncompliant with GPL, and it&#x27;s unprofessional for FSF to even humor him on this. If there were a hundred Oles the FSF would have a real problem on its hands, it&#x27;s only because he&#x27;s N=1 jerk that this is remotely tolerable. reply aidenn0 15 hours agorootparentIt doesn&#x27;t look like it obligates me to do anything. It contains a request to cite, not a requirement: If you use programs that use GNU Parallel to process data for an article in a scientific publication, please cite: Tange, O. (2023, July 22). GNU Parallel 20230722 (&#x27;Приго́жин&#x27;). Zenodo. https:&#x2F;&#x2F;doi.org&#x2F;10.5281&#x2F;zenodo.8175685 This helps funding further development; AND IT WON&#x27;T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. More about funding GNU Parallel and the citation notice: https:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;parallel&#x2F;parallel_design.html#citation-notice To silence this citation notice: run &#x27;parallel --citation&#x27; once.[edit]This is really similar to the kerfuffle where the maintainer of Home Assistant asked distros to not repackage HA; it&#x27;s a request, a bit at odds with community norms for libre software, but one that people are legally free to ignore. replypaulmd 18 hours agorootparentprevYou cannot include a message that requires the formation of a binding contract. This is the old “you can fire someone for no reason but not any reason”, and if the message your product shows is a prompt forcing the user to agree to a binding contract, its not GPL compatible.I agree that in this case it’s likely not enforceable&#x2F;binding especially since the GPL specifically allows you to ignore those terms. Hopefully that’s legally binding in your jurisdiction vs the other party.But it’s a straightforward clickwrap agreement, even if the terms are non-monetary the GPL simply doesn’t allow these at all. Can’t place any stipulations on how the user uses the software. reply Matl 20 hours agoparentprevIf you&#x27;re a startup and relicensing previously open source code under a restrictive license or doing other shady things you&#x27;ll get plenty of defenders to line up to say &#x27;hey, they have to make a living somehow&#x27;, but if a single guy tries to make a living via a simple message in a widely used program all hell breaks loose.What gives? reply electroly 19 hours agorootparentNotably, GNU Parallel did not relicense; it&#x27;s still GPL. The author wants to have his cake (gain the popularity benefits of being a GPL-licensed GNU tool, be able to carpetbomb Stack Overflow with \"use GNU Parallel\" answers, etc.) and eat it too (get people to cite or pay him as a condition of using the product). Since this isn&#x27;t possible (GPL doesn&#x27;t allow additional restrictions), but the author still really wants it, he went the route of making the extra condition non-legally-binding but then getting publicly upset at people for using the product under its actual license. That&#x27;s the part that GNU Parallel is doing that people don&#x27;t like, and that other projects are not doing.The startups you mention actually changed their license. That&#x27;s what GNU Parallel would have to do to make this extra condition ok, but he won&#x27;t do it because being a GPL-licensed GNU tool is critical to its popularity in the first place. reply dehrmann 17 hours agorootparentSo if it&#x27;s GPL, I can remove the citation bits, release it under the GPL as free-parallel, and everything&#x27;s OK? reply electroly 15 hours agorootparentYes. The GPL explicitly says this about \"further restrictions\":> If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term.https:&#x2F;&#x2F;www.gnu.org&#x2F;licenses&#x2F;gpl-3.0.en.htmlHowever, this doesn&#x27;t really even come into play because the citation request is not a restriction on the license. It&#x27;s not anything. As far as the GPL is concerned, it&#x27;s just some code, and the GPL grants you the right to redistribute modified copies.And by renaming it to \"free-parallel\" you have respected the author&#x27;s trademark. You can absolutely do this, at the cost of the author being upset at you. They might get upset that \"free-parallel\" is too close to their \"GNU Parallel\" trademark but I (IANAL) don&#x27;t think they&#x27;d be legally right about that. GNU Parallel coexists with other software called \"parallel\". reply depaulagu 17 hours agorootparentprevYep: https:&#x2F;&#x2F;git.savannah.gnu.org&#x2F;cgit&#x2F;parallel.git&#x2F;tree&#x2F;doc&#x2F;cita... reply Atotalnoob 18 hours agorootparentprevShouldn&#x27;t gnu put a stop to this? reply electroly 18 hours agorootparentThey ensured that the citation request was not actually an additional requirement and has no legal meaning. Beyond that, GNU&#x27;s interests are better served by retaining GNU Parallel as a GPL-licensed GNU product than by losing it to another organization or another license. I wouldn&#x27;t expect movement from GNU beyond their existing acknowledgement that the citation request is not a legal requirement and does not modify the GPL. In any event, GNU tends to be hands-off on contributed packages (i.e. the ones that Stallman wasn&#x27;t involved in writing). reply dahart 19 hours agorootparentprevI think there’s a reasonable question in there, but I don’t agree with this framing. Shady relicensing isn’t legal, and it doesn’t matter if there are armchair defenders. But, Ole does have defenders, so it’s not one-sided.Part of the issue is that Ole’s citation notice doesn’t appear at first glance to some people to be compatible with the GPL. You have to read the language carefully, and read the history of GNU Parallel’s citation notice, to understand that the notice is not a licensing term.Another part of the issue is that the notice doesn’t sound like someone just trying to make a living. It sounds like a demand or even a veiled threat, and one that is inflicted on everyone, not just academics. It’s not exactly clear about what the legal requirements even are.I’m in favor of Ole getting citations, and I’m in favor of his right to ask. But the way it’s being asked for rubs me the wrong way a little bit, and it’s rubbed other people the wrong way a little bit ever since it was introduced. BTW, the whole reason it seems like all hell breaks loose, and the only reason this matters is precisely because the software is widely used. If it wasn’t widely used and it didn’t sit under the GNU umbrella, you’d never hear about this. reply lolinder 19 hours agorootparentprevI had no opinion until I read through the patch that Arch uses to remove the notice [0]. The creator comes across as whiny, entitled, and aggressive. They have comments in the source like \"You accept to be put in a public hall-of-shame by removing the lines\", \"YOU will be harming free software by removing the notice\", and \"That includes you, George and Andreas\". The whole thing is pretty unprofessional, and based on the false premise that every tool used during research traditionally gets a citation.[0] https:&#x2F;&#x2F;gitlab.archlinux.org&#x2F;archlinux&#x2F;packaging&#x2F;packages&#x2F;pa... reply akerl_ 20 hours agorootparentprevYou&#x27;ll note there are also plenty of \"defenders\" on this page. It turns out the community is made up of people with a wide range of opinions. reply Matl 20 hours agorootparentOf course, but HN does tend to have a lot more patience for scrappy startups that scrappy lone, non-commercial devs from what I can observe.The question was rhetorical, I know that this place is frequented by quite a lot of people who wish to be part of the next YC batch, so they see themselves in the shoes of the startup, rather than the solo dev.Still, I don&#x27;t think it should be this way. reply akerl_ 20 hours agorootparentI&#x27;m saying I don&#x27;t think it is that way. I think you&#x27;re seeing through the lens of your expectations. dang has written about this at length, but the gist is that it&#x27;s very easy and tempting to see posts you disagree with more visibly and feel them more viscerally than posts you agree with. reply ilyt 20 hours agorootparentprev>Of course, but HN does tend to have a lot more patience for scrappy startupsLook what parent company of HN does reply smcleod 20 hours agorootparentprevI could be wrong here and this is the first I’ve heard of this but I suspect it’s the language &#x2F; way he goes about communicating. On the surface at least it comes across as a little annoying &#x2F; demanding, things like him having a website where he shames people that don’t cite him by name, I suspect the ‘legal’ claims being made aren’t that solid either. Don’t get me wrong it’s a neat tool - but it’s just one in a huge ecosystem of many people’s efforts. reply 542458 20 hours agorootparent> I suspect the ‘legal’ claims being made aren’t that solid eitherSection 7 of the GPL specifically says that additional restrictive terms on GPL software (like “pay me $1000 or cite me”) can be ignored or removed. If the software’s author doesn’t want people to remove his additional terms he shouldn’t have used the GPL. Publicly shaming other open source contributions for doing something that the GPL explicitly and deliberately permits (removing additional restrictive terms) is extremely improper in my opinion. reply philshem 20 hours agoparentprevHere it’s discussed on the gnu mailing list in 2013:https:&#x2F;&#x2F;lists.gnu.org&#x2F;archive&#x2F;html&#x2F;parallel&#x2F;2013-11&#x2F;msg00006... reply daveguy 15 hours agoparentprevRush is an alternative to gnu parallel that is MIT licensed:https:&#x2F;&#x2F;GitHub.com&#x2F;shenwei356&#x2F;rushAs you mention xargs has parallel capabilities and gargs is Apache licensed software that fixes some of xargs shortcomings:https:&#x2F;&#x2F;GitHub.com&#x2F;brentp&#x2F;gargsNo reason to use gnu parallel. reply dec0dedab0de 19 hours agoparentprevSpamming stack overflow is bullshit, but I&#x27;m fine with citations and selling exceptions. reply tempaccount420 20 hours agoparentprevWhy not just cite it? reply capableweb 21 hours agoparentprev> Is the author still adding the \"cite me or pay 10000€\" notice to the output? And calling that GPL?Where you get the \"or pay 10000€\" part from? As far as I remember, the software, unless told otherwise, asks authors of scientific papers to cite GNU parallels if they used it when writing their papers. And it doesn&#x27;t force it, it&#x27;s not part of the license, but asks you to do so as it&#x27;s academic tradition to use citations.You could just ignore the citation and not break the license, no one would think less of you for doing so. reply badsectoracula 21 hours agorootparent> Where you get the \"or pay 10000€\" part from?Most likely from the manpage: If you use --will-cite in scripts to be run by others you are making it harder for others to see the citation notice. The development of GNU parallel is indirectly financed through citations, so if your users do not know they should cite then you are making it harder to finance development. However, if you pay 10000 EUR, you have done your part to finance future development and should feel free to use --will-cite in scripts. If you do not want to help financing future development by letting other users see the citation notice or by paying, then please consider using another tool instead of GNU parallel. You can find some of the alternatives in man parallel_alternatives.FWIW some distros remove the nagging message (e.g. mine - openSUSE - has it removed and the patch seems to come from Debian so i&#x27;d guess Debian and its derivatives also remove it). reply capableweb 20 hours agorootparentAgain, it&#x27;s not part of the license nor are you forced to select between \"cite GNU parallels or pay 10000 EUR\". You&#x27;re free to use it however you want since the software is GPL, including ignoring any of the output from using the tool if you so chose to. reply paulmd 15 hours agorootparent> including ignoring any of the output from using the tool if you so chose to.the user isn&#x27;t merely ignoring the output though, they are actively interacting with the program in a way that the program is presenting as accepting of the agreement being presented to the user.the agreement is plainly presented in a way that implies that it&#x27;s an obligation, like any other clickwrap agreement. and everyone except ole and stallman seems to agree that it&#x27;s self-evidently apparent that it&#x27;s a clickwrap agreement restricting the freedoms of the user.\"free software that only prints a message and exits unless you agree to a clickwrap with further licensing terms\" is not a road that FSF should go down. And it&#x27;s only because of the GPL severability clause that it&#x27;s not a crisis, everyone knows it&#x27;s a farce, except for a bunch of the users, who are affirmatively taking action to indicate consent with an additional licensing agreement.it&#x27;s not facially clear that in most jurisdictions that the clickwrap agreement is null and void merely because the software is free. you can end up paying for lots of free stuff in life if you&#x27;re not careful. you agreed to the agreement, it&#x27;s on you.you are of course free to remove the prompt and use the software yourself, and ole rants and raves about that on his website. but, agreeing to the license is a separate thing from the GPL license, most likely. just like paying for credit monitoring is different from getting your free credit reports or freezes - they&#x27;ll try and railroad you into paying, definitely! and just because it&#x27;s supposed to be free, doesn&#x27;t mean you&#x27;re not getting charged if you agree to it! reply badsectoracula 20 hours agorootparentprevI answered where that comes from, not if it is part of the license (it isn&#x27;t). reply akerl_ 21 hours agorootparentprevhttps:&#x2F;&#x2F;gitlab.archlinux.org&#x2F;archlinux&#x2F;packaging&#x2F;packages&#x2F;pa...\"If you pay 10000 EUR you should feel free to use GNU Parallel without citing.\" reply bombcar 21 hours agorootparentI read that more as a “common courtesy asks that you do X, but I’m negotiable - send a check and I won’t care about courtesy anymore.”Which is a fair enough position to take, in my opinion. reply akerl_ 21 hours agorootparentThat&#x27;s a very liberal interpretation of the reasonably plain language in the notice. reply jacoblambda 20 hours agorootparentIf you read the FAQ they have regarding the citation notice for GNU Parallel, it&#x27;s made clear that it is not part of the license in any way and only applies to projects that are part of&#x2F;the basis for academic papers. If it does apply to your project and you don&#x27;t cite, at absolute worst you could get in trouble with your university or the academic community but even then it&#x27;s almost certainly going to be mild at worst.But importantly you can use the software however you want that is compatible with GPLv3. That includes ignoring or removing the citation notice without paying a cent. However just because it&#x27;s legal doesn&#x27;t mean it won&#x27;t come with the potential for social consequences. == Is the citation notice compatible with GPLv3? == Yes. The wording has been cleared by Richard M. Stallman to be compatible with GPLv3. This is because the citation notice is not part of the license, but part of academic tradition. Therefore the notice is not adding a term that would require citation as mentioned on: https:&#x2F;&#x2F;www.gnu.org&#x2F;licenses&#x2F;gpl-faq.en.html#RequireCitation The link only addresses the license and copyright law. It does not address academic tradition, and the citation notice only refers to academic tradition. [...]https:&#x2F;&#x2F;git.savannah.gnu.org&#x2F;cgit&#x2F;parallel.git&#x2F;tree&#x2F;doc&#x2F;cita...and from the GPL faq itself (which said citation FAQ references): Does the GPL allow me to add terms that would require citation or acknowledgment in research papers which use the GPL-covered software or its output? (#RequireCitation) No, this is not permitted under the terms of the GPL. While we recognize that proper citation is an important part of academic publications, citation cannot be added as an additional requirement to the GPL. Requiring citation in research papers which made use of GPLed software goes beyond what would be an acceptable additional requirement under section 7(b) of GPLv3, and therefore would be considered an additional restriction under Section 7 of the GPL. And copyright law does not allow you to place such a requirement on the output of software, regardless of whether it is licensed under the terms of the GPL or some other license.https:&#x2F;&#x2F;www.gnu.org&#x2F;licenses&#x2F;gpl-faq.en.html#RequireCitation---TLDR: The citation notice is a \"cite it in academic works or pay me\" agreement that is as legally binding as a pinky promise. You can break it without concern but some people may look negatively on that and it may come with social consequences. reply rcxdude 21 hours agorootparentprevThe author apparently will think less of you, as made abundantly clear by the tone of the message and especially the comments around it in the code reply Matl 21 hours agorootparentNot really. It&#x27;s more like if you use it for free please cite but if you&#x27;re so averse to citing that you&#x27;d rather send a gazillion money then feel welcome to do so, at least that is the way I read it. reply ssddanbrown 21 hours agoprevLove finding a good use-case of parallel as an easy way to gain massive time savings, especially on the modern high-threaded CPUs of today. Most recently found it useful when batch-compressing large jpeg images to smaller webp files, via use with find and ImageMagick: find .&#x2F; -type f -iname &#x27;*.jpg&#x27; -size +1M -print0parallel -0 mogrify -format webp -quality 80 {} reply wiredfool 21 hours agoparentXargs is a nearly drop in replacement and probably already installed by default in most distros. You may need the -n 1 (one file per) and -P to parallelize. xargs -n 1 -P 8 reply indymike 7 hours agorootparentActually, parallel is a drop in for xargs as xargs has been around longer. Parallel has a few big improvements:* Grouped output (prevents one process from writing output in the middle of another&#x27;s output) * In-order output (task a output first, task b output second even though they ran in parallel) * Better handling of special characters * Remote executionMore here: https:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;parallel&#x2F;parallel_alternatives.... reply c-hendricks 13 hours agorootparentprevfind + xargs has become my go-to \"process files in parallel\". Tho now I&#x27;m wondering if I should be using `-n` instead of `-L` #!&#x2F;usr&#x2F;bin&#x2F;env bash set -e main() { if [ \"$1\" = \"handle-file\" ]; then shift handle-file \"$@\" else find . \\ -type f \\ -not -path &#x27;*&#x2F;optimized&#x2F;*&#x27; \\ -print0 \\xargs \\ -0 \\ -L 1 \\ -P 8 \\ -I {} \\ bash -c \"cd \\\"$PWD\\\" && \\\"$0\\\" handle-file \\\"{}\\\"\" fi } handle-file() { echo \"handle-file $1 ...\" } main \"$@\" reply toastal 20 hours agoparentprevYou should batch compress to JPEG XL too with cjxl --lossless_jpeg=1 --quality=80 --effort=9 {} {&#x2F;.}.jxl (or magick) reply asicsp 21 hours agoparentprevAny particular reason to use -print0 and pipe instead of -exec? reply Gabrys1 21 hours agorootparent-exec would not be parallel, pipe to parallel makes it parallel replytitzer 19 hours agoprevI didn&#x27;t know about this, and reading through the comments, I found out that xargs can also do batching and parallelism (nice!). However, it appears that if you pipe the output of an xargs-parallel command into another utility, it jumbles the output of the multiple subprocesses, whereas GNU parallel does not.I was a little put off by the annoying&#x2F;scary citation issue mentioned by another commenter, so I am not sure I will use parallel.I want to pipe the output of parallel processes into a utility that I wrote for progress printing (https:&#x2F;&#x2F;github.com&#x2F;titzer&#x2F;progress), but I think that neither of these solutions work; my progress utility will have to do this on its own. reply cb321 18 hours agoparentYou can probably do something that creates as many FIFOs as you have parallelism and just be careful about emitting whole records like https:&#x2F;&#x2F;github.com&#x2F;c-blake&#x2F;bu&#x2F;blob&#x2F;main&#x2F;doc&#x2F;funnel.md . That one&#x27;s Nim, but the meat is only like 50 lines and easily ported to C like your progress tool. ( EDIT: and it will also probably be drastically lower overhead than `parallel` which has over 70X worse time overhead and 10X the RAM overhead of tools written in fast, native-compiled languages: https:&#x2F;&#x2F;github.com&#x2F;c-blake&#x2F;bu&#x2F;blob&#x2F;main&#x2F;tests&#x2F;strench.sh ) reply titzer 17 hours agorootparentThanks for the suggestion! reply cb321 17 hours agorootparentAlso, the last time I tried, to do similar with FIFOs (no &#x2F;tmpwhatever storage like other e.g.s here https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37211687), GNU parallel needed some - for me - specially compiled Perl interpreter with threads enabled to use its `parcat` program which is also probably slow. Besides the nagware insanity, `parallel` seems just not a very compelling tool in either machine|human overheads unless -- maybe -- you already know Perl (which I always found a supremely forgettable language). reply bloopernova 22 hours agoprevThere&#x27;s a shell script version of GNU parallel that&#x27;s great for CI&#x2F;CD pipeline tasks. You just keep it in your repo and source it as needed. It&#x27;s incredibly useful, we use it in one build to batch process a few thousand things in groups of 25.Edited to add: finally got signed in to work, you create the script via: parallel --embed > scriptname.shIt&#x27;s about 14,000 lines of awesome and works on \"ash, bash, dash, ksh, sh, and zsh\" reply notpachet 21 hours agoparentMaybe this is a silly question, but what advantage do you get from checking that huge file into VC instead of just installing parallel ahead of time on the CI images? reply bloopernova 19 hours agorootparentNot a silly question!In this case, we don&#x27;t have control over the docker images used to build our apps. reply ilyt 20 hours agorootparentprevParallel was born way before docker and modern CI practices. Having one script that did it all was more of a benefit before those become commonplace reply Decabytes 18 hours agoprevI’ve been writing a lot of PowerShell recently and discovered the ForEach-Object cmdlets with the -parallel parameter and it has been addicting to parallelize my scripts, so I totally understand why parallelizing using a command line tool is attractive reply rhysrhaven 21 hours agoprevI much prefer rush over parallel. Namely that everything is executed as a bash shell.https:&#x2F;&#x2F;github.com&#x2F;shenwei356&#x2F;rush reply asicsp 22 hours agoprevDidn&#x27;t know about the book: https:&#x2F;&#x2F;zenodo.org&#x2F;record&#x2F;1146014 (discussed 4 years back: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=20726631)See also https:&#x2F;&#x2F;hn.algolia.com&#x2F;?q=gnu+parallel for other related discussions. reply SPBS 22 hours agoprevxargs is more useful because it&#x27;s posix so you can always guarantee it to be there (whereas with GNU Parallel you probably have to reach for a package manager to install it first). The ergonomics are worse though, as usual. reply ketanmaheshwari 22 hours agoparentThe entirety of GNU Parallel is just one Perl program. It could be copied over and used in a pinch. The installation itself is very simple and no special dependencies or privileges are needed. reply em500 21 hours agorootparentExcept Perl isn&#x27;t always present by default either (e.g. in Arch Linux or FreeBSD). reply adrian_b 19 hours agorootparentThere are also many Linux distributions that do not install by default all the POSIX utilities, but only the minimal set that is needed to bootstrap the system.On all such systems, it is very easy for the user to install any missing POSIX utility, but it is also easy to install any non-POSIX GNU utility.So not even xargs is certain to exist by default on all systems.Moreover, POSIX xargs is restricted to execute sequentially all processes.Any use of xargs for parallel execution is non-POSIX, so in that case there is no reason to not use \"parallel\" instead. reply bloopernova 21 hours agorootparentprevnext [–]parallel --embed > parallel.shThen store that in your source repo and use it wherever shells are used! reply em500 20 hours agorootparentOn Debian 11.7: $ parallel --embed > parallel.sh Unknown option: embed[edit] Ran it in Ubuntu 22.04, it does output a bash script ... which still depends on Perl. reply commonlisp94 19 hours agorootparentisn&#x27;t perl always installed? reply philkrylov 18 hours agorootparentIt is not, at least on FreeBSD and NetBSD. reply dpkirchner 20 hours agorootparentprevWould this taint the other code in your repo with the GPL? I&#x27;d guess it would depend on how it is distributed. reply klyrs 13 hours agorootparentIf you&#x27;re running on your private build infra, it&#x27;s fine. If you&#x27;re pushing that repo to somewhere public, it&#x27;s now GPL. reply OJFord 19 hours agorootparentprevTo install parallel, first run parallel --embed? reply bloopernova 21 hours agoparentprevSee my comment above, there&#x27;s a shell version you can store in your project repository and use wherever you want with zero installation!https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37208250 reply Joel_Mckay 21 hours agoparentprevIndeed, xargs can be a better option, but it has trouble doing some tasks efficiently.For example, translating a large list of IPv4 ranges into a standard format for a firewall rule-set parser:cat ~&#x2F;blacklist.p2pparallel --ungroup --eta --jobs 20 \"ipcalc {}sed &#x27;2!d&#x27; \"grep -Ev &#x27;^(0.|255.|127.)&#x27; >> ~&#x2F;blacklist_p2p_convertedMakes an annoyingly slow task tolerable, as parallel doesn&#x27;t block while fetching to preserve order. We probably should rewrite this to be more efficient, but this task is run infrequently.Happy computing =) reply adrian_b 21 hours agoparentprevGNU Parallel has been created precisely for solving some deficiencies of xargs.While there are cases when it makes sense to stick to what is specified by POSIX, there are also cases when the POSIX specification is so obsolete that using POSIX instead of some free ubiquitous programs is a big mistake.Among these latter cases are writing scripts for a POSIX shell instead of writing them for bash and using xargs instead of parallel. reply CJefferson 21 hours agoparentprevLast time I checked (which was a few years ago, admittedly), some popular ystem&#x27;s xargs were too old to support parallelism -- Mac in particular. reply krackers 4 hours agorootparentThis is not the case I think, xargs on mac supports parallel, and does so back to 10.9 or older reply AvImd 20 hours agoprevIf none of the examples from the article work, make sure you are running GNU Parallel and not an identically named utility from moreutils. reply TZubiri 19 hours agoprevFirst paragraph: I want to test my tests.Second paragraph: I want to test my test-tester.OP 100% fell down a rabbit-hole. reply latchkey 19 hours agoparentExactly! I was kind of shaking my head over this one...\"they execute extensive scenarios against a live service over HTTP\"Any time I&#x27;ve seen people think they&#x27;ve needed to test live services, over HTTP... it means that there are far deeper issues. reply ranting-moth 21 hours agoprevLearning Parallel pays high dividends for the rest of your life. reply bloopernova 21 hours agoparentSimilarly with the command line in general. Yet you&#x27;d think it was torture to some developers I know! reply pimpl 22 hours agoprevHaving a layer of parallelisation on top of good old sequential code seems like a very neat idea. It resolves headaches of learning how to run code in parallel in languages that aren’t necessarily my primary language (e.g. short, one-off scripts). Thanks for sharing!! reply figomore 11 hours agoprevI use GNU Parallel to render Blender videos distributed by a bunch of nodes https:&#x2F;&#x2F;github.com&#x2F;tfmoraes&#x2F;blender_gnu_parallel_render reply ogou 21 hours agoprevSomeone gifted an old blade server to me a few years ago. Very slow, but 16 cores and 24 gig of RAM. At the time I was making a lot of video art with ffmpeg, without a GPU. That version of ffmpeg wasn&#x27;t optimized for multiple cores so rendering was really slow and sequential. I discovered Parallel and set the server to process large videos with most of the cores in parallel. Voila, it chewed through a massive amount of media fairly quickly. Faster than the hard drives actually. reply bcjordan 21 hours agoprevFolks who are here and interested in parallelization for CI&#x2F;CD may also be interested in Dagger.io — I had heard about it on HN over the years but not played w it. It&#x27;s basically a more fine-grained Docker-like executor with better caching and utilities for spinning up services and running tests.Curious if anyone else has experiences with it, honestly been surprised at how little I&#x27;ve heard about it reply jamietanna 20 hours agoprevOne thing I&#x27;ve used parallel before is to add the ability to add straightforward retry mechanisms, and it was great! https:&#x2F;&#x2F;www.jvt.me&#x2F;posts&#x2F;2022&#x2F;04&#x2F;28&#x2F;shell-queue&#x2F; reply rubicks 19 hours agoprevI can appreciate that GNU parallel exists. I always use `xargs -P0` in my own work, though. reply jooz 18 hours agoprevI try to use it last week to run 10 instances of curl against a webserver.I was expecting something simple as &#x27;parallel -j10 curl https:&#x2F;&#x2F;whatever&#x27; but couldnt find the right syntax in less time that took me to prepare a dirty shell script that did the same. reply brabel 18 hours agoparentIf you want a simple load testing tool for HTTP, use wrk2[1]. wrk -t2 -c100 -d30s -R2000 http:&#x2F;&#x2F;127.0.0.1:8080&#x2F;index.html> This runs a benchmark for 30 seconds, using 2 threads, keeping 100 HTTP connections open, and a constant throughput of 2000 requests per second (total, across all connections combined).Some distros include `ab`[2] which is also good, but wrk2 improves on it (and on wrk version 1) in multiple ways, so that&#x27;s what I use myself.[1] https:&#x2F;&#x2F;github.com&#x2F;giltene&#x2F;wrk2[2] https:&#x2F;&#x2F;httpd.apache.org&#x2F;docs&#x2F;2.2&#x2F;programs&#x2F;ab.html reply b5n 13 hours agoparentprevQuick solution: parallel -j 10 curl 2> &#x2F;dev&#x2F;null \\ ::: $(for i in {1..10};do echo &#x27;https:&#x2F;&#x2F;whatever.com&#x27;;done) reply nateb2022 21 hours agoprevThere&#x27;s also PaSh: https:&#x2F;&#x2F;github.com&#x2F;binpash&#x2F;pash reply heinrichhartman 19 hours agoprevAs the answer to the question was not actually given in the post: &#x2F;usr&#x2F;bin&#x2F;parallel reply grepfru_it 20 hours agoprevThe same can be implemented with just bash using jobs and wait. Useful if parallel is not available in your pipeline reply aquir 21 hours agoprev\"Do one thing and do it well\" reply jp57 18 hours agoprevSeems like you could accomplish the same thing more cleanly (IMO) with make. You can create a target for each test, which can be done with patterns, and then use `make -j` to run them in parallel. reply herrkanin 20 hours agoprevI have wanted to parallelize my .zshrc file for a while – all those environment setup scripts for nvm, pyenv, starship, etc really makes the startup time noticably slow. Does anyone know how to do this? reply dahart 19 hours agoparentOoh nice thought. I’m not certain, but I kinda doubt it’s possible, because those startup scripts need to modify the current shell environment. I believe GNU parallel runs in a subshell and launching new tasks in separate processes, so fundamentally doesn’t operate the same way that e.g. sourcing the nvm script does, unfortunately. Even if there was some way to hack it, I’d be nervous about changing environment variables in parallel, to me that sounds like asking for really nasty race condition bugs. reply b0afc375b5 22 hours agoprevWhat about & and wait? Could it have been an adequate alternative? reply capableweb 22 hours agoparentProbably for very simple use cases, but the real power in parallel really comes from the myriad of switches that enables so much more than what \"&\" and \"wait\" could do.Here are a bunch of examples: https:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;parallel&#x2F;parallel_examples.htmlA fun one I end up using ~monthly or so for various things (usually with more switches added as needed): GNU Parallel as queue system&#x2F;batch manager # start queue true >jobqueue; tail -n+0 -f jobqueueparallel # add job echo my_command my_arg >> jobqueue # to start queue for remote execution true >jobqueue; tail -n+0 -f jobqueueparallel -S .. reply klyrs 13 hours agoparentprevWhen I&#x27;m using parallel, it&#x27;s usually because I have thousands of jobs. Worse, they have nontrivial memory requirements. When you background processes with &, the system starts timeslicing. Each process gets to allocate its memory before being paused to make time for the next process. Your system will almost immediately crumple under load. Hopefully, the oom killer will target your backgrounded jobs... but the script spawning them will go untouched because it isn&#x27;t the thing hogging memory.Before I learned of parallel, I tried a hack where I&#x27;d manually assemble jobs into batches, and wait on the batches before starting the next. It achieved very low system utilization, because inevitably, one job each the batch takes much longer than the rest. A slight improvement (still not good), is to use `split` to chop your jobs file into $num_cores chunks, and background each chunk. But still, this gets low utilization. Problem being that you aren&#x27;t using a thread&#x2F;worker pool.Parallel (or, TIL, xargs) can maintain 100% system utilization, until the very last $num_cores jobs. reply eisbaw 22 hours agoparentprevNo, that is more messy and can easily leave lingering processes.But it can be done in pure BASH: https:&#x2F;&#x2F;gist.github.com&#x2F;mped-oticon&#x2F;b11dafa937e694ce4fa6fbf2...GNU parallel supports expansion, which bash_parallel doesn&#x27;t. However bash_parallel works with bash functions, which GNU parallel doesn&#x27;t. reply untilted 22 hours agorootparentGNU parallel supports bash functions, provided you \"export -f\" them beforehand reply agumonkey 22 hours agoparentprevYou just taught me something reply toastal 20 hours agoprevI use this with Nix all the time. Great utility. reply tomberek 18 hours agoparentEspecially with the remote SSH features one needs a way to ensure everything needed for your process is on the target machine; Nix makes this easy.Nix + GDAL + GNUParallel + autoscaling groups === massive geospatial data processing pipeline reply timtom39 20 hours agoprevLove the tool. One of my favorite snippets adds parallel processing to jq#!&#x2F;bin&#x2F;bashcat -parallel --line-buffer --pipe --roundrobin jq \"$@\" reply sneak 21 hours agoprevSee also: ppss (parallel processing shell script) https:&#x2F;&#x2F;github.com&#x2F;louwrentius&#x2F;PPSS# reply morbidious 20 hours agoprevLooks like a great tool!Thanks for the link to the book: https:&#x2F;&#x2F;zenodo.org&#x2F;record&#x2F;1146014 reply pmarreck 20 hours agoprevHIPS (Hiding In Plain Sight)! reply lfconsult 21 hours agoprevWonderful... Thanks for sharing. reply nullc 21 hours agoprevparallel is great but its default behaviors never quite seem to match my needs, so every time I use it I have to spend some time consulting the man page. Fortunately, the man page is more than up to the task.But because of the mini learning curve on each use and because I find I need a little more boiler plate to use parallel, I use xargs -P more often, only using parallel when I need its special features (e.g. multiple hosts or collating the output streams).Oh also, parallel itself can be a bit of a resource hog. (Obviously that depends a lot on how you&#x27;re using it-- but I mean in cases where xargs&#x27; usage is unnoticeable I sometimes have to change the size of my jobs to get parallel out of the way). reply amelius 21 hours agoprevAnother reminder that you shouldn&#x27;t use Bash to write scripts.E.g. in Python this would all be very easy to do. Just start a bunch of threads and e.g. invoke subprocess.run() from them. reply bloopernova 21 hours agoparentYou don&#x27;t have to reinvent the wheel for your script, all the parallel options are ready for you to use and are well documented. It&#x27;s also packed with features that might take a long time to write into your Python script.I am trying to use Python by default when writing scripts nowadays, but sometimes the best tool for the job isn&#x27;t Python or writing your own Python. reply imajoredinecon 20 hours agorootparentIMO, effective \"scripting\" just means the ability to solve ad hoc problems easily by writing task-specific glue that delegates the hard parts of the program to (1) an effective set of libraries you&#x27;ve written yourself and (2) external code or tools when it makes sense.From this perspective, the languages of the glue, the libraries, and the external code all matter less than the ease of writing the glue; interfacing with the external code; and maintaining the libraries. The best language for this probably comes down to a combination of what you&#x27;re comfortable writing (and reading, and maintaining) and what kinds of tasks you&#x27;re trying to solve.For me personally, using Python glue and libraries strikes a pretty good balance here. Writing a script \"in Python\" doesn&#x27;t mean you need to reinvent the wheel. If you think `parallel` provides a better interface for map-reduce parallelism than `subprocess` (or than a library f",
    "originSummary": [
      "The blog post discusses the author's experience using GNU Parallel for gathering statistics on end-to-end test pass/fail rates.",
      "Parallel execution using GNU Parallel improved performance and offered additional functionality.",
      "The post emphasizes features of GNU Parallel, such as concurrency control, output capture, and advanced capabilities described in the author's book."
    ],
    "commentSummary": [
      "The discussion explores the advantages and usefulness of GNU Parallel compared to shell commands and alternative tools like YSH, Nushell, and Fish in parallel computing.",
      "Alternative languages like Perl, Python, and Haskell are mentioned as well as tools like zargs in zsh and DASK in Python for parallel processing.",
      "The debate includes a comparison between CPU cores and GPU WGP/SM, the inclusion of citation notice in the GPL license, and the role of Richard Stallman in interpreting the GPL. The importance of citations in software research and controversy surrounding GNU Parallel's author requesting citations or payment is addressed. The pros and cons of using xargs and parallel for parallel execution and the benefits of using a thread/worker pool are discussed. GNU Parallel is celebrated for its versatility and effectiveness in parallel computing."
    ],
    "points": 432,
    "commentCount": 249,
    "retryCount": 0,
    "time": 1692617475
  },
  {
    "id": 37210953,
    "title": "Don't fire your illustrator",
    "originLink": "https://sambleckley.com/writing/dont-fire-your-illustrator.html",
    "originBody": "Sam Bleckley Software Services Writing Art Contact Don't Fire Your Illustrator Understanding (and Art Directing) AI image generators August 20, 2023 Oh no, the robots are doing the fun human work instead of the degrading mechanical work My academic training is in Fine Art, painting, and printmaking. My professional career for the past 20 years has been in software engi- neering, including machine learning. This makes me uniquely situated to panic about discuss image-generative AI systems like Midjourney, DALL-E, etc. This essay comes to you in two parts (both of which are right here on this page). Part I is a mostly-un-opinionated technical description of how one popular branch of AI image generation currently works. If you’re already familiar enough with stable diffusion to understand the terms “latent space” and “text transformer,” you can skip ahead. Part II is a very opinionated prediction of how this technology will be successfully used and by whom. PART I: Stable diffusion I a) The Latent Space If you want to talk about colors, there are more and less useful ways to name them for different tasks. Take the color “pinkish purplish autumn mist” and make it a little warmer; what color is that? Mix a little ultra- marine, a little alizarin crimson, a tiny dot of cadmium yellow, and a good blob of titanium white. Make that a little warmer; what color is that? Take the RGB color 234, 182, 227, and make it a little warmer; what color is that? Or the HSV color 308° 22° 92°. Or the Lch color 80/ 30/330. When we use lists of numbers to name points in a physical space, we don’t do so randomly. We pick a numbering to ensure that when the numbers are close, the locations they name are close, and when the numbers are very different, the locations they name are far apart. We can also use a list of numbers to describe a non-physical space, like the space of colors: And we want the same rules to apply: similar colors should have similar numbers, and similar numbers represent similar colors. There are many ways to give colors numbers — RGB, Lch, CMYK — and each numbering results in slightly different relationships between those colors. The goal is always “put similar things near to one another,” but we might define “similar things” in a variety of subtly different ways. The numbers don’t contain the appearance of colors nor the literal pigments they’re made from — it’s a labeling system, not a filing system. The numbers for a color are both a label and a set of instructions: mix this much red light, this much green light, and this much blue light, and you’ll get the color that these numbers label. Words are more complicated than colors. Still, one could imagine assigning every word a bunch of numbers — perhaps hundreds, instead of just three — in such a way that “mom” and “dad” have similar numbers, and “mom” and “prestidigitation” are further apart. Researchers have used AI to take lots and lots of text and (by assuming that words that appear near each other in text are related in some way) build a space of words that’s like that. Whole images are even more complicated than single words or colors, but (by using thousands of numbers) we can imagine “spaces” where similar images are represented by similar lists of numbers. We could do that by listing every RGB color of every pixel in an image — that will make some similar images close to each other! There are downsides, though. That method takes millions of numbers. Worse, some similar pictures won’t be near to one another at all: for instance, the RGB pixels in a picture of a black cat and the RGB pixels in a picture of a white cat will be very, very different, even if they’re otherwise very similar pictures of cats. So how do we build a useful space for images? It’s one thing to assume words that appear near one another are similar because the same word gets used in millions of different situations, with heaps of nearby words. Most images only get used once, and maybe not near any other images at all! We can borrow our latent space of words, though: Lots of pictures have captions, labels, or words nearby. Researchers have used AI to build image spaces where pictures with similar captions are near one another. Pictures labeled “cat” are near to each other and pictures labeled “car” are near to each other, and pictures labeled “Miyizaki cat-bus” are somewhere between. In generative AI research, these image-spaces space is called image “latent spaces.” A latent space can give you a list of numbers that approximates any image you might ever want to see, and every random uninterpretable image, too. It’s Borges’ Library of Babel but for pictures. How do we navigate that space full of sense and nonsense to find the numbers for images we want to see? I b) Stable diffusion (Understand that what I’m about to describe, called stable diffusion, isn’t the only way to accomplish this, but it’s a popular one) Imagine seeing a picture of a cat on a staticky television: If you squint, you can make out the cat. If you wanted to, you could paint out the static and reveal the cat more clearly. Imagine seeing a cat behind a lot of static. You could squint, sketch in some lines, squint at those lines, and probably get a picture of a cat, though not exactly the same picture of a cat. And maybe you can imagine seeing pure static and convincing yourself there’s a cat in there somewhere, and with a lot of squinting, slowly teasing out a staticky picture of a cat, then a less staticky one, and then a clear picture of a cat. This is stable diffusion: We took millions of images, made them a little noisy, put them in latent space, and trained the computer to clean them up. And then, we took the noisy images, made them noisier, and trained the computer to make them less noisy. And we made those noisier, and then even noisier, until the images were obliterated, and the computer could, step by step, hallucinate its way back to some image. Not necessarily a good one, or a useful one, but a non-noisy image. We can also take our word space and train the computer to try and asso- ciate an image in image-latent-space to some words in word-latent- space and say how likely an image is to have a particular caption. A picture of a cat is likely to have the caption “my cute kitty mister french fry” and unlikely to have the caption “the engine from a 1959 Austin Healey.” The last piece of the puzzle is called “cross attention,” which is a fancy way of saying, “glue several AI systems together, so they can do two things at once.” In particular, we can ask the computer to remove some static from an image AND nudge that image to be more likely to be capti- oned with some specific text. And that’s it — that’s generative AI. Note some important things: while every image in the training set is representable by some numbers in latent space, the images themselves are not there in any specific way. To steer the process, the words you type get turned into points in a word latent space, which then gets retranslated into a movement in image latent space. That’s hard! And making small adjustments to the result using just words can be very hard! The kinds of images that are very common — statistically likely — get organized in bigger and more well-organized parts of latent space. There are other systems for steering the denoising process — using text labels is just one of them — but all of them involve cross-attention between the denoising and some other goal. A test of understanding: why does this produce extra fingers and deformed hands? The wrong way of thinking: every input image the model has trained on has correct hands, so it should learn to draw correct hands! The latent space describes every possible image. The nudge away from noise prioritizes clarity. The nudge towards a text label is satisfied by any image that would best be labeled by that text. Image captions rarely mention people’s hands, especially not with correct and incorrect numbers of fingers. There’s not much pressure toward perfect hands, and adding negative labels like “no deformed hands” relies on a very small number of images out there labeled “deformed hands.” (You might have better luck with a negative label for “polydactyly” since that label correlates very strongly to extra fingers) Most generative systems have separate components specifically to correct faces because the same issue applies; the core system is content if there’s something face- like, while our eyes are very picky about faces being correct, with the eyes pointed in the same direction most of the time. A test of understanding: why does adding an artist’s name, like James Gurney, produce better images overall? Most image captions are nouns: a person, an object — the picture’s focal point. The background can be nonsense; if the foreground is a teapot, it’s fair to caption it as “a teapot.” Non-teapot parts of the image aren’t under much pressure. An artist’s style is a gestalt; it doesn’t exist in just one part of the image, but the whole thing; every pixel is under correlated pressure, so a coherent outcome is a little more likely. PART II: Who should use AI image generation, and how There are some opinions here that will be unpopular with my artist friends. Some will be unpopular with technologists or neophilic managers. I certainly don’t want you to think I am pleased about any of these opinions, or that I want them to come to pass; these are simply my predictions based on a goodish understanding of both generative AI and traditional image-making. An opinion popular with creatives: Physical media artists will keep their jobs. The human desire for real paintings, sculptures, woodcuts, embroide- ries, and so on isn’t going to vanish. I don’t have much to say about that; I mostly mention it so we can set that segment of artists aside and concentrate on the larger swath of commercial image-makers. An opinion popular with tech and unpopular with creatives: The output of physical media will continue to be used in training genera- tive AIs. I absolutely understand the desire to prevent this. Knowing your work has been used without permission to train a computer to replace people’s livelihoods is extremely violating. But understanding the technical basis, I don’t see any plausible way to outlaw it while still allowing fair use in all the ways human artists have been for thousands of years. Images similar to those used to build the latent space may be recoverable with the right prompt and some luck, but they’re not inher- ently there, any more than my memory of an Andy Warhol is inherently a copyright violation. I can sell Andy Warhol pastiches I make based on that memory. I can augment my memory by having a morgue file of images to train my memory on. If you have a vision for how this can be structured legally, restricting ML uses of imagery without restricting human uses, I’d love to hear about it! An opinion popular with tech-loving managers and unpopular with creatives: Generative AI will replace a slice of illustration and writing: in particular, the kind where the content doesn’t actually matter: This blog post needs a header image that’s vaguely related, not because it needs illustration but to fit the page layout. Or This spammy site needs a new blog post once a week for SEO reasons. No one from one end of the process to the final consumer particularly cares about the image or the text as long as it doesn’t stand out; it is furniture. This kind of work never paid particularly well and is now rapidly vanishing. My heart goes out to the people suffering because this work is vanishing, but I also can’t see any way out, even with much stronger legal regulation of generative AI than I expect we’ll ever see. Why only that slice? If you’ve ever used a generative system, I can pretty much guarantee that you spent an embarrassing amount of time making tiny adjustments to your prompt and retrying. Producing a compelling image with genera- tive AI is pretty easy; maybe one in ten images it generates will make you say, “Wow, cool!” But producing a specific image with generative AI is sometimes almost impossible. If you visit (often NSFW, beware!) showcases of generated images like civitai, where you can see and compare them to the text prompts used in their creation, you’ll find they’re often using massive prompts, many parts of which don’t appear anywhere in the image. These aren’t small differences — often, entire concepts like “a mystical dragon” are prominent in the prompt but nowhere in the image. These users are playing a gacha game, a picture-making slot machine. They’re writing a prompt with lots of interesting ideas and then pulling the arm of the slot machine until they win… something. A compelling image, but not really the image they were asking for. Why is it so hard to get what you want? Let’s return to the technical discussion for a second. Text is a difficult way to steer an image because while the text latent space is related to the image latent space, there are still multiple trans- lation steps: from the actual prompt to the text latent space to a function in the image latent space. The process can only accommodate so much text at a time (usually ~75 words; if there are more than that, you must break the prompt into separate guiding systems in cross-attention). OK. Are there better ways to direct image generation to have specific results? Yes! It’s much easier to translate an image into a latent space constraint. Images translate very well into image latent space; that’s what image latent space is for. Here are some ways folks have invented to prompt image generation using images rather than words Create an image that would reduce to the same line art as another image Create an image that would reduce to the same depth map as was pulled from another image Create an image with matching poses pulled for another image Create an image whose style matches another image even though the content differs match perspective lines of another image match the colors palette of another image These are all very powerful constraints that can exert precise control over the content and composition of a generated image. The only challenge in using them is: where do all these guiding images come from? Who can take the time to understand the concepts we want illustrations of and turn them into a line drawing, a sketch of poses, or a style? Is there some existing job title for that? An opinion popular with creatives and unpopular with techy managers: Generative AI isn’t much use for sophisticated needs if there isn’t an illustrator involved. I believe that will continue to hold true even for future versions of Midjourney, DALL-E, and so on; I think the amount of text they can handle will increase, and the quality and resolution of images they produce will increase, but the fundamental challenge of getting specific imagery is not going to vanish without more fundamental changes in the approach. Finally, an opinion popular with no one: Commercial illustrators will keep their jobs, but will mostly need to learn to use AI as a part of their workflow to maintain a higher pace of work. This doesn’t mean illustrators will stop drawing and become prompt engineers. That will waste an immense amount of training and gain very little. Instead, I foresee illustrators concentrating even more on capt- uring the core features of an image, letting generative AI fill in details, and then correcting those details as necessary. Here’s a process for digital painting that I’ve tested and found… plau- sible: Produce a line drawing traditionally, focusing on the composition and key ideas Have the generative AI suggest a dozen potential approaches to color and lighting; pick one or two Paint almost entirely over those AI generated pixels, adjusting and correcting the color to suit my vision Obviously, this is not a workable approach for artists that put great care and emotion into their color choices. I don’t think there will be any one approach that works for all artists. But for artists working on deadlines, I foresee them using AI to fill in whatever step is the least important and most tedious: crowd scenes, cityscapes, vegetation. Just like a blog post header image is furniture for the page, there is furniture for many images — not important, but still necessary. For better or worse, that furniture is becoming the territory of generative AI. The more concerning problem is that while generative AI research is heading in this direction, offering more and more ways to direct image generation using image inputs, the products that are entering the market are not easy to slot into an illustrator’s workflow at all. All my experi- ments have been done running open-data models on my own computer in order to have useful levels of control. I have more to say on the subject of machine creativity and also the gacha-like nature of generative AI, but I think it best to leave this post here, with that vision of commercial illustration yet to come, and the hope that generative AI products will start catering to it. My books are open! I am currently taking contracts for the second half of 2023. Drop me a note. sam@sambleckley.com",
    "commentLink": "https://news.ycombinator.com/item?id=37210953",
    "commentBody": "Don&#x27;t fire your illustratorHacker NewspastloginDon&#x27;t fire your illustrator (sambleckley.com) 360 points by todsacerdoti 18 hours ago| hidepastfavorite291 comments easyThrowaway 16 hours agoAnother opinion popular with no one: AI will have on artists the same impact that Spotify had on the music industry that is, it will kill any revenue flow for anyone outside of the publishers and big artists&#x2F;players.Spotify basically killed any money coming from the physical distribution - Worse than piracy, which was inevitable too at the time, but at least you didn&#x27;t have to pay your lawyers to renegotiate with your label on top of NOT getting any money.Adobe, OpenAI, whatever: they want artists to draw for them for peanuts to train their model, sign a waiver saying \"I&#x27;m ok not getting any money from any AI art made from this\", and then resell the output for $$$ on something like Splice[1], at the same time overtraining such models in ways that make extremely obvious whose artist made them in first place.At the end of the day the model itself is going to be basically irrelevant, while knowing whose works were actually used to train it being the truly differentiating feature.But you know, \"the AI did this picture, so we don&#x27;t have to pay you.\"[1] https:&#x2F;&#x2F;splice.com&#x2F;features&#x2F;sounds reply vouaobrasil 15 hours agoparentI agree completely, and I have been constantly speaking about how AI will be a wealth concentrator, replacing a mass of jobs more diverse than previously seen. Unlike previous machines which can take 1-2 jobs, when humans get REALLY efficient at training AI, it will replacing hundreds en masse.AI will also have an additional effect: it will be isolating in the sense that the need for other humans will decrease.These two points alone, strengthened by many others, have led me to conclude that the world is MUCH better off with AI and that tech companies are ruining the world with their abominations. reply tivert 14 hours agorootparent> These two points alone, strengthened by many others, have led me to conclude that the world is MUCH better off with AI and that tech companies are ruining the world with their abominations.Do you mean \"world is MUCH better off without AI.\"What you wrote doesn&#x27;t make much sense withing the context of your comment, but I have to ask because there are some software engineers that find abominations appealing for some reason, or just lack the ability to tell the difference between desirable technology and a technological abomination. I think a big component of the latter is many software engineers&#x27; overconfidence in their abilities that makes them easy marks, and the willingness of many kinds of hype men to exploit that to con them with propaganda. reply firebirdn99 13 hours agorootparentThere has to be UBI for A(G)I. Period. reply nelsonic 9 hours agorootparentThe math doesn’t work for UBI at scale. Unless there’s a 99% tax on the 1%. And how likely is that? reply pcthrowaway 3 hours agorootparentWhat about socialized housing, food, and health care then?Socialize the essentials, let people work for the non-essentials.If there isn&#x27;t enough work to go around for people who want more than a substistence living, start reducing the definition of \"full-time\" until there is. If only 50% of working aged people can find work, redefine full-time as 24 hours&#x2F;week reply zhoujianfu 8 hours agorootparentprevIt definitely can work… it depends on the size of ubi and how creative we get. We could for example just say that banks no longer get to do 10-1 fractional reserve banking and instead all the free money gets distributed to their customers accounts. And do a cap and trade carbon system with auctions where all the revenue goes to ubi. And all the revenue from spectrum auctions. And repurposing some existing spending. And printing a little more money. And congestion pricing. Etc, etc… reply logifail 1 hour agorootparent> banks no longer get to do 10-1 fractional reserve bankingFractional reserve banking is pretty much an urban myth. Banks create money when they make commercial loans. The Bank of England explains it quite nicely here:https:&#x2F;&#x2F;www.bankofengland.co.uk&#x2F;-&#x2F;media&#x2F;boe&#x2F;files&#x2F;quarterly-... reply dleeftink 9 hours agorootparentprevWhat doesn&#x27;t work about the math? There are many alternate taxation schemes and strategies that can be tried, such as increasing VAT on certain products, adjusting tax brackets to income changes, higher capital gains taxes on investments, and making tax evasion more difficult. reply zarzavat 4 hours agorootparentDon’t you think if it were possible for governments to gain multiples more tax revenue, they would do it already?Taxes are competitive. If you have extremely high taxes, then the businesses that can move out will move out, leaving a smaller tax base and requiring you to raise taxes even more in a death spiral. reply dleeftink 2 hours agorootparentPossibly, but depending on the level of budgetary entrenchment it would prove difficult for some governments. In any case, a staged roll-out is more likely to succeed, with many hurdles along the way.It&#x27;s worth repeating that doing nothing increases overall societal costs if large parts of the population become unemployed, whether due to lower statewide income taxes, increased welfare costs, people resorting to grey collar work, and other secondary effects from increased displacement (lower consumer spending, rentals sitting empty, health epidemics, increased crime, etc.). UBI does not have to be a blanket instrument either: rather than income, we can focus on making certain goods and services universally available, such as access to food surpluses that would otherwise be overturned or basic internet access to enable people to remain connected without expensive contracts.A solution may even exist beyond taxation: making reschooling and job pivoting more accepted within industry, lowering admission costs to tertiary education, or guaranteeing placement of employees when let go on account of automation or cost-cutting. What way the pendulum will swing remains to be seen. reply account42 49 minutes agorootparentprevThose corporations still want access to the US &#x2F; EU &#x2F; etc. customers so if they move elsewhere to dodge taxes you need to increase tariffs to compensate or deny them access to the market entirely. Those businesses are not irreplacable and they are worth nothing without customers. reply tivert 5 hours agorootparentprev> There has to be UBI for A(G)I. Period.There won&#x27;t be UBI, period. Though I could see a future where obsolete people are warehoused in sex-segregated poor houses until they die out, if it&#x27;s determined that their freedom is threat to stability. reply totetsu 7 hours agorootparentprevIt just balances out that fact that ai is trained on the output of people in the first place. reply HenryBemis 13 hours agorootparentprevI am not a software engineer. When (for my work) I&#x2F;we need a decent chunk of development done, we get the pros.BUT, sometimes I want something that will automate the fudge out of my PC (imagine command prompt on overdrive). I usually DDG for the solution and end up in some 10yo solution in StackExchange, which doesn&#x27;t do the thing.My friends have all forgotten their DOS skills.. so I turn to ChatGPT and boom! I get me 2 paragraphs script in 30secs.Do I hire devs? Hell yeah and we pay well, and we will continue to do so for many years. Do I use ChatGPT for the small (personal) stuff? Hell yeah too.Now, if a company wants to outsource everything to an LLM&#x2F;AI then I wish them the best of luck, coz when something will break (and oh IT WILL), Tthe contractor they screwed over should charge them x50!!!! reply ceroxylon 12 hours agorootparentDefinitely agree, LLMs are only as useful as the person interpreting and implementing the output; if someone doesn&#x27;t have enough knowledge or context about the thing they are trying to solve&#x2F;create then copy & pasting blindly while asking the wrong questions will lead projects to disaster.I have witnessed this firsthand when I dove into the deep end on something over my head, GPT-4 Code Interpreter went into an error loop and I had to learn all of the background knowledge I was foolishly trying to avoid. reply vouaobrasil 7 hours agorootparentprevYes, of course, I meant without AI. reply hnhg 15 hours agorootparentprevAnother side effect: the wealth will be concentrated in rich tax-avoiding corporations and elites, meaning that the tax burden for society will fall even harder on the remaining middle and working classes, who will have to pay for the upkeep of everything. reply dahwolf 10 hours agorootparentAnd yet another side effect, the one that I believe trumps them all: a loss of meaning.If somebody with zero skill in the arts can produce output of similar quality as a craftsman and about a thousand times faster, what is the point of art anymore? Sure, one can enjoy the very act of creating art, but we can&#x27;t deny that art has value in relation to an audience, and is also a display of skill and a source of pride.What if AI generates the perfect music just for you, based on your taste? Here we lose any and all social&#x2F;cultural aspects of music. There&#x27;s no point in discussing music as we have no shared experience. There&#x27;s no point in emphasizing your favorite song because everybody exclusively listens to favorite songs.What if you need to write a long essay and use AI to help write it. I receive it and use AI to summarize it. Other than this interaction being supremely depressing, what is the point of it at all? Just submit it to the big machine and perhaps some of it will show up in my use of ChatGPT-17.So you were faster to write something whilst I was faster to consume it. This allows the both of us to do more in a single day. This \"big win\" won&#x27;t gives us back free time nor raise our wages though. I just means that the nature of the work is for us to take the job of being guard rails for AI, a soul crushing job in itself but also temporary, until the rails are no longer needed. reply davkan 7 hours agorootparentAI can probably replace Katy Perry, but could AI generated music ever replace Rancid or Junior Kimbrough or Fela Kuti? I don’t think so personally. I think truly human music will continue to stand apart.I do agree with you in large part. I think I’m just slightly more optimistic that people who are driven to create will continue to do so and that people who really want real and human experiences and interactions will be able to find them with effort. Probably not anywhere on the mainstream internet though. Maybe even only in person. reply ux-app 7 hours agorootparent>could AI generated music ever replace Rancid or Junior Kimbrough or Fela Kuti?AI is ok, but it will never ....until it does, and then the goalposts will move.the only logical endgame that I can see is AI replacing all human endeavour (creative, technical, physical, mental).There will eventually be philosophers trying to make sense of the profound understandings coming from machines. reply davkan 5 hours agorootparentEver is a strong word. I shouldn’t rule out a future like that. But I don’t see the through line from our current ai to one that has entirely supplanted all human creation.That consideration seems more along the lines of worrying about the eventual need to escape earth than a future on a closer horizon worth worrying about.I’m more concerned about how every facet of our children’s lives will become inundated with shoddy ai being used to extract maximum profits at the cost of any humanness, and the death of all genuine communication on the internet. reply zarzavat 3 hours agorootparentAI has, in a few short years, gone from hardly being able to string words together, to writing coherent grammatical sentences, to being more proficient than an untrained human in many cases. ChatGPT is way better at writing poems than me, for example. Its style transfer capabilities are out of this world.Thinking that the progression is going to slow down is just wishful thinking. reply Dalewyn 8 hours agorootparentprev>If somebody with zero skill in the arts can produce output of similar quality as a craftsman and about a thousand times faster, what is the point of art anymore?I like to think \"AI\" will make art better reflect its real value, devoid of the tangential flat costs associated with housing, clothing, and feeding humans in the process of producing art.The consumers at large demand driving down the cost for consuming and enjoying art, and raise hellfire if there is so much as a suggestion of raising that cost. Remember how much controversy there was and still is about raising the standard price of video games from $60 USD to $70 USD? And that $60 USD today is pennies compared to $60 back in, say, 1995.If the consumers at large demand the cost of art to go down and \"AI\" will make the process of producing that art better reflect that real value, isn&#x27;t this overall a good thing insofar as making the price tag more clear and agreeable and closing down sweatshops? reply blibble 11 hours agorootparentprev> Unlike previous machines which can take 1-2 jobs, when humans get REALLY efficient at training AI, it will replacing hundreds en masse.more like hundreds of millions> AI will also have an additional effect: it will be isolating in the sense that the need for other humans will decrease.unless there&#x27;s a complete restructuring of our society then a repeat of the late 18th century seems to be the likely outcomewith their stake in society gone: the peasant class get fed up of eating dirt and storm the bastille(I really, really hope the AI revolution turns out to be just hype) reply zamadatix 8 hours agorootparentI think they mean jobs as in lines of work not jobs as in instances of employment.One thing I wonder about a repeat of history is if the lowest classes still get enough of a share of the increased output in income&#x2F;QoL would there still be a revolt about the increasing wealth concentration? reply HellDunkel 14 hours agorootparentprevShould it say „without AI“? Makes no sense like this.. reply badpun 15 hours agorootparentprev> Unlike previous machines which can take 1-2 jobsThere are many machines replacing hundreds or even thousands of people- farm equipment, trains, tunnel boring machines etc. reply lancesells 13 hours agorootparentNot a real equivalent. Those machines are made by many, many people along the way. Industries exist from those machines.With software you could say chip makers, developers, and energy companies will get stronger but I don&#x27;t think there&#x27;s a comparison. The keyholders will be a much smaller group with a greater power if we stay onboard the AI train. reply orangecat 6 hours agorootparentNot a real equivalent. Those machines are made by many, many people along the way. Industries exist from those machines.Sure, and even taking that into account we produce far more food with far less human labor than we did 100 years ago, and that&#x27;s a good thing. reply vouaobrasil 15 hours agorootparentprevI meant KINDS of jobs. reply wwweston 16 hours agoparentprev> Spotify basically killed any money coming from the physical distribution - Worse than piracy, which was inevitable too at the time, but at least you didn&#x27;t have to pay your lawyers to renegotiate with your label on top of NOT getting any money.It’s even worse than you say — it was murder on digital retail too, right at the time when it was on track to compete with or exceed old physical sales.Spotify adopted the economics of piracy and stamped them with the false veneer of legitimacy. reply amadeuspagel 16 hours agorootparentFundamentally, neither spotify nor piracy matter. People enjoy making music. Today, there are more people able to make and publish music then ever, but the day still only has 24 hours, you can&#x27;t listen to more music then before. Unlimited supply, limited demand. reply easyThrowaway 14 hours agorootparentWe&#x27;re talking about the business side of the whole ordeal. It&#x27;s not just about \"enjoying making music\". It&#x27;s about paying mixing and mastering. It&#x27;s about paying NTS, Rinse FM, and the constellation of medium-small distribution channels. It&#x27;s about distributing on labels like DFA, !K7 or whatever. It&#x27;s about making sure that Fabric, Rex Club and Sneaky Pete can keep the lights on so they can play your music, so you can get paid, so you can keep making music instead of ahem having to become a webdev and write angry comments on HN.It&#x27;s about keeping an entire industry, live or recorded, and their milieu alive.The truth is that what happened wasn&#x27;t a liberation. It was a methodical purge of the medium-sized side of the music industry. Now we&#x27;re reaching the point of having 5-6 industry giants taking all the money plus...yes, an inordinate amount of people making mostly self-referential music in their own bedroom on weekends, music that will reach no-one outside whatever local scene they hang around. But most of them were making music even before, and were by their own choice irrelevant to the industry. (True, now they can also become influencers on Twitch and maybe one out of thousands can make a living by streaming their life 24h&#x2F;day. One ticket for the lottery, please). Whoever was between them and the majors is being squeezed out of the game. reply ddq 15 hours agorootparentprevFundamentally, the artists getting paid doesn’t matter because they enjoy making music? As a musician, your comment is completely ignorant, self-centered, and totally irrelevant to the discussion of people getting economically screwed. reply raincole 15 hours agorootparentNo? You completely misread what he said.As more people are able to produce music (due to cheaper tools like DAWs, more accessible music theory education, etc etc), if the demand of music doesn&#x27;t grow proportionally, the average income of musicians&#x2F;songwriters would decline.The above will happen regardless of Spotify&#x27;s existence. Thus, Spotify doesn&#x27;t matter (much). reply skinner927 15 hours agorootparentThat&#x27;s like saying as the price of circular saws drop in price, hand made furniture becomes cheaper.You&#x27;re just going to end up with a bunch of sloppy tables.People still want to listen to quality music from artists who have years of practice and experience. You can&#x27;t reliably get years of experience unless you&#x27;re getting paid to do it.Sure, there are exceptions, but it&#x27;s not the rule. Michael Jackson would not have existed if there was no money in the career. The money is why his father pushed so (insanely) hard.The counter argument is trash music will just be the norm. And maybe for a while that would happen, but eventually we&#x27;ll see someone (similar to the private search engines we see today) come out with a new platform with the selling point that artists get a living wage -- as long as the people demand it, and I believe they will. reply raincole 15 hours agorootparent> That&#x27;s like saying as the price of circular saws drop in price, hand made furniture becomes cheaper.Uh... and it&#x27;s true? If the price of circular saws drop in price, and the demand for hand-made furniture doesn&#x27;t change, then they&#x27;ll become cheaper. How much cheaper is another question, as circular saws are already very cheap today, compared to hand-made furniture.So yeah, you&#x27;re right, it&#x27;s just like saying that.> if there was no money in the careerIt&#x27;s unlikely to decline indefinitely. Piracy, Spotify, more youtube channel teaching how to make music... all these didn&#x27;t prevent Billie Eilish from becoming a star. reply wwweston 7 hours agorootparent> Uh... and it&#x27;s true? If the price of circular saws drop in price, and the demand for hand-made furniture doesn&#x27;t change, then they&#x27;ll become cheaper.I don&#x27;t know whether circular saws are likely to be the biggest input into a piece of handmade furniture, but my guess is they&#x27;re not and it&#x27;s time invested in the specific work and practicing the art rather than industrial capital.Similarly, while DAWs while can (though not necessarily do) reduce capital necessary to do certain aspects of production, they don&#x27;t represent the most significant investment into writing music. Also time, both in the creation of the specific work AND in terms of time practicing the art.> It&#x27;s unlikely to decline indefinitely. Piracy, Spotify, more youtube channel teaching how to make music... all these didn&#x27;t prevent Billie Eilish from becoming a star.Survivorship bias. Billie Eilish or any other individual success are no more an indication that all is well with the status quo than blue zone anecdotes are promises anyone who chooses can be a centenarian. reply ilyt 14 hours agorootparentprev>That&#x27;s like saying as the price of circular saws drop in price, hand made furniture becomes cheaper.>You&#x27;re just going to end up with a bunch of sloppy tables.Well, yes, and that&#x27;s how IKEA and mass production in general made many people that would be making furniture out of the job.Even in tailor-made stuff good cheap tools does make work of skilled maker far quicker. And you can get more people trying to get into that if the tools are cheap.Hardware is cheap, software is free&#x2F;near free so there is far more people trying, when you no longer need to spend small car worth of money just to say play electronic music> People still want to listen to quality music from artists who have years of practice and experience. You can&#x27;t reliably get years of experience unless you&#x27;re getting paid to do it.Most musicians got that by playing in garage bands and doing concerts.And many of them did it entirely for free, out of passion, till they were good enough, far before fancy computers were in everyone&#x27;s pockets.> The counter argument is trash music will just be the norm.It is the norm far before Spotify happened I&#x27;m afraid reply Terr_ 14 hours agorootparentprev> You&#x27;re just going to end up with a bunch of sloppy tables.That&#x27;s only true if you assume all the customers desire (or are willing to settle-for) arbitrarily bad tables for cheap. That isn&#x27;t guaranteed, but even then... why are you so certain their decision is wrong? Maybe they simply care about something else more than their tables.Meanwhile, the section of customers who still desire good tables will find those good-tables more affordable than before, even if they&#x27;re a relatively smaller slice of the expanded table-market pie.Sure, there are crappy $5 T-shirts, but today I could buy silk and lace enough to embarrass a king. Terribly an artful books exists to come up, but I could still accumulate a library in my pocket that would be the envy of any ancient monastery or place of learning. reply foobarian 12 hours agorootparent> Sure, there are crappy $5 T-shirts, but today I could buy silk and lace enough to embarrass a king.Actually I think something has happened to the textiles industry whereby demand must have driven a certain band of suppliers out of business, and now try as I could I can&#x27;t get polo shirts in the same thick quality cotton weave I could 30 years ago. There is probably some niche source possibly online but I don&#x27;t know how to discover it; the standard \"throw money at luxury mall brand\" route seems to not work any longer as the brick and mortars have watered down their materials as well. Sic transit gloria mundi reply chefandy 3 hours agorootparentIt&#x27;s a well-documented escalation of planned obsolescence and it&#x27;s true for everything from your washing machine to your polo shirts to your car. If you make it cheaply so it deteriorates quickly, constantly bring out new styles to make your current thing seem prematurely out of date, and make it juuuust cheap enough, you can sell people 10 shirts over 10 years instead of 2.I like wearing industrial clothing (like red kap cotton work shirts) and to my eye seem like they&#x27;re made about the same quality they always were. reply 8note 5 hours agorootparentprev> Michael Jackson would not have existed if there was no money in the career. The money is why his father pushed so (insanely) hard.I think the existence of Micheal Jackson is quite tragic, so I don&#x27;t think it&#x27;s a good thing that a system tortured him into being a famous musician reply badpun 15 hours agorootparentprevThere&#x27;s still money in making music, just not in selling recordings. Biggest touring artists (the Beyonces etc.) bring in millions. They, in turn, require skilled producers to make their songs, who are also paid well. reply wwweston 7 hours agorootparentprevMight have been interesting to ever find out if this was true.What happened instead was that Spotify led the pricing change by taking capital, cheating policy, and producing a consumption avenue that cut the price by orders of magnitude.And meanwhile:> cheaper tools like DAWs, more accessible music theory educationThe gains in education are fractional. The library or a neighborhood piano teacher were good enough resource wise. YouTube eliminates the trip (and the funny thing is that we&#x27;re iffy on even rewarding those people proportionally), but isn&#x27;t a new opportunity.And even for materials that are better in the way that 3Blue1Brown is for math... just like you&#x27;re going to have to sit down and spend a lot of time actually doing problems rather than just watching the videos if you have any hope of really getting it, the constraint when it comes to producing music is still sitting down and putting in the time, not only on the specific problem&#x2F;work in front of you but in the background to do it elegantly.DAWs are great and can make up for some margin of missing virtuosity, but you have to put in the time practicing using them too -- they become their own instrument.The constraint on making music has always been time. And what gets you more time to do something? Either having another source of wealth, or getting economically rewarded for doing that thing.Spotify and the damage it&#x27;s done the market absolutely matters. Just because music is getting through the damage doesn&#x27;t mean there wasn&#x27;t some lost, and not just quantity, level that could have been leveraged to through the magic of compounding focus. Anybody who&#x27;s read Graham&#x27;s \"maker schedule&#x2F;manager scheduler\" should already know this. reply 8note 5 hours agorootparentWhat even was the music market though? I think it was a spot where record labels could get rich off of music while musicians still had to pay their way with live shows and merch.Why do I care if spotify&#x27;s investors have replace the record label investors? reply chefandy 3 hours agorootparentprevI&#x27;m never not astonished by the sweeping pseudo-philosophical bullshit conflating hobby art with literally millions of people&#x27;s livelihood, visual and other creative culture, etc. There has to be an echo chamber in some subreddit where people who have no idea what they&#x27;re taking about all slap each other on the back for their ill-conceived musings about the disposition of artists in our society and the nature of art. reply ghaff 15 hours agorootparentprevHow would demand grow proportionately? People have a limited attention budget to listen, watch, and read things. reply raincole 15 hours agorootparentYes, and that&#x27;s exactly what the GP was trying to say. reply ghaff 15 hours agorootparentAh yes, didn&#x27;t read far enough upthread.Of course, this isn&#x27;t new. Tim O&#x27;Reilly said something similar in the context of book publishing probably getting on to 20 years ago at this point. reply paulddraper 15 hours agorootparentprevThey matter.Also, like everything in the universe, they are subject to supply vs demand.And fundamentally the supply exceeds the demand. reply easyThrowaway 14 hours agorootparentProblem is, Spotify is engineered to make sure the supply stays concentrated in a very, very small amount of hands. reply developer93 15 hours agorootparentprevWhat&#x27;s your opinion of bandcamp? reply jrockway 9 hours agorootparentprevIncreasing the size of the market is a well understood phenomenon, but it isn&#x27;t the only way a business can be successful. The music companies would prefer that you spent less time watching TV and more time listening to music. Or, they would prefer that you had less meetings at work so you&#x27;d need music to fill the silence. Or, Artist A would like to convince you that they are better than Artist B.Humanity has always been constrained by the 24 hour a day thing, but the economy has grown nonetheless.That last little bit is interesting. Back in the physical media era, if Artist B fell out of your rotation, you could sell your record&#x2F;tape&#x2F;CD and decrease the size of their new market a little bit. Then we went to DRM, and every song you bought was a sunk cost; if you didn&#x27;t listen to it, you still payed. Now with streaming, it&#x27;s back to the downsides of physical media; if you stop listening to Artist B, they stop getting paid. reply kouru225 15 hours agorootparentprevAn unlimited supply of unoriginal music because artists don’t have the luxury to experiment anymore. reply jononor 15 hours agorootparentThat is not what is happening? There are soo many niches and subgenres these days, and it is evolving year over year. reply matwood 12 hours agorootparentprevDid you miss a &#x2F;s? There is more varied music being created every day than ever before right now. There are sub sub sub genres you can seek out if you want. Contrast this with when I was a kid and we basically had what the radio played or what cassette we could buy with our $10.The problem now is that we have so much content (music, books, movies, short vids, long vids, etc...), and not enough aggregate time to consume it all. reply wwweston 7 hours agorootparentprevIf music was fungible for any other music and a musician&#x27;s day had an unlimited supply of hours, this might be a reasonable position.There was already \"enough\" recorded music decades ago that someone could fill all their living hours with constant listening and not exhaust it. If that&#x27;s really all there is to it, I&#x27;m sure you&#x27;ll have no problem committing to never listen to anything after around 1970. If you have any hesitation about that then you might start to see serious shortcomings in this conception of supply.\"there are more people able to make and publish music then ever\" also papers over nearly everything that matters about the statement. \"There are more people\" is the defensible part. There&#x27;s half an argument we have greater access to affordable digital tools for production than ever -- but I&#x27;m not even sure it&#x27;s half. The constraining factor on music composition, performance&#x2F;recording, production is always time. Even where the tools themselves save time that&#x27;s a series of converging terms that stops at a limit because to make good music you have to practice using those tools plus others. A lot.Set up a system which rewards those people in proportion to the audience they find and those people are both equipped and incentivized to spend more of their time into making not only more but making better, because they aren&#x27;t required to spend their time doing other things.Set up a system which says \"Oh yeah, we shouldn&#x27;t reward any of this, people should just do it in their spare time\" and sure, some will do it in their spare time. But they&#x27;ll miss out on the compounding effects of focus and its power laws because they&#x27;re occupied with whatever other stuff policy+markets have been set up to value instead. And their audience and the rest of the world will miss out on their power law peaks.Which is why I was too generous with my earlier \"never listen to anything after around 1970\" thought experiment. Really, don&#x27;t listen to anything except debut releases before 1970. Some of the debuts are really good, of course, and labors of love (or capital-backed love) as you say. But the post-debut work is what&#x27;s enabled by the economic feedback.There will, of course, likely often be survivors to bias ourselves to the status quo with. And perhaps that&#x27;s good enough for some. Hell, maybe we&#x27;re even rapidly getting to a point where we don&#x27;t even need most artists at all, we can simply have software trained on all the work of all the artists that have ever recorded produce music for you, and be done with not only the pesky idea of rewarding musicians whose work we appreciate but having a pesky human being involved in direct production in the first place. reply bombolo 15 hours agorootparentprevPeople being able to afford professional equipment and professional session musicians vs a guy recording himself in a bedroom over a MIDI karaoke track is not the same at all.If you can&#x27;t hear the difference, see a doctor. reply ben_w 14 hours agorootparentMost people are not professional music critics, and most of their consumption is as a backing track to the rest of their life.You could replace most of this category with a Markov chain bouncing up and down a simple key without most people even thinking about it, and I know because this is exactly how I made music for my shareware video games a decade ago. reply wwweston 5 hours agorootparent> You could replace most of this category with a Markov chain bouncing up and down a simple key without most people even thinking about itThat actually makes Spotify worse, because they could have offered that product instead of using huge sums of capital to reshape the expectation anybody with a device is entitled to listen to any work on demand for free.I guess the good news is that it wouldn&#x27;t cause a fuss if someone were to change policy so that you can&#x27;t pay out buffet streaming like it&#x27;s digital radio and people ended up having to buy songs or at least do the honest work of piracy if they won&#x27;t accept the app directing programming. After all, most people are just as happy listening to a Markov chain generate bloops for free. reply bombolo 12 hours agorootparentprevMost people are not professional movie critics and enjoy more a hollywood film rather than me recording barbie dolls and making them talk.Did your video game sell as much as outcast? A game with a proper music score.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Outcast_(video_game)Does your game have a wikipedia entry?Could I assume that people enjoyed outcast more than your hobby game? reply ux-app 6 hours agorootparentlol, what a fantastically obscure game to choose! reply t0bia_s 15 hours agorootparentprevMany don&#x27;t see a difference. Just amount of coolnes.You can apply this on professional filmmaking or vlogging. I guess amount of time consumed audiovisual production today is much higher on amateurish production thanks to antisocial networks. reply bombolo 12 hours agorootparentThey might not be able to point exactly the problem, but they will most certainly enjoy better produced content. reply t0bia_s 11 hours agorootparentIf you are used to fast editing, loud music and cheap filters, you&#x27;ll get hard time to watch ie Malicks films, listen concertos or go to photography exhibition. No doubled about qulity.Nowadays most valuable is attention. Cheap stimuli is easier to consume. That&#x27;s what technology teach us. reply bombolo 5 hours agorootparentI don&#x27;t think i was discussing addiction. reply t0bia_s 3 hours agorootparentYou can be addicted to quality production as well. replydeadbeeves 15 hours agorootparentprevIt sounds like you&#x27;re saying that because one is more expensive than the other it is therefore better. reply ddq 14 hours agorootparentNo, I&#x27;m sorry but you are ignorant. reply bombolo 12 hours agorootparentprevI agree that a 2 million $ guitar isn&#x27;t better than a 2k$ guitar.But a 2k$ guitar is certainly better than a 50$ guitar. Not only in how it sounds but in how easy it is to play it.My 1st guitar was bad so I couldn&#x27;t do barrè chords. I thought I was bad and pros could do it. Turned out pros just had better guitars.Better guitars also have less noise, better cables are shielded.Yes, more expensive is better (up to a point). reply deadbeeves 11 hours agorootparentYes. However, the instrument being better just means the sound it makes will be better, not that the music the musician makes with it will be better.EDIT: I should say the sound will likely better, not just better. reply bombolo 5 hours agorootparentI just gave you an example of how it&#x27;s easier to play, allowing more music to be played...It&#x27;s not just about the sound. reply ilyt 14 hours agorootparentprevYou can make great sounding music using nothing but free or extremely cheap software. But yeah, still need a good mic.Even when you go into hardware you can still get plenty for cheap. reply bombolo 13 hours agorootparentYou need time, which isn&#x27;t cheap :) reply rightbyte 15 hours agorootparentprev> > Spotify basically killed any money coming from the physical distribution - Worse than piracy, which was inevitable too at the time> Spotify adopted the economics of piracy and stamped them with the false veneer of legitimacy.As a side note, in the beginning Spotify used pirated music off The Piratebay without asking for permission from the copyright holders. reply patwolf 15 hours agorootparentI used to purchase mp3s from Amazon, and there was one song that had a glitch in it, like it was a bad rip. I always wondered if they were using pirated copies as well. I just re-downloaded for fun and the glitch is still there. reply rightbyte 12 hours agorootparentIt wouldn&#x27;t surprise me if e.g. people at Microsoft ran pirated copies of Office or whatever. Or like Photoshop at Adobe. Getting hold of licenses can be a nightmare, and Microsoft products more so in the past. Nowadays, every Microsoft license seems handled by some enterprise admin account. reply dharmab 41 minutes agorootparentI can speak for Adobe- Adobe employees have work accounts with full access to the Adobe suite, an internal web portal to get annual free licenses for their personal accounts, and an internal web store to purchase heavily discounted licenses to gift to friends and family. No one within Adobe is pirating Photoshop.Anecdotally, I had a friend at Microsoft hook me up with discount Windows OEM licenses for my PC builds and it seemed similarly easy to get licenses. reply pdntspa 15 hours agorootparentprevIt was employees&#x27; personal MP3 collections that seeded their library, so while that statement is true it is a little disingenuous without further contextThere are lots of other examples of this happening too... I believe some of the early nintendo retro releases were emulators running pirated roms reply ben_w 14 hours agorootparent> It was employees&#x27; personal MP3 collections that seeded their library, so while that statement is true it is a little disingenuous without further contextIf anything, that feels even worse.> I believe some of the early nintendo retro releases were emulators running pirated romsIf Nintendo has a licence for the game that the ROM was an unlicensed pirate of, while that&#x27;s weird, it doesn&#x27;t seem fishy in the same way. reply digging 11 hours agorootparentprevThat is not at all better. reply pdntspa 9 hours agorootparentI don&#x27;t understand how employees contributing their personal collections is somehow worse than company agents trolling torrent sites specifically to stream.Like, every nerd from the era has an mp3 collection. Mine is literally the only data that I have that&#x27;s been around since I was 15 and survived multiple HD crashes.How are you going to get the streaming business up and running without some seed data?Are you also mad at Uber and Lyft? replyradley 15 hours agoparentprevEveryone overlooks the fact that it will still take someone (i.e. a graphic artist) to produce great AI imagery.First, AI generated art is random and disposable. Yes, you&#x27;ll get a great image that you can use once, but then what? You can&#x27;t build a campaign on it.Second, AI generated art can&#x27;t be copyrighted, so knockoff competitors are free to use your AI-generated marketing images.At the very least, you can seed the AI with a paid graphic artist&#x27;s work (seed-based AI images can be copyrighted). But that artist will do it better than your unpaid intern. reply karaterobot 14 hours agorootparentMmm, I don&#x27;t know about this. At the very least AI lowers the bar for how talented a graphic artist needs to be to produce professional work, which means it&#x27;ll be easier to undercut them, which means it&#x27;ll get much harder to make a living as a graphic designer. It amounts to the same thing as killing off the profession, as seen from the perspective of someone in the profession as opposed to someone without skin in the game. It&#x27;s like saying push-button elevators didn&#x27;t hurt the profession of elevator operator, because somebody&#x27;s still got to push those buttons. reply danenania 13 hours agorootparentI think AI in general, across almost every industry, will shift value away from technical proficiency and toward creativity and taste. Implementation of an idea&#x2F;vision will be commoditized, but having a great idea, a unique insight, the taste and ability to identify top-tier work will still be highly valuable. This could well remain true post-AGI.In graphic arts, the overlap between people with technical proficiency and vision&#x2F;taste is probably quite high, but it&#x27;s not one-to-one. There are people with excellent taste who can identify great art or design when they see it, and who can perhaps imagine incredible masterpieces in their minds, but cannot draw a convincing stick figure. On the other side, there are people who can expertly make someone else&#x27;s concept real, but can&#x27;t come up with a compelling concept themselves. AI will be great for the former, and bad for the latter (or at least force the latter to adapt).Whether this will have the effect of concentrating wealth or distributing it more widely strikes me as a very difficult question. It may be devastating for certain professions, but could also enable a whole new class of entrepreneurs. I could see it going either way, or the two effects may cancel each other out and economic equality stays about where it is. We&#x27;re in the realm of complex systems here, so I wouldn&#x27;t put much stock in anyone&#x27;s prediction. reply bsder 12 hours agorootparent> I think AI in general, across almost every industry, will shift value away from technical proficiency and toward creativity and taste.The problem is that an artist still needs to eat in the 10-20 years it takes to develop \"creativity and taste\".What AI will do&#x2F;is doing is knock out the entry-level jobs. If you can&#x27;t train humans on the entry-level, you will eventually have no experienced people. reply 8note 5 hours agorootparentThe artist will have creativity and taste by the age 16, then evolve it through their 20s.The ai generation of artists will grow up with ai tools reply orbital-decay 14 hours agorootparentprevIt also raises the bar of what&#x27;s possible. What counts as \"professional level\" changes each time some new technique emerges. A skilled artist will always be better than a random person.The visual entertainment \"supply\" is not limited by the current state of tools. It&#x27;s always limited by the skills of the top crop. Professionals are always ahead and hard to come by. The industry&#x27;s self-regulating mechanism is novelty; what is abundant becomes fundamentally uninteresting and dies. reply radley 12 hours agorootparentprev> AI lowers the bar for how talented a graphic artist needs to be to produce professional workI think it&#x27;s a different kind of talent, and not automatically a lower bar. The key to being a professional artist is being able to offer variants based on given direction. Either way, it&#x27;s much much more than pushing a button or holding a lever in place for a period of time. reply madeofpalk 12 hours agorootparentprevThis is the march of progress. Digital brushes in Procreate lowered the bar for how talented an artist needs to be to create an oil ‘painting’. The camera lowered the bar for creating portraits. reply ranguna 2 hours agorootparentprev> Yes, you&#x27;ll get a great image that you can use once, but then what? You can&#x27;t build a campaign on it.Checkout confyui, it has an incredible amount of composability that allows you to generate new images based on others. Like image to image but on steroids.For example, you can generate a character sheet and use it to generate the same characters on different poses using controlnet. Or you can have a base image for an object and use that to generate the same object from different angles and&#x2F;or different colours etc. reply charlieyu1 13 hours agorootparentprevI agree with you, but the main problem is that illustrators are under-appreciated. We are in a world where management with no technical knowledge are having too much power and stealing paychecks. reply radley 12 hours agorootparentI totally know. I started as an illustrator:https:&#x2F;&#x2F;radleymarx.com&#x2F;work&#x2F;elemental&#x2F; reply prox 14 hours agorootparentprevAlso people cannot judge great art or imagery. Unless you have had the training. But the average person? Nope. You can tell what you LIKE but that’s not the same. reply charlieyu1 13 hours agorootparentI don’t have much training but it is not that difficult to spot AI arts which is pretty repetitive. The first couple are awesome but it gets old really fast. reply michaelmrose 14 hours agorootparentprev> Second, AI generated art can&#x27;t be copyrighted, so knockoff competitors are free to use your AI-generated marketing images.No. First off trademarks exist and they found that work done solely by the machine couldn&#x27;t be treated as a work for hire copyrighted by the machine and assigned to the operator. There is no reason to believe that work couldn&#x27;t be treated directly as copyrighted by the human operator who has creative input nor is the matter with the images used to train the model truly settled.>First, AI generated art is random and disposable. Yes, you&#x27;ll get a great image that you can use once, but then what? You can&#x27;t build a campaign on it.You can already get variations on a them and text driven modification eg make the blank a blank or make the blank blanker. reply orbital-decay 13 hours agorootparentTrademarks are different from copyright.Random variations aren&#x27;t interesting, they just make something abundant even more abundant and secondary. Unless you have a model with sufficient intelligence that can create something conceptually original (at which point we&#x27;re all fucked, not just artists or programmers), it&#x27;s not going to fly. Text driven modifications imply conceptual human input; besides, they are inherently worse than higher-order input, just like text to image alone is worthless for anything meaningful. reply michaelmrose 11 hours agorootparentThere exist systems where you can describe not only initial scenes but successive textual modifications to existing images and furthermore variations aren&#x27;t random. Successive selections are a way to zero in on a concept.You are about a year behind the state of the art. reply orbital-decay 11 hours agorootparentAKA tell me you haven&#x27;t spent time with diffusion models, without telling it :)I actually did figure out what works and what doesn&#x27;t in real artistic use. Which is the entire point of the article in OP which nobody seem to have read - text doesn&#x27;t work well beyond the basic use or amateur play, regardless of it being the initial prompt or editing; you need sketching and references (and actual skill) to do real work. I don&#x27;t think anybody&#x27;s using available methods of textual modifications for anything complex - they are cumbersome and unreliable, even worse than textual prompts. In fact, I haven&#x27;t seen anyone using them at all.Besides the implementation details, natural language just doesn&#x27;t have enough semantic density and precision to give artistic directions, even for a human or AGI. That&#x27;s a fundamental limitation. Higher order guidance, style transfer, and compositing is how it&#x27;s done. reply gamblor956 13 hours agorootparentprevThere is no reason to believe that work couldn&#x27;t be treated directly as copyrighted by the human operator who has creative input nor is the matter with the images used to train the model truly settled....Other than the USPTO and the federal court system issuing multiple ruling stating the opposite, including a decision last week which specifically stated that the output of an AI model is not copyrightable, upholding an earlier decision by the USPTO... (https:&#x2F;&#x2F;www.hollywoodreporter.com&#x2F;business&#x2F;business-news&#x2F;ai-...) reply michaelmrose 11 hours agorootparentExcept for the part where the court didn&#x27;t find that. It found that work only created by the AI didn&#x27;t qualify. Had it asked if a work created by the AI AND the person qualified it would no doubt have qualified as is already clear from using photoshop not serving to remove your ability to produce copyrightable works. The case didn&#x27;t ask that and therefore it wasn&#x27;t answered in any meaningful fashion.The act of prompting and customizing iteratively especially in systems which allow the user to submit a prompt that modifies the existing work for example \"replace the human being with a monkey\" \"make the monkey pink\" etc are clearly creative works that USE an AI not uncopyrightable.If you want to argue that point you absolutely cannot do so on the basis of a case that literally never addressed that issue unless you would like to traverse the muddy ground between actuality and fiction. reply gamblor956 11 hours agorootparentThe ruling stated that the Constitutional justification for copyright (and other IP) laws was to incentivize creators. AI does not need incentives, and thus AI-generated content cannot qualify for copyright. Under this line of reasoning, neither can patents (though note that trademarks derive value from the resources and effort spent promoting them, not from their creation, so trademarks are unaffected).The act of prompting and customizing iteratively especially in systems which allow the user to submit a prompt ...are clearly creative works that USE an AI not uncopyrightable.If you want to argue that point you absolutely cannot do so on the basis of a case that literally never addressed that issue unless you would like to traverse the muddy ground between actuality and fiction. The case literally deals with the output of the AI model, not the input. But on that note...under existing law, code can be copyrighted but not its output. Thus, it is logical to reason that prompts to an AI model can also be copyrighted to the extent they are not strictly functional.But with AI models and content generally, nobody cares about the prompts&#x2F;inputs. The output is what matters. (For comparison: Deep Impact and Armaggedon were both the results of the same input: disaster movie in which a team of astronaughts has to go to the asteroid to blow it up before it destroys Earth. The \"models\" were different screenwriters and directors. Compare the outputs: one is a blockbuster classic, and most people don&#x27;t remember the other movie.) reply michaelmrose 4 hours agorootparentThere was a statement on the prior thread that described the situation particularly well I&#x27;ll reproduce it herein and link the original comment rather than trying to better it.> The headline doesn’t seem to be what actually happened. The filer was arguing that the ai created the work on its own as a work for hire and thus the ai was the author with the computer scientist merely being the owner of the copyright as it was made for hire. I don’t think the argument that ai is a tool and the human operating it is the author was considered because the filer explicitly didn’t want to consider it.> In the review being appealed herehttps:&#x2F;&#x2F;www.copyright.gov&#x2F;rulings-filings&#x2F;review-board&#x2F;docs&#x2F;...> It makes it clear that the computer scientist doing the filing was trying to argue this was a work made for hire with the author being the computer. They wanted to argue that copyright can be assigned to non humans, but that just isn’t how the law works. The summary makes it clear early that it’s just taking their word that the work had no human input and was thus purely the creation of the computer.This seems to be a a better article https:&#x2F;&#x2F;www.millernash.com&#x2F;industry-news&#x2F;paradise-denied-cop...https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37189599In short A: Computer generated efforts virtually certainly qualify for copyright.B: Non-artists can in fact iterate and modify work not just randomly generate shit.C: This will virtually certainly get much better over time.You will still get much better work out of a professional who can both utilize such tools when desired, and actually create not just copy or prompt art. This thesis is supportable but we shouldn&#x27;t build it on sand lest it look more vulnerable than it is. replyThe_Blade 14 hours agorootparentprevsewing machine reply soligern 12 hours agorootparentprevAI generated art may be disposable but it certainly is very, very good. Midjourney makes plenty of impeccable art and photorealistic images that have no flaws. Also, even if there are flaws a week with some YouTube videos can teach anyone how to fix them, you don’t need someone with five years of deep experience. reply d1sxeyes 14 hours agoparentprevRecorded music was going this way whether it was Spotify or someone else that drove the final nail into the coffin.I remember when I was a child, on a Sunday afternoon, my dad would put on an album and listen to it. Just listen. Very, very few people do that now.Now we have a lot of demand for “incidental music”. Something you listen to while you do something else. Driving, reading, surfing the net, coding, cleaning…There was a fundamental shift in how people consumed music that started around the time music became portable. Spotify won the race, but if it hadn’t been Spotify, it would have been someone else. reply btilly 14 hours agorootparentI&#x27;m curious. When do you think music became portable?The transistor radio was invented in the 1950s. And quickly became used as background music as life progressed.Also incidental music is not a new thing. Tavern musicians as background music have been around for centuries. It is hard to prove, but likely for thousands of years. reply d1sxeyes 13 hours agorootparentFair, I should perhaps have clarified that I meant music chosen by the listener. Roughly I would say around the time the Walkman was invented. reply _glass 14 hours agorootparentprevA positive effect for performers is that people still want to go to concerts, but less and less people know how to play an instrument. The market is really much better now than even 10 years ago. reply throwaway290 14 hours agorootparentprevNo no. Incidental music and \"music you listen to while doing something\" are not the same.Listening to incidental music all the time devalues music. And we do it not because we wanted it but because Spotify, Apple music etc promote it. Until then \"just play random stuff that this ML thinks is similar\" was not a thing. But subscriptions make them more money than if they just let you buy albums and stream what you bought. I wish more artists didn&#x27;t sign up for this but unfortunately big labels did.But you can listen to non incidental music that you have specifically chosen while doing something. Even your dad could be doing something while listening to music (thinking). reply btilly 14 hours agorootparentAn example of \"music you listen to while doing something\" that is not incidental music are many sea shanties.The music had a purpose. reply SnowdustDev 15 hours agoparentprev> Spotify basically killed any money coming from the physical distribution - Worse than piracyAny sources for this?I&#x27;m of the impression physical distribution is on the rise compared to the earlier days of digital music. This has nothing to do with Spotify, and all about the digitization of music itself.Anecdotally many people I know now purchase merchandise and media as a way to support an artist they like, rather than listen to the music they make in a physical format. reply notefaker 14 hours agoparentprevThis is factually incorrect. If you own your master recordings, you stand to make $3,500 to $5,500 per million streams on Spotify. Apple Music and Tidal pay even better. This is why Taylor Swift is re-recording her entire Big Machine Records catalogue. While Spotify did shift consumers away from buying singles and albums as individual items, they also opened a new revenue source for independent artists. reply easyThrowaway 14 hours agorootparentCan you point me to a current-day independent artist which hasn&#x27;t been signed to a label that is pulling this amount of money just on streaming?If you&#x27;re already big enough that, i.e., XL Recordings can ask you to make a record without getting rights on the master, I wouldn&#x27;t count it as a good example of \"indie artist\". reply raviparikh 11 hours agorootparentI make about $4,000 per million streams on Spotify for the tracks I’ve released independently. For label releases I make less, but the label promotes them so that sometimes results in more net revenue. I have a bit over 10M Spotify streams over the last 3 years.Also, Spotify promotes my music via editorial playlists and algorithmic (eg Radio or Discover Weekly), so I’m probably making a lot more total revenue than I would have on iTunes. reply franl 13 hours agorootparentprevRussEDIT: Not making Taylor Swift money, but not many are reply easyThrowaway 12 hours agorootparentare you sure he’s doing 5k&#x2F;month just by streaming? No syncs nor shows? Also if Wikipedia is right he’s signed with Columbia Records. AFAIK the only artists making that kind money just by streaming while having no strings attached EVER (No label distro, no label A&R, no big tent agencies) are Macklemore and Chance The Rapper. Just two guys over millions of artists on Spotify. reply franl 12 hours agorootparentI can’t find the article from before he signed with Columbia (might’ve been a YouTube interview with him, can’t remember for sure), but yes, I’m fairly certain he was doing well over 5k per month with no major label.Also note the terms of his deal with Columbia are unlike most major deals in that he has a 50&#x2F;50 profit split after his advance payment got recouped, retains either full control or 50&#x2F;50 control of masters, etc. reply franl 12 hours agorootparentprevHere you go, he mentions it in the first 30 seconds of this video. He says roughly $100k per month before any label involvement: https:&#x2F;&#x2F;youtu.be&#x2F;OebNTkTfzHU replydeskamess 15 hours agoparentprev> \"the AI did this picture, so we don&#x27;t have to pay you.\"If the court rulings hold and AI works cannot be copyrighted then us end users do not have to pay for it either... but that seems like a race to the bottom. Like the end of a craft. Why would anyone create art if it has no&#x2F;minimal downstream value?Artists need to band together in some sort of union or not agree to do art with that AI clause or perhaps only do art with a no-AI use clause. And have an allowed AI-clause that is prohibitively expensive (like in the multi hundred millions per piece). That way &#x27;accidents that happen&#x27; have a prescribed recovery amount plus other requirements like pulling the generated artwork. \"Hey, we understand it may have been accidental, but here is the bill.\" reply staunton 14 hours agorootparentIt&#x27;s not the end of a craft, it just means that the prestige of \"made by human\" will increase even more and be pushed by by companies as a means of making money through copyright. That means that the few artists at the top will be rich while the niche between \"art\" and \"craft\" disappears. Professions involving visual art become like the music business. reply t0bia_s 15 hours agoparentprevImagine Spotify, rather than paying to musicans, just invest and publish AI generated music. Sounds like more profitable business to me.I&#x27;m not saying that I agree with this approach. reply dsign 15 hours agorootparentI honestly wonder if people would consume the music they know is AI generated. And by \"honestly\", I mean \"I don&#x27;t really know but I want to.\"I&#x27;ve been watching videos of Guy Michelmore in youtube. Not because I will ever write any orchestral music, but because I like his energy and envy his shed. Would I bother if Guy Michelmore were an AI? reply ilyt 14 hours agorootparentEntirely depends on how good it would become.It could also have some interesting avenues, like feeding some variables to the AI from say a video game (number and type of monsters on screen, mood etc.) to generate music reacting to what is happening on screen reply t0bia_s 12 hours agorootparentprevIt depends on how you define art. You can play music or shoot film or paint a picture. AI could do it as well. But the essence of what makes good art comes from soul, from experience by living, from relationships between us... that is created for stories that inspire.That is not what AI would ever generate. reply paul_funyun 12 hours agorootparentprevI would. Out of the bands I listen to maybe 5 of them I could name a single member. I&#x27;m a big reader but I couldn&#x27;t tell you one thing about most of the authors other than their names. reply yellow_postit 14 hours agorootparentprevAnd absent some major technology changes Spotify in your example has no way to do credit assignment back to the training set for any attempts at royalties should they be so inclined. reply mr_toad 11 hours agorootparentprevWithout copyright protection anyone could copy their entire library and set up a rival streaming service. It certainly wouldn’t be worth much investing in the AI part of the business. reply easyThrowaway 16 hours agoparentprevAlso, if you&#x27;re wondering \"Well, I could get better terms for my art\" - Like I said, when Spotify arrived and you were signed on a label you HAD to sign the part that said \"Yes, you can put my music online on Spotify and I will get paid peanuts\" or else, unless you were Madonna or Taylor Swift.Or, sure, you can also terminate your record deal. Hope you have 500 grands around just for that.Frankly I don&#x27;t see it ending much better for visual artists. reply munk-a 15 hours agorootparentThere was another path here - collective bargaining. When small individuals are bullied by large corporations it&#x27;s because those corporations want something from the small individuals... they certainly don&#x27;t care about one or two small artists walking away from the platform - but if artists can organize and bargain as a group they can ensure a fair outcome.I think the modern world has become too complacent in terms of labor organization - the time of plenty left a lot of people content to take whatever was given to them because there was such a glut of excess that it was freely shared. That sharing is coming to an end and we&#x27;re returning to a time when we need to demand fair and equitable treatment. reply EatingWithForks 15 hours agorootparentFreelancers in the united states are not allowed to bargain collectively for better prices, as that&#x27;s considered market manipulation&#x2F;price fixing. [\"Independent Contractors\" are literally banned from forming a union in the USA.] reply Buildstarted 14 hours agorootparentSerious question: Are actors, writers, etc not considered \"Independent Contractors\" in the US? reply biogene 15 hours agoparentprevWhenever there are implications to people&#x27;s lively hood, its always a serious matter - but I hope people are able to transition to other roles.I think Gen AI will commoditize the mundane and \"typical\", and heavily push people into creating something extraordinarily unique. I think there is the same pressure even without AI, when as a creator you have to standout amongst the sea of people vying for people&#x27;s attention.I believe GenAI can be useful in a way too. For e.g. If I&#x27;m an artist looking for inspiration, I can have a GenAI tool create some \"random\" works that I can get inspired from. reply wslh 16 hours agoparentprevThe Spotify example is similar to the Google impact: the last mile is the search engine UI that controls your access to content. Spotify is another UI as they are streaming services, etc.Seems like a natural iteration in the ordering of complex systems. Beyond legal regulations it would be great to start to think about new solutions, if they ever exist. reply adamc 11 hours agoparentprevSpotify has been a disaster, but unless the artists walk away (very hard to do), I don&#x27;t see our political system as caring enough to do anything about it. reply bandrami 12 hours agoparentprevOne thing that will really matter is that the output of AI cannot be copyrighted. If producers really go all-in on generation we&#x27;re going to rapidly see a situation where huge amounts of material will enter the public domain all at once, and we don&#x27;t really have a precedent for what happens then. reply Andaith 6 hours agoparentprevA fun solution to this would be to remove copyright protections for anything generated by an AI. Now nobody profits! reply franl 13 hours agoparentprev> Another opinion popular with no one: AI will have on artists the same impact that Spotify had on the music industry that is, it will kill any revenue flow for anyone outside of the publishers and big artists&#x2F;players.Maybe I’m misunderstanding you, but how much money do you think the 7500 creators on Spotify making $100k+ [1] would be making without Spotify or other streaming platforms? My guess is closer to zero than 100k.Also 0.09 percent of 8 million creators making 100k+ [1] sounds horrible, but my guess is that should be taken with a grain of salt. How many folks are included in that 8 million who registered, but uploaded nothing? How many uploaded once or twice? How many uploaded and did ZERO promo of themselves? How many are just plain terrible musicians?A number of years ago when I stumbled on him, Russ was pulling in a few hundred thousand per year from streaming. Looks like he’s making 100k per week as of a couple of years ago [2]. Yes, he’s probably an outlier. But he works his butt off on his craft, handles production and writing himself, and markets himself well.Headlines like “Big tech and AI destroying the indie music industry” get more clicks and attention than “Streaming platforms provide income where once there was none” so shrug.[1] https:&#x2F;&#x2F;www.digitalmusicnews.com&#x2F;2021&#x2F;02&#x2F;24&#x2F;spotify-artist-e... [2] https:&#x2F;&#x2F;twitter.com&#x2F;russdiemon&#x2F;status&#x2F;1325853093074923520 reply franl 10 hours agorootparentFound the video where Russ says he was making around $100k per month before any label involvement: https:&#x2F;&#x2F;youtu.be&#x2F;OebNTkTfzHU. I know this is the land of “that’s just survivorship bias!” And I certainly agree that luck and timing plays a massive role in billionaire level startup success, but this guy in particular is a few orders of magnitude of success below that (even if he’s still an outlier). I’m sure he still benefited from luck and timing, but he also was methodical about creating music non-stop, getting better at production, rapping, and writing, and marketing himself. My point being show me someone who has worked as hard and as smart as he has, who picked a niche of music that has large audiences (aka high Total Addressable Market), and who released as much content as him, and I will show you someone who is having non-trivial streaming success - again maybe not $1M+ annually - but something material beyond just scraping by. That doesn’t mean Big Tech is absolved of sin in how it distributes profits or exerts monopoly control or whatever, but I think we often overlook the opportunities these networks have provided for people that would otherwise live in obscurity with no audience whatsoever. reply karaterobot 14 hours agoparentprevNo offense, I don&#x27;t think this is an unpopular opinion. The comparison to Spotify is apt though. reply soligern 12 hours agoparentprevA human looking at someone’s artwork is “training a model”. It’s bullshit and anti progressive to say someone or something that is creating derivative works is stealing. reply Retric 12 hours agorootparentThe second half of derivative worlds is creating an imitation of the original not just looking at it, but this isn’t some grey area.Even just training the model requires someone to copy the original work from somewhere and store it into a database to use to train the model. If they don’t have permission to make that copy then it’s commercial copyright infringement independent of anything done by the model after that point.Thus the companies themselves are frequently breaking the sale even if nobody ever uses these systems. reply leeoniya 12 hours agorootparentprevnot stealing the work, just stealing the revenue...for very little investment.> A human looking at someone’s artwork is “training a modelsure, except that model often takes months or years to train (wall clock years, not 1000-core cpu-years). and the end result is not a human that can stamp out new&#x2F;competing artwork every 100ms.for any kind of creative&#x2F;performance&#x2F;art work, these are watershed times. us coders are not super far behind. reply Vt71fcAqt7 15 hours agoparentprevThe music industry has always had a long tail. Its very much a go big or go home industry. Do you have any data around revenue change for small artists before and after spotify? reply easyThrowaway 13 hours agorootparentSorry, no hard data. Mostly the perception of the industry at the time. Lots of tales of people quitting, moving, or going on \"indefinite hiatus\".TBH \"Fly or Die\" was way more common on the US side of the industry. And even in the USA by the late &#x27;90s to the end of the 2010 it was somewhat doable if you were skilled enough to make a living solo (we&#x27;re talking 60-80K&#x2F;year max) as a \"jobber\" opening for bigger acts on local venues.Like, the entire NYC indie scene got a start from this premise. If you get a chance, give a look to \"Meet Me in the Bathroom\"[1], which is a documentary specifically of this timeframe.[1]https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=n71c1Szjv08&themeRefresh=1 reply gamblor956 13 hours agoparentprevAI will have on artists the same impact that Spotify had on the music industry that is, it will kill any revenue flow for anyone outside of the publishers and big artists&#x2F;players.then resell the output for $$$ on something like SpliceThis is silly. The USPTO and Courts have repeatedly stated that AI-generated media is not subject to copyright protection, so there are no licensing revenue opportunities for the big publishers&#x2F;artists&#x2F;whatever. This means: AI-generated content is not protected by copyright, so anyone can use a piece of AI-generated art however they want without a license and unless the law changes AI has no value to the content industries.EDIT: Also, the USPTO has noted that the use of AI-generated content in a work will mean that the entire work will be presumed AI-generated except for the portions the content owner can demonstrate were generated by humans. The backend costs of maintaining AI-supplemented works will almost as expensive and burdensome as the costs associated with patents.Also, I think people on HN have a very glorified view of how much money musicians make from streaming or cd&#x2F;album sales: basically zilch, unless they&#x27;re popular enough to be in repeat on the radio. Most musicians made their money from performing: generally a little bit from ticket sales or venue incentives (like % of booze sales) but the real money for the performers was from the sales of band merch, which is why it gets pushed so heavily.At the end of the day the model itself is going to be basically irrelevant, while knowing whose works were actually used to train it being the truly differentiating feature.Yes, by lawyers, when they sue the owners of the AI model for copyright infringement, because this would not be a use protected by fair use doctrine. This will actually make human-generated works more valuable because now every work used to generate an AI work is now worth at least $75,000, even if its market value would be significantly less (or even commercially worthless) today.Due to the costs associated with licensing of human works, if AI-content becomes a thing, it will probably be more expensive than hiring a human to do the same thing, because the model will have to account for the cost of paying a license fee for every work that was incorporated into a specific output. reply orbital-decay 17 hours agoprevPosts like that nearly always assume the text-to-image and \"prompt engineering\" being used, usually due to the lack of experience with those models. This is categorically not the way to do it outside of having fun. The way it&#x27;s done for predictability and control looks much more like \"draw the rest of the owl, in a manner similar to my other hand-drawn owl\" combined with photobashing and manual fixing&#x2F;compositing. It&#x27;s a hybrid area similar to 3D CGI that requires both artistic and technical skills if you want to create something non-boring.This has nothing to do with the model&#x27;s poor understanding of natural language, and will not change until we have something that could reasonably pass for AGI, and likely not even then. Your text prompts simply don&#x27;t have enough semantic capacity. reply jefftk 17 hours agoparentYou might be interested in the \"Commercial illustrators will keep their jobs, but will mostly need to learn to use AI as a part of their workflow to maintain a higher pace of work\" section of the article, which gets into this more. reply orbital-decay 17 hours agorootparentYou&#x27;re right! I&#x27;ve stumbled upon the prompt engineering part and rolled my eyes, which was clearly too soon. reply makeitdouble 10 hours agorootparentprevThe more plausible evolution is that people drawing the base concept are not \"commercial illustrators\" nor have art training.If a magazine editor with run of the mill drawing skill can feed the prompt a sketch with stick figures and object outlines, and get back a good enough rendition with an improved composition, the job of the illustrator will be a side job of that editor.I&#x27;m partial to the argument that being able to fix the generated image in post is a valuable skill, but on that part we already have decades of progress and people are usually more comfortable with editing tools than drawing tools. reply danenania 16 hours agoparentprev\"This has nothing to do with the model&#x27;s poor understanding of natural language, and will not change until we have something that could reasonably pass for AGI, and likely not even then. Your text prompts simply don&#x27;t have enough semantic capacity.\"I don&#x27;t think it&#x27;s going to take AGI to get to this point. It&#x27;s &#x27;just&#x27; going to take a top-tier model adding robust multi-modal input imho. A detailed prompt plus a bunch of examples of the style you&#x27;re looking for seems like it would be enough.That&#x27;s not to say it isn&#x27;t really hard, but it doesn&#x27;t seem like it requires fundamental innovations to do this. The building blocks that are needed already exist. reply jwells89 14 hours agorootparentThe biggest problem I see with LLM-generated imagery is a near total inability to get details right, which makes perfect sense when one considers how they work.LLMs pick out patterns in the data they&#x27;re trained on and then regurgitates them. This works great for broad strokes, because those have relatively little variance between training pieces and have distinct visual signatures that act as anchors.Details on the other hand differ dramatically between pieces and have no such consistent visual anchor. Take limbs for example, which are notoriously problematic for LLMs: there are so many different ways that arms, legs, and especially hands and fingers can look between their innumerable possible articulations, positions relative to the rest of the body, clothing, objects obscuring them, etc etc and the LLM, not actually understanding the subject matter, is predictably terrible at drawing the connections between all of these disparate states and struggles to draw them without human guidance.You see this effect in other fine details, too. Jewelry, chain-link fences, fishing nets, chainmail, lace, etc are all near-guaranteed disasters for these things. reply orbital-decay 13 hours agorootparentIt&#x27;s mostly a problem of resolution, model size, and dataset quality, which can be mitigated with compositing. Larger models don&#x27;t have problems with hands, and if they do, it can be solved by higher-order guidance (e.g. controlnets) and doing multiple supersampled passes on regions to avoid to fit too much detail in one generation. Even SD 1.5 (a notoriously tiny model) issues with faces and hands can be solved with multiple passes, which is what everyone does. reply orbital-decay 15 hours agorootparentprevThere are two problems with this: a) natural language is inherently poor at giving artistic directions compared to higher-order ways like sketching and references, even if you got a human on the other end of the wire, and b) to create something conceptually appealing&#x2F;novel, the model has to have much better conceptualizing ability than is currently possible with the best LLMs, and those already need some mighty hardware to run. Besides, tweaking the prompt will probably never be stable, partly due to the reasons outlined in the OP; although you could optimize for that, I guess.That said, better understanding is always welcome. DeepFloyd IF tried to pair a full-fledged transformer with a diffusion part (albeit with only 11B parameters). It improved the understanding of complex prompts like \"koi fish doing a handstand on a skateboard\", but also pushed the hardware requirements way up, and haven&#x27;t solved the fundamental issues above. reply danenania 14 hours agorootparentI think you&#x27;re right about the current limitations, but imagine a trillion or ten trillion parameter model trained and RLHF&#x27;d for this specific use case. It may take a year or two, but I see no reason to think it isn&#x27;t coming.Yes, hardware requirements will be steep, but it will still be cheap compared to equivalent human illustrators. And compute costs will go down in the long run. reply florbo 17 hours agoparentprevThe post actually goes into a bit of detail on that process. reply yieldcrv 15 hours agoparentprevThis was essentially my postIts like you take your AI to school, or do a Matrix-style data upload into your AI so its up to speed on a new conceptProfessionals will learn how to do that, the market will cater to people that want to do that reply thelazyone 17 hours agoprevWell put. Big fan of the \"Commercial illustrators will keep their jobs, but will mostly need to learn to use AI as a part of their workflow to maintain a higher pace of work\" part.I&#x27;m a sometimes-illustrator (but my style is pretty far from what Generative AI is doing), and I recently published a 1.1 of a game manual which uses Midjourney images. I&#x27;m currently investing in a \"proper\" illustrator because the MDJ images lack character, but it&#x27;s also true that in a few months from now this might change: I&#x27;ll stick with the illustrator to have more consistency in the images, but probably the AI could do a fancier job there.Besides, the \"things will change in 2 months\" point is a good one, but it&#x27;s been used since a year and a half and things haven&#x27;t changed yet. Sure, the quality of the produced images improved, but not in a qualitative scale.Side note: the link civitai to leads to https:&#x2F;&#x2F;sambleckley.com&#x2F;writing&#x2F;civitai.com&#x2F;images which is a dead link. reply rcarr 17 hours agoparent> I&#x27;m a sometimes-illustrator (but my style is pretty far from what Generative AI is doing)Why not train your own personal AI on your artwork? Corridor Digital did this in the latest attempt to automatise animation, they hired an illustrator to create an animation style for them, then trained the AI on their drawings.Link: https:&#x2F;&#x2F;youtu.be&#x2F;FQ6z90MuURM?t=329 reply woolion 15 hours agorootparentI&#x27;ve actually done it [0], I&#x27;d like to have an AI assistant that I could directly use the results from, and the results were really terrible, mostly laughably terrible. I think it was too far from what the models handled correctly at the time, and given that issue it was not enough training images. Although I had also tried with a model that was better at handling stylised 2D. I&#x27;d like it to work, but I don&#x27;t think it&#x27;s viable for most people.[0] https:&#x2F;&#x2F;woolion.art&#x2F;2022&#x2F;11&#x2F;16&#x2F;SDDB.html reply toasted-subs 16 hours agorootparentprevSeems kind of shady imo. I know businesses is businesses but that&#x27;s seems a bit too mean for my tastes. reply rcxdude 16 hours agorootparentEthics of the use of generative AI in the first place aside, I&#x27;m pretty sure the illustrator was aware of what they were intending to do with their work (they even were interviewed about it in the behind the scenes video) reply __loam 15 hours agorootparentI view this in the same way I view the use of an actor&#x27;s voice for ai generations. Even if the person knows what you&#x27;re doing with their data, it still feels really scummy and unethical. The idea that we can sample someone else&#x27;s labor and be able to own that and generate shit from it in perpetuity (probably without paying them) feels very alienating. reply rcxdude 11 hours agorootparentLike being employed to write some code which then is owned by someone else? reply __loam 3 hours agorootparentMost software jobs have equity compensation now. reply rahkiin 16 hours agorootparentprevThis could have been all with consent and adjusted payments. AI does not just replace an artist, it can also speed up the work tremendously. It gives new possibilities using volume. reply breischl 15 hours agorootparentprevI&#x27;m not in illustration, but isn&#x27;t it already common to hire someone to create a \"style book\" of what it should look like, and then have other illustrators follow that? eg, I recall animated shows working that way.Doesn&#x27;t seem so incredibly different from that. reply Bjartr 16 hours agorootparentprevThe illustrator was aware their work was going to be used in that way. reply throwuxiytayq 16 hours agorootparentprevCare to expand? I have no idea what you’re on about. reply atleastoptimal 17 hours agoparentprevThe question is, since commercial illustrators can be more efficient using AI, will the total number of jobs in the space lower, or will the expectation for commercial illustration increase, thus increasing the workload and keeping the number of jobs the same. reply satvikpendem 16 hours agorootparentIn all of human history, work has always increased. This is akin to Parkinson&#x27;s Law, where work expands to fill the time (and now resources) available. reply singlow 16 hours agorootparentI don&#x27;t disagree, but concerning particular trades this is not true. In the mid-19th century there were more than seven thousand blacksmith shops in the US, which employed over fifteen thousand people, but today there are fewer than one thousand professional blacksmiths. Many of the products they produced either have lower demand or are produced by other means. If you consider the entire metalworking industry, we have many more total workers, but very few have the skills of a blacksmith.The number of people who do the current work of an illustrator might go down eventually due to AI, but there will likely be more total people employed in the process of producing illustrations. It is just likely that fewer of them will have the skills that today&#x27;s illustrators need, and also likely that fewer of them will command extraordinary wages. Many of the jobs that replace it will likely be closer to the median wage than today.Also we will eventually turn the corner and start having population decline. For the US this might be just a few decades away. And some time after that, work would eventually decrease. reply atleastoptimal 16 hours agorootparentprevWork has always increased, but work in a specific profession doesn&#x27;t necessarily increase. There are certainly fewer phone switchboard operators today than there were 100 years ago. reply satvikpendem 16 hours agorootparentIndeed, but that just means that humans will have to find new jobs, not that jobs will become obsolete. How well they will find new jobs, though, is another story, based on socio-politico-economic conditions of the country they reside in. reply addcommitpush 16 hours agorootparentprevThis is completely false; working hours per worker have declined after the Industrial Revolution [0].[0] https:&#x2F;&#x2F;ourworldindata.org&#x2F;working-hours#are-we-working-more... reply satvikpendem 16 hours agorootparentThat does not say anything about how much work exists in aggregate. The human population has gone up, so it can be simultaneously be true that the amount of work being done increases even as each worker works fewer hours. As well, this also says nothing about the quality of work, as GDP is going up, so it can also be simultaneously true that the quality of work increases even as each worker works fewer hours. reply LordDragonfang 13 hours agorootparentprevIn most of human history, the type of jobs available were relatively stable century to century; today, the types of jobs aren&#x27;t even stable decade to decade.The automation of physical labor let us turn to intellectual labor and creative labor. The coming automation of intellectual and creative labor is not like the previous automations of physical labor, because it leaves human jobs no where else to turn to.CGP Grey&#x27;s \"Humans Need Not Apply\" video[1,2] covered this almost a decade ago:> Imagine a pair of horses in the early 1900s talking about technology. One worries all these new mechanical muscles will make horses unnecessary.> The other reminds him that everything so far has made their lives easier -- remember all that farm work? Remember running coast-to-coast delivering mail? Remember riding into battle? All terrible. These city jobs are pretty cushy -- and with so many humans in the cities there are more jobs for horses than ever.> Even if this car thingy takes off you might say, there will be new jobs for horses we can&#x27;t imagine.> But you, dear viewer, from beyond 2000 know what happened -- there are still working horses, but nothing like before. The horse population peaked in 1915 -- from that point on it was nothing but down.> There isn’t a rule of economics that says better technology makes more, better jobs for horses. It sounds shockingly dumb to even say that out loud, but swap horses for humans and suddenly people think it sounds about right. [1] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=7Pq-S557XQU [2] (transcript) https:&#x2F;&#x2F;www.cgpgrey.com&#x2F;blog&#x2F;humans-need-not-apply reply bsder 12 hours agorootparentprev> In all of human history, work has always increased.Production has increased. It&#x27;s not clear that work has increased.Mills and factories used to employ people by the hundreds of thousands and maintain people in a blue-collar standard of living. Now, no manufacturer even exists in the top 25 employers in the US--it&#x27;s all service industry.The vast majority of the decendants of the people working those manufacturing jobs are not working in better jobs than those were. reply canvascritic 17 hours agoprevMy partner and I run a handful of small internet side businesses. One of our content-driven D2C businesses heavily relied on bespoke illustrations for our display ad creatives. we found that our ctr was decent, pretty average, but the CPC was killing us and ROAS really sucked.Several months ago we decided to A&#x2F;B test SD against our usual illustrators. In our case the results were pretty dramatic, we actually found that the ctr shot up by almost 20% and cvr showed a consistent uptick. I don&#x27;t agree with the blog post&#x27;s claim that AI generated images work best in businesses where the content doesn&#x27;t actually matter; this particular venture is a fantastic counter example. In our case the AI-generated images seemed to resonate more with our target audience, as we were able to achieve much more granular personalization at lower cost than before. not only did it reduce the CPA significantly, but the tight control we had over creative variations meant we could optimize in realtime based on audience segmentation.Not to mention that our time-to-market for launching new campaigns went down by half. no more back-and-forths over design nuances, missed deadlines, or creative blocks.And I do feel a bit mixed about the diminishing role of human touch in creative processes. But from a purely growth-hacking POV, this was a gamechanger, and we have the numbers to prove it.Overall I think this is a net win, especially because I don&#x27;t think this needs to be the end of the road for human illustrators, but this will force them to adapt and bring more sensitivity to the needs of their clients. It makes no sense for even a content business to be subject to so much friction in the procurement of creatives, and this forces more consideration to our needsAnywho there&#x27;s efficiency, and then there&#x27;s soul. Hats off to the robots for (mostly) nailing the former, and sometimes surprising with the latter. reply thwarted 15 hours agoparentno more back-and-forths over design nuances, missed deadlines, or creative blocks.This evoked, for me, the \"can I get the icon on cornflower blue\" scene in Fight Club.How much of this reduction in back-and-forth is influenced by the immediate&#x2F;interactive response (dealing with fewer humans) and how much is due to a level of trust-of&#x2F;delegation-to the machine? \"A machine generated this icon based on my description, there&#x27;s no need for me to question its choice of colors.\" — really the classic problem of considering machines as infallible and more expert than humans.It&#x27;s probably some of both. reply chefandy 14 hours agoparentprevI think your usage of \"matter\" and theirs is different. It&#x27;s furniture. Furniture \"matters\" in a restaurant and having the wrong furniture can hurt your business— but compared to the food, it&#x27;s essentially inconsequential.There&#x27;s a spectrum of how much furniture matters in any given place ranging from very short stay waiting areas to architect&#x27;s offices, and commercial art is no different. If that image was truly inconsequential, you wouldn&#x27;t need one there. Non-informational graphics on most non-professionally designed power point decks likely matter less. I&#x27;d say there&#x27;s about a zero percent chance of a two page spread opening a feature article in a magazine being ai-generated unless it&#x27;s an article about ai-generated images, and even then, it probably took professionals longer to massage it into shape than all of the rest of them. Specificity and per-pixel control is just so important in professional graphics workflows and despite what a huge stack of people who aren&#x27;t professional designers will tell you, they are simply the wrong tool for the job. It&#x27;s fundamentally the wrong interface. Maybe what Adobe or another player who knows what the industry needs will nail it, but it won&#x27;t look like Midjourney— that&#x27;s for sure. reply satvikpendem 16 hours agoparentprevWhat is your type of business and what kinds of images did you generate? Curious as I was thinking of doing something similar for mine. reply thebooktocome 16 hours agoparentprev> Overall I think this is a net win, especially because I don&#x27;t think this needs to be the end of the road for human illustrators, but this will force them to adapt and bring more sensitivity to the needs of their clients.The advantages of AI that you crow over simply can’t be met by any human professional artist. A human can’t do hundreds of revisions profitably. There’s increased “sensitivity” and then there’s needing to read the client’s mind.If you think this isn’t a death knell for human illustrators in this particular market, you’re deluding yourself. reply jononor 14 hours agorootparentA professional artist that is proficient in the latest generative image models can increase their ability to attend to client needs. reply thebooktocome 14 hours agorootparentThe client \"needs\" in question here are low cost overall, low marginal cost for each revision, and a totally-interactive \"do what I mean\" interface.Shoving a human artist in the middle is a liability on each front. reply jononor 12 hours agorootparentYour definition of the needs does not have any requirements on the fitness of the output, nor on time spent on customer side. That does not seem realistic. replyarvidkahl 17 hours agoprevIve tried letting AI write my articles. It was horrible. I tried ignoring AI-powered tools (such as grammar checkers, summarizers, rewriters, speech-to-text apps), and the writing process felt sluggish.The middle ground is what works best for me. I use generative AI exlusively mid-process, but neither for input (ideas) nor output (actual drafts.)Here&#x27;s how I write:- I source my ideas from contemplation or conversations on social media. Topics discussed there have at least some pre-validated relevance - I sit down for ten minutes and dictate my thoughts into a tool like AudioPen (no affiliation, just a fan) which summarizes my 10 minutes in 5 or 6 paragraphs. THIS is the AI step. The tool suggests a few paragraph structures that I cycle through until I find a good one. - From there, I write my draft, following that outline. No more AI tools here other than grammar checking at the end.AI is a great writing partner. It&#x27;s a horrible writer. reply earthboundkid 8 hours agoparentThat’s been my experience so far too. It’s good for giving you some suggestions for things to structure and things you may have missed, but it is a terrible writer ATM. reply satvikpendem 17 hours agoprev> Commercial illustrators will keep their jobs, but will mostly need to learn to use AI as a part of their workflow to maintain a higher pace of work.This is exactly what I&#x27;ve found to be the case too. People outside of this AI media generation community still think it&#x27;s entering some text and getting some output. In reality, there are entire workflows constructed to get the exact type of image one wants.Look at: https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;StableDiffusion&#x2F;comments&#x2F;14ye2eg&#x2F;co...The second image is the output image, but the first is even more interesting. It is a node based interface more commonly seen in game development tools like Unreal Engine which has a similar interface [0]. It is akin to hooking up APIs together to get the resultant image. I see the future of image generation being more akin to backend programming than actually drawing anything, which is to be expected as the actual drawing part is getting automated while the creativity now rests in the workflow itself (at least until we automate the workflow part too, but that&#x27;s a far ways off as computers can&#x27;t read minds yet to even know what the user wants).[0] https:&#x2F;&#x2F;docs.unrealengine.com&#x2F;5.2&#x2F;en-US&#x2F;nodes-in-unreal-engi... reply 01100011 15 hours agoprevPost seems very biased towards the now. Stable diffusion et al are very successful with a certain technique but it is foolish to think that is a method which will simply be improved indefinitely.Generative \"AI\" will take many forms. Ultimately it will likely remove much of the \"technique\" element to creation, depriving artists and content owners of income and relevance.Will this happen overnight? No. I suspect over the next, say, decade, AI will be a beneficial tool more than a threat.At some point, I expect generative AI to become multi-sensory(sight, sound, touch). Such systems will work from physical models of subjects&#x2F;environments to produce novel and accurate representations based on rich descriptions and deep contextual awareness of culture. These systems will not think in pixels but in objects and relationships which are then simulated, rendered and filtered to match the desires of the users.I do applaud efforts of the writers and actors to protect themselves from competition but I believe it will ultimately be in vain. It will be interesting to watch the legal developments in this space. It may be necessary for future generative systems to provide an audit trail showing how they gained an understanding of the world to prove no unauthorized training was performed. This merely raises the bar slightly and does not prevent future generative systems from deriving important relationships via other means, such as &#x27;clean room&#x27;, high-level descriptions being given(perhaps by other automated processes).For example, while it may be illegal to train an AI to reproduce Harrison Ford using his copyrighted works or even images captured in a public space, I can reduce Harrison Ford to a set of characteristics which can be passed to a generative system to produce something indistinguishable from the real Harrison Ford. If I am able to document this procedure I see few ways for the legal system to prevent it but then again I am no expert in this area.For what it&#x27;s worth, I&#x27;m not a fan of current \"AI\". I have found LLMs to be particularly unreliable and mostly useless. I also find most \"AI\" generated art to be either boring, inaccurate, or in some way not compelling. That said, I think the trend is becoming clearer. reply joshstrange 17 hours agoprev> Finally, an opinion popular with no one: Commercial illustrators will keep their jobs, but will mostly need to learn to use AI as a part of their workflow to maintain a higher pace of work.> This doesn’t mean illustrators will stop drawing and become prompt engineers. That will waste an immense amount of training and gain very little. Instead, I foresee illustrators concentrating even more on capturing the core features of an image, letting generative AI fill in details, and then correcting those details as necessary.I&#x27;m not sure why they think this is unpopular with no one. This seems like the logical path forward. In the same way that CoPilot isn&#x27;t going to replace me but it&#x27;s makes certain boilerplate much less painful and avoids the \"blank page\"&#x2F;\"writers lock\" that can happen when I go to write a function sometimes. It&#x27;s just nicer to start from something then modify it until I have what I need (even if I end up replacing 80-99% of it).In the sam",
    "originSummary": [
      "The author explores the use of AI image generators and how they can impact various groups, including physical media artists and commercial illustrators.",
      "The concept of stable diffusion is explained, along with the challenges of using text to direct image generation.",
      "AI is predicted to replace certain types of illustration and writing tasks, but achieving specific results can be challenging, and the use of image constraints is suggested. The author emphasizes the importance of AI products that seamlessly integrate into an illustrator's workflow and expresses hope for future AI products catering to commercial illustration."
    ],
    "commentSummary": [
      "Artificial intelligence (AI) has the potential to significantly impact the art and music industries.",
      "Concerns include the decline in revenue for artists and the concentration of wealth in corporations.",
      "The music industry has already experienced changes due to streaming services, and AI may further disrupt human endeavors.",
      "AI is also playing a role in music production, raising questions about the value and quality of AI-generated content.",
      "Copyright issues and the need for new platforms and solutions are topics of debate.",
      "The overall impact of AI on these industries remains uncertain, with both positive and negative consequences."
    ],
    "points": 360,
    "commentCount": 291,
    "retryCount": 0,
    "time": 1692631991
  },
  {
    "id": 37211230,
    "title": "Saving Lives (2004)",
    "originLink": "https://www.folklore.org/StoryView.py?story=Saving_Lives.txt",
    "originBody": "HomeAbout FolkloreThe Original Macintosh: 73 of 127 Saving Lives Author: Andy Hertzfeld Date: August 1983 Characters: Steve Jobs, Larry Kenyon Topics: Software Design, Inspiration Summary: Steve wants us to make the Macintosh boot faster We always thought of the Macintosh as a fast computer, since its 68000 microprocessor was effectively 10 times faster than an Apple II, but our Achilles heel was the floppy disk. We had limited RAM, so it was often necessary to load data from the floppy, but there we were no faster than an Apple II. Once we had real applications going, it was clear the floppy disk was going to be a significant bottleneck. One of the things that bothered Steve Jobs the most was the time that it took to boot when the Mac was first powered on. It could take a couple of minutes, or even more, to test memory, initialize the operating system, and load the Finder. One afternoon, Steve came up with an original way to motivate us to make it faster. Larry Kenyon was the engineer working on the disk driver and file system. Steve came into his cubicle and started to exhort him. \"The Macintosh boots too slowly. You've got to make it faster!\" Larry started to explain about some of the places where he thought that he could improve things, but Steve wasn't interested. He continued, \"You know, I've been thinking about it. How many people are going to be using the Macintosh? A million? No, more than that. In a few years, I bet five million people will be booting up their Macintoshes at least once a day.\" \"Well, let's say you can shave 10 seconds off of the boot time. Multiply that by five million users and thats 50 million seconds, every single day. Over a year, that's probably dozens of lifetimes. So if you make it boot ten seconds faster, you've saved a dozen lives. That's really worth it, don't you think?\" We were pretty motivated to make the software go as fast as we could anyway, so I'm not sure if this pitch had much effect, but we thought it was pretty humorous, and we did manage to shave more than ten seconds off the boot time over the next couple of months.World Class CitiesBack to The Original MacintoshStolen From AppleLogin Account Name: Password: Create new account Related Stories • Reality Distortion Field Rating Overall Rating: 3.90 (good) Login to add your own ratings 5 Comments Show Comments Add New CommentThe text of this story is licensed under a Creative Commons License.",
    "commentLink": "https://news.ycombinator.com/item?id=37211230",
    "commentBody": "Saving Lives (2004)Hacker NewspastloginSaving Lives (2004) (folklore.org) 317 points by compiler-guy 18 hours ago| hidepastfavorite283 comments jonahhorowitz 16 hours agoA story an old engineer at Apple told me:When working on MacOS 8.x (not sure which point release), they surveyed users, and their number one complaint was boot time. It took long for the system to boot (around 45s on average at the time). They looked into it but also asked the question, why do people care about boot times at all? At this point, the systems were capable of sleeping, so reboots should be rare.They found that people were rebooting because of instability, not just once a day or once a week. While they did improve the boot times, they put more effort into making the OS more stable. When the new release shipped, people stopped complaining about boot time, but not because it was vastly improved, instead because they were doing it less often.The moral of the story is to make sure you understand both what your customers are asking for and why your customers are asking for it. reply Someone 15 hours agoparent> When working on MacOS 8.x (not sure which point release), they surveyed users […] They found that people were rebooting because of instability, not just once a day or once a week.That didn’t require a survey. The OS didn’t have memory protection and typically got patched at startup by ten or so different extensions from both Apple and numerous third parties.The rules for patching were unclear, to say the least (1), so an extension might, for example, have a code path where it allocated memory inside a patch to a system call that might be moving memory around (a no-no, as the memory manager wasn’t reentrant)And that had to run code that typically was compiled with a C compiler of the time, with very, very limited tools to prevent out of bounds memory writes. reply pvg 16 hours agoparentprevApple&#x27;s customers had been screaming for better stability for years and Apple repeatedly tried and failed to deliver a meaningful solution. Even MacOS 8 introduced very limited memory protection that didn&#x27;t help much in most practical cases. In context, it&#x27;s really a story about an organization&#x27;s capacity and will to rationalize - this very nearly killed Apple as a business. reply musicale 8 hours agorootparent> In context, it&#x27;s really a story about an organization&#x27;s capacity and will to rationalize - this very nearly killed Apple as a businessWhat damaged Apple&#x27;s Mac business in the 1990s might been due to tunnel vision and self-delusion, but the driving issue was a loss of obvious differentiation vs. cheaper PCs running Windows. They were all beige boxes with a serviceable GUI that ran the same software, and customers didn&#x27;t see the value in paying Apple&#x27;s premium prices.With the return of Steve Jobs, Apple resolved the PC differentiation issue by refocusing on design in both hardware (iMac) and software (OS X); Apple also sidestepped Windows dominance by focusing on non-PC devices such as the iPod, iPhone, and iPad. reply jeffbee 16 hours agoparentprevThat certainly sounds about right. I definitely lost more time to the fact that a Quadra would freeze with high probability during a scan than I ever lost to intentional reboots. reply JKCalhoun 14 hours agoparentprevMac OS X took time to shut down though.When a friend first showed my wife Mac OS X and went to shut it down she frowned, \"That&#x27;s something I liked about the Mac, it would shut down instantly.\"\"You&#x27;ll have to find something else to like about Mac OS,\" he said. reply rob74 3 hours agorootparentWell, most of the time it&#x27;s not like you have to sit beside your (desktop) computer and wait until it has finished shutting down, so I guess that&#x27;s less of an issue than the startup time... reply ninkendo 13 hours agoparentprevIt&#x27;s an immutable law of the universe that consumer computers will always take at least 30-45 seconds to boot. If yours is faster, wait a few years... the developers will allow enough regressions to slip in that it&#x27;ll go back up again. reply bombcar 10 hours agorootparentEvery time I’ve had a computer that would boot faster than that, it seems I’d be stuck with a monitor that would take 30 seconds to come on and decide to display something. reply jimmaswell 3 hours agorootparentMy Kaypro 1 boots into CP&#x2F;M right away if the screen is already warmed up. reply interpol_p 6 hours agorootparentprevProbably because your monitor’s OS is booting reply bombcar 5 hours agorootparentI should start thingsthathaveanOSandshouldnt.net or something. reply quickthrower2 9 hours agorootparentprevThe hack around this would be to use your smartphone as your computer. Connect a keyboard to it. Do heavy stuff in the cloud. reply noduerme 8 hours agorootparentOr run the 68k mac OS 9 in emulation. It&#x27;s fun to watch it boot up like lightning. reply musicale 8 hours agoparentprevBy the time Mac OS 8.0 was released in 1997, Macs all had MMUs and Apple was already working on merging Mac OS and NeXTSTEP; the first iteration was released in 1998 as Rhapsody. reply otikik 16 hours agoparentprevTrust people when they report there&#x27;s a problem, but don&#x27;t trust them with the solution.Otherwise we would get faster horses instead of cars. reply munificent 13 hours agorootparentIf I remember right, in \"The Inmates are Running the Asylum\", Alan Cooper says there are two golden rules:* The user is always right.* The user is not always right.And then the explains the first point is that the user should be treated as the authority on what their problem is. You can&#x27;t just tell them they&#x27;re \"doing it wrong\" or rationalize away their pain.The second point is that users are not designers and shouldn&#x27;t have to be. They&#x27;ll often come up with ideas for solutions, but you shouldn&#x27;t take those as what needs to be done. reply Pannoniae 13 hours agorootparentThe first point is really common in programming. If you ask a \"stupid\" question, you don&#x27;t get an answer like \"here&#x27;s how to do it, but by the way, you could also do this instead\" but just flame you with \"you shouldn&#x27;t have been doing X\".Good example is FTP. Obviously, for anything requiring any kind of security, use SFTP. But I kid you not, almost all FTP-related questions on the internet have answers like \"are you still using that INSECURE protocol in 2020??\" without being constructive at all. Even if it&#x27;s just some random hobby project. Or a legacy system they can&#x27;t change. Doesn&#x27;t matter, it&#x27;s more important to score points from virtue-signaling than actually helping the poster. reply reaperducer 10 hours agorootparentEven if it&#x27;s just some random hobby project. Or a legacy system they can&#x27;t change.Or a modern system.My brand new image scanner only transfer files wirelessly via HTTP or FTP.People in places like HN freak out with \"Oh noes! Teh securities!\" But its wireless connection is as a wifi access point that only allows one client to connect, and only stays active for a few minutes.Not every computer is inside NORAD. reply deanCommie 9 hours agorootparentprevBill Hader puts it this way for writers&#x2F;artists&#x2F;creatives, and I think it applies perfectly here too:\"When people tell you that there is a problem, they&#x27;re always right.When they tell you how to fix it, they&#x27;re always wrong.\" reply neilv 8 hours agorootparentI like this, and I think it applies best to creative works, in which the creator is the expert on the characters and storytelling. (A screenwriter needs to know the audience isn&#x27;t feeling the romantic chemistry between the two leads, but probably doesn&#x27;t want to hear their steamy fanfic scenes.)But when designing solutions for people who are the domain experts (and this might even just be the domain expert of how a particular factory line works in practice), the dialogue with them likely includes their ideas for solutions. These ideas don&#x27;t have to be \"right\" as-is, but might suggest the right direction, or just be loaded with bits of relevant knowledge that inform whatever the solution ends up being. reply deanCommie 7 hours agorootparentValid! But I think it still works. Because when you&#x27;re designing software for domain experts, they know their domain, but they don&#x27;t know yours (software design)So yeah, they&#x27;ll say why something won&#x27;t work, but the solution will probably involve another button, another drop-down, a hamburger-menu, or another option in the settings.And then it&#x27;s your job to figure out if it&#x27;s the right solution. reply eviks 6 hours agorootparentprevWe&#x27;d also get intergalactic ships reply biogene 16 hours agorootparentprevThat&#x27;s a nice way to put it! reply tmpz22 16 hours agoparentprev> The moral of the story is to make sure you understand both what your customers are asking for and why your customers are asking for it.One reason engineers enjoy questioning the premise of a difficult feature is to avoid the work entirely. The problem with this is not that engineers are lazy its that the success metrics after the goal posts are moved can be futzed in a way that ultimately is detrimental to users.Did Apple really improve boot times and OS instabilities to a complete resolution or did an aspiring PM or Lead achieve the bare minimum of the goal to claim victory internally? reply neurocline 16 hours agoprevI must have heard this story and forgot it, because I used this argument on my team when I ran the group at Blizzard that did installing and downloading and patching. “We have 10 million people downloading and installing this patch, so every minute extra we take is another fraction of a human life we’re spending”. Sure, overly dramatic, and corny, but helped drive improvements.The other more important metric I pushed was “speed of light”. When installing from a DVD (yeah, olden times), the “speed of light” there was the rotational speed of the disc and so we should install as close to that speed as possible. Keep improving speed of operations until you butt up against whatever physical limits exist. Time is precious, you don’t get more of it. reply opportune 14 hours agoparentI wish more engineers thought this way. As someone who works in infrastructure it’s the story I tell myself to justify&#x2F;rationalize my place in the world. When I ship big infrastructure performance improvements it’s not about the speed or money saved per se, it’s less CO2 in the atmosphere and more human life (amortized over millions of people) spent on something other than waiting for a computer to respond.We aren’t doctors saving individuals’ lives but what we can do is give people fractions of their lives back. Some software is used by hundreds of millions or billions of people, so small changes there can save many “lives” worth of time. reply tazjin 15 hours agoparentprevBack in the day I was hacking on WoW-related stuff like server emulators, and it was always very noticeable how much care Blizzard put into this kind of stuff. The (iirc) torrent-based patch distribution for WoW etc. was really well done. Kudos, especially in such a high-pressure industry! reply whartung 8 hours agoparentprevHaving lived through the early horrors of WoW patches and updates, I have nothing but praise for how WoW works today in terms of updates and distribution.My favorite feature is how it supports incremental loading. WoW is a huge game, but you can start playing with a fraction of the assets. It will play in a downgraded way with place holders and lower quality assets as well as skipping entire areas completely.You can reinstall from scratch and be up and playing in minutes. It’s one of the hidden joys of the platform that players mostly take for granted, but I appreciate the no doubt legion effort involved to pull this off, to change the wheels on a moving train, and to deliver just uncountable amount of data with little drama and great performance.So kudos to you for whatever your contribution was to making such a core facility to the system so painless for the end user. reply hobs 16 hours agoparentprevThat last part is important. I have worked with many engineers who I would even classify as hard working, but spent little to no time understanding the hardware they were running on and the possibilities that it provided them.I have heard \"that&#x27;s slow\" or \"that&#x27;s good\" too many times in performance talks that have completely ignored the underlying machine and what was possible. reply TillE 15 hours agorootparentLearning about how the CPU cache works is probably the most useful thing you can do if you write anything that&#x27;s not I&#x2F;O limited. There are definitely a ton of experienced programmers who don&#x27;t quite understand how often the CPU is just waiting around for data from RAM. reply mcculley 13 hours agorootparentIt is a shame that there are not better monitoring tools that surface this. When I use Activity Monitor on macOS, it would be useful to see how much of “% CPU” is just waiting on memory. I know I can drill down with various profilers, but having it more accessible is way overdue. reply saagarjha 7 hours agorootparentInstruments? reply sakras 4 hours agorootparentInstruments is not nearly good enough for any serious performance work. Instruments only tells me what percent of time is spent in which part of the code. This is fine for a first pass, but it doesn’t tell me _why_ something is slow. I really need a V-Tune-like profiler on macOS. reply saagarjha 4 hours agorootparentI’ve used it professionally and generally been happy with it. What are you missing from it? reply sakras 3 hours agorootparentI’ve tried to use it professionally, but always end up switching to my x86 desktop to profile my code, just so I can use V-Tune.It’s missing any kind of deeper statistics such as memory bandwidth, cache misses, branch mispredictions, etc. I think fundamentally Apple is geared towards application development, whereas I’m working on more HPC-like things. replysaagarjha 7 hours agorootparentprevIt’s only useful once you understand how algorithmic complexity works, and how to profile your code, and how you language runtime does things. Before that your CPU cache is largely opaque and trying to peer into it is probably counterproductive. reply arrowsmith 14 hours agorootparentprevOkay, you&#x27;ve made me want to learn about it. Where do I start? What concepts do I need to understand? Any reading recommendations? reply Mockapapella 13 hours agorootparentHaven&#x27;t read through it, but I suspect this would be a good place to start: https:&#x2F;&#x2F;cpu.land&#x2F;HN Discussion: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36823605 reply hayley-patton 8 hours agorootparentprevDrepper&#x27;s \"What every programmer should know about memory\", though you mightn&#x27;t find it all interesting. https:&#x2F;&#x2F;gwern.net&#x2F;doc&#x2F;cs&#x2F;hardware&#x2F;2007-drepper.pdf reply llimos 13 hours agoparentprev> Time is precious, you don’t get more of it.In this particular example, the time saved on the download will go towards the noble cause of ... playing video games? Is that so much better use of time than the wait for it to download? reply niels_bom 13 hours agorootparentThat’s assuming people play more when the download is faster.And to answer your question: for everybody involved it’s better yes. reply npalli 16 hours agoprevSteve Jobs would always make up stuff (\"reality distortion field\") to motivate and push people. One of his famous stories that I found very funny --According to Mike Slade, he was working at Microsoft around 1990, and Jobs was trying to recruit him to NeXT. (Bear in mind that Microsoft was only a few years from launching its mega-hit Windows 95, while NeXT was struggling to sell computers.)During a conversation, Jobs told Slade he would find his talents wasted in Seattle. In contrast, Jobs called Silicon Valley a hub of excitement and activity where Slade could blossom.Jobs then launched into a spontaneous, impassioned speech. He described Palo Alto, California, as a “special place” and likened it to Florence during the Italian Renaissance. There was so much talent in the area, Jobs said, that you could walk down the street and bump into a scholar one moment, an astronaut the next.Jobs’ off-the-cuff description of the place bowled over Slade. It was a twist on Jobs’ famous pitch to Pepsi CEO John Sculley. (Jobs asked whether Sculley wanted to sell sugar water his whole life or join Apple and change the world.)After the talk, Slade agreed to pack up his stuff and move to Palo Alto.Jump forward a year, and Slade and his wife were eating in Il Fornaio, an Italian chain restaurant with a location on University Avenue in Palo Alto.“We were sitting there, in early ’91, and I’m reading the menu,” Slade recalled. “And on the back of the menu at Il Fornaio it says, ‘Palo Alto is like Florence in the Renaissance…’ And it goes through the whole spiel! The fucking guy sold me a line from the menu! From a chain restaurant!! Bad ad copy from Il Fornaio, which was his favorite restaurant, right? Such a shameless bullshitter!”https:&#x2F;&#x2F;www.cultofmac.com&#x2F;573753&#x2F;how-jobs-poached-a-microsof... reply thatfrenchguy 16 hours agoparentIt&#x27;s really funny when you think about how underwhelming Palo Alto is too. reply musicale 7 hours agorootparentIn harmony with Silicon Valley, a nearby university, and a popular fruit-named computer company, Palo Alto cultivates an arrogance and narcissism that I would not describe as \"underwhelming\".So I&#x27;m completely unsurprised that Il Fornaio&#x27;s pitch would resonate with Jobs and that he would quote it verbatim.Be that as it may, a number of cool things - and successful companies - have in fact come out of Palo Alto. And you can meet some interesting people there. I believe you can still catch Pac-4 football games there as well.Il Fornaio and Palo Alto might also have been better in 1990, when Jobs quoted his pitch. reply shostack 10 hours agorootparentprevDifferent strokes for different folks. I quite like PA in relation to other cities on the peninsula. It&#x27;s upscale, has a good mix of cuisines and fanciness&#x2F;expense for them, has interesting differing types of shops to explore on a few different streets so it isn&#x27;t just me main strip like Castro in Mountain View, or Laurel in San Carlos, and is generally safe and clean.I&#x27;m not sure what would make it more appealing to you, but it may be that you&#x27;re just seeking a different vibe or are at a different point in your life where you may not value some of those things the same. reply nine_k 9 hours agorootparent...but can you bump into an astronaut walking down a street? reply jonny_eh 15 hours agorootparentprevOren&#x27;s Hummus is pretty good ¯\\_(ツ)_&#x2F;¯ reply wmf 14 hours agorootparentprevRadiohead even wrote a song about it. reply yongjik 15 hours agoparentprevMan, Steve Jobs&#x27; Palo Alto must have been a truly special place. The only memorable thing I encountered in Palo Alto&#x27;s street (while working there a few years ago) was the overwhelming stench of urine in the underpass beneath the Caltrain Station. reply musicale 7 hours agorootparentThe Smell of Palo Alto! You know it too?More seriously, there really is an absurd shortage of clean, available, public restrooms that are open 24 hours. It&#x27;s a huge issue in the Bay Area and SF especially, but it&#x27;s bad in many US cities.At some point we seem to have decided that clean streets (and train stations) are not worth the changes in regulatory requirements and funding that would be required. Palo Alto actually has a pleasant and quaint waiting room from its Southern Pacific days - but of course it&#x27;s always closed, especially post-pandemic.I was shocked recently though when I was in a BART station that had a public restroom that was actually open and maintained. Seems like a good idea considering how often escalators are \"closed for maintenance.\" reply ryandrake 15 hours agoparentprevSeems kind of apocryphal. You mean to tell me a smart professional engineer working at one of the biggest and most prestigious (at the time) companies of the world is going to quit that job, uproot his life, and move to an entirely different state, just from a single \"Trust me, Bro, it&#x27;s awesome\" endorsement from a potential employer? I&#x27;d have wanted to at least fly down there, look at a few apartments, visit the office, and so on, before making that kind of commitment. It makes a cool story, but there must have been more to it. reply adrianmonk 13 hours agorootparentSteve Jobs, whatever else you want to say about him, had charisma. It&#x27;s a big part of why he was successful. So that&#x27;s kind of the point. He had an ability to take a message like \"trust me bro, it&#x27;s awesome\" and say it in a way that it would resonate, and that ability was most of the secret sauce of being Steve Jobs. reply pixl97 14 hours agorootparentprevEh, at least this short story did not say that. What it stated is the &#x27;hook&#x27; line that got him was pulled from a menu. Not that this guy didn&#x27;t at least to go Palo Alto first and make sure it wasn&#x27;t a total shithole. reply pokstad 16 hours agoparentprevThis is my new favorite Jobs story. reply john-radio 7 hours agoparentprev> \"reality distortion field\"Huh - I was about to post \"I just listened to that podcast!\" and bro out with you about Dan Carlin and Hardcore History, but it now occurs to me, and Googling confirms, that \"reality distortion field\" in that podcast was probably a reference to a known saying about Steve Jobs rather than an original thought. reply bombcar 5 hours agorootparentIt was a well known term, and even Apple fans would refer to the RDF (often as an excuse why something hadn’t turned out to be amazing as rumor had it). reply shortrounddev2 12 hours agoparentprevHe Keyser Soze&#x27;d him reply pengaru 15 hours agoparentprev> Bad ad copy from Il Fornaio, which was his favorite restaurant, right?Funny story, but I find it hard to believe Il Fornaio, with its mediocre Italian fare, was Jobs&#x27; favorite restaurant.This is the restaurant we&#x27;d go to when all other options were booked or it was too late to drive further. reply reidjs 14 hours agorootparentJust because he is great at business doesn’t mean he has great taste in Italian restaurants reply pengaru 14 hours agorootparentI&#x27;d actually argue there&#x27;s more evidence of Jobs having good taste in general than being good at business. reply elwell 14 hours agorootparentprevDo they sell fruit? reply DoneWithAllThat 13 hours agoparentprevIt’s a funny story, but… yeah, the early 90s was a special time in Silicon Valley. It was THE center of the computing world. And you really did just randomly bump into amazing people at Fry’s or restaurants or bars or whatever. I don’t think younger people understand how much around them today, when it comes to technology, can trace its roots to 90s South Bay and Peninsula. reply shortrounddev2 12 hours agorootparentI object to the idea that San Francisco, with its yuppie tech culture, was truly comparable to Florence in the Renaissance. The Renaissance produced works of culture and art in addition to the technological advances. In that regard, Seattle produced the best music of the decade and would be an equal contender to the title. reply racl101 11 hours agoparentprevHe sounds like a sociopath. I could believe him gaslighting Wozniak out of the money he should&#x27;ve paid him for the Atari gig. reply nonethewiser 10 hours agorootparentAs time goes on that seems to be how he’s remembered more and more. A weird psycho. reply DonHopkins 15 hours agoparentprevI would have moved there just for Chef Cho&#x27;s potstickers!https:&#x2F;&#x2F;www.paloaltoonline.com&#x2F;blogs&#x2F;p&#x2F;2018&#x2F;12&#x2F;04&#x2F;after-39-y...https:&#x2F;&#x2F;kellanskitchen.com&#x2F;menu&#x2F;chos-the-end-of-an-era&#x2F;https:&#x2F;&#x2F;www.masterstech-home.com&#x2F;the_kitchen&#x2F;recipes&#x2F;Interna... reply bhauer 15 hours agoprevProgrammers and engineers have to apply this thinking holistically. The totality of waiting for slow software is enormous. Performance needs to be given a higher priority by more development teams.I don&#x27;t tend to consciously sum all of the time I spend waiting on slow software and slow services. But waiting on slow software impacts my subconscious in the moment, making me feel uncomfortable and frustrated with the system, as if it is antagonistic. If I do spend any time consciously thinking about it, I feel disdain for the engineers and project leaders who believed that what they had produced was good enough to ship.With the processing capacity of modern computers, waiting for hundreds of milliseconds for trivial requests, or much longer for only modestly-complex requests is evidence of gross negligence on the part of the programmers. reply Nevermark 10 hours agoparentI commented elsewhere about ADHD. So here is a story about a myself I won&#x27;t name. O_oMy Thursday night girlfriend wanted me to clean up an old MacBook. Just a few steps, unlinking accounts tied to hardware, figuring out how to remove a firmware key some other me must have set, a clean install, updates, etc.It took me 6 months, because several steps or restarts took more than single digit seconds ... and my work was a siren.After many aborted attempts, I put it on my desk next to my keyboard. It only took 30 minutes, spread across 6 hours. Victory!If someone chained my hands to the laptop it would have gone faster, but the suffering incurred by the forced observation of blank screens, status bars and busy balls would have been unimaginable. reply phist_mcgee 7 hours agorootparentYou have a different girlfriend every night of the week? reply Nevermark 6 hours agorootparentThat would be something! But sounds complicated.I have a committed girlfriend, and another girlfriend who just wants a man for Thursday nights. I do what I can! reply khaledh 13 hours agoparentprevYep. Computers should wait for people, not the other way around (unless it&#x27;s a long running batch job). reply MichaelZuo 17 hours agoprevIt&#x27;s a pretty good point, ordinary computers could boot up from cold in under 30 seconds on 5400 rpm spinning rust, so why can&#x27;t they boot up in under 1 second on the latest and greatest NVMe SSDs? reply Syonyk 17 hours agoparentComplexity. Size.Windows 95 was about 50MB installed with most features.Windows 2000 fit on a CD for the install.Current Windows 10 installers won&#x27;t even fit on a single layer DVD anymore, and forget doing the install with a FAT32 USB stick (some older UEFIs won&#x27;t handle exFAT yet).The fastest computer I&#x27;ve ever used, perceptually, was a dual Pentium 3 866, with Rambus, booting XP (probably SP1 or so) on 15k U320 SCSI disk. The thing was telepathic. reply morelisp 16 hours agorootparentThe P3 era was really a golden age. Clock speeds were still rapidly doubling, you could get SMP but most people didn&#x27;t so everything had to optimize single-threaded perf, and likewise \"normal\" memory spanned 32MB to 512MB so you could really keep multiple programs&#x27; full working sets ready at once. reply water9 16 hours agorootparentI would&#x27;ve said P4 era with hyperthreading opening the door to multi-core programming paradigms. Clock speeds mostly capped around 5ghz since that era reply jorvi 16 hours agorootparentI’d rather call that the Athlon era. P4’s ran like (literal) hot garbage, Athlon’s absolutely crushed them. reply Affric 11 hours agorootparentAthlon was amazing. reply cmrdporcupine 7 hours agorootparentprevIf we&#x27;re going back to that time period... Alpha.How I coveted my roommate&#x27;s machine that had one in it. replycrazygringo 13 hours agoparentprevIcons used to be 32x32 monochrome with a mask. Now they&#x27;re 512x512 in 48-bit color. System fonts used to have ~200 characters, now they have tens of thousands.Extrapolate to everything else and it becomes pretty clear. There&#x27;s just so much more to load. reply MichaelZuo 13 hours agorootparentThe math indicates otherwise, as another user pointed out, a 9.54 MHz Tandy 1000RL could load to MS-DOS in 2.2 seconds with 512 KB of very very slow RAM and a very slow 20MB drive.Even factoring in 100x more resource usage for a 2023 computer to deliver all the features expected, it definitely should be way under 2.2 seconds. reply crazygringo 13 hours agorootparentYou&#x27;ve got to go way more than 100x. An 80x24 character screen used 2K of memory. Running two 4K monitors today uses 50MB of memory.That&#x27;s 25,000x more usage of memory for the interface alone. reply MichaelZuo 11 hours agorootparent100x total system resource demands.Display memory usage made nowhere near a straight multiplier of a difference even in 1989, as demonstrated by the 1000RLX vs 1000RL, which you would have known if you followed the link in the other comment and watched the video.You can verify this yourself by hooking up a VGA resolution display, the same as the 1989 Tandy 1000RLX shown, to a modern desktop computer with VGA out and it doesn&#x27;t reduce boot times by any significant amount. reply crazygringo 10 hours agorootparentIt was just one example.Total resource demands are still way, way, way over 100x. Data speeds. Peripheral inputs. Storage. Basically everything.And no, of course a computer doesn&#x27;t boot at a different speed depending on display size. It&#x27;s about the assets and code that fill displays of that size -- all the graphics you&#x27;ve got to load, all the code that has to draw the antialiasing and transparencies and shadows and subpixel font ligatures and everything else.Same way the code for dealing with storage capabilities is way more than 100x as complex. For peripherals. Etc etc etc. reply MichaelZuo 7 hours agorootparentIf you don&#x27;t fully understand the topic, the smart choice would be to re-examine your own assumptions.Do you understand how VGA displays or graphics rendering work? Or how computers boot up?A modern linux system, RHEL, Debian, etc..., isn&#x27;t going to try to load 4K graphics on a single connected VGA display, especially if you use it without any third party video drivers or adaptors that support 4K video out.Many motherboards, even in 2023, have a direct VGA out port that it defaults to, reliably. Which is what this typically refers to.If your still worried, then there&#x27;s always the option to manually verify the installed files and boot sequence to confirm that it isn&#x27;t attempting to force it. reply crazygringo 7 hours agorootparentI think you&#x27;re misunderstanding me. This has nothing to do with VGA. I was using 4K screens as just one example of the many, many dimensions of growth.It&#x27;s simply the point that there&#x27;s so much more to load during booting. Your contention that computers only use 100x more resources than in 1984, and should therefore boot in \"way under 2.2 seconds\" is way, way off.Computers use way, way, way more than 100x resources compared to two decades ago. Hence, booting still takes a bit of time. It&#x27;s pretty simple. reply MichaelZuo 4 hours agorootparentYou appear to have lost track of the conversation?> Icons used to be 32x32 monochrome with a mask. Now they&#x27;re 512x512 in 48-bit color. System fonts used to have ~200 characters, now they have tens of thousands. Extrapolate to everything else and it becomes pretty clear. There&#x27;s just so much more to load.If so, let me spell it out step by step. That was the initial reply to me. Therefore...> I was using 4K screens as just one example of the many, many dimensions of growth.This example, is likely close to meaningless, as elaborated on previously.Hence why I suggested to review whether &#x27;32x32 monochrome&#x27; or &#x27;512x512 in 48 bit color&#x27;, etc., has any observable effect. With the help of a VGA display showing, presumably, graphics roughly corresponding to the first, another display corresponding to the second, and so on.If you want to discuss something later on, then it should be in your interest to resolve the first claim as soon as possible in your favour.For example, if you disagree and still think resolution makes a noticeable difference, then show that convincingly, especially as it&#x27;s a positive claim, which HN readers tend to treat more critically.It really seems a bit odd to try to skip that discussion and then claim it &#x27;has nothing to do with VGA&#x27; which can only reduce the possible avenues to prove your credibility.i.e. You are the one who raised the possible \"to do\" regarding lower resolutions. The reason why I started discussing &#x27;VGA&#x27; at all was because of that comment. replymewse-hn 16 hours agoparentprevRecently, I was able to get a NVME SSD into an old dell (i5-4590) using a modified bios and a PCIE adapter card. It booted into fresh win 10 in seconds.I think it&#x27;s the old problem where the more crap windows accumulates, the longer it takes to boot. reply fatnoah 16 hours agoparentprevMy Windows 11 PC boots in about 20 seconds. Over half of that time is the POST. Once that&#x27;s done, I see the Windows login in about 5-10 seconds. It&#x27;s fast enough that I don&#x27;t really notice. reply jeffbee 17 hours agoparentprevMy NUC boots Ubuntu in 3 seconds flat, including POST. reply walteweiss 16 hours agorootparentHow is this possible? Is it super new? reply jeffbee 16 hours agorootparentThere were two things I had to do to shave the last few seconds, the most beneficial was disabling all the unnecessary peripherals in the BIOS. When I looked at the Ubuntu boot log it said it spend 1.7 seconds uploading firmware to the bluetooth controller, which at that point was like 95% of the post-POST boot time, and not needing that I just turned it off in the BIOS. reply the8472 16 hours agoparentprevMostly a matter of software not being written to make use of the SSD capabilities. You need parallelism or prefetching to keep the IO queues non-empty. If you have a single-threaded workload which interleaves blocking IO with CPU work and the IO patterns are not amenable to readaheads the SSD will be mostly idle. Similarly anything calling fsync or performing other file system operations that trigger synchronous writes on the critical path will stall the entire boot process. Due to caching writes are fast no matter the medium as long as you don&#x27;t demand instant durability. reply biogene 16 hours agoparentprevThey do, on the same workload. But if you look at the virtual memory breakdown, the vast majority of pages are non-executable data pages. Just did a rough check with Firefox and the executable pages are ~200MiB compared to ~2GiB of Private+Shared Pages. So its not so much the code, its all the data - the graphics, dictionaries, icons, fonts, textures, cached data, etc, etc. reply b20000 16 hours agoparentprevbecause parts of code these days is written in languages like java, python, etc which means at least some software runs slower.add to that that people think that because machines are faster they don’t need to optimize anything. reply amarshall 17 hours agoparentprevNot every part of the boot process is bottlenecked by disk I&#x2F;O. reply jdiff 17 hours agorootparentEvery part it&#x27;s bottlenecked is similarly exponentially improved from the olden days though. reply tempestn 13 hours agorootparentOne of the slowest parts of boot-up is memory checking, where the speed has increased exponentially, but so has the size. reply jdiff 12 hours agorootparentMaybe it&#x27;s the single slowest individual item, but it&#x27;s very far from being a significant fraction of boot time. And the capacity really hasn&#x27;t kept up the way speed has. My desktop has 24GB of DDR3 1600 and manages to post in under 2 seconds. And that&#x27;s pretty old by today&#x27;s standards. Mid level modern hardware runs at least a circle or two around this system in terms of speed, but in terms of capacity it&#x27;s still right in line with a higher end system today. Maybe I&#x27;m atypical but my boot time is dominated by my OS spinning itself up, by a long shot. reply tempestn 9 hours agorootparentI suppose it depends. My AMD DDR5 machine spends most of its boot time on memory training. Once that&#x27;s done it&#x27;s only a few seconds into the OS. (I know I could enable fast boot to skip that most of the time, but I rarely reboot and would rather have the guaranteed stability.) replyrocky1138 16 hours agoparentprevRelevant: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37212557 reply MichaelZuo 16 hours agorootparentThat is speedy, 2.2 seconds on a 9.54 MHz Tandy 1000RL!The ~4 seconds to boot up to a GUI desktop is actually even more impressive: https:&#x2F;&#x2F;youtu.be&#x2F;JIEPqD4luG8?si=9gVtFCIxFYma1erC&t=556My top of the line i9-9880H Macbook Pro from 2019, with PCIe 4.0 NVMe speeds, needs over 20 seconds to boot up in comparison... reply toast0 15 hours agorootparentI had a Tandy 1000 TL&#x2F;2; it had a tandy specific MS-DOS 3.3 with Deskmate setup in ROM, booted pretty darn fast; but you had to give that up if you wanted to boot a newer dos. A newer MS-DOS still booted quick, and there wasn&#x27;t much to the BIOS before it hit the drives, but you couldn&#x27;t run Deskmate on standard MS-DOS. reply rocky1138 10 hours agorootparentI realize at the time it wasn&#x27;t very easy for most people but a computer that often receives upgrades via ROM is the Atari ST. I (sort of) recently upgraded my 1040. I bet one could produce DOS 6.22 replacement ROMs for the Tandy! reply cocodill 16 hours agoparentprevmy fresh PC with 64GB DDR5 takes a minute til POST. reply dist-epoch 14 hours agorootparentDDR5 memory has this thing where it needs to be \"trained\" to figure out the best settings for a particular memory&#x2F;motherboard combination.Maybe your PC \"trains\" the memory every boot instead of just the first one.https:&#x2F;&#x2F;www.crucial.com&#x2F;support&#x2F;articles-faq-memory&#x2F;ddr5-mem... reply water9 16 hours agorootparentprevCheck bios for fast boot setting perhaps? reply beebmam 17 hours agoparentprevMy PC takes about 5 seconds to boot to be usable. reply pixl97 14 hours agoparentprevMost likely because that ordinary computer of that time wasn&#x27;t trying to bring up any network devices.Simply put, strip down an OS to the same feature set of that ancient computer and the modern OS will be a lot faster. Some of the networkless VMs I mess with boot in a second or two, but you see we&#x27;ve abstracted most of the hardware away. So, mostly the problem is a hardware one. reply MichaelZuo 11 hours agorootparentWindows 8.1 boots up in under 30 seconds on a 2012 mid-range Thinkpad X220 with the stock HDD. reply cmrdporcupine 17 hours agoparentprevNot my recollection, TBH. Yes, my VIC-20 or C64 turned on to immediate usability, but it had no spinning media or real operating system. My Atari ST took quite a few seconds to spin the floppy and dump to desktop. My next computer in the early 90s, a 486 50 running Linux I think would seem interminably slow to me now; Linux boot was faster than DOS&#x2F;Win3.1 but still we&#x27;re talking a big chunk of time.Honestly, things are much faster now than they used to be.Plus I can shut my laptop lid, use basically no power, and come back to my session as-is almost instantly. That&#x27;s new and way better than the 80s and 90s. Then you either had to leave the machine on or suffer slow cold boots. reply Symbiote 16 hours agorootparentRISC OS, the operating system that ran on the first few generations of ARM CPUs in the 1980s and early 1990s, was stored on ROM chips. It booted in a few seconds, to a real OS with a GUI etc.https:&#x2F;&#x2F;youtu.be&#x2F;5M6OIOIND-0?t=1278 — I think about 12 seconds, or which 3-4 is waiting for two hard drives to spin up. reply bluGill 16 hours agorootparentWhich is great and fast - until you want&#x2F;need to upgrade the OS. Security hole, too bad, that is baked into ROM and can&#x27;t be fixed... reply Symbiote 14 hours agorootparentRISC OS could selectively replace parts of the ROM (in RAM) with new code&#x2F;data, for upgrades, new device drivers and so on.(I think some viruses loaded themselves with this mechanism. And virus checkers.)https:&#x2F;&#x2F;www.riscosopen.org&#x2F;wiki&#x2F;documentation&#x2F;show&#x2F;File%20fo... reply bluGill 12 hours agorootparentSure, but everytime you need to do that boot time goes down and so what was the point? reply Symbiote 12 hours agorootparentIn practise I don&#x27;t remember this being a big deal. At some point I remember helping my dad upgrade us from RISC OS 3.something to 3.11, by replacing the ROM chips, but patches to the OS loaded into RAM were unusual.The OS in ROM was 2MiB, and looking at some module files intended for potential loading at boot time I have in an emulator, they are around 5-40kiB.The computers typically had 2 or 4MiB RAM, so there isn&#x27;t space to replace a significant amount of the OS anyway. (1MiB or 8MiB was possible, but unusual.) reply cmrdporcupine 10 hours agorootparentWow, that&#x27;s a large ROM. Even in their latest, 68030 based models, Atari never shipped a ROM bigger than I think 512kB (in the TT).If you wanted the full multitasking modern version of the OS (MultiTOS), you loaded parts of it from disk. Or ran SysV Unix (or NetBSD or Linux, later).Then again, binary sizes would be smaller on a CISC 68k machine I&#x27;d expect. replycmrdporcupine 13 hours agorootparentprevAtari ST also booted from ROM. But it also expected a floppy disk to be in the drive, to check for auto boot programs, etc. So that slowed the boot. If there was no floppy, it would hang for a while waiting for one, even. Poor choice. reply Symbiote 12 hours agorootparentIn RISC OS that was optional. There was a setting[1] in NVRAM which set whether or not to look for a boot device, and what that boot device was (floppy disc, hard disc, network).I don&#x27;t remember what happened if you configured it to look for extra boot files on a floppy disc, but the drive was empty. I think it would give up very quickly (1-2 seconds), as it was a normal way to load a program on the earlier BBC computers — insert the program disc, which would be bootable, and press the key combination (Shift+Break) to reset.\"Podules\" (expansion cards) could also map extra modules into the OS from their own ROM, usually the required device drivers for the card.[1] https:&#x2F;&#x2F;www.riscosopen.org&#x2F;wiki&#x2F;documentation&#x2F;show&#x2F;*Configur... reply cmrdporcupine 7 hours agorootparentOn the ST you could hit ESC to make it skip the floppy check, I believe. reply jameshart 16 hours agorootparentprevOn those old eight bits it wasn’t immediate either. You had to wait for the memory to all get zeroed out and for the CRT capacitors to charge. reply seeknotfind 17 hours agoprevIf you have to wait on a computer, it&#x27;s not fast enough.Steve&#x27;s argument here is widely used in the industry. It&#x27;s almost emotional blackmail (fail and be a killer) but classic nonetheless. reply dijit 17 hours agoparent> It&#x27;s almost emotional blackmail (fail and be a killer) but classic nonetheless.I read it much more as inspiring people to consider that they have an impact on peoples lives.It&#x27;s strikingly easy to blame the user for slow software, or blame the PM or Org for pushing features and speed of development over speed of the product.Steves mantra here is that software performance has a material impact on daily lives. Pointing something out is not emotional blackmail. reply JohnFen 16 hours agorootparent> Pointing something out is not emotional blackmail.True, but disingenuously implying that something like slow boot times costs lives is. reply dijit 16 hours agorootparentNot costing lives. But saving lifetimes across a population. reply JohnFen 14 hours agorootparent\"So if you make it boot ten seconds faster, you&#x27;ve saved a dozen lives.\"That&#x27;s emotional blackmail. The implication is failing to do that will cost a dozen lives. It&#x27;s also incorrect. Making it boot ten seconds faster saves zero lives. reply dijit 1 hour agorootparentI don&#x27;t know how to understand it for you.You could make claims like that about anything. You could equally claim \"Have a nice day\" is a threatening remark.You have to be spectactularly stupid to think that not making software faster is going to cost people their life.But it could, across a population, cost entire lifetimes worth of time.So yes, it&#x27;s important to consider your software&#x27;s cost to humanity.The issue here is that you&#x27;re rejecting a clear explanation of your externalities.Stop rationalising the externalisation of your costs; they still exist even if you don&#x27;t enjoy being confronted with it. reply xcdzvyn 10 hours agorootparentprevI&#x27;ll go out on a limb and suggest the clever people at Apple were aware making ill-performing operating system software isn&#x27;t going to literally kill people. reply icepat 14 hours agorootparentprevI can easily imagine a situation where life support hardware needs to reboot, and taking too long to do so would be life threatening. reply JJMcJ 16 hours agorootparentprevWasting people&#x27;s time. That&#x27;s a good enough reason. reply JohnFen 14 hours agorootparentSure, I agree. I just take issue with the framing. It&#x27;s highly manipulative. reply jonny_eh 15 hours agorootparentprevWhat is a life if not time well spent? reply xsmasher 15 hours agoparentprevThis dovetails with another Jobs story -> After the iPad launch, Jobs supposedly walked into a meeting with the Mac team, carrying an iPad. He woke up the iPad, which happened instantaneously. Then he woke up a Mac, which took a while to come out of sleep. Then he asked something like, “Why doesn&#x27;t this do that?Without the iPad there to show it was possible there would have been arguments about memory speed and disk speed etc. And faster Mac sleep&#x2F;wake put pressure on Windows to up their game. reply Krssst 17 hours agoprevIf this is valid, how about the countless animations everywhere in UIs today that waste time for no other reason than looking pretty the first hundred times? The application switcher on a phone I use has a switch time of 0.5s-1s with animations, practically instant without. reply jdiff 16 hours agoparentThere&#x27;s real UX benefit to it is why. Things instantly changing to entirely different layouts takes time to process visually, if things lerp to their new positions then that processing time is cut down to the length of the animation, which are usually around a quarter of a second, not half or a whole. It might get in the way of speedrunners and power users, feel free to disable them, but you&#x27;re not the target audience. It&#x27;s the average user who doesn&#x27;t have every UI nook and cranny burned into muscle memory. reply modeless 16 hours agorootparentIt&#x27;s a nice theory but it only works if the animations are smooth and designed to improve understandability. The vast majority of UI animations are pure visual flourishes that take twice as long as they should and don&#x27;t make any kind of sense spatially or physically or improve the user&#x27;s understanding of what&#x27;s happening at all. There&#x27;s a lot of cargo cult UI design out there.And what&#x27;s worse is that most of the animations either don&#x27;t start at the initial state of the UI or finish at the final state, or perform so badly that they hardly show any frames in between, so you have the worst of both worlds: abrupt jerky transitions and wasted time.UI transitions that make spatial sense, are fast enough, are fluid, and don&#x27;t slow down typical use of the UI are rare unicorns. reply jdiff 14 hours agorootparentI unfortunately 100% agree. While an amount of whimsy should be everywhere, animation shouldn&#x27;t be used as just eye candy. Like every other aspect of UI design, it has to be used with purpose and care. And yeah, that&#x27;s way rarer than it should be. reply yomlica8 15 hours agorootparentprevFunny. I&#x27;ve had people hovering over my shoulder comment how my PC is so much faster than theirs when it was actually an RDP session to another PC, which seems to disable almost all window animations by default. reply sedatk 13 hours agoparentprevNot all animations are useless. Actually, any useless animation has no place in the UI.- Some animations can be overlapped with time-taking tasks to keep user engaged but waiting at the same time. I think iOS does that when switching to an app that was swapped out to the disk. Loading takes time, so the animation compensates for some of the delay while the app&#x27;s resuming. If there was no animation, the user could think that they didn&#x27;t perform the action correctly, and might be inclined to repeat it, causing frustration.- Some animations are necessary to orient the user in UI flow. For example, the minimization animation moves the window to the icon that user needs to click in order to restore the app. The animation also makes user differentiate between close and minimize operations.- Some animations are necessary to give user proper feedback while keeping the responsiveness. One example would be the spring animation you get at the end of a list when scrolling using a touch screen. If there was no spring animation there, user would have no way to know that if that was the end of the list, or the touch screen stopped working. reply lucky_cloud 16 hours agoparentprevA lot of software also puts in some kind of input delay&#x2F;rate limiting for no apparent reason.Video game console system UIs and some game menus seem to be really bad about this for some reason. reply Too 15 hours agoparentprevCheap phones have terrible frame rate so they have to make the animations long to appear smooth.Imagine short animation in 200ms at 25fps only gives you 5 frames. It’s going to look janky and tacky. Make it 1000ms and it looks smooth and nice, except hopeless to use.(Unpopular?) solution: get an iphone. Their app switcher works as fast as your finger moves, with no problem of delivering consistent 60fps. reply Krssst 9 hours agorootparentThe phone in question has smooth animations. They&#x27;re just very slow as if to show how smooth and cool they are. It&#x27;s also from a very well-known brand. I could double animation speed with developer settings but even with this I felt like animations were too slow.Solution was to disable animations. However this sometimes breaks things. For a few month this broke multi-tasking (two apps on the same screen), but the vendor fixed it (though they reworked multi-tasking at the same time to prevent switching one app so it became almost useless for me at the same time)Unfortunately I like having the ability to install what I want too much to get an iPhone but I understand how people valuing stability may prefer it. reply xcdzvyn 10 hours agoparentprevI used to think there was something wrong with _two_ of my Plasma installs because everything felt generally sluggish. It wasn&#x27;t unusably slow, just enough to notice it.Turns out it&#x27;s because the animation speed was so low (default). I doubled it and everything feels 1,000x better. reply yreg 14 hours agoparentprevIf you are on iPhone, you can switchSettings -> Accessibility -> Motion -> Reduce MotionThe Android a11y menu probably has something similar. Try it out and see if you like it more. reply msephton 12 hours agorootparentYou can do it in a per-app basis. I turn it off globally and then turn it back on for Home screen, Books, and a couple of other apps. reply jeffbee 17 hours agoprevWindows 11 takes about 12 minutes to boot from an HDD. Imagine trying to boot it from an FDD.Installing Windows 11 and then waiting for all the updates to install on a HDD takes about 8 days. reply 1023bytes 16 hours agoparentCome on now, HDDs aren&#x27;t that bad.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=MpNagBwWlNk reply jeffbee 16 hours agorootparentWell that is not at all what happens on my machine, which is a Core i9 13900K with 128GiB of memory. It just grinds and grinds and grinds for ages. reply omnibrain 16 hours agorootparentWhy do you use a HDD? Of course, I ask in jest, but I&#x27;m also a bit curious. reply jeffbee 16 hours agorootparentI have a very excellent SSD which I removed from that system because I am certain that the Windows installer would fuck it up, and I did not want the hassle of trying to fix it. So I pulled it out of the box to keep it from harm, and the only media I had at hand at that moment was a WD SATA HDD. I thought it would be slow, not kill-me-now slow.I do not \"use an HDD\" of course. It was improvisational. reply deathanatos 15 hours agorootparentprevThe same reason anyone has always used a HDD? … they&#x27;re dirt cheap, compared to SSDs.I&#x27;d consider hybrid being the best cost option, with a small SSD backing frequently used data, like the OS. But there&#x27;s more complexity in that setup. I&#x27;m also a Linux user, and boot times don&#x27;t bother me. reply xboxnolifes 12 hours agorootparent> they&#x27;re dirt cheap, compared to SSDs.If you need very large (4TB+ drives) maybe, but 1-2TB SSDs are so cheap nowadays. 2TB SSDs today are cheaper than 2TB HHDs from 10 years ago, and the price discrepancy is quite narrow unless you&#x27;re looking at 4TB+ drives.I don&#x27;t even bother looking at HHDs for my own computers anymore unless I need bulk storage for videos or something. reply pixl97 14 hours agorootparentprevDirt cheap in which measure?I was at Microcenter and some 1TB (rather questionable) NVMe drivers were $30 on special. Going to be difficult to get cheaper than that.Now, lets turn your equation around. What is the cost per IOPS of your HDD versus SSD? HDDs start to get expensive very fast in that measure. reply deathanatos 14 hours agorootparentIn terms of $&#x2F;B.Yes, HDDs are slower than SSDs. If that axis matters to you, you&#x27;d use an SSD, particularly NVMe. (Which is sort of implied by the hybrid setup I suggest.) If storage capacity matters, HDDs. You can see this reflected in market prices, though it does look like SSDs are surprisingly cheap these days, comparatively. Historically this has not been the case. (I wonder if economies of scale are now working against HDDs suddenly, or what? There&#x27;s no reason for them to cost the same or more than an SSD — the market would collapse. Although I swear market pricing for many components hasn&#x27;t made a lot of sense, recently… i.e., RAM has seemed horrendously expensive.) reply toast0 13 hours agorootparentThere&#x27;s some sort of big SSD price drop in the past 3 months. I dunno what that&#x27;s about, but I did upgrade a machine, so that&#x27;s nice.There does definitely seem to be a pricing mechanic in that hard drives never really scaled down in minimum unit cost; the basic parts of a hard drive still cost real money, so if you can do 2TB per platter, and a top of the line drive has 10 platters, a single platter 2TB drive costs a lot more than 10% of the top of the line drive. OTOH, flash controllers aren&#x27;t that expensive and&#x2F;or the cost of the controller scales with the capacity, so SSD prices tend to be more linear with capacity.If you need a lot of space, $&#x2F;B means a lot, but if you just need an ample amount of space, $&#x2F;device is more important, and SSD drives have hit the point where an ample amount of space is available for less than any hard drive. reply XTHK 13 hours agorootparentprevThe most efficient cost option is to have one cheap SSD for booting and a handful of apps that need the speed and then using a HDD for storage. Been that way for 10+ years reply judge2020 13 hours agorootparentprevThe slowness you see with NVME isn&#x27;t in boot anymore - instead it&#x27;s in BIOS. As memory times get faster, it takes longer for the motherboard to train to hit those XMP targets, especially with memory still super far away from the CPU. For me, rebooting has ~20 seconds of staring at a blank screen with the Motherboard doing memory training&#x2F;initialization on 6000 MHz ram. reply deathanatos 15 hours agorootparentprev… that video really doesn&#x27;t sell Windows very well. My Linux laptop boots ~40s faster. reply wsc981 16 hours agoparentprevI fucked up the partitions on my 2017 iMac with Fusion drive a short while ago trying to create a dual boot system and even since my Mac was slow.I think from beginning of start-up to a somewhat usable system was maybe 5 minutes? Quite long either way.But just last weekend got sick of the slowness and found there&#x27;s a &#x27;diskutil resetFusion&#x27; [0] command that restores the partitions to the default. So I ran this command, reinstalled the OS and now my iMac is pretty speedy again. Not great mind you, but way better then before.Lesson learned: dual boot on a Fusion drive is a bad idea.---[0]: https:&#x2F;&#x2F;support.apple.com&#x2F;en-us&#x2F;HT207584 reply biogene 16 hours agoparentprevNot seeing those boot times, but I rarely reboot. I usually reboot my W10 box once every few months or so. Our IT department commissions our Windows PCs in about an hour. Something seems very very wrong here, but I&#x27;m not an IT expert. reply pixl97 14 hours agorootparentIf you&#x27;re not rebooting your W10 box every month, then every time you do reboot you&#x27;re doing windows updates. reply biogene 12 hours agorootparentTrue, but our IT only lets important&#x2F;critical updates through, so its not really a big burden. reply water9 16 hours agoparentprevThey must have a lot of \"Telemetry\" to collect on you. reply mixmastamyk 16 hours agorootparentI thought telemetry was supposed to improve the experience. Not make it worse? reply lowercased 15 hours agorootparentIt improves \"the\" experience; not \"your\" experience. ;) reply JohnFen 14 hours agoparentprev> Windows 11 takes about 12 minutes to boot from an HDD.Mine doesn&#x27;t. It takes 3-4 minutes (which can easily feel like an hour). reply objektif 16 hours agoparentprevThis cannot be true. reply alephxyz 16 hours agorootparentI have a W10 install on a 7200rpm HDD and I believe it. reply justsomehnguy 15 hours agorootparentprevCan confirm the boot time with Win10 on HDD. Can&#x27;t argue about updates, took about a couple of hours, def. not days. reply hosteur 17 hours agoparentprevWow. That is outrageous. reply jeffbee 16 hours agorootparentI only know this because I needed to use a utility from Asus to update the Intel ME, and it only runs under Windows. I naively assumed it would not be that much trouble to throw a hard disk that was laying around into that PC and install Windows thereupon. reply gsich 16 hours agorootparentprevAlso false. reply hospitalJail 16 hours agoparentprevTry to remove Windows&#x2F;Microsoft from your life, Microsoft no longer is decent.We need to migrate to Linux. reply toast0 16 hours agoparentprevI&#x27;m kind of surprised Windows 11 allows you to install to or run from something that isn&#x27;t an SSD. Windows 7 ran just fine on spinners, but Windows 10 is pretty bad; I&#x27;m not surprised Windows 11 is worse, but they really should just disallow it. reply soupfordummies 16 hours agorootparentMaybe they shouldn&#x27;t have such resource-creep that REQUIRES an SSD. Maybe they WOULDN&#x27;T if there weren&#x27;t mountains of bloatware and telemetry bs. reply toast0 16 hours agorootparentI mean, that would be great; but if nobody is holding the line on resource-creep, as is obviously the case, and nobody is testing if releases are acceptable on HDDs, as is obviously the case, they should just change the published requirements to reflect reality. reply wtallis 15 hours agorootparent> they should just change the published requirements to reflect reality.Marketing won&#x27;t let them so long as it would piss off PC OEMs that still ship crappy systems and want to use the Windows logo. reply toast0 14 hours agorootparentShouldn&#x27;t be a hard sell for OEMs; official specs are 64 GB is enough storage for windows 11, and I can get a 128 GB SSD for $15 retail, whereas the lowest price hard drive I can find is $25 retail (500 GB, but 3.5\"), so if you&#x27;re a cheap PC OEM, putting in a crappy, tiny, SSD saves money. And the only systems without SSDs I saw on BestBuy were refurbished machines shipping with Windows 10. reply pixl97 14 hours agorootparentprevWhy shouldn&#x27;t they? People don&#x27;t buy operating systems to be slimmed down...If you got a computer and it didn&#x27;t come with all the needed drivers and a web browser along with most of the functionality needed to print, you&#x27;d most likely wonder what decade it came from. All that stuff I listed, without the telemetry is still going to run like dog shit on a HDD.I honestly think users are forgetting just how badly fragmented hard drives of days yonder used to run, and those same spinning disks are not any faster these days. Cutting down the OS to barely do anything still took more time than the complete boot cycle of my current computer up to a browser on an SSD. reply toast0 13 hours agorootparentYeah, hard drives are never going to be great (although 15k rpm drives aren&#x27;t too bad), but IMHO, the real thing that causes perf to be awful is that windows 10 (and I assume 11 has gotten worse) can&#x27;t seem to ever stop writing to the disk. Those writes seem to interrupt reads enough that you never can get good sustained read speeds, so loading anything is painful.I&#x27;m not going to setup a system to test, because it&#x27;s too painful, but I&#x27;m now idly wondering if you could set the checkbox on a hard drive for \"Turn off Windows write-cache buffer flushing on the device\", and if that would help. Doing a aggregated write of a couple MB once a minute would probably work better than doing a few KB every second. Of course, at great risk of data loss, but YOLO. (a smidge of research seems to indicate this is for asking the device to pretty-please flush its internal write cache, so that might help a bit, but probably not very much; maybe there&#x27;s a knob somewhere to tune the system file cache) replyjjkaczor 16 hours agoprevWell... I remember some press and discussion about \"InterBase\" (now FireBase) - and it&#x27;s storage&#x2F;self-healing recovery model being critical for some scenarios \"back in the day\", some quotes:\"AFATDS includes 935,000 lines of Ada code, running on an HP RISC Workstation and the Army&#x27;s Light Weight Computer Units,\" according to John Williams, spokesman for Magnavox Electronic Systems Company, the prime contractor on the project. \"We needed to work with a single database that could scale and operate across Unix and PC platforms. The product also had to install quickly and provide high availability without monopolizing our systems resources.\"\"Decision support of this nature requires a modular and flexible architecture that would support both distributed processing and distributed databases. That&#x27;s why we chose InterBase. It out performed the competition and convinced us that it would be reliable in life and death situations.\"The exact nature of the discussion was that in some situations, the firing of the main weapon in certain tanks would generate an internal EMP event, so systems would reboot - they had to have extremely fast reboots and recovery-times... so they could fire again... reply stevenfoster 13 hours agoprevIf only he knew how many millions of lives would be lost indefinitely scrolling on a small sheet of glass. reply cmancini 8 hours agoprevReminds me of this scene with Stanley Tucci talking about building a bridge in the movie Margin Call: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=m8Mc-38C88g reply Justsignedup 16 hours agoprevErrr my biggest shock and awe moment was Guild Wars 2. A bit after launch I was playing and an update came in. \"Please restart the client now after patching\"Okay... Let&#x27;s click that button!!!!Game... shuts down... downloads an update... patches... starts up... loads me back into where I was.All this in... 1 minute flat! Baldur&#x27;s Gate 3 can&#x27;t do that on today&#x27;s hardware with an SSD and a much faster processor, and 4x the ram, compared to a game 13 years ago on significantly crappier hardware.That&#x27;s what solidified to me that the game was rock-freaken-solid. reply mbork_pl 17 hours agoprevSort of related: https:&#x2F;&#x2F;www.hillelwayne.com&#x2F;post&#x2F;performance-matters&#x2F;. Well, not really - that one is about saving lives with performant software, but more literally. reply leo150 14 hours agoprevI recommend reading “Revolution in the valley” by Andy Hertzfeld, who is also the author of this story. The book is a compilation of all stories from folklore.org including more interesting details about development of the Macintosh. reply varjag 14 hours agoprevThis is also a great argument for power saving. Shave a Watt or two of consumption from your mass market device or application, and suddenly you&#x27;ve saved hundreds of Megawatt-hours over the years. reply titaniumtown 17 hours agoprevIt&#x27;s very fascinating how small amounts of time people take to do&#x2F;wait for something add up over a huge population. reply euroderf 4 hours agoprevIn a comment here: \"We have 10 million people downloading and installing this patch, so every minute extra we take is another fraction of a human life we’re spending.\"Following the logic of an eye for an eye, the failure of developers to remove long waits in mass-market software products should be punishable by... death ? reply bombcar 5 hours agoprevIf we took all the lives lost to waiting for Jira, we’d have the time to speed up things like Jira. reply JJMcJ 16 hours agoprevI certainly wish Windows would do something about these endless boot and even worse shutdown times.Even worse, I want to go home, not wait 30 minutes for updates to install. reply overgard 16 hours agoprevI think about this a lot whenever I&#x27;m waiting on a long compile. How many lives has complicated template metaprogramming in C++ taken? reply yellow_lead 14 hours agoparentI was reading this while compiling reply teo_zero 16 hours agoprevNicely played on the double meaning of \"save\". Couldn&#x27;t be done in every language. reply huy77 15 hours agoprevIt’s not Steve. It’s the engineers who care about saving lives. I have tried to pitch the idea of saving lives to different people. Many of them think it’s nonsense to care about other people business. reply Nevermark 10 hours agoprevIt probably saved the careers of thousands of ADHD people, who just wanted to start working before they got distracted! reply amatecha 14 hours agoprevAh, funny, I just shared this link in a comment a couple weeks ago: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37053941It&#x27;s applicable in conversation so frequently around software&#x2F;computers as it reflects a really empathic mindset that I feel is becoming more and more rare... reply FBT 14 hours agoprevThere are about half a million minutes in a year, so 50 million seconds is a year and two thirds. At the rate of saving 50 million seconds a day, in a year you&#x27;ll have saved around 608 years—which is only a dozen lifetimes if a lifetime is around 50 years. Still, that&#x27;s a pretty close approximation for an off-the-cuff guess. reply msephton 12 hours agoparentI&#x27;m sure he&#x27;d have planned or thought about this before hand.Steve&#x27;s famous \"computers are a bicycle for the mind\" was refined over a long period of time and countless interviews. We only hear about the one time where he perfected it, where it made an impression. Many other instances are on YouTube, in one you can see him trying out different alternative lines. reply datadrivenangel 13 hours agoprevI wish someone at microsoft would do this for o365. Losing 2-5 seconds any time I click a link is painful. reply divbzero 15 hours agoprevThis is good bullshit because it’s close to the truth. Not quite a dozen lives but order of magnitude right: (50,000,000 seconds saved per day) ÷ (60 seconds &#x2F; minute) ÷ (525,600 minutes &#x2F; year) ≈ 1.6 years saved per day (1.6 years saved per day) × (365 days &#x2F; year) ≈ 580 years saved per year reply d0100 9 hours agoprevThis story really drives in the Bill Burr joke about Steve Jobs real legacy being all about screaming at engineers to do stuff reply alex_suzuki 16 hours agoprevI remember my MacBook booting up lightning fast in 2013 (Leopard? Earlier? Dunno). Those days are gone. reply msephton 12 hours agoparentMy M1 MBP on Monterey boots in a few seconds. reply firebirdn99 13 hours agoprevI didn&#x27;t know this before, but it&#x27;s cool that originally back then Apple&#x27;s directory explorer was still called &#x27;Finder and it&#x27;s not changed since. reply tpmx 17 hours agoprevI wonder what he would have said about the 20 minutes when you can&#x27;t use the computer and the 1+ GB download it takes to update a state-of-the-art mac from macOS 13.5 to 13.5.1 that has one (1) bug fix (\"macOS Ventura 13.5.1 fixes an issue in System Settings that prevents location permissions from appearing.\")https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37206660I miss having Steve running Apple. reply throitallaway 16 hours agoparentComing from the patching experience on various Linux distros (and even Windows), I really want to know what Apple is doing under the hood with macOS updates. Their point updates are multiple gigabytes and often take 20-40 minutes to install. My Arch system updates itself in a couple minutes (even if I haven&#x27;t updated for a month) and there&#x27;s no \"unusable\" phase of the upgrade process, other than a normal reboot for kernel updates. reply NobodyNada 15 hours agorootparentA few years ago, they moved the OS to a “sealed system volume” — basically, the entire OS is stored on an immutable disk image, signed and verified with a Merkle-tree sort of structure. This has a few advantages: malware cannot modify the OS, you can’t brick your system by accidentally deleting OS files, updates are far more robust (they don’t have to change files on your root filesystem), and the OS can be stored unencrypted meaning you can boot the system without requiring the user’s password first. (And of course, there’s an opt-out if you really want to modify OS files.)The big downside is that installing an update means you have to rebuild and re-sign the entire OS image, which takes forever. When they first introduced this model, I was surprised at this: I expected they could generate the new OS image in the background, while you’re still using the computer, then just swap over to the new image with a single reboot, instead of requiring a ton of downtime. I think they might finally be doing this with macOS 14&#x2F;iOS 16 — I’ve been running the betas for both and noticed restarting to install updates has become far, far faster — like maybe a minute or two. reply tpmx 14 hours agorootparentI’ve been running the betas for both and noticed restarting to install updates has become far, far faster — like maybe a minute or two.Nice! (And thanks for the backgrounder. It&#x27;s the first time I&#x27;ve seen this explanation on HN.) reply diskzero 16 hours agoparentprevI assume he would say the same things I would hear him say in meetings where the installer team would show him the lastest versions of the application. A special memory comes from the time where the installer progress bar starting going in reverse. The installer and mail teams received a lot of abuse. It took a special person to stay motivated given all of the challenges they faced and the feedback they got from SJ. reply tpmx 16 hours agorootparentAs a customer (my personal computer&#x2F;display&#x2F;phone&#x2F;etc spend with Apple over the past 20 years or so: $25k+): I would prefer having someone in charge who can tell&#x2F;understand&#x2F;sense and say that something is clearly not good enough and then actually getting it solved. Tim Cook does not strike me as that kind of guy.The abuse isn&#x27;t required though. reply tpmx 13 hours agorootparent(These extremely slow updates thing has been going on for many years now. So many lifetimes wasted.) reply fjni 13 hours agoprevWhat Andy giveth, Bill taketh away. reply didip 14 hours agoprevJust load an addicting easy game during boot process. Then users won’t even notice :) reply sedatk 13 hours agoparentWhen our website was down due to maintenance, we used to run JSTetris on the error page, so people would stay on the page, and they would get redirected to the web site as soon as the maintenance was over.Some people even complained that they shouldn&#x27;t be redirected automatically because they&#x27;d lost their progress :) reply sbuk 13 hours agoparentprevGames on the Commodore 64 started doing that in the early 80s. Loading from the cassette could take ages, so the devs would put in something like space invaders or missile command to entertain while the user waits for the main event. reply msephton 12 hours agoparentprevNamco had the patent on that. reply drivers99 16 hours agoprevI was hoping it would explain what they did to speed it up. reply DonHopkins 15 hours agoprevBack when most everybody ran Connectix RAM Doubler and Connectix Speed Doubler on their Macs (which actually worked!), I was praying for Connectix to release Boot Doubler, that made every other boot instant!https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Connectixhttps:&#x2F;&#x2F;apple.fandom.com&#x2F;wiki&#x2F;RAM_Doublerhttps:&#x2F;&#x2F;apple.fandom.com&#x2F;wiki&#x2F;Speed_Doublerhttps:&#x2F;&#x2F;68kmla.org&#x2F;bb&#x2F;index.php?threads&#x2F;connectix-speed-doub...https:&#x2F;&#x2F;www.betaarchive.com&#x2F;forum&#x2F;viewtopic.php?t=31852https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=21768641 reply karol 15 hours agoprevNostalgia this and that... in 1983 I had a calculator and flew to Venus... reply zython 16 hours agoprevI never understood why people calculate time savings like this. Similar for a developer 1 times 5 hours yields not the same producitiy&#x2F;results as 5 times 1 hours, due to \"context switching overhead\" for example.Claiming you saved a couple of lifetimes when all you can gain is a couple of seconds is so misleading. reply haswell 16 hours agoprevThis reminds me of the “XY Problem” framing [0], a concept that has been very helpful over the years when communicating with customers about feature requests.Many people can imagine how they’d solve an immediate problem, but never pause to examine whether or not this solution is ideal, or generalizes beyond a specific situation.Another phrase that comes to mind is “fall in love with the problem, not the solution”. If you understand the problem space deeply, either many solutions can emerge, or one solution emerges as clearly the best place to focus.In my years as a product manager, it surprised me how many PMs don’t think this way, and just tack on feature after feature, convinced this is the best thing for the customer, when often the thing they need is not something they know how to ask for.- [0] http:&#x2F;&#x2F;xyproblem.info&#x2F; reply sebzim4500 16 hours agoparent> This reminds me of the “XY Problem” framing [0], a concept that has been very helpful over the years when communicating with customers about feature requests.It also ruined stackoverflow, since replies which ignore the question and assume that the OP really meant something else end up being so much easier to write&#x2F;vote on than an actual answer. reply adamc 14 hours agorootparentMany, many things are wrong with stackoverflow. Insisting that every discussion be factual and opinion-free pushes you deep into the McNamara fallacy of believing that things that cannot easily be quantified don&#x27;t matter.It&#x27;s a site I sometimes use but dislike intensely. reply sanderjd 13 hours agorootparentI think the absolute hardest thing to get information on is \"I have XYZ problem, I am aware of solutions A, B, and C. What is the best solution among these, what are the trade-offs between them, and what solutions am I not aware of?\". Now, this is just a truly difficult question, but Stack Overflow solves that problem by forbidding such questions, which is understandable, but I think also a shame. At one point in time, I thought maybe Quora would try to fill this gap, but they went off in some other direction that I never understood. Most other \"social\" things (reddit, etc.) are discussion rather than Q&A. Or they are blog posts, where the focus is usually on solution A, with solutions B and C presented only for contrast, because solution A is what motivated the author to write the post.I kind of want Wirecutter, but for technologies. reply adamc 13 hours agorootparentYes, that would actually be much more useful to me than what stackoverflow is. A vast number of the questions found that can be easily answered by RTFM and&#x2F;or doing some direct experimentation. The harder ones would be more useful. reply sanderjd 13 hours agorootparentYeah. I think it&#x27;s also why chatgpt (and copilot, etc.) actually did turn out to be a strong SO competitor, because it actually can do a pretty good job on these factual questions.But unfortunately it&#x27;s pretty bad at this other kind of judgment-based compare-and-contrast question. It&#x27;s especially bad at the \"what other solutions am I not aware of?\" part, because it isn&#x27;t kept up to date. reply hunter2_ 13 hours agorootparentIsn&#x27;t Bard kept up to date? reply nonameiguess 13 hours agorootparentprevYou seem to be trying to replace a basic peer-review of an engineering design that typically involves a paid team with advice from poorly-known, pseudonymous strangers with reputation crowd-sourced from a web site&#x27;s user-rating system.Frankly, I think that&#x27;s asking a bit much. If you want a high-quality peer review of design proposals to bounce ideas off of others and discuss tradeoffs, you need a team. Maybe something like a meetup group or mailing list for a specific technology, programming language, or industry sector. But it goes beyond one-off Q&A, and I can also understand why Stack Overflow, with a goal of becoming a repository of perpetually useful knowledge that is general enough to be useful for anyone into the indefinite future, does not want to host such project-specific discussions.Why not just develop in the open and collaborate explicitly with other parties also working on the same project? What you&#x27;re asking for sounds close to something like the various special interest groups and public discussion of improvement proposals you see in things like the Python programming language or Kubernetes, or discussion on LWN about specific challenges the Linux kernel team faces. reply NoMoreNicksLeft 12 hours agorootparentI don&#x27;t think that&#x27;s asking a bit much.If it were, then there&#x27;d be no reason to prohibit such questions... people wouldn&#x27;t ask them, because they would never be answered. The only reason to prohibit them is because they would get attention&#x2F;answers where none was desired.The trouble with StackOverflow, is that what the users want and need does not match what the owners want. The owners want something monetizable, something that can look elegant and beautiful (hence the PR release a couple of years ago where they were positioning it as some \"encyclopedia of computer science\" or whatever). They figured out that the users could be denied what they want, while still (slowly) creating what the owners themselves wanted.> and I can also understand why Stack Overflow, with a goal of becoming a repository of perpetually useful knowledge that is general enough to be useful for anyone into the indefinite future, does not want to host such project-specific discussions.I&#x27;m not sure I&#x27;d characterize them as wanting that, but if they did... how would that be at all useful to anyone except CS undergrads trying to get someone to do their homework for them? Literally nothing of what people ask there day to day will be generally useful into the indefinite future. What do you want to ask, that will be useful 40 years from now? Neither anything language specific, nor anything domain specific will be relevant to anyone not a historian. Even the cutting edge stuff today will have long since been wrapped up into some blackbox library that everyone will use without unerstanding it.If you were correct, SO could never be anything more than some useless little dumpster where the same 5 people whine n about the quickest sort algorithm. reply AlbertCory 11 hours agorootparentAll true, to which I&#x27;d add:It&#x27;s like giving a big sales force a financial incentive: you have to be careful because they&#x27;ll just game it, relentlessly, all day long. They won&#x27;t care about your corporate priorities -- just getting that incentive money.On SO, people get \"reputation points.\" Those \"same 5 people\" game that system like salespeople winning that prize. You answered a question? They don&#x27;t want you as a competitor, so they downvote you. You don&#x27;t like their answer? Too bad, you don&#x27;t have enough reputation points to downvote them.To pick another analogy: they&#x27;re like high school cheerleaders voting on who can become one of them. reply bee_rider 13 hours agorootparentprevThis is the most techie social media site I use, and I see constant complaints about the other techie social media site, StackOverflow. Why doesn’t someone test the theory and come up with some competition?I think this is normally an unreasonable ask (when we’re complaining about, like, cars, clearly that’s not in this site’s aggregate wheelhouse). But I mean this is a website about start-ups, full of techie web-devs complaining about a website that they all use. reply Aeolun 11 hours agorootparentI think it’s because Stack Overflow is so close. It does 80% of what you need with a bunch of major warts.But nobody wants to duplicate the 80% just to solve the warts. reply seedboot 12 hours agorootparentprev> Why doesn’t someone test the theory and come up with some competition?ChatGPT Has entered the chat. reply spencerchubb 13 hours agorootparentprevI don&#x27;t see that as a bad thing. Stack Overflow only wants to focus on questions that have a verifiable answer. Other types of questions still matter, they just don&#x27;t matter on Stack Overflow. reply haswell 16 hours agorootparentprevLike anything, it needs to be applied appropriately, and I agree that blindly redirecting every request to this framing is not helpful.But the number of times that it is helpful has been pretty high for me over the years. This probably depends a lot on the customer’s own ability to comprehend the true nature of the problem. I worked in the enterprise&#x2F;B2B space, where a significant number of requests came from people not technical enough to fully know what to ask without some deeper exploration. reply hooverd 16 hours agorootparentAgreed. But sometimes, especially if you know about your problem domain, it feels like asking \"how do a I keep water out of my basement\" and all the answers are \"simply rebuild your house at the top of the hill.\" reply hunter2_ 13 hours agorootparentIt&#x27;s a matter of vastly different costs, in that case: the solution to the modified problem costs much more to solve than the originally stated problem. The trick is avoiding such a large gap, hopefully with a breakeven that comes in the foreseeable future, if not immediately.For example: how do I repair water damage on my ceiling in a way that&#x27;s quick enough to do it after every storm? You mean how do I repair my roof so I only have to repair the ceiling one more time? It&#x27;s more upfront cost to do both now, but the breakeven is only a small handful of storms away, which is palatable enough to get serious consideration. If the breakeven was (for some reason, hypothetically) 20 years away, actually figuring out how to make quick work of repeated ceiling repairs might be more desirable. reply jameshart 16 hours agorootparentprevAlso when, for example, someone suggests a strategy that is useful in scenario X, but because it can be problematic in scenario Y, they get a bunch of replies warning them about that - even though they had no intention of advocating applying it in scenario Y. That’s also a kind of XYing - “oh don’t do that, it’s bad if you re trying to Y…” when we’re not, we are trying to X.For example, when someone says they think the XY problem model is a useful framing when evaluating customer feature requests in product design, they are talking about using it in scenario X.But inevitably they will attract a bunch of replies telling them how bad it is to apply the XY problem approach when answering questions in a technical Q&A forum. That would be scenario Y. reply pierat 15 hours agorootparent\"You keep mentioning XY problem, but you really meant the AB problem, and that answer is ......\"That&#x27;s it in a nutshell. And concur with this de-framing non-answer as one of the leading causes of bad StackOverflow solutions. reply asveikau 14 hours agorootparentStack overflow started out with a lot of Microsoft ecosystem people, eg. Joel Spolsky. I worked at Microsoft in 2008 and this kind of de-framing was a bit of a corporate cultural obsession there at that time. You&#x27;d report a bug internally and PMs would ask you what you were really trying to do ... It was frustrating when you wanted people to just fix their shit. Instead people would universally treat you like you didn&#x27;t know what you were doing and really meant to ask something else. I saw this trait a lot on SO around the same time. reply jameshart 14 hours agorootparentprevApparently I was too subtle so let me put a lampshade on it.The replies to the post which said that the XY problem approach is useful in product development, which are talking about XY reframing being a problem on stackoverflow are XY reframing the parent post.They are doing exactly what they decry.The smell of irony is apparently not as thick in the air as I thought it was. reply haswell 11 hours agorootparentFor what it’s worth, I saw what you did there and appreciated&#x2F;enjoyed it. reply yowzadave 14 hours agorootparentprevEven if you know that the strategy is problematic in scenario Y, other viewers of the reply may not; you are only one of the many potential consumers of the response. Isn&#x27;t it useful to flag the potential gotchas of a given approach for a naive reader?I feel like many of the complaints Stack Overflow users come down to this: in many users&#x27; minds, the site is a Q&A forum, while the SO team wants it to be an authoritative repository of technical knowledge. reply bombolo 15 hours agorootparentprevMost people ask how to make some absurd hack when there is an easy and proper way to solve their problem. reply lolinder 15 hours agorootparentSometimes what you think is an absurd hack is still what I want to do after having thoroughly considered all other options. It&#x27;s infuriating in those cases to end up on a Stack Overflow question where someone wanted to do exactly what I want to do, and the only answers are redirecting them to other solutions that I&#x27;ve already considered and ruled out. reply bombolo 4 hours agorootparentThe huge majority of people asking questions on SO are noobs and most likely haven&#x27;t thoroughly considered all other options.If they did, they should say so in the question.The majority of people answering questions are also noobs, and this should be taken into account. Experts in their domain don&#x27;t need SO, and so don&#x27;t go there at all.When I was writing my thesis, years ago, SO was already basically useless to me because nobody could solve any of the problems that I was encountering then.I just use it when I can&#x27;t be bothered to look up stuff in the documentation, but I see it mostly as a resource for people who are learning or are very early in their career. reply dgb23 16 hours agoparentprevThere’s more general concept of perception here that is worth thinking about.Users can get awfully confused by generic, misleading or overly technical error messages. So they call&#x2F;write you and confuse you even more.“There is something wrong about X.” Where X is some misinterpreted partial of a message. This only gets cleared up if you let them walk you through what happened step by step and&#x2F;or examine logs etc.Error messages are an important part of a UI. No matter if they’re user errors or internal errors.There are always errors that you don’t foresee and just need to display generic messages for. But even then there should be a very clear, short(!) description and a unmistakable call to action. reply pixl97 14 hours agorootparentUgh, not 15 minutes before this I was testing a new yet to be released version of my companies software. And while testing I get an error message like\"Cannot do X with upload\"Number one this is a behavior change and should not have been changed in the first place.But number two, all the error had to say was \"Cannot do X with upload because application is set to Y\"The first one generates a support ticket, the second one gives a legitimate reason on why the failure occurred and what they can do about it. reply albertzeyer 16 hours agoparentprevThe XY problem:> This leads to enormous amounts of wasted time and energy, both on the part of people asking for help, and on the part of those providing help.This is not really true though.The time spent to answer is not wasted. There are people searching for it via Google, e.g. how to get the last N characters from a variable, and they will find the correct answer.The time spent by the asker is never wasted. I sometimes know that this is not directly the thing I want to solve, or how I stumbled upon this question. Still, it&#x27;s a question I have because I&#x27;m curious and I just want to know. So, in any case, the person asking for help will learn something.And all other people on the Internet who stumble upon the question are likely searching for exactly the answer to this exact question, so they get some good value out of it. Or even if not, it likely will have references to what they are interested in. Those other people are ignored here. reply sopooneo 15 hours agoparentprevFor all it is rightfully derided, it is this aspect of \"user story phrasing\" I find valuable. If you can politely ask stakeholders to state their problem in the form \"As a _____ I want to _____ so that I can ______\", then you can find out that why as filled in on the last blank. And then you can use that why to figure out the best actions to take, being careful that you still scratch the itch the that middle blank in that story brought up. reply IshKebab 16 hours agoparentprevUnfortunately the XY problem is now mostly used by know-it-alls trying to show off. At least in my experience.If you ever find a question that you think is an XY problem, answer X first and then say \"did you want Y?\".The worst possible answer is \"you should be asking Y\". reply dpkirchner 16 hours agorootparent\"Here&#x27;s the answer to what I wish you asked...\" reply PlunderBunny 15 hours agorootparentPoliticians do it all the time: \"Answer the question you wish you were asked, not the question you were actually asked.\" And reporters are pretty bad at taking this on. reply lolinder 15 hours agorootparent> reporters are pretty bad at taking this on.The format of a typical press conference is designed to make it hard for a reporter to follow up when the politician dodges their question, because the politician usually moves on to the next reporter. If they ever get a chance to ask a follow-up, it&#x27;s after the original context is long gone from anyone&#x27;s working memory. reply teddyh 14 hours agorootparentIf reporters really wanted an answer to the question, the next reporter to be called on could just press for an answer to the previous question. But they don’t; in a press conference situation, the goal of reporters is to be seen, so their fame goes up, and to avoid antagonizing the host, since if they do, they won’t be invited to the next press conference. reply lolinder 14 hours agorootparentEh, that&#x27;s part of it, but it&#x27;s also that the next reporter already knew which question they wanted to ask. They probably didn&#x27;t pay that much attention to the answer to the previous question because they were busy formulating their own question. reply teddyh 14 hours agorootparentprev> And reporters are pretty bad at taking this on.If they do, they won’t get the interview next time. reply haswell 16 hours agorootparentprevWhile I agree that it’s not useful if people are using this to show off, I’d prefer to deal with a few know-it-alls if it means that better product decisions are being made, and dev teams are spending less time building things that customers can’t use or didn’t even want.The way I see it, there are failure modes with both extremes. I’d prefer the failure mode that involves some occasional annoyance over the failure mode that results in significant amounts of wasted code&#x2F;effort, and a return to the XY framing anyway when things go wrong.Ideally, people who are using this find a balance, and can recognize the difference between an obviously straight-forward request and something that needs deeper exploration.It’s not perfect, but I think it’s a better default. reply shawnz 16 hours agorootparentprev> answer X first and then say \"did you want Y?\".That&#x27;s a surefire way to cause your suggestion of Y to get ignored and proliferate the bad practice of X.It&#x27;s not anyone&#x27;s responsibility to explain how to do things in a way that they believe is wrong. reply lolinder 14 hours agorootparentIf they don&#x27;t want to explain how to do things in a way they disagree with, then the appropriate response is to not say anything at all.The current culture on SO is to flood questions with \"don&#x27;t do X, do Y\", then upvote those answers. The result is that questions look answered but actually aren&#x27;t, so the questions stay unanswered. When I come along months or years later having already considered all options, I don&#x27;t want to have my time wasted by a question that perfectly matches my goal but was never answered because it got drowned in alternative approaches that I already ruled out. reply JohnFen 14 hours agorootparent> The current culture on SO is to flood questions with \"don&#x27;t do X, do Y\", then upvote those answers. The result is that questions look answered but actually aren&#x27;t, so the questions stay unanswered.I think this is the #1 reason why SO isn&#x27;t a great resource for me. reply shawnz 11 hours agorootparentprevIsn&#x27;t it the question author who gets to choose when an answer is satisfactory or not on SO? If a question is full of answers that aren&#x27;t marked as satisfactory, then there&#x27;s still an opportunity for someone to come in and get the points by providing a different one. What more can they do, ban people from trying to provide alternative solutions? Surely that is going to create much more harm than good. reply lolinder 11 hours agorootparent> Isn&#x27;t it the question author who gets to choose when an answer is satisfactory or not on SO?This would be a fine policy if SO didn&#x27;t also make a huge stink about duplicate questions. As is, there&#x27;s one canonical copy of each similarly-phrased question, and a re-ask that says \"but for real, I actually want to do it this way\" is going to get shut down as a duplicate.> If a question is full of answers that aren&#x27;t marked as satisfactory, then there&#x27;s still an opportunity for someone to come in and get the points by providing a different one.The system rewards being one of the first responders, not the one who actually answers the question. This is especially true now that they&#x27;ve updated the system to place the highest-voted answer first rather than the accepted answer.> What more can they do, ban people from trying to provide alternative solutions? Surely that is going to create much more harm than good.I don&#x27;t know that there&#x27;s anything the company can do, since it&#x27;s pretty clear that they&#x27;ve lost control of most aspects of the culture. reply shawnz 11 hours agorootparentFair enough, I totally agree that SO moderators are way too overbearing when it comes to duplicates. reply jldugger 15 hours agorootparentprevOkay, but I&#x27;ve been in plenty of conversations where I ask \"I read in a book that we should be doing X, how are people doing X?\"[1], and the answers I got, _from a community that included the book author_, were \"first, make sure you&#x27;re doing A, B and C.\"[2] When in fact I am doing that already. Do I have to really preface every question with \"i promise i&#x27;m not the idiot you assume I am?\"1: \"This book says to monitor ML systems for distribution shifts; what tools are people using to store that data and monitor for changes?\" 2: \"Make sure you&#x27;re monitoring normal SRE statistics like request failure rate\" reply shawnz 15 hours agorootparent> Do I have to really preface every question with \"i promise i&#x27;m not the idiot you assume I am?\"Yes, first of all I do think it&#x27;s up to the person looking for help to fully elaborate their situation in such a way that makes it clear why the X&#x2F;Y problem doesn&#x27;t apply to them, since other people with similar issues who stumble upon your thread might not realize that you have that additional context, and the answer is just as much for them as it is for you (if not moreso, since you&#x27;re just one person).Secondly, even if you did fully elaborate your situation, it may be that there are people interested in trying to help who don&#x27;t know the answer to X but do know the answer to Y, and by answering Y they are still providing more value than not answering at all. There&#x27;s nothing about answering Y that prevents X from being answered by someone else. reply IshKebab 15 hours agorootparentprevIt&#x27;s not anyone&#x27;s responsibility to answer at all. reply shawnz 15 hours agorootparentAgreed! Which is why I think it&#x27;s especially disrespectful to criticize people making honest efforts to help as being \"know-it-alls trying to show off\" in cases where their idea of the ideal kind of help is different than what the original poster had in mind. reply IshKebab 14 hours agorootparentIt&#x27;s frequently NOT an honest effort to help. It&#x27;s just \"well that&#x27;s a stupid question, let me show you how I know more...\"When you really are trying to help and you think it&#x27;s an XY you can answer politely by actually answering their question and then saying \"but you may want to do this instead\". Try it. reply petsfed 13 hours agorootparentIndeed, a good answer to X will make clear why Y is the better option in most cases. But its a thin line to tread between subtly implying that X is bad, and saying \"only idiots do X, anyway here&#x27;s how an idiot would do X\". reply shawnz 13 hours agorootparentprevYou suggested that in your previous comment, and I explained already why I don&#x27;t think that&#x27;s a good idea: it&#x27;s liable to cause your alternative suggestion to get ignored and proliferate bad practices.If someone has a genuine desire to help, then they also inherently have an interest in making sure people don&#x27;t continue down paths which are likely to lead to more problems in the end. Otherwise, you might end up spending more time supporting the follow-on issue",
    "originSummary": [
      "Steve Jobs motivated the engineers working on the Macintosh computer to improve the boot time by telling them that every 10 seconds saved would save a dozen lives.",
      "The engineers succeeded in reducing the boot time by more than 10 seconds in the subsequent months.",
      "This story highlights Steve Jobs' leadership style and the dedication of the engineers in meeting his expectations."
    ],
    "commentSummary": [
      "Understanding customer needs and addressing underlying issues is crucial for software developers to prioritize user satisfaction.",
      "Slow software can negatively impact user experience and should be optimized for faster performance.",
      "Advancements in hardware and software optimization have led to faster boot times, improving overall user experience.",
      "ROM-based operating systems have both advantages and disadvantages and should be carefully considered for specific use cases.",
      "UI animations can enhance user experience, but they should be used judiciously to avoid overwhelming or distracting users.",
      "SSDs offer significant benefits over HDDs, including faster data access times and improved system responsiveness.",
      "Saving time in software development is essential for efficiency and productivity.",
      "Slow updates and insufficient communication platforms can lead to frustration among software users and developers.",
      "Stack Overflow has received criticism and concerns, highlighting the need for alternative platforms and better moderation.",
      "When seeking help, it is important to provide clear and actionable information to receive effective assistance."
    ],
    "points": 316,
    "commentCount": 283,
    "retryCount": 0,
    "time": 1692633078
  },
  {
    "id": 37211519,
    "title": "I Made Stable Diffusion XL Smarter by Finetuning It on Bad AI-Generated Images",
    "originLink": "https://minimaxir.com/2023/08/stable-diffusion-xl-wrong/",
    "originBody": "Max Woolf's Blog Posts Search GitHub Patreon Home » Posts I Made Stable Diffusion XL Smarter by Finetuning it on Bad AI-Generated Images August 21, 2023 · 11 min Inside you there are two AI-generated wolves. Last month, Stability AI released Stable Diffusion XL 1.0 (SDXL) and open-sourced it without requiring any special permissions to access it. Example SDXL 1.0 outputs. via Stability AI The release went mostly under-the-radar because the generative image AI buzz has cooled down a bit. Everyone in the AI space is too busy with text-generating AI like ChatGPT (including myself!). Notably, it’s one of the first open source models which can natively generate images at a 1024x1024 resolution without shenanigans, allowing for much more detail. SDXL is actually two models: a base model and an optional refiner model which siginficantly improves detail, and since the refiner has no speed overhead I strongly recommend using it if possible. Comparisons of the relative quality of Stable Diffusion models. Note the significant increase from using the refiner. via Stability AI The lack of hype doesn’t mean SDXL is boring. Now that the model has full support in the diffusers Python library by Hugging Face with appropriate performance optimizations, we can now hack with it since the SDXL demos within diffusers are simple and easy to tweak: import torch from diffusers import DiffusionPipeline, AutoencoderKL # load base SDXL and refiner vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\",torch_dtype=torch.float16) base = DiffusionPipeline.from_pretrained( \"stabilityai/stable-diffusion-xl-base-1.0\", vae=vae, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True, ) _ = base.to(\"cuda\") refiner = DiffusionPipeline.from_pretrained( \"stabilityai/stable-diffusion-xl-refiner-1.0\", text_encoder_2=base.text_encoder_2, vae=base.vae, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True, ) _ = refiner.to(\"cuda\") # generation using both models (mixture-of-experts) high_noise_frac = 0.8 prompt = \"an astronaut riding a horse\" negative_prompt = \"blurry, bad hands\" image = base( prompt=prompt, negative_prompt=negative_prompt, denoising_end=high_noise_frac, output_type=\"latent\", ).images image = refiner( prompt=prompt, negative_prompt=negative_prompt, denoising_start=high_noise_frac, image=image, ).images[0] I booted up a cloud virtual machine with a new midrange L4 GPU ($0.24/hr total with a Spot instance on Google Cloud Platform) and went to work. With a L4 GPU, each 1024x1024 image takes about 22 seconds to generate and you can only generate one image at a time on midrange GPUs unlike previous Stable Diffusion models since it uses 100% of the GPU’s power, so some more patience is necessary. You can generate at a smaller resolution faster but it is strongly not recommended because the results are much, much worse. diffusers also implemented support for two new features I haven’t experimented with in my previous Stable Diffusion posts: prompt weighting and Dreambooth LoRA training and inference. Prompt weighting support with diffusers leverages the Python library compel to allow weighting of terms more mathematically. You can add any number of + or - to a given word to increase or decrease its “importance” in the resulting positional text embeddings, and therefore the final generation. You can also wrap phrases: for example, if you are generating San Francisco landscape by Salvador Dali, oil on canvas and it does a photorealistic San Francisco instead, you can wrap the artistic medium such as San Francisco landscape by Salvador Dali, (oil on canvas)+++ to get Stable Diffusion to behave as expected. In my testing, it fixes most of the prompt difficulty introduced in Stable Diffusion 2.0 onward, especially with a higher classifier-free guidance value (by default, guidance_scale is 7.5; I like to use 13) All generated examples from the LoRA models in this blog post use a guidance_scale of 13. LoRA the Explorer But what’s most important is Dreambooth LoRA support, which is what makes bespoke Stable Diffusion models possible. Dreambooth is a technique to finetune Stable Diffusion on a very small set of source images and a trigger keyword to allow the use a “concept” from those images in other contexts given the keyword. Demo image of how Dreambooth works. via Google Training Stable Diffusion itself, even the smaller models, requires many expensive GPUs training for hours. That’s where LoRAs come in: instead, a small adapter to the visual model is trained, which can be done on a single cheap GPU in 10 minutes, and the quality of the final model + LoRA is comparable to a full finetune (colloquially, when people refer to finetuning Stable Diffusion, it usually means creating a LoRA). Trained LoRAs are a discrete small binary file, making them easy to share with others or on repositories such as Civitai. A minor weakness with LoRAs is that you can only have one active at a time: it’s possible to merge multiple LoRAs to get the benefits of all of them but it’s a delicate science. Before Stable Diffusion LoRAs became more widespread, there was textual inversion, which allows the text encoder to learn a concept, but it takes hours to train and the results can be unwieldy. In a previous post, I trained a textual inversion on the memetic Ugly Sonic, as he was not in Stable Diffusion’s source dataset and therefore he would be unique. The generation results were mixed. Ugly Sonic, but not the good kind of ugly. I figured training a LoRA on Ugly Sonic would be a good test case for SDXL’s potential. Fortunately, Hugging Face provides a train_dreambooth_lora_sdxl.py script for training a LoRA using the SDXL base model which works out of the box although I tweaked the parameters a bit. The generated Ugly Sonic images from the trained LoRA are much better and more coherent over a variety of prompts, to put it mildly. Ugly Sonic, but with teeth. WRONG! With that success, I decided to redo another textual inversion experiment by instead training a LoRA on heavily distorted, garbage images conditioned on wrong as a prompt in the hopes that the LoRA could then use wrong as a “negative prompt” and steer away from such images to generate less-distorted images. I wrote a Jupyter Notebook to create synthetic “wrong” images using SDXL itself, this time using a variety of prompt weightings to get more distinct examples of types of bad images, such as blurry and bad hands. Ironically, we need to use SDXL to create high resolution low quality images. Examples of the synthetic wrong images, which unintentionally resemble 2000’s-era punk rock album covers. More examples of the synthetic wrong images, which focus on the uncanny valley aspect of modern AI-generated images in which they look normal at a glance but looking closer reveals incremental horror. This is also why it’s important to generate examples at the full 1024x1024 resolution. I trained and loaded the LoRA into Stable Diffusion XL base model (the refiner does not need a LoRA) and wrote a comparison Jupyter Notebook to compare the results with a given prompt from: The base + refiner pipeline with no LoRA. (our baseline) The pipeline with no LoRA using wrong as the negative prompt (to ensure that there isn’t a placebo effect) The pipeline with the LoRA using wrong as the negative prompt (our target result) Each generation has the same seed, so photo composition should be similar across all three generations and the impact of both the wrong negative prompt and the LoRA vs. the base should be very evident. Let’s start with a simple prompt from the SDXL 0.9 demos: A wolf in Yosemite National Park, chilly nature documentary film photography The wrong prompt on the base model adds some foliage and depth to the forest image, but the LoRA adds a lot more: more robust lighting and shadows, more detailed foliage, and changes the perspective of the wolf to look at the camera which is more interesting. We can get a different perspective of the wolf with similar photo composition by adding “extreme closeup” to the prompt and reusing the same seed. An extreme close-up of a wolf in Yosemite National Park, chilly nature documentary film photography In this case, the LoRA has far better texture, vibrance, and sharpness than the others. But it’s notable that just adding a wrong prompt changes the perspective. Another good test case is food photography, especially weird food photography like I generated with DALL-E 2. Can SDXL + the wrong LoRA handle non-Euclidian hamburgers with some prompt weighting to ensure they’re weird? a large delicious hamburger (in the shape of five-dimensional alien geometry)++++, professional food photography The answer is that it can’t, even after multiple prompt engineering attempts. However, this result is still interesting: the base SDXL appears to have taken the “alien” part of the prompt more literally than expected (and gave it a cute bun hat!) but the LoRA better understands the spirit of the prompt by creating an “alien” burger that humans would have difficulty eating, plus shinier presentation aesthetics. A notable improvement with Stable Diffusion 2.0 was text legibility. Can SDXL and the wrong LoRA make text even more readable, such as text-dense newspaper covers? lossless PDF scan of the front page of the January 2038 issue of the Wall Street Journal featuring a cover story about (evil robot world domination)++ Text legibility is definitely improved since Stable Diffusion 2.0 but appears to be the same in all cases. What’s notable with the LoRA is that it has improved cover typesetting: the page layout is more “modern” with a variety of article layouts, and headlines have proper relative font weighting. Meanwhile, the base model even with the wrong negative prompt has a boring layout and is on aged brown paper for some reason. What about people? Does the wrong LoRA resolve AI’s infamous issue with hands especially since we included many examples of such in the LoRA training data? Let’s revamp a presidential Taylor Swift prompt from my first attempt with Stable Diffusion 2.0: USA President Taylor Swift (signing papers)++++, photo taken by the Associated Press Look at Taylor’s right arm: in the default SDXL, it’s extremely unrealistic and actually made worse when adding wrong, but in the LoRA it’s fixed! Color grading with the LoRA is much better, with her jacket being more distinctly white instead of a yellowish white. Don’t look closely at her hands in any of them though: creating people with SDXL 1.0 is still tricky and unreliable! It’s now clear that wrong + LoRA is more interesting in every instance than just the wrong negative prompt so we’ll just compare base output vs. LoRA output. Here’s some more examples of base model vs. wrong LoRA: realistic human Shrek blogging at a computer workstation, hyperrealistic award-winning photo for vanity fair — Hands are better, lighting is better. Clothing is more detailed, and background is more interesting. pepperoni pizza in the shape of a heart, hyperrealistic award-winning professional food photography — Pepperoni is more detailed and has heat bubbles, less extra pepperoni on the edges, crust is crustier (?) presidential painting of realistic human Spongebob Squarepants wearing a suit, (oil on canvas)+++++ — Spongebob has a nose again, and his suit has more buttons. San Francisco panorama attacked by (one massive kitten)++++, hyperrealistic award-winning photo by the Associated Press — The LoRA actually tries to follow the prompt. hyperrealistic death metal album cover featuring edgy moody realistic (human Super Mario)++, edgy and moody — Mario’s proportions are more game-accurate and character lighting is more edgy and moody. The wrong LoRA is available here, although I cannot guarantee its efficacy in interfaces other than diffusers. All the Notebooks used to help generate these images are available in this GitHub repository, including a general SDXL 1.0 + refiner + wrong LoRA Colab Notebook which you can run on a free T4 GPU. And if you want to see the higher resolutions of generated images used in this blog post, you can view them in the source code for the post. What’s Wrong with Being Wrong? I’m actually not 100% sure what’s going on here. I thought that the wrong LoRA trick would just improve the quality and clarity of the generated image, but it appears the LoRA is making SDXL behave smarter and more faithful to the spirit of the prompt. At a technical level, the negative prompt sets the area of the latent space where the diffusion process starts; this area is the same for both the base model using the wrong negative prompt and the LoRA which uses the wrong negative prompt. My intuition is that the LoRA reshapes this undesirable area of the vast highdimensional latent space to be more similar to the starting area, so it’s unlikely normal generation will hit it and therefore be improved. Training on SDXL on bad images in order to improve it is technically a form of Reinforcement Learning from Human Feedback (RLHF): the same technique used to make ChatGPT as powerful as it is. While OpenAI uses reinforcement learning to improve the model from positive user interactions and implicitly reducing negative behavior, here I use negative user interactions (i.e. selecting knowingly bad images) to implicitly increase positive behavior. But with Dreambooth LoRAs, you don’t nearly need as much input data as large language models do. There’s still a lot of room for development for “negative LoRAs”: my synthetic dataset generation parameters could be much improved and the LoRA could be trained for longer. But I’m very happy with the results so far, and will be eager to test more with negative LoRAs such as merging with other LoRAs to see if it can enhance them (especially a wrong LoRA + Ugly Sonic LoRA!) Believe it or not, this is just the tip of the iceberg. SDXL also now has support for ControlNet to strongly control the overall shape and composition of generated images: Examples of SDXL generations using ControlNet specifying the (former) Twitter/X logo. ControlNet can also be used with LoRAs, but that’s enough to talk about in another blog post. A note on ethics: the primary reason I’ve been researching into improving AI image generation quality is for transparent AI journalism, including reproducible prompts and Jupyter Notebooks to further the transparency. Any new novel improvements in AI image generation by others in the industry may no longer be disclosed publicly given that you can make a lot of money by doing so in the current venture capital climate. I do not support or condone the replacement of professional artists with AI. Max Woolf (@minimaxir) is a Data Scientist at BuzzFeed in San Francisco who works with AI/ML tools and open source projects. Max’s projects are funded by his Patreon. Stable Diffusion Image Generation Textual Inversion Ugly Sonic LoRA NEXT » The Problem With LangChain Copyright Max Woolf © 2023. Powered by Hugo & PaperMod. Blog posts are open-sourced on GitHub.",
    "commentLink": "https://news.ycombinator.com/item?id=37211519",
    "commentBody": "I Made Stable Diffusion XL Smarter by Finetuning It on Bad AI-Generated ImagesHacker NewspastloginI Made Stable Diffusion XL Smarter by Finetuning It on Bad AI-Generated Images (minimaxir.com) 301 points by minimaxir 18 hours ago| hidepastfavorite63 comments theptip 16 hours agoIn general I&#x27;m really interested by the concept of personalized RLHF. As we have more and more interactions with a given generative AI system, it seems we&#x27;ll start to have enough interaction data to meaningfully steer the output towards our personal preferences. I hope the UIs improve to make this as transparent as possible.Just thinking about how to productize this flow, it should be quite easy to implement the \"thumbs up&#x2F;down\" feedback option on every image generated in the UI, plus an optional text label to override \"wrong\". Then when you have enough HF (or nightly) you could have a batch job to re-train a new LoRA with your updated preferences.In principle you could collect HF from the implicit tree-traversal that happens when you generate N candidate images from a prompt and then pick one to refine. Or more explicitly, have a quick UI to rank&#x2F;score a batch, or a trash bin in the digital workspace to discard images you don&#x27;t like at each iteration of refinement (batching that negative feedback to update your project&#x2F;global LoRA later).Going further I wonder what the fastest possible iteration loop for feedback would be? For images in particular you should be able to wire up a very short feedback loop with keypresses in response to image generation. What happens if you strap yourself to that rig for a few hours and collect ~10k preferences at 1&#x2F;s? Can you get the model to be substantially more likely to output the sort of images that you&#x27;re personally going to like? Also sounds pretty intense, I&#x27;m getting Clockwork Orange vibes.I didn&#x27;t spot in the article, how many `wrong` images were there? From a quick skim of the code it looks like maybe 6 per keyword with 13 keywords, so not many at all. ~100 is surprisingly little feedback to steer the model this well. reply davely 14 hours agoparent> Just thinking about how to productize this flow, it should be quite easy to implement the \"thumbs up&#x2F;down\" feedback option on every image generated in the UI, plus an optional text label to override \"wrong\". Then when you have enough HF (or nightly) you could have a batch job to re-train a new LoRA with your updated preferences.The AI Horde [1] (an open source distributed cluster of GPUs contributed by volunteers) has a partnership with Stability.ai to effectively do this [2]. They are contributing some GPU resources to AI Horde to run an A&#x2F;B test.If a user of one of the AI Horde UIs (Lucid Creations[3] or ArtBot[4]... made by me) requests an image using an SDXL model, they get 2 images back. One was created using SDXL v1.0. The other was created using an updated model (you don&#x27;t know which is which).You&#x27;re asked to pick which image you like better of the two. That&#x27;s pretty much it. The result is sent back to Stability.ai for analysis and incorporation into future image models.EDIT: There is a similar partnership between the AI Horde and LAION to provide user-defined aesthetics ratings for the same thing[5].[1] https:&#x2F;&#x2F;aihorde.net&#x2F;[2] https:&#x2F;&#x2F;dbzer0.com&#x2F;blog&#x2F;stable-diffusion-xl-beta-on-the-ai-h...[3] https:&#x2F;&#x2F;dbzer0.itch.io&#x2F;lucid-creations[4] https:&#x2F;&#x2F;tinybots.net&#x2F;artbot[5] https:&#x2F;&#x2F;laion.ai&#x2F;blog&#x2F;laion-stable-horde&#x2F; reply minimaxir 16 hours agoparentprev> I didn&#x27;t spot in the article, how many `wrong` images were there? From a quick skim of the code it looks like maybe 6 per keyword with 13 keywords, so not many at all. ~100 is surprisingly little feedback to steer the model this well.Correct: 6 CFG values * 13 keywords = 78 images. Some of them aren&#x27;t as useful though; apparently \"random text\" results in old-school SMS applications sometimes!LoRAs only need 4-5 images to work well, although that was for older&#x2F;smaller Stable Diffusion which is why I used more images and trained the LoRA a bit longer for SDXL. The Ugly Sonic LoRA in comparison used about 14 images and I suspect it overfit. reply theptip 15 hours agorootparentIt&#x27;s really weird that this works. I can see how LoRA on a specific fine-grained concept like Ugly Sonic can work with so few samples, but naively I&#x27;d think such a diffuse concept as \"!wrong\" should require more bits to specify! Like, isn&#x27;t the loss function already penalizing the model for being \"wrong\" on all generated images?(I wonder if there is a follow-up experiment to test if this LoRA&#x27;d model actually has better loss on the original training dataset? There&#x27;s a very interesting interpretability question here I think. Maybe it&#x27;s just doing much better on a small subset of possible images, but is slightly worse on the remainder of the training data distribution.) reply astrange 8 hours agorootparentprevI noticed some of your bad prompts are a little \"wishcasted\", although that&#x27;s pretty common.People put stuff like \"bad hands\" into every model assuming it&#x27;ll work, but it only works on NovelAI descendents because that&#x27;s based on Danbooru which has a \"bad hands\" tag. reply minimaxir 7 hours agorootparentSome of the generated hands are really bad: I opted not to include them to avoid disturbing imagery. reply leopoldhaller 15 hours agoparentprevYou may be interested in the open source framework we&#x27;re developing at https:&#x2F;&#x2F;github.com&#x2F;agentic-ai&#x2F;enactIt&#x27;s still early, but the core insight is that a lot of these generative AI flows (whether text, image, single models, model chains, etc) will need to be fit via some form of feedback signal, so it makes sense to build some fundamental infrastructure to support that. One of the early demos (not currently live, but I plan on bringing it back soon) was precisely the type of flow you&#x27;re talking about, although we used &#x27;prompt refinement&#x27; as a cheap proxy for tuning the actual model weights.Roughly, we aim to build out core python-level infra that makes it easy to write flows in mostly native python and then allows you track executions of your generative flows, including executions of &#x27;human components&#x27; such as raters. We also support time travel &#x2F; rewind &#x2F; replay, automatic gradio UIs, fastAPI (the latter two very experimental atm).Medium term we want to make it easy to take any generative flow, wrap it in a &#x27;human rating&#x27; flow, auto-deploy as an API or gradio UI and then fit using a number of techniques, e.g., RLHF, finetuning, A&#x2F;B testing of generative subcomponents, etc, so stay tuned.At the moment, we&#x27;re focused on getting the &#x27;bones&#x27; right, but between the quickstart (https:&#x2F;&#x2F;github.com&#x2F;agentic-ai&#x2F;enact&#x2F;blob&#x2F;main&#x2F;examples&#x2F;quick...) and our readme (https:&#x2F;&#x2F;github.com&#x2F;agentic-ai&#x2F;enact&#x2F;tree&#x2F;main#why-enact) you get a decent idea of where we&#x27;re headed.We&#x27;re looking for people to kick the tires &#x2F; contribute, so if this sounds interesting, please check it out. reply MuffinFlavored 12 hours agoparentprev> RLHFReinforcement Learning from Human FeedbackAren&#x27;t these systems already trained to score good things higher and bad things worse dictated by human feedback? reply BoorishBears 15 hours agoparentprevImplicit RLHF works better than explicit.It&#x27;s just like the Mom test: if you ask people to rate you affect their ratingYou can have the upscale flow, but you&#x27;re not limited like Discord based Midjourney was: you can even show all the full sized images and detect that the person copied&#x2F;saved&#x2F;right clicked for example reply rabuse 17 hours agoprevCreating art with stable diffusion has become such a fun hobby of mine. The difference between SD 1.5&#x2F;2.0 and SDXL is massive, and it&#x27;s impressive how quickly the quality is improving with this stuff. reply hospitalJail 16 hours agoparent>The difference between SD 1.5&#x2F;2.0 and SDXL is massive,Can you explain?I havent used SDXL yet, but I spent a ton of time in 1.5.So far I gathered:>Higher res>higher &#x27;quality&#x27;But given I was using realistic vision 3 for so long, I never had a quality issue. With upscaling, I never needed higher res. reply davely 15 hours agorootparentI hope you&#x27;ll forgive me for a bit of a self promotion here, but I think I have an interesting example of SD 1.5 (what most people are familiar with and what most models are based off of) vs SDXL.Before Phony Stark shut down the Twitter API, I was running a bot that created landscape images with Stable Diffusion v1.5. Its name is Mr. RossBot [1]. Check out the Twitter page for some examples of the quality.This weekend, I finally updated the code to get it running on Mastodon. In the process, I updated the model to use SDXL [2]. It&#x27;s running the exact same code otherwise to randomly generate prompts.The image caption is a simplified version of the prompt. e.g., \"Snowcapped mountain peaks with an oxbow lake at golden hour.\"Behind the scenes, a whole bunch of extra descriptive stuff is added, so the prompt that SD v1.5 &#x2F; SDXL get is: \"beautiful painting of snowcapped mountain peaks with an oxbow lake at golden hour, concept art, trending on artstation, 8k, very sharp, extremely detailed, volumetric, beautiful lighting, serene, oil painting, wet-on-wet brush strokes, bob ross style\"Anyway, I feel like the quality of SDXL is sharper and it just nails subjects a lot better. It also tries to add reflections and shadows (not always correctly), whereas that didn&#x27;t happen as much with SD v1.5.I&#x27;m pretty impressed! Especially because Stability.ai had released an update model of Stable Diffusion before SDXL: SD v2.0 and SD v2.1. The results (IMHO) were absolute garbage using the same prompts.[1] https:&#x2F;&#x2F;twitter.com&#x2F;mrrossbot[2] https:&#x2F;&#x2F;botsin.space&#x2F;@MrRossBot reply AuryGlenz 13 hours agorootparentprevHere&#x27;s an example using my dog - a trained checkpoint on one of the nicer SD 1.5 models and a LoRA for the SDXL ones: https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;PklEKwCThe first 3 images are some of my attempts at making her into a Pokemon. Some turned out pretty good (after generating 50+ per type), but I struggled with water in particular. It was hard to get her to have a fin, especially with no additional tail.I haven&#x27;t done many in SDXL, but that&#x27;s the point. I&#x27;ve probably generated..10 images of her as a Pokemon, just when I first trying out the LoRA. The next 2 images are from that, and that was before I had a good ComfyUI workflow to boot.The rest are various sample images from SDXL showing how versatile it is. In most of those, I only had to generate a few images per prompt to get something pretty darn great. In the Halo 2 one the prompt was literally \"an xbox 360 screenshot of cinderdog in Halo 2, multiplayer.\"And it made her into a freaking Elite, and it worked wonderfully. I previously tried to generate ones like those candyland images in 1.5 models and the foreground and background just didn&#x27;t look good. In SDXL it just works. reply jononor 13 hours agorootparentVery cool! How many images did you use to create the LoRa of your dog? Do you have any guide to recommend? reply AuryGlenz 12 hours agorootparentIt was about 30 images, though I&#x27;m planning on adding more and training again sometime. Either that or splitting it up between when her hair is short and when it&#x27;s long, as it really changes how she looks.This isn&#x27;t what I used for my dog&#x27;s LoRa but I used it for my wife and it worked better than what I was doing before (Adafactor): https:&#x2F;&#x2F;civitai.notion.site&#x2F;SDXL-1-0-Training-Overview-4fb03...I&#x27;d recommend increasing the network dimension to at least 64, if your VRAM can take it. I can do 64 with my 12GB card. At least for people, I&#x27;ve had better luck using a token that&#x27;s a celebrity. I&#x27;m not sure how to try that with my dog - perhaps just \"terrier dog\" or something. reply carbocation 6 hours agorootparentThat&#x27;s a very low learning rate -- between 2-3 orders of magnitude lower than what I&#x27;ve seen for that number of steps. I&#x27;ll have to give it a try. reply AuryGlenz 4 hours agorootparentI should have been clear - I&#x27;m using the Prodigy settings on that page, not the Adafactor one. You set the learning rate to 1 and the scheduler to cosine, but the real learning rate is figured out by the optimizer. reply jononor 11 hours agorootparentprevThanks! Looks like I&#x27;ll need to rent a GPU to use SDXL fine tuning. Poor old RTX2060 not gonna cut it. reply zirgs 15 hours agorootparentprevFrom my experiments it seems that SD XL understands prompts much better. While SD 1.5 is great at generating your typical \"anime girl with big boobs\" stuff - if you try to generate something a little bit more unusual - it usually doesn&#x27;t generate exactly what you want and seems to straight up ignore large parts of the prompt.SD XL seems to understand weird and unusual prompts a lot better.SD XL is capable of generating 1024x1024 images without hacks like \"hires fix\". That&#x27;s a very good thing, because hires fix sometimes introduces additional glitches while upscaling. Especially at higher denoising strength. Hires fix fixed the broken face - yay, but the subject now has 3 legs instead of two. Things like that happen far less often with SD XL. reply 3abiton 14 hours agorootparent> While SD 1.5 is great at generating your typical \"anime girl with big boobs\" stuff - if you try to generate something a little bit more unusual - it usually doesn&#x27;t generate exactly what you want and seems to straight up ignore large parts of the prompt.Pretty much experience with SD 1.5, but I&#x27;ll give XL a try. reply SkyPuncher 6 hours agorootparentprevFor simplicity, it feels like SDXL has better \"defaults\". You don&#x27;t have to include a bunch boilerplate keywords to wrangle it into generating good images.The flip side is I&#x27;ve found it a bit harder to tweak prompts reply esperent 6 hours agorootparentI&#x27;ve found it very hard to create different styles with SDXL. If you want photorealism, anime, sci-fi, or somewhere in between, it&#x27;s amazing.But I&#x27;ve been trying to get it to generate equivalent quality in other styles, e.g. watercolor, abstract painting etc. It doesn&#x27;t seem to be easy - the quality drops a lot and it&#x27;s harder to avoid weird results like people wearing enormous hats or distorted perspective.Admittedly I haven&#x27;t spent a huge amount of time on this because generation is just a bit too slow to be enjoyable on my machine. Has anyone else had success here? reply Sharlin 16 hours agorootparentprevYes, currently SDXL doesn&#x27;t really beat the best SD1.5 checkpoints quality-wise. But it (and the currently available checkpoints) shows awesome promise, so give it a six months or so. reply CuriouslyC 14 hours agorootparentThe best 1.5 checkpoints are constrained in their output flexibility to achieve the quality they get though, and they don&#x27;t follow prompts nearly as well as SDXL, so if the model doesn&#x27;t naturally gravitate towards doing what you want it&#x27;s very hard to steer it anywhere. SDXL also does a better job with full anatomy, which is the reason shared 1.5 generations tend to be torso up or portrait shots. reply AuryGlenz 14 hours agorootparentprevCurrently SDXL is better than SD1.5 checkpoints at pretty much everything other than portraits (or anime drawings) of pretty women.Unfortunately it seems that&#x27;s all people want to generate, as is evident when you search for SD on Twitter. reply Sharlin 11 hours agorootparentYes, point conceded, I should&#x27;ve said something about the flexibility and capability of SDXL rather than just image quality in a narrow sense. reply ChatGTP 13 hours agorootparentprevStable diffusion doesn’t grant the user a good imagination or taste unfortunately replyFootnote7341 14 hours agoprevIt became a trend among some data scientists maybe 5 years ago to start recording every keystroke they made on their PC. I&#x27;m kind of jealous now when that data is actually kind of useful.I have a large 30,000 image collection of anime art that I like, that I even competitively ranked for aesthetic score 5 years ago that would come in useful for something like this. reply yantrams 16 hours agoprevVery cool. Will give this idea a spin soon. I&#x27;m a bit of a scientist myself too :)Here&#x27;s something interesting I did few days ago.- Generated images using mixture of different styles of prompts with SDXL Base Model ( using Diffusers )- Trained a LoRA with them- Generated again with this LoRA + Prompts used to generate the training set.Ended up with results with enhanced effects - glitchier, weirder, high def.Results => https:&#x2F;&#x2F;imgur.com&#x2F;gallery&#x2F;vUobKPKI’m gonna train another LoRA with these generations and repeat the process obviously!This is a pretty neat way to bypass the 77 token limit in Diffusers and develop tons of more styles now that I think about it.You can play around with the LoRA at https:&#x2F;&#x2F;replicate.com&#x2F;galleri5&#x2F;nammeh ( GitHub account needed )Will publish it to CivitAI soon. reply politelemon 17 hours agoprevPlease consider posting the LoRa on civitai.com as well as the stable diffusion Reddit.These results look pretty good, looking forward to trying it out. I hadn&#x27;t realized that the generative images buzz was dying out, since I&#x27;m using it regularly I guess it is always in buzz for me. reply minimaxir 17 hours agoparentI posted the original release to &#x2F;r&#x2F;StableDiffusion but all the comments are \"why not compatable with A1111?\" and I can&#x27;t find a good script to do the conversion: https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;StableDiffusion&#x2F;comments&#x2F;15r5k3i&#x2F;i_...Civitai has syndicated the LoRA: https:&#x2F;&#x2F;civitai.com&#x2F;models&#x2F;128708&#x2F;sdxl-wrong-lora reply Zetobal 16 hours agorootparentYou will get more users if you provide a safetensors file instead of bin and pickletensors a lot of people have gotten really scared by the malware scare that was going through social media a few months ago. reply araes 15 hours agorootparentThank you for note on this. I had not heard there were already trojan horse malware being slipped into tensor files as python scripts. Apparently torch pickle uses eval on the tensor file with no filter.Heard surprisingly little commentary on this topic. The full explanation of how Safetensors are \"Safe\" can be found from the developer at: https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;safetensors&#x2F;discussions&#x2F;111 reply homarp 14 hours agorootparentalso safetensors security audit: https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;safetensors-security-audit reply Sharlin 15 hours agorootparentprevAnd for a good reason. A big hunk of floating-point numbers really shouldn&#x27;t be able to execute arbitrary code. Or any code at all. reply 0cf8612b2e1e 13 hours agorootparentprevI would also ask that sha hashes are posted somewhere. It annoys me to know end how difficult it can be to confirm you are using the real model. reply chankstein38 16 hours agoparentprevAgreed I feel like, and I do this a lot as well, people have a tendency to track their habits and assume everyone follows that. From my perspective, the gen image buzz is still as hot as ever!If I lacked excitement for SDXL it was because it felt like the there was no massive jump in image quality to me. Sure the size doubling is great, but it also presents a problem, as I don&#x27;t always want to generate 1024x1024 images. I still use third party trained 1.5 models because they create damned good outputs and I have like 5 different upscaling solutions and at least one will add new detail as things are upscaled. reply Sharlin 15 hours agorootparentSDXL is more resolution-agnostic than SD1x, 768x768 works fine, but admittedly going down to 512x512 does tend to produce cropped images. reply Jackson__ 11 hours agoprev>The release went mostly under-the-radar because the generative image AI buzz has cooled down a bit. Everyone in the AI space is too busy with text-generating AI like ChatGPT (including myself!).I disagree with this statement. The release went mostly under the radar for 2 reasons, according to the people I&#x27;ve talked to.1. Higher vram and compute requirements2. Perceived lower quality outputs compared to specialized SD1.5 models.If either of these points had been different, it would have gained a lot more popularity I&#x27;m sure.But alas, most people now simply wait and see if specialized SDXL models can actually improve upon specialized 1.5 models. reply SV_BubbleTime 9 hours agoparentLower quality output. It’s that.I think most people casually associated with it find it as a toy they mess around with for a minute. The hardcore SD fans… are making hardcore I think.XL is bad at porn. Stability got scared of what they created and tried to hedge towards “safety”. Can’t have your Kate Middleton or Emma Watson porn being TOO convincing.People will stick with 1.5 until something is better… for porn. reply carbocation 16 hours agoprevTangentially related: for reasons I don&#x27;t yet really understand, the LORAs that I build for Stable Diffusion XL only work well if I give a pretty generic negative prompt.These are fine-tuned on 6 photos of my face, and if I use them with positive prompts, the generated characters don&#x27;t look much like me. But if I add generic negative terms like \"low quality\", suddenly the depiction of my face is almost exactly right.I&#x27;ve trained several models and this has been true across a range of learning rates and number of training epochs.To me, this feels like it will somehow ultimately be connected to whatever is driving minimaxir&#x27;s observations in this post. reply msp26 17 hours agoprev>A minor weakness with LoRAs is that you can only have one active at a timeUh this isn&#x27;t true at all, at least with auto1111. reply minimaxir 17 hours agoparentIIRC it does merging&#x2F;weighting behind the scenes. reply cheald 16 hours agorootparentI&#x27;m pretty sure that it&#x27;s just serially summing the network weights, which results in an accumulated offset to the self-attention layers of the transformer. It&#x27;s not doing any kind of analysis of multiple networks prior to application to make them \"play nice\" together; it&#x27;s just looping and summing.https:&#x2F;&#x2F;github.com&#x2F;AUTOMATIC1111&#x2F;stable-diffusion-webui&#x2F;blob... reply Der_Einzige 17 hours agorootparentprevSource for this? reply usrusr 12 hours agoprevMust be the formative years spent in the nineties&#x27; contradiction field of \"counter culture vs also counter culture, but counter culture that&#x27;s on MTV\": there&#x27;s something about prompts ending with tag references like \"award winning photo for vanity fair\" (or whatever the promptist&#x27;s standard tag suffix turns out to be in these posts) that inspires a very deep desire in me to not be part of this generative image wave. reply minimaxir 11 hours agoparent\"award winning photo for vanity fair\" is more a trick for good photo composition (e.g. rule of threes) than anything else. reply sorenjan 17 hours agoprevThis is really interesting. Like mentioned in the article, this is a kind of RLHF, and that&#x27;s what takes GPT3 from a difficult to use LLM to a chat bot which is able to confuse some people into thinking is has consciousness. It makes it much more usable.I don&#x27;t know how these models are trained, but hopefully future models will include bad results as negative training data, baking it into the base model.It&#x27;s only mentioned in passing in the article, but apparently it&#x27;s possible to merge LoRAs? How would you do that, I&#x27;d like to use one LoRA to include my own subjects, this LoRA to make the results better, and maybe a third one for a particular style. reply minimaxir 17 hours agoparentMerging LoRAs is essentially taking a weighted average of the LoRA adapter weights. It&#x27;s more common in other UIs.diffusers is working on a PR for it: https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;diffusers&#x2F;pull&#x2F;4473 reply nullc 12 hours agoprevI wonder how much of this effect is just undoing stabilities fine tuning against inappropriate images. reply letitgo12345 14 hours agoprevSimilar to https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.12950 reply Der_Einzige 17 hours agoprevThis concept is not new. Lots of \"negative embeddings\" that you put into negative prompts to fix hands and bad anatomy on civit.ai reply minimaxir 16 hours agoparentThat was my previous textual inversion experiment that I mentioned in the post: https:&#x2F;&#x2F;minimaxir.com&#x2F;2022&#x2F;11&#x2F;stable-diffusion-negative-prom...This submission is about a negative LoRA which does not behave the same way at a technical level. reply kwhitefoot 17 hours agoprev [–] > XLExtra Large? 40 times? reply brianjking 17 hours agoparentWhat? XL is the current version of Stable Diffusion. reply pbjtime 17 hours agorootparentIt&#x27;s already on version 40? reply bckr 17 hours agorootparentExtra large reply jfoutz 17 hours agorootparentIt’s a Roman numeral joke. reply ShamelessC 15 hours agorootparentNot a very good one. replysschueller 17 hours agoparentprev [–] 1024 x 1024 instead of 512 x 512. reply Taek 17 hours agorootparent [–] XL more likely refers to the parameter count, which is 3 billion instead of <1 billion reply not2b 17 hours agorootparent [–] No, I think it is mainly because it&#x27;s optimized for 1024 x 1024 images, rather than 512 x 512 as the previous version was. reply Our_Benefactors 16 hours agorootparent [–] It’s both. More pixel space and more parameters. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Max Woolf's blog post introduces Stable Diffusion XL 1.0 (SDXL), an open-source model for generating high-resolution images.",
      "The author shares his process of using a cloud virtual machine with a midrange L4 GPU and highlights new features in the diffusers Python library.",
      "The post explores the use of LoRAs for finetuning SDXL, showcases image generation examples with and without LoRAs, and discusses the \"wrong prompt\" technique to improve AI-generated images."
    ],
    "commentSummary": [
      "The author shares their experience in fine-tuning the Stable Diffusion XL model using AI-generated images.",
      "The article suggests implementing a feedback option for personalized improvements in the model.",
      "AI Horde and LAION are mentioned as platforms for testing and collecting user preferences in fine-tuning the model."
    ],
    "points": 301,
    "commentCount": 63,
    "retryCount": 0,
    "time": 1692634167
  },
  {
    "id": 37213479,
    "title": "5 years ago Valve released Proton",
    "originLink": "https://www.gamingonlinux.com/2023/08/5-years-ago-valve-released-proton-forever-changing-linux-gaming/",
    "originBody": "GamingOnLinux Sections Contact Us Forum IRC Discord Support us on Patreon to keep GamingOnLinux alive. This ensures all of our main content remains free for everyone with no article paywalls. Just good, fresh content! Alternatively, you can donate through PayPal or Buy us a Coffee. You can also buy games using our partner links for GOG and Humble Store. We do often include affiliate links to earn us some pennies. See more here. 5 years ago Valve released Proton forever changing Linux gaming By Liam Dawe - 21 August 2023 at 11:08 am UTCViews: 52,578 Has it really been that long? Apparently so. Valve originally announced their rebranding of Steam Play with Proton back on August 21st, 2018. Seems like a good time for a quick reflection being halfway to a decade old now. The problem: before, during and just after the original Steam Machine push developers just weren't porting many games to Linux, and on the whole really when you look back Linux gaming was in a period of mostly stagnation. Nothing much was happening. A lot of the early porting work that came along was slowly dying off since the Steam Machines didn't provide the boost Valve and Linux gamers were hoping for. Originally, “Steam Play” simply meant you could buy a game on Steam and get access to all versions of it. So if you purchased a game on Windows, you could play it on Linux if it had a Native Linux version (same again for macOS). Clearly though, that wasn't enough for what Valve had planned. So if Valve wanted to make Linux a better platform for people to actually use it, they needed something more. Popular games needed to be able to run on Linux if more people were to use it, because what good is a platform for gaming if tons of the most played games weren’t compatible? As Valve saw with the Steam Machines (not the only reason of course) it doesn’t end well. I remember being sat in my office at the time, going over my TODO list of various articles and games to cover, planning the week ahead. Ready to then probably go to bed and then BAM — out of nowhere this announcement comes along from Valve that basically said “you're going to be hella busy, better get a lot of coffee”. Valve had partnered up with CodeWeavers and Proton was the answer. Proton being the name Valve gave to their fork of Wine with a bunch of added extras, and it has changed Linux gaming forever. Steam Play is more than just Proton though, be sure to read my full Steam Play Proton guide. Side-note: John Carmack (id Software / Oculus VR / Keen Technologies) even thought Wine was the solution back in 2013. Of course Proton wouldn't have been possible without all the many years of work that went into the Wine project in the first place, and everyone who contributed to Wine should be applauded for their effort. Valve has funded a lot of extra work though to get things like DXVK and VKD3D-Proton for the translation from Direct3D to Vulkan into a state where performance can be really great! Valve also funds work on Linux graphics drivers, Linux kernel work and the list goes on. Just how much has Proton changed things though? Well, we certainly wouldn't have the Steam Deck if it wasn't for Proton. There's no way Valve would have released their handheld without as many games as possible, clearly learning their lesson on the failure of the Steam Machines. But as we all know, the Steam Machines burned so Proton and the Steam Deck could rise from the ashes. Proton just makes a lot of sense. It didn't take long for Valve to expand Proton to go initially from a few select Valve-approved titles, to being able to run anything we choose to try with it. From there, Linux gaming just seemingly exploded. And then eventually we saw why Valve made Proton with the Steam Deck announcement coming less than three years later in July 2021. When you look at the crowd-sourced reports on ProtonDB over 11,000 titles are reported to work by multiple people. It's a small fraction of Steam's overall game count but ProtonDB relies on people actually going and writing a report. Valve's own Deck Verified rating system just for the Steam Deck hit plenty of milestones since the Steam Deck's release too, now having over 10,000 titles rated to be Playable or Verified. Although both ProtonDB and Deck Verified include a mixture of Native Linux releases and Windows games run with Proton. The point is - being on Linux now for a gamer means quite often a huge amount of your games do actually \"just work\". It's almost magical. The real number of games playable on Linux will never be truly known though, because on Steam there's many tens of thousands of games and a lot of them will likely \"just work\" with Proton if they don’t have a Native Linux build available…and many more are releasing every week. This is a truly incredible place to be in. When you think about Linux and Steam Deck together having just less than a 2% user share on Steam overall — these are some insane numbers for game compatibility for a niche platform. Thanks to Proton, I've been able to discover a ton of new favourite games, some I would never have played before. Games like Deep Rock Galactic, God of War, Death Stranding, Baldur’s Gate 3, Brotato, Beat Saber and so on. You get the idea, there’s a truly ridiculous selection of games available and at times it’s a little paralysing scrolling through my Steam Library deciding what to play — a delightfully annoying problem to have huh? Pictured - Baldur's Gate 3, shot taken on Kubuntu Gamers using Linux (be it desktop or Steam Deck) in 2023 are now in a place where they can get excited alongside friends who use Windows or traditional consoles, because they know there's a good probable chance at release that whatever new exciting game coming out will work without much trouble. Proton is far from perfect though and it does mean in a few ways that Valve, CodeWeavers and everyone else working on it are playing catch-up with Microsoft on compatibility and whatever changes Microsoft suddenly decides to announce that affects gaming. That, and how many different ways game developers can abuse various APIs to do things in weird ways. Valve and co are a dedicated bunch though, constantly fixing up issues from AAA games like ELDEN RING where Valve quickly worked to implement optimizations, same again for problems with Dead Space to the likes of PooShooter: Toilet Invaders (what a fun highlight that was eh?). There's also the constant third-party launcher breakage, that Valve are also fixing up each time it happens. Valve produce updates to Proton constantly to improve compatibility, with over 300 revisions to the main changelog (although some a minor text corrections) it's clear to see how much work goes into it. With various new main versions of Proton through 4, 5, 6, 7 now onto 8 and multiple updates to Proton Experimental almost every month. So here's to Proton, the magnificent tech that allows playing all kinds of games across Linux systems from desktop to Steam Deck and wherever else you decide to stick Linux. Nice one Valve. Cheers. Tags: Steam Deck, Steam Play, Editorial, Misc, Steam, Valve, Wine 43 Likes Share About the author - Liam Dawe I am the owner of GamingOnLinux. After discovering Linux back in the days of Mandrake in 2003, I constantly came back to check on the progress of Linux until Ubuntu appeared on the scene and it helped me to really love it. You can reach me easily by emailing GamingOnLinux directly. Find me on Mastodon. See more from me Some you may have missed, popular articles from the last month: The Black Pepper Crew is a promising looking tactical turn-based roguelite Linux continues hitting above 3% desktop user share on Statcounter Baldur’s Gate, Neverwinter Nights, Pathfinder and more in this bundle 43 comments Page: 1/5» Go to: 1 2 3 4 5 dpanter about 23 hours ago Link View PC info Mega Supporter Steam YouTube twitch 5 years! Feels like Valve time, but good. :) 13 Likes, Who? Linux_Rocks about 23 hours ago Link View PC info Steam YouTube twitch gogprofile To protect your privacy, external media requires approval to load. Source: www.techcrunch.com View cookie preferences. Accept & Show Accept All & Don't show this again Direct Link 6 Likes, Who? Pengling about 23 hours ago Link View PC info Supporter Proton lets me play the modern Bomberman games on Linux, and therefore it is Pengling Approved. Seriously, though, after I left consoles behind just shy of two years ago Proton has opened up a whole new world of content on the OS that I was already using for the last 16 years. Until then, I didn't even know it existed, or even that there was a Linux version of Steam (I was only super-distantly aware of the Steam Machines, and misunderstood the reason for their failure - I had perceived it as being due to redundancy, since consoles and self-built PCs already did that job). In those two years alone I've seen things go from \"This Proton thing is like magic!\" to \"Steam Deck is coming soon\" to \"Oh look, a new Windows release runs better on Linux at launch than on Windows\" to, most recently, \"Cool! Devs are now making Linux-specific fixes even without native builds!\". Valve is playing a very long game here, and I'm very interested in seeing where it goes next. Thanks to all involved in making things what they are right now! 13 Likes, Who? a0kami about 23 hours ago Link View PC info Steam twitch mastodon I'll never forget Cheese's overview of the Linux gaming ecosystem back then. I felt so happy and relieved, then applied to Feral and everything went haywire, for the better however. I'll never be able to conveniently express my gratitude to neither Feral Interactive or Wine,DXVK,VKD3D-Proton teams and contributors or Valve or even the whole community, thank you so much. We made it possible and it's just the beginning. Now, sure the Steam Deck helped, and Deckard's going to be insane, but we just need a few more linux native cross-platform gamedev tools so we'd get a whole lot more native games. I place my bets on Godot, we'll see how it goes from there. 6 Likes, Who? Turkeysteaks about 22 hours ago Link View PC info Steam Oh man, can't believe it's been 5 years already I remember trying to get warframe to work on linux over the course of a few years prior to proton. Always still somewhat broken, and eventually gave up on it to play other games. Now, it's click and play (though the curse of the 1000+ library; I still don't play it!) as are almost all my games. I used to spend hours on each game to get it to work, and now I feel I am far more tech illiterate than I used to be simply because I have not *needed* to tinker far in depth for any game in the past couple years. The steam deck too, that as a product is mindblowing to me and I love mine so much. Excited to see how it carries on moving forwards. 6 Likes, Who? CatKiller about 22 hours ago Link View PC info Supporter Plus \"People don’t realize how critical games are in driving consumer purchasing behavior.\" -- Gabe Newell, 2012. 10 Likes, Who? whizse about 22 hours ago Link View PC info Supporter Happy fifth anniversary to Proton and Valve, and a big thank you to Liam and GoL for the fantastic coverage over the years! 8 Likes, Who? lejimster about 22 hours ago Link View PC info Steam I remember testing the early versions of proton when it first was announced. I was messing around with The Witness and the performance had dropped 50% from an early proton release. It was soon discovered they had accidentally released a debug version of proton. While the wine devs deserve a lot of credit, Vulkan and dxvk was the real game changer in terms of performance. So many titles run at 90+% of native because of dxvk and Vulkan titles often run better on Linux than they do Windows. 8 Likes, Who? Ehvis about 21 hours ago Link View PC info Supporter Plus Steam Quoting: lejimster While the wine devs deserve a lot of credit, Vulkan and dxvk was the real game changer in terms of performance. So many titles run at 90+% of native because of dxvk and Vulkan titles often run better on Linux than they do Windows. I'd say this is the key as well. DXVK was a major leap as it demonstrated how well Vulkan could be applied to D3D translation. 3 Likes, Who? StoneColdSpider about 21 hours ago Link View PC info Proton was one of the main reasons I finally ditched Windows....... And I was not disappointed..... Proton has been really damn good........ So if you want to blame anyone for me hanging around here annoying you all....... Its Lord Gabens fault...... 5 Likes, Who? 1 / 5 » While you're here, please consider supporting GamingOnLinux on: Reward Tiers: Patreon. Plain Donations: PayPal. This ensures all of our main content remains totally free for everyone with no article paywalls. We also don't have tons of adverts, there's also no tracking and we respect your privacy. Just good, fresh content. Without your continued support, we simply could not continue! You can find even more ways to support us on this dedicated page any time. If you already are, thank you! Login / Register Stay logged in. Note: Checking this box requires a cookie. It is not required. Login Or login with... Sign in with Steam Sign in with Google Social logins require cookies to stay logged in. Register Forgot Login? ★ SUPPORT US Popular this week 5 years ago Valve released Proton forever changing Linux gaming Star Labs reveal their new StarLite, a Surface-like Linux tablet Linux players getting banned on Apex Legends again Baldur's Gate 3 is going to get some big updates Happy Debian Day - going 30 years strong > See more here Search or view by category Contact Email Us Latest Comments No Man’s Sky has a teaser for the 'Echoes' update to … 1 hour ago - Luke_Nukem Linux players getting banned on Apex Legends again about 2 hours ago - Geppeto35 5 years ago Valve released Proton forever changing Linu… about 3 hours ago - Pengling 5 years ago Valve released Proton forever changing Linu… about 3 hours ago - omer666 5 years ago Valve released Proton forever changing Linu… about 4 hours ago - Pengling See more comments Latest Forum Posts Windows 11 VM or Native? about 2 hours ago - Smellbringer Weekend Players' Club 8/18/2023 about 4 hours ago - Pengling AMD Game Bundle Starfield seems also to work explicitly with Linu… about 16 hours ago - Alexander Openmohaa 0.58.1-alpha is available for Linux a day ago - gbudny Multi-distro users, what all are you running? a day ago - Grogan See more posts Misc Cookie Preferences Support Us About Us Website Statistics Join us RSS Feeds GamingOnLinux © 2023 Unless otherwise noted, articles are licensed as: CC BY-SA 4.0 About Us, Contact Us, Privacy, Ethics",
    "commentLink": "https://news.ycombinator.com/item?id=37213479",
    "commentBody": "5 years ago Valve released ProtonHacker Newspastlogin5 years ago Valve released Proton (gamingonlinux.com) 274 points by chungus 15 hours ago| hidepastfavorite218 comments stevepike 14 hours agoProton has earned steam my business forever. I haven&#x27;t booted into Windows in over a year. reply sph 11 hours agoparentI hate DRM, I hate monopolies, I welcome competition, but if one builds a massive empire by just creating a bonafide good platform, single-handedly making open source desktop better, with good customer support and treating users with respect, they deserve the money honestly.If one day I manage to build a billion dollar empire, my sole inspiration on how to conduct business is Gabe Newell. [1]Which is exactly the thing Epic can&#x27;t compete on. They can give away all the free games they want, but Steam and Valve have done much more than offering games on sale.(I got a 13 year old account on Steam, more than 500 games bought, almost $10k spent on the platform. No Windows partition for the past 3 years)---1: I honestly couldn&#x27;t name anybody else that has kept their company private, grown it to such heights and stayed true to their founding principles, without selling out to shareholders and advertisers for an easy buck. reply igor_akhmetov 2 minutes agorootparentJetBrains, while maybe not comparable in size, is still private. reply jonny_eh 11 hours agorootparentprevWith the xbox 360 marketplace shutting down next year, will Steam be the longest running digital marketplace? That longevity is what earns my trust.Update: iTunes Store started in April 2003, Steam started selling third-party titles in late 2005 (around when the 360 launched). reply Macha 9 hours agorootparentprevYeah, pretty much all my PC game purchases go to either Steam (for their work on Linux gaming) or GOG (for their DRM free stance). Luckily my financial position is such that it really doesn&#x27;t matter what sale is on Epic or Uplay or Origin to change that. reply johnnyanmac 7 hours agorootparentprev>Which is exactly the thing Epic can&#x27;t compete on.I mean, with these criteria is Epic really that far behind? They made desktop a better experience for devs, have decent enough customer support, and they don&#x27;t exactly shit talk their users like other parts of the industry. The only arguable part is good platform, but it depends on what you need out of the platform. Does a platform have to offer a way to play windows games on linux to be \"good\"?>I honestly couldn&#x27;t name anybody else that has kept their company private, grown it to such heights and stayed true to their founding principles, without selling out to shareholders and advertisers for an easy buck.hard to find platforms like that, but there are certainly creators that stayed small and humble despite growing huge in influence and pull. reply neurostimulant 2 hours agorootparentWhen they buy out any game exclusivity (so the games are only sold on Epic store on pc), they would make the game drop linux support even when the games previously can run on linux natively (e.g. rocket league, payday 2, etc). That&#x27;s a huge negative in my book, enough to make me never consider using Epic store. reply ardaria 6 hours agorootparentprevI won&#x27;t say that epic didn&#x27;t delivered for the gamers. UE5 is honestly a technology marvel and bring so much values for players. I don&#x27;t even talk about lumen&#x2F;nanite but all the others tools made for developers to push the limits of what is doable in a game, at runtime. With UE5.3 we even get more productivity tools, which means we can deliver faster or bigger.Don&#x27;t get me wrong, many features are experimental since 4.26 but getting them production ready is the game changer. reply erinnh 14 hours agoparentprevAbsolutely agree.I was starting to diversify my store fronts before Valve came out with the Deck.But the release of it and the message it sent (that they take this seriously), made me reverse course and now I always buy my games on Steam, even if it costs 10 euro more. reply solardev 13 hours agorootparentFYI Steam has a ton of legit resellers with their own sales. The keys all activate on Steam.Isthereanydeal.comValve will be fine :) reply erinnh 13 hours agorootparentOh, I use those. But even then it’s sometimes more expensive to buy it on steam than say Uplay or Epic. reply ClassyJacket 11 hours agorootparentprevSame. I just hate that so many games I want are exclusive to Epic. It&#x27;s like they&#x27;re trying to make that store unpleasant to use. reply FirmwareBurner 11 hours agorootparent>I just hate that so many games I want are exclusive to Epic.I miss the days when PC games weren&#x27;t tied to some online webstore but came on a CD and the only DRM was a CD-key, no always online, no proprietary web store, just physical media that you could share with your friends.Sure, Steam and GoG are probably the best kind of on-line webstore so far, bur I still love my collection of .ISO games that I can mount and play whenever I want without depending on any invasive DRM, accounts or internet connection. reply makomk 14 hours agoparentprevValve have funded some good work on the underlying compatibility code, but their big contribution was really fixing a problem they themselves created. Lots of games expect to be able to call back to the Steam client for a license check, and this used to require running the Windows version of Steam which didn&#x27;t work well (huge compatibility issues and it really didn&#x27;t like being run at the same time as the Linux version). One of the big things enabling easy Linux gaming was a Valve-approved way to run Windows games under a compatibility layer and have them still connect to the same Linux client used to run native games. reply pixelatedindex 14 hours agorootparent> but their big contribution was really fixing a problem they themselves created.I think it’s a little bit of a stretch to say it’s a problem they created - they were just a victim of their own success. They can’t force developers to write games in a particular platform, and making games work under Linux is no small task.It was probably easier to stream games from a Windows machine, which was their first approach with Steam Link and Steam Machines. It kinda sorta worked, and what they saw was enough encouragement to go and build the Steam Deck. On top of that, CPUs&#x2F;GPUs just weren’t good enough 10 years ago to do what Valve wanted to do.So I think it’s a little unfair to say they “created” this problem. reply henryfjordan 14 hours agorootparentprevValve may have contributed to the issue initially, but can you blame them? There wasn&#x27;t exactly an abundance of Linux Support before Valve came along either. Why would they support a Linux Steam client and the whole compatibility layer when the demand just didn&#x27;t exist.Valve becoming big enough to break into the hardware market (first Steam Machines and then the Steam Deck) was the first time they had any incentive to care about the OS layer. They could&#x27;ve made some deal w&#x2F; Microsoft but instead went the open-source route to the benefit of everyone. Kudos to Valve. reply johnnyanmac 6 hours agorootparent>but can you blame them?Sure I can. They decided ultimately to make an indirect dependence on Windows instead of encouraging devs to make good native linux ports. They more or less made the deal with Windows without having to get their hands dirty. reply Vt71fcAqt7 11 hours agorootparentprevThe big problem was directx, not steam_api.dll, which can be solved in many ways. reply rossy 2 hours agoparentprevI love Steam, Valve and Proton, and what they&#x27;ve done for Linux gaming, but nothing will make me compromise on DRM. To me, ending DRM is just as important as supporting free operating systems, so unfortunately no amount of good will gives Valve a free pass in my eyes.So I&#x27;ve started buying games from DRM-free platforms like GoG and adding them to Steam as non-Steam games. Same compatibility and no need to use an external tool to maintain Wine&#x2F;Proton prefixes. With the boatloads of money Valve makes from Steam, they probably won&#x27;t miss me too much. reply outcoldman 14 hours agoparentprevAre you saying, that it is actually a good experience to run Linux on the gaming laptop instead of Windows with Proton? I already have Steam Deck, and love it. But if I want to play games in better resolution, I might boot my Windows Laptop. Have not tried Linux with Proton. On Linux laptop I have a NVIDIA some kind of 3xxx series card. reply kruczek 1 hour agorootparentI switched to Ubuntu for my gaming PC over a year ago and from my Steam library of hundreds of games so far I only found maybe 1 or 2 that did not work on Linux, but worked on Windows. All the others work flawlessly** there is one caveat to that though - if you have a VR headset, then there are many people reporting performance issues on Linux with those headsets. And personally I also find VR performance subpar on Linux (although it still improved in the last year). reply majewsky 11 hours agorootparentprev> it is actually a good experience to run Linux on the gaming laptop instead of WindowsNot a laptop, but I am livestreaming games from a Linux desktop. In the roughly two years since I&#x27;ve started doing this on a regular basis, I have played 23 games on stream using Steam Proton. My experience matches what Steam Deck owners told me about compatibility. It&#x27;s not been completely glitchless, but all the problems I ran into were minor graphics glitches. When I buy games, I check protondb.com in advance, and it&#x27;s always either Platinum or Gold for me these days.I will note though that all the games I played were either single-player or cooperative multi-player. With competitive multi-player, there is an entire can of worms because of anti-cheat software. Valve&#x27;s own anti-cheat should be fine, but if the game developer is using a different anti-cheat, it usually relies on Windows kernel drivers or some other shenanigans that conflicts with Wine in a major way. reply ElectricalUnion 5 hours agorootparentA surprisingly big number of anti-cheat games work on Proton:https:&#x2F;&#x2F;areweanticheatyet.com&#x2F; reply stevepike 13 hours agorootparentprevFor me it is. Partly though that&#x27;s because I boot windows so rarely that when I do I have to sit through a whole bunch of updates.I prefer linux to windows generally so the ability to game without re-booting is a bonus on top of that. If I preferred being in windows I wouldn&#x27;t run linux just for gaming. reply pdpi 13 hours agorootparentprevYou&#x27;re already gaming on linux with Proton on the deck. That&#x27;s pretty much the experience you can expect. reply nstbayless 14 hours agorootparentprevI found that Elden Ring ran better on Proton on Linux than on native Windows on the same device. Loaded faster and ran more smoothly. I do not know why. reply ribosometronome 13 hours agorootparentThere was a while around release for Elden Ring where Valve was able to rapidly deploy fixes that resulted in the Steam Deck running without stuttering issues that were effecting powerful Windows builds. https:&#x2F;&#x2F;www.techradar.com&#x2F;news&#x2F;steam-deck-plays-elden-ring-b...I&#x27;d hope that those issues eventually got resolved on Windows too... reply heyoni 13 hours agorootparentprevI wonder if that has to do with anticheating mechanisms having to do a lot less work on a wine-based windows system than a full blown windows install. reply sph 11 hours agorootparentIn the case of Elden Ring, not really, it was due to DXVK (whose developer is sponsored by Valve) having custom patches to workaround the weird DirectX streaming logic of the game that caused constant frame hiccups.So, on day 3 or something of release, Linux was the best platform to play the newest AAA game.The year of Linux gaming has been here for a while. reply _kidlike 13 hours agorootparentprevIt usually has to do with a translation layer, converting DirectX to Vulkan (there are a few different ones, depending on the version of DirectX).And yes, gaming on Linux has been infinitely smoother and gets leaps of improvements every year. We&#x27;re at a point where unreleased games already play on Linux, and typically with better performance than naively on Windows.Controllers, audio, etc, all play out of the box, perfectly, since years now reply mhh__ 13 hours agorootparentprevMy steam deck has been basically flawless with its software with the exception of assetto corsa, because it needs a million .net installs. reply neogodless 13 hours agorootparentprevGaming on a Linux laptop with AMD CPU and Nvidia GPU was pretty good for the 2 months I did it. Not great but definitely good. Biggest problem was with a AAA game on launch day. It actually worked decently, but had a crashing bug that was quickly fixed, and the performance was never as good as what it was once I switched back to Windows and ran that game. Another game had an issue with connecting to multiplayer games that I never resolved, but otherwise worked as well as Windows (for single player.) Everything else I played seemed basically the same as my experience in Windows.Longer version: https:&#x2F;&#x2F;www.retorch.com&#x2F;blog&#x2F;linux-mint.htm reply fzzzy 11 hours agorootparentprevI was pleasantly surprised to discover almost my entire steam library works. reply thewataccount 13 hours agorootparentprevI&#x27;ve used linux as my desktop for 2+ years.The games I play like CSGO work very well*, you have a steamdeck so you know how game support especially with anti cheat is.However, I&#x27;ve had many bugs. Nvidia on linux is painful. Just linux things - I get a bar at the top of the game when starting sometimes and have to change my resolution back and forth to fix it, I&#x27;ve had to restart pipewire to get my audio to reappear, I&#x27;ve had to replace pulse with pipewire (mid game).Linux is not smooth, ESPECIALLY with nvidia. If the games you play are well supported, use AMD graphics, and doesn&#x27;t tinker with their system at all, on a very stable OS - maybe you could call it stable?Also if you want smooth - for the love of god don&#x27;t use a rolling distro. If you check the arch wiki you&#x27;ll see various nvidia&#x2F;steam&#x2F;wine&#x2F;proton issues occur every few weeks. Many completely break playing games for several days unless you downgrade packages.tl;dr - I would not recommend linux as a desktop to anyone who doesn&#x27;t mind having their nose in their terminal desperately trying to figure out why you have no audio while your friends grow tired waiting for you.The steamdeck specifically is very well managed by valve and I&#x27;m incredibly impressed that they made it work so well. reply stevepike 13 hours agorootparentMostly agreed to all this.I&#x27;ve had a better experience since I switched from arch to ubuntu. For example steam remote play works w&#x2F; my Apple TV upstairs. Under arch I couldn&#x27;t ever get it to work.I&#x27;ve had mixed experience with AMD vs Nvidia. I bought a 5700xt which was way less reliable than my old nvidia 980ti, which never once crashed under linux. I upgraded to a 6700xt last year and that&#x27;s been smooth. I&#x27;d originally bought the AMD card hoping to run Wayland but am still on X11.I&#x27;m mostly playing single player games. reply shmerl 13 hours agorootparentprevI&#x27;m running all my games on Linux on a desktop. Haven&#x27;t touched Windows in years. reply barbs 14 hours agorootparentprevWhat do you mean by \"better resolution\"? In my experience the resolution is the same on both OSs reply imtringued 14 hours agorootparentprevPeople still use windows? reply johnnyanmac 6 hours agorootparentProton has to pull from somewhere. reply INTPenis 13 hours agoparentprevI got rid of windows decades ago, so Steam has been the number one cause for my drop in productivity the last few years. And I love it. reply jjoonathan 14 hours agoparentprevNice! HDR is the one thing that keeps me booting to windows for games, but I am eagerly watching the progress. One day soon! reply heyoni 13 hours agorootparentis ray tracing also unsupported on linux? reply badsectoracula 12 hours agorootparentFor me it is supported better on Linux than on Windows because my GPU (RX 5700 XT) does not actually have HW raytracing but Mesa has a software fallback for it :-P. If nothing else i was able to see what the fuss was all about[0][1][2].Though, well, performance was a tiny bit lacking as you can see (i actually had to modify the Quake RTX code to let me use lower resolution scale than the official binary allowed :-P).[0] https:&#x2F;&#x2F;i.imgur.com&#x2F;3XNakAs.png[1] https:&#x2F;&#x2F;i.imgur.com&#x2F;AKJInNg.png[2] https:&#x2F;&#x2F;i.imgur.com&#x2F;faagg2Q.png reply squeaky-clean 13 hours agorootparentprevVaries from game to game. I know it works in Cyberpunk 2077 but you need to include some special launch parameters. Portal RTX just worked. RTX in Control doesn&#x27;t work even with config changes. reply SkyMarshal 11 hours agoparentprevI haven&#x27;t booted into Windows since 2009, thanks to Wine. Proton certainly expands its compatibility and support though, and I&#x27;ll support any company that supports Linux, and Valve is one of the best ones. reply gumballindie 13 hours agoprevLike many others here have said, Valve has won a loyal customer. The impact proton has over linux as desktop is unmeasurable. They did open source right, and they will likely benefit massively from it. reply mschuster91 14 hours agoprevOne thing that I&#x27;d love to see is a Proton version for M1&#x2F;M2 Apple Silicon... UTM is the only thing that runs x86 VMs and for whatever reason its QEMU guest tool drivers are all completely buggy and everything is dog slow (and of course, Windows refusing to load unsigned drivers on Win7 x64 makes trying out different drivers pretty much impossible). reply jshier 12 hours agoparentApple&#x27;s Game Porting Toolkit (https:&#x2F;&#x2F;developer.apple.com&#x2F;videos&#x2F;play&#x2F;wwdc2023&#x2F;10123&#x2F;) is essentially Proton for macOS. However, they&#x27;ve bizarrely restricted its use to testing your own games, and don&#x27;t offer it as a general Proton-like solution for playing Windows games on macOS. You can&#x27;t ship a game that uses it. reply kcb 13 hours agoparentprevBlame Apple for not implementing Vulkan on their GPUs. If they had a fully compliant Vulkan driver I&#x27;m sure gaming on Mac would be at parity with Linux very quickly. reply bhj 13 hours agorootparentMoltenVK exists. You might also be interested in this thread: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30755407 reply kcb 13 hours agorootparentFrom reading about dev effort to support Mac with DXVK, there are certain things Metal&#x2F;MoltenVK doesn&#x27;t support directly. So current compatibility requires more hacking as well as the additional layer of translation to hurt performance.Metal is fine, Vulkan support as well would be ideal. reply shmerl 13 hours agorootparentprevMoltenVK is explicitly not enough for vkd3d-proton. It lacks features that are now mandatory for acceptable performance. So lack of native Vulkan there is surely a deal breaker.I think VK_EXT_descriptor_buffer is a critical one, and there are probably more: https:&#x2F;&#x2F;www.khronos.org&#x2F;blog&#x2F;vk-ext-descriptor-bufferAnd that&#x27;s Vulkan 1.3 which MoltenVK doesn&#x27;t support (yet): https:&#x2F;&#x2F;github.com&#x2F;KhronosGroup&#x2F;MoltenVK&#x2F;issues&#x2F;1776 reply stetrain 14 hours agoparentprevYep, if Apple could get over their stubbornness and work with Valve on this it would be fantastic for customers of both companies.Apple has done some work with their game porting toolkit to support certain flavors of DirectX games with a combo of Wine and Rosetta. But this isn’t officially pitched as tool for end users to run Windows games. reply johnnyanmac 6 hours agorootparentI mean, they made an entirely new architecture just to keep their vertical integration running. Stubborness is basically in the DNA of the company. They still have enough draw that they can get away with it, so I guess they were pulling off \"Valves\" before Valve got popular (minus the whole \"open source\" moniker of course. but then again, Valve didn&#x27;t exactly care about FOSS per se; just a smart move for independence when you&#x27;re not yet a billion dollar corportation). reply mschuster91 1 hour agorootparent> I mean, they made an entirely new architecture just to keep their vertical integration running.No, they went for running with their own silicon because Intel couldn&#x27;t be arsed to provide them with actually power-efficient, performant and new CPUs and they were locked in to AMD for GPUs (because that is something Apple doesn&#x27;t like).It speaks volumes about just how badly Intel (and AMD) were stuck in the past when Apple blew them to pieces with the M1 performance. reply gumballindie 13 hours agorootparentprevWhy not switch to linux though? You know there&#x27;s some real cool hardware available out there that looks and feels way better than a mac, and battery aside, can do a lot more, is upgradeable and not locked in. Just a thought. reply swozey 12 hours agorootparentThis is hilariously biased. This is HN, you&#x27;re not talking to people who don&#x27;t know what Linux is. Hell, a huge portion of us are linux engineers of various sorts. You&#x27;re also in a thread literally about a linux app.Anyway. I would never, ever use Linux as a desktop environment over OSX after the experiences I&#x27;ve had with it over the last 20+ years. OSX GUI applications absolutely blow everything that Linux has out of the water. I don&#x27;t care if we&#x27;re talking KDE, i3, dwm, cinnamon. The worst part about OSX applications is they aren&#x27;t cheap. Even small apps like Soundsource a https:&#x2F;&#x2F;rogueamoeba.com&#x2F;soundsource&#x2F; are usually $30+ but they are fantastic.The thought of using pipewire and all the configuration hell you have to do to it to make it like Soundsource is a complete turn off. OSX apps work. They integrate. They look great and there are a LOT of them that do pretty unique things that I haven&#x27;t seen in Windows or Linux.I work on Linux 8 hours a day from OSX and about 30% of my time is using Win11 on my gaming desktop.Also, now that I&#x27;m on an m1 max there is no way in the world I&#x27;m going back to x86 as my productivity machine. reply augusto-moura 11 hours agorootparentThis reply is also biased, maybe we had some bad experience in the past, but the example you made, Pipewire, is one of the things I was stunned that worked out of the box. It is relatively recent but it is already installed by default on most modern distros, and no \"configuration hell\" required. Linux applications might not be as good-looking as most of Apple store ones, but most of them sure work.I&#x27;m not saying that you should switch to Linux, you do you, and to be honest Linux is not that friendly anyway. But that is not reason to stone linux to death, the parent comment is not even that offensive for MacOS users, it just asks if switching to linux would be an option, as an thought reply swozey 11 hours agorootparentOf course it comes installed so you can listen to audio. That isn&#x27;t what Soundsource does - you CAN make Pipewire work like soundsource, though. Of course my post is biased. I gave my opinion as someone who uses both vs someone who only uses linux and doesn&#x27;t know osx. Any Linux user telling me that \"Linux does more than OSX\" is getting my biased reply.Per app audio redirection, per app effects, per app volume control, system-wide effects, headphone equalizer, auto change inputs&#x2F;outputs etc. I haven&#x27;t worked with pipewire in a long time but I had to use another app with some complicated GUI of \"wires\" or pipes to get the same affect. I can&#x27;t remember the name of it now.edit: Wireplumber or Mixx&#x2F;Raysession, I forget. It was a huge pain comparatively.Mixx - https:&#x2F;&#x2F;canada1.discourse-cdn.com&#x2F;free1&#x2F;uploads&#x2F;mixxx&#x2F;origin...wireplumber -https:&#x2F;&#x2F;forum.level1techs.com&#x2F;uploads&#x2F;default&#x2F;original&#x2F;4X&#x2F;e&#x2F;... reply tiberious726 8 hours agorootparentYou sure you aren&#x27;t thinking of pulseaudio? Pipewire really hasn&#x27;t even been out \"a long time\", and does do everything you listed kinda shockingly (to me) easily.To be fair, I was a fan of pulseaudio even in the early days: being able to set up my desktop, with its decent speakers as just another audio sink for my laptop with its horrible speakers was like magic to me back in the day. reply gumballindie 10 hours agorootparentprevMy mac mini and macbook pro would attest that i do know osx and indeed i have been using it for 15 years in work environments. Right after i switched to these from linux, to which i switched from windows and freebsd. Now back to linux. It’s come a long way. reply swozey 9 hours agorootparentGotcha, sorry. reply domdabgnad 10 hours agorootparentprevlol. SoundSource looks identical to `pavucontrol` which I&#x27;ve been using for close to a decade, without ever touching a single bit of configuration, ever, for PulseAudio or PipeWire.Or a single line to install EasyEffects which adds effects in the mix, also with zero configuration.I&#x27;m not exaggerating. Tell me something you think SoundSource can do, and I&#x27;ll show you me doing the same thing with Pavucontrol or EasyEffects in my entirely stock setup.But sure. You found a screenshot of the ultimate Pipewire power tool that allows you to do things I guarantee SoundSource doesn&#x27;t, and ran with it.>They look great and there are a LOT of them that do pretty unique things that I haven&#x27;t seen in Windows or Linux.Oh? reply swozey 10 hours agorootparentlol cool. This app looks like complete trash, and I guarantee isn&#x27;t as feature filled out of the box, like I said.But sure.pavucontrol https:&#x2F;&#x2F;freedesktop.org&#x2F;software&#x2F;pulseaudio&#x2F;pavucontrol&#x2F;&#x2F;scr... soundsource https:&#x2F;&#x2F;rogueamoeba.com&#x2F;soundsource&#x2F;images&#x2F;hero@2x.pngI love those of you who think an OSX user wouldn&#x27;t know linux&#x2F;linux apps.edit: Yeah, this does nothing that soundsource does. I wish some of you would actually read the features and stop posting these volume control apps. Not to mention Soundsource was just a quick example. I could name 50 more apps that don&#x27;t have a Linux alternative or one that is of any quality.Literally - \"A simple volume control tool (mixer) for the PulseAudio sound server.\" reply domdabgnad 10 hours agorootparentThen you would name a feature that SoundSource does that Pavucontrol doesn&#x27;t. But you don&#x27;t actually know what you&#x27;re talking about.For all the talk, it should be trivial for you to make me look foolish right now. Like, so easy, right? reply swozey 10 hours agorootparentnext [–]Show me the UI for telling a specific application to redirect its audio output to a specific device automatically. Show me the UI where I plug in a specific device it switches a specific applications (not all) default automatically to the new device. Show me it&#x27;s headphone EQ. Show me per-app headphone EQ. Show me per app EQ settings. Show me the built in 3D&#x2F;spatial audio for headphonesThen show me the config file you had to edit to set all of this up. And show me how clean and attractive the UI doing all this is. Then show me it in Pipewire because I don&#x27;t care about Pulseaudio anyway and would never use that in 2023 with my need for low latency due to my DAW&#x2F;equipment. Then, in Pipewire, show me how to all this without using Wireplumber or Mixx.I didn&#x27;t say you couldn&#x27;t do things in XYZ. I said the configuration of doing so sucks compared to doing it in another app. Soundsource was just an example. I don&#x27;t know why you care so much. I could&#x27;ve said Pixelmator (GIMP?) or 30 other apps.PS: \"For all the talk, it should be trivial for you to make me look foolish right now. Like, so easy, right?\"You&#x27;re this heated up because I like an OSX app more than Linux options? You&#x27;re acting like I personally care about \"winning\" this argument. Even if it has these features I still wouldn&#x27;t use the app. Aesthetics and ease of use is important to my day to day machine. Win the \"argument\" all you want. reply allarm 13 minutes agorootparent> lol cool. This app looks like complete trash> You&#x27;re this heated up because I like an OSX app more than Linux options? You&#x27;re acting like I personally care about \"winning\" this argument. Even if it has these features I still wouldn&#x27;t use the app. Aesthetics and ease of use is important to my day to day machine. Win the \"argument\" all you want.I use osx myself and do not like desktop linux for multiple reasons. Though, the fanboys like you are exactly the reason I want to migrate just to not have anything in common with that group. tiberious726 7 hours agorootparentprevI think the application you&#x27;re looking for is easy effects, which absolutely does profile autoloading. reply Macha 9 hours agorootparentprevpavucontrol works on Pipewire too, not just Pulseaudio, for what it&#x27;s worth replygumballindie 12 hours agorootparentprev> I would never, ever use Linux as a desktop environment over OSX after the experiences I&#x27;ve had with it over the last 20+ years.You probably had enough of it. I’d be sick of it after 20 years even if it were the shiniest option out there. reply swozey 12 hours agorootparentlol You&#x27;re 100% right. I don&#x27;t enjoy fixing linux at all after work. I used to do the VFIO GPU gaming desktop w&#x2F; linux + windows passthrough but I finally got over it and just installed windows on my desktop. reply gumballindie 11 hours agorootparentAnd there’s not a single problem with that. Though these days you dont need to hack much for gaming. reply wiseowise 12 hours agorootparentprev> You know there&#x27;s some real cool hardware available out there that looks and feels way better than a macSuch as? And does it have Mac keyboard layout? Because it is absolutely superior to anything on non-Mac land. reply gumballindie 12 hours agorootparentI like the asus proart studiobook. It has the option of a 3d display without glasses. Dial doesnt work though, but the specs are pretty decent and build quality is great. Battery life is shit unless you shutdown some cores and apply other scriptable settings. Keyboard feels better than any macbook i ever owned, better than the magic keyboard. Ram, hdds and wifi card are upgradeable and are of pretty good specs. Screen resolution is high, has a touchscreen, high refresh rate, usb, thunderbolt and hdmi ports, etc.But you know taste is personal and there are plenty other options. reply franga2000 12 hours agorootparentprevWhat&#x27;s different about the mac layout? Looks fairly standard to me, even the labels seem the same, apart from the trivially remappable control&#x2F;alt&#x2F;command&#x2F;meta&#x2F;whatever keys... reply ncallaway 9 hours agorootparentprev> You know there&#x27;s some real cool hardware available out there that looks and feels way better than a macThis is obviously a personal opinion, but just isn&#x27;t close to true for me. I stopped using apple laptops 3 years ago, and I&#x27;ve never found a non-apple device with a trackpad as good as the mac. reply MBCook 13 hours agorootparentprev> if Apple could get over their stubbornness and work with ValveMaybe Value should give more an 10% of a thought to the Mac and make Steam not be terrible on the Mac first.They don’t care about the Mac. They only care about Linux because it lets them ship the Steam Deck and have a hedge against MS.The Mac is worse than an afterthought. reply grug_htmx_dev 13 hours agorootparentMac is just another tightly closed platform, Valve could get squeezed from by Apple at any time. There&#x27;s just no reason to invest in it.If you can afford a Mac, you can afford a Steam Deck. reply MBCook 7 hours agorootparent> just another tightly closed platformNo it’s not. You can download and install anything you want from the web. Just like Windows.The Mac App Store isn’t required. Largely NO ONE uses it. reply augusto-moura 13 hours agorootparentprevValve already had Wine on Linux to work with. Although we have Wine on Mac now, it is a subpar experience. Aside from that, Vulkan on MacOS never received official support from AppleI would say Valve stand on the shoulder of giants with Wine and Vulkan, they just had tl connect the dots between the two.If Apple itself would respect its gamer users it could spend sometime on these open projects, but knowing Apple, if they do anything it would be on a closed-source vendor lock-in style reply MBCook 7 hours agorootparentNone of that explains why the Steam client on Mac is so terrible and has been for a very long time.Games have their own issues (you mentioned some). But the client is inexcusable. reply sph 11 hours agorootparentprevThey did care about Linux, which was an even smaller market that doesn&#x27;t like to spend money and doesn&#x27;t like closed source and DRM. Yet now everybody is singing their praises.It stands to reason that Steam on Mac is bad in comparison because of Apple. reply MBCook 7 hours agorootparent> It stands to reason that Steam on Mac is bad in comparison because of Apple.How? It’s software like any other. There’s TONS of great software on the Mac made by 3rd parties. It can be nice. It can run fast. MS does it. Adobe does it. Indie developers do it. So why is it Apple’s fault?The story tends that gets passed around is that no one at Valve (more or less) works in it.I’m not talking about the games Steam sells. I’m talking about the Steam client.If they rewrote it in Electron (instead of whatever custom thing it uses) they could do a terrible job and it would still be faster.I’m not even asking for it to be “Mac like”, just act like showing a store page isn’t a Herculean effort. reply adhamsalama 13 hours agorootparentprevWhy would Valve do this for a trillion-dollar walled-garden? reply johnnyanmac 6 hours agorootparentBecause they \"put customers first\"? That seems to be the PR that people feel had them invest in Linux, but they forget it was more of a desperation move in a time where Windows may have been trying to strongarm 3rd party game stores out.Very smart to leverage that failure of Steam Machine into a portable form factor. A bit ugly and clearly some cut corners, but the price saved on no Windows license speaks for itself. reply MBCook 10 hours agorootparentprevWhy release a crappy product on Mac and leave it that way for over a decade?Don’t know. But that’s what they’ve done.If I were Apple, I wouldn’t partner with someone who clearly didn’t give 2 shits about my platform.(In reality Value doesn’t need Apple. And Apple would never partner anyway, they don’t do that) reply MBCook 7 hours agorootparentprevAlso, since I can’t edit my other reply, the Mac is not a walled garden.You can use any tool to make apps and sell them online without having to get them approved in the Mac App Store (which almost no one buys from). reply mschuster91 11 hours agorootparentprevBecause at least CPU-wise, the M SoCs blast a lot of the x86&#x2F;64 competition to pieces. GPU is another story, although unified memory is inherently better than the current situation on x86&#x2F;64. reply gumballindie 13 hours agoparentprevSurely, Apple can sponsor contributions to make it work, as Valve does. reply e12e 11 hours agorootparentWhat about the Apple tax, though? They have their own store to think of. reply mschuster91 11 hours agorootparentThat is only a thing on i(Pad)OS. There&#x27;s nothing to my knowledge preventing anyone from running their own app store on macOS - Steam purchases work fine, the entire Adobe suite does, as do Macports and Homebrew. reply cyberax 14 hours agoparentprevWine already supports running 64-bit binaries on Apple Silicon through Rosetta. 32-bit support is being worked on (it&#x27;s experimental).So Proton should support it eventually. reply kcb 13 hours agorootparentThe main thing that made Proton possible was the advent of DXVK. reply deergomoo 13 hours agoprevI picked up a Steam Deck recently and aside from when I’ve booted into the desktop mode to set up some emulators you’d never know it was running games through a compatibility layer. Truly incredible work from Valve.Technical impressiveness aside it’s a really nice device too—I like having something that feels mostly like a console in the “it just works” factor, but still allows me to do some fiddling if and when I want to. reply noxvilleza 9 hours agoparentIt&#x27;s truly a brilliant holiday device: gaming on the go, but also able to plug into a keyboard&#x2F;mouse (maybe a TV) and get some minor work done. reply abtinf 14 hours agoprevIt’s been many years since I was current on the state of cross platform gaming.Is it possible that proton could become the de facto target platform?That is, if a developer builds to ensure that their game works correctly under proton, then will it also work correctly under windows? So by just ensuring it works under proton, which seems to be minimal effort, do they get access to the expanded market for “free”?Do any of the consoles support proton, so that the only barrier to releasing a game are the legal agreements? reply peoplearepeople 14 hours agoparent> Do any of the consoles support proton, so that the only barrier to releasing a game are the legal agreements?Steam deck reply suprjami 9 hours agoparentprevTargeting Windows and Proton is already the status quo if you want the green tick on Steam, and apparently tens of thousands of game developers do because Valve promote Deck-compatible games heavily and that audience is more likely to spend money.You can build a \"Steam box\" console with any Linux distribution and Steam&#x27;s Big Picture mode. Autologon and start Steam in Big Picture mode on startup. I have this on Debian on a NUC, and there&#x27;s a distro ChimeraOS which offers a polished pre-installed A&#x2F;B update experience too. reply augusto-moura 13 hours agoparentprevSupport for Proton doesn&#x27;t imply full support on Windows out of the box, although it should be easier to fix problems on Windows than the other way around.The thing is, currently, having a game built for Windows directly produces a much more performant game on Windows. Wine still has some hiccups here and there. reply tmccrary55 14 hours agoparentprevMicrosoft definitely wouldn&#x27;t let that happen. reply chongli 14 hours agorootparentHow will they prevent it? reply erinnh 14 hours agorootparentForce the move to their UWP platform even more?Only release DirectX 13 with UWP support etc. reply SoKamil 13 hours agorootparentUWP is dead. reply erinnh 13 hours agorootparentIs it?Not really up to date on Windows stuff, but isn’t Microsoft only using UWP for all their (new) stuff?Xbox launcher and all games on it are UWP, aren’t they? reply DiabloD3 11 hours agorootparentUWP is dead, and a lot of the people who were pushing that weirdly locked in platform have been pushed out of the company.All future stuff is moving to WinUI 3.x (which belongs to the same family of C# XAML UI libraries, and Visual Studio has a wizard to help you turn your UWP app into a real C# app) and the Microsoft Store is no longer locked to UWP apps only and WinUI 3.x is officially supported on other non-CLR langs (such as calling it from C++ or Rust) and WinUI 3.x is coming to non-Windows platforms (such as Linux).Microsoft isn&#x27;t going to transform all their own apps overnight (for simple apps, updating a UWP app to a C# WinUI 3.x app buys you nothing), but will happen over time as WinUI 3.x-only features are added to those apps.Microsoft set themselves on this path starting uh, like 5 years ago? They publicly announced it about 3 years ago. reply AtlasBarfed 11 hours agorootparentWell, it&#x27;s Microsoft, so every 5 years they release a new API&#x2F;framework and declare the old one dead&#x2F;obsolete. But they usually maintain backwards compatibility to their credit, and I almost never credit Microsoft with anything.whatever was before MFC (I remember using a Charles Petzold book in 1997) --> MFC --> dotNet (many versions) --> XAML&#x2F;WPF --> UWPand I guess WinUI now. reply DiabloD3 10 hours agorootparentThat isn&#x27;t quite right.Notice how WinUI 3.x is the first thing named publicly named WinUI, but its already at 3? UWP is 2.x, Metro&#x2F;WinRT&#x2F;WPF is 1.x. They&#x27;re all part of the same family of XAML UIs.The part that got hard killed, and backwards support was absolutely 100% murdered was the Silverlight stuff, which was a XAML predecessor (and RIA runtime to compete with Flash&#x2F;Air) written entirely in C#. For performance reasons, the WinRT&#x2F;WPF->UWP->WinUI 3 branch of the family is largely WinRT-dialect C++.So no, Microsoft actually hasn&#x27;t abandoned the tech, they abandoned the dumb idea that all apps need to work on the Windows Phone (which no longer exists) and must be released on the Microsoft Store (which now allows apps in other languages), and everything has to be written in C# (which, again, is no longer forced on programmers), and everything needs to be Windows-only (Project Reunion aka MAUI brings both CLR and non-CLR WinUI apps to Linux).Also, I&#x27;m not sure why you&#x27;re branching WinUI family off from .net, as WinUI is the UI toolkit for CLR apps. replysznio 12 hours agorootparentprevWine on Windows. replyeverdrive 11 hours agoprevI&#x27;ve been running solely on Linux since something like 2015. I was primarily on Linux from ~2006 or so, but did switch to Windows for a couple of years to play some Windows games. (this was before Proton really existed)I&#x27;m probably less technically savvy than a lot of the HN crowd, and I just have not ever had a significant problem running linux. For sure, I&#x27;ve avoided some problems by being picky about what PCs I buy. But for the most part it&#x27;s been pretty pain free. Running Linux full-time has been effortless and easy. And thanks to Valve, gaming on Linux gets better every year. I played Elden Ring on launch, and on launch, it actually played better than PC.To the extent that HN types have trouble with Linux, I can only imagine it&#x27;s because they&#x27;re doing _more_ on their PC than me. ie, they have some software or project that just needs to run a certain way. For sure, Linux isn&#x27;t always perfect for that. reply sph 11 hours agoparentLinux is great for both the non-techie layman and the computer wiz. Those that complain are the majority on people you frequent on HN which know enough shell commands to be dangerous but couldn&#x27;t unfuck a broken fstab.Which is fine, mind you, just pointing out that most people that are NOT in tech would find Linux very usable, if not more usable than Windows. reply NelsonMinar 12 hours agoprevProton is a remarkable accomplishment; I never believed it would have worked as well as it does. It helps that newer games are increasingly based on a couple of engines (Unity in particular) but its support for older games is also impressive.Proton is the basis of Chromebook&#x27;s experimental Steam games support. It works pretty well, as well as Proton itself as far as I can tell. reply MrDresden 14 hours agoprevThrew my Windows partition away 4 years ago and haven&#x27;t looked back since. Steam has earned my business with their work on Proton. reply heyoni 13 hours agoparentI keep windows for 1 reason: call of duty. all those games with kernel level anticheats have trouble running on linux apparently :\\ reply gpderetta 12 hours agorootparentI have had luck running CoD on a VM with GPU passtrough. It is not necessarily less cumbersome than dual booting though. reply heyoni 12 hours agorootparentThat is my next goal right right after I master NixOS! The learning curve is steep for nix itself and I know vfio is no walk in the park. I want to make sure if I&#x27;m doing complicated stuff like that, I can at least roll things back to a clean state if they break. reply frio 12 hours agorootparentI followed this path too; it all worked, but the magic vfio incantations can change kernel to kernel, so there was always a fair bit of maintenance involved. I&#x27;d use Proton for 95% of my games, so whenever I had to fall back to vfio&#x2F;Windows for a game for a while, everything would&#x27;ve changed. NixOS made this manageable, but it was never pleasant.Have fun with the project; I&#x27;d be curious to know if (like me) you end up (sadly!) continuing to dual-boot for Windows only games. VFIO seemed like the dream end goal, and maybe it&#x27;s gotten better since 2021, but for me at least, it got in the way more often than not.I have rigged up a non-VFIO VM that boots off the same Windows partition though, so that I can log into it and run Windows updates once in a while without needing to do a full reboot. reply heyoni 12 hours agorootparentI&#x27;ll let you know. My priorities as far as \"difficult things i desperately need to do\" include better note-taking (org mode&#x2F;roam) and learning enough about AI to put it to use. VFIO is definitely a dream but with dual booting it&#x27;s also at the bottom of my list. replyZhyl 14 hours agoprevWhen Proton came out I had multiple people on HN going on quite long rants about how Linux would never be viable for gaming.One multi-million-unit-selling handheld gaming system later and I don&#x27;t see those arguments much anymore. reply throwaway89201 13 hours agoparentIt makes for a very boring discussion (and makes this a boring comment) to recall some random person who ranted in a way that didn&#x27;t age well. If you&#x27;re going to make that point, at least link to the thread to allow reading some interesting comments there instead.If your memory is about [1], which is the first main topic on Proton, the criticism seems quite muted (and otherwise downvoted).[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=17815892 reply johnnyanmac 6 hours agoparentprevI never particularly doubted Linux working for gaming, but I&#x27;m not sure if a compatibility layer to the actual dominant platform is really the long term win to celebrate here. Great for customers, but a ticking time bomb for the FOSS crowd (aside from Steam being proprietary ofc. More talking about the Microsoft dependency). reply AtlasBarfed 11 hours agoparentprevIt was a safe prediction to make. Even SEGA couldn&#x27;t do a console anymore, Linux was behind MacOS in games support, all app developer companies whined about Linux being unsupportable and no market. \"The year of the Linux desktop\" was a recurring joke (still is with the GNOME &#x2F; Unity churn, Wayland switchover, snaps breaking how GUI apps interact with a filesystem, and the usual perpetual churn for churn&#x27;s sake making system support and OS upgrades a crapshoot). reply idle76 14 hours agoprevAlso check out Glorious Eggroll on Gihub for even better performance. reply FirmwareBurner 14 hours agoparentOn the topic of GE, anyone try his custom Fedora distro, Nobara[1] and has any feedback on it?Is it daily-drivable on a laptop without any issues, and stable long term without any breakages? Not just for gaming, but for daily entertainment and productivity task of those with one distro for everything?I tried installing it a while back and found the default installer partitioning much more janky and messed up GRUB making my system unbootable, unlike other \"Just Works™\" distros like Ubuntu, EndevourOS or Mint where they worked flawless on multi-boot systems.I&#x27;m not knocking it, I can imagine it&#x27;s tough for a single developer to do all that and test everything, it would just be cool to know if it&#x27;s gotten some polish now and how stable it is to daily drive.[1] https:&#x2F;&#x2F;nobaraproject.org&#x2F; reply johnny22 14 hours agorootparentI&#x27;ve only heard good things about it, but i see no reason to use it. making fedora do what it does is easy enough. reply FirmwareBurner 14 hours agorootparent>i see no reason to use it. making fedora do what it does is easy enoughYes, people working in tech and with free time can do that, but many don&#x27;t want to go down that rabbit hole anymore and fiddle with their OS to get it to where Nobara is, and would rather go for something that&#x27;s already preconfigured out of the box for the best gaming experience where everything is already set and you can immediately start installing and playing games after installing the OS, similarly to Windows is. That&#x27;s the whole point of these distros. reply slikrick 14 hours agorootparentprev\"Easy enough\" sure, you keep up with all those patches, manually, on your own system, then say it.This site literally can&#x27;t not be pretentious reply FirmwareBurner 14 hours agorootparent>This site literally can&#x27;t not be pretentiousInfamous Dropbox comment: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=9224 reply johnny22 11 hours agorootparentprevmost of those things other than the codecs and steam from rpmfusion just aren&#x27;t necessary for most folks is all.I&#x27;m not trying to shame folks from using an easy route. You read my comment extremely uncharitably.I do think one should be careful of relying on a project with a bus factor of 1 for your OS. reply oaththrowaway 14 hours agorootparentprevI have it installed on my gaming PC. It is fine reply haunter 14 hours agoprev11 years ago today (2012, Aug 21) Valve released CSGO forever changing gamingсука блять reply sph 11 hours agoparentFor the first time. I&#x27;m surprised it&#x27;s been only 11 years. It feels like aeons ago since I got my MG2 rank playing competitive pretty much every day.I guess I&#x27;ll have to brush up my rusty AK one taps again when the new Counter Strike comes along. reply calderknight 14 hours agoparentprevDon&#x27;t look at a calendar, it will be a depressing experience. reply asmor 14 hours agorootparentto be fair, CSGO was a failure for the first few years of its existence. so it&#x27;s likely you&#x27;re remembering it best from around 2016.all I know is that I have the operation payback challenge coin, but no real memory of those maps. i do wish they&#x27;d bring back some of the operation maps... (santorini anyone?) reply freeflight 12 hours agorootparentAs far as I remember CSGO did way better than CS:Source, which never really caught on until the point where it became a bit of a \"retro\" curiosity. reply noxvilleza 9 hours agorootparentMost of the pros stuck with 1.6 instead of moving to Source (and most existing tournaments stuck on 1.6 - although notably CGS went with CS:S). In Source the movement was clunky, the sound system was awful, and the shooting & spray felt bad. The overall physics was just very strange, it felt like you were walking in honey - although did get better (a long time later).Almost a year before CSGO official launch there was a showmatch for it and it looked awful. A bunch of small things were improved by the launch, but it was not good at all (look back at 2012 CS:GO footage and you&#x27;ll be shocked). Sound was inaccurate and inconsistent, molotovs and smokes were glitchy, wallbangs didn&#x27;t work in a sane manner ... it was pretty atrocious. That said, 1.6 was basically ancient and Valve (who didn&#x27;t develop the game) pushed for CS:GO to be used in competitions, so it was used. Over time it was increasingly improved and there was an influx of existing (1.6 & CS:S) pro players towards the end of 2012 and early 2013; and an explosion towards the end of 2013. CS:GO came out in August 2012, but only crossed over the 100k concurrent player mark (which is the all-time CS:Source record) in ~ December 2013.Skin betting became rampant on CS:GO after the August 2013 Arms Deal update, with sites like CSGOLounge having absolutely huge overall parimutuel pots on basically ever game. This pushed up the popularity of the game - even a small local tournament could have 10k viewers if the game was listed on CSGOLounge [and the bigger matches had millions on the line].Eventually, years later, CS:GO was improved to the point where it could stand alone without propping up by the skin gambling - and since then it&#x27;s grown much further; but in reality both CSGO and CS:S had very similar and buggy first few years. reply calderknight 13 hours agorootparentprevAs it happens, I was an early adopter. I have my 10-year badge just from CSGO. I was SMFC in ~2013 (it was easy at that point in time, I get destroyed now).The game has changed a lot. The original version of CSGO wasn&#x27;t even made by Valve Software. But every map in the Active pool is either a totally new map, or is an old one that&#x27;s been re-built from the ground up.And I have a few skins and a bunch of cases left over from back then. They were worth $0.03 each back then. But the game has grown 500x so demand has increased while the cases actually reduce in supply over time... reply haunter 14 hours agorootparentprevDamn yes my bad reply calderknight 14 hours agorootparentCovid years flew past reply baq 13 hours agorootparentThe days were long though reply freeflight 12 hours agoparentprevI&#x27;d argue the original mod&#x2F;1.6 had a much bigger impact on gaming than CSGO.The impact of Steam, as the first digital distribution platform for video games, is probably magnitudes bigger again.Valve was so far ahead of the competition, and still remains there to this day.Which is kind of a miracle, in some alternate timeline we could have ended with EA as the patron of PC gaming. reply sebazzz 14 hours agoprevI run winesapOS[0] which is based on SteamOS and was able to repurpose an old Toshiba notebook for my kids, running Freddy Fish* and other games. Windows 10 doesn&#x27;t run smoothly (even though it came with Windows 10) and I&#x27;m not going to run anything older than that.Steam also makes it very easy to switch between Proton versions. If version X doesn&#x27;t work, perhaps version X-1 or X+1 does work, or even the experimental version.* Yes, ScummVM is available for Linux but the Steam Linux distribution included an old ScummVM which wasn&#x27;t able to load some libraries. Running the Windows version through an compatibility layer was easier than modifying shell files to run the latest ScummVM through flatpak.[0]: https:&#x2F;&#x2F;github.com&#x2F;LukeShortCloud&#x2F;winesapOS reply gabereiser 14 hours agoprevProton was the missing piece to Linux gaming&#x2F;desktop. It’s also a vital part of steam deck. Maybe someday we’ll get a standard rendering pipeline :&#x2F; &#x2F;s. reply NietTim 12 hours agoprevJust went and checked, out of my top 20 games only flight simulator and space engineers do not work. I had completely missed that proton has come so far...I think this means I can finally truly ditch windows on my game&#x2F;virtualisation machine reply zeta0134 11 hours agoprevWell this is a topical place to ask this tangent I suppose. My GPU is on the fritz and I&#x27;m looking to replace it. Has anyone gotten SteamVR working decently under Linux through Proton? If so, what GPU do you have? It&#x27;s basically impossible to search for information on this, but I really want to develop for VR, and I&#x27;d like to not have to boot into Windows to do it reply sharts 4 hours agoprevIs proton actually good now?I tried it a few years ago to play some not-uncommon game and it was absolutely terrible, regardless of the hardware power available. reply roody15 12 hours agoprevPlaying Baldurs gate three on unbuntu. Haven’t booted a windows machine at my home now in years!! reply SirMaster 12 hours agoprevProton might be great for the game itself. But something needs to be done about anti-cheat.I only play multi-player games, and when I search ProtonDB none of the games I play say they are supported.Call of Duty, Battlefield 2042, Rainbow 6 Seige, PUGB, Destiny 2 reply Apocryphon 11 hours agoprevFor someone with a Mac (non Apple Silicon), does it make sense to run Linux in a VM in order to play Proton-supporting Steam games? reply jwells89 13 hours agoprevI&#x27;d love to go Linux full-time on my gaming machine, but for now VR (Beat Saber, primarily) is keeping me tethered to Windows. It&#x27;s no fault of Valve&#x27;s, though… the blame falls entirely on Facebook for insisting on their proprietary Oculus client, not supporting Linux with that client, and not doing anything to make that client work with Proton&#x2F;WINE. reply jauntywundrkind 14 hours agoprevI don&#x27;t know how common it is, but the AI War developers posted recently about their new game & said Steam recommended they drop their Linux port, saying that just relying on Proton would be better performing & a better use of time.I&#x27;m very curious how widespread this advice is. It&#x27;s a little sad & ironic to me if Proton is now entrenching Windows as the only target platform. I&#x27;m hoping this wasn&#x27;t blanket advice Valve is handing out but really something specific to the team. reply FirmwareBurner 14 hours agoparent>Steam recommended they drop their Linux port, saying that just relying on Proton would be better performing & a better use of time. I&#x27;m very curious how widespread this advice is. It&#x27;s a little sad & ironic to me if Proton is now entrenching Windows as the only target platform.Very. Win32 ABI is the most stable and rock solid compared to the messy moving target of the Linux world, so recommending it is not entrenching Windows, just very good advice to save time and resources to target a platform that&#x27;s alredy known and stable, and since the target is fixed and well known, it makes it also very easy to emulate on their end for Linux.As much as FOSS-Linux evangelists hate it, Win32 emulated on Linux is better for everyone, than tryin to port games natively on Linux, and makes the most business sense of you want the same games on Linux. reply jhbadger 14 hours agorootparentIt will be interesting to see if the Win32 ABI survives as a standard into the future beyond the point that Windows itself will continue to run it in in the same sense that commodity PCs are still based on the IBM PC architecture even years after IBM itself stopped making PCs. reply extraduder_ire 14 hours agorootparentprevIs it still called win32 nowadays? Because that name has existed for a very long time now, and early win32 stuff doesn&#x27;t even run on windows without sysWOW64 stuff. I&#x27;m sure some versioning more granular than windows version names exists, but the extent of my windows knowledge is occasionally reading newoldthing. reply runjake 14 hours agorootparentIIRC, it&#x27;s called the Windows API now, with Win64 being the active variant on modern Windows. It is essentially Win32 with 64-bit additions.The Win32&#x2F;low-level Windows programmers I know still just refer to it as \"Win32\", though. reply FirmwareBurner 14 hours agorootparentprevI don&#x27;t know what the modern ABI is called, I only quoted what was in the back of my head so I could be wrong on the name but my point still stands. reply seabrookmx 14 hours agorootparentprevThe other technical solution to this problem is flatpak (and the others in this space). They vendor the libs (including libc) so you avoid the linking issues.In the gaming world where steam installs apps itself and you need the Windows version for commercial viability anyways, it does make sense to push them to win32.Outside of gaming though, I don&#x27;t really see this being the case. You random desktop apps are better off with flatpak and distros like Fedora are already moving everything that direction. Flathub also has the benefit of allowing devs to push to one repo and support multiple distros (again, like Steam). reply sph 4 hours agorootparentProton btw uses the same technology as flatpak for its sandboxing capabilities. And Flatpak Steam has been working great for a while.It&#x27;s all Linux containers&#x2F;namespaces after all. reply memefrog 14 hours agorootparentprev>Win32 ABI is the most stable and rock solid compared to the messy moving target of the Linux world,Blatant lie bordering on disinformation.The only reason it is a \"stable ABI\" is that every dev of every windows program bundles every library with their program. You could do the same thing on Linux and the stability \"story\" would be exactly the same, or in fact even better on Linux because Win32 isnt actually stable and the Linux system call ABI is. reply kcb 13 hours agorootparentPlenty of games require VC++ redistributable to be installed. I have all of them from 2008 to 2022 installed right now without issue. reply memefrog 8 hours agorootparentThat is meant to be distributed with the game (hence the name). reply becurious 11 hours agorootparentprevNo, they don’t. They may bundle additional DLLs but all those DLLs call the win32 APIs. reply memefrog 5 hours agorootparentAnd? All of the shared objects you bundle on Linux will eventually call system calls to do anything.What does \"no they don&#x27;t\" mean? People absolutely do bundle all their dependencies on Windows that aren&#x27;t part of the stable platform guaranteed to be around forever.On Linux the part of the stable platform guaranteed to be around forever the system call ABI. reply mistrial9 14 hours agorootparentprevdo you have first-hand knowledge of the structure of the code base that ports to both Linux and Windows for this particular game? reply FirmwareBurner 14 hours agorootparentValve and others have explained it already why in general terms that apply to all games on the native vs emulated situation, no need to go into the pedantic details of one specific game. reply mistrial9 14 hours agorootparentas a framework author and C++ coder, the code base structure is not \"excessively concerned with minor details and rules or with displaying academic learning.\" -- certainly not for you in any case, by your own statement reply FirmwareBurner 14 hours agorootparentNo need to be snarky and flash your coding street cred to make yourself big and make me look dumb.I&#x27;m not the smartest tool in the shed, but I was not trying to be snarky and undermine your superiority or downplay your question, I was just saying it doesn&#x27;t matter as the end goal is to get Windows games playable on Linux, and for that Proton emulation is the way that makes sense financially and practically for both developers and gamers, so no need to dive into the codebases and compare them, just to prove a point that&#x27;s moot from the get-go. replyCOGlory 14 hours agoparentprevProton almost always runs better than Linux native. Windows is the only stable ABI on Linux, now. reply saltcured 14 hours agorootparentProton leads to some bizarre behaviors though. My worst experience is on a surplus \"mobile workstation\" with Optimus graphics. It has a pretty robust Intel quad-core and one of their better iGPUs as well as an NVIDIA Quadro M5000M which has decent VRAM and should compare almost to a GTX 1650.The packages pulled in by steam define a bunch of vulkan providers which then confuses steam. Many games won&#x27;t even launch and you have to manually kill off a bunch of steam worker processes to even successfully shutdown and restart steam. You can&#x27;t uninstall the package due to dependencies, so instead have to manually move&#x2F;rename some files under &#x2F;usr&#x2F;share&#x2F;vulkan&#x2F;icd.d&#x2F; to only leave the one for NVIDIA.Then, games launch but have very inconsistent performance. I don&#x27;t know if this is because Optimus is competing with the game for PCIe bandwidth, or something else still going awry with the driver stack. reply gjsman-1000 14 hours agorootparentprevArguably, WINE has always been the most stable ABI on Linux. Good luck just getting a 5-year-old binary of any desktop file manager running on a modern Linux installation.I’m dead serious. Grab a copy of Nautilus or GNOME Files from Ubuntu 16.04; try running it on Ubuntu 22.04. It isn’t easy. Now imagine a game. reply crickey 14 hours agorootparentThis is only because of the nature of dynamic linking. Have a statically linked executable and you should be fine. Not that it should be an issue to get old software to run you simply need to download the dependent lib versions. Anyone who sais this i feel hasnt worked very much with software, not that you should need to thats up to the one distributing the executable reply brigade 14 hours agorootparentWhat you mean is: the only stable ABI in Linux is the Linux kernel’s itself.Windows is the opposite: the only stable ABI is the dynamically linked user space ABI. So yes, it’s perfectly possible to have a stable dynamic ABI across a dynamically linked boundary. reply memefrog 14 hours agorootparent>Windows is the opposite: the only stable ABI is the dynamically linked user space ABI.... to the kernel. So not the opposite at all. In fact exactly the same. reply brigade 11 hours agorootparentNo. The userspace&#x2F;kernel boundary is explicitly not stable on Windows, and binaries that try to use that interface have broken when it changed. reply memefrog 5 hours agorootparentThat is completely irrelevant to the discussion. The point is that both systems have very stable ABIs. Windows has a somewhat higher-level stable ABI, but as a result it is a much wider surface to keep compatible, it breaks much more often, it requires a lot more hacks to keep it stable over time (program-specific hacks kept around for decades), etc.This is the point of difference: the layer at which each is stable. NOT whether Linux is stable. reply brigade 4 hours agorootparentThe nature of stability literally is the discussion; I replied to a post that blamed the lack of a Linux equivalent to Win32 on dynamic linking. That the stable ABI you get on Linux requires you to bundle literally every dependency you have as if distros don&#x27;t exist... And then still have issues because the kernel ABIs for graphics are entirely GPU dependent...The two approaches are definitively not the same (as you claim), and the significant shortcomings of Linux&#x27;s approach are why Win32 is becoming the ABI devs target even on Linux. reply Macha 9 hours agorootparentprevThis is true for basically ever OS other than Linux, as the Golang developers learned the hard way on macOS and BSD. reply memefrog 5 hours agorootparentwhich is why the Linux approach is superior: it has a stable ABI that is language-agnostic. reply imtringued 13 hours agorootparentprevThere used to be operating systems where the only stable interface was glibc. reply gjsman-1000 14 hours agorootparentprevUnless you statically linked GTK, possibly Xorg, and more into a ludicrous bundle size, you’d still be screwed.As for “just get the libs,” that’s hilariously easier said than done. Try my example - it’s enough to make an engineer cry. reply filmor 14 hours agorootparentYour example doesn&#x27;t make sense. Linux distributions have always had this trade off: Binaries provided through the package manager work with the libraries that are provided by the same package manager. I have a bunch of older gog or Humble Bundle Linux releases of games that still work fine on my system because (Windows style) they carry around all of their libraries with them. Linking Xorg doesn&#x27;t make sense and applications linked statically against libX11 will work perfectly fine even with Xwayland. reply crickey 14 hours agorootparentprevhttps:&#x2F;&#x2F;www.x.org&#x2F;wiki&#x2F;Releases&#x2F; https:&#x2F;&#x2F;www.gtk.org&#x2F;docs&#x2F;installations&#x2F;linux&#x2F; I mean it will probably not be painless and other applications u run might break* but xorg is relativly stable. Liba are out there are free to get. Usually people are arguing that the conveniences isnt there not that its not possible.* if u dont sandbox this a bit with custom lib paths reply gjsman-1000 14 hours agorootparentI didn’t dispute that it isn’t possible.What I am disputing is how this comes off to a game developer; 5 years from now, heck, 2 years from now when their games require library surgery to keep running... that’s just an awful experience.That is not what a developer would consider a stable ABI. They could look into Flatpak - but look at what’s trending on Hacker News today - a rant against Flatpak.Win32 over Proton is the winner for them; all other proposed solutions are hilariously naive and optimistic to what game development requires. No game developer is ever going to individually package, and consistently repackage, their game for 20 distributions. That’s never going to happen. reply memefrog 14 hours agorootparentThere is no \"library surgery\". This is EXACTLY what people are forced to do on Windows already: bundle all dependencies.>No game developer is ever going to individually package, and consistently repackage, their game for 20 distributions. That’s never going to happen.Nobody has suggested they should. reply smoldesu 14 hours agorootparentprev> No game developer is ever going to individually package, and consistently repackage, their game for 20 distributions.Nor do they. Steam Linux Runtime exists. reply crickey 14 hours agorootparentprevWell u sure made an effort to exclaim how hard it would be. If a developer had an install guide with links to dependencies or mirrors to those dependencies it wouldnt be very hard as they should have internally for their dev&#x2F; testing. Do windows devs not track their dependencies? Relying only on Win32 ? Whos the naive one ? reply memefrog 14 hours agorootparentprevNo video game has any use for GTK+.XLib is tiny by modern standards. reply simion314 12 hours agorootparent>No video game has any use for GTK+my memory might be wrong but I had issues with native linux games that had level editor based on GTK and python, could not get them to run after 3 years since launch, I do not claim it is impossible just that I could not do it with some a few hours effort. reply memefrog 14 hours agorootparentprevWhy would you try to run Nautilus from 16.04 on 22.04? It was distributed in a way designed to be run on 16.04.Are you under some weird delusion that you are forced on Linux to develop software in one particular way? reply baq 14 hours agoparentprevSad? A bit. Wrong? Not in the slightest. WinAPI is the stable Linux ABI now, that’s simply the status quo. reply memefrog 14 hours agorootparentRepeating this FUD meme over and over again does not make it true. Yes we get it. You read an upvoted HN comment once that said \"Windows API is the stable Linux ABI now\" and know you can get upvoted if you just repeat it. But it is simply false. reply baq 13 hours agorootparentMy steam deck doesn’t need opinions to work. It just does. reply ShamelessC 14 hours agoparentprevDo you want more games to be played on Linux or not? Because we&#x27;ve been trying the whole \"please target native Linux\" thing for awhile now and it simply isn&#x27;t appealing enough to ever work.Evangelism is useless in the face of results. reply memefrog 14 hours agorootparent>Do you want more games to be played on Linux or not?This is meaningless if you change the definition of \"on Linux\" to mean \"on Windows\". reply ShamelessC 13 hours agorootparent“on Windows” is no longer a worthwhile phrase when it simply means “targeting a Linux&#x2F;Windows shared compatibility ABI”.There are more functioning games on Linux than I ever would have thought possible because of this (very difficult and certainly open source) work that’s been done by Wine, Proton, Valve and many many more. But of course, someone has to find a way to discourage all of that because it isn’t pure enough or whatever. reply mcpackieh 14 hours agoparentprevIf the devs test with Proton, then I don&#x27;t see a problem. Proton is open source, so the future of games that run well with Proton seems secure. reply stonogo 13 hours agoparentprevSeems a pretty obvious way to preserve value-add in the face of competing game services. GOG and so forth don&#x27;t have Proton, so the simplest moat to build is to curtail native Linux development and protect Proton&#x27;s status as the \"default\" approach to getting games running there. reply joveian 7 hours agorootparentUnfortunately, GOG seems to have mostly given up on Linux and often doesn&#x27;t provide available Linux builds (I think there may have been one main employee who pushed for better Linux support who left years ago). There is at least one \"Proton without Steam\" build that I haven&#x27;t tried:https:&#x2F;&#x2F;github.com&#x2F;GloriousEggroll&#x2F;wine-ge-customPersonally, I&#x27;ve been using Windows fully offline with GOG games, although I&#x27;d like to some day try putting together a Linux based offline game console (or maybe NetBSD based if Wine is sufficient). reply ndjdjfjfj 14 hours agoparentprev> relying on Proton would be better performing I tried Overwatch 2 with Proton and performance was abysmal. My computer seemed to be melting and FPS was jumping all over the place.It does 144fps on Windows 11 without sweating (same PC). reply hackyhacky 14 hours agorootparentGP isn&#x27;t saying that Linux is faster than Windows; they are saying that a Windows app running under Proton under Linux is faster than a native Linux app running under Linux. reply netbioserror 14 hours agorootparentprevI highly recommend tweaking things like Proton versions (try GloriousEggroll for example). I&#x27;ve been playing OW and OW2 on Linux for years, had very few issues since the beginning and excellent framerates. reply smoldesu 13 hours agorootparentYep. It also really depends on your hardware.In my experience, I had no problem playing Overwatch 1 at 144fps through Proton on my 1050ti. reply acomjean 14 hours agorootparentprevI have a notebook (Nvidia 10xx something gpu), 8th gen intel I think. It gets plenty warm playing games, but the games play fine on PopOS. The fact that they run is pretty great.But I think of the steam deck which is basically a Linux PC. Its lowerer spec, low power, but it seems to run pretty well without terrible battery life. reply vinyl7 14 hours agoparentprevGame developers have stated that the support requests they get for Linux outweighs the revenue they get from supporting it. There are so many distros with different packages&#x2F;dependencies, drivers, package managers etc. that it&#x27;s very time consuming to keep up and not worth it. reply taeric 14 hours agorootparentI recall seeing similar with major caveats, though. The general \"make linux support and they will come\" crowd is clearly too big to be meaningful. That said, there was a story of someone that had a bug cut from a linux user that commented how it was one of their best support cases ever. User was engaged and willing&#x2F;able to work with them in ways that few support cases are.As such, this is a mixed bag. Actual users that cut support issues from random machines will be worth trying to engage with. General cries that you should support it with no payment in play should likely be ignored, though. reply johnny22 14 hours agorootparentprevSteam has provided their own container runtime for quite some time, so the packaging and dpendendies are a lot of a problem if you only distribute on steam.I still prefer they go with the proton approach though. reply olliej 14 hours agoparentprevI mean that&#x27;s what Apple has done with \"Game Porting Kit\" - valve has pushed \"windows is the platform\" to such an extent that no one even tries to write portable code for games and just targets directx. reply tegmark 9 hours agoprevi cant wait for counter strike 2 reply senectus1 9 hours agoprevVR and DRM are the two things that keep me from wiping my windows partition. reply hanniabu 14 hours agoprev [–] I really wish the linux community would polish off the UX for ubuntu so it&#x27;s more attractive and usable for mainstream reply troad 33 minutes agoparentI direct newbies to Kubuntu; I&#x27;ve never had anyone struggle with using it - modern KDE is an incredibly intuitive experience - and any tech support is easy as they&#x27;re pretty much on stock Ubuntu.Honestly I think a lot of newbie gripes about \"Linux\" are just gripes about Gnome, sometimes quite justified ones. reply ilc 12 hours agoparentprevI really wish Ubuntu would stop going their own way and work with everyone else.And that&#x27;s a more general comment than just this. reply shmerl 13 hours agoparentprevYou don&#x27;t need to use Ubuntu. A distro with recent KDE release is better. reply barbs 13 hours agoparentprevI think these days there are other distros that do it better, like Pop!_OS reply hanniabu 13 hours agorootparentIn your opinion how does Pop!_OS it better? reply dcgudeman 11 hours agoparentprev [–] If you think that UX polish is the reason why \"mainstream\" people use microsoft&#x2F;apple products over linux you are really deluding yourself. reply mmercedes 11 hours agorootparentnot a productive comment to call someone deluded without at least explaining why. The UX does seem like a significant hurdle to maintream adoption to me. reply dcgudeman 3 hours agorootparentDoes it really require an explanation? All you have to do is observe that literally everyone besides a few free software zealots use \"non-free\" operating systems. What company is developing consumer linux computers for retail sale? If someone even wanted to buy one they would have to really look. There are many unknowns about what programs will run on linux. Will a buyer encounter a situation where they are literally blocked from doing what they need to do because they are using a fringe operating system? There are a million services that have integrations into these operating systems that would go out the window with linux. And this doesn&#x27;t even begin touch the entire ecosystem of enterprise device management. But yeah maybe it&#x27;s the UX. Just a couple more polished features in GNOME and then Walmart will roll out an entire fleet of new linux computers for their financial department! And grandma will finally switch over to that sweet FOSS laptop! And genz will drop their nice macbook pros and get on a nice fedora laptop! Just one more UX release. reply hanniabu 6 hours agorootparentprev [–] If you make it polished, it will be usable to more people. More users equal more of their favorite apps being made for linux. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Valve's release of Proton, a modified version of Wine, transformed Linux gaming five years ago by enabling popular games to run on the platform.",
      "This sparked a surge in Linux gaming and contributed to the creation of the Steam Deck.",
      "Proton's ongoing enhancements have improved compatibility and performance, granting Linux gamers access to a diverse selection of games, leading to enthusiastic user feedback."
    ],
    "commentSummary": [
      "Valve's Proton gaming platform has gained popularity among users as a viable alternative to Windows for gaming on Linux.",
      "Valve is praised for its customer support and dedication to open source desktop, while Epic is criticized for lacking in these areas.",
      "Concerns are raised about Epic's exclusivity deals and the decline of DRM-free platforms."
    ],
    "points": 274,
    "commentCount": 217,
    "retryCount": 0,
    "time": 1692642517
  },
  {
    "id": 37208211,
    "title": "The ideal viewport doesn't exist",
    "originLink": "https://viewports.fyi/",
    "originBody": "SKIP TO CONTENT SET STUDIO THE IDEAL VIEWPORT DOESN’T EXIST Before you settle on basing design decisions on a handful of strict breakpoints, make sure you consider the vast fragmentation of screen sizes and browser viewports. Here at Set Studio, we conducted a little casual experiment to answer “how fragmented are viewport sizes?”. We gathered over 120,000 datapoints with over 2,300 unique viewport sizes. The data mainly came from users in the USA and Europe, therefore it is not necessarily representative of a global audience, but still useful for this article. The experiment only ran for 48 hours, but the data we got was pretty interesting. Let’s dive in and take a look. WHAT DOES 120,000 DATAPOINTS REPRESENT? It's important to understand just how many 120,000 is in relative terms. For comparison, let’s presume each datapoint is a person. Wembley stadium has a capacity of 90,000, so our datapoints could fill Wembley once and still fill another third of the available capacity. The population of our home town, Cheltenham, is around 116,000 so our datapoints could almost populate the entire town! WHAT ARE THE TOP VIEWPORT SIZES If we slice out the top 20 unique viewports from the gathered datapoints, it’s primarily small sizes. You could presume these are mobile devices — especially the top 10 — but it’s also worth considering that viewports vary on conditions too. Even on one iOS device, there's a minimum of 3 environments a website could find itself in, based on operating system states. IN SAFARI IN-APP BROWSER 3D TOUCH PREVIEW buildexcellentwebsit.es This isn't a problem if you build in a fluid, flexible manner. That is illustrated in this diagram. If however, you tend to build with very specific breakpoints and hard values for typography, sizing and spacing, you might find that even with the best intentions, you’re not providing the optimal user experience. Let's take an example of a “pixel perfect” UI with a fixed header and/or footer. It might look great when you shrink your development browser down, but how does it look in the conditions outlined earlier? How does it look when you visit from a tiny viewport like a smart watch? How does it look when you visit from a landscape phone? Based on some of the combinations of aspect ratio and dimensions, we're confident those cases were represented in our data. Also, people told us too. Before you commit to fixed headers and/or footers, consider how much content your users will actually be able to see in less than ideal conditions \"MOBILE\" VS \"DESKTOP\" We've only captured width and height for each datapoint for this experiment and these dimensions were gathered using window.innerWidth and window.innerHeight. We decided to treat any width greater than 800px as “desktop”, or as we prefer to call it, a large viewport. MOBILE DESKTOP You might be thinking “800px is way too small for desktop”, which if we were measuring screens, you would be right. We’re measuring viewports here though. A viewport is the window size of the browser, not the screen size. If you’re on a desktop device reading this, how many windows are filling the entire screen? How much screen space does the browser you’re reading from take up? It’s safest to presume that users on desktop or laptop devices are not filling their entire screen with a browser. Heck, even tablet users don't fill the screen with a browser, so make sure you consider that when figuring out your larger viewport designs — especially if you’re concealing content for small screens and showing it for “desktop users”. Ask yourself if needing to hide content for small screens and not larger screens means that content is as necessary as you initially thought. Also consider the sheer fragmentation of “desktop” viewports when working with layouts. Yes, configuring layouts and font sizes using media queries might make sense if you’re using the classic 768px, 1024px and 1280px breakpoints, but what about the vast array of sizes in between? LET’S SEE ALL THE VIEWPORTS Ok, we've passed on our advice but really, you're here to see a visualisation of the viewports aren't you? Your wish is our command. Taking inspiration from the 2015 Open Signal report on Android screen fragmentation, we've laid the top 150 in a masonry layout. You can see all 2,300 too. WIDTH 390px HEIGHT 670px COUNT 8691 WIDTH 393px HEIGHT 666px COUNT 4800 WIDTH 375px HEIGHT 635px COUNT 4618 WIDTH 390px HEIGHT 664px COUNT 2503 WIDTH 393px HEIGHT 660px COUNT 2318 WIDTH 414px HEIGHT 721px COUNT 2130 WIDTH 375px HEIGHT 641px COUNT 1890 WIDTH 428px HEIGHT 752px COUNT 1692 WIDTH 375px HEIGHT 559px COUNT 1643 WIDTH 375px HEIGHT 629px COUNT 1393 WIDTH 430px HEIGHT 746px COUNT 1231 WIDTH 412px HEIGHT 783px COUNT 824 WIDTH 390px HEIGHT 663px COUNT 766 WIDTH 393px HEIGHT 722px COUNT 577 WIDTH 414px HEIGHT 715px COUNT 565 WIDTH 390px HEIGHT 669px COUNT 543 WIDTH 396px HEIGHT 727px COUNT 540 WIDTH 412px HEIGHT 784px COUNT 528 WIDTH 412px HEIGHT 771px COUNT 523 WIDTH 1920px HEIGHT 937px COUNT 512 WIDTH 412px HEIGHT 786px COUNT 504 WIDTH 360px HEIGHT 649px COUNT 469 WIDTH 750px HEIGHT 346px COUNT 456 WIDTH 390px HEIGHT 666px COUNT 443 WIDTH 393px HEIGHT 659px COUNT 441 WIDTH 414px HEIGHT 725px COUNT 441 WIDTH 414px HEIGHT 789px COUNT 429 WIDTH 414px HEIGHT 787px COUNT 428 WIDTH 412px HEIGHT 770px COUNT 422 WIDTH 428px HEIGHT 746px COUNT 417 WIDTH 390px HEIGHT 665px COUNT 404 WIDTH 375px HEIGHT 548px COUNT 374 WIDTH 375px HEIGHT 630px COUNT 367 WIDTH 414px HEIGHT 790px COUNT 360 WIDTH 412px HEIGHT 760px COUNT 354 WIDTH 360px HEIGHT 560px COUNT 354 WIDTH 1194px HEIGHT 766px COUNT 333 WIDTH 430px HEIGHT 740px COUNT 322 WIDTH 390px HEIGHT 661px COUNT 320 WIDTH 375px HEIGHT 634px COUNT 319 WIDTH 1920px HEIGHT 961px COUNT 301 WIDTH 375px HEIGHT 631px COUNT 296 WIDTH 360px HEIGHT 682px COUNT 285 WIDTH 393px HEIGHT 736px COUNT 285 WIDTH 360px HEIGHT 512px COUNT 267 WIDTH 408px HEIGHT 764px COUNT 266 WIDTH 432px HEIGHT 784px COUNT 254 WIDTH 390px HEIGHT 662px COUNT 250 WIDTH 360px HEIGHT 667px COUNT 249 WIDTH 360px HEIGHT 670px COUNT 236 WIDTH 393px HEIGHT 662px COUNT 232 WIDTH 750px HEIGHT 307px COUNT 224 WIDTH 414px HEIGHT 785px COUNT 223 WIDTH 1728px HEIGHT 1000px COUNT 222 WIDTH 2560px HEIGHT 1289px COUNT 222 WIDTH 393px HEIGHT 698px COUNT 221 WIDTH 1920px HEIGHT 929px COUNT 219 WIDTH 1920px HEIGHT 944px COUNT 214 WIDTH 414px HEIGHT 719px COUNT 212 WIDTH 360px HEIGHT 648px COUNT 212 WIDTH 768px HEIGHT 960px COUNT 209 WIDTH 393px HEIGHT 737px COUNT 208 WIDTH 384px HEIGHT 726px COUNT 204 WIDTH 1920px HEIGHT 947px COUNT 203 WIDTH 384px HEIGHT 723px COUNT 201 WIDTH 375px HEIGHT 628px COUNT 199 WIDTH 724px HEIGHT 292px COUNT 195 WIDTH 734px HEIGHT 349px COUNT 193 WIDTH 375px HEIGHT 633px COUNT 192 WIDTH 384px HEIGHT 713px COUNT 190 WIDTH 375px HEIGHT 632px COUNT 190 WIDTH 393px HEIGHT 780px COUNT 186 WIDTH 1920px HEIGHT 919px COUNT 185 WIDTH 378px HEIGHT 711px COUNT 184 WIDTH 375px HEIGHT 626px COUNT 184 WIDTH 1536px HEIGHT 827px COUNT 183 WIDTH 320px HEIGHT 460px COUNT 180 WIDTH 393px HEIGHT 658px COUNT 178 WIDTH 2560px HEIGHT 1287px COUNT 175 WIDTH 1366px HEIGHT 956px COUNT 175 WIDTH 428px HEIGHT 748px COUNT 173 WIDTH 393px HEIGHT 732px COUNT 170 WIDTH 384px HEIGHT 754px COUNT 170 WIDTH 485px HEIGHT 921px COUNT 170 WIDTH 414px HEIGHT 714px COUNT 169 WIDTH 360px HEIGHT 643px COUNT 165 WIDTH 375px HEIGHT 644px COUNT 165 WIDTH 375px HEIGHT 627px COUNT 163 WIDTH 712px HEIGHT 331px COUNT 160 WIDTH 712px HEIGHT 292px COUNT 160 WIDTH 1800px HEIGHT 1006px COUNT 159 WIDTH 360px HEIGHT 617px COUNT 158 WIDTH 1680px HEIGHT 914px COUNT 158 WIDTH 820px HEIGHT 1112px COUNT 158 WIDTH 1412px HEIGHT 767px COUNT 157 WIDTH 414px HEIGHT 699px COUNT 156 WIDTH 1512px HEIGHT 833px COUNT 154 WIDTH 378px HEIGHT 707px COUNT 153 WIDTH 2560px HEIGHT 1297px COUNT 151 WIDTH 1728px HEIGHT 994px COUNT 150 WIDTH 393px HEIGHT 473px COUNT 150 SEE ALL THE VIEWPORTS INTERESTING COMPARISONS Breakpoints from popular frameworks md Commonly 768px width. COUNT: 501 PERCENT: 0.42% lg Commonly 1024px width. COUNT: 367 PERCENT: 0.31% xl Commonly between 1200px and 1300px width. COUNT: 2566 PERCENT: 2.14% Breakpoints from Figma default \"desktop\" 1440px x 1024px COUNT: 20 PERCENT: 0.02% iphone 14 390px x 844px COUNT: 36 PERCENT: 0.03% (THIS IS FULL SCREEN THO) Just on width (390px) it's a much different picture. COUNT: 14742 PERCENT: 12.29% iphone 13 mini 375px x 812px COUNT: 0 PERCENT: 0% Just on width (375px) it's a much different picture. COUNT: 14557 PERCENT: 12.13% DOWNLOAD JSON DATA DOWNLOAD CSV DATA WHAT’S THE POINT OF ALL OF THIS? The main point we’re trying to get across is that you simply do not know how users are going to visit your website or web app. Instead of making design decisions on strict, limited breakpoints, keep in mind the sheer amount of fragmentation there is in viewports. Our recommendation to clients is always to be the browser’s mentor, not its micromanager. Create flexible rules and allow the browser to do what it does best: calculate the best outcome based on the conditions it finds itself in. This goes all the way back to planning your projects too. When mapping out page content, ask yourself how it will be for the weird viewport sizes that don’t fit the typical mould? Always try to simplify and condense content to make it useful for everyone. And finally, always remember that you do not know what conditions your website will be visited on and you have little to no control over that. Accept that lack of control and use the limitations to breed creativity and also, laser focus your UX work. It’s certainly how we do it, here at Set Studio. HEY THERE, WE ARE SET STUDIO. We help organisations set clear goals and deliver them with confidence, by helping them produce stunning websites that work for everyone. We would love to help you produce stunning websites, too, so why not see what we're all about? SEE WHAT WE’RE ALL ABOUT By Leanne Renard, Liridon Hasani and Andy Bell, for Set Studio",
    "commentLink": "https://news.ycombinator.com/item?id=37208211",
    "commentBody": "The ideal viewport doesn&#x27;t existHacker NewspastloginThe ideal viewport doesn&#x27;t exist (viewports.fyi) 264 points by Kye 22 hours ago| hidepastfavorite173 comments mouzogu 20 hours ago> \"The main point we’re trying to get across is that you simply do not know how users are going to visit your website or web app. Instead of making design decisions on strict, limited breakpoints, keep in mind the sheer amount of fragmentation there is in viewports.\"I don&#x27;t know how useful this is.setting breakpoints allows some sanity in the build and testing process otherwise you have an infinite scope for issues which of course would be pretty lucrative for a boutique agency.> \"This goes all the way back to planning your projects too. When mapping out page content, ask yourself how it will be for the weird viewport sizes that don’t fit the typical mould? Always try to simplify and condense content to make it useful for everyone.\"it&#x27;s really not that complicated. keep the amount of crap (stuff that can go wrong) like animations, weird font sizes and font faces, javascript to a minimum. and dont do stuff like hijacking the scroll, zoom or other common behaviours. just dont do anything unless you absolutely have to.but thats why the web is such a hostile platform. difficult to manage high user&#x2F;client expectations with pragmatism. reply Sohcahtoa82 17 hours agoparent> and dont do stuff like hijacking the scroll, zoom or other common behaviours.The hijacking of scrolling pisses me off so god damn much that I&#x27;ve considered building Firefox from source and modifying the code to completely eliminate a website&#x27;s ability to set the scroll position.[0]Front end devs, I implore you. Stop acting like you think you know what the user wants in regards to scrolling behavior. Smooth scrolling already exists natively in every browser. There&#x27;s no need to try to re-implement it in JavaScript. Your implementation will not work in every browser, and will only cause strange stuttering, bouncing, or even end up somehow completely disabling scrolling altogether. Do not try to get fancy and implement \"momentum\" into scrolling. You&#x27;re changing a well-understood behavior into something that is unexpected and jarring, and likely it won&#x27;t work anyways.Do not change the scrolling amount. My wheel sensitivity and browser setting are configured so that 1-click ~= 2.5 lines of scrolling. Do not impose your preference of 1 click ~= 1 line on me. You do not know better than me.And disabling zooming? WHO EVEN DECIDED THAT WEBSITES SHOULD BE ALLOWED TO DO THIS?! It destroys accessibility! Sometimes there&#x27;s text or an image that&#x27;s just a little too small to recognize. I&#x27;d pinch to zoom in...but some moron front-end dev has adopted the beyond-bone-headed mentality of \"removing features is a feature!\" and makes their site tell the browser to not allow zooming because...why? Someone please, tell me why.[0] I&#x27;m sure it&#x27;s possible to write an extension that could do this, but any time you&#x27;re manually setting the scroll position in code, you&#x27;re bound to fuck it up. Rather just completely eliminate the ability to set the scroll position entirely. reply paulddraper 15 hours agorootparent> And disabling zooming? WHO EVEN DECIDED THAT WEBSITES SHOULD BE ALLOWED TO DO THIS?!Applications such as maps, image editors, presentations, and flowcharts benefit from having control over zoom. (And you&#x27;ll notice that almost every one of them does.)This of course is only one difference of many between \"documents\" and \"applications\" and the web is being used for both. reply debugnik 14 hours agorootparentNo, they benefit from overriding the pinch gesture or the scroll wheel. Browser zoom shouldn&#x27;t need to be a casualty for those features to work, it ought to stay available in some form. reply paulddraper 10 hours agorootparentYes, that&#x27;s what people mean.Browser zoom cannot be overridden. reply gnarbarian 13 hours agorootparentprevyes you&#x27;re right. we have a map with a toolbar at the top of the screen. when zoom was enabled people were constantly zooming in preventing them from being able to see some or all of the buttons hindering their ability to use the app. and causing confusion because they didn&#x27;t realize they were zoomed in. as soon as we disabled zoom the complaints and questions stopped. reply throwaway14356 12 hours agorootparentsetting just the toolbar to not zoomable should have been the obvious solution.(In stead I have to disable zooming in on images to preserve the next&#x2F;previous&#x2F;close buttons and implement zoom from scratch. wtf?) reply _jal 15 hours agorootparentprevMaps drive me fucking nuts, taking over zoom. As do nearly every other app that does it. They think they know what&#x27;s good for me, and they don&#x27;t, and it leads to me using them less. Yes, paper maps just work better for me a lot of the time.The only thing maps.app or Google maps are good for are finding places to spend money or driving to places to spend money. If you have any other spatial interests, they&#x27;re almost pointless.I would happily run a build that removes this capability. reply paulddraper 15 hours agorootparentI would love to see your maps app that doesn&#x27;t touch zoom. reply throwaway14356 12 hours agorootparentNo offense, I had to try that..https:&#x2F;&#x2F;upload.wikimedia.org&#x2F;wikipedia&#x2F;commons&#x2F;3&#x2F;3d&#x2F;LARGE_el...Look how fast and responsive it is! reply mkl 4 hours agorootparentThat supports touch zoom just fine, it just has a low limit to how far in it goes. reply aendruk 11 hours agorootparentprevWith progressive decoding and spatial range requests this could actually be a reasonable replacement for a slippy map. reply paulddraper 10 hours agorootparentprevI&#x27;ll use it on my drive home. replykyleyeats 13 hours agorootparentprevYou&#x27;re mad at the wrong people here. I&#x27;ve never met a frontend dev who doesn&#x27;t hate moving stuff vertically or scrolljacking. It&#x27;s the marketing people that want it. reply ncallaway 11 hours agorootparentThe one that hurt me the most was making a page that started at the bottom and scrolled up. I left a comment somewhere in the JavaScript apologizing for it. reply andrei_says_ 3 hours agorootparentprevIn pay experience, scrolljacking happens after front end devs are told to implement it by the designers and the designers are told they must do it by the marketing people who in turn have been begged by both the front end devs and the UX team and. The design team to please record wider this insane decision - but insisted on it anyway.I have a collection of articles and studies and discussions on scrolljacking that I show to the business stakeholders when it comes up.I make sure thy hear from multiple parties independently when such an idea starts making its way toward implementation.But it is exhausting because trendy flashy websites still use the technique and do catch the eye and I have to explain why we can’t and should not have the shiny toy - despite big brand x y or x having it.In which process I am saying no to the very people deciding on my bonus.Just a bit of perspective. reply zimpenfish 15 hours agorootparentprev> And disabling zooming?I had a weird experience with Google Groups recently - I zoomed in because the text was too small and ... the page resized the viewport to its original pixel size even though the font was scaling. Ended up with about 10 characters in an unscrollable viewport. HILARIOUS. reply aidenn0 15 hours agorootparentI had the opposite problem with a web page the other day. When I zoomed in, the text resized itself to still fit the same amount of text in the viewport, but since other elements would zoom correctly, the more I zoomed in, the smaller the text got. reply drewrbaker 11 hours agorootparentprevAs the poor sap who’s had to build lots of these types of sites, I’ll tell you that it’s not the devs that want this. It’s the damn client that keeps complaining about the “gravity” or the “momentum” or the “scroll speed” of the site. Locomotive.JS being the main thing clients want us to use… no amount of explanation of all your valid points will persuade them if “this one cool site we like had it”.I will say this… the scroll speed being different between Chrome, Safari and Firefox doesn’t help our cause… wish these were normalized at the very least so we can avoid “it feels better in X, but we use Y browser” notes.Allowing an easing function API for scroll would be a middle ground I could live with. It’s better than what we have now (a bunch of award winning sites emulating scroll with translates). reply hnick 7 hours agorootparentprevDo you feel the same way about scroll-enabled menus? I had to implement a menu recently (similar to the one on https:&#x2F;&#x2F;getbootstrap.com&#x2F;docs&#x2F;5.3&#x2F;getting-started&#x2F;introducti...) and my first draft used traditional anchor-to-id links.It was very jarring and also felt weird being flush with the top - we added a jQuery smooth scroll to go to just a little above the item in a few hundred ms and it felt much better. I say this as someone who generally hates scrolljacking and custom scrollbar rendering on sites. reply depressedpanda 11 hours agorootparentprev> I&#x27;d pinch to zoom in...but some moron front-end dev has adopted the beyond-bone-headed mentality of \"removing features is a feature!\" and makes their site tell the browser to not allow zooming because...why? Someone please, tell me why.For sites that have no business disabling zoom: no idea, and it annoys me too. I just chalk it up to dev, designer and&#x2F;or managment incompetence, close the tab and never return again.&#x2F;Moron frontend dev reply shadowgovt 15 hours agorootparentprev> The hijacking of scrolling pisses me off so god damn much that I&#x27;ve considered building Firefox from source and modifying the code to completely eliminate a website&#x27;s ability to set the scroll position.[0]That&#x27;ll break just about every cloud logging UI I&#x27;m aware of, but what you do to your own UA is your business. reply smarkov 20 hours agoparentprev> I don&#x27;t know how useful this is.I think very. I&#x27;ve never designed anything based on arbitrary breakpoints (md, lg, xl and whatever numbers they might translate to) because that forces me to make design choices around those constraints.The only way to sanely add breakpoints in my opinion is to gradually reduce the width of your page and search for things that start looking off. Are your paragraphs starting to look a little cramped? Add a breakpoint at that width, maybe remove the sidebar, maybe lower paddings and margins to allow it to stay a little longer, maybe manually reduce font size or switch your column layout to rows if applicable. Keep doing that until you get to ~350px width and everything looks fine. You decide what needs to change when it makes sense rather than being told that something needs to change at some specific breakpoint. reply crazygringo 10 hours agorootparentThat&#x27;s a fine (and arguably superior) philosophy for a simple personal site.But it just isn&#x27;t practical for a commercial product that needs manual testing. If you have a single breakpoint, you only have 2 versions to test. If you ultimately have 30 different breakpoint values, that&#x27;s 31 versions to test. And that&#x27;s a lot of opportunity for CSS rules designed in isolation to wind up colliding with undesired results.Not to mention a nightmare for any designer to attempt to document expected behavior. reply bryanrasmussen 17 hours agoparentprev>I don&#x27;t know how useful this is.>setting breakpoints allows some sanity in the build and testing processclamp allows for sane responsivenesshttps:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;CSS&#x2F;clampand if needed you can add some minimal breakpoints.The fact is that most front end devs don&#x27;t know about it, and there is no framework that I know of which is based around its use (and everything nowadays seems to be about the frameworks), thus you end up with inefficient multiple breakpoints which may seem sane until you get too close to one of the breakpoints and your design looks like crap until that point is hit and you can switch to the new values set for the breakpoint.breakpoints are a great solution if you happen to be living and working in the web of 3 years ago. But the clamp, min, and max functions have been available in every major browser since 2020 - even Opera.People keep telling me 3 years is an eternity in internet time, so why are there still all these damn breakpoints? reply mouzogu 2 hours agorootparent> why are there still all these damn breakpointsthey&#x27;re simple, easy to understand and well tested.looking at clamp it smells of over engineered complexity. 3 arguments. difficult to visualise.margin, padding, box sizing, line-breaks, non-breaking spaces that can all impact how text flows and wraps - clamp is stacking on top.most css is written, forgotten and ime never refactored. so what&#x27;s fast and dirty often is good enough, within reason. reply bryanrasmussen 1 hour agorootparent>looking at clamp it smells of over engineered complexity. 3 arguments. difficult to visualise.although you can use clamp to do complicated things that are difficult to understand, as is the case with all technology the simplest way to use it is quite clear.(static minimum size you want for something, percentage of something, static maximum size you want for something)thus width: clamp(100px, 80vw, 750px)and now someone says well why wouldn&#x27;t you just do min width max width etc. with that - no reason it&#x27;s just nicer to have it one line to see IMO. But of course you can put clamp anywhere you can calculate anything so.margin-left: clamp(20px, 5vw, 50px)ORmargin-right: clamp(var(--minmarg),var(--margpercent), var(--maxmarg))then you change your minmarg, margpercent, maxmarg as needed by normal css variable rules.This should be an easy to understand system for a developer. It is easy enough for me to understand and visualize, and I suck at visualization.This allows one to finally get rid of most breakpoints and say not just what the min and max heights and widths are of things but what the min and max and preferred margins, paddings, font-sizes etc. are of things - which without that IMO the max height and max width is just sort of stupid. But with that you can make designs flow and look good in a manner that can actually be described as &#x27;responsive&#x27;Now as I said - one can do all sorts of complicated and clever stuff on top of this sure. And if people do that they might end up with something that is difficult to reason about.But if people can&#x27;t reason about the margin example above then I think they should probably switch from Frontend development.on edit: grammar change. reply throwaway14356 12 hours agorootparentprevit seems much cleaner to give each size its own css? reply traceroute66 18 hours agoparentprev> I don&#x27;t know how useful this is.The website that is the subject of this discussion comes from Andy Bell.I would encourage you to see his other work, e.g. Every Layout[1], and the website[2] linked to a recent talk he gave[3].In particular the answer to your question can be found at 14:38[4] in the talk, perhaps more precisely the slide he shows at 14:59.[1]https:&#x2F;&#x2F;every-layout.dev [2]https:&#x2F;&#x2F;buildexcellentwebsit.es [3]https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=5uhIiI9Ld5M [4]https:&#x2F;&#x2F;youtu.be&#x2F;5uhIiI9Ld5M?t=878 reply hammock 13 hours agoparentprevUsing the data provided, I calculated the minimum window size in order to have full viewability among a percentile subset of the audience. Here are the answers: Mobile: 50% of viewports: Width 375, Height 635 80%: Width 375, Height 635 90%: Width 360, Height 560 95%: Width 360, Height 550 99%: Width 320, Height 500 Desktop: 50% of viewports: Width 1440, Height 900 80%: Width 1024, Height 600 90%: Width 1024, Height 600 95%: Width 1024, Height 600 99%: Width 800, Height 300 reply qingcharles 9 hours agorootparentI&#x27;ve been using a min width of 320px lately due to some unusual displays, mainly on vertical flip phones with super-tall screens. That justifies my insanity. Thank you. reply johnnyworker 20 hours agoparentprevFor me it really just involves squishing the browser window in all sorts of ways and see what happens, and if I get any ideas on what I would want to change. I don&#x27;t see what the big deal is.> crap (stuff that can go wrong) like animations, weird font sizes and font faces, javascriptNone of these per se are the issue, and you can still have the issue with zero animations, fonts or JavaScript. reply depressedpanda 18 hours agoparentprev> setting breakpoints allows some sanity in the build and testing process otherwise you have an infinite scope for issues which of course would be pretty lucrative for a boutique agency.Honestly, now that container queries are available, I see very little use for breakpoints. Container queries allow for easy reuse of components, and a truly fluid and responsive design. reply tshaddox 17 hours agorootparentIMO the promise of container queries isn&#x27;t that breakpoints will go away, but rather that breakpoints can emerge from the bottom-up composition of components rather than top-down from your CSS framework or Tailwind config or whatever.You&#x27;re still probably going to want the left sidebar of your multi-column layout to collapse behind a menu on narrow viewports and abruptly appear when the viewport width gets to >= n [0]. But it&#x27;s conceivable that the value of n emerges from some constraints you specify (such as the minimum desired width of the sidebar and the main content column), instead of being chosen upfront from a small list of predertmined breakpoints.[0] E.g. https:&#x2F;&#x2F;tailwindui.com&#x2F;components&#x2F;application-ui&#x2F;application... reply depressedpanda 17 hours agorootparentWhile I suppose breakpoints might still have some use cases, I believe an overwhelming amount of the stuff we currently use them for can be better and more fluidly accomplished with container queries.The example you give of a sidebar collapse with a menu button replacement can easily be accomplished with a container query on the wrapping box, no?I&#x27;m honestly curious about a use case that a media query breakpoint can handle, but which a container query can&#x27;t. reply danielvaughn 18 hours agoparentprevThe problem is that at a real company, this is very difficult because you have to argue with your product&#x2F;design team and explain why you don&#x27;t want to implement [x] feature because it&#x27;s \"too complex\". Inevitably they&#x27;ll point to some competitor website that has the same feature, which makes you look like you&#x27;re just lazy. It&#x27;s possible to succeed in those arguments, but it&#x27;s hard.It&#x27;s one thing to convince them not to do some weird scrolljacking, but even something like a dropdown or a popover adds a ton of complexity for responsive sites. reply withinboredom 18 hours agorootparentIf your company cares one iota about accessibility, you can usually get those features off the table instantly just by arguing that point. \"Yeah, well it&#x27;s pretty clear that competitor doesn&#x27;t care about blind people\" is a pretty easy argument.I invite you to learn to use a screen reader and try using your app (whatever it happens to be). It&#x27;s seriously pretty terrible for some of these websites with all the fancy crap. reply hagg3n 17 hours agorootparentI envy you cause the teams and companies that I&#x27;ve worked with basically say fuck em to that argument.I have lost jobs and annoyed people trying to argue for greater care with accessibility. It&#x27;s somewhat depressing. reply esdott 17 hours agorootparentThis is exactly what happens. Especially if your voice is in the minority Or you’re the only one. You’re seen as a trouble maker. We all know what happens to trouble makers when margins are on the line. reply withinboredom 13 hours agorootparentI just stab them in the eyes and then say \"now you&#x27;ll care about blind people,\" then calmly walk away. Works every time. &#x2F;sIn all seriousness, I ask something similar to the above: \"Would you feel the same way if you got in an accident that left you blind for the rest of your life?\" Sometimes this moves a few people to your side. I don&#x27;t want to be the only one, but I do care about accessibility. I care a lot, for personal reasons. But like you said, you don&#x27;t want to be the minority trying to support minorities. reply depressedpanda 11 hours agorootparentprevAnother comment suggested you try to appeal to empathy. It might work, it might not.If it doesn&#x27;t you can instead appeal to the bottom line. Tell &#x27;em that they might get targeted by an accessibility troll: https:&#x2F;&#x2F;www.wrightlawgroup.com&#x2F;blog&#x2F;attack-of-the-ada-trollsAs far as trolls go, I consider these fairly benign, as their behavior actually promotes something good; we got hit with such a lawsuit, and all of a sudden a11y became a big thing as our CEO didn&#x27;t want to give a dime to those &#x27;greedy assholes&#x27;. reply avmich 16 hours agorootparentprevI think there are some laws which require a certain degree of accessibility for at least government websites. And if your big company deals with government, they&#x27;d better think about that. reply eternityforest 16 hours agoparentprevThere&#x27;s one thing I absolutely will disable and that is the MFing pull to refresh. I hate that feature more than pretty much anything else about the Internet.Users can&#x27;t, for some hideous reasons that should be illegal due to accessibility concerns, disable it, and it is an excellent way to accidentally throw away your work several times a month if you don&#x27;t have very steady hands, depending on if the back button decides to not restore. reply depressedpanda 10 hours agorootparentIf a website allows you to throw away a lot of work by accidentally refreshing the page, it&#x27;s poorly written. There are multiple ways to deal with this: a warning, a confirmation prompt, storing data in sessionStorage, etc. reply eternityforest 3 hours agorootparentNone of which HN appears to do... ideally you&#x27;re right, it shouldn&#x27;t be possible, but disabling pull to refresh helps a lot on a crappy legacy site.Plus it&#x27;s also just plain annoying. If a site has no well defined concept of pages, you&#x27;ll probably be in a whole different dynamically generated page. Not that I think sites like that are all that great to begin with... reply brailsafe 14 hours agoparentprevI wouldn&#x27;t say it&#x27;s a hostile platform, I&#x27;d say that it&#x27;s a volatile design medium. Regarding breakpoints, I&#x27;ve always agreed with Jeremy Keith&#x27;s opinion on the subject, which in some sense is to design for the content and fix it when it breaks, but to assume that it will and test it as best as you can. Breakpoints are just a way of demarcating broken&#x2F;not-broken layout. I also use them for generalizing design if I&#x27;m doing it before writing the layout in code, which you can do if you start with no assumptions as to how it will be viewed, and just choose some arbitrary constraints to move things around within, then refine when you actually have something in the browser. reply dsr_ 18 hours agoparentprevFundamental dilemma:* The web is a great way to publish information and access a few services.* The web is the cross-platform operating system for applications.Everything stems from the difference in those two approaches. Both are correct. reply mmcnl 11 hours agoparentprevI agreed with you until you went overboard with your last sentence. A \"hostile\" platform? What in the world? You literally just mentioned how easy it can be to avoid all this crap. There is nothing \"hostile\" about the web. I really don&#x27;t like these overly strong statements. reply ArtWomb 17 hours agoparentprevIf you need to get weird now, webgpu has arrived, what we really require is a dedicated demoscene on the live web ;)Your first WebGPU app (Conway&#x27;s Game of Life)https:&#x2F;&#x2F;codelabs.developers.google.com&#x2F;your-first-webgpu-app reply PMunch 21 hours agoprevI wish more pages took the pointer and resolution media queries into account. So many times I open windows side-by-side on my desktop and end up with the menu collapsing into a hamburger menu which fills the entire screen when clicked. Many phones today have very high DPI and very low precision in touch, so it makes sense to have massive visual elements and buttons, but my desktop with the same size in pixels as my phone but a much lower DPI and much higher input precision doesn&#x27;t really need them. reply tacker2000 19 hours agoparentThis. Unfortunately today, the desktop is an afterthought and everything is designed for “mobile first”. I even see this in SaaS apps that should actually never be used on a phone. Or desktop OSes like windows and mac are increasingly using mobile paradigms on the desktop.There is so much whitespace, huge fonts, huge buttons, hamburger popup menu everywhere (aaaargh!!)Basically it boils down to the “information density” which is dumbed down to the lowest possible denominator.There should be a measure for this and websites&#x2F;apps should be rates based on that. reply mmis1000 16 hours agorootparentIt sounds like you are talking about gmail web at md2 era. The giant sidebar item that can only literally show 6 item on a 1080 screen. The giant mail row that display only 10 or maybe 15 mail on whole screen. And new mail button on right bottom that requires you move your mouse across the whole screen to write a new mail.It really makes me wonder &#x27;wtf? Who designed that, did he actually try to use it by himself?&#x27;.At least it seems they steps back a bit now. Also they moved the new mail button back to top left. reply crazygringo 19 hours agoparentprevI&#x27;m actually not entirely sure what you&#x27;re talking about.I&#x27;m very familiar with websites switching to mobile layout with hamburger menu instead of a navigation bar, when you reduce the width of the browser window on your desktop. BUT I don&#x27;t think I&#x27;ve ever come across a resolution change where the website elements (either hamburger menu or content) get massively larger.Can you provide an example of one or two mainstream sites that change resolution like that?The reason I&#x27;m confused is because in CSS everything is based on logical pixels, not actual device pixels. E.g. a \"1 px\" width will be 2 or 3 hardware pixels wide on an iPhone. Similarly \"1 em\" will be 16 hardware pixels on a standard resolution display, but 32 or 48 hardware pixels on a 2x or 3x display. So web designers don&#x27;t have to do anything special to accomodate hi-DPI screens in the first place. reply withinboredom 18 hours agorootparentThe resolution doesn&#x27;t change, but a 64x64px icon button doesn&#x27;t need to be that big on a desktop. reply crazygringo 18 hours agorootparentBut this is my point, I haven&#x27;t been seeing any 64x64 px (logical pixels) hamburger menu icons out in the wild. That would be too big for mobile as well.The hamburger menu buttons all seem normally sized to me when I make a browser window narrow on my desktop. reply withinboredom 18 hours agorootparentYes, if you are on a touch screen, you need &#x27;normally sized&#x27; hamburger menus. When using a mouse, a 16x16px hamburger is plenty big enough. reply crazygringo 18 hours agorootparentI think you may have misunderstood my comment. I&#x27;m saying I am seeing normally sized hamburger menus on desktop. You&#x27;re using an example of 64x64, I&#x27;m saying I don&#x27;t see that.Although 16x16 is a little on the small side even for desktop. The point isn&#x27;t just to click it, but to have it be prominent enough to see and notice as a primary action. It&#x27;s about visibility, not touch area.For example, on the SquareSpace homepage (an example someone else brought up) it&#x27;s 30x18. That seems like a nice size to me. It&#x27;s two pixels taller than the 16 you suggest, but the extra width helps make it a little more prominent. Especially since the width isn&#x27;t taking away from anything else. reply withinboredom 18 hours agorootparentIt is 44x44 on my computer. Absolutely massive.https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;dL1Se2v reply crazygringo 17 hours agorootparentIt can&#x27;t be 44x44, it&#x27;s not even square.I can&#x27;t tell what units you&#x27;re measuring in, are you measuring hardware pixels on a hi-DPI screen? We&#x27;re talking about logical pixels which is the only thing that makes sense to measure in and compare.But comparing it with e.g. the refresh button on your browser, it&#x27;s merely 17% taller than that, in your screenshot. So I literally don&#x27;t know what \"absolutely massive\" you&#x27;re talking about.It seems perfectly fine to me. Maybe I&#x27;d make it a little narrower, but they probably wanted to balance the logo in the top left corner in terms of visual weight, so it makes sense. reply omegabravo 17 hours agorootparentThe button is 44x44. The SVG is 30x18 reply crazygringo 17 hours agorootparentIf that&#x27;s the clickable area, is someone complaining that&#x27;s too large?You&#x27;ve been able to select radio buttons by clicking on their text for decades now. On desktop. Often literally hundred of pixels wide.Generous margins for clickable elements seems like a feature, not a problem. As long as they don&#x27;t interfere with anything else (which they don&#x27;t, here). reply omegabravo 5 hours agorootparentI happen to agree, just trying to point out the miscommunication reply withinboredom 12 hours agorootparentprev> Often literally hundred of pixels wide.And 16 pixels tall... reply rendaw 17 hours agorootparentprevThe folder on your desktop is roughly the same size. replyaimor 19 hours agoparentprevI know the official line is that \"media types have proven insufficient as a way of discriminating between devices with different styling needs.\" But as far as I can tell most people really do just want to know if the visitor is viewing on a monitor or a phone. Trying to back that out through media features like viewport resolution and pointer types has been a big mess. I don&#x27;t think the plan to replace media types with media features is wrong, but so far we haven&#x27;t been given the right features to do what we want. reply TT-392 17 hours agoparentprevWhen I visit my bank, with my browser taking up half my 1080p desktop display, it tells me to use the app instead. reply Technotroll 20 hours agoparentprevThis is an interesting observation. Do you have some examples of this? reply bandergirl 20 hours agorootparent> Do you have some examples of this?Sorry if it sounds rude, but have you used the web on the desktop in the past 10 years? Anything under 1000px-wide windows and sometimes 1200px-wide ones gets you the “mobile menu” on most websites. It’s a consequence of Bootstrap and other fixed-breakpoint frameworks. Overflow menus like what you see on GitHub repositories are the minority. reply aendruk 18 hours agorootparentprevScreenshots: https:&#x2F;&#x2F;cloudflare-ipfs.com&#x2F;ipfs&#x2F;QmUL69cYwCs2sWV7eHqtt14BEcn... reply qingcharles 9 hours agorootparentThose are great examples, thank you for spending the time to make them. reply throwaway290 20 hours agorootparentprevreact.devAnd for an example that does it better, legacy.reactjs.org :)When I saw it I wondered how they managed to drop the ball so hard and how much they paid for it. All they needed is to expand and slightly restructure the docs, but apparently someone sold them a complete redesign that just made the whole docs way less usable (a11y aside, can&#x27;t speak for that). reply carlosjobim 20 hours agoparentprevIt doesn&#x27;t work like that. Web browsers do not count actual pixels when rendering. They adjust high density pixels to the equivalent real life size of low density pixels. reply afavour 20 hours agorootparentThe OP is talking about things like media queries that only look at screen width and imply other things like input method from that.IIRC SquareSpace does exactly what they&#x27;re saying: on desktop sites will have a horizontal list of links but on mobile it&#x27;s a hamburger icon that takes over the entire viewport when clicked. Functionality exists to target pointer type and things like that in media queries, it&#x27;s just very rarely used. reply johnnyworker 20 hours agorootparentOhhh, I had no idea! Thank you.https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;CSS&#x2F;@media&#x2F;poin... coarse - The primary input mechanism includes a pointing device of limited accuracy such as a finger on a touchscreen. fine - The primary input mechanism includes an accurate pointing device, such as a mouse.This solves everything! What I really want is having more padding on mobile, that&#x27;s it. I prefer things to be compact otherwise, and it seemed hard to square that circle... with the pointer media query it&#x27;s so trivial.Gotta love how every time you don&#x27;t watch CSS like a hawk, it spawns 500 new features. Thanks again. reply withinboredom 18 hours agorootparentI&#x27;m actually surprised this isn&#x27;t a selector in tailwind css. reply pests 17 hours agorootparentThat is one of the issues trying to re-implement every css property:value pair with a class name inherent in Tailwind. reply crazygringo 19 hours agorootparentprevBut that&#x27;s what you want, no? I just visited SquareSpace and their homepage seems to work as I&#x27;d want. When my viewport is narrow enough that there isn&#x27;t space for the horizontal navbar at the top, it switches to hamburger menu.Surely this is better than cutting off the navbar and half the page content in the middle vertically, and requiring the user to scroll horizontally? reply afavour 18 hours agorootparentThe hamburger menu could just be a vertical drop down stack rather than something that takes over the entire screen.The point is that the viewport-covering menu is huge because it’s designed to accommodate touch (i.e. imprecise) interaction. It could be much smaller when the user is using a mouse. But you rarely see such accommodation being made. The assumption is low width = mobile = touch. reply crazygringo 18 hours agorootparentI still don&#x27;t get it.The actual menu items you select are 20 px font size in the hamburger menu, and 18 px font size from the navbar menu. That&#x27;s just 11% larger. Nothing about that is \"huge\".The hamburger menu taking over the whole (narrow) screen doesn&#x27;t bother me at all. If I&#x27;m using the menu, it&#x27;s not like I need to be looking at the rest of the screen.And it just feels silly to create a third design. We already have navbar for widescreen and hamburger menu for narrow screen. Now you want a third version -- a hamburger menu that is pop-up rather than fullscreen -- for narrow desktop usage? I sure wouldn&#x27;t want to do all that extra work. replynickdothutton 17 hours agoprevI&#x27;m not, and never was, a \"front-end guy\". I&#x27;ve noticed more and more quirks (bugs?) in UIs where something is off screen, or unclickable, or suffers extremely unhelpful placement that prevents or significantly impedes my use of a web site&#x2F;app. Rotate phone&#x2F;tablet, change font size, hide URL bar, mental note to try it on a desktop or laptop \"later\". This feels like the front-end version of \"it works in dev\". reply bityard 16 hours agoparentI have this shaky hypothesis where the majority of UX devs today (not just web devs) grew up consuming content on smart phones, ipads, and small laptop screens. Their default way of interacting with technology is one app or web page at a time, all in maximized windows taking up the whole screen, and switching between between full-screen apps to multi-task. Perhaps this pattern carries over into their adult work, where they design sites and apps that look great when taking up the whole screen of a typical PC or tablet, but outright fail when exposed to a typical \"windowed\" browser width.My normal browser width is about 1300 pixels and I see so many web sites and apps these days that can&#x27;t tolerate what I consider a very reasonable browser width that this is the only explanation I can come up with. reply zlg_codes 13 hours agorootparentGood points! The unfortunate side effect of having 4k and 8k screens is that 1080p is still common. I could understand why they&#x27;d assume nobody leaves a browser window covering only a quarter of their screen, but at the same time, both major browsers have a Responsive Mode where you can test screen resolutions, orientations, and tap events.There are devices with resolutions under 800x600 still accessing the Web. Every developer has to place where their own minimum threshold of consideration is. I personally aim for those old-school resolutions as the minimum, and may move upwards to something like 720p if it&#x27;s a bit more complex. My screen itself is 1080p, so if it looks good at full screen, it will probably look decent on 4k as well as long as I&#x27;m using scaling units. reply carlosjobim 11 hours agorootparentprevYour visitors and paying customers access your site on mobile. Mobile is the majority. If you don&#x27;t design for mobile you lose business. It has nothing to do with the UX devs. If I notice 80% of my customers come from Japan, you better believe I will make everything available in Japanese. The same with mobile. Most people do not use computers. Most people who use computers only use them for work in specific applications. Most people who have a computer at home haven&#x27;t been able to use it in years, because every time they try to boot it, Windows Update takes up all the system resources and bandwidth. reply bobthepanda 17 hours agoparentprevIf you’ve worked in front-end dev for a while you can be aware of all the issues.The problem, generally, is that frontend dev is taught poorly if it’s taught at all, and the barrier to entry is so low that there’s an Eternal September problem. reply calderwoodra 16 hours agorootparentI&#x27;d think that eventually we&#x27;d have a large enough cohort of folks that know web dev well since the barrier is so low, but I think most people try to get out of frontend development asap. reply zlg_codes 13 hours agorootparentI&#x27;m trying to learn some JS in my spare time, if only to show in interviews that I can do it, but I understand why one wouldn&#x27;t want to. I&#x27;m just doing bare JS for now, but even with a framework, there is so much to manage in the DOM to make consistent interfaces, and CSS3 is so much more complex (but also convenient, in some ways!), putting together good interfaces is legitimately hard. Responsiveness is the big hurdle in creating a well-behaving app, from my perspective.I think I could&#x27;ve learned a GUI toolkit and had something working in the time it&#x27;s taken me to pick up building two JS apps. The barrier to entry may be low, but that&#x27;s only because the feedback loop for webdev is super tight. Great for prototyping, experimentation, and getting more newbies in the field. But it&#x27;s crazy that anything productive gets built on JS and DOM.The backend, with databases and storage and caches, service management and sysadmin, that&#x27;s where the fun is! reply thrashh 10 hours agorootparentWeb dev frontend has a thousand quirks but has a billion more hours of human hours development time spent on it. CSS is annoying but way better than trying to jiggle random attributes and subclassing stock components to do something basic like right align a label.You&#x27;ll notice that a lot of GUI toolkits have already adopted something like CSS (Qt Style Sheets, JavaFX CSS, etc.). Yet they will never become as good, just because there are much fewer people working on it. reply bobthepanda 11 hours agorootparentprevJS and DOM are popular mostly because they exist everywhere, and there&#x27;s a large ecosystem of support. In practice, given (until recently) difficulty in hiring, places found it easier to have one JS team than to have one native team for every possible platform (Windows, Mac, iOS, Android, maybe even Linux). And it also does not help that each native platform can have wildly divergent frameworks and practices. reply jonny_eh 11 hours agorootparentprevI dunno, I love it, quirks and all. The ability to make and deploy new UIs quickly is unmatched elsewhere. I also may be weird in that JS&#x2F;TS is my favourite programming language. reply calderwoodra 10 hours agorootparentI&#x27;m in the same camp, I&#x27;m always 10x more proud of my frontend work if for no other reason than it&#x27;s closer to the users. Once you learn a framework and build up a few tools, you can move insanely quickly. replylucideer 19 hours agoprevThere&#x27;s an elephant in the room here, and it&#x27;s Adobe.The author&#x27;s put forward a lot of valid criticisms of the breakpoint-based approach & also provided a really good guide to doing proper web design, but what&#x27;s skipped over is that their guide only works for developer-designers, & won&#x27;t fit the workflow of a large proportion (majority?) of people doing visual design for web. Those designers are have backgrounds in multimedia or have academic backgrounds in design-forward colleges with extremely poor html-css modules. And their tool is Creative Suite - largely because Adobe has a stranglehold on design education.The likes of Sketch have shaken things up somewhat so we at least have XD, and slightly fewer people are designing entire websites in Photoshop & Illustrator alone, but even so - the ability of tools like XD&#x2F;Figma to allow the designer to model their work fluidly in a way that matches web viewports is not mature, not least because Adobe have been historically hostile to the idea (XD being a very reluctant response to new competitors after already killing off Fireworks). reply madeofpalk 16 hours agoparent> and slightly fewer people are designing entire websites in Photoshop & Illustrator aloneAcross the wide industry, sure, but I haven&#x27;t worked with a designer who uses Photoshop in 10 years...Granted, I&#x27;m in my own bubble, but over a varity of gigs it&#x27;s been Sketch, and now universally figma. reply lucideer 14 hours agorootparentAll designers I currently work with are 100% figma, but I work for bigcorp where things are a bit more formalised & designer friends working in the startup world are much more on the multitool&#x2F;Adobe wagon.Even so though, Figma, while an improvement, is still not where tools should be in this regard. It&#x27;s a compromise between what&#x27;s expected by Adobe-educated users and how things actually work on the web. reply agos 15 hours agoparentprevI&#x27;ve started doing frontend work back in 2008, and I immediately noticed the friction with designers coming from Adobe tools and their equivalents. It&#x27;s disheartening to see that 15 years later the problem is still there reply avmich 17 hours agoprevThe visualization, I think, could be improved. Make a graph of points, where X and Y coordinates correspond to width and height of the viewport, this is like imagining all those viewports having the same left top corner and recording bottom right corner. Those points on the graph which correspond to more frequently occurring viewports should be brighter (if not bigger circles). Many points for large number of small viewports will be clustered in left top corner, to make points more evenly spaced, the coordinates could be logarithmic. +--------------------+. .| *. .*.| .**. ..| ..*.| . .| .+--------------------+ reply karmakaze 20 hours agoprevLandscape on mobile is the least well supported configuration I know. Static headers&#x2F;footers commonly take 50% or more vertical space with no way to dismiss or shrink them. reply coldpie 15 hours agoparentI&#x27;m going to change your life: https:&#x2F;&#x2F;github.com&#x2F;t-mart&#x2F;kill-stickyIt&#x27;s a bookmarklet that gets rid of sticky elements. It nukes those stupid fucking headers and footers and kills most cookie banners and \"give me your email\" type pop-ups. Works in every browser (I use it in Safari on iOS and Firefox on desktop). I almost reflexively go hit this bookmarklet on every website now. It makes the web so much better. reply tantalor 19 hours agoparentprev> no way to dismiss or shrink themYou can shrink them by rotating your phone back to the correct, portrait orientation. reply layer8 19 hours agorootparentIt also happens on tablets and notebooks, in particular with larger font sizes or zoom factor configured. reply karmakaze 19 hours agorootparentprevOr shrink everything to unreadable sizes with \"Desktop site\". reply _gabe_ 19 hours agoprevThis article was great! I think the overall premise is probably more useful for graphic designers than programmers, because I believe many programmers kind of implicitly know this advice. However I’ve worked with graphic designers in the past that were obsessed with having their Figma design translate to the web page in a pixel perfect manner, and they would always be confused when things didn’t align correctly. This inevitably leads to breakpoint Hell, where they design pixel perfect designs for tons of breakpoints and ask you to implement them.While this is laudable, I think it’s important to remember that a design should focus more on how the layout flows when resizing the viewport and focusing on a couple target breakpoints. I think that’s what this article is trying to emphasize. reply prizzom 13 hours agoprevThings I like about this article:+ A focus on viewport size instead of screen size.Few people browse websites in full screen in fact \"Full Screen\" browsing is used almost entirely for people using a web browser for presentations or physical kiosks. A completely different use-case than regular desktop browseing.I loathe the fact that when I tile my windows to half my screen size the website \"helpfully\" switches into a tablet&#x2F;mobile layout. Incidentally WCAG (which I consider a well-meaning but ultimately largely useless set of guidelines) can be blamed for a lot of this nonsense.Things that I hate about this article:- Like many \"analysis\" articles they start with a misleading validation of their sample size. 120,000 datapoints is not terribly big.- Implying that these screensizes can&#x27;t be grouped together. Resolutions #3 and #4 are practically identical 393x660 vs 390x664 is essentially a rounding error.- Implying that any sane person should be considering how their desktop&#x2F;mobile website should be displaying on a smart watch. This is totally different use case and (admittedly having never built anything for a smartwatch) I assume (hope?) that unless you&#x27;ve somehow identified your design as being smart watch compatible the browser is very liberally going to strip most of your layout and styling anyway to just text and headings.- Implying that anyone should care about the minor differences in screen sizes. As a veterean of the Flip-phone days when the scourge of \"form over function\" phones (think Beyonce clamshell phone) was at its previous highest. There is no solving this problem. Apple swooped in an took a strong-armed approach to screen sizes that made developing on iOS EXPONENTIALLY EASIER than supporting MIDP or Android.- A useless masonry visualization of viewports in a garish orange&#x2F;purple contrast that is impossible to read. Thanks for nothing. reply RugnirViking 21 hours agoprevNice web design but this jumped out at me:(they have 120000 viewpoints):\"Wembley stadium has a capacity of 90,000, so our datapoints could fill Wembley once and still fill another third of the available capacity.\"the way this is written adds to confusion rather than enlightens. \"another third of the available capacity\" makes it sound like there is space left over in wembley i.e. you haven&#x27;t fully filled it.I think people know how big 120.000 is, but if you must, just say its more than wembley reply paxys 17 hours agoparentSuch meaningless comparisons also often have the opposite of the desired effect. I read it and went \"there are >8 billion people all over the planet, and you can fit all the possible viewports into a single football stadium. Neat!\" reply pdabbadabba 11 hours agoparentprev> I think people know how big 120.000 is, but if you must, just say its more than wembleyAgreed. And if you must compare to a stadium, why not just pick a bigger stadium? You can pack at least 115,000 into Michigan Stadium, which would get you a lot closer than Wembley.(What&#x27;s that you say? You&#x27;ve never been to Michigan Stadium? That&#x27;s probably another reason not to rely on this kind of comparison! I, for example have never been to Wembley and have no idea how big it is.) reply shortrounddev2 20 hours agoparentprevAlso, working in an industry with a lot of data, 120k isn&#x27;t a lot reply afavour 20 hours agorootparentCome on, that&#x27;s a silly statement. There&#x27;s no point comparing completely context free \"amounts of data\".If the post was “the incredible difficulty of inserting 120k rows in database” you’d have a point. But it isn’t. reply paxys 18 hours agoprevPretty ironic that a post about accessible web design is unreadable because of scroll hijacking that makes it constantly jump to the top (at least on mobile). reply rmilejczz 17 hours agoparentya this gave me quite a hearty chuckle. I’m sure the content of the article is excellent but it’s impossible to read on my viewport reply Jabrov 21 hours agoprevLiterally can&#x27;t read through this on Chrome iPhone 13 because it keeps jumping back to the top :( reply csydas 20 hours agoparentI was seeing something similar with desktop Safari (15.6.1) on MacOS. Resizing the browser window, and looks like (but not sure) that it just switches to another breakpoint (their words) for a new viewport, and they didn&#x27;t design a way to pass the page scroll position to the new view port so it just resets the page position.Their \"all viewports\" page also isn&#x27;t visible on my browser if the window is too small as the titles on each viewport aren&#x27;t visible. reply cal85 19 hours agoparentprevSame on iOS Safari. Jumps back to the top seemingly at random. Sometimes I can get a few scrolls down the page but then it jumps back. reply tacker2000 20 hours agoparentprevSame on iphone 12 pro, ios 16 reply smilespray 20 hours agoparentprevSame on my iPad Air 4. reply adithyassekhar 20 hours agoprevI was hoping to read some kind of solution at the end of all this huge text. Some radical way to do layouts without using viewports. Instead I got a link to buy a book from some author.Why is this even here? This is a long ad with useless data points. reply return_to_monke 19 hours agoparentThere is, it&#x27;s called flexbox and the CSS grid.I&#x27;ve found that on simple websites a lot of problems are solved with just a few lines of flexbox config (flex-wrap, etc) reply adventured 17 hours agoparentprevThere&#x27;s a way to do it entirely without viewports, although it&#x27;s extremely unpopular with designers.Present a .css file for mobile or for desktop, based on the presence of \"mobile\" in the agent string. Chrome continues to report that agent data and it&#x27;s very accurate for this use case (where your primary aim is to detect mobile equivalent, else present for desktop).You can drop the viewport html tag. All major phones, including iOS and Android operating systems going back at least a decade, will automatically scale your site for mobile without viewport. You customize the mobile version of your .css file for just mobile. And it&#x27;s very easy these days to cross check to make sure the look and compatibility is correct (for Chrome and Safari mobile in particular). You present different site UI&#x2F;UX based on the \"mobile\" detection; if mobile, present mobile layout, else present desktop layout.Google will punish you (SEO) slightly for the lack of a viewport tag however.This is a far easier way to design for mobile & desktop vs trying to deal with many different viewports (which is a ridiculous, backwards problem that represents an industry failure). reply depressedpanda 11 hours agorootparentFor any aspiring frontend devs that might not know better: they&#x27;re talking about user agent string parsing.Please don&#x27;t do this. There&#x27;s a reason it&#x27;s an unpopular approach. reply temac 16 hours agorootparentprevI&#x27;m not even sure what mobile means in this context. What happens when you plug your phone on a screen? What happens when you use a tablet? reply adventured 16 hours agorootparentThat&#x27;s very straightforward. Obviously the user agent is reported by the browser&#x2F;device as mobile or not (it either includes that keyword or not). In nearly all cases mobile means smartphone or tablet, whether Android or iOS, Chrome or Safari. iPad commonly includes the mobile keyword in its user agent string, unless desktop is enabled by default and then they&#x27;ll just get the desktop version (you could trivially detect for the iPad keyword and force mobile; given the userbase size of iPad it would probably be worth it, it&#x27;d take seconds to do). This approach covers such a comprehensive percentage of users that it&#x27;s only going to be an issue if you absolutely need to cover every single possible variation, and then you have to do it another way to go from 99% to 100%.Your desktop browser doesn&#x27;t include the keyword \"mobile\" in the agent string. Your Safari browser on iOS does, ditto for Chrome on Android.So here&#x27;s the iPhone 13 Max user agent string:\"Mozilla&#x2F;5.0 (iPhone14,3; U; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit&#x2F;602.1.50 (KHTML, like Gecko) Version&#x2F;10.0 Mobile&#x2F;19A346 Safari&#x2F;602.1\"The \"mobile\" keyword gets you what you want.Here&#x27;s iPhone 8:\"Mozilla&#x2F;5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit&#x2F;604.1.34 (KHTML, like Gecko) Version&#x2F;11.0 Mobile&#x2F;15A5341f Safari&#x2F;604.1\"The \"mobile\" keyword gets you what you want.ChromeOS desktop (Chrome browser), Mac desktop (Safari), Windows desktop (eg Chrome or Firefox). They don&#x27;t include the \"mobile\" keyword.It&#x27;s not flawless, getting to 100% requires a lot more effort. This approach is far beyond good enough however, particularly for an average site. If you&#x27;re building a giant enterprise app and want to appeal to every possible user and you have a team, maybe you&#x27;ll throw a lot more resources at getting the small number of problematic edge cases.And if you&#x27;re using Nginx, which I commonly do, you can map the agent key in your http segment, eg:map $http_user_agent $mobilekey {\"~*Mobile\" mobile; default desktop;} (or vise versa)and then you can utilize that for caching related tasks (attach it to your proxy_cache_key to differentiate a mobile vs desktop cache).And as I&#x27;ve noted, this is widely despised by the HN crowd despite the fact that it works so well and so easily. reply madeofpalk 16 hours agorootparentprevCannot tell if this is sarcasm or not... reply adventured 16 hours agorootparentNope, it works very well. I have been testing it routinely for a decade now.The core to it is simply getting the \"mobile\" keyword out of the user agent string. That takes care of nearly everything, from there all you have to do is a simple css split for either desktop or mobile.It very effectively covers all the major browsers, all the major platforms, going back at least a decade. reply esdott 16 hours agoprevFor those who think that you can simply build something “fluid” or “flexible,” it’s a lot harder than it seems. A lot of the industry jargon comes from print and the printing process; margins, padding, kerning, spacing, etc. There is no such thing as a fluid layout on a printing press (as far as I know :-). So we are stuck with a language to describe design based on a different era. Additionally, in the design phase you HAVE to select a layout&#x2F;viewport for proofs and examples. Which in turn the client will expect to look exactly right on every surface. There is obviously room for client education and pushback but fluid designs seem like an afterthought in html&#x2F;css where we are adding new features on top of html&#x2F;css that are best used in a fixed width based system. reply eternityforest 16 hours agoparentIf you learn non fluid design first it seems hard. In absolute terms, it&#x27;s really just a matter of using flexbox and percents, and being willing to just scrap your design idea and do something else if it doesn&#x27;t map nicely to something fluid.Client education is a challenge and all, they often have ideas in mind that are very specific (For some reason clients invariably like the simplest thing that&#x27;s the least general and most direct of a translation from a basic analog system...). But on the technical side... once you stop making designs that rely on being able to control exactly where everything goes it gets a lot easier. reply probablynish 7 hours agoprevI know that this article is probably &#x27;objectively good design&#x27; in some way, seeing as its written by front-end experts of some sort, but I found the content much tougher to read and digest than a much plainer format. All the different bold colors, graphics, and large&#x2F;varied font sizes were tougher for me to absorb than something more straightforward (eg https:&#x2F;&#x2F;www.arslonga.email&#x2F;how-to-connect-zotero-with-obsidi... - just the most recent example I could think of). Anyone else feel the same? reply beeslol 11 hours agoprev> If you’re on a desktop device reading this, how many windows are filling the entire screen? How much screen space does the browser you’re reading from take up?> It’s safest to presume that users on desktop or laptop devices are not filling their entire screen with a browser.Is this true? If you&#x27;re reading this casually (and you are reading this casually) is your browser not at full size? I almost exclusively use my desktop and laptop with windows maximized unless I&#x27;m doing some work which requires me to split up my screen (for example writing while looking at docs or program output). Am I the outlier here? reply pdabbadabba 11 hours agoparentFor me, it varies by OS. I&#x27;m currently reading this on a Mac, and the window is not filling my entire screen. If I were using my Windows PC, however, I would be much more likely to have the window maximized. I find that Windows makes is easier to track all the windows I have open even when one is maximized. reply aendruk 11 hours agoparentprevI almost exclusively use my laptop split-screen unless I’m doing some work that requires maximization.Over the last few years increasingly many websites when allocated half the laptop screen have needlessly contorted into less functional layouts appropriate for a phone, but stretched huge. reply rbarden 8 hours agoparentprevNot an outlier, I almost exclusively use full-screen browser windows. Even if I need to see content in close proximity, I usually just switch between two workspaces (vim on one, browser on another) and flip back and forth pretty easily to see the output. reply pie_flavor 11 hours agoprevMy favorite viewpoint anecdote: Many sites like to assume (like the article mentions) that a good &#x27;mobile view&#x27; cut point is 768px. And some of those sites, in particular NodeBB (back when I used it), use the particular media queries `max-width: 767px` and `min-width: 768px`. On the right laptop, on the right version of everything, if you drag the title bar to one side of the screen so it resizes to fit half the screen, Chrome will report the width 767.5px and the site will immediately break fantastically. reply cthor 20 hours agoprevThat masonry layout diagram is horribly deceptive. The boxes aren&#x27;t to scale at all. reply mcv 19 hours agoparentI see it often, and I hate it. It standardises on width, so everything that&#x27;s wide (landscape oriented photos, or in this case desktop windows) gets shrunk dramatically.In this particular case, it makes the largest viewports look the smallest, and makes the smallest ones look largest. It&#x27;s indeed very deceptive. reply quickthrower2 21 hours agoprevScroll hijack makes this unusable reply ovao 21 hours agoparentI didn’t experience any issues on my first read through, but on the second visit it now randomly scrolls back to the top of the viewport. reply aidenn0 15 hours agoprevAndroid app authors ought to read this too. I recently had an android app where the button on one screen was past the bottom of my screen and it wasn&#x27;t scrollable. I went to report the bug and someone else had already reported it.Given that the workaround was to set the system font-size small enough that the button would appear back on the screen, this would also be an accessibility problem for those who have their font-size set larger, even if their screen would be normally large enough to display it. reply larrik 14 hours agoprev> Even on one iOS device, there&#x27;s a minimum of 3 environments a website could find itself in, based on operating system states.Besides the fact that I&#x27;m not convinced the 3d touch one is a real viewport, how can he miss the fact that there is landscape mode on these devices, which have completely different dimensions? reply Devasta 18 hours agoprevThe fix is simple: Lobby congress to ban everything except 1024 x 768 resolution screens. reply Solvency 17 hours agoprevMany parts of this article are unviewable on mobile. Irony? reply terracottalite 13 hours agoprevLOL. Line 1268 of that CSV file [0] has a negative height. And it actually is in the visualization [1]. Wonder what kind of device reports a -2000 height :D[0]:https:&#x2F;&#x2F;viewports.fyi&#x2F;data.csv [1]:https:&#x2F;&#x2F;viewports.fyi&#x2F;all&#x2F; reply antod 6 hours agoparentMaybe someone holding their phone upside down? reply orliesaurus 16 hours agoprevNot sure if the author will read this but, I think there&#x27;s a typo on that page towards the end:\"it&#x27;s a much difference picture.\" should be \"it&#x27;s a much differeNT picture.\" reply kosasbest 16 hours agoprevResponsive sites have to be like elastic &#x27;jelly&#x27; and accommodate every view-port resolution. The only exception being non-mobile-friendly web apps, in which case some sort of manifest should be read by the browser and presented to the user clearly stating mobile isn&#x27;t supported and they should use a desktop browser instead.Then there&#x27;s the progressive web app (PWA) debacle, where users don&#x27;t even know what a PWA is and don&#x27;t know that they can pin sites to their home-screen, simulating an app. reply depressedpanda 11 hours agoparent> Then there&#x27;s the progressive web app (PWA) debacle, where users don&#x27;t even know what a PWA is and don&#x27;t know that they can pin sites to their home-screen, simulating an app.Users don&#x27;t need to know what a PWA is. Thereason users don&#x27;t know that they can be &#x27;installed&#x27; is because they&#x27;re on iOS, where Apple has been effectively hamstringing them in favor of the Apple App Store. There is a way to install PWAs on iOS, but it&#x27;s quite hidden. reply calderwoodra 16 hours agoparentprevOur product is the exception here (desktop only web app).We basically just account for 800-1300px width range and call it a day. On the low-side, we show a \"desktop only\" overlay. On the high side, we block the content from expanding. reply locusofself 18 hours agoprev\"The population of our home town, Cheltenham, is around 116,000 so our datapoints could almost populate the entire town!\"120,000 > 116,000, so their 120k data points could more than populate the town... reply ciwolsey 18 hours agoprevI refuse to read anything even remotely related to accessibility written by someone dumb enough to put white text on a yellow background. reply teekert 18 hours agoparent? I don&#x27;t see that anywhere? reply qingcharles 9 hours agoparentprevScreenshot? reply adventured 18 hours agoparentprevI&#x27;m on desktop so maybe it&#x27;s different on mobile, however I don&#x27;t see a single example on their site of white text on yellow background, only black text on yellow background. reply chiefalchemist 20 hours agoprevWith flexbox and grid, and dynamic text size, how necessary are breakpoints at this point? reply seydor 17 hours agoprevi wish the fad of responsiveness went away. i m fine with zooming in and out in pages, and i like thinking of a document as a 2d map. reply Am4TIfIsER0ppos 16 hours agoprevhttp:&#x2F;&#x2F;motherfuckingwebsite.com&#x2F; unironically reply swayvil 18 hours agoprevCould an AI write a JIT custom window manager? Or would that be too unreliable? reply ghusto 21 hours agoprev [–] > It’s safest to presume that users on desktop or laptop devices are not filling their entire screen with a browser.Not on literally any laptop I&#x27;ve ever seen. I&#x27;ve never seen a screen where the browser _isn&#x27;t_ filling all space available to them, sometimes even the entire screen (in fullscreen mode on MacOS). reply jerf 21 hours agoparentI believe the intention of that statement is to not assume that it&#x27;s filling the entire screen, that is, not to count on it in some manner.A lot of people still make this mistake one way or another. In about the last year I finally gave up; my tiling window manager used to have the browser at 2&#x2F;3rds the screen with the remaining 1&#x2F;3 a terminal, but more and more I&#x27;ve been encountering websites that are very unhappy with that width; not small enough to kick in mobile mode or they detect that I&#x27;m on a desktop browser and don&#x27;t adjust, but not large enough for the design they serve me, creating overlapping regions, or horizontal scrolling, etc. So my browser is full screen a lot more often now. On the one hand I&#x27;m annoyed, but on the other, an implication of the article is that you essentially can&#x27;t test at \"all\" viewport sizes anymore and it&#x27;s just so easy to accidentally write a website that is unhappy at exactly pixel widths 892-945 when used with my exact font setup and&#x2F;or zoom setting that I can&#x27;t really stay mad at some of these sites. Some of the sites I have trouble with are clearly trying, and are competent in many other ways. You can hardly expect your designers to visit every page on your site and test every single width from 200 to 2000 one pixel at at time, let alone ask them to do it again with a variety of zoom settings.Heck, I can&#x27;t even tell you my own website can do that. I&#x27;ve checked it on a phone and in my desktop browser at a couple of sizes, but I can&#x27;t do it anymore than anyone else can nowadays. reply Kerrick 20 hours agorootparentI’ve noticed similar annoyances lately browsing sites on my 4K displays set to 200% scaling and positioned vertically. Sites tend not to work or look their best at 1080px wide anymore.It almost makes me nostalgic for 960px being the defacto standard width for desktop sites. reply PMunch 21 hours agoparentprevYou never side by side your browser and another application like a terminal or text editor? And even barring that the taskbar of your OS along with window decorations means that your browser is seldom 100% of the screen. This is also talking about the viewport, so even though they clumsily refer to it as the \"browser\" the website won&#x27;t be given 100% of your resolution even if your browser did have 100% simply because of things like the tabs bar and other navigation. Can you full-screen a website? Sure. But that&#x27;s not the common way to browse. reply MrJohz 20 hours agoparentprevFor general browsing, that&#x27;s often the case, but for solving tasks, I often see people using the \"snap to side\" feature in Windows. It means you can view two windows side-by-side without faffing around dragging window sides around, and it&#x27;s really useful if you&#x27;re trying to cross-reference data on a smaller screen.Although I think that sentence would probably be better written \"It&#x27;s safest not to presume [...]\", as in, don&#x27;t make assumptions about screen sizes here. reply kube-system 17 hours agoparentprevI presume you&#x27;re talking about maximized windows, but that still does not equal \"filling the entire screen\", because browsers have their own user interface around the viewport.Go maximize your browser and enter this into your dev tools: console.log(`${window.innerWidth}x${window.innerHeight}`)Does that equal your screen resolution? reply madeofpalk 16 hours agorootparentI like this because opening the dev tools has a high chance of changing innerHeight :) reply kube-system 12 hours agorootparentAnd even if you open dev tools in another window, many people run browsers that are configured to display other controls... like tabs, or an address bar. reply DylanSp 21 hours agoparentprevMy guess is that filling the entire entire screen is common, especially on laptops that aren&#x27;t hooked up to a monitor, but not common enough to assume that&#x27;ll always be the case. reply amiga-workbench 20 hours agoparentprevIts a Mac user thing, they have a collage of overlapping randomly shaped windows splayed everywhere. Windows users tend to just maximise their programs or snap them when needing to multitask.Gnome pushes users to fullscreen programs too, with a quick mouse twitch to get the overview open to switch programs. reply chc 14 hours agorootparentA lot of Linux users tile their windows too. reply flohofwoe 21 hours agoparentprevThat&#x27;s the definition of &#x27;anectodal evidence&#x27; though. The nice thing of desktop environments is that people (still) have enough freedom to organize windows the way they want without a platform owner telling them what&#x27;s \"best for them\". reply bee_rider 20 hours agoparentprevIt just seems like confusing wording. I think you are right that most people use their programs full-screen on laptops (As an aside, it is funny that thin-and-light-ism has managed to progress to the point where the average person finally gets the 2008 experience).But I think what they meant is: it is safest not to presume that desktop or laptop users are filling their entire screen with a browser.The way they wrote it suggest that the assumption of the negative is safest, while the way I wrote it says it is safest not to make the assumption. This has a general smell of the inverse fallacy, although it isn’t like they are trying to make some bulletproof logical deduction anyway. reply catapart 18 hours agorootparentI&#x27;m kind of surprised here, myself, and am very interested in seeing real numbers because, just from the replies to this comment I&#x27;m seeing a lot of difference of opinion.Personally, I always use a browser full screen and most everything else is &#x27;windowed&#x27;. My browser usually ends up being the \"background\" of my monitor, only traded out when I&#x27;m writing code, in which case my IDE is the \"background\" (full screen). Everything else except games runs in a smaller window. Dragging and dropping still works, like that. It&#x27;s just really convenient.From mostly working with other devs, this is usually what I see. This is how I see MOST people use their desktops. Full screen browser, always. Then we get to this article and this comment section and people are talking like it&#x27;s a given that MOST people use their browsers in windowed mode. Personally, the only time I can even remember someone doing that is when they snap it to an area and then snap something else beside it.My suspicion is this is a Windows&#x2F;Linux vs Mac thing? Everything in Mac defaults to \"floating\" app windows, instead of defaulting to a single app window, maximized, with sub-panel windows inside the app renderer. So people just kind of mentally map it that way, depending on the OS? But then, that&#x27;s just a guess!Obviously, I don&#x27;t mean to imply that one way is better or worse. I&#x27;m just kind of intrigued by how blindsided I - and apparently others - are about this.Anyway, you&#x27;re right about the article&#x27;s hand-waving. It&#x27;s not as compelling to me, because I&#x27;ve had users file tickets when our \"anything past 1060px gets centered on the screen, instead of expanding to fill\" method was displayed on a 4K screen. They claimed it was \"unusable\", even though it was entirely the same, it just didn&#x27;t expand as much as they would have like (yes, even on a TV; other users were using it just fine). So I don&#x27;t think anything past 800 is less worth consideration. At the very least, I&#x27;d say that number is more like 960 or 1400. The upper end of standard desktop sizes instead of the lower. At 800px width, even default browser font sizes look big. Scaling down to a 10 or 11px font size is fine for complex utilities, but when you&#x27;re trying to size it for big, finger-sized buttons, 800px gets cramped, fast.But yeah, the overall advice holds. reply sebzim4500 21 hours agoparentprevI think most people have a task bar at the bottom, at least on Windows and Linux. reply marginalia_nu 21 hours agoparentprev [–] You&#x27;ve never seen a 17\"+ laptop I guess? replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article discusses the fragmentation of screen sizes and browser viewports and the importance of considering this in design decisions.",
      "The authors conducted an experiment with over 120,000 data points and found that the top viewport sizes were primarily small, potentially mobile devices.",
      "The article advises against using specific breakpoints and hard values, instead advocating for flexibility and allowing the browser to determine the best outcome based on conditions."
    ],
    "commentSummary": [
      "The article and comment section delve into the complexities of web design, discussing topics such as viewports, responsiveness, and usability across different devices.",
      "The conversation emphasizes the challenges faced by designers when dealing with client demands, disabling zooming capabilities, and implementing accessibility features.",
      "The need for thoughtful and adaptable designs to improve the user experience on various devices is emphasized, along with the limitations of current design tools and the struggles of creating fluid and flexible layouts."
    ],
    "points": 264,
    "commentCount": 173,
    "retryCount": 0,
    "time": 1692618570
  },
  {
    "id": 37216647,
    "title": "Growing share of Americans favor more nuclear power",
    "originLink": "https://www.pewresearch.org/short-reads/2023/08/18/growing-share-of-americans-favor-more-nuclear-power/",
    "originBody": "NUMBERS, FACTS AND TRENDS SHAPING YOUR WORLD NEWSLETTERS PRESS DONATE MY ACCOUNT CONTACTED BY US? Read our research on: World LeadersArtificial IntelligenceScience Issues RESEARCH TOPICS ALL PUBLICATIONS METHODS SHORT READS TOOLS & RESOURCES EXPERTS ABOUT Home Research Topics Science Science Issues Climate, Energy & Environment AUGUST 18, 2023 Growing share of Americans favor more nuclear power BY REBECCA LEPPERT AND BRIAN KENNEDY Diablo Canyon, the only operational nuclear power plant left in California, is seeking to extend operations past its scheduled decommissioning in 2025. (George Rose/Getty Images) As the first new U.S. nuclear power reactor since 2016 begins operations, more Americans now say they favor expanding nuclear power in the United States than a few years ago, according to a recent Pew Research Center survey. How we did this A majority of Americans (57%) say they favor more nuclear power plants to generate electricity in the country, up from 43% who said this in 2020. Americans are still far more likely to say they favor more solar power (82%) and wind power (75%) than nuclear power. All three energy sources emit no carbon. Advocates for nuclear power argue it could play a crucial role in reducing carbon emissions from electricity generation. Critics highlight the high cost of nuclear power plant projects and the complexities of handling radioactive waste. Support for nuclear power has increased among both parties since 2020. Half of Democrats and Democratic-leaning independents now say they favor expanding nuclear power, an increase from 37% in 2020. And two-thirds of Republicans and Republican leaners now favor more nuclear power, up 14 percentage points since 2020, when 53% said they support more nuclear power. When asked about the federal government’s role, 41% of Americans say it should encourage the production of nuclear power. Some 22% think the federal government should discourage the production of nuclear power, and 36% think it should neither encourage nor discourage it. The share of Americans who think the federal government should encourage nuclear power production is up 6 points from last year. Still, a far larger share of Americans think the federal government should encourage the production of wind and solar power (66%). Gender, partisan differences in views of nuclear power Attitudes on nuclear power production have long differed by gender and party affiliation. Men are about twice as likely as women to say the federal government should encourage the production of nuclear power (54% vs. 28%). Similarly, men are far more likely than women to favor more nuclear power plants to generate electricity (71% vs. 44%). Views differ by gender globally, too, according to a Center survey conducted from fall 2019 to spring 2020. In 18 of the 20 survey publics, men were more likely than women to favor using more nuclear power as a source of domestic energy. In the U.S., Republicans are more likely than Democrats to favor more nuclear power and to say the federal government should encourage its production. Two-thirds of Republicans say they favor more nuclear power plants to generate electricity, compared with half of Democrats. Republicans have supported nuclear power expansion in greater shares than Democrats each time this question has been asked since 2016. The 17-point partisan difference on nuclear power is smaller than those for other energy sources, including fossil fuel sources such as offshore oil and gas drilling (48 points) and coal mining (47 points). A look at U.S. nuclear power reactors The U.S. currently has 93 nuclear power reactors, plus one that’s under construction in Georgia. These reactors collectively generated 18.2% of all U.S. electricity in 2022, according to the U.S. Energy Information Administration. Half (47) of the United States’ nuclear power reactors are in the South, while about a quarter (22) are in the Midwest. There are 18 reactors in the Northeast and six in the West, according to data from the International Atomic Energy Agency (IAEA). The number of U.S. reactors has steadily fallen since peaking at 111 in 1990. Nine Mile Point-1, located in Scriba, New York, is the oldest U.S. nuclear power reactor still in operation. It was first connected to the power grid in November 1969. Most of the 93 current reactors began operations in the 1970s (41 reactors) or 1980s (44), according to data from the IAEA. (The IAEA classifies reactors as “operational” from their first electrical grid connection to their date of permanent shutdown.) One of the many reasons nuclear power projects have dwindled in recent decades may be perceived dangers following nuclear accidents in the U.S. and abroad. For example, the 2011 Fukushima Daiichi accident led the Japanese government to greatly decrease its reliance on nuclear power and prompted other countries to rethink their nuclear energy plans. More recently, Russian military attacks in Ukraine have raised fears of nuclear power plant accidents in the area. Note: Here are the questions used for the analysis, along with responses, and its methodology. This is an update of a post first published March 23, 2022. Topics Climate, Energy & EnvironmentEnergy SHARE THIS LINK: Rebecca Leppert is a copy editor at Pew Research Center. POSTS BIO EMAIL Brian Kennedy is a senior researcher focusing on science and society research at Pew Research Center. POSTS BIO TWITTER EMAIL Sign up for our weekly newsletter Fresh data delivered Saturday mornings SIGN UP RELATED SHORT READJUL 13, 2023 How Americans view electric vehicles SHORT READOCT 29, 2021 Fast facts about international views of climate change as Biden attends UN COP26 conference SHORT READOCT 14, 2021 67% of Americans perceive a rise in extreme weather, but partisans differ over government efforts to address it SHORT READOCT 4, 2021 Most U.S. Latinos say global climate change and other environmental issues impact their local communities SHORT READJUL 23, 2021 On climate change, Republicans are open to some policy approaches, even as they assign the issue low priority TOPICS Climate, Energy & Environment Energy MOST POPULAR 1 32% of Americans have a tattoo, including 22% who have more than one 2 Little change in Americans’ views of Trump over the past year 3 What Americans Know About AI, Cybersecurity and Big Tech 4 Why Some Americans Do Not See Urgency on Climate Change 5 Martin Luther King Jr.’s Legacy 60 Years After the March on Washington 1615 L St. NW, Suite 800 Washington, DC 20036 USA (+1) 202-419-4300Main (+1) 202-857-8562Fax (+1) 202-419-4372Media Inquiries RESEARCH TOPICS Politics & Policy International Affairs Immigration & Migration Race & Ethnicity Religion Age & Generations Gender & LGBTQ Family & Relationships Economy & Work Science Internet & Technology News Habits & Media Methodological Research Full topic list FOLLOW US Email Newsletters Instagram Twitter LinkedIn YouTube RSS ABOUT PEW RESEARCH CENTER Pew Research Center is a nonpartisan fact tank that informs the public about the issues, attitudes and trends shaping the world. It conducts public opinion polling, demographic research, media content analysis and other empirical social science research. Pew Research Center does not take policy positions. It is a subsidiary of The Pew Charitable Trusts. Copyright 2023 Pew Research Center About Terms & Conditions Privacy Policy Cookie Settings Reprints, Permissions & Use Policy Feedback Careers Privacy Notice We use cookies and other technologies to help improve your experience; some are necessary for the site to work, and some are optional. Learn more in ourprivacy policy. Accept ALL cookies Accept only necessary cookies Cookie Settings",
    "commentLink": "https://news.ycombinator.com/item?id=37216647",
    "commentBody": "Growing share of Americans favor more nuclear powerHacker NewspastloginGrowing share of Americans favor more nuclear power (pewresearch.org) 221 points by freedude 10 hours ago| hidepastfavorite237 comments robviren 8 hours agoWorked in nuclear power for five years and wish people could divorce themselves of bias when considering power. When you plug things in it has a cost greater than the dollars you are paying. In raw deaths per megawatt nuclear wins above all else. I&#x27;m not saying we should avoid solar and wind. I&#x27;m saying we should dive headlong into every form of power that seems a reasonable risk to human life and the environment before 1000+ human death wildfires, starvation, and every other natural disaster becomes common place. Belching coal exhaust or gas turbine exhaust into the air seems a careless act when I have personally sat inside, on top of, under, and to the side of nuclear reactors and have yet to fall over dead. Spent fuel can be handled and most import you don&#x27;t need to breath it and it doesn&#x27;t warm the planet. Everything has a cost and I would gladly bare the cost of nuclear for a world worth living in. reply FunnyLookinHat 7 hours agoparentCannot recommend this podcast enough: https:&#x2F;&#x2F;freakonomics.com&#x2F;podcast&#x2F;nuclear-power-isnt-perfect-...It goes into detail about the argument you are proposing. reply asynchronous 6 hours agoparentprevPlus then we could get more Fallout themed public places which IMO is a huge plus reply noogle 6 hours agoparentprevWe need to shift from thinking of solar&#x2F;wind as \"electricity sources\" to thinking of them as \"fuel sources\". The marginal cost of producing a transferrable fuel from solar&#x2F;wind is already lower than current electricity prices. The challenge is the capital expenses on equipment (hydrolyzers, fuel cell etc.) reply AtlasBarfed 5 hours agorootparentOf course the article is just a puff public opinion piece. It doesn&#x27;t change the economics:Point ONE: nuclear is more expensive CURRENTLY that solar&#x2F;wind. SOlar&#x2F;Wind is currently the cheapest by far, and here&#x27;s the scary thing for gas turbine: solar + storage is very close to druopping under gas combined cycle generation. Look up Lazard 2023 LCOE study. Solar&#x2F;Wind have dropped to 24$ per MWHr. The cheapest nuclear produces is $141&#x2F;MWHr. That&#x27;s right, nuclear is almost 600% more expensive.Point TWO: Say you miraculously got 100 billion dollars through congress for new plants to start immediate projects. Not a single reactor will come online for 10 years. Now, look at the cost improvement rate of solar, wind, and storage for the last 10 years. 10% year-on-year improvements or more. Even if we had half of that for the next 10 years (and there will be 10 more years of cost improvement), solar &#x2F; wind very likely will be HALF the current cost.So you&#x27;d have 100s of nuclear plants come online in 10 years generating electricity that costs 10-20x more than electricity from wind&#x2F;solar. That simply is not a viable national power strategy.If we had these plants already because construction started in the year 2000, that would probably be a different story.The only hope for nuclear is probably in about 15-20 years where the cost improvement curves finally stabilize for wind&#x2F;solar, and then a stable price point can be targeted with new nuclear designs. I personally think that only something like a novel MSR&#x2F;LFTR which can scale down to mass producable sizes, uses all the fuel, breeds. doesn&#x27;t have solid fuel rod reprocessing and waste transport&#x2F;storage, and can use the Brayton cycle for more efficiency has a chance of competing with mature solar&#x2F;wind.Puff pieces like this are really about the current nuclear plants and keeping them on funding life support, which I generally support for now. The industry sees that Lazard LCOE curve just like any other person would: do you want to pay solar&#x2F;wind costs for electricity, or 6x that for nuclear?The existing nuclear industry can&#x27;t survive without subsidies. reply frafra 2 hours agorootparentThe final price does not depend only on the production price. You cited Lazard 2023: if you read the firming intermittency price estimations, you figure out that solar and wind in the California grid costs 2&#x2F;3 times more than when just considering the LCOE. Solar and wind production can be cheaper, but the final price is way higher, and it further increases when renewables take a larger share of the energy production. reply ftth_finland 4 hours agorootparentprevLazard only does LCOE for short term storage, since there is little to none long term storage. reply jayd16 6 hours agoparentprevWhat is the major cause of solar death? Roofing falls? reply gcanyon 6 hours agorootparentAmazingly enough, yes: https:&#x2F;&#x2F;www.nextbigfuture.com&#x2F;2021&#x2F;07&#x2F;2020-fatalities-for-us... reply jandrese 5 hours agorootparentprevYep, and the major killer with Wind is people falling off of the turbines.Hydro is extremely safe except for a handful mass casualty incidents that completely skew the numbers. Nuclear is in the same boat, with really just 1 significant incident. reply jayd16 4 hours agorootparentI think if we&#x27;re honest, it&#x27;s obvious what the average voter is scared of. If you separate these deaths into workplace and bystander deaths it paints a different picture. reply brucethemoose2 6 hours agorootparentprevOpportunity cost in locations solar isn&#x27;t so efficient.Nuclear is a good \"filler\" for environments that aren&#x27;t conducive to other renewables. reply bilbo0s 7 hours agoparentprevThe problem is not deaths per megawatt. Nor bias.Look at the economic disaster that is Vogtie.People need to be reasonable. If we want nuclear, the government will need to pay for it. Right now everyone&#x27;s waiting for energy investors and the financial engineering that backs them to throw money into nuclear. It won&#x27;t happen. We want rich people to throw their billions at nuclear for a return years and years from now, instead of spending it just slapping up some windmills and getting a return next month.T Boone Pickens was one of the most conservative energy investors that ever lived, and he was slapping up windmills left and right. That won&#x27;t stop until the tax payer agrees to insulate investors from any losses.Everyone&#x27;s saying, \"We want nuclear.\" But when we give them the financial consequences of nuclear like we did at Vogtie, they get pissed. You can&#x27;t have it both ways. You want it? Cool. Don&#x27;t complain about being told you have to pay off your region&#x27;s reactors.A little more cooperation from everyone on the finances would go a long way here. reply liveoneggs 7 hours agorootparentVotgle 3 has been running for almost a month. It started construction August 26, 2009 and is finishing $17B over budget so far, total about $30B. It bankrupted \"Westinghouse Electric Company\".Compared to the forever&#x2F;oil wars it seems like a pretty good deal, to be honest.If we had not invaded Iraq we could have built 30 Votgles. USA has spent 2-3 votgles on Ukraine so far since the war began. reply JamesBarney 6 hours agorootparentA lot of this is not inherent to building nuclear reactors. We&#x27;ve regulated nuclear power to the point where it&#x27;s cost prohibitive. If we required every solar panel installer to have a phd, and only let them work 1 hr per day solar would be incredibly expensive too.Imagine we were willing to accept the same death rate as natural gas. Reducing regulations to the point of having 100x more nuclear incidents. How much cheaper would they be to build? reply Panzer04 5 hours agorootparentThe problem with nuclear accidents is the consequences can be so catosrophic that it&#x27;s virtually impossible to accept failure. Irradiate a few hundred sqkm in an accident and you easily exceed every other expense of building the plant (being generous).It&#x27;s funny how usually people argue for more regulation of side effects, but as soon as nuclear gets involved it&#x27;s the reverse. Nuclear must be absolutely safe, because no one but the government (and by extension, the general populace) can afford the expense of insuring it. Reducing regulation is tantamount to giving investors permission to pull as much money as they like out of the plant and leaving us with the risk. reply ryandrake 5 hours agorootparentWe also know that corporations are going to be corporations. The business guys will eventually take the wheel and absent sufficient regulation they will inevitably cost-cut and corner-cut their way to disaster. It’s totally predictable.If there is ever even a remote profit motive, everything, including safety, will eventually be thrown under the bus to maximize profit. It’s how all businesses are run. reply jandrese 5 hours agorootparentprevIf we had built 30 Votgles they wouldn&#x27;t have cost $30B each. One of the huge problems with modern Nuclear plants is there is no economy of scale. They&#x27;re all bespoke projects with fresh people re-learning the skills of the past. If they were being mass produced there would be an established industry with veterans working on most of the plants, saving enormous amounts of time and money.This is where Wind and Solar are really winning. The industries building them are growing mature and as a result efficiency goes up and costs go down.This was the promise of nuclear microreactors: that they could be mass produced in a factory and shipped where needed. But it seems like they&#x27;re never going to get off of the ground. reply ttul 7 hours agorootparentprevInteresting analysis. Indeed without the need for oil and gas, Russia would not have the cash to invade Ukraine. reply whatshisface 6 hours agorootparentOr a motive to secure the resources lying beneath the east of the country. reply bamboozled 7 hours agorootparentprevBut there is money in oil and gas, big money, which is why you don&#x27;t get nuclear... reply onlyrealcuzzo 6 hours agorootparentprevIsn&#x27;t US energy generation ~1200GW? If a Vogtle is 1.1GW - 30 of them isn&#x27;t even 3% of US energy generation.We&#x27;d still need a ton of oil... reply knowaveragejoe 7 hours agorootparentprev> USA has spent 2-3 votgles on Ukraine so far since the war began.We all know this isn&#x27;t even remotely the case, right? reply jakeinspace 6 hours agorootparentPeople might not be aware, but the bulk of the support given to Ukraine has been military surplus gear, rather than cash. The US has sent tens of billions in cash for humanitarian and some military support as well, but the bulk of what’s counted in the $100B+ of aid has been in the form of weapons and ammunition and vehicles. reply JamesBarney 6 hours agorootparentMost of which we were going to decommission anyway which would have cost us money. (With the patriot missile battery being a notable exception) reply drno123 5 hours agorootparentYeah, it is profitable for the US that Ukraine people keep getting slaughtered. reply Marrand 3 hours agorootparentHow? replyAccujack 7 hours agorootparentprevThis is the classic anti-nuclear rhetoricAlways bring up the cost of nuclear power plants and the time to build them, but ignore that all the power plants built in the US in recent years use 1950s technology and are made expensive by 50 years of excessive regulation driven by fear. reply BenFranklin100 6 hours agorootparentTo further your point, almost all nuclear power plants to date have essentially been customized, one-off projects built by inexperienced teams.If we built exactly the same design tens to the low hundreds of times by the same handful of teams, the cost per unit would be dramatically lower. This is especially true of the modern, safer, and more cost effective designs. reply laurencerowe 6 hours agorootparentHave any of these modern cost effective designs been built yet?All of the third generation EPR projects being built in Europe have run hugely over budget. I really thought if anyone could build nuclear plants cost effectively it would have been France so it&#x27;s really made me lose hope in nuclear power. reply Gud 4 hours agorootparentWhat about China? UAE? These countries actually know how to build large scale infrastructure projects and are highly successful building nuclear power plants. reply ETH_start 5 hours agorootparentprevWind and especially solar were massively subsidized for decades before accumulated innovations from the subsidized at-scale manufacturing, and economies of scale, pushed prices per unit down to levels that are competitive in the market.What nuclear needs is massive subsidies, especially in reactors using small modular designs that are more amenable to mass-manufacturing techniques and transportability, and it needs them sustained for decades until the competitive forces of the large market for SMRs has brought the price of reactors down to levels where they&#x27;re competitive in the market without subsidies, at which point the growth of privately funded nuclear power would be self-sustaining. reply laurencerowe 5 hours agorootparentThe question is will these massive subsidises be better spent on nuclear or renewables and storage?Over the years nuclear has enjoyed enormous subsidies. If it were not for the huge decrease in costs for renewables over the past couple of decades and huge increase in cost for nuclear I’d be much more enthusiastic about nuclear power. reply Gud 4 hours agorootparentOn both. Oil, gas and coal are the enemy here, not nuclear.I can’t believe that people are still arguing against nuclear. It’s an extremely proven technology. It can be built fast, cheap and safely. reply laurencerowe 2 hours agorootparentI’m completely against closing existing nuclear plants as they are doing in Germany. The big problem with new nuclear energy seems to be build cost so it makes sense to get as much as we can from existing plants.But at the end of the day real resources matter. Where should labour effort be directed? Costs reflect that effort.I’m not against nuclear but if redirecting effort (money, labour) from nuclear to renewables and storage seems likely to reduce co2 emissions faster then let’s redirect that effort. It’s fine for cool technologies to be abandoned when cheaper alternatives come along.If it’s possible to come up with a far cheaper form of nuclear power then great! But it’s hard to compete with the big fusion reactor in the sky. reply magicalist 4 hours agorootparentprevNuclear is massively subsidized and still no one wants to build it. reply lamontcg 6 hours agorootparentprev> made expensive by 50 years of excessive regulation driven by fear.they&#x27;re expensive because we can&#x27;t build ANYTHING big in this country without cost overruns. reply pstuart 7 hours agorootparentprevTheoretically the regulation was known at bidding time and factored in. Or they didn&#x27;t understand them and were caught by surprise. Or perhaps they did understand and lied by underbidding?I was anti-nuke when I was younger but recognize we fucked up terribly by not getting nuclear right and have now cooked the planet.The appealing solution (to me) is to get SMRs into play with their associated promises of cost control and safety. My understanding is the blocker is really the existing insiders of the nuclear industry who see it as a threat to their work. reply imperfectcats 6 hours agorootparentI wish regulation worked that way! Regulations change all the time - if not in law certainly in practice&#x2F;interpretation.That is what makes industries with heavy regulation such risky investments. reply jdadj 7 hours agorootparentprev> That won&#x27;t stop until the tax payer agrees to insulate investors from any losses.What does this mean? Are corporate structures (corporations, LLCs, etc.) insufficient as liability shields? reply TheDudeMan 7 hours agorootparentprevGet rid of the red tape and nuclear becomes much cheaper (over time). reply bamboozled 7 hours agorootparentFukushima is what happens when you get rid of the red tape, go look at the cause of the accident and you will see why the red tape exists [1].The Japanese tax payer is still paying for this disaster, and the environment is paying in more ways imaginable, all of the cleanup operation so far has been running on fossil fuels and it&#x27;s an significant amount of cleanup still going onto this day, hundreds of thousands of tons of top soil have been moved around the country and stored in disintegrating plastic bags which end up in rivers, for example.People avoid buying produce and seafood from the area too which has had enormous impact on the economy and tens of thousands of people have been displaced with little to no compensation or anywhere to go.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fukushima_nuclear_disaster#200... reply MostlyStable 7 hours agorootparentYou mean the disaster that killed between 0 (from radiation) and 2000 (from the evacuation) people? Compared do the literal millions that die every year from fossil fuel pollution?It would take about 10x the amount of nuclear power plants that we currently have for global energy to be totally nuclear. If we therefore 10x every single nuclear reactor disaster from the past 70 years (~ how long since the first production plant came on line in the USSR), global deaths would be between between 80,000 and 40,000,000 (using the implausibly high Greenpeace estimate of Chernobyl associated deaths).I think that all of those estimates are implausibly high, but even if we just take the 40,000,000 number, the most ridiculously, obviously wrong number, the estimated deaths every year from fossil fuel pollution (not climate change...direct particulate matter air pollution) is between 2 and 8 million every year. So go ahead, tell me again how we can&#x27;t afford to relax safety standards on nuclear power. reply lamontcg 6 hours agorootparentZero deaths is debatable, and it is only possible due to a trillion dollar cleanup cost. reply endominus 5 hours agorootparentPer https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fukushima_disaster_cleanup, \"In 2016, Japan&#x27;s Ministry of Economy, Trade and Industry estimated the total cost of dealing with the Fukushima disaster at ¥21.5 trillion (US$187 billion)\" (with breakdown following). Hyperbole does not serve to convince intelligent people of your point, much less your objectivity in the matter. reply lamontcg 3 hours agorootparentASME estimated closer to US$500b:https:&#x2F;&#x2F;asmedigitalcollection.asme.org&#x2F;memagazineselect&#x2F;arti...JCER estimated from US$470b to US$660bhttps:&#x2F;&#x2F;www.scientificamerican.com&#x2F;article&#x2F;clearing-the-radi...The official govt estimates have risen from $50b to $100b to $200b and are probably too low. replyhayst4ck 6 hours agorootparentprevAdmiral Rickover (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hyman_G._Rickover), the father of the nuclear navy would disagree with you, but probably in a much more nuanced way than direct contradiction.It is not the red tape, but the difference between oversight and culture. You can not regulate safety and expect safety. Safety does not come from oversight. Safety comes from culture and a shared culture of responsibility for safety.Therefore the adversarial nature of red tape, implying an entity that does taping and therefore bears primary responsibility for safety, encodes within it the set of incentives that will result in eventual doom because alignment is always for cheaper and not for the long term. Safety requires individual responsibility and therefore a culture of responsibility, not to shareholders or next quarters profits, but to a set of values such as safety and competence.This hearing where the head of the nuclear navy tells congress about their philosophy of safety is very much worth a read: https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20060629082752&#x2F;https:&#x2F;&#x2F;www.navy....The end result is that if you are talking about red tape at all then you have not reached a state of understanding because you must breach the topic of culture and alignment. Capitalist alignment of nuclear energy directly results in disaster. A culture that venerates next quarters profits should not run 30 year+ programs for which failure can result in centuries of contamination.Do you want people cut from the same cloth that the people who run BP are cut from running nuclear? https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Deepwater_Horizon_oil_spillA congress that asks for cheaper will get cheaper. Unfortunately what is cheap in dollars can be expensive in property, lives, and well-being. A simple word like cheaper is too naive. reply jauntywundrkind 7 hours agorootparentprevIf nuclear is going to happen it should be a campaign to improve & get better. Today & long term. Systems like Integral Fast Reactor & then PRISM & S-PRISM & so many others show huge promise at being absurdly safe & being able to help us burn existing waste, without having to deeply enrich: a huge win. But no one is going to foot the bill, no one is going to take risk on advanced nuclear, when we&#x27;re not even sure about the plodding low effort ultra-lofi nuclear we still can&#x27;t do cost effectively today.I agree so much, this needs to be a bigger collectivized effort if it&#x27;s going to happen. Ideally it should be something we can as a civilization improve on, where we keep improving together. Making a bunch of new proprietary designs then having 20 years of patent protection before anyone else can try & iterates or improve is a death knell for the industry as a whole. But no commercial entity is going to choose to participate in this prisoners dilemma today. reply fnord77 7 hours agoparentprevbig oil astroturfed a campaign against nuclear in the 70s. It was framed as hippies warning nuclear would mean certain death for everyone.that perception stuck reply JamesBarney 6 hours agorootparentIt was not big oil that pushed the anti-nuclear campaign. It was environmentalists. And they&#x27;re still active today, sometimes pushing for solar. Sometimes fighting solar and windmills to protect wildlife.Now I&#x27;m sure some oil and gas companies decided to fund some of these groups because they saw these people could be useful, but that doesn&#x27;t mean the movement was astroturfing. reply Gud 4 hours agorootparentIt was both. And in addition, some of these so called environmental organizations were funded by big oil. reply odiroot 2 hours agorootparentprevKremlin also played a role in this. reply midoridensha 8 hours agoparentprev>In raw deaths per megawatt nuclear wins above all else.Just a quibble here: your post is obviously pro-nuclear-power, however this sentence seems to claim that nuclear power causes more deaths than any other power source, which is not what I think you meant. reply arcticbull 7 hours agorootparentI believe the OP is saying that it \"wins\" by having the fewest deaths per TWh, and it does - it&#x27;s the same as solar and wind. [1]By far the worst is coal which [1] has conservatively at 25 deaths per TWh vs nuclear at 0.04.The worst nuclear incident of all time killed 4000 people, and Fukushima killed exactly 1 person. The third-worst, Three Mile Island killed 0. This isn&#x27;t even low sample size. Nuclear has generated 20% of the entire US power demand for decades.Even if you attribute all the deaths from the bungled evacuation of Fukushima to nuclear power (2300), 10X more people would have died in the normal course of operating a coal power plant at the site over the course of the reactor&#x27;s life (1485TWh from 1979 to 2011 @ 25 deaths&#x2F;TWh = 37,000).[1] https:&#x2F;&#x2F;ourworldindata.org&#x2F;safest-sources-of-energy reply mcronce 8 hours agorootparentprevI think that depends on your perspective - specifically, whether you think more deaths or fewer deaths is winning:) reply dv_dt 7 hours agoparentprevI treat public industry estimates of death from nuclear power which the same integrity I treat the fossil fuel industry reports on climate change. They are both intrinsically motivated to systematically deny the ill effects. And if you dig into the deaths you’ll find that basically any plausibly deniable cancer death is discounted. And accounting for disasters like Fukushima will try to discount deaths due to evacuation afterwards to bad policy arguing that somehow we should have let everyone to stay in the area be irradiated.The estimates also suffer from minimization from bad human intuition - as there will be years of future deaths from Chernobyl and Fukushima but because they are both difficult to peg and small at any single point in time they get discounted.In general the strategy is very similar in deniability effectiveness to the chemical industry in cancers and other illnesses from exposures from everything like pesticides to PFAs. reply imperfectcats 6 hours agorootparentDitto - and the same with the claims of solar and wind companies because of - [Goodhart&#x27;s Law - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Goodhart%27s_law - \"When a measure becomes a target, it ceases to be a good measure\"But even if we take that view, how many deaths would nuclear have to have caused to break even with fossil fuels? https:&#x2F;&#x2F;ourworldindata.org&#x2F;grapher&#x2F;death-rates-from-energy-p... puts nuclear at 0.03 and hydropower at 1.3. So it is ~50 times less than hydro. That&#x27;s a lot of wiggle room! reply pseudosavant 9 hours agoprevI don&#x27;t know about others, but I have had a lot of stuff about nuclear power make its way into my YouTube feed in the last few years. Once I learned more about it, it is clear that it is by far the cleanest energy we currently know how to capture.Everything has a footprint: solar power has panels and other components that last 10-30 years, hydroelectric is often terrible for local biodiversity, windmills have huge blades that wear out, etc. Nuclear creates far less waste and spent nuclear fuel can be recycled even.I wish the US had gone all in on nuclear like France. reply rayiner 8 hours agoparentFun fact: if everyone in earth had the same carbon footprint as folks in France, we could bring the entire third world up to first world living standards without increasing the world’s total carbon emissions. reply stephenr 9 hours agoparentprev> spent nuclear fuel can be recycled evenWhere has this shown to be realistic at scale?Every \"pro nuclear\" argument always seems to include some experimental technology that&#x27;s either never been made cost effective or never made it out of the lab.> components that last 10-30 years,It&#x27;s not like nuclear plants last forever. Most seem to run for about 50 years and then need another 50 to decontaminate.The only reason there&#x27;s not an existing industry to mass process old solar cells and windmill parts is that they&#x27;ve only recently started mass adoption - there weren&#x27;t enough being decommissioned for an industry to exist around it yet. reply GeneralMayhem 9 hours agorootparent> Where has this shown to be realistic at scale?France.But also, even if the fuel isn&#x27;t optimally recycled, nuclear power is realistic. \"Put it in a can and bury it under a mountain\" is a sufficient waste disposal strategy for a pretty long time, and certainly much less damaging that the current fossil fuel \"strategy\" of \"blast it all into the atmosphere and let God sort it out\". The US produces about 18% of its energy from nuclear today, and generates about 2000 tons of waste per year. Presumably that means that going all-nuclear would generate about 11,000 tons per year. The Yucca Mountain facility alone had a capacity of 77,000 tons - so about 7 years&#x27; worth of powering the whole country. reply walls 8 hours agorootparent> \"Put it in a can and bury it under a mountain\" is a sufficient waste disposal strategy for a pretty long timeIt&#x27;s simple in theory, not so much in practice: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Yucca_Mountain_nuclear_waste_r... reply arcticbull 7 hours agorootparentThere&#x27;s absolutely no reason you can&#x27;t put it into Yucca mountain other than anti-nuclear &#x27;nimbys&#x27; (in quotes because Yucca is nobody&#x27;s backyard) -- but worst of all, it&#x27;s an asinine point. Yucca Mountain is directly adjacent to the Nevada Test Site where the US has detonated 928 nuclear weapons, most of them underground. [1]The Nevada Test Site is the most radiologically contaminated land on the face of the earth already. Honestly if any spent fuel leaked out it might reduce the average radioactivity at the NTS.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nevada_Test_Site reply asynchronous 6 hours agorootparentExcellent point, also I’d like to point out it buys us quite a bit of time to figure out how to create reactors that eat spent fuel and turn them into nothing, something the DOE has been researching for the last few years. reply lo_zamoyski 8 hours agorootparentprevI wonder if hurling it into the sun could be made cost effective. reply stephenr 8 hours agorootparentThe problem is you need to get it into space first, which means putting it on top of a rocket.Rockets have this nasty habit where sometimes they blow up, and whatever they&#x27;re carrying gets vaporised into the atmosphere. reply ericbarrett 8 hours agorootparentprevIt actually takes much less energy to throw something out of the solar system than into the sun: 17 km&#x2F;s deltaV versus 28 km&#x2F;s. reply midoridensha 8 hours agorootparentprevNo. Kurzgesagt has a good video about this on YouTube. Launching nuclear waste into space is not even remotely cost effective (and launching it to the Sun is even worse). reply preisschild 4 hours agorootparentprevThat would be a waste, because we could reuse most of it. reply smileysteve 7 hours agorootparentprevWhy the sun? It came from the earth; the core can process it back to a future mine. reply AndrewPGameDev 6 hours agorootparentprevJust some napkin math:(Suppose, as it hasn&#x27;t been tested that the) SpaceX Starship can carry up to 100 tons of material to LEO. It costs 100 million USD per launch. (11,000 tons &#x2F; 100 tons) * 100 million USD = 11 billion dollars. The US Department of Energy (DOE) has a budget of 149 billion, so under these pessimistic conditions it would cost around 7% of the budget. That&#x27;s not good, but its not crazy either.Elon Musk has stated that he thinks the cost of launching Starship will go down to 10 million per launch in the next 2-3 years (not sure if thats realistic). On the other hand, we probably don&#x27;t want to unload a bunch of nuclear material in LEO, but much further away from Earth.Just watched the Kurzgesagt video. They make the point that more radioactive material is put into the world by burning coal, which I think is a valid point. They use the Falcon 9 exclusively to calculate the cost of launching, and they never indicate in the video that the cost of putting 1 kg of material into space might go down. Falcon Heavy can already launch (reusable) 63.8 tons to LEO at a cost of 97 million, which is around 1500USD&#x2F;kg instead of the 4000 they cite.They also strangely compare it to the cost of the fissile material, which doesn&#x27;t make any sense as the majority of the cost for a nuclear reactor is in safety precautions, not in the actual production of nuclear fuel.In their sources they&#x27;re a little less forward: Electricity from nuclear reactors is produced for about $70 per MWh, which is 7 cents per kWh. The fuel costs represent just 0.46 cents per kWh or 6.5% of the total cost. If we had to get rid of nuclear waste by putting it on rockets, causing fuel costs to rise 3.4 times to 1.57 cents per kWh, the total cost would be increased to 8.1 cents per kWh. This means the total cost becomes 16% higher.In the video they state \"In 2021, we saw a record 135 launches into space. If we repurposed each of those rockets and filled them all with nuclear waste, the total amount that could be lifted into a Low Earth Orbit, which is the closest orbit above the atmosphere, is nearly 800 tonnes. \"And then they extrapolate this out to... forever, I guess? Either way there were 178 successful launches in 2022. There have already been 123 (successful) launches in 2023. Regardless it seems reasonable to expect this number to go up and the average tonnage per rocket to go up. It just seems strange to make this video that is very explicitly only about 2021 and to have the economics change so much even in 2 years. I would&#x27;ve preferred if they stated in the video that it doesn&#x27;t make sense today, but it could in 5 or 50 years, and made more arguments in favor of storing it deep underground.---The biggest risk (as another commenter mentioned) is the massive risk to the environment by trying to shoot it into space. If we really needed to get rid of it, we could bury it under a mountain for 100 years, dig it back up, and shoot it into space when the cost is much lower, and its much safer to launch.Or just never launch it at all. I don&#x27;t really buy the idea that burying nuclear waste in remote areas is dangerous. Critics say that the problem is that you have to keep waste contained continuously (say for the next 10,000 years) but I&#x27;m near certain we will have some way of truly getting rid of it in the next 500 years. Whether that&#x27;s blasting it into space or reprocessing or something else entirely. reply foolfoolz 9 hours agorootparentprevnuclear has been declining in france for 20 years reply para_parolu 8 hours agorootparentAnd now they have to play putin games because they need gas. reply crote 8 hours agorootparentNot really.Pre-war it only imported 17% from Russia, and like most European countries it has since switched to other suppliers. Turns out bringing LNG regasification plants online can be done in a few months, and plenty of countries are willing and able to supply that. reply ClumsyPilot 8 hours agorootparentYeah now we in Europe pay 3x more for energy that we used to, and the process of liquifying natural gas generates 2-se the emissions that we did when we got gas from pipeline. Shipping LNG around the world is a greenwashing disaster, it&#x27;s like when Europe burns biomass from trees chopped in the amazon and calls it &#x27;sustainable&#x27; reply crote 7 hours agorootparentElectricity peaked in August 2022, and has returned pretty much back to pre-war levels. You might be paying 3x more, but your electricity provider definitely is not.Besides, the claim I responded to was that the decline of nuclear in France led to a dependency on Russia for gas. The fact that Europe rapidly switched to LNG shows that this is not true, regardless of your opinion about anything else. Nobody is pretending that LNG is green, and biomass is heavily criticized for exactly that reason. reply evilos 4 hours agorootparentprevThis was literally legislated by green party politicians. They passed a law mandating that the nuclear fleet must go from 75% to 50% of generation. They also forced the company that owns the fleet (EDF) to sell power at a loss to energy traders and placed a tax on all power bills that went directly to funding their renewable portfolio.Thankfully, they have changed course since the war and are now planning a new nuclear buildout as well as have removed the 50% cap. https:&#x2F;&#x2F;www.reuters.com&#x2F;world&#x2F;europe&#x2F;french-parliament-votes... reply stephenr 8 hours agorootparentprevIf your argument (or the argument you&#x27;re standing by) says \"the spent fuel can be recycled\", but then you say \"well even if we can&#x27;t recycle it we can just bury it\" (which by the by, is what happens a reasonable chunk of French nuclear waste), it makes your entire argument (or the argument you&#x27;re standing by) sound... how do I put this.... juvenile.We know it can be buried. That’s what we&#x27;re arguing against.What kind of mentality does it take to think that \"aha! We can just bury it\" is a gotcha response when someone says \"but what about all the nuclear waste\".That&#x27;s like saying \"what should we do about plastic waste?\" \"Aha! We can bury it\".> so about 7 years&#x27; worth of powering the whole countryWow, what a long term solution you hate there.> less damaging that the current fossil fuel \"strategy\"You realise that literally no one arguing against nuclear power, thinks fossil fuels are the alternative right?What is it with nuclear proponents and ridiculous straw man arguments. reply kuchenbecker 8 hours agorootparentThe point is this is tractable. Your argument is Sophist deconstructing the previous argument without contributing to the discussion by nitpicking points. reply credit_guy 8 hours agorootparentprev> Where has this shown to be realistic at scale?From a pro-nuke guy: no, it has not been shown. The vast majority of the nuclear fuel is low enriched Uranium, meaning U-238 makes 95% of it or more. A little is transmuted to Plutonium during operation, but more than 90% of the spent fuel is still U-238. We don&#x27;t currently have the capability to burn that. We may have it one day, but that day is many decades in the future.What nuclear recycling does is to extract unburned U-235 and plutonium from spent fuel. That can only be done once, after that the plutonium is too rich in Pu-240, and only the Pu-239 isotope is optimal for fission. So, those who say we can keep recycling spent fuel, either don&#x27;t know the whole story, or choose not to tell it.But many companies are working on developing fast reactors. These reactors can burn U-238. In principle they can burn it all. In reality, I think in the short term they&#x27;ll burn just some part of it. For example, current nuclear plants can generate about 40 to 60 GW-day per ton of fuel. Various new plant proposals can go to 180 GWd&#x2F;ton [1]; this is impressive, but it&#x27;s not the factor of 20 you&#x27;d expect from fully burning all the uranium in the nuclear fuel. In the link I provided you&#x27;ll see two reactor types with higher burnups; they are both envisioned to burn thorium. Fingers crossed, but thorium is much more unproven than uranium.In any case, even when burning spent fuel will be possible, it will initially be economically non-profitable. And it will be so for decades.But not for centuries. That&#x27;s important. Naysayers will try to convince you that spent fuel will be a problem for ten thousand years. But that is only assuming that we freeze all progress to where we are today. Why such an assumption?If you were to place a bet where you miraculously wake up in the future 200 years from now, would you wager that we won&#x27;t be able to burn the 20&#x27;th and 21&#x27;st century spent nuclear fuel, or that we will be able to?[1] https:&#x2F;&#x2F;aris.iaea.org&#x2F;sites&#x2F;burnup.html reply SoftTalker 7 hours agorootparentI also think that fast reactors are the answer, and we just have to get over the \"proliferation\" worry. I mean with all the sanctions the world has imposed we have been unable to prevent Iran, North Korea, other countries from conducting entrichment operations and building weapons. The cat is out of the bag. reply dotancohen 7 hours agorootparentprevWhy isn&#x27;t the spent nuclear fuel used to conductively pre-heat the water going to the turbine? Then less energy would be required to convert it to steam, making the whole plant more efficient. reply SoftTalker 7 hours agorootparentThe steam water is a closed loop. It&#x27;s boiled, passes through the turbine, condensed back to water, and boiled again. It&#x27;s already \"hot\" and doesn&#x27;t need pre-heating. reply jauntywundrkind 7 hours agorootparentprevI&#x27;m still so sad that John Kerry had the Integral Fast Reactor funding killed. On-site electro-processing avoided all the scary things about reprocessing being enrichment (and often being off-site). Very fail-safe design. We&#x27;d already done prototyping with Experimental Breeder Reactor II. Huge loss to not have the government doing this pioneering. They more than anyone have to be leading the charge; the commercial space is aligned to such a more limited perspective. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Integral_fast_reactorA variety of derivative designs have kicked around & a bunch seemed like they were going to get built. Recently GE Hitachi & one of their derived PRISM designs & TerraPower travelling wave folks agreed to try something in Wyoming, but there&#x27;ve been so many similar such plans that&#x27;ve fallen through that I&#x27;m not holding my breath. reply aoeusnth1 9 hours agorootparentprevThe strongest pro nuclear argument is just to look at France’s vs Germany’s carbon intensity. This does not require experimental technology. reply Qwertious 7 hours agorootparentThe problem with that comparison is that it&#x27;s a political one, not an economic one - France&#x27;s nuclear power has been largely publicly owned, whereas Germany&#x27;s has been largely private, and what&#x27;s more Merkel&#x27;s energy policy 20 years ago took the Greens plan of \"shut down nuclear and replace it all with renewables\" and took out half of the renewables forcing the country to backfill with coal&#x2F;gas. So the carbon intensity was a failure of politics.I think it&#x27;s kind of ironic, really - there are three arguments against nuclear (safety, cost, and political feasibility), and political feasibility is by far the biggest problem - safety is overblown (in no small part because right now we&#x27;re using coal&#x2F;gas which is toxic AF), cost is a genuine issue but frankly irrelevant if nuclear will genuinely speed up decarbonization (climate change is hella expensive), so the big problem with nuclear is that it&#x27;s so easy to scuttle at any point with fearmongering, so if it delays renewables it could be literally worse than useless due to politics. reply BurningFrog 8 hours agorootparentprevThe simple technology for storing nuclear fuel is a box.The volume of waste is very small and is easily stored as is. reply II2II 7 hours agorootparentStatements like this make me more nervous about the advocates of nuclear energy than of the mass adoption of nuclear energy itself.In the simplest of terms: radioactive material emits energetic particles and photons. In itself, this is not much of a problem for naturally occurring radioactive materials since it is usually in low concentrations and captured by the crust of the Earth. The problem with radioactive materials used in reactors is that it has to be concentrated, otherwise the reaction rates would be too low to be useful.Disposing of that nuclear waste isn&#x27;t as simple as sticking it in a small box then burying the box in a hole. At the very least, you&#x27;re going to want that hole to be geologically stable (to ensure nature doesn&#x27;t interfere with how it is stored) and physically secure (to ensure humanity doesn&#x27;t interfere with how it is stored). You&#x27;re going to want to consider what is absorbing those emitted particles and photons, since you want to minimize how much is being absorbed by the nuclear waste stored alongside it.So I hope you don&#x27;t mind if I dismiss your claims that it is easily stored and put more faith in engineers and scientists who specialize in nuclear waste disposal. Sure, their systems may be over engineered and more expensive than they need to be. On the other hand, they have put a lot more thought into it. reply Sabinus 5 hours agorootparentFor an description of nuclear waste storage from someone in the industry I recommend this video:https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=KnxksKmJa6U reply stephenr 8 hours agorootparentprevI guess that&#x27;s why the US still stores a bunch of used fuel at reactors in concrete and steel vessels, with zero long term storage plans... because it&#x27;s \"easy\". reply zdragnar 7 hours agorootparentWe have had many long term storage options evaluated over the years, none more popularly known than Yucca mountain.Every time the political parties exchange power, it either gets renewed or cancelled. According to the GAO, it is not for technical or other reasons, just purely political ones. reply midoridensha 4 hours agorootparentprevIt&#x27;s a lot easier than pumping pollution into the atmosphere where we all get to breathe it and it causes climate problems. But I guess you think that&#x27;s a better solution. reply stephenr 1 hour agorootparentI&#x27;m just going to literally copy and paste a response I wrote 6 hours ago:> You realise that literally no one arguing against nuclear power, thinks fossil fuels are the alternative right?> What is it with nuclear proponents and ridiculous straw man arguments. reply kalleboo 5 hours agorootparentprev> Every \"pro nuclear\" argument always seems to include some experimental technology that&#x27;s either never been made cost effective or never made it out of the labTo be fair, the same is true of pro-renewable arguments. Storage is always hand waved away with pumped hydro (not enough suitable reservoirs), \"molten salt!\", some wild globe-spanning HVDC network or an absolutely unfathomable amount of chemical batteries.Energy is a really difficult problem and we have no obvious solutions. reply stephenr 1 hour agorootparent> To be fair, the same is true of pro-renewable arguments.Do some people claim the solution is non-existent (at scale) solutions? Sure. Does it come up anywhere near as often as \"we just need \"? Not in my experience.> Storage is always hand waved away with pumped hydro (not enough suitable reservoirs)Keep in mind that \"how do we power the world\" isn&#x27;t a US-centric problem. ~35% of Switzerland&#x27;s electricity is already generated from pumped hydro.> Energy is a really difficult problem and we have no obvious solutions.It&#x27;s so difficult that South Australia (a state with a relatively small economy - about on par with South Dakota, with double the population) went from 1% to more than 70% electricity generated from renewables (wind and solar), in the space of 15 years.> an absolutely unfathomable amount of chemical batteriesI wanted to come back to this after talking about South Australia. SA hit 70% renewables, with less than 1% accounted for by battery storage.Renewable energy has increased exponentially in technological advances and installed capacity in the last few decades, and while some people may be pinning their hopes and dreams on as-yet-unproven \"renewable\" pipe dreams like world-wide HVDC networks, there are real-world technologies out there doing what opponents have claimed for years, is impractical (it was claimed for a long time that renewables could at best provide 20% of South Australia&#x27;s power needs). reply rtpg 5 hours agorootparentprevJapan does fuel reprocessing for its nuclear reactors, my understanding is that this allows them to just not need to import any more uranium. I believe there is still some waste product at the end of all of this, but it is a very manageable amount of it? That&#x27;s my understanding at least. reply stephenr 52 minutes agorootparentNot because they want to. Japan is what you or I might call \"seismically active\", so picking a place that&#x27;s going to be secure for 100K years is even harder than other places.They determined in the 90&#x27;s that MOX reprocessing costs 4x what it would cost to bury the stuff.. by 2017 the price had quadrupled again from the &#x27;94 cost. reply car 8 hours agoparentprevLet’s consider, that France is importing power from Germany, since at times more then half of it’s nuclear power plants are off-line. Many for months on end, due to their dated and complex technology and lately corrosion problems. Some during the dry summers months, since not enough water is available for cooling.Edit: Added english article. Thanks for the downvotes.[0]https:&#x2F;&#x2F;www.grs.de&#x2F;en&#x2F;news&#x2F;situation-nuclear-power-plants-fr...In German: [1]https:&#x2F;&#x2F;www.iwr.de&#x2F;news&#x2F;stromausfall-edf-abgeschaltete-atomk... reply preisschild 4 hours agorootparentFrance is already an exporter again, and was an exporter for 50 years. They had to import electricity for one year.And, of course, they investigated why this happened, and it was because of anti-nuclear politics.A Whole power plant with multiple reactors was closed to appease Germany, for example.https:&#x2F;&#x2F;www.euractiv.com&#x2F;section&#x2F;energy&#x2F;interview&#x2F;french-mp-...And enough water IS available for cooling. The output was only reduced in some power plants because the temperature difference between river inlet and river outlet was too big, which could cause issues to animals in the water. This was less than 0.2% of the total electricity produced by nuclear in 2022. There is an easy fix though: cooling towers. reply lostlogin 8 hours agorootparentprevCould someone who disagrees with the above comment explain what’s wrong with it?Edit: it was downvoted but seems to have come back up. reply epistasis 7 hours agorootparentI didn&#x27;t disagree with the comment, but I fear that pro-nuclear advocates have become almost as irrational as the far more irrational anti-nuclear advocates that they fight against all the time. Wrestling with pigs makes you muddy too, after all.Disclaimer: I wouldn&#x27;t consider myself anti-nuclear according to any of the standard anti-nuke arguments, and was extremely pro-nuclear power in the US in the 2000s. However the experiences of advanced economies with very high productivity and labor costs (France, US, Finland) have completely soured me on the possibility of nuclear in the future, as the tech doesn&#x27;t work that well and is very expensive. Even the country very best at construction, China, is only planning another 50GW or so, which is a tiny tiny fraction of any other new energy source that they are investing in. And if the best in the world at large construction projects is only putting in a token effort, I don&#x27;t know why we in the US, who suck at large construction, should attempt more boondoggles. And in researching the history of nuclear, I find that it has always been this way, not so great. I think it only gets ardent supporters these days because of its prominence in SF. reply Krssst 5 hours agorootparentprev> since at times more then half of it’s nuclear power plants are off-lineHappened a few times only in 2022 because of covid + an issue affecting many plants being discovered as far as I know. Now being fixed. Otherwise worked pretty well for decades.NREs being extremely often at (Montel) French utility EDF is unable to self-finance the construction of new nuclear reactors due to its EUR 65bn debt and so needs state funding, CEO Luc Remont told a hearing of France’s lower house on Wednesday.https:&#x2F;&#x2F;www.montelnews.com&#x2F;news&#x2F;1511372&#x2F;french-state-must-fu...Meanwhile Germany and the US is making billions by auctioning the right to build off-shore wind.> Germany’s first dynamic bidding process, covering four offshore wind zones with a combined capacity of 7 GW, has generated EUR 12.6 billion in proceeds, according to the Federal Network Agency.https:&#x2F;&#x2F;www.offshorewind.biz&#x2F;2023&#x2F;07&#x2F;12&#x2F;breaking-germany-rak...> U.S. Offshore Wind Power Auction Nets Record $4.37 Billionhttps:&#x2F;&#x2F;www.wsj.com&#x2F;articles&#x2F;u-s-offshore-wind-power-auction...I have a hard time seeing France as a success story to emulate today. Choosing nuclear was the right choose to gain energy independence in the 70s, today the equivalent choice are renewables. rtpg 5 hours agorootparentprevFrance is a nuclear success story, but they&#x27;re also a good indicator that even when you&#x27;re all bought in, there are logistics difficulties that come from your power generation being centralized to certain locations.France has like 80% nuclear power, and most of the nuclear plants are actually underutilized! Why would France not use the plants at 100%? Because electricity is kind of fungible, except at massive scales where it totally isn&#x27;t, and you can&#x27;t \"just\" have power generated in the south be used in the north.Nuclear is more expensive than gas, but once again it is worth pointing out that it is more expensive than solar and wind! Does this mean that we can replace all of the reactors with solar or wind? Obviously not. But if you&#x27;re sitting there trying to produce energy in places that are not as close to the existing reactors, building energy production facilities that don&#x27;t take decades to build (for \"building big things takes time\" reasons) and can work without a constant inflow of water (which is a practical and real concern for these power plants). reply preisschild 4 hours agorootparent> France has like 80% nuclear power, and most of the nuclear plants are actually underutilized! Why would France not use the plants at 100%?Because more electricity is needed in during certain times, like during the day or when it&#x27;s winter. reply rtpg 4 hours agorootparentBut that&#x27;s the point, really. Nuclear reactors are not economical if you&#x27;re not really going for it, so in an environment where you are relying heavily on nuclear reactors, suddenly your entire endeavor is not very practical without massive state aid.This is less of a problem in other places that have less nuclear power setup because they can be used to cover the baseline. Now, EDF is turning off reactors over the weekend cuz the power isn&#x27;t needed, and in periods of high demand EDF doesn&#x27;t actually have the capacity, because they don&#x27;t have much peak generation!Building more nuclear plants to cover peaks would make everything even more expensive, And decomissioning reactors entirely would cause their own massive issues because, again, electricity generation is not fungible.One would (correctly!) point that most renewables don&#x27;t help with peak generation either, and you would be right. But if you&#x27;re planning on decomissioning a nuclear power plant, replacing some of that capacity with renewables at least gets your operating costs down. And energy storage solutions that are more fungible... well... you walk and chew bubble gum. Use what you have, and build what makes sense. Just in 2023 your choices for clean power generation for energy storage are vast. reply car 3 hours agorootparentprevRidiculous? Well, pardon me. This wasn&#x27;t meant to be an argument against nuclear power, hopefully obvious when read calmly, but rather a statement of facts. I didn&#x27;t want the nuke cheerleading go on without pointing out some inconvenient details.It&#x27;s a complex topic, and I appreciate the more detailed responses about the french situation.Edit: Let me add the fact, that France seems to have trouble training enough personnel for these highly complex plants, rightfully so to protect the environment from the dangers of radiation. Considering the demographics of Europe in general, that is a troubling development for maintenance of these power plants decades into the future. reply tinco 2 hours agorootparent> Ridiculous? Well, pardon me.You are excused.In my opinion even though indeed you present facts neutrally, your choice of facts seemed intended to make it seem as though nuclear reactors being turned off is somehow something generally a problem with nuclear reactors.In the same line of reasoning as before, if France and the rest of the world had continued their investment in a thriving nuclear industry then getting new talent might not have been such a pressing issue. reply ponorin 4 hours agorootparentprevPro-nuclear side seem to argue for science rather than feelings, and proceeds to downvote non-corroborating evidences anyway. Horseshoe theory? reply rtpg 8 hours agoparentprevI don&#x27;t really feel the need to shut down nuclear stuff, but my impression from the past couple of years of these discussions is that nuclear is not only super costly, but logistically very hard (ignoring regulatory environment issues! Just logistics), and is just super hard to pull off. Meanwhile lots of other things have shown up, are very cheap, don&#x27;t necessarily require huge water inputs, etc.France is the nuclear success story, of course! But the irony is that these power generators can only make money when used at 80-90% capacity. France&#x27;s reactors don&#x27;t get that high, and so they&#x27;re quite a money pit. There&#x27;s the argument that \"losing money\" generating power is fine (we don&#x27;t expect to make money on libraries or streets), but it&#x27;s important when people argue about regulation being the problem. The most open and willing ecosystem for nuclear power is one that exists due to massive state subsidies.Yeah lots of renewables aren&#x27;t 100%, but power is power is power, and it feels like doing a bunch of easy stuff that also happens to make money for private actors (sidestepping some of the trickiness of larger gov&#x27;t-funded projects) is a thing that our modern society is well oriented towards.EDIT: to be clear, if there was a huge pro-nuclear movement that showed up tomorrow that could get things done, I&#x27;d be all for it. I just like the idea of doing easy things that are accepted by wide swaths of the population rather than just sitting around lamenting that people are \"being irrational\" or w&#x2F;e reply hn_throwaway_99 9 hours agoparentprevI don&#x27;t disagree with what you&#x27;ve said, but in many ways I think it&#x27;s \"too late\" for nuclear. There was a good article I read recently that made the argument that the gargantuan cost of nuclear, coupled with long regulatory lead times and lots of local opposition make it much too slow to deploy, when we need drastic changes ASAP.Deployment of solar and wind is just much faster and ends up being much more cost competitive. \"Overbuilding\" renewables, coupled with multiple storage technologies, is a better option than nuclear at this point. reply bastawhiz 9 hours agorootparent> Deployment of solar and wind is just much faster and ends up being much more cost competitive.There&#x27;s literally nothing stopping us from building _both_ solar+wind and nuclear. There&#x27;s no shortage of construction workers or materials. And it avoids the need to build large, expensive, and relatively short-lived (and resource-intensive) battery installations that are needed to displace fossil fuel-based energy generation. Solar and wind have an 80&#x2F;20 problem, and nuclear easily slots in as a great solution for that 20%. reply epistasis 7 hours agorootparentWe have finite resources, and that is the main argument against nuclear. If cost was no object, if we could pay lackadaisical EPC to waste billions and throw timelines off track, if we could bankrupt more design firms like Westinghouse due to complete incompetence in delivery of design, sure, why not just fritter our finite labor pool and resources away.But if want to be effective as possible at fighting climate change, we need to Marshall our resources as if we were at war, and put all the effort into the most effective routes. reply crote 8 hours agorootparentprev> There&#x27;s no shortage of construction workers or materials.In many countries there is a shortage, actually. Especially for construction workers.And a big limiting factor is the lack of money: money isn&#x27;t unlimited and you can only spend each dollar once. You have to choose, so better spend it on the option with the biggest impact. reply XorNot 8 hours agorootparentprevWhile true, there&#x27;s a question of why would you bother if you can build nuclear? 60-70% of my states&#x27; electricity use is 24&#x2F;7 constant. Outside of that number, the downward slope between maxium and minimums is very long, and very slow and mostly at night. Nuclear plants are easily variable in fuel burn over these sorts of rates.The problem with solar and wind is that they can trip offline from the grid very quickly, so the entire question is whether you can plausibly backstop the grid for long enough without the need to spin up an extra powerplant.With a frontloaded capex asset like nuclear, once you are building it there&#x27;s a big question as to how much value you get from not using it - i.e. the main benefit of solar and wind is if with storage they stop you needing to build a new nuclear plant.I would contend that the storage answer is very inadequate: it&#x27;s not nearly good enough to have a grid which is only running with 1 day worth of banked power when you could have much longer periods of underproduction (which is the way most solar&#x2F;wind storage solutions are presented). To safely have a distributed grid, you need months worth of stored power to average out the fluctuations. Storage just isn&#x27;t cheap enough at that scale compared to nuclear reactors. reply asynchronous 6 hours agorootparentprevChina builds almost one nuclear reactor a year.This is because we over-regulate and have let all the nuclear talent die, while they have done the opposite.It’s possible we just in the West don’t want to imagine it is. reply philwelch 4 hours agorootparentWe build more than that, actually, it’s just that they’re inside of submarines and aircraft carriers. reply dataflow 9 hours agorootparentprevThis doesn&#x27;t make sense. I suppose it depends what your goals are? It might be \"too late\" if your goal is to stay below N degrees, but that doesn&#x27;t mean you should give up on it. It could still be the way to prevent N + 1 degrees. reply bende511 5 hours agorootparentprevThe best time to plant a tree is 20 years. The second best time is now.So to with nuclear power and other long term projects. Just because we needed these things built yesterday should not stop us from building them now. reply philwelch 8 hours agorootparentprev> There was a good article I read recently that made the argument that the gargantuan cost of nuclear, coupled with long regulatory lead times and lots of local opposition make it much too slow to deploy, when we need drastic changes ASAP.Fixing the regulations and committing to a large scale construction of at least 1,000 nuclear power plants in the United States could solve these problems pretty easily. reply epistasis 6 hours agorootparentNobody has a single proposal on what regulation to fix, or even the more basic problem of what regulations are unnecessary.However, if you look into detail at what happened at Vogtle, you find absolutely incompetent management that thought they could just ignore all the basic quality requirements and get away with it. Leading to massive delays, etc. etc. etc.Regulations are a red herring, IMHO. Idiotic construction companies are however a huge problem, as are absolutely idiotic design companies like Westinghouse, which many times delivered \"unconstructable\" plans!Ironically if there was more regulations, and plans were required to be checked by regulators both for safety as well as constructability, perhaps these projects could have been completed closer to on time and on budget.But that&#x27;s just because of all the people working on these things, only the regulators are doing their job as if they were accountable entities, and the utility, the EPC, and the designer are all looking simply to pass the buck and hope that in the eventual lawsuit they can take minimal damages. reply philwelch 4 hours agorootparentOn average the US has commissioned more than one nuclear powered naval submarine or aircraft carrier every year. There’s no fundamental reason we can’t also cost-effectively build on land.Brian Potter’s “Construction Physics” substack has a series of articles about the cost issues with nuclear power:https:&#x2F;&#x2F;www.construction-physics.com&#x2F;p&#x2F;why-are-nuclear-power...https:&#x2F;&#x2F;www.construction-physics.com&#x2F;p&#x2F;why-are-nuclear-power...https:&#x2F;&#x2F;www.construction-physics.com&#x2F;p&#x2F;why-are-nuclear-power...All in all, the issues with nuclear construction are largely down to regulation, and can be overcome through massive economies of scale. Building 1000 nuclear power plants within the next ten years is achievable if the United States, to use your words, “[marshalls] our resources as if we were at war”. The primary reason this hasn’t happened already are (a) activists like you spent the last half century trying to prevent it, eroding the practical expertise necessary and (b) natural gas is still probably more cost effective. reply mc32 8 hours agoparentprevYeah… there was one movie star who was heavily influenced by a movie she was in and became an activist against nuclear despite not having any background in nuclear power. Yet, ironically, many environmental movements and organizations hopped on her[1] bandwagon. Most have realized the error in judgement. Better late than never.She raised the anti-nuke monster that is only now being neutralized.That’s forty years lost right there.[1] Jane Fonda. reply bryanlarsen 9 hours agoparentprevIt&#x27;ll cost several trillion dollars more to switch the US to nuclear electricity than it would to switch to renewable electricity. Those trillions of dollars could clean up a lot of waste if that is your primary concern. reply whimsicalism 9 hours agorootparentWill we be able to replace current loads with renewable power?All I know is that France has effectively been GHG neutral for decades because they invested in this. I know renewables have gotten way cheaper, but if GHG neutral is so achievable with them why haven&#x27;t we just done it? reply bryanlarsen 9 hours agorootparentWe are doing it. We&#x27;re doing it at a rate of about 5% per year, and that rate is increasing every year. Given how big our grid is, 5% per year is pretty massive. reply coding123 7 hours agorootparentNuclear is at 20% of total US grid power. So 4 years ago it was 0%?This graph is showing nuclear as being pretty flat GW production last 10 years.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nuclear_power_in_the_United_St... reply Qwertious 7 hours agorootparentprev>but if GHG neutral is so achievable with them why haven&#x27;t we just done it?Because France&#x27;s energy system is&#x2F;was largely publicly run, and I expect the US coal lobby would have objected pretty strongly to direct competition against coal plants being paid for and subsidized by government money. reply seadan83 9 hours agorootparentprevFrance gets a large portion of electricity from Nuclear, though the data I could find does not say France is GHG neutral.The second largest source looks to be oil at around 35% [1]This source, [2], goes on to state: \"In 2022, France&#x27;s power sector carbon emissions nearly reached 40 million tons of carbon dioxide equivalent (MtCO2e), down from a century-peak of over 50 million tons recorded in 2005\"[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Energy_in_France#&#x2F;media&#x2F;File:E...[2] https:&#x2F;&#x2F;www.statista.com&#x2F;statistics&#x2F;1290541&#x2F;power-sector-car... reply whimsicalism 9 hours agorootparentEnergy != electricityThat said, what I probably should have said is the capability to go GHG neutral. I believe that France has more domestic nuclear capacity than its domestic electrical conception. reply ClumsyPilot 8 hours agorootparentprev> why haven&#x27;t we just done itbecause fossil fuel companies engage in grand fraud, for example they create astroturfing groups like &#x27;citizens for responsible solar&#x27; which go around the country and file objection to every solar project that needs planning permission.They got on-shore wind turbines banned in Britain. They got construction of solar panels banned on all land except low-grade non-agricultural land, so like steep hills of Scotland where it is impractical. The underground power cable to France was blocked because it would &#x27;ruin the landscape&#x27;. reply rayiner 8 hours agorootparentprevExcept we could have avoided enormous amounts of emissions if every first world country had followed France’s lead back in the 1970s. By the time renewables technology advances sufficiently (grid-scale storage is still a necessary but unsolved piece of the puzzle) we will have wasted a 50+ year hard start we could’ve had with nuclear. reply crote 8 hours agorootparentUnfortunately most countries didn&#x27;t follow France, and unless you have a time machine that is going to be quite difficult to change.When it comes to climate change, the most important goal is to reduce emissions as quickly as possible. This means switching from 100 units gas to 90 units renewable 10 units gas in 5 years is way better than switching to 100 units nuclear in 15 years. If you figure out a solution to the remaining 10 units of gas somewhere within 100 years, you end up ahead! reply hackerlight 2 hours agorootparent> When it comes to climate change, the most important goal is to reduce emissions as quickly as possible. This means switching from 100 units gas to 90 units renewable 10 units gas in 5 years is way better than switching to 100 units nuclear in 15 years. If you figure out a solution to the remaining 10 units of gas somewhere within 100 years, you end up ahead!This is an important refutation of \"we need grid-scale storage before we can build out renewables\". The total area under the curve is all that matters. 90% renewables + 10% gas can mean less overall emissions than 100% nuclear if it occurs sufficiently faster. reply bryanlarsen 6 hours agorootparentprevYes, in 1980 or 2000 or 2010 or even maybe 2015 it would have been a fabulous idea to start a whole bunch more nuclear plants. But in 2023 we have better options. reply evilos 4 hours agorootparentWe&#x27;ve been saying this for decades. It&#x27;s still not true because we haven&#x27;t figured out massively scalable grid scale energy storage. The only thing that gets close is PHES and we don&#x27;t have enough of it. reply hackerlight 3 hours agorootparentprev> Except we could have avoided enormous amounts of emissions if every first world country had followed France’s lead back in the 1970s.We should have, but we didn&#x27;t, and we can&#x27;t change the past, so it&#x27;s irrelevant.Not only can&#x27;t we change the past, but it&#x27;s an apples to oranges comparison to compare what we ought to have done in the past to what we ought to do today, given how drastically renewables have decreased in cost between the 1970s and the year 2023.> By the time renewables technology advances sufficiently (grid-scale storage is still a necessary but unsolved piece of the puzzle) we will have wasted a 50+ year hard start we could’ve had with nuclear.There is no need to wait for grid-scale storage. That piece of the puzzle can be added later to replace last mile gas peaker plants.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37219148 reply edgyquant 9 hours agorootparentprevCan you provide a citation explaining why this a definite fact? Sounds very untrue considering we still have problems to solve to go 100% renewable which requires R&D reply XorNot 9 hours agorootparentprevDo you have any research which supports those numbers? Because I haven&#x27;t seen a single analysis of renewable energy which manages to come up with a plausible figure for solving the energy storage problem with realistic assumptions about overbuild (i.e. a pumped hydro storage system which only holds 1 day of energy isn&#x27;t robust if you have a grid which doesn&#x27;t have fossil fuel fallbacks). reply crote 8 hours agorootparentThe overnight cost of building nuclear is about $7000 &#x2F; kW. Wind and solar are closer to $1500 &#x2F; kW before taking storage into account. That&#x27;s plenty of margin for a wide variety of storage solutions - including simply building a huge overcapacity and letting it idle, or doing carbon capture on a peaker gas plant. reply XorNot 8 hours agorootparentBut there isn&#x27;t, is the thing. Nuclear might cost $7000&#x2F;kW but it has a capacity factor of 80%[1]. Solar has a capacity factor of 25%. So while nuclear is closer to $8750&#x2F;kW after capacity factor, solar is $6000&#x2F;kW after capacity factor...before storage.And storage doesn&#x27;t have a simple analysis: you can only charge pumped hydro at a certain rate, so on any given day if you don&#x27;t have excess solar, you might not fully recharge at all (and that&#x27;s before the problems of storing exactly 1 day&#x27;s worth of excess energy when you don&#x27;t have a large, dispatchable fossil fuel grid to fall back on).And how much is pumped hydro per kW, naively? Varies - an optimistic assessment is $1500 per kW, potentially as high as $5,505 per kW (in capex)[2].[1] https:&#x2F;&#x2F;www.pnas.org&#x2F;doi&#x2F;10.1073&#x2F;pnas.2205429119[2] https:&#x2F;&#x2F;www.hydroreview.com&#x2F;business-finance&#x2F;business&#x2F;nrel-i... reply epistasis 6 hours agorootparentCapacity factor is an energy 101 concept, and evaluating energy based on overnight capital costs plus capacity factor misses out on the an exceptional number of crucial factors: lifetime of capital, O&M, cost of capital, and these days, dispatchability and time of delivery.The levelized cost of energy takes most of these into account (except dispatch), and shows that nuclear is >4x as expensive, when it gets built and not abandoned halfway through construction. This risk is significant for large projects, and why in 2008, utilities used lobbyists in state legislatures in Georgia and South Carolina to stick customers with the bill for failed construction projects, without which neither Vogtle nor VC Summer would have started. No sane investor would risk billions on new nuclear construction without placing the risk on others.Real life deployments in free market systems like Texas&#x27; ERCOT shows that investors who put their own money on the line to generate electricity cheaply contrast heavily with regulated utilities which profit by rate-basing and therefor want electricity as expensive as possible. And when investors are putting their own money on the line to generate cheap electricity, they don&#x27;t want to build nuclear or coal or hydro, they are massively investing in wind, solar, and battery storage. reply XorNot 6 hours agorootparent> Capacity factor is an energy 101 concept, and evaluating energy based on overnight capital costs plus capacity factor misses out on the an exceptional number of crucial factors: lifetime of capital, O&M, cost of capital, and these days, dispatchability and time of delivery.Oh, so like maybe throwing a quote about the cost of solar around without considering storage, capacity factor etc. you know - the $&#x2F;kW numbers being cited above unsourced...might not be an honest way to talk about costs?> dispatchability and time of delivery.Seriously? You&#x27;re going to argue that these aren&#x27;t being fairly considered when I express skepticism that solar is \"cheaper\"? reply kalleboo 5 hours agorootparentprevSolar&#x2F;wind is cheap as long as you still have something else to back it up when it fails. Battery is cheap and profitable as long as you&#x27;re only building a couple minutes of capacity to play grid stabilizer&#x2F;arbitrage games.It&#x27;s similar to the problem in privatized health insurance. It&#x27;s very cheap and profitable to insure young healthy people. Especially when you can dump the responsibility of all the old and sick people on the government.Of course investors invest in these things - they give quick, high returns on investment. That doesn&#x27;t mean they&#x27;re good for long-term stability. reply bryanlarsen 9 hours agorootparentprevhttps:&#x2F;&#x2F;mitpress.mit.edu&#x2F;9780262545044&#x2F;electrify&#x2F; reply scrum-treats 6 hours agoparentprevFrance going nuclear [1]: \"France’s championing of nuclear power as a way of ensuring its energy sovereignty sounds great. But a group of researchers says it&#x27;s a red herring given France imports all its uranium.\"The push for nuclear in the US is a strategic maneuver to increase our position in Africa. It is not about increasing \"clean\" energy production. What else can you make with uranium and plutonium?? Hmm...Further, France is slowing nuclear power generation due to heat waves [2]. Further evidence that the nuclear path is not a scalable, viable solution. Meaning, it&#x27;s a bad idea to invest nuclear energy, let alone for the purpose of transitioning off fossil fuels.[1]https:&#x2F;&#x2F;www.rfi.fr&#x2F;en&#x2F;france&#x2F;20220223-does-nuclear-power-gua....[2]https:&#x2F;&#x2F;efe.com&#x2F;en&#x2F;economia&#x2F;2023-08-17&#x2F;france-to-reduce-nucl... reply botanical 13 minutes agoprevNuclear costs too much, but if small modular reactors can be built quickly and cheaply like what South Africa was trying with the Pebble bed modular reactor¹, (now those people are at X-energy² and Stratek Global), then it would be beneficial in the long run to have consistent output without the need for batteries and energy storage.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pebble_bed_modular_reactor[2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;X-energy reply psunavy03 8 hours agoprevIf you don&#x27;t take nuclear power seriously, you don&#x27;t take climate change seriously. Period, dot, full stop. We should have gone all-in on nuclear in the 1970s and 1980s but for a few screeching NIMBY so-called \"environmentalists.\"Yes, if you screw up badly enough, you can have a disaster. If I drove drunk and ran my Jeep down a crowded city sidewalk, you would also have a disaster, but I would never be that negligent, callous, and stupid. The US Navy has run reactors for 65 years with a flawless safety record . . . it can be done. reply SoftTalker 7 hours agoparentIt is deliciously ironic that the people most against nuclear power in the 1970s and 1980s are the reason we have a \"climate crisis\" today. reply magicalist 4 hours agorootparentAh yes, some of the least effective propagandists (seriously, they fought coal for generations and the thing that actually broke its stranglehold was...fracking) were the reason, not the now well documented corporate giants injecting FUD about climate change into all media for decades.This narrative seems to be popping up a bit recently, but mostly from folks who find it \"deliciously ironic\" (gross) or put \"climate crisis\" in quotes. reply XorNot 7 hours agoparentprevUpvoting this because this is where I end up. Nobody proposing solar, wind and storage ever wants to back it up with hard numbers. They back it up with soft numbers - lies of omission.\"Look how expensive nuclear is compared to solar!\" they say, ignoring capacity factors, reliability or storage. \"Look how many gigawatts of batteries were installed last year\" while ignoring that the energy-storage of those batteries is maybe 3 hours and the total power of batteries scales alongside energy.\"We&#x27;ll just overbuild\" they say, ignoring that the 4x the solar is now within a stone&#x27;s throw of being the same capex as equivalent nuclear, but still needs storage.Everyone has some just-so narrative which omits numbers. Because you can make green hydrogen they assume it must be cost-effective to do so. Because someone said \"solar is cheap\" they assume it must be too cheap to meter. As though you can wave away multiple 50% energy loss steps as insignificant.I&#x27;m open to being convinced I&#x27;m wrong, but show me the numbers which prove it. Show me that you&#x27;re talking about a today technology, not \"by 2028 it&#x27;s expected costs will reduce...\" while saying nuclear will \"take too long to build\". Don&#x27;t tell me power-to-methane exists, show me the numbers saying it works and include the costs of storing it (stored methane is superb on energy density for the grid, but about 30% efficient panels-to-power plant and has a cost per kWh in storage...and has a suspicious paucity of information surrounding operating costs of plants versus \"future projections\"). reply Qwertious 6 hours agorootparentFocusing on batteries is thinking too long-term IMO. If you have 50% renewables or less in the grid, you basically don&#x27;t need batteries yet, and our deadline isn&#x27;t time here but greenhouse gases emitted - getting to 50% renewables will give us more time to tackle the rest of our emissions. I expect nuclear to easily beat solar&#x2F;wind+batteries for the last 10% of the grid, but frankly the last 10% of the grid almost doesn&#x27;t matter - if we&#x27;ve reduce our emissions by 90%, then we have 10x the time to tackle the rest.Of course, I&#x27;ve been wildly conflating electricity with all emissions, but I think the low-hanging fruit argument still applies. reply XorNot 6 hours agorootparentBut you can&#x27;t run the grid on 50% renewables without solving storage. It&#x27;s just not possible: the grid has to run. Peak usage isn&#x27;t in the middle of the day, it&#x27;s around 6-8pm when people are home. In winter that&#x27;s pitch black, in summer depending where you are there&#x27;s still very little solar output, so you better hope the wind is blowing. And it is...mostly, but not on demand.Germany&#x27;s power grid has started to struggle because they&#x27;re at 30+% solar capacity, but not reliability - the grid can&#x27;t sustain losing large swathes of generation capacity suddenly because a cloud passed overhead, or if a region is overcast that day then you&#x27;ve got to make up that shortfall - which you can do so long as you have basically the same installed capacity of gas, coal and oil on standby where they can just turn on an extra-turbine or boiler on demand.But if you want to actually decarbonize, then you need to solve that problem. Otherwise you end up with the perverse situation that increased renewable investment necessitates new fossil fuel power plants, because you still need to backstop the intermittent sources.The NSW Australian power grid has this problem right now: the reality is that no one wants to invest in fossil fuel power plants. Great! You&#x27;d say. But the grid actually does need a new reliable generator and power price pressure is a big political issue. Getting reliable watt-hours onto the grid - not more intermittents - needs to happen. The catch-22 is people rightly don&#x27;t want to subsidize more fossil fuels, but the available renewable options are being installed anyway - but they won&#x27;t solve the actual availability problem we have (and in a country with bush fires it&#x27;s a big one - 3 months of summer with overcast skies along the east coast from smoke means no one&#x27;s solar panels were working well, and grid-interconnects have limited capacity). reply cesarb 5 hours agorootparent> But you can&#x27;t run the grid on 50% renewables without solving storage.Counter-example: Brazil&#x27;s national grid routinely runs on over 85% renewables with essentially zero storage (there might be a couple of experimental battery storage units, but if any they&#x27;re much less than 1%; and all it has of pumped hydro storage is one partial preliminary study of possible locations). reply chelical 4 hours agorootparent65% of Brazil&#x27;s energy is coming from hydropower, which is extremely reliable, but geography dependent. When people make that statement, they&#x27;re usually referring to wind + solar, which are intermittent. reply XorNot 4 hours agorootparentprevThis right here is an example of a lie of omission: over what time frame? With what renewable?Oh right - hydroelectricity. Providing 65% of it&#x27;s total electricity.[1] Wind is 11% and solar is 2.5%.And they&#x27;re also building a new nuclear reactor at their existing plant.[2][1] https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Renewable_energy_in_Brazil[2] https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Angra_Nuclear_Power_Plant reply cesarb 4 hours agorootparent> This right here is an example of a lie of omission: over what time frame?Sorry about that. It&#x27;s the average over a whole week (I looked at the weekly bulletin of a random week from last month, but they&#x27;re all similar).> With what renewable?Around 62% hydro, 22% wind, and 1% solar (we&#x27;re in winter right now, so we have less solar and more wind; on summer we&#x27;d have more solar and less wind).> And they&#x27;re also building a new nuclear reactor at their existing plant.And that reactor is the main reason I&#x27;m very skeptical of nuclear power: it&#x27;s been under construction for nearly 40 years, and it&#x27;s still not near done. All the parts have already been bought and have been stored on site for a long time; just the upkeep to make sure none of these parts degrades while under storage is already a high cost. replyepistasis 6 hours agorootparentprevI disagree with this 100%, the only people using numbers to mode future carbon-free grids are ending up on massive amounts of solar, wind, and storage, with a tiny bit of nuclear. But that nuclear is so difficult to build that the models that end up adding nuclear also have unrealistic costs and deployment timelines associated with it.We have ~100 reactors in the US, nearly all are close to end of life. We would need to start building ~100 reactors today in order to keep the same amount of nuclear in the future. We have one, just one, nearing completion, and no more that have any chance of getting funded and starting construction within a few years. We are going to go through a long period with far less nuclear than we have today, if we ever get to a period with more nuclear than today. I doubt we ever will.If you want numbers, check out Christopher Clack&#x27;s modeling. It is by far the most detailed, and the most complete, and the most innovative.But to say that nuclear advocates run the numbers is just false. They can&#x27;t even run the numbers on their own construction plans, much less the numbers on the system level of what it would take to keep nuclear a significant part of our grid. Nuclear advocates are the least numerate folks in the energy business, again and again, since the big burst of reactors in the 70s. reply hackerlight 4 hours agorootparentprev> Nobody proposing solar, wind and storage ever wants to back it up with hard numbers.For hard numbers, see real-world case studies like Denmark:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Renewable_energy_in_DenmarkThey get 50% of their electricity from just wind. So that&#x27;s the lower bound on what you can achieve in the real world today. Here&#x27;s how you increase that without needing any storage technology:(1) Be a very large land mass like the United States, where wind variability is more likely to cancel out.(2) Add solar instead of only relying on wind. Solar cancels out wind variability because it&#x27;s more windy when it isn&#x27;t sunny.(3) Overbuild by 2-3x, so that the troughs in energy aren&#x27;t so bad. Even if you overbuild by this much, it&#x27;s still a lot cheaper and faster to build out than nuclear.(4) Import solar&#x2F;wind from neighboring countries, losing only 10-15% due to resistivity.I don&#x27;t know exactly what % renewables you can achieve by picking all this low hanging fruit while still being cheaper than Nuclear. 80%? 99%? If you add solar and you overbuild and you have access to a large landmass like the US, you can probably get very, very high, with the last few % being powered by natural gas peaker plants as a temporary stop-gap until storage technology becomes good enough.As a bonus, if you overbuild, you can sell surplus energy to Mexico and make money! reply frafra 1 hour agorootparentDenmark&#x27;s electricity is almost three times more expensive than in France. Solar+wind does not cancel out variability. Backup is needed. LCOE does not mean lower prices. Higher shares of renewables drives up costs for the network to rebalance. Producing a lot of energy when it is not needed means producing low value energy. It really depends on the electricity mix. reply crote 8 hours agoprevI am not surprised.Over the last few years, social media has been absolutely flooded with people claiming that nuclear is the \"one weird trick THEY don&#x27;t want you to know about\" to solving climate change. It is supposed to solve every single problem, and probably even cure cancer in the process - often relying on theoretical or experimental concepts which have never been tried in practice.Meanwhile, the reactors currently &#x2F; recently constructed are massively over budget and years behind schedule. Nobody is willing to invest in them unless the government is willing to guarantee a massive subsidy on the electricity produced. Additional reactors will at best come online in a decade or two. And during all this solar and wind are rapidly increasing their market share while being weaned off the initial subsidies. Literally everyone wants to build it and in many areas we are now seeing problems because too many people are building it, leading to overcapacity.Nuclear was a great idea in the 70s, but got a significant reputation hit due to high-profile nuclear incidents, cost overruns, and long construction delays. It was essentially killed by cheap coal becoming available with the rise of surface mining, just like coal is now being killed by cheap gas.In 2023 the economic reality is that nuclear simply isn&#x27;t a viable option anymore, and it would be quite foolish to rely on it to solve climate change in any way. reply SamBam 8 hours agoparentWhile this is the standard answer that people who have looked into it deeper give, it still surprises me.The largest solar farm in the US is about 570 MW. Some have calculated that we&#x27;re going to need to build two 400 MW farms every week for the next 30 years to reach a \"middle-road renewable energy scenario.\" [1]That&#x27;s insane.(Or we could be building one 400 MW solar farm and one 400 MW wind farm every week instead for 30 years. Point is, though, we&#x27;re far from being able to do that.)So we&#x27;re still going to be struggling in 20 years. Wouldn&#x27;t we be in a better position then if we had also started the process of building new nuclear plants 20 years prior? \"The best time to plant a tree was 20 years ago, the second best time is today,\" etc.1. https:&#x2F;&#x2F;papers.ssrn.com&#x2F;sol3&#x2F;papers.cfm?abstract_id=4443474 reply crote 8 hours agorootparentThe US installed 25.5GW of clean power generation capacity in 2022.[0] That&#x27;s 490MW a week, so it isn&#x27;t really all that insane. Meanwhile, the US is on pace to install 3GW of nuclear in 25 years.Surely increasing the construction speed of renewables from 490MW to 800MW &#x2F; week is easier than increasing the construction speed of nuclear from 2.3MW to 800MW &#x2F; week?[0]: https:&#x2F;&#x2F;cleanpower.org&#x2F;market-report-2022&#x2F; reply daemonologist 8 hours agorootparentprevIn 2023 utility-scale solar generation capacity is expected to grow by 29 GW, and wind by 7.5 GW[1] - that&#x27;s 700 MW per week, so we&#x27;re almost on track already. (The EIA&#x27;s \"expectations\" might be optimistic, but I don&#x27;t think it&#x27;s at all out of the question to roll out wind and solar on this scale.)1. https:&#x2F;&#x2F;www.eia.gov&#x2F;electricity&#x2F;monthly&#x2F;update&#x2F;archive&#x2F;febru... reply epistasis 6 hours agorootparentNot sure why you are downvoted, other than you used numbers and real data which contradicts a nuclear narrative.The EIA can be trusted with their reports on actual numbers. But their projections are absolutely embarrassing and underestimate solar deployments so much that they have completely discredited the entire enterprise of projections. The graphics showing how embarrassing they are all use the IEA numbers, but the EIA projections are just as badhttps:&#x2F;&#x2F;www.bu.edu&#x2F;eci&#x2F;2021&#x2F;11&#x2F;10&#x2F;signs-of-hope-reflections-... reply solraph 8 hours agorootparentprev> (Or we could be building one 400 MW solar farm and one 400 MW wind farm every week instead for 30 years. Point is, though, we&#x27;re far from being able to do that.)That&#x27;s actually lower than what the USA is doing right now. \"The US solar industry installed 6.1 gigawatts-direct current (GWdc) of capacity in the first quarter of 2023\"[0]That is approx 469 MW of power being installed every week.[0]https:&#x2F;&#x2F;www.seia.org&#x2F;us-solar-market-insight (first link I could find) reply SamBam 8 hours agorootparentSorry, I realize I was being confusing by measuring in 400 MW farms: the total required was 800 MW&#x2F;week, which we&#x27;re not hitting yet.We are at the right order of magnitude, though, as you show. The question is whether we can hit the actual number required (which is close to double what we have now) and sustain it for another three decades. reply marcosdumay 6 hours agorootparent> sustain it for another three decadesYou keep saying \"sustain\", but solar has been growing exponentially and there is still no sign of an inflection. \"Sustaining\" a rate of growth is the wrong concept to model it. reply epistasis 6 hours agorootparentprev> That&#x27;s insane.This type of innumeracy from nuclear advocates is rampant. It&#x27;s not insane, you just put out a number that seems big, to you, and said \"wow this must be impossible!\"Meanwhile, we deploy that all the time.If nuclear advocates were better at numbers, maybe all the nuclear projects wouldn&#x27;t be 2x-3x over budget and off schedule, and there would be more nuclear built. reply jncfhnb 8 hours agorootparentprevIt really depends on the cost. Whatever your analysis says, surely it says something different if the cost of solar goes down 90% over the next decade. reply hedora 8 hours agoparentprevIf you price in externalities, as we will have to do in order to slow down climate change, then nuclear is way less expensive than coal or gas.Also, other countries have historically been able to build plants on reasonable timeframes and budgets.If there was political will, we could have a large fleet of nuclear plants in under a decade. reply crote 8 hours agorootparentOh absolutely, but that&#x27;s not going to make the comparison with solar and wind any better. It&#x27;d probably make carbon capture for gas a really interesting peaker solution, though!Considering that during the 1970s nuclear boom a single plant still took a decade to construct, there is simply no way we could possibly have a \"large fleet\" in under a decade. The only country with the capability of doing so is China - and they are not going to be of much use to Western countries. reply asynchronous 6 hours agorootparentChina is living proof we could do it- they are currently on track to build a _reactor a year_. We let all our nuclear talent age out and die. reply epistasis 6 hours agorootparentChina is not building nearly as much nuclear as they are solar, wind, and storage.A reactor a year is a pathetic pace! 1GW&#x2F;year! Even I f the US matched that, we will see most of our nuclear fleet age out before it was replaced.For contrast, China is expected to be able to produce 6TWh&#x2F;year of batteries in 2030, ~260 \"gigafactories\" worth:https:&#x2F;&#x2F;source.benchmarkminerals.com&#x2F;article&#x2F;over-400-gigafa...China&#x27;s reluctance to invest as heavily in nuclear should be a clear sign that even with great competence at building nuclear, it&#x27;s just not that attractive of a tech. It&#x27;s barely functioning as a hedge, and is probably only being used for its side industries rather than as a serious power source. reply evilos 4 hours agoparentprevIf you look at large nations that have actually decarbed their grid, they do it with hydro first and nuclear second.Show me even a medium sized nation that is predominantly powered by solar&#x2F;wind and isn&#x27;t utterly reliant on imports.Nuclear is absolutely economically viable so long as you build one design several times and don&#x27;t let regulators insist on crazy handycaps. That&#x27;s how France, Sweden, Korea, China, and Japan successfully built out. Japan&#x27;s median built time was 3.8 years during its build out. China builds plants today with $3000&#x2F;kw capital costs which is as cheap as coal.Nuclear technology is a proven solution despite all the mental gymnastics people do to insist otherwise. reply rgmerk 8 hours agoprevRooftop solar power is roughly as popular as puppies and kittens.But the USA has very little of it because it’s expensive in the US (due mostly to regulation and utilities being obstructionist).By contrast, rooftop solar is very cheap in Australia, so it’s everywhere.Nuclear is safe and clean, but until it’s cheaper nobody is going to go near building any more of it, and the nuclear industry has been incapable so far of making it cheaper. reply asynchronous 6 hours agoparentIs it cheaper in Australia because it’s publicly subsidized? From what I understand in the states the only reason it’s even remotely affordable is because it’s already subsidized. reply epistasis 6 hours agorootparentNo, not at all.The reason it&#x27;s expensive in the US is all regulatory, and because of the ever changing patchwork of utility rules that make it economical or not to attach your own generation to the grid.Last I heard, roughly 30% of the cost of solar in the US is spent merely on customer acquisition. Because the only forms that survive are those that swoop in when there&#x27;s a window of economic plausibility for solar to work for homeowners, before the utility in a particular small area changes their mind and buys off the regulators to make it cost infeasible for homeowners to generate.There&#x27;s a huge advantage to the grid of having generation be more local: more than half of energy costs are transmission and distribution, and these are costs that utilities can reliable charge a profit for, so they try to make these costs as high as possible. When customers install their own solar, this reduces the peak daytime need for T&D, and the grid is sized for peak usage. So everyone would be better off with far more local solar, except for the utility. reply kalleboo 4 hours agorootparent> this reduces the peak daytime need for T&D, and the grid is sized for peak usageWhere I&#x27;m from, peak usage is on cold, dark, winter days. So there are no savings to the grid from locally generated solar as they still have to be",
    "originSummary": [
      "A recent survey by Pew Research Center shows that 57% of Americans now support expanding nuclear power in the United States, up from 43% in 2020.",
      "However, solar power (82%) and wind power (75%) remain more favored by Americans compared to nuclear power.",
      "Support for nuclear power has increased among both Democrats and Republicans, with men and Republicans being more likely to support it and believe the government should encourage its production. The United States currently has 93 nuclear power reactors, mainly located in the South, but the number has declined since its peak in 1990, potentially due to concerns over accidents and international tensions."
    ],
    "commentSummary": [
      "The debate surrounding nuclear power is multifaceted, with some arguing for its benefits and others advocating for renewable energy sources like solar and wind.",
      "Proponents of nuclear power emphasize its cost-effectiveness and environmental friendliness compared to other forms of energy.",
      "Opponents of nuclear power raise concerns about its high costs, potential dangers, safety issues, regulation, storage of nuclear waste, and the influence of politics and industry on public perception."
    ],
    "points": 221,
    "commentCount": 236,
    "retryCount": 0,
    "time": 1692660086
  },
  {
    "id": 37216800,
    "title": "Podman Desktop celebrates 500k downloads",
    "originLink": "https://blog.podman.io/2023/08/celebrating-500k-downloads-the-podman-desktop-journey-%f0%9f%8e%89/",
    "originBody": "Skip to content Containers GitHub Blogs, articles, and tips from the Podman development team on Podman, Buildah, Skopeo, and a host of other related container projects. Explore more → ANNOUNCEMENT, PODMAN DESKTOP Celebrating 500K Downloads: The Podman Desktop Journey 🎉 August 17, 2023 · Stevan Le Meur Hello Podman community! Today is a special day for all of us – we’ve officially hit 500,000 downloads of Podman Desktop! We want to say a big thanks to each and every one of you. It feels like yesterday that Podman Desktop was merely an idea. Over the last year, the initiative started with different groups and multiple prototypes have been made. This allowed us to understand the real need and interest for a desktop GUI. More importantly, it helped define our mission and goal: enable simple workflows for developers working with containers and Kubernetes. More and more of you are leveraging Podman Desktop in your daily developer workflows and this is truly amazing! What makes Podman Desktop special? Podman Desktop is completely open source. Some of you have been contributing in many different ways but your feedback and your use cases are central to the way we prioritize our efforts on the tool. We love to hear from you, we love to understand the pain points you are facing in your daily workflows, and that is the core foundation of how we operate. Thank you to all of you who have been contributing and helping the tool to be what it is today – either by reporting a bug, or proposing a pull request! More than anything, with Podman Desktop, we aim to bridge the gaps between a developer environment and a production environment – so you can benefit from consistency and predictability to streamline your developer workflows. If you still haven’t seen what Podman Desktop can do, give a look at this quick overview video: https://www.youtube.com/watch?v=OeJ_cwdoQm8 Intro video on Podman Desktop On to the next milestone: 1M downloads! The journey has just begun for Podman Desktop and we are committed to improve, enhance and mature it everyday. We’ll continue to focus on experiences working with containers. In the next few months we’ll also enrich the capabilities for working with Kubernetes – and obviously continue on our mission to help you transition from containers, to pods, to Kubernetes. If you are interested in contributing, we would like to invite you to join the following discussions: Inspect objects in Kubernetes deployment environments – We will expand our Kubernetes (K8s) capabilities with a new K8s dashboard and additional K8s objects support so you can better understand and compare K8s environments. Review our early ideas and tell us what would help you. Install and configure additional features more easily – Check out our designs for an upcoming onboarding feature to provide guided walkthroughs to help you install and configure new container capabilities. Find answers to questions quickly and easily – Podman Desktop’s documentation is expanded and improved; what are we missing? What would you like to see us cover? Let us know. Final Thoughts Thank you again to our users and contributors. We’re more than happy with this milestone and we are looking forward to the next one! Please continue to help the project, by reporting issues and feedback. You can also show your support in three ways: Give us a star on the github repository: https://github.com/containers/podman-desktop Add yourself in the list of adopters: https://github.com/containers/podman-desktop/blob/main/ADOPTERS.md Enable telemetry data in the tool which helps us understand how to improve the tool and diagnose certain issues (the data are anonymized). With gratitude, The Podman and Podman Desktop teams! In: Announcement, Podman Desktop Leave a Reply Subscribe Sign up with your email address to receive updates by email from this website. Name(required) Email(required) BY SUBMITTING YOUR INFORMATION, YOU'RE GIVING US PERMISSION TO EMAIL YOU. YOU MAY UNSUBSCRIBE AT ANY TIME. Subscribe Latest news Celebrating 500K Downloads: The Podman Desktop Journey 🎉 Podman v4.6 Introduces Podmansh: A Revolutionary Login Shell Podman v4.6 released! Podman Performance: Root and Rootless Rootless Podman user-namespaces in plain English Categories Announcement (13) Articles (5) Blog (18) Buildah (2) Podman (20) Podman Desktop (1) Releases (7) Tutorials (4) Uncategorized (2) Search Search Search Containers The Containers Community creates a collection of open source tools that create, configure, and work with containers. This is our publication where you can read articles, tutorials, and release announcements about our tools. Twitter Community Community Code of Conduct Contributor Guidelines Our GitHub page Join our community Found a bug in this website? Sitemap Archives buildah Subscribe to our mailing list. Sign up with your email address to receive regular discussion emails about our projects: Containers Community Mailing List Copyright © 2022 Containers Org Powered by WordPress. Based on the Uniblock theme by WPZOOM. Designed by Máirin Duffy",
    "commentLink": "https://news.ycombinator.com/item?id=37216800",
    "commentBody": "Podman Desktop celebrates 500k downloadsHacker NewspastloginPodman Desktop celebrates 500k downloads (podman.io) 219 points by twelvenmonkeys 10 hours ago| hidepastfavorite94 comments anbotero 8 hours agoKeep it up!What trickery does OrbStack[0] use to be fast?We went back to Podman Desktop because of the license[1], but the interactions on macOS are almost as slow as Docker Desktop.As for Podman itself, happy users here, and our workflows have been working for a while now. We are only missing faster local development.----[0]: https:&#x2F;&#x2F;orbstack.dev&#x2F;[1]: https:&#x2F;&#x2F;docs.orbstack.dev&#x2F;faq#free reply moondev 8 hours agoparentI&#x27;m fairly certain orbstack runs \"machines\" as lxc containers on the host vm, and then container runtime inside. Not sure if that would make container operations any faster but it would speed up \"machine\" lifecycle.You can do this yourself via multipass, it&#x27;s an interesting solution specifically on Apple Silicon because nested virtualization is not available and lxc does not require a hypervisor . reply kdrag0n 7 hours agoparentprevOrbStack dev here — sorry, can&#x27;t reveal too much, but some of it is documented here: https:&#x2F;&#x2F;docs.orbstack.dev&#x2F;architectureBy \"we\" I assume you&#x27;re referring to your company&#x2F;employer, so I&#x27;m curious if there&#x27;s any way the license could be more palatable short of being 100% free? reply alberth 6 hours agorootparentI think GP just means that OrbStack will no longer be free post beta, and that was their objection.I have no problem paying for quality products, which OrbStack appears to be … but developers hear the words “license” or “free” and then get hopeful they can get a free lunch.As such, a suggestion, just remove this line from your FAQ:“Educational and non-profit open-source licenses subject to approval. Commercial open-source developers must purchase a license.”Talking about open source in the same context of a paid subscription might be confusing people. reply rhaps0dy 5 hours agorootparent> Talking about open source in the same context of a paid subscription might be confusing people.It&#x27;s making some useful work though. It&#x27;s only going to be free for non-profits who develop open-source software.(I work at a non-profit which makes some closed-source software, and I&#x27;m an Orbstack user, so I expect to pay when the time comes) reply benterix 2 hours agorootparent> I work at a non-profit which makes some closed-source softwareOut of curiosity: is it for internal use? reply kdrag0n 3 hours agorootparentprevYeah, I can understand that, but since it&#x27;s for work I figured I&#x27;d ask whether it&#x27;s a flat-out objection to anything paid, or if it&#x27;s more to do with something specific about the license.Thanks for pointing that line out — just reworded it to be more clear. reply zxexz 6 hours agorootparentprevNothing against this model, but your website states “OrbStack is completely free to use during beta, but it will become a paid product afterwards.”[0]Also, the rest of the page implies OrbStack will be charging for any commercial use.If this is the case, I don’t see how this is “free” in the sense that most people on this site interpret it. Have you thought about the FOSS model with support, maybe a Jetbrains style thing - adding enterprise&#x2F;paid features on top of the FOSS stack?[0] https:&#x2F;&#x2F;docs.orbstack.dev&#x2F;faq reply satvikpendem 6 hours agorootparentFree can mean free as in freedom or free as in beer, most people even on this site do not use \"free\" to mean exclusively the former, or \"libre\" as it&#x27;s better distinguished. It is well understood exactly what the sentence you quoted means, especially as they disambiguated it through saying it will be a paid product afterwards, so obviously meaning free as in the free as in beer sense. reply zxexz 5 hours agorootparentOh, I full-heartedly understand and agree!However, I think referring to something as \"free\" when it&#x27;s explicitly \"free right now and probably not later\" is a bit misleading. I doubt GP was trying to pull any wool over any eyes. In the enterprise context, especially, the differentiation is pretty important. Also, the page I linked does imply that it will become not free in the lunch sense of the term, for commercial users (already not free in the libre sense of the term, and I&#x27;m not saying there&#x27;s anything wrong with that). reply satvikpendem 5 hours agorootparentThey explicitly say free during beta and paid afterwards so if anyone misunderstands that, that&#x27;s on them. It&#x27;s quite reasonable to have such phrasing and many other products also have similar business models. reply zxexz 5 hours agorootparentI was referring to GP’s comment, not the website. Otherwise, pretty much agreed. replyrhaps0dy 5 hours agorootparentprevThis diagram explains it very well, thanks!So there&#x27;s only 1 VM, running a modified Linux kernel (https:&#x2F;&#x2F;github.com&#x2F;orbstack&#x2F;linux-macvirt&#x2F;commits&#x2F;mac-pub) with a barebones userland. Then inside there&#x27;s a bunch of souped-up chroots (probably LXC&#x2F;LXD), running the various linux distribution VMs as well as one that just runs `dockerd`.Here it specifies that all the VMs share 1 kernel: https:&#x2F;&#x2F;docs.orbstack.dev&#x2F;architecture#linux-machinesOn top of this, it&#x27;s fast because it uses Rosetta (which is very fast) and kdrag0n spent a lot of time optimizing various parts of the kernel and the resulting environment. reply kiratp 7 hours agoparentprev> We are only missing faster local development.Developer time is probably your most expensive resource. Saving $8&#x2F;mo at the cost of dev time seems… suboptimal. reply dijit 3 hours agorootparentSometimes its more the issue that not owning your tools causes:A) forced changes which can eat your “savings”B) rent seeking (or: non-optional price increases like I’ve experienced with gitlab)C) The company pivots and leaves your tool behind to languish reply Alupis 9 hours agoprevCongrats to the Podman team - it&#x27;s impressive to build anything that gets 500k eyeballs let alone downloads.With that said, we have yet to see if Podman can stand the test of time.Millions of people flocked to Threads because of a dust-up on Twitter... and then immediately went back to Twitter. Can Podman manage to keep it&#x27;s traction and continue growing... that will be something to see. reply risho 6 hours agoparent...? podman has been around for years and has been slowly increasing users for years. it is the exact opposite of a flash in the pan. reply jshen 7 hours agoparentprevYou can’t talk about the test of time and then use an analogy that hasn’t stood the test of time ;) reply ec109685 9 hours agoprevStill feels crappy what Docker did by being free since inception, becoming the standard and then springing license fees onto companies that couldn&#x27;t migrate off of it in time.Hopefully, podman can solve \"basic dcoker desktop\" needs for most companies. reply cosmojg 8 hours agoparentPodman has been a very straightforward drop-in replacement for Docker at my company. Even if that didn&#x27;t work out, things like Linux namespaces, chroot, bubblewrap, and so on are all beautifully well-documented, battle-tested tools.Docker has no moat. reply Spooky23 8 hours agoparentprevYeah some of my colleagues missed the memo and we got the extortion attempt. The crazy thing is the part of the company that want you to pay the troll is not the same as the field sales.That’s probably the most noxious business model ever. I’ve dealt with 5-6 companies doing that, and whenever renewal&#x2F;retention is a different sales organization, i spin up a company project to plan the exit strategy.The nickel and dime cost stuff is one thing, but the bigger problem is that these companies and products always decline. reply pigeonhole123 7 hours agorootparentAren’t most sales teams separate from customer success? Can you expand on this insight? reply krmboya 7 hours agorootparentDid you mean &#x27;customer support&#x27;? reply evilduck 6 hours agorootparentProbably not. Customer success are teams dedicated to keeping customers happy and paying and finding ways to ensure that they remain customers. Customer support is typically who you contact when you don&#x27;t have any leverage and encounter a problem.If you spend a million dollars a year on a contract, you will typically have someone you know by name and meet with regularly who will legitimately hear your feedback and work to resolve your problems. reply satvikpendem 6 hours agorootparentHow is that not just another name for customer support? They&#x27;re both kinds of support for the customer, even if one has a more palatable name. In my experience, one is simply an alias for the other, as the sibling commenter notes, I have not seen those two roles split out. reply evilduck 6 hours agorootparentIn my own experience, I&#x27;ve seen them as distinct groups from customer support both at the companies I&#x27;ve worked for and companies I&#x27;ve worked with. I view it more as an alias to account rep, but sometimes people account reps will delegate to.Customer support will file a ticket for me when something isn&#x27;t working. Maybe help if something on their script matches existing known issues. Customer success will proactively see that I&#x27;m not getting the expected value from my already committed spending and utilization and reach out to make sure everything is configured right so that I renew next year. reply NhanH 6 hours agorootparentprevCustomer success is an alias of customer support. reply lbourdages 8 hours agoparentprevI&#x27;ve been using Rancher Desktop for the past 6 months and I haven&#x27;t missed Docker Desktop a single time. reply stock_toaster 7 hours agorootparentAnyone know how does it compares to podman desktop? reply slimsag 7 hours agorootparentI believe Rancher desktop has more of an out-of-the-box focus on Kubernetes, it uses Rancher&#x27;s k3s and whatnot. Podman desktop treats k8s as more of an optional thing, e.g. you use Kind to create a cluster.Of course there are also organizational differences, Podman seems to be more of a community led open-source project whereas Rancher is ultimately trying to sell enterprise kubernetes management software.In my experience, k3s looks great on the surface but I ran into quite a few issues operating it in a single-node use case.If I was looking into this today, I&#x27;d definitely explore Podman&#x2F;Kind instead. reply stock_toaster 1 hour agorootparentThat’s good info. Thanks! reply hedora 8 hours agoparentprevI’ve had good luck with ubuntu multipass as a docker desktop replacement on macos:https:&#x2F;&#x2F;ubuntu.com&#x2F;blog&#x2F;replacing-docker-desktop-on-windows-...It runs on windows too. reply naikrovek 6 hours agoparentprevDocker is still free.it is only Docker Desktop which is no longer free.most power users do not use or need Docker Desktop. reply phoenk 6 hours agorootparentThe desktop application is the easiest way to get up and running on Mac or Windows where a VM is necessary. Of course you can still set up docker engine manually in a VM, but even then it doesn&#x27;t offer the same level of integration, like host mounts, Rosetta on Mac. Desktop Linux is the only environment where the desktop app offers little benefit over just running the engine. reply gettodachoppa 1 hour agorootparentI always forget there&#x27;s people who do software development on something other than Linux.I mean I use Windows too (my laptop has poor Linux support, plus Proton wasn&#x27;t a thing when I bought it so I went for something which is guaranteed to run games well). But I do all my development in a Linux VM in Virtualbox. If you have an SSD, performance is indistinguishable from native. reply sbjs 9 hours agoparentprev> businesses of fewer than than 250 employees AND less than $10 million in revenueSeems like they can afford it. reply andrewmutz 9 hours agorootparent> Seems like they can afford it.This is a low bar for ethical business practices.If lure people to my ecosystem with low prices and then jack them up later when the switching costs are high, it&#x27;s fine if they can afford it?If the electric company jacked their rates by 10X for consumers who make over 100K a year would that be fine since \"they can afford it\"? reply skinner927 4 hours agorootparentElectric company is a bad comparison because they’re a utility and you usually can’t switch.If grocery store A started charging $30 for walking into the store, you better bet everyone’s gonna head to grocery store B. reply memefrog 5 hours agorootparentprevYes, because they would lose all their customers to their competitors, which is exactly what has happened to Docker. reply dwattttt 9 hours agorootparentprevTo complete the context, both of those are required for free use of Docker Desktop; 250 employees and $1k revenue would still require you to license it.EDIT: https:&#x2F;&#x2F;docs.docker.com&#x2F;subscription&#x2F;desktop-license&#x2F; reply throwawayit1234 9 hours agorootparentThat sounds like a bad business. reply Apfel 9 hours agorootparentOr a government organisation. Or a charity. reply willsmith72 8 hours agorootparentHaving worked in government, I don&#x27;t see why they should be excluded from paying.But I think a charity could fall under the 3rd one here.\"Examples of freely permitted usage include:Personal projects with less than $10 million revenue per year Students and teachers (whether in an educational or professional setting) Research at not-for-profit institutions Personal contributions to non-commercial open source projects\" reply Spooky23 8 hours agorootparentSecond that. The crazy t&c required by gov have to be accounted for. reply Zetice 8 hours agorootparentprevNot-for-profit institutions are exempt. reply Barrin92 9 hours agorootparentprev>250 employees and $1k revenuewhat is this, the world&#x27;s most overstaffed lemonade stand? reply sbjs 9 hours agorootparentRarely does an HN comment make me laugh. Well done. reply colechristensen 9 hours agorootparentprevWhich is fine.250 employees earning the median US wage is 11 million per year to the employee before all sorts of overhead so more like 20 million in wages per year. reply dwattttt 9 hours agorootparentWhile true, many businesses are not US or staffed by the median waged US employee reply ElectricalUnion 6 hours agorootparentprev250 employees earning the average AR wage (542400 ARS&#x2F;year) is around 360000 USD per year. You can pay 10 average AR workers for the wage of a single US minimum wage worker. reply DiggyJohnson 4 hours agorootparentRespectfully, I had no idea what you were talking about without a few google searches.AR == Argentinian Peso542k ARS == $1600 USDSo I guess I’ll ask some questions:1) Is the Argentinian software labor pool large and organized enough to be accessible to international firms?2) Is this the average wage of a software developer in Argentina? Or some larger group?3) Why is this relevant in this discussion? reply ElectricalUnion 4 hours agorootparent> AR == Argentinian PesoNope, ISO 4217 says it&#x27;s ARS.AR is the ISO 3166 contry code for Argentina.> 1) Is the Argentinian software labor pool large and organized enough to be accessible to international firms?Yes?> 2) Is this the average wage of a software developer in Argentina? Or some larger group?Average wage of Argentina.> 3) Why is this relevant in this discussion?People like to think people only live in the US. reply colechristensen 4 hours agorootparentprevI worked with some great Argentine contractors some years ago, I understand there is a nontrivial offshoring software labor pool.There is a point that offshore contractors getting access to such software represents a ... larger proportion of compensation, especially among the several such SaaS per-user fees most tech companies will pay.But there&#x27;s also a counterpoint... docker wants somewhere between $5 and $30 US dollars per month per user, yes this is annoying because the value doesn&#x27;t seem to be there for docker desktop which is all most people want anyway... on the other hand it&#x27;s not such a moral issue... it&#x27;s not much money at all. replyuser3939382 8 hours agoprevSomething feels very wrong about this class of tool (Docker Desktop) being mandatory GUI. I want 0 of the features provided by the GUI that aren&#x27;t already in the CLI. reply aequitas 6 hours agoparentTry Colima [0], it’s a CLI only Docker Desktop replacement built on Lima.[0] https:&#x2F;&#x2F;github.com&#x2F;abiosoft&#x2F;colima reply nabogh 7 hours agoparentprevWhat feature is available in the desktop that&#x27;s not available in the cli? reply seabrookmx 7 hours agorootparentx86-64 container support on Apple silicon comes to mind. reply technofiend 7 hours agorootparentI have limited experience with podman on M1, having only pivoted recently thanks to the docker license issues but my experience so far on an M1 Mac was I couldn&#x27;t initialize or start the application from the gui.Initialize timed out due to slow proxy speeds, so I had to use \"podman machine start\" at the command line to wait for the image download and install to complete. For whatever reason I couldn&#x27;t start using the UI after a reboot because I got an EACCESS error and again command line worked which was then acknowledged as running via the UI.This is in an environment where the end-user has no root and podman is running in rootless mode. Once I got it running, I was able to crank up x86 http and postgres containers from the command line mostly with no issues, although pod termination seems to work better from the UI. In fact starting containers at command line throws a warning that your container architecture doesn&#x27;t match your host architecture but that&#x27;s it and podman just carries on after that.When you say x86-64, is there something special about 64-bit containers that makes them only work via the UI? reply seabrookmx 5 hours agorootparentI was talking about Docker not podman, as that&#x27;s what I assumed the parent comment was discussing.Docker CE does not offer an emulation layer for x86(-64) on ARM, to my knowledge. reply no_carrier 5 hours agorootparentprevCould this not be achieved by managing your own vm on Apple silicon, if you&#x27;re that averse to the Podman Desktop GUI? reply LightFog 4 hours agorootparentIt is simple on the CLI with podman using the ‘platform’ argument. They were confusingly only referring to Docker it seems. replyandersrs 8 hours agoprevHow is macbook battery life with Podman? Docker really kills battery life. reply coldtea 5 hours agoparentDo you frequently use the macbook on airplanes or cafes or something for regular development? On mains power it&#x27;s fine! reply taspeotis 9 hours agoprevI switched to Podman on Windows, from Docker Desktop, and it was a bit rough 6mo ago but it’s fairly good now. reply op00to 8 hours agoparentPodman Desktop on Windows? Why not podman on WSL2? I&#x27;ve been using Podman on a Fedora 37 WSL instance and it&#x27;s fantastic. reply bricss 7 hours agorootparentPodman Desktop on Windows works on top of WSL2 with Fedora image nowadays, but one of the advantages of that, since v4.6.0, it&#x27;s new --user-mode-networking option for init to bypass harsh VPNs such as Cisco AnyConnect VPN \\m&#x2F;https:&#x2F;&#x2F;docs.podman.io&#x2F;en&#x2F;latest&#x2F;markdown&#x2F;podman-machine-ini... reply porjo 7 hours agorootparentprevI have podman desktop installed just to get podman machine auto started on boot, otherwise I&#x27;m using podman within WSL cli. Is there a better way? reply brittle2528 9 hours agoparentprevDoes it have a better response when clicking something than Docker Desktop? reply gbraad 8 hours agoparentprevwhat made it rough? reliability or just the integration? reply wiradikusuma 7 hours agoprevSo Podman Desktop&#x2F;OrbStack are Docker Desktop replacement?Do you need Docker (CLI and Desktop) installed? Or, you should uninstall them first? reply naikrovek 7 hours agoparentuninstall them first. podman desktop is a drop-in replacement, iirc. reply willsmith72 9 hours agoprevAnyone know how the performance is vs docker desktop on an Intel mac? I moved to using hosted services for local dev because even 1 container running would kill my MacBook reply zamalek 8 hours agoparentIt could be faster. DD uses a proprietary VM solution to run Linux, while Podman uses Lima. Lima is an extremely active project, while Docker seem happy doing the bare minimum (from my experience at least). reply gbraad 8 hours agorootparentwe are adding vfkit, which is our implementation of the native hypervisor for macos. by default it would currentky use qemu, but lima is an option. reply bongobingo1 5 hours agorootparentWho is \"we\", Docker or (co)Lima. reply DiggyJohnson 4 hours agorootparentHad the same question. My 60 second (somewhat creepy) smoothing was not enough to produce an answer, unfortunately. reply willsmith72 8 hours agorootparentprevCool! I tried lima a while ago and it was way quicker, but painful to set up. Maybe I should try podman reply pxc 9 hours agoparentprevHow much RAM does your Mac have and what was allocates to Docker Desktop? reply willsmith72 8 hours agorootparent8GB, 3 for docker (I would run 1 container at a time) reply jbverschoor 7 hours agoprevI will hereby celebrate one week with orbstack! reply swozey 9 hours agoprevCongrats! I&#x27;m glad yall added plugins&#x2F;extensions awhile back reply osigurdson 8 hours agoprevIt would be a lot more exciting IBM wasn’t behind it. reply naikrovek 6 hours agoparenttry Rancher Desktop, then. reply osigurdson 4 hours agorootparentI like podman, it definitely seems like the right tool for local development. Docker still has better tooling in some areas though of course due to maturity. reply renewiltord 9 hours agoprevMy only experience with this software is getting the error Error: OCI runtime error: runc: runc create failed: cannot use rootless systemd cgroups manager on cgroup v1When I upgraded to a new Ubuntu and ended up with this which I solved by running it as `sudo`. If it were in somewhere critical I&#x27;d worry. reply pxc 9 hours agoparentWhat version of Ubuntu? I thought they switched to cgroups v2 years ago.> If it were in somewhere critical I&#x27;d worry.Looks like you can enable the rootless systemd cgroups manager by adding systemd.unified_cgroup_hierarchy=1to your kernel boot params. reply renewiltord 8 hours agorootparentThank you for that. I couldn&#x27;t find that info anywhere. reply pxc 7 hours agorootparentI was surprised at how poorly documented&#x2F;indexed it was when I searched around, too! reply conradev 9 hours agoprev [–] OrbStack functions better than Podman and Docker Desktop combined on macOS: https:&#x2F;&#x2F;orbstack.dev&#x2F;Lightweight, native app, drop in replacement. It is funny that you need to launch an entire web browser to then launch a single container. reply mianos 9 hours agoparentNot comparable to podman: \"Is OrbStack free? OrbStack is completely free to use during beta, but it will become a paid product afterwards. We&#x27;re still working out the details, but this is the plan so far (subject to change):Personal use: free Business and commercial use: $8&#x2F;user&#x2F;mo \" reply op00to 8 hours agoparentprevit&#x27;s funny that you feel the need to drop your SPAM in this thread. reply conradev 5 hours agorootparentIt&#x27;s unfortunately a genuine unaffiliated endorsement reply xwowsersx 9 hours agoparentprev [–] Hadn&#x27;t heard of this, thanks for sharing. Should be a full drop-in replacement? reply jacklbk 4 hours agorootparent [–] Yes. I replaced Docker Desktop with it and I haven&#x27;t found anything lacking for my use cases so far. It&#x27;s a great alternative, just install and a few clicks, that&#x27;s it. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Podman Desktop, an open-source container project, has achieved a significant milestone with over 500,000 downloads.",
      "The Podman team expresses gratitude for user contributions and feedback and is committed to enhancing the tool's functionality for Kubernetes usage.",
      "Users are encouraged to participate in discussions centered around improving Kubernetes support, streamlining feature installation and configuration, and expanding documentation."
    ],
    "commentSummary": [
      "Users are discussing various container management tools such as Podman, OrbStack, and Docker Desktop.",
      "They express the need for quicker local development and have concerns about licensing and pricing.",
      "Users are sharing their experiences and preferences, exploring alternatives to Docker Desktop."
    ],
    "points": 219,
    "commentCount": 94,
    "retryCount": 0,
    "time": 1692661494
  },
  {
    "id": 37211675,
    "title": "Upcoming .com and .xyz domain price increase",
    "originLink": "https://www.namecheap.com/blog/upcoming-com-and-xyz-domain-price-increase/",
    "originBody": "Skip to main content blog Go To Namecheap.com Domains Upcoming .COM and .XYZ domain price increase Namecheap StaffAugust 21, 2023 0 2 min read At Namecheap, we’ve consistently stood up for our users by challenging arbitrary domain price increases. As we approach another price increase for .COM and .XYZ domains this September, we wanted to ensure our customers are informed so you can continue to get the best value for your investments. Key information for Namecheap users All .COM domain renewals will see an approximate 9% increase. This price increase will happen across registrars, not just Namecheap. The new prices will take effect on September 1st. .XYZ domains will also experience a price increase. We recommend our existing domain customers renew .COM domains before September to lock in the current rates for the coming year. Prospective registrants should likewise consider registering before the price increase to lock in existing prices. Alternatively, you might consider alternatives to .COM. Depending on the top-level domain, these options could be more budget-friendly Namecheap’s pricing beginning September 1 .COM: Renewals – $15.88 .XYZ: Registrations/Transfers – $12.98 .XYZ Renewals – $14.98 Verisign’s pattern of increases Verisign, the entity overseeing .COM domains, has pricing authority based on their agreement with the Internet Corporation for Assigned Names and Numbers (ICANN). The periodic price changes for .COM domains are in line with this arrangement. While we at Namecheap understand how this cost compounds over time, especially for those managing multiple domains or planning to acquire more, the increases are, therefore, out of our control. Namecheap’s commitment Back in 2019, we made a stand against ICANN’s decision to remove price caps for .ORG and .INFO domains. Our prediction in 2020 about Verisign’s potential price changes has, unfortunately, come true, reaffirming our commitment to ensuring fair pricing and transparency in the domain industry. At Namecheap, we value our customers and believe in transparent communication. We’re here to support you during these industry changes, offering advice and alternatives to ensure your online presence remains strong and affordable. Renew your domains now If you currently hold any .COM or .XYZ domains with Namecheap, you can renew between now and the end of August to keep current pricing, regardless of when the domains are set to expire. Go to Namecheap.com and sign in to review your domain list and renew your domains. 3 Article Rating Was this article helpful? 3 Show the conversation Help us blog better What would you like us to write more about? ALL STARTUP & BUSINESS ONLINE MARKETING SECURITY & PRIVACY PRODUCTS, APPS & DEALS TECH TIPS & TOOLS WORDPRESS NEWS DOMAINS HOSTING Send Suggestion Namecheap Staff More articles written by Namecheap. More articles like this Exploring the idea of decentralized DNS Gary S.Aug 15, 2023 6 min read 0 Creating a killer marketing plan for your startup Kristin J.Jul 11, 2023 7 min read 0 Get the latest news and deals Sign up for email updates covering blogs, offers, and lots more. I'd like to receive: New blog post alerts Regular newsletter and special offers Subscribe Your data is kept safe and private in line with our values and the GDPR. Current Deals at Namecheap Previous Post Tech Beat by Namecheap – 18 August 2023 Read More Domains Domain Name Search Transfer New TLDs Personal Domain Marketplace Whois Lookup PremiumDns FreeDNS Hosting Shared Hosting UPDATED WordPress Hosting Reseller Hosting VPS Hosting Dedicated Servers Private Email Hosting Migrate to Namecheap WebSite Builder WordPress Shared Hosting WordPress Hosting UPDATED Migrate WordPress Security Domain Privacy PremiumDNS VPN UPDATED 2FA Public DNS Transfer to Us TRY ME Transfer Domains Migrate Hosting Migrate WordPress Migrate Email SSL Certificates Comodo Organization Validation Domain Validation Extended Validation Single Domain Wildcard Multi-Domain Blog Domains Deals Engineering Entrepreneurial Lifestyle Managing a Business Marketing Tips News Products Security & Privacy Technology WordPress Working From Home Sign up for email updates Current Deals GDPR Commitment Contact Us Go to Namecheap.com Status & Updates The entirety of this site is protected by copyright © 2023 Namecheap. All rights reserved. Terms and Conditions Privacy Policy GDPR Cookie Preferences Your privacy is important to us We use cookies to enable you to use our site, understand how you use our site and to improve your overall experience. Cookies also allow us to personalize content and provide advertising that may be relevant to you. You can learn more about our cookies and manage your preferences using this tool. Please note that Strictly Necessary cookies are required and cannot be disabled. As such, they are the only cookies that are enabled by default. By continuing to use our site, you accept our use of cookies, revisedPrivacy Policy,Terms of Service andCookie Policy. ACCEPT ALL REJECT ALL MANAGE PREFERENCES",
    "commentLink": "https://news.ycombinator.com/item?id=37211675",
    "commentBody": "Upcoming .com and .xyz domain price increaseHacker NewspastloginUpcoming .com and .xyz domain price increase (namecheap.com) 217 points by nonoesp 17 hours ago| hidepastfavorite313 comments agwa 15 hours agoNamecheap&#x27;s current .com renewal price of $14.58 is broken down as: $0.18 ICANN fee $8.97 Verisign&#x27;s current registry fee $5.43 Namecheap&#x27;s markupNamecheap&#x27;s new .com renewal price of $15.88 will be broken down as: $0.18 ICANN fee (no change) $9.59 Verisign&#x27;s new registry fee (7% increase) $6.11 Namecheap&#x27;s new markup (13% increase)So the price increase is not entirely \"out of [Namecheap&#x27;s] control\". They are also increasing their markup.Edit: fixed error in Namecheap&#x27;s markup - thanks everyone for pointing that out! reply tiffanyh 14 hours agoparent$9.73 renewals at Porkbun.https:&#x2F;&#x2F;porkbun.com&#x2F;tld&#x2F;com reply agwa 14 hours agorootparentThat&#x27;s presumably going to increase on September 1 or else they&#x27;ll be taking a $0.04 loss every time they sell a .com. reply ceejayoz 14 hours agorootparentWhich they might be willing to do, if the average domain generates enough upsell revenue with other services. reply KomoD 8 hours agorootparentprevYes, it&#x27;s rising to $10.37. reply brewdad 10 hours agorootparentprevThere is definitely an increase coming for Porkbun but it is still a tiny amount. I just moved four domains from Namecheap to Porkbun. reply jfoster 8 hours agorootparentprevPricing like that makes me uncomfortable.Even if they have a tiny margin on the domain costs, that means that they are probably a loss-making business. So they plan to just sell to Google, Amazon or Microsoft in the future, and we don&#x27;t yet know which one of those it&#x27;s going to be?Even if they had a small margin, does that mean that there&#x27;s poor quality support, despite domains being mission-critical to businesses? reply jbc1 3 hours agorootparentSelling headline products at or near break even and then making your margin on add ons is a fairly standard pricing strategy. Doesn&#x27;t mean they plan to sell any time soon.I do agree that having something so mission critical be managed by someone making practically nothing off of you is concerning, but I dont think that changes all that much at double their price. Either way the second something goes wrong, you leaving as a customer is much cheaper for them than it is for someone to spend any amount of time fixing your problem.Unless you&#x27;re markmonitor big then as far as I can tell the best bet is to just go off of general sentiment about domain providers. Or maybe have it through a cloud company if you&#x27;re spending enough on other things to warrant proper tech support? reply jfoster 58 minutes agorootparentThose are great points. Agree with everything, except does Porkbun actually make a decent margin on any of their offerings? They all seem quite cheap, but it&#x27;s possible that some of the hosting could be shared out to a ridiculous degree, so it might be a bit difficult to say for sure. reply Tokumei-no-hito 7 hours agorootparentprevProbably not google. They just sold google domains off reply jfoster 57 minutes agorootparentYou&#x27;re making me nostalgic for the first time they retired one of their chat offerings... reply RexM 6 hours agorootparentprevI moved ~10 domains from namecheap to porkbun awhile back, since they were already cheaper. reply chuckreynolds 11 hours agorootparentpreveh they too will increase prices over time as namecheap has; just a matter of time reply fullstackchris 11 hours agorootparentprevWow! Porkbun! I nearly forgot about that place!Shamelessly been using Namecheap for a while now... their UI is a bit old school but they have some of the best prices around (at least did) reply Ayesh 4 hours agorootparent> their UI is a bit old schoolThis honestly is a (positive) feature IMO. reply KronisLV 2 hours agorootparent> This honestly is a (positive) feature IMO.Personally, I rather like old school UIs, since the usability is pretty good!That said, Namecheap&#x27;s website has always felt a little bit on the slow side: every page navigation for me takes at least 2-3 seconds and that&#x27;s on a pretty decent connection. Luckily I don&#x27;t have to use it daily.Overall, the prices and services that they offer seem decent to me. They are even supported well by local software like ddclient (dynamic DNS, if ever needed): https:&#x2F;&#x2F;www.namecheap.com&#x2F;support&#x2F;knowledgebase&#x2F;article.aspx...Except for their VPS hosting: there are more affordable and nicer options out there like Hetzner (as long as you can get an account) and perhaps Contabo (as long as you don&#x27;t mind setup fees). Unless you want to have everything on as few platforms as possible, I guess. reply hnarn 12 hours agoparentprevEver since I found out that Cloudflare does not charge any markup on the domains you buy from them, I&#x27;ve decided to buy all my domains from them. They are very transparent with their pricing, and also has a notice about this increase; in their case it will go up from $9.15 to $9.77 -- which seems to check out with the sum of the registry fee plus the ICANN fee. reply hk1337 12 hours agorootparentIs it just domain registration? Do you have to use Cloudflare for DNS if you register a domain with them or can you set the DNS somewhere else? reply agwa 12 hours agorootparentYes, you have to use their DNS, which is tightly integrated with their CDN in ways that are fine for some people but others may find undesirable (e.g. they automatically obtain SSL certificates for your domains). reply hnarn 11 hours agorootparentIt’s possible you have to use their DNS servers (I haven’t checked) but I know for a fact that the rest is not mandatory, because I don’t do any of it. reply CameronNemo 11 hours agorootparentprevYou can use their DNS without the CDN. reply HWR_14 11 hours agorootparentprevI&#x27;ve heard people suggest you should split registrar and dns solution, but I don&#x27;t really understand why that would be best practice. reply hotshiitake 10 hours agorootparent> I&#x27;ve heard people suggest you should split registrar and dns solution, but I don&#x27;t really understand why that would be best practice.Simplifies migration. If your DNS records are tied to the registrar, and you need to move the record (maybe selling or moving to another registrar), then you can run into a problem where the DNS records are not accessible while the name is being transferred. Not an issue if the nameserver record points somewhere else.Anecdotally, many registrars I have worked with (including Namecheap&#x2F;GoDaddy) have terrible DNS management consoles&#x2F;APIs, and limited options for access control. I have also had issues with certain TLDs not being available to move to better registrars, though I&#x27;m not sure if that is still an issue. Either way, moving DNS to a separate, standard provider definitely makes things easier to manage, especially if you are working with a lot of domains across different registrars. reply crazygringo 13 hours agoparentprevIs that just tracking inflation though? What was the date of their last markup increase, and what has inflation been since then?Presumably their employees need to be paid more to keep with inflation and all. reply agwa 12 hours agorootparentIf their costs have gone up due to inflation they should be honest about that. Their blog post implicitly blames Verisign for the price increase, which is not the whole truth. reply jimmaswell 5 hours agoparentprevSo it went up by less than $2.. per year.. and it&#x27;s still less than $20.. per year. How are people talking about switching providers over this. reply geetee 5 hours agorootparentthe principle of the matter reply jimmaswell 4 hours agorootparentIt could double and I wouldn&#x27;t be inconvenienced enough to care, different story if you have many domains though. I&#x27;m fine with NameCheap getting a little more profit for the good work they do. reply inspector-g 15 hours agoparentprevHere to point out (friendly) a typo - their markup is 13% higher now reply echelon 14 hours agoparentprevNamecheap should use some of this new revenue to fix their UI.Owning more than 10 domains on Namecheap is a burden. Trying to manage more than 50 is an outright headache [1]. I&#x27;m nearly to the point where I&#x27;m going to transfer all of my domains just to escape the poor management console. I&#x27;ve been giving them this same complaint for awhile [2].I&#x27;m no fan of Godaddy [3], but they really did a good job with bulk management and organization.Any recommendations for alternative registrars on the dimensions of price, security, TLD support, DNS, and bulk operation &#x2F; organization features?[1] I&#x27;m not a squatter. I own typos and alternative TLDs of my primary product domains, and I operate lots of websites for various side projects.[2] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29406698[3] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32470260 reply mort96 14 hours agorootparentI literally moved away from Namecheap when they did a UI redesign many years ago. Their old control panel was ugly but functional. Their new one looked much more \"modern\", but was way less information dense, and they introduced jank and made stuff take more clicks and hid stuff behind collapsed-by-default menus iirc.They also made their billing details UI no longer accept the letter \"ø\". The billing details UI which made clear the importance of making sure the name you enter matches the name on the card. And the name on my card happens to contain an \"ø\". That doesn&#x27;t exactly instil confidence. reply costco 5 hours agorootparentYou probably already know this but there&#x27;s no way for them to verify name on card. They can only verify address with AVS. reply usr1106 11 hours agorootparentprevInteresting, your passport cannot contain an ø (in the machine readable part). What is the allowed character set for names on a credit card? reply qingcharles 10 hours agorootparentThat&#x27;s interesting. I&#x27;d never thought of it before. I just tried to find images, but can&#x27;t find any example credit cards with non \"Roman\" lettering, just like passports.It sucks because the transliterations for some languages are so far off the native pronunciations. reply PascalW 13 hours agorootparentprevHave you considered managing DNS records with Terraform or Pulumi? That way you can easily automate (bulk) changes.Edit: this is possible with Namecheap as well, see https:&#x2F;&#x2F;registry.terraform.io&#x2F;providers&#x2F;namecheap&#x2F;namecheap&#x2F;.... reply jrockway 13 hours agorootparent\"Bring your own UI\" is the ideal user experience. reply skinner927 7 hours agorootparentprevThis must be a Godaddy ad or you’ve completely missed the multitude of posts where Godaddy has royally screwed over their own customer. reply echelon 6 hours agorootparentDid you read my linked post?I&#x27;ve had to acquire some domain names via Godaddy recently, and I do like their management console. I really dislike the company, though. They screwed me out of one of my domains when I was just a teenager. reply listenallyall 14 hours agoparentprev.com is the most desirable TLD by far and also one of the least expensive..io, .me, .shop, .info, .site are all more, often significantly more.Like everyone else I would love less expensive .com prices but honestly Verisign could 10x the cost of .com and only lose a mild percentage of registrations. reply gabereiser 14 hours agorootparentIt’s also the most exhausted. The min char available now is in the 7 range. Prefixes and&#x2F;or suffixes and even hyphens are being used to find land. It’s brutal. reply 3abiton 14 hours agorootparentReminds me of that south park episode. reply Jack000 12 hours agorootparentprevThere are still some good 6 letter names out there, if you&#x27;re willing to use a brandable (made-up) word. Some I randomly made just now: pletha.com encryx.com oxiply.com reply verve_rat 11 hours agorootparent> oxiply.comToilet paper infused with oxycontin?I like this game. reply Jack000 10 hours agorootparentoxycotton.com and oxyloo.com are taken unfortunatelywashluv.com is still available though reply no_wizard 11 hours agorootparentprevI thought toilet paper infused with OxiClean, for that freshness reply brewdad 9 hours agorootparent“The burn tells you it’s working!” reply eastbound 13 hours agorootparentprevHence, a x100 price hike on .com would ensure that low-cost domain parking is squeezed out.It’s not linear. A domain at $2000, which costs the squatter $14 a year, would certainly go down by 75% if it costed the squatter $200 a year. Because the prospect of keeping it for 10 years goes from $140 to $2000, so the seller would make more benefit selling at $500 today than $2500 in 10 years.A squatter who’s squatting 1000 domain would go from $14k a year to $1m. reply gabereiser 8 hours agorootparentYeah, no. That&#x27;s entirely contrary to the point of the domain name system. It was always designed with a capitalistic, first-come-first-served, mentality. Stake claim to your domain and hold it. The issue is those domains that are parked and never used should be forfeit, recycled back into the pool. DNS should work both ways. You&#x27;re free to add records, so long as they can verify usage. It&#x27;s the last part that isn&#x27;t effective right now. Some way to prove that it&#x27;s being used other than a parking service. Anything but a parking service.A few times I&#x27;ve given up squatting on a domain simply because I thought someone else would probably do better with the domain name. I was right a few times. A few made it into the hands of martech unfortunately but some of my gaming domains have now grown into decent sites under the current stewardship. It makes me happy.I also think there should be a gTLD for people. You&#x27;re own personal domain, that&#x27;s yours, and no one else&#x27;s, for life. You&#x27;re free to post up whatever within the law of your land. We already have things like social security numbers and the like. Why not have http:&#x2F;&#x2F;john.jingle.heimer.schmitt.id? Apple thought about this a long time ago and registered me.com with the thought that every apple customer would have @me.com (which they do!). There&#x27;s no clear way around this mess other than more gTLD&#x27;s. reply danjoredd 12 hours agorootparentprevIt would also make a lot of legitimate .com owners have to give up their domains. IDK that I would be able to pay that much for the domain I own for my personal blog reply gabereiser 8 hours agorootparentsame. I&#x27;ve had my personal domain name for a long time. I wouldn&#x27;t keep it if it cost me property tax. reply danjoredd 6 hours agorootparentI bought mine for about 100$ for 10 years which is about 10$ a year. If it went 10x times the price, then it would be $1000 that I would owe when it comes time to renew replymegous 12 hours agorootparentprev10x price increase would mean a lot of broken links and significant destruction of value on the web. reply smnrchrds 15 hours agoparentprev$5.43 -> $6.11 is 12.5% increase, not 1.13%. reply CameronNemo 17 hours agoprevHow is this anything but rent seeking on the part of Verisign?Edit: not surprised how this got regulatory approval... https:&#x2F;&#x2F;www.politico.com&#x2F;news&#x2F;2022&#x2F;04&#x2F;09&#x2F;website-domain-more... reply monetus 17 hours agoparent“I think calling them a monopoly at this point is an unfair comparison. Verisign is no more a monopoly than your Ford dealer is a monopoly,” Redl said. “It’s not the original days of the internet where that was the only top-level domain.”It is bothers me that the monopoly is excused this way. reply thayne 16 hours agorootparentIf you already have a .com domain that all your customers are familiar with, its not like you can just switch to a different domain. _Maybe_ I could excuse this if it was only for new registrations, but left the price for renewals the same, at least for existing domain owners. reply throitallaway 16 hours agorootparentIf you have customers familiar with your domain, annual domain renewal is an extremely negligible cost to your business. reply ivanhoe 15 hours agorootparentIf you run a successful restaurant paying for \"protection\" to mobsters is a negligible cost to keep your business running (and your kneecaps not broken), but that doesn&#x27;t make the racketeering acceptable... reply midasuni 14 hours agorootparentIf you run a successful restaurant and the landlord hikes your rent you can hardly move town.Yes it’s rent seeking. It’s also the way capitalism works. reply pwpw 14 hours agorootparentI can’t imagine a world where Google would move their primary domain from .com to .io. Over the past 20 years, I’ve witnessed numerous successful restaurants move locations due to rent hikes that are still in business in their new locations indicating continued success. I’m having trouble thinking of any major companies that have changed their domains after already being established and successful. Migrating domains feels a smidge different. reply jonas21 14 hours agorootparentThe reason it&#x27;s different is that domains are so cheap and the price increases are so small. Why would a major company (or even a tiny company) change their domain over an increase of a dollar or two per year? It doesn&#x27;t make sense.If those restaurants&#x27; landlords had hiked their rent by a few dollars per year, they wouldn&#x27;t have moved either. But it&#x27;s not uncommon to see increases of thousands of dollars per month, even for a small restaurant, when the lease comes up for renewal. reply ivanhoe 2 hours agorootparentThe problem here is how justifiable is the change in prices, not the amount. If you raise prices because you really have to and I can&#x27;t keep up, that&#x27;s OK, that means that my business is not viable anymore, I need to charge more also or cut on expenses (or go out of business, which is also OK). But if you keep milking me for more money just because you can, as I have no other options, it&#x27;s not OK at all. reply easrng 11 hours agorootparentprevDiscord moved from discordapp.com to discord.com reply brewdad 9 hours agorootparentI’m willing to bet discord.com was purchased from another owner&#x2F;squatter once their business grew large enough to justify the expense. reply zamadatix 13 hours agorootparentprevThere are some differences in the regulation of DNS assignments and real estate that makes this comparison not as direct as it may have been intended. Namely who can own the assets and if they are transferable. In real estate both of these lean much more on the open side, creating market forces which allows capitalism to work. Even if you yourself don&#x27;t own property in e.g. NYC there is more than one investment company looking to buy and sell in NYC. In DNS these assignments enforce a system of renting and registry lock in. There is little room for capitalism to do its thing when avenues for competition are removed in this way.Even seeming outs such as \"invest in a gTLD\" don&#x27;t provide the opportunity. Aside from being the virtual equivalent of \"why don&#x27;t you just go build your hair salon 50 miles out of town where nobody owns anything yet for 1,000x the cost of a building in town?\" gTLDs require being a well established organization, among other eligibility requirements, which creates a bit of a chicken and egg problem.Both do have a 3rd component of \"group good overhead\" but the IANA fees of ~18 cents don&#x27;t seem to be the problem so there&#x27;s no sense in comparing&#x2F;contrasting these differences. reply ivanhoe 2 hours agorootparentprevWell, you usually can simply move just for a few streets&#x2F;blocks, it&#x27;s very unlikely you&#x27;ll have to move out of the town...But let&#x27;s imagine if many little shops would be forced to move out of the town because of high rents - wouldn&#x27;t the town come up with some regulations to prevent that?Capitalism is fine when it benefits the overall economy, but when it starts self-imploding because of interests of a few, then government jumps in usually - that&#x27;s why we have anti-monopoly laws, because monopoly hurts the progress... reply seszett 15 hours agorootparentprevWhat if you have family or friends \"familiar with your domain\", but no paying customers? reply redog 15 hours agorootparent...charge them for dinner. -verisign reply uxp8u61q 15 hours agorootparentprevThen you send them to the new website. It&#x27;s about as much a bother as changing postal addresses. reply paulmd 15 hours agorootparentmmhmm, and what happens to their email at the old address? reply joshmanders 14 hours agorootparentI am not exactly happy about the price increase, but if the domain is this important, is $15&#x2F;yr really that bad?Like I&#x27;m flabbergasted at people complaining about the burdens of doing something nobody would do, because the cost isn&#x27;t worth the effort.Now if they raised prices to $100+&#x2F;yr I could imagine, but $15&#x2F;yr is negligible costs. reply sentientslug 11 hours agorootparentThe point is that they could raise the prices to $100+&#x2F;yr if they damn well pleased. That&#x27;s why the outrage, not over the 15 dollars reply Spivak 14 hours agorootparentprevYou tell people to use the new email. I don&#x27;t understand this, if you make a thing the root of your identity then you have get everyone to migrate when it changes. There is no solution to this problem that doesn&#x27;t involve implementing USPS mail forwarding for email. reply macintux 11 hours agorootparentThere is a solution to the problem: price caps. replyAndrew_nenakhov 16 hours agorootparentprevUnless the price suddenly increases, say, 20000x. reply Spivak 14 hours agorootparentThen you bite the bullet send out an email to your customers that you&#x27;re moving domains and why, rip the band-aid off and move on with your life. reply Andrew_nenakhov 3 hours agorootparentUnless you are hosting an email service and now all your users have to bite the bullet and send out emails to all their contacts, right. reply listenallyall 14 hours agorootparentprevIf your business is at a desirable address that customers are familiar with, and have visited for years... should you be entitled to 4 different landlords available to negotiate your next lease? Might be a nice thought but it&#x27;s not a reality. You&#x27;ve got one landlord, make peace with that, or move. reply Joeri 16 hours agorootparentprevWhen I live somewhere and the owner of the water company decides they need to increase prices 9% despite having lower costs, they are definitely exercising their monopoly, even when there are several other utilities coming to my home and so many other kinds of beverage I might acquire. reply joshuamcginnis 16 hours agorootparentHow do you know they have lower costs? My local water municipality recently increased prices citing increased pricing across the entire spectrum of equipment, materials, chemicals and labor. This in addition to state-mandated improvements to waste water management that must be completed within a certain timeframe. reply fifteen1506 15 hours agorootparentIf you&#x27;re in Britain most likely your water company has been saddled with debt [so it could pay more to its shareholders] and is in serious economical difficulties. reply davewashere 14 hours agorootparentprev\"Companies that don’t want to pay Verisign’s price hikes can choose an address ending in .biz or .info, for example.\"A real-life \"we have TLDs at home\" from someone who has probably never seen the related meme. reply eclipticplane 14 hours agorootparentprevWhen you have a monopoly, you spend all of your time explaining why you don&#x27;t have a monopoly. When you don&#x27;t have a monopoly, you spend all your time building toward a monopoly. reply benbristow 16 hours agorootparentprevIt is a monopoly. - .com domains have the brand-recognition and trust behind them. Other TLDs still feel &#x27;knock-off&#x27; especially for lesser technically aware people.Would you trust myshop.com or myshop.xyz more? reply CameronNemo 16 hours agorootparentprevAlso are Ford dealers not doing very similar anticompetitive things? Demanding high markups over msrp for electric vehicles, or refusing to offer them altogether, for example. reply paulddraper 15 hours agorootparentprevWell, I can own a Ford. reply sva_ 16 hours agorootparentprevThat comparison makes no sense.All TLDs in this case would be equivalent to all car brands. (All car brands => Ford)(All TLDs => .com) reply jonathankoren 15 hours agorootparentprevTLDs may sort of be fungible, but not really. No one looks at widgets.com and widgets.tube as identical, whereas no one really cares about one car versus another beyond feelings of personal worth. reply listenallyall 14 hours agorootparentI think Hyatt and Hilton have a lot stronger brand names than \"a friendly lady named Shiela\" but Airbnb built an empire on the latter. reply jonathankoren 14 hours agorootparentComeback when anyone builds an iconic brand around .$BRAND_NAME or even .museumSeriously. It’s not an issue of brand recognition. It’s an issue of decades of basic phishing defense. reply listenallyall 11 hours agorootparentbit.ly reply hedora 16 hours agorootparentprevWell, if you don&#x27;t like Verisign&#x27;s service, you can always move one county over, and deal with a different TLD domain registrar for .com, just like with your Ford dealer.(As an aside, I&#x27;d pay for a better-curated DNS infrastructure. For instance, google&#x27;s font domains of whatever could just resolve to something federated, and that has TLS certs that are trusted by the alternative infrastructure. Google&#x27;s chain of trust could be on a certificate revocation list.) reply wmf 15 hours agorootparentI&#x27;m not sure if this is a joke, but it&#x27;s not correct. Verisign handles all .com domains; all the different registrars are just frontends to Verisign. reply tg180 15 hours agorootparentprevI hope that someone sooner or later will be able to propose an alternative model to the current DNS infrastructure.Exceeding the current limits and the abuses that derive from it is certainly something extremely difficult to achieve, but I think there&#x27;s a market in trying to fix the status quo. reply dismalpedigree 17 hours agoparentprevAlso rent seeking by namecheap. But agreed. It’s parasitic. reply ksec 17 hours agorootparent>Also rent seeking by namecheapWhat has it got to do with namecheap when they dont own the registry of .COM or .XYZ? reply CameronNemo 16 hours agorootparentI think because the Verisign price increase is only 7%, and presumably namecheap has a margin on top of that. So increasing the price by 9% and blaming the registry could be considered unreasonable. reply skilled 16 hours agorootparentBut isn&#x27;t the 7% already agreed upon in the wholesale agreement?https:&#x2F;&#x2F;itp.cdn.icann.org&#x2F;en&#x2F;files&#x2F;registry-agreements&#x2F;com&#x2F;c...I was under the impression that the other $6 comes from Namecheap, no? Maybe I misunderstood it. reply CameronNemo 15 hours agorootparentThis comment breaks it down. Namecheap is using the opportunity to sneakily increase their markup, which is a bit underhanded IMO.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37213773 reply meltedcapacitor 15 hours agorootparentprevThey use the occasion to get more commission, by encouraging renewals further in the future to \"lock\" the current price. Presumably they get the full 10 years worth of commission today when a .com is renewed for that long. reply MuffinFlavored 17 hours agorootparentprevI&#x27;m not saying they are doing anything right or wrong but they are probably a profitable company who could have theoretically spared to absorb some (or all) of it without passing it through to their customersInstead, they are passing it on 100%. reply Vanclief 16 hours agorootparentWhy should namecheap subsidize the cost? That makes 0 sense and I say it as a namecheap customer. I don&#x27;t see how its fair for them to cover the cost and I don&#x27;t want them to do so because I am a happy customer and don&#x27;t want them to go out of business or lower their product quality. reply ksec 9 hours agorootparentPrecisely, Not to defend Namecheap but we are in a different time. I don&#x27;t know everyone&#x27;s age on HN. But If they were just 40s, they would have lived in a zero interest era for 15 years and nearly all of their professional lives. We are having inflation, cost of money going up. The margin that used to work for Namecheap may no longer works for them now. And you have plenty of option outside of NameCheap. And again, none of this could be called rent seeking.Of course there is an argument in US and Tech today ( as shown in many of the comment ) that margin is somehow evil. And should be as low as possible. I guess that is a different argument. reply MuffinFlavored 15 hours agorootparentprev> Why should namecheap subsidize the costWhy are they entitled to have their profit margins protected no matter what?https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Wage-price_spiralhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;HyperinflationCertain businesses get made obsolete over time. Namecheap decided to pass along 100% of the cost, what if their competitor passes along 50%? reply tedivm 14 hours agorootparentNamecheap is passing along 209% of their cost. They&#x27;re raising prices on their cut more than Verisign is on theirs. reply tedivm 14 hours agorootparentprevVerisign is adding $0.62, while Namecheap is increasing their cut by $0.68.They&#x27;re passing it through by 209%, not 100%. reply midasuni 14 hours agorootparentprevCan you not simply move your registration to another provider (Amazon etc) reply willio58 14 hours agoparentprevYep, digital landlord that should be abolished from the system. reply thayne 14 hours agoparentprevIf the other TLDs are actually providing competition, and TLDs are fungible, wouldn&#x27;t that drive the price down, not up? reply scrollaway 14 hours agorootparentBut TLDs are not fungible. If Apple switched to apple.name or apple.hospital because \"it&#x27;s cheaper\" you&#x27;d raise more eyebrows than you have. reply thayne 13 hours agorootparentThat&#x27;s part of my point. Since customers can&#x27;t actually replace a \".com\" domain with a domain from a \"competitor\" TLD, it isn&#x27;t actually competition.If .name was just as good as .com then if verisign increased the price, they would lose customers who would use .name instead. So the fact that verisign can increase the price when .com is _already_ more expensive than other TLDs is evidence that they do have a monopoly. reply robomartin 16 hours agoparentprev> How is this anything but rent seeking on the part of Verisign?While I generally agree with you --this smells like potential rent-seeking--, in order to be able to conclusively label it as such we have to know about the real cost structure that drives a company like Verisign. All of that information is here:https:&#x2F;&#x2F;investor.verisign.com&#x2F;financial-information&#x2F;annual-r...I wish I had the time to dive into this. I just don&#x27;t. I generally try to avoid making assertions without having done some work in support of them. That&#x27;s why I can&#x27;t reach this conclusion --I can suspect it to be true though.As a reminder, \"rent\" in \"rent seeking\" isn&#x27;t the colloquial \"rent\", as in what you pay to rent a car or a house. Economic rent, as a term of trade, is related to financial gains obtained without increases in productivity. As such it has been \"fuzzified\" to make it apply to all kinds of things that have nothing whatsoever to do with economic rent-seeking. As an example of real rent-seeking, an article in Forbes describes how writing an essay in college to obtain a grant (and maybe even admission or tuition discounts) is classic rent-seeking. Same with a company lobbying government for subsidies --they didn&#x27;t make their process or product better in exchange for the financial gain.In other words, the question here is squarely centered around the real cost structure at Verisign. Frankly, I don&#x27;t know everything they do and how much it costs to support the TLD&#x27;s they administer. I still remember when domains were free --as a fool, I didn&#x27;t register a pile of them back then.It sure feels like rent-seeking. That doesn&#x27;t mean it is. Without the proper analysis of their accounting this characterization might not be correct. In other words, it might be quite possible that they can fully justify the increase in rates due to increases in costs.Not to go too far, wages have gone up across the board (in numerical, not real terms) and inflation has made everything more expensive. Power, taxes, food, transportation, labor, etc. Every single business has had their cost structure increase, in some cases dramatically so. Given this framework, I&#x27;d be cautious about ascribing nefarious intent to any business increasing their pricing.Put a different way: A rent-seeking claim needs to include a \"Minimum Viable Financial Analysis\" in support of this conclusion. Prices going up isn&#x27;t enough evidence of this at all. Not liking price increases is no evidence at all. reply CameronNemo 15 hours agorootparentI think the onus is on Verisign to justify their price increases, not the consumer if the monopoly.So far I&#x27;ve seen them hand waive about inflation and \"demand for domains\".As of 2022, their operating margin was about 65%... That doesn&#x27;t indicate a company that needs to increase prices. reply robomartin 14 hours agorootparent> hand waive about inflationThere&#x27;s nothing whatsoever hand-wavy about inflation. It&#x27;s as real as can be, and it affects everyone and everything.> As of 2022, their operating margin was about 65%... That doesn&#x27;t indicate a company that needs to increase prices.It indicates nothing. That isn&#x27;t even close to proper analysis of their financials. As is often the case with chat room discussions, people just love to grab onto one number or one fact and use it to flog everyone to death. Well, I have news, reality --as opposed to fantasy-- is a complex multivariate problem. A single number is meaningless.Note that I am not at all defending Verisign. I know nothing about their operations and don&#x27;t have the time to dive into it. For all I know this actually is 100% rent-seeking, in the full economic sense of the term.I am defending reason and the requirement for solid analysis and justification before accusing anyone of anything. This, in sharp contrast with the typical lynching mob mentality that permeates online conversations.What I am sick of and will not do, is people just jumping at labelling things (people, businesses, etc.) because they don&#x27;t like something rather than through critical thinking, demonstrable and reproducible analysis of facts in full context. You know, like calling someone \"racist\" because they bought vanilla ice cream (not making light of real racism, but one has to admit we have taken that term to insane lengths and depths).Maybe this is rent-seeking. Don&#x27;t know. What I do know is that the claim has been made in this thread and the only real support is feelings, not well-presented evidence.Also, saying someone makes 65% operating margin as a measure of evil-ness is ridiculous. Who put anyone in charge of deciding how much margin makes someone worthy? 5%? 10%? 25%? 0%? What happens to that fake virtuous badge when things go wrong (pandemic, economic downturn) and the company has to fire half the staff because they were labeled evil for making more than 5%? Yeah, the people who lost their jobs are going to think very highly of the virtue police on that day.As someone who has founded and operated multiple businesses in the last four decades, this kind of thing really drives me up a wall. People who have never risked a dime of their own trying to make a non-trivial business go actually think they understand business. It&#x27;s both the saddest and the funniest thing seen online and, with some frequency, on HN. reply CameronNemo 14 hours agorootparentMaybe this is rent-seeking. Don&#x27;t know. What I do know is that the claim has been made in this thread and the only real support is feelingsWho made the claim?Also, saying someone makes 65% operating margin as a measure of evil-ness is ridiculousYou said evil, not me. Projection? reply teaearlgraycold 16 hours agoparentprevIf we&#x27;re going to fix issues with domains I would like to find a way to ban domain parking and \"premium\" domain prices sold by registrars.I&#x27;m fine with an individual holding a handful of unused domains, but the legislation should eliminate anyone holding domains as their primary source of income. reply scblzn 17 hours agoprevThe ICANN wholesale prices to registrars, from 1st of September are $9.59 per domain (+ $0.18 ICANN fee per domain) for registrations and renewals [1][1]: https:&#x2F;&#x2F;itp.cdn.icann.org&#x2F;en&#x2F;files&#x2F;registry-agreements&#x2F;com&#x2F;c... reply xd1936 15 hours agoparentAnd as long as you don&#x27;t need custom nameservers, Cloudflare&#x27;s Domain Registrar sells domains at that price.https:&#x2F;&#x2F;www.cloudflare.com&#x2F;products&#x2F;registrar&#x2F; reply hnarn 12 hours agorootparent> as long as you don&#x27;t need custom nameserversAre you saying that it&#x27;s not possible to use your own nameservers for domains purchased through Cloudflare? reply e12e 11 hours agorootparenthttps:&#x2F;&#x2F;developers.cloudflare.com&#x2F;dns&#x2F;zone-setups&#x2F;zone-trans...Kinda, but enterprise only?There&#x27;s also vanity DNS for business and enterprise plans - but AFAIU that&#x27;s basically just slapping another name on cloudflare DNS - not ability to point ns records to non-cf servers?https:&#x2F;&#x2F;developers.cloudflare.com&#x2F;dns&#x2F;additional-options&#x2F;cus... reply petecooper 17 hours agoprevPorkbun pricing:.com (9.73USD): https:&#x2F;&#x2F;porkbun.com&#x2F;tld&#x2F;com.xyz (9.92USD): https:&#x2F;&#x2F;porkbun.com&#x2F;tld&#x2F;xyz reply dchest 15 hours agoparentAlso increasing due to Verisign:\"We expect our pricing to change from $9.73 to around $10.37 on September 1, so don&#x27;t wait to lock in our low rate today!\" reply dewey 16 hours agoparentprevDid they react to the announcement yet and said it&#x27;ll stay that way? Otherwise that&#x27;s not very useful so soon after the announcement. reply petecooper 1 hour agorootparentIt was more a data point for compare & contrast, to be fair. I&#x27;m a Porkbun user but not blinkered to other registrars. reply greenSunglass 17 hours agoparentprevI use porkbun for 10 domains now for the last 5 years. They are great. Highly recommend reply lxe 17 hours agoparentprevWow. What is porkbun and how is it so much cheaper? Is this worth transferring over? reply mikea1 15 hours agorootparent> how is it so much cheaperPorkbun doesn&#x27;t make money when you buy a domain name, but they may make money when you do not renew it:> At about 21 days into the Auto-Renew Grace Period, the expired domain will be submitted to third-party auction services.https:&#x2F;&#x2F;kb.porkbun.com&#x2F;article&#x2F;37-what-happens-after-a-domai...Other registrars, like GoDaddy, do this too. reply shiftpgdn 17 hours agorootparentprevDomains are a loss leader product for everyone in the internet&#x2F;hosting industry. Porkbun’s goal is to get you to buy secondary products. reply blairbeckwith 16 hours agorootparentThis is true, but Porkbun couldn&#x27;t be further from the big registrars like GoDaddy in their approach. It is actually difficult to buy a domain on GoDaddy for most people without getting sucked in to buying extras, whereas Porkbun barely even suggests extras at any point since I&#x27;ve been a user, and what are extras other places are offered for free. reply morkalork 17 hours agorootparentprevPretty much. Everyone wants to upsell you on their hosting, page creation and blogging tools where they have ridiculous margins. How much disk space and bandwidth do you think a random small business or Jane Rando&#x27;s recipe blog actually use vs. what they&#x27;re being charged for? Not to mention things like \"brand protection\" where they&#x27;ll do you a solid and buy&#x2F;squat your domain on the .xxx and .sucks tlds for extra $$ each month. reply tiltowait 15 hours agorootparentprevI&#x27;ve been slowly migrating all of my domains over and have been very happy with them. Lower prices, faster website, almost ridiculously clean interface. Even has passkey support. reply ineedtosleep 17 hours agorootparentprevFWIW, I like their pricing and overall marketing approach and transferred my domains over a few months ago. Great experience overall. reply verst 17 hours agoparentprevFor now - but surely they too will increase as part of this. What are the rates after the increase which will impact every registry? reply judge2020 17 hours agorootparentEvery registrar*Registry is like verisign owning .comRegistrar is all of the people who sell you .com domains, by having a contract with the registry. reply verst 12 hours agorootparentFunnily I even looked up this distinction before writing because I very much know of the difference (see my comment history) but wrote exactly the wrong one. Oh well. Yes I meant registrar, not the TLD registry. reply BXlnt2EachOther 15 hours agorootparentprevCloudflare upcoming price changes -- yes, they like other registrars will be affected.The announcement page might be behind a login, couldn&#x27;t seem to link it directly .com $9.15 -> $9.77 .xyz $9.33 -> $10.18 .org $10.11 (today&#x27;s price... not affected?) .net $10.10 (today&#x27;s price... not affected?) reply bityard 16 hours agoparentprevCloudflare pricing:.com (9.15).net (9.95).org (10.11).xyz (9.33) reply hobs 17 hours agoparentprevJust switched to porkburn for a bunch of domains after google sold to squarespace, works great. reply 9g3890fj2 17 hours agoprev.XYZ domains were already too difficult to use for anything other than a regular site since they have such a bad reputation (however warranted it may be) as being used for spam. Not sure what the point is in paying even more for a TLD that&#x27;s discriminated against by default. reply poyu 15 hours agoparentI&#x27;ve been using .xyz as my primary email address. Everything from banking to shopping, to governmental stuff. The number of sites that gave me problems is probably less than 10 that I can remember. I have around 700 accounts in my password manager so go figure. reply 9g3890fj2 15 hours agorootparentIt&#x27;s not the receiving that&#x27;s the problem, but the sending. Even with all necessary records in place and using a reputable email provider isn&#x27;t enough in a lot of cases. You&#x27;ll just end up in spam. reply echelon 15 hours agoparentprev.xyz is one of the most popular TLDs for crypto and web3, but it has a horrible email spam reputation and some gateways blacklist the entire TLD. With the exception of a16z, VCs aren&#x27;t funding these businesses much anymore, either.https:&#x2F;&#x2F;techcrunch.com&#x2F;2021&#x2F;12&#x2F;28&#x2F;wtf-is-xyz&#x2F;The biggest user might be block.xyz..io, .co, and .ai are still the most popular alternative TLDs for startups. reply stOneskull 14 hours agorootparent> .io, .co, and .ai are still the most popular alternative TLDs for startups.is there a good link for this ranking? i&#x27;m curious as to 4th place and below too. what position does .app or .tech or .cloud come, etc. reply kristopolous 17 hours agoprevI&#x27;m ok if this leads to de-squatting.I wish there was some kind of bulk price increase.I know all the issues but if you own 1,000 domains you&#x27;re just sitting on, that 1,001th you&#x27;re trying to snatch should be more expensiveHoarding domains for ransom shouldn&#x27;t be a business modelI guess another model is you could regulate the transfer and selling of domains to a certain cap. If the most you can legally get is say $5000, then people wouldn&#x27;t collect and squat in such giant volumes reply autoexec 17 hours agoparent> I&#x27;m ok if this leads to de-squatting.It wont. If anything it&#x27;ll just consolidate the squatted domains into fewer hands.Progressively increasing prices for each domain purchased seems pretty reasonable, but unless it raises very quickly it&#x27;ll still end up being worth it to a wealthy few. Combining those raising prices with capping the resell price of domains seems like it would actually work! Someone somewhere might find it worth it to buy their 600th domain at a huge price, but if they can only sell that domain for a small fraction of what they paid for it they&#x27;ll lose money if they aren&#x27;t planning to use it themselves. reply kristopolous 16 hours agorootparentThere&#x27;s a balance I&#x27;m trying to strike. If you&#x27;re sitting on say, news.com then I get it, that&#x27;s a good asset to have.It&#x27;s those other groups that throw combinatoric dictionaries at the registrars that force the latest round of startups to have a bunch of letters smashed together for a name, they&#x27;re the problem.I&#x27;ve got half a mind to just go with katakana for my next company, register a domain like ツイッター.com and just say \"well, there&#x27;s 46 characters. You can memorize it in like 2 weeks. Not my problem!\" reply Spivak 14 hours agorootparentprev> It wont. If anything it&#x27;ll just consolidate the squatted domains into fewer hands.This is actually great if your end is to get rid of squatting, consolidate the market into a number of throats that&#x27;s feasible to choke and then do it via regulation. reply autoexec 12 hours agorootparentSeems like it&#x27;d be easier to get throat choking regulation in place before ownership is consolidated into a small number of people with deep pockets, vast resources, and disproportionate influence. The nice thing about policy is that it can be applied to huge numbers of entities instantly. Why risk creating behemoths too big to fight before you even start? reply skinner927 7 hours agorootparentprevRegulations have never worked when there are few throats to choke. The few either band together and create their own regulatory association or bribe, sorry, lobby their way out of it. reply AnthonyMouse 16 hours agoparentprev> If the most you can legally get is say $5000, then people wouldn&#x27;t collect and squat in such giant volumesQuite the opposite. They squat on giant volumes so they can stick ordinary people for $2500 instead of $10.They should just prohibit selling domain names. It would solve 99% of the problem because then they couldn&#x27;t use domain parking pages or otherwise openly offer them for sale.Someone would still manage to sell million dollar domains by some subterfuge where it&#x27;s claimed to be part of the sale of a company, but that was never the problem and has enough overhead to make it uneconomical for the low value domains that cause them to register every plausible variant of English text. reply arp242 16 hours agorootparent> They should just prohibit selling domain names.So Joe Squatter will retain ownership, but permits company.com to use it for 99 years for the same price as he would have sold it for.Or something like that... I don&#x27;t really disagree with you as such, but where there&#x27;s a will, there&#x27;s way, and never underestimate the creativity and twattery people will come up with to make a buck. Banning this will be hard, and I&#x27;m not sure it&#x27;s worth the downsides. reply kristopolous 15 hours agorootparentit&#x27;s kind of an internet georgism https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Georgism reply AnthonyMouse 15 hours agorootparentThe problem here is that it&#x27;s really artificial scarcity. The true value of the names is negligible because there are plenty of good names that aren&#x27;t actually in use, but they are registered.It&#x27;s like selling access to sunlight. There is more than enough to go around until some jackass puts a massive wall in the sky and wants to charge money to remove it. reply AnthonyMouse 15 hours agorootparentprev> So Joe Squatter will retain ownership, but permits company.com to use it for 99 years for the same price as he would have sold it for.Then company.com is going to object that they would lose the domain they&#x27;ve had for 99 years or be subject to an extortionate price increase, because they really actually do want to own it. And you can prohibit leasing domain names too.I mean this isn&#x27;t that hard. You know who these companies are. You prohibit sales, you go to the website of the company trying to sell a million domains and see what scam they&#x27;re running now, and two hours later you prohibit that too. Eventually they&#x27;ll make a mistake and do something which is explicitly prohibited and forfeit all of their domains.\"We should not attempt to solve this problem because the first attempt might be less than 100% effective\" is pointless defeatism. So what if they come up with a way around it? Prohibit whatever they do until they go out of business. reply arp242 12 hours agorootparent> company.com is going to object that they would lose the domain they&#x27;ve had for 99 years or be subject to an extortionate price increaseThen do a perpetual lease, or 999 year lease, or pay every year, or [...]How would you enforce a prohibition of leases? Plus, the tricky bit here is there are lots of reasons for a domain owned by entity A to be used by entity B (companies might be related, might just be allowing a friend to use a domain, etc.)I don&#x27;t think it&#x27;s defeatism at all, by my estimation it will do very little or nothing at all, while creating a hassle for everyone. I guess I could be wrong about that, but that&#x27;s what I&#x27;d expect. reply AnthonyMouse 3 hours agorootparent> I don&#x27;t think it&#x27;s defeatism at all, by my estimation it will do very little or nothing at all, while creating a hassle for everyone. I guess I could be wrong about that, but that&#x27;s what I&#x27;d expect.We have a longstanding counterexample: Trademarks. You can&#x27;t register a trademark without using it and you can&#x27;t sell it.And the latter is practically in name only -- you can sell the \"goodwill\" associated with the trademark and transfer the trademark with it.Yet with as little as that, we don&#x27;t have companies squatting on millions of trademarks or shaking down small businesses who just want to use a name nobody is already using. reply AnthonyMouse 6 hours agorootparentprev> Then do a perpetual lease, or 999 year lease, or pay every year, or [...]A perpetual or 99 year or 999 year lease is constructively a sale. Anything which is constructively a sale is banned.Anything which is not constructively a sale will be rejected by the buyer, because they actually want to own the domain and not pay the danegeld to the squatter in perpetuity or be subject to extortionate price increases after they&#x27;ve committed to using the domain by publishing it in their advertising etc.> How would you enforce a prohibition of leases?If you offer to provide someone with use of a domain name in exchange for more money than you pay to the registrar to register it, you lose the domain name.> Plus, the tricky bit here is there are lots of reasons for a domain owned by entity A to be used by entity B (companies might be related, might just be allowing a friend to use a domain, etc.)You can use it all you want as long as you never pay them for it.It doesn&#x27;t matter if you can get around this with complicated corporate shenanigans because you can&#x27;t sell that to arbitrary strangers who don&#x27;t actually want to pay a premium for an unused domain.If you advertise it in such a way that strangers understand you to be constructively selling the domain then they report you to the registry and you lose the domain, which then becomes unregistered and they can go register it for the ordinary nominal fee. If you don&#x27;t advertise it in such a way that strangers understand you to be constructively selling the domain then you will have a much harder time finding anyone to pay you for it, as intended.This isn&#x27;t like drugs where both the buyer and seller want the transaction to happen. The seller wants the transaction to happen and the buyer wants the seller to go out of business and DIAF so the domain will be unregistered and they don&#x27;t have to pay a shakedown tax for it. It&#x27;s optimized for putting the sellers out of business because that&#x27;s what the buyers want -- because the sellers provide no value to anybody and nobody but themselves wants them to exist. reply kristopolous 15 hours agorootparentprevI appreciate the ideals. I&#x27;m entertaining the idea that there&#x27;s a non-asshole way to do domain brokering that&#x27;s more like a store and less like a hostage negotiation. Maybe that&#x27;s not possible. reply AnthonyMouse 15 hours agorootparentWhat is the point of domain brokering? A company shows up to squat on a domain nobody was using and then extort a premium when someone comes along who actually wants to use it. It&#x27;s pure useless rent seeking. They add no value to anything, they merely tax the public in exchange for nothing. reply kristopolous 13 hours agorootparentthat argument can be given to stock trading, art and stamp collecting, and basically any speculative investment. Are you suggesting there should be none of that? reply AnthonyMouse 6 hours agorootparentThe value of stock speculation is that it gives the original owner a way to raise capital for a company that could make money later but doesn&#x27;t yet. It&#x27;s how founders and growing businesses raise money. And the prospect of speculators being able to sell to future speculators makes the original speculators willing to pay more to the business itself.A speculator in these markets also has the incentive to preserve the art or stamp and protect it from damage. Which also applies to stocks, because the speculator has the incentive to vote their shares in a way that maximizes the value of the company.None of that is happening with domain names. The squatter didn&#x27;t create the name, no one did and no one needs to be compensated for that. If no one \"maintains\" the name it doesn&#x27;t degrade in any way, it just goes unregistered until someone wants to actually use it. replypetecooper 17 hours agoprevPorkbun pricing:.com (9.73USD): https:&#x2F;&#x2F;porkbun.com&#x2F;tld&#x2F;com.xyz (9.92USD): https:&#x2F;&#x2F;porkbun.com&#x2F;tld&#x2F;xyz reply jl6 16 hours agoprevDoes ICANN have a position on whether domain name prices should be high (e.g. to discourage squatting), or low (e.g. to avoid being a rent&#x2F;tax)? Because the price seems entirely driven by whatever ICANN wants it to be (by virtue of assigning the monopoly to Verisign, with an ICANN-defined cap on price rises), rather than any market mechanism.For example, if a startup approaches ICANN saying they can manage the .com registry while charging only $1 per domain per year, is that attractive to ICANN? reply mikea1 12 hours agoparentI think ICANN would not entertain a startup taking over registry operations for .com or .net. ICANN recently defended their no-bid contract renewal for .net with Verisign:> If ICANN were to put every TLD out for bid every renewal cycle to give it to the lowest bidder there would be no incentive for registry operators to invest in long-term stability and growth of the TLD(s) they operate.https:&#x2F;&#x2F;domainnamewire.com&#x2F;2023&#x2F;08&#x2F;16&#x2F;icann-says-putting-tld...I find it odd that \"growth\" is a justification. I was unaware that ICANN has a mandate to promote growth of specific TLDs.These justifications, especially of \"stability\", make more sense to me in the context of the root DNS server that Verisign operates. Verisign would not agree to run a critical part of DNS infrastructure without a big TLD contract. I&#x27;m certain that maintaining and running those DNS servers with 100% uptime is an engineering accomplishment and its stability requires safe hands. reply ted0 12 hours agoprevTed from Namecheap here. We have launched a new beta registrar, Spaceship.com, which has wholesale pricing on most extensions. A reminder that wholesale pricing is set by each TLD’s registry, not us.Would love your feedback on this early version of Spaceship. reply dawidpotocki 9 hours agoparentYeah, nah.What&#x27;s the point of this? If you think this pricing model is sustainable, why won&#x27;t you just lower the Namecheap prices?It feels like a bait. What service are you gonna launch after you jack up the prices to the moon on Spaceship? How about Submarine?Also I already noticed deceptive pricing, ICANN fees are added in the checkout instead of being in the listed price the domain. reply geetee 12 hours agoparentprevIf I&#x27;m leaving Namecheap, I&#x27;m leaving Namecheap. reply ted0 11 hours agorootparentWell fair but I welcome you to at least check it out! It’s an entirely new codebase and platform. reply dbbk 11 hours agoparentprevI am very confused by this. What&#x27;s the purpose? Why not just improve Namecheap? reply ted0 11 hours agorootparentIt’s an entirely new brand and platform that’s been in the works for a while. reply qingcharles 10 hours agoparentprevLong time, big user of Namecheap here.I&#x27;m so confused about Spaceship.Why would I use Spaceship over Namecheap, and vice-versa? reply hnarn 12 hours agoparentprevspaceship.com has a \"sale\" on .com domains, $7.88 now and $8.80 list price. since both of those are lower than the current fee of $9.15, how is anyone expected to trust that the price will not increase in the future if you&#x27;re selling them at a loss? reply ted0 11 hours agorootparentWe are committed to having competitive pricing at Spaceship for the long term. If you’re concerned, you can also lock in current pricing with multi-year registration.Fwiw, you could say the same about anyone else in the market. Wholesale pricing is controlled by the registry, retail pricing can be updated by the registrar at any time. reply ShadowBanThis01 11 hours agorootparentNo. You&#x27;re creating a pain in the ass for Namecheap customers. Your only chance of keeping my business (and probably that of other pissed-off customers) is to give us a one-click way to migrate all of our domains to your new \"less of a rip-off\" brand.I&#x27;m not about to reward a company that WASTES MY TIME by shuffling my business between THEIR brands. I will make damned sure that my next registrar has nothing to do with Namecheap. reply ted0 11 hours agorootparentOne click migration is not out of the question. Super early days for Spaceship. There’s no shuffling happening here, I’m simply letting you know about a new platform we’re excited about. reply ShadowBanThis01 1 hour agorootparentYou&#x27;re holding out \"Spaceship\" as an alternative, which absolutely DOES mean shuffling our registrations from Namecheap to Spaceship.Now you&#x27;ve really insulted us, so I don&#x27;t give a shit if you offer migration or not. I&#x27;m gone. replyShadowBanThis01 11 hours agoparentprevI consolidated my domains on Namecheap, but this behavior pisses me off enough to move ALL of them. And I have dozens. Right after I post this screed I&#x27;m going to deactivate auto-renewal on every goddamned one.Don&#x27;t rip us off and then launch a new service and point to it saying, \"Hey look, since you caught us ripping you off, try this one!\"Unbelievable. reply ted0 11 hours agorootparentHow would you call this ripping you off? Seems like quite the stretch here. Namecheap is a business — we can choose to take a markup on registrations, which by nature is transparent given that wholesale prices are public.Spaceship has been in the works for years and it’s something we’re excited about, hence why I’m letting you know about it. reply ShadowBanThis01 1 hour agorootparentYou&#x27;ve been called out right here for pretending that the price increase is out of Namecheap&#x27;s control, when in fact the vast majority of it is Namecheap&#x27;s markup.So that&#x27;s how I call it ripping us off. reply nerdjon 15 hours agoprevI have been a bit reluctant to complain too much about prices going up recently since in many ways it feels like a hopeless battle.But in this particular case, I don&#x27;t quite understand why Verisign needs to increase the cost of this. I can&#x27;t imagine the infrastructure costs are really that high for this. So I am really curious where this extra money is really going?Sure it&#x27;s not much, but I would still like a justification as to why.I am also curious that I can&#x27;t find any information about AWS increasing their cost for .com domains. They are still sitting at $13 for both .com and .xyz reply sigio 11 hours agoparentThey dont _NEED_ to raise their pricing, but the contract they have with ICANN states they _MAY_ raise their prices with a fixed percentage, which means as much that they WILL raise their prices with this percentage every year. It&#x27;s not like you are going to migrate your domains to a different TLD. reply swozey 17 hours agoprev.xyz was a terrible TLD. I used it a few years ago and giving people my email address was so annoying. Even some logins didn&#x27;t accept it. No benefits whatsoever.\"yourname@domainxyz.com??\" reply CameronNemo 17 hours agoparentOh but if you don&#x27;t like .com name prices you can always buy a .biz name! &#x2F;s reply jimmySixDOF 17 hours agoparentprevI have a .live domain I am afraid to advertise for the same reason. But nowadays .xyz has become popular with anyone involved with 3D and it was good value. I swapped everything to Cloudflare as Registrar following advice on HN and their policy not to raise the rates but not much you can do about this kind of thing. reply swozey 16 hours agorootparentI use .dev now. It&#x27;s not much better tbh. I usually have to spell .dev \"like developer,\" but I think people get it a lot more quickly than they did xyz. It&#x27;s been awhile since I&#x27;ve had to say it to anyone though. reply stOneskull 14 hours agorootparentbe a .guru reply nonoesp 17 hours agoprevWhat about CloudFlare&#x27;s \"at-cost pricing for registration and renewal,\" they don&#x27;t make a profit? [1][1] https:&#x2F;&#x2F;www.cloudflare.com&#x2F;products&#x2F;registrar&#x2F; reply nimish 17 hours agoparentNot off domain registration, clearly. It&#x27;s a loss-leader and gets you in their product. Significantly cheaper than any of the \"low cost\" registrars I&#x27;ve seen for `.net` and I already used them for DNS anyway. reply garganzol 16 hours agoparentprevI find things like this to be a shady business practice. Not antitrust level shady, but still. For customers it&#x27;s a win, but it kills competition long-term, turning the mid-term wins into a long-term loss. reply Kwpolska 17 hours agoparentprevTheir prices are going to increase too (by $0.62 if my math is right). reply squokko 17 hours agoparentprevI wouldn&#x27;t want my domain to be a marketing expenditure for a tech giant. Then you get a Google Domains style shitshow. reply benabbottnz 4 hours agorootparentIs Cloudflare known for constantly starting up and shutting down products left, right, and centre though? reply bsimpson 16 hours agoparentprevhow hard is it to transfer?i was part of the godaddy > namecheap exodus, and it was such a pain in the ass that i never wanted to touch it again. reply schemescape 15 hours agoprevIs there a TLD that has a reasonable chance to stay low cost?I just want a domain for hobby projects and I don’t really care which TLD as long as it’s short, but I don’t like having to guess which TLDs are trying to lure people with low prices, only to suddenly raise prices in the future (I’ve seen this happen a few times).I’d also like to use a TLD that is unlikely to be bought by private equity (I vaguely recall this being attempted with a popular TLD; edit: it was .org, but the purchase got blocked after an outcry).I just heard that .tk allows free registration, so that seems promising, but I doubt it will last since there are real costs involved.For now, I’m sticking with .com because it’s the most popular and I’m hoping that, as such, price hikes will cause enough outrage to be addressed eventually, but I’m hoping there’s a better option! reply donmcronald 15 hours agoparentI would say .com, .net, and .ca if you happen to live in Canada.> I’d also like to use a TLD that is unlikely to be bought by private equity (I vaguely recall this being attempted with a popular TLD).That was Ethos Capital and .org IIRC. They’ve managed to acquire two major registry operators [1]. A huge number of the new gTLDs are under that umbrella. The new gTLDs are a great idea, but (IMO) the poor management and greed that comes along with private equity makes them too risky to use.1. https:&#x2F;&#x2F;ethoscapital.com&#x2F;portfolio&#x2F; reply schemescape 15 hours agorootparentThanks! Why .net? It seems to be owned by Verisign but isn’t as critical as .com, so probably more likely to be exploited. reply donmcronald 13 hours agorootparentI don’t know this for sure without looking it up, but I include .net because I think .net and .com are both using a legacy registry agreement. Everything else uses a new agreement that has fewer protections for registrants. For example, there are no price caps on the new gTLDs IIRC. reply schemescape 13 hours agorootparentThank you! reply sigio 11 hours agoparentprev.eu is quite cheap, and will most likely stay this way, same for many cc-tld&#x27;s like .nl and .de reply stOneskull 14 hours agoparentprev> .tk allows free registration, so that seems promising, but I doubt it will last.tk has been free for years and years. i think it&#x27;s lasted ok. it won&#x27;t be good for search engine results though, and probably isn&#x27;t good to use for email. it&#x27;s good if you have no money.. like those free1000.hostingforyou.net hosts. you can put together a warez site from the street! reply schemescape 14 hours agorootparentThat’s a shame that .tk is getting abused, but not surprising.I guess, in addition to maintaining the database, a TLD probably needs to prevent abuse to be a good citizen. Add in fees and I have no idea what a reasonable price would be. Maybe I’ll stop complaining… reply dbbk 11 hours agorootparentprevAh that takes me back to being 13. reply hysan 17 hours agoprevHow do other registrars compare with Namecheap? I&#x27;ve been using them for a long time and haven&#x27;t had any problems with their services, so I stopped keeping track of how the industry has evolved. reply mplewis 17 hours agoparentPorkbun has provided me better service at an extremely competitive price. reply kyrra 16 hours agorootparentI just moved all my domains there. Porkbun&#x27;s UI is fast and simple, I love it. reply MerelyMortal 17 hours agoparentprevI like&#x2F;use Dynadot: cheap, free privacy, and they pass on the ability to domain taste (get refunds within 72 hours). reply stavros 16 hours agoparentprevI use Cloudflare, they rent their domains at cost, and provide privacy by default. reply ntac 14 hours agoparentprevI can&#x27;t say how they compare, but I&#x27;ve had no complaints with Domain Monger in 20+ years. reply greatgib 3 hours agoprevAnd just to support how much verisign business is racketeering based on their Monopoly, look at their financials: https:&#x2F;&#x2F;www.macrotrends.net&#x2F;stocks&#x2F;charts&#x2F;VRSN&#x2F;verisign&#x2F;fina...They have stable negligible costs. R&d is just symbolic. Their biggest costs are administration fees that we can probably explain by their executives paying themselves q lot more than what their deserve.The biggest part is just pure revenue. And they try to increase that consistently without valid reason. reply tiku 16 hours agoprevThose database records are getting expensive. Guess I got to charge my clients more as well. reply durpleDrank 17 hours agoprevI wanted to share my experience with Namecheap over the years. I made the switch from GoDaddy to Namecheap back in the day when they launched a notable campaign against elephant poaching around 2009. At that time, Namecheap seemed like a solid choice, even if it meant paying around $100 annually. However, times have changed, and my opinion has shifted.Lately, I&#x27;ve noticed that shared hosting with Namecheap has lost its edge. The performance has taken a hit, making it hard to justify the cost. Notably, the speed has slowed down significantly, and there are certain limitations on access that were not there before. Unfortunately, the support, which used to be a strong point, has also declined.As a result, I&#x27;m currently in the process of migrating most of my content away from Namecheap. I&#x27;m on the lookout for an alternative hosting provider that offers a robust and affordable package without compromising on speed. If any of you have recommendations for a hosting service that strikes that balance between quality and affordability, I&#x27;d love to hear about it. reply kwanbix 16 hours agoparentYou are mixing domain registration with shared hosting.Did you ever have a problem with domain registration? reply CharlesW 16 hours agorootparentI have. I bought a great name from them, after which they voided the purchase and made it a $1,500 premium domain. Obviously, I will never buy domains from Namecheap again. reply 0x0000000 16 hours agorootparentThat&#x27;s on the registry, not the registrar. Point your anger at whoever owns that TLD. reply CharlesW 16 hours agorootparentSo does your response mean (1) \"take it easy on Namecheap because this kind of bait-and-switch is common among registrars\", or (2) \"take it easy on Namecheap because it was unintentional incompetence\"? reply 0x0000000 15 hours agorootparentI&#x27;m saying take it easy on Namecheap because it&#x27;s out of their control, and would have happened regardless of the registrar you used for that particular domain. The incompetence is on the part of the registry, for not having properly identified the given domain as premium until post-registration. reply CharlesW 14 hours agorootparent> I&#x27;m saying take it easy on Namecheap because it&#x27;s out of their control…If we assume that Namecheap is incapable of malice, then it means Namecheap&#x27;s systems and&#x2F;or processes are broken. (Registry agreements don&#x27;t allow registries to retroactively reclassify domains as \"premium\".) reply 0x0000000 10 hours agorootparent> Registry agreements don&#x27;t allow registries to retroactively reclassify domains as \"premium\".On what do you base this? Registration agreements allow quite a lot in favor of registrars and registries. replyMs-J 16 hours agorootparentprevI&#x27;ve had a problem with domain registration when I purchased a .com and they asked for all kinds of business licenses and asking what the purpose of the domain is. Obviously I was shocked as they have no right to ask for the purpose of the domain and I explained that this was not for a business but for a privately administered website.My registration was being denied again after back and forth with support, without a refund as well. I eventually had it escalated to their legal team and they were able to clear the issue up and offer me an extension on the registration for the hassle.I&#x27;m happy that they corrected their errors and there hasn&#x27;t been issues since, but that type of process was beyond what I&#x27;m willing to go through as a customer. I then registered a very similar domain name using one of the largest providers without any incident.Edit: Does anyone have a recommendation for a registrar that just completes the registration and doesn&#x27;t \"flag\" domains or ask silly questions? reply CameronNemo 15 hours agorootparentDid you accidentally register as a business instead of an individual?I can&#x27;t imagine why that happened, but I can say that my registrations have never been flagged and have always been straightforward. reply kalleboo 4 hours agorootparentMy guess is his domain name contained a phrase that triggers US sanctions alarms. Things like \"NICO\" etc (don&#x27;t put that in the reason for a bank transfer or it will get blocked) reply Ms-J 13 hours agorootparentprevNo, not a business. reply Vox_Leone 15 hours agorootparentprev>Did you ever have a problem with domain registration?Recently, I painstakingly created a five-letter domain name, and researched extensively to see if it was already taken [I don&#x27;t use registrar lookup and advise you not to either]. I verified that it was totally free and unpublished.When using the namecheap control panel to register it, I was advised that it was a \"premium\" domain [why thanks!] and that, therefore, I would have to pay a corresponding amount.Summary: My beautiful domain was simply hijacked and if I&#x27;m ever to register it I will have to participate in an auction. I was able to register the corresponding domain without problems, on equal terms with all other competitors, through Registro.br[0] - although it can&#x27;t be as cool as a pure .com domain.Would you consider this a problem?The domain registration industry in the United States is completely prostituted and I&#x27;m not happy to say it.[0]https:&#x2F;&#x2F;registro.br reply donmcronald 13 hours agorootparent> When using the namecheap control panel to register it, I was advised that it was a \"premium\" domain [why thanks!] and that, therefore, I would have to pay a corresponding amount.The registry sets those premium prices, not the registrar (aka Namecheap). reply CameronNemo 15 hours agorootparentprevI&#x27;m not sure what you&#x27;re claiming here.Namecheap has asserted continuously that they do not front run domain registration, and they only charge premium prices for domains when the registry demands it. reply TimCTRL 15 hours agoparentprev> If any of you have recommendations for a hosting service that strikes that balance between quality and affordability, I&#x27;d love to hear about it.CloudFlare reply marvinblum 16 hours agoparentprevHetzner reply rewmie 16 hours agoparentprev> Lately, I&#x27;ve noticed that shared hosting with Namecheap has lost its edge.I&#x27;m not aware of anyone paying the likes of namecheap for hosting. I don&#x27;t even understand the thought process involved. reply greatgib 4 hours agoprevThese increases are so frustrating. Verisign is making so much money basically selling you an idea with almost zero cost.I&#x27;m surprised that there wasn&#x27;t an internet revolution about that. But probably the bad apple&#x2F;corrupted part here is the ICANN that let it go. We got some proof of that with the .org case. reply ajonit 15 hours agoprev.com is not going anywhere.Verisign sits on gold with the most popular TLD on the planet. Any registry, if offered a chance, will takeover .com in a heartbeat at any price.Continuous price increase is unwarranted. reply p1mrx 15 hours agoparentIt seems you&#x27;re describing exactly why the price increase is warranted. reply CameronNemo 15 hours agorootparentNo, he is describing a situation where a single company has monopolized the stewardship of a common resource, and the government has not sought out alternative bids despite that being best practice. reply tomschlick 17 hours agoprevCloudFlare is only increasing from $9.15 to $9.77 reply politelemon 16 hours agoparentHow are Cloudflare and Porkbun keeping the cost still &#x27;low&#x27;, isn&#x27;t everyone affected by Verisign&#x27;s increases? reply patmcc 16 hours agorootparentCloudflare wants you on their platform so they can sell you everything else - it&#x27;s a loss leader. Porkbun I don&#x27;t know.Namecheap, I think domains are their primary business - they need to make money on them. reply Someone1234 16 hours agorootparentCan I say something stupid but maybe true: Do you want to be on a domain registrar for whom registration is a loss-leader or their core business?Like, I like cheaper too, but even just in the last few years I can think of a few \"loss-leader\" registrars who decided they no longer wanted that division operating at a loss and scrapped it. Google Domains being the most popular example[0], but Gandi[1] is another where they operated at a loss then got acquired with huge price increases.Disclosure: I&#x27;m on NameCheap and AWS&#x27;s Route53.I&#x27;ve seen way too many NameCheap alternatives come & go or turn \"evil.\" If NameCheap starts being sketchy or misleading, I&#x27;ll move, but that hasn&#x27;t happened yet. I want a rock solid registrar for tens of years, with a business strategy that supports that, rather than saving $4&#x2F;year.[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Google_Domains[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Gandi#History reply patmcc 16 hours agorootparentYep, I&#x27;m still on NameCheap, for all the same reasons. They&#x27;re not the cheapest any longer, but I&#x27;ve also never had an issues with them. And I like knowing that they&#x27;re making a profit on me - keeps them likely to continue doing what they&#x27;re doing. reply Spivak 14 hours agorootparentprevGiven you&#x27;re on Route53 I&#x27;m surprised you would even bother with anyone else. Route53 is hands down the best registrar no questions asked. They have no side hustle to auction off domains that fall off renewal and all their customers would be pisssed if that ever happened. They&#x27;re a company where the incentives actually align. reply stOneskull 14 hours agorootparentprevbefore i moved to porkbun, i had 12 domains. now i have 23. their great domain search, their friendly, funny words everywhere, and cute graphics.. it gets people to buy more domains than they normally would, and not want to move away. that&#x27;s my guess.edit: here is their announcement..august 8 - https:&#x2F;&#x2F;mailchi.mp&#x2F;porkbun.com&#x2F;porkbun-unwrapped-whats-in-st...\"We expect our pricing to change from $9.73 to around $10.37 on September 1\" reply hinkley 16 hours agorootparentprevI would think DNS registration is a value-add&#x2F;loss leader for Cloudflare, not something they anticipate large numbers of users taking advantage of without signing up for other Cloudflare services, no? reply marcosdumay 16 hours agorootparentprevI expect Cludfare to operate at a loss. Porkbun is harder to explain, maybe they will announce price increases soon. reply timbit42 8 hours agorootparentAt least two other comments on this post says their prices are also going up soon. reply Sytten 17 hours agoprevI just increased mine to the max of 10 years, I guess I should have done that anyway just for security. What happens though if namecheap goes out of business during that? reply scblzn 17 hours agoparent\"What happens though if namecheap goes out of business during that?\".COM agreement between registrars and ICANN requires registrars to regularily store all registrant contact infos in IronMountain and set ICANN as escrow, therefore allowing ICANN to contact all registrants should a registrar fold and allow them to transfer to another registrar. Another reason to keep the domain ownership informations up-to-date. [1][1]: https:&#x2F;&#x2F;www.icann.org&#x2F;en&#x2F;system&#x2F;files&#x2F;files&#x2F;rde-agreement-09... reply pests 17 hours agoparentprevNamecheap can hand over their records to another registar before they go out of business or ICANN can step in after the fact and give their domains to another registar to manage. The new registar would likely reach out to introduce themselves. reply dathinab 17 hours agorootparentAdditionally there is a good chance that a different registrar would buy it up to acquire the customers and then transfer them to their system, etc. reply fckgw 13 hours agorootparentThis happened several years ago with a registrar I had at my domains at called RegisterFly. Upper management fraud and fighting led to the company spiraling, ICANN accreditation getting canceled and the site shutting down. ICANN transferred all ownership over to GoDaddy at the time.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;RegisterFly reply bjord 17 hours agoparentprevYou can transfer it elsewhere at any point and you&#x27;ll still get credit for the years you&#x27;ve paid for (though the receiving registrar will likely make you buy at least one additional year). reply shiftpgdn 17 hours agoprevThe people on here complaining about a $1&#x2F;year price increase while earning $400,000&#x2F;year as a software engineer never fail to make me laugh. reply Ekaros 16 hours agoparent7-9% increase doesn&#x27;t really seem the biggest problem with .com domains. I think there is lot to fix before that.And I&#x27;m not entirely even sure if it could be run too much cheaper. And by whom and what process. reply Sohcahtoa82 16 hours agoparentprevI was thinking basically the same thing.The cost of a domain for a year is still less than a lunch at Five Guys.Even when I was working at Subway for $10&#x2F;hr, I wouldn&#x27;t sweat a $1&#x2F;year raise on my domain price. reply 62 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Namecheap, a domain registrar, is notifying customers about upcoming price increases for .COM and .XYZ domains in September.",
      ".COM domain renewals will experience a 9% increase across all registrars, including Namecheap.",
      "Customers are encouraged to renew their .COM domains before September to secure the current rates or explore alternatives to .COM domains."
    ],
    "commentSummary": [
      "Namecheap recently raised the prices for renewing .com domains, leading to discussions about the implications for businesses.",
      "Alternative domain registration providers like Porkbun and Cloudflare are mentioned, but concerns about low prices and potential future price increases are raised.",
      "The price increase is attributed to Verisign's permission to raise prices, sparking frustration and discussions about the value and accessibility of websites."
    ],
    "points": 215,
    "commentCount": 313,
    "retryCount": 0,
    "time": 1692634802
  }
]
