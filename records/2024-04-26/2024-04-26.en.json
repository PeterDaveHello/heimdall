[
  {
    "id": 40160429,
    "title": "FCC Restores Net Neutrality Rules Amid Legal Challenges",
    "originLink": "https://www.nytimes.com/2024/04/25/technology/fcc-net-neutrality-open-internet.html",
    "originBody": "ADVERTISEMENT SKIP ADVERTISEMENT F.C.C. Votes to Restore Net Neutrality Rules Commissioners voted along party lines to revive the rules that declare broadband as a utility-like service that could be regulated like phones and water. Share full article Jessica Rosenworcel, chairwoman of the Federal Communications Commission. Credit... Pool photo by Oliver Contreras By Cecilia Kang Cecilia Kang has reported on net neutrality since it was first introduced during the Obama administration. April 25, 2024 The Federal Communications Commission voted on Thursday to restore regulations that expand government oversight of broadband providers and aim to protect consumer access to the internet, a move that will reignite a long-running battle over the open internet. Known as net neutrality, the regulations were first put in place nearly a decade ago under the Obama administration and are aimed at preventing internet service providers like Verizon or Comcast from blocking or degrading the delivery of services from competitors like Netflix and YouTube. The rules were repealed under President Donald J. Trump, and have proved to be a contentious partisan issue over the years while pitting tech giants against broadband providers. In a 3-to-2 vote along party lines, the five-member commission appointed by President Biden revived the rules that declare broadband a utility-like service regulated like phones and water. The rules also give the F.C.C. the ability to demand broadband providers report and respond to outages, as well as expand the agency’s oversight of the providers’ security issues. Jessica Rosenworcel, the chairwoman of the F.C.C. and a Democrat, said the rules reflected the importance of high-speed internet as the main mode of communications for many Americans. “Every consumer deserves internet access that is fast, open and fair,” Ms. Rosenworcel said. “This is common sense.” Broadband providers are expected to sue to try to overturn the reinstated rules. “This is a nonissue for broadband consumers, who have enjoyed an open internet for decades,” said Jonathan Spalter, the president of a broadband lobbying group, USTelecom. The organization said it would “pursue all available options, including in the courts.” In a letter sent to Ms. Rosenworcel this week, dozens of leading Republican lawmakers warned that regulating broadband providers like a utility would harm the growth of the telecommunications industry. The core purpose of the regulations is to prevent internet service providers from controlling the quality of consumers’ experience when they visit websites and use services online. When the rules were established, Google, Netflix and other online services warned that broadband providers had the incentive to slow down or block access to their services. Consumer and free speech groups supported this view. There have been few examples of blocking or slowing of sites, which proponents of net neutrality say is largely because of fear that the companies would invite scrutiny if they did so. And opponents say the rules could lead to more and unnecessary government oversight of the industry. “The internet in America has thrived in the absence of 1930s command-and-control regulation by the government,” said Brendan Carr, a Republican commissioner. A decade ago, the potential new regulations prompted raucous demonstrations. At the time, telecom companies were losing business to online streaming services. Sites like Facebook, Google and Amazon feared they would be forced to pay telecom companies for better delivery of their services. During the Trump administration, the F.C.C. rolled back net neutrality. Republican lawmakers and F.C.C. commissioners have balked that the rules were unnecessary and government overreach. Democrats have argued they are critical to consumer protection. In the vacuum of federal regulations, several states including California and Washington created their own net neutrality laws. Cecilia Kang reports on technology and regulatory policy and is based in Washington D.C. She has written about technology for over two decades. More about Cecilia Kang A version of this article appears in print on , Section B, Page 4 of the New York edition with the headline: F.C.C. Votes To Bring Back Net Neutrality. Order ReprintsToday’s PaperSubscribe Share full article ADVERTISEMENT SKIP ADVERTISEMENT",
    "commentLink": "https://news.ycombinator.com/item?id=40160429",
    "commentBody": "FCC votes to restore net neutrality rules (nytimes.com)828 points by throwup238 16 hours agohidepastfavorite463 comments tbeseda 14 hours agohttps://archive.li/ITyf1 martinbaun 12 hours agoparentnext [1 more] Thanks reply karaterobot 16 hours agoprevI'm fully in support of net neutrality, but I'm somewhat surprised they're restoring it, as I have not really heard a peep about it since it was repealed in the first place. From my perspective, nothing about the internet changed since then (my experience did not upgrade or downgrade). People stopped talking about it, there weren't major protests, news about it even largely disappeared from the front page of HN (!). So, I would be beyond shocked if this was an election year issue of substance. What, then, is the impetus for restoring the net neutrality rules, given there is always some political cost to any action like this? Has the lack of net neutrality caused issues that I just have not heard about? reply Cody-99 15 hours agoparent>So, I would be beyond shocked if this was an election year issue of substance. Because it isn't an election year issue. This has been in the works since at least 2022.https://www.cnet.com/home/internet/net-neutrality-will-make-... The rule making process takes time! >From my perspective, nothing about the internet changed since then (my experience did not upgrade or downgrade). People stopped talking about it, there weren't major protests, news about it even largely disappeared from the front page of HN (!). ... What, then, is the impetus for restoring the net neutrality rules, given there is always some political cost to any action like this? Has the lack of net neutrality caused issues that I just have not heard about? IMO it seems likely ISPs knew the rules were likely to come back so they avoided most of the practices that would generate outrage (throttling streaming and other popular services unless you pay an additional fee). I have no doubt if they could get away with it they would haha. Many providers did roll out zero rating programs. As for why this is important just because ISPs aren't currently doing it on a large scale doesn't mean steps shouldn't be taken to prohibit it. We already know what happens in the long run when ISPs are allowed to double dip https://restofworld.org/2024/south-korea-twitch-exit-problem... reply pseudalopex 15 hours agorootparent> IMO it seems likely ISPs knew the rules were likely to come back so they avoided most of the practices that would generate outrage (throttling streaming and other popular services unless you pay an additional fee). And several states passed their own laws. reply rixthefox 15 hours agorootparent... and then these same ISPs complained that there was no single \"law of the land\". We heard it would be an \"unnecessary burden\" for these ISPs to have to deal with Net Neutrality in a state-by-state basis. As a group, they sure do love to complain. They voted to get rid of the national standard in the first place! Then when real solutions are being voted on they love to yell and screech about how it's \"THE END OF THE INTERNET AS WE KNOW IT!!!\" yeah... Bunch of whiners. reply parineum 9 hours agorootparentprev> I have no doubt if they could get away with it they would haha. Many providers did roll out zero rating programs. This isn't a hypothetical, this is the case now and it's not happened. The reason is because of public backlash which is a market effect. reply kelnos 4 hours agorootparentIt had nothing to do with market effects. Some states and even quite a few local governments made their own net neutrality laws once the Trump admin nixed it federally. Complying with NN laws in some places but not others would have been way too complicated, so they just let it be. NN being saved by consumer backlash doesn't really make sense in the US, anyway, where many (most?) people only have one or two choices for internet service. ISPs don't really need to care if their customers don't like their policies. reply tivert 14 hours agorootparentprev> The rule making process takes time! No, it doesn't take this much time. It's just that net neutrality wasn't a priority for the Biden administration, so they dragged their feet until the very last minute. IIRC, there's been a flurry of rule-making just now because they are running up against a Congressional Review Act deadline. reply pseudalopex 14 hours agorootparent> It's just that net neutrality wasn't a priority for the Biden administration, so they dragged their feet until the very last minute. The FCC was deadlocked until September 2023 and started this process a few days later. Maybe they could have started in 2022 if Biden had nominated someone else after Republicans blocked his 1st choice. But Democrats believed Republicans would block anyone who would restore net neutrality. reply tivert 13 hours agorootparent> The FCC was deadlocked until September 2023 and started this process a few days later. I am aware of that. > Maybe they could have started in 2022 if Biden had nominated someone else after Republicans blocked his 1st choice. But Democrats believed Republicans would block anyone who would restore net neutrality. And they were proven wrong, and didn't even try to test their theory until half his term was over. That counts as \"not a priority\" in my book. reply fnordpiglet 8 hours agorootparentThat’s not entirely what happened. The prior candidate didn’t withdraw until March 2023. Biden nominated Gomez in May 2023. Presumable the two months intervening included negotiations and background. That doesn’t sound like a priority issue. reply EasyMark 8 hours agorootparentprevI think you expect that government works \"fast\" and that is usually not the case unless it's a dire emergency. The wheels of government are just slow. Net Neutrality is important to codify/enact and that's what they've done, I'm certainly not going to complain about it. There are a lot of other nits I have to pick with Biden's policies but this isn't one of them, better late than never like it was going to be under a second Trump term, and could be again. reply OkayPhysicist 14 hours agorootparentprev> The rule making process takes time! It really didn't have to in this case. It would have been perfectly acceptable to crib California's NN law, ctrl-r \"California\" \"United States of America\" and call it a day. reply throwup238 12 hours agorootparentThe FCC has a legally mandated process (see Administrative Procedure Act) including a public comment period that is open to judicial review. They can’t just copy California’s law and call it a day, they have to actually take public comments into consideration. If they don’t follow this process the courts will overturn the rules. reply dragonwriter 12 hours agorootparentprevNo, literally, there is a legal requirement for certain process; debates over whether it was properly followed tied the Trump repeal up in court for a while though it was eveentually resolved in favor of the Administration. Not even bothering to follow the clear objective formal requirements of that process (the question about Trump was more about good faith in the substance) would make it trivial to defeat in court. reply BolexNOLA 11 hours agorootparentprevThey couldn’t seat the fifth person (democrat) because the GOP was blocking it. As soon as the person was seated, they moved forward with restoring net neutrality. Hands were tied until then because they couldn’t get the 3-2 vote. They didn’t have 5 until September 2023 so it’s been just over half a year reply pseudalopex 7 hours agorootparentRepublicans and Manchin. reply mgiampapa 11 hours agoparentprevBecause California saved it. https://en.wikipedia.org/wiki/California_Internet_Consumer_P... If any of the companies that wanted to exploit the lack of FCC enforced net neutrality did business with California they would have had a big problem. reply eftychis 10 hours agorootparentThis can't be stated enough. They could not get away with it. Otherwise, they would. There is little to no competition in the segment. And that must change. reply pixelsort 9 hours agorootparentprevThis, and that it is far more profitable for ISPs to aggregate our traffic patterns and sell them to ad companies and governments than to drive people to VPNs by raising awareness of the reasons we can't trust them. reply nashashmi 10 hours agorootparentprevthe neutrality rule would be applying to ISPs. Those would be local to California. Outside of California, we would see the effects of no net neutrality reply acdha 10 hours agorootparentYes, but the big ISPs would have a much harder time explaining the difference. Imagine them going into court or Congress having to explain why they needed to shakedown Netflix in NYC but not LA or explain why it suddenly became cost-prohibitive to run a network when you cross the border into Oregon or Arizona. reply dragonwriter 9 hours agorootparentprevThere were lawsuits over the repeal under Trump raising uncertainty. That lawsuit wasn't resolved until 2019. California adopted it's net neutrality law in 2018. 12 other states adopted net neutrality laws or executive actions, and over 100 local governments also did so, some before and some after the lawsuit over the federal repeal was resolved. Democrats in Congress in 2019 moved to legislatively reverse the repeal, and that passed through one house. Biden was elected in 2020, and either a legislative or executive reinstatement of net neutrality was expected. All of this made meant that big ISPs would have to have patchwork rules in different jurisdictions if they wanted to skirt net neutrality and face a significant risk of having to unwind them. So, generally, no one did much that would go against net neutrality. reply mgiampapa 9 hours agorootparentprevI believe a side effect of the way the legislation was written included that if they weren't neutral, then they couldn't do business with the State of California either or anything the state runs, like pension plans. How much can you make doing business with or in CA vs. grifting the rest of the nation and bad press? It's very risky move. CA won and Verizon et all blinked. reply throwup238 15 hours agoparentprev> What, then, is the impetus for restoring the net neutrality rules, given there is always some political cost to any action like this? Has the lack of net neutrality caused issues that I just have not heard about? The rule was always going to get reversed eventually. Several major factions within the Democratic party are strong supporters of net neutrality and they've become increasingly more powerful over the last two decades, at the expense of its detractors like the media conglomerates and ISPs. It only took this long because of the Administrative Procedure Act [1] which regulates how agencies make rules. They can't just flip flop the second a new political party gains power because of judicial review - they have to follow a process (though they probably also timed this for an election year). [1] https://en.wikipedia.org/wiki/Administrative_Procedure_Act reply notatoad 15 hours agoparentprev>What, then, is the impetus for restoring the net neutrality rules, given there is always some political cost to any action like this? there's usually some principled people in the government, and every now an then when an issue is obscure enough they can manage to get something done without the other side caring too much. what's the impetus for blocking this? reply dantheman 12 hours agorootparentit's not needed, the fcc doesnt have the authority, keeping the government away from internet is a good thing reply polygamous_bat 10 hours agorootparent> keeping the government away from internet is a good thing See, I would have agreed more with this if most of our internet infrastructures were not controlled by three megacorps with more power than many small to medium sized economies in the world. As it stands, the only valid option is to fight fire with fire. reply barney54 10 hours agorootparentAnd what has happened after the Trump FCC un-wound the previous net neutrality rules? Did the internet go to hell? reply Dou8Le 9 hours agorootparentNo, likely in anticipation of the rules being changed back. Better question for you. Why did ISPs attempt to fake support for repealing Net Neutrality [0][1], as well as spend money lobbying Congress? You'll note in that article that there were also fake comments in support of Net Neutrality, apparently mostly generated by one individual, but many, many fake comments against it from ISPs that even used real people's identities [2]. These aren't the actions a company takes if they don't have incentive. [0] https://ag.ny.gov/press-release/2021/attorney-general-james-... [1] https://www.pewresearch.org/internet/2017/11/29/public-comme... [2] https://mashable.com/article/fake-net-neutrality-comment-fcc reply kelnos 4 hours agorootparentprevNo, because California and 12 other states, as well as quite a few local governments, passed their own net neutrality laws. The larger, national ISPs were pretty hamstrung: they couldn't really follow the NN laws in the places where they existed, but then impose non-neutral terms in the places where they didn't, without running into lots of trouble. A federal rule is good, though, to harmonize things, even if the state/local laws were more or less already doing the job. reply LastTrain 10 hours agorootparentprevIn some libertarian dream the FCC lacks authority... reply komali2 9 hours agorootparentThe American libertarian dream confuses me because unlike libertarians abroad (where it's a synonym with \"anarchist\") they stop with political authority, and seem to have no issue with corporate authority. The ISP business in the USA is very clearly an oligopoly with the top players colluding. Not sure how a rugged individual is supposed to fight back against that. reply int_19h 9 hours agorootparentThe usual claim in right libertarian circles is that monopolies only arise because they can bribe the government into passing laws that enable them to exist. reply dragonwriter 4 hours agorootparentThe usual claim in right-libertarian circles is that it is only possible for monopolies to arise through government action (bribery is sometimes a means to encourage that action, but its not always intentional or that kind of specific corruption, but it is, in most libertarian explanations, always government action.) And for this purpose, “government action” excludes protection of what the libertarian in question thinks of as proper property rights, which almost dogmatically have no adverse consequences. reply pseudalopex 7 hours agorootparentprevOr everyone is happy with the monopoly. reply int_19h 4 hours agorootparentYep, the Peter Thiel school of thought. But people like that tend to not stay libertarian in any meaningful sense for long; to quote Thiel himself, \"I no longer believe that freedom and democracy are compatible\". That's how you get neo-reactionaries, basically. reply EasyMark 8 hours agorootparentprevlibertarians are a subgroup of classical liberalism; limited government and socially liberal, or at least the right to live as one wants within reason in a society. Anarchists, at least to Americans, have so many subgroups I don't even know where to start, it always seems completely watered down to me other than the \"no central government\" part. reply dragonwriter 3 hours agorootparent> libertarians are a subgroup of classical liberalism No, they aren’t. There is some overlap between “libertarians” and groups from left to right (in the modern sense) that are grounded primarily in classical liberalism, and those include the bulk of what tend to get labeled “libertarians” in America (which are mostly the center-to-right subset of the classical liberal subset of libertarians.) But “libertarian” also encompasses anarchists, libertarian socialists, and a number of other left-libertarian ideologies that are not particularly grounded in what would usually be regarded as classical liberalism (most of them are grounded in newer philosophies which could reasonably be viewed as later developments from or reactions against – but not in a reverse direction – classical liberalism.) reply mindslight 4 hours agorootparentprevIt's just another system of control. Temporarily embarrassed millionaires and all that. And once the desire for freedom has been transmuted into support for corporate authoritarianism, the money flows and the political hacks get to work shoring up the platform for the sponsors. I don't think 'libertarian' has to be synonymous with 'anarchist', but US libertarianism desperately needs an analog of anarchism-without-adjectives and to drop the axiomatic-fundamentalist approach that ends up fooling so many into supporting authoritarianism. Coercion is not some binary thing, but rather a matter of degree based on power differentials. reply kelnos 4 hours agorootparentprevAh yes, keeping the US government away from the thing they created in the first place. That's seems workable, sure. reply EasyMark 8 hours agorootparentprevIs keeping the government away from roads a good thing? From helping poor people with basic necessities of life? Keeping children out of workhouses? reply 4RealFreedom 6 hours agorootparentWhere did anyone say that? reply kelnos 4 hours agorootparentI think the point the person you're replying to was trying to make is that the line drawn between things the government should have a hand in vs. things they should leave alone is fairly arbitrary, and is a matter of opinion. So saying the government should be kept away from the internet is just one place to draw the line, and it's perhaps interesting to know of other places where someone might draw that line, in order to get a baseline, and determine if it's even worth trying to have a productive discussion with them about government regulation. reply callalex 14 hours agoparentprevWhat changed for me is that my home internet provider (Comcast) implemented an overly-burdensome impossible data cap that I can only get rid of if I agree to use their router with deep packet inspection, ad injection, and more. reply julienb_sea 11 hours agorootparentFwiw you can set their router to bridge mode and use your own. It is probably still doing some traffic analysis but certainly no ad injection. This is what I do to get unlimited data without paying their exorbitant standalone fee. reply saagarjha 1 hour agorootparentThe parent post misses the main reason they sell these. It's not for the ad injection but because they broadcast a Wi-Fi hotspot that you cannot turn off, which shares your internet connection. reply thisgoesnowhere 10 hours agorootparentprevThis is a completely out of reach solution for most people. reply kelnos 4 hours agorootparentI doubt it's out of reach for someone who already wants to use their own equipment, like the person upthread who brought up this topic. reply callalex 10 hours agorootparentprevIf configuring a router into bridge mode is too burdensome of a step, then Comcast is actually providing that person a service by forcibly managing equipment for them at that point. If only the stalking component of it could be made illegal with proper privacy laws instead of piecemeal app bans. reply LastTrain 10 hours agorootparentWhat a weird way to think about it. I often wonder why I have to take those brain-dead ethics courses at work, then someone like you comes along and reminds me. Comcast can only fully take advantage of people who don't have the technical skills to not get fucked, that is what is happening. reply Andrex 10 hours agorootparentprevThat sounds absolutely horrendous. I keep getting surprised by how shitty Comcast can be, and at this point I don't know how. I'd get a 5G hotspot before I use somebody else's router. reply miohtama 9 hours agorootparentprevHow does it work, because there is no way to inject anything to HTTPS connections? reply randerson 9 hours agorootparentISPs can monitor what you're browsing through DNS requests and SNI host headers and sell that data to advertisers who then inject personalized ads into ad supported websites. reply kelnos 4 hours agorootparentThe thing I don't get is why they need a spyware router in everyone's home. They own the infrastructure and know where all the traffic is coming from. They can do this with their own hardware outside people's homes. I do wonder if they're sucking up LAN traffic data too, though, some of it which might be unencrypted, like smart devices talking to each other. reply kelnos 4 hours agorootparentprevNot sure where you're located, but in California at least, I was able to add unlimited data for an extra $30/mo. I am still using my own modem and router. It's incredible bullshit that they can pull this crap, but... well, at least it's possible. Here, anyway. Dunno if they offer that everywhere. reply RRWagner 15 hours agoparentprevI was one of the \"peeps\" testifying to the California Assembly committee that promptly made Net Neutrality a CA thing even if not yet Federal. reply romwell 11 hours agorootparentWhich is why most consumers didn't notice. CA making something a rule makes it a very strong incentive to follow it nationwide. reply mastre_ 12 hours agoparentprevThis reminds me when I got a “survey” email from ERCOT, the entity that oversees Texas’ “deregulated” energy provider racket. I was ready to lay into them hard, but starting with the second or third questions, it was clear that all they were concerned about was to _sell_ new products — ZERO interest in hearing feedback of how terrible the system for end users, they just want to sell some sort of outage insurance product (“would you pay $5 to be protected from a 30 minute or less outage one time?”). I wouldn’t be surprised if there isn’t a less than noble ulterior motive behind this push, although I’m hoping for the best. Sounds like the main reason it may actually make sense to bring it back from their PoV is because ISPs have to deal with individual state laws. reply tacocataco 10 hours agorootparent> I wouldn’t be surprised if there isn’t a less than noble ulterior motive behind this push \"Oh your power is out? Guess you should have purchased a ElektriciT+ subscription! YOUR FAULT!\" reply spamizbad 12 hours agoparentprevISPs predicted this would happen and didn't want to have to revert back everything. reply bko 13 hours agoparentprevThe proponents of net neutrality thought without it ISPs would just arbitrarily block data and require bribes from data providers to even serve up their data. In reality, no net neutrality would mean things like Netflix not counting as data on your mobile plan through some kind of sponsorship, or free basic internet like Wikipedia and news at a lower cost. I don't support legislation that bans something undertaken voluntarily unless it proves to be very harmful and the last few years have proven that we don't need this legislation. reply throw10920 7 hours agorootparent> In reality, no net neutrality would mean things like Netflix not counting as data on your mobile plan through some kind of sponsorship, or free basic internet like Wikipedia and news at a lower cost. You seem pretty pro-free-market, so here's the free-market angle: things like zero-rated Netflix on your mobile plan and free \"basic\" internet are market distortions. Companies are abusing the lack of net neutrality to engage in bundling, discounting, and collusion practices, which are bad for you as a consumer - these are anti-competitive practices! Everyone likes getting something for cheap/free, but that doesn't mean that it's actually good for you, other people, the market, or society as a whole. I agree that most of the bad things that net neutrality advocates predicted would happen wouldn't, but the things that did happen are still bad. reply ryukoposting 11 hours agorootparentprev> free basic internet like Wikipedia and news at a lower cost. I can't comprehend why you think ISPs would feel compelled to be so altruistic without any government intervention. Except, since when was free wi-fi an impossible thing to find? Ever been to a coffee shop? Even in the \"free shitty half-internet for everyone\" pipe dream, the costs of such a service don't just magically disappear. Either way, someone's paying for that free internet, and it isn't the ISP. Telecoms likely didn't deploy anything because this was obviously going to get overruled by the next non-Trump FCC. Even Ajit Pai has a long record of advocating for modernizing the FCC, which would explicitly involve the regulation of internet services. Abolishing net neutrality is only universally popular among communities where the underlying philosophy is \"government is bad, and I'm gonna prove it by running it badly.\" reply bko 7 hours agorootparent> I can't comprehend why you think ISPs would feel compelled to be so altruistic without any government intervention. Price discrimination. No altruism necessary. Kind of like my isp offering me different speeds. Meta tried to offer free limited internet to poor rural Indians but idealistic tech workers from wealthy neighborhoods opposed it on moral grounds since it was against net neutrality so then they got no internet https://finance.yahoo.com/news/why-india-rejected-facebooks-... reply pyuser583 8 hours agorootparentprevThe internet, in it’s current non-net-neutral form, is very popular. reply rixthefox 6 hours agorootparentSo the Internet before 2016 wasn’t popular because it had net neutrality? That’s definitely a new one. Is that you Pai? reply Barrin92 12 hours agorootparentprev> and the last few years have proven that we don't need this legislation. Given that large states like California and New York passed independent net neutrality laws and there were continuing legal battles in almost half of all US states I don't think you can draw many conclusions. ISP behavior very likely never changed because they knew they were just one decision away from having to comply. Sort of proven by this very decision we're commenting on. reply bko 12 hours agorootparentHere is something I found. Seems like \"unfair\" since they're favoring their videos but as a user its okay by me since I get something for free and preventing them from not counting their content doesn't mean they'll necessarily just drop their data cap. If ISPs just behave because it's always just a ruling away, then I'm fine with that status quo. I don't want unintended consequences from invasive legislation that could eventually be used to control what ISPs can show us https://arstechnica.com/information-technology/2016/02/veriz... reply acdha 9 hours agorootparent> invasive legislation that could eventually be used to control what ISPs can show us What specific legal principle do you think would lead to this? Network neutrality is the polar opposite of that - it’s like arguing that we shouldn’t have restaurant health codes because the government could start requiring us to eat peas. reply komali2 9 hours agorootparentprevI'd genuinely like to understand why you think corporations, whose success is measured by profit and basically nothing else, are more likely to do things that are good for people than governments, whose success is measured at least a little bit by the wellbeing of their constituents. I really want to better understand the thinking of people who hold opinions like yours. reply bko 7 hours agorootparentIt’s easier for me to switch which business I give my money to than it is for me to move or change governments. Most of the services I use are provided by private industry and I have choices. Everything from food, clothing, shelter. All private corporations I choose to buy from. I guess I can go get my food from a government soup kitchen or apply for government housing but my experience is these services are not competitive with private market even at the lower price (or free) reply komali2 7 hours agorootparentInteresting. Personally I believe people should have total freedom to change governments, but I'm a utopian thinker so /shrug though I wonder in such a world whether you'd feel the same way. \"Too Like the Lightning\" explored this if you enjoy sci-fi. I'm hung up on something though - in this specific subject, there's been massive market capture in the USA by one to four ISPs, depending on region. For most of rural america (something insane like 80% of the geography) there's only one provider. In these situations, the provider provides subpar service, often asking for handouts from the government before being willing to build more infrastructure (hm.. is that still \"private?\"). On the other hand, some local governments have simply built their own broadband networks, with far better results: https://communitynets.org/content/community-network-map and they have some of the highest satisfaction ratings in the nation https://www.consumerreports.org/electronics-computers/teleco... If the private market is better, why does Comcast, which routinely wins \"worst company in america\" awards, still exist, despite providing abysmal service to its customers? Surely a private enterprise could have eaten their lunch by now? If the private market is better, why are local governments providing the highest rated internet services in America? So basically, your feeling rests in the belief that you have more choice when it comes to private options - but in telecom, that doesn't seem to be the case, and of the options available, they're all widely considered to suck. Perhaps this isn't true for every industry, Stalin and Mao certainly showed us that it doesn't work for food, but does that mean the private option is better for everything we use? What does it mean to have a \"private highway\" system, or a \"private fire department?\" reply umanwizard 6 hours agorootparentprevIn theory, if people don’t perceive corporations as improving their well-being, they will stop being customers. Also in theory, if people don’t perceive governments as doing so, they will vote in different politicians. In practice, both effects exist, but are not perfectly efficient for lots of different reasons. That’s why neither Stalinist planned economies nor right-libertarian total lack of regulation work well overall, and the correct approach is somewhere in the middle and different for different sectors (and different countries). reply aprilnya 12 hours agoparentprevI have a friend in Texas who had some issues because of net neutrality being gone (huge throttling on some sites) reply gwbas1c 15 hours agoparentprev> nothing about the internet changed since then I had an extended outage and could not contact my ISP. They kept sending me to a bot, and I had no idea if anyone actually knew about the outage or was doing anything to fix it. reply callalex 14 hours agorootparentThat has absolutely nothing to do with net neutrality. reply EGG_CREAM 11 hours agorootparentIf you read the article, it does have to do with the ruling. Part of regulating ISPs as a utility is that they can regulate/enforce rules on how ISPs handle outages. reply advael 10 hours agoparentprevI wouldn't be surprised if your experience hadn't changed: Net neutrality rules were gutted at the same time as the internet has been largely consolidated, so major players paying for \"fast lanes\" and the ISPs throttling other kinds of traffic is, statistically, likely to have gone mostly unnoticed by you as an end user. If you have internet use cases beyond that which is endorsed by corporate tech, you will likely have noticed a stark difference. I've found that things like SSH tunnels have been less reliable, that there is noticeable slowdown when I find myself on a smaller website (Like those maintained by a shrinking minority of local vendors and artists who don't do everything on instagram). The most obnoxious thing about shady degradations of infrastructure in the name of profit is that these changes are often made in a way that's hard to specifically pinpoint, and by entities that make it somewhere between infuriating and futile to address any kind of complaint to. reply andygeorge 15 hours agoparentprev> lack of net neutrality caused issues given it's all still just regional ISP monopolies, there is decided _not_ a lack of issues reply dmix 9 hours agoparentprev> it since it was repealed in the first place. From my perspective, nothing about the internet changed since then I got downvoted heavily years ago on HN for predicting this when it was making the rounds There is almost no evidence of a tiered model either working or being legitimately attempted, even globally in places without these rules. The only evidence I was ever given was some tiny Portugese mobile network entirely serving the lowest end of the market, and even that barely made a dent in the local market. I want a free internet as much as anyone but people like to fear monger scenarios they invent in their heads, and pointing at vaguely defined wealthy people conspiring to do so behind the scenes, even when theres little evidence it was ever a plausible market nor technically coherent scenario. But I guess people fear that sort of chaos where every detail isn't in a neat box clearly defined by the government, even if it means finite regulatory time/resources gets redirected from pre-existing tangible issues like privacy and spam. reply dadjoker 9 hours agorootparentThis rule does, however, effectively regulate the prices that broadband providers charge consumers, as it disallows high-volume customers from being charged a higher periodic rate than lower-volume consumers. If that's not regulating prices, it's not at all clear what might be. Just like Obamacare, another gift from the left that has worked out so well... reply int_19h 8 hours agorootparentIt does not disallow charging consumers for traffic used. But why should they be able to charge for it twice? reply sabarn01 9 hours agoparentprevThis should be a reminder that almost all dire consequences from any government action are overblown. I also think the net neutrality was an important thing 15 years ago for how the internet worked then it has little practical value now. reply feoren 14 hours agoparentprevnext [4 more] [flagged] thegrim33 10 hours agorootparentYour first post in this thread containing partisan flamebait getting flagged into destruction wasn't enough for you? You toned down the cultish political speech about 30% but posted effectively the same comment again. Your posts do not create useful discourse, they just create political flame wars with echo chamber 1 fighting echo chamber 2. [Edit] Looking through your history you seem to often make similar political posts making biased, (in my opinion hate-based), partisan assertions and having people reply to you telling you this. There's Reddit/Twitter/etc for you to do this sort of thing. reply kibwen 6 hours agorootparentAs someone who was fortunate enough to see their previous comment, it sure wasn't wrong, though it sure was inconvenient for the Temporarily Embarrassed Billionaires here who are happy to sleepwalk the rest of us into fascism if it means making another buck. As for rooting through their comment history, those in glass houses shouldn't throw stones, my friend. So far yours is \"cars good\", \"California bad\", \"won't someone think of the poor companies (unironically)\", \"public transit bad\", \"everyone does propaganda so it's okay when Russia does it\". Is this comment hate-based, in your opinion? reply thisgoesnowhere 10 hours agorootparentprevYeah also the non compete stuff is absolutely fantastic even if it only affects those with middle class income. Shits happening again and I'm here for it. Constant gridlock in passing laws is terrible. reply BikiniPrince 14 hours agoparentprevI believe they are only restoring it to enact “security” aka more spying. I would like to see what the actual text of these policies are. The administration has its tentacles into too many tech companies already. reply nobody9999 8 hours agorootparent>I would like to see what the actual text of these policies are. They aren't a secret: https://docs.fcc.gov/public/attachments/DOC-401676A1.pdf reply geuis 16 hours agoprevHere's the FCC announcement (pdf) for those interested: https://www.fcc.gov/document/fcc-restores-net-neutrality reply reaperman 13 hours agoparentI'm looking for full text of the actual action / implementation. Like the document containing the text that they actually voted on, specifically. Edit: https://news.ycombinator.com/item?id=40160960 reply TechDebtDevin 8 hours agorootparentVote first, plan the project later. Allocate money to your donors then let them figure out how to go over budget and ask for 1.5X more in 5 years. All congressional laws are essentially money laundering operations now where the main priority is getting govt funds to your best donors. Gg. reply vampiresdoexist 11 hours agoprevI’m very surprised by some of the comments here questioning the value of restoring net neutrality. Times have changed. reply ohdannyboy 7 hours agoparentIt's probably because none of the hysterics or doomsday propaganda actually came to pass. reply kelnos 4 hours agorootparentThe doomsday \"propaganda\" didn't come to pass because several states and localities promptly passed their own net neutrality laws after it was deregulated at the federal level. The larger ISPs couldn't find a workable way to implement their non-neutral bullshit in some markets but not others, and the local ISPs in places with no net neutrality laws never really had enough clout to do crappy things in the first place. reply ohdannyboy 4 hours agorootparentI didn't know that. That's actually a good explanation for the why. reply fragsworth 3 hours agorootparentIf that didn't happen, and the ISPs started profiting off non-net-neutral tactics, it could have been permanently fucked. Once someone depends on a legal source of income, if that source of income gets banned in the future, they generally get to keep that source of income \"grandfathered in\" if they take the issue to court. reply dragonwriter 3 hours agorootparent> Once someone depends on a legal source of income, if that source of income gets banned in the future, they generally get to keep that source of income “grandfathered” forever if they take the issue to court. That’s… not true. Otherwise, all the people depending on selling drugs that were later banned would have been grandfathered in when the drugs were prohibited. Even when there is a regulatory taking (that is, government regulations eliminate the value of existing property in a way that is considered a taking under the 5th amendment), the remedy is compensation for the lost value of the property, not a lifetime exemption from the regulation. reply vampiresdoexist 7 hours agorootparentprevHm. I would encourage a different, less intense angle here. It’s possible the doomsday didn’t come to pass because a lot of passionate people worked very hard to make sure we avoided it. reply ohdannyboy 6 hours agorootparentPossible, but is there any reason to believe so? I'm open to hear it. The whole point was that companies like Comcast don't give a crap what we think and will engage in this anti competitive behavior unless the FCC stops them. Don't get me wrong, I have no doubt they would if it was in their financial interest. But can we agree that it is also possible that market incentives aligned and the infographics depicting tv-bundle-like internet packages weren't actually around the corner? To me it seems like the easier explanation. The incentive could be as simple as Comcast not wanting a new monopoly court case or to start being classified as a utility in areas where they have no real competition. reply vampiresdoexist 5 hours agorootparentSure, maybe those bundles weren’t right around the corner. But the fight for NN probably incentivized the MBA grads to not explore those options with fervor. And it’s very reasonable to assume that avoiding a monopoly case or being classified as a utility is enough of an incentive. But I have a preference for putting up the defenses on all fronts when it comes to ISPs and their unlimited creative chicanery. reply bagavi 5 hours agorootparentprevThe null hypothesis is that market forces takes care of it. Like your airline ticket prices. The onus of proof is on you to market forces aren't enough. reply codewiz 3 hours agorootparentAirline fares are regulated by the FAA and the DOT to disallow deceptive business practices and require minimum service levels: https://www.transportation.gov/individuals/aviation-consumer... Similarly, the FCC net neutrality rules allow telcos to charge any price for the service while disallowing blocking or throttling particular Internet sites or protocols. If such rules weren't indeed necessary, big telcos wouldn't be spending their money campaigning against them, would they? reply idle_zealot 4 hours agorootparentprevThat's an insane null hypothesis. reply pyuser583 8 hours agoprevI thought Net Neutrality was a “has been” idea … Opponents have been doing a victory lap for some time. COVID especially showed how much better the US Internet expands and contracts based on demands. As far as I know, nobody has accused ISPs of throtteling Netflix. The whole idea behind CDNs is we should stop treating all Internet users as equals, and connect based on geography. Not dystopian censorship, but the sort of thing neutrality enforcers would have to approve. reply snailmailman 4 hours agoparentMany plans do throttle. On my “unlimited data” cell phone plan, YouTube, Netflix, etc all can only really load at 480p, even in areas where speeds are fast enough for hd video. In those areas, I can use a vpn and easily get hd video. Although, the cell network is pretty terrible where I am, and more often than not there is no hope for streaming hd video. reply itopaloglu83 27 minutes agorootparentIt’s really infuriating when T-Mobile forces YouTube down to 420p and then says (roughly) “you can get 720p with only $10 a month” condescendingly. reply wmf 6 hours agoparentprevSeveral ISPs used intentional congestion to extort Netflix into paying peering fees they shouldn't have to pay. AFAIK Netflix is still paying. reply clarkdale 6 hours agoparentprevThis is why Netflix built fast.com reply _heimdall 10 hours agoprevInteresting to see this come through effectively at the same time as the law granting powers that allows the government to ban TikTok and others in the future. I can't help but assume there's a connection there. I also don't know why the new law allowing a ban on foreign influenced social media would be necessary if the FCC decides again that it can regulate ISPs as utilities. Weren't the powers there already strong enough to force an ISP-level ban on a service deemed a national security threat? reply pseudalopex 9 hours agoparentYour assumption is wrong. The net neutrality fight is older than TikTok. The process for this vote started in September. It would have started in 2021 if Biden's FCC nominee was approved. The Protecting Americans from Foreign Adversary Controlled Applications Act does not involve the FCC. Regulating ISPs as utilities does not empower the FCC to force ISPs to block services. Never mind regulate app stores and hosting services. reply _heimdall 7 hours agorootparentI wasn't actually proposing that the FCC would have the power to remove apps from the app store, only that they could force ISPs to block specific servers. I was thinking the FCC regulations would have that power to implement such a ban based on national security, though I could be wrong. I'd have to look back at the Patriot Act as well, I'd expect that to offer similar powers but I don't remember for sure. reply jacob019 4 hours agoprevT-Mobile has a variety of plans that selectively throttle video streaming for known streaming services. I wonder if this will force them change it. reply mise_en_place 16 hours agoprev> Safeguard National Security – The Commission will have the ability to revoke the authorizations of foreign-owned entities who pose a threat to national security to operate broadband networks in the U.S. The Commission has previously exercised this authority under section 214 of the Communications Act to revoke the operating authorities of four Chinese state-owned carriers to provide voice services in the U.S. Any provider without section 214 authorization for voice services must now also cease any fixed or mobile broadband service operations in the United States. That seems rather vague. The timing is also rather interesting, given the forced divestiture of TikTok. reply pdabbadabba 15 hours agoparent> That seems rather vague. The timing is also rather interesting, given the forced divestiture of TikTok. This is just a press release. The actual decision is more than 400 pages long and will come out in the next few days. Here's the draft of the order released three weeks ago: https://docs.fcc.gov/public/attachments/DOC-401676A1.pdf (Of course, parts of this will inevitably be vague as well.) The timing is almost certainly a coincidence. They started the process of adopting these rules as soon as they could after democrats regained a majority of seats on the FCC last year and got them done as fast as they could. reply reaperman 12 hours agorootparentI'm not sure that's the order itself or just a very detailed \"fact sheet\" about the orde. It seems like it references the content of the order in great detail, allowing someone to figure it out, but I don't see the raw text of the rule there unless I just don't understand what FCC rules look like. I read a lot of FTC rules and court documents, but this the first time I'm looking for the full text of something the FCC voted on or was something close to it (like an earlier version of the exact document they voted on). reply nobody9999 8 hours agorootparenthttps://docs.fcc.gov/public/attachments/DOC-401676A1.pdf reply reaperman 6 hours agorootparentYes that is just the \"FCC FACT SHEET\" as it says in the top title of the document. It is not the actual rule/action. It is also the exact same link that the poster just above me already gave. reply ostenning 11 hours agoprevThis battle has been happening for a better part of a decade and won’t seem to go away. Every time it’s defeated it seems to pop back up. reply dragonwriter 10 hours agoparent> This battle has been happening for a better part of a decade Closer to 3; it started almost immediately after the 1996 Telecommunications Act, and the FCC first adopted nondiscrimination prinicples that underlie net neutrality as a basis for policy (but not as regulation) in 2004. reply devindotcom 10 hours agoparentprevin a sense it goes back to 1966 https://techcrunch.com/2017/05/30/commission-impossible-how-... reply xondono 2 hours agoprevThe amount of pro-net neutrality in here is a clear demonstration of the opinion forming power of John Oliver, by dressing the issue as affecting users instead of companies. A lot of people seem very confused about what “neutrality” means, and it’s consequences. As an analogy, VAT is an equal tax (everyone pays the same VAT) but it’s a very non-progressive tax (it burdens poor people more than rich people. Your ISP doesn’t really care about your speed, it could increase yours and all your neighbors speed by a big chunk and it won’t really notice it. The problem is that to handle a Netflix they need to do a massive investment. Yes, non net neutrality is about creating differentiated “highways”, but you are not going on that “highway” no matter what. The discussion is if internet is considered infrastructure (as roads are) and thus they should be built with everyones money, no matter how specific they are to a single company, or if we should leave it to the market. I’m still waiting for someone to explain to me why a company that makes massive amounts of money from the internet shouldn’t be paying a higher proportion of the infrastructure costs than a user. reply nabla9 1 hour agoparentNet neutrality laws are not an US only thing. EU (The Net Neutrality Regulation 2015) and many other countries have net neutrality laws. >I’m still waiting for someone to explain to me why a company that makes massive amounts of money from the internet shouldn’t be paying a higher proportion of the infrastructure costs than a user. Because ISP is in business of selling internet access to consumer. ISP can sell different tiers of service to the consumer, but can't sell the product twice. Netflix pays huge sum in their end. This is how money flows: customer--->[ISP]-->|backbone|The framework we adopt today does not prevent broadband providers from asking subscribers who use the network less to pay less, and subscribers who use the network more to pay more You see. reply diordiderot 10 minutes agoparentprevISPs don't want to charge they want to extort. reply andy_xor_andrew 16 hours agoprevgreat, what are the odds they reverse the reversal next year in a hypothetical new administration? reply kelnos 4 hours agoparentFortunately this won't be the end of the world, as quite a few states and localities now have net neutrality laws of their own, which would presumably go back into effect if it were deregulated at the federal level again. Of course, the FCC could presumably create a rule that explicitly allows ISPs to do non-neutral shenanigans, and then the DoJ could start suing states, saying the federal rule preempts them. Not sure how that would pan out, though I'm sure the current composition of SCOTUS would be fine backing the FCC in this case, if the challenges got that far. reply anderber 16 hours agoparentprevI'd say 50/50 reply Nemo_bis 3 hours agoparentprevDepends on who controls the US Senate, presumably! reply paulddraper 11 hours agoparentprevIf there is a new administration, close to 100%. reply devindotcom 10 hours agorootparentyep imo it's dead in that case: https://techcrunch.com/2024/04/04/net-neutrality-wont-surviv... reply tootie 14 hours agoparentprevBy \"they\" you mean voters. This policy isn't top priority for very many voters, but the battle lines on this are clear. Trump will overturn (he already did once). Biden will protect it. A vote for Trump is a vote to overturn. reply qingcharles 11 hours agorootparentObama set the FCC on a course to lower jail and prison phone call prices (which is understood to decrease recidivism by keeping prisoners in contact with their support systems). Trump came in and replaced the FCC head with this guy: https://nypost.com/2017/08/10/fcc-chairman-under-fire-for-co... reply euroderf 3 hours agoprevWhile they're at it they could restore the FCC fairness doctrine, repealed in 1987. reply codewiz 3 hours agoparentAs a United States immigrant, I had never heard of the fairness doctrine before. My first thought is: how would it be compatible with the freedom of the press granted by the First Amendment? reply BugsJustFindMe 4 minutes agorootparentIt should hopefully be obvious that even the most succinctly and universally stated rights have certain correct limits necessary for protecting society from individual selfishness. For example, the right to free speech is not a right to commit fraud or slander against another. If it were, society would fall into catastrophic disarray. So the fairness doctrine can be seen as compatible with the first amendment in exactly the same way that consumer protection laws against false advertising are compatible with the first amendment. reply mvkel 14 hours agoprevThe quote \"markets can stay irrational for longer than you can stay solvent\" comes to mind. Governments can stay irrational longer than you can stay vigilant. It's frustrating that a decision can be made at great effort in support of net neutrality, only for a new bill to easily be introduced that undermines it yet again. I guess that's a feature of democracy, not a bug. But I can imagine these battles gets harder and harder to win as time progresses. reply pictureofabear 10 hours agoparentIf Congress stepped in to mandate it, the flip-flopping wouldn't happen. You said it. This is a feature of the US government. It allows prototyping of policies before codifying them. reply Spivak 6 hours agorootparentIs that actually true or would the flipping just happen every time the majority party changes? reply kelnos 4 hours agorootparentDespite the required process for changing regulations in a log of executive branch agencies, I feel like laws Congress passes are a bit more durable. Even with a different majority, there's still horse-trading that needs to go on to get things done, and it's not always easy to push through things that are unpopular with the minority party. With executive branch agencies, whoever is in the White House pretty much has complete control, modulo rules that slow things down, anyway. reply komali2 9 hours agoparentprevIMO this is why communities should do everything they can to build their own infrastructure independent of these massive institutions that can't possibly represent their needs - some being comcast, others being the USA federal government. I find the concept of \"the People's internet\" fascinating https://urbanomnibus.net/2019/10/building-the-peoples-intern... not to mention distributed networks like this are more rugged in the face of disaster. reply deviantbit 3 hours agoprevThese are not the net neutrality rules I personally was looking for. They allow a base traffic speed with data caps, and then you buy a la carte for the additional speeds you want. They still allow prioritizing traffic. There is nothing neutral in these new rules. The rules under Obama were far better, and strangely better under Trump. They have taken the privacy provisions back that were allowed previously. Please read the fine print, and call your congress person, and senator and let them know you demand true net neutrality, and your privacy needs to be protected, with emergency services only having priority above other traffic. Please read these new rules. reply zer00eyz 16 hours agoprevFCC rules... The other day it was FTC and banning non competes, there's also talk right now on the home page regarding KYC and an executive order. I would love for us to be able to get back to making laws in the US. Executive orders and agency rulings are a bad way to run a \"democratic republic\" reply redserk 16 hours agoparentIt's worth noting that these agencies and their powers did not spring up out of thin air. Various elected Congress sessions wrote the laws that created and empowered these agencies to create rules. This is a reasonable implementation of a \"democratic republic\" as Congress still has oversight. reply wtallis 16 hours agorootparentThe problem with merely having regulations rather than laws is not a concern that they may not have proper legal authority, but that they are less durable and more easily overturned than laws passed by Congress and signed by the President. reply redserk 16 hours agorootparentI agree, and I'd rather Congress weigh in now that we've had this specific issue flip-flop twice. I do not like the implication that agency rulemaking is anti-democratic though. We have utilized this structure for well over 120 years, or practically half of the country's history. reply wtallis 15 hours agorootparentYou may not like it being pointed out, but having rules made by appointed regulators rather than elected legislators is obviously anti-democratic. Yes, delegating powers like this is a practical necessity, but having made that reasonable tradeoff does not erase the reality that it's a less than perfectly democratic process. So is the structure of Congress itself. reply redserk 15 hours agorootparentFirst, the US is not a pure democracy. We elect representatives on our behalf to handle voting on matters. So dismissing something as \"anti-democratic\" is not applicable here. Our elected officials set up a system where a series of agencies under the Executive Branch may create rules, but the elected officials have oversight authority. If you disagree, you may petition your state government for a constitutional amendment that prohibits this practice and advocate for additional states to join in. reply wtallis 15 hours agorootparent> So dismissing something as \"anti-democratic\" is not applicable here. [...] > If you disagree, you may petition your state government for a constitutional amendment I think you're misinterpreting what's being said here in order to over-react. I don't think anyone in this thread is saying that executive orders and delegating powers to appointed regulators should be expunged from our system of government. But they should be acknowledged as a necessary evil, and their use minimized when possible, and not allowed to completely replace the legislative process. Whereas you seem to be defending taking those practices to the extreme simply because of historical precedent. reply redserk 15 hours agorootparent> I think you're misinterpreting what's being said here in order to over-react. If you can point out how I'm misinterpreting, I'm open to discuss. From what it appears though, we have a disagreement on what we wish to delegate to different branches of government. > But they should be acknowledged as a necessary evil, and their use minimized when possible I disagree that executive agency rulemaking is a \"necessary evil\". Congress can simultaneously be derelict in their duties as a legislative body while having a executive regulatory apparatus that creates rules under their purview. > Whereas you seem to be defending taking those practices to the extreme simply because of historical precedent. If not for historical precedent and recognizing the practices we've been utilizing for 4-5 generations of people, what should we prioritize? reply gwbas1c 15 hours agorootparentprev> rules made by appointed regulators rather than elected legislators is obviously anti-democratic The people making the appointments are elected. It is obviously democratic. The general population can't get together to vote on everything, so we elect representatives to do that job for us. Our representatives can't make rules on minutia, so they appoint regulators. Don't like the regulators? Go talk to your representative. The opposite is worse: I live in a town that still has old-style town meeting where any resident can show up. It's tyranny of whoever has time to show up and stay up late, because someone will always create an amendment at 11PM to overrule a town-wide vote. reply zer00eyz 15 hours agorootparentprevWe have always had things like executive orders. Just an insane number are issued between the Great Depression and WWII, and then we have 100 years of using them as a ham fisted tool for policy. The FTC ruling on non competes... Great, except that getting rid of that rule doesn't create its complementary law around \"rading\" (see this about ca law: https://www.flclaw.net/is-poaching-employees-illegal-califor... ). And yes we have used this structure for a long time, but not to this extent, not as a political football for democratic impasse. reply lr4444lr 15 hours agorootparentprevThe scope creep of these agencies in recent decades is substantial, though. It's one thing to set rules for dumping that protect wildlands, or verify drugs in the medical supply chain aren't toxic. Deciding the rules of commerce? I'm less than thrilled. reply acdha 13 hours agorootparentIt’s not scope creep as much as recognizing that Congress is less functional than it used to be. Obstruction has been normalized since the backlash to Obama’s election – think about how often people claim you need 60 votes in the senate – and that means anyone who sees a problem has an incentive to figure out how to do it without needing timely action. reply adrr 8 hours agorootparentprevThat was their design to be agile. Regulations can get passed in 100 days and not years. reply sophacles 13 hours agorootparentprevYou'd rather have some idiotic trash that's been elected to congress have to decide what a safe dose of a drug is than an agency largely staffed by people with deep medical training? You'd rather have such a decision be at the whims of political showboating and culture wars than what can be proven safe and effective with actual medical testing? I'd argue that a better use of legislature time would be to find ways to reduce the clout of political beliefs in people appointed to high level positions in the agencies rather than requring the useless fools eleceted to congress getting final say in what the rules are. Seriously do you think the jewish space laser lady should have any say in sattelites or forest fires? Do you really want the moron that thinks injecting bleach is a viable cure to decide what makes for good medicine? Do you want a fool who think's an ar-15 with a certain set of cosmetics is a scary bad gun, but an ar-15 with hunting stocks isn't the exact same weapon to decide firearm policy? Those are the people you are suggesting should make the decisions on specifics? reply SamoyedFurFluff 16 hours agorootparentprevI blame this squarely on the congress. Congress has been the weakest it’s ever been, passing almost nothing substantial. If we had to rely on them to ensure basic things like drug approvals we never have anything. They can barely get funding passed to fund themselves! reply mrguyorama 15 hours agorootparentWhy do you blame \"congress\" instead of Republicans? reply gojomo 15 hours agorootparentPerhaps because Democrats control half of Congress today, and the general trend of Congressional avoidance-of-clear-rulemaking has been the same even during those periods that Dems or Republicans control both chambers. reply kaibee 14 hours agorootparentThe filibuster makes this kind of 'control' moot. You need a filibuster proof majority in the Senate and a majority in the House to actually get anything done (and the Presidency, to not veto). 'Control of half of Congress' when that half is the house, is meaningless. reply gojomo 10 hours agorootparentThat applies equally when each party is the filibuster-sized minority in the Senate. And: if Senate majorities really want to pass something, they can change the filibuster rules – and have, for some topics. Otherwise, the filibuster is maintained out of tradition, courtesy, and its usefulness as a change-of-control 'debounce' mechanism – as well as providing a convenient excuse for posturing more and doing less, as Congress is wont. Still, in other eras, Congress was able to move compromise legislation forward. Recently, Congress has been unable to – both parties, no matter the relative control. Any belief that it's only \"the other guys\" is partisan myside blindness. reply backtoyoujim 14 hours agorootparentprevAgencies are not beholden to Congress; they are beholden to the executive branch that creates them. That is why Nixon created the EPA so that there would not be a Department of the Environment that was out of the hands of executive power. reply adrr 11 hours agorootparentCongress creates and funds agencies. Agencies write the regulations. This is all specified in the law that was passed. FCC commission makeup is defined by law and their authority is defined by law. reply rascul 14 hours agorootparentprev> Nixon created the EPA Only because Congress allowed it. https://www.epa.gov/history/origins-epa reply pseudalopex 14 hours agorootparentprevCongress created the FCC. Congress passed many laws governing agencies. Departments are not out of the hands of executive power. reply rsanek 14 hours agorootparentprevhttps://en.m.wikipedia.org/wiki/Congressional_Review_Act reply sabarn01 9 hours agorootparentprevCongress has the legislative power all agencies derive their power from some act of congress. reply chrisfinazzo 16 hours agorootparentprevYet the Chevron decision empowers agencies to make rules independent of Congress in cases where the rules don't already exist or are unclear. https://en.wikipedia.org/wiki/Chevron_U.S.A.,_Inc._v._Natura.... Unsurprisingly, Kavanaugh and the rest of the conservatives would prefer this approach be relegated to history. Of course, the areas of particular interest that he cites as examples (securities e.g, finance, communications, and environmental laws) just happen to be those where the two parties could not possibly be further apart in their approaches. https://www.scotusblog.com/2024/01/supreme-court-likely-to-d... reply moduspol 15 hours agorootparentAlso gun laws. Any firearm enthusiast can tell you how inconsistent and incoherent various ATF rulings and determinations have been. reply fallingknife 16 hours agorootparentprevIt seems to me that the Chevron doctrine has essentially created a fourth branch of government with minimal democratic oversight. It feels like an end run around the constitution. In many cases the agencies exercise legislative, executive, and judicial powers all at the same time. reply willmadden 16 hours agorootparentprevNot if the agencies have leverage over Congress. reply jandrese 15 hours agoparentprevCongress does not want to have to learn the minutia of every aspect of things that are regulated. Delegating responsibility to the relevant agencies is exactly how Congress operates. reply darkwizard42 13 hours agoparentprevThis is the equivalent of a CEO/C-suite delegating decision making to various teams and leaders below them. They still add laws and appoint the leaders of those organizations, but can't be involved in every decision. Can't expect every single item in the government to get direct democracy, the world would grind to a halt due to the sheer number of decisions needed to be made. reply unreal37 15 hours agoparentprevThe congress doesn't seem to be able to pass anything itself without it being tied to an increase in the military budget... reply babypuncher 16 hours agoparentprevOur legislative branch abdicated its power when they stopped bothering to pass laws that people actually want. If the FTC and FCC weren't doing either of these things, they simply wouldn't happen. As soon as a Net Neutrality or non-compete clause ban bill makes it to the senate floor, Republicans will just filibuster it, even though public opinion is overwhelmingly in support of both these measures. reply fallingknife 15 hours agorootparentWhile I support both of those things, I don't see any problem requiring the legislature to actually legislate to make them happen. If the public felt strongly about these issues they would just remove their representatives next election. Just because I happen to agree with the actions of the agency in this case is not enough to justify handing legislative power over to bureaucratic agencies that do not have any of the checks and balances that are supposed to exist in our system. reply throwup238 15 hours agorootparent> that do not have any of the checks and balances that are supposed to exist in our system. But they do have the same checks and balances. All of these rules are open to judicial review and there is a whole process in place due to the Administrative Procedure Act. In fact there are more rules for these agencies like having public commenting periods after which they're required by law to consider that input when making their rules. reply babypuncher 11 hours agorootparentprevOne of the things the legislative branch can do is delegate their powers to organizations better equipped to understand complex issues. These organizations, which function as part of the executive branch, are still subject to checks and balances from both the legislative and judicial branches. The legislative branch has the power to change the laws that govern what these agencies can or cannot do, and the judicial branch has the power to determine if their actions go against either the laws passed by the legislature or the constitution. Banning regulatory agencies from doing their job would hamstring our government's ability to regulate anything, which is probably why monied interests like to argue that their very existence is unconstitutional. reply jonathankoren 16 hours agoparentprevIn case you are unaware, but congress has been DEEPLY dysfunctional for the past 30 years, and has been getting worse every session. Even this week it was shocking news that a bipartisan bill managed to even come to a vote. This is what happens when the party that doesn't have the White House chooses obstruction and enforces the the Hastert Rule. https://en.wikipedia.org/wiki/Hastert_rule reply chrisfinazzo 16 hours agorootparentYet in functioning legislative bodies (think: parliamentary systems), employing something like Hastert doesn't require any enforcement at all. They don't typically require supermajorities to pass laws, and those in the minority don't have the means to substantively object to bills they disagree with. A man can dream. reply jonathankoren 15 hours agorootparentYou may not realize it, but this is exactly how it works in the House of Representatives today, and is the exact cause of dysfunction. reply chrisfinazzo 15 hours agorootparentI should have been more precise - the Senate's rules are garbage and should be hurled into the Sun. More generally, my comments come from watching PMQ's in the House of Commons and seeing that the party out of power really doesn't have many tools to slow down the opposite sides agenda. If such a system was implemented in the US, it would force politicians to more carefully consider their positions -- no confidence votes and a motion to vacate serve the man functional purpose as a stick to get people in line, which might not otherwise be possible if they consistently took unpopular positions. reply jonathankoren 12 hours agorootparentYou’re fundamentally misunderstanding what is going on. There are a majority number of votes to support popular legislation. These bills are simply not brought to a vote BY THE MAJORITY PARTY due to internal majority party politics. Nothing in your facile proposal would remedy this. What would fix the problem would be change to the rules so that simple majority could bring legislation to a vote. This does not exist in any functional way. And we haven’t even touched on the fact that the majority of seats are often controlled by a minority of voters due to gerrymandering and the constitutional structure of the senate. reply asynchronous 13 hours agorootparentprevYou really blame republicans like when the shoe is on the other foot the other party doesn’t do the exact same tactics of blatantly stalling bills they don’t like and overall slowing government to a crawl. This is politics in the modern era. reply surge 11 hours agoprevThe FTC chair and this honestly is the best reason to vote for the Biden Administration (I feel like at this point whose in office largely doesn't matter 98% of the time, they're too old and or self absorbed to be heavily involved). Really just voting for the people they put in charge of everything below them, which was always the case, just more so now. reply kelnos 4 hours agoparentI think these are great reasons to vote for Biden (or Democrats in general), but... I mean... the best reasons? I'd think the best reason would be not putting the country back into the hands of a wannabe dictator who has said he will target his political opponents if he's re-elected. That seems quite a bit more high stakes than the good work that the FTC and now FCC have been doing on Biden's watch. reply quasse 11 hours agoparentprevThis has generally been my opinion on the office of the President. The actual quality of the administration comes from the level underneath the chief executive and I have been very pleased with the people in this administration. The FTC, Department of Interior and FCC all seem like they have very competent (and non-corrupt!) people running them. Can't say I have strong feelings on Biden but I think he's shown good sense in who he appoints to actually manage the Executive Branch. reply surge 11 hours agorootparentNow if only Buttigieg was a more than a do nothing position and the FAA/FDA, etc stopped acting like captured agencies and do their jobs. Boeing is like what, our one major airline manufacturer and because they're part of the military industrial complex, they get a free pass and get to murder whistle blowers after asking them to stay an extra day in town. reply mkoubaa 11 hours agorootparentprevI agree for all departments except the state department which seems to be as incompetent as they come reply jojobas 7 hours agoprevCommon Carrier ISPs when? reply qwerty456127 5 hours agoprevIt's going to be sad yet funny to see the same agency repeal it again once (and if) Donald Trump wins the coming elections. reply ChrisArchitect 16 hours agoprev[dupe] More discussion: https://news.ycombinator.com/item?id=40159776 reply exabrial 15 hours agoprevI mean great. I don't really want the things it's trying to ban, so good? But this is sorta like plastic straw bans: 0.0000000001% actual impact, all while making HUGE headlines, while doing absolutely zero to solve root systemic issues: Entrenched Local Monopolies by telco providers. So yeah, good, glad my Nebula won't be slower than my YouTubes, while all along I just wanted to ditch the assholes in the first place and use a different ISP. reply tentacleuno 15 hours agoparent> glad my Nebula won't be slower than my YouTubes To be fair, wouldn't this still be the case? Google peer with many ISPs, and have a lot of server / networking prowess, so the YouTube experience is normally pretty good across the board. Nebula, on the other hand, is a fairly new player from my understanding. reply mdasen 14 hours agorootparentYes, YouTube might have better CDN solutions, but ISPs can't unfairly discriminate against Nebula. Nebula is a new player, but presumably they're using CDNs with good reach. The point isn't that Google can't build a better CDN. The point is that we don't want ISPs creating a situation where they've inked a deal with Google to prevent good performance from YouTube competitors. We don't want a situation where \"YouTube is the exclusive 4K video provider on ISP-X.\" reply lolinder 10 hours agorootparentprevNebula fares just fine on my internet at 1080p and 2x speed. reply demondemidi 9 hours agoprevAfter countless useless online protests it just randomly gets restored. reply Zenzero 14 hours agoprevAs much as I support the decision are we just going to keep playing this game flipping back and forth across administrations? reply ImJamal 12 hours agoparentYes until the congress actually does their job and passes a law. reply feoren 14 hours ago [flagged]prevnext [82 more] This flip-flopping happens because there is no commonality to find. One of the only (effectively) two major political parties of the United States is completely uninterested in governing. They have no policy. They have no plan for governing; they don't like governing. Their only goal is the piecemeal selloff of government powers to the highest bidder, and they convince their stalwart followers of this by making sure they get their daily dose of other people suffering. As long as others are suffering, their base will support the wholesale takeover of government by the rich. The other half is pretty bad at governing, but at least they try to govern. So when they're in power, the first thing they have to do is try to build back up the institutions that have been disabled or dismantled by the party of government-cannibals. Don't ask me which half is which. You know. reply dang 12 hours agoparentPlease don't start tedious political flamewars on HN. It's not what this site is for, and destroys what it is for. https://news.ycombinator.com/newsguidelines.html reply skyfaller 13 hours agoparentprevI have to disagree that the Republicans do not have a plan. They have a very clear and public plan: https://en.wikipedia.org/wiki/Project_2025 Yes, they spend a lot of energy on obstructing the government from functioning, and creating a naked kleptocracy to use the government to funnel money into their own pockets. But they are also moving openly towards a fascist dictatorship with very specific ideas about how society should function, and how (and when) people should be permitted to live. reply a_wild_dandan 12 hours agorootparentI'd argue that they planned: 1. Installing a conservative super-majority to the Supreme Court. [Criminalized abortion in half the US. Blocked student loan relief. Gutted voting rights. Environmental protections. Health mandates. Firearm restrictions.] 2. Indiscriminate obstruction. [Months of crucial Ukraine aid. Blocked voting rights bill. Immigration reform. Firearm safety. Tax relief.] It's honestly difficult to pick the Greatest Hits, given how much damage they've done. reply bedhead 13 hours agorootparentprevRemember when there was no net neutrality and everything...worked great? reply a_wild_dandan 12 hours agorootparentRemember having net neutrality and everything...worked great? reply sophacles 13 hours agorootparentprevThat was before the people most opposed to net neutrality had started lobbying local governments to give them monopoly access and to make it illegal for local governments to try and encourage competition. Basically I find a good rule of thumb to be: if comcast is against it, it's probably going to improve the lives of everyone via some form of competition between businesses. reply MaxfordAndSons 12 hours agorootparentprevISPs knew this would happen if Trump lost in '20, they never acted on it's repeal in the first place. We'll almost certainly get to see what the no-neutrality internet really looks like if Trump wins this year... reply s3r3nity 13 hours agorootparentprev> But they are also moving openly towards a fascist dictatorship Democrats have been calling on Biden to increase his power through just writing Executive Orders to act on their platform, and bypass all other branches of government...but THAT's not dictatorship? reply lokar 13 hours agorootparentBypass all other branches? Have you seen him defy the courts? Threaten judges? reply noahtallen 13 hours agorootparentprevIf you look at the number of executive orders per president, republicans tend to have slightly more over the past couple decades. (Bush more than Obama, Trump more than Biden.) I don’t think executive orders are that concerning when the legislative body has problems getting shit done. It’s a normal political tool that both parties use (relatively) evenly. What is concerning is gravely anti-constitutional movements to overturn the results of democratic elections. reply magicalist 13 hours agorootparentprev> calling on Biden to increase his power through just writing Executive Orders to act on their platform, and bypass all other branches of government Executive orders are one instantiation of what's literally the job of the executive branch: executing the law. Executive orders operate within the authority granted by the legislative branch as judged by the judicial branch, and that authority can also be removed by the legislative branch. You can say an order is unconstitutional or unlawful, but it's still not dictatorship. reply PawgerZ 13 hours agorootparentprevTrump wrote 220 in 4 years, averaging 55 EOs/year Biden has written 138 in 3.25 years, averaging 42 EOs/year reply a_wild_dandan 13 hours agorootparentprevNope! That's called doing your job with the tools you've been given. reply redeeman 13 hours agorootparentfunny, thats definitely not what it was called when trump issued executive orders, there we had to endure things like the definition of dictator which tends to include \"rules by decree\" etc, as proof of him being a dictator :) oh how the winds are fleeting reply xbar 13 hours agorootparentEvery President is equivalently, and rightly, castigated by the opposition party over their executive orders. Every executive order by every President is an abuse of power, as far as I'm concerned. 55/yr for Trump? 44/yr for Biden? No one should be proud of their side. reply hathawsh 13 hours agoparentprevDon't you think that is a very cynical view? The party or parties you disagree with may not share your views, but they do have many things in common with you. In order to build bridges with other parties, it's important to believe that the majority of people who get involved in government, regardless of party, are motivated primarily by the desire to serve their neighbors and their country; otherwise they would find better ways to spend their time. Without that belief, it will be near impossible to form agreements across the aisle. reply lokar 13 hours agorootparentI mostly agree with you, but disagree on one point: What is a party? I think there still is a Republican Party that fits your description. I meet them in every day life all the time. They are reasonable, agree on many things, and are willing to seek compromise. I hope they are the majority, if not the voting majority. But they are not well represented by 90% of the current Republican office holders. reply tacocataco 11 hours agorootparentPerhaps those conservatives you're referring to would like to vote for a more moderate candidate, but they are chained to the Republicans via First Past The Post voting. reply lokar 6 hours agorootparentThat, and some just don’t vote reply dragonwriter 8 hours agorootparentprev> Don't you think that is a very cynical view? It's a realistic view. > The party or parties you disagree with may not share your views, but they do have many things in common with you. In The major party I disagree with least doesn’t share my most of views but has many things, in broad focus, in common with me. That’s very much not true of the major party I disagree with most. > In order to build bridges with other parties, it’s important to believe that the majority of people who get involved in government, regardless of party, are motivated primarily by the desire to serve their neighbors and their country Why would “building bridges with other parties” be a goal? A lot of people seem to have gotten ideas that the long realignment period from 1930s to the 1990s when the salient political divides were not along the same axis as the divide between the major parties (though they were approaching alignment at the end of the period) was a norm and not an aberration, and thus have fetishized bipartisanship which was simply a result of ideological factions crossing partisan boundaries rather than generally being contained within major parties. When that applies, you don't need to build bridges between parties, the factions inherently provide it; when it doesn't, you don't have a commonality to build on. And, in any case, this is the fallacy of argument to the consequences of belief – you are justifying a belief in a fact claim not by any evidence that it represents the actual facts, but by the notionally desirable consequences of believing it independent of its truth. > Without that belief, it will be near impossible to form agreements across the aisle. I actually think that its a lot easier to achieve agreements across the aisle, where there is utility in doings so, by observing the actual things that the specific goals the other side has in concrete terms and appealing to them, rather than fantasizing a distant abstraction like “serving the neighbors and their country”. The latter is only useful once you determine a concrete operationalization that comports with the actual behavior of the individuals involved, but that offers nothing between a low-level concrete model of interests that avoids any high-level abstractions. Now, to the extent that its often a concrete low-level interest that they want to be seen as motivated by the desire to serve their neighbors and their country, that may be useful, but that’s different than believing that that is their actual motivation. reply Aloha 13 hours agorootparentprevit is cynical - but its not wrong either. Nothing will change until average Americans are fed up with the status quo, and force change - that goes for basic things like making the parties work together. reply a_wild_dandan 13 hours agorootparentprevThat's a wonderful perspective, and largely shared by one party! If you can make the other party act like adults, we'll be in business. reply redeeman 8 hours agorootparentthats what both parties say. Those who make blanket statements like this tends to be partisan hacks filled with extremely low levels of information. reply thomastjeffery 13 hours agorootparentprevI won't believe that after seeing the overwhelming evidence to the contrary. reply the_gastropod 13 hours agorootparentprevLook, I'm not saying the Republican Party are Nazis. But let's just imagine they were. Would we still have to believe they were good-faith actors just trying to improve their country? I do not believe the majority of Republican politicians today are trying to improve the country. I think the majority are self-serving, self-interested, and corrupt. This isn't the party of George Bush—who I disagree with about virtually everything, but seemed genuinely interested in trying to do a good job. This is now the party of Donald Trump, Jim Jordan, Matt Gaetz, Marjorie Taylor Greene, James Comer, Ted Cruz, and so on. There is no equivalent to any of these characters on the left. There is no compromising with obviously bad-faith actors like this. reply iaaan 13 hours agorootparentAs a trans person (and you can substitute pretty much any identity that is commonly understood to be marginalized and the point stands), there is no middle ground to be found working with republicans. I'm either allowed to exist, work, own property, access healthcare, etc., or I'm not. I'm either being discriminated against or I'm not. I'm not interested in compromise here. reply lolinder 13 hours agoparentprevThis comment isn't directed at OP, it's directed at anyone reading this who might be tempted to get swept up in OP's stereotypes without thinking critically about them. I'm writing this as a left-leaning moderate who grew up among staunch conservatives and understands their philosophy very well. Conservatives sincerely believe that government bureaucracies are less efficient than a free market economy. That's not a cover or a motte and bailey, it's legitimately and literally true. Conservative politicians dismantle government when given the opportunity because that's what their base wants them to do because, again, their base sincerely believes that the government is bad at most things it does. It's true that Republican politicians (like most politicians) are mostly charlatans who are intentionally creating circumstances that reinforce the belief in the ineffectiveness of government, but OP's stereotype of conservative voters as simply wanting a \"daily dose of other people suffering\" is baseless, wrong, offensive, and extremely counterproductive. This stereotype is a misrepresentation of the other core tenet of conservative philosophy, which is that what is right and wrong is not up to humans to decide, it comes either from God or from long-standing and proven traditions. Conservative opposition to LGBT rights and similar have nothing to do with wanting to see people suffer, they have to do with their deep-seated belief that some things are simply wrong because something greater than us has said so. They can be wrong in that deep-seated belief, but it's unfair of OP to characterize it as sadism. reply whatshisface 13 hours agorootparentPart of the reason liberals hate conservatives and vice versa is that they think the government is actually representing their opponents. The reality is that influence is severely concentrated on every \"side,\" and things that average people believe are only used to justify actions that a truly influential coalition wants to take. Your disagreeable family relations are as powerless to get a new issue introduced as you are, but they're going to be blamed for whatever advances the oligarchs who are opposed to your oligarchs have recently made. reply thomastjeffery 12 hours agorootparentOn the contrary: most liberals (or anyone else who doesn't identify as a [neo]conservative) are painfully aware that the democratic party is failing to represent them. We just know that that failure is less damaging than what the Republican party is up to. The Republican party is the party of unification and engagement. The Democratic party is the tent for everyone else. The presence of the former demands the existence of the latter. reply Sohcahtoa82 10 hours agorootparent> The Republican party is the party of unification and engagement. The anti-LGBT party with a well-known track record of racism is the party of unification? No...no it's not. They're the party, that when asked to NOT be anti-LGBT and not be racist, cries about their freedom being repressed. The Republican party is the party of authority, tradition (Which is not necessarily a virtue), and conformity. They're the party of freedom, but only if you're a white Christian male, bonus points if you're rich. The Republican seeks to oppress minorities, and then when asked to not be hateful, act like they're a victim of thought policing. They spew hateful messages on social media, get rightfully banned for it, and then pretend they got banned for their conservative views, which of course is pretty telling. No, they're anything BUT the party of unification. They USED to be, but they let some loudmouth idiots become the face of the party. reply jl6 13 hours agorootparentprevIn this thread we see the iron law of 21st century American polarization and the uttermost death of nuance. I’m sure someone will come along to argue how nuance is a luxury we can’t afford in the face of these communist/fascist maniacs. reply Sohcahtoa82 10 hours agorootparentHonestly, I put some blame on the Internet. Before the Internet, people talked politics in person and nuance was included. Communication was synchronous, with instant feedback, and basically required engagement. You couldn't just walk away without upsetting social norms. But the Internet (and especially Twitter), changed all that. People don't want to discuss, they want to \"win\", so you get 1-sentence \"owns\" that are just straw men. Nuance gets thrown out the window. If someone you're arguing with comes up with an excellent point that you can't counter, it's easier to just not reply. You're not on the spot, facing a human, and having to admit out loud that they've got a point. Nope. Much easier to just ignore it and remain entrenched in whatever bullshit you believe. The other half of the blame is 24-hour cable news that has to constantly come up with shit to show, and now entertainment and news have become intertwined with a disastrous result. reply lokar 13 hours agorootparentprevI agree with your statements, and they were true until sometime between Newt taking over and Trump being elected. They used to have a a coherent positive viewpoint and policy to support it. And they sought to advance that policy through normal democratic means: convincing a majority of voters. That has stopped being their approach. They no longer seek a genuine popular majority. They are turning inwards, adopting ever more extreme positions disconnected from genuine ideals. They seek only the power to impose their worldview on others. They no longer feel constrained by long standing traditions and institutions. Any act is justified in their minds. reply ModernMech 8 hours agorootparentIt stopped being true after the Romney loss. They wrote a report [1] that basically outlined the fact that due to demographic trends and the makeup of Republican electorate, the RNC would have to start becoming a big tent, multicultural party in order to succeed in the future. The decided exactly the opposite -- they elected Trump and decided to become a party based on white Christian grievance. [1] https://en.wikipedia.org/wiki/Growth_%26_Opportunity_Project reply thomastjeffery 12 hours agorootparentprevRepublicans either want the suffering directly, or - what is most often the case - they want the system that guarantees that suffering will happen, and will refuse any alternative whatsoever out-of-hand. My parents don't want trans people to suffer: they want trans people to find happiness through the impossible avenue of just not being trans anymore. My parents don't want illegal immigrants to be incarcerated or murdered by border authorities: they want illegal immigrants to find liberty through the impossible process that is just becoming a legal immigrant, or living peacefully in whichever failed country they were born. My parents don't want people with substance abuse disorders to live and die on the streets: they want people with substance abuse disorders to overcome them through the impossible avenue of simply curing their own addiction without any outside support, safety, or encouragement whatsoever. I cannot convince them that any of this is the case. On the other hand, people like Tucker Carlson, Rush Limbaugh, and Glen Beck can convince them of just about anything. Why? Because right-wing talking heads have a foot in the door: belief. They abuse every belief that a conservative holds dear, and turn it into engagement. Critical thought has no air to breathe in a world made of belief. It doesn't matter what people want. It matters what people do. reply the_gastropod 13 hours agorootparentprevI hate to tell you this, but if you can believe it, they're for the second time now, electing the most sadistic candidate to represent their party. This guy has promised to deport millions of people, put them in \"camps\", use the military to quell \"woke\" protests, etc. The sincere Conservative electorate had every opportunity to choose a less-sadistic option. They chose. OP's characterization is perfectly valid. reply whatshisface 13 hours agorootparentTrump supporters resonate with that rhetoric because the rent is too high, groceries are too expensive, and inflation doesn't seem to apply to wages. The causes of unrest haven't changed in thousands of years, but they can be convenient to forget. reply PawgerZ 13 hours agorootparentI just don't understand how, if their problem is rent is too high and inflation doesn't apply to wages, they vote for Trump. He has made money his whole life by jacking up rent prices and paying people as little as he's legally allowed to (or less than that). reply whatshisface 12 hours agorootparentThink of it like a riot. What does smashing windows have to do with anything? Yet one follows the other. reply Retric 13 hours agorootparentprevDon’t confuse talking points with the underlying reality. Trump supporters existed when inflation was basically non existent. His support is really independent of the economic situation. It’s going to be interesting to see what happens in this time. He barely beat one of the least popular candidates in decades and then got crushed the next election cycle. Opposition candidates tend to do well when the economy is doing poorly, but he’s got a lot of baggage and the poles are dead even right now. reply whatshisface 12 hours agorootparentThe camps rhetoric is new. If anything that's further evidence that it's caused by the times rather than the personalities. reply Retric 9 hours agorootparentWhen support stays constant despite changing rhetoric it’s not about the rhetoric. reply Sohcahtoa82 10 hours agorootparentprev> Trump supporters resonate with that rhetoric because the rent is too high, groceries are too expensive, and inflation doesn't seem to apply to wages. And so they vote for the party that is against rent controls, against expanding food stamps, and against raising the minimum wage? Make it make sense. reply whatshisface 10 hours agorootparentSee my response to a sibling comment: \"Think of it like a riot. What does smashing windows have to do with anything? Yet one follows the other.\" Very few people have any idea about the causes of their suffering. reply lokar 13 hours agorootparentprevAnd Trump (and his sycophants) seek to take advantage of this feeling. Using the age old approach of blaming \"the other\" and seeking not any real improvement in conditions, but a consolidation of power in their hands. reply StillBored 13 hours agorootparentprevWell, then, maybe they should consider solutions for solving those problems rather than yelling \"big government\"/etc at every opportunity and further eroding the protections the previous generations put in place to keep things like this from happening. AKA, a lot of this is the result of generations of poor education, an education system that is strongly biased propaganda based on provably wrong economic models that tell k-12th graders that the best and only choice is the one where the free market runs roughshod over anyone who can't afford the rent, etc because that's simply \"capitalism\" and all the other choices are worse. reply the_gastropod 13 hours agorootparentprevMaybe? But how is Trump or the Republican Party planning to address any of these? Remember, Trump successfully pressured the Fed to lower interest rates while the economy was strong. Think that contributed a bit to the inflation we've been dealing with? Are they recommending corporate tax increases? New marginal tax brackets? No? Did they add tax loopholes for private jets and yachts while they were last in power? You bet! No, what they're doing instead is trying to scapegoat things like \"woke\" college students and immigrants. reply da_chicken 13 hours agoparentprevI think they have a clear plan in two parts. 1. Since the opposition seeks progress in many forms, blindly obstruct them in all cases. 2. Legislate the country back to 1953. When that is accomplished, legislate the country back to 1853. The only part I'm unsure about is whether they're interested in renaming the nation \"Gilead.\" reply redeeman 13 hours agorootparentif you're just halfway serious, perhaps its time to seek some medical help reply spaceguillotine 13 hours agorootparentThey published the plan and its pretty much what the parent comment said. https://en.wikipedia.org/wiki/Project_2025 The GOP wants fascism everywhere reply MOARDONGZPLZ 13 hours agorootparentprevThat’s a bit of an extreme comment “seeking medical help” but to be fair I’m only aware of the “to 1953” legislation and not the additional “to 1853” legislation. reply ModernMech 12 hours agorootparentAre you aware that Arizona recently reinstated an abortion ban from 1864? https://www.nytimes.com/2024/04/09/us/arizona-abortion-ban.h... reply redeeman 8 hours agorootparentdid they actually reinstate? isnt it just that newer legislation was made illegal, and it DEFAULTED back to this, where there is proposals by republicans to make legislation that in many ways mirrors most EU countries, but that is unacceptable to the democrats? reply ModernMech 8 hours agorootparent\"just that\".... the newer legislation was made illegal by Republicans, and Republicans decided the 1864 law is still valid. This is how you roll back the country 160 years. And it's not just that this particular law is literally from 1864, it's that even new laws are mirroring these 160 year old laws. Many have no exception for rape, incest, or the life of the mother [1]. That's barbaric. The reason that Democrats find all of this unacceptable is that Republicans have either failed to anticipate the chaotic consequences of their actions, or -- as they argue in court in support for jail terms for people who have abortions, and that doctors should not have exception in the case of life of the mother or rape -- that they intend the chaos and resulting harm to women. As people suffer and die due to their actions, it's not hard to see why rolling back laws to 1864 is objectionable to Democrats. I appreciate you bringing up EU",
    "originSummary": [
      "The Federal Communications Commission has voted to reinstate net neutrality rules to stop internet providers from blocking or slowing down competitors' services.",
      "This decision renews the government's control over broadband companies, likely sparking legal battles from the industry.",
      "Initially introduced during the Obama era, the rules were scrapped under the Trump administration, now stirring up renewed debate on the free internet, drawing both support and criticism."
    ],
    "commentSummary": [
      "The discussion involves restoring net neutrality rules by the FCC and debates on ISP compliance burden, government regulation, and societal impacts of net neutrality laws.",
      "Various perspectives on political dynamics, monopolies, libertarianism, and government agencies in policymaking are examined, alongside concerns on ISP practices and consequences of repeal.",
      "The dialogue touches on criticisms towards the Republican party, legislative decision-making, polarization effects on political discourse, media influence, and the impact on marginalized groups and social progress."
    ],
    "points": 826,
    "commentCount": 460,
    "retryCount": 0,
    "time": 1714065780
  },
  {
    "id": 40163405,
    "title": "Microsoft and IBM Partner to Open Source DOS 4.0",
    "originLink": "https://www.hanselman.com/blog/open-sourcing-dos-4",
    "originBody": "Open Sourcing DOS 4 April 25, 2024 Comment on this post [7] Posted in Open Source Sponsored By See the canonical version of this blog post at the Microsoft Open Source Blog! Ten years ago, Microsoft released the source for MS-DOS 1.25 and 2.0 to the Computer History Museum, and then later republished them for reference purposes. This code holds an important place in history and is a fascinating read of an operating system that was written entirely in 8086 assembly code nearly 45 years ago. Today, in partnership with IBM and in the spirit of open innovation, we're releasing the source code to MS-DOS 4.00 under the MIT license. There's a somewhat complex and fascinating history behind the 4.0 versions of DOS, as Microsoft partnered with IBM for portions of the code but also created a branch of DOS called Multitasking DOS that did not see a wide release. https://github.com/microsoft/MS-DOS A young English researcher named Connor \"Starfrost\" Hyde recently corresponded with former Microsoft Chief Technical Officer Ray Ozzie about some of the software in his collection. Amongst the floppies, Ray found unreleased beta binaries of DOS 4.0 that he was sent while he was at Lotus. Starfrost reached out to the Microsoft Open Source Programs Office (OSPO) to explore releasing DOS 4 source, as he is working on documenting the relationship between DOS 4, MT-DOS, and what would eventually become OS/2. Some later versions of these Multitasking DOS binaries can be found around the internet, but these new Ozzie beta binaries appear to be much earlier, unreleased, and also include the ibmbio.com source. Scott Hanselman, with the help of internet archivist and enthusiast Jeff Sponaugle, has imaged these original disks and carefully scanned the original printed documents from this \"Ozzie Drop\". Microsoft, along with our friends at IBM, think this is a fascinating piece of operating system history worth sharing. Jeff Wilcox and OSPO went to the Microsoft Archives, and while they were unable to find the full source code for MT-DOS, they did find MS DOS 4.00, which we're releasing today, alongside these additional beta binaries, PDFs of the documentation, and disk images. We will continue to explore the archives and may update this release if more is discovered. Thank you to Ray Ozzie, Starfrost, Jeff Sponaugle, Larry Osterman, our friends at the IBM OSPO, as well as the makers of such digital archeology software including, but not limited to Greaseweazle, Fluxengine, Aaru Data Preservation Suite, and the HxC Floppy Emulator. Above all, thank you to the original authors of this code, some of whom still work at Microsoft and IBM today! If you'd like to run this software yourself and explore, we have successfully run it directly on an original IBM PC XT, a newer Pentium, and within the open source PCem and 86box emulators. About Scott Scott Hanselman is a former professor, former Chief Architect in finance, now speaker, consultant, father, diabetic, and Microsoft employee. He is a failed stand-up comic, a cornrower, and a book author. About Newsletter Hosting By Comment on this post [7] Share on Twitter or Facebook or use the Permalink April 26, 2024 1:04 Did I miss it because I’m on my phone e, or is there no link to the source and downloads? Paul April 26, 2024 1:13 https://github.com/Microsoft/MS-DOS here it is starfrost April 26, 2024 1:59 This is wonderful stuff. I hope we can eventually get DOS 5 released - this was the first of the modern DOS's (DOS 5, 6.x being very similar), well getting the last ones would be good, I guess the drivespace thing may complicate things from the 6 side though. Stu April 26, 2024 3:04 @Stu The entire MS-DOS 6.22 source (along Windows 2000, NT, and bits of XP) were leaked a few years ago. They're still floating around the torrents and some websites. Nobody April 26, 2024 4:39 @Nobody Actually it was an incomplete unbuildable 6.0 beta, not 6.22 anonymous April 26, 2024 12:13 Why not simply release all MS-DOS source code up to the latest ones included with Windows 9x? Especially if 6.22 has leaked as a previous comment says. Hotdog April 26, 2024 13:52 wow.. but the problem is most of the developers are not able to understand source codes as they are written by GOD like developers. Is there any way to understand these codes what like what happens if I write a command and press enter? Sujit https://techsujit.com https://kaizen-apps.com sujit Singh NameEmail(will show your gravatar icon) Home page (optional)5+1=?Comment (Some html is allowed: a@href@title, b, blockquote@cite, em, i, li, ol, pre, strike, strong, sub, super, u, ul) where the @ means \"attribute.\" For example, you can useor . Live Comment Preview",
    "commentLink": "https://news.ycombinator.com/item?id=40163405",
    "commentBody": "Open Sourcing DOS 4 (hanselman.com)554 points by ndiddy 12 hours agohidepastfavorite213 comments pavlov 3 hours agoWhen I was nine years old, I liked poking around with a hex editor on my dad’s PC. I didn’t speak English and MS-DOS wasn’t yet localized to Finnish in 1989, so I decided to try translating it myself with a dictionary by manually finding and replacing strings in the SYS/COM files. The result worked and my dad was suitably impressed, if probably a bit peeved that nothing worked anymore in the shell as expected (since I had replaced all the basic command names too — “dir” became “hak” and so on). It’s pretty cool to see those strings again in src/MESSAGES. At the same time, it feels a bit sad that today’s kids can’t get the same feeling that the computer is really theirs to modify. Modern operating systems don’t run binaries tampered with a hex editor. Most kids are on operating systems like iOS where they can’t even run a C compiler. They can play with code in various sandboxes locally and on the web, but the computer fundamentally belongs to someone else today. reply zooq_ai 45 minutes agoparentMy favorite Hex Editor hack was when I cracked a Windows software simply by changing the instruction \"Equal to\" to \"Not Equal to\" where it matches for Software Key with user entered key. reply lm2s 1 hour agoparentprevI did that with GTA, translating it to Portuguese. It was then that I learned that I could overwrite the strings with the Hex Editor, but not insert anything because it would stop working. And thus began my dive into computers. Great memories, thanks for making me remember it. * Actually now that I really think about it, it wasn't with an Hex Editor, it was with Edit! Fun times. reply smokel 2 hours agoparentprevI remember trying to rename LILO (from \"Loading Linux\" fame) to PIPO [1] by simply editing the bytes with a hex editor. Turned out that didn't work, because there was an additional sanity check that halted the boot process if the \"LI\" bytes were corrupted. Of course I put through and was a happy user of PIPO for some years, until Grub came along. [1] https://en.m.wikipedia.org/wiki/Pipo_de_Clown reply thaumasiotes 1 hour agorootparentWhen I was running Gentoo, I wanted to replace the GNOME foot that appeared on the dropdown menu with a Gentoo-fish-in-a-wizard-hat icon. I found documentation suggesting that the icon shown on the menu was set in a certain configuration file, and changed that file. This meant that, when I was using the normal UI to customize the GNOME topbar, the icon associated with that menu, in the GUI, was the fish-wizard icon. But it did not change the icon displayed in the menu itself. I always resented that. I still don't like the concept of hiding configuration lest the user change it. reply bilekas 2 hours agoparentprev> MS-DOS wasn’t yet localized to Finnish in 1989 That's genuinely something today I appreciate but when putting the 10th floppy in to update windows3.x. relative these days! > it feels a bit sad that today’s kids can’t get the same feeling that the computer Can't agree more. trying to get my cousins and nephews interested is in their term \"Not important\" reply josefx 2 hours agoparentprev> Modern operating systems don’t run binaries tampered with a hex editor. Luckily that isn't universally true. I had to do a decent amount of binary modifications on Linux to deal with bugs and glibc compatibility issues. reply stepupmakeup 3 hours agoparentprev>Modern operating systems don’t run binaries tampered with a hex editor. do you mean non-system ones? reply grishka 1 hour agorootparentMacOS is notorious for this. By default, it would only run binaries signed with an Apple-issued certificate. You can bypass this multiple different ways, of course, but that requires knowing that it can be bypassed in the first place. Then there are mobile OSes where you don't get to see the binaries at all. Yes you can repack an apk but again, that's a more involved process requiring specific tools and knowledge (and very awkward to do on the device itself), and iOS is completely locked down. reply eru 3 hours agorootparentprevSome OSs want their binaries to be signed and probably have checksums etc. It would be hard to keep those valid when mucking around with a hex editor. reply conkeisterdoor 10 hours agoprevIt looks like \"brain damaged\" was the developer's go-to insult when frustrated :D 2024-04-25 19:35 ~/sort/dl/MS-DOS % grep -nri 'brain[ -]damage' . ./v4.0/src/DOS/STRIN.ASM:70:; Brain-damaged TP ignored ^F in case his BIOS did not flush the ./v4.0/src/DOS/PATH.ASM:24:; MZ 19 Jan 1983 Brain damaged applications rely on success ./v4.0/src/DOS/FCBIO.ASM:28:; MZ 15 Dec 1983 Brain damaged programs close FCBs multiple ./v4.0/src/DOS/FCBIO2.ASM:28:; MZ 15 Dec 1983 Brain damaged programs close FCBs multiple ./v4.0/src/BIOS/MSBIO1.ASM:82:; REV 2.15 7/13/83 ARR BECAUSE IBM IS FUNDAMENTALY BRAIN DAMAGED, AND ./v4.0/src/CMD/PRINT/PRINT_R.ASM:1772: ; See if brain damaged user entered reply ssklash 5 hours agoparentSuper cool to see MZ initials, which are for Mark Zbikowski. They are still to this day at the beginning of every Windows executable/PE file. reply aquir 3 hours agorootparentWhat? Is this what MZ means? Fascinating! reply stubborngoat 2 hours agorootparent\"He was the designer of the MS-DOS executable file format, and the headers of that file format start with his initials: the ASCII characters 'MZ' (0x4D, 0x5A)\" Ref: https://en.wikipedia.org/wiki/Mark_Zbikowski reply thrdbndndn 42 minutes agorootparent> In 2006, he was honored for 25 years of service with the company, the third employee to reach this milestone, after Bill Gates and Steve Ballmer. He retired the same year from Microsoft Considering he \"only\" joined MS in 1981 (which was found in 1975?), I'm surprised no more people between him and Bill stayed at MS. Also shouldn't Paul Allen still count (from Wikipedia, \"Allen resigned from his position on the Microsoft board of directors on November 9, 2000, but he remained as a senior strategy advisor to the company's executives.\")? Edit: wow, never knew he worked at Valve, too. reply HeckFeck 27 minutes agoparentprev> ./v4.0/src/BIOS/MSBIO1.ASM:82:; REV 2.15 7/13/83 ARR BECAUSE IBM IS FUNDAMENTALY BRAIN DAMAGED, AND Hardly surprising. If you grep the leaked NT3.5 sources for the f-word, you will find similar comments directed towards IBM. reply blobbers 10 hours agoparentprevLet's bring back those DOS 4.0 period sayings. Does this mean if you happen to be talking to BillG you could talk about brain damaged programs and he'd nod appreciatively? reply mormegil 4 hours agorootparentOh, you mean like it's some period saying? I'm using it normally. But then again, it's not like I'm keeping my finger on the pulse of the current trends... See also http://catb.org/jargon/html/B/brain-damaged.html reply asveikau 5 hours agorootparentprevSomehow seeing the phrases \"brain damaged\" and \"DOS 4.0 period\" makes me think... Bill Cosby's \"Brain Damage\" bit, from the album \"Himself\": 1982 DOS 4.0: 1988 Those comments also could have also sat in the tree for a few years. reply glhaynes 8 hours agorootparentprevIt was BillG who famously called the 286 \"brain damaged\" after all. EDIT: Upon doing a web search, actually it looks like it was \"brain dead\". reply sponaugle 10 hours agoprevAs Scott mentioned in the blog post, we were able to get this running on one of my original IBM XTs with an original IBM monochrome display adapter and display. It was very cool to be able to switch between a running version of a small game, Turbo Pascal, and a DOS prompt with a single key press. It is always great to have period software on period hardware! (added: Short video of it running - https://www.youtube.com/watch?v=YPPNbaQaumk) reply mattl 9 hours agoparent> It is always great to have period software on period hardware! Really is. This is why I keep a load of old hardware around. Stuff like Mac OS 9 should be run on real hardware and same for old MS-DOS. reply at_a_remove 4 hours agorootparentIf not for space considerations, I would be right there with you. I still have hardware and software to get me back to NT 4.0 or Windows 95 (OSR 2, please, it wasn't tolerable before that). I haven't needed to in a while, but in a previous job, we'd run across old disks in some author's archive and I'd go home to dig around, find a 5.25\" drive and rig something up, reach back in time. I could maybe do Windows for Workgroups 3.11. If shipping weren't so brutal, I would love to send off my old stuff to someone who would use it. I still have working SCSI equipment! I bet somewhere there is someone stymied on trying to liberate some ancient works but for the necessary hardware/software setup. reply metadat 4 hours agorootparentSCSI drives almost never go bad. Compared to IDE/SATA, they were significantly better built and had lower failure rates. I still have a few 15k RPM Cheetahs that still work, last I checked :). reply HeckFeck 25 minutes agorootparentThe SCSI drive in my Mac SE is older than me and it still works. Delightfully clicky, in fact louder than the floppy drive! reply moody__ 10 hours agoprevI appreciate the work in getting this open sourced but I find it telling that this had to be done through an outside motivator. There seems to be no internal \"ticking clock\" to get some of these things out in to the open. That's fine no one is owed the source code for this stuff or anything, but it would be nice if there was more interest on the side of the companies to get some of their formative history out so people can learn from it. reply shanselman 8 hours agoparentThat's valid feedback. There is no clock, but there maybe should be. In this case, yes, Jeff and I had to PUSH. And that's a hassle. I'll ask around. reply davidferguson 46 minutes agorootparentIt's fantastic work you've done. As someone who works at a older software company (founded early 80s), I'm sad that there isn't a push internally for us to make our old software source available, or even just the binaries available! What sort of tactics did you use to convince them? Maybe I can apply them to where I work too... reply dlachausse 6 hours agorootparentprevI appreciate your hard work on open sourcing this! If you’re taking requests I’d love to see MS-DOS 5.0 or at least QBasic next. reply shanselman 6 hours agorootparentYep, I need to get 3.3 and then do 5, 6 reply justin66 2 hours agorootparentIronically those words capture how those of us who bought a computer with DOS 4.00 felt about it. :D But this is really great. reply atlas_hugged 3 hours agorootparentprevLegend. Keep being the Bob Ross of IT. Love what you do. Thank you. reply raverbashing 1 hour agorootparentprevTrue, I don't know when Qbasic (and Edit?) went into dos (according to Wikipedia they were on 5.0) reply HeckFeck 22 minutes agorootparentprevWindows 95 OSR5 - Open Source Release. Someday soon? reply macdice 3 hours agorootparentprevThis is fantastic work, thanks. Hmm, what else... let's see... Xenix also really, really wants to be free! What a magnificent piece of forgotten computer history it is. https://en.wikipedia.org/wiki/Xenix reply justin66 2 hours agorootparentThat actually would be pretty wild. reply chucky 3 hours agoparentprevIt might not be a problem for DOS 4, but often the source code of software that was only ever meant to be published as closed source contains source code that was licensed from 3rd parties. This license may not allow publishing the source code. Doing an investigation of what licensed software was used and possibly trying to get permission from the relevant rights holders (if you can even figure out who owns the rights so many years later) can be a big and expensive task, unfortunately. I understand why companies might not want to take that on (even though it sucks). reply skissane 3 hours agorootparentFor DOS, I believe the core was only ever Microsoft or IBM. Some DOS versions bundled add-ons by third parties, but they are hardly essential for operation - e.g. MS-DOS 6 included DEFRAG and MSBACKUP (both licensed from Symantec) and MSAV (licensed from Central Point Software) Similarly, with Windows, the third-party components are generally inessentials such as certain device drivers, games, some optional system components like the ZIP file support in Windows Explorer-you would still have a usable OS with these bits ripped out. Parts of NTVDM are third-party licensed, although I believe that’s mainly the software CPU emulator used on RISC platforms, I think x86 was mostly Microsoft’s own code reply chucky 1 minute agorootparentYeah, but it's the \"mostly\" and \"I think\" that will cause lawyers to start sweating and force someone to do a bigger and more expensive investigation. reply NewsaHackO 6 hours agoparentprevFor some insight, look how people are combing for curse words/ devs making jokes about people being brain damaged etc. There is no upside for the company, and all that has to happen is some unsavory politically incorrect joke to get missed from sanitization and the are on the cancelled chopping block. reply Someone 6 minutes agorootparentLegal also may be concerned that having source makes it easier to detect patent infringement and code copying. Even if you deem the risk zero that that actually happened, why run the risk of somebody claiming you did? For the company there are as good as zero downsides to not doing anything, and a few small upsides and a few low risk, but potentially very costly (in dollars or reputation) if they happen downsides. That makes not doing anything the default choice for the company. For (former) employees who worked on this, the upsides are higher; they’ll get some of their work published for the first time. That’s why we see individuals push for this every now and then. reply HeckFeck 18 minutes agorootparentprevI think I lost it when they suggested we stop using the term 'sanity check' or 'sane defaults' because they might offend, well, brain damaged people. I am close to writing a browser extension that does a find and replace to reverse change these imposed, humourless, coddled changes. reply userbinator 6 hours agorootparentprevThe \"cancel\" stuff only comes from a tiny minority of vocal extremists. Everyone else is entirely unfazed. Of all the things people here probably hate about the current \"modern\" Microsoft and its products, political incorrectness in decades-old code is far down the list or not even a consideration. reply saagarjha 5 hours agorootparentprevNo upside for the company? Do you not see all the people who are delighted to be able to browse the code? reply ssdspoimdsjvv 1 hour agorootparentNo commercial upside for sure. Just some extra reputation points among a very niche group and maybe some happy employees. reply sponaugle 9 hours agoprevHere is a small video of it running on original IBM XT: This was right after we got the floppies read successfully. Serious Kudos to Scott for getting these floppies and the permission to release them! https://www.youtube.com/watch?v=YPPNbaQaumk reply jmspring 7 hours agoprevMy brain is rusty, but I feel like MSDOS 5.11 was where things finally just worked. TSRs, memory managers, etc. Moving a lot and not being a packrat I've lost some of that history. It'd be interesting to see 5.x and 6.x released. reply jagged-chisel 6 hours agoparentIIRC 6.22 was the pinnacle of MS-DOS, with all the comfort and niceties one could expect at the time. Then began the Windows era. reply userbinator 6 hours agorootparentI think the pinnacle goes to MS-DOS 7.1, which while bundled with Windows was also usable as a DOS by itself and contained features like FAT32 support. MS-DOS 8 was the last version that came with the ill-fated Windows Me and significantly neutered. reply jmspring 6 hours agorootparentprev6.22 would likely be what I was remembering. reply Zardoz84 3 hours agorootparentprevI had good memories of DR-DOS 5 reply ListenLinda 6 hours agoparentprevI think it was 3.3x where things started working. I don't recall 4.x being around much. I do remember 5 and 6. For some reason 4 never made a splash in my circle of friends. reply billforsternz 4 hours agoparentprevI was kind of happy with MSDOS 2.11, I felt that they'd got the basics in place (in particular hard disk / subdirectory support) and that bloat hadn't started. From memory I used this for years and years although I was young so time didn't rush past so quickly so who really knows. I kept a version of MSDOS 2.11 debug.com around for decades (patched with itself so it wouldn't just do a version check then quit). From memory it was something like 12K bytes whereas debug.exe from MSDOS 6.x was more like 60K bytes. reply nullindividual 11 hours agoprev> ; Check for presence of \\dev\\ (Dam multiplan!) [0] I assume this refers to the spreadsheet application. [0] https://github.com/microsoft/MS-DOS/blob/main/v4.0/src/DOS/D... EDT: That one policy hadn't quite gone into effect yet ;-) [1] [1] https://github.com/microsoft/MS-DOS/blob/main/v4.0/src/CMD/C... For posterity... > RESEARCH: > invoke path_search ; find the mother (result in execpath) > or AX, AX ; did we find anything? > je badcomj45 ; null means no (sob) > cmp AX, 04H ; 04H and 08H are .exe and .com > jl rsrch_br1 ; fuckin' sixteen-bit machine ought > jmp execute ; to be able to handle a SIXTEEN-BIT reply qingcharles 11 hours agoparentLOL. Nice find. Company open-sources old product? Search for curse words...! reply keyle 11 hours agorootparentI obliged DOSHIT: INC [HIT_DOS] reply zdimension 11 hours agoprevhttps://github.com/microsoft/MS-DOS/blob/2d04cacc5322951f187... > ; REV 2.15 7/13/83 ARR BECAUSE IBM IS FUNDAMENTALY BRAIN DAMAGED, AND Someone was angry that day. Wonder if any IBM dev ever stumbled upon this when MS and IBM worked together. reply freedomben 10 hours agoparentThis was the good 'ol days, back when you could put stuff in comments like that and not worry about legal reply elwell 10 hours agorootparentNow you can't even say \"sanity check\", much less mention brain damage. reply JoachimS 4 hours agorootparentAnd please don't use 'brain dead', which is a mortuist expression. Using 'brain dead' in a negative way may be insensitive to zombies. Also avoid the expression 'zombie process' or the word 'expired'. You don't want to step on the toes of a zombie do you? (It's probably gross.) reply steelbrain 9 hours agorootparentprevWait, I missed the bus on this one. What's wrong with \"sanity check\"? reply arp242 8 hours agorootparentAccording to some it's ableist language, e.g.: https://developers.google.com/style/inclusive-documentation#... Not really interested in discussing this and not saying I agree (or disagree!) with that. Just answering the question. reply leeoniya 8 hours agorootparentprevsince 1984 no one was offended by it, but in 2024 it became offensive to insane people AFAIK \"sanity check\" doesnt even have a history like \"retarded\" (which actually became an offensive slur). maybe some folks just feel left out if they cant rage about and feel offended by something these days. ::eyeroll:: reply rnd0 8 hours agorootparentprevThese days it would be considered a problematic allusion to people with cognitive or emotional issues. reply Stratoscope 8 hours agorootparentprevAs someone who recently asked our QA team to change the name of one of our test suites from \"sanity test\" to \"quick test\", maybe I can provide some perspective. Many software developers like me have faced challenges to our mental health. Indeed, there has been a time or two when I questioned my own sanity. And perhaps the sanity of some of my colleagues! It's not so much that there is something inherently wrong with the term \"sanity check\". We all know what it means. It's just that there are more descriptive and neutral terms available to us, so why not use them? reply bigstrat2003 8 hours agorootparent1. Other terms are neither more descriptive nor more neutral. 2. If someone has a problem with the term \"sanity check\", they are overly sensitive and they need to adjust to the realities of life. It is unreasonable to expect everyone to coddle them. reply elwell 6 hours agorootparentCan we please change PHP's die() function to unalive()? reply eesmith 4 hours agorootparentBut that's what you call to make your process be a zombie. ;) https://en.wikipedia.org/wiki/Zombie_process reply duskwuff 6 hours agorootparentprevIt's been synonymous with exit() since at least PHP 4. reply kyleee 6 hours agorootparentprevunalive() offends me, let’s do sunset() instead reply latentsea 6 hours agorootparentsunset() triggers me due to all the times I've seen companies \"sunset\" APIs only to still have them in production 5 years later. Let's change it to cancelled() reply randomdata 5 hours agorootparentcancelled() makes it seem as if PHP is a sexual predator – which may be the case, but as a matter of course... How about goodbye()? reply elwell 4 hours agorootparentgoodbye() could offend someone whose father \"went out to get a pack of smokes\" and never came back reply mikestew 5 hours agorootparentprevWe shouldn’t be using judgemental words like “good” and “bad”. Let’s use bye(). reply handojin 4 hours agorootparentprevseems lacking in gratitude. how about kthxbye()? reply exe34 4 hours agorootparentprevThis is double plus good. reply acuozzo 7 hours agorootparentprev> It's just that there are more descriptive and neutral terms available to us, so why not use them? \"quick test\" does not convey what is usually intended with \"sanity test\". The intention of the latter is to verify that a base set of assumptions hold true whereas \"quick test\" can be just about anything. reply WalterBright 8 hours agorootparentprev> It's just that there are more descriptive and neutral terms available to us, so why not use them? Because it's boring. reply Stratoscope 6 hours agorootparentWalter, you and I have known each other for a long time. Not in person; we've never met. But through our interactions here on HN. I have a lot of respect for you and for everything you have accomplished. So I have to ask you directly: Is being \"boring\" or not the way we should decide how to express ourselves? Regarding a \"quick test\" vs. a \"sanity test\". Instead of judging this on what is \"boring\", why can't we make a choice on which is more respectful to our peers and colleagues? reply drey08 4 hours agorootparentHave you ever heard someone get called slow? That's an insult too. So now you have quick tests. That's disrespectful towards me because I got called slow once. This can go on forever. reply justinclift 2 hours agorootparent\"The test who is on a performance-improvement-plan\" ;) reply exe34 4 hours agorootparentprevCan we get rid of the \"failing\" terminology? I think we should call it something more meaningful, such as \"it shows there's scope for improvement\", or \"it highlights areas for more focus\". reply at_a_remove 4 hours agorootparentprevWe should make choices based on communication, first and foremost. Words that do not communicate are not useful. A \"quick test\" implies almost nothing, other than it is faster than some other, unnamed test out there. Is a quick test good or is it bad? No way to tell. This term, quick test, it does not inform. On the other hand, a sanity test, well, it is more evocative. You definitely want to pass a sanity test. If you reach back to your psychology classes, they talked about a four-part test that is useful for determine if a behavior is sane or not. 1) Is it abnormal? Unusual? Out of the ordinary? for the situation. 2) Is it unjustifiable, unreasonable given the circumstances? 3) Is it counter-productive? Which is to say, does the behavior serve the individual, or does it in fact make things worse? Or simply do nothing? 4) (and this is where my memory is fuzzy) I think it involves personal distress. For the life of me I can't find it via Google and it annoys me. Now, interestingly, a sanity test matches the first three! (The fourth, well, there is no \"I\" in the program to be distressed) In programming, a sanity test looks for something abnormal, not reasonable for the program, and represents a state that won't get us the desired output. In short, it's highly congruent to the other context for sanity. This term, it communicates, and in a way \"quick test\" does not. In general, one of the main critiques of political correctness which few really notice is that the new term is less specific and less useful, that it fails at communicating. Consider when \"Oriental\" fell off of the euphemism treadmill for \"Asian.\" We all knew that the first term referred to a particular part of the world, from the Latin for \"east.\" (Ex lux orient and so on) \"Asian,\" however, could refer to people in Russia or people in India, but nobody in the US does (I note in the UK India does get the \"Asian\" pass). So here the new term is less specific and less useful, and is confusing to boot because we deliberately ignore parts of Asia when using the term \"Asian.\" If you want something to supplant \"sanity test,\" you gotta work harder for it than \"quick test.\" reply eesmith 4 hours agorootparentWe have literally no idea what the 'sanity test' involved actually did, and the common use in computing has little connection to any such use in psychology. https://en.wikipedia.org/wiki/Sanity_check informs me: \"A sanity check or sanity test is a basic test to quickly evaluate whether a claim or the result of a calculation can possibly be true.\" ... \"In computer science, a sanity test is a very brief run-through of the functionality of a computer program, system, calculation, or other analysis, to assure that part of the system or methodology works roughly as expected. This is often prior to a more exhaustive round of testing. \" ... \"In software development, a sanity test (a form of software testing which offers \"quick, broad, and shallow testing\"[1]) evaluates the result of a subset of application functionality to determine whether it is possible and reasonable to proceed with further testing of the entire application.\" and further comments that \"sanity test\" for some people is interchangeable with \"smoke test\". It also adds: \"The Association for Computing Machinery,[8] and software projects such as Android,[9] MediaWiki[10] and Twitter,[11] discourage use of the phrase sanity check in favour of other terms such as confidence test, coherence check, or simply test, as part of a wider attempt to avoid ableist language and increase inclusivity. \" I could not find a definition for \"sanity test\" in psychology. I know about the cognitive test that Trump made famous with ‘Person, woman, man, camera, TV‘. reply at_a_remove 4 hours agorootparentI cannot cop to \"smoke test\" being interchangeable with \"sanity test,\" in any way. Smoke tests are obvious crashes, often before any real input. I had a friend on the build team of Windows NT 5.0 (until it was called Windows 2000) and their version of smoke test was \"can it boot up to the login screen,\" and that's similar to working in electronics, where powering up your machine or circuit is all you do and you hope it doesn't release the magic smoke, before you actually do anything with it. On that alone, I am very doubtful on that article. This is also distinguished from what I have taken to calling a \"pre-flight checklist\" at the start of large programs, making sure databases are connected, requisite tables are present with the correct columns and datatypes, that specific files or directories exist, that files may be of a certain \"freshness,\" and so on. I'll stand by it: sanity test is a lot more descriptive and useful than \"quick test.\" Those footnotes are only kicking the can down the road of justifiability. But let's examine them according to the aforementioned The PC Replacement is Less Useful criterion. Recommending just \"test\" is ... well, double-plus ungood, in the sense that we now have no idea of the qualities of this test and how it could be distinguished from some other test. Hard pass. \"Confidence test\" only implies a statistical likeliness, like a confidence interval. It does not imply the This Ought Not to Happen of a sanity test. \"Coherence test\" is interesting but ... coherent against what? If someone wants to sell me on a replacement, they are free to try, but the replacement must be at least as good at communicating what it does as the term sanity test. If this fails, then it will be scoffed at, and should be. reply justinclift 2 hours agorootparentprev> Many software developers like me have faced challenges to our mental health. It's a stressful industry at times, probably almost everyone has had crap times at one point or another. And then there's Covid, which affected everyone. Doesn't mean every mention of the word \"sanity\" needs to be expunged from our language though. :( :( :( That way lies er... madness. (!) ;) reply exe34 4 hours agorootparentprevI've struggled with mental illness all my life and have made several attempts to catch the bus. I'm diagnosed with several mental issues. Somehow I'm still not offended with the phrase \"sanity check\". What am I doing wrong? Is it possible to learn to get offended by this sort of thing? Will it make me happy? reply com2kid 3 hours agorootparentSoftware engineering is a field full of neurodiverse people. Trying to police each other's language around mental health in a field where, at least historically, the majority of practitioners are going through some sort of mental health struggle, is borderline rude IMHO. reply HeatrayEnjoyer 4 hours agorootparentprevI've always thought it a strange and insensitive phrase. reply eesmith 4 hours agorootparentprevOh, you had to worry about legal. But since lawyers are concerned with risk, if the source is all proprietary and internal, the risk was lower. When Mozilla went open source back in the 1990s, Netscape's lawyers required the source first be bowdlerized. (Search for \"censorzilla\" for some examples.) reply justinclift 11 hours agoparentprevWell they're not wrong, so maybe the comment was just prescient? :) reply wormius 10 hours agoparentprevIt's what happens after Ballmer exceeds the peak. reply msoad 1 hour agoprevSome fun things I found in the source code * \"TWIDDLE\" [1] ; DO THE 2 SECOND TWIDDLE. IF TIME >= 2 SECONDS, DO A VOLID CHECK. * \"This is my employee serial!\" [2] db '21',0,0 ;J.K. 11/8/86 This is my employee serial # !!! * Lots os \"brain damaged comments [3] [1][https://github.com/microsoft/MS-DOS/blob/2d04cacc5322951f187...] [2]https://github.com/microsoft/MS-DOS/blob/2d04cacc5322951f187... [3]https://github.com/search?q=repo%3Amicrosoft%2FMS-DOS+BRAIN+... reply lchengify 11 hours agoprevSo if MS-DOS 4 was released in 1986, and it is now 2024, that's a 37 year gap between release and open source. That means Windows XP should be open sourced by ... 2038. Not as far away as it seems. I'll add it to my calendar. reply grishka 6 hours agoparentI doubt Microsoft would ever open-source any NT Windows versions because the current ones are based on the same code, just with added touchscreen nonsense, adware, and overt contempt for the user. We may see Windows 9x open-sourced. But then again, it's a stretch because Win32 API is still in wide use today. Releasing the sources for 32-bit Windows versions even this old may have an adverse effect on Microsoft's market domination. But maybe ReactOS will reach beta by 2038. Does this count as an open-source version of Windows XP? :D If you really wish to look at XP sources and don't care much about the legal aspect of it, you can do so right now. They were leaked. reply redox99 1 hour agorootparent> Releasing the sources for 32-bit Windows versions even this old may have an adverse effect on Microsoft's market domination. I disagree that releasing Windows 9x source code would have any impact on MS market domination. > I doubt Microsoft would ever open-source any NT Windows versions because the current ones are based on the same code Nowadays releasing something NT like XP may seem crazy. But in 15 years it will be so far away from future Windows, that it won't be that crazy. reply grishka 1 hour agorootparent> But in 15 years it will be so far away from future Windows, that it won't be that crazy. It's not like the NT kernel will be going away from current Microsoft products anytime soon. reply pjmlp 2 hours agorootparentprev> I doubt Microsoft would ever open-source any NT Windows versions because the current ones are based on the same code, just with added touchscreen nonsense, adware, and overt contempt for the user. Initiatives like MinWin and OneCore, secure kernel, device guard,... caused lots of rewrites and moving code around. reply rasz 4 hours agorootparentprevNT sources leaked, same for 2000. There is also leaked DOS 6 beta. The only thing releasing stuff this old brings is nerd goodwill. reply grishka 4 hours agorootparentAll open-source projects that deal with reimplementing parts of Windows, particularly Wine and ReactOS, consider those leaked sources radioactive and would not accept any patches if there's even a slightest suspicion that the patch author gleaned anything from those sources. Those same sources officially released under an open-source license would change that. reply justin66 1 hour agorootparentI wouldn’t assume Microsoft execs view increased capabilities to run windows programs in Linux as a bad thing, when they think about the matter at all. They would certainly prefer that such a capability be developed by someone else, so they don’t have to support it. reply isp 11 hours agoparentprev2038, you say? If your calendar is based on Unix epoch time, then ensure that you have upgraded to 64-bit timestamps before then. reply hypercube33 10 hours agorootparentReactOS will still be buggy AF by then I'm sure. I had hoped they'd at least have it to Windows 2000 alpha levels by now. reply nullindividual 11 hours agoparentprevEven ancient Windows includes many 3rd party libraries. I would not expect any Win 9x or NT 3.51+ version of Windows to be open sourced in it's entirety. I hope I'm wrong. reply philistine 8 hours agorootparentYeah, just the font stuff was such a mess. I’m hopeful someone will power through those problems. reply GalaxyNova 11 hours agoparentprevA lot of XP components are still in use in modern windows, whereas DOS was completely replaced around the time Windows XP came around. reply chx 11 hours agorootparentAround the time Windows 2000 came around. Up to Windows 3.11 it was a GUI on top of DOS. Windows 95, 98, Me used DOS to boot and it was still possible to stop the booting process at a DOS prompt (although in Me this was no longer official). Finally Windows 2000 had nothing to do with it as it is NT based. reply optimalquiet 11 hours agorootparentWindows 2000 was part of the professional NT line, though, and was the companion of Me for the millennium releases. As far as I know, 2000 wasn't marketed to home users. I think what the comment you replied to is saying is the the transition away from DOS wasn't completed for both professional and home markets until XP, which unified everything under NT for all markets. reply stevetron 9 hours agorootparentAround the year 2000, I was studying computer science at a university. Most of their PC's ran on Windows 3.1. I was using it at home. But one day, Microsoft sent me an offer: I could purchase the student release of Windows 2000 workstation for a mere $25.00. I went for it, and found it better than the Windows NT nap-sayers at school said. I don't know why I was contacted. Probbably because of other Microsoft programs I'd bought at the student bookstore. reply tracker1 8 hours agorootparentWindows 2000 was a pretty great OS. Used to enjoy using a Litestep shell instead of explorer. While it wasn't great for a lot of games, many did run fine. I liked it a lot better than OS/2 that I ran previously. I generally ran 2-4x the amount of RAM as most did. Still do. Pretty sure this made a lot of the difference. reply blkhawk 1 hour agorootparentHey, Listestep what a blast from the past :) I rain it until it wouln't run sensible anymore in Windoes 10. I then ditched Windows for Linux soon after - I can recommend KDE Plasma if you want to have something thats sorta configureable enough like Litestep was. reply autoexec 5 hours agorootparentprevWindows 2000 Pro was what I used at home for a long time and it was great. NT 3 and 4 were absolutely terrible which might explain your NT naysayers at school. I never once had to reapply a service pack in Win2k reply ZiiS 2 hours agorootparentStill remember the first time I touched Windows NT 4. Half an hour into work experience: Opened up a printer dialogue set a setting that hard crashed the PC; then slowly every other PC in the building as soon as they tried to print (i.e. just as they had _finished_ whatever they were working on; but often just before they _saved_ it). reply skeeter2020 10 hours agorootparentprevthis is accurate; the 2000 line targeted business, and if you remember having a consumer computer with 2000 pro it didn't support a lot of hardware. reply freedomben 10 hours agorootparentCan confirm. I upgraded my 98 box to 2000 and never did get some of my hardware working. When I told people I was using 2000 everybody assumed I had stolen it from work. I didn't. My friend stole it from work and shared it with me ;-) reply 486sx33 10 hours agorootparentprevNice part of that pain came when XP was released. Win 2000 drivers mostly all happily loaded into Win XP ! reply voidfunc 10 hours agorootparentprevDrivers were kinda a mess from what I remember in 2000 especially on the graphics card side of things. The HW vendors needed more time to switch over. reply fuzztester 9 hours agorootparentprevTangent, but Windows NT had a POSIX subsystem for a while. reply Dwedit 9 hours agorootparentprevWindows 2000. Also NT4, NT3.51, NT3.5, NT3.1... reply Zardoz84 2 hours agorootparentprevWindows 9x and ME, yet used bits Iog DOS beyond bootstrapping. They were using config.sys to load drivers reply userbinator 9 hours agoparentprevI'm more interested in them open-sourcing something from the 3.x/9x line. NT seems to have been far more studied, and of course there were the infamous leaks of those along the way. reply 1970-01-01 11 hours agoparentprevWe need to wait for the NSA backdoors to expire first ;) reply hudo 1 hour agoprevAh, good ol' days of configuring AUTOEXEC.BAT and CONFIG.SYS to squeeze few more kb of RAM:) And setting IRQs for some weird sound card that just doesn't work! reply dale_glass 40 minutes agoprevThere's a thing I've been curious about for a while, might as well try and ask here. What does \"DEVICE=\" in config.sys actually do? As in, why is this in config.sys: device=c:\\dos\\emm386.exe and not just: c:\\dos\\emm386.exe in autoexec.bat? reply dlachausse 12 hours agoprevHoly cow, this is huge! DOS 1.x and 2.x are too old to run a lot of vintage DOS software, but 4.x would run just about everything. reply chx 11 hours agoparentNo, you are thinking of 4.01 this is 4.0. Those are very different operating systems, this is DOS 3.2 + later abandoned very crude multitasking features. Roughly. And this matters because DOS 3.3 was a milestone. 4.01 comes from 4.00 which has nothing to do with 4.0 (yay for versions). reply happycube 9 hours agorootparentThe source released is for the IBM-started version of DOS 4.0, but most of the talk here is about the Multitasking DOS 4 files (a binary copy of the first beta) reply rnd0 11 hours agorootparentprev>And this matters because DOS 3.3 was a milestone DOS 3.3 couldn't understand large partitions -except for Compaq dos 3.31. But regular dos 3.3 couldn't. I don't think dos 3.2 could even understand extended partitions/logical drives -much less large disks. Still -pretty neat! reply tssva 8 hours agorootparent“DOS 3.3 couldn't understand large partitions -except for Compaq dos 3.31.” This is not accurate. Several OEMs added proprietary variations of FAT which supported larger partitions. For instance I run Zenith MS-DOS 3.30+ which has this ability on a Zenith Z-161 XT compatible luggable. Compaq’s 3.31 added FAT16B support which allowed larger partitions and was the standard for larger partition support going forward in standard MS-DOS. reply cout 4 hours agorootparentprevThanks for pointing that out. I was wondering why I couldn't find the source for dosshell! reply geoelectric 11 hours agorootparentprevIt’s stuff like this that sent me over to DR-DOS back in the day. reply snvzz 8 hours agorootparentDR-DOS also open source now. reply roytam87 8 hours agorootparentBut only OpenDOS 7.01, not older(DR DOS 3.30-7.00) or newer(7.02-8.0/8.1) versions. reply davidgnz 6 hours agorootparentOpenDOS isn't open-source, its source-available. The license reads more like trial software: \"Caldera grants you a non-exclusive license to use the Software in source or binary form free of charge if your use of the Software is for the purpose of evaluating whether to purchase an ongoing license to the Software. The evaluation period for use by or on behalf of a commercial entity is limited to 90 days; evaluation use by others is not subject to this 90 day limit but is still limited to a reasonable period\" reply rnd0 7 hours agorootparentprevThe whole opendos thing is pretty questionable, too. CPM is open source as is its' derivatives. Cool so far. But is DR-DOS a derivative of it? Or is it bound by the 'non commercial' license of the 90's which a) was revoked b)isn't exactly open source (limits distribution) in the first place. Microsofts' releases have the benefit of being unambiguous. reply asveikau 11 hours agoparentprevI feel like FreeDOS could already run just about everything for 20 years or longer. If your goal is to run DOS software your use case was probably already adequately covered with free software. An interesting thing about DOS is the OS wasn't very involved. Programs did a lot of things we now think of as the realm of an OS, like talking directly to I/O addresses or installing interrupt handlers. I feel like a DOS implementation doesn't even need to do a lot of things, maybe part of why DOS4 is \"good enough\". reply hi-v-rocknroll 10 hours agoparentprev3.31 and 6.22 were the lands of stability for pre-Chicago MS-DOS for older and newer applications respectively. reply LeoPanthera 11 hours agoparentprevAlthough 5.0 was the first version to include HIMEM.SYS reply bombcar 9 hours agorootparentDOS 5.0 was peak perfection. 6.22 or whatever was just stupid double stack and other worthless garbage. reply Scaevolus 11 hours agoparentprevAre there specific parts of the DOS API that existing emulators like DOSBOX don't handle accurately enough? I don't understand if this source can be usefully integrated into modern DOS preservation projects. reply actionfromafar 11 hours agorootparentIt was a long time since I messed with things, but \"net drives\" something didn't work in FreeDOS a long time ago. This was useful, because it meant you could from within a PC emulator access your host file system. It's entirely possible that works on FreeDOS now. reply somat 10 hours agorootparentThis is almost completely unrelated to your comment, but it sparked a fun memory. At one point I had a system set up that would boot dos via ipxe with iscsi drives. I thought it was almost magical how dos had no clue it was using a network drive. I still don't know exactly how it worked. but I suspect ipxe was patching the bios. reply bombcar 9 hours agorootparentDOS was pretty reliable about using BIOS interfaces for drives; if you imitate the BIOS interface it’ll just work. The problems came from limitations of the BIOS interface (especially size) reply snvzz 8 hours agoparentprevIF running software is what matters, there's dosemu2 and dosbox-x. For the actual hardware or PCem, FreeDOS exists and is alive. DR-DOS has also been open sourced. reply davidgnz 6 hours agorootparentDR-DOS hasn't been open sourced. Caldera did release the source for the kernel and a few other bits, but the license only allowed free use for evaluation purposes. After 90 days (for a company) or \"a reasonable period\" for non-commercial entities you were required to buy a license. Bryan Sparks did open-source CP/M a little while back, but AFAIK he hasn't said anything about DR-DOS so far. reply snvzz 6 hours agorootparentThere was an actual open source version, which was retracted[0]. Fortunately for the commons, what's done is done. 0. https://archiveos.org/drdos/ reply dosman33 11 hours agoprevMSDOS 4 was reportedly an overall bad release and was not in wide circulation, in all my days I think I only came across it once. This is why DOS 3.3 and 5.0 were much more common to find in circulation together. I'm sure the source for 4 will make for some interesting bug hunting. Anyone remember the MUF list? \"Microsofts Undocumented Features\". reply rnd0 11 hours agoparentThis is \"multitasking dos 4\" though, which isn't the same as the much-reviled ms-dos 4. As I understand it, it's a lot closer to dos 3 than it is to dos 4 in terms of functionality. I wouldn't expect this to understand extended partitions, much less large partitions (that dos 4 uses) reply rep_lodsb 15 minutes agorootparent>I wouldn't expect this to understand extended partitions, much less large partitions (that dos 4 uses) Most of the source code (everything outside of -ozzie) is for regular DOS 4.0 and supports 32-bit sector numbers. They planned to add it in the multitasking version as well [1], but from reading IBMDSK.ASM it isn't there yet. Also that driver talks directly to the hard disk controller instead of going through the ROM BIOS, and will only support XT drives, not IDE/ATA. Apparently the goal was to be able to do background I/O on an XT, where there is no BIOS support for that. [1] see driver docs at https://raw.githubusercontent.com/microsoft/MS-DOS/main/v4.0... reply tuzemec 1 hour agoprevI was having a chat with one of the young guys in the office yesterday. He was complaining that his first PC had windows 7 and was slow, because he only had 2GB of RAM. And I was thinking: Gosh, probably he never typed \"dir\" or \"c:\" in his life... I feel sooo old :/ reply dailykoder 1 hour agoparentNext year is 10th Windows 10 aniversary :-) reply thesuperbigfrog 12 hours agoprevInteresting MS-DOS history to be uncovered with the source code of MS-DOS 4. I wonder if we will see MS-DOS 5 or 6 soon? FreeDOS, an open source DOS clone, has been around for a while: https://freedos.org/ https://github.com/FDOS reply tracker1 8 hours agoparentI'd like to see NTVDM open sourced... It's been leaked and there are unofficial builds to get support into 64-bit Windows through the emulation code. Could be a huge boost for general support. Not sure how much work it would take for Linux or even just wine. But might displace DOSbox. reply tiernano 12 hours agoprevDirect GitHub link to the repo. https://github.com/Microsoft/MS-DOS reply alexandrukis16 50 minutes agoprevhttps://github.com/microsoft/MS-DOS/blob/2d04cacc5322951f187... still waiting for this one reply MaxLeiter 11 hours agoprev; At this time, for DOS 4.00, we only have maximum 11 bytes left ; for translation.!!!!!!!!!!!!!!!!!!!!! https://github.com/microsoft/MS-DOS/blob/main/v4.0/src/BOOT/... reply ircicq 1 hour agoprevUnfortunately this is only consumer's part of DOS. For us developers, it is very desirable to have access to SDK sources: MASM, C compiler, LINK. reply jmclnx 11 hours agoprevWhat about DOS 3.3 ? to me that was better than 4.0, but maybe people who know DOS 4 src and fix the issues :) reply xvilka 10 hours agoprevIs there anything useful that isn't already implemented in FreeDOS? reply WalterBright 8 hours agoprevAh, all the utterly useless knowledge about DOS that's taking up space in my brain. reply galkk 9 hours agoprevInteresting, I expected it to have more C, but it's mostly assembler with rare exceptions of C code. reply anothername12 6 hours agoprevCurious if DOS 3.30 will follow soon. It felt like I was on that version for ages growing up, messing with assembler on the families Amstrad 2086.. good times. Edit: really appreciate the effort being put into preserving this software history btw reply hernandipietro 7 hours agoprevI remember looking at MSDOS binaries with hex editors and seeing \"MS Runtime Library\", like many tools were written in C -- may be my memory is completely failing ... but probably MSDOS 5.0 included many utilities written in C? I used 5.0 with a 386, circa '93. reply 1970-01-01 11 hours agoprevI never thought I would be able to build DOS from scratch, but here we are! reply glonq 11 hours agoprev4.0 was a buggy release. Where's the DOS 4.01 source? reply aap_ 11 hours agoparentI would assume that with some disassembling 4.01 could be reconstructed from 4.0 without too much trouble. reply hi-v-rocknroll 10 hours agorootparentNo. The comments aren't there and 4.01 was also a major rewrite after Microsoft took the project back from IBM. reply freedomben 10 hours agoprevEven just seeing some of these filenames is a major blast from the past! Love it CPY.BAT MAKEFILE RUNME.BAT SETENV.BAT reply dewey 7 hours agoparentMakefiles are still going strong! reply userbinator 9 hours agoprevI believe MS has now officially released the sources to MS-DOS 1.x, 2.x, and 4; but most of 3.x and 6.x were leaked many years ago, so I wonder if that's the reason they skipped 3. reply Karellen 2 hours agoprevWait, why is `git pull` for an OS that ran on computers with memory and floppy-disk sizes measured in 100s of KiB taking more than a couple of seconds to download? ...Oh, there's 100MiB of PDFs in there, which are high-resolution scans of the paper technical documentation. Fair enough. reply lynx23 1 hour agoprevWhenever I think about the good old DOS days, QEMM and DESQview come to mind. Esp. DESQview which was a rather essential part of my DOS experience... Sources for these would be fun to look at. reply atlas_hugged 3 hours agoprevScott is such a treasure. Love that guy. reply blobbers 10 hours agoprevDoes this have a how to on hacking AUTOEXEC.BAT to avoid TSRs and get enough of that sweet sweet 640KB to run whatever program you needed? reply smolsky 11 hours agoprevOMG, I remember copying official 5.25 disks back in the 90s... reply hi-v-rocknroll 10 hours agoparentClunk, clunk, clunk, bzt-urg, clunk, clunk, clunk, clunk... :) While hole punchers and opaque tape worked for notching and denotching, there were also floppy notchers. It turns out these bits of accessories are rarely found on secondary markets or go for something like $80 USD. There are still quite a few 5.25\" drive cleaning kits still for sale. I don't miss floppies because they were slow, fragile, and prone to developing unrecoverable errors. I did discover through experimentation that 3.5\" floppies were fairly resistant to crude direct magenetic attack. I had to open the window and touch the media surface to a small speaker's magnet to induce errors. Praise be to the greaseweazle and the Copy II PC Deluxe Option Board 2.0 (the 1.0 doesn't support 3.5\" 1.44 MB). 2.88 MB drives are worth a small fortune because they are still used for industrial purposes. Also, SCSI floppy drives exist. reply bfors 11 hours agoprevLooking at this code is a nice reminder that I have virtually no real understanding of how computers work, and every part I work with is a nice polished abstraction that hides 99% of the complexity involved in actually creating value with a computer. reply _whiteCaps_ 10 hours agoparentYou might like: https://www.amazon.ca/Definitive-Guide-How-Computers-Math/dp... reply ingend88 4 hours agoprevFor playing around and better understanding this opensource repo.you can use www dot getarchieai dot com. reply pjmlp 12 hours agoprevI guess now we can finally try to understand what went wrong with the multitasking attempt. reply h2odragon 11 hours agoparentGraphics. Managers wanted \"windowing systems\" as well as multitasking. People who just wanted mutitasking DOS had DesqView and ... \"concurrent\"? was it? i forget. I wrote TSR's that snatched cycles off the timer interrupt and rode 21h for opportunity to flush buffers to/from disk. reply cout 5 hours agorootparentThere was a Concurrent DOS, and there were quite a few others: topview, taskview, doubledos, vm/386, to name a few. I remember reading this article (p. 22) sparking my imagination as a kid: http://bitsavers.trailing-edge.com/magazines/Microsystems_Jo... reply esafak 11 hours agorootparentprevhttps://en.wikipedia.org/wiki/Multiuser_DOS reply brightsize 6 hours agorootparentprev> I wrote TSR's that snatched cycles off the timer interrupt We've all been there. How else could you get anything interesting done back in the day? reply cout 5 hours agorootparentFor a long time I figured that was just the right way to do multitasking. Hook the interrupt and pass control to the next task when you are done. I also figured most of what I wanted was popup TSRs anyway - how can a human possibly pay attention to more than one program at a time? Makes me wonder how much my brain has changed over the years by being exposed to a multitasking and multiple windows on a large screen. reply ofrzeta 2 hours agoprevWhat the ..? Wake me up when they open source DOS 6.22. What's holding them back? reply SuperHeavy256 8 hours agoprevDoes this benefit FreeDOS btw? reply rwl4 11 hours agoprev [–] Weird that for a couple minutes, these paths existed: * https://github.com/microsoft/MS-DOS/tree/main/v4.0/bin * https://github.com/microsoft/MS-DOS/tree/main/v4.0/bin/DISK1 * https://github.com/microsoft/MS-DOS/tree/main/v4.0/pdf But they disappeared as I browsed the repo. I guess they didn't want that part public? Edit: I knew I wasn't seeing things! Somebody forked it along with those files: https://github.com/OwnedByWuigi/DOS/tree/main/v4.0 reply farmerbb 11 hours agoparentThey force-pushed the repo to remove an insult towards Tim Patterson in one of the source files. reply skissane 11 hours agorootparentThey changed line 70 of v4.0/src/DOS/STRIN.ASM from [0]: ; Brain-damaged Tim Patterson ignored ^F in case his BIOS did not flush the to [1]: ; Brain-damaged TP ignored ^F in case his BIOS did not flush the [0] https://github.com/OwnedByWuigi/DOS/blob/ffd70f8b4fb77e2e6af... [1] https://github.com/microsoft/MS-DOS/blob/main/v4.0/src/DOS/S... reply bombcar 11 hours agorootparentIf Tim is still around he should PR a change back; I'd not want my name shortened to \"TP\". reply EvanAnderson 11 hours agorootparentHe should make a PR to change it back to the correct spelling of his name. reply hi-v-rocknroll 10 hours agorootparentprevAgreed. My initialisms are almost as bad. (Thanks clueless parents!) reply gigel82 9 hours agorootparentprevhttps://en.wikipedia.org/wiki/Tim_Paterson From Wikipedia, the free encyclopedia Tim Paterson (born 1 June 1956) is an American computer programmer, best known for creating 86-DOS, an operating system for the Intel 8086. This system emulated the application programming interface (API) of CP/M, which was created by Gary Kildall. 86-DOS later formed the basis of MS-DOS, the most widely used personal computer operating system in the 1980s. reply Dalewyn 10 hours agorootparentprevIf this release is for historical research purposes, the release should be pristine including the unsavory bits. Whitewashing of history should never be accepted. reply tom_ 8 hours agorootparentPerhaps this release is not for historical research purposes. reply Dalewyn 7 hours agorootparentIt is for historial research purposes: >The MS-DOS v1.25 and v2.0 files were originally shared at the Computer History Museum on March 25th, 2014 and are being (re)published in this repo to make them easier to find, reference-to in external writing and works, and to allow exploration and experimentation for those interested in early PC Operating Systems. >For historical reference >The source files in this repo are for historical reference and will be kept static, so please don’t send Pull Requests suggesting any modifications to the source files, but feel free to fork this repo and experiment. reply senorrib 9 hours agorootparentprev100%! reply esafak 11 hours agorootparentprevDon't leave us hanging!! reply farmerbb 11 hours agorootparentAt https://github.com/microsoft/MS-DOS/blob/main/v4.0/src/DOS/S... it used to have Tim Patterson's full name whereas after the force push they abbreviated it to his initials \"TP\". I had the repo cloned before the force push and when I went to pull it, this file was the only one that contained a conflict. reply esafak 10 hours agorootparentLooks like the handiwork of Mark Zbikowski, whose initials adorn every EXE file. https://en.wikipedia.org/wiki/Mark_Zbikowski reply farmerbb 9 hours agorootparentMark is the one that committed today's source release, going off of the \"MZ is back\" commit message from GitHub user \"mzbik\". reply shanselman 11 hours agoparentprevYa sorry we're moving stuff around reply rozzie 9 hours agorootparentThx for clearing the rights and for releasing, Scott. And of course thanks to Microsoft and IBM. It would be fun at some point down the road to get some of the older code building and running again - particularly '84/'85-vintage Windows & Notes builds. Quite a lot of work, though, not just because of hardware but also likely because of toolchain gaps. reply rwl4 10 hours agorootparentprevWell, thanks for putting this up! It's really a treasure for those of us who used it as our daily driver so many years ago. reply Karellen 1 hour agorootparentprevWhy is that, no matter how much you check and proofread your work before you push/publish, you'll always find something obvious you missed 5 minutes after it's gone up? To change a public branch, or not to change a public branch, that is the question. Edit: Muphry's law strikes again - s/or to not/or not to/ reply electroly 11 hours agoparentprevThey just changed the folder. All of those files are now in https://github.com/microsoft/MS-DOS/tree/main/v4.0-ozzie reply willxinc 11 hours agoparentprevLooks like they were moved to the Ozzie subfolder: https://github.com/microsoft/MS-DOS/tree/main/v4.0-ozzie reply rnd0 11 hours agorootparentThat was literally just added; I have a tab open that doesn't have that folder. This is wild and strangely exciting to see released in real time. reply shanselman 11 hours agorootparentYa we wanted to separate the MS-DOS and MT-DOS stuff, it was confusing as it was reply rnd0 10 hours agorootparentI'm not complaining, that's for sure! Thanks for all you're doing. reply nullindividual 11 hours agoparentprev [–] It is all the MT-DOS content; binaries and docs. reply qingcharles 11 hours agorootparent [–] Multi-Tasking MS-DOS Beta Test Release 1.00 Release Notes Enclosed you will find Microsoft's first beta release of Multi-tasking MS-DOS. This version is based upon MS-DOS Version 2 sources, we will be reimplementing the multi-tasking enhancements on top of Version 3 sources shortly. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Microsoft, with IBM's collaboration, has unveiled the source code for MS-DOS 4.00 under the MIT license, containing beta binaries, documents, and disk images.",
      "The 8086 assembly code of MS-DOS 4.00, a significant historical piece, is shared to foster open innovation, following a conversation with ex-Microsoft CTO Ray Ozzie.",
      "The source code is accessible on GitHub, enabling users to operate it on authentic hardware or emulators."
    ],
    "commentSummary": [
      "GitHub users are nostalgically discussing older MS-DOS versions, sharing memories of hacking software with a hex editor.",
      "The conversation touches on open-sourcing old software like MS-DOS, terminology shifts in programming, and the sensitivity of language.",
      "Microsoft's recent actions on GitHub, including releasing and removing historical MS-DOS files, including the Multi-tasking MS-DOS beta, are highlighted and debated."
    ],
    "points": 542,
    "commentCount": 206,
    "retryCount": 0,
    "time": 1714081326
  },
  {
    "id": 40156330,
    "title": "Customize Confetti Animations with Canvas-Confetti",
    "originLink": "https://github.com/catdad/canvas-confetti",
    "originBody": "Demo catdad.github.io/canvas-confetti Install You can install this module as a component from NPM: npm install --save canvas-confetti You can then require('canvas-confetti'); to use it in your project build. Note: this is a client component, and will not run in Node. You will need to build your project with something like webpack in order to use this. You can also include this library in your HTML page directly from a CDN:Note: you should use the latest version at the time that you include your project. You can see all versions on the releases page. Reduced Motion Thank you for joining me in this very important message about motion on your website. See, not everyone likes it, and some actually prefer no motion. They have ways to tell us about it and we should listen. While I don't want to go as far as tell you not to have confetti on your page just yet, I do want to make it easy for you to respect what your users want. There is a disableForReducedMotion option you can use so that users that have trouble with chaotic animations don't need to struggle on your website. This is disabled by default, but I am considering changing that in a future major release. If you have strong feelings about this, please let me know. For now, please confetti responsibly. API When installed from npm, this library can be required as a client component in your project build. When using the CDN version, it is exposed as a confetti function on window. confetti([options {Object}]) → Promise|null confetti takes a single optional object. When window.Promise is available, it will return a Promise to let you know when it is done. When promises are not available (like in IE), it will return null. You can polyfill promises using any of the popular polyfills. You can also provide a promise implementation to confetti through: const MyPromise = require('some-promise-lib'); const confetti = require('canvas-confetti'); confetti.Promise = MyPromise; If you call confetti multiple times before it is done, it will return the same promise every time. Internally, the same canvas element will be reused, continuing the existing animation with the new confetti added. The promise returned by each call to confetti will resolve once all animations are done. options The confetti parameter is a single optional options object, which has the following properties: particleCount Integer (default: 50): The number of confetti to launch. More is always fun... but be cool, there's a lot of math involved. angle Number (default: 90): The angle in which to launch the confetti, in degrees. 90 is straight up. spread Number (default: 45): How far off center the confetti can go, in degrees. 45 means the confetti will launch at the defined angle plus or minus 22.5 degrees. startVelocity Number (default: 45): How fast the confetti will start going, in pixels. decay Number (default: 0.9): How quickly the confetti will lose speed. Keep this number between 0 and 1, otherwise the confetti will gain speed. Better yet, just never change it. gravity Number (default: 1): How quickly the particles are pulled down. 1 is full gravity, 0.5 is half gravity, etc., but there are no limits. You can even make particles go up if you'd like. drift Number (default: 0): How much to the side the confetti will drift. The default is 0, meaning that they will fall straight down. Use a negative number for left and positive number for right. flat Boolean (default: false): Optionally turns off the tilt and wobble that three dimensional confetti would have in the real world. Yeah, they look a little sad, but y'all asked for them, so don't blame me. ticks Number (default: 200): How many times the confetti will move. This is abstract... but play with it if the confetti disappear too quickly for you. origin Object: Where to start firing confetti from. Feel free to launch off-screen if you'd like. origin.x Number (default: 0.5): The x position on the page, with 0 being the left edge and 1 being the right edge. origin.y Number (default: 0.5): The y position on the page, with 0 being the top edge and 1 being the bottom edge. colors Array: An array of color strings, in the HEX format... you know, like #bada55. shapes Array: An array of shapes for the confetti. There are 3 built-in values of square, circle, and star. The default is to use both squares and circles in an even mix. To use a single shape, you can provide just one shape in the array, such as ['star']. You can also change the mix by providing a value such as ['circle', 'circle', 'square'] to use two third circles and one third squares. You can also create your own shapes using the confetti.shapeFromPath or confetti.shapeFromText helper methods. scalar Number (default: 1): Scale factor for each confetti particle. Use decimals to make the confetti smaller. Go on, try teeny tiny confetti, they are adorable! zIndex Integer (default: 100): The confetti should be on top, after all. But if you have a crazy high page, you can set it even higher. disableForReducedMotion Boolean (default: false): Disables confetti entirely for users that prefer reduced motion. The confetti() promise will resolve immediately in this case. confetti.shapeFromPath({ path, matrix? }) → Shape This helper method lets you create a custom confetti shape using an SVG Path string. Any valid path should work, though there are a few caveats: All paths will be filed. If you were hoping to have a stroke path, that is not implemented. Paths are limited to a single color, so keep that in mind. All paths need a valid transform matrix. You can pass one in, or you can leave it out and use this helper to calculate the matrix for you. Do note that calculating the matrix is a bit expensive, so it is best to calculate it once for each path in development and cache that value, so that production confetti remain fast. The matrix is deterministic and will always be the same given the same path value. For best forward compatibility, it is best to re-generate and re-cache the matrix if you update the canvas-confetti library. Support for path-based confetti is limited to browsers which support Path2D, which should really be all major browser at this point. This method will return a Shape -- it's really just a plain object with some properties, but shhh... we'll pretend it's a shape. Pass this Shape object into the shapes array directly. As an example, here's how you might do a triangle confetti: var triangle = confetti.shapeFromPath({ path: 'M0 10 L5 0 L10 10z' }); confetti({ shapes: [triangle] }); confetti.shapeFromText({ text, scalar?, color?, fontFamily? }) → Shape This is the highly anticipated feature to render emoji confetti! Use any standard unicode emoji. Or other text, but... maybe don't use other text. While any text should work, there are some caveats: For flailing confetti, something that is mostly square works best. That is, a single character, especially an emoji. Rather than rendering text every time a confetti is drawn, this helper actually rasterizes the text. Therefore, it does not scale well after it is created. If you plan to use the scalar value to scale your confetti, use the same scalar value here when creating the shape. This will make sure the confetti are not blurry. The options for this method are: options Object: text String: the text to be rendered as a confetti. If you can't make up your mind, I suggest \"🐈\". scalar Number, optional, default: 1: a scale value relative to the default size. It matches the scalar value in the confetti options. color String, optional, default: #000000: the color used to render the text. fontFamily String, optional, default: native emoji: the font family name to use when rendering the text. The default follows best practices for rendring the native OS emoji of the device, falling back to sans-serif. If using a web font, make sure this font is loaded before rendering your confetti. var scalar = 2; var pineapple = confetti.shapeFromText({ text: '🍍', scalar }); confetti({ shapes: [pineapple], scalar }); confetti.create(canvas, [globalOptions]) → function This method creates an instance of the confetti function that uses a custom canvas. This is useful if you want to limit the area on your page in which confetti appear. By default, this method will not modify the canvas in any way (other than drawing to it). Canvas can be misunderstood a bit though, so let me explain why you might want to let the module modify the canvas just a bit. By default, a canvas is a relatively small image -- somewhere around 300x150, depending on the browser. When you resize it using CSS, this sets the display size of the canvas, but not the image being represented on that canvas. Think of it as loading a 300x150 jpeg image in an img tag and then setting the CSS for that tag to 1500x600 -- your image will end up stretched and blurry. In the case of a canvas, you need to also set the width and height of the canvas image itself. If you don't want to do that, you can allow confetti to set it for you. Note also that you should persist the custom instance and avoid initializing an instance of confetti with the same canvas element more than once. The following global options are available: resize Boolean (default: false): Whether to allow setting the canvas image size, as well as keep it correctly sized if the window changes size (e.g. resizing the window, rotating a mobile device, etc.). By default, the canvas size will not be modified. useWorker Boolean (default: false): Whether to use an asynchronous web worker to render the confetti animation, whenever possible. This is turned off by default, meaning that the animation will always execute on the main thread. If turned on and the browser supports it, the animation will execute off of the main thread so that it is not blocking any other work your page needs to do. Using this option will also modify the canvas, but more on that directly below -- do read it. If it is not supported by the browser, this value will be ignored. disableForReducedMotion Boolean (default: false): Disables confetti entirely for users that prefer reduced motion. When set to true, use of this confetti instance will always respect a user's request for reduced motion and disable confetti for them. Important: If you use useWorker: true, I own your canvas now. It's mine now and I can do whatever I want with it (don't worry... I'll just put confetti inside it, I promise). You must not try to use the canvas in any way (other than I guess removing it from the DOM), as it will throw an error. When using workers for rendering, control of the canvas must be transferred to the web worker, preventing any usage of that canvas on the main thread. If you must manipulate the canvas in any way, do not use this option. var myCanvas = document.createElement('canvas'); document.body.appendChild(myCanvas); var myConfetti = confetti.create(myCanvas, { resize: true, useWorker: true }); myConfetti({ particleCount: 100, spread: 160 // any other options from the global // confetti function }); confetti.reset() Stops the animation and clears all confetti, as well as immediately resolves any outstanding promises. In the case of a separate confetti instance created with confetti.create, that instance will have its own reset method. confetti(); setTimeout(() => { confetti.reset(); }, 100); var myCanvas = document.createElement('canvas'); document.body.appendChild(myCanvas); var myConfetti = confetti.create(myCanvas, { resize: true }); myConfetti(); setTimeout(() => { myConfetti.reset(); }, 100); Examples Launch some confetti the default way: confetti(); Launch a bunch of confetti: confetti({ particleCount: 150 }); Launch some confetti really wide: confetti({ spread: 180 }); Get creative. Launch a small poof of confetti from a random part of the page: confetti({ particleCount: 100, startVelocity: 30, spread: 360, origin: { x: Math.random(), // since they fall down, start a bit higher than random y: Math.random() - 0.2 } }); I said creative... we can do better. Since it doesn't matter how many times we call confetti (just the total number of confetti in the air), we can do some fun things, like continuously launch more and more confetti for 30 seconds, from multiple directions: // do this for 30 seconds var duration = 30 * 1000; var end = Date.now() + duration; (function frame() { // launch a few confetti from the left edge confetti({ particleCount: 7, angle: 60, spread: 55, origin: { x: 0 } }); // and launch a few from the right edge confetti({ particleCount: 7, angle: 120, spread: 55, origin: { x: 1 } }); // keep going until we are out of time if (Date.now() < end) { requestAnimationFrame(frame); } }());",
    "commentLink": "https://news.ycombinator.com/item?id=40156330",
    "commentBody": "A useful front-end confetti animation library (github.com/catdad)443 points by blini2077 22 hours agohidepastfavorite111 comments lukax 20 hours agoSo the trick for performant animation here is to draw on canvas and put the canvas in front of all other elements but disable pointer events on canvas so that you can still interact with the page. reply ABNW 20 hours agoparentCorrect! Disable Pointer Events has surprising utility! reply amelius 18 hours agorootparentWhat if you wanted to add the requirement that the user had to click on 1 specific piece of confetti? reply MartijnHols 16 hours agorootparentAdd a click event listener to the body and overlay the event-coordinates on the canvas. reply amelius 15 hours agorootparentWill the canvas allow you to hit-test the confetti piece given the coordinate? reply eyelidlessness 14 hours agorootparentCanvas draws raster images, anything resembling an object in your drawing logic is already tracked separately by necessity. So regardless, you’d presumably check against whatever data model you’re using to determine what to draw. reply lupire 11 hours agorootparentBy what time the user clicks, there's no reason for the program to need to remember what they drew where. reply eyelidlessness 7 hours agorootparentIf that’s the case, what other object with coordinates would you reference on a canvas to determine whether it was clicked? reply nicky0 18 hours agorootparentprevThen you have a fun programming challenge. reply jjice 20 hours agoprevThis reminds me of some good times doing web development in high school in 2015. I made a small website with confetti to ask a girl to homecoming (very nerdy looking back). Back when making a website felt like a superpower to a kid. It seems like it wasn't this package based on age, but it was a really nice animation. I love a fun little projects like this that are purely for a good time. That's why I started programming and it's still a driving force. reply askonomm 20 hours agoparentDid it work? Did she say yes? reply jjice 20 hours agorootparentShe said no - but we went on to be great friends still to this day, so I'd say it did work. We joke about it still. reply sa-code 18 hours agorootparentI knew someone that made a PowerPoint presentation for their partner for Valentine's Day. So you're doing great in comparison reply Atotalnoob 11 hours agorootparentMy online dating profile was a PowerPoint presentation before I met my partner. It easily quadrupled my matches. reply xprn 10 hours agorootparentAnd now we live in a world where “powerpoint presentations as dating profiles” are basically the norm reply oli-g 18 hours agorootparentprevThat's nice :) I'm curious though - did you use tabs or spaces? reply jjice 14 hours agorootparentIt was definitely tabs - I'll ask her if she's a space kind of gal. Some things can't be compromised! reply TeMPOraL 19 hours agoprevFrom the demo page: > If you happened to get curious and changed the particle count to 400 or so, you saw something disappointing. An even \"flattened cone\" look to the confetti, making it look way too perfect and ruining the illusion. I love it! This kind of attention to detail is rare in this world, and I cherish it wherever I find it - whether it's in statistical visualization, movie props, or website confetti. (As a solution, I'd probably go for changing the random distribution directly. I'd check of course, but I'm guessing the real-life distribution approximates a gaussian.) reply fromwilliam 19 hours agoprevAside from being a cool and useful library, this is a good example of what John Ousterhout calls \"deep modules\" in Philosophy of Software Design. It's very easy to use the most basic version of this library (summon confetti) but you can get a lot out of it by exploring the options presented (snow, specific colours, different confetti effects, etc.). reply Zebfross 17 hours agoprevI added confetti to our salespeople's admin dashboard for when they make a sale. Surprisingly enjoyable and motivating. reply RamRodification 21 hours agoprevI wish they would have called the reset function confetti.resetti() reply jerf 20 hours agoparentThis being Javascript, you can at least fix this locally with a simple \"confetti.resetti = confetti.reset\". There are some software engineering costs to this approach, but, as is transparently obvious to all thoughtful observers, the benefits massively outweigh them, so I say go for it. reply krsdcbl 8 hours agorootparentomw to \"consologgi = console.log\" reply ilaksh 31 minutes agoparentprevmaybe you could make a PR reply waldothedog 21 hours agoparentprevSomebody give this person a job! If they already have a job, give them a cookie reply thih9 19 hours agoprevThis is cool and impressive. At the same time I don't want to see it running on any website that I use. And especially - I wouldn't want to see confetti accompany newsletter popups or adding products to basket. reply EMM_386 18 hours agoparentThe strange thing is this can be used effectively. I don't know about full-screen like this, but we were visiting a client recently who used a certain project management software and when you closed an item out, the button changed to green and had this effect on it. It was subtle, but noticeable enough that after the meeting another developer and I both said \"that was a pretty neat effect\". It conveyed the sense of \"yay, progress!\". Just make it optional. reply jihadjihad 16 hours agorootparentAsana does that, and it's optional. I like it whenever it happens too! reply Andrex 18 hours agoparentprevI think the only \"legitimate\" use would be something like YouTube's like button, which has a cool animation (and vibrates the device if using the mobile app). Very pleasant UX. reply wasteduniverse 15 hours agorootparentHaving the like button do an animation when your video mentions that liking it would be helpful is the only good UX choice I have seen on YouTube since... playlists? reply bradly 18 hours agoparentprev> https://developer.mozilla.org/en-US/docs/Web/CSS/@media/pref... You can site your browser to prefer reduced motion. Site owners and library maintainers should be respecting this when implementing things like confetti. This library in particular has a `disableForReducedMotion` option. reply huhtenberg 18 hours agoparentprevThis has its place, e.g. on a completion of a game or some such. reply ivanbozic 16 hours agorootparentYep, on one of the products I built, I use this when a person subscribes to the paid tier. It's really nice, not too obtrusive, but it's fun! Plus, it only happens on that specific scenario, so it's not overdone. Pipedrive CRM also does this when you close a deal, they even previously had a person shooting a hoop or something, that was nice. It's rewarding! reply memonkey 9 hours agoparentprevWe use this library when a person qualifies for something. It's a neat effect to our onboarding flow. reply lovegrenoble 21 hours agoprevYou have also Party.js library: https://party.js.org/ reply flanbiscuit 20 hours agoparentAh, but which one is smaller? 10.4 kB Minified, 4.2kB Minified + Gzipped https://bundlephobia.com/package/canvas-confetti@1.9.2 28.3kB Minified, 7.4kB Minified + Gzipped https://bundlephobia.com/package/party-js@2.2.0 caveat: I don't know how bundlephobia works, it might not give the best representation of the final size of the package. Probably doesn't take into account code-splitting or being able to import just what you need. I'm just using it as a quick and general overview. Looks like confetti one beats it by a few KB gzipped so either works depending on which one has the feature you need, unless you're really trying to squeeze those kb. reply scoot 19 hours agorootparent>Probably doesn't take into account code-splitting or being able to import just what you need Correct. It doesn't. reply taikahessu 20 hours agoparentprevOP script feels way more performant on mobile. reply apocalyptic0n3 19 hours agorootparentAnd on Firefox for Mac. Noticed dropped frames pretty consistently in party.js. No such problem with Canvas Confetti, even when there are significantly more particles on screen. I had to increase the count to a point where I couldn't see gaps between them before I saw any lag whatsoever. reply baobabKoodaa 18 hours agorootparentprevOP is more performant on desktop Windows as well. The other one visibly lags when animating even a single cannon of confetti. reply henriquecm8 14 hours agoparentprevOP's library feels a lot more performant, in my old work computer with 3 click you can notice some lag with Party.js. With canvas-confetti, it only starts to lag when I click non-stop for a few seconds, probably invoking more than 30 instances of confetti and a lot of particles. reply albert_e 17 hours agoprevI solve crosswords on downforacross.com and solving a puzzle there results in confetti. Maybe they could use some of this better performing code to make it feel lighter. Other than \"fun\" sites and sparing usage ... I wouldn't want to see such animations everywhere though. reply wackget 17 hours agoprevUseful for annoying your visitors? Why on earth would they not enable the setting which honours the `prefers-reduces-motion` media flag for users who don't want stuff flying all over their screens?! reply atonse 12 hours agoparentThey have it in the Readme where they're considering making it a default in the future. reply kieloo 17 hours agoparentprevAgreed. I see really no good reason not to enable this by default. reply devsatish 20 hours agoprevThis is elegant. Shows the passion that goes into building some thing that looks so beautiful and easy to integrate. reply h1fra 20 hours agoprevVery nice, not a fan of this kind of animation but at least it's smooth! reply humanfromearth9 21 hours agoprevI think there is no need to specify 'useful' in the title. reply refset 21 hours agoparentHow about as a motivational aid and means of verifying that your code has compiled: https://squint-cljs.github.io/squint/ reply meiraleal 21 hours agoparentprevTrue. But also the use of the word made me genuinely interested and I laughed by how not really useful it is. Upvoted. reply fsckboy 19 hours agoparentprevit's as useful as actual confetti irl, so: 100% reply kc10 11 hours agoprevWe built similar animation part of a product few years ago. The flow was something like - when a new user signs up and users our product for the first time and creates certain artifact, the confetti animation would be displayed. Product managers loved it and they would show it off to execs as playful, refreshing, etc. But later on after UX reviews, accessibility testing, the feature was ultimately removed from the product. It was fun to present it in demos, but it can also be annoying to users. reply carimura 12 hours agoprevI remember putting falling snow on our ecommerce site in like 2005 and just how amazing that feeling was. Oh how far we've come! sort of! reply rockwotj 20 hours agoprevUsed this to build a confetti inbox for Shortwave's April fools [1] a few years back. Really run library, and easy to integrate. [1]: https://www.youtube.com/watch?v=_ROTg3KcoIA reply guappa 21 hours agoprevJust fyi, confetti are candies in italy. Not bits of paper. reply drivers99 18 hours agoparentInteresting. It's also cognate with \"confection\". https://www.etymonline.com/search?q=confetti > 1815, \"small pellets made of lime or soft plaster, used in Italy during carnival by the revelers for pelting one another in the streets,\" [...] the custom was adopted in England by early 19c. for weddings and other occasions, with symbolic tossing of little bits of paper (which are called confetti by 1846). reply ale42 21 hours agoparentprevCan someone code and make a pull request to properly handle display of those for Italian users? (see https://it.wikipedia.org/wiki/Confetto for what they look like) reply TeMPOraL 19 hours agorootparentAh, so basically the same thing, but they hit much harder when they land? reply ale42 13 hours agorootparentI guess they also fall down much more quickly reply guappa 4 hours agorootparentNot if there is no atmosphere! reply languagehacker 20 hours agoprevCute animation, but I find it hard to say that the confetti animation is useful reply bangaroo 20 hours agoparentthis says more about you than it does about the confetti animation reply kaptainscarlet 20 hours agorootparentTwitter uses a similar animation on your profile when it's your birthday. That's one use case and I am sure there are more. reply TeMPOraL 19 hours agorootparentI think Facebook did something similar if you typed \"congratulations\" in a comment. reply jaffathecake 21 hours agoprevI used this in a prototype recently and was impressed by how configurable it is. reply graypegg 20 hours agoprevI love this. Lovely work! I had done something similar a few years ago for an edtech company I was working at. [0] It uses threejs and tries to \"bake\" the particle animation before trying to play it, but I don't think it ever made a huge difference in performance. I think I'll reach out and mention they could change to your library for a nicer experience! [0] https://github.com/graypegg/xello-confetti reply DowagerDave 12 hours agoprevThis is one of the most point-less / awesomest things I've seen on the internet this week. I can't imagine how much work went into building it, and how much time I will now be spending to incorporate into something that definitely doesn't need it. Thanks! reply SchizoDuckie 11 hours agoprevI use this on unicornpoep.nl, a little multiple choice toy i built for my daughter to teach her multiplication tables. This library is an awesome chromebook killer. I need to find something more optimized. But it does the trick! reply __MatrixMan__ 21 hours agoprevWouldn't it be crazy if there was a snow storm that had so many flakes, the gravitational constant was reduced because of the extra time it took to render them all? reply seabass-labrax 21 hours agoparentThe excellent performance characteristics of the natural world may indeed be the best rebuttal of the popular theory that we live in a computer simulation! reply yetihehe 20 hours agorootparentGood simulations have constant time step and may not run in real time. From inside such simulation, you would never know. reply __MatrixMan__ 19 hours agorootparentIt's only meaningful to consider that our world is a simulation if it is an imperfect one, otherwise you're just using \"simulation\" as an awkward synonym for \"reality\". Click the \"snow\" button enough (https://catdad.github.io/canvas-confetti/) and you'll get a horizontal line separating the pre-slowdown flakes from the post-slowdown flakes. I suppose that's the kind of simulation imperfection that we might look for. Personally, I don't think we're hacking our way out of this one any time soon, so I'm happy to just call it reality. reply OkayPhysicist 10 hours agorootparentWhat would it even mean for a simulation to be imperfect, though, from the perspective within the simulation? You can only observe the simulated phenomena. So it would be perfectly normal, say, if things become non-deterministic at the hardest to observe small scales, or if there were minor inconsistencies between the smallest scale behaviors and the largest. You'd just call it \"physics\". reply __MatrixMan__ 7 hours agorootparentSee that's hard because I do call it physics, and I do not call it a simulation. I am here in this world, and from where I stand it's as real as anything will ever be for me. My point is that I don't think there's any sense in entertaining counterfactuals that, if true, will be impossible to come up with evidence for, and I think the assertion that our world is a simulation is one such counterfactual. That is, unless the physics gets so absolutely insane that \"it's just physics\" fails to scratch the itch. One example would be if we discover an artifact that lets us see each particle's corresponding unique ID such that, once we have that ID, we can then type it into a console and arbitrarily set properties like its mass. If simulated entities gain control over the parameters that govern the simulation itself... well is it really a simulation anymore? reply yetihehe 3 hours agorootparentHow to detect imperfect simulation: some unexplainable missing wavelength bands, or quantisation of results where it should be continuous would hint at \"cutting corners\" in simulation, like steps in energy levels from very distant xray sources perhaps? As far as we know, our \"physics\" does not show any possible imperfections, or we didn't thought of all imperfections which could arise in simulation. reply braden-lk 20 hours agorootparentprevI was introduced to this idea by the book Permutation City. Great read. reply marcosdumay 20 hours agorootparentThe idea on Permutation City is way crazier than a mere computer simulation. reply __MatrixMan__ 19 hours agorootparentI agree, it's awesome. I wish more scifi authors were as ambitious as Egan. reply lpapez 15 hours agoprevThis reminds me of the early Internet in the 90s and 2000s, you had falling snow and star animations everywhere. This is a Javascript library, but it couldn't have been JS back in those days. Anyone have an idea how people built stuff like this without JS? I'm curious. reply isp 15 hours agoparentJavaScript has been about since 1995, and certainly by no later than the early 2000s was used for these animations. reply Rumudiez 15 hours agoparentprevtiled gifs, most likely reply GrantMoyer 19 hours agoprevThe effect is surprisingly smooth on my outdated mobile phone. I recently implemented a similar effect with a hardware accelerated 3D rendering API (albiet with limited collision physics), and it didn't perform nearly as well targeting WebGL. reply ericol 18 hours agoprevGo multi click on the run button for the \"School Pride\" demo. I double dare you. reply OkayPhysicist 10 hours agoparentHell, they let you edit the code samples. Slap the code in a for(let i = 0; iwhat is up with people who believe JS should not exist on the web They said it: privacy reply naikrovek 15 hours agorootparentYep, privacy. Those of you who work writing javascript to fingerprint browsers and collect user behavior in exchange for a paycheck, you are the worst kind of person. There are a lot of you on here and I expect to be \"punished\" for this view, but my view on this will never change. reply justneedaname 20 hours agorootparentprevThank you for the laugh haha reply kaptainscarlet 20 hours agorootparentprevI threw away my laptop, now the website is completely gone. reply cseleborg 20 hours agoparentprevThe developer has of course provided a print stylesheet (expect no less from a pro). Just print it, frame it, hang it up on the wall and voilà! A no-JS confetti demo right there in your living room. reply graypegg 20 hours agorootparentSadly, the demo does require scissors and a colour printer/coloured markers to experience fully. reply lambaro 20 hours agoparentprevHard to get much more obvious than that. How else did you think this was implemented as a front-end library? reply chedabob 20 hours agorootparentIf someone told me it was pure CSS, I'd believe them. I'm not a frontend dev, so I'm always impressed by what you can now do without having to import a whole JS library. reply ecuaflo 17 hours agorootparentI am a frontend dev and agree it could be done with CSS similar to this [0] [0] https://codepen.io/jh3y/pen/oNqdmbW reply 91bananas 18 hours agorootparentpreva whole JS library gasp libraries are in every language. reply jazzyjackson 20 hours agoparentprevnow that you mention it it would be neat to implement this as a bunch of overlapping gifs or a fanciful css animation reply Zenzero 12 hours agoparentprev [–] I haven't understood this approach. If you're going as far as not using js, do you not use a mobile phone? No loyalty programs? No credit cards? How intense is your approach towards security? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The canvas-confetti module enables customized confetti animations on webpages, offering adjustable parameters such as speed, gravity, shapes, and colors.",
      "Users can enhance performance by caching matrix values, disable motion, create custom shapes, resize the canvas, leverage web workers, and stop/clear animations.",
      "The module showcases examples of creating diverse confetti animations, launching them in different methods, and sustaining continuous animations from various directions."
    ],
    "commentSummary": [
      "The discussion explores the utilization of front-end confetti animation libraries on GitHub to develop high-performance animations employing canvas and disabling pointer events.",
      "Users exchange personal anecdotes and insights regarding confetti animations across various applications, expressing diverse views on their efficacy.",
      "The conversation touches on the history of confetti, its role in festivities, and even ventures into discussing the concept of living in a simulated environment, along with debating technical implementation, privacy implications of JavaScript, and alternative methods for web animations."
    ],
    "points": 443,
    "commentCount": 111,
    "retryCount": 0,
    "time": 1714045984
  },
  {
    "id": 40156275,
    "title": "TSMC Introduces A16 1.6nm Technology with Backside Power Delivery",
    "originLink": "https://www.tomshardware.com/tech-industry/tsmc-unveils-16nm-process-technology-with-backside-power-delivery-rivals-intels-competing-design",
    "originBody": "Tech Industry TSMC unveils 1.6nm process technology with backside power delivery, rivals Intel's competing design News By Anton Shilov published yesterday TSMC goes angstrom-class nodes. Comments (10) (Image credit: TSMC) TSMC announced its leading-edge 1.6nm-class process technology today, a new A16 manufacturing process that will be the company's first Angstrom-class production node and promises to outperform its predecessor, N2P, by a significant margin. The technology's most important innovation will be its backside power delivery network (BSPDN). Just like TSMC's 2nm-class nodes (N2, N2P, and N2X), the company's 1.6nm-class fabrication process will rely on gate-all-around (GAA) nanosheet transistors, but unlike the current and next-generation nodes, this one uses backside power delivery dubbed Super Power Rail. Transistor and BSPDN innovations enable tangible performance and efficiency improvements compared to TSMC's N2P: the new node promises an up to 10% higher clock rate at the same voltage and a 15%–20% lower power consumption at the same frequency and complexity. In addition, the new technology could enable 7%–10% higher transistor density, depending on the actual design. (Image credit: TSMC) The most important innovation of TSMC's A16 process, which was unveiled at the company's North American Technology Symposium 2024, is the introduction of the Super Power Rail (SPR), a sophisticated backside power delivery network (BSPDN). This technology is tailored specifically for AI and HPC processors that tend to have both complex signal wiring and dense power delivery networks. Backside power delivery will be implemented into many upcoming process technologies as it allows for an increase in transistor density and improved power delivery, which affects performance. Meanwhile, there are several ways to implement a BSPDN. TSMC's Super Power Rail plugs the backside power delivery network to each transistor's source and drain using a special contact that also reduces resistance to get the maximum performance and power efficiency possible. From a production perspective, this is one of the most complex BSPDN implementations and is more complex than Intel's Power Via. (Image credit: TSMC) The choice of backside power rail implementation is perhaps why TSMC decided not to add this feature to its N2P and N2X process technologies, as it would make using the production nodes considerably more expensive. Meanwhile, by offering a 1.6nm-class node with GAA nanosheet transistors and SPR as well as 2nm-class nodes with GAAFETs only, the company will now have two distinct nodes that will not compete with each other directly but offer distinctive advantages for different customers. (Image credit: TSMC) The production timeline for A16 indicates that volume production of A16 will commence in the second half of 2026. Therefore, actual A16-made products will likely debut in 2027. This timeline positions A16 to potentially compete with Intel's 14A node, which will be the Intel's most advanced node at the time. Stay on the Cutting Edge Join the experts who read Tom's Hardware for the inside track on enthusiast PC tech news — and have for over 25 years. We'll send breaking news and in-depth reviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. Anton Shilov Freelance News Writer Anton Shilov is a Freelance News Writer at Tom’s Hardware US. Over the past couple of decades, he has covered everything from CPUs and GPUs to supercomputers and from modern process technologies and latest fab tools to high-tech industry trends. MORE ABOUT TECH INDUSTRY Russian media claims server and storage supply has returned to pre-sanctions levels despite ongoing restrictions Nvidia CEO hand-delivers world's fastest AI system to OpenAI, again — first DGX H200 given to Sam Altman and Greg Brockman LATEST Best Flash Drives 2024: Fast, Roomy, Pocketable USB Storage SEE MORE LATEST ► SEE ALL COMMENTS (10) 10 Comments Comment from the forums usertests So it's a half node that introduces backside power delivery. The power reduction is where it's at. Those handhelds need every bit they can get. Reply CerianK Just in case anyone needs to hear it, Angstrom-class refers to any process node moving forward where it would otherwise be necessary to include a decimal point in the node descriptor, so 16 Å, rather than 1.6 nm. That should work just fine for the next 10+ years, barring any sudden paradigm shift (given the number of advanced technologies vying for an opportunity to displace silicon at smaller scales). Reply JamesJones44 usertests said: So it's a half node that introduces backside power delivery. The power reduction is where it's at. Those handhelds need every bit they can get. The issue is people who don't understand resolutions and distance will be demanding 8K on those handhelds by then and will sap any energy savings gained. Reply usertests JamesJones44 said: The issue is people who don't understand resolutions and distance will be demanding 8K on those handhelds by then and will sap any energy savings gained. I think Steam Deck is leading the pack of x86 handhelds in sales... with a 720p screen. I can't imagine most buyers caring about more than 1080p anytime soon. BTW, still waiting for 8K mini tablets: https://www.anandtech.com/show/13742/new-8k-oled-displays Reply JamesJones44 usertests said: I think Steam Deck is leading the pack of x86 handhelds in sales... with a 720p screen. I can't imagine most buyers caring about more than 1080p anytime soon. BTW, still waiting for 8K mini tablets: https://www.anandtech.com/show/13742/new-8k-oled-displays I was exaggerating a bit with 8K, but I read people complaining about smart phone screen resolutions all the time even though you would have to stick you face on the screen to see a pixel for higher end models (which is where I see people saying dumb things like, oh still no 4K...). People also complain a lot about the switch being locked to 720p when in hand held mode, it's not unique to smart phones (heck I see people looking for 8k monitors on random reddit posts to play games). User's who don't understand how resolutions relate to size and distance tend to push these silly things is what I was trying to say. Reply zoridon JamesJones44 said: I was exaggerating a bit with 8K, but I read people complaining about smart phone screen resolutions all the time even though you would have to stick you face on the screen to see a pixel for higher end models (which is where I see people saying dumb things like, oh still no 4K...). People also complain a lot about the switch being locked to 720p when in hand held mode, it's not unique to smart phones (heck I see people looking for 8k monitors on random reddit posts to play games). User's who don't understand how resolutions relate to size and distance tend to push these silly things is what I was trying to say. I agree, I'm running a 32 inch curved 1440p monitor and sometimes I feel I could hold off an extra two years between gpu upgrades if I went back to 1080p with a 27 inch monitor and up my refresh rate at the same time. I don't think I can tell the difference until I hit around the 30 inch level but the extra frames would be nice. Reply brandonjclark zoridon said: I agree, I'm running a 32 inch curved 1440p monitor and sometimes I feel I could hold off an extra two years between gpu upgrades if I went back to 1080p with a 27 inch monitor and up my refresh rate at the same time. I don't think I can tell the difference until I hit around the 30 inch level but the extra frames would be nice. I remember the guy at microcenter trying to tell me I'm wasting my 3090 pushing too few pixels at 1440p (32inches). My guy, I care about REFRESH RATE. Reply DougMcC CerianK said: Just in case anyone needs to hear it, Angstrom-class refers to any process node moving forward where it would otherwise be necessary to include a decimal point in the node descriptor, so 16 Å, rather than 1.6 nm. That should work just fine for the next 10+ years, barring any sudden paradigm shift (given the number of advanced technologies vying for an opportunity to displace silicon at smaller scales). If they had just gone with F instead of A they could have been done for good. :-( Reply DougMcC JamesJones44 said: The issue is people who don't understand resolutions and distance will be demanding 8K on those handhelds by then and will sap any energy savings gained. 8k is a good choice for lossless content scaling though. How fine a pixel pitch you can perceive is not the only factor in how good content looks on your device. Reply TerryLaze DougMcC said: 8k is a good choice for lossless content scaling though. How fine a pixel pitch you can perceive is not the only factor in how good content looks on your device. Huh?! Isn't anything with a 1:1 ratio a \"good choice\" for scaling? Too many pixels on too small of a space will just look like one big chunky pixel to the user, so 8k on a tiny display should look worse than 4k on the same area screen. Reply VIEW ALL 10 COMMENTS Show more comments MOST POPULAR TSMC readies lower-cost 4nm manufacturing tech: Up to 8.5% cheaper Qualcomm responds to benchmark cheating allegations — Snapdragon X Elite/Plus benchmarks claimed to be fraudulent (Updated) TSMCs 2nm nodes get NanoFlex, N2P loses backside power delivery TSMC unveils 1.6nm process technology with backside power delivery, rivals Intel's competing design More AMD Zen 5 CPUs spotted in Linux patch notes US investigates China's access to RISC-V — open standard instruction set may become new site of US-China chip war Fallout-inspired game runs in Excel — a spreadsheet-powered wasteland escape from your daily corporate wasteland AMD Strix Point engineering sample underwhelms in early Geekbench 6 results AMD-powered classic Macintosh Classic-Inspired mini PC available for retail — Ayaneo's new AM01 released after a successful Indiegogo campaign Raspberry Pi Compute Module 4S memory variants announced TSMC's labor practices draw serious concern in Arizona — the company's new chip plant allegedly plagued by worker abuses",
    "commentLink": "https://news.ycombinator.com/item?id=40156275",
    "commentBody": "TSMC unveils 1.6nm process technology with backside power delivery (tomshardware.com)360 points by elorant 22 hours agohidepastfavorite180 comments futureshock 20 hours agoComments about the marketing driven nm measurements aside, this still looks like another solid advance for TSMC. They are already significantly ahead of Samsung and Intel on transistor density. TSMC is at 197 MTr/mm2 wile Samsung is at 150 MTr/mm2 and Intel is at 123 MTr/mm2. This 1.6nm process will put them around 230 MTr/mm2 by 2026. When viewed by this metric, Intel is really falling behind. reply Workaccount2 20 hours agoparentIntel has a 1.4nm process in the pipeline for ~2027. They just took delivery on their first high NA EUV machine in order to start working on it. Their gamble however is that they need to figure out DSA, a long storied technology that uses self-forming polymers to allow less light to sharply etch smaller features. If they figure out DSA, they will likely be ahead of TSMC. If not, it will just be more very expensive languishing. reply Sysreq2 18 hours agorootparentThe nomenclature for microchip manufacturing left reality a couple generations ago. Intel’s 14A process is not a true 14A half-pitch. It’s kind of like how they started naming CPUs off “performance equivalents” instead of using raw clock speed. And this isn’t just Intel. TSMC, Samsung, everyone is doing half-pitch equivalent naming now a days. This is the industry roadmap from 2022: https://irds.ieee.org/images/files/pdf/2022/2022IRDS_Litho.p... If you look at page 6 there is a nice table that kind of explains it. Certain feature sizes have hit a point of diminishing returns, so they are finding new ways to increase performance. Each generation is better than the last but we have moved beyond simple shrinkage. Comparing Intel’s 14A label to TSMCs 16A is meaningless without performance benchmarks. They are both just marketing terms. Like the Intel/AMD CPU wars. You can’t say one is better because the label says it’s faster. There’s so much other stuff to consider. reply martinpw 11 hours agorootparent> This is the industry roadmap from 2022: https://irds.ieee.org/images/files/pdf/2022/2022IRDS_Litho.p... Very interesting document - lots of numbers in there for real feature sizes that I had not seen before (Table LITH-1). And this snippet was particularly striking: Chip making in Taiwan already uses as much as 10% of the island’s electricity. reply Workaccount2 14 hours agorootparentprev\"Likely better\" doesn't come from 14A vs 16A. It comes from Intel using High NA-EUV vs TSMC using double pattern Low NA-EUV. If Intel pulls off DSA, they will be using a newer generation of technology compared to TSMC using an optimized older generation. Could TSMC still make better chips? Maybe. But Intel will likely be better. reply brennanpeterson 9 hours agorootparentI am not sure where that would come from. There is nothing about dsa that means this. Dsa is one of many patterning assist technologies, just...an old one. Neat, but not 'new'. You use patterning assist to make smaller, more regular features, which is exactly what the 16a vs 18a refers to. That has somewhat less to do with performance, which is tied as much to material, stress, and interface parameters. Nothing gets better from being smaller in the post dennard scaling era, the work of integration is making better devices anyway. Patterning choices imply different consequences. For example,.a.double euv integration can take advantage of spacer assists to reduce ler and actually improve cdu even with a double expose. Selective etch can improve bias, spacer trickery can create uniquely small regular features that cannot be done with single patterns. Conversely, overlay trees get bushier, and via CD variance can cause horrific electrical variance. It is complicated, history dependent, and everything is on the developmental edge. reply Workaccount2 6 hours agorootparentDSA is what is going to make it possible for Intel to compete at all. Without it, they are going to have fancy machines in fancy foundries that are too expensive to attract any customers. To the best of my knowledge, DSA never made it out of the lab. reply 3abiton 30 minutes agorootparentBut still, what is stopping others from also developing DSA? I am not sure the technology alone will be Intel's savior. They've been on the decline for a while, ever since they took a jab at Nvidia for releasing CUDA, they demonstrated a narrow vision, consistently, and now they're playing a catch up game. reply tivert 12 hours agorootparentprev> If Intel pulls off DSA, they will be using a newer generation of technology compared to TSMC using an optimized older generation. Could TSMC still make better chips? Maybe. But Intel will likely be better. Is Intel working on \"an optimized older generation\" as a backup plan? I don't follow semiconductors very closely, but my impression is the reason they're \"behind\" is they bet aggressively on an advanced technology that didn't pan out. reply Golden_Wind123 11 hours agorootparentFrom what I remember the aprocyphal story is that Intel dragged their feet on adopting EUV and instead tried to push multi-patterning past it's reasonable limits. If that's the actual root cause, then Intel's lagging is due to optimizing their balance sheets (investors like low capital expenditures) at the expense of their technology dominance. reply confused_boner 16 hours agorootparentprevwould performance per watt be the right way to benchmark? reply futureshock 15 hours agorootparentBenchmarks are tricky because it all depends on workload and use case. If you are in a VR headset for example, it’s all about power envelope and GPU flops. If you are in a Desktop used for productivity it might be all about peak CPU performance. reply wtallis 13 hours agorootparentprevWhen comparing fab processes, you wouldn't want performance of a whole processor but rather the voltage vs frequency curves for the different transistor libraries offered by each fab process. reply saganus 19 hours agorootparentprevDo you know the name of the company that produces the EUV machine? is it ASML? It is my understanding that only ASML had cracked the EUV litography, but if there's another company out there, that would be an interesting development to watch. reply Rinzler89 18 hours agorootparent>It is my understanding that only ASML had cracked the EUV litography Ackshually, EUV was cracked by Sandia Labs research in the US, with EUV light sources built by Cymer in the US. ASML was the only one allowed to license the tech and integrate it into their steppers after they bough Cymer in 2013. Hence why US has veto rights to whom Dutch based ASML can sell their EUV steppers to, as in not to China, despite ow much ASML shareholders would like that extra Chinese money. reply ahartmetz 14 hours agorootparent>EUV was cracked More like it was started. There were a ton of gnarly problems left that took over ten years and billions of € to solve. Producing a few flashes of EUV and getting a few photons on the target is relatively easy. Producing a lot of EUV for a long time and getting a significant fraction (...like 1%) of the photons on the target is very hard. reply hajile 11 hours agorootparentEUV was supposed to ship in the early 2000s and the work on it started in earnest the 90s. It turned out that EUV is way harder than anyone imagined. The only reason we did stuff like immersion DUV was because we couldn’t get EUV to work. reply ano-ther 18 hours agorootparentprevMore on the history here: https://www.asml.com/en/news/stories/2022/making-euv-lab-to-... reply oivey 16 hours agorootparentPretty cool story of the National Lab system and the closest thing the US has to “government scientists” massively shaping the future. reply EMCymatics 16 hours agorootparentprevThanks for the source. I'm glad Bell labs pounced on the opportunity. reply saganus 16 hours agorootparentprevDid not know that. Thanks reply s3p 18 hours agorootparentprevThey don't need anyone else's money. Intel has bought all of TSMC's EUV and High-NA EUV machines for the next several years. reply Rinzler89 17 hours agorootparent>They don't need anyone else's money There's no such thing in capitalism. Limited supply semi is(was) a bidding war, where China had a blank cheque and was willing to outbid everyone else to secure semi manufacturing supremacy. Do you think Intel or TSMC could have scored all that ASML supply at that price if China would have been allowed to bid as well? ASML would have been able to score way higher market prices per EUV stepper had China been allowed in the game, and therefore higher profits. You think ASML shareholders hate higher profits or what? Nvidia sure didn't during the pandemic. You wanted a GPU? Great, it would cost you now 2x-3x the original MSRP because free market capitalism and the laws of supply and demand. reply nabla9 16 hours agorootparentprevYes, and Intel was the first to install the first High-NA EUV machine from ASML, TWINSCAN EXE:5000 https://www.youtube.com/watch?v=3PCtf1ONYMU ps. Intel used to own 15% of ASML in 2012, now they own less than 2%. reply singhrac 19 hours agorootparentprevIt’s ASML. reply hackernudes 19 hours agorootparentprevDSA = Directed Self-Assembly reply dev1ycan 18 hours agorootparentprevIntel says a lot of things but until they put them out in the field I don't believe a word they say reply kumarvvr 15 hours agoparentprevIts so hard to even fathom 200+ Million Transistors in 1 square millimeter ! And, to think, it's all done with light ! We live in interesting times ! reply seastarer 13 hours agoparentprevTranslating 197 M/mm2 into the dimensions of a square, we get a dimension of 71nm. If we compute the \"half-pitch\", that's 35.5nm. 230 M/mm2 translates to 33nm \"half-pitch\". Of course, transistors aren't square and aren't so densely packed, but these numbers are more real IMO. reply black_puppydog 15 hours agoparentprevStupid beginner question: is MTr/mm² really the right thing to be looking at? Shouldn't it be more like mm²/MTr ? This feels kind of like these weird \"miles per gallon\" units, when \"gallons per mile\" is much more useful... reply simpsond 15 hours agorootparent200 million transistors per square millimeter. Gallons per mile only makes sense when you are talking about dragsters. reply baq 14 hours agorootparentgallons per hundred miles make much more sense than that. incidentally, this is the measure rest of the world is advertising, except usually in liters per 100km. there's a good reason for this: comparisons linear instead of inversely proportional. 6l/100km is 50% better than 9l/100km. 30mpg vs 20mpg is... not as simple. reply gameshot911 59 minutes agorootparent>30mpg vs 20mpg is... not as simple. How so? reply Detrytus 14 hours agorootparentprevGallons per hundred miles would be more useful. I guess it's a matter of approach. Europeans are traveling familiar, constant distances and worry about fuel cost. Americans just fill up their tank and worry how far they can go :) reply satiric 10 hours agorootparentIf I buy 2 gallons of gas, and my car gets 30 mpg, then I can go 60 miles. Doesn't seem that hard to me. Need the other way around? I need to go 100 miles. At 30 miles per gallon, that's a little over 3 gallons. This is simple mental math. reply bee_rider 11 hours agorootparentprevWhat’s the argument for either? They are, of course, equivalent… might as well pick the one where bigger=better. reply alberth 12 hours agoparentprev> This 1.6nm process will put them around 230 MTr/mm2 Would it be that x2 (for front & back)? E.g., 230 on front side and another 230 on back side = 460 MTr/mm2 TOTAL reply zeusk 9 hours agorootparentBSPDN is not about putting devices on the front and back, the logic layer is still mostly 2D, it's about the power connects moving to the back of the chip so there's less interference with logic and larger wiring can be used. reply ReptileMan 20 hours agoparentprevNot understanding chip design - but is it possible to get more computational bang with less transistors - are there some optimizations to be had? Better design that could compensate for bigger nodes? reply 7thpower 18 hours agorootparentYes, generally one of the trends has been movement toward specialized coprocessors/accelerators. This was happening before the recent AI push and has picked up steam. If you think of an SOC, the chip in your phone, more and more of the real estate is being dedicated to specialized compute (AI accelerators, GPUs, etc. vs general purpose compute (CPU). At the enterprise scale, one of the big arguments NVIDIA has been making, beyond their value in the AI market, has been the value of moving massive, resource intense workloads from CPU to more specialized GPU acceleration. In return for the investment to move their workload, customers can get a massive increase in performance per watt/dollar. There are some other factors at play in that example, and it may not always be true that the transistors/mm^2 is always lower, but I think it illustrates the overall point. reply merb 15 hours agorootparentJavaScript accelerator would probably half the power consumption of the world. The problem is just, that as soon as it would have widespread usage it would probably already be too old. reply robocat 14 hours agorootparentReminiscent of the Java CPUs: Not even used for embedded (ironically the reason Java was created?). And not used at all by the massive Java compute needed for corporate software worldwide? reply crote 12 hours agorootparentWeren't they used in Java Cards? Basically, every single credit card sized security chip (including actual credit cards, of course) is a small processor running Java applets. Pretty much everyone has one or more in their wallet. I'd assume those were actual Java CPUs directly executing bytecode? reply robocat 8 hours agorootparent> Weren't they used in Java Cards? Not sure: https://en.wikipedia.org/wiki/Java_processor doesn't seem to mention any in current use. I am ignorant of the actual correct answer: I had simply presumed it is simpler to write the virtual machine using a commercial ISA than to develop a custom ISA. Java Card bytecode run by the Java Card Virtual Machine is a functional subset of Java 2 bytecode run by a standard Java Virtual Machine but with a different encoding to optimize for size. https://en.wikipedia.org/wiki/Java_Card_OpenPlatform has some history but nothing jumped out to answer your question. reply greenavocado 7 hours agorootparentpicoJava processors (Sun Microsystems) https://www.cnet.com/culture/sun-releases-complete-chip-desi... Patriot Scientific's Ignite processor family https://www.cpushack.com/2013/03/02/chuck-moore-part-2-from-... ARM Jazelle technology https://developer.arm.com/documentation/ddi0222/b/introducti... https://www.eetimes.com/nazomi-offers-plug-in-java-accelerat... It's all dot-com era stuff and Sun Microsystems also created a Java OS that could run directly on hardware without a host operating system. That's about it reply greenavocado 7 hours agorootparentprevNo, credit cards do not have CPUs or run Java code. reply choilive 15 hours agorootparentprevI believe ARM has some instructions that is JavaScript specific so we're kinda in that direction already. reply zeusk 9 hours agorootparentBack in the day, they supported byte code execution - https://en.wikipedia.org/wiki/Jazelle reply merb 14 hours agorootparentprevYeah the https://developer.arm.com/documentation/dui0801/h/A64-Floati... but a full jit helper that is generalized is way way harder and as said will take a long time tobe generally available. Just look at wasmgc and that only has the minimum denominator. reply jjk166 16 hours agorootparentprevThe design optimization software for modern semiconductors is arguably the most advanced design software on earth with likely tens if not hundreds of millions of man-years put into it. It takes into account not only the complex physics that apply at the nano-scale but also the interplay of the various manufacturing steps and optimizes trillions of features. Every process change brings about new potential optimizations, so rather than compensating for bigger nodes it actually widens the gap further. By analogy, the jump from hatchet to scalpel in the hands a layman is far less than the jump from hatchet to scalpel for a skilled surgeon. reply polishdude20 8 hours agorootparentHow is this kind of software developed without becoming a massive pile of spaghetti code? reply smj-edison 8 hours agorootparentprevHeh, as long as you're not talking about UX... reply kjkjadksj 15 hours agorootparentprevWhat about the effect of heat? More transistors per area equate a hotter chip, no? reply robocat 14 hours agorootparentMore transistors switching I thought? It isn't about raw transistors e.g. memory isn't as power hungry as GPU? reply mort96 19 hours agorootparentprevIn general, yes, but all the chip design companies have already invested a whole lot of time and engineering resources into squeezing as much as possible from each transistor. But those kinds of optimizations are certainly part of why we sometimes see new CPU generations released on the same node. reply pjc50 18 hours agorootparentprevEveryone generally does that before sending the design to the fab. Not to say that improvements and doing more with less are impossible, they probably aren't, but it's going to require significant per design human effort to do that. reply pixelpoet 19 hours agorootparentprevSuch optimisation would apply equally to the more dense processes, though. reply potatolicious 18 hours agorootparentprevSome yeah, but many of these optimizations aren't across-the-board performance improvements, but rather specializations that favor specific kinds of workloads. There are the really obvious ones like on-board GPUs and AI accelerators, but even within the CPU you have optimizations that apply to specific kinds of workloads like specialized instructions for video encode/decode. The main \"issue\", such as it is, is that this setup advantages vertically integrated players - the ones who can release software quickly to use these optimizations, or even going as far as to build specific new features on top of these optimizations. For more open platforms you have a chicken-and-egg problem. Chip designers have little incentive to dedicate valuable and finite transistors to specialized computations if the software market in general hasn't shown an interest. Even after these optimizations/specialized hardware have been released, software makers often are slow in adopting them, resulting in consumers not seeing the benefit for a long time. See for example the many years it took for Microsoft to even accelerate the rendering of Windows' core UI with the GPU. reply rowanG077 17 hours agoparentprevAre intel really just the best chip designers on the earth or why can they compete with such densities with AMD? reply smolder 16 hours agorootparentI'd say they haven't been very competitive with AMD in performance per watt/dollar in the ryzen era, specifically due to process advantage. (On CPU dies especially, with less advantage for AMDs I/O dies.) I'd agree they have done a good job advancing other aspects of their designs to close the gap, though. reply Havoc 21 hours agoprev>1.6nm Gotta love how we now have fractions of a near meaningless metric. reply audunw 15 hours agoparentThe metric means what it has always meant: lower numbers mean higher transistor density on some area (the digital part) of the chip. What more do you want? It’s as meaningful as a single number ever could be. If you’re a consumer - if you don’t design chips - lower number means there’s some improvement somewhere. That’s all you need to know. It’s like horse powers on a car. The number doesn’t tell you everything about the performance under all conditions but it gives a rough idea comparatively speaking. If you’re a chip designer then you never cared about the number they used in naming the process anyway. You would dig into the specification and design rules from the fab to understand the process. The “nm” number might show up in the design rules somewhere.. but that’s never been relevant to anyone, ever. I really don’t understand why so many commenters feel the need to point out that “nm” doesn’t refer to a physical dimension. Who cares? It doesn’t have any impact on anyone. It’s very mildly interesting at best. It’s a near meaningless comment. reply Havoc 12 hours agorootparent>The metric means what it has always meant That's just patently wrong. The measurement used to refer to feature size [0]. It used to be a physical measurement of a thing in nanometers hence nm Now it's a free for all marketing team names it whatever the fuck they want. That's why you get stuff like intel 10nm being equivalent to AMD 7nm. [1]. It's not real. >Who cares? You...else we wouldn't be having this discussion [0] https://en.wikipedia.org/wiki/Semiconductor_device_fabricati... [1] https://www.pcgamer.com/chipmaking-process-node-naming-lmc-p... reply spxneo 15 hours agorootparentprevwhat is a rough rule of thumb for each 1nm reduction? What does that increase in transistor density look like each year? If the jump is bigger (5nm->3nm), does that rate of change increase too? trying to understand the economic impact of these announcements as I don't understand this topic well enough reply blackoil 21 hours agoparentprevNumber represents transistor density. 2nm has ~twice the density of 4nm. If you ignore nm as unit of distance it makes sense. reply misja111 19 hours agorootparentNope that's what it meant a long time ago. Nowadays, the nm number represents the smallest possible element on the chip, typically the gate length, which is smaller than the size of a transistor. This means that when different manufacturers use a different transistor design, their 'nm' process could be the same but their transistor density different. reply audunw 16 hours agorootparentYou have it the wrong way around. The nm number used to mean minimum gate length. But when moving to FinFET and other transistor innovations that provided increased density, they decreases the “nm” number AS IF the increased transistor density came from shrinking gate length with a traditional planar transistor. The nm has always been a very rough approximate proxy for transistor density. Nothing has really changed about how what the number implies. I find it so weird how so many people on hacker is so hung up on the “nm” numbering. None of these people are designing standard cell libraries and those that actually design standard cell libraries never cared about what’s in the marketing material anyway. Lowe number means higher transistor density in some ways, on some parts of the chip. That’s all reply 0x457 17 hours agorootparentprevNot even that. Those numbers just mean, \"this 3D 2nm-ish process performs similar to a 2D 2nm process\". It's the same situation as when AMD started putting numbers in their Athlon XP SKUs to tell \"this CPU performs just as good as intel of that frequency\". I, for one, think it makes sense. Because as I consumer I don't care about actual transited length or density, all I care about is: performance, efficiency, price. I can assume that going from TSMC N5 to TSMC N3 would result in an increase in efficiency and performance. People that do care about those things (i.e. electrical engineers) most likely don't use marketing materials... reply yau8edq12i 18 hours agorootparentprevNo, it's not even related to gate length or any kind of physical feature anymore. It's a purely marketing term. A \"3 nm\" node has a gate length around 16-18 nm. reply s3p 18 hours agorootparentOP isn't wrong. Once the switch was made from 2D to 3D gates, transistor numbering became \"this is the performance you would expect if you COULD shrink a gate to x nm\", and while it's inaccurate it lets people understand how differentiated a new generation is from the previous ones. It also lets them stay consistent with understanding the progression of chips over time as in most of the history of silicon the generations were defined by the gates' nm sizing. reply hinkley 17 hours agorootparentWe should have switched to gates per mm² long ago. IMO that covers the 3D problem fine. Doesn’t cover variance in feature size by task though. If the gates for a multiplier or cache need more space per gate than a logical OR then the marketing guys still get a little victory over truth. We could just divide total gates by surface area for the chip, but that would confuse the process efficiency with the implementation details and then we sacrifice instructions per second for a marketing number. reply maqp 18 hours agorootparentprevI don't see it as an issue if the marketing term maintains the original context: \"how large a planar FET should be for the transistor density used in this process\" Is this the case, I'm not entirely sure. If it is, there be a better unit for this measurement, absolutely. PLANETS: PLANar Equivalent Transistor Size Or hire a marketing team to figure a good one. reply _heimdall 20 hours agorootparentprevThey're using the wrong units if we need to consider the nanometer as something other than a measure of distance. reply defrost 20 hours agorootparentIn some applications it helps to be fluid in exchanging distance as time with time as distance. http://dataphys.org/list/grace-hopper-nanoseconds/ and the field of play here isn't 1D linear .. it's somewhere sort of a bit slightly > 2D (elements can be stacked, there are limitations on how much and how close) reply SAI_Peregrinus 18 hours agorootparentprevThink of them as \"node metrics\". 1.4 node metrics, not nanometers. They haven't referred to any distance in years, so make up your favorite backronym. Å can be Åwesomes, so 1.4 node metrics = 14 Åwesomes. Embrace nonsense, the chip makers did! reply exe34 18 hours agorootparentprevWait till you talk to theoretical physicists who set c=h-bar=1. reply _heimdall 12 hours agorootparentI have more fundamental concerns to talk with a theoretical physicist about first :D Time to kick an ants nest here... Quantum mechanics is a useless study that had no business being a scientific field. The most fundamental principles, like quantum superposition, can never be tested or validated and therefore could never be used to be predictive of anything other than similarly theoretical ideas. reply exe34 5 hours agorootparentShut up and calculate. reply bee_rider 10 hours agorootparentprevI don’t think they drop the dimensions, just the constants. reply exe34 5 hours agorootparentIt's equivalent to the 1.6nm != 1.6 nm though. reply phonon 20 hours agorootparentprevDimensions go by 1/root(2) to double density...so about .7 per generation. reply 2OEH8eoCRo0 16 hours agorootparentprevWouldn't 2nm have 4x the density of 4nm? reply __alexs 20 hours agoparentprevTSMC N5 is indeed about 30% more Transistors/mm2 than N7 as you might expect so seems reasonably meaningful? reply thsksbd 20 hours agorootparent1.6 nm is 16 A. If they continue the BS for much longer, the \"feature size\" will be smaller than the lattice of Si. reply __alexs 20 hours agorootparentYou're the one calling it a feature size, not TSMC. reply thsksbd 18 hours agorootparentThey're the ones specifying units. reply __alexs 1 hour agorootparentI'm begging even 1 person in this thread to do even 5 minutes of research on where these numbers come from. reply lostlogin 20 hours agorootparentprevI’m sure you agree that the measurement given is a marketing term and is generally unhelpful beyond ‘it’s smaller than the current number’. reply Tagbert 19 hours agorootparentit also seems to be a roughly proportional change in density so why would it be unhelpful? Just because you can't point to a specific feature and say that is 1.6nm, doesn't make the label meaningless as so many try to assert. It is a label that represents a process that results in a specific transistor density. What label would you prefer? reply __alexs 16 hours agorootparentprevThat you think it needs to convey anything other than \"better than the last version\" suggests a fundamental misunderstanding of marketing. It's actually even kind of decent because it is actually approximately more dense as you might suspect given the numbers. reply phkahler 20 hours agorootparentprevAnd it's not that much better than the previous 2nm node, so it seems like the number is just meant to be smaller than Intel 18A. I wish Intel in the switch to \"A\" would have at least returned to a physically meaningful measure. Then just maybe the industry could agree to use that for a while. Like 180A or whatever it needed to be - maybe just the smallest metal pitch or something. reply s_dev 21 hours agoparentprevWhy is it meaningless? reply blagie 21 hours agorootparentBecause it lost meaning somewhere between a micron and 100nm. From roughly the 1960s through the end of the 1990s, the number meant printed gate lengths or half-pitch (which were identical). At some point, companies started using \"equivalences\" which became increasingly detached from reality. If my 50nm node had better performance than your 30nm node because I have FinFETs or SOI or whatever, shouldn't I call mine 30nm? But at that point, if you have something less than 30nm somewhere in your process, shouldn't you call it 20nm? And so the numbers detached from reality. So now when you see a 1.6nm process, it's think \"1.6nm class\", rather than that corresponding to any specific feature size, and furthermore, understand that companies invent class number exaggerations differently. For example, an Intel 10nm roughly corresponds to Samsung / TSMC 7nm (and all would probably be around 15-30nm before equivalences). That should give you enough for a web search if you want all the dirty details. reply api 20 hours agorootparentThe same thing happened with chip frequency around the end of the 1990s. Chip frequencies stagnated (end of Dennard scaling if I remember correctly) giving the impression that single threaded performance had stagnated, but since then chip makers have used increasing data and instruction parallelism to squeeze even more apparent single threaded performance out of chips. A 3ghz chip today is usually way faster on average code than a 3ghz chip 15 years ago. They also started expanding into multiple cores and adding more cache of course. For fab processes we should just switch to transistor density. That still wouldn't capture everything (e.g. power efficiency) but would be a lot better than not-actually-nanometers. For performance we really don't have a good single metric anymore since all these performance hacks mean different levels of gain on different code. reply thsksbd 20 hours agorootparent\"For fab processes we should just switch to transistor density.\" Indeed May I propose transistor density divided by the (ergodic) average transistor switching power? reply Denvercoder9 15 hours agorootparentprev> The same thing happened with chip frequency around the end of the 1990s. Not really the same thing, though. If I buy a chip that's advertised as 3.4GHz, it'll run at 3.4GHz. Maybe not all the time, but it'll achieve that speed. If I buy a chip advertised as being produced with a 3nm process, there's nothing even resembling 3nm on there. reply antisthenes 20 hours agorootparentprev> For fab processes we should just switch to transistor density. The marketing departments of silicon companies are saving that as their ultimate weapon. reply jl6 19 hours agorootparentprevIt’s not meaningless, it’s near meaningless. That’s what the nm stands for, right? reply chmod775 21 hours agorootparentprevBecause it's not measuring anything, except the tech generation. It conveys about as much information as \"iPhone 14\". reply gwervc 21 hours agorootparentIt's at least more easily understandable (lower is newer) than the average tech product naming scheme, especially those by Microsoft. reply tommiegannert 20 hours agorootparentThe problem is it sounds like something any engineer can understand without domain knowledge, but interpreting it that way is completely wrong. The worst kind of naming. Not just IKEA-style random names (and I say that as a Swede,) but reusing a standard, while not keeping to what the standard is normally used for, and what it previously meant even in this domain. N1.6 is much better for naming node processes. Or even TSMC16. reply bmicraft 20 hours agorootparentNormally? That \"standard\" hasn't been used \"normally\" for 20 years now. Arguably the new way is normal in every sense of the word reply tommiegannert 16 hours agorootparentYes. By standard I mean that nanometers (\"nm\") is used to describe physical distances. That's it's normal use, understood by all engineers. That's also how it was born into the semiconductor domain. In that domain, it should have either stayed a description of physical distances, or been replaced. They could invent codenames, or start using a better (physical) metric, if this one is no longer relevant. reply dmix 19 hours agorootparentprevDo you think they'll be advertising 0.9nm in the future or switch it up at some point? reply bmicraft 18 hours agorootparentIntel has already switched to Ångström for the most part reply smallmancontrov 18 hours agorootparentprevI see MTr/mm2 a lot, it feels like a better measure, and it feels like the time is ripe for marketing to make a jump. Bigger is better, \"mega\" is cool, the numbers are hundreds to thousands in the foreseeable future so no chance of confusion with nm. What's not to love? But hey, I don't make these decisions, and I see no indication that anyone in a position to make these decisions has actually made them. Shrug. reply brandall10 20 hours agorootparentprevWhile it may be divorced from any particular measurement, doesn't it give an approximation of increased density? reply Havoc 21 hours agorootparentprevIt stopped being an actual measure of size long ago. The nm is t Nanometer anything it’s just a vague marketing thing attempting some sort of measure of generations reply andrewSC 21 hours agorootparentprevprocess/node \"size\" has, for some time now, been divorced from any actual physical feature on or of the chip/transistor itself. See the second paragraph of: https://en.wikipedia.org/wiki/2_nm_process for additional details reply helsinkiandrew 20 hours agoparentprevWonder how long until its back to round numbers? - 800pm reply mazurnification 20 hours agorootparentIt will be 18A first. reply highwaylights 20 hours agoparentprevI hear it also has many megahertz. reply 654wak654 21 hours agoprevSounds like a response to Intel's 18A process [0], which is also coming in 2026. [0] https://www.tomshardware.com/tech-industry/manufacturing/int... reply HarHarVeryFunny 19 hours agoparentIntel is the one trying to catch up to TSMC, not vice versa! The link you give doesn't have any details of Intel's 18A process, including no indication of it innovating in any way, as opposed to TSMC with their \"backside power delivery\" which is going to be critical for power-hungry SOTA AI chips. reply shookness 19 hours agorootparentWhile you are correct that it is Intel trying to catch TSMC, you are wrong about the origin of backside power delivery. The idea originated at Intel sometime ago, but it would be very ironic if TSMC implements it before Intel... reply sct202 18 hours agorootparentIntel is not the inventor of backside power, they are the first planning to commercialize it. It's similar to finfets and GAA where Intel or Samsung may be first to commercialize an implementation of those technologies, but the actual conceptual origin and first demonstrations are at universities or research consortiums like IMEC. Example Imec demonstrating backside power in 2019 https://spectrum.ieee.org/buried-power-lines-make-memory-fas... far before powerVia was announced. reply langsoul-com 9 hours agorootparentprevIntel has been out of the game for so long. Their deadlines are just PR speak, in reality, they'd definitely run into Road blocks. Not saying TSMC won't, but they have so much more experience in the cutting edge. reply s3p 18 hours agorootparentprevOP never said Intel wasn't trying to catch up. As far as backside power delivery, this is literally what Intel has been working on. It is called PowerVia. https://www.intel.com/content/www/us/en/newsroom/news/powerv... reply hinkley 17 hours agorootparentprevExcept for backside power. Intel published and had a timeline to ship at least one generation ahead of TSMC. I haven’t been tracking what happened since, but Intel was ahead on this one process improvement. And it’s not a small one, but it doesn’t cancel out their other missteps. Not by half. reply ajross 12 hours agorootparentprevThis isn't a comparison of shipping processes though, it's just roadmap products. And in fact until this announcement Intel was \"ahead\" of TSMC on the 2026 roadmap. reply HarHarVeryFunny 10 hours agorootparentI can't see that going down too well on an earnings call. \"Well, yes, we're behind TSMC in technology, but in our dreams we're way ahead!\" reply Alifatisk 21 hours agoparentprevThis is a response to something that’s coming in 2 years? Incredible how far ahead they are reply tgtweak 20 hours agorootparentFabs have a 3+ year lead time from prototyping the node to having it produce production wafers. reply blackoil 21 hours agorootparentprevThis will also come in 2026, so not that far ahead. reply nerbert 20 hours agorootparentDepends who will ultimately deliver. reply tvbv 21 hours agoprevcould someone ELi5 the backside power delivery please ? reply blagie 20 hours agoparentELI5: ICs are manufactured on silicon disks called wafers. Discs have two sides, and traditionally, everything was done on top. We can now do power on the bottom. This makes things go faster and use less power: * Power wires are big (and can be a bit crude). The bigger the better. Signal wires are small and precise. Smaller is generally better. * Big wires, if near signal wires, can interfere with them working optimally (called \"capacitance\"). * Capacitance can slow down signals on the fine signal wires. * Capacitance also increases power usage when ones become zeros and vice-versa on signal wires. * Big wires also take up a lot of space. * Putting them on the back of the wafer means that things can go faster and use less power, since you don't have big power wires near your fine signal wires. * Putting them in back leaves a lot more space for things in front. * Capacitance between power wires (which don't carry signal) actually helps deliver cleaner power too, which is a free bonus! This is hard to do, since it means somehow routing power through the wafer. That's why we didn't do this before. You need very tiny wires through very tiny holes in locations very precisely aligned on both sides. Aligning things on the scale of nanometers is very, very hard. How did I do? reply AdamN 19 hours agorootparentThere's still the question though of why they didn't do this decades ago - seems very obvious that this layout is better. What changed that made it possible only now and not earlier? reply blagie 19 hours agorootparentMy knowledge isn't current enough to offer more than speculation. However, something like an 80286 didn't even require a heatsink, while my 80486 had a dinky heat sink similar to what you might find on a modern motherboard chipset. At the same time, on a micron node, wires were huge. A few special cases aside (DEC Alpha comes to mind), power distribution didn't require anything special beyond what you'd see on your signal wires, and wasn't a major part of the interconnect space. Mapping out to 2024: 1) Signal wires became smaller than ever. 2) Power density is higher than ever, requiring bigger power wires. So there is a growing disparity between the needs of the two. At the same time, there is continued progress in figuring out how to make through-wafer vias more practical (see https://en.wikipedia.org/wiki/Three-dimensional_integrated_c...). I suspect in 2000, this would have been basically restricted to $$$$ military-grade special processes and similar types of very expensive applications. In 2024, this can be practically done for consumer devices. As costs go down, and utility goes up, at some point, the two cross, leading to practical devices. I suspect a lot of this is driven by progress in imagers. There, the gains are huge. You want a top wafer which is as close as possible to 100% sensor, but you need non-sensor area if you want any kind of realtime processing, full frame readout (e.g. avoiding rolling shutter), or rapid readout (e.g. high framerate). The first time I saw 3D IC technology in mainstream consumer use were prosumer-/professional-grade Sony cameras. I have strong fundamentals, but again, I stopped following this closely maybe 15 years ago, so much of the above is speculative. reply DSMan195276 18 hours agorootparentprev> seems very obvious that this layout is better \"Better\" is relative, the layout introduces more fabrication steps so it's only better if you actually get some benefit from it. Decades ago designs didn't require as much power or have as many transistors to wire so it wasn't an issue. reply pjc50 18 hours agorootparentprev> why they didn't do this decades ago You might as well ask why, since we can do it now, Shockley didn't simply start at 3nm. It's all a very long road of individual process techniques. > You need very tiny wires through very tiny holes in locations very precisely aligned on both sides. Key word here is \"both sides\". It has challenges similar to solder reflow on double sided boards: you need to ensure that work done on the first side isn't ruined/ruining work on the second side. https://semiwiki.com/semiconductor-services/techinsights/288... seems to be a good description. \"The challenges with BPR are that you need a low resistance and reliable metal line that does not contaminate the Front End Of Line (FEOL). BPR is inserted early in the process flow and must stand up to all the heat of the device formation steps.\" Contamination = metals used musn't \"poison\" the front-side chemistry. So they end up using tungsten rather than the more usual aluminium. (Copper is forbidden for similar chemistry reasons) It also (obviously) adds a bunch of processing steps, each of which adds to the cost, more so than putting the rails on the front side. reply whereismyacc 9 hours agorootparentprevI assume the idea itself is pretty obvious, but the manufacturing techniques to actually implement it are complicated reply pedrocr 20 hours agorootparentprev> You need very tiny wires through very tiny holes in locations very precisely aligned on both sides. Aligning things on the scale of nanometers is very, very hard. Do you need to align that precisely? Can't the power side have very large landing pads for the wires from the signal side to make it much easier? reply SAI_Peregrinus 18 hours agorootparentNot big enough, they still need to fit the size of the transistors on the signal side. reply 1oooqooq 18 hours agorootparentprevso the wafer is a huge ground plane? still can't see how one side is separate from the other if its the same block. reply blagie 15 hours agorootparentThe wafer is thick. Let's call it a mm thick (not quite, but close). Devices are tiny. The claim is 1.6nm, which isn't quite true, but let's pretend it is, since for the qualitative argument, it doesn't make a difference. That's on the order of a million times smaller than the thickness of the wafer. Historically, everything was etched, grown, deposited, and sputtered on one side of the wafer. The rest of the wafer was mostly mechanical support. The other side of the wafer is a universe away. The world is more complex today, but that's a good model to keep in mind. For a 3d integrated circuit, you would do this, and then e.g. grind away the whole wafer, and be left with a few micron thick sheet of just the electronics, which you'd mechanically place on top of another similar sheet. That's every bit as complex as it sounds. That's why this was restricted to very high-end applications. As for whether the wafer is a huge ground plane, that's complex too, since it depends on the top of the device and the IC: * First, it's worth remembering a pure silicon crystal is an insulator. It's only when you dope it that it becomes a conductor. The wafer starts out undoped. * Early ICs had the whole wafer doped, and the collector of all the NPN transistors was just the wafer. There, it was a ground plane. * SOI processes deposit a layer of glass on top of the wafer, and everything else on the glass. There, the wafer is insulated from the circuit. So all of this can very quickly go in many directions, depending on generation of technology and application. I'm not sure this post is helpful, since it's a lot of complexity in an ELI5, so I'll do a TL;DR: It's complicated. (or: Ask your dad) reply 1oooqooq 15 hours agorootparentnow I'm even more confused. why start with Si if you're going to put a glass layer before anything else? why not start with glass right away? reply blagie 14 hours agorootparentErgo, the TL;DR :) Even so, I oversimplified things a lot (a lot of the processes to leverage the silicon wafer, but some don't): https://en.wikipedia.org/wiki/Silicon_on_insulator One of the things to keep in mind is that a silicon wafer starts with a near-perfect silicon ingot crystal: https://en.wikipedia.org/wiki/Monocrystalline_silicon The level of purity and perfection there is a little bit crazy to conceive. It's also worth noting how insanely tiny devices are. A virus is ≈100nm. DNA is 2nm diameter. We're at > There are something like ≈100 billion transistors per IC for something like a high-end GPU, and a single failed transistor can destroy that fancy GPU. No, it can't thanks to this fancy marketing strategy where you sell faulty GPUs at lower price, as lower-tier model. reply pmx 20 hours agorootparentprevThanks this was a very useful summary for me! reply vintendo64 20 hours agorootparentprevthank you for the explanation! It really helped reply consp 20 hours agorootparentprevnext [3 more] [flagged] lostlogin 20 hours agorootparent> I detest ELI5 as it's managerial nonsense. I thought it came from Reddit, which is about as far from ‘managerial’ as one can get. reply claar 19 hours agorootparentWhat is it you think managers do, if not waste all day on reddit? reply bayindirh 21 hours agoparentprevSignal wires and power wires are not routed together, but separately. As a result, transistors are sandwiched between two wiring stacks. Resulting in lower noise, but makes heat removal a bit harder AFAICS. See the image at: https://www.custompc.com/wp-content/sites/custompc/2023/06/I... reply Tempest1981 18 hours agorootparentHow many power layers are there? Or how do the power \"wires\" cross over each other? Are they sparse, like wires? Or solid, like the ground plane of a PCB? Are there \"burried vias\"? reply __s 21 hours agoparentprevhttps://www.imec-int.com/en/articles/how-power-chips-backsid... has some explanation that's a bit beyond ELI5, but they have 2 pictures: frontside: https://www.imec-int.com/_next/image?url=%2Fsites%2Fdefault%... backside: https://www.imec-int.com/_next/image?url=%2Fsites%2Fdefault%... Another graphic samsung used: https://www.thelec.net/news/photo/202210/4238_4577_1152.jpg From https://www.tomshardware.com/pc-components/cpus/samsung-to-i... > Typically, backside power delivery enables thicker, lower-resistance wires, which can deliver more power to enable higher performance and save power. Samsung's paper noted a 9.2% reduction in wiring length, enhancing performance reply barfard 21 hours agorootparentthe pics go to 404 reply __s 20 hours agorootparentthanks, I've fixed them btw, you appear to be shadow banned, probably on account of being downvoted for making short comments like \"good\" or \"I don't get it\" Short & simple responses may be adequate at times, but usually people will view it as not adding anything Perhaps reviewing guidelines https://news.ycombinator.com/newsguidelines.html will give an idea of the ideal being yearned for (granted, it's an ideal). In general, trying to enhance the conversation with relevant information/ideas. \"your links are 404\" was definitely relevant here reply gcanyon 20 hours agorootparentHow can you tell someone is shadowbanned? reply __s 19 hours agorootparentTheir new posts will be dead. You can enable seeing dead posts in settings. Then if you click on their account & view their comments, it'll all be grayed out I vouched for barfard's comment in this thread reply Keyframe 21 hours agoparentprevinstead of signals and power path going through the same side (frontside) causing all sorts of issues and inefficiency, they're decoupling where power is coming from (from the other, backside, err side). More importantly, intel saw it as one of two key technologies of them moving into angstrom era, and was touting itself they'll be the first one to bring it to life (not sure they did).. so this seems to be more of a business power move. more on all of it from anandtech: https://www.anandtech.com/show/18894/intel-details-powervia-... reply brohee 21 hours agoparentprevNot ELI5 but I found this 20mn video pretty informative https://www.youtube.com/watch?v=hyZlQY2xmWQ reply wwfn 21 hours agoparentprevI'm also clueless. First search brings up https://www.tomshardware.com/news/intel-details-powervia-bac... with a nice diagram It looks like the topology for backside moves the transistors to the middle so \"singal wires and power wires are decoupled and optimized separately\" instead of \"compete[ing] for the same resources at every metal layer\" reply gostsamo 21 hours agoparentprevAn article on the topic though about Intel: https://spectrum.ieee.org/backside-power-delivery reply AnimalMuppet 21 hours agorootparentSo, if I understood that correctly, \"frontside\" actually is the top side, and \"backside\" is actually the bottom side. They're delivering power from the bottom (substrate) side, and connecting the data on the top side. reply vollbrecht 21 hours agoparentprevthere is a good youtube video that explains it https://www.youtube.com/watch?v=hyZlQY2xmWQ reply brennanpeterson 21 hours agoparentprevHalf your wires deliver power, half deliver signal. So if you do both on the same side, you need twice the density of wires. If you split the delivery into two parts, you get double the density without needing to make things smaller. reply blagie 20 hours agorootparentThis isn't quite right. Big wires (ideally entire planes) deliver power. Small wires deliver signal. Half and half isn't the right split, and you don't want to make power wires smaller. The very different requirements of the two is where a lot of the gains come in. reply brennanpeterson 9 hours agorootparentTrue! I went a little far in the name of 'eli5'. I think it roughly holds that you gain about a factor of 1.5 in routing density by removing the power distribution, so you can relax some critical patterning. But I havent looked closely in a long time. reply mbajkowski 20 hours agorootparentprevIt is going to get even a bit more interesting when you consider power gaters and virtual power supplies. Now the real power will be on the back side and the virtual power will be on the front side. Fun time for power analysis. reply ThinkBeat 15 hours agoprevI am not sure I understand backside in this instance and the illustration in the article didn't entirely help. In general, at least in older time, one side of the CPU has all the nice pins on it, and the motherboard has a pincushion that the pins match nicely. At the top of the CPU you put a HUGE heatsink on it and off you go. In this configuration the power delivery must be via the pincushion, through some of the pins. Intuitively that sounds to me like the power is coming in the backside? But given that it is a big deal I am missing something. Is the power fed from the \"top\" of the cpu where the heatsink would sit? reply sgalloway 14 hours agoparentHere's a very good youtube video on the subject of backside power delivery I watched a few days ago. https://youtu.be/hyZlQY2xmWQ?si=JuXtzRy0U0onEzoe reply tgtweak 20 hours agoprevA16 in 2027 vs Intel's 18A in full swing by 2026 feels like a miss on TMSCs behalf. This looks like an open door for fabless companies to try Intel's foundry service. reply akmittal 20 hours agoparentIntel has made lots of big promises, they have yet to catch us to TSMC reply SilverBirch 17 hours agoparentprevThere are definitely going to be people taking a bet on the Intel foundry, but Intel has tried this before and it has worked badly. reply onepointsixC 14 hours agorootparentSounds like an easy short for you then. reply crowcroft 20 hours agoparentprevIntel hasn't got a great track record in recent history. Really hoping they can hit their timeline though. reply drekipus 10 hours agoprevif people find the topic interesting, I recommend the book \"Chip War\" [1] It's a great read. very dense in narrative facts. [1] https://www.amazon.com.au/gp/aw/d/1398504122/ref=ox_sc_act_i... reply sylware 21 hours agoprevIt seems the actually impressive thing here is the 15-20% less power consumption at same n2 complexity/speed. reply bitwize 8 hours agoprevAvailable in Apple products this Christmas, everybody else's in the back half of the decade, maybe. reply RecycledEle 8 hours agoprevI love that they're using the back side now. It reminds me of finally using the back of a PCB. reply gosub100 14 hours agoprevThe \"Hollywood accounting\" transistor density aside, I think a new metric needs to become mainstream: \"wafers per machine per day\" and \"machines per year manufactured\". Getting these machines built and online is more important than what one machine (that might be less than 6 per year) can do. The information, I'm sure, is buried in their papers, but I want to know what node processes are in products available now. reply m3kw9 18 hours agoprevSo much for physical limitations, it seems you can keep halving smaller till infinity. reply gtirloni 20 hours agoprev [–] > This technology is tailored specifically for AI and HPC processors that tend to have both complex signal wiring and dense power delivery networks Uh? reply magicalhippo 20 hours agoparentI imagine it's because AI and HPC processors are typically utilized much more fully than your regular desktop processor. A typical desktop CPU is designed to execute very varied and branch-heavy code. As such they have a lot of cache and a lot of logic transistors sitting idle at any given time, either waiting for memory or because the code is adding not multiplying for example. You can see that in the die shots like this[1] for example. I imagine the caches are relatively regular and uniform and as such as less complex signal wiring, and idle transistors means lower power requirements. AI and HPC processors are more stream-oriented, and as such contain relatively small cachees and a lot of highly-utilized logic transistors. Compare the desktop CPU with the NVIDIA A100[2] for example. Thus you got both complex wiring, all those execution units needs to be able to very quickly access the register file, and due to the stream-oriented nature one can fully utilize most of the chip so a more complex power delivery network is required. edit: Power delivery tracks can affect signal tracks due to parasitic coupling if they're close enough, potentially causing signals to be misinterpreted by the recipient if power usage fluctuates which it will do during normal operation (if say an execution unit goes from being idle to working on an instruction, or vice versa). Thus it can be challenging to fit both power and signal tracks in close proximity. [1]: https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-... [2]: https://www.tomshardware.com/news/nvidia-ampere-A100-gpu-7nm reply gtirloni 18 hours agorootparentThe needs of AI/HPC at the chip level weren't clear to me. Thanks for the insightful answer. reply mazurnification 20 hours agoparentprevThere are certain trade offs when designing process. TSMC researched with partners how AI and HPC (high performance computing) chips most likely will look like and adjusted process accordingly. In fact this is big deal as until recently the processes were more tailored toward mobile application (that is were trading some switching performance for lower power consumption). Look like we are back in 2000s when speed/density is again more important than power consumption. reply mort96 19 hours agoparentprevAs opposed to low power processors (e.g laptops/phones), that tend to have less complex wiring and less dense power delivery networks reply mastax 20 hours agoparentprev [–] Pardon? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "TSMC unveiled its advanced 1.6nm-class process technology, A16, incorporating backside power delivery network (BSPDN) and gate-all-around (GAA) nanosheet transistors.",
      "A16 boasts enhanced performance, efficiency, power consumption, and transistor density compared to its forerunner, set to rival Intel's 14A node.",
      "The introduction of BSPDN, notably the Super Power Rail (SPR), in A16 enhances its complexity but provides substantial advantages for AI and HPC processors."
    ],
    "commentSummary": [
      "TSMC unveiled a cutting-edge 1.6nm process with backside power delivery, while Intel utilizes DSA technology for a 1.4nm process, highlighting the industry's performance-oriented focus.",
      "The discussions delve into topics such as semiconductor manufacturing supremacy, transistor density, chip design optimization, and the hurdles of EUV lithography technology.",
      "Advancements in AI/HPC processors, through-wafer vias, tungsten use in manufacturing, and complexities in silicon tech are also key points in the conversation, emphasizing the ongoing evolution and challenges in the semiconductor sector."
    ],
    "points": 360,
    "commentCount": 178,
    "retryCount": 0,
    "time": 1714045656
  },
  {
    "id": 40159278,
    "title": "NAND: Web-Based Programmable Computer for DIY Tech Enthusiasts",
    "originLink": "https://github.com/ArhanChaudhary/NAND",
    "originBody": "I am proud to present my solo hobby project NAND. This year-long undertaking follows the completed Nand to Tetris course, but ported to the web with its own runtime, user interface, and IDE. Using the \"Load example program\" selector, you can try out some programs I wrote on NAND&#x27;s emulated hardware such as 2048, a genetic algorithm, and a manual stack overflow to corrupt the screen.Check out NAND at https:&#x2F;&#x2F;nand.arhan.shAdditionally, I&#x27;ve authored an extensive writeup about the project. Read about it on the GitHub repository&#x27;s readme.",
    "commentLink": "https://news.ycombinator.com/item?id=40159278",
    "commentBody": "I made a programmable computer from NAND gates (github.com/arhanchaudhary)325 points by ArchAndStarch 16 hours agohidepastfavorite53 comments I am proud to present my solo hobby project NAND. This year-long undertaking follows the completed Nand to Tetris course, but ported to the web with its own runtime, user interface, and IDE. Using the \"Load example program\" selector, you can try out some programs I wrote on NAND's emulated hardware such as 2048, a genetic algorithm, and a manual stack overflow to corrupt the screen. Check out NAND at https://nand.arhan.sh Additionally, I've authored an extensive writeup about the project. Read about it on the GitHub repository's readme. rpmw 15 hours agoWow that is a great side-project, and a great README to boot. I've been meaning on working through Nand to Tetris after playing around some with Ben Eater's 6502 Computer (https://eater.net/) reply laweijfmvo 14 hours agoparentWould it be at all feasible to build a physical NAND-to-tetris computer? Or is it purely a virtual exercise? reply ArchAndStarch 14 hours agorootparentA few nand2tetris fanatics have actually done this! And by a few, I mean quite a lot of people. Here's one such hardware project of nand2tetris: https://gitlab.com/x653/nand2tetris-fpga/ But you can Google \"nand2tetris fpga\" for more. reply moefh 13 hours agorootparentprevThere's this one that goes one step beyond that, it's built out of 40,000 discrete transistors: https://www.youtube.com/watch?v=z71h9XZbAWY EDIT: there's more information here: https://www.megaprocessor.com/ reply dhosek 13 hours agorootparentI kind of want something midway between the FPGA version and the all-transistor version, something that just uses 7400 series chips (or, presumably there’s a 26-pin equivalent with 6 gates instead of three). Heck, I think even something that goes ahead and uses the full panoply of basic logic chips available could be kind of cool to see. reply moefh 12 hours agorootparentI think Ben Eater's 8-bit computer is closer to what you want: https://eater.net/8bit/ It's been a few years since I studied it (I even built the clock module, registers and the ALU), but from what I remember the biggest departing point from what you want is that the control logic (from instruction decoding to deciding which sub-units to activate for each instruction) is done with an EEPROM instead of individual logic gates, as described here: https://eater.net/8bit/control reply dailykoder 14 hours agorootparentprevProbably doable, but takes a lot of dedication. Especially debugging such physical endeavors is crazy reply theanonymousone 11 minutes agoprevNice job. Now we should program it in subleq2[0] :D [0] https://en.wikipedia.org/wiki/One-instruction_set_computer reply kristopolous 15 hours agoprevI could make a few college classes out of this. Well done material. reply farhanhubble 7 hours agoprevGreat work! You have seen the levels of abstraction that most programmers won't throughout their careers. reply not2b 9 hours agoprevDoing a design for this (specifically, design a microcoded, pipelined RISC processor, from the bottom up, with nothing but NAND gates) was the main problem on the Computer Hardware quals exam at UC Berkeley in the early 1990s. We didn't have to physically build it, though, just produce the detailed design on paper. reply ryeguy_24 13 hours agoprevThis is amazing work. I wanted to build something similar (virtual) while I was taking the Nand2Tetris course. I'm so impressed that you actually did it. You must have a really good understanding of how computers work now. reply marai2 12 hours agoparentAnd I was just thinking about the same thing this morning, using SVG to model the basic components. And lo and behold somebody has done a magnitude more amazing job then what I was imagining! reply mikestew 13 hours agoprevAwesome work! Bookmarked for in-depth perusal later. As a fan of NAND-to-Tetris, but never made it all the way through, I look forward to poking around in your project. reply IncreasePosts 6 hours agoprevLiar. You used NAND gates and a clock. reply userbinator 6 hours agoparent...a clock which can be made from a ring oscillator, consisting of an odd number of NAND gates wired as NOT gates. reply ArchAndStarch 6 hours agorootparentOh wow, I didn't actually know that. Thanks for the interesting trivia reply schoen 5 hours agorootparentprevHow do we know that that will converge to a single constant period of oscillation? Could you have a few different-sized square waves continue to cycle through the circuit? (I've never built or simulated that, I'm just trying to imagine what could happen!) reply rational_indian 1 hour agorootparent>Could you have a few different-sized square waves continue to cycle through the circuit? No. reply schoen 8 minutes agorootparentCould you help improve my intuition about that, or give me a reference where I could learn more? reply gmiller123456 14 hours agoprevCurious, how many NAND gates are there in total? reply ArchAndStarch 13 hours agoparentI've inspected my code closely. Every clock cycle, the NAND gate is used 3,234 times :) reply greenavocado 9 hours agoprevCan anybody recommend challenges similar to this one? reply wayoverthecloud 8 hours agoparentTry emulating Message Passing Interface. Could be lot more challenging though. reply apienx 15 hours agoprevThank you. First principles FTW! reply Animats 14 hours agoprevSeymour Cray would have loved this. Some of his computers were all NAND gates. reply dhosek 13 hours agoparentThe supercomputers (all?) used wirewrap rather than PCBs. I heard a story once about someone coming in for a demo of a supercomputer and Cray realized there was a bug in the hardware during the demo and while the potential customers were at lunch, he rewired the machine to fix the bug. reply Animats 13 hours agorootparentRight. Seymour Cray said that the two big problems in supercomputing were \"the thickness of the mat\" (of wires on the backplane) and getting rid of the heat. This is a Cray-I backplane.[1] [1] https://www.flickr.com/photos/geekmuseum/2926520634/ reply 2OEH8eoCRo0 14 hours agoprevFantastic work. NAND to Tetris helped me land my first job out of college. reply SilasX 14 hours agoparentHow did it help? reply 2OEH8eoCRo0 13 hours agorootparentResume padding and conversation starter during interviews. It also filled in some gaps in knowledge. reply greenavocado 9 hours agoprevIncredible achievement! Good job. reply cubefox 13 hours agoprevCool project. It reminds me of a theoretical issue. As the project page says, this system is clearly Turing equivalent. Since it runs software, it even implements a _universal_ Turing machine. But the design uses only (synchronic) sequential logic [1] and Wikipedia seems to suggest that automata theory considers sequential logic only equivalent to finite state machines. Not Turing machines. Isn't that clearly a major bug in automata theory? My guess is that automata theory consideres it critically important that a \"Turing machine\" has an infinite tape, while intuitively it instead seems relevant that it has something like a tape at all, some sort of random access memory, even if it is finite. I think such a memory system can't be implemented with classical finite state machines, at least not with comparable time complexity for read and write, but can be realized with sequential logic. [1] https://en.wikipedia.org/wiki/Sequential_logic reply tomstuart 12 hours agoparentReal-world computers are equivalent to linear bounded automata, not true Turing machines, because they have finite memory. This technicality is mostly ignored because a computer with a large finite memory is a decent enough approximation to a Turing machine for practical purposes. But, for example, the halting problem is decidable for linear bounded automata — because there are only finitely many states, every computation must either halt or eventually revisit an earlier state and get stuck in a loop — so in theory it’s an important distinction. reply DEADMINCE 1 hour agorootparent> because there are only finitely many states, every computation must either halt or eventually revisit an earlier state and get stuck in a loop Yet we know this doesn't happen in practice. reply fire_ball 15 hours agoprevthis is fantastic! great work... reply userbinator 9 hours agoprevNAND is popular probably because of nand2tetris, but it's worth mentioning that NOR is also a universal gate; and many early computers like the https://en.wikipedia.org/wiki/Apollo_Guidance_Computer#Logic... were entirely made of NOR gates. reply andai 8 hours agoparentI thought it's the other way around, nand2tetris used NAND because it was already popular? At least I remember hearing in university that NANDs are used for everything? Can't remember why they're used for everything though (and why not NOR, for example). reply ajross 8 hours agoparentprevThat's because in NMOS logic (maybe there's a symmetric reason in TTL, but I don't know for sure) you can implement a NOR with two parallel transistors between a pullup and ground, producing a zero output if either input is high. The symmetric NAND circuit requires two transistors in series, and therefore switches more slowly. reply bramhaag 15 hours agoprevTuring Complete[0] is a fun game similar to this where you create your own computer from NAND gates, including your own assembly language. [0] https://store.steampowered.com/app/1444480/Turing_Complete/ reply elevatedastalt 12 hours agoprevWow, seriously impressive. And the fact that this is the work of basically a high-schooler. I fear for the kind of competition my kids will have just to make it to college. reply naikrovek 12 hours agoparentThis is a natural extension/expansion of the “NAND to Tetris” course on coursera, and is free if you don’t want to be graded. The course walks you through it all, and there is an accompanying book that you do not need to buy to finish the course. Anyone who wants to do this and can focus on it for enough time can complete it and extend it into whatever shape they like, like this person. It really is a good course. reply ArchAndStarch 12 hours agorootparentI primarily used the physical book to learn about the nand2tetris platform. I highly recommend it, it's an enthralling read reply brailsafe 12 hours agorootparentprevAbsolutely true, I'm working my way through it now; it's challenging and time consuming, totally worthwhile imo. reply pyuser583 8 hours agoprevGround up projects like this are fascinating! It’s also neat how “ground” has been deepening. It used to mean building mainframe from source. Then building a compiler. Now building from logic gates. How much deeper can you get? Building a mainframe out of Gödel numbers? reply ArchAndStarch 8 hours agoparentOne curious idea my friends have entertained is to go one level even deeper and emulate the very transistors that make up the NAND gates on the web, too. It would certainly spell disaster for performance, but it's without-a-doubt interesting. reply vitiral 7 hours agorootparentLike... the physics? If not, I think a NAND gate is made of just two transistors, so if you mean emulating how transistors should behave then I don't think it will affect performance more than ~50% reply pyuser583 8 hours agorootparentprevThat would be fascinating! Do you know any resources that document the transistor to logic gate translation? reply schoen 4 hours agorootparentIn https://nandgame.com/ (mentioned elsewhere, a game version of NAND to Tetris) you start by making a NAND gate out of relays. The relays are electromechanical components, but you can choose to think of a transistor (within certain \"regimes\") as being directly electrically equivalent to one. (This simplification isn't appropriate for all engineering tradeoff purposes, although I don't know the details of how it fails or how we can benefit from knowing more about transistors' actual behavior.) The electromechanical relay is a very simple device to understand, if you're willing to just believe that electromagnets produce magnetism (without understanding why the universe works according to Gauss's laws on the relationship between electric current and magnetism). It's a coil of wire where an electric current produces magnetism that physically pulls a switch open or closed. reply cyanfrog 15 hours agoprevhttps://www.nand2tetris.org/ reply cweagans 15 hours agoparentSee also https://nandgame.com reply m3kw9 15 hours agoprev [3 more] [flagged] emoII 15 hours agoparent [–] I think that’s a massive achievement reply m3kw9 6 hours agorootparent [–] I think so too reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The creator introduces NAND, a personal project offering a web-based adaptation of the Nand to Tetris course, featuring a custom runtime, user interface (UI), and Integrated Development Environment (IDE).",
      "Users can experiment with programs such as 2048 and a genetic algorithm on NAND's simulated hardware.",
      "More details are available on the project's website and GitHub repository."
    ],
    "commentSummary": [
      "Users are discussing creating a programmable computer named NAND using NAND gates, influenced by the Nand to Tetris course.",
      "The conversation includes building a physical computer solely with NAND gates, designing a microcoded RISC processor, creating clocks from NAND gates, finite memory importance in computers, and NOR gates in logic circuits.",
      "Participants share their learning experiences from nand2tetris.org and nandgame.com, expressing interest in constructing computers from logic gates."
    ],
    "points": 318,
    "commentCount": 51,
    "retryCount": 0,
    "time": 1714061285
  },
  {
    "id": 40161697,
    "title": "Building a Wireless Internet Service Provider (WISP) from Scratch",
    "originLink": "https://startyourownisp.com/",
    "originBody": "Start Your Own ISP # This site is dedicated to helping you start your own Internet Service Provider. Specifically this guide is about building a Wireless ISP (WISP). This guide is focused on the very earliest stages of starting a WISP - determining feasibility up through connecting the first few customers. There are many challenges that will come up at 100, 1,000 or 10,000 customers that are not (yet) covered in this guide. Recent Webinar: Vertical Real Estate For ISPs Check out a recording of our latest webinar collaboration with Outpost Plus on 30 Aug 2023 at 12:00PM Pacific. Learn about strategies and solutions for aproaching MDU owners and managers, installing and building out MDU sites, and converting residents into subscribers. Recorded Webinar What’s New# Check out our past webinars here. Get a head start on marketing in Q1 with our 60% off sale on Q1 sponsorship packages right now. Community# If you’d like personalized assistance with a project feel free to book some time with me. Join the discussion! Chat with me (the author) and others interested in this kind of thing here: SYOISP Discord Server. Follow Along on Twitter @syoisp Getting Started# What is a WISP? And why might you want to build one? Also defines some terminology. Costs What does it cost to build a wireless Internet Service Provider? (Link to a Google Sheet that you can copy and customize.) About Me Who am I? Why am I doing this? Step by Step Guide# Step 1: Evaluate an Area: Make sure your area is a good candidate for a Wireless Internet network. Step 2: Find a Fiber Provider: Find a building where you can purchase a fiber connection and use the rooftop to start your wireless network. Step 3: Find Relay Sites: Extend your network wirelessly toward your customers. Step 4: Pick a Hardware Platform: Evaluate available options for wireless hardware. Step 5: Billing and Customer Management: Make sure you’re able to get paid and support your customers. Step 6: Network Topology: Design your network topology to make your network reliable and scalable. Routers, switches, IP addresses, VLANs, etc. Step 7: Build your Infrastructure: Install hardware for your fiber connection and your relay sites. Step 8: Install a Customer: Get your first customer online! Step 9: Marketing: Let people know about your service so they can experience a better Internet connection! Step 10: Maintenance: Keep your network running smoothly, even in bad weather. Signup for our monthly newsletter# Miscellaneous# Form 477: How to prepare and file with the FCC Form 477 is used by the FCC to determine which providers are servicing which areas. ISPs must file this form twice a year. Tools you’ll want to have A list of the tools you’ll need to install relays sites and customers. Aim a Backhaul COMING SOON A guide describing the proper techniques for aiming backhauls. Designed to be printed out and taken to the site for reference. Backhaul List If you just need to get a solid wireless connection from Point A to Point B then use this list to pick the right equipment and get it set up. RF Basics and Channel Planning COMING SOON Avoid self interference by carefully choosing channels for your access points and backhauls. MDUs (Multiple Dwelling Units) Best practices for providing service to apartment buildings, condos, attached townhomes, etc. Guide to Google Earth Some tips and tricks for using Google Earth to plan and build your network. Roof and Ladder Safety COMING SOON Stay safe out there! How can we help? Outpost Plus is the organization behind startyourownisp.com. Our experienced professionals are happy to discuss your needs and offer advice and assistance. Contact us to discuss your project. Looking to donate? Patreon or BTC 1JKa1Kdrp3r4xPSXBRJ6nPC6YYdLcqQ4Bp. Many thanks! © 2024 Graham Castleton, Outpost Plus, LLC – Documentation built with Hugo using the Material theme. Next Introduction",
    "commentLink": "https://news.ycombinator.com/item?id=40161697",
    "commentBody": "Start Your Own ISP (startyourownisp.com)322 points by agomez314 14 hours agohidepastfavorite86 comments throw0101c 13 hours agoAnyone interested in actually doing this may wish to look up Jared Mauch: * \"Man who built ISP instead of paying Comcast $50K expands to hundreds of homes\": https://news.ycombinator.com/item?id=32411493 * \"NLNOG: Getting Fiber To My Town [video]\": https://news.ycombinator.com/item?id=24424910 * \"Jared Mauch didn’t have good broadband–so he built his own fiber ISP\": https://news.ycombinator.com/item?id=25753360 * \"How To Create Your Own ISP with Jared Mauch\": https://www.youtube.com/watch?v=kJH9Emr99KI reply rrix2 9 hours agoparentThat NLNOG talk is a great talk, thanks for resurfacing it! reply protocolture 9 hours agoprevA good wisp can be competitive vs starlink. The issue is that wisp owners are cowboys, who dont ever follow through with good practices. Their networks, often due to guides like this website, end up riddled with technical debt (Technical debt is usually but not always branded Mikrotik). Starlink doesnt offer layer 2 services. Starlink doesnt offer half rate backup services. Starlink doesnt offer installation. Starlink can often be very congested. A wisp operator can: 1. Pull fibre into an area and then distribute it via 60GHz. 2. Pull fibre a bit further away and use decent APs with good MIMO. 3. Use profits from the above to pull fibre closer and ultimately overbuild themselves in areas with enough density 4. Service extrmely deep rural customers who dont have other options. 5. Service MDU's with a reverse model of 60GHz to the building, then fibre to the appt. What actually happens in practice is that anything more complex than bouncing 5ghz off of a tower is too hard, hiring someone intelligent to do it for them is too expensive and too hard and so small wisps just sell to bigger wisps who sell to fibre carriers or go bust. reply _huayra_ 8 hours agoparent> Technical debt is usually but not always branded Mikrotik Why this association? At least on the consumer side, I've really enjoyed using Mikrotik's stuff for my homelab. Is it just a sign of \"someone not wanting to pony up for Real Networking Gear\" or something similar? reply AnarchismIsCool 5 hours agorootparentI love Mikrotik but it kinda checks out. They're a high grade consumer/light commercial vendor more than utility scale commercial grade. I'd use them if I was trying to setup internet for my street or a small-medium sized office but not for my whole neighborhood. Their remote management utilities and software updates are good but not enough to keep me from having to ever get physical access to everything. reply donw 4 hours agorootparentWho would you go with? reply wil421 22 minutes agorootparentUbiquiti can provide everything on his list for wireless. They also sell OLTs and CPE devices if you want to build a smaller fiber network. If you want real telco stuff look at Edge core or Nokia. reply RajT88 2 hours agoparentprevI worked at such a place from 2003 - 2005. Mostly right, but the boss hired pretty smart people fresh out of college (cheap). They are still around and their website still sucks. Last I heard they were doing fine. reply somat 47 minutes agorootparentA website that sort of sucks is one of my business legitimacy tests. But it has to be the correct sort of suck, it's got to suck in the \"I'm an old sys-admin and can't be arsed to care about these javascript frameworks\" sort of way and not in the \"I am in sales and need 150 tracking agents\" manner. reply nickthegreek 14 hours agoprevPreviously: https://news.ycombinator.com/item?id=27539165 (June 17, 2021 — 607 points, 153 comments) https://news.ycombinator.com/item?id=20726906 (August 17, 2019 — 635 points, 95 comments) https://news.ycombinator.com/item?id=16160394 (January 16, 2018 — 938 points, 193 comments) reply theideaofcoffee 13 hours agoprevI'm somewhat skeptical about the advice given out on this site, while it looks ok on first glance, can I really trust their \"professional advice\" if their corporate parent site [0] is serving up, as of this writing, an expired SSL cert? What other things might they be missing? [0] https://outpost.plus reply protocolture 8 hours agoparentI had a client who had quite a decent network, but was always having issues. Upon investigating and subsequently asking him how he designed his last mile, he pointed us at the facebook wisp operators group. He was servicing high value customers with large dedicated speeds, based on a network design that kind of works for servicing trailer parks in rural USA. Mostly because the loudest voices in the space come from the jankiest moron wisp operators. The issue is that a lot of the advice is bad, the good advice costs money and is indistinguishable for the lay person from the bad advice, and its very easy to access terrible advice. This website isn't terrible, but it would serve to pay for the best advice possible before spending even more money on the wrong hardware in the wrong area. reply ikiris 5 hours agorootparentThis is the most accurate description of wisp information handoff I've ever seen. reply ShakataGaNai 10 hours agoparentprevCan you trust the advice of any random website? Can you trust the advice of someone here on Hacker News? Can I trust your commentary about their advice? The the answer largely depends on what you're trusting the advice for. Is it medical advice? Trust none of it. Is it the advice going to cost you a lot of money? I'd ask an expert. The site here is purely informational-use-at-your-own-risk. No different than anything else on the internet. It'll get you started on your research so you can have some basis upon which to ask/talk to each other. Hey, even links to a community discord where you can talk to like minded individuals and maybe get some more clarity. reply OJFord 10 hours agorootparentEh in general sure. But if you're giving networking advice you definitely want to either opinionatedly not use TLS ('use case doesn't warrant it, so am against HTTPS all the things'), or have a valid cert. reply DanAtC 7 hours agorootparentReal network engineers don't concern themselves with anything above layer 3. reply bhhaskin 11 hours agoprevLots of people are mentioning starlink. But there is a hard cap to how many subscribers can be in a given zone. Which means that although it is cheap right now, there is a hard cap for supply. That's the main reason the marine and RV packages/plans cost more. reply Gigachad 4 hours agoparentIsn't the point of starlink to service extremely remote areas? If there are too many subscribers for the system to work, it sounds like it should be dense enough to have fiber run to houses. reply woleium 10 hours agoparentprevthat cap is based on the number of satellites, which is not fixed. reply giantg2 9 hours agorootparentIt's fixed for a point in time. Launches of new satellites have considerable lead time. reply root_axis 9 hours agorootparentAnd they only last for five years. reply protocolture 8 hours agorootparentprevEh theres downlink capacity and then IP transit capacity from land sites too. reply toast0 5 hours agorootparentCapacity at the teleports is probably pretty easy to upgrade. If possible they're near internet exchanges, and if not, they've most likely got good fiber connectivity and can update the optics on both sides to get more capacity easily. Maybe they've got some sites in the middle of nowhere, but I'd bet they worked with fiber vendors to pick places that were easy to hook up to existing fiber, because why not. I think I recall seeing someone had done some sleuthing and found that the teleports were sometimes being placed along rail right-of-ways, where there's often also fiber and maybe even power. reply protocolture 4 hours agorootparentCapacity is still a cost factor. But they do choose places on major fibre paths. They have a sweetheart deal with vocus in oz, and as far as I can tell, they do their best to colocate. reply dboreham 12 hours agoprevFun and all, but hard to make money with Starlink and 5G providers are ready to eat your lunch. reply Alupis 11 hours agoparentI have yet to see a 5G home internet solution that's actually useful. It seems to be a \"budget\" internet option more than a viable alternative to most other solutions. Latency and speed are slow, and some of the providers mess with/block certain traffic (IKEv2 etc). reply kiwijamo 10 hours agorootparentHere in New Zealand 4G has proved to work surprisely well for FWA (Fixed Wireless Access). I think the important thing is that the providers need to be very strict at limiting the number of customers they sell to. Here, the providers will proactively stop selling new FWA connection in a certain area the tower sector serving that area is getting close to capacity limits. They'll also check where the customer is located and ensure they can actually provide a decent service. This check is done automatically off coverage and capacity data. Most of the major FWA providers are mobile phone carriers, so both mobile and FWA customers actually use the same tower/spectrum/etc so the general increase in demand (especially from mobile given they can't stop selling new mobile servcies) sometimes results in a good service in Year 1 degrading to poorer service in subsquent years. However the carriers can easily resolve that by adding more 4G carriers, deploying 5G, and even building new cell sites (which kills two birds with one stone -- better coverage for mobile users and more capacity for FWA users). When FWA first came out I confess that I thought it was a silly idea until my eldery mother accepted an incentive from her provider (cheaper monthly fee) if she moved onto FWA (from ADSL previously). She's zero complaints. And sure enough it works well for a low-end user -- emails, Facebook, WhatsApp, Netflix, YouTube, etc all work just as good on FWA as it does on fibre/DSL/Cable/etc. She happens to be close enough to the tower and that tower has also been upgraded and has heaps of 4G carriers so the service is consisently good. YMMV but when done well, 4G/5G FWA is a great option for low-end users. reply OJFord 9 hours agorootparentprevI'm writing from 4G, the same that I use to work remotely as a software engineer. It's not even a dedicated 'home internet' solution, just an unlimited data plan that I use with a decent modem/antenna. (Yes obviously latency is worse than fibre, I would probably hate it if I was still into FPS gaming etc., but in practice it's fine.) (When I set it up it was better and cheaper than the copper options, fibre not available. Fibre or Starlink would now probably be better, but still each much more expensive, even ignoring one-time costs. I don't need it/not worth it. Idk about latency but I could certainly get more bandwidth out of one-time costs on LTE too.) reply posguy 9 hours agorootparentCGNAT is killer though, the random connection drops when you don't get a static IP from your cellular provider cause random connection drops whenever their CGNAT gateway burps or misbehaves. Most people on cellphones don't notice, but it becomes oh so noticeable when your interacting with it every day. reply toast0 5 hours agorootparentprevI tried out (US) T-Mobile's small business internet, and while it wasn't useful for me, if I was a little more normal or had worked a little harder with sales to get what I needed, it would be fine. The v6 connectivity was pretty good, v4 is CGNAT, unless you sign up for a static IP, which needs an EIN business account; the sales rep can sign you up for a static IP with a SSN business account, but it will be deleted and they won't add it back. Also, I forgot which equipment they gave me, but it had terrible buffer bloat, and did some nasty nasty arp spoofing when I placed it on my network and made everything bad, and it wouldn't let me put it on XX.2.0/24 and have a static route for XX.0.0/24 so maybe I could keep it roughly contained. Maaaaybe I could have done something with VLANs, but I was done at that point. I'm still grumpy because it took 3 months and contacting the CEO email address to get the bill settled, but they did settle it. Not exactly a risk free trial in my book. Speed and latency was good though, as long as you didn't hit bufferbloat. I don't remember exactly, but 500M+ down, 200M+ up,reply grogenaut 4 hours agorootparentsips another glass at my winery, welp at least I've got wine reply spxneo 14 hours agoprevtoo bad you can't do this in Canada reply getToTheChopin 5 hours agoparentThere are plenty of small ISPs operating in Canada. Many have been acquired by Bell / Telus / Rogers in recent years, but there are still dozens of players offering fixed wireless or fibre connections -- mostly in rural areas. https://en.wikipedia.org/wiki/List_of_internet_service_provi... https://www.infrastructureontario.ca/en/what-we-do/projectss... reply Szpadel 14 hours agoparentprevwhy? reply Nextgrid 13 hours agorootparentRegulatory capture and the regulator being in bed with the incumbents. Not Canada-specific even, this is a common problem in a lot of countries when it comes to telecoms. reply betaby 13 hours agorootparentprevOverregulation. Source, I work in telecom in Canada. reply protocolture 8 hours agorootparentReally? I found Canada one of the least regulated. Although there were a few weird hoops. Out of ~10 countries I have supported WISP's in, I have never seen a country with less of a barrier to entry. We did have a theory that our customers were rural, and that the cities might present different issues. But trying to do what they did in Australia would be add 2 zeroes to the end of every cost. reply loceng 13 hours agorootparentprevAnd now all telecom companies are required to interconnect, right? Or did that not happen? The government mandated that after the \"Rogers outage\" - so that other providers can act as \"backups\", so \"that never happens again.\" reply betaby 13 hours agorootparent> And now all telecom companies are required to interconnect, right? No. That's not the case at all. > so that other providers can act as \"backups\", so \"that never happens again.\" Also, no. Also what do you mean by interconnect? BGP IP peering is a thing for a while. Big folks peer with big folks. reply loceng 11 hours agorootparent\"Ottawa announces it will require telecoms to provide backup for each other during outages following Rogers system failure\" - https://www.thestar.com/business/ottawa-announces-it-will-re... Are you aware of this? reply kazinator 11 hours agorootparentWhen there is only \"telecom\" and not \"telecoms\", Ottawa will have to rephrase that. reply loceng 9 hours agorootparentCanada will be abbreviated to CCP at that point. reply HeatrayEnjoyer 3 hours agorootparentEven mainland china's ISPs are not entirely monolithic. reply ThePowerOfFuet 13 hours agorootparentprevAsk TekSavvy that question. reply betaby 12 hours agorootparentOr Ebox. In fact some of hurdles were documented on dslreports forum. Starting with wholesale access to cable ( worked OK for a while ) continuing with wholesale access to fiber ( never happens ) and ending with the end of the 'sane' rates for cable ( final nail before Ebox, Destributel and other were sold to Bell/Telus/Cogeco ) reply throw0101c 10 hours agorootparent> continuing with wholesale access to fiber ( never happens ) * https://mobilesyrup.com/2023/11/06/bell-and-telus-must-offer... Or you can do what Beanfield did and build out your own fibre infrastructure to customers. reply posguy 9 hours agorootparentFighting every municipality and Homeowners Association (HOA, aka Strata) along the way. Running fiber is tough here in the US, even motivated incumbents see a minimum of a 6 month delay waiting for permits in many municipalities to run fiber through existing conduit owned by said incumbent. reply throw0101c 10 hours agorootparentprevOr Beanfield. reply anthk 13 hours agoparentprevNether in Spain... reply dotBen 12 hours agorootparentNeither in San Francisco. In 2005 Google wanted to do a nice gesture and offer free wifi across SF. The Board of Supervisors literally asked Google how much they were intending to pay the city in order to offer free wifi... SMH. It of course never happened. reply cdea 7 hours agorootparentThere are a few in SF. MonkeyBrains being a local-friendly top of mind name. Downside is the smallest vibrations, wind gusts, or rain often leads to service degradation. reply protocolture 9 hours agoparentprevUh there are tons of small wisps in canada what are you smoking? reply pushedx 7 hours agoprevUnfortunately the cost of IPv4 space at auction alone is prohibitively expensive. reply cdea 7 hours agoparentThere are Federal programs you can apply for that will subsidize this cost. Otherwise, you're looking at roughly a 2~ year wait after being approved for the ARIN waitlist (max allocation size of /22 i think) reply renewiltord 13 hours agoprev [–] Man, this was exciting back in the day, but now big risk you'll get blown out by Starlink. Starlink can just put high-speed internet into everyone's backyards. reply bityard 13 hours agoparentAt twice the price of cable and fiber, at least in my backyard... reply lh7777 13 hours agorootparentIn my area, fiber and cable aren’t available. If you’re lucky, you can get 5 mbps DSL for $70+ per month. The only credible options are WISP, Starlink, and cellular. WISP is the same price as Starlink and is slower / less reliable. Cellular is cheaper but gets slow at peak hours. In short, the local WISP has been losing a lot of customers to Starlink and T-Mobile Home Internet. reply Alupis 13 hours agorootparentprevAnd vastly slower speeds... Starlink is a replacement for dial-up, satellite, and in some cases legacy DSL- pretty much nothing else. reply dboreham 12 hours agorootparentThat's simply not true in my direct experience. I built a WISP back in the day, and although I don't have customers now, I still use the backhaul network to feed my office (typing this over that network). I have Starlink as a backup. Starlink is about as fast as my fixed wireless terrestrial link. It's often faster for download, and about 2/3 as fast for upload. Way, way faster than dialup and DSL. reply Alupis 12 hours agorootparentWhat kind of equipment are you using? Have you realigned it recently? You can push gigabit[1] ptp over the air these days for pretty cheap. Starlink seems to top out around 200Mbps in the best case. [1] https://store.ui.com/us/en/collections/uisp-wireless-airfibe... reply voidmain0001 5 minutes agorootparent1Gbs? Why not go for 6Gbs? https://ca.store.ui.com/ca/en/pro/category/all-60ghz-wireles... zachmu 11 hours agorootparentprevI'm on the basic plan and usually see about 75mbps. It can drop to 20 or so during peak netflix hours. But that's still plenty fast enough for multiple HD video streams. What we had before was much slower and much, much less reliable. If you can get a fast wired connection, do it. Starlink is for people who can't, and it's far and away the best option for them. reply Alupis 11 hours agorootparentWhat kind of wired connection did you have previously? The target audience for Starlink in the continental US seems to be people who's other options are traditional satellite, dial-up, or sometimes DSL (typically implying you're in a more rural area, but not always). For people in those situations, Starlink can be a good alternative. However, if you have access to modern cable or FTTH... well, it's not a substitute. reply freedomben 12 hours agorootparentprevI think this is going to depend somewhat on how congested your area is, but Starlink is my home ISP and I sometimes get faster (download) speeds at home than I do at the office where we have fiber at 200 Mbps down. It has a little more variance but is consistently quite fast. If the backend is on GCP, it amazes me how fast it will go. reply stephen_g 9 hours agorootparentWhy does your office only have fibre at 200Mbps down? Last office I was involved in setting up, we didn’t need symmetrical gigabit but the cost was fairly inconsequential compared to lower speeds, so we just went with the gigabit… Really strange that service providers even bother running fibre at lower speeds for commercial accounts… reply butshouldyou 13 hours agorootparentprevRegional pricing means it's 500-560 USD up-front to buy the receiver. Then, 50-100 USD per month for the residential plan, which includes unlimited data. That's not too bad. However, there's no speed listed. Third party reviews state 100/10 Mbps (up/down), which is not too bad considering the same site states a UK average of 75/15. The vast majority of the UK has pretty slow speeds. That said, unlimited internet subscriptions over fiber can be had for as little as 20 USD per month, which is far cheaper. reply dlachausse 12 hours agorootparentSpectrum is charging me $85 a month (and constantly increasing) for internet (JUST internet) that clocks out at about 200 Mbps, so that really doesn't sound all that bad. reply quinncom 13 hours agoparentprevUntil Starlink can lower prices to < $10/month, community-level ISPs will still be relevant to many locations (LatAm, etc). Starlink uplinks might even be used by the ISPs. I lived in a village in Colombia, population about 2000, which had four competing wireless ISPs. The quality was extremely low, but so were the prices. reply TheRealPomax 12 hours agoparentprevHmm, not really. Starlink only offers 150mbit, at $150/mo, whereas the costs for starting your ISP as presented here is about $25k up front and $3k/mo to keep it running. That's ridiculous for one person, but entirely economically viable if you're literally starting your own local ISP to service a few hundred homes, driving down the one-time signup and monthly fees to something drastically lower than what Starlink charges, for speeds that are drastically higher than what Starlink offers. But as any business venture: if you don't know whether you can get the customers, you better have the cash lying around to pay for everything yourself =D reply komali2 8 hours agoparentprevDon't Starlink satellites only last 5 years? I'm not sure it's a sustainable solution compared to towers and lain cable. reply dotnet00 8 hours agorootparentConsidering that they're still iterating on and launching satellites, it doesn't really matter that they'll only last 5 years. It isn't like they're launching the constellation and then sitting on it waiting for the satellites to fall out of the sky. For now they're having to launch a Falcon 9 twice a week to build up the constellation, but with Starship intended to be able to launch the equivalent of several F9 launches in one go, maintaining the constellation will become much easier. reply KerrAvon 12 hours agoparentprev [–] Starlink can't currently compete with cable and fiber speeds, and it's a Musk business, so it'll probably collapse when his house of cards comes down. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The website focuses on assisting individuals in establishing their Wireless Internet Service Provider (WISP) and offers a detailed, step-by-step guide for setting up and managing a WISP.",
      "It provides resources on various aspects including costs, terminology, infrastructure, marketing, and maintenance along with tools and guides on topics like backhauls, RF basics, MDUs, and utilizing Google Earth for network planning.",
      "Users can also benefit from personalized help, engage in community discussions, attend webinars, and contribute donations to sustain the site."
    ],
    "commentSummary": [
      "The post explores the feasibility of establishing an independent internet service provider, citing Jared Mauch's accomplishments with a fiber ISP.",
      "It discusses the competition between small WISPs and major players like Starlink, using equipment from companies such as Mikrotik, Ubiquiti, Edgecore, and Nokia for network infrastructure.",
      "Users share experiences with different providers like Starlink, 5G, and the success of 4G for Fixed Wireless Access in New Zealand, highlighting challenges in various regions and concerns regarding Starlink's sustainability and cost-effectiveness versus traditional cable and fiber ISPs."
    ],
    "points": 316,
    "commentCount": 84,
    "retryCount": 0,
    "time": 1714072018
  },
  {
    "id": 40156534,
    "title": "Tribler: A Resilient Micro-Economy Revolutionizing Media",
    "originLink": "https://github.com/Tribler/tribler/wiki",
    "originBody": "Tribler / tribler Public Notifications Fork 441 Star 4.6k Code Issues 356 Pull requests 7 Discussions Actions Projects Wiki Security Insights Home Jump to bottom Johan Pouwelse edited this page · 148 revisions Tribler: an attack-resilient micro-economy for media Anonymous Tor-like downloads and fast search Earn seeding tokens Reward content creators Tribler is a Bittorrent-compatible alternative to Youtube. It is designed to protect your privacy, build a web-of-trust, be attack-resilient, and reward content creators directly. We are building a micro-economy without banks, without advertisers, and without any government. Together with Harvard University, the Tribler team deployed one of the first fully distributed ledgers in August 2007, see BBC News coverge and a New Scientist article. In coming years we will further expand our micro-economy based on bandwidth tokens. We aim to become the key place where audiences find their torrents, creative talents get discovered, and artists get financial rewards from their fans. Tribler is the place where 100 percent of the money goes to artists and the people that run the infrastructure. Our mission: re-inventing media and money. Over 2 million people have used Tribler over the years. The Tribler project was started in 2005 at Delft University of Technology and over 100+ developers contributed code to it. We are continuously improving it and further expanding the scientific developers team. Technical foundations of Tribler are the Bittorrent protocol, an overlay for P2P communication across NAT/firewalls, gradual building of trust in public keys with Bittorrent seeding, and our token economy with incentives for Tor-like relaying and hidden seeding. For 12 years we have been building a very robust self-organising Peer-to-Peer system. Today Tribler is robust: \"the only way to take Tribler down is to take The Internet down\" (but a single software bug could end everything). Current items under active development This wiki page contains our main technical documentation, highlights: Trustchain: our 10.000 transactions per second ledger Token economy and decentral market Topic and open Github issue Researcher On-device decentralised AI - phd level Petru Trustworthy data for generative AI - phd level Marcel 5G overlay network for decentralised On-Device Machine Learning Orestis Kanaris True decentralised on-device machine learning Quinten exploring LLM as a database Xueyuan Chen Offline digital Euro: a first design and implementation Leon passport-grade digital identity with DDoS protection using IP reputation Adrian Web3Recommend: Decentralised Web3 social recommendations with trust and relevance balance Rohan Madhwal Making Trustchain scale to enterprise level with a large stress testing experiment Bulat Nasrulin IPv8 resilient overlay: Sybil-resilience through latency-based shadow-banning Quinten Stokkink Universal wallet for identity, attestations, and money Rowdy Chotkan Gossiping torrent popularity to scale to millions of torrents Sandip, Alexander and Andrei Open projects for new TUDelft master thesis students: Tor-like streaming, self-sovereign identity and authentication on Android, relevance ranking of search results (+swarm popularity), perfect metadata through distributed crowdsourcing, self-reinforcing trust, and perfect network connectivity using NAT/Firewall traversal. Speculative projects with long-term focus: prediction market for climate change. A market designed against frontrunners and high-frequency trading abusers in general. Project: Waiting for new developers Prior Dev Deceptively simple trust model Alexander Stannat Beyond decentral exchanges: Single universal global market Joost V. EuroToken - an open alternative to Facebook Libra and JPMorgan coin Wessel Blokzijl ArtistCoin - Fairness for artists, audio streaming service without any intermediaries Tim W. Real-time updates to the trust model and visualisation of random walks Can Umut Youtube-like scale: Gigachannels with 1 billion magnet links Vadim A live token economy and distributed marketplace for bandwidth tokens Martijn de Vos Low-level debugging of Tor-like tunnels and performance in general Vadim Blockchain: detect freeriders, refuse service; anon compatible Ewout Bongers self-sovereign identity+trust: overview, biometric validation, boosted privacy, and voting pass 8 students Distributed Apps: autonomous code execution using IPv8 plugins Mitchell Olsthoorn P2P 4G - Universal connectivity using imperfect hardware Matt S. Financial Engineering: decentralised non-profit payment services Jetse Brouwer Walker infrastructure with 48 NAT boxes and automated NAT puncturing Remko Naber Autonomous self-replicating code buy servers with Bitcoins 4 students Prototype projects \"Blockchain Engineering\" Master course around threshold encryption, trustchain, self-sovereign ID, etc 45 master students Bottom-up consensus model with full scalability using checkpointing Kelong Cong PageRank-like trust model with Sybil-attack resilience Pim Otte Towards global consensus on trust within the Tribler micro-economy Jan-Gerrit Harms Decentral market primitives: market order and execution engine fairness Marc Juchli Decentral market: privacy for traders and spam-resilience Bas van Ijzendoorn Secure hardware storage of keys using PUF hardware Ade Ade Setyawan Sajim Blockchain: self-reinforcing trust with collection of credit records Pim Veldhuisen Fast anonymous streaming with Tor-like onion routing Quinten Stokkink Attack-resilient social media on mobile devices, using LibTribler Paul Brussee Blockchain walker with attack-resilience and integrated NAT puncturing, trusted peer discovery Changliang Blockchain: earn credits with seeding on Kodi-like devices Bohao Zhang crowdsourcing of rich metadata Stijn van Schooten Determine popularity+age of content with spam and attack resilience, swarm size community Chengxin Ma Adversarial search: blockchain-based spam resilience in Youtube-like systems Jelle Licht Scalability: donating TeraBytes to crowdsourcing projects Wouter Smit Connecting banks to decentral markets through PSD2 open APIs Kypianou Crowdsourcing and investments Bart Gout re-use our decentral market platform for real-world business case, crowdsourcing real-estate 4 bsc students Establish + Real-time display of Blockchain trust 20 Context project students Aim: solving trust Social media today is obsessed with profit, filled with advertisements, overflowing with falsehoods, and infested with fake news. We're trying to fix these hard problems in a unique way: by building trust. Our audacious ambition is a clean-slate re-creation of The Internet itself with foundations of trust. Craiglist and eBay showed us in 1995 that trustworthy trade was possible online. Uber, Etsy, and AirBnB show that entire industries can be disrupted by a single platform with a natural monopoly. For the past 18 years we have build and deployed platforms to create trust. Before Wikipedia and Youtube existed we studied the mechanisms behind trust and user-generated content on a small scale. Several years before Wikipedia emerged we deployed a music encyclopedia with unconstrained write access, it never became popular because we focused too much on software, instead community growth. Today we keep a narrow focus and continuously expand Tribler with trustworthy decentralized technology. We launched sub-second keyword search for Bittorrent swarms without any server back in 2010 (see our old Google Tech Talk on this topic). One of our operational trust browsing prototypes: Further reading: Our work from 2004, 2-year in-depth measurement and analysis of Bittorrent (.pdf 25 pages), largest measurement to date. Covers eight months of the BitTorrent/Suprnova.org file sharing ecosystem. In particular, we show measurement results of the popularity and the availability of BitTorrent, of its download performance, of the content lifetime, and of the structure of the community responsible for verifying uploaded content. Tribler features and innovations Tribler supports torrent search without websites, anonymous downloading, torrent streaming, channels of torrents, and sharing content for tokens. Overview of Tribler (.html 5 pages). All Tribler features are implemented in a completely distributed manner, not relying on any centralized component. Still, Tribler manages to remain fully backwards compatible with BitTorrent. The 2006 overview of Tribler (.pdf 6 pages) featuring taste groups, friends, friends-of-friends and faster downloads by donating bandwidth to friends (protocol spec of friend boosting). Note that the 2006-2009 Tribler protocol specification (.pdf 47 pages) is now mostly outdated, as we switched to our new synchronization protocol called Dispersy (see below). Trust in social media content is essential for a sustainable ecosystem. We introduced channels of Bittorrent swarms in 2009 with the Tribler 4.x release. Each user can vote on channels to increase their visibility and tell everybody the channel owner is not a spammer and not spreading fake items. The reputation of both the voters and channel owner are important. Tribler protects your privacy by not storing anything on any server. To protect your privacy even more, we have prototyped search algorithms based on homomorphic cryptography. We presented a new algorithm system for privacy-respecting scalable Gnutella-like search in 2014. Our approach to scalability is a similarity function in the encrypted domain (homomorphic), enabling semantic clustering with privacy. Back in 2006 we introduced long-lived identities to separate trustworthy peers from freeriders and spammers (PermID). To protect your privacy further we also devised an alternative to onion routing which potentially could have stronger security guarantees (correlation attack). See the details in this thesis on Multi-core architecture for anonymous Internet streaming which includes a performance analysis of running code. Further reading for developers: Running Tribler from sources in Eclipse Jenkins server for continuous integration, unit tests, installer builders and performance testing. You will find a lot of automatic running scripts there for things like correctness, NAT puncture performance and GUI tests. Tribler development pointers / starting point for new developers Python source code doc directory Tribler can run as a background process with this API Our primitive 2007 distributed ledger and Trustchain (2012) We deployed one of the worlds first fully distributed ledgers in August of 2007. For over a decade we meticulously measured, analysed, improved, and enhanced this live system. Today it defines the state-of-the-art in blockchain research, but in the early days it barely functioned at all. A total of five Ph.D. students of Delft contributed key parts and upgrades. At launch we called our initiative \"bandwidth-as-a-currency\". Today we have specific terminology for what we did: a token economy. We are making Internet bandwidth a tradable commodity without any middleman or need for any centralised governance. Our efforts span over a decade, making us the veterans in the field. Our ledger provides an incentive for Bittorrent seeding and Tor-like relaying. For numerous years the tit-for-tat algorithm provided the only incentive for contributions in Bittorrent. No incentive for seeding existed, except when central servers kept track of your uploads and downloads. We measured closed invite-only communities for numerous years and mathematically showed their rich-get-richer properties. For details see Fast download but eternal seeding: the reward and punishment of sharing ratio enforcement and our measurement paper understanding bandwidth economics and ratio enforcement (.pdf 5 pages). We measured 508,269 peers in 444 swarms within five BitTorrent communities, ranging from public to highly elite. We observe download performance, connectability, seeder/leecher ratios, seeding duration, and statistics regarding the resource supply. We got inspiration for a novel blockchain design based on operating our own ledger and studying token economies. Our current work is called Trustchain, a unique design from 2012 where all participants have their own personal blockchain and create their own genesis block. Our older work used a graph-based approach and graph-based reputation algorithms. Trustchain records transactions in a tamper-proof and scalable manner. It does not require mining and does not try to solve the double spending problem. Our primitive 2007 ledger pre-dates Bitcoin, additionally our 2012 DAG-based approach pre-dates IOTA and the Texas DAG patents. We are fans of Bitcoin, but also showed in an early analysis the flaws in this concept. Our approach to digital signatures is the essential difference which sets us apart from others. Mono-signatures form the foundation of all other projects we have seen in the past decade. Meaning, in systems such as Bitcoin a transaction is already valid with a single signature. Our Trustchain design does not permit transactions with merely a single signature. Trustchain only supports multi-party agreement recording, others are not valid. We believe that we created a more powerful system by removing single-signature transactions. Only time can tell the usefullness of this academically-pure and minimal design. The foundation of our approach is making repeated successful interactions between actors explicit and durable. Cryptographically signed records of successful encounters serve as proof-of-work certificates. The validity and value of these certificates is determined by a trust and reputation system. Relaying for anonymity and seeding in Tribler constitutes work which is rewarded with a signed certificate. Helping others and uploading in Bittorrent swarms is rewarded with bandwidth tokens (e.g. signed certificates). Mining in our system becomes download parts of a swarm and uploading them to multiple interested parties. In 2013 we got the credit mining part of our system operational in early Beta. The screenshot below from November 2013 shows the boosting of various swarms. Note the investment yields of \"struck gold\" and \"poor\" in the right column. Further reading: 2024 decentralised AI demo, critically relies on a trust framework Trustchain IETF Draft Internet Standards proposal Scientific 2010 publication on BarterCast ledger (.pdf 8 pages). Our original BarterCast protocol publication 2009. The DropEdge enhancement was proposed to BarterCast together with Harvard and Berkeley scientists (.pdf 42 pages) which makes the ledger harder to attack. Reducing the storage cost of our ledger and reputation system form 2012. Key Internet deployment evaluation: A Network Science Perspective of a Distributed Reputation Mechanism (.pdf 9 pages), from 2013. Our methodology: keep focus and dream big For our narrow focus of a Bittorrent client we are exploring the fundamentals of identity, trust, and trade. With over 1 billion users of Youtube and Bittorrent we know there is a mass audience ready for something better. Our approach has very boring foundations, when compared to newer and more sexy work, like IPFS, FileCoin, or Storj. We first measured Bittorrent in 2002, it is a flourishing mature ecosystem and ready for an upgrade. Bootstrapping an ecosystem is hard, we designed and deployed a superior alternative to Bittorrent. It became an official IETF Internet Standard, but completely flopped. This formed our preference for simplicity, elegance and our allergy for bloatware, clean-slate work, and over-engineering. Numerous other projects try to create a generic approach using an ICO for funding and promising the early adopters a dazzling return-on-investment. Tribler is different. rant warning. We are non-profit academics. We do not want to replace the old elite with a new crypto-currency elite. What is changed if we replace backroom deals, lobbyists, middleman, and legal monopolies with the tools of the new elite: algorithms, early investor rewards, proof-of-dominating-stake, and smart contracts? Replacing the analog world and breading digital-native inequality does not make the world a better place. We are creating a micro-economy based on fairness, trust, equality, and self-governance. By design we banish rent-seeking. Critical infrastructure rarely makes profit. We are trying to build critical infrastructure. Tor-inspired onion routing As of December 2014 Tribler has a build-in version of a Tor-like anonymity system. This is completely disconnected from 'The' Tor network. It is still ongoing work. It gives you probably superior protection than a VPN, but no protection against resourceful spying agencies. We have implemented the main parts of the Tor wire protocol within Tribler. Instead of the TCP protocol that 'the' Tor network uses, we use UDP. The enables us to do NAT puncturing and traversal. We have created our own network using this Tor variant, our code is not compatible with normal Tor. Work started as a small trial in December 2013 with anonymous Bittorrent downloading. Essential part of our work is that everybody who downloads anonymously also becomes a relay. This brings the Bittorrent tit-for-tat idea to darknets. With this ongoing work we aim to offer in 2018 with Tribler V7.0 proxied downloading for any Bittorrent swarm. Lengthy documentation in the form of two master thesis documents is available. First is a general documentation of the tunnel and relay mechanism, Anonymous HD video streaming, .pdf 68 pages. Second is focused on encryption part, called Anonymous Internet: Anonymizing peer-to-peer traffic using applied cryptography, .pdf 85 pages. In addition, there are the specifications for the protocols for anonymous downloading and hidden seeding on this wiki. overlay protocol for synchronization The current foundation of Tribler is the Dispersy overlay. Dispersy functionality includes: making connections, sending messages, puncturing NAT boxes, and distributed database synchronization. Every 5 seconds Dispersy sends out a message to establish a new connection or re-connect to a known peer. Note that we are transitioning to a new overlay for the durations of 2018. Overlay communication, peer discovery and content discovery (keyword search) are essential building blocks of a peer-to-peer system. Tribler preserves the content and peers it discovered in the past. Every Tribler client runs a full SQL database engine. Several times per second each Tribler peer sends and receives updates for this database. Our protocol for distributed database synchronization is called Dispersy. See a simple messaging client written with just a few lines of code as a simple tutorial example; outdated broken tutorial. The detailed wire protocol specification: introduction-request-1 Dispersy is a fully decentralized system for synchronization (.pdf), capable of running in challenged network environments. Key features of Dispersy are stateless synchronization using Bloomfilters, decentralized NAT traversal, and data bundle selection algorithms that allow the system to scale over 100,000 bundles in the presence of high churn and high-load scenario's. Dispersy uses a simple database schema, with the sync table containing the data bundles to synchronise across peers in the packet field. Android port of LibTribler Android porting teams are working on the downloading and Tor-like protocol part of Tribler and the overlay, channels and search portions. As of June 2014 there is initial running code. The focus is on stability and creating a mature build environment using Jenkins. See below two actual screenshot of current running code. Download the alpha .APK here: https://jenkins.tribler.org/job/Build-Tribler_Android-Python/lastBuild/ Stealth app for Android The following work is ongoing. We have an operational Android app that can spread itself via NFC. The app can spread viral via friends, even if it is blocked from a central app store. Original student assignment: The aim is to create an Open Source Android smartphone app to help bypass restrictions by non-democratic governments. The Arab Spring showed the importance of video recording of mass protests. However, possession of a video recording on your phone of human rights violations and mass uprisings brings grave danger. The idea is to make this app “check-point-proof”, meaning that a somewhat knowledgeable person will not detect the presence of the app and will not discover any video content. The app itself should be hidden, you can make a “stealth” app by somehow removing the app icon from your app list (sadly it simply still shows up in the uninstall app list). The app is activated simply by “dialing” a secret telephone number or other method your deem secure. Starting point for your work can be found here: http://stackoverflow.com/questions/5921071/how-to-create-a-stealth-like-android-app. Your Stealth app need to be able to virally spread and be able to bypass an government restrictions on the official app store. Include the feature for NFC and direct-wifi transfer of the .apk with an easy on-screen manual and steps. Thus users can pass your app along to their friends. NAT Traversal: 80% success rate Peer-to-Peer (P2P) networks work on the presumption that all nodes in the network are connectable. However, NAT boxes and firewalls prevent connections to many nodes on the Internet. We created a method to puncture NATs which does not require a server. Our method is therefore a simple no-server-needed alternative to the complex STUN, TURN and ICE approaches. We conducted one of the largest measurements of NAT/Firewall behavior and puncture efficiency in the wild. Our method is a UDP hole-punching technique. We measured the success rate using volunteers running Tribler. Number of users in our trials are 907 and 1531 people. Our results show that UDP hole punching is an effective method to increase the connectability of peers on the Internet: approximately 64% of all peers are behind a NAT box or firewall. More than 80% of hole punching attempts between these peers succeed. Brief description of our UDP puncture method in IETF draft Lengthy thesis work on UDP puncturing from 2005 Roadmap 2030: a proven alternative model for capitalism As Tribler scientists and engineer we are actively trying to make a better world. Our micro-economy is our living lab for experimenting with alternative models for capitalism. We aim to re-invent money by creating the first sustainable economy without any moral hazards from bankers, politicians, and megacorporation. Citizens and only the citizens are in control with self-governance. Our grand vision in a 1+ hour lecture given at Stanford University, via their Youtube channel. We want to do more then be a Youtube alternative. Our grand vision is liberating both media and money. See the talk Abstract and slides (.pdf 78 pages). Keywords: transform money, “Bank-of-Bits”, global financial meltdown isolation. Use cooperation&stability, not volatility&greed. Alter the essence of capitalism (rich get richer) by abolishing compound interest rate and facilitation of safe zero-cost money transfers & lending. We aim for a direct assault on the essence of capitalism, aiming even further then the Bitcoin accomplishment (bypassing the central bank). Further reading: Our writup on InternetSociety.org on liberating the media and Internet itself. The challenge is to design a micro-economy where the attacker might even control the underlying infrastructure. Our 2012 annoucement of our new focus on attack-resilience was covered by numerous news organisations. Fox News and Russian Today called us the the new weapon in the battle for Internet liberty. Tribler history 2019: Release of Gigachannels and Python3 compatibility 2018: Release of Tribler 7 2017: Release of IPv8 digital identity framework, successor of Dispersy 2017: First live tests with decentral marketplace 2016: New blockchain deployment testing 2014: Test network goes live for anonymous Tor-like downloading (not connected in any with with 'the' Tor project) 2013: Anonymous Tor-like download trial 2012: Tribler Mobile live streaming from a phone camera 2011: Libswift accepted as an upcoming IETF Internet Standard 2010: Release of Dispersy network overlay framework 2010: Splash framework for data synchronization tested 2010: Wikipedia.org uses our technology for live trial 2009: Large HD streaming trial with BBC 2008: Social network without servers and \"easy\" invites 2007: Our distributed ledger launched in the wild 2006: Tribler 1st release 2005: First Tribler code = social Bittorrent 2004: Slashdot for first time with largest Bittorrent study Pages 14 Home Tribler: an attack-resilient micro-economy for media Current items under active development Aim: solving trust Tribler features and innovations Our primitive 2007 distributed ledger and Trustchain (2012) Our methodology: keep focus and dream big Tor-inspired onion routing overlay protocol for synchronization Android port of LibTribler Stealth app for Android NAT Traversal: 80% success rate Roadmap 2030: a proven alternative model for capitalism Tribler history \"TrustChain\" architecture Anonymous Downloading and Streaming specifications Compiling Tribler from sources under Eclipse Getting back your Tribler state dir after update to 7.4.2 Hidden Services Specifications for anonymous seeding How to move Tribler state directory to another computer MasterThesis Paying out bandwidth tokens in TrustChain Related Research Groups Removal Guide Scientific publication venues for ledger science The design of a trustworthy ecosystem around media sharing Tribler Development Pointers Clone this wiki locally",
    "commentLink": "https://news.ycombinator.com/item?id=40156534",
    "commentBody": "Tribler: An attack-resilient micro-economy for media (github.com/tribler)264 points by thunderbong 21 hours agohidepastfavorite108 comments heycosmo 11 hours agoI think many proposed solutions to the creator compensation problem end up glossing over a fundamental difficulty: once an easily-distributed work (like anything digital) is in a consumable state (and thus copy-able), it becomes basically free. The idea that $10 for a digital copy of an album that is already on youtube (or a friend's harddrive) should be a viable business model is weird to me in this day and age. I have recently been wondering about a threshold-based \"media economy\" where creators don't actually show us anything (except for clips or samples or low-res versions, etc) until they are guaranteed a certain amount of income. It's basically kickstarter. A musician makes an album, goes on kickstarter and asks for $10,000 to release it. Once $10k is reached, the songs go up on a server, or are released on bandcamp, spotify, or any of the usual channels. Additional money beyond the threshold can be made, but it will be as difficult as it is now. But they have already reached $10k (set by them) so everyone can feel good that the musician has earned what they feel they deserve. I'm sure there are many problems with this. For one, many artists aren't creating just for money. They want to show us their creations, and with a threshold, they would have to hold back until it is reached (in the case of musicians, they might not even be able to play a new song at a show until the threshold is reached, b/c smartphones). There may be a critical mass problem, too. If two artists are similar and one releases immediately while the other waits for the threshold payment, the latter may drift into obscurity. There must be some allure to the withholding, though? What other problems kill this approach? Could it work for open source software, too? Make your thing, don't share it. Demo it, ask for the release payment, then put it on github. reply corimaith 2 hours agoparentThe problem with kickstarter is that alot of creators end up not fulfilling their pledges, even as they receive far more than they asked for. To be fair, that's more for risky ambitious projects like mmorpgs that even AAA devs fail at. reply tw04 11 hours agoparentprev>Could it work for open source software, too? Make your thing, don't share it. Demo it, ask for the release payment, then put it on github. I think it would be far more reasonable to put the source into escrow, to be released when a threshold is met. I've seen closed source vendors do that when they're smaller to ensure a large customer is not left high and dry should they go bankrupt or be acquired by someone who kills the product. I don't foresee anyone being willing to see a demo of a piece of software, then writing a check for it before using it. In the closed source world you pretty much ALWAYS have to do some sort of POV/POC before anyone will buy your stuff. reply Eisenstein 11 hours agoparentprevPlenty of creators make a decent living selling their content digitally. Once you democratize the tools and distribution, you remove the media companies that traditionally take the lions share of all the money. In the traditional setup a few business people and a few artists get rich and everyone else is broke. In an economy where the creator distributes directly via digital then a bunch of people get decent incomes. The second option is the better one, IMO. Once we do away with the notion that creating art could make you rich, then it become less necessary to make sure that we have some centralized way to collect money for art. reply synctext 1 hour agorootparentAgreed! Direct distribution will completely re-shape the content landscape, I believe. Probably starting with the most dysfunctional industry of \"producing\" music. The intermediaries are borderline parasitic there. We now have \"Decentralised AI\" working in the lab last month. So also the new music discovery, recommendation, fuzzy keyword search, spam filtering can be realised with full decentralisation (in principle). See live demo of our toy example [1]. Broad writeup [2] [1] https://huggingface.co/spaces/tribler/de-dsi [2] https://torrentfreak.com/researchers-showcase-decentralized-... reply synctext 21 hours agoprevTribler founding professor here, AMA. 2x on HN frontpage! Most attention we had during 18 years of coding. reply DRosario 18 hours agoparentHave you checked out Farcaster? https://docs.farcaster.xyz/ FC built a sufficiently decentralized platform, which seems to align with Tribler. They already have apps to compete with twitter/reddit (warpcast), tiktok (drakula), and others. A video service would be a great fit in the ecosystem. reply pyinstallwoes 18 hours agorootparentAny service built on cryptocurrency is a terrible idea for the future; there exists no such thing as scarcity in cyberspace. reply idiotsecant 15 hours agorootparentThat's a weird take. After all, 'cyberspace' is not some abstract realm divorced from the universe at large. It's still subject to scarcity of time, energy, and information. reply hkt 10 hours agorootparentTell that to John Perry Barlow: > Governments of the Industrial World, you weary giants of flesh and steel, I come from Cyberspace, the new home of Mind. https://www.eff.org/cyberspace-independence reply synctext 3 hours agorootparentIndeed! Trust is a precious good in society and economy. In Dec 1960 we \"decentralised communication\", now called The Internet. Bittorrent decentralised broadcasting, leading to streaming revolution. Bitcoin pioneered decentralised money. At Delft University I've worked for 25 years to decentralised trust, democracy, and economic cooperation in general. As an academic this focus on running code and societal change obviously kills you career protects. See my writing from prior century on \"Open Information Pools\" (pre-wikipedia era) [0]. Essentially what others called the Global Brain. We deployed a decentralised trust algorithm based on the interaction graph to 94k people, see [1]. Theoretical foundations are based on proving that the Harvard impossibility result against Sybil attacks made too strict assumptions, [2]. Leading to trust scores with resilience against fake identities: MeritRank [3]. This is being released in first version in this Tribler version. So hopefully this trust framework will help stop spammers a bit. We now pioneered \"Decentralised AI\", that critically relies on such a trust framework to function in a trustworthy manner [4]. [0] https://www.usenix.org/conference/2000-usenix-annual-technic... [1] https://research.tudelft.nl/files/89353583/1_s2.0_S138912862... [2] https://pure.tudelft.nl/ws/files/96914542/p1263.pdf [3] https://arxiv.org/pdf/2207.09950 [4] https://huggingface.co/spaces/tribler/de-dsi reply DRosario 15 hours agorootparentprevthanks for the gut reaction but FC has nothing to do with scarcity. It has to do with ownership, control over your digital footprint, and censorship resistance. reply pyinstallwoes 8 hours agorootparentAll those are forms of coercion and finite resources, which are artificial limitations that are naturally non-existent in cyberspace. What you are describing are vestiges of the physical world being forced upon the unphysical - it makes no sense. reply kouru225 17 hours agorootparentprevScarcity is just a natural consequence of trust reply Temporary_31337 14 hours agorootparentThere is some relationship but some scarcity is natural- where is trust in that? reply jolmg 16 hours agorootparentprevWhat do you mean? reply kouru225 7 hours agorootparentCrypto is a solution to establishing trust in a decentralized ledger system: proof of stake and proof of work prevent people from adding fake transactions to the blockchain. It’s just a natural consequence that this creates scarcity. reply HeatrayEnjoyer 3 hours agorootparentWhat practical advantages are there to a decentralized ledger? Does it offer the trust that centralized ledgers and legal avenues do? reply pyinstallwoes 5 hours agorootparentprevIt doesn’t establish trust. It’s the opposite of trust. It establishes paranoia, and even then, how can you be sure your screen is the truth? reply persnickety 3 hours agorootparentIt establishes both trust by separating it from mistrust (paranoia if you will) by making the division explicit and assigning trust to some people and operations and not others. If you think it's concerned about screens, you're wrong. It's concerned about \"did this person really do this\"? reply ajb 17 hours agoparentprevDo content creators currently make any income from your platform? If so, what are the statistics of this (best, typical, etc) reply __MatrixMan__ 20 hours agoparentprevI want to see your project succeed, and I'm probably not alone. What kind of help do you need? reply synctext 19 hours agorootparentWe love to have more help! Hardest part is debugging. So running Tribler and reporting bug in Github. We are in desperate need of Win/Mac/Linux users which will help us with reproduce bugs. One time we found bugs in Python Async IO standard lib [1]. The 'once in a week' bugs are difficult to capture. [1] https://github.com/Tribler/tribler/pull/7926 reply wongarsu 17 hours agorootparentFrom my outsider's view, the part where you need the most help is actually your website. From the https://tribler.org homepage it's very hard to figure out what tribler actually is, and the only screenshots are hidden away in the support and developer categories, and all feature vastly different menu items (without clear indication how to get those features, if they even still exist). The API documentation isn't linked anywhere. And while installing the client and downloading your first torrent is easy enough, there isn't a lot of info on how to do anything else. And the help that does exist is outdated or wrong. The https://www.tribler.org/howto.html seems to be for a completely different version than what I get when I download and install the Windows version, and 3 out of 4 steps don't work as described (The text in 2 is completely wrong/outdated, I don't even have the menu item for 3, nor the icon for 4) reply Animats 14 hours agorootparent> it's very hard to figure out what tribler actually is From the buzzwords, some kind of crypto scheme: \"Micro-economy\", \"self-sovereign\", \"reward content creators directly\", \"micro-economy without banks\", \"fully distributed ledgers\". It reads like crypto bolted onto torrents. reply __MatrixMan__ 13 hours agorootparent\"Trust Chain\" has this as one of its design decision: - No global consensus which I think sets it quite apart from anything on a blockchain. reply SkyMarshal 11 hours agorootparentI noticed that too: https://github.com/Tribler/tribler/wiki/%22TrustChain%22-arc... But not much else about it. Would be interested to read more. Using torrent seeding as a form of Proof-of-Work that rewards tokens is actually an interesting use case for cryptocurrency, and not as energy-hungry. But no global consensus is different from any crypto I've ever heard of. How does it keep a consistent ledger or who owns what tokens? Edit: full explanation here - https://github.com/Tribler/tribler/wiki/The-design-of-a-trus... reply __MatrixMan__ 9 hours agorootparentI've always had a sort of knee jerk reaction against distributed systems that enforce global consistency at the protocol level. Wherever there's a conch to have or not have, also there will be the haves and the have nots. Better, says my gut, to let either sides of a contradiction compete for legitimacy in the eyes of whatever local audiences are relevant. Assuming consensus from the get go just doesn't seem to square with how large groups of people actually work. So I hope that these alternatives work out for them because I'd like to have more examples to point at when I try to express this. reply SkyMarshal 8 hours agorootparentHaha, good Lord of the Flies reference. Yes let's decentralize all the conchs, make them all local. Though, due to positive feedback and winner-take-all effects in complex systems like human economies, I don't believe that's a stable equilibrium. The critical resources of systems will concentrate and consolidate over time. The question is whether there's any way to manage that in the architecture or protocol to minimize the resulting harm, or whether it's better not to try. reply nathan_compton 18 hours agorootparentprevThe reason I don't run TOR exit nodes is that I neither wish to support criminal activity (of at least certain types) nor do I want to get entangled with law enforcement for doing so. Since this is TOR-like, what are my legal liabilities if I am running this software? reply synctext 18 hours agorootparentTribler does not include a Tor exit node! reply wongarsu 17 hours agorootparentBut can you download data from regular bittorrent peers using Tribler? And if yes, how does my traffic reach them? Suppose I want to download a torrent that's only seeded by one person running Deluge. My understanding was that this would involve making a connection to another Tribler user, and that user making the connection to that seeder. That would make every client a sort of exit node for bittorrent traffic, even for torrents they don't download. Is that not how it works? reply thejohnconway 19 hours agoparentprevI’m curious about the rewards/tokens aspect. Is this done using cryptocurrency (you mentioned bitcoin, urg, upthread), or something else? reply hanniabu 18 hours agorootparentThey smartly aren't answering b/c of how hostile HN is to crypto. Look how receptive these comments have been so far, that's a clear sign of HN's bias. reply ddtaylor 18 hours agorootparentIt's a subject that I am interested in and involved in yet I will never discuss it on Hacker News again. The attempts I made to genuinely interact related to crypto have been terrible on this website. It's unfortunate because I feel there is a very good discussion there regardless on whether you are for or against it but we don't get to see that discussion. reply synctext 18 hours agorootparentThere is no crypto in Tribler. We have something much older then Bitcoin. It's a simple ledger who helped whom in the network. Simple case of earning points by helping others. Economically, its complex. Coin creation is decentralised, everybody prints their own 'money'. The value of that help-currency is based on how connected you are to the globally connected transaction graph. Then we maintain fairness in this micro-economy against freeriders. See study with 160 million trust records and 95k users [1]. [1] https://research.tudelft.nl/files/89353583/1_s2.0_S138912862... reply jerry1979 5 hours agorootparentInteresting, it looks like it relies on a fraud detection scheme. Have you considered pairing Chaumian ecash with your reputation system? reply kmacdough 17 hours agorootparentprevInteresting I'll have to study this. I'm curious how this fares in the face of well- resourced adversaries. Also funny that the guy complaining about crypto convos went straight into hate mode for \"not quite crypto\" without any research or support. Presumably y'all have thought through this. reply hanniabu 15 hours agorootparentIf it were able to withstand adversaries it would have been bitcoin before bitcoin reply hanniabu 17 hours agorootparentprev> We have something much older then Bitcoin Then it's flawed. There's a reason why all earlier implementations failed. They were incomplete. reply pessimizer 17 hours agorootparentYes, Tribler has completely failed as a cryptocurrency. reply 0x457 14 hours agorootparentTo be fair, Bitcoin also failed as a cryptocurrency. reply unclebucknasty 17 hours agorootparentprevI seem to remember a time when HN was much friendlier towards crypto. But, you have to admit the crypto community hasn't done itself any favors: Hype and endless promises that never came to fruition, astronomical transaction fees, frictionful technologies, exhausting volume of promising (\"world-changing!\") projects that under delivered (putting it mildly), rug pulls, thefts, and other outright scams, massive use cases for money laundering and criminal activity. Kind of hard to stay positive through all of that. reply spencerflem 17 hours agorootparentyeah- from the github repo linked \"Numerous other projects try to create a generic approach using an ICO for funding and promising the early adopters a dazzling return-on-investment. Tribler is different. rant warning. We are non-profit academics. We do not want to replace the old elite with a new crypto-currency elite. What is changed if we replace backroom deals, lobbyists, middleman, and legal monopolies with the tools of the new elite: algorithms, early investor rewards, proof-of-dominating-stake, and smart contracts? Replacing the analog world and breading digital-native inequality does not make the world a better place.\" totally agreed, that's my problem with all of the bitcoin hype - it's mostly there to make the early investments more profitable which doesn't excite me much reply hanniabu 16 hours agorootparent> it's mostly there to make the early investments more profitable So like middlemen and the big businesses that exist now? Crypto democratizes reply Dylan16807 4 hours agorootparent> So like middlemen and the big businesses that exist now? If we're talking about investing in currencies, anyone that could have made the same kind of investment in today's major currencies would have had to do it so long ago that they're dead now. If we're talking about investing in smaller entities, then you have the opportunity to get on the ground level of a thousand companies every day. That's not something that needs crypto. reply spencerflem 16 hours agorootparentprevyeah, a lot like that actually I like cryptography, the blockchain, decentralization etc. but the pitch of almost every ICO is- get in now and become massively rich (at the expense of people joining later), almost the exact same dynamic as investing in a company like Visa. reply unclebucknasty 12 hours agorootparentprevTo be clear, I was referring to my parent comment about crypto discussion in general and was not making a judgement about this particular project. But, yeah I relate to the comment in your last paragraph. There was a lot of of early talk about democratization, etc. but at the end of the day it seemed to be more about replacing the old centralized incumbents with new ones. Or, in some cases, just giving the old incumbents a new way to extend their incumbencies. There was really nothing to insulate the space from the latter. This all became really apparent during the \"DeFi\" craze. I do think a lot of earnest folks got caught up in the hype and were sincerely invested in the idea of democratization. The scammers and grifters just seemed to overwhelm the idealism. The Web3 hype was probably the apotheosis, before it popped. What's interesting is how much VCs seemed to rush into that space, yet there was barely a whisper when it all came crashing down. Kind of makes you wonder what it was really all about. reply ddtaylor 14 hours agorootparentprevI get there is a lot of trash in the crypto finance space, but I just wanted to see and have conversations about the very fascinating technologies, ZK proofs, etc. Every time it comes up we just get drowned in a thought-terminating-cliche style discussion rehashing other (often entirely unrelated) events that happened in the past that were egregious or comical. reply unclebucknasty 10 hours agorootparentI hear you. Once the community turns in a certain direction, it can be hard to break through. Some of the technologies were/are interesting. It just seemed that, even so, the applications never fully materialized. So the tech started to feel like solutions looking for problems. And, after a while, the excitement of the promise wears thin. A lot of the proposed use cases were duplicative of existing capabilities and were frequently some variation of, \"but, this is trustless/decentralized\". I just don't think that was as compelling as assumed for most people who routinely give up their data, location, etc. in exchange for convenience. And, the crypto-based \"solutions\" frequently required tech experience and/or some inconvenience to onboard. Turns out, centralization is pretty convenient. Then, after all of that, we're still left with some form of centralization in the form of node operators, foundations, etc. So, there was frequently a gap between the tech and the social aspect. Other use cases were a little grifty from the start. reply ceejayoz 15 hours agorootparentprev> Look how receptive these comments have been so far, that's a clear sign of HN's bias. Does \"people don't like my idea, they must be biased\" apply to murders, urethral sounding, and kicking puppies? Have you considered the possibility that some concepts earn hostility? reply toofy 8 hours agorootparentprevwhat you call “bias” i call “learning from mistakes.” yes, i am biased against putting my hand on a burning stove top. reply phone8675309 16 hours agorootparentprevI'll be less hostile to crypto when the benefits of cryptocurrency outweigh the environmental destruction caused by its mining. reply hanniabu 15 hours agorootparenthttps://ethereum.org/en/energy-consumption/ reply phone8675309 14 hours agorootparentEthereum is useless as a payment mechanism unless you like paying wildly varying fees, and therefore its benefits do not offset it (relatively less) energy use. reply digdugdirk 8 hours agoparentprevCould this be run in the browser (via wasm, lets say) as a base layer service to build decentralized versions of twitter/facebook/etc on top of? reply jszymborski 15 hours agoparentprevWhile I can't say I've really used Tribler a lot, I've been following the project for most of those years. Thanks for working so hard on a such an admirable project. reply troyvit 20 hours agoparentprevHow well would tribbler work for disseminating content, such as a documentary, that might get a person jailed if they put it out on more mainstream social media? reply synctext 20 hours agorootparentWhistleblowing documentary spreading in Europe or upper Americas might go OK with solid operational measures such as open wifi war driving, etc. China, Russia, etc: nope. reply zadler 18 hours agoparentprevHi I’d like to try plugging into Tribbler as a content backend for beastie.fi, which seems to have similar goals. reply CapitalTntcls 20 hours agoparentprevHi, I'm curious about your alternative model for capitalism. How does it aim to change the logic of capital accumulation? Since companies are driven by profit, the entire system revolves around capital growth and competition, which ultimately leads to the emergence of monopolies and billionaires. How does your model address these issues? reply synctext 19 hours agorootparentWe simply iterate on Linux and Wikipedia work. The principle we hope will work is to out-compete abusive platforms. Textbooks say capitalism requires realistic future profit and growth. The goal is to show Wall Street that profitability of Big Tech advertisement model is doomed. Forming non-profit collectives we aim to organise alternatives which are superior to existing offerings. So you still end up with a monopoly, just under democratic governance. This is in-line with the thinking at European Commission level, DG Grow [0]. We are trying to invent the tech to form digital collectives which scale beyond millions. Very hard. Plus collective decision making. Then you have self-sovereign citizens owning these collectives, not markets. By design Tribler is self-organising and self-scaling. We have build a DAO using shared Bitcoin capital [1] with one extension using fancy crypto based on FROST [2]. [0] https://etendering.ted.europa.eu/cft/cft-display.html?cftId=... [1] https://dl.acm.org/doi/pdf/10.1145/3565383.3566112 [2] https://repository.tudelft.nl/islandora/object/uuid:f45f85a0... reply jerry1979 5 hours agorootparentIf you are worried about users holding the DAO hostage by not signing cooperatively, you might want to check out ROAST which is basically FROST done in rounds in such a way that you can withstand some malicious participants. reply phone8675309 16 hours agorootparentprevOkay, so how do creators get paid on your platform? reply tribler 12 hours agorootparentDirect Bitcoin donations by fans. 100% to artist. Think Taylor Swift of tomorrow. reply azinman2 19 hours agorootparentprevIf advertising model is doomed (so far all the signs suggest otherwise), wouldn’t “big tech” just adjust models? What makes you think they just go away? I’m not sure why technology is the “solution” to an alternative… people seem to just want good content delivered well. Content creators want to make as much money as possible. And that’s for “honest” content… the internet is filled with disinformation and people trying to spread conspiracies, recruit for X, or otherwise mass influence the entire population. How a decentralized Bitcoin based model magically get us amazing content, something people want to use, and minimization of negative forces? Why is technology the key issue? reply synctext 19 hours agorootparentIndeed, its not about the tech. Changing the business model is key. It might be hard to re-imagine the content industry without the current monopolists. Linux showed how disruptive an open model can be. See here a description + full implementation of a music industry based on Creative Commons content. Artists release their music and receive direct Bitcoin donations from fans. 100% artists, 0% music label, 0% Big Tech, 0% credit card fee. It's a Bitcoin DAO with Spotify-inspired music discovery. [1] https://github.com/Tribler/tribler/files/11814767/First.Depl... reply stcredzero 18 hours agorootparentIndeed, its not about the tech. Changing the business model is key. It might be hard to re-imagine the content industry without the current monopolists. Linux showed how disruptive an open model can be. There is a stark difference between the case of Linux and content. In the case of Linux, ROI is measurable in dollars. In the case of content, value is in large part the perception of customers. This is going to be very difficult in the particular use case of news media, which is arguably the most critical area. We're already in a situation there where \"value\" is in the form of the strengthening of biases and misinformation. The point here, is that to succeed, Tribler might have to find a niche where superior value generation becomes undeniably obvious to some sizable segment. (Perhaps music can serve this function.) reply apantel 19 hours agorootparentprevI think the advertising model is bound to collapse once GPT agents / assistants progress to the point where anyone can set one up to do a lot of their internet searching for them. The GPT will take your query and provide you an answer, bypassing both search engines and websites. If you control the GPT, then of course you can simply use one that distills whatever it finds into useful information free of ads. If it is designed to be able to evaluate product listings, it can simply find you the product you are actually looking for, bypassing sponsored results. Once the technology reaches this level of capability, any trend toward wide adoption will pull the rug out from under most forms of web advertising. reply foobarian 18 hours agorootparentI have zero faith in this direction. How will you pay for the GPT agent operations? These things are expensive to run, which means there are going to crop up cheaper ad-supported alternatives and we're back to square 1. reply apantel 11 hours agorootparentThey’re expensive to train, not to run. A technical person could produce and run the kind of agent I’m talking about today on pro-sumer hardware. It would take a lot more effort to make something that non-technical people can use. reply foobarian 9 hours agorootparentWell, sure. Technical people even today are able to run ad blockers and pi-holes and use DDG or Kagi or what have you. But what will non-technical people do? They will have to buy a device with a hardware accelerator, and ad people are going to get into that game just like they are with smart TVs today. reply ementally 17 hours agoprevGreat project, but can the devs comment on https://lists.torproject.org/pipermail/tor-dev/2014-December... >[tor-dev] N reasons why the spooks love Tribler (Number N' will surprise you) reply simcop2387 15 hours agoparentLooks like there was a discussion back then on the github issues, https://github.com/Tribler/tribler/issues/1066 EDIT: Had a chance to look through it now, looks like they addressed all the concerns back by 2015 i.e. 1. Replacing the custom crypto code with more standard libraries (looks like they settled on NACL/libsodium's implementations). 2. Switched to AES-GCM and then later ChaChaPoly 3. Fixed up the tor protocol issues too. Probably more but there's a lot going on. reply amenhotep 17 hours agoparentprevECB and random.randint, wow. I'm not sure any dev comment could redeem crypto sins like that in such a project. reply tribler 17 hours agorootparentDeadly mistakes from 2014. Full redesign and new Rust code. reply jazzyjackson 10 hours agoparentprevAre you suggesting the subject line is inappropriate? I just thought spook meant spy (probably the only audience to whom strong encryption and anonymity actually matter) I guess there's an older more offensive meaning. reply _nalply 20 hours agoprevCorporate has learnt to misuse honorary or voluntary non-paid work in the software ecoystem and grabbing power. For example one big media outlet could adopt Tribler. At first everyone rejoices because it is recognition but what if it turns out to be an attack? How is Tribler resilient against taking over from Corporate? Of course, it's Open Source and everybody can fork. But still, could an attack be possible? reply ccvannorman 18 hours agoparentreminds me of \"EEE\" for incumbents to destroy new platforms; \"Embrace, Extend, Extinguish\" reply inSenCite 20 hours agoprevThis is a super cool project (first time I'm hearing about it), great work to everyone involved! Is the intent to have multiple tribler-like instances serving different (content) domains or more of a one-spot search that content providers can serve their content through? I ask as I'm wondering about how you foresee this \"degrading\" as it scales as that is where most current content platforms fail apart as they try to grow/maximize audience. reply synctext 20 hours agoparentGlad to hear it! Indeed, most platforms have a central point of failure somewhere. Note that Bittorrent swarms never get overloaded, we use that same technique: extreme decentralisation. With increased load that website, discovery server, or load balancer gets overloaded. With Tribler we decentralised everything to the extreme of Bitcoin and Bittorrent. So there is no \"degrading\", as long as the freeriders are somehow detected. See our 2007 architectural documents [1] [1] https://git.gnunet.org/bibliography.git/plain/docs/Concurren... reply spxneo 15 hours agoprevWhat is the onramp/offramp process for the tokens used in this micro-economy? If somebody seeds Independence Day 2 and by some miracle becomes the most seeded movie, how does that somebody cash out his tokens? If somebody wants to download Independence Day 2 what and how is it being converted into tokens? reply yobbo 15 hours agoprevThe goal here seems to be to incentivise bandwidth-sharing, but this is not the main problem. For a \"decentralized youtube\" to ever make sense, the problems that need solving are how to compensate content-creators/owners, and how to prevent piracy. reply chottocharaii 5 hours agoparentTwo problems: - Incentivising decentralised infrastructure (this solves that) - Incentivising content creation (completely different issue, will always require a political solution, and cannot be solved by technologists) reply hgyjnbdet 17 hours agoprevLast I looked at tribler it was an attack resilient torrent/tor-like network client. Now it's a micro-economy for media. That's a pivot! reply Qem 20 hours agoprevTime to test tribler again. Great project. I installed it 3-4 years ago. Looked great, but I couldn't get it working properly, because IIRC it required two open ports, one for regular torrent protocol, another for their content discovery protocol, and my VPN provider allowed only one port available to forwarding, per user per IP. reply synctext 20 hours agoparentthnx Qem for you enduring patience! We're now multiplexing everything on 1 port. If you use our Tor-like decentralised onion routing feature that works. Sadly, in normal mode we need two forwarded ports. Both Libtorrent download lib and our decentralised gossip requires their own port. We gossip about trust, content discovery, and torrent health. reply Qem 20 hours agorootparentThank you all Tribler developers. Your project is awesome. With today Internet siloed in, projects to re-decentralize are a critical need. reply James314 15 hours agoprevDoes the Tribler Client automatically turn my computer into an exit node? reply FearOfTheDuck 2 hours agoparentNo reply thomastjeffery 16 hours agoprevThere's a lot of focus here on the alternative economy part, but I think that's way less interesting than the rest of what this tech offers. What we really need is alternative moderation. The most fragile/vulnerable part of traditional torrent trackers is centralized forums. It's also the most problematic aspect of social media. A successful decentralized alternative to content moderation would drastically change the world. reply wizardwes 12 hours agoparentInteresting, but I wonder how feasible this actually is. The closest thing I can think of is the ability to selective defederate in things like Mastodon or Lemmy, where the host of your instance can do broad sweeps of moderation by essentially blocking instances that don't fit your instance's stance. The issue is that any moderation past personal blocks amd filters requires leaning against an authority, whether that be algorithmic, democratic, or our traditional style of moderation. And really, that's, in a way, what people want, someone who can handle removing unfavorable content before it gets to you most of the time. I'd love to hear ideas counter to this though. reply runeks 20 hours agoprev> Earn seeding tokens How can this be possible in a non-centralized manner? reply __MatrixMan__ 20 hours agoparentWhy wouldn't it be possible? My friends know what when I show up to a party, I usually bring a drink or a dish or something. There's no centralized database for this, they know because they've eaten and drank those things. We don't use tokens to keep track of this, but we could. The situation seems similar with seeding. It's just not commonly done because you have to build and maintain a web of trust for it to to work, and that's often a level of user responsibility that's hard to cultivate. But if you need that web of trust anyway (e.g. for filtering out ads ad other disinformation) then you might as well use it for consensus about who is a good citizen and who is not. (I have no idea if this is Tribler's approach, it's just on my mind because I've been designing something quite like Tribler, and it's my approach.) reply fwip 18 hours agoparentprevIt uses Bitcoin reply hanniabu 17 hours agorootparentThey said it's not > We have something much older then Bitcoin https://news.ycombinator.com/item?id=40159144 reply 1oooqooq 15 hours agorootparentit's their artisanal trust me token. at least it's not monetary (yet). the technical description is a pull based delayed ledger. the paper they link just show that by harming newcomers to the network you reduce free loaders, but they interpret that it solves fraud. reply yieldcrv 20 hours agoparentprevIt’s pretty easy to build an emission or reward schedule into the rules of a decentralized network when the software that nodes are running see another compliant node, they expect for it to be rewarded so they all conform to the additional issuance of tokens you just need to make sure that sybil attack attempts improve the network, as the attacker adds more nodes to earn more reply hanniabu 19 hours agoparentprevWith blockchain, but HN would hate to hear there's a usecase reply hkt 20 hours agoprevAlways lovely to see projects like this. I used it years ago after thinking \"surely there must be some kind of tor-like thing for bittorrent\" and lo and behold, there was tribler. At that point hidden seeding and downloading were both done via tor-like outproxies, eg, out to the regular internet. I recall talk from the issues page about intra-tribler media, eg, anonymised from end to end - does anyone know if this has been achieved? reply synctext 20 hours agoparentYes, you can do end-to-end encrypted hidden seeding. See technical docs (slightly outdated) [1]. Everything in Python was too slow. With Rust implementation we get 10x the speed, 160 Mbit/s anon tunneling. [1] https://github.com/Tribler/tribler/wiki/Hidden-Services-Spec... reply aldeluis 17 hours agoprev [–] The use of Tribler was problematic in my case because it uses a random torrent cache that was instrumentalized for lawfare after a political problem with my public employer in Spain. I was accused of having pedophile content on my computer after the prosecution's expert selected the corresponding torrents and downloaded them. It seems there are always pedophile torrents in a random sample of torrents. The political problem was that I refused to alter statistical data for a \"scientist\" that wanted to publish that women after abortion develop mental health issues. They search my job computer for something to kill me and found tribler cache. https://www.publico.es/actualidad/rioja-paga-estudios-salud-... It was hard. Lost job, six years under juditial prosecution... at the end, the case was dismissed, I could show that the torrent cache was not personal, but the damage was great. Be careful if you are an activist or have political involvement. I'm unaware of the workings of the current version, hope it encrypts the torrent cache somehow. reply nextaccountic 16 hours agoparentWhy does tribler automatically download random torrents without user intervention? Is it just to perform distributed search? In this case, it downloads random torrent _metadata_ right? How could the case be brought with just metadata? Regardless of whether the torrent cache was personal or not, if it was just metadata it still didn't contain anything illegal reply jazzyjackson 15 hours agorootparentlinking to illegal data may be prosecuted as distribution, even if you're not the host if you're contributing to a distributed index where people are searching and retrieving material thanks to the meta data on your drive, IMHO that's pretty close to distribution reply amelius 31 minutes agorootparentSo if someone puts a sticker on your car that has the url of an illegal website, then you may be prosecuted for illegal distribution? reply 1oooqooq 15 hours agorootparentprevhere's the thing with law, it's not code. if they argue nonsense and the judge buys, it's that. law enforcement uses hashes of bad content. torrent conveniently uses hashes. the expert can argue if you have the hash you have the content because how torrent works. the judge that accepted the argument about how torrent works but refused the argument about how tribler or freenet etc works should be disbarred imho. reply amelius 1 hour agoparentprevDid you consider that it might have been your employer who tricked your Tribler software to download the illegal files in the first place? reply doctor_eval 12 hours agoparentprev [–] “Problematic” is perhaps the biggest understatement I’ve ever read. Sorry that this happened to you. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tribler is a privacy-focused Bittorrent-compatible platform from Delft University of Technology, promoting a micro-economy without banks or advertisers.",
      "With over 100 developers, Tribler continually enhances its Bittorrent protocol, P2P communication, and introduces a token economy for artists' financial rewards.",
      "Trustchain, a unique blockchain, encourages Bittorrent seeding, highlighting anonymity features and transitioning to a new overlay system for a secure environment and innovative capitalism models."
    ],
    "commentSummary": [
      "Tribler proposes a \"media economy\" based on thresholds to address creator compensation issues in the digital media field, emphasizing decentralization, trust, and non-profit academic principles.",
      "Discussions involve cryptocurrency's role, digital inequality worries, and the potential downfall of the advertising model within Tribler's platform.",
      "Legal hurdles linked to encryption, protocols, and unauthorized content sharing pose challenges for Tribler in reshaping the content industry monopolized by major players."
    ],
    "points": 263,
    "commentCount": 107,
    "retryCount": 0,
    "time": 1714047231
  },
  {
    "id": 40161794,
    "title": "Increase Emphasizes Real-World Events in API Design",
    "originLink": "https://increase.com/articles/no-abstractions",
    "originBody": "Products Developers Pricing Updates Sign in Get started Article No Abstractions: an Increase API design principle API resources are the nouns of your API. Deciding how to name and model these nouns is arguably the hardest and most important part of designing an API. The resources you expose organize your users’ mental model of how your product works and what it can do. At Increase, our team has used a principle called “no abstractions” to help. What do we mean by this? Much of our team came from Stripe, and when designing our API we considered the same values that have been successful there. Stripe excels at designing abstractions in their API — extracting the essential features of a complex domain into something their users can easily understand and work with. In their case this most notably means modeling payments across many different networks into an API resource called a PaymentIntent . For example, Visa and Mastercard have subtly different reason codes for why a chargeback can be initiated, but Stripe combines those codes into a single enum so that their users don’t need to consider the two networks separately. This makes sense because many of Stripe’s users are early startups working on products totally unrelated to payments. They don't necessarily know, or need to know, about the nuances of credit cards. They want to integrate Stripe quickly, get back to building their product, and stop thinking about payments. “For Increase users, trying to hide the underlying complexity of these networks would irritate them, not simplify their lives.” Increase’s users are not like this. They often have deep existing knowledge of payment networks, think about financial technology all the time, and come to us because of our direct network connections and the depth of integration that lets them build. They want to know exactly when the FedACH window closes and when transfers will land. They understand that setting a different Standard Entry Class code on an ACH transfer can result in different return timing. Trying to hide the underlying complexity of these networks (by, for example, modeling ACH transfers and wire transfers with a single API resource) would irritate them, not simplify their lives. Early conversations with these users helped us articulate what we dubbed the “no abstractions” principle as we built the first version of our API. Some examples of the way this mindset has subsequently affected its design: Real-world naming Instead of inventing our own names for API resources and their attributes, we tend to use the vocabulary of the underlying networks. For example, the parameters we expose when making an ACH transfer via our API are named after fields in the Nacha specification. Immutability Similar to how we use network nomenclature, we try to model our resources after real-world events like an action taken or a message sent. This results in more of our API resources being immutable. An approach that’s worked well for our API is to take a cluster of these immutable resources (all of the network messages that can be sent as part of the ACH transfer lifecycle, for example) and group them together under a state machine “lifecycle object”. For example, the ach_transfer object in our API has a field called status that changes over time, and several immutable sub-objects that are created as the transfer moves through its lifecycle. A newly-minted ach_transfer object looks like: { \"id\": \"ach_transfer_abc123\", \"created_at\": \"2024-04-24T00:00:00+00:00\", \"amount\": 1000, \"status\": \"pending_approval\", \"approval\": null, \"submission\": null, \"acknowledgement\": null // other fields omitted here for clarity } After that same transfer has moved through our pipeline and we’ve submitted it to FedACH, it looks like: { \"id\": \"ach_transfer_abc123\", \"created_at\": \"2024-04-24T00:00:00+00:00\", \"amount\": 1000, \"status\": \"submitted\", // immutable, populated when the transfer is approved \"approval\": { \"approved_by\": \"administrator@yourcompany.com\", \"approved_at\": \"2024-04-24T01:00:00+00:00\" }, // immutable, populated when the transfer is submitted \"submission\": { \"trace_number\": \"058349238292834\", \"submitted_at\": \"2024-04-24T02:00:00+00:00\" }, // immutable, populated when the transfer is acknowledged \"acknowledgement\": { \"acknowledged_at\": \"2024-04-24T03:00:00+00:00\" } // other fields omitted for clarity } Separating resources by use case If, for a given API resource, the set of actions a user can take on different instances of the resource varies a lot, we tend to split it into multiple resources. For example, the set of actions you can take on an originated ACH transfer is different (the complete opposite, really) than the actions you can take on a received ACH transfer, so we separate these into ach_transfer and inbound_ach_transfer resources. This approach can make our API more verbose and intimidating at first glance — there are a lot of resources on the left-hand side of our documentation page! We think it makes things more predictable over the long-term, though. Importantly, our engineering team has committed to this approach. When you design a complex API over several years, you make small incremental decisions all the time. Committing to foundational principles upfront has reduced the cognitive load for these decisions. For example, when sending a wire transfer to the Federal Reserve, there’s a required field called Input Message Accountability Data which serves as a globally-unique ID for that transfer. When building support for wire transfers, an engineer in an abstraction-heavy API might have to deliberate how to name this field in a “user-friendly” way - trace_number? reference_number? id? At Increase that hypothetical engineer names the field input_message_accountability_data and moves on. When an Increase user encounters this field for the first time, while it might not be the most immediately recognizable name at first, it helps them understand immediately how this maps to the underlying system. No Abstractions isn’t right for every API, but considering the level of abstraction that’s appropriate for the developers integrating against it is a valuable exercise. This will depend on their level of experience working with your product domain and the amount of energy they’ll be committing to the integration, among other things. If you’re building an abstraction-heavy API, be prepared to think hard before adding new features. If you’re building an abstraction-light API, commit to it and resist the temptation to add abstractions when it comes along. Interested in working at Increase? Email jobs@increase.com to learn more about our open roles. Products ACH Bank Accounts Cards Checks Real-Time Payments Wires Developers Documentation API Reference Changelog Status Company Pricing Updates Privacy Security Terms Banking services provided by First Internet Bank of Indiana, Member FDIC. Increase is a financial technology company, not a bank. Cards Issued by First Internet Bank of Indiana, pursuant to a license from Visa Inc. Deposits are insured by the FDIC up to the maximum allowed by law through First Internet Bank of Indiana, Member FDIC.",
    "commentLink": "https://news.ycombinator.com/item?id=40161794",
    "commentBody": "No Abstractions: our API design principle (increase.com)266 points by jackflintermann 14 hours agohidepastfavorite98 comments Karellen 11 hours ago> If you’re building an abstraction-heavy API, be prepared to think hard before adding new features. If you’re building an abstraction-light API, commit to it and resist the temptation to add abstractions when it comes along. You could always do both. Provide a low-level abstraction-light API that allows fine control but requires deep expertise, and write a higher-level abstraction-rich API on top of it that maps to fewer simple operations for the most common use cases - which some of your clients might be implementing their own half-baked versions of anyway. If you maintain a clean separation between the two, having both in place might mean there is less pressure to add abstractions to the low-level API, or to add warts and special-cases to the high-level API. If a client wants one of those things, it already exists - in the other API. Bonus points for providing materials to help your clients learn how to move from one to the other. You can attract clients who do not yet have deep knowledge of payment network internals, but are looking to improve in that direction. reply jampekka 10 hours agoparentThis. There should be a low level API to be able to do rarer more complicated cases, and a higher level simple API for common cases built on the lower-level API. Just today I was working with the Web File System API, and e.g. just writing a string to a file requires seven function calls, most async. And this doesn't even handle errors. And has to be done in a worker, setting up of which is similar faff in itself. Similar horrors can be seen in e.g. IndexedDB, WebRTC and even plain old vanilla DOM manipulation. And things like Vulkan and DirectX and ffmpeg are even way worse. The complexity can be largely justified to be able to handle all sorts of exotic cases, but vast majority of cases aren't these exotic ones. API design should start first by sketching out how using the API looks for common cases, and those should be as simple as possible. E.g. the fetch API does this quite well. XMLHttpRequest definitely did not. https://developer.mozilla.org/en-US/docs/Web/API/FileSystemS... Edit: I've thought many a time that there should be some unified \"porcelain\" API for all the Web APIs. It would wrap all the (awesome) features in one coherent \"standard library\" wrapper, supporting at least the most common use cases. Modern browsers are very powerful and capable but a lot of this power and capability is largely unknown or underused because each API tends to have quite an idiosyncratic design and they are needlessly hard to learn and/or use. Something like what jQuery did for DOM (but with less magic and no extra bells and whistles). E.g. node.js has somewhat coherent, but a bit outdated APIs (e.g. Promise support is spotty and inconsistent). Something like how Python strives for \"pythonic\" APIs (with varying success). reply cpeterso 4 hours agorootparentMy favorite quote about abstraction is from Edsger Dijkstra: ”The purpose of abstraction is not to be vague, but to create a new semantic level in which one can be absolutely precise.” If only the “A” in API stood for “abstraction”. For many APIs, it probably stands for “accreted”. :) reply dumbo-octopus 8 hours agorootparentprevIt's 4 async calls, and it can be done entirely in the main thread so long as you use the async API. https://developer.mozilla.org/en-US/docs/Web/API/FileSystemW... reply jampekka 1 hour agorootparentFor OPFS it's still 6 calls. And the async API doesn't support flushing writes. https://developer.mozilla.org/en-US/docs/Web/API/File_System... reply metalspoon 9 hours agorootparentprevTbf Vulkan is not intended for an endprogrammer. It is a deliberately low level standardization to allow directly control GPU hardware. The high-level approach (OpenGL) failed. The endprogrammer is supposed to use a third party middleware, not Vulkan itself. reply withinboredom 3 hours agorootparentSomeone has to write that middleware… reply Groxx 9 hours agoparentprevI'm particularly fond of this pattern when you can implement the high level API that you want outside the library, which ensures that your low level API is sufficiently flexible and means you're dogfooding your own stuff as a user. It's far too easy to get used to the internal side of a tool you're building, and forget how people actually use it. reply InvisibleUp 9 hours agoparentprev.NET also does this a lot. Here's a recent devblog post looking at file I/O: https://devblogs.microsoft.com/dotnet/the-convenience-of-sys... reply yen223 10 hours agoparentprevIt does double your API surface area, so that's the tradeoff you'll have to consider. It can be the correct decision in a lot of cases. reply campbel 10 hours agorootparentUnlikely to double. The low level API exposes all capabilities, the high level API exposes a subset of those capabilities under a smaller surface. The high level API will not be as large as the low level. reply blowski 3 hours agorootparentThis reminds me of the Kubernetes API. reply Pxtl 10 hours agorootparentprevIt doesn't double the security surface area if the abstracted API goes through the low-level API. The outer one is just chrome, and so the risks of screwing something up there is far lower. Unless you're using a trash language where even simple wrappers could buffer underrun or something. reply lmz 10 hours agorootparentIsn't there the issue of modifying the state enough through the low-level API such that it breaks the assumptions of the high-level one? reply withinboredom 3 hours agorootparentThe high level API shouldn’t care about state. In other words, your “readers” should merely aggregate state and your “writers” should only care about subsets of state. Think about file permissions in Linux. Running ls just shows you gross file perms (current user, group, and global) but you can also grant access to other individual users, or even make a file immutable that still shows up as writable to ls. The high level api doesn’t know or care about the low level state except where it is relevant. reply eru 4 hours agorootparentprevJust say that you don't support mixing both APIs. reply lazyasciiart 2 hours agorootparentNow you’re telling users to check the source code of any tools they use with your stuff? “We recommend using ToolOne only on content you do not also manage with ToolTwo” reply actionfromafar 4 hours agorootparentprevThat’ll fix it. ;-) reply eru 4 hours agoparentprevCompare also exokernels, and how they delegate abstraction to libraries, not the OS. reply paulddraper 11 hours agoparentprevGit is an example of this. [1] There are high-level \"porcelain\" commands like branch and checkout. And then there are low-level \"plumbing\" commands like commit-tree and update-ref. [1] https://git-scm.com/book/en/v2/Git-Internals-Plumbing-and-Po... reply mbork_pl 1 hour agorootparentCame here to say that. Also, to some extent, Emacs. There are thousands of functions (actually, a bit less than 10k in stock Emacs without packages, and over 46k in my Emacs) performing various low-level tasks, and much fewer commands (~3k in stock Emacs, almost 12k in my config), i.e., interactive functions, often higher-level, designed for the user. reply zellyn 6 hours agoparentprevProblems can sneak in when you use the low-level API to do something to an object that can't be cleanly represented in the higher-level API. You need some kind of escape hatch, like a list of links to or ids of low-level details or a blob (Map of miscellaneous data that can hold the low-level additions. Hopefully the top-level important concepts like \"amount_due\" will still reflect the correct numbers! reply withinboredom 3 hours agorootparentThose problems usually present themselves by people overthinking the high level api and trying to be smart. As an example, you can use chattr to make a file in Linux immutable. ls still shows that you have permission to write to the file, even though it will fail. When people try to overthink the api and have it determine if you really can write to a file, people will try using the high level api first (chmod) and it won’t work because it has nothing to do with permissions. KISS is really needed for high level APIs. reply andenacitelli 7 hours agoparentprevAnother example of this is AWS CDK. There are a few “levels” of constructs - high level ones that are simpler and apply “presets” that are enough for 80% of users, but the core low level ones still exist if you have a nonstandard use case. reply devjab 2 hours agoparentprevI’m genuinely curious as to what an abstraction-rich api would look like and why it would be useful. I’ve mainly worked in enterprise organisations or in startups transitioning into enterprise which is sort of where my expertise lies. I’ve never seen an API that wasn’t similar to the examples in this case. I mean… I have… but they wouldn’t be labelled as high-abstraction api’s. If they needed a label it would be terrible APIs. Like sending table headers, column types in another full length array with a line for each column, and then the actual data/content in a third array. Even sending the html style that was used in what ever frontend, so that some data is represented as “some data” and other is represented as [“some data”, [“text-aligned”, “center”…],… . Yes, that is an actual example. Anyway I’ve never seen a high abstraction api and I feel like I’m missing out. reply lwhi 3 hours agoparentprevI like this idea a lot. One level of API for implementation model. And second level for mental model. reply RobotToaster 2 minutes agoprevNo abstractions? So their API lets me control the individual registers on their CPU then? reply summerlight 13 hours agoprevI like the part that explains why Increase choose a different approach. Contexts matter a lot when you design something fundamental, but people usually don't appreciate this enough. reply spandrew 14 hours agoprevLove the article. If you love Stripe (and as a designer and tech entrepreneur I do – Stripe's simplicity and front-end skill is incredible) you might look at them and copy their ability to simplify and deliver polished experiences. But the real mastery of Stripe is that they know their customers — and the simplicity they crave. By this article is sounds like Increase does as well and has forged a similar laser-focus on what their customers need to build terrific design guidelines for making products. Inspiring to see. reply esafak 11 hours agoparentHow Stripe builds APIs and Teams: https://www.youtube.com/watch?v=IEe-5VOv0Js reply rtpg 12 hours agoparentprevYeah I do think you can see in Stripes API places where there are differing tensions between “let’s make this potentially universal” and “let’s accept that this stuff is going to probably only apply for one payment method in one market”. Personally I appreciate when the latter happens, but there’s an aesthetic decision there reply theptip 4 hours agoprevThis is a great example of the concept “ubiquitous language” from Domain Driven Design. Use language that your domain experts understand. If your users know about NACHA files, using other terms would mean they need to keep a mapping in their head. On the other hand, in Stripe’s case, their users are not domain experts and so it is valuable to craft an abstraction that is understandable yet hides unnecessary detail. If you have to teach your users a language, make it as simple as possible. reply Nevermark 3 hours agoparentOr to put it another way, they are domain experts in the kinds of transactions they want to perform, not how transactions are implemented in the financial system. reply cpeterso 12 hours agoprevThis is similar to Domain-Driven Domain's \"Ubiquitous Language\" design pattern, making your implementation use the same real-world terminology used domain experts. https://thedomaindrivendesign.io/developing-the-ubiquitous-l... reply hinkley 4 hours agoparentI was introduced to this concept a good while before DDD came along, when someone opined that if the nouns and verbs in your code don't match the problem domain that's an impedance mismatch and it's going to get you into trouble some day. It really reads like a shame response to me. People are so pathologically allergic to saying \"I was wrong\" or \"we were wrong\" that they end up pushing their metaphors around like a kid trying to rearrange their vegetables on their plate to make it look like they ate some of them. It's also smacks of the \"No defects are obvious\" comment in Hoare's Turing Award speech. reply tegling 4 hours agoprevOne tricky thing to model neatly in payment APIs is that payment schemes indicate the roles of payer and payee in payment returns in different ways. E.g. for one particular scheme the payer and payee may be kept in the same position as in the initial payment (creditor of payment return is actually the one sending funds) whereas in another one they are switched (creditor is the one receiving funds in the return). I'd be curious to see how they are handling this case as it can be a real head-scratcher. reply l5870uoo9y 12 hours agoprev> Monthly fees for users building on Increase vary by use case. I am currently adding public API access to AI-powered text-to-SQL endpoint with RAG support and the my biggest issue is the pricing. Anybody have a ballpark figure what we could be talking about here? Pricing must account for OpenAI tokens (or perhaps letting them add their own OpenAI token), database usage and likely caching/rate limiting setup down the line. reply chaos_emergent 12 hours agoparentFoundationally, pricing should be based on value, not cost[1] so you should think about what the value is to your customer and go from there. Ex: I know that Gong costs a ton of organizations over 100k/year, and there's no way that, accounting for storage, CPUs, and all the other OpEx, that the cost comes anywhere close to the cost of compute - it's likely at least an order of magnitude greater. But because sales teams bring in so much revenue so directly, any leverage that they can buy in the form of a tool like Gong is immediately and obviously valuable. [1]: the exception to avoiding cost-plus pricing is if you're selling a commodity. But you're not in that boat! reply lwhi 11 hours agoprevSo they say parts of the API structure are based 1-1 on externally controlled specifications. What happens if those specifications evolve or change? New API? reply jackflintermann 14 hours agoprevAuthor here - this has been a useful mindset for us internally but I'm curious if it resonates externally. I'd love your feedback! cratermoon 12 hours agoprevNo Abstractions here really means \"just use terms from the underlying system\", which is a good naming principle in general. Problems inevitably arise over time when there's multiple underlying systems and they have different names for the same thing, or, arguably worse, use both use a name but for different things. In this example, what if the underlying payment providers have different models? Also, what if the Federal Reserve, deprecates Input Message Accountability Data and switches to a new thing? Maybe things are a lot simpler in the payment industry than they are in transportation or networking protocol. If I built a packet-switching product based on X.25 and later wanted to also support tcp/ip, what's the right abstraction? reply jackflintermann 11 hours agoparentI appreciate the thorough read! For deprecations we're lucky in that the underlying systems don't change very much (the Input Message Accountability Data isn't going anywhere). But we'll run into collisions when we, for example, start issuing cards on Mastercard as well as Visa. We have experimented with a couple of, um, abstractions, and may do so there. One rule we've stuck to, and likely would as well, is to keep the \"substrate objects\" un-abstracted but to introduce higher-level compositions for convenience. For example, there is no such thing as a \"Card Payment\" (https://increase.com/documentation/api#card-payments) - it's just a way to cluster related card authorization and settlement messages. But it's extremely useful for users and nontrivial to do the reconciliation, so we tried it. But we think it's essential that the underlying network messages (the \"substrate objects\") are also accessible in the API, along with all the underlying fields etc. Unfortunately 100% of the public APIs I have worked on are in payments. I wish I had another lens! reply advisedwang 12 hours agoparentprev> No Abstractions here really means \"just use terms from the underlying system\" The article clearly says it also means \"no unifying similar objects\", which enables the naming decision. reply cratermoon 11 hours agorootparentHow does that work if, for example, the example given of \"Visa and Mastercard have subtly different reason codes for why a chargeback can be initiated, but Stripe combines those codes into a single enum so that their users don’t need to consider the two networks separately.\". Unfortunately, the article doesn't explain how Increase handles that overlap. Presumably, as the article states, their customers are the sort that do care about Visa reason codes vs Mastercard reason codes, so what's the design of a \"no abstraction\" API in that case? reply travisjungroth 10 hours agorootparentI’m just reasoning from my limited experience and the article. Stripe unifies those reasons codes. Increase doesn’t. It might be that the Chargeback has the processor and chargeback code as attributes. So rather than have a universal “goods and services not received”, it’s a 13.1 for Visa, a 4855 for MasterCard and a F30 for Amex. This matters when the boundaries are different. For example, they all split up the categories of fraud differently. reply Terr_ 9 hours agoparentprev> No Abstractions here really means \"just use terms from the underlying system\" Which sounds a bit like Domain Driven Design, although the \"underlying system\" in this case may be a bit too implementation-centered to be considered a real business domain. To expand on that a bit: In DDD you generally defer to the names and conceptual-models the business-domain has already created. Trying to introduce your own \"improved\" [0] model or terms creates friction/miscommunications, adds opportunities for integration bugs, and ignores decades or even centuries of battle-tested specialized knowledge. [0] https://xkcd.com/793/ reply chowells 13 hours agoprevIf there's no abstraction, what's your value-add? I don't care enough to read your marketing BS to see where you claim to be special, but... If your API is doing the exact same things as an underlying service is doing, you're just a middleman extracting rents. You might find it more valuable to state your position as \"carefully scoped abstractions\" to make it clear what value you add. reply koreth1 13 hours agoparentBased on my previous experience on payment systems, there's a surprising amount of value in not having to maintain direct business relationships with the underlying payment providers. It is much, much easier to work with a company like Stripe than to work directly with Visa and MasterCard and the ACH network, and heaven help you if you're a small company that needs to do automated cross-border payments to a wide range of countries without a middleman. You'll probably also get much better support from a tech-focused company when an API starts freaking out. reply jackflintermann 13 hours agorootparentYes, exactly, the important thing to us is that our users don't need to build an additional mental model between us and the networks we sit atop. If you know the network, we want you to be able to intuit how our API works. There's a very real difference (arguably the fundamental value-add of our company) in the transport layer, though. The actual mechanics of integrating with, say, FedACH, are a bit long to get into here (we get into it a bit here if it's of interest: https://increase.com/documentation/fedach) but suffice to say it doesn't have a REST API. reply koreth1 13 hours agorootparentThat's an excellent point too. Some payment systems have abysmal technology. The product I worked on was focused on international payments and in a couple cases, the \"API\" was literally, \"Upload a CSV file via FTP, and at some later point, another CSV file might appear on the FTP server with some of the payment results, but if it doesn't, call us because we probably just forgot to upload it.\" reply nijave 12 hours agorootparentBatch jobs and (S)FTP. In a bit of a weird twist, back when I worked at Chase, they were innovating on the ancient technology but it was things like \"better batch job management/orchestration\" and SFTP proxy to route between different servers and centralize key management reply wodenokoto 4 hours agorootparentprevBut it must have _some sort_ of API. Since your rest API is modelled on their API it made me really curious about how you communicate with those networks. reply jrochkind1 8 hours agorootparentprevI'm curious to learn more about what your customers look like, what sorts of businesses and activities they are in. Where stripe's customers are working on products unrelated to payments, yours are working on products related to payments? I'm having trouble conceiving of examples. reply fendy3002 6 hours agorootparentprevIdk how this one works, but credit card processors need license. If you can use credit card service without requiring that license then it'll be the best additional value. reply exe34 13 hours agorootparentprevI thought I understood everything you said, but isn't Stripe a middleperson here? > without a middleman. reply koreth1 13 hours agorootparentRight, Stripe is a middleman and part of the value they're giving you is that you don't have to work directly with the underlying payment companies. If you had to support the same range of payment options without a middleman, you'd need to have business relationships with a bunch of payment companies, which would be a lot more difficult and time-consuming. Hope that's clearer! reply exe34 12 hours agorootparentMakes sense thanks! reply OJFord 13 hours agorootparentprevYes, GP's point is 'good luck to you doing that yourself, without a middleman [such as Stripe]'. reply conroy 13 hours agoparentprevIn this case, a HTTP API is the abstraction. Integrating with ACH and other payment rails requires a lengthy integration process. Sometime you have to send binary files using FTP! reply jiggawatts 12 hours agorootparentThe article says “no abstractions”, but HTTP is often exactly that: an abstraction over lower-level protocols. reply jackflintermann 12 hours agorootparentI guess the phrase \"no abstractions\" is specifically valuable to us when designing our REST API resources - our whole stack is certainly an abstraction of sorts, but we don't want to add yet another abstraction in that specific layer. reply chaos_emergent 12 hours agorootparentprevperhaps what you're thinking of is \"equal entropy abstractions\" - HTTP is just a way of standardizing logic, but the complexity of the shape and behavior of the API remains. reply arrowsmith 13 hours agoparentprevPresumably the value comes from providing a single unified platform that means you don't have to integrate with every underlying service separately. I know nothing about the lower-level details of payment networks but the mere fact that this company exists and has customers would suggest that there's a value-add. reply contravariant 11 hours agoparentprevThere is abstraction there's just not an additional layer of abstractions on top of it. Which to be honest is quite good, there's lots of things you can solve with an additional layer of abstraction but not having too many layers of abstraction. It's also rare to be able to identify an abstraction that correctly cuts things off at the right layer. reply alex_lav 5 hours agoparentprev> If there's no abstraction, what's your value-add? But then > I don't care enough to read Hmmmmmm. reply west0n 8 hours agoprevIf we didn't have abstractions like POSIX, applications would need to write an adaptor for every supported file system. reply danecjensen 1 hour agoprevStrong work Jack! reply dheera 6 hours agoprevJava engineers need to see this. Goddamn Proxies, Factories, and Beans. Fragments, Surfaces, Runnables. reply bvrmn 10 hours agoprevAlso it greatly helps to not overuse nouns and try to forcefully model verbs with resource entities. reply lolpanda 12 hours agoprevfor any APIs related to money, should the currency be in strings as opposed to in floats? This will prevent accidental float arithmetic in the code. I always find it tricky to work with currency in javascript. reply benhoyt 12 hours agoparentFrom their docs [1] it looks like they do everything using integers: the amounts are integers in the \"minor unit\" of currency, for example cents if the currency is dollars. So 1000 means $10.00. In languages like JavaScript where everything is a float64, you can still accurately represent integers up to 2^53, which would be $90 trillion. [1] https://increase.com/documentation/api#transactions reply crabmusket 11 hours agorootparentThis isn't sufficient to represent prices which often include fractional amounts of cents in non-retail scenarios. Think of AWS server prices per hour. reply krainboltgreene 11 hours agorootparentFunny, because that's exactly what Stripe does (https://docs.stripe.com/billing/subscriptions/usage-based/pr...) reply GeneralMayhem 3 hours agoparentprevIdeally integers, but at a large multiplier. Google's money proto [1] has units and nanos. Any competent ad-tech system will use something similar: integer number of micro-dollars, nano-dollars, etc. You want a fair amount of precision, so just tracking whole cents isn't enough, but you want that precision to be (linearly) equally distributed across the value space so that you can make intuitive guarantees about how much error can accumulate. [1] https://github.com/googleapis/googleapis/blob/master/google/... reply freedomben 12 hours agoparentprevYes, never use floats for currency. I typically use integers and for USD for example, measure in \"cents\" rather than dollar. I try to avoid the fallacy of appeal to authority, but this is what Stripe does. You can also use the Decimal type in javascript and convert to/from strings to cross API boundaries. reply tadfisher 12 hours agoparentprevI will be the contrarian: JSON numbers are not floating point values, they are strings of characters matching the format \"-?(?:0|[1-9]\\d*)(?:\\.\\d+)?(?:[eE][+-]?\\d+)?\". You can choose to parse them however you want, and parsing libraries should provide a mechanism to decode to an arbitrary-precision value. reply int_19h 2 hours agorootparentRegardless of what the libraries should be doing, there is the reality of what they are doing. reply meekaaku 2 hours agorootparentprevYes, but say in javascript if you do a JSON.parse(), it will give you a double float right? reply lolpanda 10 hours agorootparentprevGood point. it's not a problem with JSON. It's just that most of the JSON libraries by default parse numbers into floats. reply koreth1 11 hours agorootparentprevBy way of example: when I worked on payment code in Java, we accepted numeric JSON values in request payloads but they were deserialized into \"BigDecimal\" fields in our payload classes, not \"float\" or \"double\". reply trevor-e 12 hours agoparentprevI've always seen currencies multiplied by 100 to remove the need for floating point. reply kadoban 4 hours agorootparentIf you use a higher constant, 10000 or 1000000 or something, you give yourself a good amount of more fleixibility. reply cateye 10 hours agorootparentprevSome currencies use more than 2 decimal places. For instance, the currencies of Algeria, Bahrain, Iraq, Jordan, Kuwait, Libya, and Tunisia are subdivided into 3 decimals. reply akavi 11 hours agorootparentprevThat's not quite a sufficient rule. Eg, 1 Bahraini Dinar is 1_000 Bahraini Fils. reply nijave 12 hours agorootparentprevYeah, this seems like a common pattern. Not sure about currency with arbitrary place values though (like Bitcoin) reply deathanatos 12 hours agorootparentI'm not sure what you mean by \"arbitrary place values\" with Bitcoin; if you are implying it's infinitely divisible, it isn't. You'd do the same trick with Bitcoin: represent it as an integer¹. The value 1 is 1 sat, or 0.00000001 BTC. ¹(where you need to; otherwise, I'd use some fixed point type if my language supports it) reply cratermoon 12 hours agoparentprevneither. Use rational or some other better type. reply endofreach 12 hours agoparentprevYou should never use floats for dinero. And it has nothing to do with JS, though i find it funny that you mention JS. reply xxgreg 12 hours agorootparentDon't use floats if you're trying to represent an exact value, i.e. someones bank account. But in financial modelling you're generally dealing in probabilistic \"expected values\", it's common and fine to use floats. Having said that, half the world seems to run on Excel spreadsheets, which are full of money values, and Excel is basically all floats (with some funky precision logic to make it deterministic - would be curious to know more). https://stackoverflow.com/questions/2815407/can-someone-conf... reply kikimora 13 hours agoprevI think this is better than Stripe’s abstract everything approach even for people who are not into payments. Stripe has built a very leaky abstraction. reply adelineJoOs 12 hours agoparentHow is the leakage noticeable? reply kikimora 8 hours agorootparentI’m not saying Stripe API is bad. But there are limits to how much differences you can hide behind a generic API while keeping it consistent. Off the top of my head I can think of a few cases I would qualify as a leaky abstraction. To start with - there is a payment method abstraction and there is SetupIntent that works with it. Normal use case is tokenizing a CC. But for ACH it does something different if ever works. Same setup intent would work with debit cards, but not in Brazil because of local regulations. I don’t remember if you get a decent error code when attempt to tokenize a Brazilian debit card. Customers making cards payments can initiate a dispute which would cost you 15 usd + payment amount if they win. This cannot happen with some other payment methods. It became important when you implement Stripe connect because you might want to set different fees for different payment methods to account for cost of disputes. The leaky abstraction part here is as soon as you start creating certain type of payment intents you also have to subscribe to Stripe webhooks for disputes. To save on refund fees you may want to authorize payments (confirm payment intent) and capture them after a period of time. During that window you can cancel the payment and pay only authorization fee instead of paying full refund fee. This strategy works only for payment methods supporting authorization and capture semantics and having favorable commission structure. Max amount of time between confirm and capture depends on the payment method as well. Not specific to Stripe Terminals but still. Tapping a card gives you an anonymized payment method while dipping the same card reveals some cardholder data. This is beyond Stripe control, but puzzling at first because at the API level you deal generic PaymentMethod object. With Stripe connect what happens after the payment is defined in terms of abstract transfers between Stripe accounts. In some regions transfers works across countries while not in the others. One example is Canada-USA vs Brazil and rest of the world. From one end you have abstract transfers API to move money between Stripe accounts. From another you have to implement a number of workarounds to make transfers work in all interesting scenarios because of regional and currency conversion considerations. For example in some cases you do transfers while in other you do payment intents. What I’m trying to say here is you have to know specifics of payment methods, underlying technologies and regions you work with. By looking at high-level API you may think it is easy to support many payment methods when in fact many of them would require very specific code. reply shironandonon_ 8 hours agoprevTLDR: we are special and created our own standard. reply ngrilly 2 hours agoparentThey are saying the opposite: we follow existing standards as much as possible. reply andrewstuart 12 hours agoprev [–] I hate abstractions. Program the thing as it is intended. Why do programmers always need a library between them and the API? reply Jtsummers 11 hours agoparent [–] > Why do programmers always need a library between them and the API? You do know that libraries present an API, right? Very few people program on Linux or other OSes without using libc or the OS/distribution equivalent, and for good reason. Those libraries provide a degree of compatibility across hardware systems and operating systems (and even the same OS but different versions). Your question is about as sensible as asking \"Why do programmers always need a programming language between them and the machine code?\" Because it improves portability, reusability, reasonability, and on and on. Though, since you hate abstractions, maybe you do only program in machine code. reply koreth1 11 hours agorootparent [–] I kind of hate the fact that the term \"API\" has lost its generality in the minds of a huge number of practitioners, and people now assume it refers to a set of network (usually HTTP) request and response formats. It's great that we have a succinct word to describe programmatic interfaces built on top of HTTP. It's not great that there's no longer a universally-understood word for the original more general meaning even though, as this thread demonstrates, the original meaning is still as relevant as ever. reply compootr 9 hours agorootparent [–] I think context has to be taken into account people here are referring to some financial service on the internet, whose API is invoked over http An article about some library might be viewed differently, i.e \"X's API is better than Z's\"...etc reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Increase's API design principle \"No Abstractions\" advocates naming API resources after real-world events, leveraging network terminology, and categorizing resources based on specific use cases.",
      "Influenced by Stripe, this strategy targets Increase's users familiar with payment networks, prioritizing resource immutability and clear naming for streamlined integration and a transparent system mapping.",
      "While not universal, this principle enhances user comprehension and decreases cognitive burden in decision-making, potentially offering substantial value in certain API contexts."
    ],
    "commentSummary": [
      "The article emphasizes achieving a balance between abstraction-heavy and abstraction-light APIs by providing both low-level and high-level APIs to address different user needs.",
      "It highlights the challenges of maintaining consistency and flexibility in API design using examples from platforms like node.js and Vulkan, while also underlining the importance of domain-specific terminology, simplicity, and pricing strategies.",
      "The discussion includes insights on representing prices in non-retail scenarios, using integers for currency precision, limitations of Stripe's API, authorization strategies for saving refund fees, and leveraging tools like Stripe webhooks for disputes."
    ],
    "points": 262,
    "commentCount": 97,
    "retryCount": 0,
    "time": 1714072511
  },
  {
    "id": 40164199,
    "title": "Jeff Lawson and Ben Collins Acquire The Onion",
    "originLink": "https://www.nytimes.com/2024/04/25/business/media/the-onion-sold.html",
    "originBody": "ADVERTISEMENT SKIP ADVERTISEMENT The Onion Is Sold by G/O Media The satirical news website was bought by a new firm in Chicago that took inspiration for its name, Global Tetrahedron, from a book written by The Onion’s staff. Share full article The Onion’s new owners have promised to keep the staff on board and in the publication’s home base of Chicago. Credit... David Kasnic for The New York Times By Katie Robertson April 25, 2024 G/O Media announced on Thursday that it had sold The Onion, a satirical news site, to a group of digital media veterans. The Onion, which started in 1988 in Wisconsin as a weekly satirical newspaper and later became a website, is known for its parodies of current events. For the last decade, it has republished the same headline after nearly every mass shooting: “‘No Way to Prevent This,’ Says Only Nation Where This Regularly Happens.” In an email to G/O Media staff that was obtained by The New York Times, Jim Spanfeller, the chief executive, said the company was “undergoing an extensive review of our portfolio with the intention of coring down to our leading sites in terms of audience and revenues.” He said G/O Media had agreed to sell to “a new Chicago-based firm called Global Tetrahedron.” “This company is made up of four digital media veterans with a profound love for The Onion and comedy-based content,” Mr. Spanfeller wrote. “The site’s new owners have agreed to keep The Onion’s entire staff intact and in Chicago, something we insisted be part of the deal.” The name Global Tetrahedron is, in true Onion fashion, a winking reference to a sinister fictional company featured in the book “Our Dumb Century,” which was written by The Onion’s staff and published in 1999. The real-life Global Tetrahedron is owned by Jeff Lawson, a co-founder and former chief executive of the technology communications company Twilio. The chief executive is Ben Collins, who was a senior reporter at NBC News until recently. In an interview, Mr. Lawson said that he had long wanted to buy The Onion and had pursued the project at various points in time before linking up with Mr. Collins, who started pondering the idea early this year. “The world needs laughter; it needs satirical criticism more than ever,” Mr. Lawson said. “And that’s why we think this is the right time and the right way to help The Onion continue to grow, continue to flourish, and frankly I’m concerned if we hadn’t done this, I don’t know what would have happened.” Mr. Collins said audiences had a longstanding connection to The Onion and noted that many of the website’s writers and editors had been there for years. “Our goal is to be stewards for this thing,” he said. “We’re keeping all the writers, we’re going to work with the union, we’re going to make it so they can hopefully get paid a little bit more money, and we’re going to give them the room to grow.” The new owners said they planned to improve user experience on the website and expand into multimedia, but otherwise wanted The Onion’s staff members to continue to do the work they’ve been doing. Mr. Lawson and Mr. Collins, who were in Chicago Thursday to meet The Onion’s editorial staff, declined to disclose the deal price. Noah Shachtman, the former editor in chief of Rolling Stone, has advised on the project, according to two people with knowledge of the deal. “The Onion is just an institution,” Mr. Lawson said, adding: “It should be preserved and it should be great.” The website is the latest to be shed by G/O Media, which still publishes a few stalwart internet brands like Gizmodo, The Root and Quartz. In recent years, the company sold off Jezebel, Lifehacker, Deadspin and the A.V. Club. G/O Media was formed in 2019 by the private equity firm Great Hill Partners after it bought a collection of websites that were once part of Gawker Media. Katie Robertson covers the media industry for The Times. Email: katie.robertson@nytimes.com More about Katie Robertson A version of this article appears in print on , Section B, Page 2 of the New York edition with the headline: G/O Media Sells Onion To Digital Media Group. Order ReprintsToday’s PaperSubscribe Share full article ADVERTISEMENT SKIP ADVERTISEMENT",
    "commentLink": "https://news.ycombinator.com/item?id=40164199",
    "commentBody": "Jeff Lawson buys The Onion (nytimes.com)268 points by coloneltcb 10 hours agohidepastfavorite216 comments goles 9 hours agohttps://archive.is/dvoCb pavel_lishin 10 hours agoprevFrom a different article: https://www.businessinsider.com/twilio-founder-jeff-lawson-b... > When asked whether he had purchased The Onion, Lawson played coy. \"What's The Onion?\" he replied. Then, \"What's a Tetrahedron?\" > Business Insider was unsure how to respond to these questions. reply SushiHippie 8 hours agoparent> \"Am I talking to Twilio founder Jeff Lawson or am I just taking crazy pills today?\" Business Insider's reporter replied. Lawson did not respond. This article is absolute gold, thanks for sharing haha reply tootie 7 hours agoparentprevHad no idea Ben Collins had become an executive at whatever this thing is. He was an excellent reporter and a guy who really, really understands how the internet works and how communities flourish or go toxic. He reported a lot about Kiwi Farms and Cloudflare's response to public pressure. https://www.wnycstudios.org/podcasts/otm/segments/worst-plac... reply dev_by_day 3 hours ago [flagged]rootparentnext [16 more] Ben collins is not an excellent reporter, he is an extreme leftist ideologue. He has a terribly biased view of \"how the Internet works\" e.g. its toxic and dangerous when not aligned with the extreme American left and its good when its aligned with leftist ideology, even when being toxic to everyone else. He was literally suspended from covering elon musk by NBC for his biased views. The only reason you probably respect ben collins is he spoon feeds biased American leftist view points to you. That makes him a good \"leftist\" but a terrible reporter of facts, which is maybe why you like him so much. reply crashmat 1 hour agorootparentcould I ask what kinds of positions the 'extreme American left' takes on issues to distinguish them from the moderate left? reply n4r9 1 hour agorootparentprevThat's a lot of accusation. I've not heard of him before but you can view a list of his articles on NBC. He does seem left-leaning for the US, doesn't look that extremist to me. HNers can decide for themselves: https://www.nbcnews.com/author/ben-collins-ncpn858396 reply qsdf38100 3 hours agorootparentprevYou just used \"extreme\", \"left\" and \"leftist\" 7 times in 4 sentences. Sounds a bit \"extreme\" and \"biased\" to me. reply dev_by_day 3 hours agorootparentIm pointing out a reporter is biased in a specific direction, of course I'll point out which direction that is. Are you even going to deny thar he's not left wing and covers stories from that perspective? I'm a leftist personally but I'm sick of the bias never being mentioned or called out. reply dev_by_day 3 hours agorootparentprevnext [12 more] [flagged] hnlmorg 3 hours agorootparentI really get sick and tired of this ongoing pseudo-religious war between the left and the right. Can’t you express your opinion in a way that isn’t antagonistic? reply dev_by_day 3 hours agorootparentBen collins is a promoter of this left right war, he was literally suspended for being biased. That is not a good reporter, he is a beneficiary of the culture war and its disrespectful to actual factual journalism to call him anything else but a political stereographer who is out to push ideology above facts. reply hnlmorg 3 hours agorootparentCollins isn’t the person writing your comments. You are. reply concordDance 2 hours agorootparentprevWorth noting that this reply has a lot of the same content as your first one but isn't flagged. This is mostly due to it being phrased in a far less inflammatory manner I think. reply orf 3 hours agorootparentprevYou don’t sound healthy. Reflect on this while you can. reply reaLg_move_in_3 3 hours agorootparentPointing out bias and calling it out when people try to promote reporters who were suspended for being biased is very healthy. Its very unhealthy to push biased view points that come from a group of primarily upper class white people. Reflect on that while you still can and you'll understand why so many non-white and working class don't like the cultural imperialism they try to normalize. reply tommica 2 hours agorootparentYou do have a valid point in what you are trying to say (that Collins got suspended for his strong bias), but the way you communicate it is really doing harm to your message - you are antagonizing too much. reply bigfudge 3 hours agorootparentprevIt’s so niche that it regularly wins the popular vote in the US despite massive voter suppression. The only reason the god n guns lobby wins in the US is 1. the constitutional settlement which massively overweights rural voters and 2. corporate lobbying in a winner takes all system which is prepared to hold its nose and bankroll racist hicks if it means they get their corporate welfare. You are a great nation, but very little that anyone admires about the US has come from the flyover states (and even then probably because of federal pork barrel science and defence funding for those educated elites you hate so much). reply dev_by_day 3 hours agorootparentYou misinterpreted Democrats and progressives, most Democrats, minorities and liberals are closer to the center left rather than extreme left. Polling from many reliable places, including pew, shows Progressive ideology is dominated by upper class white people. Why do you think stripping resources from the police was so popular amongst upper class intectuals but not what black Americans actually wanted according to the actual data. Progressivism in America is an ideology that represents the upper class, not minorities nor traditional liberals nor the working class. reply defrost 3 hours agorootparentprevI'm from Australia. I grew up on a cattle station in the Nor'west. I'm downvoting your comments for their strong foaming at the mouth unhinged vibes, and I'd do the same if they ranted in a similar manner against some other part of US culture. I dare say you could make a comment with fewer empty buzz phrases if you tried. reply concordDance 2 hours agorootparentprevI think the downvotes are because we don't really want the culture war on hackernews. It tends to not be a very productive topic and makes people emotional, making discussion quality even worse than most other politics (like economic policy, regulations or international relations). reply lubujackson 8 hours agoprevI'm old enough to remember when it was a physical paper you'd get for free in Harvard Square... wish I kept some of them. I particularly enjoyed the old \"Point/Counterpoints\" such as: https://www.theonion.com/u-s-out-of-my-uterus-vs-we-must-dep... https://www.theonion.com/americas-homeless-want-a-hand-up-no... https://www.theonion.com/my-computer-totally-hates-me-vs-god... Also, shout out to the \"Oh! Mumford\" comic strip that was insane absurdist meme-fuel. Might have to go to Internet Archive to unsurface those. reply modeless 8 hours agoparentMy favorite Point/Counterpoint is https://www.theonion.com/according-to-the-economist-nasa-is-... \"According To The Economist, NASA Is An Industrial Subsidy In Disguise vs. Oooh, Look At Me, I Read The Economist!\" I also have to give a shout out to the time they predicted the five blade razor a year before it came out: https://www.theonion.com/fuck-everything-were-doing-five-bla... reply Scarblac 3 hours agorootparentAs a nice crossover, the Economist predicted the number of blades to go to infinity in this beautiful graph: https://www.economist.com/science-and-technology/2006/03/16/.... reply imron 2 hours agorootparentAnd then there's this from 1992 - https://www.youtube.com/watch?v=YleuLyCUx28 reply kevinmchugh 6 hours agoparentprevThe point/counterpoint that I have thought about at least once a month for at least fifteen years: https://www.theonion.com/this-war-will-destabilize-the-entir... reply bitsinthesky 8 hours agoparentprevI have a stack about a foot tall of copies i collected from the Obama era. Anyone idea what i should do with them? Perhaps donate them to the library as a public good? They are still hilarious. My favorite is one where they pretended a Chinese ceo of a fish company bought the paper. It was filled with articles with bad statistics trying to get the common American to eat more fish, how wonderful the ccp is, and how weak and decadent the American people are. They also made the grammar terrible. Definitely riding the line with seeing what they could get away with! reply qingcharles 2 hours agorootparentI'm building a free magazine encyclopedia wiki. My email is on my profile. Happy to scan them and upload them if they are not already available in electronic format. reply pabs3 7 hours agorootparentprevYeah, or send them to archive.org for scanning and publishing on the web. reply bitsinthesky 2 hours agorootparentNice. Okay, I’ll give it a shot :) reply bitsinthesky 7 hours agorootparentprevAnd these fullpage scans provide value above what theonion.com already provides? reply Intralexical 3 hours agorootparentYou're never quite sure what you're actually looking at on the web. Was this edited at any point after the listed publish date? Did it survive intact during the backend migrations since? Is that formatting supposed to be a bit weird? Is there originally supposed to be an image after that oddly placed paragraph break? What did it look like back when `` and `` were still a thing? Did it look better at 800x600? Did it show the same page to all user-agents? Hard copies, physical reality (or at least rasters), are still the ultimate form of immutability. reply skyyler 7 hours agorootparentprevA lot of people will be interested in the ads and other signs of the times. reply ht_th 5 hours agorootparentprevBesides, who knows how long theonion.com will stay up. Or keeps giving access to all old articles? After all, they've just been bought. Usually this results in changes, despite new owner's loud promises that nothing will change. reply ajkjk 1 hour agorootparentWell in this case everyone wants something to change. The onion's glory days were in the past. reply OldGuyInTheClub 8 hours agoparentprevI learned about it via \"Monk Gloats over Yoga Championship\" https://www.theonion.com/monk-gloats-over-yoga-championship-... I also frequently mention I don't watch television. In fact, I don't even own one. https://www.theonion.com/area-man-constantly-mentioning-he-d... reply saalweachter 8 hours agorootparentFor me, it was \"Our Long National Nightmare of Peace and Prosperity Is Finally Over\": https://www.theonion.com/bush-our-long-national-nightmare-of... reply toyg 2 hours agorootparentThe article is funny and I agree with overall sentiment, but it's always surprising how people just forget Kosovo and Somalia. Clinton bombed a European country and doubled down on a hopeless mission in an African hellhole (before giving up), not exactly \"sustained peace abroad\". Even if you agree with the moves, you can't say it was all peace and love. reply jl6 2 hours agorootparentTrue, but those were tiny interventions that hardly affected mainstream USA at all, unlike the brutalizing experience of 9/11, Iraq, and Afghanistan in the following decade. reply OldGuyInTheClub 7 hours agorootparentprevThe foresight... still hurts to this day. Wish it had gone this way. From 1998: https://www.theonion.com/clinton-threatens-to-drop-da-bomb-o... reply romanhn 7 hours agorootparentprevYes, this was an incredibly prescient article. reply tootie 7 hours agorootparentprevThis one just keeps on aging like fine wine. reply hinkley 8 hours agoparentprevRed Meat still exists, but absolutely nobody knows what I mean when I say, \"I hate you, Milkman Dan\" and hasn't for 25 years. reply creamyhorror 3 hours agorootparentOh gosh Milkman Dan, what a blast from the past. All the good memories of early edgy net absurdism. reply bartc 8 hours agorootparentprev… … Is it still there? reply hinkley 8 hours agorootparentIt is. Don’t look. One of my favorites. reply kbenson 8 hours agorootparentprevI used to love that comic. I suspect I still will, but I'll know for sure after binge the 20+ years I missed. :) reply gcanyon 8 hours agorootparentprevI see you, Papa Moai! Taking a shot: Rehabilitating Mr. Wiggles? reply jonathankoren 8 hours agorootparentprevI loved Red Meat. I didn’t know it still existed, but I do have three of the books. I also have the complete Jim’s Journal, but alas I never got a t-shirt. “I went to class even though I didn’t real want to” was my favorite reply longdustytrail 6 hours agoparentprevThese are all great, another good one is “This War Will Destabilize The Entire Mideast Region And Set Off A Global Shockwave Of Anti-Americanism vs. No It Won’t” https://www.theonion.com/this-war-will-destabilize-the-entir... reply reducesuffering 4 hours agorootparentLike fine wine. We need a new 2024 one: Why Republicans Have Little Cachet Amongst the Educated In The Past 2 Decades A Total Fucking Mystery reply DoreenMichele 8 hours agoparentprevI don't think I'm the target audience. I don't really get it. I spent years homeless. I try to write about meaningful ways to help the homeless while preserving their agency. I try to write about systemic issues I feel contribute to the problem. I don't find that second link funny or relatable or anything that makes sense to me. reply SOLAR_FIELDS 8 hours agorootparentI don’t think any of the articles that grandparent linked are really the most exemplary Onion articles. The onion is really at its best when it manages to nail it exactly right. Sometimes they are just so perfectly in touch with cultural zeitgeist it’s uncanny. Here are two of the most memorable ones in recent memory: https://www.theonion.com/no-way-to-prevent-this-says-only-na... https://www.theonion.com/the-onion-stands-with-israel-becaus... reply justin66 1 hour agorootparent> ‘The Onion’ Stands With Israel Because It Seems Like You Get In Less Trouble For That This is a recent Onion classic. The thing about the Onion is, once you’ve got a great headline - and maybe a photo, which will usually be a head shot - the text of the article sometimes seems largely superfluous. They actually ought to be doing better in the age of Twitter and so on, but it didn’t work out that way. reply modeless 8 hours agorootparentprevFunny, to me these are the worst type of Onion articles, pandering to people on one side of a political issue. I think it's lazy and predictable. reply internetter 8 hours agorootparentThe onion has always been political commentary. In my opinion, it is some of the most impactful, even to this day. Every time there is a mass shooting, the onion reposts the same article — linked above. All they do is edit the time and place. They also bump every other article they published to the front page. It's incredible commentary on the fact these things keep happening in completely predictable ways. reply DoreenMichele 8 hours agorootparentprevOkay, I chuckled at the Ted Kaczynski signature, aka The Unabomber. I assumed all the other \"board member signatures\" were similarly infamous bad guys but I can't figure out what they all are and when I search on Steve Hannah, he apparently actually was the CEO of The Onion at one time. I agree with some of their points about it being a conflict going back hundreds of years. I personally think it's inevitable that war broke out given how psycho controlling Isreal is about Gaza's water supply. I still probably am not the target audience for The Onion and won't really understand a lot of it. I was full-time homemaker a lot of years for an American woman my age. I did well in school. I value HN and have participated here a lot, but have zero friends, professional contacts etc via HN. I don't generally \"go along to get along.\" I try to avoid social friction by other means and the result appears to be I don't ever really fit in anywhere because I'm not willing to mouth empty agreement with the group consensus. And, I mean, there's likely other reasons I fail to fit in anywhere but I strongly suspect that's a really large factor. reply webnrrd2k 7 hours agorootparentI really like the T. Herman Zweibel signature at the bottom. He was by far my favorite recurring character. Not the Onion, but a list of H. T. Zweibel articles: https://muckrack.com/t-herman-zweibel/articles reply DoreenMichele 7 hours agorootparentFYI if anyone else here is as clueless as I am: Publisher Emeritus, T. Herman Zweibel, writes frequent editorials, growing more and more erratic until he's removed from power by the board of directors in the 1950s. https://en.m.wikipedia.org/wiki/Our_Dumb_Century reply jimbokun 5 hours agorootparentprevDo you realize The Onion is a satirical comedy newspaper? reply DoreenMichele 5 hours agorootparentAbsolutely. reply bbarnett 3 hours agorootparentprevSorry what? As a Canuck, US politics is typically indecipherable, eh! Most news makes no sense, until I found The Onion. Now you are telling me, you bunch of hosers made it all up! Signed \"Bob Jenkins, Canadian Minister of National Affairs\" \"You made it all up! Ah, damn you! God damn you all to hell!”, Charlton Heston, a Great Canadian reply saagarjha 5 hours agorootparentprevWell, The Onion is not there to write about meaningful ways to help the homeless. Their job is to satirize the dumb views that people push. reply chrisweekly 5 hours agorootparentprevTangent: congrats on no longer being homeless! Your comment piqued my curiosity so I checked out your profile, and wow! is it great, full of interesting links etc. Glad you're here on HN. P.S. Yeah, the onion is pretty hit-or-miss, tho the ratio is pretty good IMHO. YMMV but one of my all-time favorites was about Harry Potter being a sinister secret recruiting vehicle for satan worship -- and I learned about it from a friend whose parents were among the huge number of christianists who thought it was legit journalism and forwarded it in one of the biggest ~early-internet-days chainmail events ever. https://www.theonion.com/harry-potter-books-spark-rise-in-sa... reply tootie 7 hours agorootparentprevAs with any comedy, they are hits a misses. And probably more misses than hits. But they just keep shooting and people mostly remember the gems. reply scoot 2 hours agoparentprevIgnoring for the moment that \"unsurface\" isn't even a word, I think you mean the opposite, \"surface\" (as a slightly pretentious way of saying \"discover\"). reply bag_boy 8 hours agoparentprevHoly shit lol that was sooo funny. “Never mind that Dr. Glickman screwed up and bought this colossal ditz of a receptionist more computer than she could ever possibly need for record-keeping at a small dentist's office.“ reply lapcat 9 hours agoprevI think this is good news, because G/O Media websites are the worst. I was at UW-Madison when The Onion started. We had a wealth of student newspapers at the time, along with The Daily Cardinal and The Badger Herald. Scott Dikkers and Todd Hanson, writers for The Onion, also wrote great comic strips for the Cardinal (Jim's Journal and Badgers and Other Animals, respectively). Some favorite Onion headlines that I remember: https://www.theonion.com/stretch-of-highway-learns-it-was-ad... https://www.theonion.com/sudanese-14-year-old-has-midlife-cr... https://www.theonion.com/christ-returns-to-nba-1819563859 And I can't find a link for it, but \"Special Olympics T-Ball Stand Pitches Perfect Game\". Incidentally, Jeremy Scahill of The Intercept was also writing for the student newspapers way back in the day, though he was obviously doing political stuff and not comedy. reply Gregordinary 4 hours agoparent\"Archaeological Dig Uncovers Ancient Race of Skeleton People\" https://www.theonion.com/archaeological-dig-uncovers-ancient... \"Trekkies Bash New Star Trek Fillm as Fun, Watchable\" https://www.theonion.com/trekkies-bash-new-star-trek-film-as... \"Jurisprudence Fetishist Gets off on Technicality\" https://www.theonion.com/jurisprudence-fetishist-gets-off-on... reply trimbo 6 hours agoparentprev\"Secondhand smoke linked to secondhand coolness\" \"Schaumburg Man Dimly Aware Of Shadowy, Non-Schaumburg World Out There\" \"Clinton Injected With Highly Unstable Experimental Growth Serum\" \"Beer! It kicks ass!\" Lastly, the 9/11 issue is one of their greatest achievements. They struck the perfect balance/tone, avoiding \"too soon\": https://www.theonion.com/issue-37-34-the-september-11th-issu... reply sharkweek 5 hours agorootparent“CIA Realizes It's Been Using Black Highlighters All These Years” https://www.theonion.com/cia-realizes-its-been-using-black-h... They also had one that they didn’t run making a joke right after 9/11 to the effect of… “America Stronger Than Ever, Say Quadragon Officials.\" Also the routine “No Way To Prevent This” headline after every mass shooting only continues to gain potency. reply jazzyjackson 5 hours agorootparent\"After Obama Victory, Shrieking White-Hot Sphere Of Pure Rage Early GOP Front-Runner For 2016\" https://www.theonion.com/after-obama-victory-shrieking-white... reply bozhark 5 hours agorootparentStrange, he won reply jasonkester 4 hours agorootparentprevThat’s not the 9/11 issue though. (Check the date). And strangely, I can’t seem to find a link to the issue that they published that morning. Which is a shame, because it was the perfect counterpoint to what everybody else was reporting. Just a full page photo of the plane hitting the tower and a giant headline: “HOLY FUCKING SHIT!!!” reply haroldp 3 hours agorootparentThat was the first real laugh after. It was just perfect. reply GeekyBear 6 hours agoparentprevSome personal favourites: Drugs win Drug War New President Feels Nation's Pain, Breasts Area Man Passionate Defender of What He Imagines Constitution to Be Awesome Toy Recalled Thanks to Three Stupid Dead Kids Ninja Parade Slips Through Town Unnoticed Once Again Alzheimer's Sufferers Demand Cure For Pancakes reply IanCal 3 hours agorootparentOn this style, I've loved Archaeological Dig Uncovers Ancient Race Of Skeleton People - https://www.theonion.com/archaeological-dig-uncovers-ancient... Study: Dolphins Not So Intelligent On Land - https://www.theonion.com/study-dolphins-not-so-intelligent-o... reply optimalsolver 1 hour agorootparentprevArea Man Always Nostalgic For Four Years Ago https://www.theonion.com/area-man-always-nostalgic-for-four-... reply nindalf 1 hour agorootparentNow and then some of their articles have a little too much truth to them. Go easy on us Onion folks! reply aklemm 5 hours agorootparentprev“Ringo Next” after George Harrison died. reply jasonkester 4 hours agorootparentprevSammy Hagar discovers second way to rock reply Teever 5 hours agorootparentprevMy personal favourite was when The Onion chose 'The Man' as their 1997 man of the year. https://www.theonion.com/the-onions-1997-man-of-the-year-181... reply labrador 5 hours agoparentprevWhen I was a young man I felt personally called out so much that I changed up the way I did things: \"I'll Try Anything With A Detached Air Of Superiority\" https://www.theonion.com/ill-try-anything-with-a-detached-ai... reply beastman82 8 hours agoparentprevMy favorite was \"Klemke wins!\" the day after Obama was elected. https://www.theonion.com/klemke-wins-1819570336 reply defrost 8 hours agorootparentIt would have been a vastly different story if only Cosgrove kept her mouth shut about access to the new all-weather Sedgwick County Park municipal pool. Pool politics have torn America apart. Here in Australia we have to make do with The Shovel https://theshovel.com.au/2024/04/24/rest-of-world-offers-tru... reply Daz1 6 hours agorootparentnext [3 more] [flagged] defrost 5 hours agorootparentIt would seem not. I did a quick household poll and, sadly, not one found your comment funny regardless of political affiliation, gender, or species :/ Even the dog yawned. reply simonh 6 hours agorootparentprevBritish conservative, I think it’s hilarious if sweary, but that’s just Australians. Outside the US conservatism and Trump support are, shall we say, loosely coupled. More British conservatives would prefer a Biden victory to a Trump one (40% to 31%). reply OldGuyInTheClub 8 hours agoparentprevAnother great God (sort of) returns: https://www.theonion.com/god-finally-gives-shout-out-back-to... And a brutal Special Olympics article: https://www.theonion.com/independent-investigation-special-o... reply saboot 1 hour agoparentprev\"Osama Bin Laden Found Inside Each Of Us\" https://www.theonion.com/osama-bin-laden-found-inside-each-o... reply wnevets 5 hours agoparentprevCan't me mention the T-ball one without also quoting > Little League Pitcher Just Getting Fucking Shelled reply KingOfCoders 6 hours agoparentprevMy favorite \"More American Workers Outsourcing Own Jobs Overseas\" https://www.theonion.com/more-american-workers-outsourcing-o... reply Dwedit 4 hours agoparentprevRight now, Kinja is hosting all the content for The Onion, so it would take a lot of data migration to get all the content onto a different web host. reply parpfish 7 hours agoparentprevAnd don’t forget that Dan savage worked at 4 star video at the time reply benzible 5 hours agoparentprevI was there too and I remember thinking that lots of universities must have humor newspapers as funny as The Onion. Nope! BTW Dan Vebber also wrote a strip for the Daily Cardinal and wrote for The Onion. He went on to write for Buffy and a lot of other shows, currently exec. producer of The Simpsons. He’s also the genius behind The Two Felipes https://www.mkepunk.com/releases/the-two-felipes-eat-your-fi... reply davelondon 4 hours agoparentprev\"Startling Report Finds Evidence Democrats May Have Attempted To Influence 2016 Election\" reply mixmastamyk 6 hours agoparentprevClinton Deploys Vowels to Bosnia https://www.ling.upenn.edu/~beatrice/humor/clinton-deploys-v... Cities of Sjlbvdnzv, Grzny to Be First Recipients reply bandyaboot 5 hours agorootparentThis is my personal favorite. reply jansan 5 hours agoparentprevIMO with the 9-11 special edition The Onion reached its peak. https://www.theonion.com/issue-37-34-the-september-11th-issu... reply ipython 8 hours agoprevI remember my first introduction to the onion was the print edition at my university. What a great publication- and they authored what has to be the best amicus brief to the Supreme Court ever. https://www.supremecourt.gov/DocketPDF/22/22-293/242292/2022... reply electrondood 4 hours agoparentI was aware of the brief, but until now had never taken the time to read it. It's probably the funniest amicus brief ever written. Thanks for linking. reply pants2 9 hours agoprevThis is the best birthday present I could have asked for. The Onion News Network has the funniest, most prescient videos online and I love going back to watch them. Even if the Onion comes back half as funny as they were it's going to be fabulous. Here are a few of my lesser-known favorites: https://www.youtube.com/watch?v=nJdP1zK15bE https://www.youtube.com/watch?v=TRgRz3nSG7o https://www.youtube.com/watch?v=XUT8ec24anM reply thwarted 4 hours agoparentI watch \"The Onion's Future News From The Year 2137\" regularly. You have to pause it to catch everything, it's packed with little bits of humor in the on-screen graphics. https://www.youtube.com/watch?v=iKC21wDarBo reply tkgally 7 hours agoparentprevOne of my favorite videos from that era: “Concentric Circles Emanating from Glowing Red Dot” https://youtu.be/8wHMaJ6AtNs?si=GMQSQDYXbTQfGEI9 reply nilram 5 hours agoparentprevExperts Agree, Giant Bioengineered Crabs Pose No Threat https://youtu.be/-Uq9pp586AE?si=NxdI4vwYZQr817QI Reporters having a panel discussion on teevee. I stopped the video and read each of the supporting print stories that flashed by and they were perfect, too. reply lelandfe 6 hours agoparentprevNew Live Poll Lets Pundits Pander To Viewers In Real Time https://www.youtube.com/watch?v=uFpK_r-jEXg reply jackcosgrove 7 hours agoparentprevNation's Girlfriends Unveil New Economic Plan: \"Let's move in together\" https://youtu.be/7ADncN9HIa4 reply monero-xmr 3 hours agorootparentIncredible, beginning to end reply tverbeure 9 hours agoparentprevThe scrollbar at the bottom always warrants a second viewing. \"Rescue St. Bernard eats avalanche victim\" reply jojobas 7 hours agoparentprevhttps://www.youtube.com/watch?v=Q4PC8Luqiws reply roughly 9 hours agoprevThis is great news. Jim Spanfeller and G/O media are pretty well known as a chop shop for media properties - there's not a single part of the former Gawker empire that's done even half-decently under their control. > In an email to G/O Media staff that was obtained by The New York Times, Jim Spanfeller, the chief executive, said the company was “undergoing an extensive review of our portfolio with the intention of coring down to our leading sites in terms of audience and revenues.” Gawker put a bunch of famous sites together, but The Onion is the only one that I'd expect anyone who wasn't terminally online to recognize. I'm not sure how that didn't make their list of \"leading sites\", but I'm grateful. (Also, Jim Spanfeller is a herb.) reply TheCleric 6 hours agoparentI can confirm, Jim Spanfeller is indeed an herb. reply 12_throw_away 8 hours agoparentprev(that dude is parsley, sage, rosemary _and_ thyme) reply Tanoc 7 hours agoparentprevThere's a bit of vicarious schadenfreude that Spanfeller's tactics were so horrific that nearly all the staff of G/O media from before 2020 abandoned ship and went on to create sites that are more successful than what the staff escaped from. Some even went on to be major journalists for massive publications, like Tyler Rogoway and Jason Schreier. The Onion has been the last holdout because it's a storied name, unlike Jalopnik or The Root, and now it's finally free. reply resolutebat 7 hours agoprevSince we're all posting our favorites, here's a few more: Situation in Nigeria Seems Pretty Complex https://www.youtube.com/watch?v=gEyFH-a-XoQ Prague's Kafka International Named Most Alienating Airport https://youtu.be/Pwom49awRKg reply creamyhorror 3 hours agoparentThe Nigeria video is a masterclass in subtle comedic scripting and acting. Old favourite. reply Wowfunhappy 10 hours agoprev> When asked whether he had purchased The Onion, Lawson played coy. \"What's The Onion?\" he replied. Then, \"What's a Tetrahedron?\" Business Insider was unsure how to respond to these questions. Sounds like The Onion is in good hands. reply tommica 2 hours agoprevOh, we are sharing our favorite onion bits? I love these ones: \"How To Channel Your Road Rage Into Cold, Calculating Road Revenge\" https://www.youtube.com/watch?v=vuKnR8RvxHY \"Prison Economy Spirals As Price Of Pack Of Cigarettes Surpasses Two Hand Jobs\" https://www.youtube.com/watch?v=K2IYIJc1f00 \"FDA Official: \"Just Eat A Goddamn Vegetable\" https://www.youtube.com/watch?v=BOyebcrVWb4 \"DEA Official Announces Successful Drug Bust On Son's Room\" https://www.youtube.com/watch?v=s2uqJ4xTx8k reply askl 1 hour agoparent\"Apple Introduces Revolutionary New Laptop With No Keyboard\" https://www.youtube.com/watch?v=9BnLbv6QYcA \"Markets In Turmoil As Price Of Money Skyrockets To $90 A Dollar\" https://www.youtube.com/watch?v=kaoejgnFFP8 reply alexey-salmin 2 hours agoparentprev\"How To Play Golf Against The Man Whose Wife You're Banging On The Side\" https://youtube.com/watch?v=PyV-oTTyIWg \"Parenting Expert Has Nerve To Tell You How To Raise Your Own Goddamn Kids\" https://youtube.com/watch?v=hKmDGWv9gRk \"Brain-Dead Teen, Only Capable Of Rolling Eyes And Texting, To Be Euthanized\" https://youtube.com/watch?v=MGXSPf9b-xI reply ghostpepper 5 hours agoprevSince everyone is just posting their favourite headlines: Report: Average Male 4,000% Less Effective In Fights Than They Imagine What makes the onion writing truly great is how they discipline themselves to stick to a single bit for a given article. The entire punchline is in the headline, and the rest of the article is about as straight-laced as you could imagine, as if it was written by a real reporter in a world where the absurd thing was not unusual at all. It's a sort of written version of a deadpan delivery. reply jamiek88 3 hours agoparentThat’s why the Colbert Report was so good as well. reply atleastoptimal 9 hours agoprevThe Onion was great back when their Youtube channel created real videos. Now it's a mild chuckle every few months reply modeless 9 hours agoparentThey had amazingly high production values on classics like \"Apple Introduces Revolutionary New Laptop With No Keyboard\" https://www.youtube.com/watch?v=9BnLbv6QYcA and \"Sony Releases Stupid Piece Of Shit That Doesn't Fucking Work\" https://www.youtube.com/watch?v=8AyVh1_vWYQ I wonder why they stopped. reply jkestner 9 hours agorootparentFrom what I gather, they invested in Onion News Network for IFC, and when that got canceled relatively quickly, they didn't risk online-only distribution. Maybe 2011 was too early to pivot to video? https://en.wikipedia.org/wiki/Onion_News_Network My favorite bit, still stuck in my head whenever I see politicians from both parties rush to the scene of a protest or whatever, is \"New Live Poll Lets Pundits Pander To Viewers In Real Time\": https://www.youtube.com/watch?v=uFpK_r-jEXg reply Talanes 4 hours agorootparent>Maybe 2011 was too early to pivot to video? The whole pivot to video era that crippled a whole bunch of online writing outlets was still four years out, so definitely too early. reply dralley 9 hours agorootparentprevI assume video content is much more expensive They may also have been victimized by the scandal where Facebook was lying about the engagement that short-form videos generated over other kinds of content, leading several content producers into bankruptcy or near bankruptcy through loss of ad revenue when they pivoted towards it. https://slate.com/technology/2018/10/facebook-online-video-p... https://www.ccn.com/facebook-lied-about-video-metrics/ reply actionfromafar 3 hours agorootparentNever trust a con artist. reply jenny91 9 hours agorootparentprev~1:40 of the Apple video has this amazing headline scroll through the bottom: > Congress approves intellectual stimulus package giving every American a graduate degree reply PyWoody 9 hours agorootparentprevHP Offers 'That Cloud Thing Everyone Is Talking About' https://www.youtube.com/watch?v=9ntPxdWAWq8 reply swyx 9 hours agorootparentprevpepperidge farm remembers when the onion produced hilarious articles. lets collate the best of the best. i'll start: https://www.theonion.com/protagonist-scrolls-intensely-throu... https://www.theonion.com/jumbled-nest-of-cords-makes-move-to... reply dyno12345 8 hours agorootparentprevI think they raised a bunch of funding at one point and then ran out of it reply karaokeyoga 9 hours agorootparentprev“EVERYTHING is just a few hundred clicks away” reply Avshalom 9 hours agoparentprevBen Collins says they're bring back ONN https://bsky.app/profile/bencollins.bsky.social/post/3kqyj7u... reply culturthrowaway 8 hours agorootparentThis is a splash of cold water if you're familiar with who Ben Collins is. This will not not be a return to The Onion you fondly remember. Most likely it will be a continuation in the same direction it's been going. reply afavour 7 hours agorootparentOK, I’ll bite: why? I don’t know who Ben Collins is and at a first read of his bio I’m not really sure who would. My first instinct is that this is one of those terminally online things, like how I kept reading references to “Taylor Lorenz” as if she were the devil incarnate and when I finally looked into it she was just another pretty boring journalist. reply starmftronajoll 6 hours agorootparentSince you asked, a genuine answer. I suspect it's because Ben Collins is a veteran of the left-wing \"disinformation\" beat, most recently for MSNBC until he lost that gig -- so the admittedly cryptic parent comment is positing that as a seasoned culture warrior, Collins will likely apply The Onion as a tool in the culture war. The Onion's comedic point-of-view used to be less predictable, but as the culture war as progressed, so too has The Onion's voice, and it's a more reliable voice for the left these days. If you rue that trend, for whatever reason, Collins provides little reason to imagine things will change. reply jzebedee 7 hours agorootparentprevI'm not familiar enough with either him or culture war dynamics to know what you mean by this. reply squabbles 6 hours agorootparentprevOn the other hand he does have experience in the field. reply chomp 8 hours agorootparentprevBen Collins, the guy who got suspended from MSNBC for criticizing Elon Musk too hard? Upset Nate Silver because he criticized him too hard? Upset Matt Taibbi for making fun of him for doing PR work for a billionaire? I think he’s perfect for the job honestly. reply thrownawayfray 3 hours agorootparentIf _MSNBC_ thinks he’s too overtly partisan and not up to journalistic standards, I’d say Collins fails to clear the absolute lowest bar in news media ethics. reply tootie 7 hours agorootparentprevI was genuinely excited to see him be the new boss. He's incredibly smart and incisive and really understands internet culture. reply dontupvoteme 9 hours agoparentprevThey were great when they weren't shackled 'Little Boy Heroically Shoots, Mutilates Burglar' 'Black Man given Nation's worst job' (After Obama won) I've never laughed so hard at headlines reply pants2 9 hours agorootparentAnd who can forget their famous headline \"Jurisprudence Fetishist Gets Off On Technicality\" reply hyperhello 9 hours agorootparentStandard Deviation Not Enough for Perverted Statistician reply paulsmith 8 hours agorootparentThis, this is the GOAT. reply mangosteenjuice 9 hours agorootparentprevThe Onion had two events that resulted in a huge drop in writing quality and edginess: OG writers refusing to relocate from NYC to Chicago in 2012 when the HQ was moved, choosing to part ways instead. Then, the remaining writers being called out for the following tweet, somewhat rightly, but being neutered forever after: https://crasstalk.com/2013/02/sorry-former-onion-staffers-ca... reply Nition 8 hours agorootparentI don't know any of the context around that tweet, but The Onion has obviously chosen the most innocent, blameless celebrity they could think of to make sure it's a clear joke to everyone, right? Like finding the skinniest person and calling them obese. The writer of that article seems to have still somehow misunderstood and come to the conclusion that The Onion actually hates Quvenzhané Wallis. I wonder if they really don't get it, or if they're just sort of performatively not getting it for the outrage article. reply justin66 28 minutes agorootparentOn the contrary, the context was that the kid was getting a lot of very stupid criticism already. It’s certainly why they attempted the joke. I don’t think anything about that situation was somehow off limits or immune to satire, they just didn’t pull off the joke. It’s not like the kid was likely to have seen the joke, so no worries there. The problem is that when you use language like that and you’re not even funny, you’re operating at the Andrew Dice Clay level of comedy. That should inspire suicidal ideation in any comedian (who isn’t Andrew Dice Clay… I guess…). reply justin66 41 minutes agorootparentprevIt seemed to me they were well into the current era of lessened relevance before that stuff happened. reply ddingus 8 hours agorootparentprevYes, both sad days for what was razor sharp satire. reply astrange 8 hours agorootparentprevThey also seem to think their Diamond Joe Biden character made real Joe Biden too popular, and their self-seriousness about this is making it hard for them to do anything political. reply benstein 9 hours agorootparentprev\"Special Olympics T-Ball Stand Pitches Perfect Game\" is the greatest headline ever written reply unzadunza 9 hours agorootparentMy vote goes to “Supreme Court Overturns ‘Right V. Wrong’” reply theremin808 1 hour agorootparent\"Area Bassist Fellated\" reply swyx 9 hours agorootparentprevdid you misremember the second one? https://www.theonion.com/black-guy-asks-nation-for-change-18... reply kevinmchugh 6 hours agorootparentThey did not. The one you linked is from March 2008. https://www.theonion.com/black-man-given-nations-worst-job-1... reply OJFord 9 hours agoparentprevYou're clearly not alone in this, but I never realised they even did videos, fwiw. Similar era I was enjoying great content on the site, and it just sort of faded but didn't disappear, from my perspective, so I don't know to what extent it's purely about rise and fall of video really. (I did just watch the very good MacBook Wheel one though.) reply vundercind 9 hours agoparentprevI think they’re great and I’ve only seen like two or three of their videos ever. reply astrange 8 hours agorootparentTheir Autistic Reporter series is probably the best thing they did. reply dj_gitmo 9 hours agoparentprevThe was the greatest legacy of the ill-fated “pivot to video” era. reply KittenInABox 9 hours agoparentprevAccording to the new buyer, he is letting the writers do what they want (more freedom), so I assume they were increasingly tamped down on reply tims33 5 hours agoprev\"Dad Suggests Arriving At Airport 14 Hours Early\" is my all-time fav. https://www.theonion.com/dad-suggests-arriving-at-airport-14... reply jakedata 9 hours agoprevMissing from the Internet is \"The Onion Guide to Actual Reality\", a Flash based interactive feature from the 90s. It was brilliant and prescient like all good Onion features but it may never be ported to Ruffle or whatever else can render that content. Maybe Global Tetrahedron will invest in preserving some of that history. Also, le boy picked up my copy of Our Dumb Century and read it cover to cover a few years ago. I was pleased. reply cm2012 9 hours agoprevThe onion got really unfunny because they got too moralizing on their pet issues. Hopefully they can fix that. reply snowwrestler 7 hours agoparentExactly! Comedians aren’t funny when they have opinions. They’re only funny when they tell jokes that are carefully calculated not to make anyone upset about anything. reply djaykay 6 hours agorootparentI’d say there’s a difference between expressing an opinion, and moralizing about it. One can make great comedy with the former, while the latter sucks because the comedy is completely subjugated by the message. It’s the same difference between heavy metal and Christian metal. Same instruments, same basslines, etc., except the latter is trying to push you towards belief, and thus it’s annoying. reply saagarjha 5 hours agorootparent…what exactly do you think heavy metal is? reply defrost 4 hours agorootparentHilarious: Mac Sabbath Pair-a-Buns https://www.youtube.com/watch?v=p7kFfLRcHjU Playful: Leo Moracchioli feat. Mary Spender Sultans of Swing https://www.youtube.com/watch?v=x0RV0kgdqJU It's not like the Church of Satan is actually pushing an agenda to make anybody believe in an actual Satan. reply hinkley 8 hours agoparentprevWas that before or after Truth became stranger than Fiction? reply internetter 8 hours agoparentprevthe onion has always been political commentary? reply ametrau 6 hours agorootparentYeah but it wasn't so ham fisted / power to truth. reply rmason 2 hours agoprevHere's Jeff Lawson answering why he bought The Onion https://twitter.com/jeffiel/status/1783674204820262958 reply latexr 1 hour agoparentPeople without a Twitter account can’t read past the first post, which doesn’t explain anything. https://threadreaderapp.com/thread/1783674204820262958.html reply simonw 8 hours agoprevDid they buy the A.V. Club too? That went down hill enormously a few years ago, would be great to see that regain its former glory. reply r721 7 hours agoparentWikipedia says: >G/O Media sold The A.V. Club in March 2024 to Paste Magazine. Terms of the deal were not disclosed. https://en.wikipedia.org/wiki/The_A.V._Club reply jakedata 9 hours agoprevAlso \"SICKOS\" I tried to explain to my wife why I bought that shirt. No comprende... (YES... HA HA HA... YES!) reply Affric 1 hour agoparentThem, after I show them the comics: “and so you bought that shirt?” reply internetter 9 hours agoprevgift link: https://www.nytimes.com/2024/04/25/business/media/the-onion-... reply prpl 4 hours agoprevI liked the early video stuff too, the Onion News Network - especially the - “Sony Releases Stupid Piece Of Shit That Doesn't Fucking Work” video reply RustyRussell 8 hours agoprevI still refer people to the classic film review of The Sound of Music: https://www.theonion.com/the-onion-looks-back-at-the-sound-o... reply ChrisArchitect 8 hours agoprev[dupe] Some more in the Business Insider article which is even more on point because they weren't \"sure\" if it was real: https://news.ycombinator.com/item?id=40162523 reply labster 10 hours agoprevArea man suckered into purchase of declining fake news website reply extraduder_ire 9 hours agoparentI expected the site to have a story about grocery inflation gone wild, remarking at the absurd cost of a single root vegetable. reply gneray 9 hours agoparentprevWell done reply glokta 12 hours agoprevI hope they bring back T. Herman Zweibel. Always found his editorials edifying and educational. reply gadders 1 hour agoprevOh good. It might get funny again. reply adultSwim 9 hours agoprevBreaking News: Give Us $1 Or 'The Onion' Disappears Forever https://www.theonion.com/give-us-1-or-the-onion-disappears-f... reply knodi123 7 hours agoparentI tried but the checkout insisted that my phone number (that I've had for 20 years) was invalid. Oh well! reply LargeWu 9 hours agoparentprevI paid them a dollar just to see what happens. reply hinkley 8 hours agorootparentWell, of course he's not gonna go away, Mary! You give him a dollar, he's gonna assume you got more! reply swyx 9 hours agorootparentprevwell? reply LargeWu 8 hours agorootparentWell, I have one dollar less than I did before. reply internetter 8 hours agorootparentprevThis: https://shottr.cc/s/17JR/SCR-20240425-umk.png reply rmason 13 hours agoprevJeff Lawson has state of Michigan connections. A University of Michigan grad he started a digital textbook company, but was just too early. Then he was founding CTO of StubHub which is based in Detroit. I regret never actually meeting him though we have some mutual friends in common. Tried to get him to speak at a hackathon I was running in Detroit during his early days at Twilio but he was simply too busy which I can understand. Got Dug Song, cofounder of Duo Security, and he did a terrific job of inspiring all of us. reply tanaros 8 hours agoparentWhy is Michigan relevant to buying The Onion? reply rmason 3 hours agorootparentIt is not, just sharing some information about the guy's background a lot of folks might not know. Might explain why he didn't insist in moving them from Chicago and relocating them to the West coast. reply jrflowers 6 hours agorootparentprevhttp://michiganonion.com/ reply iszomer 6 hours agoprevStill can't get over \"The Sony P.O.S. that doesn't goddamn do what it's supposed to\" video.. reply captcanuk 9 hours agoprevAlright, stop what you're doin' 'cause I'm about to ruin The image and the style that you're used to All you had to do was give Jeff a chance And now I'm gonna do my dance And to the Onion readers Peace and Humptiness forever reply alberth 4 hours agoprev35 Sponsored (Paid) Articles. That's how many paid articles are on the front page. That's insane. reply khazhoux 9 hours agoprevThe funnest fun fact about The Onion is that it was started by the guy that did Jim’s Journal comic strip, back in the mid 90s. reply lapcat 9 hours agoparentThat's not a fact. Scott Dikkers didn't start The Onion, though he was a very early employee: https://en.wikipedia.org/wiki/The_Onion#Madison_(1988–2001) Jim's Journal started in the late 80s, by the way. reply defrost 8 hours agorootparentFact adjacent comments SLAMMED by critic on Internet!! (No beef, here, just very hard to resist in context - mea culpa for my frivolity) reply khazhoux 7 hours agorootparentprevSorry, but it's now an internet fact: > Jim's Journal is a comic strip written and drawn by Scott Dikkers, co-founder of The Onion https://en.wikipedia.org/wiki/Jim%27s_Journal Take it up with Jimmy Wales. reply jamestimmins 9 hours agoprevTBD if this works, but this is 100% the kind of thing I want billionaires doing. More expensive, weird projects that are essentially public art. reply coloneltcb 13 hours agoprevthis is one of those stories where you first hear it, you stop and think for one beat and then you say \"of course, what an obvious move\" reply salad-tycoon 8 hours agoprevWell, not like the onion could get any worse (frankly) so I’m hopeful. I think this world would do fine with a bit more self depreciating and satirical humor. reply HeatrayEnjoyer 3 hours agoparentWhat's bad about The Onion? I've always enjoyed their work. reply gedy 9 hours agoprevAnyone aware of an archive for each of their stories? Not the entire website/home page archive. Would love this in markdown format for an eInk story-a-day, etc. reply afavour 7 hours agoprevBring back more millionaires doing eccentric things with their money, please. reply paulpauper 9 hours agoprevI guess it's cheaper than a real news site, and is there any difference anymore? Babylon Bee has become defacto news now. I don't mean this in terms of blaming the left or the right, but just that it's hard for any entity to have any monopoly on the truth anymore. Mainstream, established news sites have to compete against AI-generated content and influencers in the 'news space', which has blurred the lines between truth or fiction or made it hard to know what is real or not. Sometimes stories can be true and and yet undisguisable from 'fake' news, hence why r/nottheoinion is a thing. I have seen people on twitter post literal fake news, and it goes viral until later debunked or deleted, which can still take hours even with community notes. Even reputable news sites are still occasionally forced to amend or even outright retract stories. When something chaotic and unexpected happens for which narration is unreliable, like a school shooting, who is right? It's just speculation for the first hour or so until something approaching 'truth' or an 'official narrative' coalesces or congeals. it's be awesome if the three ppl who downvoted my post could elaborate how i was wrong . It is possible I am way off and someone can guide me to the truth that I am missing here. I think communication is helpful for resolving differences and maybe someone can communicate how I erred that would be more helpful. Thanks. reply modeless 6 hours agoparentI wish that there was an equivalent to The Onion with a different political slant, but the quality of the writing in all of the Onion copycats is just plain bad. The Babylon Bee is awful. reply spencerflem 3 hours agoparentprevI downvoted because: 1. Babylon Bee has like 1 good article for every 20-30 lame ones and in general spreads anti-trans hate. It is not news & 2. fake news is a problem, but fake news doesn't make the onion no different from a real news site. that's silly reply orf 3 hours agoparentprevThe Babylon Bee has got to be the most braindead thing I’ve stumbled upon in a long time. reply xippy 16 minutes agorootparentIt has some quite amusing headlines on its front page right now, very Onion-like: Hamas Thanks College Student Supporters By Promising Them A Quick Death During Global Intifada College Student So Caught Up Harassing Jews He Forgot About Term Paper On Inclusion This is one of my favorites from recent years, both funny and really gets to the point: NCAA Swimming Champ Caught In Possession Of Performance-Enhancing Testicles reply bhouston 9 hours agoprevThe Onion is still hilarious. reply alberth 8 hours agoprev [–] He wants to be Marc Benioff so bad. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Onion, a satirical news site, has been acquired by Global Tetrahedron from G/O Media, aiming to enhance user experience and dive into multimedia while maintaining the existing staff in Chicago.",
      "G/O Media's decision to sell The Onion reflects its focus on key platforms.",
      "The new owners, Jeff Lawson and Ben Collins, renowned in digital media, are determined to uphold and elevate The Onion's quality as loyal fans of the publication."
    ],
    "commentSummary": [
      "The focus is on The Onion, a satirical news outlet, and its acquisition by different parties, raising issues of reporting bias, cultural imperialism, and voter suppression in the US.",
      "Discussions delve into debates on humor, relevance, and ethical considerations, alongside The Onion's influence, notable articles, and shift towards video content.",
      "The post also touches on comparisons with other satirical news platforms such as The Babylon Bee."
    ],
    "points": 262,
    "commentCount": 214,
    "retryCount": 0,
    "time": 1714086674
  },
  {
    "id": 40160331,
    "title": "IBM Acquires HashiCorp for $6.4 Billion Amid Corporate Shift Away from Open Source",
    "originLink": "https://www.jeffgeerling.com/blog/2024/corporate-open-source-dead",
    "originBody": "April 25, 2024 IBM is buying HashiCorp for $6.4 billion. That's four months after HashiCorp rugpulled their entire development community and ditched open source for the 'Business Source License.' As someone on Hacker News pointed out so eloquently: IBM is like a juicer that takes all the delicious flavor out of a fruit skywhopper replied: HashiCorp has done a good job of pre-draining any flavor it once had. Some people wonder if HashiCorp's decision to drop open source was because they wanted to juice the books for a higher price. I mean, six billion dollars? And they're not even a pointless AI company! This blog post is a transcript of the video I posted today, Corporate Open Source is Dead. You can watch it on YouTube. Meanwhile, Redis dropped the open BSD license and invented their own 'Source Available' license. And last year, I covered how Red Hat found a way to just barely comply with the open source GPL license for their Enterprise Linux distro. Other companies like MongoDB, Cockroach Labs, Confluent, Elasticsearch, and Sentry also went 'Source Available'. It started with some of the smaller players, but as rot sets in at even the biggest 'open source' companies, open source devs are choosing the nuclear option. When a company rug pulls? Fork 'em. Literally! Terraform, HashiCorp's bread and butter, was forked into OpenTofu, and adopted by the Linux Foundation. Companies who built their businesses on top of Terraform quickly switched over. Even juicier, OpenBao—a fork of HashiCorp's other big project Vault—is backed by IBM! What's going to happen with that fork now? At least forks seem pretty straightforward in Hashi-land. In the wake of Redis' wanton destruction, it seems like there's a new fork every week! And some developers are even exploring ditching the Redis code entirely, like redka's an API-compatible wrapper on top of SQLite! After Red Hat closed its door—most of the way, at least they didn't try pulling a switcheroo on the license itself! Oracle, SUSE, and CIQ scrapped together the OpenELA alliance to maintain forks of Enterprise Linux. And CentOS users who'll be left in a lurch as June marks the end of CentOS 7 support have to decide whether to use AlmaLinux or one of the ELA projects now. All these moves shattered the playbook startups and megacorps used—and now we're seeing, abused—to build up billions in revenue over the past decade. It was all in the name of 'open source'. As free money dries up and profits slow, companies slash headcount almost as fast as community trust. 2024 is the year corporate open source died 2024 is the year Corporate Open Source—or at least any remaining illusions about it—finally died. It's one thing to build a product with a proprietary codebase, and charge for licenses. You can still build communities around that model, and it's worked for decades. But it's totally different when you build your product under an open source license, foster a community of users who then build their own businesses on top of that software, then yoink the license when your revenue is affected. That's called a bait-and-switch. Bryan Cantrill's been sounding the alarm for years—yes, that Bryan Cantrill, the one who posted this gem: Brian's presentation from 12 years ago is worth a watch, and the bottom line is summed up by Drew DeVault: [Contributor License Agreements are] a strategy employed by commercial companies with one purpose only: to place a rug under the project, so that they can pull at the first sign of a bad quarter. This strategy exists to subvert the open source social contract. By working on a project with a CLA, where you sign away your code, you're giving carte blanche for the company to take away your freedom to use their software. From a company's perspective, if they want CLAs or if they want to use an anti-open-source license, they do not care about your freedoms. They're protecting revenue streams. They'll often talk about freeloaders, whether it's Amazon building a competing hosted solution, or some startup that found a way to monetize support. But in the end, even if you have GPL code and you charge people to get it, it's not truly free as in freedom, if the company restricts how you can use, modify, and share the code. But there's a distinction here, and I know a few people watching this are already yelling at me. There's \"free\" software, and there's \"open source.\" People in the free software community correctly identified the danger of calling free software 'open source.' I don't think we have to be so dogmatic about it, but there is a fundamental philosophical difference between the free software community, with organizations like the Free Software Foundation and Software Freedom Conservancy behind it, and the more business-oriented 'open source' culture. Open source culture relies on trust. Trust that companies you and I helped build (even without being on the payroll) wouldn't rugpull. But time and time again, that trust is shattered. Is this slow death of corporate open source bad? Well, it's certainly been annoying, especially for devs like me who felt connected to these communities in the past. But it's not all bad. Why it's not bad for corporate open source to die In fact, this could be a huge opportunity; what happened to the spunky startups like Ansible, HashiCorp, Elasticsearch, or Redis? They were lighting their industries on fire with great new software. What happened to building up communities of developers, crossing cultural and economic barriers to make software that changed the world? There are still projects doing that, but so many succumb to enterprise money, where eye-watering amounts of revenue puts profit over philosophy. But as money dries up, as more developers get laid off after the insane hiring trends of the past five years, maybe small dev teams can move the needle. The AI bubble hasn't popped yet, so some great people are getting sucked into that vortex. But someone else could be on the cusp of the next great open source project. Just... don't add a CLA, okay? And it's not just devs; big companies can join in. Historically bad players like Microsoft and maybe even Oracle—man, it pains me to say that. They've even made strides in the past decade! IBM could even mend some wounds, like they could reunite OpenTofu and Terraform. There's precedent, like when IO.js merged back into Node.js after a fork in 2015. People asked what Red Hat could do to get me interested in Enterprise Linux again. It's simple: stop treating people who don't bring revenue to the table like garbage. Freeloaders are part of open source—whether they're running homelab or a competing business. Companies who want to befriend open source devs need to show they care about more than just money. Unfortunately, the trend right now is to rugpull to juice the quarterlies, because money line always goes up! But you know what? I'd just prefer honesty. If revenue is so dependent on selling software, just... make the software proprietary. Don't be so coy! But to anyone who's not a multi-billion dollar corporation, don't be a victim of the next rugpull. The warning signs are clear: Don't sign a CLA. Stay away from projects that require them. Stick to open source licenses that respect your freedom, not licenses written to juice revenue and prep a company for a billion-dollar-buyout. Maybe it's time for a new open source rebellion. Maybe this time, money won't change company culture as new projects arise from the ash heap. Maybe not, but at least we can try. Further reading Cracks are showing in Enterprise Open Source's foundations I'm done with Red Hat (Enterprise Linux) Fork Yeah! Examining open source history after Red Hat's move open source ibm red hat cla freedom programming youtube video Add new comment Comments Jacques Marneweck – 7 hours ago Jeff, you have misspelt Bryan Cantrill's name. Reply Jeff Geerling – 7 hours ago In reply to Jeff, you have misspelt… by Jacques Marneweck Gah! Sorry about that, fixed the spelling in the blog post. Reply Jeff Strupp – 5 hours ago I would say that this starts at the top - Where the Linux Foundation and other entities have been usurped by Corporations like Microsoft who want to control Open Source and direct the money towards their interest. Reply me – 1 hour ago Got lost in all your ranting there. At some point somebody has to pay 'something' for the labor and other expenses including legal. There's a point where free doesn't keep the lights on. Confused which project(s) meet your idea of purity. Perhaps Sqlite (https://www.sqlite.org/copyright.html) or are you thinking more like how Nginx seems to be semi-open if I remember correctly. Some features require $$$, or they used to be that way some years ago.... Reply Jeff Geerling – 50 min ago In reply to Got lost in all your ranting… by me Drupal comes to mind (this blog runs on it). Instead of one company trying to hoover up all profits associated with it, there is a fairly wide ecosystem of companies that work together on it, many of them employing one or more full-time open source devs. \"A rising tide lifts all ships,\" and I don't think it or Linux would be better had they adopted a license other than GPLv2. Drupal's had its own failures, but licensing was not one of them! Reply",
    "commentLink": "https://news.ycombinator.com/item?id=40160331",
    "commentBody": "Jeff Geerling: Corporate Open Source Is Dead (jeffgeerling.com)214 points by ngetchell 16 hours agohidepastfavorite132 comments Pannoniae 13 hours agoNone of these blogposts (including this one) have any realistic solution to the problem of making OSS software and being able to live from it, and prevent others from exploiting you in the process. Hyperscalers like Amazon exploit OSS projects by reselling them as a cloud service and they earn a gigantic sum in the process. But this is not a neutral thing to do - the OSS project is still responsible for maintenance! (And in many places, the \"no warranty\" clause seems completely disregarded - users and corporations demand bugfixes since it's a \"critical library\") The most telling sentence is \"Open source culture relies on trust. Trust that companies you and I helped build (even without being on the payroll) wouldn't rugpull.\"... where is any trust in exploiting someone's work without giving anything back? the hyperscalers routinely break the OSS social contract, but because they abide to the letter of the licences, they get a free pass and many white knights from even the OSS community and even OSI itself. A business model of \"you can see the source, you can modify it but you can't offer it as a service or resell my work\" is much more honest and trustworthy than the \"develop a library, a cloud service picks it up then pressures you with PRs and issues until you permanently burn out from the whole thing\" This is partly addressed by the post - \"But you know what? I'd just prefer honesty. If revenue is so dependent on selling software, just... make the software proprietary. Don't be so coy!\". This is not honesty though. Claiming that anything not party-approved.... I mean OSI-approved is not open source and it's proprietary is a very myopic thing. For users and developers, it's much more beneficial if they can see or even modify the source even if they don't have an unrestricted right to use and modify it however they want. This absolutist, black-and-white approach could potentially lead to many pieces of software becoming fully proprietary, all-rights-reserved in the future since the open source community harasses source available projects quite frequently, and not many have the patience to put up with that. And that would be a sad outcome indeed for user freedom, repairability, portability and other values RMS and the FSF dearly holds. reply sangnoir 12 hours agoparent> None of these blogposts (including this one) have any realistic solution to the problem of making OSS software and being able to live from it... Should OSS solve that \"problem\"? Free software predates billion-dollar OSS / open-core companies, it also doesn't require the maintainer directly earn a living from it. Perhaps the era of OSS megacorps and the concomitant VC/startup dreams is setting, as these large, formerly OSS companies are rejecting OSS licenses in favor of bigger profits. To that, I say good riddance, not every useful git repository has to be incorporated. > ...and prevent others from exploiting you in the process. The whole point of free and open source software is anyone can use it and improve it - including Amazon and the person you hate. An OSS license is not a growth-hack for adoption: if you want to discourage Amazon from using your product, use AGPL. If you're worried others won't use your AGPL software, then you're growth-hacking. FL/OSS will be fine between evening/weekend hackers (xz), small-to-medium companies/consultancies (Rails), and our neo-feudal tech overlords (React). reply Pannoniae 12 hours agorootparent>Should OSS solve that \"problem\"? If you don't want a cyberpunk-like dystopia of megacorps controlling everything, yes. This is not just affecting the VC-funded startups' profitability but it also affects ordinary software developers' lifestyle businesses too. Sure, the core principles of OSS don't require that you'd be able to earn money from your work, but then perhaps it is OSS which is not fit for purpose, not the desire to earn money from creating software. >if you want to discourage Amazon from using your product, use AGPL. The AGPL is still not restrictive enough because you can resell the software without contributing anything back if you don't modify it. Stuff like the SSPL or the Commons Clause are much better to prevent that kind of exploitation. >FL/OSS will be fine between evening/weekend hackers (xz), small-to-medium companies/consultancies (Rails), and our neo-feudal tech overlords (React). I find the xz example especially funny because that shows how it is very much not fine. Single developer, burnt out, not getting paid a penny for his thankless work, a bunch of state-sponsored actors pressure him to hand his project over -> backdoor injected. If Lasse Collin could get support for his project, this almost certainly would not have happened. https://xkcd.com/2347/ is very relevant here. reply prepend 12 hours agorootparent> The AGPL is still not restrictive enough because you can resell the software without contributing anything back if you don't modify it Why would you want to restrict reselling? If you want free software then you’re going to have reselling. If you don’t want reselling, then don’t make your software free/open source. I’m not sure why people are getting mixed up. Anyone can make proprietary software and restrict all they want. There’s tons of commercial software. But it doesn’t have the features of free/open source. reply Pannoniae 12 hours agorootparent>Why would you want to restrict reselling? Depends on what your philosophy is. You believe in the software commons and the free sharing of software? You would want software to remain as a public good so no one can profit from it exclusively. Are you a business who made some software? You don't want others to resell it because it's your work, you made it and you don't want others to freeload. >If you don’t want reselling, then don’t make your software free/open source. Sure, and the consequences of this will be much more all-rights-reserved proprietary software. A huge loss to users and developers. >But it doesn’t have the features of free/open source. Yes it does. Just because it doesn't conform to all tenets of the OSI definition, many useful things are still kept. The most obvious is the right to view the source code - which is highly liberating in an age where many vendors lock their systems down or try to offer everything as a SaaS where you don't even see the binaries. You usually also have the right to modify the software under most of these licences - you are the user but you find a bug in the software, you don't need to wait for the vendor to fix it, you can fix it and share your modifications to other users (either paid or not). You can also have the right to contribute changes if contributions are allowed, which is also a positive for everyone. There are many freedoms which can be granted without necessarily going for a maximalist open-source approach but still preserving the right to exploit your own work and prevent others from exploiting it. reply sangnoir 12 hours agorootparent> Depends on what your philosophy is. You believe in the software commons and the free sharing of software? You would want software to remain as a public good so no one can profit from it exclusively. > Are you a business who made some software? You don't want others to resell it because it's your work, you made it and you don't want others to freeload. I cant help but notice that you assume everyone uses money as a measure of desirable/undesirable outcomes. Whenever I publish a library/project with a GPL license (my preferred license), it's so that others can use it. I have a well paying job that's enabled in part by others sharing their work[1], so I let the \"freeloaders\" have at it - I too am one, after all. At the time I release the software, my problem is adequately solved: I already did the work , maybe someone else may benefit. Creating/sharing value with others is the point - I don't mind if someone else sells it : if they abide by the license, that means I can sell something identical for half of whatever they sold it for if I suddenly felt competitive. 1. I'm amazed that I can use Linux, git, hypervisors, orchestrators, various computers and interpreter, libraries, ops tools and more all for free,and with the ability to pry them open and make any changes I want! How frigging cool is that?! I'm compelled to pay it forward in any way I can. reply Pannoniae 11 hours agorootparentIf money is not important, what's the harm in making it available as open source, but with a non-commercial clause? reply sangnoir 9 hours agorootparentBecause I don't mind people using my software commercially if they are respecting the license. I wouldn't want to be charged for all my commercial usage of Linux, Git, Postgres, zsh, the rust toolchain, etc. I believe if everyone in our industry tried to capture all/most of of the economic value of their work, we'd all be poorer for it, metaphorically and literally since a huge part of high SDE salaries is due to high productivity enabled by the open source ecosystem. We are all freeloaders, to varying degrees. Without FL/OSS, most of these source-available companies would never have gotten off the ground. reply jratkevic 8 hours agorootparentExactly. I don't understand the drama on this at all. \"Corporate Open Source is dead\"?? I don't even know what that means! Having launched at least 20 open source companies, several that went public -- this is not news that this faction of companies were NEVER going to make it (AND WE SAID SO). And that is ok. reply buildfocus 14 hours agoprevThere are uses for CLAs besides rug pulls. For example, if you want to offer software as AGPL, accept community contributions, but be able to _also_ offer a non-AGPL option to paying customers (who effectively pay to be allowed to integrate the license without themselves being subject to licensing risk). Quite a few big orgs have a full ban on internal use of AGPL software so this can be very valuable. That requires a CLA (as I understand it, IANAL) because you're relicensing a contributor's contribution. At the same time though, I wouldn't consider it a rugpull - contributors lose nothing here, and the open source project gains a funding mechanism (a rare thing in open source). reply pxc 14 hours agoparentI think you can do this in a principled way with some extra corporate/bureaucratic machinery, like the Free Qt Foundation, so that it doesn't allow rug pulls. reply GardenLetter27 13 hours agoparentprevEven just stuff like releasing a GPL'd game on Steam or consoles can have the same issue if you need to keep some parts proprietary. reply yencabulator 12 hours agorootparentThere's no such thing as a \"GPL'd game with proprietary parts\". GPL was designed to prevent that. You can say \"Steam and game consoles only allow publishing software with proprietary components\", but don't blame that on GPL. reply cogman10 14 hours agoparentprevFunnily, one of the best places for this in practice (IMO) is microsoft. Who is now, again funnily enough, GPLing a lot of their software but have CLAs so they can do long term support for people that want to pay for it. Latest versions stay fully open source while back ported fixes are a paid feature if you don't keep your software up to date. That, to me, is a win win. Oracle is doing the same with the JDK. reply jabl 13 hours agorootparent> Funnily, one of the best places for this in practice (IMO) is microsoft. Who is now, again funnily enough, GPLing a lot of their software but have CLAs so they can do long term support for people that want to pay for it. Huh, what software is MS releasing under the GPL now? reply cogman10 13 hours agorootparentNot as much as I thought. MIT is more common it looks like. https://github.com/orgs/microsoft/repositories?q=license%3Ag... reply int_19h 8 hours agorootparentMIT is the standard OSS license for Microsoft stuff that is open source. You also see Apache for some projects that predate standardization on MIT, and GPL where there's no way to avoid it (i.e. when you have to build on code that is itself GPL). That said, peak OSS at Microsoft is already past. These days, it's all about advertising products as OSS while quietly close-sourcing parts of it (e.g. just about the only thing in VSCode that's fully open source is JS support, every other language extension is closed to some extent). reply trueismywork 13 hours agoparentprevThat's a rug pull for a price. I don't think it's a problem because the software is still there. But it's a rug pull. reply barfbagginus 13 hours agoparentprevHmm. Orgs which ban AGPL should be marked and targeted for dismantling. Any way to scope them out, or public registry of them we can build? Orgs banning AGPL should not be allowed to exist at all, or use open source at all. They should all be systematically sabotaged by the OSS community, and if possible acquired/reformed, or destroyed. And it would be very convenient for me to be able to relicense an AGPL contributor's work as if it were mine all along, like an evil genius capitalist, tee hee hee :3 But I would fork a project that tried it, and that is the attitude we should encourage. Those aren't valid reasons for CLAs in my organizations and projects, since they do damage to AGPL as I envision it. I wonder if there are other reasons for a genuine AGPL radical to support a CLA? Seems AGPL is good enough on its own, and most CLAs just weaken it - both technically and in a spiritual and ethical sense. Could we strengthen it instead? reply wmf 14 hours agoparentprevThis is abuse of the AGPL and open source in name only. reply piaste 14 hours agorootparentRichard Stallman, of all people, is not as intransigent as you. > I've considered selling exceptions acceptable since the 1990s, and on occasion I've suggested it to companies. Sometimes this approach has made it possible for important programs to become free software. > [..] [S]elling exceptions permits limited embedding of the code in proprietary software, but the [non-copyleft] X11 license goes even further, permitting unlimited use of the code (and modified versions of it) in proprietary software. If this doesn't make the X11 license unacceptable, it doesn't make selling exceptions unacceptable. > I consider selling exceptions an acceptable thing for a company to do, and I will suggest it where appropriate as a way to get programs freed. https://www.gnu.org/philosophy/selling-exceptions.html reply yencabulator 12 hours agorootparentThe continuous-rugpull part of demanding CLAs is selling other people's work as proprietary. You'll hear much less complaining if you're selling your own work under a second license. reply piaste 12 minutes agorootparentIt feels unfair to describe it as a \"rugpull\" when the first bullet point, in bold, reads: > *Grant of copyright license*. You give HashiCorp permission to use your copyrighted work in commercial products. https://www.hashicorp.com/cla Unless that text was recently changed, or unless Hashicorp went out of its way to verbally reassure people that they didn't intend to ever exercise that option, I feel very little sympathy for any open source contributor who clicked that link and then was dismayed to find Hashicorp using his work in a commercial product. Read what you sign, and take responsibility. reply leetharris 15 hours agoprevSource available from the beginning is ethical. Open source that becomes something else is shady. I am glad they tried. I know there's lots of cynical stuff on the internet, but realistically the open source progress on stuff like Terraform or similar wouldn't have happened if these companies didn't try an alternative. I'm not sure what people want these companies to do. If there's no money, if the idea didn't work, they need to pivot to something that does work. reply phkahler 14 hours agoparent>> Source available from the beginning is ethical. It's also mostly pointless. If someone wants to develop features for software they pay for I guess that's their choice. >> I'm not sure what people want these companies to do. Not rug-pull. reply johnmaguire 14 hours agorootparentThere are many reasons people prefer open source software other than wanting to do their own development: Security auditing, the ability to reverse engineer protocols / serialization formats, or simply to better understand a product's features. reply kevingadd 15 hours agoparentprevIf you do actual open source and your product is any good, somebody else is just going to repackage it and sell it, either as SAAS/IAAS or as a boxed product. So why not get there first? It's depressing. That's been my experience every time I gave code away, at least. Somebody else made money off it. reply kube-system 15 hours agorootparentWhy is it depressing? It was your choice to share it. reply bigstrat2003 13 hours agorootparentExactly. If I put something out in the world for the benefit of others, and someone packages it up for profit, I'm still achieving my goal. In fact, that helps my goal to advance even further! That's not depressing to me, that's exciting. reply kevingadd 14 hours agorootparentprevI think at this point it's undisputed that modern society relies on a lot of permissively licensed software, and the people who maintain that software aren't getting paid enough (or at all) to do it. xz is only the most recent example, NTP and OpenSSL come to mind. I used to do OSS development full time, but it wasn't financially sustainable for me. reply kube-system 14 hours agorootparentIMO, that's more of a sad story about the state of respect for supply chain risk management in the software engineering discipline. It may be convenient at first to add a bunch of dependencies for free that solve a particular task, and ignore the part that says \"AS IS WITHOUT WARRANTY OF ANY KIND\". The only thing a FOSS license gives anyone is code. It never gave anyone maintenance or support. Any organization that uses software that is \"as is\" should have a plan to maintain that software themselves, or mitigate the risk in other ways. And many of the large players in this industry do exactly that, and their full-time employees are top contributors to many large FOSS projects. reply jahewson 12 hours agorootparentprevA great many open source maintainers are employed by a corporation that pays them well, if not fantastically well, for maintaining it. Over time, the trend has been increasingly in that direction. reply adrianN 15 hours agorootparentprevYou could make it AGPL if you don’t want that. reply kevingadd 15 hours agorootparentNot unless all the contributors signed an Evil CLA first so I can relicense it :-) reply adrianN 15 hours agorootparentI start all my stuff with AGPL . reply vertis 14 hours agorootparentHaha jokes on you. I sucked your AGPL code into my LLM and now you have to prove I'm actually using your code when it generates the same thing. Good luck with that. reply bcrosby95 13 hours agorootparentCopyright laundering has never been so cheap. reply phicoh 14 hours agorootparentprevIf you make software with a small team and you have about 2 dozen customers who pay for a support contract, then those contracts are actually surprisingly cheap. reply aborsy 15 hours agorootparentprevThat’s guaranteed to happen. Even beyond that, how are developers going to earn a living with FOSS, at least with open licensing? We have seen maintainers burnt out. It takes work to write code. reply davisr 15 hours agorootparentI make substantial income by selling copies of my AGPLv3-licensed program for $12/ea. reply johnmaguire 14 hours agorootparentYou've piqued my interest. Can you share more? Do you have a blog post about your product and how you sell it? reply davisr 14 hours agorootparentI don't have a post about it, but the equation is pretty simple: good cross-platform software + good documentation + responsive development = good product. The trick is to write good software -- the license doesn't matter so much. But, making it free/libre is competitively advantageous, all else considered. Some customers have told me they would not have considered paying for a proprietary option. My project is called reMarkable Connection Utility (RCU), and it makes about $2.1k MRR, selling ~200 copies/month. Some months are worse, some are better. I've never purchased any advertisement. Its users advocate for it through online forums. That genuine attestation, to its quality and also to me as a person/programmer, helps it sell. That its users, too, are free to share their copy with friends helps people encounter the program who otherwise would not have purchased forthright. I sell it from my personal website with links to PayPal and Stripe. The return URL is its download page. No JavaScript anywhere. I provide the entire Git history of the project, as well as binaries for all major distros of GNU/Linux, FreeBSD, Windows, and macOS. I also provide two mailing lists -- one for all customers to be informed of updates, and another for customers to talk with each other, see my latest code patches, and talk about its general development and the reMarkable tablet. http://www.davisr.me/projects/rcu/ reply jcheng 13 hours agorootparentThere's nothing to stop any of your paying customers from setting up their own site and giving away (or selling for cheaper) copies of RCU, right? I'm guessing you understand that and figure the downside risk is low--small numbers, low stakes, consumer focused, good vibes? reply davisr 12 hours agorootparentThat's right, and they have, but I still earn enough income to develop and maintain the program. Free redistribution does not hurt my sales because people who would have paid for the program do, and those who wouldn't can end up trying it, a fraction of whom go on to become customers because they want updates and support from the most-trusted source (me). You'll see a very similar thing elsewhere: illicit file sharing increases box office revenue and increases album sales. Anyone who wants a no-fee copy of RCU can find it many places by searching. If someone ever extends my program with a cool new feature, their version must be released as AGPLv3+, and that means I can re-incorporate their improvements into my own offering. The market dynamics of free/libre software are such that since I have the reputation, the first-mover advantage, and the top search engine rankings, then anyone looking to compete needs to do so on quality of features -- which I have the license to copy, just as they copied from me. Anyone who wants to compete with me needs to compete with all that. Anyone who shares my program ends up promoting it in the best way possible, by talking about it and telling others it's good and worth using. reply vinceguidry 13 hours agorootparentprevWhen copying a business you have to also copy their marketing channels. Product is only half the story. reply tecleandor 12 hours agorootparentprevThanks! I bought it like a month ago! Haven't had time to use it much, but it came handy to downgrade my reMarkable 2 :) reply chrisweekly 14 hours agorootparentprevSee his bio http://www.davisr.me/ I'm actually a paying customer of his RCU software (for managing reMarkable tablet firmware, backups, etc). reply davisr 13 hours agorootparentThank you. It would not be possible without the financial support and advocacy from people like you. reply TheRealPomax 15 hours agorootparentprevIt's why things like Captura no longer exist. You make something fantastic with an open source license, someone else renames it and sells it in app stores, and the people running the app stores go \"the license allowed for this, we see nothing wrong\". It's well past time for getting rid of licenses that say \"you can do what you like with this software\", and replace them with licenses that allows you to sell as long as the original project gets a cut, without taking on any of the liability. The idea that you just mark your work as MIT/GPL/CC0 may have made open source more popular, but it also ruins lives when that thing you worked on for years and never made you anything makes a random drive-by asshole more than your annual salary. reply TillE 15 hours agorootparentThere is absolutely nothing wrong with making and selling proprietary software. If you want to make money, you should probably just do that. I'm very happy to have created a mildly popular permissively-licensed library. It's a contribution to a community with zero intention of profit. That is, for the most part, what open source is all about. reply TheRealPomax 14 hours agorootparentIf you read me as saying making and selling proprietary software was wrong, you read a very different text from what I wrote. Making and selling software is perfectly fine, and building that software on top of libraries specifically released for that purpose is equally fine. Open source licenses that in perpetuity lock you out of any form of gains from your own work, even when that work is the very backbone of someone else's non-open-source, for-pay product, are morally objectionable licenses. reply johnmaguire 14 hours agorootparentI believe the parent is responding primarily to this point: > The idea that you just mark your work as MIT/GPL/CC0 may have made open source more popular, but it also ruins lives when that thing you worked on for years and never made you anything makes a random drive-by asshole more than your annual salary. There's no reason this should ruin your life - your quality of living didn't suddenly decrease. If you are unhappy with this outcome, you probably didn't really intend to create open source software, and should stick to writing proprietary software. > Open source licenses that in perpetuity lock you out of any form of gains from your own work, even when that work is the very backbone of someone else's non-open-source, for-pay product, are morally objectionable licenses. It is simply false that open source licenses lock you out of any form of gains from your own work. They allow others the opportunity to benefit from it, as you yourself can too. reply devsda 13 hours agorootparentIt is possible that someone wanted to share their work with others without any payment expectations (but not realize the value of their work) and chooses to license it permissively.The product becomes successful, but the only ones earning money through the product are big companies. ( One may argue that project became successful because of its license or because it was used by the said company, but they are difficult arguments to prove and accept). I won't fault them for expecting to get paid if they later realize the value of their work and know that it is generating significant commercial money. It is natural to feel like being taken advantage of by the commercial entity when they don't get compensated (even if that was their initial choice). When someone says \"you should have chosen to make it proprietary or use a more restrictive license from beginning\", it might come across as a polite way of saying \"Too bad, no backsies! You were foolish enough to chose a permissive license. So, accept the consequences\". If a developer wants to get paid for their work with something other than \"feel good OSS points\", I don't see many options out there. They need to solicit donations(which doesn't have good track record) or re-license. I think as more products go through this cycle of permissive license, monetization by third-party and re-licensing, we'll be seeing a significant number of projects erring on the side of caution and use restrictive licenses from the start. reply phkahler 14 hours agorootparentprev>> are morally objectionable licenses. Easy on the drama there. Nobody makes someone use a particular piece of software. Don't build your product with pieces that don't fit your plans. reply RetroTechie 14 hours agorootparentprev> MIT/GPL/CC0 There's (many) different licenses - for a reason. If someone makes a small game, wants users to enjoy all the freedoms its developer had, but does not want random 3rd party to include it in a product without telling their users about that, use a GPL style license. If someone is working on a network protocol, an image format, or audio/video codec, and have that go everywhere to become a defacto standard is more important than other considerations, go for a BSD/MIT style license. Some vendors don't mind their users looking under the hood. But still want to retain control. Enter 'shared source' (=not open source) or dual licensing (eg. free for non-commercial use, commercial users: pay up). Documentation is different. So is hardware. If you're a developer (or vendor): think about this, and pick appropriate license. Just don't complain when you did open source something, and a 3rd party runs with it. reply matheusmoreira 15 hours agorootparentprev> Why I (A/L)GPL https://web.archive.org/web/20120620103603/http://zedshaw.co... > I want people to appreciate the work I’ve done and the value of what I’ve made. > Not pass on by waving “sucker” as they drive their fancy cars. reply margorczynski 13 hours agorootparentThis is the correct approach - double license, if you're non-commercial then you'll get GPL/AGPL/etc. and if you're a company and want to use it to make money this way or another then pay up. A fair approach that provides a way of supporting the people making it and further development. reply stcredzero 15 hours agorootparentprevThe idea that you just mark your work as MIT/GPL/CC0 may have made open source more popular, but it also ruins lives when that thing you worked on for years and never made you anything makes a random drive-by asshole more than your annual salary. [Citation Needed] (and a couple of questions) 1. How has this ruined someone's life? That mechanism isn't easy for me to see. 2. If the license allowed for it, why should any one see something wrong there? 3. Aren't you putting short shrift on the effort it takes to productize something? reply skywhopper 15 hours agoparentprevJust because they’re failing doesn’t mean switching licenses will help. HashiCorp did not increase the popularity of Terraform Cloud by clamping down on the ability of other companies to host Terraform services. There were any number of things HashiCorp could have done to improve revenue generation from their products without changing the source license. When they realized they weren’t building the best SaaS version of their products, they didn’t try to improve their service. Instead they tried to run the competition out of business. reply jacobr1 14 hours agorootparent> When they realized they weren’t building the best SaaS version of their products, they didn’t try to improve their service They went there several versions of their cloud products over the years, and pivoted a few times during that span reply twojobsoneboss 14 hours agorootparentprevIt’s a business. It will by default do the easiest and least riskiest thing to make as much money as possible. reply jackthetab 14 hours agorootparent> ~~It’s a business. It~~ will by default... They're humans. They will by default... reply jprete 12 hours agorootparentIndividuals almost always have values beyond monetary gain. Businesses usually don't. reply kleiba 15 hours agoparentprev> Source available from the beginning is ethical. Not to rms. reply davisr 15 hours agorootparentYou must read https://www.gnu.org/philosophy/selling.html reply kleiba 14 hours agorootparentI have but don't get your point. Could you elaborate, please? reply Decabytes 14 hours agoprevI'm glad Jeff pointed out the RMS article on Free software > For the free software movement, however, nonfree software is a social problem, and the solution is to stop using it and move to free software. > “Free software.” “Open source.” If it's the same software (or nearly so), does it matter which name you use? Yes, because different words convey different ideas. While a free program by any other name would give you the same freedom today, establishing freedom in a lasting way depends above all on teaching people to value freedom. If you want to help do this, it is essential to speak of “free software.” > We in the free software movement don't think of the open source camp as an enemy; the enemy is proprietary (nonfree) software. But we want people to know we stand for freedom, so we do not accept being mislabeled as open source supporters. What we advocate is not “open source,” and what we oppose is not “closed source.” To make this clear, we avoid using those terms. People in the FOSS world has been beating this drum since the very beginning. Free software was always an ideology. I get that that turns people off, just like projects wrapped up in religion and politics do, but it's the only way to ensure that stuff like this doesn't happen. I concede that the people who champion FOSS are not always the kindest people to be around, see the response from people about Guix when I mentioned it as an alternative to Nix. But when I was in Boston I was a card carrying member of the FSF. I went to a few libre planets, and I met some real nice people in real life who also cared about FOSS very deeply, so I know not everyone in the community is a jerk. reply chefandy 13 hours agoparentI think the FSF's take is philosophically solid, and I've gotten a ton of of their software and advocacy over the years, as most software developers surely have. However, developers and other technical people are nearly the only people for whom that's true. So much of the FOSS world blames marketing for the lack of non-technical users, routinely ignore, or even deride the prospect of prioritizing the most important factor non-technical users choosing software: usability. The people who do so dismiss interface and experience design as superfluous aesthetics. It might be true for people with a working mental model of software architecture in their back pocket, but to others, those little 'trivial' bits of prerequisite knowledge aggregate into a giant frustrating roadblock. reply downrightmike 14 hours agoparentprev\"What we advocate is not “open source,” and what we oppose is not “closed source.” I have zero idea what this means and I've been on Debian since Woody reply runjake 13 hours agorootparentThe GNU philosophy believes Free Software[1] and Open Source are two different things[2]. 1. https://www.gnu.org/philosophy/free-sw.en.html 2. https://www.gnu.org/philosophy/open-source-misses-the-point.... reply crandycodes 13 hours agorootparentprevIt's possible that \"Open Source\" here is the OSI definition, which FSF has some objections to. If \"Closed Source\" just means \"not Open Source as OSI defines it\", then it is true that some non-OSI software is \"free\" software. Free software still requires source to be available and that you're allowed to modify and distribute it. reply Cheer2171 15 hours agoprev\"Corporate Open Source\" is many things. Look at how many companies are paying for membership on the Linux Foundation Board: Platinum costs $500k a year, gold is $100k a year, and silver is $5-20k based on size: https://www.linuxfoundation.org/about/members reply 1oooqooq 15 hours agoparentLinux foundation was never about Linux or open source. quite the contrary. the foundation started as a way for companies to band Against gpl. and they won the day Linus have up and allowed tainted kernel be the default and every device distributor moved their proprietary code to modules and rejoiced. their focus then was on gpl3 fud because google et al have much more money than modem vendors. reply calibas 14 hours agorootparentThe resistance to GPL isn't just companies, I think you'll find the average programmer prefers using libraries licensed under MIT when given the choice. It seems like there's a kind of political ideology over what \"open source\" even means and how it should be practiced.There's a school of thought that says we should switch everything over to viral copyleft licenses. This will \"protect\" the code and ensure all derivative work is always open source. Then there's others, like myself, who believe if you want something that's truly \"free and open\" it should come with as few conditions as possible. reply DaiPlusPlus 13 hours agorootparentWhat about LGPL? It used to be everywhere when people were using SourceForge - but post-GitHub it seems to have dropped-off in popularity. When libs are distributed as binaries (.NET, Java, etc) it feels LGPL vs MIT/Apache is moot as hardly anyone actually modifies libraries, it seems. I wonder how different it is for Go and C/C++… reply ahartmetz 14 hours agorootparentprevUnfortunately true. I like to call the Linux foundation the megacorporations LUG (Linux user group). They want to use Linux alright, but they would prefer to do with it whatever they want, so they are not really friends of the GPL and a few other things. reply ergonaught 15 hours agoprevProducts aren't projects, which has confused people for decades. If open source products retreating from some of the \"freedom\" elements bothers you, then you should be focusing your ire on the megacorporations and overfunded startups who simply refuse to contribute to the financial viability of the products that sustain them. For some reason \"we\" celebrate the exploitative, though, so I guess that's out. reply nrawe 14 hours agoparentYep. The author complains that there used to be plucky startups with Open Core business models who've now gone \"extinct\", while simultaneously sharpening their pitchforks because those same companies got successful and _slightly_ amended their licenses to prevent loosing revenue against IaaS giants so that they can, in turn, continue innovating. I guess money grows on trees where they live. reply geerlingguy 14 hours agorootparentThe projects I think of most are redis and ansible—both of which I've used extensively, and quickly went from 'neat thing I spotted on Hacker News' to 'now it's a startup', then a few years later 'it's just another corporation, the open source project is buried somewhere deep inside'. The projects weren't as much open core as they were literal open source projects made to solve some problem the author had with other existing tooling. The open core part was injected when the startup decided it had to turn into a platform to sustain the beefy staffing budgets while they figured out how to generate revenue off something that was freely available. reply nemothekid 13 hours agorootparentI'm not sure what the alternative is or what you are advocating. Antirez stopped working on Redis 4 years ago. We were lucky someone created Redis \"for the love of the game\". Now before you there are a couple options 1. Antirez2 pops up and works on Redis because they love working on Redis 2. Someone is incentivized, with money, to work on Redis. (1) didn't happen, so we must go with (2), and with (2) comes the problem of the actual business model that will be used to sustain (2). A. Donations/Support contracts B. Open \"Core\" aka I'm the only one allowed to sell this as SaaS. Method (A) has shown to be an abject failure while AWS takes your revenue. Companies have successfully chosen method (B). Either you have a method (C) in mind, or you just aren't being reasonable in asking people to work on OSS for free. You can be mad about the \"rugpull\", but between the choice of \"rugpull\" and abandon the project, I don't see how \"rugpull\" is the worse option reply johnmaguire 14 hours agorootparentprevYou can add nginx to the list as well. reply godelski 14 hours agoparentprevWe saw this with xz. Many, including me, suggested this is an example of how paying developers could reduce likelihood of attack and speed up progress. These people make these products in their spare time. You know... After work. So they clearly would rather do that than their job. But a common response was \"what, so we could pay the hacker?\" Which missed the entire point. I'm also reminded by a relatively recent episode of either PIMA or Freakanomics (I forget which) where Steve asks Bill Gates why charities don't pay as well as corporate gigs and Gate's response was \"it's a charity\". We seem to forget that things take work. Just because someone does that work on their own time and without asking for compensation doesn't mean that they don't deserve it. It just means their more passionate. reply deathanatos 14 hours agoprev> And they're not even a pointless AI company! Are you sure? (/s), because IBM mentioned \"AI\" no less than 10 times in their announcement of the purchase: > AI-driven application growth … IBM's deep focus and investment in […] AI … The global excitement surrounding generative AI … HashiCorp's capabilities and talent will create a comprehensive hybrid cloud platform designed for the AI era … generative AI deployment continues to grow … IBM's commitment to […] AI innovation … today's AI revolution … AI-driven complexity … IBM is a leading provider of global […] AI, … IBM's breakthrough innovations in AI Thankfully the press managed to delete most of those, though there were some announcement that nonetheless took the bait. reply geerlingguy 14 hours agoparentAh, maybe IBM was duped into thinking HashiCorp was an AI play? reply Aissen 20 minutes agorootparentObviously, you discover and talk to AI services with consul, store AI secrets in vault, deploy AI platform nomad with AI infrastructure software terraform. reply iforgotpassword 15 hours agoprev> By working on a project with a CLA, where you sign away your code, you're giving carte blanche for the company to take away your freedom to use their software. Ok it's been a while so I don't remember the details or how it played out, but when Linux introduced a CoC, there were people who contributed to the kernel in the past that threatened to withdraw their code from the kernel, which would've been a nightmare to handle and clean up. How much power does a contributor have if my project is using a standard licence like GPL or BSD? Does the contributor hold any copyright over their code? Im not talking about rug pulls here, let's just say the contributor gets really mad at me for some reason. reply mfuzzey 14 hours agoparentI don't think a contributor can unilatary withdraw their code from a code base unless there is license change. Then it wouldn't be about \"withdrawing\" their code but refusing to allow it to be distributed under a different license than the one that applied when they submitted it. So if Linus wanted to relicense the kernel under a different license than GPL2 he couldn't do it by himself without tracking down all the contributors and getting them to agree to the license change or removing the code in question which, for someting as large and with as many contributors as the kernel, would be basically impossible. The CoC is completely different. It's not a license change but a process change. If a contributor doesn't like a new CoC or a new processs they can decide to no longer contribute in the future under that process, including no longer maintaining \"their\" code already in the kernel but they can't force anyone to remove it. reply goku12 15 hours agoparentprevI think that the developer can demand a withdrawal of their contribution, though I don't know what happens to the code that's already published under a FOSS license. Regardless, many projects including the Linux kernel require the developer to sign off their contributions under the terms of the 'developer certificate of origin' [1]. While the developer retains the copyright to the contribution under DCO, they acknowledge that it will be distributed in perpetuity under the original license. So, retracting the contribution may not be an option. [1] https://developercertificate.org/ reply Macha 12 hours agorootparentThe main purpose of a DCO is to take the portion of a CLA that guarantees the developer actually has a right to license the code under the project's license, as it's something that some users worry about using the software without. Basically so if someone copies some code from one of the Windows leaks into Linux and Microsoft comes to sue the Linux Foundation over it, they can point to it and say \"we had legal sign off they had the right to license this code, please sue them instead\". In terms of irrevocability, the DCO is not that different from most open source licenses who themselves claim to be perpetual. The arguments for exceptions to that being true all either apply to e.g. both the GPL and the DCO, or to neither. Generally the consensus is that an irrevocable license grant means irrevocable, though there are some concerns about examples set by e.g. the precedent that lets artists claw back rights from record labels after 30 years. reply LukeShu 15 hours agoparentprevWithout a separate CLA or copyright assignment, the contributor retains the copyright to their code, and licenses it to the project under the terms of the GPL (or whatever license). The license is basically a legally-binding promise not to sue, as long as you follow the rules in the license. So the general consensus is that they can't terminate the license unless you violate the license. (Though there are dissenting opinions on that.) reply gnabgib 16 hours agoprevHN title guidelines[0]: If the title includes the name of the site, please take it out, because the site name will be displayed after the link. [0]: https://news.ycombinator.com/newsguidelines.html reply WJW 13 hours agoparentIt's weird because in this case you could also read it as \"Jeff Geerling says that corporate open source is dead\", which just happens to be posted on his personal blog that has a domain name which is his personal name. It's not as if it's the site that called \"Jeff Geerling\" after all. reply OJFord 13 hours agorootparentThe title should be as posted on the blog, it would be weird if he put Jeff Geerling there (and he didn't). Rule's the same for bloggers without their name (obviously) in the domain, e.g. Julia Evans at jvns.ca is often submitted; not with a 'Julia Evans:' prefix. reply amouat 14 hours agoprevThe declining share price and profits is exactly what made it possible for IBM to buy Hashicorp. The license change didn't juice things -- it watered down the price. $6 billion is a snip compared to the $14 billion IPO valuation. Fintan Ryan has a nice write-up here: https://medium.com/@fintanr/on-ibm-acquiring-hashicorp-c9c73... reply pjmlp 13 hours agoprevWhat is dead is the utopian dream that making companies whose main product is raw software, is possible to do in a sustainable way, while keeping everything open source. There are bills to pay, donations only give so much, very few buy books from community members, consulting doesn't apply to all software, trainings even less, not everything can be a SaaS,... reply jillesvangurp 13 hours agoprevMost successful open source projects are actually backed by companies and most of those projects are of course alive and kicking. The entire fortune 500 runs on software and the most of that consists of massive amounts of open source with some sprinklings of proprietary stuff mixed in. Linux would have stayed a silly hobby project without the possibility for this. What's dead, or rather was never really that much alive is the notion of not quite so open source projects where a single corporate entity attempts to dictate the rules and actively discourages outside contributions, and people profiting from \"their\" source code. As a proportion of most widely used OSS software the amount of software and the number of developers involved with it is a rounding error. There are two main problems with corporate OSS: 1) it stifles the formation of a healthy community of outside contributors. This endangers long term success of projects. The more restrictions exist (e.g. copy right transfers or aggressively anti commercial usage licenses like AGPL), the more likely it is that would be contributors will take the hint and stay away. 2) it limits growth to the strategy, financial success, and imagination/skills of just one company. Because with projects being bottle necked on the financial success of just one company and cut of from outside help, absolutely nothing happens unless that one company pays for it. And with finances effectively supplied by VC investors interested more in IPOs and quick exits than good software, OSS is just a buzzword that goes on the investor deck and not something they actively value or appreciate. Hence legal monstrosities like BSL that make no sense whatsoever from the point of view of nurturing a healthy community of external developers. Most VC companies of course fail. That kind of is the point. And most of their restrictively licensed software projects die along with them and don't survive the implosion of these companies. Developers move onto other things and the software gets peddled to hedge-funds or companies like IBM. The only exception to this is properly licensed open source that can simply be forked. Oracle, Redis, Red Hat, Hashicorp, Elasticsearch, etc. found that out the hard way. The answer is not getting more proprietary about OSS software and preventing forks. The software gets forked precisely because these projects are too valuable to let it rot away behind corporate pay walls. This is not a failure of open source but actually a huge success. The software will long survive the misguided corporate shenanigans. The software and community will be fine. Those companies, possibly a lot less. reply pjmlp 13 hours agoparentWhich hard way? None of those forks are relevant enough to matter. reply Macha 11 hours agorootparentThe Jury is still out. I suspect in 5 years, Valkey and OpenTofu will be more popular than Redis and Terraform, at the very least for new products, but probably also for most actually maintained products. There's a good chance OpenSearch will also displace Elasticsearch. reply getcrunk 9 hours agoprevThis goes against the foss ethos but given that foss is ultimately rooted in freedom I think what I will suggest can be reconciled with the primary principle. I’ve been thinking the only way to truly protect freedom of individuals and of foss development is to have a dual license which is foss unless you deploy it to over 10m users or 10m revenue or belong to list of companies like the eu designated gatekeepers. In that case each project should have the latitude to decide if they will enforce terms if any. Meaning that given the needs of the developers being otherwise met they can always forego any additional requirements reply 8organicbits 13 hours agoprevI started (and just published) a website[1] that tries to track relicensing risks across a number of OSS projects. The current methodology looks at trademark holders, licenses, and CLA/DCO requirements. I think there's community value in educating develpers, surfacing risks, tracking when a project changes it's posture, and promoting forks. If this sort of thing is interesting to you, I'm looking to expand the project listing with community support. I suspect the relicensing trend is on its way up. [1] https://alexsci.com/relicensing-monitor/ reply stuff4ben 15 hours agoprevI don't blame the big corps like IBM (disclaimer: I work there, I don't speak for them) and Microsoft and Google. I really blame investors who are looking to get their money back. When they force an IPO but there's no business model to sustain the company, this is what happens. CEOs resort to developer-hostile actions which kills off the community and generates ill-will towards the company. People here shit on \"lifestyle companies\" but IMO that's the best model for sustainablility and developer communities (if that's what you're into). reply Ekaros 14 hours agoparentIf you offer something for free. And it becomes popular partly because it is free. Well it should be pretty clear that there is not huge money in free product... And at some level someone packaging with some margin your free product is well... reply bsnnkv 13 hours agoprevI recently re-licensed a large open source project from MIT to a source-available license which restricts redistribution after someone decided to fork it, completely rearrange the code and introduce subtle breaking interface changes, and then package it up with basic Electron UI for sale on a platform digital store. I don't know what the answer is, but the OSI definition is clearly not built to withstand this era of late stage capitalism and the \"hustle culture\" that permeates through it. reply 8organicbits 12 hours agoparentTrademark law should handle this (at least in some countries). Register a trademark and then send a cease and desist to the fork as they are creating confusion in the marketplace by distributing something they created using your registered name. Perhaps consider using Apache 2.0 license, section six has language that specifically addresses trademarks. reply wmf 12 hours agorootparentA lot of open source devs are allergic to promoting their work so owning the trademark won't help them. For example, the above developer could have created their own official GUI and put it in the app store but they won't. Realistically the value is in GUIs, SaaS, marketing, etc. not in backend code. reply bsnnkv 12 hours agorootparentReplying to both parent comments here: I do a fair bit of promotion on YouTube[1] which has resulted in a vibrant community around the software, and I am now creating an official, free GUI[2]. In this specific case even if there had been a trademark in place, the person in question had not used the name of the project at all, but had decided to aggressively market it in community spaces. Ultimately after much reflection and experiencing first hand how easy it is for people to abuse (in my opinion) the option of deliberately hard-forking and introducing breaking changes to funnel people towards a paid piece of software (in this case, a GUI) that will _only_ work with their hard-fork, I concluded that preventing this kind of ecosystem fragmentation is more important to me than an \"OSI-approved\" badge. [1]: https://www.youtube.com/@LGUG2Z [2]: https://www.youtube.com/watch?v=zZKjBMt4kZ4 reply penguin_booze 6 hours agoprev> Bryan Cantrill's been sounding the alarm for years—yes, that Bryan Cantrill, the one who posted this gem The video is of Brendan Gregg, not of Bryan. Am I missing something? reply nsteel 2 hours agoparentYes: Bryan posted the video of Brendan shouting. reply tzs 11 hours agoprevOT: if the author of the article sees this, there's an error in a quote. You quote HN user skywhopper as saying > HashiCorp already did a great job pre-draining all their flavor but their quote actually says [1] > HashiCorp has done a good job of pre-draining any flavor it once had [1] https://news.ycombinator.com/item?id=40135686 reply geerlingguy 11 hours agoparentAh sorry about that, was paraphrasing for the video, but the blog post should have the direct quote since I don't have a snapshot. I'll update the page when I'm back at my computer later! reply qwertox 14 hours agoprevIt's basically become market research where the amount of devs which get hooked indicate success and these devs also double as the client (a trap). I try to stay with my stack, which also makes me miss out on things like the highly recommended Tailscale-service. reply flerchin 13 hours agoprevTeraform and serverless doing the exact same thing at the exact same time really indicates that the market conditions that allowed for software like this was not long term sustainable. As a builder that leverages them both, I'm saddened, but I get it. reply TheDudeMan 14 hours agoprevI just watched this interesting Node.js doc (Open-source Node.js was purchased by a corp and there was some drama along the way). https://youtu.be/LB8KwiiUGy0 reply j-cheong 14 hours agoprevI think the anger comes from the rug pull, not that they chose to be source available, right? If they chose to be source available from day 1, then no harm done? reply geerlingguy 14 hours agoparentYep; though HashiCorp would be a much smaller company had they not started with an open source license. reply renewiltord 15 hours agoprevIf you can't capture sufficient value, you're not going to be able to make a business around it. The problem isn't value creation, which the OSS model does do. The problem is value capture. That's why so many people go around saying \"We need to pay X more\". That's a sign that creation and capture have a gap. You'll see that with traditional open-source. With companies that attempt to capture the value, your customers will always hate you unless you're careful. Of course using the Elastic license family from the beginning is one way there. The Llama license family is another way. But perhaps my favourite observed thing has to be Kong's licensing: the base thing is Apache 2.0 but when you sign a contract with them, they'll give you access to the Enterprise plugins and you can edit their source. I loved working with them. They seem to have done a good job with value capture. I think they're leaving a lot on the table, but there is significant path dependence on what they've done, so they don't have the option any more. But good job. reply shkkmo 15 hours agoprevSo the TLDR; is don't trust any corporate open source that requires a CLA and don't contribute to those projects unless you are prepared to fork if the rug gets pulled. reply TheRealPomax 15 hours agoparentAny project with a CLA was already subject to that stipulation. You get to do work for a company for free with the added benefit that you're now also on the hook for any legal problems that might arise from the code you contributed. What a deal, you get to put in the time and effort AND potentially fight a legal battle when a patent troll decides to go \"wouldn't it be fun if\" all on your own! reply jratkevic 8 hours agoprevWhat is C reply matheusmoreira 15 hours agoprevAlways remember: It's either AGPLv3 or proprietary all rights reserved. Anything else means you're giving your labor away for free to the beggar barons. Biggest wealth transfer in history, from developers and straight into the pockets of billionaires. It's just irrational. Also remember: Whoever owns the copyright gets to do whatever they want. The licensing security of free software, defined here as the likelihood of free software remaining free software, is proportional to the number of copyright owners involved. Changing the license requires agreement between all copyright holders, once a sizeable number of them has built up it becomes all but impossible. Therefore, anyone who asks you to assign your copyright to them should be viewed with suspicion. A true proponent of free software would want to maximize the number of copyright holders involved, not centralize the copyrights under a single entity. reply busterarm 15 hours agoparentMaybe I like writing software that I don't bother to monetize but allows anyone who wants to come along and use it to eat to do so. Don't give things away if you care about what happens to them afterwards. So many people completely misunderstand what open source is about. reply matheusmoreira 13 hours agorootparent> Don't give things away That's exactly the point I was making. My whole point was: DO NOT just give things away. That's just irrational. It brings you no profit and doesn't actually create long lasting wealth and freedom in the form of copyleft software. All it does is enable your exploitation. All it does is enrich billionaire corporations who are free to take the work of others, use it to make a killing and then laugh all the way to the bank at the suckers who made it all possible for free. Don't just give your code away for free. Either make it proprietary all rights reserved, or make it AGPLv3. Those are the two choices. Maximum profit or maximum freedom. Either they pay you lots of money, or they adhere to the full set of conditions spelled out by the AGPLv3. Don't just give your stuff away. Attach lots of strings to it. Strings that force others to benefit you in return for your generosity. Either by paying you lots of money or by being equally generous to you in return. Don't just give software away. That was the point. There is exactly one scenario where \"permissive\" licensing makes sense: a world without copyright. In other words, a world without licensing at all. They can copy your software and you can copy theirs. Until such a day comes that copyright is abolished though, \"permissive\" licensing is just nonsense, it just gives away all your leverage. reply layer8 13 hours agorootparentYou cut off “if you care about what happens to them afterwards”. It’s perfectly fine for somebody to just give software away as long they’re fine that someone else will make money off it. You don’t seem to be fine with it, but others are. reply matheusmoreira 12 hours agorootparentI've seen quite a few of these people become very bitter over the years. Taken for fools and exploited. They were supposedly \"fine\" with it at first. At some point it must have hit them. I don't blame them. Their reaction is understandable. I blame whoever came up with this \"permissive licensing\" psyop. reply busterarm 12 hours agorootparentThat's not correct though. Charity is a valid thing. Expectations-based giving should honestly always be frowned upon. Once you give something up it's no longer in your control. If somebody has a problem with that, it's actually them that's the problem. I think a lot of folks don't think through the licenses they choose before they use them. They never really thought about if they'd be mad if somebody else commercialized their code. You give up that right when you chose that license but that's okay. Licensing is a choice. You're also free to use AGPL if you want or even go proprietary -- but don't blame open source licenses if you chose them by mistake. reply matheusmoreira 12 hours agorootparentWho's doing this out of charity though? I'm willing to bet almost everyone who does free and open source software is secretly hoping it will come back to them somehow. Usually as an actual job, maybe as a consultant for the software they created, maybe custom feature work. These days it's often a patreon or github sponsorships which are ethical ways to make money. Maybe it's the prestige of having one's full name attached to some major project that drives them. Maybe it's the enthusiasm for computing freedom and the copyleft ideals: sharing your work and benefiting from the work of others in return, knowing that you're adding to a commons and that your freedom is ensured. Expectations can be as low as basic respect and acknowledgement. But they do exist. You're absolutely right that most people don't seem to think through the licenses that they choose. That's my point though. That's why I came here. To tell everyone that it makes no sense to choose anything but AGPLv3 or proprietary. If you agree with me, then upvote the comment so that more people will see it and hopefully avoid falling into the permissive licensing trap. reply busterarm 11 hours agorootparentMost of my open source contributions were because I was a user and had a problem to solve. Usually in the course of doing my paid job, but not always. reply matheusmoreira 11 hours agorootparentThen you do have expectations. Of continued maintenance. I hope whoever was maintaining the software had their expectations fulfilled too. We've all seen what can happen when they aren't. reply twojobsoneboss 14 hours agoparentprevThose “robber barons” are also the source of most tech jobs and how most of us here earn a living reply int_19h 7 hours agorootparentThe source of most jobs (tech or not) are, ultimately, people and their needs. reply matheusmoreira 14 hours agorootparentprevNot robber barons. Beggar barons. https://zedshaw.com/blog/2022-02-05-the-beggar-barons/ Trillion dollar corporations. That beg for free labor. That exploit the good will of free and open source software developers. Here's the latest example: https://news.ycombinator.com/item?id=39912916 https://news.ycombinator.com/item?id=39909949 reply NuSkooler 15 hours agoprev [–] TL;DR: Once again, capitalism takes it's toll. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "IBM is acquiring HashiCorp for $6.4 billion, following HashiCorp's transition to a 'Business Source License,' indicating a shift in the software industry.",
      "Companies are increasingly moving from open source to proprietary licenses, causing divisions like forks in projects such as Terraform and Vault.",
      "The trend away from corporate open source raises worries about trust, income, and the sustainability of community-driven software, potentially paving the way for smaller teams to spearhead future open source initiatives independently."
    ],
    "commentSummary": [
      "The article explores how corporations benefit from open-source software without reciprocating, suggesting using stricter licenses like SSPL or Commons Clause to promote fair business practices.",
      "There is a debate on developers' dilemmas between making a living and upholding open-source principles, emphasizing the need to strike a balance between profit and collaboration in the open-source community.",
      "Contributors discuss challenges in sustaining open-source projects financially, stress the significance of selecting the appropriate licenses, and warn against the risks of relicensing and potential drawbacks of corporate involvement in open source."
    ],
    "points": 214,
    "commentCount": 130,
    "retryCount": 0,
    "time": 1714065401
  },
  {
    "id": 40156890,
    "title": "Exploring Rust Stream API: Balancing Concurrency and Performance",
    "originLink": "https://github.com/alexpusch/rust-magic-patterns/blob/master/rust-stream-visualized/Readme.md",
    "originBody": "Rust Stream API visualized and exposed Managing concurrency in real-world applications can be quite tricky. Developers must grapple with concurrency control, back pressure, error handling, and more. Thankfully, Rust provides us with the async/await mechanism, and on top of that, the Stream API. The Stream methods allow us to elegantly define a pipeline of asynchronous operations with a nifty abstraction addressing common use cases. Unfortunately, elegance sometimes masks complexity. Can you look at a streaming pipeline and understand how many operations would run in parallel? What about the order? I found it trickier than it seems, so naturally, as a complete overkill, I wrote a Bevy visualization to investigate it. This investigation brought light to some truly unexpected results - so unexpected, that in some cases, you might want to reconsider using this API. Overview of Stream API Let's start with a brief overview of the Stream API. The following code defines an async pipeline that iterates over integers from 0 to 10 and executes the async_work method with a concurrency limit of 3. The result is then filtered using the async_predicate method. This is awesome! With just a few lines of code, we've created a non-trivial async control flow. async fn async_work(i32) -> i32 {...} async fn async_predicate(i32) -> Option {...} async fn buffered_filter_example() { let stream = stream::iter(0..10) .map(async_work) // async_work returns a future. The output of this stage is a stream of futures .buffered(3) // polls stream of futures and runs at most 3 concurrently .filter_map(async_predicate); // filters out the results of the previous stage using async_predicate function pin!(stream); while let Some(next) = stream.next().await { println!(\"finished working on: {}\", next); } } Amm, we can already see some complex elements. For instance, why did I use filter_map instead of filter? What's this pesky pin!(stream) doing? I won't digress into these questions. Instead, here are some informative links: Put a Pin on That How will futures::StreamExt::filter work with async closures? The goal of this investigation is to get a better understanding of the execution order, concurrency, and back pressure characteristics of such pipelines. For example, in the code above, the map method executes 3 async_work concurrently, but what if async_predicate is a long operation? will it continue to concurrently run more async_work? Supposedly after it completes 3 invocations, it should be able to run more while async_predicate runs in the background right? If so, will it take an unbounded amount of memory? What about filter_map? it does not have a clear concurrency parameter. Does it runs the provided method serially? or with unlimited concurrency? The documentation leaves some of these questions unclear. We need to see it with our own eyes. Experiment tool - visualizing Rust streams I used Bevy to visualize the flow of data in a streaming pipeline. The idea involves defining a streaming pipeline with methods that report their progress via a channel. I used Bevy's EventWriter to forward this information to a Bevy rendering system. Here's how it looks: In the visualization, we see a representation of each streaming item navigating through different stages of the pipeline. Units of work start from the source and move to the map(..).buffered(..) stage. To simulate real world async work I used a small loop of sleep() calls. This represents real world scenarios where async methods have multiple await calls and allows us to visualize the future run progress. for i in 0..5 { tokio::time::sleep(duration / 5).await; tx.send(/* update bevy rendering system */).unwrap(); } We visualize future progress via a tiny progress bar on each item. After an item completes the buffered stage, it proceeds to the sink and finishes its journey. It is important to note that the visualization is sourced from actual running Rust code. This isn't a simulation; it is a real-time visualization of the Rust Stream pipeline. You can find the source code here. Experiment 1: buffered stream::iter(0..10) .map(async_work) .buffered(5); buffer up to at most n futures and then return the outputs in the same order as the underlying stream. No more than n futures will be buffered at any point in time Experiment questions Will the buffered method fetch a new unit of work from the source stream as soon as any unit of work completes, or only when the earliest unit of work completes and sent as output to the next stage? All right! look at it purr! As expected, each item goes through async_work. The .buffered(5) step runs at most 5 futures concurrently, retaining completed features until their predecessors completes as well. Experiment result The buffered method does not acquire a new unit of work once an arbitrary item completes. Instead, it only does so once the earliest item is completed and advances to the next stage. This makes sense. A different behavior would require the buffered method to store the results of an unbounded number of futures, which could lead to excessive memory usage. I wonder if there's a case to be made for a buffered_with_back_pressure(n: usize, b: usize) method that will allow some items to be taken from the source stream, up to b times. Experiment 2: buffer_unordered stream::iter(0..10) .map(async_work) .buffer_unordered(5); buffer up to n futures and then return the outputs in the order in which they complete. No more than n futures will be buffered at any point in time, and less than n may also be buffered Experiment questions Will the buffer_unordered method take a new unit of work from the source stream as soon as any unit of work completes, or only when the earliest unit of work is completed and sent to the next stage? Unlike buffered, buffer_unordered does not retain completed futures and immediately makes them available to the next stage upon completion. Experiment result The buffer_unordered method does fetch a new unit of work as soon as any unit of work completes. Contrary to buffered, the unordered version does not need to retain completed future results to maintain output order. This allows it to process the stream with higher throughput. Experiment 3: filter_map stream::iter(0..10) .filter_map(async_predicate); Filters the values produced by this stream while simultaneously mapping them to a different type according to the provided asynchronous closure. As values of this stream are made available, the provided function will be run on them. Experiment questions Does the filter method executes features in parallel or in series? Experiment result No surprises here. The filter operator processes each future in series. If we want to accomplish async filtering with concurrency we can use a blend of map, buffered, and filter_map(future::ready). The map().buffered() duo would calculate the predicate concurrently while filter_map remove failed items from the stream stream::iter(0..10) .map(async_predicate) .buffered(5) .filter_map(future::ready); // the ready function will return the predicate result wrapped in a ready future Experiment 4: buffered + filter_map stream::iter(0..10) .map(async_work) .buffered(3) .filter_map(async_predicate); Experiment question How will a long-running filter_map step affect the concurrency of the buffered step? Ok, this is unexpected! The stream does not function as I initially thought. While async_predicate is being executed, no async_work future is progressing. Even further, no new future starts to run until the first batch of five is complete. What's going on? Let's see what happens when we replace buffered with buffer_unordered. The situation is pretty much identical. Again, the async_work futures are suspended until async_predicate is completed. Could it be something to do with filter_map? Let's attempt to stick two buffered steps sequentially: Nope, the behavior remains the same. What's going on? Turns out I'm not the first that encounters this difficulty. This is the same issue Barbara battled with. To truly grasp what's happening, we need a solid understanding of Futurus, async executors, and the stream API. Resources such as The async book and perhaps fasterthanlime's Understanding Rust futures by going way too deep can serve as good starting points. I'll attempt to give you some intuition. The first clue comes from the question - when does Rust run two futures concurrently? There's the join! and select! macros, and the ability to spawn new async tasks. However, the Stream API neither join nor select over futures created by different pipeline steps, nor does it spawn new tasks each time it executes a future. A Deeper Dive Let's take a closer look at our example and try to analyze the control flow. let stream = stream::iter(0..10) .map(async_work) .buffered(5) .filter_map(async_predicate); pin!(stream); while let Some(next) = stream.next().await { println!(\"finished working on: {}\", next); } First we create the stream instance. Futures in Rust aren't executed until they are awaited. Therefore, the first line of the example has no standalone effect. Lets look at the type definition of the stream variable: FilterMap>, fn async_work(i32) -> impl Future>>, impl Future>, fn async_predicate(i32) -> impl Future > After the initial shock we find five nested structs Range within Iter within Map within Buffered within Filter. These types of structs are referred to as \"adapters\". Each adapter holds state and data and implements some trait, in our case, Stream. They wrap their own logic around this trait. For example, the Buffered adapter owns a source stream and a in_progress_queue: FuturesOrdered to manage the buffering. Elegantly skip over pin!. So, what happens on the first stream.next().await command? The Next future callsstream.poll_next_unpin(cx), where stream is an instance of FilterMap. In turn, the FilterMap::poll_next implementation polls its inner stream - the Buffered stream - and executes async_predicate on the result. The Buffered::poll_next method polls its inner stream at most max times, until the inner buffer is filled. For each such poll, the Map stream fetches an item from its source stream and runs the async_work method that returns a future. Note that the only place where futures are executed concurrently is the FuturesOrdered instance in the Buffered::poll_next implementation. We can loosely transform the example to follow this pseudo code: let range_stream = stream::iter(0..10); let in_progress_queue = FuturesOrdered::new() loop { // buffer at most 5 items to queue while in_progress_queue.len()i32 { sleep(Duration::from_millis(100)).await; x } The second version has 5 calls to sleep(20ms). In this case each consequent .await might suffer from the delayed polling again and again. This is the case for the futures visualized in this investigation, and probably a better simulacrum for real world use cases. async fn async_work(x: i32) -> i32 { sleep(Duration::from_millis(20)).await; sleep(Duration::from_millis(20)).await; sleep(Duration::from_millis(20)).await; sleep(Duration::from_millis(20)).await; sleep(Duration::from_millis(20)).await; x } Experiment summary Our experiments revealed that the Stream API pipelines can be surprisingly suboptimal. Looking naively at a pipeline, we might imagine everything running concurrently. However, the reality doesn't meet these expectations. Should you use the Stream API? As with many other things in our profession, this depends on the trade-offs. On one hand, this API allows us to quickly meet our needs with a clear and elegant API. On the other hand, the pipeline throughput will not be optimal. In my opinion, in many cases, dropping this API might be considered a premature optimization. Nevertheless, these findings definitely worth your consideration.",
    "commentLink": "https://news.ycombinator.com/item?id=40156890",
    "commentBody": "Rust Stream API visualized and exposed (github.com/alexpusch)208 points by PaulHoule 21 hours agohidepastfavorite10 comments withoutboats3 18 hours agoThe problem here isn't with the concept of Streams (which are good) but with specifically the \"buffered stream\" APIs provided by the futures crate (i.e. the buffered and buffer_unordered methods). Their lack of concurrency with processing before or after is a known problem as the blog post alludes to at the end; I would discourage users from using these APIs without considerable care. I've explored this subject on my blog, including possible solutions to the problem with these APIs: https://without.boats/blog/futures-unordered/ https://without.boats/blog/poll-progress/ https://without.boats/blog/the-scoped-task-trilemma/ Also, the rendering and visualization aspect of this is very cool! reply gdcbe 17 hours agoparentPlease keep on blogging like you do withoutboats, your articles are a gem that I learn something new from every time. Due to the work of you and others I do have hope it will all be better in future. That said, might be my low standards due to many scars from my c++ background, but I’m already plenty happy with what we have today, so the fact that it will get even better in the next years is like cherry on the cake for me. reply zamalek 16 hours agoparentprevIs there any reason why one couldn't? stream::iter(0..10) .map(async_work) .map(|t| spawn(t)) .buffered(3 - 1) // The line above act as a buffered slot .map(unwrap_join) .filter_map(async_predicate); reply demurgos 11 hours agorootparentThe poll_progress post linked above explains the situation. When polling the overall stream, you alternate between awaiting in the buffered interface or in the subsequent adapters. This is because the different futures are not peers with regard to the executor, but there's a chain of futures and `FilterMap` only calls `poll` on its parent when it's done with the current item. This post was also helpful to understand the issue: https://tmandry.gitlab.io/blog/posts/for-await-buffered-stre... reply 0x457 16 hours agoprevSo, buffer and buffer_unordered only make sense at the end of stream and only if the receiving side is slower than the rest of the \"pipeline\"? reply qwertox 54 minutes agoparentNot really, because items in the pipeline won't be able to get processed as long as the receiving side is not yielding. reply low_tech_punk 17 hours agoprevGood visualization is worth a thousand words! I wonder if Rust stream can contain streams themselves, i.e. higher order streams as in seen in RxJS? I found it very difficult to visualize anything that is of higher order. The RxJS marble diagram was helpful to some extent but they are static. reply macawfish 17 hours agoparentYes, higher order streams are possible in Rust. I appreciate that in Rust they are also typed. In JavaScript it's sometimes tricky to reason about higher order streams without types. reply renewiltord 17 hours agoprev [–] This is an incredible animation engine. I'm going to check it out. reply roland35 17 hours agoparent [–] Bevy is a full-blown game engine, which is an awesome idea for visualizing rust programs. Maybe it would be good for generating advent of code diagrams next year... (Who am I kidding, I barely get to day 12 most years!) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article delves into the Rust Stream API, discussing the challenges of handling concurrency and back pressure in practical scenarios.",
      "Rust's async/await feature and Stream API present sophisticated ways to design asynchronous workflows, yet executing them can pose unexpected complexities.",
      "Using Bevy for visualization, the author showcases various Stream methods' behaviors, uncovering surprising outcomes and emphasizing the need for a comprehensive grasp of Rust futures and async executors."
    ],
    "commentSummary": [
      "The Rust Stream API is a hot topic, being showcased and discussed on GitHub for its visualization capabilities.",
      "Some users warn about specific buffered stream APIs in Rust lacking concurrency, while others appreciate the visualization feature.",
      "Discussions touch on solving API problems through blog posts, potential for higher order streams, and visualizing Rust code with Bevy, a game engine."
    ],
    "points": 207,
    "commentCount": 10,
    "retryCount": 0,
    "time": 1714049504
  },
  {
    "id": 40162433,
    "title": "Backpage Co-Founder Michael Lacey Acquitted on Most Counts",
    "originLink": "https://reason.com/2024/04/25/judge-acquits-backpage-co-founder-michael-lacey-on-most-counts/",
    "originBody": "Backpage Judge Acquits Backpage Co-Founder Michael Lacey on Most Counts The court found insufficient evidence to sustain 53 of 84 remaining counts against Lacey. Elizabeth Nolan Brown4.25.2024 1:06 PM Share on FacebookShare on XShare on RedditShare by emailPrint friendly versionCopy page URL Media Contact & Reprint Requests (screenshot from U.S. Courts video) A federal judge has acquitted Backpage co-founder Michael Lacey of dozens of counts, including a majority of those on which federal prosecutors planned to retry Lacey later this year. U.S. District Judge Diane Humetewa also acquitted former Backpage executives Jed Brunst and Scott Spear on multiple counts of which they were convicted by a jury last fall. \"After viewing the record in the light most favorable to the Government, the Court finds there is insufficient of evidence to support convictions under Counts 19–51 as to Mr. Lacey and Counts 66–99 as to Messrs. Lacey, Brunst, and Spear,\" concluded Humetewa. In November, a jury found Lacey guilty of just one the 86 counts against him and not guilty of one count as well. The jury was hung on the other 84 counts, including all charges that Lacey actively facilitated prostitution or participated in a conspiracy to facilitate prostitution via the online classifieds site he founded with his longtime newspaper partner James Larkin. (Larkin took his own life last summer a few days before the trial was scheduled to begin.) The feds then decided to retry Lacey on those 84 counts, despite the fact that there had already been two trials on the same charges. (The first, in 2021, was declared a mistrial after prosecutors and their witnesses couldn't stop talking about sex trafficking despite none of the defendants facing sex trafficking charges.) Now, Humetewa has acquitted Lacey on 53 of the remaining 84 counts against him. Additionally, Humetewa acquitted Spear, former executive vice president of Backpage, of 10 of the counts on which he was found guilty by the jury and acquitted former Chief Financial Officer Brunst of 18 of the counts on which he was convicted. Two of the other defendants were acquitted on all charges by the jury. Lacey, Spear, and Brunst could still be in serious trouble. Lacey is still looking at a retrial later this year on the remaining counts against him, which include one count of conspiracy to violate the federal Travel Act by facilitating prostitution, 17 counts of violating the Travel Act by facilitating prostitution, and several different money laundering counts. And on June 17, Lacey is scheduled to be sentenced on the one count—international concealment of money laundering—on which the jury found him guilty. It comes with a possible sentence of up to 20 years in federal prison. Lacey plans to appeal his conviction on this count, and there seems like a good chance it will be successful, since the money he allegedly \"concealed\" was reported to the federal government with all the proper paperwork. But he could still face prison time as that appeals process plays out. Brunst is scheduled to be sentenced along with Lacey in June and even with Humetewa's acquittals, he still faces sentencing on 14 counts. And Spear, who is scheduled to be sentenced on July 9, still faces sentencing on 29 counts.",
    "commentLink": "https://news.ycombinator.com/item?id=40162433",
    "commentBody": "Judge acquits Backpage co-founder Michael Lacey on most counts (reason.com)190 points by perihelions 13 hours agohidepastfavorite143 comments nadermx 13 hours ago\"And on June 17, Lacey is scheduled to be sentenced on the one count—international concealment of money laundering—on which the jury found him guilty. It comes with a possible sentence of up to 20 years in federal prison. Lacey plans to appeal his conviction on this count, and there seems like a good chance it will be successful, since the money he allegedly \"concealed\" was reported to the federal government with all the proper paperwork. But he could still face prison time as that appeals process plays out.\" How was he found guilty of concealed money when he reported it to begin with? Then could still serve prison time for it. reply beaeglebeachh 12 hours agoparentAnd how is it money laundering if it turned out the money wasn't proceeds of crime. reply brookst 11 hours agorootparentI spent a few minutes trying to figure that out, but Reason is incredibly slanted, they just link to some even nuttier site, and even starting with news I couldn't find specifics of the one count he was convicted on. But I'd be a little skeptical of Reason's handwave \"properly reported to the government\". Reported on taxes? To SEC? As part of disclosure for this trial? One way it could be money laundering without being proceeds of a crime is if he took $1M from Pablo Escobar for a huge advertising commit and then refunded $900k to Able Paleo Bars, LLC for unused ad spend. reply Niten 7 hours agorootparentMedia Bias Fact Check ranks Reason as a \"Right-Center\" biased website with a \"High\" rating for factual reporting: https://mediabiasfactcheck.com/reason/ They certainly approach stories with an editorial perspective, but they're generally factually reliable and hardly \"incredibly slanted\". reply Retric 6 hours agorootparentReason’s libertarian bias sits outside the mainstream Left-Right axis of US political discourse. So what looks centrist or right leaning on a 2D scale is actually heavily biased in a different political direction. reply pjscott 5 hours agorootparentYes, thank you for pointing this out – but they do tend to report the facts correctly. reply yareal 6 hours agorootparentprevI don't think there's much Left in US politics, unless you mean in the language of US politics, where somehow Democrats are considered \"Left\". reply prerok 2 hours agorootparentWell, left and right originated based on the parliament seating arrangement in France: https://en.m.wikipedia.org/wiki/Left%E2%80%93right_political... So, traditional left/right split may not make much sense anymore and each country could have their own split for left and right sides. The split that's now the norm in most western countries AFAICT is liberal on the left and conservative on the right. reply Retric 6 hours agorootparentprevParty stances are always going to reflect the country they operate in. On an absolute scale Republican’s support for expanding Medicare drug coverage was left leaning compared to at the time current law even though it’s to the right of many countries and Democrat’s stance on the issue. reply inglor_cz 3 hours agorootparentprevThe Left no longer means pushing for worker unionization or trying to expand welfare programs, at least not in the rich countries. This traditional left has become fringe almost everywhere in the top 20 economies. The Left is now a mostly academic movement dealing in obscure words. reply the-smug-one 2 hours agorootparent>The Left no longer means pushing for worker unionization or trying to expand welfare programs, at least not in the rich countries. [...] The Left is now a mostly academic movement dealing in obscure words. This is as factual as rain is dry. >This traditional left has become fringe almost everywhere in the top 20 economies. What is traditional left? Marxism-Leninism? Socialism is and has been a wide spectrum since before the Russian revolution. Right now we're seeing an uptick in extreme right tendencies in Europe but top 20 economy countries such as the UK, France, Brazil and Germany do have solid leftist parties. reply concordDance 2 hours agorootparentprevThere's a few distinct axes commonly labelled \"left\". The Democrats are quite left on some axes (e.g. concern about racism) and less so on others (e.g. siezing the means of production). reply petesergeant 2 hours agorootparentWe used to call this liberalism reply jasonlotito 4 hours agorootparentprev“They often publish factual information that utilizes loaded words (wording that attempts to influence an audience by appealing to emotion or stereotypes) to favor conservative causes“ So, when they present facts, they surround them with emotional words to try to sway opinion. reply dannyobrien 9 hours agorootparentprevWhat is this \"nuttier\" site you're talking about? Do you mean https://frontpageconfidential.com/aboutfpc/ , the site run by Lacey and Larkin and their fellow journalists, in order to defend themselves? reply ianhawes 7 hours agorootparentprevReason would indeed seem slanted compared to traditional liberal media, specifically as it relates to not regurgitating the DOJ’s narrative. Happy Reason subscriber here. reply webspinner 8 hours agorootparentprevReason is not slanted. It's a very libertarian site and a lot of people come from the left. reply klausa 8 hours agorootparent>X is not slanted. It's a . reply webspinner 6 hours agorootparentI meant like say brieghbart is, it's a conservative source. I've never known of reason to be such thing though. reply yareal 6 hours agorootparentprevAll news media has a bias, it's literally impossible to serve a finite news set and not exert some editorial control. However, if you describe a news site as (very political ideology) I think it's fair to consider it slanted. reply roenxi 6 hours agorootparentWhile that is philosophically true, but this thread is illustrating why that isn't a helpful perspective . Consider a media organisation has a bias towards accurate reporting with a view to helping readers achieve the best outcome for themselves and good outcomes for society. We could correctly call such an organisation \"slanted\", but it is missing the point that there some slants are privileged by virtue of leading the reader to act in a reasonable and rational manner while others don't. I'm not sure if the US - or the world - has any of those. But the generic \"there is a slant\" point is useless. What matters is what the slant leads people to do, what information the slant is omitting and whether said slant is a good one on balance. If calling a media organisations \"slanted\" doesn't imply \"slanted [in a way I think is potentially bad and relevant]\" then the word is useless and shouldn't be used. reply klausa 5 hours agorootparent> Consider a media organisation has a bias towards accurate reporting with a view to helping readers achieve the best outcome for themselves and good outcomes for society. I think you'll find that literally every media organisations thinks it's doing exactly that — the issue is that people have _wildly_ different conceits of what \"best outcome\" for them is. reply yareal 4 hours agorootparentprev> Consider a media organisation has a bias towards accurate reporting with a view to helping readers achieve the best outcome for themselves and good outcomes for society. Are there media organizations that don't believe they are doing this? reply dragonwriter 6 hours agorootparentprev“libertarian” is as much of a slant as left or right is; that its largely orthogonal to the left/right axis, and lots of people think only in terms of that axis (on which Reason is clearly on the right, but not as far from center on that axis as it is on the libertarian one) doesn't change that. reply webspinner 6 hours agorootparentThere's about a 50/50 split in the libertarian party so it really isn't. Actually, I would say most of the people there lean left, or center. reply solumunus 6 hours agorootparentI don’t think you understood their comment as yours doesn’t seem to make sense as a response. They’re saying that libertarian can be left/right but Reason in particular skews right. reply refurb 17 minutes agorootparentprevMoney laundering laws doesn't require that the money be proceeds of crime. It only requires that you do things in an attempt to obscure the source of the money. For example structuring cash deposits. If you have a pile of legitimate cash, say through a cash business (you pay all taxes), but you intentionally make multiple $9,000 deposits to avoid generating a CTR (currency transaction report for cash deposits over $10,000), congrats, you violated money laundering laws. reply dmurray 11 hours agorootparentprevYou could receive money legitimately (let's say, donations to your synagogue) and funnel it to politically disfavoured causes (for example, anti-war protesters in the Middle East). It doesn't fit the metaphor of money laundering, of turning dirty money into clean, but it's usually prosecuted under the same statutes. reply petertodd 11 hours agorootparentThat isn't what money laundering is. Money laundering is taking illegally gotten funds and turning it into apparently legal funds. What you're describing is fraud: you took money for one purpose, and used it for something the donators didn't want it used for. Notably, it is not money laundering to obscure the source of funds when they didn't come from an illicit source. Nor is it money laundering to simply obscure the source of funds, so they don't come from any apparent source. If that were money laundering, it would be illegal to withdraw cash from an ATM, which is obviously absurd. reply dmurray 10 hours agorootparentNo, the donators gave you the money to use as you see fit. You're not betraying their trust: you're betraying your government, by funding a movement with aims inimical to them. A relevant statute in the US is the USA PATRIOT Act [0], specifically Title III, which deals with \"International money laundering abatement and terrorist financing\". It absolutely does restrict transmission of clean money to non-favoured political entities, often in the same breath as it forbids actual money laundering. It does not forbid withdrawing cash from an ATM, but it does forbid a bank from issuing you an ATM card if they should have guessed you would use the money for the wrong purposes. [0] https://www.congress.gov/107/plaws/publ56/PLAW-107publ56.htm reply webspinner 8 hours agorootparentThe PATRIOT act is bad, and it needs to go the way of the creamitorium. reply bagels 10 hours agorootparentprevWhat statute? I don't know either way what the actual definition is. reply CPLX 10 hours agorootparentprevMoney laundering can be defined as concealing either or both the source and destination of the funds. Channeling legal money into illegal activities qualifies as well. Not clear either is applicable to this case however. reply yieldcrv 6 hours agorootparentprevmmm actually no. money laundering - the federal crime - relies, absolutely relies on their being an illicit origin. it cannot be charged independently and relies on discovering there was an illicit origin and moving to prosecute that. think harder about it and successful money laundering is impossible to be convicted of as an illicit origin is never discovered and probable cause is never established to get the subpoenas. seems that nobody ever imagined that the jury convicts someone on the money laundering charge and the judge acquits on the illicit origin charge amongst others. this does seem to require an appeal to rectify and everyone agrees he has a big chance of this getting dropped on appeal too, since its just dangling disconnected from its requirements. the other things you’re talking about would be prosecuted under different laws. reply refurb 9 minutes agorootparent> money laundering - the federal crime - relies, absolutely relies on their being an illicit origin. it cannot be charged independently and relies on discovering there was an illicit origin and moving to prosecute that. There are money laundering laws that require it be illicit funds. But there are also transaction reporting requirements - like generating a currency transaction report (CTR) - that don't require the funds to be illicit. It just requires that you attempt to circumvent them. \"The transaction is designed to evade any regulations promulgated under the Bank Secrecy Act\" reply webspinner 8 hours agoparentprevThat's a question and a half. Since he reported it, it isn't hidden. reply underseacables 12 hours agoparentprevOverzealous prosecution. reply azinman2 11 hours agorootparentHow do you know that? reply busterarm 10 hours agorootparentBecause we know that trying to destroy Backpage was the personal crusade of Cindy McCain and it became very personal. This has long been written about. reply paganel 21 minutes agorootparent> She is the widow of U.S. Senator John McCain from Arizona, who was the 2008 Republican presidential nominee. Nepo-wife. And they were saying the Soviets were a decrepit society at the top, the US from today is not much better. reply beaeglebeachh 9 hours agorootparentprevAny links? John bang a Backpage ho, or why the hate? reply etc-hosts 7 hours agorootparentthere's tons in Arizona media. https://www.phoenixnewtimes.com/news/michael-lacey-jim-larki... reply nullc 9 hours agorootparentprevAmong other reasons, because the DOJ accidentally leaked their internal communication about the case to the defense which turned out to be fairly exculpatory: https://www.documentcloud.org/documents/6345276-Backpage-DOJ... https://www.documentcloud.org/documents/6345275-Backpage-DOJ... The defendants are precluded from using or mentioning any of this information even though it shows that the case is being wrongfully prosecuted because it is the state's attorney client privileged material which was released by mistake. reply reaperman 5 hours agorootparentIt's crazy to me that our judicial system would be happy to knowingly find innocent people guilty, just because they feel like it's more important to pretend that exculpatory evidence simply doesn't exist. reply BlackFly 3 hours agorootparentIf you want a silver lining then consider that, unlike most trials, you know with much more certainty which is the just result. When that is not reached, or reached in a roundabout uncertain way (like the ongoing money laundering conviction) then you learn about the ways in which the legal system fails to be a justice system. Arguably we learn about things we should fix. reply bawolff 4 hours agorootparentprevI think its the lesser of two evils. reply starspangled 3 hours agorootparentThe greater evil being what? reply echelon 10 hours agorootparentprevnext [6 more] [flagged] schoen 10 hours agorootparent> All of this stems from religion, yet our system is supposed to have a separation of church and state. Restrictions on prostitution are pretty far downstream of core church-state separation territory. The original constitutional prohibition is \"Congress shall make no law respecting an establishment of religion\" (\"establishment clause\") \"or prohibiting the free exercise thereof\" (\"free exercise clause\"). The core territory of the establishment clause is that there can't be an official state church or an officially designated state religion. Later court cases extended that by analogy to require fairly broad religious neutrality, as well as to avoid various forms of \"entanglement\" between church and state. But so far, there's no official interpretation of it that implies that laws that were originally motivated by religious sentiment or religious morality are invalid. (If I remember correctly, Scalia's dissent in Obergefell expressed concern that the majority was gesturing in that direction, although the majority definitely didn't phrase it that way.) Particularly, there's no clear constitutional law that all legislation always has to be justifiable in terms of addressing a harm (usually that only comes into play when another constitutional right is implicated). I agree with the idea of legalizing prostitution and I agree with the idea of separation of church and state, but I don't think you can directly get one from the other. reply webspinner 8 hours agorootparentYou could in the 1930s, but I think we keep these laws on the books for cops. To give them something to do when they're on the job. reply throw10920 7 hours agorootparentprevThis comment clearly breaks the HN guidelines: > Eschew flamebait. Avoid generic tangents. Omit internet tropes. https://news.ycombinator.com/newsguidelines.html reply dgfitz 10 hours agorootparentprevThe state of Nevada (10 of 17 counties at least) would like to inform you that it has already happened. reply rayiner 8 hours agorootparentprev> All of this stems from religion, yet our system is supposed to have a separation of church and state. That phrase appears nowhere in the U.S. constitution. The first amendment guarantees the free exercise of religion (which includes voters having religious motivations for supporting otherwise permissible laws). It also precludes regulating state established churches, which were a thing at the time. (The more expansive modern interpretation favored by some was created out of thin air in the 1940s by a former KKK member: https://scholarship.law.upenn.edu/cgi/viewcontent.cgi?params... (pp. 588-589) It’s one of the shoddier Supreme Court opinions, basing its understanding of the First Amendment on the comments of Thomas Jefferson, who was in France when the amendment was drafted. Jefferson was always a Francophile and favored the revolutionary French view of religion—which does embrace a separation of church and state—but that has little to do with what the constitution says.) All that being said, even the more expensive view of the Establishment Clause doesn’t prohibit legislators from having religious motivations for laws otherwise within their power to enact. In that context, religion is no different than any other ideology (socialism, capitalism, etc) that might motivate legislative behavior. “Sex is shameful” isn’t any more of an improper basis for legislation than “free markets are good” or “society a an obligation to support poor people.” reply wmf 12 hours agoprevI feel sorry for any jury who has to keep track of 86 counts. The Ross Ulbricht and SBF trials look simple by comparison. reply Tomte 3 hours agoparentThere is a great judgment of Germany‘s highest courts from some years ago, where a criminal verdict was reversed and remanded. Almost in passing the court noted that the (professional) judges should have been more careful. Of the xx counts, some were acquitted, some convicted, some both, and some neither. Of the convictions the lower court had sometimes given prison term y in the upper part of the judgment, but prison term z lower down in the text. And so on. The higher court dryly wrote that special care has to be given when dealing with many counts. reply webspinner 8 hours agoparentprevI've seen state cases that have come close to that. They're usually mass shootings, or someone ran over 75 people with a car. reply yieldcrv 11 hours agoprev> The first [trial], in 2021, was declared a mistrial after prosecutors and their witnesses couldn't stop talking about sex trafficking despite none of the defendants facing sex trafficking charges Thats always been my take on the backpage case, but I didnt feel comfortable talking about it because people are too emotionally invested in curbing sex trafficking We’d be better off just treating it as labor trafficking, and not bothering with the non-trafficked people just like the rest of the job market. reply lupire 10 hours agoparentEstimated no one cares about labor trafficking. They care about other people having sex. reply busterarm 10 hours agorootparentThe irony of this being Arizona we're talking about and the significant contribution of illegal immigrants to the state economy. reply stavros 9 hours agorootparentWhy is that ironic? Of course nobody will care about stopping labor trafficking if it benefits them. reply vuln 8 hours agorootparent> Of course nobody will care about stopping labor trafficking if it benefits them Same with sex work it seems. reply verve_rat 2 hours agorootparentSex worksex trafficking. reply _DeadFred_ 9 hours agorootparentprevThey care about people being forced to engage in sex. Many people are trafficked into doing something they do not want to do by a partner or someone they think cares about them (look at the Andrew Tate situation). reply concordDance 2 hours agorootparentAfter this comment I finally looked up Andrew Tate on wikipedia and was surprised to discover he has not actually been convicted of sex trafficking yet. Had the impression he had been... commenting in case it surprises anyone else. reply lmm 8 hours agorootparentprevRight, and they abuse anti-trafficking laws as a way to punish people for having sex. reply yieldcrv 7 hours agorootparentprevThe US and states have very active labor trafficking investigations, raids, busts, outreach, you name it. There are deep investigations many into outfits quite regularly, involving the FBI and other agencies. If it feels the opposite, it turns out we just have a large labor trafficking problem, which includes adult services/entertainment. reply solumunus 6 hours agorootparentprevI can’t tell if you’re saying that sex trafficking doesn’t exist, isn’t a problem or that people don’t care about it. All positions are bonkers, but which one? reply yieldcrv 4 hours agorootparentthey’re saying that “downgrading” the enforcement approach of sex trafficking to just labor trafficking would mean all the enforcement and task forces go away which I disagree with reply imperio59 8 hours agoparentprevnext [3 more] [flagged] fader 8 hours agorootparentYou just proved the parent's point > none of the defendants facing sex trafficking charges responded to immediately with > This sounds like an apology for sex trafficking It's another formulation of \"why keep secrets if you have nothing to hide\", but followed with \"and why do you support sex trafficking\" for extra emotional impact (and libel). If you really wanted to stop sex trafficking, you'd oppose FOSTA/SESTA[0]. Backpage was protective of sex workers! But why let the facts get in the way of a good moral outrage? [0]: https://decriminalizesex.work/advocacy/sesta-fosta/what-is-s... reply yieldcrv 7 hours agorootparentprevlabor trafficking investigations address slavery, coercive situations and forced situations rape investigations address rape reply ashish10 4 hours agoprevSuch a sheer waste of time, money and efforts which could have been directed elsewhere. reply ALittleLight 12 hours agoprevSeems like there should be an optional trial end, in addition to conviction or acquittal, for \"prosecution shouldn't have brought this case\", where the accused would get some kind of compensation. I don't know the details of this case, but I remember hearing about it many years ago. I can't imagine the expense and stress of such a trial, dragging on for so long, and then at the end it's just \"Okay, guess you're not guilty.\" Sometimes it seems like prosecutors brought a reasonable case given the evidence and tried it fairly. In this case, I believe acquittal is the right call. Other times it seems like a case that shouldn't have been brought and you should be compensated for that and/or the prosecutor should be punished. reply petertodd 11 hours agoparentNo need to make it optional. If the state can't convict on all counts, you should get full compensation for all expenses, including your legal bills, any time spent in prison, lost income, etc. In a case like this it would be millions of dollars, maybe tens of millions. Of course, this case probably wouldn't have happened in the first place if that was the way prosecutions worked... Most criminal cases are open-and-shut deals where the defendant is clearly guilty, so this wouldn't change much on average. But it would ensure that 1) prosecutors only prosecute what they can actually prove, 2) innocent peoples' lives aren't destroyed. reply mywittyname 11 hours agorootparentThis is one of those things that sounds good in theory, but is terrible in practice. It would incentivize prosecution to avoid court at all costs. Which means de facto immunity for wealthy people who can easily afford to go to court. And it will probably make law enforcement even worse about railroading people who can't afford any legal representation. reply petertodd 11 hours agorootparent> It would incentivize prosecution to avoid court at all costs. Their job is to prosecute. Their only option is to pick cases that can be won. Which isn't hard, as the average person who is charged with some criminal act is not only guilty, but clearly guilty. > Which means de facto immunity for wealthy people who can easily afford to go to court. Money can't make a guilty man innocent. If you've actually done something wrong and there is evidence against you, the prosecution should have no qualms about prosecuting. In rare cases bad luck might lead to a payout due to a botched case or other unusual circumstance. But governments have enough money to self-insure for rare cases like that. It probably wouldn't even be a once-in-a-career event for the average prosecutor. > And it will probably make law enforcement even worse about railroading people who can't afford any legal representation. Rather the opposite: since full compensation is guaranteed if you win, it would be much easier to get a lawyer to work on contingency if you can convince them you are innocent. Right now that is very hard because even if you win, it's quite difficult to get the state to pay your legal bills so lawyers have no incentive to help you. reply wheelerwj 10 hours agorootparentnext [2 more] [flagged] mindslight 10 hours agorootparentFunny. OP sounds like someone who has experience with the justice system to me. Most everyone else isn't so painfully aware of the personal collateral damage the justice system causes. reply delfinom 10 hours agorootparentprevBut they already avoid court at all costs. It's why plea bargins are such a problem. reply MRtecno98 4 hours agorootparentprevTo add to what other people said: we're talking about criminal trials, here \"on average\" isn't enough. Criminal procedure requires such a high standard of certainty(proving without a reasonable doubt, unanimity of jurors) because unlike civil trials it actually sends people to jail. So being right \"on average\" isn't enough because we can't afford to punish innocent people, we would rather not punish a guilty person than wrongfully punish an innocent one. reply gnicholas 11 hours agorootparentprevWe have a very high bar for guilt in criminal trials. This is because we would rather have many guilty people go free than send an innocent person to prison (there are still mistakes, of course). Given this situation, it wouldn’t make sense to assume anyone who is not convicted on even one count is actually innocent. reply petertodd 11 hours agorootparent> We have a very high bar for guilt in criminal trials. If we do, then my suggestion should be easy to implement and will only impact a tiny minority of cases. Of course, in this case it's pretty clear that the prosecution made up a bunch of charges that they knew they'd lose on to try to drain the resources of the people they were charging, as well as punish them pre-emptively. The prosecution also clearly broke the rules in other ways, eg by getting a mistrial when they kept on bringing up sex trafficking, a crime the Backpage founders simply weren't charged with. There are plenty of actual criminals in the world that need to be prosecuted. This case is clearly a politically motivated exception. reply gnicholas 8 hours agorootparentSounds like a false dichotomy between \"cases that will always result in guilty verdicts\" and \"cases that the prosecution knew they'd lose but brought anyway to drain resources\". You don't seem to recognize that there are many, many cases that could go either way, based on how judges rule about evidentiary matters, what the composition of the jury is, etc. As someone who was a lawyer for years, and who has served on a jury, I have seen the ways that things can evolve in unexpected ways. IMO the vast majority of cases could go either way, depending on how rulings and jury composition turn out. Any system that doles out taxpayer money whenever the prosecution doesn't run the table is utterly naive. It would result in less prosecution, more criminals going free, and more victimization by people we didn't lock up. reply iraqmtpizza 6 hours agorootparentYeah, cause then prosecutors would actually have to look for exculpatory evidence before charging instead of just sitting on their asses and relying on juries assuming guilt (because why else would someone be on trial?) reply MRtecno98 3 hours agorootparentThat... sounds like an unreasonable bias to me reply FireBeyond 11 hours agorootparentprev> This is because we would rather have many guilty people go free than send an innocent person to prison No we wouldn't. Or we wouldn't utilize the plea deal in 93% of criminal cases. And we use it with little-to-no oversight. Unless what a prosecutor offers is so -utterly egregiously inappropriate- that it causes a judge to double take, they can pretty much do as they like. Which is why we use it more than 60x more per capita than any other country on earth, in those countries that even allow it (it's not even officially sanctioned in the UK, though it has happened - fun fact, nearly 40% of the cases appealed as a \"miscarriage of justice\" in the UK involved plea deals). > A government spokesperson said: “Ensuring defendants plead guilty at the earliest possible opportunity means victims and witnesses do not have to relive their potentially traumatic experiences in court.\" What? Not one word of innocence or presumption. Just \"plead guilty early, we know you did it\". Even in countries that do use plea deals more often, there's strict oversight into what the deals entail and understanding of outcome. Here, there's no incentive to rock the boat. Prosecutors push plea deals HEAVILY, innocence be damned, threatening the costs and risks of a trial (and the US over-charges people heavily) with a quick plea (that ever so conveniently allows our elected prosecutors to point to high conviction rates every re-election). > it wouldn’t make sense to assume anyone who is not convicted on even one count is actually innocent Oof. Not only is it \"possible\" you're not actually innocent, your attitude is \"it doesn't even make sense to assume that\". Screw it, why do we need a justice system? You were arrested, let's just take you straight to prison. reply gnicholas 8 hours agorootparent> Oof. Not only is it \"possible\" you're not actually innocent, your attitude is \"it doesn't even make sense to assume that\". Screw it, why do we need a justice system? You were arrested, let's just take you straight to prison. Of course it's possible. But if someone is charged with 5 crimes and convicted of 4, why should we reimburse their legal fees? That is what GP proposed, and it would be nuts. If the prosecution struck out and went 0 for 5, that might make sense. But requiring them to run the table makes no sense. reply lazide 11 hours agorootparentprevCan you imagine the public angst around the OJ trial if THAT was what happened? reply petertodd 11 hours agorootparentI'd rather people like OJ get some money out of a botched prosecution than the alternative. Most likely, we'd see a lot less botched prosecutions. reply mywittyname 11 hours agorootparentHow many botched prosecutions are there? Most have a 90%+ conviction rate because normal people can't stand up to professional interrogation. Even innocent people can incriminate themselves to the point where it isn't worth the risk of trial, so they plead to a lesser offense. Spend thousands going to court and possible spending 5 years in jail vs. plead out for probation. The fact that going to court and winning might see you get legal fees plus a bit of money back doesn't change the calculus -- the worst case outcome is still prison. reply petertodd 11 hours agorootparentIt certainly changes the calculus: it's much easier to get a lawyer willing to work on contingency if there is a financial reward for winning. In civil cases it's quite common for lawyers to work on contingency because so many civil cases are obviously winnable, and have an immediate payout. Of course, in practice what really would happen is it would be far less common for prosecutors to prosecute people when there isn't a solid case against them. If what I'm suggesting was how criminal cases worked, I doubt that Backpage would have been charged at all. Note that I also think that plea deals should be much less common, or even totally banned. reply webspinner 7 hours agorootparentprevSimpson would be more deserving of it than most anyone. reply lupusreal 39 minutes agorootparentprevIn the trial where somebody is accused of first degree murder and they are instead convicted of second degree murder, the murderer deserves all that compensation? You haven't thought this out well. reply eitland 11 hours agoparentprevIn Norway we have two different \"not guilty\" outcomes: - \"frifunnet på grunn av bevisets stilling\" (~\"acquitted because of the situation with the evidence\" my best translation at 00:02 in the night) which I think in practice means it wasn't proven that the person did it and so (s)he is acquitted. - \"henlagt som intet straffbart forhold\" (~\"acquitted as no punishable offense\" my best translation at 00:04 in the night) which I think means the investigation not only failed to prove guilt but also proved that a person was not guilty. I tried to find better translations but wasn't able to. Feel free to fill in. reply schoen 10 hours agorootparentThis probably matches reasonably well with the three-verdict situation in Scots law. https://en.wikipedia.org/wiki/Not_proven > Under Scots law, a criminal trial may end in one of three verdicts, one of conviction (\"guilty\") and two of acquittal (\"not proven\" and \"not guilty\"). reply webspinner 7 hours agoparentprevIt's definitely one of those ridiculous cases, that's for sure. The feds seem to bring a lot of those. reply webspinner 8 hours agoprevWell he should be acquitted on all counts to do with the founding of the site. If he laundered money, that isn't good. reply kaliqt 6 hours agoparentYou have got to be kidding me. The government is the biggest criminal, this guy is nothing, this shouldn't even be a discussion. reply nailer 10 hours agoprevAmerica is bizarre. Technically prostitution is illegal in my state, Googling “new York city escort” shows thousands of results. reply ahazred8ta 8 hours agoparentBarney Miller had an episode where a gal was delivering I♥NY buttons to hotel rooms for $75 per button. reply superkuh 13 hours agoprevTypical FBI. Make up dozens and dozens of bogus charges and commit to constant harassment until enough of the people targeted kill themselves. Then when a real judge finally gets involved slink away. Just like with Aaron Swartz and Qwest CEO Joseph Nacchio. Dear people flagging this, please stop perceiving my statements through your domestic political lenses. I am not talking politics here. Just because your team is currently pro-FBI or against-FBI doesn't mean I am talking about how the FBI interacts with your team. I am not. reply banish-m4 12 hours agoparentThe police-prison-industrial complex is motivated to convict and keep millions of people incarcerated. Apropos book: Three Felonies a Day by Silerglate reply petertodd 12 hours agoparentprevIt would help a lot if charges were an all or nothing basis. Either a jury convicts on all counts, or you go free, with all your legal bills paid and compensation for the false arrest and imprisonment. Most criminal cases are open-and-shut deals where someone is clearly guilty of something, so for typical cases this policy wouldn't change much. But it would significantly reduce the power of government to harass innocent people. reply underseacables 12 hours agorootparentI agree with part of your statement. I think if you were found not guilty, the state should have to pay your legal fees. It's an unfair fight against the full might of the government. reply petertodd 12 hours agorootparentAnother way to do it would be to require the state to provide funds for legal defense for everyone, regardless of income or assets, with at least with the same budget as they use for the prosecution. Again, this won't change anything in the typical case, because the typical criminal case involves something who is clearly guilty. But it'll keep innocent people out of jail. reply throwaway173738 4 hours agorootparentOr we could hire anyone who needed it a lawyer for their defense. We could call them public defenders. reply okasaki 12 hours agoparentprevIndeed - https://en.wikipedia.org/wiki/FBI%E2%80%93King_suicide_lette... reply busterarm 12 hours agoparentprevIt also doesn't help here that the McCain family has personal grievance with the Backpage founders and used all of their influence to sic the full force of the federal government after them. But hey, John McCain spent his whole life being a piece of shit. Nothing new. reply jrflowers 12 hours agorootparentIs there documentation of this? reply busterarm 11 hours agorootparentThis is answerable with really the most basic of google searches. reply javawizard 1 hour agorootparentI'll save everyone else the Google search then: https://www.phoenixnewtimes.com/news/michael-lacey-jim-larki... reply jrflowers 10 hours agorootparentprevMaking grandiose claims and then tut-tutting anyone that dares ask for simple clarification is a heck of a hobby. The rush is like skydiving I’ve heard reply busterarm 10 hours agorootparentI can't help you if you're this bad at google. All you have to do is google the terms mccain and backpage and you'll get all the reading material that you desire. This information is table stakes for anyone who has been following this story over the years and I'm not your link retrieval service. Lacey and Larkin ran a newspaper Phoenix New Times that was on McCain's case from the very beginning of his political career in 1981. Mainly because they thought it was distasteful that he divorced his wife so that he could remarry to a rich heiress who would bankroll his political dreams. They reported on things like Cindy's opiate addiction and how she stole prescription narcotics from the medical-aid charity that she was running way back in '94. The McCain hatred for them is DEEP. There's nothing grandiose about this. reply jrflowers 9 hours agorootparentPersonally I think it’s totally normal to take time out of my day to insult strangers for asking simple questions online. It may take considerably much more work than just posting a link, but at least as the end of the day you’ve entirely avoided educating or influencing opinions, which is the goal when making a point on the internet reply busterarm 9 hours agorootparentYou're taking the time to act like I'm pulling shit out of thin air when you can just type 14 characters into your search bar. reply jrflowers 9 hours agorootparentWhile posting a link would be an instantaneous satisfaction of a simple request, that would rob one of the chance to invent an argument that no one is having and rebut things that no person has said, which, again, is the purpose of making a point online reply busterarm 9 hours agorootparentnext [2 more] [flagged] jrflowers 9 hours agorootparentIf only there was a way to prove that something isn’t made up that could assuage your feeling that your point wasn’t received the way you wanted it to. Alas no such possible action exists, the only option is to “abuse” a stranger reply huytersd 11 hours agorootparentprevMcCain is a goddamn hero. You’re borderline regarded to even post something like this against an American hero. reply busterarm 10 hours agorootparentI'm a big supporter of the military, but McCain is no hero. There's a long list of people who served with him who hated his fucking guts. Man was a showboat and a habitual liar. Most of the press reported this way until he did the in vogue thing and went against Trump. \"Oh I'm such a Maverick\" is short for not doing what your constituents want ever. McCain was the most spineless, immoral bastard in the last hundred years of American politics...followed in a close second and third by our current and most recent former president. You're literally calling Mr \"Next Up, Baghdad\" and \"Bomb, Bomb, Bomb, Iran\" a hero. The guy was incoherent. reply huytersd 6 hours agorootparentShowboat and habitual liar in the same sentence as Trump without being directed at Trump is quite an achievement. reply busterarm 5 hours agorootparentOf the many negative things I can say about Trump, at least it's not that he was a bloodthirsty war hawk whose preferred first step in conflict resolution was to send bombs. We should thank our lucky stars that we avoided a McCain/Palin presidency. That isn't brought up enough either -- the absolute carnival sideshow he chose for a running mate. reply huytersd 4 hours agorootparentI’d rather a person willing to go to a reasoned war to maintain US hegemony over someone that actively ruins US global hegemony while dismantling democracy at home. reply huytersd 11 hours agoprev [–] Good. Prostitution should be legal and regulated to begin with. People are getting HIV at an alarming rate because the whole system is unregulated. reply hi-v-rocknroll 10 hours agoparentRaising HIV against prostitution is like bringing up animal cruelty for veganism: it's not really the most important of many overlapping concerns surrounding the issue. Anti-prostitution and anti-abortion laws in the US exist because evangelical Christian white supremacists are able to tell heathens what they can't do with their bodies and pass laws to force their views on other people. These come with the burdens of jail time in the case of prostitution or losing the ability to reproduce or one's life in the case of eliminating the possibility of emergency abortions. Most of America: \"The Bible says...\" reply huytersd 6 hours agorootparentIf I’m not mistaken, animal cruelty is the number one reason for vegans being vegans. What exactly are you saying? reply hi-v-rocknroll 5 hours agorootparentWhile it should be a top reason, it's an extremely unpersuasive reason in the real world because people want their bacon burgers so long as they don't have to see what happens in a slaughterhouse or pay for the total cost of externalities of their consumption lifestyle choices. There are much more pressing, self-interested reasons to advocate veganism: 1. Meat ag and bushmeat lead to pandemics: bovine tuberculosis, \"Spanish\" flu, Nipah, SARS, H7N7, H1N1, MERS, COVID. There are others. 2. Meat ag abuses the same antibiotics humans take (but manufactured in the veterinary supply chain) for purely economic reasons (mostly, to make animals grow faster) leading to the evolution of antibiotic resistant microbes and making antibiotic medications in humans less effective. 3. Climate change. It contributes 1/6 of all human GHG activity. 4. Air, soil, and water pollution are the result of mega grain farms, large pig farms, and CAFOs. 5. It's resource intensive and drives up the cost of all food by shunting more water, diesel fuel, arable land, etc. to less output. 6. Along with palm kernel oil, it's a leading driver of old growth deforestation. 7. And it hurts both the cute and the ugly critters. reply Marsymars 4 hours agorootparent> While it should be a top reason, it's an extremely unpersuasive reason in the real world Well it’s conditionally persuasive depending on the animal and people’s priors. Dogs vs horses vs pigs, etc. Agreed on all your points though. reply hi-v-rocknroll 4 hours agorootparentYeah. Sigh. I'm realistic because the \"meat is murder\" with gross-out signage of slaughterhouse porn billboard-sign-on-cars approach hasn't really taken off. In other news, Moolec is developing GMO \"Piggy Sooy\" to taste like pork by inserting pork genes into beans. (I'm wondering: will it be Halal or Kosher?) Impossible Foods' \"beef\" relies on heme from GMO yeast, while Moolec plans to have GMO \"beef\" too. reply blitzar 2 hours agorootparentyeast is an organism too - bread is murder reply WorkerBee28474 7 hours agorootparentprev> evangelical Christian white supremacists Ah yes, the ol' reliable 'everyone who disagrees with me is a Nazi' approach reply yareal 6 hours agorootparentI think they described the situation fairly? The folks who legislate against sex work tend to tick most of those boxes. reply hi-v-rocknroll 5 hours agorootparentTo be fair, there is real shade to throw at traditional neoliberal uppercase Democrats of the 90's-10's too: copying features of conservatism but then putting off most of the country with bicoastal elitism. The Clintons and Reagans were both out of touch and announced policy on issues without much community engagement. Sex work is legal and more-or-less regulated in most of the UK and Europe even where Le Pen-flavor nationalism is ascending in prevalence. The US has a special kind of polarized infighting and much more variance in rights and restrictions that the rest of the world. For example, if I were to say \"I'm pro conceal carry guns for people who continue to show demonstrably sane and stable residents.\" this wouldn't fly in most of Europe or Australia, regardless of right or left party across the way except for perhaps the fringe far-right. That is to say: norms in America differ significantly from the rest of the world in sometimes disquieting ways to outsiders, due to enduring historical roots and political interests. Perhaps a compromise to placate the majority of Americans would be: \"don't touch the right to bear arms in exchange for bodily autonomy and healthcare.\" The 1995 Oklahoma City bombing didn't happen in a vacuum. It was 100% homegrown terrorism that thrived and came to pass in a culture of government conspiracy theories, Waco, white nationalism, and failure to care for and reintegrate Gulf War vets. Chris Hedges wrote a book on the topic: American Fascists (2008). Excluding Jan 6th as different kind of event, that there hasn't been another domestic terrorism mass casualty event due to diligent work by law enforcement combined with a tinge of dumb luck. reply yareal 5 hours agorootparentI'd argue plenty of democrats fit that description as well. > Perhaps a compromise to placate the majority of Americans would be: \"don't touch the right to bear arms in exchange for bodily autonomy and healthcare.\" Most of the leftists I know are extremely strong opponents to gun control. Marx was pretty clear on the importance of an armed working class, and leftists tend to take that to heart ime. reply AuryGlenz 5 hours agorootparentprevWhite supremacists? Really? And something tells me Muslims, practicing Jews, etc. aren’t a fan of prostitution either. So really you could just say “religious people,” and it’d be far more accurate. reply yareal 5 hours agorootparentDo we have many conservative Muslim or Jewish legislators? I would figure they tend to come from more progressive districts... reply hi-v-rocknroll 4 hours agorootparentDon't be surprised. I'm sure there is at least one Jainist Republican American. Religion and conservatism seem to be correlated positively with one another. No particular group or identity membership mandates membership in another. And I'm sure there are a zillion libertarian agnostics and atheists who follow some styles of the broad tent of conservatism without the authoritarian forcefulness of others who want to limit the rights of others, on both sides of the political spectrum. It's my unpopular and minority opinion that the world would be better without some or most religions but would be better off with the trappings of what churches provide: community, friend networks, singing together, reflecting on philosophy, and mutual aid. Although, I'm absolutely sure the world would be better without the cult of Scientology. https://www.scientology-austin.org/about-us/grand-opening.ht... reply iraqmtpizza 6 hours agorootparentprev'laws against child murder are because of white supremacists and their barbarous religion' reply throw_m239339 3 hours agoparentprevThis wasn't prostitution on trial, it was a bunch of pimps and human traffickers. Just because they \"are just an app\" doesn't change the nature of their crime. Not sure what your comment HIV has to do with anything either when it comes to that case. reply lupire 10 hours agoparentprevHIV turned out to be easier to treat than prevent. reply hi-v-rocknroll 9 hours agorootparentSort of. PrEP exists for high risk populations. For cures: Currently, it is if you're able and willing to wipe out your own bone marrow and able to find a compatible donor who's immune. 2 giant expensive and risky IF's. CRISPR personalized medicine may also work: https://www.technologyreview.com/2023/10/25/1082306/gene-edi... reply huytersd 6 hours agorootparentNo one does that surgery unless you have very advanced lymphoma. It’s not a cure. reply hi-v-rocknroll 4 hours agorootparentYou're weaseling with the ambiguity of the word \"cure\". I already stated the extreme conditions that approach would require. It has resulted in deliberate, although incidental, cures several times. It was certainly a cure for them. CRISPR personalized approaches are more promising by precisely excising vDNA. There is no panacea here. reply huytersd 9 hours agorootparentprevIf treat means a lifetime of a much higher OI risk and a 2000% higher risk of cancer. reply robocat 7 hours agorootparentOI? Please expand acronyms: we're not all in your context. reply huytersd 6 hours agorootparentOpportunistic infections. TB is the number one killer of people with HIV that are medicated. Along with a host of other viruses, fungi and MRSAs. You might live for 30 years after contracting HIV but they’re going to be sick, medically expensive years. reply yieldcrv 6 hours agoparentprev [–] I personally like the legal and regulated approach, specifically because it comes with routine worker testing and consumer protections and labor rights apparently sex workers like decriminalized, because non-compliance with the regulated regimes have so far been a parallel illegal system but I cant think of any industry we would listen to who answered “how about just a completely hands off decriminalized approach” if asked to choose between illegal, unregulated, legal and regulated I don’t think decriminalized comes with adequate consumer protection, and the problems with legalized systems can be addressed by fixing how non-compliance is penalized, and by providing incentives to join the regulated system instead of just penalties for not regarding trafficking, well keep prosecuting that. as labor trafficking. make more avenues of support for the laborer which is far easier with labor rights. what people find hardest to reconcile is the idea of giving up on preventing trafficking, nothing they support right now is doing that or capable of doing that while burdening everyone else. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A federal judge acquitted Backpage co-founder Michael Lacey of several counts, along with former executives Jed Brunst and Scott Spear, leading to a potential retrial for Lacey on remaining charges like conspiracy and money laundering.",
      "Despite the possibility of a successful appeal, Lacey could still face prison time for his involvement in international money laundering, while Brunst and Spear are awaiting sentencing on multiple counts."
    ],
    "commentSummary": [
      "The debate revolves around the legal case of Backpage co-founder Michael Lacey, including bias in news media and money laundering laws, touching on labor and sex trafficking.",
      "Discussions also cover compensation for the wrongly accused, legal reform, and the intersection of politics, religion, and social issues.",
      "Additional topics include the regulation of prostitution, veganism, healthcare, and HIV treatment, emphasizing the complexity of these issues and the necessity for balanced and accountable solutions."
    ],
    "points": 189,
    "commentCount": 140,
    "retryCount": 0,
    "time": 1714076050
  },
  {
    "id": 40159766,
    "title": "Connecting OLED Display to Laptop via DDC Protocol: Linux Challenges and Solutions",
    "originLink": "https://mitxela.com/projects/ddc-oled",
    "originBody": "DDC OLED 31 Mar 2022 Progress: Complete I have a proclivity to stupid and/or pointless projects. This is one of them. Conceived from a conversation that ended with \"Hey, it would technically be possible to...\" – sure, let's do it. DDC, display data channel, is a protocol for reading information about what resolutions and so on a monitor supports. It was later extended to DDC/CI, that lets you set brightness and other parameters, but fundamentally, the original idea was to stick a cheap i2c eeprom on each device with some basic info on it. (Technically, the original idea was even simpler than that, but let's not get into that.) It began in the VGA days, but has become so entrenched that even modern hardware with HDMI or DisplayPort supports it. That's right, in an HDMI cable, nestled amongst the high-speed differential pairs, there's an exceedingly slow i2c bus. Tiny OLED dot-matrix displays often have an i2c controller, so I had the idea to try and plug one directly into an HDMI port. Hilarious! Let's do it. Wiring I chopped up a broken HDMI cable and found the pins we care about: SCL, SDA, 5V, DDC-GND, and HPD (Hot Plug Detect). A quick google got us the pinout: This diagram shows an HDMI socket, if you're poking pins into the cable then flip left to right. HDMI Pin Number Signal 1 TMDS Date 2+ 2 TMDS Data 2 shield 3 TMDS Data 2- 4 TMDS Data 1+ 5 TMDS Data 1 shield 6 TMDS Data 1- 7 TMDS Data 0+ 8 TMDS Data 0 shield 9 TMDS Data 0- 10 TMDS Clock+ 11 TMDS Clock shield 12 TMDS Clock- 13 CEC 14 HEC Data- 15 SCL (Serial Clock for DDC 16 SDA (Serial Data Line for DDC 17 DDC / CEC / HEC Ground 18 +5 V Power (50 mA max) 19 Hot Plug Detect (1.3) / HEC Data+ (1.4) I've a tendency to choose low-risk options when it comes to hardware hacking, no one likes seeing blue smoke, especially if the dev board was expensive. Today though I feel like living on the edge, and I'm going to solder this display directly onto the severed HDMI cable coming out of my reasonably new laptop. What a thrill! If we mess up, this stupid experiment could be very expensive. You have to register to download the HDMI spec which is more effort than I have for this, but the Hot Plug Detect pin has a pretty descriptive name. I guessed that this either has to be pulled up or pulled down to signal that a cable is connected. Sticking a 20K resistor to the 5V pin seemed to do the trick. With the oscilloscope, we can now see activity on the SCL/SDA lines when it's plugged into the laptop. I then boldly soldered a header connector to the four lines we care about. I'd ordered a couple of OLED screens for this experiment, they both use the SSD1306 controller, and come on breakout boards with the four pins on a header. i2c and SMBus On linux we can access i2c devices by loading the i2c-dev module (modprobe i2c-dev) which makes a bunch of i2c devices appear at /dev/i2c-*. My laptop shows nine i2c devices. Some of these are in fact SMBus, which is a subset of i2c. As far as we're concerned it's just i2c with a bunch of extra restrictions, such as limiting transactions to 32 bytes. It's also worth installing the i2c-tools package which comes with the i2cdetect utility and sets a udev rule for group permissions. To access i2c devices without sudo, add yourself to the i2c group (sudo usermod -G i2c -a username) and log in again for it to take effect. I also had to run udevadm trigger for the udev rule to take effect. Might have been simpler to reboot (never!). Beware: the i2c device naming is not consistent. I figured out that /dev/i2c-3 was the HDMI DDC line I'd soldered to, but after unloading and re-loading the module, it became /dev/i2c-4. We need to be really careful about this, writing (or even reading) to the wrong i2c device could easily muck up some of the laptop hardware. I installed another package, ddcutil, only to be able to do ddcutil detect. This lists displays and their associated i2c bus. It's also possible to do i2cdetect -l which lists the i2c devices and their description. In my case, three of the i2c lines had \"i915 gmbus\" in their description, i915 is the intel graphics driver. ddcutil is still probably the easiest way to figure it out. Initial tests The scope showed the SCL/SDA lines are already pulled up, so we should be able to connect the screen without any other hardware. The 5V line on an HDMI port can apparently source up to 50mA, so we don't even need a power supply. Neat! i2cdetect can scan an i2c bus for devices. As expected, without the cable connected, it detected nothing on the bus. But when I connected my severed cable, with the hot plug detect resistor in place, a whole load of responses appeared. I don't know quite what's going on here (does the video hardware expose a bunch of stuff when the cable is connected?) but the important point is that when I connected the display, an extra device showed up at 0x3c. The quickest way to talk to the display is with a python script. The bundled smbus library lets us get going very quickly. import smbus bus = smbus.SMBus(4) # for /dev/i2c-4 i2caddr = 0x3c bus.write_i2c_block_data(i2caddr, 0, [0xaf] ) # turn display on There's a bunch of commands we need to send before we can actually display anything, including enabling the charge pump. Note that the SSD1306 datasheet, at least the copy I found, has an appnote appended onto the end of it that explains the initialization process more clearly than the main document (some of the commands are not documented in the main command table). As always, the fastest way to get going is to look at the source code to existing libraries, so I found somebody else's library for the SSD1306 and copied their init commands. The display sprang to life! I also found a script to draw text to an SSD1306, and quickly patched in my smbus stuff. Success! No microcontroller, no other hardware, just an SSD1306 OLED plugged straight into the HDMI port. I find this very satisfying. Dumping data to it Sticking with the python script for now, I'd like to be able to take a 128x64 pixel image and dump it onto the display. The text-drawing routine I borrowed uses SSD1306 commands to control the column and page address that data is being written to, so a single character can be drawn without affecting the rest of the display (hence the uninitialized background pixels remaining in that image above). There's a whole load of different memory addressing modes for this thing, along with confusing terminology. SEG or COL is the X coordinate, COM is the Y coordinate, but these are grouped into pages. The datasheet has some diagrams. The display is monochrome, each page is 8 rows (COMs) and when we pipe data to the display, each byte is one page, one column of pixels. It may have made more sense to configure the display for vertical addressing mode, so the bits would all be in order, but I figured it would be quickest to just do the bit-shuffling at our end. With python PIL (pillow) we can convert an image to monochrome with .convert(1) and serialize it with .tobytes(). This will have each byte represent 8 horizontal pixels, but we want each byte to represent 8 vertical pixels. Instead of doing some tedious bitwise logic, the fastest way to fix this is by rotating the image 90 degrees before we serialize it, then loading those bytes into a numpy matrix and transposing it. It's the kind of thing that either works perfectly first time, or outputs a complete mess, in which case you just permute the order of operations until it works. So much easier than thinking. As I mentioned, SMBus won't let us send more than 32 bytes at a time, even though this device is just plain i2c. We can get around this by accessing the i2c device directly from python. The trick is to use ioctl to configure the slave address. In the kernel header file i2c-dev.h there are definitions for the constants needed, we only care about I2C_SLAVE. import io, fcntl dev = \"/dev/i2c-4\" I2C_SLAVE=0x0703 # from i2c-dev.h i2caddr = 0x3c bus = io.open(dev, \"wb\", buffering=0) fcntl.ioctl(bus, I2C_SLAVE, i2caddr) bus.write(bytearray([0x00, 0xaf])) By alternately sending 1024 bytes of zero or 0xFF, I could gauge how quickly this updated the display. Seemed to work fastest by sending 256 bytes at a time, not sure if that's a limitation of the i2c hardware (is there some extra layer of buffering?). With this I could get between 5 and 10 frames per second (compared to about 2FPS with the SMBus limitation). I think the DDC is running at 100kHz, but regardless this is certainly pushing the limits of what it was intended for. Make it a monitor We could just write our application to draw directly to this screen, but that's not good enough, I want it to be a monitor. (I'm not sure what our application here even is, but that's beside the point. I want it to be a monitor!) We could write our own video driver. As educational as this sounds, it would be a colossal amount of work and I was rather hoping to have this wrapped up within the evening. There are a bunch of dummy video drivers in existence, these are intended for headless machines in order to enable VNC and so on. xserver-xorg-video-dummy may function for us, but I have a terrible feeling this won't play well at all with us also having real display outputs. There's Xvfb, a virtual framebuffer, but this won't do us much good if we want to have our desktop extend onto it. Since I'm using xorg, it seems the right way to fake a monitor, without spending days on it, is to go through xrandr. xrandr is both a library, and a userspace commandline utility. It took me a while to get to grips with the xrandr terminology. It's not particularly well explained. The \"framebuffer\" is the whole desktop, i.e. what gets saved if you take a screenshot. An \"output\" is a physical video output. A \"monitor\" is virtual concept, that normally is mapped to all or part of the framebuffer, and normally corresponds to one output. If you maximize a window, it fills the dimensions of the monitor. However, you can have one display output be more than one monitor (for instance, to split a widescreen display into effectively two monitors) Or, multiple outputs can be one monitor, i.e. multiple physical screens can be treated as if they were a single display, maximizing a window would cover all of them. a \"mode\" is a video format, consisting of at least width, height and framerate. Specifically, VESA CVT modelines are used, and can be generated with the cvt utility. xrandr's addmode and delmode refer to associating an existing mode with a display output xrandr's newmode and rmmode refer to adding a new mode to the server, that can then be associated with an output Note that this list is specific to xrandr, in other aspects of linux, the terms \"output\", \"display\", \"monitor\" and \"screen\" are often used differently. On my laptop, calling xrandr shows five video outputs: eDP-1, which is the main screen with a bazillion modes available, and four disconnected (HDMI-1, HDMI-2, DP-1, DP-2), presumably three of which are available via thunderbolt or something. Faking a monitor, attempt 1 Looking around, it seems the recommended way to do this is to convince xrandr that one of the unused video outputs is connected. For things like VNC there's a whole market for \"dummy plugs\" which make a video card think a monitor is connected. We obviously don't want or need to do that, we should be able to coax xrandr into behaving through software. In order to output our abnormally low resolution of 128x64 on HDMI, in theory we first generate a CVT modeline: $ cvt 128 64 # 128x64 39.06 Hz (CVT) hsync: 3.12 kHz; pclk: 0.50 MHz Modeline \"128x64_60.00\" 0.50 128 136 144 160 64 67 77 80 -hsync +vsync then we add this mode to the x server: $ xrandr --newmode \"128x64_60.00\" 0.50 128 136 144 160 64 67 77 80 -hsync +vsync At this point, xrandr shows the unused mode at the end of its output. Confusingly it looks like the mode is part of the last output listed, but it isn't (yet). We next add this mode to one of the outputs: xrandr --addmode HDMI-1 128x64_60.00 and finally try to use it: xrandr --output HDMI-1 --mode 128x64_60.00 --right-of eDP-1 I should point out, I've a hotkey on my laptop which cycles through sane display modes, so I'm comfortable trying whatever here, but otherwise there's a chance you end up unable to see anything. It should still be possible to access the other virtual terminals with ctrl+alt+F2 etc, since these configure the display using KMS (Kernel Mode Setting) that sits a layer below the X server. I tried this with both HDMI-1 and HDMI-2. Both of them are listed as disconnected. Our cable connected to HDMI-1 is pulling the Hot Plug Detect pin high, but not responding to regular DDC queries. I may not have exhausted all possibilities, but I couldn't get this to work. I suspect the video driver simply can't cope with this ludicrously nonstandard resolution, and the modeline is just junk. The 39.06Hz certainly raised one of my eyebrows. I tried again specifically setting the framerate to 39.06Hz also, to no avail. Honestly, abusing the video outputs like this feels like a poor solution anyway. To clean up this mess, first use --delmode to free up the modes from any outputs, then --rmmode to remove them from the X server. Faking a monitor, attempt 2 When you change display settings xrandr generally sets all the relevant settings automatically, but if we go deeper we can manually fiddle with them. Following another idea on the internet, we should be able to make a virtual monitor by simply extending the framebuffer, and defining a monitor to be there, without bothering to associate it with an output. Interestingly, if you make the framebuffer bigger than needed, by default it will automatically pan when your mouse approaches the border. Useful to know, but here we need to specifically stop that happening. The --panning option takes up to twelve parameters, for panning area, tracking area, and border. Tracking is the area our mouse cursor is limited to. Normally, panning, tracking and framebuffer are all set to the same size. I'm not sure what \"border\" represents in the context of panning, it didn't seem to have any effect when I played with it. Setting panning to 0x0 will disable it, but that also limits the tracking area, so our mouse won't be able to reach the new bit of framebuffer. Instead we limit panning to the size of the main monitor, effectively disabling it, and extend the tracking area into our new chunk of framebuffer. The full command: xrandr --fb 2048x1080 --output eDP-1 --panning 1920x1080/2048x1080 Then we can define a new monitor to exist in this new chunk of framebuffer: xrandr --setmonitor virtual 128/22x64/11+1920+0 none The size is set in both pixels and mm, I guessed it's approximately 22m by 11mm, it doesn't really matter though. \"virtual\" is the name of this monitor, we could call it anything. \"none\" is the output. We can see monitors with xrandr --listmonitors and later undo this muck with xrandr --delmonitor virtual. I can now point my script to dump that bit of framebuffer onto the OLED screen. Hurrah! One slight issue with this method is that the tracking is not L-shaped, my mouse can access the strip of framebuffer that doesn't correspond to any monitor. I don't know if there's an easy fix for this, but if it really bothered us we could enforce valid cursor positions through Xlib in our script. Reading the framebuffer I assumed I'd need to throw away the python script at this point but there's python-xlib which gives us access to most of what we need. It's a little irritating that there isn't really any documentation, and the method names are not identical, for instance XGetImage is now root.get_image. Here is some trivia: did you know that the mouse cursor is rendered by hardware? It makes sense, I suppose. It also explains why the mouse cursor isn't normally captured when you take a screenshot. But we want to capture the framebuffer and the mouse on top of it so there's a lot more work involved. Getting the cursor image would normally be achieved through XFixesCursorImage but python-xlib hasn't yet implemented all of XFixes. I was prepared to start over in C until I spotted someone's done all the work for me with this repo which binds to X11/XFixes using ctypes specifically to get the cursor information. We now have everything we need to capture the new virtual monitor image, superimpose the cursor in the right place (remembering to adjust for xhot and yhot, the pointer/cursor image offset), convert the result to a monochrome image with the right amount of bit-shuffling and pipe it to the display continuously. That's i3 workspace four, with a completely crushed i3status and incomprehensible dithered top corner of my background image. Beautiful! Demo Conclusion To improve the framerate, we could enhance our script and only send the changes instead of redrawing the display each frame. As fantastic as this could be, given that I have absolutely no use for this tiny second screen anyway, I'm not particularly inclined to make it happen. If for some mad reason you want to try this out yourself, the script can be found on github. Update: How can we make the smallest and worst \"HDMI\" display even sillier? Make it steampunk. ~ mitxela.com » Projects » Hardware » DDC OLED Questions? Comments? Check out the Forum Support mitxela.com",
    "commentLink": "https://news.ycombinator.com/item?id=40159766",
    "commentBody": "DDC OLED (2022) (mitxela.com)184 points by fanf2 16 hours agohidepastfavorite20 comments dang 13 hours agoDiscussed at the time: The smallest and worst HDMI display - https://news.ycombinator.com/item?id=30869140 - March 2022 (162 comments) reply sgroppino 14 hours agoprevNice one :D reminds me of my little project back in 2020 with a raspberry pi and a similar type of display... https://news.ycombinator.com/item?id=25566132 reply amelius 15 hours agoprevHere's a video about making an OLED device from scratch, and all the physics involved: https://www.youtube.com/watch?v=qg8pMUd-tSk reply kurthr 12 hours agoparentThanks for this more foundational information. It's a bit simplified since most displays are flexible now and multi-color, but at least it doesn't ignore the entire OLED material, TFTs, and display driver IC as if the digital communication to the controller was the most important or even remotely common solution. reply Liftyee 15 hours agoprev [–] Fun little project, wonder if DDC could be used as a way to control devices like HDMI capture cards or KVM switches? Honestly surprised that bare i2c interface is exposed to the OS and not hidden by some firmware... reply myself248 14 hours agoparentI don't know about that, but I've used it to reprogram the i2c EEPROM in SFP transceivers. Cooking up a little board to make that easy, actually; I'll publish the files as soon as I prove the prototype works. reply 1oooqooq 15 hours agoparentprevit's hidden on most windows drivers. on Linux it's exposed and have user tools for most standard things. it's infuriating how changing the settings on random external displays is easier than most integrated laptop screens. some kvm do abuse that comms channel already. forgot which brand but it completely hijacks it and my brightness hacks wouldn't work reply extraduder_ire 8 hours agorootparentDDC works over HDMI on linux? I switched to a displayport cable because I thought it wouldn't, and wanted to use ddcutil for setting brightness. I'll have to check that out. It might make the lack of CEC on PCs less annoying. reply IshKebab 15 hours agoparentprev [–] Yeah, the main issue with I2C for this sort of thing is device discovery. It doesn't have any mechanism for it so you would need to magically know your device's address. I believe for things like controlling monitors they just hard-code the addresses. I think that's how it works anyway. Unfortunately the DDC/CI spec is really badly written, incomplete, and full of legacy stuff that nobody actually implements. reply metaphor 10 hours agorootparentConfusing remark; I haven't sleuthed the standard in over a decade, but DDC/CI v1.1 (available for free here[1]) § 2.3 makes this abundantly clear: >> The DDC/CI display is considered a fixed address display device at adddress 0x6Eh / 6Fh, and uses only I2C slave mode to communicate with the host. ...while § 3.3.1 specifies addresses for external display dependent devices. [1] https://vesa.org/vesa-standards/ reply toast0 15 hours agorootparentprev [–] > Yeah, the main issue with I2C for this sort of thing is device discovery. It doesn't have any mechanism for it so you would need to magically know your device's address. The typical address space is 7-bits, so you can just try a transaction to everyone... Or have some convention about what device number the display is? reply utensil4778 14 hours agorootparentI2C has no concept of a discovery or identification packet. The host can write any data to any address, and you have to already know in advance what address and data structure you need. The device can't tell you what its data format is. An I2C transaction begins with a client address, and the host waits for an acknowledge signal that a client has answered the message. That's it, that's the only standard part. From there, the host typically sends a register address and then some number of data bytes, or the client sends back some number of bytes. What those addresses and bytes mean are totally application-specific. There is no standard at all here. You could absolutely broadcast standard DDC commands to all addresses, but you have no way of knowing what actually happened. You might have turned off one monitor and set another to VGA mode. You might have just crashed some random device on the bus. You have no way to tell. All your host can possibly know is that there is a device at a given address, and it either did or did not send an acknowledge bit after a command. That's all the information available to you, and it is not enough to do what you suggest reply FredFS456 13 hours agorootparentThat's not technically true - you can probe for I2C devices by only sending a START, an address, waiting for the client device to ACK (or not), and then sending a STOP. I don't think the linux kernel driver allows us to do this, but I've implemented this on some microcontrollers with lower-level control over the I2C hardware. It worked on all clients I tried it on. reply joezydeco 11 hours agorootparentYou can install i2c-tools and run i2cdetect from userspace. https://learn.adafruit.com/scanning-i2c-addresses/raspberry-... reply rasz 4 hours agorootparent\"Warning This program can confuse your I2C bus, cause data loss and worse!\" reply BobbyTables2 7 hours agorootparentprevIndeed, I’ve also done the same. It’s frustrating how Linux doesn’t allow this. Part of me wonders if all devices actually handle a STOP before the first data byte correctly — without side effects… reply myself248 14 hours agorootparentprevIn recognition of this chaos, https://i2cdevices.org/ is putting together a pretty good list. reply dmitrygr 15 hours agorootparentprev [–] Problem is: every i2c transaction is a READ or a WRITE. The spec is mum on the semantics of what that means. Some devices will get into a weird state or irrevocably pop data off a FIFO if you READ them (eg: some IMUs). Others take a WRITE, even with no data, to have a non-idempotent meaning (eg: SMBUS). Some devices will not even ACK their address on a READ request unless it is preceded by a write and a RESTART. Basically there is no safe way to scan an i2c bus and be sure you did not modify some state or put some device into some weird state. reply buescher 11 hours agorootparent [–] Right. But if you have a finite space of possible i2c devices it's not quite that bad. You can scan for known addresses and there are frequently mitigations for clogging FIFOs - like clocking out the bus or forcing a reset - that you might have to do anyway, depending on what your failure modes for power sequencing are. i2c is just fine when it works, but there are reasons why it can be worth spending the extra pins for SPI. reply dmitrygr 10 hours agorootparent [–] The post I was replying to was suggesting scanning the bus for any kind of device. That is impossible. I could make a i2c device right now and not tell you how it works. You will have no way to scan for it safely (say I specify that unless the first access to it is a write of 0x55 0xAA, it’ll not talk again, but if it is, the byte sent after 0xAA becomes its i2c address) I have seen devices that respond to address zero, which is against the spec. Stopped nobody. I have seen devices that use the bus arbitration meant for multi-master during a READ to allow only one of them to win after responding to a zero-address read. I’ve used an i2c device that had an additional chip select line for some reason. Unless you lowered that, it would not respond to any traffic on the bus. You cannot even imagine the fucked up shit that happens over i2c There is no safe, generic way to scan an i2c bus. If the world of possible devices is limited, it may be possible. But there is no safe generic way. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author describes connecting an OLED display to a laptop's HDMI port via the DDC protocol, overcoming Linux i2c device challenges.",
      "They use a Python script to interact with the display, improve update rates, and experiment with converting it into a monitor using xrandr.",
      "Despite resolution and video driver constraints, the author manipulates the framebuffer to show part of it on the OLED screen, resulting in a distinctive display configuration shared for feedback on Github."
    ],
    "commentSummary": [
      "Users discuss DDC OLED, a compact HDMI display, focusing on utilizing I2C for control and device detection on Linux.",
      "Challenges shared include issues with device recognition, risks linked to scanning an I2C bus, and the intricacies of the DDC/CI specification.",
      "The conversation emphasizes the intricacies of managing I2C for display device control, showcasing the varied responses of I2C devices to commands."
    ],
    "points": 182,
    "commentCount": 20,
    "retryCount": 0,
    "time": 1714063325
  },
  {
    "id": 40158183,
    "title": "Ex-AD arrested for AI-generated fake voice framing",
    "originLink": "https://www.thebaltimorebanner.com/education/k-12-schools/eric-eiswert-ai-audio-baltimore-county-YBJNJAS6OZEE5OQVF5LFOFYN6M/",
    "originBody": "The Baltimore Banner thanks its sponsors. Become one. clear_day 45° clear_day 45° clear_day 45° clear_day 45° search person close Education Higher education K-12 schools personSign InsearchSearchmailNewslettersphone_iphoneApp SECTIONS Key Bridgeadd Politics and poweradd Educationadd Community issuesadd Businessadd Cultureadd Sportsadd Healthadd Opinionadd Regionsadd Collectionsadd Contact Us Group & enterprise sales Customer care Contact The Newsroom Submit a Tip Advertise with us Feedback The Banner About Us Our impact Culture & careers Newsroom policies & code of ethics Creatives in residence Donate Sponsored Content Impact Maryland Subscribe for $1 © 2024 The Baltimore Banner. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Service and Privacy Policies. The Baltimore Banner may receive compensation for some links to products and services on this website. Offers may be subject to change without notice. See our Cookie Policy, RSS Terms of Service, Submissions Policy, Ad Choices, Do Not Sell My Personal Information, and CA Notice at Collection at Privacy Notice. Click here to view our Terms of Sale. The Baltimore Banner is a trademark registered in the U.S. for The Venetoulis Institute for Local Journalism, a 501(c)(3) nonprofit organization. Click here to learn more about supporting local journalism. Baltimore County Ex-athletic director accused of framing principal with AI arrested at airport with gun Kristen Griffith and Justin Fenton 4/25/2024 8:44 a.m. EDT, Updated 4/25/2024 5:58 p.m. EDT The principal of Pikesville High School was investigated after audio purporting to be his voice circulated on social media. Police have charged the former athletic director who they say faked the recording using artificial intelligence software. Baltimore County Police arrested Pikesville High School’s former athletic director Thursday morning and charged him with crimes related to the alleged use of artificial intelligence to impersonate Principal Eric Eiswert, leading the public to believe Eiswert made racist and antisemitic comments behind closed doors. Dazhon Darien, 31, was apprehended as he attempted to board a flight to Houston at BWI Airport, Baltimore County Police Chief Robert McCullough said at a news conference Thursday afternoon. Darien was stopped for having a gun on him and airport officials saw there was a warrant for his arrest. Police said they did not know whether Darien was trying to flee. Darien was charged with disrupting school activities after investigators determined he faked Eiswert’s voice and circulated the audio on social media in January, according to the Baltimore County State’s Attorney’s Office. Darien’s nickname, DJ, was among the names mentioned in the audio clips authorities say he faked. “The audio clip ... had profound repercussions,” police wrote in charging documents. “It not only led to Eiswert’s temporary removal from the school but also triggered a wave of hate-filled messages on social media and numerous calls to the school. The recording also caused significant disruptions for the PHS staff and students.” The Baltimore Banner thanks its sponsors. Become one. Police say Darien made the recording in retaliation after Eiswert initiated an investigation into improper payments he made to a school athletics coach who was also his roommate. Darien is also charged with theft and retaliating against a witness. Darien was allowed release on $5,000 bond and waived an attorney at an initial court appearance, according to court records. Attempts to reach him by phone and at his home were unsuccessful. Read More Baltimore County principal’s racist comments faked by AI, experts say Apr 25, 2024 Opinions vary over Pikesville principal as public debates recording’s authenticity Jan 24, 2024 Baltimore County Public Schools investigates offensive recording alleged to be principal Jan 22, 2024 Eiswert’s voice, which police and AI experts believe was simulated, made disparaging comments about Black students and the surrounding Jewish community and was widely circulated on social media. Questions about the audio’s authenticity quickly followed. Police wrote in charging documents that Darien had accessed the school’s network on multiple occasions in December and January searching for OpenAI tools, and used “Large Language Models” that practice “deep learning, which involves pulling in vast amounts of data from various sources on the internet, can recognize text inputted by the user, and produce conversational results.” They also connected Darien to an email account that had distributed the recording. The Baltimore Banner thanks its sponsors. Become one. Many current and former students believed Eiswert was responsible for the offensive remarks, while former colleagues denounced the audio and defended Eiswert’s character. Eiswert himself has denied making those comments and said the comments do not align with his views. The audio, posted to the popular Instagram account murder_ink_bmore, prompted a Baltimore County Public Schools and Baltimore County Police investigation. Eiswert has not been working in the school since the investigation began. The voice refers to “ungrateful Black kids who can’t test their way out of a paper bag” and questions how hard it is to get those students to meet grade-level expectations. The speaker uses names of people who appear to be staff members and says they should not have been hired, and that he should get rid of another person “one way or another.” “And if I have to get one more complaint from one more Jew in this community, I’m going to join the other side,” the voice said. Darien was being investigated as of December in a theft investigation that had been initiated by Eiswert. Police say Darien had authorized a $1,916 payment to the school’s junior varsity basketball coach, who was also his roommate, under the pretense that he was an assistant girls soccer coach. He was not, school officials said. The Baltimore Banner thanks its sponsors. Become one. Eiswert determined that Darien had submitted the payment to the school payroll system, bypassing proper procedures. Darien had been notified of the investigation, police said. Police say the clip was received by three teachers the night before it went viral. The first was Darien; a third said she received the email and then got a call from Darien and teacher Shaena Ravenell telling her to check her email. Ravenell told police that she had forwarded the email to a student’s cell phone, “who she knew would rapidly spread the message around various social media outlets and throughout the school,” and also sent it to the media and the NAACP, police said. She did not mention receiving it from Darien until confronted about his involvement. Ravenell has not been charged with a crime and could not immediately be reached for comment. Both Darien and Ravenell have submitted their resignations to the school system, according to an April 16 school board document. The resignations are dated June 30. Baltimore County Public Schools Superintendent Myriam Rogers said school system officials are recommending Darien’s termination. She would not say, however, if the other employees named in the charging documents, including Ravenell, are still working at the school. The Baltimore Banner thanks its sponsors. Become one. Rogers in January called the comments “disturbing” and “highly offensive and inappropriate statements about African American students, Pikesville High School staff, and Pikesville’s Jewish community.” Rogers said Kyria Joseph, executive director for secondary schools, and George Roberts, a leadership consultant for the school system, have been running Pikesville High School since the investigation started. They will continue to do so for the remainder of the year. She said they will work with Eiswert to determine his duties for next school year. Billy Burke, head of the Council of Administrative & Supervisory Employee, the union that represents Eiswert, was the only official to suggest the audio was AI-generated. Burke said he was disappointed in the public’s assumption of Eiswert’s guilt. At a January school board meeting, he said the principal needed police presence at his home because he and his family had been harassed and threatened. Burke had also received harassing emails, he said at the time. “I continue to be concerned about the damage these actions have caused for Principal Eiswert, his family, the students and staff of Pikesville High School, and the Black and Jewish community members,” Burke said in a statement on Thursday. “I hope there is deliberate action to heal the trauma caused by the fake audio and that all people can feel restored.” The Baltimore Banner thanks its sponsors. Become one. Police said the school’s front desk staff was “inundated with phone calls from parents and students expressing concern and disparaging remarks toward school staff and administrators.” The flood of calls made it difficult to field phone calls from parents trying to make arrangements for their children and other school functions, officials told police. “The school leadership expressed that staff did not feel safe, which required an increase in police presence at the school to address safety concerns and fears,” police said. Teachers, under the impression the recording was authentic, “expressed fears that recording devices could have been planted in various places in the school,” police said. “The recording’s release deeply affected the trust between teachers and the administration,” police said. “One individual shared that they fielded sensitive phone calls in their vehicle in the parking lot instead of speaking in school.” “Hate has no place and no home in Baltimore County,” said Johnny Olszewski Jr., the Baltimore County executive. He called the developments of AI “deeply concerning” and that it’s important for everyone to remain vigilant for anyone using the technology for malicious reasons. There should also be more investment in technology that identifies any inauthentic recording made with AI, he said. Experts in detecting audio and video fakes told The Banner in March that there was overwhelming evidence the voice is AI-generated. They noted its flat tone, unusually clean background sounds and lack of consistent breathing sounds or pauses as hallmarks of AI. They also ran the audio through several different AI-detection techniques, which consistently concluded it was a fake, though they could not be 100% sure. The police also sought the expertise of two professors familiar with AI detection to assist in their investigation. Catalin Grigoras, a forensic analyst and professor at the University of Colorado Denver, concluded that the “recording contained traces of AI-generated content with human editing after the fact, which added background noises for realism,” the charging documents stated. Hany Farid from the University of California, Berkeley, who’s also an expert in forensic analysis, determined “the recording was manipulated, and multiple recordings were spliced together,” according to the documents. AI voice-generation tools are now widely available online, and a single minute’s recording of someone’s voice can be enough to simulate it with a $5-a-month AI tool, the Nieman Journalism Lab reported in February. There are few regulations to prevent AI imitations, called deepfakes, and few perpetrators are prosecuted. Cindy Sexton, president of the Teachers Association of Baltimore County, said AI should be a concern for everyone, especially educators. She said the National Education Association is working to address their concerns, but in the meantime, she’s not sure what else should be done. “We have to do something as a society, but ‘what is that something’ is of course the big question,” Sexton said Baltimore County State’s Attorney Scott Shellenberger said this is the first time this type of case has been taken up by the district. And it’s one of the first his office was able to find around the nation. There were some legal statutes they used that were “right on point,” he said, but the charge of disrupting school activities only carries a six-month sentence. ”It seems very clear to me that we may need to make our way down to Annapolis in the legislature next year to make some adaptions to bring the law up to date with the technology that was being used,” he said. Baltimore Banner staff writers Cody Boteler and Kaitlin Newman contributed to this report. Correction: This story has been updated to correct the spelling of Hany Farid’s name. RECOMMENDED FOR YOU TRENDING ON THE BANNER Education Higher education K-12 schools personSign InsearchSearchmailNewslettersphone_iphoneApp SECTIONS Key Bridgeadd Politics and poweradd Educationadd Community issuesadd Businessadd Cultureadd Sportsadd Healthadd Opinionadd Regionsadd Collectionsadd Contact Us Group & enterprise sales Customer care Contact The Newsroom Submit a Tip Advertise with us Feedback The Banner About Us Our impact Culture & careers Newsroom policies & code of ethics Creatives in residence Donate Sponsored Content Impact Maryland Subscribe for $1 © 2024 The Baltimore Banner. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Service and Privacy Policies. The Baltimore Banner may receive compensation for some links to products and services on this website. Offers may be subject to change without notice. See our Cookie Policy, RSS Terms of Service, Submissions Policy, Ad Choices, Do Not Sell My Personal Information, and CA Notice at Collection at Privacy Notice. Click here to view our Terms of Sale. The Baltimore Banner is a trademark registered in the U.S. for The Venetoulis Institute for Local Journalism, a 501(c)(3) nonprofit organization. Click here to learn more about supporting local journalism.",
    "commentLink": "https://news.ycombinator.com/item?id=40158183",
    "commentBody": "Ex-athletic director arrested for framing principal with AI-generated voice (thebaltimorebanner.com)182 points by timcobb 19 hours agohidepastfavorite79 comments ceejayoz 18 hours ago> Darien was being investigated as of December in a theft investigation that had been initiated by Eiswert. Police say Darien had authorized a $1,916 payment to the school’s junior varsity basketball coach, who was also his roommate, under the pretense that he was an assistant girls soccer coach. He was not, school officials said. Eiswert determined that Darien had submitted the payment to the school payroll system, bypassing proper procedures. Darien had been notified of the investigation, police said. > Police say the clip was received by three teachers the night before it went viral. The first was Darien; a third said she received the email and then got a call from Darien and teacher Shaena Ravenell telling her to check her email. Ravenell told police that she had forwarded the email to a student’s cell phone, “who she knew would rapidly spread the message around various social media outlets and throughout the school,” and also sent it to the media and the NAACP, police said. So, in this case, clear motive, a prominent figure, and a suspicious chain of custody gave the cops a reason to dig in a bit, but not everyone's gonna be that dumb about it. The swatters who do this to random people are gonna have a field day with this. reply TrainedMonkey 18 hours agoparentConcur OPSEC is not great, there is an easy to follow insertion vector. Imagine instead if he got a burner phone, made a new TikTok account and posted this tagging a few students. reply setgree 17 hours agoparentprevThat's true, but also the person allegedly used a school computer to research his crime: > Police wrote in charging documents that Darien had accessed the school’s network on multiple occasions in December and January searching for OpenAI tools, and used “Large Language Models” that practice “deep learning, which involves pulling in vast amounts of data from various sources on the internet, can recognize text inputted by the user, and produce conversational results.” They also connected Darien to an email account that had distributed the recording. One of our best defenses against crime is that criminals are often kind of dumb. Tyler Cowen makes the same point this morning regarding airport security: \"a lot of criminals are simply some mix of stupid and incompetent or poor on execution\" (https://marginalrevolution.com/marginalrevolution/2024/04/wh...). reply sjducb 16 hours agorootparentThe ones we catch are. reply nojvek 16 hours agorootparentI wonder how many millionaires there are who have done a white collar crime once, got their money, cleaned it using some business and now living off it never to do it again. Take that secret to the grave. Not just white collar, but many people go missing never to be found. So I assume there are likely competent murders who understand enough of biology and chemistry to not leave a forensic trail and not attract too much attention. reply colpabar 15 hours agoparentprev> Ravenell told police that she had forwarded the email to a student’s cell phone, “who she knew would rapidly spread the message around various social media outlets and throughout the school,” and also sent it to the media and the NAACP, police said. That seems like a really terrible way to handle it. reply ceejayoz 15 hours agorootparentIt sounds very much like she was in on it; joint call with the perpetrator to the third teacher, subsequent resignation? Probably just not enough proof. reply surfpel 18 hours agoprev> Experts in detecting audio and video fakes told The Banner in March that there was overwhelming evidence the voice is AI-generated. They noted its flat tone, unusually clean background sounds and lack of consistent breathing sounds or pauses as hallmarks of AI. They also ran the audio through several different AI-detection techniques, which consistently concluded it was a fake, though they could not be 100% sure. All of these problems will be resolved. In which case most people either won’t raise the question of authenticity or won’t trust audio recordings to begin with. Elections at all levels are at risk along with accountability more generally. reply londons_explore 18 hours agoparentAmazingly, most people do seem to trust a simple screenshot of a conversation, even though it has been easy to fake those for 20+ years. A screenshot is considered far more trustworthy by non-tech people than \"I spoke to Fred and would you believe it, he told me X and Y were doing Z behind the bike sheds!!!\" reply duxup 18 hours agorootparentI was following a local case where some inappropriate texts were sent. The evidence submitted with the legal case was a cell phone with the messages app open ... and it appeared to be photocopied. Then they scrolled up ... another photocopy, and on and on and on. Granted further investigation can be done, but it was amusing to see. reply vkou 17 hours agorootparentprevA screenshot or a diary entry locks you into a snapshot of a single story. Someone saying something about what they talked about does not. You can say one thing today and another tomorrow. This is why the written word has magic that the spoken word does not. And the longer the append-only chain of written words goes, the more magic it has. reply DonHopkins 15 hours agorootparentprev>\"he told me X and Y were doing Z behind the bike sheds!!!\" They were arguing about what color to paint them! How scandalous!!! reply IshKebab 18 hours agorootparentprevYeah because it takes significantly more effort to fake a screenshot than to just make stuff up. That's entirely logical. reply ceejayoz 17 hours agorootparentIt really doesn't. https://imgur.com/RNwCHSa reply abrichr 17 hours agorootparenthttps://ifaketextmessage.com/K7dQ/ reply IshKebab 12 hours agorootparentprevYeah but a) most people don't know that and b) that's still significantly more effort! We're comparing this to literally just saying something. reply ceejayoz 11 hours agorootparentPerhaps we have differing definitions of “significant” and/or “effort”. reply andrewstuart2 18 hours agorootparentprevI wouldn't call a quick search for \"fake text message\" which returns dozens of generators significantly more effort. Harder to do on the spot, maybe, but the effort is negligible. reply 9question1 17 hours agorootparentYou underestimate the laziness of most humans. To be clear, screenshots are still not trustworthy. But the \"negligible\" relatively more effort could matter in practice. Cryptography sometimes seem to rely on a stronger version this too. With enough computing power, you can brute force a lot. Some authentication seems just expensive enough that only a nation-state actor would have enough resources to break it, and then rests on the assumption that the small subset of people who could put in enough effort won't care enough to do anything worth being concerned about. This is also why, say, people lock their doors even if they have windows. reply londons_explore 16 hours agorootparentI think a good chunk of non-tech people in my social circles would be appalled that such sites exist, and assume that running them would be illegal. reply BobaFloutist 15 hours agorootparentI mean you can buy lockpicks online and learn how to open almost any lock on YouTube, but we still lock our doors. reply spuz 18 hours agoparentprevWe have to realise the privilege of living during a time when a single piece of evidence could allow us to draw such definitive conclusions. That wasn't the case until audio and video recordings were invented. We've grown used to shortcutting the standard investigative process of looking at the chain of custody and looking for corroborative evidence because it's easy to jump to a conclusion when it's almost certainly correct and it means we can do less work. We now have to adjust our expectations back to when we had to rely on multiple pieces of evidence from multiple sources and evaluating the trustworthiness of those sources before we get out the pitchforks. reply barbariangrunge 17 hours agorootparentWe’ll, in those olden days, pitchforks came out an inappropriate times fairly often, sometimes literally reply kragen 18 hours agoparentprevgoogle's wavenet solved all of those problems years ago reply jsheard 18 hours agoprevMy condolences to all the podcasters and YouTubers who are probably going to get bombarded with extortion attempts after people use their hundreds of hours of public clean voice recordings to make a perfectly convincing voice clone, down to the tiniest quirks of pronunciation, cadence and so on. Who would have thought that would become an opsec risk. reply okhugres 17 hours agoparent> Who would have thought that would become an opsec risk. I did. So I’m sure many others too. They just likely met the same opposition that I did. Without defensive strategies today that allow those at risk to continue their risky behavior unencumbered then any warnings of tomorrow get drowned out by the demands of today. reply SuperNinKenDo 8 hours agoparentprevThings are such that it's really not going to matter that much, 100s of hours vs a couple hours is probably not going to make a world of difference. Many professionals give talks that are recorded, whether lectures, promotional materials, Q&A, etc. People can be recorded surreptitiously, etc. I think the only way to have avoided this is to be so paranoid as to have never allowed anyone near you with a phone or other recording device. reply DontchaKnowit 17 hours agoparentprevHonestly? The writings been on the wall for at least a decade if not longer. This shouldnt really be surprising to anyone with a large corpus of recorded material online. reply jsheard 16 hours agorootparentMaybe to people who were closely following the research, but I very much doubt it was on most peoples radar until maybe a year or so ago when ElevenLabs became available to the public and it started being used in viral memes of Trump and friends. reply DontchaKnowit 11 hours agorootparentYeah I mean fair enough. Im no AI enthusiast and I knew about it a long time ago, however, I also spend a ton of time on the internet, read computer related news etc, so I spose your right. But I remember watching northernlion like 8 years ago and thinking \"you could probably spoof this guys voice really easy with a nueral net\" reply causal 18 hours agoprevUp next: Some enterprising startup founder's \"AI-voice detector\" landing people in prison because they can't keep up with ML advances but authorities trust them anyway. reply dotnet00 18 hours agoparentThere have already been cases like professors failing entire classes for plagiarism without any critical thought because an AI claimed their work was AI generated. Those were reversed because they were particularly egregious, but I can completely see individual students getting falsely accused of using AI and treated as guilty until proven innocent, going totally under the radar because single incidents don't get much public scrutiny. reply causal 18 hours agorootparentConsidering the public's persistent faith in lie detectors for humans, I don't hold a lot of hope we will be wiser about lie detectors for computers. reply ryandrake 15 hours agorootparentIt's going to get worse before it (unlikely) will get better. I predict within 10 years, we'll have at least one jurisdiction experimenting with AI-generated evidence, to help prosecutors get convictions. Prosecutor will push a button and computer will spit out a sworn statement that the defendant is guilty, this will be admissible, and juries will convict based on that evidence alone. Drug dogs who \"hit\" on command are already used as probable cause generators. Breathalyzers are basically magic boxes that produce \"evidence\" of a crime. It's inevitable that we're going to keep using AI and computers to automate convictions. reply causal 13 hours agorootparentThis seemed implausible to me at first, then I remembered we've already had people submitting ChatGPT-hallucinated cases in court, so maybe not :( reply hdlothia 18 hours agoprevI don't think we're ready for the consequences of this technology. My parents are immigrants and my first thought was this might get someone killed or locked up in the old country. These free generators need to include some kind of audio watermark or key to indicate they are ai imitations. At least raise the barrier for this kind of action to being able to run your own llm or something. reply yjftsjthsd-h 18 hours agoparent> These free generators need to include some kind of audio watermark or key to indicate they are ai imitations. At least raise the barrier for this kind of action to being able to run your own llm or something. It might be worth trying, but I'd bet that it's less than 6 months before running it locally means \"download the app off the front page of your app store of choice\". reply rurp 17 hours agorootparentThere could be a requirement that app store listed apps need to include some sort of audio watermark. While that wouldn't be perfect, since there will always be ways around it, this would still raise the barrier significantly and cut down on much of the abuse. Most criminals are lazy and/or not very tech savvy. Raising barriers and prosecuting the worst offenders cuts down on all sorts of malicious behavior that is technically feasible. reply bonton89 17 hours agoparentprev> At least raise the barrier for this kind of action to being able to run your own llm or something. I think that would result in the average person being less aware of the capabilities existing and therefore being less prepared to defend against it. It isn't like this would be a world law that was universally enforced anyway. reply snoman 17 hours agoparentprevI don’t think we’re ready either but I also don’t see how you’d get ready without the pressing need to. That is to say: I think this was inevitable. reply shombaboor 18 hours agoparentprevI totally blame the companies and VCs for lack of foresight and ethics. They've effectively built a weapon without a safety. reply jsheard 17 hours agorootparentIt's the same pattern over and over - they develop a technology, acknowledge the risk of it being abused and the need for safeguards, but then realize that building in those safeguards will get in the way of turning it into a product and just YOLO release it into the wild anyway. The same thing happened with LLMs, which were deemed \"too dangerous to release\" due to the risk of producing a massive tidal wave of spam and propaganda, and yet here we are under a massive tidal wave of LLM spam and about to head into the first US election in the unrestricted LLM era. The very first paper on image generation diffusion models called out the risk of it being used for malicious purposes, such as deepfake nudes, and yet here we are in the era of one-click zero-effort deepfake nude generation services using that very technology. What's the point of considering potential abuses if you're just going to facilitate them regardless? If anything that's worse than not considering abuse at all, because it implies that you know what you've created will result in kids killing themselves after fake nudes of them spread around their school, or enable rampant fraud and extortion through voice cloning, but you believe that's just the price of progress. reply grugagag 17 hours agorootparentprevMaybe they’re supposed to be held responsible or liable? reply hdlothia 17 hours agorootparentThat's a good point, we might not even need new policies. I bet the detectives or a court subpoena could get records of internet history. People might just be able to sue whoever generated the deep fakes that caused damages. reply thraway3837 18 hours agoparentprevI'm really not sure why you're getting downvoted. It's almost as though HN readers are fully on the AI bandwagon and can't let anything bad be said about it. I assume this is the same crowd that also scoffs at any regulations in tech. reply bcrosby95 17 hours agorootparentCommenting on downvotes is pointless, and it's even more pointless the earlier in a comment's life you do it. I've seen wild swings in comments before based upon the time of day. E.g. mid-day upvotes, late day downvotes. Or vice versa. reply educasean 17 hours agorootparentprevYou were right the first time: You really don't understand why HN readers are downvoting the OP comment. Yet that didn't appear to have stopped you from spinning up this AI-bandwagon-riding, regulation-hating strawman to further entrench your hatred towards. reply hdlothia 17 hours agorootparentIf you're a downvoter, why? I would love to hear why you disagree with my comment. reply aingisni_del 14 hours agorootparentThat’s not what a straw man is (it’s two words btw). This is a known and often repeated trend in tech: make something under the guise of disruption, with no regard for safety or regulations. Complain that those things are a hindrance to progress and then spend billions and destroy lives permanently to eventually fix the problems when the regulators issue an ultimatum that professionals warned you about. reply markhaslam 18 hours agoprevHere is the audio clip in question: https://www.instagram.com/reel/C2NEEDrMo8_/ reply SuperNinKenDo 8 hours agoparentDefinitely some hallmarks of AI if you know what you're looking for. But I have to admit, that is more convincing than I would have expected. A bit more manual intervention to change volume and balance and I might find itnsignificantly more difficult to be certain. reply shombaboor 18 hours agoprevWhat is the best argument why this technology exists or should exist? It's fun?Live translation is the only one I can think of, and even then the benefits of the persons 'real' voice are scant. reply grugagag 17 hours agoparentScooping up from the middle class, one class of professions at a time. But this is not just the like how the internet disrupted ‘everything’ and moved things online, this is going to take all the liveleyhoods and suck them dry filling up the technobros coffers until there’s nothing left. But there are upsides too, the potential for good applications is quite broad. reply MiguelHudnandez 16 hours agoparentprevIt's already actively in use to edit and clean up podcast recordings. This use case is basically identical to the malicious case, it's all about who's doing it and what their intent is. If it's fixing a word you stumbled on, or replacing an inaccurate quote with an accurate one, that's fine, but other things are problematic. Someone who is losing the ability to speak might want to have this tech so they can still have somewhat normal phone calls with their loved ones. I think the potential for abuse is pretty high with this tech but it's foolish to pretend we can keep it from being used. reply breakpointalpha 17 hours agoparentprevIf I want to do a voiceover of a ten minute video, I can type out the transcript and produce a flawless one-take audio track. This saves hours of time finding a quiet place to record, saying the lines, having a good mic, doing post-recording cleanup to remove coughs or passing airplane noise, multiple takes because I goofed a word up, etc. I don’t know if this is the best use of the tech we’ve found yet, but it’s already a huge time saver. reply SuperNinKenDo 8 hours agoparentprevTechno determinism. The belief that we can't choose not to develop a technology as a species, so various people decide they don't want the other guy to get it first. We all end up worse off. It's a kind of self-conscious, sometimes ideological, prisoner's dilemma. reply IncreasePosts 17 hours agoparentprev\"it's cool\"? reply cbsmith 18 hours agoprevOh boy. This is the beginning, not the end. reply waldrews 9 hours agoprevI think the lesson here is obvious. Who among us hasn't been terrorized by PhysEd teachers, forced to run laps or climb thing or something against our will? Athletic directors must be even worse. Ban athletic directors! reply itqwertz 18 hours agoprevWe are rapidly advancing towards the logical conclusion of the post-modernist question, “What is truth?” Mobs will always remain dumb and quick to anger, so this will not be an isolated case. reply barbariangrunge 17 hours agoparentI feel like the post modernist stance isn’t to question what constitutes truth, as the ancient Greeks and epistemology nerds attempted, but rather to posit: there is no truth. I’m not a fan of that stance, except for-fun in certain art forms; similar to how violence is fun in art but not in real life reply wpollock 17 hours agoprev>Eiswert determined that Darien had submitted the payment to the school payroll system, bypassing proper procedures. How did he allegedly bypass payroll procedures? An athletics director should not have passwords to the school payroll system. I wonder if social engineering was used? In any case, their security procedures need an audit! reply educasean 18 hours agoprevAs these synthetic voice generations become less and less obviously detectable, I fear two things will happen. 1. The obvious: There will be a lot of fake speeches floating around that spout fake news and hate-filled views. 2. The less obvious: The prevalence of hatred and sensationalized rumors will embolden those who find themselves agreeing with the extreme views seemingly endorsed by some authority figure, and will add their support and authentic voices to the mix. Perhaps this is just an extension of what has already been happening with the internet. We will find ourselves more fragmented and divided than ever, filling our chambers with literal echos of synthetic voices. reply FromOmelas 16 hours agoparent3. A reversion to in-person interaction for anything important (exams, certifications, payments, loan applications, ...) Society benefited from a productivity gain by moving everything online, in a (relatively) high-trust environment. That is now becoming more expensive (due to higher % of frauds), or even infeasible. So, a drag on economic growth for years to come. reply lancesells 17 hours agoparentprev3. A distrust in everything leading to more sway in public opinion. 4. Deniability in everthing that wasactually said. I think it's either Elon Musk or Donald Trump whose lawyers have argued in court/public that you can't be certain they actually said it. reply endisneigh 18 hours agoprevImagine if he was cleverer - was in an office, feigning a conversation with them, with kids who would be “witnesses”. Monitor the principal to the alibi isn’t solid or somehow supports it. Wild times. reply sandspar 16 hours agoprevThe future will be stupider than we can even imagine. reply FrustratedMonky 18 hours agoprevSo it begins. If any joe blow ex-athletic director can do it, think about how wide spread this could become. reply xhkkffbf 18 hours agoprevDon't let anyone say that the teachers aren't keeping up with technology. Certainly not the gym teachers. reply beaeglebeachh 18 hours agoprevWait till the kids figure out how to do it. Principal at my school tried to have me expelled, by lying saying I was inciting violence. Soon it will be easy for kids to turn the tables on these tyrannical administrators in their insular fiefdoms. reply dvaun 18 hours agoparentWhile your situation is unfortunate, you can’t paint a broad stroke and assume all administrators are “tyrannical”. Most of these people are simply trying to perform their jobs. reply beaeglebeachh 18 hours agorootparentThey extract their salary by evicting old ladies who can't afford their property taxes. They are no better than Al Capone. reply TheFreim 18 hours agorootparentYou think school principals are the ones evicting people? Or do you think school principals are just tasked with evicting old ladies in particular? reply filoleg 16 hours agorootparentYeah, I have no idea at all how school administrators are related to evicting old ladies and collecting property taxes either. reply PhasmaFelis 18 hours agoparentprevI'm sorry your principal was a dick, but it's weird how you're acting like \"teenagers now can fraudulently incriminate anyone they want\" is somehow a good thing. reply beaeglebeachh 18 hours agorootparentI think it could be a good thing. When the tyrants have to face their own behavior, they'll be forced to raise the evidentiary standards high enough their own lies won't work. reply jbullock35 17 hours agorootparent> When the tyrants have to face their own behavior, they'll be forced to raise the evidentiary standards high enough their own lies won't work. I don't think that tyrants work this way –- at least not in America's educational system. They're very happy to employ double standards. reply rurp 17 hours agorootparentprevThat's not really how actual tyranny works. There is no requirement for equal treatment. reply heavyarms 18 hours agoprev [–] There are lots of valid use cases for speech synthesis and text-to-speech technology, and there are like 1 or 2 valid/legal use cases for voice cloning that I can think of. Ignoring the moral and ethical questions, why would anybody devote time and resources building a company around a very niche solution... one in which your customer churn rate is partially dependent on users not ending up in prison. edit: typo reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A former athletic director at Pikesville High School, Dazhon Darien, was arrested for using artificial intelligence to fabricate a recording of the principal making racist and antisemitic remarks.",
      "The incident resulted in disruptions at the school, prompting increased police presence to maintain safety, with Darien facing charges of disrupting school activities, theft, and retaliating against a witness.",
      "Experts highlight the escalating concern of using AI to generate fake recordings, emphasizing the necessity for enhanced regulations to tackle this issue."
    ],
    "commentSummary": [
      "An ex-athletic director was arrested for using AI-generated voice to frame a principal in a theft investigation, sparking concerns about security and technology misuse.",
      "Discussions highlight skepticism towards the credibility of audio recordings and screenshots, as well as the risks and ethical implications of AI in privacy and trust.",
      "Concerns are raised about the misuse of AI-generated evidence in legal cases, the absence of regulations, and the ethical dilemmas associated with developing deepfake technologies, emphasizing the impact on professions and society."
    ],
    "points": 182,
    "commentCount": 79,
    "retryCount": 0,
    "time": 1714056207
  }
]
