[
  {
    "id": 39981034,
    "title": "Physicist Peter Higgs, Discoverer of Higgs Boson, Dies at 94",
    "originLink": "https://www.theguardian.com/science/2024/apr/09/peter-higgs-physicist-who-discovered-higgs-boson-dies-aged-94",
    "originBody": "View image in fullscreen Prof Peter Higgs in his office at Edinburgh University in 2007 with a description of the Higgs model on the blackboard behind him. Photograph: Murdo Macleod/The Guardian Peter Higgs Peter Higgs, physicist who proposed Higgs boson, dies aged 94 Nobel-prize winning physicist who showed how particle helped bind universe together died at home in Edinburgh Severin Carrell and Ian Sample Tue 9 Apr 2024 12.54 EDT Share Peter Higgs, the Nobel prize-winning physicist who proposed a new particle known as the Higgs boson, has died. Higgs, 94, who was awarded the Nobel prize for physics in 2013 for his work in 1964 showing how the boson helped bind the universe together by giving particles their mass, died at home in Edinburgh on Monday. After a series of experiments, which began in earnest in 2008, his theory was proven by physicists working at the Large Hadron Collider at Cern in Switzerland in 2012; the Nobel prize was shared with François Englert, a Belgian theoretical physicist whose work in 1964 also contributed directly to the discovery. Peter Higgs interview: ‘I have this kind of underlying incompetence’ Read more A member of the Royal Society and a Companion of Honour, Higgs spent the bulk of his professional life at Edinburgh University, which set up the Higgs Centre for Theoretical Physics in his honour in 2012. Prof Peter Mathieson, the university’s principal, said: “Peter Higgs was a remarkable individual – a truly gifted scientist whose vision and imagination have enriched our knowledge of the world that surrounds us. “His pioneering work has motivated thousands of scientists, and his legacy will continue to inspire many more for generations to come.” Prof Fabiola Gianotti, the director general at Cern and former leader of the Atlas experiment, which helped discover the Higgs particle in 2012, said: “Besides his outstanding contributions to particle physics, Peter was a very special person, a man of rare modesty, a great teacher and someone who explained physics in a very simple and profound way. “An important piece of Cern’s history and accomplishments is linked to him. I am very saddened, and I will miss him sorely.” The evening before the discovery of the particle was announced, Peter was invited to a small celebration at the home of John Ellis, the former head of theory at Cern. “A giant of particle physics has left us,” Ellis told the Guardian. “Without his theory, atoms could not exist and radioactivity would be a force as strong as electricity and magnetism. “His prediction of the existence of the particle that bears his name was a deep insight, and its discovery at Cern in 2012 was a crowning moment that confirmed his understanding of the way the Universe works.” Jon Butterworth, a member of the Atlas collaboration, said Higgs was “a hero to the particle physics community”. “Even though he didn’t much enjoy it, he felt a responsibility to use the public profile his achievements brought him for the good of science, and he did so many times. The particle that carries his name is perhaps the single most stunning example of how seemingly abstract mathematical ideas can make predictions which turn out to have huge physical consequences.” The Royal Swedish Academy of Sciences, which awards the Nobel, said at the time the standard model of physics which underpins the scientific understanding of the universe “rests on the existence of a special kind of particle: the Higgs particle. This particle originates from an invisible field that fills up all space. “Even when the universe seems empty this field is there. Without it, we would not exist, because it is from contact with the field that particles acquire mass. The theory proposed by Englert and Higgs describes this process.” An immensely shy man who disliked the fuss, Higgs had left home for a quiet lunch of soup and trout in Leith on the day of the announcement, to be stopped by a former neighbour who gave him the news on his way home. Born in Newcastle upon Tyne, Higgs leaves two sons, Chris and Jonny, his daughter-in-law Suzanne and two grandchildren. His wife, Jody, a linguistics lecturer from whom he was separated, died in 2008. Explore more on these topics Peter Higgs Higgs boson Particle physics Physics Scotland Nobel prizes People in science news Share Reuse this content",
    "commentLink": "https://news.ycombinator.com/item?id=39981034",
    "commentBody": "Peter Higgs, physicist who discovered Higgs boson, has died (theguardian.com)870 points by angrygoat 17 hours agohidepastfavorite94 comments vazma 15 hours agoI was incredibly fortunate to meet him at CERN the day before the Higgs boson announcement. As an intern, I encountered him the evening prior; he was dining alone in the CERN cafeteria, blending in like a kindly elderly gentleman. He was exceptionally humble and courteous. I feel so lucky that I mustered the courage to speak with him and shake his hand. Rest in peace, Mr. Higgs. reply teekert 3 hours agoparentThese are the kind of opportunities one must take when they present themselves. Well done. reply dotnet00 17 hours agoprevThe story of how Higgs predicted his mechanism was part of what got me into physics as a kid. It fit 10 year old me's obsession with the 'soft-spoken genius' archetype perfectly and formed a pillar in my belief that 'genius' was made through hard work (and some amount of luck) rather than being born with it. The announcement of the detection at the LHC is a core memory of mine, I still distinctly remember where I was, what I was doing and very excitedly trying to explain how cool it was to my parents at the time. reply moelf 16 hours agoparentSame, same, and same, + currently working on one of the LHC detectors as a phd student -- but it can all be traced back to the lore of the golden age of particle physics and the discovery of Brout–Englert–Higgs boson in 2012. reply KingFelix 17 hours agoparentprevThat is awesome, we need some more amazing scientists in the public eye to inspire the young. I know there are a lot of amazing scientists, but somehow need to get them on tiktok or something? Can you share the story? reply moelf 16 hours agorootparentSpecific to Particle physics and Peter Higgs, this book (https://www.basicbooks.com/titles/frank-close/elusive/978166...) by Frank Close is fantastic. reply dotnet00 16 hours agorootparentprevMy parents had a hard time explaining my dad's job to me, so they just told me he was a scientist because he had a masters in physics (he was a diplomat). Like any little kid, my parents were my heroes, so of course this led me to becoming obsessed with scientists and reading up on science stuff, eg trying to read my sister's university textbooks. I developed a hobby of reading up on and sharing factoids I found online, and found one about the 'god particle'. At first I thought it was cool because it seemed to basically talk about a particle that causes mass (of course, this was actually wrong, but that didn't really matter to a 10 year old), but reading about how it was predicted 40-50 years ago and the largest single machine humanity had built was being used to try to find it made it my favorite factoid and I'd excitedly start talking all about it the moment anyone showed even the slightest bit of interest. In 2012 when the detection was announced, we were on a short 2-3 day vacation in Dubai and were having breakfast in the hotel. The TV was right next to us, and seeing the news I was trying (and failing) to explain to my parents how the Higgs boson had been predicted 50 years ago and it took that long for the technology to finally catch up to be able to verify it, and how this would represent one of the last remaining pieces of the standard model (although back then I didn't quite grasp that the standard model was not a full theory of everything). I was trying to explain to them the size of the LHC, how it was the biggest single machine we've built, how when they were turning it on for the first time, there were fears about it creating micro-black holes which might swallow the Earth. I think that while we need scientists in the public eye, we don't need them as social media entertainers, a lot of well known science communicators on social media come off as attention-seeking charismatic fakes/frauds to me (eg NDT). Stuff like the interviews and documentaries Stephen Hawking had appeared in (or to a lesser extent, the ones Michio Kaku has appeared in) did much more for me in being inspired, even without having known what research they were known for. I think we could also do with more books like Hawking's 'A Brief History of Time' and encouraging kids to read them. Also, instead of over-simplifying everything and passing off scientists as geniuses in the traditional sense, we should be more open in showing that the people who made these discoveries or predictions were not inherently born with it, the vast majority of them were completely normal people who worked very hard to build skills in the thing they enjoyed. Another discovery I feel was somewhat similar is that of discrete time crystals, casually predicted in 2012, turned out to actually be possible in 2018 and has a similar 'cool' factor. reply cge 14 hours agorootparentI think a difficulty with science communication is that it is very rare to have someone who is both a great scientist and great science communicator, and even with the ability, it is difficult for them to devote enough time and focus to be great at both. Feynman was, if flawed in some ways as a communicator. Hawking was, but I got the sense that at some point later in his life his focus on communication to the public limited his ability to continue doing research rigorously (after somewhat idolizing him as a child, as a graduate student I went to a research talk he gave to the theory side of our department, possibly in the context of TAPIR, that was both embarrassing and depressing, as it both felt like he really wanted to keep doing good research and very clearly couldn't manage to, and it seemed like everyone in the room knew it, including him). Einstein, despite having the ability to draw a public audience, arguably wasn't a great communicator. On the other side, while yes, NDT is problematic, I think there is a value to people who are great science communicators without being great scientists. Sagan was arguably a great science communicator and not a great scientist per se. But his communication to the public was inspiring and educational, with enough rigor but not too much complexity, with a sense of wonder but not too far into speculation presented as science, with intuitive explanations but without too disastrously overburdened metaphors. There's the view that his talent for communication and broad intuitive understanding was such that even his contributions to research came primarily from his ability to be, in Kuiper's words about him, a \"liaison between sciences\". But even when just to the public, someone devoted to that sort of work, and good at it, is not less valuable than a scientist. reply moomin 12 hours agorootparentThere are definitely science communicators who _are_ committed to getting it right. Dr Brian Cox and Bill Nye strike me as good examples. The late Patrick Moore was another. NDT, embarrassingly, often is just plain wrong. He treats “science” as a side, not an investigation into the wonder of the natural world. reply sonofaragorn 15 hours agorootparentprevI get your sentiment, but I think it's important for science communication to adapt to the times. Decades ago (and even as little as one decade ago), most scientists (maybe Hawking being the exception) who would dare appear in these 1hr documentaries would be belittled by the \"hardcore\" scientists with the same words you used \"Science should not be over-simplified like that\", \"they are not real scientists, they just want to be on TV\", etc. The truth is that young people are mostly on TikTok et al, so this type of content needs to get there. reply godelski 8 hours agorootparent> \"Science should not be over-simplified like that\", It is a difficult balance to strike, but science should be > as simple as possible, and no simpler The hard truth is that any simpler means inaccurate, which means when educating the public there __are__ inaccuracies. So the balance to strike is accuracy vs understanding. Most people do not understand the Schrodinger's Cat thought experiment, wave-particle duality, or many similar things (like how one of my namesake's Incompleteness Theorem, Super Position, and the Halting Problem are linked) yet will confidently correct explanations given that do hold more accuracy (even \"teaching\" those where it is clear one side is vastly more qualified than the other). But think about it this way, the people getting mad at the over-simplification are not a force preventing public explanation but rather a pressure to find a better and more accurate explanation. The problem is when we frame these things as adversarial in the sense of enemies fighting rather than adversarial in the sense of improving one's self/group's position/arguments/discourse. Both sides can benefit from a reframing of these interactions by not holding the positions as equivalent to one's intellect but rather recognize that statements are independent. We are better as a coalition than separated, even with disagreement (especially with). It is clear here that everyone is on the same side after all, as all parties involved are seeking the same goal: better public education. It then becomes the duty to read between the lines to extract what the actual complaint is, because this is often also difficult to express (without a lengthy process) given we do not know one another's priors. We should not confuse critiques for attacks or dismissals. Nor should we dismiss or attack when we should critique! Though it is acceptable when errors are egregious or when people intentionally mislead. Unfortunately there is a large amount of that, but let's also distinguish idiocracy from maliciousness, as the former can be fixed (if we formulate with the above framing). reply mercacona 12 hours agorootparentprevWon’t say that TikTok audience is a pound where you’ll find future scientists. I’ll invest on promoting alternative spaces both virtual and local best. reply dotnet00 15 hours agorootparentprevYeah that's a fair point. As an early career scientist myself now and as someone not that interested in current social media trends, I certainly do risk being in the same spot as those 'hardcore' scientists. reply wolverine876 11 hours agorootparentprev> My parents had a hard time explaining my dad's job to me, so they just told me he was a scientist because he had a masters in physics (he was a diplomat). I'm just curious about when you found out what he really did, and if you had known earlier if you would have been a diplomat today rather than a scientist. reply dotnet00 11 hours agorootparentI was able to understand it roughly around when I entered my teens, in part because he was transferred to another country, where the embassy was a lot smaller and the country more insular, so it was common to get kids involved in embassy stuff to get the otherwise lacking sense of community. It's hard to say if I would've been a diplomat if I had known earlier. I feel like being a diplomat is harder to innocently romanticize and turn into hobbies for a child in the way that 'scientist' can be. reply cdelsolar 14 hours agorootparentprevhow is ndt a fake/fraud? reply dotnet00 14 hours agorootparentHe gives me a similar feeling to say, SBF did prior to the FTX collapse. This isn't to say he's a scammer like SBF, but rather that he has a similar hard to describe 'dishonest' air around him, where I feel he's deliberately trying to make himself seem smarter than he is for the sake of the attention alone, which makes me distrustful of him. I'm not really sure how to describe the feeling besides \"attention-seeking charismatic fakes/frauds\", but as another example, Bill Nye also gives me the same feeling. His X shenanigans don't help either, where he has a reputation of engagement farming by posting dumb somewhat condescending \"but akschually\" type comments on things people are enjoying. Eg, when the last American total solar eclipse happened in 2017, he posted something along the lines of \"ignore people when they tell you eclipses are rare\", it's technically correct that eclipses happen fairly often, but he obviously had to know that what makes them exciting is that they're rare for the location the viewers are at. It's become somewhat of a meme to call someone NDT when they're being a buzzkill. reply wpietri 14 hours agorootparentprevSome of the concerns are documented here: https://rationalwiki.org/wiki/Neil_deGrasse_Tyson reply selimthegrim 14 hours agorootparentprevHe switched PhD programs from Texas to Columbia and some people thought he took too long or something reply queuebert 11 hours agorootparentNormally PhD programs have a timeline of progress. If you don't make regular progress toward those very generous deadlines, you are kicked out. Neil did not make sufficient progress at Texas. Then he did something weird, where he wasn't ready to defend yet got a postdoc at (iirc) Princeton. Princeton had to rescind the offer because you (ahem) need a doctorate to be post doctorate. Then he got into Columbia's program and finally finished. He had a lot of hobbies and interests other than astronomy, and he is actually a very smart guy, but it took him a bit longer to get his shit together. I think he's found his calling and is quite good at he does. reply selimthegrim 10 hours agorootparentI have some sympathy for that because I had a couple advisors quit my program on me and the program still tried to enforce time to degree deadlines on me without asking my then advisor (in a different department by then) and misstating factual information about credits expiring (which is determined at the school level not the department level). I had to get the deans involved. reply tunesmith 15 hours agorootparentprevGreat story! Thanks. reply amelius 12 hours agoparentprevCan you recommend any physics books that were of value to you as a kid and perhaps sparked your interest? reply dotnet00 11 hours agorootparentAs a 5-10 year old, the books I remember liking the most were on the outer planets, with high quality full page photos of the planets and their moons. Closer to photobooks than books on the solar system targeted at children. One book was just images of the moons, mainly focusing on Saturn, I used to just look at the images and admire them even if initially I didn't quite understand the details. Most of my physics reading came from random sources on the early internet. As a 12-13 year old, Stephen Hawking's A Brief History of Time and The Grand Design were by far my most memorable reads, although I already had my developing interest in physics by then. reply cm2187 14 hours agoparentprevDid you succeed with your parents? It's hard to be excited about theoretical physics as a layman. It has been a long time since any of those theories had any practical application. reply dotnet00 14 hours agorootparentThey still aren't all that interested in this stuff, and on their advice I ended up studying computer engineering instead of physics, but I've still found myself working at one of the other labs with big particle accelerators, as a researcher who can link the computing side with enough of the physics side to work with physicists. They don't fully understand what I do and don't really care too much about the details, but when they saw pictures of where I worked, they did immediately bring up that I used to go on and on about something that seemed similar, so they at least did understand what I liked. I guess the closest they get to being interested in theoretical physics is that my Dad, having run through his stash of novels during covid, eventually read my left-behind copy of A Brief History of Time, and occasionally quotes it when he's in the mood to wax philosophical. My Mom instead tries to keep up with my other interest of space exploration/astronomy. reply mptest 7 hours agorootparentDamn. Tell your parents you love them often. This comment made me deeply pine for parents that feigned even the tiniest bit of interest in what I'm interested in. Your folks seem wonderful. reply wolverine876 12 hours agorootparentprev> It has been a long time since any of those theories had any practical application. That's a cliche, not why people are ignorant of science (if it's even true, which it's not), IMHO. When you kiss your spouse or watch a sporting event or (go bird watching / play D&D / play your trumpet / ), does it have a pratical application? Practical applications tend to be kind of boring, actually. If you can't get excited about the fundamental laws of nature and a person's actual discovery of one - the reason for mass (such an incredible concept that it would be absurd to say if it wasn't true) - then the issue isn't partical physics. For the broader public, I think these things just aren't explained well, and now there's the anti-science mis/disinformation. reply rmbyrro 15 hours agoparentprevWith a layperson in mind, curious about physics, do you recommend any resource (hopefully not too math-intensive) to learn how the higgs boson actually \"gives mass\" to stuff? reply dotnet00 14 hours agorootparentSince you said 'not too math-intensive', I figure you're fine with getting more details and background than usual just without having to parse equations, in which case PBS Space Time is great: https://www.youtube.com/watch?v=G0Q4UAiKacw reply MathMonkeyMan 13 hours agorootparentprevSean Carroll's pandemic era youtube series \"The Biggest Ideas in the Universe\"[1] goes into scalar fields and some gauge theory, but I don't remember if he covers the Higgs mechanism. Might be in one of the Q&A videos. [1]: https://www.youtube.com/playlist?list=PLrxfgDEc2NxZJcWcrxH3j... reply tekla 15 hours agorootparentprevhttps://www.amazon.com/Massive-Missing-Particle-Sparked-Grea... reply lapetitejort 16 hours agoparentprevSome of my professors during my physics BS worked with the LHC during the mad scramble to find the particle. I remember people saying tongue in cheek \"The Higgs particle doesn't exist, but it's inside this energy range.\" reply lupire 16 hours agoparentprevWhat makes you think he wasn't born with it? I can see in own kids that each one is built differently, with different inmate propensities despite similar environment. reply dotnet00 16 hours agorootparentMost childhood prodigies don't end up being all that different from the average person as an adult, and of course no one's born with the knowledge of some revolutionary discovery in their head. While all of us have things we find easier to do than the average person, short of a literal mental disability, we can build up skills in things we are not good at through practice. I used to barely pass in math class and didn't even understand the concept of negative numbers until 8th grade. A year of practicing daily for 2 hours after school, and my fundamentals had gotten good enough that I unknowingly derived a calculus-based solution to some problems I was stuck at, 2 grade levels before when I'd actually start learning calculus and got to skip a year as a result. Similarly, I've been teaching myself to draw despite having been pretty terrible at it and discovering how 'deliberate' most professional artists have to be with practice and building skills. I think it's pretty common for people to write off their inability to do something as just a lack of innate ability, when it's really just that no one really sees the struggle anyone famous for their work/skill has gone through to get there. reply tgv 15 hours agorootparentBut there definitely is a wide variation in peak capability. I have been playing keyboard instruments almost my whole life, and while I can play relatively complex pieces, I've never gotten at the level of professional musicians, let alone the greats. It's true they wouldn't have gotten where they are without practicing, but practicing is just not enough. reply dotnet00 15 hours agorootparentBut has the goal you've been aggressively working towards been to reach those levels? or have you been playing just for the enjoyment of playing? Not to suggest that the latter is wrong, just interested in your actual goal. In teaching myself to draw (anime art specifically), I'm aiming to reach a professional level, but am not interested in becoming a professional artist. The only factor I've felt would limit my ability to achieve this is time commitment (since research is pretty time consuming already). I'm not interested in committing as fully to it as someone who makes their living off art, so I don't expect to match them in all ways. So, for instance, while I expect to eventually be able to match in terms of overall result, I expect to not be anywhere near as fast as a professional can be. reply wholinator2 12 hours agorootparentprevWell you probably don't practice at much as they do. reply wpietri 13 hours agorootparentprevI think the point is that if one aspires to be a figure like Higgs, one can't just coast on \"innate propensities\". It requires fiendishly hard work. That really resonates with me. When I was a kid I got complimented a lot for being smart, especially when I did something quickly and easily. This trained me pretty well in seeming smart, but really discouraged me from things that required hard work or persistence through failure. It took me years to get over that. reply layer8 16 hours agorootparentprevSome have higher inmate propensities than others. ;) reply wumeow 13 hours agorootparentprevPeople with innate ability almost always take it for granted. reply UncleSlacky 16 hours agoprevLame claim to fame: Higgs was the PhD supervisor of one of my university professors. He told us that Higgs left a message on his desk before going hiking one weekend to the effect that he'd had a great idea and would tell them all about it when he got back. reply ColinWright 16 hours agoparentI wonder if he got the idea from Hardy, who before undertaking a journey on a very small boat sent a postcard saying he had proved the Riemann Hypothesis: Hardy stayed in Denmark with Bohr until the very end of the summer vacation, and when he was obliged to return to England to start his lectures there was only a very small boat available…. The North Sea can be pretty rough, and the probability that such a small boat would sink was not exactly zero. Still, Hardy took the boat, but sent a postcard to Bohr: “I proved the Riemann Hypothesis. G.H. Hardy.” If the boat sinks and Hardy drowns, everybody must believe that he has proved the Riemann Hypothesis. Yet God would not let Hardy have such a great honor and so He will not let the boat sink. -- https://home.cc.umanitoba.ca/~mcleod/Riemann/Hardy.html reply anthony__j 15 hours agorootparenthaha, that's pretty clever. makes you wonder if fermat was doing a similar prank with his margins reply ColinWright 15 hours agorootparentIt's generally believed that Fermat thought he had a proof, but probably almost immediately remembered that not everything is a Unique Factorization Domain, so the \"obvious proof\" fails. Then he didn't bother returning to correct the error. So no, probably not. (+) I should go and learn more about the specifics of this to make sure I'm relating it correctly. EDIT: (++) OK, here's what I was thinking about: https://math.stackexchange.com/questions/953462/what-was-lam... EDIT2: (++) Second link with similar details: https://math.stackexchange.com/questions/324740/fermats-proo... reply ducttapecrown 14 hours agorootparentI just recently learned that the note in Fermat's margin was published posthumously by his son! So Fermat never necessarily publicly claimed to have a proof. So I would imagine you're absolutely correct. reply pfdietz 14 hours agoprevIt was a success for particle physics that they found the Higgs, but it was also a tragedy. Discovering the Higgs and nothing else new was the nightmare scenario for the LHC, and so it has come to pass. reply joshcryer 13 hours agoparentIt really damaged string theory which is by far the greatest thing to happen with the LHC. reply pfdietz 13 hours agorootparentAh, a glass is half full person! :) reply vikramkr 5 hours agoparentprevCalling it a nightmare scenario is quite the overdramatic description of a successful experiment that validated a core prediction of the standard model reply akumetsu 17 hours agoprevSad to hear, I remember the excitement over the experimental evidence once his particle was detected. I'm always amazed by theoretical predictions that can actually be verified plus it was interrsting to hear about the higgs boson as part of my studies shortly after it was detected. Nowadays it seems many theoretical predications are not even close to being verifiable in the coming years or with the current and planned tech. Unless we are talking about superconductivity at room temperature ofc reply CapeTheory 14 hours agoprevMassive in his field. reply callumw13 13 hours agoparentthis is the kind of strong interaction I like to see on this website reply sohkamyung 10 hours agoprevI read \"Elusive: How Peter Higgs Solved the Mystery of Mass\" by Frank Close and I found it an excellent read on the elusive Higgs Particle and the elusive Peter Higgs himself (Higgs went for a walk to hide from people on the day the Nobel Prize was announced). reply jaredwiener 14 hours agoprevBut will there be a funeral mass? reply russelldjimmy 5 hours agoparentYes, about 125.11±0.11 GeV/c^2 reply silverfrost 13 hours agoprevProposed - not discovered. He put it forward as an explanation, he didn't make the actual discovery. reply toomuchtodo 17 hours agoprevhttps://en.wikipedia.org/wiki/Peter_Higgs reply neom 14 hours agoparentJust to add a couple more good Higgs resources courtesy of the fine folks at PBS Space Time: How the Higgs Mechanism Give Things Mass - https://www.youtube.com/watch?v=G0Q4UAiKacw Could the Higgs Boson Lead Us to Dark Matter? - https://www.youtube.com/watch?v=z2yLMY6Mpw8 Where Is The Center of The Universe? - https://www.youtube.com/watch?v=BOLHtIWLkHg (For me personally, I gave a massive sad sign when I saw this on the homepage. I really liked him a lot for a few reasons: He did his thinking in isolation, for a long period of time walking around the Scottish highlands . He was a keen disciple of interdisciplinary thought being pivotal to innovation. He appears to have imagined things and thought about things in a pretty weird way for his time, although that might seem obvious, how well he grappled with the reality of weirdness is exemplary. 힝) reply hiddencost 17 hours agoprevGlad he got to see the confirmation before he went. reply jszymborski 14 hours agoprevThis has given me a case of the Higgs boson blues https://youtu.be/1GWsdqCYvgw reply taylorbuley 11 hours agoprevBBC version https://www.bbc.com/news/science-environment-68774195 reply bdjsiqoocwk 3 hours agoprevJust keep in mind that if this way present day, the Sabine hosenfelds of the world would be saying iTs NoT eVeN tEsTaBlE, and HN would be cheering her for fighting the evil corrupted mainstream academia. reply brcmthrowaway 10 hours agoprevDoes the Higgs detection have any economic value, like room tenperature superconductors would? Or is it a scientific curiosity reply openrisk 8 hours agoparentWith the knowledge and technology of today there is arguably very little direct economic value to be had from this part of particle physics. One cannot preclude potential future implications though (it has happened many times in the past that understanding phenomena that were far removed from everyday experience created later the conditions for massive technological breakthroughs). Particle physicists and other researchers in fundamental science are also typically keen to point out at indirect effects. E.g., building the massive accelerators to detect particles pushes forward more conventional technologies. The Web was famously invented to serve CERN collaboration needs. reply wavemode 10 hours agoparentprevIt serves as an important confirmation of the Standard Model of particle physics, which is a foundational theory underpinning many other important discoveries. But the particle itself probably has no current practical application. reply godelski 7 hours agoparentprev> have any economic value Don't downvote them! This is actually a good question! (and gives us a chance to talk about why we should pursue these things!) It's also incredible difficult to answer! Can we define what it means? Direct or indirect? === Indirect (LHC) === Well one of the reasons the LHC was built was to find the Higgs. To do so, we had to invent a lot of shit along the way. Thing is, when you're pushing the bounds of human knowledge, you don't exactly have all the devices you need to measure and test everything. The WWW[0] is famously one of such \"spin-offs\" as we needed to connect scientists from around the globe to distribute the data from this project. Remember that it is an international project[1*]. There is also a lot about superconductors and refrigeration, both of which significantly contribute to modern medical devices. A lot for magnets, vacuum devices, and electronics, all of which have permeated into industry. These scientific projects also are a big political effort and demonstrate good will and can be grounds for collaboration and building democracies. The hosting countries also have a lot to benefit from as direct collaboration happens there. Just think of the force of putting a bunch of very smart people in a room together, especially when they are experts in very different things. It's difficult to predict the direct revenue, but at such a cheap cost, even small innovations can easily end up covering the costs. Certainly the internet has more than paid for CERN, in the form of tax revenues to each country compared to the cost they give, not to mention benefit to the public (especially considering other indirect aspects). === Direct (LHC) === Maybe a bit harder. There's some slides here [2] that claim CERN nets 3.3bn for 1993 - 2038. You can find much more detail here[3] and another independent one here[4]. I'd just like to note [4]'s last line in their abstract: We conservatively estimate that there is around a 90% probability that benefits exceed costs, with an expected net present value of about 2.9 billion euro, not considering the unpredictable applications of scientific discovery. === Specifically the Higgs === That's unfortunately impossible to say. To make use of it technologically we're at least 50 years away, which is to say \"who the fuck knows\". But also remember the cost is almost nothing. If we speculate, it is not unreasonable that the technologies that could be enabled through the understanding of this science (and the requisite further knowledge we'll need) could be insurmountable. We're talking about understanding how mass works. So if we're ever going to invent things like inertial dampeners (which would make mass an irrelevant aspect of transportation), mass effect drives, gravity generators, and so on, knowledge of the Higgs would be essential. But don't hold your breath on seeing technologies any time soon. Remember that we're playing the long game with science. It is good to think about short term, but never forget the long game. If you forget you may win battles but will lose the war. === Side Note About Money === The LHC is actually one way I like to think about the ultra billionaires (like Musk, Bezos, Gates types). The reason being that with that level of wealth we cannot ignore the effects of compound interest, as this plays a significant role. Let's take Bezos, the #2 on the list (behind Bernard Arnault) with $203.3B. We can ask, how many LHCs could Bezos make? We assume 10 years to build at 0.5B/yr and then 1B/yr to operate. We'll assume a 7% interest rate, compounded yearly, which means 14.231B/yr! So clearly Bezos is worth at least 14 LHCs! We could get more precision and actually compound, but the quick version gives us a sufficient lower bound to really put into perspective either the wealth of Bezos or how cheap the LHC is. However your want to frame it. FWIW, with the same lazy analysis we get Forbes top 10 as: Arnault @ 15.2 LHCs, Bezos @ 14.2 LHCs, Musk @ 13.72 LHCs, Zuckerberg @ 12.7 LHCs, Ellison @ 10.7 LHCs, Buffett @ 9.6 LHCs, Gates @ 9.2 LHCs, Page @ 9.1 LHCs, Ballmer @ 8.8 LHCs, and Brin @ 8.8 LHCs. I'm just saying, we could afford a lot of LHCs... [0] https://www.home.cern/science/computing/birth-web [1*] I wanted to take a minute to mention the cost, so we can better guestimate the ROI. The project took 10 years to build and cost about $5bn and costs about $1bn/yr to operate, with Germany being the largest contributor and only contributing 21%[1^]. I'm not sure how it works, but the Federal budget is about 370B euros but total gov spending was 1.76T for 2021. That would be 0.065% of the federal budget or 0.014% of the total spending. Pretty fucking cheap if you ask me! [1^] https://en.wikipedia.org/wiki/CERN#Participation_and_funding [2] https://fcc-cdr.web.cern.ch/webkit/press_material/Brochure_A... (site: https://fcc.web.cern.ch/society) [3] https://indico.cern.ch/event/760053/contributions/3152652/at... [4] https://www.sciencedirect.com/science/article/abs/pii/S00401... reply hobo_mark 17 hours agoprevHe didn't discover the Higgs, but he formulated it (along with other people) in 1964. Its discovery was not until 2012. reply empath-nirvana 16 hours agoparentDiscovery can happen in a purely mathematical/theoretical context, too. reply hbrav 16 hours agorootparentI think it's fair to say the 'discovery' that this boson exists came with the LHC experiments. But Higgs did discover in 1964 that the Higgs boson could explain why particles have mass. His paper couldn't say \"this is definitely the way the universe is\", but rather \"if the universe plays by the rules we think it does, this is a relatively simple way to explain this thing we see\". And in my mind, both of those achievements are awesome. reply godelski 7 hours agorootparentIt's definitely semantics and I'm not sure it is worth arguing if we understand what one another means. Unless we're clarifying. But I do think it is good to discuss the role that others have played in the discovery. After receiving the 2017 Kip Thorne said (Weiss and Barish were the other two) It is unfortunate that, due to the statutes of the Nobel Foundation, the prize has to go to no more than three people, when our marvelous discovery is the work of more than a thousand. Even discoveries of the past were due to the efforts of many. Einstein's shoulder's of giants, which I jokingly refer to as \"3 scientists in a trench coat, all the way down\". But with modern science, the problems are more difficult and the effort to make these breakthroughs more clearly depends upon the work of hundreds or thousands. It is good to promote these ideas and this recognition. I think it can also help motivate us to better work together, and not letting us think we are a lowly unimportant cog in a giant machine. Because while we may be small cogs, our work is still important (even if not as important as others). reply zelphirkalt 13 hours agorootparentprevExactly. Give the countless engineers behind LHC some of the credit as well! reply nilkn 6 hours agorootparentprevI still wouldn’t use the word “discover” in this context. Physics is not pure mathematics. There are many theories that just do not correspond to reality. Would you say that strings and extra dimensions have been discovered? No, we’d say they’ve been predicted by a certain theory, but not discovered yet — the prediction may not even be right! reply ayakang31415 13 hours agoparentprevPrecisely, he predicted it. reply freddealmeida 17 hours agoparentprevnext [16 more] [flagged] icepat 16 hours agorootparentIt sounds like this comes from a misunderstanding of probability and how it applies to physics. Can you explain what you mean? reply T-A 15 hours agorootparentprevhttps://medium.com/@chris.m.pease/the-higgs-boson-and-5-sigm... https://www.nature.com/articles/s41586-022-04892-x reply ajross 17 hours agorootparentprevThe same is true of every other boson, except arguably photons in the classical limit (e.g. you can put a multimeter on an antenna and detect them \"non-probabalistically\"). The Higgs detection is sound. A quick check of wikipedia says 5.9 sigma. Good enough for me, anyway. reply bee_rider 17 hours agorootparent> e.g. you can put a multimeter on an antenna and detect them \"non-probabalistically\" I think I get what you mean, but we should be clear—every measurement is probabilistic, right? We can measure photons with low enough inaccuracy that we don’t bother quantifying the probability that they might not exist. But, it is actually on some philosophical level a difference of degree, not type. I’m pretty sure. reply ducttapecrown 14 hours agorootparentWhat does it mean for a measurement to be probabilistic? I'm a math grad student, so still a layman, but here's the way I think about it: Every particle travels over every possible path as the wave spreads out. Once you measure the particle, it tells you a path. So the measured value of the particle is probabilistic, not the measurement. reply dguest 16 hours agorootparentprevI don't know where you get 5.9 sigma but that might have been the confidence at the time it was discovered in 2012. Since then the LHC took a lot more data and the confidence is more like 20 sigma. That said, these days it's harder to find a paper that gives a confidence for discovery, since they are all quoting the signal strength with respect to the standard model prediction. They generally measure this within about 5%, see for example https://arxiv.org/abs/2404.05498 reply golergka 16 hours agorootparentprevAny evidence of any physical phenomena is probabilistic. And with Bell's inequalities, we know that for a fact. reply BobaFloutist 16 hours agorootparentAny evidence of any physical phenomena is probablystic? Maybe this is true of you go deep enough into mathematical/philosophical/quantum rabbit holes, but it doesn't pass the intuitive smell test - I could off-hand name tens of physical phenomena that will perform as expected 100% of the time, deterministic physics (on the macro scale, at least) are like the entire basis for the scientific method. reply para_parolu 15 hours agorootparentYou may think you have 100% probability of event but then simulation glitches and outcome is different. reply eigenket 16 hours agorootparentprevThis doesn't follow from Bell's inequalities. There are deterministic models of quantum mechanics which are compatible with Bell's theorem. reply golergka 14 hours agorootparentThat's why I argue with people on the internet — to be corrected and learn new things in the process! Please tell more. reply eigenket 12 minutes agorootparentBells theorem says that he predictions of quantum mechanics are incompatible with any \"local hidden variable model\". Experiments show that the real world appears to follow the predictions of quantum mechanics, so no local hidden variable model can explain what we see in these experiments. Therefore all you need to understand is what a \"local hidden variable model\" is, and what the lack of one would mean. You should think of doing measurements in quantum mechanics as something like flipping a biased coin, or rolling a biased dice. Every time you do a measurement you get some outcome (maybe heads or tails or a number from 1 to 6) but the probabilities aren't generally the same, they're biased in a way which tells you something about the system. They can even be biased so much that the outcomes are deterministic (0 and 1 are still probabilities). When you flip a coin or roll a dice, the outcomes are random from your point of view, but there isn't really any fundamental randomness going on. There's a bunch of data about how your hand moves, and the local wind speed and pressure and the physical characteristics of the coin which if you knew them then you'd be able to predict the outcome with certainty. People have even been able to build robots which can reliably flip coins to always get a desired outcome. This \"extra information\" that makes the outcome deterministic if you know it we could call hidden variables. Its \"hidden\" because you don't generally know it. If Bell's theorem said that there was no hidden variables model consistent with the predictions of quantum mechanics we'd be done now. We would have decided that quantum mechanics is fundamentally probabilistic and no extra hidden information could exist to make it deterministic. But Bell emphatically doesn't do that. The theorem says there is no local hidden variable model consistent with quantum mechanics. So what does \"local\" mean here? For that you can imagine that instead of rolling one dice I'm rolling one here, you're rolling a second one wherever you live, our friend on Mars is rolling a third one, etc. A hidden variable is \"local\" if it only affects the dice in once location. Bell's theorem says that if you want to have a hidden variable model that reproduces quantum mechanical predictions then it needs to be nom-local. That is the hidden variable has to affect the experiment here, and the one with you, and the one on Mars all at the same time. We consider that non-local hidden variables are unlikely because they're essentially magic, they need to affect things arbitrarily far away from each other all at the same time. Amongst other things this is wildly on conflict with special relativity. reply marblar 16 hours agorootparentprev> We know that for a fact Probably. reply ducttapecrown 14 hours agorootparentThe only true facts are the useless tautologies... Sad for the theory of knowledge, truly... reply AnimalMuppet 17 hours agorootparentprev\"Probabilistic evidence\" != \"no evidence\". > The error rate is very high. Citation needed. More citation needed to show that it's high enough that it invalidates the evidence. reply trashface 9 hours agoprevHiggs' bosons return to the universe reply santbo 15 hours agoprevRIP. Lucky who is born in an English-speaking country with a short name easy to remember by other English monolinguals. The \"Higgs boson\" has many fathers, but his name got attached to the concept for simplicity, giving him world fame and, ultimately, a Nobel prize when he likely didn't contribute significantly more than others, cf. https://en.m.wikipedia.org/wiki/Higgs_boson#History or https://en.m.wikipedia.org/wiki/Nobel_Prize_controversies#Ph... reply robblbobbl 16 hours agoprevCondolences. reply runnr_az 15 hours agoprevnext [2 more] [flagged] lrivers 15 hours agoparentFABFOB reply freddealmeida 17 hours agoprev [2 more] [flagged] archgoon 17 hours agoparent [–] I believe that several citations of your assertions is in order. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Nobel laureate physicist Peter Higgs, known for proposing the Higgs boson particle, has died at 94.",
      "His theory was validated in 2012 at CERN's Large Hadron Collider, leading to his Nobel Prize in 2013.",
      "Higgs, who primarily worked at Edinburgh University, is remembered for his humble nature and substantial influence on particle physics."
    ],
    "commentSummary": [
      "Physicist Peter Higgs, renowned for discovering the Higgs boson, has passed away, prompting reflections on his scientific contributions.",
      "The discourse emphasizes the crucial role of science communication and science communicators like Carl Sagan and Neil deGrasse Tyson.",
      "The significance of the Higgs boson in validating the Standard Model of particle physics, its potential technological applications, and debates surrounding the economic value of fundamental scientific research are highlighted in the discussion."
    ],
    "points": 870,
    "commentCount": 94,
    "retryCount": 0,
    "time": 1712679713
  },
  {
    "id": 39980268,
    "title": "Beeper Joins Automattic to Enhance Chat Services",
    "originLink": "https://blog.beeper.com/2024/04/09/beeper-is-joining-automattic/",
    "originBody": "Beeper is joining Automattic April 9th, 2024 I’m excited to announce that Beeper has been acquired by Automattic. This acquisition marks the beginning of an exciting new chapter as we continue our mission to create the best chat app on earth. If you haven’t heard of Beeper before, welcome! We make a universal chat app – one app to send and receive messages on 14 different chat networks. You might have also heard about Beeper Mini, our briefly available iMessage-on-Android app. In many ways, our journey has only just begun. Beeper has just over 115,000 users and was, until today, in beta. Given the state of the messaging landscape today, we believe there is a huge opportunity for us to push boundaries and create new experiences in chat. The majority of other chat apps have stagnated, entrenched in their positions, with no significant new players emerging since Discord’s launch in 2015. Given the state of the messaging world, we’ve long felt the need for a strong ally with the resources to support us on our quest. Automattic has a long history of putting user control and privacy first with open source, and great bilateral relationships with Meta, Apple, Microsoft, Google, Matrix and others that we hope can usher in a new era of collaboration. It’s a fantastic match. Automattic is best known for supporting WordPress and WooCommerce – two open source software projects that underpin huge portions of the internet’s publishing and ecommerce infrastructure. Together, we’ll develop software for a third fundamental pillar of the internet: chat. Matt, Automattic’s CEO, and I have known each other for years. He was an early user, supporter and investor in Beeper. We’re very well aligned on our goal (build the best chat app on earth), approach (open source where possible), and independence (Beeper will operate independently as part of Automattic’s Other Bets division). This is a big bet. Automattic is doubling down on chat after their acquisition last year of Texts.com, a messaging app with a similar mission. Our teams and products will merge, and I will take on the role leading the team as Head of Messaging. It will take a bit of time for us to integrate and combine forces under the Beeper brand. We’ve got big plans! I’m really excited about the future of chat 📟 Eric Migicovsky Beeper CEO → Automattic Head of Messaging For Beeper users… The Beeper app you know and love is only going to get better with the support of Automattic! In fact, today we’re making Beeper available to everyone – no more waitlist. Learn more. Our privacy policy and terms of service remain the same, though they may change in the future. As always, Beeper (and now Automattic Inc.) cannot view any of your historical chat history. All messages are encrypted with a key that only you have before being stored. Read more about our security and privacy commitments. Our business model is unchanged. We build a fantastic free chat app, then charge for additional premium features. If you no longer would like to use Beeper, visit account.beeper.com to delete your account. For Texts.com users… Nothing is changing today. Texts is the same product today as it was yesterday. Beeper and Texts share a common goal of building the best chat app on earth. Texts’ commitment to local data processing will be preserved. Over time, we will work to integrate the teams and products. More news to come in the future! Share this: Twitter Facebook Like Loading… Discover more from Beeper Blog Subscribe to get the latest posts to your email. Type your email… Subscribe Leave a comment",
    "commentLink": "https://news.ycombinator.com/item?id=39980268",
    "commentBody": "Beeper acquired by Automattic (beeper.com)511 points by Belphemur 18 hours agohidepastfavorite257 comments erohead 18 hours agoExciting day for Beeper! It all started here on HN 3 years ago: https://news.ycombinator.com/item?id=25848278 reply COGlory 17 hours agoparentCongrats on the new position and future of Beeper. I think Automattic will be a great home and I've been impressed with what they've done with other projects as well. Are you planning to continue using Matrix to underpin Beeper? Now I just wish there was convenient way to screen my Beeper notifications without having to take my phone out of my pocket every time it buzzes... reply erohead 17 hours agorootparent> Are you planning to continue using Matrix to underpin Beeper? Yes, all of Beeper is built on top of Matrix. reply gigatexal 4 hours agorootparentIIRC matrix and its foundation? were hurting for funding a while back and might yet still be. Did Beeper contribute to it or does Automattic plan to? reply erohead 3 hours agorootparentWe did: https://matrix.org/blog/2023/06/beeper-joins-the-foundation/ reply gigatexal 34 minutes agorootparentBravo! reply bigiain 9 hours agoparentprevHopefully a more \"exciting day\" for the users than when Pebble sold out to Fitbit... reply gwerbret 17 hours agoparentprev> Exciting day for Beeper! Congratulations on your successful exit, but is it actually an exciting day for Beeper? Your product has been assimilated into something of a behemoth, whose goals (in spite of the text of your announcement) aren't necessarily those of a free and open internet through open communication protocols. Do you foresee a five-year timeline in which anti-monopoly legislation forces an opening up of the WhatsApp/iMessage/Messenger walled gardens? And if that were to happen, do you foresee Beeper still existing as a standalone product? Do you foresee there actually being a need for Beeper versus, say, a rejuvenated (and very open) Pidgin? Yeah, neither do I...although I do prefer to be optimistic. reply erohead 17 hours agorootparentIf you've followed Beeper, you know that roughly 50% of our product is already open source (https://github.com/beeper). You can run your own Matrix server and use our bridges, or self-host the bridges and use Beeper clients (https://github.com/beeper/bridge-manager). For all intents and purposes, Beeper _is_ the rejuvenated and very open Pidgin. Automattic is a fantastic home for us. We needed a home (or new investors) to keep working on Beeper. reply rw_grim 11 hours agorootparentFWIW, Pidgin is still very much alive, we even had a release in February... Also not sure what \"very open Pidgin\" means as we're 100% GPLv2 but whatever. reply erohead 3 hours agorootparentGrim, I apologize. I spoke too soon. I didn't mean to offend you or demean pidgin. I was reacting too harshly to the parent. reply heyoni 9 hours agorootparentprevI haven’t used pidgin in years but does that Teams plugin really work? I’ve seen projects try to integrate it into matrix and drop off because it was too hard. reply EionRobb 8 hours agorootparentWorks for me! Although I'm the plugin's developer, so I may be a little biased :D reply rw_grim 9 hours agorootparentprevIt did last I knew, but I don't use teams so I don't know for certain. reply heyoni 9 hours agorootparentYou are one lucky person and I envy you. reply rw_grim 8 hours agorootparentYep it's a great place to be especially when the CEO from a company punches down on a volunteer project on the day that CEO sold their company for 125 Million USD... /s reply heyoni 8 hours agorootparentHaha sorry I meant lucky as in not having to use Teams. reply rw_grim 7 hours agorootparentYeah I know, I was just using the opportunity to vent some frustration :) reply Grustaf 4 hours agorootparentprevIs 125 m the actual number? I would be surprised if they got more than 10. reply rw_grim 4 hours agorootparentThat's what tech crunch and others are reporting https://techcrunch.com/2024/04/09/wordpress-com-owner-automa... reply fencepost 9 hours agorootparentprevAlso worth noting that some past Automattic acquisitions have also been open sourced, e.g. Pocket Casts back in 2022. There's a LOT of stuff in the >900 Automattic repos on Github: https://github.com/orgs/Automattic/repositories reply xinayder 12 hours agorootparentprevSince now you have more resources, do you plan on open sourcing your whole solution, or will Beeper still be \"half\" open source? As far as I'm aware you still cannot use the Beeper client with your own self-hosted Matrix instance, and there's quite a lot of Beeper-only features that never became Matrix spec proposals. Is there a plan to make these features available to the whole Matrix ecosystem? Should I be worried that open source development of the mautrix bridges will cease, to focus on a commercial product? reply heyoni 9 hours agorootparentThe only thing the beeper client itself brings over your garden variety matrix client is a coat of paint. It’ll make group chats between you, your friend and the bot that’s forwarding everything look like a private chat. Or the chat between a phone # and you will look like the contact name and you and are talking (and if that contact changes so will the chat). Basically beeper’s open source contribution right now feels like it’s hit a point of no return. I say this as someone who contributes to one of the bridges on my free time. I now know enough about it to feel confident that spending time contributing is time well spent _if_ you’re ok dispensing with the creature comforts the beeper app provides. What I mean by that is that you can always run bridges against your own matrix instance using any of the other open source clients. It’ll be ugly as hell, but it will work. What we really need to worry about now are the platforms messing with our ability to take our data off of their systems and into our own. There are a million ways to otherwise hack the existing open source matrix clients to make them look however we want. reply okennedy 15 hours agorootparentprev> Automattic is a fantastic home for us. We needed a home (or new investors) to keep working on Beeper. Is the added investment necessary to scale out specific features, or is it to address challenges related to profitability? reply Grustaf 4 hours agorootparentWhat he meant was they were running out of money. reply nextaccountic 15 hours agorootparentprev> 50% of our product is already open source What parts are closed source? Are there any plans to open them up? reply spxneo 14 hours agoparentprevCongratulations! Must be an absolute thrill reply jamesmunns 18 hours agoprevI really like Beeper, and congrats to them for getting acquired, but my first reaction is \"aw damn how long until the parts I love about Beeper get incredible journey'd away\". The new android app is really good, the desktop app has always been a step up from other Element apps I've used (it is a distant fork, these days), I don't have iMessage, but it works great with Matrix and Signal and WhatsApp a ton of other \"rarely used\" apps I have. reply COGlory 14 hours agoparentAutomattic rescued Pocket Casts from NPR, and made them open source. Tumblr from Verizon. In general they're very good stewards. reply freedomben 13 hours agorootparentOh wow, thank you for posting this! I did not realize that they open sourced Pocket Casts. That is really neat of them. reply rmccue 15 hours agoparentprevAutomattic very rarely shuts down any products they acquire, so I'd certainly be less concerned with them as the buyer compared to another company. (Their failure mode for acquisitions tends to be underresourcing them and letting them stagnate, not actually shutting them down.) reply Rebelgecko 11 hours agoparentprevFWIW, Automatic has bought a few projects I use (PocketCasts, Simplenote) and seem to have been pretty good stewards. For Simplenote they actually reduced the monetization (although a decade after the acquisition they've started remonetizing in a lowkey way which is hopefully sustainable) reply lagniappe 18 hours agoparentprev>incredible journey'd away I'm calling Webster's right now. reply Philpax 18 hours agorootparentWelcome to our incredible journey: https://ourincrediblejourney.tumblr.com/ reply Analemma_ 15 hours agorootparentThat link is probably more apropos than you intend, since Automattic also now owns Tumblr and has been a decent steward of it thus far, not shutting it down even though it undoubtedly loses lots of money. Instead, they made a good-faith effort to make it profitable, before putting it in minimal-cost maintenance mode when that didn't work out. If an \"indie service\" I loved had to get bought by somebody, Automattic would be among my top choices. reply Belphemur 18 hours agoprevI wonder if the plan is to merge Beeper and Texts.com. They both look verify similar in their features. Maybe texts.com for the Apple Ecosystem and Beeper for the Android one ? After all, Beeper just released their new version of the app for Android. reply batuhanicoz 16 hours agoparentBoth Texts and Beeper are in active development while we work hard behind the scenes to combine the best parts of both under the Beeper brand. So we'll only have Beeper on every platform. reply austinkhale 18 hours agoparentprevSeems that way. From the announcement: \"This is a big bet. Automattic is doubling down on chat after their acquisition last year of Texts.com, a messaging app with a similar mission. Our teams and products will merge, and I will take on the role leading the team as Head of Messaging. It will take a bit of time for us to integrate and combine forces under the Beeper brand. We’ve got big plans!\" reply drkleiner 17 hours agoparentprevThey will likely dismantle Texts and focus on Beeper as it received more investment while taking the good of Texts. They will probably introduce Texts monetization model to Beeper though. One app for Apple and another for Android kinda defeats the purpose of \"all in one platform\", wouldn't it? reply jamiequint 16 hours agorootparentI hope not, have you used both apps? Texts is much more stable and easy to use than Beeper in my experience. reply artdigital 12 hours agorootparentNot my experience. I’ve tried (and am still trying) texts and not once I was able to have it just work. Constantly messages not showing up, accounts not loading, or something else missing. I’ve been submitting heaps of feedback to the Texts team. It looks great but feels very alpha/beta to me, and I decided to not renew my subscription reply rattray 8 hours agorootparentI'm a pretty avid Texts user, and I both agree and don't… as much as I ~regularly run into hiccups of some kind, I feel they're pretty inevitable with this sort of thing, and the parts of Texts that _could_ be stable definitely are. The overall product feels polished and snappy IMO. reply artdigital 4 hours agorootparentI've not had a day without issues. I don't know if this is something wrong with my account (instagram), but it's so unreliable that I can't trust that what I see in Texts is actually how the conversation looks like. Funnily enough, I ran Beeper in parallel as backup. Weird things from a message missing in between other messages, new messages not showing up at all, messages I send not arriving, etc. (The iOS app is on a completely different level with it not even refreshing my messages most of the time until I disable and enable certain accounts again. But it's in TestFlight so I'll treat it as a beta and not expect too much polish yet) I've religiously submitted feedback constantly to them, together with console logs and error dumps, but now my trial expired and I just can't justify paying for it in the current state :/ reply qeternity 14 hours agorootparentprevTexts.com runs locally, Beeper runs bridges. reply Grustaf 15 hours agorootparentprevThe Beeper codebase is also in a pretty bad state. reply COGlory 14 hours agorootparentWhat information is this based on? reply Grustaf 1 hour agorootparentThe codebase reply hbn 15 hours agorootparentprevDidn't they just do a ground-up rewrite? reply Grustaf 4 hours agorootparentLet’s hope so reply timdorr 18 hours agoparentprevThe bottom line of the post says as such (sorry if it was edited in after you posted) \"For Texts.com users... [...] Over time, we will work to integrate the teams and products. More news to come in the future!\" reply TechRemarker 18 hours agoparentprevYes, it mentioned Texts and Beeper teams will be merging under the Beeper brand. So essentially the end of Texts. Presumably will bring some parts of Texts app to Beeper that doesn't exist, but overall glad I abandoned Texts little while back, since while loved the idea, was very buggy with notifications delayed, or sometimes unable to send certain types of messages through certain services etc. And no pathway at the time for support on iOS. With all the EU stuff, there is a chance maybe people will be forced to open up more as would love to have a single app to manage all messaging but certainly far from hopeful that will come to be, at least without downsides. reply bluish29 18 hours agoparentprevBeeper is cross platform. They have an app for iOS and Mac. reply batuhanicoz 16 hours agorootparentBoth of our apps are cross-platform. Both Texts and Beeper are available on Windows, Linux, and macOS. Texts for iOS is in public beta, you can join the TestFlight: https://texts.com/install/ios reply ghostly_s 17 hours agorootparentprevOP didn't day Mac. They were presumably referring to Apple's main platform, iOS. reply bluish29 16 hours agorootparentI replied before the OP edited the comment substantially. reply joenot443 18 hours agoparentprevDoes anyone know how the texts.com iMessage integration works? EDIT - It’s macOS only. Nothing new to see here, I’m afraid. reply ghostly_s 17 hours agorootparentCame to ask the same, is that really true? They have a graphic on their homepage that certainly implies you can use it as a replacement for iMessage on iOS. I can't imagine anyone doing that if you can no longer actually reach anyone on iMessage. reply CharlesW 16 hours agorootparent> They have a graphic on their homepage that certainly implies you can use it as a replacement for iMessage on iOS. You can. You run a \"router\" on your Mac, which allows iMessage interop without the protocol hacking that doomed Beeper. reply gmays 16 hours agoparentprevYes, from the post: > Automattic is doubling down on chat after their acquisition last year of Texts.com, a messaging app with a similar mission. Our teams and products will merge, and I will take on the role leading the team as Head of Messaging. reply dorian-graph 17 hours agoparentprevMerging the 2 will be interesting since they take opposite approaches? Texts doesn't use bridges. reply gnicholas 18 hours agoprevDoes anyone know how much Beeper raised, or how the economics of this exit work out? Just curious how this stacks up for everyone. UPDATE: To date, Beeper had raised $16 million in outside funding, including an $8 million Series A from Initialized. Other investors include YC, Samsung Next, Liquid2Ventures, and angels Garry Tan, Kevin Mahaffey, Niv Dror, and the group SV Angel. [1] 1: https://techcrunch.com/2024/04/09/wordpress-com-owner-automa... reply BadHumans 18 hours agoprevIf they had to get acquired, Automattic is most likely the best I could've hoped for. The backing of a large company might help Beeper get more formalized APIs since they depend on hidden APIs and hacks to do what they do. reply ravetcofx 17 hours agoparentI've been pretty impressed with their Pocket Casts acquisition from NPR to date, they open sourced the apps which was a great bit of community service. reply rchaud 18 hours agoparentprevIs Automattic big enough to achieve that? You'd think they would be making a pretty penny from Wordpress.com and Tumblr, but these days you wouldn't pick either one over the competition (Wix, Squarespace Threads, IG etc). reply joenot443 18 hours agorootparentI still choose WP over Wix or SquareSpace when given the option. Self hosting a bunch of WP sites on a DigitalOcean instance and mirroring them all through CloudFlare is probably the cheapest way to run high traffic read-only sites. Toss in ServerPilot and you can spin up sites about as quickly as you could doing the Wix onboarding. My prediction is that by 2030 Wordpress will be just as popular as it is today :) reply rchaud 17 hours agorootparentWordPress.com isn't self-hosted, it's the freemium cloud version comparable to Wix and Squarespace. Automattic isn't making any money on the open-source version that can be hosted anywhere. reply joenot443 16 hours agorootparentYou’re right, that distinction’s very important. I’d never recommend WordPress.com, I’m just a fan of Wordpress the OSS. reply ravetcofx 6 hours agorootparentprevStatic HTML is even cheaper. I don't understand how some people can't think outside of CMS land reply qingcharles 12 hours agorootparentprevIn the past I would have argued against you, but the WP block editor has taken away most of the advantage that Wix/SquareSpace had. I've done the same as you now. You can buy a $3/mo Hetzner ARM box and run 100 WP sites on it. reply themoonisachees 18 hours agorootparentprevMaybe. It seems automattic is realizing that there is value in being \"the established service that does what the giants are doing but isn't the giants\". Personally and I think the HN crowd would agree with me, I see the value proposition in using Tumblr, beeper, WordPress over their competitors, simply because automattic has amassed a lot of consumer trust in not running things into the ground and making interesting things out of software that seems like it shouldn't be there. reply mbesto 17 hours agorootparentprevAutomattic does like $200~$300M in revenue. reply drkleiner 4 hours agorootparentThat doesn't sound much considering they employ like 2k people, that's $100k per worker and considering they're tech workers that would just cover their pay reply fuzztester 15 hours agorootparentprevAnd profits? reply maxloh 10 hours agoparentprevIMO hidden APIs are not quite safe to use. As a user, you are susceptible to be banned by the service when they find out that you have another active session in another country or city. Some users on Reddit reported that they get suspended by Instagram because of Beeper. reply philippgerard 1 hour agoprevI guess it's good to join forces on this. However, I really hope the Texts app on Mac will survive this merge as it is lightyears ahead of the UI insult that Beeper is. I migrated away from Beeper to get rid of the ugly interface and in order to not depend on bridges on some server somewhere (yes, I can host them myself, but who wants to?) anymore. reply alalani1 15 hours agoprevAutomattic basically consolidated the messaging aggregator market for $175M - I don't have any data but given they had 40 FTE between the two companies, I would guess not more than $10M in revenue across the two. Anyone understand the business rationale for wanting to own this market? reply xmprt 14 hours agoparentNo matter what, as long as we continue to communicate with each other, messaging will never die. Individual messaging platforms will come and go but the idea of instant messaging has existed for about as long as the internet. $175M might be a steal if Beeper gets more mass market appeal. reply alalani1 13 hours agorootparentYeah, my first response was going to be that the paid messaging app business model hasn't worked that entire time. But what we've learned from Whatsapp is that there may be other ways to monetize (Whatsapp Business). Still not an obvious bet IMO but you're right that owning the messaging layer is valuable. reply chiefalchemist 10 hours agorootparentprevYes. Messaging will never die. Given that, why has it been so difficult for so many to get a toe-hold - and be sustainable - for so long? I agree with the sentiment of your comment. However, I'm not sure it answers: what's the justification of this purchase? What does the buyer know that others have not? reply maged 8 hours agoparentprevWhere is the $175M number from? reply alalani1 8 hours agorootparentTechCrunch reported this as a $125M purchase and previously texts.com for $50M reply zamadatix 8 hours agorootparentprev125 for Beeper and 50 for Texts reply dankwizard 7 hours agoprevIsn't this the joke from The Office with WOOPH or whatever it was... and an old program called Pidgin? Nothing innovative here that's crazy reply owenpalmer 6 hours agoparentHaha I was thinking about that, but the joke is actually the reverse of this. When someone sends you a WUPHF, you receive it on every medium. Phone, email, fax, text. Beeper does the opposite, anytime you receive something on one of these platforms, it consolidates it to the Beeper app reply egman_ekki 2 hours agorootparentSo a FHPUW, then. reply pedalpete 8 hours agoprevThat's an acquisition price of $1,086/user, note that isn't active user! Amazing price Eric and the team got for an app that doesn't even mention AI! reply gmays 16 hours agoprevCongrats! Matt and the team at Automattic are a good group of folks. It seems like a great fit, and I'm excited to see what you guys do together. reply photomatt 9 hours agoparentThank you! reply philg_jr 18 hours agoprevThis is cool. I always knew that Matrix could theoretically support anything chat related, but I had no idea that Beeper existed and a company was formed around the idea. This is actually exciting to me. It sounds stupid, but I can't wait to ditch the Apple ecosystem. reply LorenDB 18 hours agoparentDitching Apple is not stupid at all. There's plenty of options that are less hostile overall and provide more choice. See also: https://youtube.com/watch?v=qcH2wgRLiV8 reply k8svet 15 hours agoprevThe Verge reports: > Since then, Beeper has rolled out some security upgrades that change the way the app handles security and prevent Beeper itself from seeing unencrypted messages from Signal, WhatsApp, and other encrypted apps. Screenshots of Beeper’s desktop and mobile apps. How? reply erohead 3 hours agoparentHere's how it works: https://blog.beeper.com/2024/04/09/how-beeper-android-works/ reply kevincox 14 hours agoparentprevBeeper has been talking about running on-device bridges rather than server-side bridges. So presumably that would allow encryption to the user's device, then re-encrypted on-device to Matrix then sent to Beeper. reply k8svet 13 hours agorootparentWow, that's ... surprising. reply btseytlin 15 hours agoparentprevProbably on device encryption before anything is sent through their servers reply Rebelgecko 11 hours agorootparentThere's an \"experimental\" option in the app to run the server locally. I don't use signal so not sure if it works as well reply k8svet 15 hours agorootparentprevI'm sorry, but that's not an answer. WhatsApp encrypts the message E2E, and then Beeper is bridging it into Matrix. The only way this would make any sense is if they've bridged E2E all the way through to ... the clients? Like, it doesn't make any sense. reply talhah 8 hours agorootparentThey actually reverse engineered Apple's iMessage and released a POC. At the time it worked and demonstrated on device bridges being a possibility. The runs locally and then encrypts it client side before it reaches beeper services. This can be enabled for Signal under experimental settings already. https://blog.beeper.com/2023/12/05/how-beeper-mini-works/ reply k8svet 13 hours agorootparentprevWell, now I regret that this comment sounds a bit dismissive, if another reply is true and they run the bridge locally. That's... impressive, and still surprising to think about. I'd honestly be fine with accepting the compromise of hosted bridges, especially since they seem to offer the ability to self-host the bridges, while using Beepers' Matrix infra. Gives them some extra cred in my book. reply iLoveOncall 15 hours agorootparentprevIt's a journalist's take, they probably mean that Bleeper doesn't phone home the messages from the client. reply xinayder 15 hours agoparentprevThey break the terms of service of said services, they started this with iMessages last year and Apple shut them down after it was an obvious break of ToS, and they keep doing the same with other IM services. reply latchkey 18 hours agoprevHere is my Beeper timeline: March 9, 2021: Paid $120 June 2, 2021: It's your turn to start using Beeper June 10, 2021: Invite code for upcoming Beeper onboarding July 2, 2021: totally locked up now At which point, I had a few more back and forth emails (looks like about 27 from 7/2/2021 - 7/16/2021), and finally gave up. Good for them to get that exit. They put a lot of work into it, but it seemed like an impossible problem to solve. reply throwitaway222 14 hours agoparentSounds like that was a time period where it was vaporware? reply idk1 2 hours agoprevMight be a bit late to the debate, but is Beeper secure enough to use? I'm apprehensive about installing it. reply diamondfist25 18 hours agoprevI wonder how texts.com and the matrix apps will work together. I’ve dug deep into both products —- texts.com basically is browser in browser app that pulls in messages from your logged in session, while matrix you have to deploy bridges that pulls messages and sync to a db in the cloud, and app then syncs to that. Matrix was a bit pain in the ass to work with, and texts.com was more straight forward reply edude03 18 hours agoprevKind of surprising, 1- because Automatic is known for wordpress, what's the play with messaging? 2- They apparently acquired texts.com as well so clearly an area they are interested in. I also would prefer not to see consolidation but also I imagine it's hard to make money on this kind of product so maybe that drove the decision to merge? reply basisword 18 hours agoparentAutomattic owns a few properties other than Wordpress now. They also own Pocketcasts (podcast client), Tumblr, and Day One (journalling software). reply artdigital 7 hours agorootparentOh TIL that Day One is also Automattic reply brianjking 8 hours agoprevCongrats to Beeper, but I think that means I'll end up having to pay $25 or something to keep using it. How will this differ from https://texts.com/ which is also owned by Automattic? reply ein0p 17 hours agoprevHow does Automattic even survive and remain profitable (?) nowadays? What’s the niche? reply rmccue 15 hours agoparentAutomattic is primarily four core businesses: 1. WordPress.com - Hosted SaaS for building websites. Very cheap to run, includes ads on free sites, and they sell users upgrades like custom domains or removing ads from their sites. 2. Jetpack - Services for self-hosted WordPress sites, like backups, social media integration, image CDN, antispam - mostly paid services, again very cheap to run. 3. WooCommerce - Open source ecommerce service. They sell add-ons like payment gateways, and have their own with a percentage fee of each transaction. 4. WPVIP - Enterprise and high-scale hosting service. Expensive ($25k/yr+) but worth it for large companies who run their business off it, like newsrooms and marketing sites. To my knowledge, they're each independently profitable. Automattic also has a lot of additional smaller services which support these, and various Other Bets. (I don't work at Automattic, but have many friends there.) reply krallja 14 hours agorootparentI am on an old Premium plan for the Day One journaling app. It was recently acquired by Automattic. Happy to keep paying, but concerned that Apple recently Sherlocked it. reply oarsinsync 3 hours agorootparentThe Apple Journal app is really really limited, and probably good enough for anyone who isn’t a serious journaler. The only “innovative” feature the Apple Journal app has is access to private data that other apps don’t, to be able to come up with ML-based journalling recommendations. However, this feature is also exposed via an API, for other journalling apps to implement too. So… if you’re not actually a journaler, the Apple Journal app is probably good enough for you. If you actually journal, you’ll probably use Day One, because the set of compromises it makes best aligns with the majority of active journalers. It upset me when they turned off iCloud sync in favour of their proprietary server sync, because they’re effectively asking me to trust them with my data. They finally implemented E2EE though, and it looks like it’s been implemented properly, so it’s less bothersome again. It’s just a shame that Diarium refuses to implement high resolution images, or they’d actually be a viable competitor with the ability to self-host your E2EE DB sync. reply ghostly_s 17 hours agoparentprevPretty sure plenty of small commercial websites still host on wordpress.com. That and selling their user data to OpenAI. reply throw_m239339 17 hours agoparentprevA big chunk of news media online use wordpress instead of a custom CMS. So it's kind of the square space / wix / ghost for news media. I imagine Wordpress offers managed website instances for these companies. reply markx2 17 hours agoprev\"Matt, Automattic’s CEO, and I have known each other for years. He was an early user, supporter and investor in Beeper. \" Beeper were not important enough to be listed: https://audrey.co/ \"Our privacy policy and terms of service remain the same, though they may change in the future.\" Reminder: wordpress.com shares data with 851 other companies. reply PaulHoule 17 hours agoparentHow does Automattic make money? Do they get paid to look the other way when people’s Wordpress blogs get spammed by SEO links? reply trickjarrett 17 hours agorootparentThey provide hosting for Wordpress blogs, and a number of their backend tools have premium versions, such as backups, etc. reply RobotToaster 16 hours agorootparentprevThey run a proprietary SaaS called jetpack that injects a lot of tracking into otherwise self hosted wordpress blogs. reply ecommerceguy 17 hours agorootparentprevWooCommerce is one. reply Contortion 17 hours agoparentprevPlus your messages will no longer be fully E2E encrypted. As per their FAQ (emphasis mine): \"For example, if you send a message from Beeper to a friend on WhatsApp, the message is encrypted on your Beeper client, sent to the bridge, which decrypts and re-encrypts the message with WhatsApp's proprietary encryption protocol.\" Directly underneath that they also say: \"Using native end-to-end encrypted chat apps independently may be more secure than connecting to them to Beeper\" https://www.beeper.com/faq#how-does-beeper-connect-to-encryp... reply COGlory 17 hours agorootparentWhat do you mean \"no longer\"? Beeper has always behaved like this. reply Contortion 17 hours agorootparentNo longer as in for a new user that currently uses individual apps. reply lolinder 17 hours agorootparentIn context your use of \"no longer\" is very confusing. This thread is talking about what might change in Beeper as a service, if you want to interject with information about how the service currently works there are other phrases that would have made that clearer. reply Contortion 16 hours agorootparentYou are correct. I was adding what I considered extra privacy-relevant information in response to GPs statement about WordPress sharing data with other companies, but the fact that I'd not heard of Beeper before unintentionally influenced my word choice. reply mintplant 16 hours agorootparentprevYou can host the bridges yourself: https://github.com/beeper/bridge-manager reply snowfield 13 hours agorootparentStill won't be E2E as per their FAQ reply tylerritchie 12 hours agorootparentthat FAQ is accurate but (rightly) doesn't cover high-security deployments. if I'm running the bridges local-to-the-client (I am, on my McBook) it's not meaningfully any less e2ee. encryption happens in the matrix client (running on the laptop), the encrypted message is sent to the homeserver on localhost, the bridge (on localhost) grabs the encrypted message and decrypts it, then the bridge re-encrypts it and sends it to Whatsapp (or wherever). the content of the message is as secure over the wire with this approach as using first-party apps directly if one hosts their own bridges they're person-in-the-middling themselves and should take all the necessary precautions. if they're using beeper's hosted options they have to delegate read/write ability to beeper (though I think the signal and imessage bridges might be device-local), and beeper is clear about that. reply agile-gift0262 12 hours agorootparentprevBut at least you are in control of the computer where the decryption and re-encryption is happening. They usually call it E2B (end to bridge) reply kayson 17 hours agoprevBummer that you have to use and link Google messages for sms now. With the old app, it used the local sms database, though performance was never as good as other sms apps for some reason. reply freedomben 13 hours agoprevand they're rewriting Beeper in PHP as a WordPress plugin! Kidding of course, but on a serious note what's the strategic value of Beeper here to Automattic? From the post: > Automattic is doubling down on chat after their acquisition last year of Texts.com, a messaging app with a similar mission. Our teams and products will merge, and I will take on the role leading the team as Head of Messaging. It will take a bit of time for us to integrate and combine forces under the Beeper brand. We’ve got big plans! I’m really excited about the future of chat Why? Are they trying to eliminate a competitor (standard big tech co move)? Do they think texts.com doesn't have a bright future? I'm not trying to be negative (in fact I have a high opinion of Automattic due to their great open source work), just trying to figure out why this makes sense for them. Is chat a diversification effort for Automattic, or does it tie into their overall strategy somehow? reply photomatt 9 hours agoparentOur three missions are to democratize publishing, commerce, and messaging. This fits in the third. reply flexagoon 13 hours agoparentprevAutomattic also acquired Pocket Casts, so they don't only own things that fit into the WP ecosystem. reply nikolay 17 hours agoprevAutomattic sometimes acquires companies and does nothing with them - like with Simplenote. As an early Beeper user, I am disappointed! reply glenstein 17 hours agoparentFor me that is a positive, not a negative. They have kept it alive, have not messed it up, and have not upset the apple cart with wild changes resulting in loss of compatibility with past versions. There can be cases where it's a negative but I think Automattic has been a near-perfect steward of Simplenote. I suppose my only concern as ever is how they keep it alive at their expense without freemium'ing and paywalling it to death. I assume they are just footing the bill for its continued operation right now. reply smcnally 15 hours agorootparentAgreed re stewardship & hope for long-term viability. Operation + updates ~3x / year for the iOS, macos and Linux clients I use daily and also for Android and Windows. Freemium and paywalling hasn’t infringed on utility yet and they’ve kept the source code available and GPLv2 https://github.com/Automattic/simplenote-electron reply luuurker 15 hours agoparentprevMany companies don't know when to stop and end up killing their products by introducing bloat which almost no one wants and costs money to develop/maintain. Simplenote works, is maintained, and does what it says on the name... more should be like them. If all they do is keep Beeper running reliably and add new services as needed, then I'd say that's a good thing for most users. reply thoughtpeddler 13 hours agoprevHow much of this is a long-term bet that the US DOJ's antitrust case against Apple (at least the iMessage-specific argument) will result in the forced opening up of the platform, such that Beeper (now Automattic) will sustainably function? reply freedomben 13 hours agoparentI'm sure that's a non-zero factor. Plus Beeper has a lot of name recognition due to their epic standoff reply xinayder 12 hours agoparentprevIs it really anti-trust if you reverse engineered something (deliberately breaking the ToS of said service - Apple can suspend your accounts for doing so) while said service ToS, a contract you sign when you use their service, explicitly forbids reverse engineering? reply dqv 9 hours agorootparentA more succinct question would be \"can antitrust laws override the ToS?\" The answer is \"yes\". The FTC can even make Apple change its ToS through a consent decree (and possibly through other mechanisms as well). A ToS can't shield you from antitrust enforcement. In this case, it might even hurt Apple, especially if it can be shown that Apple doesn't consistently apply this rule. But let's say it's not antitrust, which is what your question is trying to imply. Apple will still need to answer to the other ways it makes third-party messaging apps worse on iOS - automatic offloading and share extensions being two examples. reply duxup 16 hours agoprev>If you haven’t heard of Beeper before, welcome! We make a universal chat app – one app to send and receive messages on 14 different chat networks. I feel a little exhausted just thinking about needing to be on 14 different chat networks. What are people using that for? Marketing / customer service type chat stuff across networks? reply kevstev 15 hours agoparentThis idea has existed and been implemented for a very long time- Pidgin was the first client I can remember that would try to bridge across various chat networks. You likely are on 14 different chat \"networks\" already- you might not think of them as chat networks, but as apps with chat functionality. Just real quick off the top of my head I can name a bunch: FB Instagram Twitter WhatsApp Telegram Signal Gchat/whatever google calls the chat in gmail these days kik sms Irc Discord This is just a matter of keeping all those separate apps, or consolidating them into one single one. Back in the early 2000s when I did indeed have friends across AIM, Yahoo, MSN, etc... I did use an all in one app to consolidate everything. reply theogravity 13 hours agorootparentTrillian was my first client. It looks like it still exists and now runs on a subscription model: https://trillian.im/ Also looks like it no longer connects to multiple networks, but uses their own now. reply kevstev 12 hours agorootparentHa thanks for jogging my memory. I knew pidgin wasn't the first or even the primary one I used but I couldn't recall what else I had used. reply dqv 12 hours agorootparentprevYou can still add XMPP accounts, but that's all. reply batuhanicoz 16 hours agoparentprevRarely a single user would use 14 different chat networks. For example for me, I have two WhatsApp accounts (number for my current country + home country), Twitter, Instagram to chat with old friends, Facebook for some relatives, LinkedIn for networking, Signal to chat with my lawyer, Telegram for hobby communities. I simply wouldn't talk to a lot of people in my life if I had to use a different app for each. We also aim to be the best chat app, period. So even if you are using only a single account on a single network, we want to supercharge your chat experience with AI, snippets, good search, labels, advanced notification options, and much more. reply garbageman 16 hours agoparentprevNot a user but I imagine it's like the blackberry model where texts, messenger, whatsapp, and whatever else all go to the same inbox rather than having to check 40 different apps. I really wish that idea would come back. reply BobaFloutist 16 hours agorootparentI wish that idea came back with some sort of iron hard contract protecting user data in perpetuity. reply croes 16 hours agoparentprevI think it's about connecting people on 14 different chat networks not a single user on 14 networks? reply saynay 16 hours agoparentprevI would imagine most people are not using all 14, but some subset. If you have friends and family that are not all on the same services, having a unified inbox to talk to them all without having to remember who you talk to on what is convenient. reply leros 16 hours agoparentprevIt's exhausting having different apps. I use SMS for talking to friends in the US. I use WhatsApp for talking to most international people. I use Telegram to talk to Australians. I use Line to talk to some people in Asia. reply Grustaf 15 hours agoparentprev> I feel a little exhausted just thinking about needing to be on 14 different chat networks. That's the point, the vision is that you should only have to be on Beeper. reply Spivak 15 hours agorootparentAnd you don't have to convince everyone in your life to switch to The Messenger. You can just have it. reply wahnfrieden 16 hours agoparentprevyes and sales and regular users too linkedin, fb, ig, slacks, google stuff and so on - it adds up quickly reply andrewstetsenko 12 hours agoprevAt some point, it makes sense to join forces for Beeper and texts.com to create a unified messaging app. The idea was realized a number of times, but I hope we’ll see the best it’s iteration now. reply eugenekolo 18 hours agoprevSmart of Beeper to accept an acquisition/exit as soon as possible before their product gets further eroded by non-public APIs they're reliant on. Tough to build a business where at any time your product can get wiped out by Apple/Meta. reply theamk 18 hours agoparentI would not be surprised if this was their plan from day 1: (1) Create a product which relies on non-public APIs which can be disabled at any time. (2) Generate lots of press visibility. (3) Sell itself to some gullible org before the whole thing comes crashing down. reply mvkel 18 hours agorootparentWhat makes automattic gullible? reply wokwokwok 17 hours agorootparentI mean, maybe the reality is more nuanced but the optics… > Matt, Automattic’s CEO, and I have known each other for years. He was an early user, supporter and investor in Beeper It certainly sounds like… Beeper was in the bin after the beeper mini fiasco, and resulting massive brand damage (brand aweness is only good if it means people actually want to use your product, not active distrust it), so the friend and early investor CEO of Automattic saved their failing personal investment by bailing them out and letting automattic foot the bill. Nice having a friend CEO, huh? I mean, you’ve got to hope/believe the other folk at automattic did their due diligence, but it does look like automattic is paying the bills for their CEO to bail out their friends. That would make them gullible and seems pretty shady unless they really do have some astonishing reason to expect this to pay off… or they got it dirt cheap, in my opinion. reply mvkel 17 hours agorootparentThis isn't how business works. A \"CEO friend\" isn't going to give you $125m out of the goodness of their heart so some random shareholders can save face. Worst case, the business fails, everyone happily moves on. How do you think automattic is funding this transaction? There are underwriters. And underwriters need rigorous dd to justify $1,000 let alone $125m. Even if it's an all-stock deal, a transaction like this would need board approval, and a good board needs ... rigorous dd. reply PedroBatista 16 hours agorootparentThat's the script but reality is not exactly as the script says. There are a lot friend and family members of so-and-so who are \"worth\" a few million dollars and that money was largely a gift. They largely spend a lot of time pretending to be \"founders\", \"advisors\" and \"angels\" but no, they got lucky. Not everything is like that, but there's quite a lot of that within the billions of someone else's money's ocean that flushed SF during the last +20 years. reply mvkel 16 hours agorootparent$125 million dollars. To \"help\" a friend save face? When a similar acquisition was made a year ago? And it perfectly aligns with a their stated thesis that texting becomes a CLI in the future? The more successful an entrepreneur, the more allergic they are to spending money reply PaulHoule 17 hours agorootparentprevFrom 2005 to the end Yahoo’s role in the SV ecosystem seemed to be the people who would buy any company if the founder had the right connections, for instance the son of a private equity lord who licensed a worthless patent from Stanford that Yahoo paid $100+ for. It is a magical way to turn “anybody” into a successful founder or VC if they’ve got the right connections and in fact you can create both ex-nihilo in one transaction. If you read the newspapers you’d never find out that 70% of acquisitions achieve their goals. A lot of that is you hear more about the ones that fail and not the ones that succeed. The ones that fail give many people the impression that there is nothing rational at all about how acquisitions happen in corporate America. reply housebear 17 hours agorootparentprevThis is the same board that approved the Tumblr acquisition, presumably. So...a good board...? reply markx2 16 hours agorootparentThey approved the tumblr deal. Matt promised targets. Every target was missed and financially the deal was a failure. The board then stopped any further acquisitions. I would guess that texts / beeper were cheap and that Matt is looking at a very very long time to profit. He was once a fan of the 'pizza/team' ratio that Bezos/Amazon pushed, so maybe that's where he is looking. reply mvkel 16 hours agorootparentprevFor pennies on the dollar, and for a very specific strategy, which Mullenweg outlined pretty thoroughly. Did it work? No, but I don't know of any examples of an entrepreneur who bats 1000 reply ksherlock 17 hours agorootparentprevcounterpoint: Tesla/Musk/Solar City reply paulryanrogers 17 hours agorootparentprevDD? reply tithe 17 hours agorootparentDue Diligence reply afavour 17 hours agorootparentprev> Beeper was in the bin after the beeper mini fiasco, and resulting massive brand damage I don't think that's true at all. I'd never heard of Beeper before their integration with iMessage and the fact that it was disabled by Apple was seen as a negative on Apple, not Beeper. They raised their profile, now they're cashing out. reply Grustaf 15 hours agorootparentprevIt wasn't, they genuinely believe in Beeper, and the long term plan was rather to become the leading chat app in their own right, with users communicating over the Beeper protocol. reply LorenDB 14 hours agorootparentA bit of pedanticism here - it's not the Beeper protocol, it's the Matrix protocol. Beeper is built on top of Matrix (using bridges that you could host on your own server if you wanted). reply Grustaf 4 hours agorootparentWhat i meant was that the idea was that people would eventually stop using e.g. whatsapp on Beeper and just use Beeper, since eventually all their friends would also have the beeper app installed. Might sound a bit optimistic but there it is. reply xinayder 12 hours agorootparentprevExcept there are a lot of Beeper-only Matrix spec implementations that aren't available for everyone else. In other words, no other Matrix homeserver or client can benefit from the features Beeper has. reply nikanj 17 hours agorootparentprevMy theory is Beeper being a sacrificial lamb so Google/Samsung can cry to EU about iMessage. ”See they’re restricting this poor little startup from competing! Pay no attention to the funding coming from Samsung, this is definitely a poor underdog of a company!” reply ClaraForm 18 hours agoparentprevI'm confused, because it legally sounds like the opposite right? All these platforms are legally going to be requested to open up sooner or later because of recent EU rulings. Perhaps Beeper just ran out of runway? They thought they'd be making money in December with their iMessage move, and now it's April? reply basisword 18 hours agorootparentIf all of these services fully open up there will be 100% free and/or open source clients available and Beeper won't be able to compete. Before all of these walled gardens we have tools like Pidgin and if they open up those will be viable again. reply lxgr 18 hours agorootparentTo be fair, almost all networks that Pidgin could connect to (Jabber/XMPP being a notable exception) used proprietary protocols, and their operators heavily discouraged third-party interoperability. The difference is that there wasn't any way to do robust hardware attestation at the time (which is what Apple does to frustrate Beeper-like iMessage interoperability), so the reverse engineers usually won. Here's an interesting story from that time: https://www.nplusonemag.com/issue-19/essays/chat-wars/ reply pkulak 17 hours agorootparentprevThe bridges are already free and open source and I don't see my friends and family racing to get them running on their laptops. Pidgin is great, but something needs to be running on a server or you don't get notifications. And without notifications, what's the point of a messaging app? reply rw_grim 11 hours agorootparentI wrote more on this topic here https://dev.to/grim/why-is-there-no-mobile-version-of-pidgin... reply eugenekolo 18 hours agorootparentprevThat's a bit of my point. At any time your business model can get interrupted when you're doing things \"that the big company isn't, but holds the gate to\". Big company might get more aggressive and make your life harder developing. Big company might get less gatekeeper and open up the gates so now you have more competition. Big company might do it themselves and add interoperability and now your product is almost useless. It's a business model that works to get to some point, and then exit as soon as possible, IMO. reply ClaraForm 18 hours agorootparentI guess I didn’t think about it like that. If everyone opens up, Beeper’s competition is suddenly WhatsApp, instead of being one of their assets. reply riedel 17 hours agoparentprevThey are running Matrix bridges in the cloud for people that do not want to self host. Don't think the iMessage stunt was really planned. It was an opportunity to get visibility. Should rather thank them: they put regulatory pressure on apple. reply yoavm 18 hours agoparentprevWith all the new regulations coming out of the EU, I think that if Beeper stays around long enough they might actually live to see the day where these networks have official APIs. reply drkleiner 17 hours agorootparentThen Beeper would just be replaced by bigger fish (or open source fish) reply yoavm 17 hours agorootparentAll the bridges used by Beeper are already open source, and Beeper itself uses Matrix which is open source too. reply guluarte 15 hours agoparentprevI agree it must be a engineering nightmare, a whatsapp/imessage update and your app is gone. reply blackeyeblitzar 17 hours agoprevI wonder what the angle is here. Is Automattic going to help them fight a legal battle against Apple’s practices with locking down their computing devices and platforms? Do they even have the resources for that? Or is this just an acquihire and abandonment of that whole messaging thing? reply micromacrofoot 17 hours agoparent> Is Automattic going to help them fight a legal battle against Apple’s practices I'm no MBA, but it might be better business strategy to pile up a million dollars and light it on fire reply regularjack 17 hours agoparentprevBeeper is a unified messaging app that supports multiple services, the iMessage thing was a separate app called Beeper Mini. I think Beeper will continue to be useful to people, regardless of whether it supports iMessage or not. Obviously, supporting iMessage would be a big plus, but from what I gather, many people use it for other services. reply kilroy123 18 hours agoprevI hope they don't change the pricing after this. I've been using Beeper for a few years now. reply unshavedyak 18 hours agoparentWhat pricing? I've been on Beeper for a few months and i've not seen any. Ironically i joined because of iMessage, which got almost immediately ripped from me because Beeper thought it was good idea to piss off Apple (... sorry, annoyed). When i joined the service was great, but the last couple months (basically since the iMessage drama) it has been weirdly unstable. I see a lot of encrypted messages (as in, the message doesn't receive properly and all the receiver sees is \"message encrypted\"), images failing, gifs failing, generally bad behavior when connection is poor, etc. I was interested when i joined because i thought they were just going to offer a service i could pay for, like Kagi. Yet now this feels like another enshittification just waiting to happen. reply isx726552 18 hours agorootparent> Beeper thought it was good idea to piss off Apple… Publicity stunt to raise their visibility in hopes of acquisition. Today’s news shows it worked. reply kilroy123 18 hours agorootparentprevI paid $120 up front. I'm worried they'll switch to a monthly subscription after this. reply nicce 17 hours agorootparentSince texts.com has subscription and rumor says that they are merged, just a matter of time. reply drkleiner 17 hours agorootparent> Our teams and products will merge From the article, not a rumor. reply xenospn 18 hours agoparentprevI’m not familiar with their business model - what value are you getting by paying for a messaging app? reply bluish29 18 hours agorootparentNot having to use multiple messaging apps. Some people would pay for this convenience. i.e They support several messaging apps so I don't have to tell peopl thag I don't use WhatsApp now. reply beeboobaa3 18 hours agorootparent> so I don't have to tell peopl thag I don't use WhatsApp now Because you actually are using WhatsApp? > Some people would pay for this convenience Some remember when this used to be widespread and free: https://www.pidgin.im/ reply ixwt 17 hours agorootparentYes, and you can get most of the Matrix bridges that Beeper uses and host the service yourself. Beeper is just a skin over Matrix, and they are hosting and managing the bridges for you. So that when any of protocols they support breaks because the other company changes something (or intentionally breaks things to stop connections from clients they don't want), Beeper fixes it rather than you having to wait for the open source community to do so and you implement that fix. Which who knows how long either scenario will take. Paying for Beeper is not having to manage the self hosting, which many are not willing to do. Because as with most open source solutions, it's not very simple and is intimidating for many. Not to mention having some box open to the internet, and having to maintain security updates for that box. I would love for libpurple to come back and make these things relatively simple. But the reality is that companies have walled off their gardens to control the experience, and many other reasons ranging from greed to security to spam mitigation. reply jamesmunns 18 hours agorootparentprevYeah, having all of your messaging in one place is amazing. Especially since they support a decent long tail of DM services as well. reply xenospn 16 hours agorootparentI use iMessage, messenger/IG, WhatsApp, Snapchat and slack. Probably not much value for my specific use case. reply boomskats 16 hours agoprevThe hopeless romantic in me is hoping the real reason Automattic acquired them was actually to get some useful developer documentation written for the Beepy. reply iLoveOncall 15 hours agoparentGiven how bad the WordPress documentation is, it's a weird hope to have from that company. reply freedomben 13 hours agorootparentInteresting, I think the WordPress docs are pretty decent. There's always room for improvement, but I love how it links and shows you the source code. They could definitely expand documentation coverage for plugins and stuff though, that's a little lacking. reply max_ 13 hours agoprevWhat's the acquisition amount? reply fareesh 17 hours agoprevTheir mini tech was built on weird hacks - is all their tech the same or is there something legit powering it? reply ghostly_s 17 hours agoparentWell if you consider private APIs/impersonating 1st party clients weird hacks then yes, none of these services have official 3rd party client support. reply gtirloni 16 hours agorootparentI was thinking about this after seeing the announcement. It's fine for open integrations like Matrix/IRC but all other are with proprietary services provided by for-profit companies.. that usually doesn't go well in the long run (e.g. Twitter clients). It seems to be a really high risk investment. I wonder how they plan to pull this off if/when they start to grow and attract more attention. reply drkleiner 18 hours agoprevGood for both products, Beeper and Texts, but seems like there will be layoffs coming reply syntaxing 17 hours agoprevKinda sad. Beeper owns a ton of Matrix bridges. I wonder what will happen now. reply luuurker 14 hours agoparentMatt/Automattic is good at keeping things open source. On top of the work on WordPress and their plug-ins, when they acquired PocketCasts, they made the apps open source: https://blog.pocketcasts.com/2022/10/19/pocket-casts-mobile-... reply danpalmer 17 hours agoparentprevSounds like they're planning to still run the Beeper service, perhaps merged with Texts. I'd bet they'll still use Matrix, it seems to be the very core of how Beeper works. reply Andrex 15 hours agoprevI trust Automattic implicitly. This seems like a good move. reply markx2 13 hours agoparentOh, you really should not. - Never trust ANY money-making company IMPLICILTY. - Go to https://apple.wordpress.com See the cookie banner Click View Partners in the bottom left Browse that list, check the details Find the company that Matt has sold your data to for over 11 years. The data Matt sells is for every user and every visitor not just on wordpress.com (who clicks Agree, but we know most people do) Are you still happy? reply photomatt 9 hours agorootparentI dislike that third-party advertising network stuff too, hopefully we can get rid of it soon and replace with Blaze, no calls to third-party ad networks. reply joelhaasnoot 16 hours agoprevI just hope they don't kill Beepberry/beepy reply imwillofficial 13 hours agoprevI don't see the win for Automattic here. Beeper seems to be a dead product with its main use case gutted. Are there other capabilities that I'm missing? Either way, congrats to the team! reply throwaway6e8f 13 hours agoprevThis is an odd match, and at a time when the strategic value of Beeper seems to be very low. > Matt, Automattic’s CEO, and I have known each other for years Could it perhaps be a favor for a friend to give a plausible reason for ejecting from the business without admitting failure? reply gkoberger 12 hours agoparentSure, but Automattic also bought Texts.com... this seems to be part of a bigger strategy on their part. Even friends don't spend $150M on friends for no reason. Automattic is doing well, but they don't have $150M to drop (plus future salaries, hosting, etc) just to help someone save face. reply user_7832 18 hours agoprevHonestly given how dim of a view Apple takes of others tearing down their artificial walls, having the (legal) backing of a large company might be a good thing... assuming they don't shut it down of course. reply misnome 18 hours agoparentBeeper does plenty besides PR stunts (which seems to have paid off) reply user_7832 18 hours agorootparentYou mean the other chat apps, right? Or do they do something other than chat as well? In any case I'm not sure if I'd call their iMessage thing a PR stunt (okay, the tweets were likely PR), I'm just happy they support as many standards as they can. (I'm also not sure if my original comment was seen as wrong or controversial judging by the downvote) reply starik36 15 hours agoprevI am using Beeper desktop app on Windows for the express purpose of being an iMessage client. Their press release (https://blog.beeper.com/2024/04/09/beeper-is-now-available/) says \"Beeper does not currently support iMessage\". Does that refer to the Android client? I hope they continue to support iMessage on the desktop. reply languagehacker 16 hours agoprevI've said this before, but Automattic's catalogue of products reads like an elephant graveyard of past internet fads. I view an acquisition like this as a relative death knell, but only insofar as we have seen unified messaging attempted time and again only to fail due to combinations of social pressures and anticompetitive technical choices. Beeper will be absorbed into another also-ran messaging platform that won't be able to compete with whatever communities already have critical mass. However, as far as exits go, it sort of seems like the best possible outcome for the folks at Beeper, so congrats. reply rwbt 16 hours agoparentThat's unfair to Automattic I think. I think they're really good to maintaining the products with less relative harm. Yes, I know the recent change in terms to feed their data to AI (you can pay them to avoid it). I'm still hoping they can revamp Tumblr but atleast they keep the lights on. reply chiefalchemist 10 hours agorootparentI think what they're saying is, Automattic buys products that are on the way out, not on the way up. So while Automattic might be great stewarts of has-beens, is that a good fit for Beeper, a company in a market that demands up and comer-ness? reply gnicholas 16 hours agoparentprev> elephant graveyard of past internet fads. This makes it sound like they acquired Evernote, which perhaps wouldn't have been the worst thing ever. reply gnicholas 16 hours agoprevI find it interesting that so many people want to unify all their messages. For me, I like having different apps that I can check with different frequency. I am glad that I can get my iMessages immediately but that LinkedIn messages don't get delivered to me until I open the app/website. I have literally never had an issue where I didn't get a message in time because it was sent via the wrong platform. People who need to reach me urgently know how to do so. reply kibwen 16 hours agoparentThat's a perfectly valid workflow, but it would certainly be nice for people who do want a unified workflow to be able to have that option. reply gnicholas 16 hours agorootparentFor sure. I just wonder who these people are and how large the market is, since none of my friends/family are in it. It's possible this is much more popular on Android, or outside the US (perhaps because they use WhatsApp more than iPhone-using Americans). But if this is the case, I wonder how much they will be able to charge, since Android users are less monetizable than iPhone users, and the US has a good chunk of the highly-monetizable mobile audience. For me, my unified workflow is my phone, which I have tuned to receive notifications from various platforms in various different ways (push, pull, manual). Even if I could tune Beeper to do the same, I would prefer not to have to duplicate my effort in setting preferences, which I've already set at the OS level. reply DaiPlusPlus 16 hours agorootparentprevSuch an app/service would not work towards the interests of message recipients - but the message senders. Those interests are not aligned. reply mtlynch 16 hours agorootparentWhy are they not aligned? If I send someone a text and they receive it in their email inbox because that's their preference, what difference does it make to me as the sender? reply DaiPlusPlus 16 hours agorootparentWhen you send an SMS text message, there's a understanding (and therefore, an expectation) that it goes directly to the recipient's phone/smartwatch and that they can reply immediately; i.e.: it's a system for high-priority (and therefore highly-intrusive) communications - this is why people generally don't give-out their mobile-phone number to strangers. Unless you go to the effort to tell people that you route your SMS messages to e-mail and therefore reply in-your-own-time (hours/days/weeks delay), the people trying to contact you aren't going to expect that: they're going to expect you to reply much sooner. I'd be out of a job if my boss' panicked SMS messages about how our prod website is down went to email instead of my phone. ------------- Any kind of \"universal\" messaging platform that anyone can use to send anyone a message needs to allow recipients to set how maximially intrusive those messages are - but senders might also want a way to set how minimally intrusive those messages are. Those requirements cannot be easily reconciled in a way that protects privacy and prevents abuse while also allowing anonymous and unsolicited messages or senders. So far the \"best\" way to do that today is by segregating senders (not recipients) by system (e.g. private SMS for high-intrusive; less-private-but-guarded e-mail for low-intrusive; public social-media, etc). Consider services that tried to work-around that, such as LinkedIn's paid messaging feature - which didn't exactly go very well. reply ryukoposting 16 hours agoparentprevI like having apps I can check with different frequencies. What I don't like is when I have two apps that I have to check regularly. Thankfully, almost everyone I talk to regularly uses Signal, but there's those few stragglers who stubbornly cling to SMS. reply EduardoBautista 16 hours agoparentprevI agree. I even like to silence notifications on different messaging apps depending on my focus mode on iOS/macOS. reply mckn1ght 16 hours agorootparentiOS/Messages is almost here. You can mute certain conversations, and set certain contacts as MVPs. But it has a few gaps w.r.t. focus modes. reply DaiPlusPlus 16 hours agorootparentYou can't receive messages via iMessage without disclosing your phone-number or Apple ID e-mail address to the sender. Furthermore, doing anything that changes your phone-number (e.g. using a local SIM abroad instead of roaming) invalidates your phone-number with iMessage, but because most people use iMessage via phone-numbers it means you're forced to use roaming ($$$) when travelling, and so on and so on. So long as iMessage is sold as \"better SMS\" then that's fine, as it inherits SMS's limitations (above) - but it isn't a portable, platform-agnostic, geography-neutral, messaging platform, and I'd rather people didn't try to use it as such. reply 0xblinq 16 hours agoparentprevAnd what prevents these aggregator apps from having different settings for each network? reply gnicholas 16 hours agorootparentI imagine this is possible, but as someone who has tuned these settings in the OS-level notification preferences, why would I want to do it all over again within an aggregator app? reply jokethrowaway 16 hours agoparentprevThe problem arise when your friends can text you on Discord, WhatsApp, Telegram, Signal, Viber, Facebook Messenger, Linkedin, Instagram, Twitter, Bsky, Mastodon, Skype, Slack, Email. It's not that you don't receive messages but that you need to keep installed 14 different applications on your phone or get used to 14 different UIs to do the same thing. The major problem is the WhatsApp vs Telegram split for me, if all my friends were on Telegram I wouldn't really need it. Self hosted Matrix sounds like a good solution and I'm happy with managing my own server - but the UI quality (Elements) is just not on par with Telegram, so I'm still there. I agree with you, it's nice to relegate spam to Linkedin / Email, but that can be achieved with marking contacts as friends vs random people and having a unified messaging platform. reply brandon272 14 hours agorootparentYes, this is exactly it. And in an age where people are terminally on their phones, having 14 different apps that can all launch notifications from the background as long as they are installed and you are logged in is perhaps fine. On a Mac or PC in particular it would be nice to have other options around aggregating these messages in a single interface as we used to be able to do 20 years ago. reply gnicholas 13 hours agorootparentYeah, I can understand the appeal a bit more on a computer. On my phone, it's a problem that's already been solved at the OS level. reply alberth 16 hours agoprevPidgin Reminds me of why I used https://pidgin.im 20-years ago. It was an aggregator chat app. reply rw_grim 11 hours agoparentWe're still here and still supporting tons of stuff... https://pidgin.im/plugins/?publisher=all&query=&type=Protoco... reply xinayder 15 hours agoprevDo you plan on continuing contributing to open source, or is Automattic going to dictate your business plan and Beeper will be enshittified? reply dang 12 hours agoparentWe detached this subthread from https://news.ycombinator.com/item?id=39981276. reply jtbayly 15 hours agoparentprev“We plan to be enshittified,” said nobody, ever. reply xinayder 15 hours agorootparentThat's a pessimistic take to say that Automattic will close up Beeper and make it a worse a product. reply bobthepanda 14 hours agorootparentI think it's more of a criticism of asking the question as if that was ever going to be given as an answer, regardless of what the actual answer is. in what scenario do you see anyone giving the negative answer truthfully? reply wpietri 13 hours agorootparentI think the flavor of the answer says a lot. A vague negative answer will make me suspicious, as will one based in good intentions and moonbeams. But a negative answer rooted in principle and mentioning specific choices is one I'll take more seriously. Enshittification is the default path for leaders with MBA brain, but some still manage to avoid it. reply compootr 14 hours agorootparentprevthat's a naive take to think there's no chance they will every. company. wants. money. it's the SAME, EVERY.SINGLE.TIME reply askonomm 13 hours agorootparentThat's indeed what a purpose of companies are. Organizations that do not want money are registered as non-profit. reply cozzyd 14 hours agorootparentprevLaxative manufacturer? reply 2 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Beeper has been acquired by Automattic, known for backing WordPress and WooCommerce, signaling a new era in their quest for the optimal chat app.",
      "The merger will maintain Beeper's independence, with a focus on preserving user privacy through encryption and offering the service to all without waitlists.",
      "Texts.com users can expect no abrupt alterations; the teams and products will gradually integrate while upholding their dedication to local data processing."
    ],
    "commentSummary": [
      "Automattic acquires Beeper and Texts.com to improve messaging services, leading to discussions on Beeper's open-source future and integration with other platforms.",
      "Users show a mix of optimism and skepticism about the acquisition, citing worries about data security, app stability, pricing alterations, and feature changes.",
      "Discourse delves into unified messaging platforms, privacy challenges, and managing multiple messaging apps efficiently."
    ],
    "points": 511,
    "commentCount": 257,
    "retryCount": 0,
    "time": 1712675365
  },
  {
    "id": 39983233,
    "title": "Social Media's Impact on Teen Mental Health",
    "originLink": "https://www.afterbabel.com/p/phone-based-childhood-cause-epidemic",
    "originBody": "Share this post Yes, Social Media Really Is a Cause of the Epidemic of Teenage Mental Illness www.afterbabel.com Copy link Facebook Email Note Other Discover more from After Babel Using moral psychology to explain why so much is going wrong Over 66,000 subscribers Subscribe Continue reading Sign in Yes, Social Media Really Is a Cause of the Epidemic of Teenage Mental Illness Two major problems with a review in Nature Jon Haidt Apr 09, 2024 230 Share this post Yes, Social Media Really Is a Cause of the Epidemic of Teenage Mental Illness www.afterbabel.com Copy link Facebook Email Note Other 61 Share For centuries, adults have worried about whatever “kids these days” are doing. From novels in the 18th century to the bicycle in the 19th and through comic books, rock and roll, marijuana, and violent video games in the 20th century, there are always those who ring alarms, and there are always those who are skeptics of those alarms. So far, the skeptics have been right more often than not, and when they are right, they earn the right to call the alarm ringers “alarmists” who have fomented a groundless moral panic, usually through sensational but rare (or non-existent) horror stories trumpeted by irresponsible media. But the skeptics are not always right. I think it is a very good thing that alarms were rung about teen smoking, teen pregnancy, drunk driving, and the exposure of children to sex and violence on TV. The lesson of The Boy Who Cried Wolf is not that after two false alarms we should disconnect the alarm system. In that story, the wolf does eventually come. The question before us now, on the topic of teens and social media, is this: Are the skeptics correct that we are going through just one more groundless moral panic over teens and tech in which adults are freaking out while, in fact, the harms are so minimal that they shouldn’t be a cause for worry? Or did the wolf really arrive around 2012, and has been mauling young people ever since via their smartphones and social media accounts? (Of course, there are researchers who reside in the space between these two perspectives.) Psychologist Candice Odgers has taken the skeptical side for many years now (along with researchers including Amy Orben, Andrew Przybylski, Jeff Hancock, Chris Ferguson, and Aaron Brown), while Jean Twenge and I have been writing as alarm ringers. It has been a normal and productive academic debate. Engagement with each other’s arguments is how science makes progress. Even if we never convince each other, the broader scientific and policy communities tune in to the debate, and eventually, they’ll move one way or the other. Odgers recently stated the skeptics' case in an essay in Nature titled The Great Rewiring: Is Social Media Really Behind an Epidemic of Teenage Mental Illness? The essay offered a critique of my recent book, The Anxious Generation. Odgers’ primary criticism is that I have mistaken correlation for causation and that “there is no evidence that using these platforms is rewiring children’s brains or driving an epidemic of mental illness.” She also warns that my ringing of a false alarm “might distract us from effectively responding to the real causes of the current mental-health crisis in young people,” which, she suggests, are social ills such as racism, economic hardship, and the lingering impact of the 2008 Global Financial Crisis and its disparate impact on children in low SES families. In this response essay, I’ll present the two main problems I see with the skeptics' approach, as exemplified in Odgers’ review: Odgers is wrong to say that I have no evidence of causation Odgers’ alternative explanation does not fit the available facts. Share 1) Odgers is wrong to say that I have no evidence of causation Odgers’ central claim is that I have mistaken correlation for causation and that I have “no evidence” that social media is a cause, rather than a mere correlate, of the current epidemic of adolescent mental illness. Odgers says that I am just “making up stories by simply looking at trend lines.” In 2018, when I entered this debate as a co-author of The Coddling of the American Mind, it was true that the great majority of published studies on “digital media” and mental health were correlational, and every social scientist knows that correlational studies often suggest to our intuitive minds a causal pathway that vanishes when experiments using random assignment are published. (Think of the changing guidance on the health effects of fat, carbs, and red wine). But even in 2018 there were a few experimental studies on social media and mental health. For example, college students who were asked to reduce their social media use for three weeks generally experienced mental health benefits compared to the control group. Zach Rausch, Jean Twenge, and I began to collect all the studies we could find in 2019, and we organized them by type: correlational, longitudinal, and experimental. We put all of our work online in Google Docs that are open to other researchers for comment and critique. You can find all of our “collaborative review” documents at AnxiousGeneration.com/reviews The main document that collects studies on social media is here: Social Media and Mental Health: A Collaborative Review In that document, we list dozens of correlational and longitudinal studies. These large datasets provide us with correlation coefficients that tell us how variables in the dataset are related to each other, and they tell us when and for whom those relationships are stronger. Those studies reveal a fairly consistent relationship in which heavy users of social media are at much higher risk of mental illness or poor mental health than everyone else. A widely cited 2018 study of 14-year-olds found that girls who spend five hours or more on social media per day are three times as likely to be depressed as girls who use social media only a little or not at all. For boys, the ratio is lower, closer to two-to-one. A meta-analysis of 26 such studies found that the risk of depression increased by 13% for each hour increase on social media for adolescents (and that increase was even higher for girls).1 In that document, we also list 22 experimental studies, 16 of which found significant evidence of harm (or of benefits from getting off of social media for long enough to get past withdrawal symptoms). To give just a few examples: Alcott and colleagues (2020) randomly assigned 2743 adults to either deactivate their Facebook accounts for one month or not. This study also found that deactivation significantly improved subjective well-being and that “80% of the treatment group agreed that deactivation was good for them.” The treatment group was also more likely to report using Facebook less and having uninstalled the app from their phones post-experiment. Brailovskaia and colleagues (2022) have done one of the only studies I have seen that incorporate both social media reduction and physical activity increases. They randomly assigned 642 participants to (1) reduce social media use to 30 minutes a day for two weeks, (2) increase physical activity by 30 minutes a day for two weeks, (3) follow both instructions, or (4) do nothing. The researchers found the strongest effects within the combined condition (#3). This group reported the largest decreases in depressive symptoms and increases in life satisfaction and subjective happiness compared to other groups. There are also a number of experiments that have looked at Instagram's unique negative impacts on women, including the finding that it is more harmful to women than is Facebook. In that document, we also list nine quasi-experiments or natural experiments (as when high-speed internet arrives in different parts of a country at different times), eight of which found evidence of harm to mental health, especially for girls and women. (Odgers cited only the 9th one, which relies on a dataset that is made up of unreliable estimates that do not detect known rises in mental illness, as Zach showed here.) To give just one example: Arenas-Arroyo and colleagues (2022) looked at the links between the staggered rollout of broadband internet in Spain between 2007-2019 and hospital discharge diagnoses of behavioral and mental health cases of adolescents. They found a significant effect of the arrival of high-speed internet, but only among adolescent girls. There are also a number of studies showing mental health improvements, increases in physical activity, and reductions in bullying when schools go phone-free—one of the few natural experiments that specifically targets both adolescents and group-level effects. (These group-level effects are illustrated brilliantly here). I am not saying that academic debates are settled by counting up the number of studies on each side, but bringing so many studies together in one place gives us an overview of the available evidence, and that overview supports three points about problems with the skeptics’ arguments. First, if the skeptics were right and the null hypothesis were true (i.e., social media does not cause harm to teen mental health), then the published studies would just reflect random noise2 and Type I errors (believing something that is false). In that case, we’d see experimental studies producing a wide range of findings, including many that showed benefits to mental health from using social media (or that showed harm to those who go off of social media for a few weeks). Yet there are hardly any such experimental findings. Most experiments find evidence of negative effects; some find no evidence of such effects, and very few show benefits. Also, if the null hypothesis were true, then we’d find some studies where the effects were larger for boys and some that found larger effects for girls. Yet that’s not what we find. When a sex difference is reported, it almost always shows more harm to girls and women. There is a clear and consistent signal running through the experimental studies (as well as the correlational studies), a signal that is not consistent with the null hypothesis. Second, and most germane to Odgers’ review: She and the other skeptics are free to critique the hundreds of studies Zach and I cite in our essays at After Babel and in chapters 1, 5, 6, and 7 of The Anxious Generation. They can certainly say that they are not persuaded and that they will not be persuaded until the perfect experiment is done. But they cannot say that I am drawing only on correlational studies, or that I have “no evidence” of causation, or that I do not understand the difference between correlation and causation. I laid out the evidence for causality (not just correlation) between social media use and mental health outcomes for girls and walked the reader through the Google Doc and multiple kinds of evidence in this post in early 2023: Social Media is a Major Cause of the Mental Illness Epidemic in Teen Girls. Here’s the Evidence. I presented several conceptual problems with the skeptics’ claims about causality and evidence in this essay: Why Some Researchers Think I’m Wrong About Social Media and Mental Illness. For example, I noted that the skeptics focus on testing one narrow model of causality that treats social media consumption as if it were an individual act, like consuming sugar, and then looks for the size of the dose-response relationship in individuals. But much of my book is about the collective action traps that entire communities of adolescents fall into when they move their social lives onto these platforms, such that it becomes costly to abstain. It is at that point that collective mental health declines most sharply, and the individuals who try to quit find that they are socially isolated. The skeptics do not consider the ways that these network or group-level effects may obscure individual-level effects, and may be much larger than the individual-level effects. Third, even beyond the published experiments, there is ample evidence of causation that should be relevant to parents, lawyers, and legislators: eyewitness testimony. When you ask members of Gen Z what they think is causing their high rates of mental illness, they often point to social media and particularly Instagram as a major cause.3 (See section 4 of our main collaborative review doc.) I have been searching for essays by members of Gen Z that defend social media and the phone-based childhood as being good for mental health. I can’t find them, but I find plenty of members of Gen Z who say that social media damaged them or their generation. Also relevant: many members of Gen Z are starting organizations to fight back. Meta collected some eyewitness testimony accidentally. In that famous internal study brought out by whistleblower Frances Haugen, the researchers found that Instagram is particularly bad for girls. They wrote: “Teens blame Instagram for increases in the rate of anxiety and depression. . . . This reaction was unprompted and consistent across all groups.” Quantitative researchers such as Odgers are free to place less weight on qualitative studies, but they are a kind of evidence in social science research. They are relevant when we try to sort out multiple theories about causation, especially when the platforms will not share data with scientists so the experts with the deepest insights into what social media is doing to teens are the teens themselves. They see it happening. Do they count as “no evidence” since their claims are not peer-reviewed? Subscribe 2) An Alternative That Does Not Work The second major problem with Odgers’ review is that she proposes an alternative to my “great rewiring” theory that does not fit the known facts. Odgers claims that the “real causes” of the crisis, from which my book “might distract us from effectively responding,” are longstanding social ills such as “structural discrimination and racism, sexism and sexual abuse, the opioid epidemic, economic hardship and social isolation.” She proposes that the specific timing of the epidemic, beginning around 2012, might be linked to the 2008 Global Financial Crisis, which had lasting effects on “families in the bottom 20% of the income distribution,” who were “also growing up at the time of an opioid crisis, school shootings, and increasing unrest because of racial and sexual discrimination and violence.” I agree that those things are all bad for human development, but Odgers’ theory cannot explain why rates of anxiety and depression were generally flat in the 2000s and then suddenly shot upward roughly four years after the start of the Global Financial Crisis. Did life in America suddenly get that much worse during President Obama’s second term, as the economy was steadily improving? Her theory also cannot explain why adolescent mental health collapsed in similar ways around the same time in Canada, the UK, Australia, and New Zealand, as Zach and I showed in this post: The Teen Mental Illness Epidemic is International, Part 1: The Anglosphere Adolescent Self-harm Episodes in Five Anglo Nations Figure 1. Since 2010, rates of self-harm episodes have increased for adolescents in the Anglosphere countries, especially for girls. For data on all sources and larger versions of the graphs, see Rausch and Haidt (2023). (Data for Canada is limited to Ontario province, which contains nearly 40% of the population of Canada.) Nor can she explain why it also happened at roughly the same time in the Nordic countries, which lack most of the social pathologies on Odgers’ list, as Zach and I showed in this post: The Teen Mental Illness Epidemic is International, Part 2: The Nordic Nations Percent of Nordic Teens with High Psychological Distress (Ages 11-15) Figure 2. Percent of Nordic Teens with High Psychological Distress. Data from the Health Behavior in School-Age Children Survey (2002-2018). Graphs and data were organized, analyzed, and created by Thomas Potrebny and Zach Rausch. See 1.1.1 of Nordic Adolescent Mood Disorders since 2010. Nor can she explain why it also happened in much, though not all, of Western Europe: The Youth Mental Health Crisis is International Part 4: Europe Percent of European Teens with High Psychological Distress by Country-Level Individualism (Ages 11-15) Figure 3. Changes in psychological distress by high vs. low individualism of each country, split by sex. Source: Health Behavior in School-Age Children Survey, 2002-2018 (See Zach’s spreadsheet for data points) and Hofstede Insights.4 Nor can she explain why suicide rates for Gen Z girls (but not always boys) are at record levels across the Anglosphere, as we showed in this post: Suicide Rates Are Up for Gen Z Across the Anglosphere, Especially for Girls Rises in Suicide Among Adolescent Girls (Ages 10-19) Figure 4. All datasets and figures can be found in the post. If Odgers was correct that the “real causes” of the epidemic are America’s social ills, then we would not find these patterns in so many countries. I just can’t see a causal path by which America’s school shootings, lockdown drills, poverty, or racism caused girls in Australia to suddenly start self-harming or dying by suicide at the same time as so many American girls. An equally large problem for Odgers’ explanation is that it commits her to the prediction that the increases in mental illness were largest for teens in low SES families. After all, her explanation for why there was a four-year delay between the onset of the GFC and the onset of the mental health crisis was because the effects lasted longer for those “in the bottom 20% of the income distribution,” who “continue to experience harm.” Jean Twenge tested Odgers’ explanation by looking to see whether rates of major depressive episodes increased faster for teens in families below the poverty line (shown in red in Figure 5 below) versus those whose families’ incomes were at least double the poverty line (shown in blue). As you can see, there was no difference between the two groups up through 2012 (which is contrary to Odgers’ thesis about the differential impacts of the GFC on mental health), and then a difference opened up after 2012, but in the opposite direction of Odgers’ prediction. Percent of American Adolescents with Depression by Family Income Level (Ages 12-17) Figure 5. Percent of U.S. 12- to 17-year-olds experiencing major depressive episodes in the last 12 months, by family income level. Source: Data from the nationally representative National Survey of Drug Use and Health; analysis by Jean M. Twenge for the Generation Tech Substack.5 Twenge has also addressed 13 other possible explanations for the mental health crisis and shows why they don’t fit the data. In short: Odgers has pointed to an alternative causal explanation that A) does not fit the timing in the U.S., B) does not fit the social class data in the U.S., and C) does not fit the international scope of the crisis. Share What Now? Parents, teachers, and legislators cannot wait any longer; they want to do something about the ever-rising levels of anxiety, distraction, and suffering. Whose policy prescriptions should they follow, given the current state of the evidence and the relative risks and costs associated with each path? If leaders and change-makers were to embrace Odgers’ causal theory about the “real causes” rather than being “distracted” by mine, then the way forward is to first solve society’s biggest social problems, most of which we have been working on for decades. Perhaps this time we’ll make more progress, and in ten or twenty years, rates of teen mental illness will begin to decline. And if Odger’s causal theory turns out to be wrong? We’ll have spent another decade or two locked in the usual culture war battles over spending on social programs that may or may not be effective, and we will have lost another generation to mental illness. In contrast, if leaders and change makers were to embrace my account of the “great rewiring of childhood,” in which the phone-based childhood replaced the play-based childhood, what policy implications follow? That we should roll back the phone-based childhood, especially in elementary school and middle school because of the vital importance of protecting kids during early puberty. More specifically, we’d try to implement these four norms as widely as possible: No smartphones before high school (as a norm, not a law; parents can just give younger kids flip phones, basic phones, or phone watches). No social media before 16 (as a norm, but one that would be much more effective if supported by laws such as the proposed update to COPPA, the Kids Online Safety Act, state-level age-appropriate design codes, and new social media bills like the bipartisan Protecting Kids on Social Media Act, or like the state level bills passed in Utah last year and in Florida last month). Phone-free schools (use phone lockers or Yondr pouches for the whole school day, so that students can pay attention to their teachers and to each other) More independence, free play, and responsibility in the real world. Note that these four reforms, taken together, cost almost nothing, have strong bipartisan support, and can be implemented all right now, this year, if we agree to act collectively. And what if it turns out that I am wrong? What if, in reality, the multinational collapse of adolescent mental health in the early 2010s was not caused by the arrival of phone-based childhood; it was just a big coincidence. Will kids be damaged by these four norms? I don’t think so. What irreversible harm will be done to children who spend more time listening to their teachers during class, more time playing and exploring together outdoors, and less time sitting alone hunched over a device? We are now 12 years into a public health emergency that began around 2012. In The Anxious Generation, I offer a detailed explanation of what caused it (drawing on many academic fields) and a detailed path by which we can reverse it. I know of no plausible alternative explanation, nor have I found anyone offering a realistic alternative pathway out. We certainly need skeptics to challenge alarm-ringers, who sometimes do ring false alarms. God bless the skeptics. But at a certain point, we need to take action based on the most plausible theory, even if we can’t be 100% certain that we have the correct causal theory. I think that point is now. Subscribe 1 Odgers claims that the correlation is actually reverse correlation: depression causes teens to spend more time on social media. It’s true that some longitudinal studies have found that an increase in depression precedes an increase in time on social media, but other studies have found the reverse, and some have found that the relationship is bidirectional. 2 Yes, there could be a “file drawer problem” if researchers on one side are systematically discouraged from publishing, so the missing “positive” studies are all sitting in file drawers in researchers’ offices. But because findings of benefits would be unusual and newsworthy, I don’t believe that there is a strong or consistent bias against the skeptics. 3 I focus here on eyewitness testimony from members of Gen Z, but there is ample eyewitness testimony from teachers, and from parents, who see the effects of heavy phone use on classroom behavior, learning, and family life. 4 High individualistic countries include United Kingdom, Hungary, Netherlands, Italy, Belgium, Denmark, France, Sweden, Ireland, Latvia, and Norway. Low individualistic countries include Austria, Slovakia, Spain, Russia, Greece, Croatia, Bulgaria, Romania, Slovenia, Portugal, Ukraine, and North Macedonia. Measure for distress: We used a measure of high psychological distress from the HBSC that includes four items that tap different symptoms of psychological distress: feeling low, feeling nervous, feeling irritable, and having sleep difficulties. Respondents rated how often they felt these four symptoms over the last six months (1: Every day, 2: More than once/week; 3, About every week; 4, Every month; 5, Rarely or never). We operationalized high psychological distress as having three or more of the four psychological ailments every day or more than once a week over the last six months. Using this measure, we computed the average psychological distress score in each country, for girls and for boys separately, in each of the 5 survey years. This gave us two new datasets (one for girls, one for boys), each with 33 rows (one for each country), which facilitated the various cross-country comparisons that we report. 5 Note: This is a screening study of the population, not a study of diagnoses or treatment. Thus the differences cannot be caused by willingness or ability to seek treatment. 2020 data was excluded as sampling times varied from other years. Subscribe to After Babel Thousands of paid subscribers Using moral psychology to explain why so much is going wrong Subscribe Error 230 Share this post Yes, Social Media Really Is a Cause of the Epidemic of Teenage Mental Illness www.afterbabel.com Copy link Facebook Email Note Other 61 Share Previous",
    "commentLink": "https://news.ycombinator.com/item?id=39983233",
    "commentBody": "Yes, social media is a cause of the epidemic of teenage mental illness (afterbabel.com)430 points by throwup238 14 hours agohidepastfavorite438 comments skilled 1 hour agoI'm no saint when it comes to addiction and being on my phone for more than I should be, but I have a sister who is in her mid 20s right now, and for the last couple of years she has slowly isolated herself from life and her family, she spends most of her time in her room on the phone and does weird things like get cosmetic surgeries, ordering cosmetics, etc. It's bizarre. She does this and all the while never leaves the house, other than to go to work. She doesn't share her actual thoughts and gets angry when asked about it. You might be reading this and thinking that there's more to it, but sadly there isn't. It's her life so I leave her alone, not my place to tell her what to do, and the emotional upheaval from her isn't worth it either. But it's crazy that a person can get this lost in life and become completely devoid of purpose and meaning. It's one thing to have an issue and work through it slowly, but it's something else to isolate yourself and live your life through others - while those \"others\" prosper from your own ignorance. I'm sure her past experiences are playing a role in this behavior, but the whole cosmetics things - I know for a fact there are a lot of influencers who peddle this crap, and if you lack self-awareness then I can see how easy it is to get stuck in this cycle. I just wish there was an easy way out of it. reply apexalpha 28 minutes agoparent>It's her life so I leave her alone, not my place to tell her what to do, and the emotional upheaval from her isn't worth it either. In my culture this is exactly what family should do. \"Soft doctors make stinking wounds\". reply worthless-trash 13 minutes agorootparent> and the emotional upheaval from her isn't worth it either. If you loved your sister, you should do something, if direct confrontation isnt working, take another route. > In my culture this is exactly what family should do. I assume you mean \"family should intervene here\", because if you don't who would ? reply raziel2p 22 minutes agoparentprevI think it's a bit reductionist to say it's devoid of purpose and meaning. Have you asked her what she feels her purpose/meaning is? You can disagree with it (I would as well), but I wouldn't assume she doesn't find any purpose/meaning in it. If she gets angry about it, then probably there is something deeper going on. Or, you're just asking the wrong way (if you come in assuming there's no meaning/purpose to her life, that could easily happen). If there's something deeper going on, then social media just amplifies things. Specifically about cosmetics and the beauty industry - people have complained about the effects of supermodels on TV, movies, billboards, magazine ads etc. for more than 30 years, social media has just taken it to the next level. I don't disagree with anything you said, but I do think it's important to add depth to the argument if we actually want to change the world for a better place. reply gambiting 31 minutes agoparentprev>>But it's crazy that a person can get this lost in life and become completely devoid of purpose and meaning. The thing is, I'm sure for her it doesn't feel that way. She probably feels a lot of purpose and meaning in following all these people online and participating in social activities around them. That's part of the addiction too - it can feel meaningful for a long time. reply emmelaich 29 minutes agoparentprevAs her brother you might be the only one capable of rescuing her from a shitlife. No (wo)man is an island, entire unto itself. reply searchableguy 33 minutes agoparentprev> I'm sure her past experiences are playing a role in this behavior, but the whole cosmetics things - I know for a fact there are a lot of influencers who peddle this crap, and if you lack self-awareness then I can see how easy it is to get stuck in this cycle. I just wish there was an easy way out of it. reply One way could be to change her feed to follow more \"positive influencers\". reply gardenhedge 13 minutes agoparentprevIt just sounds like you're not part of her life. It sounds like you live together. Imo, that's absolutely normal behaviour between siblings. reply skilled 2 minutes agorootparentWe don’t live together. But we have a rocky past for sure. I was older and so I got a head start during my teens for all the bad vices during that age; drugs, alcohol, excessive partying. By the time she got around to it, I had already wisened up and basically lost communication with her. I went on to do other things with my life and actually put in effort to experience new stuff. She on the other hand did not. She found more excuses (dropped out of uni twice) than solutions. reply yawboakye 6 minutes agoparentprevi find this to be, unfortunately, the current western stance on responsibility towards anyone: irresponsibility. for some reason, leaving people to go their way, wherever it may lead them, has been interpreted to be the good and the moral duty of anyone who understands what freedom (for intelligent beings) means. with all due respect, that’s cowardice and a neglect of duty. i explained to my sibling the other day that we cannot live our lives however we wanted because of our layers of responsibility: first to ourselves (as a human beings in this civilization), then to our family and friends (who expect to count on us at some point), to the society we inhabit (i am currently an immigrant), the human race, and lastly the environment. those are not easy responsibilities and they could crush anyone. but absconding isn’t a choice: we’re not here for selfish reasons. i think you should intervene, at whatever cost to you. because no one else would, and depending on how persistent you are, she’d be grateful for the care and attention you showed. because that’s the sort of action that pure love motivates. good luck reply rgbrenner 4 hours agoprevThe author had it right in the first paragraph. In the 90s version of this hysteria, Congress passed a law that would have prevented access to education medical information, dirty curse words, and other filth from being published on the internet to protect the children. The federal government fought a case all the way to the Supreme Court to enforce it. If they had won that case, the internet would look very different today. But the Supreme Court got it right when they said it would squelch free speech. You may not like FB, IG, TikTok, etc.. I certainly don't care for any of these products. But these are communications platforms. Restricting the right to free speech does have negative consequences... from the development of critical thinking skills; development of technical skills; and limiting of educational information. Being exposed to shit on the internet teaches you there's bullshit on the internet, and not to believe everything you see. And just like the Supreme Court wrote 30 years ago, the answer is the same today: if you don't like these products and feel they are negative, then don't use them. Restrict your children's access to these platforms. I certainly dont believe anyone should be forced to use these platforms. I don't use any of these products, and havent since they launched. That's a freedom you and everyone else can take advantage of also. But those who advocate censorship aren't advocating for freedom... they're advocating for their personal parental decisions to the be decisions of the entire nation. reply raziel2p 2 hours agoparent> if you don't like these products and feel they are negative, then don't use them It's really not that simple. The products have become so widespread and influential that they change the very culture of our society for the worse. It doesn't matter whether you abstain from using Instagram or not, some of your friends will still be more or less subtly influenced by its existence in your social interactions. There's a nice quote from Marshall McLuhan's Understanding Media, which IMO hasn't aged at all in 60 years: \"Our conventional response to all media, namely that it is how they are used that counts, is the numb stance of the technological idiot. For the 'content' of a medium is like the juicy piece of meat carried by the burglar to distract the watchdog of the mind... The effects of technology do not occur at the level of opinions or concepts, but alter sense ratios or patterns of perception steadily and without any resistance.\" reply whstl 1 hour agorootparent> It doesn't matter whether you abstain from using Instagram or not Yep. The other problem is that not having social media and mobile devices can be alienating and ostracizing, especially for teenagers. Avoiding the problems of social media requires skills and restraint that even most adults don’t have. It is a “damned if you do, damned if you don’t” situation. reply myspy 1 hour agorootparentThat's true. We restrict access to Snapchat, TikTok, Instagram, FB, they can use WhatsApp, YT, iMessage, Phone and Pinterest. I'm fucking annoyed by other parents that don't set boundaries that way. I have so much discussions about other platforms. Pushing them to physically meet is hard too. We grew up at a time where SMS was a thing when I became 16. I know that keeping up is cool, but social media is a disease. The amount of dumb and uneducated people that couldn't even listen to expert advice during a fucking pandemic is driving me up the wall. I'm annoyed mainly because people around me make bad decisions that have an influence on my own life. reply nicolas_t 41 minutes agorootparentOne of the criteria I used when choosing my son's school is that mobile phones are not allowed at all in school. It's a primary school (until 12 years old) so you wouldn't think that mobile phones would be that common at that age but from what I've heard of other parents, smart phones are common already this early. I don't believe in completely forbidding access to everything when my son is older but there's a time to introducing things like this and it's not this young. reply searchableguy 43 minutes agorootparentprevI installed Instagram last year due to friends and it was a revelation on all the trends and behavior I noticed everywhere. The music people play, the decorations in the cafe, jokes, haircuts, dressing styles, and recommendations given to me all suddenly clicked. I watched one of my friend slowly descend into madness and she became a healing coach and peddles her services on the same platform. reply tossandthrow 1 hour agorootparentprev> Avoiding the problems of social media requires skills and restraint that even most adults don’t have. The parent commenter propose that you can not escape the negative consequences of social media, even if you fully restrain yourself, as you have the second order effects from your friends using it. The parent commenter says that this is not an individual problem and cannot be solved on an individual level. It is a macro issue. Just like you stopping eating meat will not really make a dent on global warming. I think that is very insightful. reply signaru 1 hour agorootparentprevAnd then there are also those companies/institutions/orgs/news/shops that make you left behind from otherwise useful information or services by not being on social media platforms. reply pjerem 1 hour agorootparentThis is a real everyday pain that anyone with social media accounts can’t fathom. Most (like 90%) businesses don’t have a website or when they have, it’s a static website. All the news are on social medias behind nag screens. reply davedx 26 minutes agorootparentprevIt really annoys me how many services - even sometimes public services like our local police - are Facebook first, with all other mediums as an afterthought. reply wepple 8 minutes agorootparentI’d aggressively support legislation against this. Luckily with Elon doing crazy things to X, NYC has divorced a bunch of public services from it. But not enough reply zer00eyz 1 hour agorootparentprevAnd then there's Therapist who keep saying everyone should be in therapy and drug companies with the latest in psychopharmacology. Histeria (and the virbrator), ice pick lobotomies, electro shock therapy, Quaalude, Benzodiazepines (valium) as mothers little helpers, MAOI's, SRRI's (PSSD), Benzodiazepines again (Xanax, klonipin). No one I've every asked about this cares about Android vs iPhone. No one. They wouldn't necessarily tell if they did though... This is more like \"I asked 100 men if they worry their D is small, and nobody said yes\". reply satvikpendem 27 minutes agorootparentprevI think you missed the entire point of the comment above. It's not about iOS vs Android but about not being included socially, regardless of the platform. Imagine in your example where one kid in a group didn't use Discord, they'd be effectively cut off. reply octopusRex 43 minutes agorootparentprevI remember when they taught mcLuhan in highschool - 1970s - in order to make us more aware of media manipulation. I wonder if they still do. Ironically, this was in Florida - probably be considered too woke by Desantis. reply epolanski 2 hours agorootparentprevI don't understand the point you're trying to make. I for one deleted all of those platforms and my life is definitely better (I'm way less distracted for once, but the list is longer). What do I care what others do? reply kredd 1 hour agorootparentIf you have children or niblings, you’ll realize how ostracized from the community one gets if they have no access to social media. It would work if parents banded together and nobody in school would get access, but otherwise it just becomes a “there’s a giant club and you’re not a part of it”. As a child, that really hurts. When I was in high school, Facebook was just opening up access to everyone. I remember literally everyone getting on it, and swapping between messages in MSN, and some posts on FB. We had whole school wide conversations, planning and etc. on social media, and on days when I didn’t get in front of the computer to check things, I remember feeling left out. It’s just so much worse now, because every kid expects every other kid to be on the loop of things 24/7. I think that’s where we screwed up. Having just casual access without mobile phones was the sweet spot, in my opinion. reply rTX5CMRXIfFG 1 hour agorootparentBut why do you make it sound like it’s not an option to teach kids to not have fear of missing out and having meaningful ways of spending their time in the real, offline world? I guarantee you that there is nothing of value that other teens are saying that your own children need to be updated 24/7 reply raziel2p 1 hour agorootparentYou seem to be ignoring the fact that people plan things in real life over the internet. Conversations started online continue IRL, and vice-versa. Social media can be really bad, and maybe it's actually mostly bad, but it does facilitate real life interactions and good things. Imagine your child going to school but not being allowed to play with other kids in recess. Lots of bullying and physical violence can happen during recess, after all. Would you use that as an opportunity to teach them the dangers of FOMO, and to just read a book? reply rTX5CMRXIfFG 58 minutes agorootparentIt’s the perfect opportunity to teach them the lack of value of FOMO, especially when it’s validation from people who would bully you that you seek. I don’t see why what you’re recommending is a healthy response. reply coldtea 16 minutes agorootparent>It’s the perfect opportunity to teach them the lack of value of FOMO Ever tried teaching anything to teenagers where their peers and society in general promote the opposite? Good luck with that... reply gommm 1 hour agorootparentprevI also guarantee you that a teen who is not in the loop is much more likely to be ostracized and bullied in the offline world. So, yes you might try teaching your kid that they're not missing out that they can find more meaningful way to spend their time in the real offline world but, the fact is, they go to schools with kid who overwhelmingly are not taught that and who will dislike your kid for being different. Your kid doesn't live in isolation, he lives in society and, during school years is when social pressure to conform to the group norm is strongest. I'm speaking from experience here, I've been bullied as a teen, I definitely would rather avoid my son going through a similar experience. reply rTX5CMRXIfFG 1 hour agorootparentEh, that’s not necessarily true. Other kids might bully your child for having a healthy set of offline interests and for not being like them who are all plugged in online, but I don’t see how it’s not an option to teach your kids to have a strong sense of identity and not give in to peer pressure while also assuring them that you’ve always got their back. What you’re describing doesn’t sound like parenting to me, it’s giving in to peer pressure. From kids. And you’re supposed to be an adult who already knows what’s right and wrong. If your kid’s peers all gain a liking for drugs or gambling or some other vice and they bully your child for not partaking, are you going to tell your child to participate? No, what you should do is show them the right way and to what’s good for them in the long term, even if it’s difficult for them to see it now because of their youth. reply michaelt 8 minutes agorootparent> What you’re describing doesn’t sound like parenting to me, it’s giving in to peer pressure. From kids. Are you a parent yourself? Just wondering. coldtea 15 minutes agorootparentprev>What you’re describing doesn’t sound like parenting to me, it’s giving in to peer pressure. Parent, after an age, has very small influence in what kids do. Kids will be spending most of their time with peers, not with you. reply gommm 34 minutes agorootparentprevI had a strong sense of identity, I had good results, a good family life, my parents had my back, etc.. That didn't stop me from being bullied or pissed on while being held down by fucking assholes. So, I'd say, you either don't know what bullying is like or you're overly naive. And by the way, having my parents having my back and telling my teachers about the bullying just made things worse. It only improved when I changed school and punched the first guy who namecalled me. Anyway, to respond to your points: > What you’re describing doesn’t sound like parenting to me, it’s giving in to peer pressure. What I'm describing is knowing how society works and planning around it. It doesn't mean that I would give unrestricted access to social medias, it also doesn't mean that I would not be there to guide my child about how to use them, what the dangers are etc... I'm saying that straight up abstinence is not a good idea and doesn't work if your child lives in a society that doesn't abstain. There are also perverse effects whereby preventing your child from completely accessing social medias, you end up with a child who just hides it from you. > If your kid’s peers all gain a liking for drugs or gambling or some other vice and they bully your child for not partaking, are you going to tell your child to participate? I'd probably consider switching my child to a different school. reply kredd 8 minutes agorootparentprevI’m sorry for sounding so harsh, but have you been through public or private school where you’re some sort of a group of at least 50+ children? Sense of belonging is so natural, especially at that age, that you can’t tell a kid that it doesn’t matter. If your child just tells you “yeah I don’t care about others’ opinions”, then I’ll have a hard time to believe as well. As we age, and grow up, sure those things matter less. But come on, we’ve all been children, we’ve all wanted to be a part of some group. Some of us got bullied, some of us got ostracized, but we always wanted to belong. reply searchableguy 22 minutes agorootparentprevBecause people will not invite you if you are not in the group chat. Unfortunately, people have forgotten how to call and are afraid of receiving calls. reply ryandrake 5 minutes agorootparentIf a friend is going to leave you out of an event purely because you do not use their preferred BigTech-facilitated chat tool, then I have done bad news for you: that person might not actually be your friend. Friends don’t treat each other that way. JoshTriplett 6 minutes agorootparentprev> Unfortunately, people have forgotten how to call and are afraid of receiving calls. That's unnecessarily reductionist. Calling is less convenient and takes longer. thinkingemote 2 hours agorootparentprevDo you live or work with others? Do you have a family or children? Do you interact in meaningful ways with people online? I ask seriously (but no need to reply, this is to aid understanding) as many here on HN do not have all of these so the range of examples may be narrowed for some people. I would however imagine you have some relationships of some kind that you care about. Can you imagine your relationships changing if these others you cared about changed their behaviours? Then extend this imaginary possibility to the relationships of these others. It's a network we are in we are not operating entirely alone. Care of the other is wanting what is good for the other. Sometimes this care is not forcing the other to change, sometimes it's encouraging them to change to benefit them as it benefited you. reply XorNot 1 hour agorootparentHow is this not the OP's exact point? If you're not forcing people to change - by banning things with the violence of government - then you're simply making an argument to people and hoping they listen, just as we ever have to. reply josephg 1 hour agorootparentprev> What do I care what others do? I’ve done the same, but it’s not without cost: - Two hobbies I’m involved with have large local communities on Facebook. I’m not in the loop, and sometimes miss out on events and catchups. - I’m not on Twitter, and there’s a massive amount of chat about my field (realtime collaborative editing & local first software) that takes place there. I miss out on what’s going on, and I’m reliant on other people to promote my work for me. Im happier. But it’s you really are cut off from a lot of society - especially in the tech world - if you aren’t on social media. reply ranguna 1 hour agorootparentYou can join those communities and discussions with \"work\" accounts. Instead of creating a personal twitter or meta account, you could create one that is only used to join those specific groups and discussion. reply pjerem 1 hour agorootparentYes you can. But then, you have an account. Most people that dont have an account on social medias just don’t want to accept the TOS. Also worth noting that it sounds like meta thinks like they already have all earth population in their products because new accounts are really easily banned for no reason. Any temporary account I made to access some information have been blocked minutes or hours after creation. reply stareatgoats 2 hours agorootparentprev> What do I care what others do? While a perfectly fine rule of thumb in general, I think you might find the sticking to \"I don't care what other people do\" as an overriding principle doesn't hold water in a substantial number of edge cases. One such case is where someone is willfully influencing others to act against their best interests, or against the best interest of society as a whole. That's when one needs to make a judgement call on a case by case basis. Is this such a case? Not sure. I just wanted to point out that \"I don't care what other people do\" can't be your guiding light, always. reply epolanski 2 hours agorootparentI might have expressed myself poorly. I \"don't care\" what others do in the strict sense that I don't let it influence me. But I can tell you that me quitting socials did impact relatives and friends to quit those too, and thus I absolutely do not relate with his statement: > It doesn't matter whether you abstain from using Instagram or not reply raziel2p 1 hour agorootparentI'm glad that you were able to convince all your meaningful friends and family to change their social media / internet behavior according to your values - but please don't assume that it's a viable option for everyone, with no downsides. reply JodieBenitez 1 hour agorootparentprev> What do I care what others do? example from real life: My local musicians community decided to use only FB to communicate their gigs dates. I refuse to use FB. I no longer know when or where is the gig. I don't see my fellows anymore. reply forgetfreeman 47 minutes agorootparentMaybe text or call a couple friends in the community once a week? reply JodieBenitez 38 minutes agorootparentBeen there, done that... \"it's all on the page, just check it mate\". And suddenly you're that annoying weirdo that refuses to do things like everybody else. reply 10729287 1 hour agorootparentprevHow hard is it to discuss this topic with other people that have access to fb when you meet them around the city ? Honest question. Sure, it require more involvement than opening the Facebook tap and getting everything instantly, but at the end of the day, isn't your goal to getting involved in the community ? This will require some energy. reply JodieBenitez 25 minutes agorootparentSee above reply coldtea 18 minutes agorootparentprevIf you don't live in a society but in some remote wilderness and hunt for your food, then you don't need to care what others do. If you do live in a society, you do need to care, as \"what others do\" affects society in general, including you in the end. Affects how they behave, how they vote, what causes they support, their mental health, and tons of other things, all of which end up also affecting those who don't use those apps. reply raziel2p 1 hour agorootparentprevMy life is also better, overall. However, I do miss out on important life updates from my friends because I'm not on Instagram. I could be a hardliner and say \"well they don't care about me enough to share 1-on-1, so they're not real friends\", but that's... stupid. I feel sad for missing out on opportunities to take part in their lives and life updates that way, and I know for a fact that there are rich connections/conversations that would happen if I were able to use these in a healthy way. reply abc123abc123 41 minutes agorootparentprevI agree. Same here. The only social media I have is mastodon and usenet. Yes, I live and work with others and email and phone is sufficient. If people try to get me to use slck I refuse. If I cannot refuse, I deploy my slck2email bridge and reply once every 24 hours and usually colleagues stop chatting with me when they realize they won't get an answer until 24 hours, or they drop by my desk, email or call me on the phone if it is urgent. I think a lot of the complaints from people who \"cannot stop\" is just laziness. They need the government to construct an excuse so they won't have to man up and take control of their own lives. That is also sad, because it means those people will never grow up but will be constant children in the eyes of the state. Then I would say there are the 0.01% who do have psychological problems and they need to see a doctor. reply EMIRELADERO 2 hours agorootparentprevThe commenter is arguing from a broad \"what's good for society\" sense, not on strict individuality. While I disagree with the whole idea of legislating access to social media/internet services in any way (RIP anonymity), I do think there can be valid places and times for regulation that seeks to change society itself, not just individuals. reply unclebucknasty 2 hours agorootparent>RIP anonymity Maybe anonymity has had a good run and is actually at the root of the issue in many contexts. After all, people don't (generally) walk around IRL wearing ski masks so they can say crazy shit or troll people without consequence. And we also get the courtesy of knowing who the people we're talking to actually are. And if you say, \"yeah but doxxing and death threats\" or \"my employer might fire me\", etc then maybe these are the actual problems that need to be addressed. And, yes, I realize that's coming from someone who's IRL neither Uncle Buck or Buc Nasty, as his handle might imply. But, obviously that's where we currently are, which is the point. reply AlexandrB 1 hour agorootparentIRL nobody is running facial recognition tech and uniquely identifying you. And, despite what your teachers told you, no one is keeping a permanent record of the minutia of your life. So I know my neighbor is \"George\", but I don't know anything about his political opinions, where he shops, how he treats waiters, or what kind of porn he watches. Thus he's not anonymous, but his daily activities are generally private and ephemeral. The internet flips this on its head because pseudonymous handles can be linked to reams of online activity that's retained effectively forever but can't be connected to a specific person. This is why \"doxxing\" is such a big deal online. If online activity was like IRL activity and ephemeral, I might agree with you. But the internet never forgets. Edit: By the way, this is not universally defined: > say crazy shit \"Crazy shit\" is very culturally and contextually dependent. If I condemn China's treatment of Uyghurs, that's fine in North America but considered \"crazy shit\" and can land you in jail in China. That's an extreme example, but there are plenty of other more banal differences in culture and what's acceptable globally. reply Y_Y 15 minutes agorootparent> IRL nobody is running facial recognition tech and uniquely identifying you. Maybe this is true in some places. In big cities in Europe and the Americas this is definitely the case. It's done by law enforcement, commercial retail, and private security. It's more or less trivial nowadays to buy a cheap IP camera and collect an database of faces across your camera network. My guess is that there's more of this going on, but I'm only listing stuff I have personal knowledge of. reply LAC-Tech 39 minutes agorootparentprevFor people who live in oppressive regimes where certain political speech is illegal, like China or the UK, anonymity is a powerful tool. I would not want to take that away from them. reply unclebucknasty 2 hours agorootparentprev>What do I care what others do Because we live in a society? Would you care if everyone around you was smoking crack, sociopathic or seriously mentally ill? reply jascination 1 hour agorootparentprev> It doesn't matter whether you abstain from using Instagram or not, some of your friends will still be more or less subtly influenced by its existence in your social interactions You could say the same thing about the Bible, or Harry Potter, or any number of things too reply raziel2p 1 hour agorootparentThe Bible, I absolutely would, the negative impact of dogmatic Christianity has been pretty bad on society IMO. Harry Potter, not so much, but still some yes. I still have to argue with some of my friends that the books have some dubious morals that I wouldn't be comfortable ingraining in my children (if I had any). The scale, extent and addictiveness of these two things are nothing compared to algorithmic social media though. reply abnercoimbre 1 hour agorootparentTangent: Children's literacy (and that of some adults for that matter) skyrocketed under the HP phenomenon, in profound ways that activists/scholars are typically grateful for. I wish I could find the studies done, sorry that I don't have them on hand. I guess I'm saying that it was worth it, dubious morals aside. reply zer0zzz 1 hour agorootparentprevThat’s an interesting thought. I wonder how detrimental Christianity’s impact on society has been compared to social media? It kind of makes the argument pointless when you frame it that way. reply yard2010 29 minutes agorootparentprevAnd you would say it as well about party/designer drugs reply thegrim000 3 hours agoparentprevIt's funny .. earlier today there was a front page HN post about the federal government mandating safer circular saws. It seemed like the majority of users in the thread were in favor of the federal government mandating technology changes to prevent harm from being done to the population. Now for this issue, there's harm being done to children, and the majority of users in the thread seem to be against government intervention; you say: \"well if you don't like it, if you think it's negative, just don't use it, don't let your kids use it\". Kind of a random parallel to draw between the two stories, but it's funny the same logic doesn't seem to apply in both cases. Why wasn't for circular saws the response \"if you think they're dangerous, don't use them\" or \"just keep your kids away from them\"? reply 0xEF 1 hour agorootparentThis is a silly comparison. The table saw was not designed to be addictive, turn its users into a highly lucrative commodity, or push algorithmically driven agendas. Social media was. The dangers being compared here are very different. reply kristiandupont 1 hour agorootparentThat seems to reinforce GP's argument, though? reply moffkalast 27 minutes agorootparentprevYou mean to say, with a saw the intent is for it to be a useful tool and the danger is an unintended side effect, while for social media the danger is the intent and it being a useful tool is an unintended side effect. reply maxioatic 2 hours agorootparentprevIt was about table saws, not circular saws. There’s a big difference between the two. Table saw accidents often result in losing fingers and it’s not that difficult to mess up while using one. There’s a well known, proven, easy solution to table saw accidents called SawStop. It’s basically as obvious to use as a seat belt is if you want to be safe. The only problem is those table saws are very expensive. Social media doesn’t have an existing and obvious solution (besides not using it). reply planede 1 hour agorootparentIsn't SawStop patent encumbered? AFAIK the three point seat belt design's patent was made open by Volvo at the time, so the patent didn't hold back adoption. reply unclebucknasty 1 hour agorootparentprevSo, if there's not an easy solution, we should de-emphasize the problem? reply XorNot 1 hour agorootparentprevI think this could be aptly summarized as \"you can't accidentally slip and become depressed\" using social media. You can absolutely slip and lose one or several fingers or your entire hand using a table saw. The more pertinent comparison would be alcohol IMO: none of the people who want \"something\" done about social media seem to have a problem with the widespread, massive use of alcohol within society and the incredible amounts of continuous and ongoing damage it does. reply raxxorraxor 2 hours agorootparentprevThe reason I am against government intervention is the fact that governments seem to not be competent enough to solve problems like this and they would use content controls for their own purposes. It is vastly different and more complex than regulating saws. The comparison falls short by a huge margin and a false conclusion that any federal legislation would be desirable just because it is the case for saws. Some suggest it would be the \"hate\" on the net that is causing the issues and we see legislation that penalizes some content already, but I heavily doubt it to be the source of any problem. Might be something similar, perhaps the strong indignations some statements on the net seem to get to some people, although these can be as politely stated as any frivolous statement can be. And the resulting expectations on opinions you are allowed to harbor. reply sloowm 1 hour agorootparentThe government is never a passive actor. So non-intervention is also active policy. You can only choose what the policy is. The active policy for the last 15 years has been to consolidate a social media oligopoly with very few restrictions. Users are being tracked, advertisement laws are being skirted and are less restrictive than in other mediums, data is being sold, dark patterns are used to keep people from making their own choices. The algorithms are actively promoting bad content because the social media companies are not held liable for their part in promoting false content. reply unclebucknasty 2 hours agorootparentprevIrrespective of where people fall on this particular issue, I find it odd how the people who are so distrustful of government would allow things that are overtly dangerous, as long as it prevents government from...governing. It's like we have this generation of people who believe government overreach is a) inevitably the outcome in every scenario; b) present in every situation; c) always the worst possible thing that could happen. As in, literally worse than mass sickness and death. reply forgetfreeman 44 minutes agorootparentprevYeah funny as a one-legged rabbit hopping in neat little circles. If I were still on social media I wouldn't want to take a long look at the quality of my interactions or the costs associated with them either. reply ImPleadThe5th 3 hours agorootparentprevWhile both paternalism. Requiring safety features on a saw does not restrict free speech. It's more akin to seatbelt laws. It's also made to protect everyone who uses a table saw and not just children. Imo, I do think social media needs to be reeled in by policy. But I can see why it makes people uncomfortable and why there is a difference with the saw. reply Capricorn2481 2 hours agorootparentHow does not having social media restrict free speech? Do you think free speech didn't exist before social media? Arguably, the presence of social media homogenizes the speech we're allowed to have. reply cnity 1 hour agorootparentHow does banning an individual from printing books restrict free speech? Do you think free speech didn't exist before the printing press? reply forgetfreeman 42 minutes agorootparentHow is access to a wholly privately owned walled garden in any way relate to printing books? Private networks are by definition not public domain and thus are totally irrelevant to any discussion of free speech. reply cnity 34 minutes agorootparentI would agree if it weren't for the complete transition to privately owned communication platforms. The answer to your question is actually quite simple: because communication via privately owned walled gardens is humanity's primary means of mass communication, just as it used to be printed media. It would be as if printing presses were so complicated and expensive that the barrier to entry was so high as to price out everyone but a few select publishers. I wouldn't try to over-extend that metaphor though. reply Capricorn2481 1 hour agorootparentprevNot really the same, because the article is not calling for banning any source of social media. How would you even classify social media? We are taking about ad infested hellholes with no incentives other than maximizing revenue, regardless of the content pushed. The proper analogy would be banning books with certain content, which we already do. You can't distribute a book calling for a specific person to be killed or doxxing them. Doing this on social media in Ethiopia is encouraged, as it drives engagement and has lead to actual deaths of people I know. They have a policy not to moderate this content despite having the resources. Just like they have a policy to make the apps as addictive as possible. More importantly, Facebook is not a \"printed book\", it is the printing press. It owns the internet. It's not remotely comparable. And that's why it is a threat to free speech reply fabatka 1 hour agorootparentprevI guess if it's only specific individuals/groups that can't print books, it's restricting free speech, if nobody can, it's not. reply sambazi 1 hour agorootparentprevgood point. a lot of ppl got reeled into the narrative that social media can democratize (free) publication of conversations and ideas, thou it is dominated by monetary incentives that mandate propaganda/advertising and in turn moderation and censorship. reply eviks 2 hours agorootparentprevIt restricts free speech in the most direct, literal sense - by... restricting your ability to freely speak. The historical existence is simply irrelevant. Just like existence of pre-TV/newspaper speech is not a relevant factor in determining whether banning all TV/newspapers in 1950 restricts free speech reply sammorrowdrums 2 hours agorootparentMaking publication easy on social media has certainly had an impact on public speech, but private platforms do not offer free speech by design. Naomi Klein went into this in No Logo with shopping malls replacing public spaces where you also don’t have a right to free speech and can be evicted arbitrarily at the owners discretion. You’ll find virtually all of social media platforms have moderation, usage policies and user banning practices that go well beyond allowing the fully legally protected free speech you are afforded in a public space (in many countries). reply cnity 1 hour agorootparentIf all spaces that attract the majority of people are private and have homogenous terms of use, then free speech ends in all ways except on this technicality. Edit: removed unnecessarily inflammatory phrasing. reply forgetfreeman 39 minutes agorootparentprevThis is a telling argument. Newspapers and television broadcasts, while geared towards broad public consumption, were never wholly democratized platforms and that didn't run afoul of the first amendment. It stands to reason that management of content on social media platforms or outright banning the same wouldn't either. reply RandomLensman 2 hours agorootparentprevWhat access to tools or avenues for speech should fall under the first amendment then? reply Ekaros 2 hours agorootparentprevI wonder how many would be for seatbelt laws if the addition of seatbelts say doubled the price of car. reply red_admiral 1 hour agorootparentI think this confuses cause and effect. Seatbelts are brought up so often precisely because they are an intervention with a huge benefit-to-cost ratio. Seatbelt laws were made long after the fact - seat belts for cars started to appear in the 1950s, with the common three-point variant in 1955; the first seatbelt law appeared in 1970 (in Australia). The US started introducing seatbelt laws for cars in the 1980s (though as far as I know, some organizations/insurers required them earlier for employees driving for business). reply throwaway2037 2 hours agorootparentprev> if the addition of seatbelts say doubled the price of car Here is the problem: They didn't. So what is your point? reply lynx23 2 hours agorootparentprevBecause it is the parents responsibility to set boundaries for their children. It can be complicated at times, granted. But that doesn't make it less of their job. Heck, I got my first CD player with 14. Yes, I felt left out at school, but... guess what, I didn't die. Children need to learn that there are rules, and someone else dictates them. Throwing tantrums is a typical reaction that needs to be weeded out as a part of growing up. Besides, the \"somebody has to think about the children\" meme is slowly but surely getting old and tiresome. Not somebody... Their PARENTS. If you dont feel like setting boundaries for your children, please, with sugar on top, dont have any. reply RandomLensman 2 hours agorootparentYou can say it should be on the parents here and reason why, but for a lot of things it is not just on the parents (children are not allowed to vote, drive a car, buy guns, go to bar and drink alcohol, gamble, ... - it is a long list). reply graemep 1 hour agorootparentSuch bans are not clear and are contentious, and often leave room for parental discretion. In most countries whether children drink alcohol at home is up to their parents. In some countries an adult can buy teenagers a drink (in the UK they increased the required age from 14 to 16 - and I think its a bad thing). There are people who think 16 year olds should be allowed to vote. Kids cannot buy guns, but can use them. I did a bit of rife shooting at school. reply RandomLensman 1 hour agorootparentIs it really contentious if 10 year old children cannot on their own buy liquor or guns, for example? I would not have put that high on the list of contentious issues. There is a difference in being granted unsupervised abilities vs supervised ones. reply hgomersall 2 hours agorootparentprevAs is pointed out in the article, a huge factor is the collective action trap. An individual set of parents can do very little to deal with mental health if they are the only ones. reply lynx23 2 hours agorootparentI totally doubt this is true. Its a nice excuse though. reply forgetfreeman 34 minutes agorootparentTell us you don't have kids without telling us you don't have kids. Short of totally unplugging your children from broader society via homeschooling, joining a commune, or similar extremes, children are exposed to whatever other children's parents permit through nothing more complicated than their interactions with other kids. An example from pre-digital times would be that one kid who's dad kept a stash of nudie mags unsecured which invariably lead to hushed giggling in the back of the bus. reply dotinvoke 2 hours agorootparentprevI was one of the last people in my class to get a phone, which taught me that not having the cool new thing was not nearly as bad as I had thought. reply NoPicklez 3 hours agoparentprevEveryone has the ability to exercise personal accountability for how much they gamble, smoke, eat junk food, play video games, use social media etc. However, if these consumer products become so addictive, or are designed to be so engaging and as a result people are by and large struggling to exercise personal accountability and it is causing adverse health outcomes. Then the government should step in, either should forceful action or through promoting healthier alternatives or shining a light on its damaging effects. reply Loic 3 hours agorootparentI explain to our kids that it is normal for them to have difficulties with stopping playing or looking at a stream from a social media platform. My explanation is that they are fighting against a team of PhDs optimizing everything to make them addicted. Luckily they all do a lot of sport and have this way disconnected time everyday. reply cdogl 3 hours agorootparentprevThe simply binary of government or individual choice eliminates the middle ground where almost all change happens: the collective aggregate result of cultural change within the community. We don’t have to pick one extreme to change the world! reply Barrin92 3 hours agorootparent>the middle ground where almost all change happens: the collective aggregate result of cultural change happened, past tense. That cultural layer, to a large extent enforced by various religious traditions both in the literal and civic sense, sometimes for better or worse is pretty much gone. Nowadays it largely is a binary question of legal action on the one hand or individual choice on the other. If you tell people they need to make change within their community 90% are going to ask you, what community? Community with a capital C where people collectively enforce binding rules, rather than occasionally go bowling isn't much of a thing. reply randomdata 2 hours agorootparentCommunity need not be tight for change to occur. A random stranger calling you out on your shit is just as effective. Maybe even more effective as it doesn't always hit so hard when someone you are comfortable with says it. reply lynx23 2 hours agorootparentprevSo, why are alcohol (and nicotine) still legal then? Maybe there is a 100 year old lesson hidden somewhere... reply TacticalCoder 1 hour agorootparent> So, why are alcohol (and nicotine) still legal then? I don't disagree with your answer but... Kids cannot buy alcohol and kids cannot buy cigarettes. It's kinda the whole point of TFA. The conclusion of TFA aren't to outlaw social media: it's preventing kids from accessing these mediocre piece of shit websites/apps before 16. reply johnchristopher 2 hours agorootparentprevAnd maybe that lesson is hiding in ellipsis or maybe it's not. Who knows... reply lynx23 2 hours agorootparentI wasn't expecting a reference to Prohibition to be that cryptic to some. https://en.wikipedia.org/wiki/Prohibition#Effects reply jimz 2 hours agorootparentprevExcept \"struggle\" is entirely subjective. In fact, outside of a few things that causes physical dependence (which vary in length except they all, eventually at least, end), the concept of addiction as used by the government does not necessarily match up to how those in the particular field of research would. Government makes laws and laws prefer bright line rules. Something like \"did you, without authorization, use the credit card given to you by your company to make purchases unrelated to your work\", for example, have concrete, definable, and answerable elements that are universal and more or less binary, provided that the statute has a definition section that makes sense. But \"so addictive\" or \"designed to be so engaging and as a result people are by and large struggling to exercise personal accountability and it is causing adverse health outcomes\" is pretty much the antithesis of that. The government would need to define \"addictive\", \"designed\" would need to have an intent element (one designs the software, sure, but you're asking for not what the software itself does but one step further - what its impact is on the population at large). How does the government prove that intent, especially since criminal law tends to define intent as intent to act which combined with the act itself, creates the crime. What counts as \"so engaging\" or even \"engaging\"? Does it require active engagement? Plenty of platforms do not require any active engagement to partake in the conventional sense, unless reading is engagement. How many people counts as \"by large\" (I assume that's what you mean, feel free to correct)? How would the government show that the product and any struggle is causatively linked and not merely correlative? How does one define struggle to exercise personal accountability? Where did the duty of exercising personal accountability even come from as to establish liability and would that criminalize those who are disabled or injured as to being unable to exercise such responsibility writ large? And what counts for adverse health outcomes? All these need to be worked out in legislation and likely argued over in court. Every single element needs to be worked on as to not to be overly inclusive or exclusive. And since it's the government, the consequences for violation is without question enormous, and therefore, anything that can be misconstrued can result in the ruin of a company or persons in a variety of ways, but do you really want to have the government determine who is an edge case that doesn't count? Because the government have done that based on assumptions of potential harm and it has caused what today would be considered horrific abuses of human rights and very little positives beyond enriching those whose income derives from the enforcement of the government's scheme. Laws are lagging indicators but they also last a long time. the CFAA was passed before the advent of the WWW and it took until 2021 to even set a basic check on the part of the Supreme Court that effective set the ground rule that to access what amounts to a computer linked to some network beyond authorization, an authorization scheme needs to exist in the first place. Before that, one can easily be charged and even sent to prison or be assessed massive fines when there's no meaningful distinction between what is authorized and unauthorized space. These were not problems in the early to mid 80s but when problems did arise, it still took a quarter century to resolve. To have one future-proof goldilocks solution is already next to impossible, but you're asking for five or six stringed together in order to have a sensible law that is well tailored enough so that it is effective without being oppressive. Not to mention that unless the behavior is generally abandoned by users, it creates black markets that are simply illegible to the state. The government then effectively loses control over what it purports to control to those with means, leaving only those without subject to the full force of the legislation. That of course all predicates on the premise that there can be commonly agreed and sensible ways to define all those, and it is in the best interest of the government to do so and passes Constitutional muster not just on speech grounds but a host of other potential issues, like, is this a purely civil matter or a purely criminal matter or both? The federal government can treat this explicitly, or kick it off to an agency as part of its mandate, but which one? Do we need a new one? Are there checks and balances that would provide some sort of agility that keeps up with the times? What if new research comes out that shows the lack of a link, but by legislating it, you've effectively frozen the relevant conclusion in time. Enforcement creates constituencies who do not care about science or potential upsides. The DEA is on the record in the federal register that patient access to legitimate medication is secondary and effectively an afterthought to enforcement of supply, because the agency's mandate presupposes that substances need controls and are presumed harmful and that enforcement, with the teeth provided for by the DOJ, will trump any study the FDA or our academic institutions can ever show. By the time that particular moral panic was given a name, the US government had been attempting repeatedly to use prohibition as a way of imposing a specific set of social mores that at first was a pretext to target specific racial minorities and when that became socially unacceptable (legally it was unacceptable under the 14th Amendment anyway, but they effectively smuggled the laws in through the Treasury Department and protectionist regimes by taxing the goods into oblivion, and avoiding the tax obviously is also a violation of the law). In that sense, the government operates very much like a machine, whereas given a concrete goal to achieve and it can likely achieve it, but the manner by which it achieves it may create additional problems and convoluted interpretations that ripples through history in ways unimaginable. The loudest voices in the room, or those with existing financial resources or interests, can use the rent-seeking system known as lobbying to shape the laws to begin with. And where does the fines end up? Certainly in most cases they are not given back to the community, but end up enriching the enforcement agencies. Go to a police auction and see how much they're raising from the sale of \"proceeds of crime\", except not all of it are crimes that are proven in court, and much of it are crimes without specific victims and so, it becomes a regime of appropriation of private property to enrich a few in the public sector. There's usually an annoying gap between concept and reality. In isolation you want a policy that can solve problems in a targeted and fair way. In reality it almost never happens. reply hgomersall 2 hours agorootparentAddiction is well defined in research and it certainly does not preclude things that don't cause \"physical dependence\" by which I assume you use to mean psychoactive drugs. Wikipedia has a better definition right at the top than I can offer: https://en.m.wikipedia.org/wiki/Addiction The crucial point is it is characterised by neurological changes that lead to short circuited reward pathways. The idea of addiction to social media a perfectly consistent with its action and moreover, consistent with the notion that its effect can be enhanced by design. reply qwery 3 hours agoparentprev> But these are communications platforms. While technically true, this is a gross misrepresentation. Calling these sites and apps \"communications platforms\" makes them sound like they're just a mail service or a telephone. This is akin to referring to a casino as \"the town square\". > That's a freedom you and everyone else can take advantage of also. This \"can\" is only true in a strict legal sense, of course. reply brylie 2 hours agoparentprevNote that the reforms suggested by the author are primarily normative, not legislative: \"\"\" More specifically, we’d try to implement these four norms as widely as possible: 1. No smartphones before high school (as a norm, not a law; parents can just give younger kids flip phones, basic phones, or phone watches). 2. No social media before 16 (as a norm, but one that would be much more effective if supported by laws such as the proposed update to COPPA, the Kids Online Safety Act, state-level age-appropriate design codes, and new social media bills like the bipartisan Protecting Kids on Social Media Act, or like the state level bills passed in Utah last year and in Florida last month). 3. Phone-free schools (use phone lockers or Yondr pouches for the whole school day, so that students can pay attention to their teachers and to each other) 4. More independence, free play, and responsibility in the real world. \"\"\" https://www.afterbabel.com/i/143412349/what-now reply Biologist123 2 hours agoparentprev> You may not like FB, IG, TikTok, etc.. I certainly don't care for any of these products. But these are communications platforms. Restricting the right to free speech does have negative consequences You’re conflating these tech platforms with freedom of speech. It might be helpful to the debate to separate out the addictive algorithm and user base from people’s right to think and speak freely. reply mxkopy 2 hours agorootparentAs always the real takeaway is to repeal Citizen’s United reply vermilingua 1 hour agoparentprevThere is a major difference between banning curse words, medical info, porn etc, and banning social media. The former is banning a type of content, the latter is banning the presentation of content; and it is the presentation that is so harmful. Banning social media optimised for “engagement” at the expense of childrens (and adults) mental health does not remove any content from the internet that could not be expressed in a less toxic way. The finesse is in defining social media in a way more complex than “this list of companies”, and I agree that (likely) no government would choose a definition that does not either have ramifications for free speech or is inadequate. reply GeoAtreides 25 minutes agoparentprev> if you don't like these products and feel they are negative, then don't use them Just don't smoke! Just don't drink alcohol! Just don't eat junk food! Just go to the gym! > Restrict your children's access to these platforms That's exactly what the article suggests should happen, plus some protections for children enshrined in law. You can find these suggestions at the end of the post, in the section \"What now?\" reply virtualritz 42 minutes agoparentprev> [...] if you don't like these products and feel they are negative, then don't use them. And have a severally impacted/constrained social life? People with all kinds of hobbies use those platforms to organize group activities. You are either on there or you miss out. I dance tango socially. The tango community, world wide, has settled on FB. Or rather: if you are a dancer, teacher, organizer or DJ, you better be on FB or else you won't know where and when to dance, how to find students, get people to attend your event or get booked. I.e. even if you decided you didn't like FB, you have no choice but to join it and thus help cement their monopoly on how people with this hobby organize themselves. reply TacticalCoder 2 hours agoparentprevAll you wrote doesn't address the elephant in the room: that social media is responsible for an epidemy of teen mental illness (which may or may not be true). If that's true, the suggested measures: no phone before high-school, no social media before 16, phone-free schools, ... do not seem crazy. Kids cannot drink, cannot smoke, cannot have sex with adults, cannot buy firearms, etc. There's a shitload of things kids cannot do: is it really an attack on free speech to have kids not have phones at school and wait until 16 before they can use these mediocre piece of shit social media platforms? reply ignoramous 1 hour agoparentprev> You may not like FB, IG, TikTok, etc.. I certainly don't care for any of these products. But these are communications platforms. Haidt argues these are a leading cause of an ongoing mental illness epidemic. Such a drastic claim deserves a thorough medical review and if true, these platforms must be regulated just like the Tobacco industry was. > But those who advocate censorship aren't advocating for freedom... they're advocating for their personal parental decisions to the be decisions of the entire nation. Don't believe freedom of speech overrides the concern of Humanity collectively and progressively going ill, anymore than freedom to self-defense warrants the use of nuclear weapons for personal use. That said, the burden of proof is on Haidt. It isn't uncommon for the older generation to be pessimistic or doomsdaying about the next one. reply danieldk 2 hours agoparentprevif you don't like these products and feel they are negative, then don't use them. Restrict your children's access to these platforms. If it was only this simple. It's similar to the green bubble problem. Disallowing your kids to use these platforms leads to social isolation. All the other kids are on these platforms and a lot of the talk is about what happened on these platforms (it's similar with games like Roblox for a certain age bracket). Excluding them is also going to teenage mental illness due to exclusion. It's only going to work if all parents would restrict access to these platforms, but that's not going to happen. A lot of parents do not see the issue or do not want to be the first mover. reply nkrisc 11 minutes agoparentprevThat’s why we allow children to purchase cigarettes and alcohol and if parents don’t want their kids partaking they can just restrict their access, right? reply janpot 24 minutes agoparentprev> But these are communications platforms. They are mostly advertisement platforms coupled to recommendation engines. The \"communication platform\" is just side business at this point. And it's being used to wave around as \"free speech\" when anyone dares to question the detrimental effect of the big mass mind control machine it actually is. reply namaria 31 minutes agoparentprev> But these are communications platforms. Restricting the right to free speech does have negative consequences... I take issue with the argument that promoting these social media platforms is tantamount to fostering free speech and denouncing them amounts to eroding this right. No one should expect technology assisted broad cast abilities as part of a doctrine of governments and the State not restricting speech. reply forgetfreeman 29 minutes agoparentprevNah. They're advocating for an obvious and well-documented societal harm vector be regulated into a less harmful configuration. This is similar in concept to regulating pollution or disease vectors. reply geocar 2 hours agoparentprev> they're advocating for their personal parental decisions to the be decisions of the entire nation. That sounds exactly wrong. I think they want their \"personal parental\" decisions to not be the decisions of the entire nation, but \"personal\" and private to themselves and to be free from judgement for wanting this thing. > prevented access to education medical information ... you don't like these products and feel they are negative, then don't use them. This is pretty important to me: Abusive parents deny their children access to communication platforms. I believe these problems cause problems for the child if the child is already lacking support, but they also represent a way to escape that abuse, so I feel strongly that controlling access to asking-for-help is not okay; This should not be a personal decision good people can make for themselves in their homes for their own kids, because they should be able to understand that \"bad\" people are actually using laws and rules like this to hurt children. > I don't use any of these products I think you're using one right now: Hacker news is absolutely a communications platform. reply madsbuch 2 hours agoparentprevyou have many ways you can solve the issues without ristricting free speach. you can ristrict how you can monitize a product - I think the problem would be much smaller if you have to pay a price congruent to the value you get. Only a few people would pay for Facebook. you can make the platforms resposinsible for what is published on them and enforce that. they would never scale this much. > And just like the Supreme Court wrote 30 years ago, the answer is the same today: if you don't like these products and feel they are negative, then don't use them. We have already collectively agreed that this is not an argument. That is why there are agencies like the FDA, etc. reply bryanrasmussen 1 hour agoparentprev>Restricting the right to free speech does have negative consequences sure, but not everywhere is America and some places seem to manage without slippery-sloping to eternal damnation. reply steeve 3 hours agoparentprevOne could make that exact argument of cigarettes too. And see why it doesn't work in the real world. reply throwaway2037 2 hours agorootparentAre you making the argument that restricting cigarette sales by age does not work? reply steeve 1 hour agorootparentQuite the opposite reply kevingadd 2 hours agorootparentprevCigarettes aren't speech. I don't understand why anyone would argue otherwise. reply boxed 2 hours agorootparentNo, but there the issue is bodily autonomy. Which arguably is much more important a right than free speech. reply suslik 30 minutes agorootparentIs bodily autonomy a recognized right in any sense? Is there a society that actually respects the right to bodily autonomy legally or in practice - meaning, using any drugs at will, the right to suicide, to agree to being eaten by a friendly cannibal, and so on? I don't think there is, and nobody is pushing for it. reply defrost 20 minutes agorootparentThe Okapa District, pre missionary, pre kiaps .. you were good to go until the mid 1950s at least .. and then colonizers spread their values. > to agree to being eaten by a friendly cannibal They had standards though, you'd have to work to gain the respect for anyone to want to eat you as a mortuary ritual. https://www.youtube.com/watch?v=-FK5N_ObFeQ https://en.wikipedia.org/wiki/Fore_people reply dotancohen 1 hour agoparentprev> Restrict your children'ss access to these platforms. I'm the only parent I know that has. And there is much resentment. reply CJefferson 58 minutes agoparentprevDo you feel the same about legalizing guns, alcohol, cigarettes and heroin for children? I don’t think this is a stupid comparison, as I believe these platforms can be very harmful to children. reply willvarfar 4 hours agoparentprevUS citizens have the right to free speech. US companies have a qualified right to free speech https://constitution.findlaw.com/amendment1/freedom-of-speec... How about AI? If it is an algorithm that is talking to you, does it have the right to do all the things that are protected by 'free speech'? And does it matter if the AI is commercial, or a home hobby project effort? reply golergka 4 hours agorootparentSomebody has bought or rented the computer that this AI runs on, somebody has launched it as a piece of code or an API call. This somebody is using his right to free speech, AI is just a tool. reply willvarfar 4 hours agorootparentCan you clarify, is your \"somebody\" a person or a company? reply Gud 1 hour agorootparentThis “somebody” is part of the wealthy ruling class who makes the laws through billions of dollars spent on bribes(“lobbying”) and propaganda. reply latency-guy2 3 hours agorootparentprev> is your \"somebody\" a person or a company? What is the explicit difference between 1 person and 10 persons when it comes to their rights? Explicit, I want to know what rights we lose as soon as \"I\" transforms into \"we\". reply Kbelicius 2 hours agorootparent> What is the explicit difference between 1 person and 10 persons when it comes to their rights? None. > Explicit, I want to know what rights we lose as soon as \"I\" transforms into \"we\". In this case a company isn't \"we\". It is usually the owner. reply latency-guy2 2 hours agorootparent> > What is the explicit difference between 1 person and 10 persons when it comes to their rights? > None. > > Explicit, I want to know what rights we lose as soon as \"I\" transforms into \"we\". > In this case a company isn't \"we\". It is usually the owner. Awesome, I'm glad that rights do not change between 1 to 1 billion, let's not assert that these rights disappear in conditions that make no difference in quality. reply EGreg 4 hours agorootparentprevPraxeology 2.0 I have heard similar arguments about corporations hehe Very Rothbardian, Mises and Say would be proud too https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8550560/ reply otherme123 3 hours agorootparentHow a corporation can have an opinion? I can give you that an AI trained to make a mixin of a lot of input texts and output a mashup of those texts, the output might not be the same of their creators. That said, it's known that AI creators/trainers can make their AI lean towards certain \"opinions\", as we saw recently with the case of Gemini and their understanding of diversity (https://www.theverge.com/2024/2/21/24079371/google-ai-gemini...). Corporations can't do that. Corporations don't ask for everybody's in the corporation opinion and then do an aggregate with all of them to write a press release. They have some people choosing what to say in the press release that goes in the interest of the corporation. reply latency-guy2 3 hours agorootparent> They have some people choosing what to say in the press release that goes in the interest of the corporation. Who are these people? Are they people or something that are not people? reply smokel 3 hours agoparentprev> then don't use them This assumes that humans are rational agents. I think that drug addiction, wars in Israel and Ukraine, conspiracy theorists, and free-climbers sufficiently prove that this is not the case. If only life were so easy, we would not even need to have this discussion. reply zigman1 3 hours agorootparentIt's the country's role or more specifically, the role of education to equip citizens with skills that will help them navigate virtual, deceiving and fake internet space. reply smokel 3 hours agorootparentNo, it's not. Where does it say so? reply lewhoo 46 minutes agorootparentYou are focusing too much on formalities. It doesn't say so specifically, but a collateral of education, or maybe it's equally important as knowledge itself, is the ability of critical thinking. reply zigman1 3 hours agorootparentprevEducational goals focus on broader set of skills and competences including critical and logical thinking. You can find this in most national and federal documents. That's because the education in its traditional sense is a country investing in its own working force. Normally, a country would want a healthy, educated and productive workforce, but if this is not the case then we have a different problem. reply smokel 2 hours agorootparentSure, but there are so many skills to focus on, that one can hardly expect people to stay off drugs because one lesson was spent on how bad it is. Companies are constantly inventing new ways to get people hooked on their platforms (or products), and it's a pretty tough race for teachers. Note also the difference in salaries for high school teachers and developers at Meta. reply zigman1 2 hours agorootparentThe point is not to have a one shitty lesson on it, probably by someone who doesn't even know what cookies are. The point is, that alongside learning math, biology and history, you should also come out of educational system emotionally mature and equipped with skills to survive in modern world. Not all skills are acquired through a deliberate lesson, the role of school is not only to teach but to upbring a generation(s). If Huberman can teach you a basics of nutrition so can a formal educational process. The problem with relying on youtube educators is then that it is down to luck if someone will come across it. reply raziel2p 2 hours agorootparentprevThat is certainly an opinion and a higher goal we could hold ourselves to, but education is generally just a way to ensure kids grow up to be good citizens - and today good tends to mean productive, contributing to the economy. reply zigman1 2 hours agorootparentSee my reply to the other comment of my same opinion :) reply passwordoops 20 minutes agoparentprevSorry to seem glib, but by this same token we should be lift all restrictions we place on youth including smoking, drinking, etc. There's a very clear causal relationship between IG and mental health that goes beyond a moral choice and ideals of free speech reply kstenerud 2 hours agoparentprevWhy stop there? Why not drop all laws prohibiting sales of tobacco and alcohol to minors? Why should these producers have their right to sell impinged upon by scared parents pushing a bunch of laws through congress? After all, you're free to use or not to use tobacco and alcohol, right? So it should be every kid's choice. In fact, why stop there? Why not allow fentanyl pills at the counter of every convenience store for anyone who wants them? You have the freedom to choose to use or not to use them, so there's no problem, right? reply jajko 3 hours agoparentprevYou dont have kids, do you. Seeing them being pushed out of entire school community due to higher principles is heartbreaking to say at least, this is place from which teen suicides come from. Parents usually cave in the pressure. I would go and even claim I would ban all current social media platforms below 18. Ther are simply not enough protections, consistently, its place ripe for abuse and tons abuse is happening every day as we speak. I know we will keep our own kids off this for as long as possible, but eventually harm from absence will be greater. Parents shouldnt be choosing lesser evil like that, just that some meta employees can cash half a million and think what a great addition to mankind they are, when reality is closer to definition of cancer. reply zigman1 3 hours agorootparent> I would ban all current social media platforms below 18 Just like porn? Do you think kids would find a way around it? reply non-chalad 3 hours agorootparentNot if we implement mandatory ID internet use and social tracking system. We could introduce it with Elmo, Kermit, and Fozzie, and call it \"Sesame Credit\"… waka waka! reply rgpenner 1 hour agoparentprev> Restricting the right to free speech does have negative consequences... from the development of critical thinking skills; development of technical skills; and limiting of educational information. That's not true. Those things are limited in bubbles like the one of the rationalists as well, so free speech has nothing to do with why people don't develop these things. It's a matter of character; and obedience to a system that establishes the rules that evaluate social, academic and economic status. The Supreme Court was capable of distinguishing between free speech and moron speech but the Party was too convincing. Most of the parents of these kids don't have the required flow of information to handle their kids consumption. And more importantly, their stress levels are too damn high already. Which is also the result of the Party's long term strategy. And if all the other parents say it's normal and the same for their kids, even those parents where it is not the case, which are those that know that's why their kids will be better off while everything collapses (vs making sure their kids are better off while nothing collapses), then there is no way but super-rationality to identify a problem and then there's the willpower to go against the accusations and the time required for researching strategies to deal with the problem while the rest of the world, and school, and one's own life keep working as they always did. If there's bait, the untrained puppy bites. Unless the untrained puppy was trained to bite and puke it out afterwards to find hints at who put the bait right in the path of all those innocent puppies. reply hackerlight 4 hours agoparentprev> if you don't like these products and feel they are negative, then don't use them. Relevant bit from the essay: \"But much of my book is about the collective action traps that entire communities of adolescents fall into when they move their social lives onto these platforms, such that it becomes costly to abstain. It is at that point that collective mental health declines most sharply, and the individuals who try to quit find that they are socially isolated. The skeptics do not consider the ways that these network or group-level effects may obscure individual-level effects, and may be much larger than the individual-level effects.\" reply GoblinSlayer 2 hours agorootparentThey meet their society in meat at school every day, how is that isolation? Also ban smartphones at school. reply lvoudour 3 hours agorootparentprevAn excellent point. Abstention = social isolation, which for young people is far worse than exposure. Restricting your children's access is not an option (lets' be real, they'll find a way to circumvent your efforts anyway) and moving the burden of restriction from society to individuals is not fair. So as a society do we let unrestrained exposure or do we take collective action? I lean on the second option, but I'm not sure what this action might be. I'm on the internet ~30 years, I loved the total anarchy of the early web, the unrestrained access to all kinds of information - good, bad and evil. It's very hard for me to get behind heavy-handed regulation. But honestly, I feel oversaturated by the modern cataclysm of information. My bullshit filters are clogged, my defense mechanisms are failing to the point I let information flow through me without an ounce of critical thinking. I can't imagine what the effect is on young untrained minds. reply Mainan_Tagonist 1 hour agorootparentSame feeling here, I loved the early internet, it played a huge part in who i am actually! This said, this is not the early internet anymore, where content was mechanically regulated by a sort of egalitarian rule. Social Media applies a power law to content, so that 80% of the viewers are aware of the 20% that's available and human nature being what it is, lowest common denominator content gets pushed to the forefront. Hence all the attention seekers on FacebInstaTok... This is further compounded by the pervert effects generated by these platforms one of them being the mimetism and the general wolfpack behaviour that can surge out of the madness of crowds. Online Bullying is real. My kids (11 and 14) are stuck on feature phones for now and i'd like, as much as possible to keep them off smartphones and their constant Notifications for the foreseeable future, until they are not kids. reply bigstrat2003 2 hours agorootparentprev> Abstention = social isolation, which for young people is far worse than exposure. Restricting your children's access is not an option... \"Everybody else is doing it\" has never been, and still is not, a valid reason for anything. If other parents choose to let their kids ingest mental poison, that does not mean that one should allow their children to do the same. Abstention is not only an option, it is something which absolutely should be enforced by any parent who cares about their child's well being. reply lvoudour 1 hour agorootparentI'm not talking about kids, I'm talking about adolescents (as is the quoted paragraph). I strongly believe that an adolescent's well being is tightly coupled with social interactions. If a restriction is not protecting them from life threatening situations, then alienating them from their peers is probably worse. reply GoblinSlayer 2 hours agorootparentprevIf they find a workaround, they will still be unable to sit there around the clock, which is decent reduction of consumption. Also there won't be many, just like smoking schoolkids, so no social pressure. You can ban it completely or you can have your lovely bookface 1 hour per day, why not, it's dangerous when they spend there 10 hours per day. reply CaptArmchair 2 hours agorootparentprev30 years ago, you didn't live vicariously through the published perception of the world you friends held 24/7. Social interactions stopped when you put down the phone or went home for the day. If your friends went on a trip, while you couldn't, you'd only hear about their stories when they got back. 30 years ago, unrestrained access was still constrained to a desktop computer hooked to dial-up. Your access was constrained to a physical location. Today, the big issue is the lure of having 24/7 mobile access to the Internet. At any moment, you can amend your own crafted online digital identity, meshing it with your real life, as you publish your location via Snapchat, Instagram or WhatsApp with your friends. Meanwhile, you can't but be confronted with notifications telling you where your friends are and what they are up to with who (\"X has posted a photo, Y is currently at Z\"). On a surface level, that lure has created a host of totally new social conventions and etiquette over the past 18 years, basically since the release of the iPhone. Social conventions to which one has to conform unless you don't want to lose out on social connections. For instance, seeing whether a recipient of a PM has \"read\" a message and then \"leaving you on read\". Having that rather unrealistic expectation that one ought to respond instantly once a message has been read. At worst, friendships are put on tenterhooks as one ties value to the time between that \"read\" notice, and the moment a response follows. In reality, the world 30 years ago wasn't more beautiful and people weren't more kind then they are today. In fact, if you weren't asked by your friends to hang out, or were left out when they went to a party and had all these in-group stories to tell, you felt socially isolated either way. That's not really new. What's new is that this new lure of 24/7 connectivity creates a potential to be confronted with those feelings pretty much every waking hour. It must be anxiety inducing to scroll through your feed, not knowing if your friends did or didn't hang out last night without asking you. To my mind, the answer isn't outright banning social media, or mobile devices. The answer is to keep having that difficult discussion about the value of the affordances - or lack thereof - the offer to foster healthy human relationships. It's about finding better ways to teach and empower young people on how to approach these tools, built by commercial enterprises, in healthy ways. And it's about being willing to properly publicly invest in aspects ranging from education to mental health support to enforcement and so on. reply unclebucknasty 2 hours agoparentprevArguments of this form are...not good This is a social problem, not a parental one. When you allow for-profit companies (or anyone) to create addictive products that intermediate the social experience of an entire generation, how is a parent supposed to stand against that? It's how kids interact and it defines their entire social experience. Disallowing them access is like sending your kids to school and not allowing them to talk to anyone. We don't allow our kids to have access to alcohol or cigarettes because it's bad for them. How is this any different, when we know it's doing harm at scale? Because \"it's speech\"? That doesn't hold up. Pornography is also generally considered protected speech, but no one lobbies for unfettered access for kids. Beyond that, restricting social media does not infringe on free speech. That assertion is so obviously wrong on so many levels that it feels silly and pedantic to start itemizing them. reply nonrandomstring 1 hour agoparentprevThere are two kinds of freedom [0]. What we are missing in our society today is some essential negative freedoms. Most of us follow J.S. Mill's idea on restricting the influence of the state. See the many comments in this thread decrying government intervention to ban social media. That's a negative freedom, from tyranny. Mostly, we tend to emphasise positive freedoms, freedom to; run a business, share speech, own technology. And that is good. But there are negative freedoms, freedom from; coercion, the scourge of drugs, poverty, censorship. Obviously many positive freedoms can be expressed as negative ones, but how that logic is formulated in law really matters. Now the controversial bit: What we are missing is laws that give people freedom from technology The supposed \"choice\" to participate is not enough. Like others here I've been a non participant in social media and smartphones. I'm not a Luddite, I'm a computer scientist, but I twigged this problem very early having dealt with addiction and recognising how abuse is mediated by technology. I even wrote a book about it [1]. The problem is, life is made very difficult for those who want to exercise choice. Presently one must live as a second class citizen, in what feels like racism and prejudice of technological snobbery. It is utterly unnecessary. Governments do not need to ban smartphones or social media for kids or for anyone. Making this only about kids is a cop-out. It's leveraging emotional messaging to side-step a bigger problem nobody wants to face - that our whole society is under siege from technology overuse. The more general problem is that we've entered a period of technological over-reach. Kids don't just feel peer pressure to get a smartphone and social media, they live in a society that wants to mandate it. Whether for kids or adults, we need to strongly protect the rights of those who want a less technologically mediated (and encumbered - yes it's not all \"convenience\") lifestyle. This needs us to maintain plurality of access; No services for government, schools, health available *only* on proprietary and \"smart\" platforms. Requirement to maintain traditional paper and interpersonal modes. End the insanity of a \"cashless society\", and \"smart societies\" that exclude basic human interaction and require and assume smartphones with apps. Strongly protect the rights of parents to choose how their kids use technology, for example in schools, and the attitudes they are raised with. Governments can ensure negative freedoms without just banning stuff. [0] https://plato.stanford.edu/entries/liberty-positive-negative... [1] https://digitalvegan.net/ reply olibhel 2 hours agoparentprevIn my country, India, these platforms are used less for free speech and more for brainwashing and spreading hate and misinformation. Most of these posts are in Hindi, a major language around here, and call for all kinds of hate such as suppression of a specific religion, call for genocide, invading and acquiring neighboring countries etc. I've tried reporting such posts multiple times but hate filled posts are neither removed, nor restricted. If a platform cannot provide adequate moderation, it should stop operating in my country and be held responsible for providing a platform for spreading hate pseudo-anonmously. reply satvikpendem 2 hours agorootparentHow is this example any different than what the parent has said? If people feel these platforms are negatively impacting them, they should stop using them. Or do you believe others or the government has a right to disallow what people want to watch via their own choices? You may call it brainwashing but others may disagree. reply olibhel 44 minutes agorootparent> do you believe others or the government has a right to disallow what people want to watch via their own choices Yes. As an extreme example: watching cheese pizza is not allowed by governments. We have collectively also come together to consider murder as socially and legally unacceptable. We can and should regulate social media if posts read as follows: - we should invade and bomb that country to bits - we should destroy all places of worship belonging to XYZ religion - we should vote for XYZ because only he is going to save our religion from PQR - and much worse which I can't type here as moderation team of HN would omit those IMHO: give the current form of social media another few decades and it will come out shinning bright just like opioids did in the USA. These same social media platforms, when required by law, become very effective in moderation but there's next to no moderation in my country and most of the hate and abuse is counted as just another engagement metrics. reply satvikpendem 32 minutes agorootparentWatching CP and murdering people is in no way comparable to any of those bullet points you made. Generally in the US at least, uniquely among many nations, the principle and constitutional right of the freedom of speech reigns supreme over many, many others, so there is no chance that any of those bullet points would (or should, given such a principle) be regulated. One can and should be able to espouse those beliefs, regardless of whether they are true or not, because the alternative is much worse, where the rights of such exposition are severely curtailed. Hell, someone got arrested for taunting the Queen in the UK, something that legally cannot happen in the US had a similar person taunted a government official. reply graemep 1 hour agorootparentprev> If people feel these platforms are negatively impacting them, they should stop using them. The problem is how to stop the mob attacking you from using them. An American equivalent might be if social media existed a 100 years ago and was being used to encourage lynchings. Yes, it really is that bad in some places. The problem is that FB does moderate things relevant to the US but ignores the rest of the world. They will remove white supremacist material in the US, but not the equivalent elsewhere. reply satvikpendem 31 minutes agorootparentThe solution is in the problem, network effects. If everyone stopped using them due to deleterious effects, the problem is would solve itself. reply graemep 1 hour agorootparentprevI have seen some of the same with Sri Lankan posts. Loathsome stuff in Sinhala. Not calling for genocide, but definitely encouraging persecution and bigotry. One group that was particularly poisonous was removed after a campaign by many people. One person complaining gets nowhere. I am sure there is more similar material elsewhere. I think the underlying issue is that American companies view everything through the lens of American culture and if its not a problem in the US, then it is not offensive. I once reported a racist comment on FB. Someone said that people of their race should not \"interbreed\" with people of another race because the latter are evil. FB said it did not violate their community standards. IMO it was probably because it was a comment by a black person (probably American) about white people. That is not the major problem is the US so its fine. reply Teever 1 hour agoparentprev> if you don't like these products and feel they are negative, then don't use them What's the secret to doing that? No really. How do I unsubscribe from the tracking that these platforms do on the web and in my apps? That's the rub! I can't not use (or be used) by these platforms -- no one can! reply newzisforsukas 3 hours agoparentprev> Being exposed to shit on the internet teaches you there's bullshit on the internet, and not to believe everything you see Maybe it taught you that, but there are plenty of people that grow up on the Internet who do not learn these lessons. Take a look at any conspiracy message board, group, etc. Akin to saying something like, \"let it happen, that'll teach 'em\" Unfortunately, not everything works itself out. reply barrysteve 2 hours agoparentprevYou are not given a choice to opt out. You are included in an ever-increasing dragnet of surveillance. The incentives are set up for it. You either play the sisyphean game of personally blocking a billion dollar company from including you. Or you reach for the long arm of the law. We all know FAANG gave us no choice but to use government ruling. reply thefz 3 hours agoparentprev> And just like the Supreme Court wrote 30 years ago, the answer is the same today: if you don't like these products and feel they are negative, then don't use them. Restrict your children's access to these platforms. 100% my reply to any critic of teenage social media, but the parents' stance is always \"but then my kid is going to FOMO and feel left out\". reply hn_throwaway_99 11 hours agoprevI feel so bad for teenagers/kids who we essentially screwed over with this tech experience. That said, I don't feel great for the rest of us either! I feel like my phone has become a significant negative in my life. And in general, I'm quite scared because we have put things in place that we know are bad for mental health: 1. Even if we may not like it all the time, there is tons of data that show that personal interactions and relationships are good for mental health. With so much technology and so many things going remote (I'm not just talking jobs, but I'm talking about the fact that it's very easy, for example, to never need to walk into a store anymore. I recently went to a fast food restaurant and there were no customers inside at lunchtime, normally a busy time, and I ordered on a kiosk and everyone else just ordered at the drivethrough) it's harder and harder to just see random people and our friends without explicit planning. 2. As someone who recently got over a severe episode of depression, I strongly believe time spent in nature and just outside in general is really good for the mental health of humans. With so much tech it's easier and easier to basically never go outside unless you make it a point to do so. reply epolanski 2 hours agoparent> With so much technology and so many things going remote it's harder and harder to just see random people and our friends without explicit planning. The average american adult went from socializing with friends and family 12+ hours a week few decades ago to less than 4. reply brailsafe 11 hours agoparentprevYour points here are worth more emphasis. As a chronically unemployed software dev who's burnt out and crashed at least 3 times, I've spent a hell of a lot of time reflecting, and try my best to communicate these ideas to people who have the opposite problem; lots of work, but no new friends since highschool, and desperately single. People tend to rely far too heavily on the easiest way to convince themselves they're having valuable social interactions, whether it's social media or betting that their work friends will still be there when they get laid off. They'll rely on Tinder for sex and try to bridge that to something more meaningful out of thin air, or they'll buy a dog and hope that solves the problem. Some of these are uniquely millenial and onward, some others carry over from Gen X and boomer culture imo, whereby you isolate yourself from the rest of society in the suburbs or wherever and count on personal relationships you acquired for free. Along with this, in many places we've let the catalysts for social growth get stripped away by commodity bullshit and simulated interaction. Costco is probably the closest thing many people have to bumping into someone, no shot are they going to do it at the adult version of the playground, because there often isn't one and they won't go. (obviously this is more true in some places and for some people than it is for people who've realized this or who innately direct their life this way). My theory is that to meet a new person and have it be substantial, you basically need to spend a few hours, a few times per week, in the same space doing some arbitrarily interesting thing for a common reason, without being too eager but with a signaled sense of openness. You don't become a pro anything spending 30 min a week on it, and no valuable personal relationships come about that way either. That's how you met people in Uni, that's how you met people at work, you gotta branch off of those places and ya gotta keep it going gradually. If you don't live in a place that facilitates that, vote with your wallet and try to find a new one. This goes for nature too, if you're only exposure is 2 days of hiking once a year when you travel, and the rest is spent in an office, it's not something you can remedy any other way. If you drive to work 1 hour each way, and work 8 hours, you're probably doomed, unless you've already done all that and can keep your existing things going. It's just not enough margin, be real about what you're sacrificing and why. reply itronitron 1 hour agorootparentI agree with all of your points and would add that the metrification of social interactions degrades social connections as it fosters a bias towards competitiveness. Furthermore, the people that are put off by that reduce their participation so it becomes a market for lemons. As a solution to teenager anxiety I would propose a compromise solution wherein all school communications, school groups, and extracurricular activities must not use any social media platforms for communication. reply firewolf34 4 hours agorootparentprev> My theory is that to meet a new person and have it be substantial, you basically need to spend a few hours, a few times per week, in the same space doing some arbitrarily interesting thing for a common reason, without being too eager but with a signaled sense of openness. I like this concept, and I feel like I've experienced this as well, but I'm having trouble picturing an example of what you're describing, practically speaking, for the average city-dweller. Care to elaborate on this? reply rsanheim 2 hours agorootparentThis the much discussed “third place” that has all but disappeared in much of western life. See _Bowling Alone_ for a very early (pre social media) analysis of this idea. A third place could be a coffee shop, a bar, a church, a softball league, a book store, or even just a nice park. By and large people don’t go to these places nearly as much anymore, except to consume and leave. And if they do go there they are on their phones until they finish their transaction and leave. reply klyrs 4 hours agorootparentprevI kinda met a gal in a class I was taking at a community center. Couldn't tell if she's into me or just nice; didn't push it. Maybe I'll see her in another class in the future, but I'm there to learn. reply throwaway2037 1 hour agorootparentWhy is this downvoted? It answers the GP. It seems reasonable to me. Taking classes at the community center sounds like a good way to meet people. reply abnercoimbre 31 minutes agorootparentMade no sense to downvote him, it's actually relevant to the discussion. reply ryandrake 11 hours agoprevIf parents really want to set a good example, they ought to also quit their smartphones and Social Media--or at least hide it better. Kids are really good at identifying hypocrisy. When they see their parents glued to their phones and scrolling Instagram all day, after being told \"Oh, don't do that, it's not good for you\" they know their parents are full of shit. And, the",
    "originSummary": [
      "The debate on social media's impact on teenage mental health is ongoing, with researchers divided on whether it minimally harms or causes mental health issues.",
      "Evidence indicates a potential causal link between heavy social media use, especially among girls, and mental illness in teenagers.",
      "Proposed solutions include restricting social media access, implementing phone-free schools, and addressing broader societal factors to enhance teen mental health."
    ],
    "commentSummary": [
      "The text highlights the detrimental impact of social media on mental health, especially concerning teenagers and children.",
      "It underscores the importance of parental involvement in establishing boundaries and promoting personal accountability when using social media.",
      "The debate delves into balancing free speech with regulation, advocating for offline activities to mitigate the negative effects of excessive online engagement."
    ],
    "points": 430,
    "commentCount": 438,
    "retryCount": 0,
    "time": 1712690972
  },
  {
    "id": 39980222,
    "title": "PriceLevel: Unveiling Companies' SaaS Costs",
    "originLink": "https://www.pricelevel.com/",
    "originBody": "Hey HN! Christine and Steven here. As a PM and engineer, we’ve both evaluated and purchased a lot of software. One of the biggest frustrations was figuring out how much it would cost us without having to go through the sales process. When we did have a quote, we had no idea if we were getting a good deal or ripped off.We built a site where you can see what other companies are actually paying for SaaS and enterprise software. Buyers contribute prices via quotes, pricing proposals, and other documentation to ensure quality.We unlocked Talkdesk for Show HN users so that you can use the product without needing to sign in or upgrade. Check it out at https:&#x2F;&#x2F;www.pricelevel.com&#x2F;showhn. Would love to hear any feedback, thank you!",
    "commentLink": "https://news.ycombinator.com/item?id=39980222",
    "commentBody": "We built PriceLevel to find out what companies pay for SaaS (pricelevel.com)401 points by cluo21 14 hours agohidepastfavorite113 comments Hey HN! Christine and Steven here. As a PM and engineer, we’ve both evaluated and purchased a lot of software. One of the biggest frustrations was figuring out how much it would cost us without having to go through the sales process. When we did have a quote, we had no idea if we were getting a good deal or ripped off. We built a site where you can see what other companies are actually paying for SaaS and enterprise software. Buyers contribute prices via quotes, pricing proposals, and other documentation to ensure quality. We unlocked Talkdesk for Show HN users so that you can use the product without needing to sign in or upgrade. Check it out at https://www.pricelevel.com/showhn. Would love to hear any feedback, thank you! passwordoops 12 hours agoLove this! As a consumer, there's nothing like open pricing. However, as a provider I can totally see a situation where I (proverbially, I'm not in this business) sue you for disclosing what amounts to a trade secret (depending on what's in the fine print) and compel you to give up all documents so I can go after my loose-lipped client too. I hope you've got your legal bases covered reply mox1 10 hours agoparentJust to let you know this kind of pricing information is available and given out during F500 SaaS deals. Our microsoft reseller was using pricing and contract info on a deal he closed last week to assure us he would and could get us a similar deal. Gartner will literally cutthtoat re-negotiate any large contract you have, using pricing data they have gathered from their members. Call them up tomorrow, tell them your current cost for Splunk, they will tell you exactly how much you can save. So this type of thing is not illegal and not even frowned upon for the big players. reply jpgvm 4 hours agorootparentOnce you are big enough to have a procurement department this isn't just not frowned upon it's expected and very much \"part of the game\". I personally hate it but it's just how the game is played and it's why all enterprise software has stupidly high advertised prices so they can give \"90% discounts\" and let people think that is a good deal when it is ultimately just the real price that all the big players are getting anyway. reply pcl 4 hours agorootparentWhen I worked at BEA (which I think published its prices), the steep discounts on a crazy high price actually had a clever reason — evidently the sales team could discount the list price pretty freely, but they were forbidden to discount the yearly support and maintenance fees. Sorta subscription pricing before it was cool. reply matt-p 3 hours agorootparentprevThere's a difference between 'a company negotiates on behalf of a number of clients and individuals happen to get a feeling for ballpark figures' verses publishing the exact figures of every deal on the internet. reply vasco 3 hours agorootparentprevMy practical experience with vendor negotiations is you ask for a discount and talk to them honestly and make the sales person have a good time with you and you don't need to play hardball, say you have some secret information or other crap. I have seen many contracts negotiated by a regular engineering manager and by \"proper\" procurement people, and the outcome is clear that hardball negotiations end up in worse outcomes than just being a nice person and speaking plainly. reply mickael-kerjean 12 hours agoparentprevLoving this as well from the point of view of a small saas provider who has no idea what typical enterprise contract runs for. reply blackeyeblitzar 10 hours agoparentprevI’m not sure you can sue to block price signals and that seems blatantly anti competitive. Also other services provide this type of intelligence already, like Vendr, Sastrify, Torii, etc. reply instagib 8 hours agoparentprevMost of the quotes I get have clauses about not disclosing them to others. Prices may be okay but the entire quote can include drawings, specific item information, etc. I had to get a lot of quotes and talking to legal is painful. reply uoaei 12 hours agoparentprevIs there precedent that points in either direction that pricing details are considered a trade secret (or whatever the legally enforceable version of that is)? reply staticautomatic 4 hours agorootparentSure. Broadly, a trade secret under state and federal law is \"information that derives independent economic value from not being generally known to the public or people who can obtain economic value from it and is the subject of efforts to maintain its secrecy that are reasonable under the circumstances.\" Price lists could clearly fall under this definition, and they do! Here's an 8th circuit case where the plaintiff \"adequately allege[d] that the information that was purportedly misappropriated— ... pricing information ... qualifies as trade secret[].\" Ahern Rentals, 59 F.4th 948 (2023). reply passwordoops 11 hours agorootparentprevI've signed and written quotes that have those provisions. What the legal standing is, I can't comment on because it never came to that. This is with the obvious caveat that public tenders are the exception, but only after it's awarded reply spxneo 10 hours agoparentprevprices aren't a trade secret and difficult to enforce even if its covered under an NDA reply staticautomatic 4 hours agorootparentThey totally can be. There's plenty of case law on it. Source: I'm a 3L about to graduate with an IP specialization and got an award for being the best student in my trade secret law class. reply spxneo 4 hours agorootparentcan you share some? reply staticautomatic 4 hours agorootparentHere's 4 from the first page of results on WestLaw: - Ahern Rentals, 59 F.4th 948 (2023) - Wilmington Star-News, 125 N.C. App. 174 (1997) - Ingram, 260 Md. App. 122 (2023) - Intertek Testing Servs., 443 F. Supp. 3d 303 (2020) reply passwordoops 10 hours agorootparentprev\"prices aren't a trade secret\" Is there a basis for this in recent case law? And how difficult would it be to enforce against a 3rd party actively publishing pricing, and therefore competitive, strategy? Nope I'm not staying they should shut down or they will get sued, but they should do the sensible thing and talk to a good lawyer reply makeitdouble 8 hours agorootparentNot a lawyer either, but I think there is a more obvious angle to this: publicly traded companies can be audited, publish costs to investors, list their creditors and amount when going bankrupt, ask for tax rebates by declaring their expenses etc. In many circumstances their money flow needs to be transparent. Not being allowed to expose the buying price of a service would go against many of these procedures/obligations. We could look at it the same way disclosing one's salary can't be illegal: how do you then apply for a loan for instance ? reply spxneo 10 hours agorootparentprevthere hasn't been any caselaw from my knowledge where they went after a customer for revealing the price of a software license prices wouldn't be enforceable because knowing how much you charge isn't revealing anything proprietary (because you have to tell your customers in order to get them to make a decision). Also not exactly possible to expect confidentiality around prices because your banks and their staff will see it.. reply passwordoops 10 hours agorootparentJust to be clear, we both agree we're not lawyers But I can argue that pricing is part of my strategy and revealing pricing information in cases where I explicitly forbade it is potentially damaging because it allows a competitor to undercut me. I'd be surprised if they do go after a client, but my concern for these guys is they are not a client. They are an aggregator of this information. If any of these quotes were given with the caveat that pricing should not be shared, then a named company who's on the pricier side might have a good leg to stand on arguing this site damaged their business with what amounts to a trade secret (pricing strategy). At three very least or could result in a letter and headaches. Asking for who gave the information is something I would totally see too, especially if this was a client who jumped ship, if anything else just as a scare tactic/revenge reply thayne 7 hours agorootparent> it allows a competitor to undercut me So, allows them to compete with you... IANAL, and don't know how the law actually applies to this, but prohibiting customers or potential customers from disclosing price information clearly seems anti-competitive to me. And in some ways it is even worse than giving you an advantage for you over your competitors, if all your competitors also keep pricing a trade secret. In that case it gives you and your \"competitors\" an advantage against the customer, because the customer can't effectively compare prices, at least without expending considerable effort, which can result in the customer paying more. Especially for smaller companies with less negotiating power. reply airstrike 7 hours agorootparentprevalso not a lawyer, but unless your contract includes specific confidentiality around the price, I don't think disclosing it is an issue. it will be very, very hard to argue it's a trade secret because it is not, in fact, a secret reply e12e 9 hours agorootparentprev> Also not exactly possible to expect confidentiality around prices because your banks and their staff will see it.. There's a big difference between seeing data in the course of your job, and being legally able to share it! reply unstatusthequo 11 hours agoparentprevPricing isn’t a trade secret. reply passwordoops 10 hours agorootparentBest you consult a lawyer on the subject first. Trade secrets can certainly include \"Pricing strategies and information\" (1) (1) https://www.lplegal.com/content/what-are-trade-secrets-and-h... reply OccamsMirror 2 hours agorootparentPricing strategy isn't the same thing as a price you gave a client. reply nosefurhairdo 10 hours agorootparentprevI'm not a lawyer, but it seems possible that an enterprise software licensing agreement could contain a non-disclosure agreement or confidentiality clause. Not making a value judgement and happy to be corrected if this is not a thing. reply willcipriano 10 hours agorootparentSounds like something the FTC should investigate. reply mediaman 8 hours agorootparentWhat are they going to investigate? The FTC is going to render contractual terms negotiated in good faith between well-represented and sophisticated commercial buyers “just because”? If a big buyer wants better terms, the seller might give it to them under the condition the pricing info isn’t leaked, spoiling the rest of their market. Otherwise they don’t give it. So the buyer agrees to confidentiality to get the best terms. There’s zero chance the FTC gets in the middle of this kind of negotiation. reply thayne 6 hours agorootparentOne might be able to argue that by keeping the pricing secret the big buyer is colluding with the seller to give the seller an advantage in in negotiating prices with other buyers, which could include competitors of the big buyer. I think that would be unlikely to go anywhere with the current FTC though. reply airstrike 7 hours agorootparentprevbut then that's a breach of an NDA which is not the same as sharing a quote-unquote \"trade secret\" reply RowanH 11 hours agoparentprevI can totally see some uninformed staff member contributing what they're paying for things, and landing their employer in deep, deep, trouble with their suppliers. I've found in competitive price analysis there's some companies that do very, very, well at hiding their pricing. If they've worked that hard, that long, to protect/hide pricing they're not going to take it lying down. reply sublimefire 2 hours agoprevThe advice I had related to pricing which captures the gist. If the website does not show prices and asks you to call then expect to pay at least a five figured sum. reply styren 2 hours agoparentmonthly or annually? reply codegeek 1 hour agorootparentAnnually to start with because most prices are quoted annually for mid to enterprise size deals. reply a13n 12 hours agoprevAs a buyer, it's definitely nice to have more data on this. Pricing can be very opaque and sometimes I just want to know the ballpark to know if it's even worth exploring. Annoying to spend 30 minutes on a call to get that when you could have it in 2 seconds. As an operator, I know a lot of SaaS agreements include language saying the customer will protect the vendor's confidential data including information about pricing. I doubt it's legal in the majority of the time for buyers to share this data. And if the buyer is sharing confidential data with you, you're likely taking on risk by sharing it publicly, even in aggregate form. You could add something saying that the buyer agrees that they have the legal rights to share the information with you, to protect yourself. But at some point you should know that the majority of the time, even if that box is checked, they don't. reply amelius 11 hours agoparentThe buyer is already sharing that information with their bank, so the confidentiality is already broken :) reply toomuchtodo 10 hours agorootparentBanks with business accounts is who this company should partner with. Sidesteps any contractual remedies a seller might attempt. Opt in of course for the customer. reply Quarrel 10 hours agorootparentBanks don't know the terms of the contract, nor the customer specifics, usually. Sure, they get some good insight, but they don't know how many seats, what level of support, etc etc. They just see a transaction $X from company A -> company B. reply e12e 9 hours agorootparentprevIsn't this like saying that if you fulfill your prescription at a pharmacy, your medical data is open? reply esafak 12 hours agoprevI think you should fuzz the prices and other specifics like the number of seats a bit for privacy. For even more privacy you could show faceted aggregates with error bars instead of individual accounts once your samples are big enough. reply cluo21 12 hours agoparentThat's a fair point. Just to make sure I understand, when you say fuzz - do you mean round up or down so it's a nice whole number? reply bigiain 10 hours agorootparentI'd round it to probably just 2 significant digits. I don't care if it's $91,132 or $90,725 or $91,412 - all of those are effectively the same as $91k (unless I'm the vendor trying to work out who's leaking my pricing). What I'm looking for as a potential new customer is to see if it's going toi cost me ~200k or ~90k or ~10k or ~3k. reply esafak 12 hours agorootparentprevI would show an interval that's within 5% or whatever, but use the same intervals for all accounts otherwise people will be able to deduce the original number by calculating the midpoint :) reply cluo21 12 hours agorootparent:thumbs-up: reply ziziyO 12 hours agoprevSubmitting fake overpriced quotes for my competitor’s products reply e12e 9 hours agoparentAh, clever. Committing document fraud? > Buyers contribute prices via quotes, pricing proposals, and other documentation to ensure quality. reply hipadev23 9 hours agorootparent> Committing document fraud Stop using words that makes it sounds like a crime. It’s a survey. Nobody is under any legal requirement to provide correct info. reply gruez 7 hours agorootparent>It’s a survey. Nobody is under any legal requirement to provide correct info. A review is arguably just a \"survey\" as well, but if you provide incorrect info (ie. lie) you can be sued for defamation. reply kaptainscarlet 3 hours agorootparentprevNot really. It's a sarcastic comment to highlight a potential loophole. reply debarshri 1 hour agoprevThis is one of the most amazing products I have seen in long time in HN. This is simple but super valuable! One thing, I would recommend is to add some indicators that can help us trust the information or the data you are providing. It would go a long way then. reply Scorpiion 12 hours agoprevLooks like a nice service, it also sounds like something the SaaS companies who's pricing information you are showing would not like. I'm curious how you look at that? Are there any legal risks for end users to share this type of information via your site? I'm thinking if the SaaS providers have some legal statements about not sharing prices (similar to how certain database providers don't allow sharing of database benchmarks). reply cluo21 12 hours agoparentBuyer anonymity is mission critical for us and we've taken some steps to do that like generalizing company size and industry. There's been some great feedback in this thread (thanks everybody!) about additional fuzzing we can do to protect our buyers that we'll implement. While there may be some SaaS companies who don't like this level of transparency, we hope this actually leads to a faster sales cycle because buyers have pre-qualified themselves. reply lvspiff 12 hours agorootparent> we hope this actually leads to a faster sales cycle because buyers have pre-qualified themselves. But by obfuscating their pricing they are able to create a marketing \"lead\" when you contact them that will one day evolve into a client and therefor its better for them to \"pre-qualify\" you. I personally hate this model as it drives me absolutely insane to spend a month of going back and forth with a vendor, going over what i'd like to do and why, investing time in understanding their stack and product, only to find out its 2x my budget or something like they wont even consider me since its less than 10k. If you hide your pricing, make me go through an extensive process, and I let you know im trying to implement a PoC or a small entry level and you come at me with $1k/month I'm likely going to walk away and be frustrated enough to not want to do business with you ever. You've created more of a negative experience that i will tell others about than a \"lead\". Yet this goes against every companies thinking (even my won) so it must work on someone. reply bigiain 10 hours agorootparent> If you hide your pricing, make me go through an extensive process, and I let you know im trying to implement a PoC or a small entry level and you come at me with $1k/month I'm likely ... ... to see if I can replicate the important parts of your SaaS for my needs, then open source it or start a competitor with a sane and transparent pricing structure to capture the rest of the market like me. reply bigiain 10 hours agorootparentprevMaybe you've done this already, but from here it looks like you're publishing 5 or 6 significant digits of someone's pricing. I know if you did that with my companies product, we'd be able to identify a customer with very very high accuracy from that. Unless \"$91,132\" is an indicative but somewhat randomised number, I'd strongly suggest listing that as \"$91k\" to reduce the information leakage about which customer gave you that number. (Same with the number of seats, although it looks like there's less entropy in this numbers at a quick glance.) reply tomrod 9 hours agoprevPrice per what unit? For example, checked out Hubspot, the price could be monthly but it's not clear to me. reply astrodust 8 hours agoparentI'd assume monthly but I'd rather not have to assume. reply deathanatos 7 hours agorootparentAnd I'd've assumed $/yr. Otherwise … $1M/y for Slack? I figure that's probably too high … or someone is getting fleeced. But yeah, the number of times I've seen management spreadsheets doing math on \"$\" where \"$\" is a weakly-typed mix of $/mo & $/yr, and then the \"result\" used to \"justify\" decisions… reply a5seo 12 hours agoprevReally great job. Whether it’s legal for customers to share this info or not is really a gray area. If the data is only shared in an aggregate fashion, I doubt they can do much without a subpoena. And then what? Sue the website? Sorry, no. Section 230. John Doe suits against anonymous customers? Nothing requires PriceLevel to retain the PII of users… they can capture the data, validate, and flush the PII. “Sorry, we have no information about the contributor of this data.” My sense is this will be the primary innovation of this service— how to get this info and keep it useful to end users without very much ability to vet it. Worth the effort. reply bigiain 9 hours agoparent> John Doe suits against anonymous customers? How many TalkDesk customers do you suppose there are paying $91,132 for 62 users? I'd guess TalkDesk know exactly who that is. (I doubt that makes it any easier to prevail if they try to sue them, but I wouldn't want to be the customer negotiating next years contract wit them. \"Hi, it's TalkDesk account management here. Just letting you know your contract expires at the end of next month. Here's a new contract for the following 12 months, with the special for you pricing of $425,762 for your 62 seats. Hope to hear from you soon, and have a nice day!\" reply kaiwenwang 7 hours agoprevReminds me of machowski's article on order books. By making these things public, I think it'll decrease the variance in costs in the space. If it was public knowledge how similar SaaS are priced, people would always go for the cheaper option (this assumes identical features, which is not the case). I guess in the end, pricing is determined by costs (materials, labor, rents) for a floor and then profits (which might as well be a random number). https://www.machow.ski/posts/2021-07-18-introduction-to-limi... reply mtillman 4 hours agoprevI pay for hubspot quarterly and still have no idea how their contract works or how much I'm supposed to be paying without letting our contract audit DNN have a whack at it. Completely opaque order forms and value. I'm convinced we should just be using a google sheet like wework did until they went public. reply moomoo11 4 hours agoparentTotally. Spreadsheets are king. reply cdchn 11 hours agoprevThought this was a great idea until I saw you had to create an account to see details. Seems pretty antithetical to the entire premise. reply joshuaturner 7 hours agoparentGonna make a new site that just embeds a Google Sheet and lists all the pricing details - PriceLevelLevel.com reply ed_mercer 9 hours agoparentprevWould you give valuable information away for free? Of course not. reply komali2 9 hours agoparentprevI think basically all non-PII data should be Free as in Freedom, but this is an interesting business problem. You can view it for free, if you submit 5 pricing datapoints of your own. If it was all free, I suppose they'd have to reduce whatever friction they have in authenticating data input (I'm assuming they ask to see invoices or contracts for now), and run the risk of competitors pricing eachother up or just bad data being entered. reply mistrial9 9 hours agoparentprevI think that the entire net economy has to make new social contracts about price discovery.. somewhere between Debian and Oracle.. for example.. and I support open data generally. But how is it antithetical to make a service for site customers with a login? given high-powered robots on the net and very unexpected guests as a fact-of-life on the network.. this seems more like locking your door at night in a mid-sized town, not some deep betrayal of cause or rank hypocrisy .. no? reply 0x6461188A 10 hours agoprevHow do you know people are entering pricing info honestly? How do I trust anonymous respondents? reply giovannibonetti 9 hours agoparentNot OP, but I suspect they wait until they have lots of quotes from different sources and then pick the median. reply rokhayakebe 9 hours agoparentprevYou can make sure people who sign up use their company email. If they are found to add wrong information you can block their account. You can also give companies an opportunity to reply to a post. reply xlbuttplug2 3 hours agorootparent> If they are found to add wrong information you can block their account. How would you prove that a submission is not truthful? You can check if it's an outlier but that's definitely not foolproof. reply swyx 7 hours agorootparentprevinteresting that this and Blind dont seem to use zero knowledge proofs when the need for provable knowledge w/ privacy would seem super useful reply hobofan 3 hours agorootparentRelated to that, I know that the SINE foundation[0] has been investigating using zero knowledge proofs for benchmarking. I also looks like they recently released a tool concretely to allow for privacy preserving benchmarking[1]. (I haven't looked into the the contents of the repo itself to check whether they are actually using zero knowledge proofs). [0]: https://sine.foundation [1]: https://github.com/sine-fdn/sine-benchmark reply komali2 9 hours agoparentprevIt looks like you can trade insights for an unlimited account, so I'm assuming they actually vet your claims, e.g. ask for identifying information on who you are and what your company is, and ask to see evidence e.g. invoices, contracts. I mean, that's how I'd do it if I was wanting to build a platform with trustworthy, essentially canonical pricing lists, and charging a couple hundred bucks a year for people to see that info. reply apimade 12 hours agoprevAdd region or locations immediately. Single biggest factor outside of customer size or how much product they’re buying. Also contract length. Pretty common to get deep discounts with multi year agreements. reply myrloc 12 hours agoparentUseful info. Might want to make the ordered quantity and contract length ranges though, re: fuzzing anything that might identify the client reply cluo21 12 hours agoparentprevMm, yea - that's a buyer attribute we can add to clarify. And definitely! We do have contract length, though that's locked away as a Pro feature reply RileyJames 11 hours agoprevGreat idea, great site. I signed up. But post-sign up on the modal explaining how it works I was unable to scroll below “Upgrade”, in order to reach the continue button. iPhone SE2. Keen to use this. Having just been through the dance to obtain Drata, I’d love to just know the price ahead of time. I know there was a YC company many years ago that “negotiated saas pricing as a service”. The pain is not just knowing the price, it’s also: - what features and versions of features are included in why I’m paying for?? - what is the value of the extra bs that’s being packaged in, and can I negotiate it away? Or how can I make use of it? I feels there’s a lot of value in a community of recent buyers of a product being able to communicate openly. Any company that takes actions against their own customers for trying to make the most of their product should be replaced. reply cluo21 11 hours agoparentOof, thank you for telling us about the modal and the iphone model - we're on it. reply neilv 3 hours agoprevLooks like you're leaking marketing data about your visitors to clearbit.com. reply nen-nomad 8 hours agoprevThis looks like a handy service for anyone evaluating enterprise software. Is there any normalization done on the pricing to account for things like contract duration, number of seats, feature tiers, etc? Or is it just the raw pricing data? reply trojanalert 4 hours agoprevBloody life saver! So hard to find this, and all I want is a basic understanding. Thank you! reply cjonas 5 hours agoprevAm I missing something is or Salesforce (the og SaaS) missing from your list? reply mikpanko 10 hours agoprevHow do you verify accuracy of contributions? What stops somebody from submitting fake (overpriced) numbers for companies they don’t like (competitors)? reply spxneo 10 hours agoparentthis is what I want to know and somebody in the comments already said they are going to do exactly that. reply hdaz0017 11 hours agoprevI am not sure where the information was gained, but I believe this is only a fraction of the overall costs for an organisation. (probably limited number of account(s)). reply guidedlight 11 hours agoprevGood idea. It needs to be clearly global though. reply tumidpandora 9 hours agoprevHow do you keep something like this up to date? Not like you could scrape it from somewhere… reply 0cf8612b2e1e 12 hours agoprevSo simple but good. Now we need a similar thing that publishes database benchmarks in spite of DeWitt. Use a few standardized datasets and boom. reply piterrro 12 hours agoprevWhat is the incentive to contribute and how do you check the legitimacy of the numbers provided? reply cluo21 11 hours agoparentContributing data is one method to gain full access to our pricing data set. Alternatively, users can also pay for access. Regarding quality, we're manually reviewing every data point for credibility before making it available. reply esafak 12 hours agoparentprevI imagine it plays into the pricing. reply smcleod 13 hours agoprevThis is a great idea. Nice work! reply cluo21 13 hours agoparentThank you! reply orliesaurus 12 hours agoprevThis is COOL! Finally some transparency honestly it was much needed reply anotherhue 12 hours agoprevSee also: https://ssotax.org/ reply xtracto 12 hours agoparentThis is also a great site! I remember there was a similar one but for \"open source\" (heavy quotes) self-hosted software. I find those sooo shady. I also remember there was some Nginx proxy or similar that enabled SSO in several of those applications. reply aleks5678 11 hours agoparentprevHow much does Vendr cost? Their own pricing is not transparent. reply anotherhue 11 hours agorootparentI don't remember, I feel like it might have had some connection to the savings but it's been a while. reply webprofusion 6 hours agoprevThis is cool, if we just have straight up pricing for a product (no custom pricing) can we just publish that so people can see it on the site? Specifically would like to list pricing https://certifytheweb.com/register (Certificate Management) reply ramonverse 8 hours agoprevCool idea, hope it grows! reply j45 11 hours agoprevGreat idea, congrats on the launch - valuable to learn where pricing deals end. SaaS sales can be guilty of drive-by sales, where the implementation fails to reach promises. It puts a black eye too often on good products. reply xsc 9 hours agoprevadblockers block logo.clearbit.com reply techcofounder 11 hours agoprevthis is cool! but how is it diff than vendr? reply aleks5678 11 hours agoparentHow much does Vendr cost? Their own pricing is not transparent. reply batch12 11 hours agoparentprevIt's spelled correctly reply moralestapia 9 hours agoprevThis is really nice and I like it a lot, disregarding of the \"transparency\" thing, I completely loathe when I have to \"call to get a quote\" for a SaaS. If you can quote a launch to orbit online [1], you can abso-fucking-lutely give me a concrete price for your relatively simple SPA when I'm browsing through it. A suggestion, the price is unclear, is it per month? per year? one time? per the events described? Make that clear. Congrats for shipping! 1: https://www.spacex.com/rideshare/ reply nine_zeros 10 hours agoprevVery nice indeed. A suggestion - I cannot tell if the pricing is per contract/per month/per year etc. Would be great to be able to collect all this info and then slice and dice it. reply enahs-sf 7 hours agoprevlevels.fyi for pricing. Love it. reply lettergram 9 hours agoprevlol this is why everything is in an NDA upfront. For us, every customer has pretty unique, so even two feature sets that look the same might have wildly different support agreements. reply yieldcrv 10 hours agoprev [–] pricing for open pricing! now this is what I come to HN for reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Christine, a Project Manager, and Steven, an engineer, have developed a platform showcasing actual prices companies pay for software, focusing on transparency for buyers.",
      "They are providing Talkdesk access to Show HN users without the need for sign-in or upgrades to gather feedback.",
      "Users can visit https://www.pricelevel.com/showhn to explore the site and share their thoughts."
    ],
    "commentSummary": [
      "PriceLevel, founded by Christine and Steven, aims to offer transparency in SaaS pricing, assisting users in understanding what other companies pay and aiding in negotiations.",
      "Debates center on the legality and ethics of sharing pricing data, treating pricing as trade secrets, and the repercussions of preventing customers from divulging pricing information.",
      "Discussions include privacy strategies, protecting buyer identities, and the advantageous role of clear pricing in streamlining the sales process, despite concerns about contract complexity and data accuracy."
    ],
    "points": 402,
    "commentCount": 113,
    "retryCount": 0,
    "time": 1712675067
  },
  {
    "id": 39981032,
    "title": "Intel Unveils Gaudi 3 AI Accelerator for Enhanced AI Compute",
    "originLink": "https://www.intel.com/content/www/us/en/newsroom/news/vision-2024-gaudi-3-ai-accelerator.html",
    "originBody": "News April 9, 2024 Contact Intel PR Follow Intel Newsroom on social: More Artificial Intelligence News Intel introduced the Intel Gaudi 3 AI accelerator on April 9, 2024, at the Intel Vision event in Phoenix, Arizona. The accelerator delivers 4x AI compute for BF16 and 1.5x increase in memory bandwidth compared with its predecessor. (Credit: Intel Corporation) Intel introduced the Intel Gaudi 3 AI accelerator on April 9, 2024, at the Intel Vision event in Phoenix, Arizona. The accelerator meets the unserved demands for choice while offering versatility through open and community-based software and open industry-standard Ethernet, helping businesses flexibly scale their systems. (Credit: Intel Corporation) Intel introduced the Intel Gaudi 3 AI accelerator on April 9, 2024, at the Intel Vision event in Phoenix, Arizona. It is designed to bring global enterprises choice for generative AI, building on the performance and scalability of its Gaudi 2 predecessor. (Credit: Intel Corporation) Intel tackles the generative AI gap by introducing the Intel Gaudi 3 AI accelerator at the Intel Vision event on April 9, 2024, in Phoenix, Arizona. Gaudi 3 gives customers choice with open community-based software and industry-standard Ethernet networking to scale their systems more flexibly. (Credit: Intel Corporation) Intel tackles the generative AI gap by introducing the Intel Gaudi 3 AI accelerator at the Intel Vision event on April 9, 2024, in Phoenix, Arizona. Gaudi 3 gives customers choice with open community-based software and industry-standard Ethernet networking to scale their systems more flexibly. (Credit: Intel Corporation) Download all images (ZIP, 27 MB) What’s New: At Intel Vision, Intel introduces the Intel® Gaudi® 3 AI accelerator, which delivers 4x AI compute for BF16, 1.5x increase in memory bandwidth, and 2x networking bandwidth for massive system scale out compared to its predecessor – a significant leap in performance and productivity for AI training and inference on popular large language models (LLMs) and multimodal models. Building on the proven performance and efficiency of the Intel® Gaudi® 2 AI accelerator – the only MLPerf-benchmarked alternative for LLMs on the market – Intel gives customers a choice with open community-based software and industry-standard Ethernet networking to scale their systems more flexibly. “In the ever-evolving landscape of the AI market, a significant gap persists in the current offerings. Feedback from our customers and the broader market underscores a desire for increased choice. Enterprises weigh considerations such as availability, scalability, performance, cost, and energy efficiency. Intel Gaudi 3 stands out as the GenAI alternative presenting a compelling combination of price performance, system scalability, and time-to-value advantage.” –Justin Hotard, Intel executive vice president and general manager of the Data Center and AI Group Why It Matters: Today, enterprises across critical sectors such as finance, manufacturing and healthcare are rapidly seeking to broaden accessibility to AI and transitioning generative AI (GenAI) projects from experimental phases to full-scale implementation. To manage this transition, fuel innovation and realize revenue growth goals, businesses require open, cost-effective and more energy-efficient solutions and products that meet return-on-investment (ROI) and operational efficiency needs. The Intel Gaudi 3 accelerator will meet these requirements and offer versatility through open community-based software and open industry-standard Ethernet, helping businesses flexibly scale their AI systems and applications. How Custom Architecture Delivers GenAI Performance and Efficiency: The Intel Gaudi 3 accelerator, architected for efficient large-scale AI compute, is manufactured on a 5 nanometer (nm) process and offers significant advancements over its predecessor. It is designed to allow activation of all engines in parallel — with the Matrix Multiplication Engine (MME), Tensor Processor Cores (TPCs) and Networking Interface Cards (NICs) — enabling the acceleration needed for fast, efficient deep learning computation and scale. Key features include: AI-Dedicated Compute Engine: The Intel Gaudi 3 accelerator was purpose-built for high-performance, high-efficiency GenAI compute. Each accelerator uniquely features a heterogenous compute engine comprised of 64 AI-custom and programmable TPCs and eight MMEs. Each Intel Gaudi 3 MME is capable of performing an impressive 64,000 parallel operations, allowing a high degree of computational efficiency, making them adept at handling complex matrix operations, a type of computation that is fundamental to deep learning algorithms. This unique design accelerates speed and efficiency of parallel AI operations and supports multiple data types, including FP8 and BF16. Memory Boost for LLM Capacity Requirements: 128 gigabytes (GB) of HBMe2 memory capacity, 3.7 terabytes (TB) of memory bandwidth and 96 megabytes (MB) of on-board static random access memory (SRAM) provide ample memory for processing large GenAI datasets on fewer Intel Gaudi 3s, particularly useful in serving large language and multimodal models, resulting in increased workload performance and data center cost efficiency. Efficient System Scaling for Enterprise GenAI: Twenty-four 200 gigabit (Gb) Ethernet ports are integrated into every Intel Gaudi 3 accelerator, providing flexible and open-standard networking. They enable efficient scaling to support large compute clusters and eliminate vendor lock-in from proprietary networking fabrics. The Intel Gaudi 3 accelerator is designed to scale up and scale out efficiently from a single node to thousands to meet the expansive requirements of GenAI models. Open Industry Software for Developer Productivity: Intel Gaudi software integrates the PyTorch framework and provides optimized Hugging Face community-based models – the most-common AI framework for GenAI developers today. This allows GenAI developers to operate at a high abstraction level for ease of use and productivity and ease of model porting across hardware types. Gaudi 3 PCIe: New to the product line is the Gaudi 3 peripheral component interconnect express (PCIe) add-in card. Tailored to bring high efficiency with lower power, this new form factor is ideal for workloads such as fine-tuning, inference and retrieval-augmented generation (RAG). It is equipped as a full-height form factor at 600 watts, with a memory capacity of 128GB and a bandwidth of 3.7TB per second. Intel Gaudi 3 accelerator will deliver significant performance improvements for training and inference tasks on leading GenAI models. Specifically, the Intel Gaudi 3 accelerator is projected to deliver on average versus Nvidia H100: 50% faster time-to-train1 across Llama2 7B and 13B parameters, and GPT-3 175B parameter models. 50% faster inference throughput2 and 40% greater inference power-efficiency3 across Llama 7B and 70B parameters, and Falcon 180B parameter models. An even greater inference performance advantage on longer input and output sequences. 30% faster inferencing4 on Llama 7B and 70B parameters, and Falcon 180B parameter models against Nvidia H200. About Market Adoption and Availability: The Intel Gaudi 3 accelerator will be available to original equipment manufacturers (OEMs) in the second quarter of 2024 in industry-standard configurations of Universal Baseboard and open accelerator module (OAM). Among the notable OEM adopters that will bring Gaudi 3 to market are Dell Technologies, Hewlett Packard Enterprise, Lenovo and Supermicro. General availability of Intel Gaudi 3 accelerators is anticipated for the third quarter of 2024, and the Intel Gaudi 3 PCIe add-in card is anticipated to be available in the last quarter of 2024. The Intel Gaudi 3 accelerator will also power several cost-effective cloud LLM infrastructures for training and inference, offering price-performance advantages and choices to organizations that now include NAVER. Developers can get started today with access to Intel Gaudi 2-based instances on the developer cloud to learn, prototype, test, and run applications and workloads What’s Next: Intel Gaudi 3 accelerators' momentum will be foundational for Falcon Shores, Intel’s next-generation graphics processing unit (GPU) for AI and high-performance computing (HPC). Falcon Shores will integrate the Intel Gaudi and Intel® Xe intellectual property (IP) with a single GPU programming interface built on the Intel® oneAPI specification. More Context: Intel Unleashes Enterprise AI with Gaudi 3, AI Open Systems Strategy and New Customer Wins (News)Intel Gaudi 3 AI Accelerator (Product Page)Intel Gaudi 3 AI Accelerator (White Paper)Intel Gaudi 2 Remains Only Benchmarked Alternative to NV H100 for GenAI Performance (News) The Small Print: Intel does not control or audit third-party data. You should consult other sources to evaluate accuracy. 1 NV H100 comparison based on: https://developer.nvidia.com/deep-learning-performance-training-inference/training, Mar 28th 2024 à “Large Language Model” tab Vs Intel® Gaudi® 3 projections for LLAMA2-7B, LLAMA2-13B & GPT3-175B as of 3/28/2024. Results may vary 2 NV H100 comparison based on https://nvidia.github.io/TensorRT-LLM/performance.html#h100-gpus-fp8 , Mar 28th, 2024. Reported numbers are per GPU. Vs Intel® Gaudi® 3 projections for LLAMA2-7B, LLAMA2-70B & Falcon 180B projections. Results may vary. 3 NV comparison based on https://nvidia.github.io/TensorRT-LLM/performance.html#h100-gpus-fp8 , Mar 28th, 2024. Reported numbers are per GPU. Vs Intel® Gaudi® 3 projections for LLAMA2-7B, LLAMA2-70B & Falcon 180B Power efficiency for both Nvidia and Gaudi 3 based on internal estimates. Results may vary. 4 NV H200 comparison based on https://nvidia.github.io/TensorRT-LLM/performance.html#h100-gpus-fp8 , Mar 28th, 2024. Reported numbers are per GPU.Vs Intel® Gaudi® 3 projections for LLAMA2-7B, LLAMA2-70B & Falcon 180B projections. Results may vary. Tags Artificial Intelligence, Data Center & HPC About Intel Intel (Nasdaq: INTC) is an industry leader, creating world-changing technology that enables global progress and enriches lives. Inspired by Moore’s Law, we continuously work to advance the design and manufacturing of semiconductors to help address our customers’ greatest challenges. By embedding intelligence in the cloud, network, edge and every kind of computing device, we unleash the potential of data to transform business and society for the better. To learn more about Intel’s innovations, go to newsroom.intel.com and intel.com. © Intel Corporation. Intel, the Intel logo and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.",
    "commentLink": "https://news.ycombinator.com/item?id=39981032",
    "commentBody": "Intel Gaudi 3 AI Accelerator (intel.com)378 points by goldemerald 17 hours agohidepastfavorite218 comments mk_stjames 14 hours agoOne nice thing about this (and the new offerings from AMD) is that they will be using the \"open accelerator module (OAM)\" interface- which standardizes the connector that they use to put them on baseboards, similar to the SXM connections of Nvidia that use MegArray connectors to thier baseboards. With Nvidia, the SXM connection pinouts have always been held proprietary and confidential. For example, P100's and V100's have standard PCI-e lanes connected to one of the two sides of their MegArray connectors, and if you know that pinout you could literally build PCI-e cards with SXM2/3 connectors to repurpose those now obsolete chips (this has been done by one person). There are thousands, maybe tens of thousands of P100's you could pickup for literally = 7.0. I support any use of old hardware but this kind of reminds me of my \"ancient\" X5690 that has impressive performance (relatively speaking) but always bites me because it doesn't have AVX. reply mk_stjames 3 hours agorootparentThis is all very true for Machine-Learning research tasks, were yes, if you want that latest PyTorch library function to work you need to be on the latest ML code. But my work/fun is in CFD. One of the main codes I use for work was written to be supported primarily at the time of Pascal. Other HPC stuff too that can be run via OpenCL, and is still plenty compatible. Things compiled back then will still run today; It's not a moving target like ML has been. reply kkielhofner 2 hours agorootparentExactly. Demand for FP64 is significantly lower than for ML/AI. Pascal isn’t incredibly cheap by comparison because it’s some secret hack. It’s cheap by comparison because most of the market (AI/ML) doesn’t want it. Speaking of which… At the risk of “No True Scotsman” what qualifies as HPC gets interesting but just today I was at a Top500 site that was talking about their Volta system not being worth the power, which is relevant to parent comment but still problematic for reasons. I mentioned llama.cpp because the /r/locallama crowd, etc has actually driven up the cost of used Pascal hardware because they treat it as a path to get VRAM on the cheap with their very very narrow use cases. If we’re talking about getting a little FP64 for CFD that’s one thing. ML/AI is another. HPC is yet another. reply gymbeaux 4 hours agorootparentprevHey that’s not fair, the X5690 is VERY efficient… at heating a home in the winter time. reply egorfine 44 minutes agorootparentEasier said than done. I've got a dual X5690 at home in Kiev, Ukraine and I just couldn't find anything to run on it 24x7. And it doesn't produce much heat idling. I mean at all. reply JonChesterfield 13 hours agoparentprevI really like this side to AMD. There's a strategic call somewhere high up to bias towards collaboration with other companies. Sharing the fabric specifications with broadcom was an amazing thing to see. It's not out of the question that we'll see single chips with chiplets made by different companies attached together. reply 01HNNWZ0MV43FF 10 hours agorootparentMaybe they feel threatened by ARM on mobile and Intel on desktop / server. Companies that think they're first try to monopolize. Companies that think they're second try to cooperate. reply rhelz 10 hours agorootparentprevWell, lets not forget, AMD is AMD because they reverse-engineered Intel chips.... reply treprinum 1 hour agorootparentIBM didn't want to rely solely on Intel when introducing PCs so it forced Intel to share its arch with another manufacturer that turned out to be AMD. It's not like AMD stole it. Math coprocessor was in turn invented by AMD (Am9511, Am9512) and licensed by Intel (8231, 8232). reply gymbeaux 4 hours agoparentprevAs “humble” as NVIDIA’s CEO appears to be, NVIDIA the company (he’s been running this whole time), made decision after decision with the simple intention of killing off its competition (ATI/AMD). Gameworks is my favorite example- essentially if you wanted a video game to look as good as possible, you needed an NVIDIA GPU. Those same games played on AMD GPUs just didn’t look as good. Now that video gaming is secondary (tertiary?) to Nvidia’s revenue stream, they could give a shit which brand gamers prefer. It’s small time now. All that matters is who companies are buying their GPUs from for AI stuff. Break down that CUDA wall and it’s open-season. I wonder how they plan to stave that off. It’s only a matter of time before people get tired of writing C++ code to interface with CUDA. reply mike_hearn 58 minutes agorootparentYou don't need to use C++ to interface with CUDA or even write it. A while ago NVIDIA and the GraalVM team demoed grCUDA which makes it easy to share memory with CUDA kernels and invoke them from any managed language that runs on GraalVM (which includes JIT compiled Python). Because it's integrated with the compiler the invocation overhead is low: https://developer.nvidia.com/blog/grcuda-a-polyglot-language... And TornadoVM lets you write kernels in JVM langs that are compiled through to CUDA: https://www.tornadovm.org There are similar technologies for other languages/runtimes too. So I don't think that will cause NVIDIA to lose ground. reply buildbot 12 hours agoparentprevThe SXM2 interface is actually publicly documented! There is an open compute spec for a 8-way baseboard. You can find the pinouts there. reply mk_stjames 11 hours agorootparentI had read their documents such as the spec for the Big Basin JBOG, where everything is documented except the actual pinouts on the base board. Everything leading up to it and from it is there but the actual MegArray pinout connection to a single P100/V100 I never found. But maybe there was more I missed. I'll take another look. reply mk_stjames 9 hours agorootparentprevUpon further review... I think any actual base board schematics / pinouts touching the Nvidia hardware directly is indeed kept behind some sort of NDA or OEM license agreement and is specifically kept out of any of those documents for the Open Compute project JBOG rigs. I think this is literally the impetus for their OAM spec which makes the pinout open and shareable. Up until that, they had to keep the actual designs of the baseboards out of the public due to that part being still controlled Nvidia IP. reply buildbot 5 hours agorootparentHmm interesting, I was linked to an OCP dropbox with a version that did have the connector pinouts. Maybe something someone shouldn’t have posted then… reply IntelMiner 3 hours agorootparentIt would be a shame if such a thing were to fall off the back of a truck as they say reply pavelstoev 6 hours agoparentprevBest Tflops/$ is actually 4090, then 3090. Also L4 reply wmf 13 hours agoparentprevWhy don't they sell used P100 DGX/HGX servers as a unit? Are those bare P100s only so cheap precisely because they're useless? reply mk_stjames 12 hours agorootparentI have a theory some big cloud provider moved a ton of racks from SXM2 P100's to SXM2 V100's (those were a thing) and thus orphaned an absolute ton of P100's without their baseboards. Or, these salvage operations just stripped racks and kept the small stuff and e-waste the racks because they think it's the more efficient use of their storage space and would be easier to sell, without thinking correctly. reply bushbaba 8 hours agorootparentA ton of Nvidia GPUs fry their memory over time and need to be scrapped. Lookup nvidia (A100/H100) row remapping failure reply formerly_proven 13 hours agoparentprevThe price is low because they’re useless (except for replacing dead cards in a DGX), if you had a 40$ PCIe AIC-to-SXM adapter, the price would go up a lot. > I'm one of those people who finds 'retro-super-computing' a cool hobby and thus the interfaces like OAM being open means that these devices may actually have a life for hobbyists in 8~10 years instead of being sent directly to the bins due to secret interfaces and obfuscated backplane specifications. Very cool hobby. It’s also unfortunate how stringent e-waste rules lead to so much perfectly fine hardware to be scrapped. And how the remainder is typically pulled apart to the board / module level for spares. Makes it very unlikely to stumble over more or less complete-ish systems. reply KeplerBoy 12 hours agorootparentI'm not sure the prices would go up that much. What would anyone buy that card for? Yes, it has a decent memory bandwidth (~750 GB/s) and it runs CUDA. But it only has 16 GB and doesn't support tensor cores or low precision floats. It's in a weird place. reply trueismywork 12 hours agorootparentScientific computing would buy it up like hot cakes. reply KeplerBoy 12 hours agorootparentOnly if the specific workload needs FP64 (4.5 Tflop/s), the 9 Tflop/s for FP32 can be had for cheap with Turing or Ampere consumer cards. Still, your point stands. It's crazy how that 2016 GPU has two thirds the FP32 power of this new 2024 unobtanium card and infinitely more FP64. reply algo_trader 10 hours agorootparentSomewhat off topic: Is there a similar \"magic value card\" for low memory (2GB?) 8-bit LLMs? Since memory is the expensive bit, surely there are low cost low memory models? reply KeplerBoy 5 hours agorootparentI believe that's what tenstorrent is aiming for. reply abdullin 4 hours agorootparentThe main offer of Tenstorrent goes into server racks and is designed to form clusters. Standalone cards are more like dev kits. (I’ve been tracking Tenstorrent for 3+ years and currently have Grayskull in ML test rig together with 3090) reply jsight 10 hours agorootparentprevIDK, is it really that much more powerful than the P40, which is already fairly cheap? reply mk_stjames 3 hours agorootparentThe P100 has amazing double precision (FP64) flops (due to a 1:2 FP ratio that got nixed on all other cards) and a higher memory bandwidth which made it a really standout GPU for scientific computing applications. Computational Fluid Dynamics, etc. The P40 was aimed at the image and video cloud processing market I think, and thus the GDDR ram instead of HBM, so it got more VRAM but at much less bandwidth. reply 7speter 6 hours agorootparentprevWell, the p40 has 24gb VRAM, which makes it the perfect hobbyist card for a llm, assuming you can keep it cool. reply 7speter 6 hours agorootparentprevThe pci-e p100 is has 16gb vram and won’t go below 160 dollars. Prices for these things would pick up if you could put them in some sort of pcie adapter reply kylixz 15 hours agoprevThis is a bit snarky — but will Intel actually keep this product line alive for more than a few years? Having been bitten by building products around some of their non-x86 offerings where they killed good IP off and then failed to support it… I’m skeptical. I truly do hope it is successful so we can have some alternative accelerators. reply jtriangle 15 hours agoparentThe real question is, how long does it actually have to hang around really? With the way this market is going, it probably only has to be supported in earnest for a few years by which point it'll be so far obsolete that everyone who matters will have moved on. reply AnthonyMouse 13 hours agorootparentWe're talking about the architecture, not the hardware model. What people want is to have a new, faster version in a few years that will run the same code written for this one. Also, hardware has a lifecycle. At some point the old hardware isn't worth running in a large scale operation because it consumes more in electricity to run 24/7 than it would cost to replace with newer hardware. But then it falls into the hands of people who aren't going to run it 24/7, like hobbyists and students, which as a manufacturer you still want to support because that's how you get people to invest their time in your stuff instead of a competitor's. reply iamleppert 12 hours agoparentprevLong enough for you to get in, develop some AI product, raise investment funds, and get out with your bag! reply fourg 10 hours agoparentprevWhat’s Next: Intel Gaudi 3 accelerators' momentum will be foundational for Falcon Shores, Intel’s next-generation graphics processing unit (GPU) for AI and high-performance computing (HPC). Falcon Shores will integrate the Intel Gaudi and Intel® Xe intellectual property (IP) with a single GPU programming interface built on the Intel® oneAPI specification. reply johnchristopher 4 hours agorootparentI can't tell if your comment is sarcastic or genuine :). It goes to show how out of touch I am on AI hw and sw matters. Yesterday I thought about installing and trying to use https://news.ycombinator.com/item?id=39372159 (Reor is an open-source AI note-taking app that runs models locally.) and feed it my markdown folder but I stop midway, asking myself \"don't I need some kind of powerful GPU for that ?\". And now I am thinking \"wait, should I wait for `standard` pluggable AI computing hardware device ? Is that Intel Gaudi 3 something like that ?\". reply forkerenok 15 hours agoparentprevI'm not very involved in the broader topic, but isn't the shortage of hardware for AI-related workloads intense enough so as to grant them the benefit of the doubt? reply astrodust 13 hours agoparentprevI hope it pairs well with Optane modules! reply VHRanger 13 hours agorootparentI'll add it right next to my Xeon Phi! reply riffic 15 hours agoparentprevItanic was a fun era reply cptskippy 14 hours agorootparentItanium only stuck around as long as it did because they were obligated to support HP. reply pjmlp 3 hours agorootparentItanium only failed because AMD was allowed to come up with AMD64, Intel would have managed to push Itanium no matter what, if there were no alternatives to a 64bit compatible x86 CPU. reply cptskippy 14 hours agoparentprevI think it's a valid question. Intel has a habit of whispering away anything that doesn't immediately ship millions of units or that they're contractually obligated to support. reply gymbeaux 3 hours agoparentprevI haven’t read the article but my first question would be “what problem is this accelerator solving?” and if the answer is simply “you can AI without Nvidia”, that’s not good enough, because that’s the pot calling the kettle black. None of these companies is “altruistic” but between the three of them I expect AMD to be the nicest to its customers. Nvidia will squeeze the most money out of theirs, and Intel will leave theirs out to dry when corporate leadership decides it’s a failure. reply neilmovva 16 hours agoprevA bit surprised that they're using HBM2e, which is what Nvidia A100 (80GB) used back in 2020. But Intel is using 8 stacks here, so Gaudi 3 achieves comparable total bandwidth (3.7TB/s) to H100 (3.4TB/s) which uses 5 stacks of HBM3. Hopefully the older HBM has better supply - HBM3 is hard to get right now! The Gaudi 3 multi-chip package also looks interesting. I see 2 central compute dies, 8 HBM die stacks, and then 6 small dies interleaved between the HBM stacks - curious to know whether those are also functional, or just structural elements for mechanical support. reply bayindirh 15 hours agoparent> A bit surprised that they're using HBM2e, which is what Nvidia A100 (80GB) used back in 2020. This is one of the secret recipes of Intel. They can use older tech and push it a little further to catch/surpass current gen tech until current gen becomes easier/cheaper to produce/acquire/integrate. They have done it with their first quad core processors by merging two dual core processors (Q6xxx series), or by creating absurdly clocked single core processors aimed at very niche market segments. We have not seen it until now, because they were sleeping at the wheel, and knocked unconscious by AMD. reply JonChesterfield 14 hours agorootparent> This is one of the secret recipes of Intel Any other examples of this? I remember the secret sauce being a process advantage over the competition, exactly the opposite of making old tech outperform the state of the art. reply calaphos 14 hours agorootparentIntels surprisingly fast 14nm processors come to mind. Born of necessity as they couldn't get their 10 and later 7nm processes working for years. Despite that Intel managed to keep up in single core performance with newer 7nm AMD chips, although at a mich higher power draw. reply deepnotderp 12 hours agorootparentThat's because CPU performance cares less about transistor density and more about transistor performance, and 14nm drive strength was excellent reply 0x457 9 hours agorootparentprevFor like half of 14nm intel era, there was no competition on CPU market in any segment for them. Intel was able to improve their 14nm process and be better at branch prediction. Moving things to hardware implementation is what kept improving. This isn't the same as getting more out of the same over and over again. reply Dalewyn 13 hours agorootparentprevOr today with Alder Lake and Raptor Lake(Refresh), where their CPUs made on Intel 7 (10nm) are on par if not slightly better than AMD's offerings made on TSMC 5nm. reply timr 14 hours agorootparentprevBack in the day, Intel was great for overclocking because all of their chips could run at significantly higher speeds and voltages than on the tin. This was because they basically just targeted the higher specs, and sold the underperforming silicon as lower-tier products. Don't know if this counts, but feels directionally similar. reply mvkel 15 hours agorootparentprevInteresting. Would you say this means Intel is \"back,\" or just not completely dead? reply bayindirh 14 hours agorootparentNo, this means Intel has woken up and trying. There's no guarantee in anything. I'm more of an AMD person, but I want to see fierce competition, not monopoly, even if it's \"my team's monopoly\". reply chucke1992 14 hours agorootparentWell the only reason why AMD is doing good at CPU is becoming Intel is sleeping. Otherwise it would be Nvidia vs AMD (less steroids though). reply bayindirh 14 hours agorootparentEPYC is actually pretty good. It’s true that Intel was sleeping, but AMD’s new architecture is a beast. Has better memory support, more PCIe lanes and better overall system latency and throughput. Intel’s TDP problems and AVX clock issues leave a bitter taste in the mouth. reply alexey-salmin 14 hours agorootparentprevOh dear, Q6600 was so bad, I regret ever owning it reply chucke1992 14 hours agorootparentQ6600 was quite good but E8400 was the best. reply astrodust 13 hours agorootparentQ6600 is the spiritual successor to the ABIT BP6 Dual Celeron option: https://en.wikipedia.org/wiki/ABIT_BP6 reply dfex 6 hours agorootparentThe ABit BP6 bought me so much \"cred\" at LAN Parties back in the day - the only dual socket motherboard in the building, and paired with two Creative Voodoo 2 GPUs in SLI mode, that thing was a beast (for the late nineties). I seem to recall that only Quake 2 or 3 was capable of actually using that second processor during a game, but that wasn't the point ;) reply bayindirh 12 hours agorootparentprevABIT was a legend in motherboards. I used their AN-7 Ultra and AN-8 Ultra. No newer board gave the flexibility and capabilities of these series. My latest ASUS was good enough, but I didn't (and probably won't) build any newer systems, so ABITs will have the crown. reply alexey-salmin 14 hours agorootparentprevE8400 was actually good, yes reply mrybczyn 14 hours agorootparentprevWhat? It was outstanding for the time, great price performance, and very tunable for clock / voltage IIRC. reply alexey-salmin 13 hours agorootparentWell overclocked I don't know, but out-of-the box single-core performance completely sucked. And in 2007 not enough applications had threads to make it up in the number of cores. It was fun to play with but you'd also expect the higher-end desktop to e.g. handle x264 videos which was not the case (search for q6600 on videolan forum). And depressingly many cheaper CPUs of the time did it easily. reply bayindirh 14 hours agorootparentprevI owned one, it was a performant little chip. Developed my first multi core stuff with it. I loved it, to be honest. reply JonChesterfield 14 hours agorootparentprev65nm tolerated a lot of voltage. Fun thing to overclock. reply PcChip 14 hours agorootparentprevReally? I never owned one but even I remember the famous SLACR, I thought they were the hot item back then reply alexey-salmin 13 hours agorootparentIt was \"hot\" but using one as a main desktop in 2007 was depressing due to abysmal single-core performance. reply tmikaeld 4 hours agoparentprevI was just about to comment on this, apparently all production capacity for hbm is tapped out until early 2026 reply sairahul82 16 hours agoprevCan we expect the price of 'Gaudi 3 PCIe' to be reasonable enough to put in a workstation? That would be a game changer for local LLMs reply wongarsu 16 hours agoparentProbably not. An 40GB Nvidia A100 is arguably reasonable for a workstation at $6000. Depending on your definition an 80GB A100 for $16000 is still reasonable. I don't see this being cheaper than an 80GB A100. Probably a good bit more expensive, seeing as it has more RAM, compares itself favorably to the H100, and has enough compelling features that it probably doesn't have to (strongly) compete on price. reply 0cf8612b2e1e 15 hours agorootparentSurely NVidia’s pricing is more what the market will bear vs an intrinsic cost to build. Intel being the underdog should be willing to offer a discount just to get their foot in the door. reply wmf 15 hours agorootparentNvidia is charging $35K so a discount relative to that is still very expensive. reply tormeh 14 hours agorootparentprevPricing is normally what the market will bear. If this is below your cost as supplier you exit the market. reply AnthonyMouse 13 hours agorootparentBut if your competitor's price is dramatically above your cost, you can provide a huge discount as an incentive for customers to pay the transition cost to your system while still turning a tidy profit. reply narrator 12 hours agorootparentprevIsn't it much better to get a Mac Studio with an M2 Max and 192gb of Ram and 31 terraflops for $6599 and run llama.cpp? reply magic_hamster 9 hours agorootparentMacs don't support CUDA which means all that wonderful hardware will be useless when trying to do anything with AI for at least a few years. There's Metal but it has its own set of problems, biggest one being it isn't a drop in CUDA replacement. reply doublepg23 6 hours agorootparentI'm assuming this won't support CUDA either? reply egorfine 39 minutes agorootparentprevFor LLM inference - yes absolutely. reply chessgecko 15 hours agorootparentprevI think you're right on the price, but just to give some false hope. I think newish hbm (and this is hbm2e which is a little older) is around $15/gb so for 128 gb thats $1920. There are some other cogs, but in theory they could sell this for like $3-4k and make some gross profit while getting some hobbyist mindshare/research code written for it. I doubt they will though, it might eat too much into profits from the non pcie variants. reply p1esk 12 hours agorootparentin theory they could sell this for like $3-4k You’re joking, right? They will price it to match current H100 pricing. Multiply your estimate by 10x. reply chessgecko 11 hours agorootparentThey could, I know they wont, but they wouldn't lose money on the parts reply Workaccount2 15 hours agorootparentprevInterestingly they are using HBME2 memory which is a few years old at this point. The price might end up being surprisingly good because of this. reply CuriouslyC 16 hours agoparentprevJust based on the RAM alone, let's just say if you can't just buy a Vision Pro without a second thought about the price tag, don't get your hopes up. reply ipsum2 11 hours agoparentprevIt won't be under $10k. reply rileyphone 16 hours agoprev128GB in one chip seems important with the rise of sparse architectures like MoE. Hopefully these are competitive with Nvidia's offerings, though in the end they will be competing for the same fab space as Nvidia if I'm not mistaken. reply latchkey 16 hours agoparentAMD MI300x is 192GB. reply tucnak 16 hours agorootparentWhich would be impressive had it _actually_ worked for ML workloads. reply huac 15 hours agorootparentThere's a number of scaled AMD deployments, including Lamini (https://www.lamini.ai/blog/lamini-amd-paving-the-road-to-gpu...) specifically for LLM's. There's also a number of HPC configurations, including the world's largest publicly disclosed supercomputer (Frontier) and Europe's largest supercomputer (LUMI) running on MI250x. Multiple teams have trained models on those HPC setups too. Do you have any more evidence as to why these categorically don't work? reply latchkey 14 hours agorootparent> Do you have any more evidence as to why these categorically don't work? They don't. Loud voices parroting George, with nothing to back it up. Here are another couple good links: https://www.evp.cloud/post/diving-deeper-insights-from-our-l... https://www.databricks.com/blog/training-llms-scale-amd-mi25... reply Hugsun 16 hours agorootparentprevDoes it not work for them? Where can I learn why? reply tucnak 16 hours agorootparentJust go have a look around Github issues in their ROCm repositories on Github. A few months back the top excuse re: AMD was that we're not supposed to use their \"consumer\" cards, however the datacenter stuff is kosher. Well, guess what, we have purchased their datacenter card, MI50, and it's similarly screwed. Too many bugs in the kernel, kernel crashes, hangs, and the ROCm code is buggy / incomplete. When it works, it works for a short period of time, and yes HBM memory is kind of nice, but the whole thing is not worth it. Some say MI210 and MI300 are better, but it's just wishful thinking as all the bugs are in the software, kernel driver, and firmware. I have spent too many hours troubleshooting entry-level datacenter-grade Instinct cards with no recourse from AMD whatsoever to pay 10+ thousands for MI210 a couple-year old underpowered hardware, and MI300 is just unavailable. Not even from cloud providers which should be telling enough. reply JonChesterfield 14 hours agorootparentWe absolutely hammered the MI50 in internal testing for ages. Was solid as far as I can tell. Rocm is sensitive to matching kernel version to driver version to userspace version. Staying very much on the kernel version from a official release and using the corresponding driver is drastically more robust than optimistically mixing different components. In particular, rocm is released and tested as one large blob, and running that large blob on a slightly different kernel version can go very badly. Mixing things from GitHub with things from your package manager is also optimistic. Imagine it as huge ball of code where cross version compatibility of pieces is totally untested. reply tucnak 13 hours agorootparentI would run simple llama.cpp batch jobs for 10 minutes when it would suddenly fail, and require a restart. Random VM_L2_PROTECTION_FAULT in dmesg, something having to do with doorbells. I did report this, never heard back from them. reply JonChesterfield 1 hour agorootparentDid you run on the blessed Ubuntu version with the blessed kernel version and the blessed driver version? As otherwise you really are in a development branch. If you can point me to a repro I'll add it to my todo list. You can probably tag me in the github issue if that's where you reported it. reply FeepingCreature 4 hours agorootparentprevSame here with SD on 7900XTX. Most of the time for me it's sufficient to reset the card with rocm-smi --gpureset -d 0. reply michaelt 2 hours agorootparentOnly \"most of the time\" ? :( You'd hope at $15,000+ per unit, you wouldn't have to reset it at all... reply JonChesterfield 1 hour agorootparentIt's $1000 per, no? This is one of the gaming cards. reply latchkey 10 hours agorootparentprevWould you like to share the model of GPU and versions of various software used? George has a nice explanation of doorbells: https://youtu.be/AqPIOtUkxNo?feature=shared&t=968 reply Workaccount2 15 hours agorootparentprevIt's seriously impressive how well AMD has been able to maintain their incredible software deficiency for over a decade now. reply alexey-salmin 14 hours agorootparentThey deeply care about the tradition of ATI kernel modules from 2004 reply amirhirsch 14 hours agorootparentprevBuying Xilinx helped a lot here. reply fpgamlirfanboy 7 hours agorootparentit's so true it hurts reply tucnak 4 hours agorootparentHey man have seen you around here, very knowledgeable, thanks for your input! What's your take on projects like https://github.com/corundum/corundum I'm trying to get better at FPGA design, perhaps learn PCIe and some such but Vivado is intimidating (as opposed to Yosys/nextpnr which you seem to hate) should I just get involved with a project like this to acclimatise somewhat? reply cavisne 7 hours agorootparentprevYeah, I think AMD will really struggle with the cloud providers. Even Nvidia GPU's are tricky to sandbox, and it sounds like the AMD cards are really easy for the tenant to break (or at least force a restart of the underlying host). AWS does have a Gaudi instance which is interesting, but overall I don't see why Azure, AWS & Google would deploy AMD or Intel GPU's at scale vs their own chips. They need some competitor to Nvidia to help negotiate, but if its going to be a painful software support story suited to only a few enterprise customers, why not do it with your own chip? reply jmward01 14 hours agorootparentprevYeah, this has stopped me from trying anything with them. They need to lead with their consumer cards so that developers can test/build/evaluate/gain trust locally and then their enterprise offerings need to 100% guarantee that the stuff developers worked on will work in the data center. I keep hoping to see this but every time I look it isn't there. There is way more support for apple silicon out there than ROCm and that has no path to enterprise. AMD is missing the boat. reply JonChesterfield 14 hours agorootparentIn fairness it wasn't Apple who implemented the non-mac uses of their hardware. AMD's driver is in your kernel, all the userspace is on GitHub. The ISA is documented. It's entirely possible to treat the ASICs as mass market subsidized floating point machines and run your own code on them. Modulo firmware. I'm vaguely on the path to working out what's going on there. Changing that without talking to the hardware guys in real time might be rather difficult even with the code available though. reply imtringued 3 hours agorootparentYou are ignoring that AMD doesn't use an intermediate representation and every ROCm driver is basically compiling to a GPU specific ISA. It wouldn't surprise me that there are bugs they have fixed for one ISA that they didn't bother porting to the others. The other problem is that most likely their firmware contains classic C bugs like buffer overflows, undefined behaviour, or stuff like deadlocks. reply JonChesterfield 1 hour agorootparentThis is sort of true. Graphics compiles to spir-v, moves that around as deployment, then runs it through llvm to create the compiled shaders. Compute doesn't bother with spir-v (to the distress of some of our engineers) and moves llvm IR around instead. That goes through the llvm backend which does mostly the same stuff for each target machine. There probably are some bugs that were fixed on one machine and accidentally missed on another - the compiler is quite branchy - but it's nothing like as bad as a separate codebase per ISA. Nvidia has a specific ISA per card too, they just expose PTX and SASS as abstractions over it. I haven't found the firmware source code yet - digging through confluence and perforce tries my patience and I'm supposed to be working on llvm - but I hear it's written in assembly, where one of the hurdles to open sourcing it is the assembler is proprietary. I suspect there's some common information shared with the hardware description language (tcl and verilog or whatever they're using). To the extent that turns out to be true, it'll be immune to C style undefined behaviour, but I wouldn't bet on it being free from buffer overflows. reply latchkey 14 hours agorootparentprevYou are right, AMD should do more with consumer cards, but I understand why they aren't today. It is a big ship, they've really only started changing course as of last Oct/Nov, before the release of MI300x in Dec. If you have limited resources and a whole culture to change, you have to give them time to fix that. That said, if you're on the inside, like I am, and you talk to people at AMD (just got off two separate back to back calls with them), rest assured, they are dedicated to making this stuff work. Part of that is to build a developer flywheel by making their top end hardware available to end users. That's where my company Hot Aisle comes into play. Something that wasn't available before outside of the HPC markets, is now going to be made available. reply jmward01 12 hours agorootparentI look forward to seeing it. NVIDIA needs real competition for their own benefit if not the market as a whole. I want a richer ecosystem where Intel, AMD, NVIDIA and other players all join in with the winner being the consumer. From a selfish point of view I also want to do more home experimentation. LLMs are so new that you can make breakthroughs without a huge team but it really helps to have hardware to make it easier to play with ideas. Consumer card memory limitations are hurting that right now. reply latchkey 12 hours agorootparent> I want a richer ecosystem where Intel, AMD, NVIDIA and other players all join in with the winner being the consumer. This is exactly the void I'm trying to fill. reply tucnak 14 hours agorootparentprev> developer flywheel This is peak comedy reply latchkey 14 hours agorootparenthttps://news.ycombinator.com/newsguidelines.html Comments should get more thoughtful and substantive, not less, as a topic gets more divisive. reply latchkey 16 hours agorootparentprevnext [8 more] [flagged] Scaevolus 15 hours agorootparentEffectively everyone that has attempted to use AMD hardware for ML comes away with these opinions, the main difference is how angrily they express it. reply latchkey 15 hours agorootparentWe are the 4th non-hyperscaler business on the planet to even get access to MI300x and we just got it in early March. From what I understand, hyperscalers have had fantastic uptake of this hardware. I find it hard to believe \"everyone\" comes away with these opinions. https://www.evp.cloud/post/diving-deeper-insights-from-our-l... reply sangnoir 15 hours agorootparentprevSo who's buying all the MI300s? Groq seems to be fine with AMD. reply latchkey 14 hours agorootparent> So who's buying all the MI300s? We are. Just closed additional funding and getting quotes now. reply tucnak 14 hours agorootparentprevGroq doesn't use AMD afaik, they had designed hardware of their own, which is actually 1000s of SRAM chips in a trench-coat. reply latchkey 14 hours agorootparentSpeaking of SRAM, I found this relevant comment insightful: https://news.ycombinator.com/item?id=39966620 reply buildbot 15 hours agorootparentprevProbably not, I have had the same experience with a 780m and a Mi60... reply kaycebasques 16 hours agoprevWow, I very much appreciate the use of the 5 Ws and H [1] in this announcement. Thank you Intel for not subjecting my eyes to corp BS [1] https://en.wikipedia.org/wiki/Five_Ws reply belval 16 hours agoparentI wonder if with the advent of LLMs being able to spit out perfect corpo-speak everyone will recenter to succint and short \"here's the gist\" as the long version will become associated to cheap automated output. reply latchkey 16 hours agoprev> the only MLPerf-benchmarked alternative for LLMs on the market I hope to work on this for AMD MI300x soon. My company just got added to the MLCommons organization. reply riskable 16 hours agoprev> Twenty-four 200 gigabit (Gb) Ethernet ports are integrated into every Intel Gaudi 3 accelerator WHAT‽ It's basically got the equivalent of a 24-port, 200-gigabit switch built into it. How does that make sense? Can you imaging stringing 24 Cat 8 cables between servers in a single rack? Wait: How do you even decide where those cables go? Do you buy 24 Gaudi 3 accelerators and run cables directly between every single one of them so they can all talk 200-gigabit ethernet to each other? Also: If you've got that many Cat 8 cables coming out the back of the thing how do you even access it? You'll have to unplug half of them (better keep track of which was connected to what port!) just to be able to grab the shell of the device in the rack. 24 ports is usually enough to take up the majority of horizontal space in the rack so maybe this thing requires a minimum of 2-4U just to use it? That would make more sense but not help in the density department. I'm imagining a lot of orders for \"a gradient\" of colors of cables so the data center folks wiring the things can keep track of which cable is supposed to go where. reply blackeyeblitzar 16 hours agoparentSee https://www.nextplatform.com/2024/04/09/with-gaudi-3-intel-c... for more details. Here’s the relevant bits, although you should visit the article to see the networking diagrams: > The Gaudi 3 accelerators inside of the nodes are connected using the same OSFP links to the outside world as happened with the Gaudi 2 designs, but in this case the doubling of the speed means that Intel has had to add retimers between the Ethernet ports on the Gaudi 3 cards and the six 800 Gb/sec OSFP ports that come out of the back of the system board. Of the 24 ports on each Gaudi 3, 21 of them are used to make a high-bandwidth all-to-all network linking those Gaudi 3 devices tightly to each other. Like this: > As you scale, you build a sub-cluster with sixteen of these eight-way Gaudi 3 nodes, with three leaf switches – generally based on the 51.2 Tb/sec “Tomahawk 5” StrataXGS switch ASICs from Broadcom, according to Medina – that have half of their 64 ports running at 800 GB/sec pointing down to the servers and half of their ports pointing up to the spine network. You need three leaf switches to do the trick: > To get to 4,096 Gaudi 3 accelerators across 512 server nodes, you build 32 sub-clusters and you cross link the 96 leaf switches with a three banks of sixteen spine switches, which will give you three different paths to link any Gaudi 3 to any other Gaudi 3 through two layers of network. Like this: The cabling works out neatly in the rack configurations they envision. The idea here is to use standard Ethernet instead of proprietary Infiniband (which Nvidia got from acquiring Mellanox). Because each accelerator can reach other accelerators via multiple paths that will (ideally) not be over-utilized, you will be able to perform large operations across them efficiently without needing to get especially optimized about how your software manages communication. reply Manabu-eo 14 hours agorootparentThe PCI-e HL-338 version is also listing 24 200GbE RDMA nics in a dual-slot configuration. How would they be connected? reply wmf 13 hours agorootparentThey may go to the top of the card where you can use an SLI-like bridge to connect multiple cards. reply gaogao 16 hours agoparentprevInfiniband I've heard as incredibly annoying to deal with procuring as well as some other aspects of it, so lots of folks very happy to get RoCE (ethernet) working instead, even if it is a bit cumbersome. reply throwaway2037 9 hours agorootparent\"RoCE\"? Woah, I had to Google that. https://en.wikipedia.org/wiki/RDMA_over_Converged_Ethernet > RDMA over Converged Ethernet (RoCE) or InfiniBand over Ethernet (IBoE)[1] is a network protocol which allows remote direct memory access (RDMA) over an Ethernet network. It does this by encapsulating an InfiniBand (IB) transport packet over Ethernet. Sounds very cool. reply buildbot 16 hours agoparentprev200gb is not going to be using CAT, it will be fiber (or direct attached copper cable as noted by dogma1138) with a QSFP interface reply dogma1138 16 hours agorootparentIt will most likely use copper QSFP56 cables since these interfaces are either used in inter rack or adjacent rack direct attachments or to the nearest switch. O.5-1.5/2m copper cables are easily available and cheap and 4-8m (and even longer) is also possible with copper but tends to be more expensive and harder to get by. Even 800gb is possible with copper cables these days but you’ll end up spending just as much if not more on cabling as the rest of your kit…https://www.fibermall.com/sale-460634-800g-osfp-acc-3m-flt.h... reply buildbot 15 hours agorootparentFair point! reply juliangoldsmith 16 hours agoparentprevFor Gaudi2, it looks like 21/24 ports are internal to the server. I highly doubt those have actual individual cables. Most likely they're just carried on PCBs like any other signal. 100GBe is only supported on twinax anyway, so Cat8 is irrelevant here. The other 3 ports are probably QSFP or something. reply brookst 16 hours agoparentprevAudio folks solved the \"which cable goes where\" problem ages ago with cable snakes: https://www.seismicaudiospeakers.com/products/24-channel-xlr... But I'm not how big and how expensive a 24 channel cat 8 snake would be (!). reply nullindividual 13 hours agorootparentI wouldn’t think that would be appropriate for Ethernet due to cross talk. reply pezezin 11 hours agorootparentThose cables definitely exist for Ethernet, and regarding cross talk, that's what shielding is for. Although not for 200 Gbps, at that rate you either use big twinax DACs, or go to fibre. reply wmf 11 hours agorootparentprevFour-lane and eight-lane twinax cables exist; I think each pair is individually shielded. Beyond that there's fiber. reply radicaldreamer 16 hours agoparentprevThe amount of power that will use up is massive, they should've gone for some fiber instead reply buildbot 16 hours agorootparentIt will be fiber, Ethernet is just the protocol not the physical interface. reply KeplerBoy 16 hours agorootparentprevThe fiber optics are also extremely power hungry. For short runs people use direct attach copper cables to avoid having to deal with fiberoptics. reply InvestorType 10 hours agoprevThis appears to be manufactured by TSMC (or Samsung). The press release says it will use a 5nm process, which is not on Intel's roadmap. \"The Intel Gaudi 3 accelerator, architected for efficient large-scale AI compute, is manufactured on a 5 nanometer (nm) process\" reply ac29 9 hours agoparentHabana was an acquisition and their use of TSMC predates the acquisition. reply modeless 6 hours agorootparentYeah, but if Intel can't even get internal customers to adopt their foundry services it seems to bode poorly for the future of the company. reply 1024core 16 hours agoprev> Memory Boost for LLM Capacity Requirements: 128 gigabytes (GB) of HBMe2 memory capacity, 3.7 terabytes (TB) of memory bandwidth ... I didn't know \"terabytes (TB)\" was a unit of memory bandwidth... reply throwup238 16 hours agoparentIt’s equivalent to about thirteen football fields per arn if that helps. reply gnabgib 16 hours agoparentprevBit of an embarrassing typo, they do later qualify it as 3.7TB/s reply SteveNuts 15 hours agorootparentMost of the time bandwidth is expressed in giga/gibi/tera/tebi bits per second so this is also confusing to me reply sliken 14 hours agorootparentOnly for networking, not for anything measured inside a node. Disk bandwidth, cache bandwidth, and memory bandwidth is nearly always measured in bytes/sec (bandwidth), or NS/cache line or similar (which is mix of bandwidth and latency). reply nahnahno 5 hours agoparentprevAbout as relevant a measure of speed as parsecs reply throwaway4good 13 hours agoprevWorth noting that it is fabbed by TSMC. reply geertj 14 hours agoprevI wonder if someone knowledgeable could comment on OneAPI vs Cuda. I feel like if Intel is going to be a serious competitor to Nvidia, both software and hardware are going to be equally important. reply meragrin_ 12 hours agoparentApparently, Google, Qualcomm, Samsung, and ARM are rallying around oneAPI: https://uxlfoundation.org/ reply ZoomerCretin 14 hours agoparentprevI'm not familiar with the particulars of OneAPI, but it's just a matter of rewriting CUDA kernels into OneAPI. This is pretty trivial for the vast majority of small (Intel Gaudi software integrates the PyTorch framework and provides optimized Hugging Face community-based models – the most-common AI framework for GenAI developers today. This allows GenAI developers to operate at a high abstraction level for ease of use and productivity and ease of model porting across hardware types. what is the programming interface here ? this is not CUDA right ...so how is this being done ? reply wmf 4 hours agoparentPyTorch has a bunch of backends including CUDA, ROCm, OneAPI, etc. reply sandGorgon 4 hours agorootparenti understand. but which backend is intel committing to ? not CUDA for sure. or have they created a new backend reply singhrac 2 hours agorootparentIntel makes oneAPI. They have corresponding toolkits to cuDNN like oneMKL, oneDNN, etc. However the Gaudi chips are built on top of SynapseAI, another API from before the Habana acquisition. I don’t know if there’s a plan to support oneAPI on Gaudi, but it doesn’t look like it at the moment. reply alecco 15 hours agoprevGaudi 3 has PCIe 4.0 (vs. H100 PCIe 5.0, so 2x the bandwidth). Probably not a deal-breaker but it's strange for Intel (of all vendors) to lag behind in PCIe. reply wmf 15 hours agoparentN5, PCIe 4.0, and HBM2e. This chip was probably delayed two years. reply alecco 15 hours agorootparentGood point, it's built on TSMC while Intel is pushing to become the #2 foundry. Probably it's because Gaudi was made by an Israeli company Intel acquired in 2019 (not an internal project). Who knows. https://www.semianalysis.com/p/is-intel-back-foundry-and-pro... reply KeplerBoy 12 hours agoparentprevThe whitepaper says it's PCIe 5 on Gaudi 3. reply MrYellowP 1 hour agoprevhttps://www.dwds.de/wb/Gaudi That's amusing. :D reply ancharm 15 hours agoprevIs the scheduling / bare metal software open source through OneAPI? Can a link be posted showing it if so? reply cavisne 7 hours agoprevIs there an equivalent to this reference for Intel Gaudi? https://docs.nvidia.com/cuda/parallel-thread-execution/index... reply amelius 12 hours agoprevMissing in these pictures are the thermal management solutions. reply InitEnabler 11 hours agoparentIf you look at one of the pictures you can get a peak at what they look like (I think...) in the bottom right. https://www.intel.com/content/dam/www/central-libraries/us/e... reply wmf 4 hours agoparentprevIt's going to look very similar to an Nvidia SXM or AMD MI300 heatsink since these all have similar form factors. reply KeplerBoy 12 hours agoprevvector floating point performance comes in at 14 Tflops/s for FP32 and 28 Tflop/s for FP16. Not the best of times for stuff that doesn't fit matrix processing units. reply andersa 16 hours agoprevPrice? reply yieldcrv 16 hours agoprevHas anyone here bought an AI accelerator to run their AI SaaS service from their home to customers instead of trying to make a profit on top of OpenAI or Replicate Seems like an okay $8,000 - $30,000 investment, and bare metal server maintenance isn’t that complicated these days. reply shiftpgdn 14 hours agoparentDingboard runs off of the owner's pile of used gamer cards. The owner frequently posts about it on twitter. reply metadat 9 hours agoprev> Twenty-four 200 gigabit (Gb) Ethernet ports are integrated into every Intel Gaudi 3 accelerator How much does a single 200Gbit active (or inactive) fiber cable cost? Probably thousands of dollars.. making even the cabling for each card Very Expensive. Nevermind the network switches themselves.. Simultaneously impressive and disappointing. reply carlhjerpe 9 hours agoparenthttps://www.fs.com/de-en/products/115636.html 2 meters seems to be about 100$, which isn't unreasonable. If you're going fiber instead of twinax it's another order of magnitude and a bit for trancievers, but cables are pretty cheap still. You seem to be loading negative energy into this release from the get-go reply metadat 9 hours agorootparentYou're going to need a lot more than 2 meters... It's probably AOC (Active-Optical Fiber Cable), they're pricey even for 40Gbit, at DC lengths. reply throwaway2037 8 hours agoparentprevWhat do you mean by active vs inactive fiber cable? I tried to Google about this distinction, but I couldn't find anything helpful. reply mpreda 16 hours agoprevHow much does one such card cost? reply einpoklum 12 hours agoprevIf your metric is memory bandwidth or memory size, then this announcement gives you some concrete information. But - suppose my metric for performance is matrix-multiply-add (or just matrix-multiply) bandwidth. What MMA primitives does Gaudi offer (i.e. type combinations and matrix dimension combinations), and how many of such ops per second, in practice? The linked page says \"64,000 in parallel\", but that does not actually tell me much. reply AnonMO 15 hours agoprevit's crazy that Intel can't manufacture its own chips atm, but it looks like that might change in the coming years as new fabs come online. reply colechristensen 16 hours agoprevAnyone have experience and suggestions for an AI accelerator? Think prototype consumer product with total cost preferably < $500, definitely less than $1000. reply jsheard 16 hours agoparentThe default answer is to get the biggest Nvidia gaming card you can afford, prioritizing VRAM size over speed. Ideally one of the 24GB ones. reply Hugsun 16 hours agoparentprevYou can get very cheap tesla P40s with 24gb of ram. They are much much slower than the newer cards but offer decent value for running a local chatbot. I can't speak to the ease of configuration but know that some people have used these successfully. reply JonChesterfield 13 hours agoparentprevI liked my 5700XT. That seems to be $200 now. Ran arbitrary code on it just fine. Lots of machine learning seems to be obsessed with amount of memory though and increasing that is likely to increase the price. Also HN doesn't like ROCm much, so there's that. reply hedgehog 16 hours agoparentprevWhat else in on the BOM? Volume? At that price you likely want to use whatever resources are on the SoC that runs the thing and work around that. Feel free to e-mail me. reply mirekrusin 16 hours agoparentprevRent or 3090, maybe used 4090 if you're lucky. reply jononor 16 hours agoparentprevWhat is the workload? reply wmf 16 hours agoparentprevAMD Hawk Point? reply dist-epoch 15 hours agoparentprevAll new CPUs will have so called NPUs inside them. For helping running models locally. reply m3kw9 14 hours agoprevCan you run Cuda on it? reply boroboro4 12 hours agoparentNo one runs Cuda, everyone runs PyTorch. Which you can run on it. reply m3kw9 9 hours agorootparentSo does it support cuda or not are are you gonna argue little things all day? reply kimixa 4 hours agorootparentCUDA is a proprietary Nvidia API where the SDK license explicitly forbids use for development of apps that might run on other hardware. You do read the licenses of SDKs you use, right? Nothing but Nvidia hardware will ever \"support\" CUDA. reply YetAnotherNick 16 hours agoprevSo now hardware companies stopped reporting FLOP/s number and reports in arbitrary unit of parallel operation/s. reply AnonMO 15 hours agoparent1835 tflops fp8. you have to look for it, but they posted it. The link in the op is just an announcement. the white paper has more info. https://www.intel.com/content/www/us/en/content-details/8174... reply brcmthrowaway 15 hours agoprevDoes this support apple silicon? reply chessgecko 15 hours agoprevI feel a little misled by the speedup numbers. They are comparing lower batch size h100/200 numbers to higher batch size gaudi 3 numbers for throughput (which is heavily improved by increasing batch size). I feel like there are some inference scenarios where this is better, but its really hard to tell from the numbers in the paper. reply whalesalad 16 hours agoprev [–] https://www.merriam-webster.com/dictionary/gaudy reply riazrizvi 16 hours agoparentThat’s an i. He’s one the the greatest architects of all time. https://www.archdaily.com/877599/10-must-see-gaudi-buildings... reply TheAceOfHearts 15 hours agoparentprevHonestly, I thought the same thing upon reading the name. I'm aware of the reference to Antoni Gaudí, but having the name sound so close to gaudy seems a bit unfortunate. Surely they must've had better options? Then again I don't know how these sorts of names get decided anymore. reply prewett 11 hours agorootparent'Gaudi' is properly pronounced Ga-oo-DEE in his native Catalan, whereas (in my dialect) 'gaudy' is pronounced GAW-dee. My guess is Intel wasn't even thinking about 'gaudy' because they were thinking about \"famous architects\" or whatever the naming pool was. Although, I had heard that the 'gaudy' came from the architect's name because of what people thought of his work. (I'm not sure this is correct, it was just my first introduction to the word.) reply whalesalad 15 hours agorootparentprevto be fair intel is not known for naming things well. reply bio-s 3 hours agorootparentThe name was picked before the acquisition reply brookst 14 hours agorootparentprevYeah I can't believe people are nitpicking the name when it could just as easily have been AIX19200xvr4200AI. reply ukuina 4 hours agorootparentAssuming that's going to be the datasheet naming. reply jagger27 16 hours agoparentprev [–] https://en.wikipedia.org/wiki/Antoni_Gaud%C3%AD reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Intel unveiled the Intel Gaudi 3 AI accelerator at the Intel Vision event, promising 4x AI compute for BF16 with enhanced memory and networking bandwidth.",
      "The accelerator targets generative AI gaps in finance, manufacturing, and healthcare, offering open software and scalable systems for enterprises.",
      "It boasts AI-specific compute engines, increased memory for LLM capacity, and scalable design, enhancing performance for GenAI models, with OEM availability in Q2 2024 and general release in Q3 2024."
    ],
    "commentSummary": [
      "The discussion delves into Intel's Gaudi 3 AI Accelerator, highlighting the use of the open accelerator module interface and comparisons with AMD's products.",
      "It covers collaboration with other companies, GPU market competition, pricing, performance, stability issues, and software support for Intel's Gaudi 3.",
      "The key points include the high-speed communication network architecture, stability concerns related to AMD's ROCm repositories, and the potential advantages and drawbacks of leveraging Intel's Gaudi 3 for machine learning and AI applications."
    ],
    "points": 378,
    "commentCount": 218,
    "retryCount": 0,
    "time": 1712679700
  },
  {
    "id": 39982024,
    "title": "Cow Magnets: Preventing Hardware Disease in Cattle",
    "originLink": "https://www.stanfordmagnets.com/cow-magnets.html",
    "originBody": "E-mail sales@stanfordmagnets.com × Standard Magnets Standard Magnets Neodymium Magnets Neodymium Magnets Neodymium Block Magnets Neodymium Cube Magnets Neodymium Arc Magnets Neodymium Disc/Cylinder Magnets With Countersunk Holes Neodymium Plate / Block Magnets With Countersunk Holes Neodymium Cylinder Magnets Neodymium Disc Magnets Neodymium Ring Magnets Special Shape Neodymium Magnets SmCo Magnets SmCo Magnets SmCo Block Magnet SmCo Disc Magnet SmCo Pot Magnet SmCo Ring Magnet SmCo Cylinder Magnet AlNiCo Magnets AlNiCo Magnets AlNiCo Bar Magnet AlNiCo Button Magnets AlNiCo Channel Bar Magnet AlNiCo Cow Magnet AlNiCo Curved-Field Magnet AlNiCo Cylinder Magnet AlNiCo Ring Magnet AlNiCo Horseshoe Magnet Ferrite/Ceramic Magnets Ferrite/Ceramic Magnets Ceramic/Ferrite Bar Magnet Ceramic/Ferrite Block Magnet Ceramic/Ferrite Disc Magnet Ceramic/Ferrite Pot Magnet Ceramic/Ferrite Ring Magnet Encased Ceramic Magnets Custom Magnets Custom Magnets Custom Neodymium Magnets Custom Neodymium Magnets Grade Coating Temperature Ratings Characteristics How to Make Applications Safety Custom SmCo Magnets Custom SmCo Magnets Grade Coating 3M Adhesive Backing Magnets Rubber/Plastic Coated Magnets Mounting Magnets Mounting Magnets Standard Mounting Magnets Rubber Mounting Magnets Stainless Mounting Magnets Rectangular Mounting Magnets Metric Mounting Magnets Stepped Magnets Press-Fit Encased Magnets Other Magnets Magnetic Assemblies Magnetic Assemblies Halbach Arrays Magnet Rotors Magnetic Separators Custom Magnetic Assemblies Custom Magnetic Assemblies Sandwich Assemblies Channel Magnet Assemblies Electric Lifting Magnet Magnetic Door Latches Torque & Linear Couplers Voice Coil Motors Other Electronic Components Magnetic Modules for Wind Power About Us About Us Why Stanford Magnets Technologies & Services Technologies & Services Low HRE & HRE Free Technology Grain Refining Technology Grain Boundary Diffusion Multipole Ring Technology Coating Blanket Order Plan Calculator Knowledge Base News FAQs Glossary of Magnet Products GET A QUOTE Cow Magnets Home Magnetic Assembly Cow Magnets Cow Magnets What is a cow magnet？Have you ever heard of this type of magnet? Actually, cow magnets are very popular with farmers, ranchers, and veterinarians since they are a well-known method of preventing hardware disease in cattle. So what’s hardware disease? Cow Magnets About Hardware Disease Hardware disease is a common term for bovine traumatic reticulopericarditis. It is usually caused by the ingestion of a sharp, metallic object. These pieces of metal settle in the reticulum and can irritate or penetrate the lining. It is most common in dairy cattle but is occasionally seen in beef cattle. It is very rarely reported in any other ruminants. It can be difficult to conclusively diagnose but can be prevented by the oral administration of a magnet around the time that the animal reaches the age of one year. Causes of Hardware Disease Cattle commonly swallow foreign objects, because they do not use their lips to discriminate between materials and they do not completely chew their feed before swallowing. Sharp metal objects (such as nails or iron wires) are a common cause of hardware diseases. The object travels into the rumen and is then pushed into the reticulum along with the rest of the feed. In some cases, contractions of the reticulum can push the object through part of the reticulum wall into the peritoneal cavity, where it causes severe inflammation. In rare cases, the metal object penetrates the entire wall of the reticulum and can pierce the heart sac, causing pericarditis. Compression by the uterus in late pregnancy, straining during parturition, and mounting during estrus can increase the likelihood of the object penetrating the abdominal wall or the heart sac. How to prevent the hardware disease? A cow magnet is a kind of veterinary medical equipment used to treat or prevent hardware diseases of cattle. Traditionally, the cow magnets are strong Alnico magnets, in the shape of a smooth rod, about 1 cm by 8 cm (0.4 by 3.1 inches). However, today they are more commonly several ring-shaped ferrite magnets attached to a stainless-steel or plastic core, in the same shape as the single-piece original. Newer designs to help increase effectiveness include a cage design, in which the magnet holds metal objects inside a protective plastic framework. Even newer designs include a stronger array of rare-earth magnets inside a stainless steel body that resembles the original Alnico design. A rancher or dairy farmer feeds a magnet to each calf at branding time; the magnet settles in the rumen or reticulum and remains there for the life of the animal. The magnet is administered after fasting the cow for 18–24 hours. This is most effective if done to the entire herd before the age of one. The cow magnet attracts such objects and prevents them from becoming lodged in the animal’s tissue. While the resultant mass of iron remains in the cow’s rumen as a pseudobezoar (an intentionally introduced bezoar), it does not cause the severe problems of hardware disease. Cow magnets cannot be passed through a cow’s 4th bonivial meta-colon. Cow magnets are widely available from veterinary, feed supply, and scientific supply sources. Conclusion Thank you for reading our article and we hope it can help you to have a better understanding of the cow magnets. If you want to learn more about magnets, we would like to advise you to visit Stanford Magnets for more information. As a leading magnet supplier across the world, Stanford Magnets has been involved in R&D, manufacturing, and sales of magnets since the 1990s. It provides customers with high-quality permanent magnets like SmCo magnets, neodymium magnets, AlNiCo magnets, and ferrite magnets (ceramic magnets) at a very competitive price. Post Views: 79,700 Tags: Alnico magnets, Ceramic Magnets, Cow Magnet, ferrite magnets, Hardware Disease, leading magnet supplier, Neodymium Magnets, rare earth magnets, SmCo Magnets, Stanford Magnets, types of magnet Leave a Comment Your email address will not be published. Required fields are marked * Add Comment Have Drawings & Need A Quote? Send us your prints and specifications detailing your custom magnet application. GET A QUOTE Our Products Neodymium Magnets Samarium Cobalt magnets 3M Adhesive Backed Magnets Rubber Coated Magnets Mounting Magnets Standard Magnets Neodymium Arc Magnets Neodymium Block Magnets Neodymium Countersunk Magnets Neodymium Cylinder Magnets Neodymium Disc Magnets Neodymium Ring Magnets Useful Links Typical Products Privacy Policy Knowledge Base FAQ Calculator Contact Us Inquiry Glossary Terms & Conditions Sitemap Recent News Alnico vs. Rare Earth Magnets: Understanding the Differences and Applications Alnico Magnets: From Electric Guitars to Particle Accelerators The Role of Electromagnets in Advanced Robotics and Automation Electromagnets in Everyday Life: From Healthcare to Transportation Halbach Arrays: The Ingenious Magnetic Arrangement You Need to Know About FIND THE PULL FORCE NOW Copyright © 1994-2024 Stanford Magnets owned by Oceania International LLC, All Rights Reserved. Leave a message",
    "commentLink": "https://news.ycombinator.com/item?id=39982024",
    "commentBody": "Cow Magnets (stanfordmagnets.com)352 points by Tomte 16 hours agohidepastfavorite120 comments plowjockey 14 hours agoWhile we had vets put magnets down cows over the years, my parents were too frugal to let me have one! There are two primary ways steel gets into the feed supply. A silage chopper can pick it up and cut it into fine bits or a hay baler picks up a stray end of wire and running large bales into a tub grinder to more easily mix feed stocks into a total mixed ration will chop the stray wire (and other parts) into moderately short bits. Many feed wagons have strong magnets under the discharge chute and grinder/mixers (used to grind grain and supplements) have likewise strong magnets just before the material enters the hammer mill. Despite that some bits will slip by due to the volume of material passing over the magnets as it comes out of the feed wagon. We do have the vets administer magnets to the heifers we keep at around 9 months of age. One has to watch to be sure they don't work them back up and spit them out! A couple of years ago we had some that did that. Symptoms of hardware is usually a cow \"off feed\" meaning she is not coming to the bunk to eat with the rest but usually stands apart and in acute cases will become hunched back. A veterinarian can usually diagnose it with a stethoscope as I think the breathing becomes labored in more acute cases. reply gumby 13 hours agoparent> Many feed wagons have strong magnets under the discharge chute.. Even though I understand it's 1/r^2 this summoned a vision of a cow magnetically stuck to the discharge tube at her neck, with the head pushed up and mooing unhappily. reply marcosdumay 11 hours agorootparentI think the force is actually 1/r^4. The magnetic field is proportional to 1/r^3. But the linear force is not proportional to it. (Rotations are.) reply planede 56 minutes agorootparentIt's 1/r^4 for magnetic dipoles, where both have fixed a fixed dipole moment. So it's 1/r^4 between two permanent magnets. Where one object is a permanent magnet and the other is some unmagnetized ferromagnetic/paramagnetic piece metal, then the dipole moment of that piece of metal also depends on the distance. Assuming it's proportional to the magnetic field of the other dipole (~1/r^3) then the force is going to be ~1/r^7 for this pair of objects. reply peterleiser 11 hours agoparentprevThat's too bad. My parents were farmers and my dad brought some home one day, even though we no longer had cattle when we lived on our ranch. He thought I'd like to play with them, and he was right! Even my friends who weren't from farm families thought they were cool. I'm going to order some for my kids. These were the classic ones in the 1980's: https://en.m.wikipedia.org/wiki/Hardware_disease#/media/File... reply DaiPlusPlus 13 hours agoparentprev> my parents were too frugal to let me have one! Your parents wouldn’t let you swallow neodymium magnets? reply throw_a_grenade 12 hours agorootparentNow that you have come of age, you can proceed to ingest the damn magnet. Free yourself from the chains (sic) of the patriarchate! reply 01HNNWZ0MV43FF 11 hours agorootparentprevHonestly it looks fun reply 0xdeadbeefbabe 13 hours agorootparentprevFrugality first safety second reply wyclif 4 hours agoparentprevAlnico magnets are the same type of magnets used in vintage electric guitar pickups. reply SubiculumCode 13 hours agoparentprevGot one once. Nice strong magnet with an odd shape....you can just imagine it going down the gullet. reply Bob_LaBLahh 12 hours agoprevI wonder if there is any correlation between cattle containing cow magnets and the strange phenomena of cattle tending to align themselves on a north/south axis. I'm guessing that the answer is \"not much\" since deer lineup too and they (probably) don't have magnets in their stomachs. I still think this deserves the Mythbusters treatment, right? https://www.npr.org/2009/03/16/101945271/power-lines-upset-c... reply AnotherGoodName 12 hours agoparentI wonder if it's as simple as wanting to catch the most sun hence a north south orientation to expose the flanks and the shadows from high voltage towers interfere with this hence the difference under those towers. reply flysand7 7 hours agorootparentI found the relevant research: https://www.pnas.org/doi/pdf/10.1073/pnas.0803650105 > Furthermore, there was no correlation between the position of roe deer and the time of day when the observation took place, meaning that the position of the sun had no influence on deer orientation. Although I'm not sure how credible this information is given that they studied google images where who knows what time of day they are taking images at. Maybe there was a correlation. reply d1sxeyes 2 hours agorootparentIt should be fairly possible to determine the time an aerial photograph was taken by studying the angle cast by shadows in the photograph. reply Bob_LaBLahh 6 hours agorootparentprevThank you! My curious but busy/lazy ass wanted to know the answer but wasn't gonna do the legwork. reply RoyalHenOil 11 hours agorootparentprevThis sounds like the likely explanation. Crop rows and greenhouses are generally aligned North-South for the same reason: to maximize direct sun exposure. reply plowjockey 7 hours agoparentprevI admit that I've never noticed that. They seem to lay whatever direction seems comfortable. In winter time the will lay east-west to catch the most warmth and will often roll on their side to really catch some rays. reply djmips 4 hours agoparentprevWhy would a cow magnet settle in the rumen any particular cow related orientation? reply inasio 12 hours agoprevOur local hackspace occasionally gets donations from Amazon, often very random things. One time we got a box full of these cow magnets. They're pretty strong, have mostly been used as fridge magnets though reply ethbr1 15 hours agoprevHuman healthcare would be much more affordable if it operated on the veterinary model. 'Nails cause problems when they pass through a bovine digestive tract.' 'Well, let's keep them from passing by feeding cows a magnet.' Problem practically solved. reply dexwiz 13 hours agoparentVeterinary models assume fairly limited lifespans and are economically focused, especially for large animals. If we followed that model it would probably assume no issues past 65 matters, because death and retirement would be the same. Also any issue that costs more to treat than the remaining economic output of that person would result in a trip to the glue factory. Animals often have relatively short lifespans. For cows its about 2 years for beef and 6 years for dairy. Would these methods work for animals that live more than a decade? reply jaredhallen 9 hours agorootparentA note on terminology. Where I come from, at leat, a \"cow\" is a mature female breeding animal. We'd expect at least ten years out of such an animal, barring any unforeseen circumstances. A young, immature female would be referred to as a heifer. When the \"cows\" \"calve\" each year, obviously some of the calves are male and some are female. Most of the males will be castrated at a young age, for the purpose of preventing testosterone from negatively impacting the musculature in relation to meat quality. A castrated bull is referred to as a steer. A steer will almost certainly be raised for slaughter, and for that particular animal, an 18-24 month lifespan is a good estimate. A heifer may either be raised for slaughter, or may be added to the breeding herd. So their lifespan depends. Then there are bulls, who are males who are left intact. These are relatively rare, and their lifespans are shorter than a \"cow,\" because their main purpose is breeding and they become less effective at that job over time. But longer than an animal that's sent to slaughter. Maybe 6 years or so. But the bulls probably don't affect the average too much. reply denton-scratch 12 hours agorootparentprev> For cows its about 2 years for beef Auguste Escoffier said that 2 years was much too young; you couldn't make good beef stock from such young bones. He said that you had to go to France to get 5-year-old stock bones, the British slaughtered them too young. This was in the early 20thC. reply tedunangst 12 hours agorootparentAnd what influence has the esteemed M. Escoffier's treatise had on the modern ranching industry? reply saalweachter 11 hours agorootparentprevWhat, he couldn't get an old dairy cow or something for his stock pot? reply ethbr1 13 hours agorootparentprevThere's also an acceptance in the veterinary profession, partly because of the economics that you describe, that 75% effectiveness at 25% of the cost is a good deal. Yet for people medicine that's anathema. Instead, we hide the cruel economic reality behind intermediaries (insurance companies, public hospitals, etc.) and then are shocked when it still leaks out. No country without infinite money can afford to spend whatever it takes, on everyone, forever. reply bborud 12 hours agorootparentprevFor livestock, yes. For pets, not so much. People spend stupid amounts of money on their pets and expect results. reply tedunangst 12 hours agorootparentprevUSDA graded beef is 30-42 months. 3 years +/- 0.5. reply dylan604 13 hours agorootparentprevnext [2 more] [flagged] dexwiz 13 hours agorootparentWhen did a self-earned 401k and a midlevel salary job become a golden parachute? Also the parachute wouldn't have heights, it would be an ivory tower from which I could jump. If you are going to insult me, then at least use proper metaphors. reply apitman 14 hours agoparentprevWhat do you call a veterinarian that only treats a single species? A medical doctor. reply shagie 10 hours agorootparentThe vet that I went to only handled cats. Every vet in that practice was a cat specialist, every room was designed with cats in mind, and everyone who worked the front desk was a cat lover themselves. While the joke is there, there are single species veterinary practices. https://catcareclinic.net https://citydogvet.com reply Ichthypresbyter 8 hours agorootparentPlus of course all of the horse specialists, particularly around the racing industry. reply kstenerud 4 hours agorootparentAnd no one can talk to a horse, of course. reply defrost 4 hours agorootparenthmm, but there are whisperers. reply nrml_amnt 14 hours agoparentprevMake an appointment at a VA clinic reply non-chalad 9 hours agorootparentWrong kind of vet. reply naikrovek 15 hours agoparentprev“Cancer treatment is expensive.” “Kill the patient” Problem practically solved? Much more affordable, sure, but not better in any other measure. reply ethbr1 14 hours agorootparentVets don't put down animals needlessly, but they do practice medicine closer to engineering. There's always a cost-benefit calculation, and the answer should never be 'Whatever the cost.' reply 082349872349872 14 hours agorootparentVets have an elevated suicide rate, which I ascribe to three factors: - they can self-prescribe - they commonly use, unlike human doctors, euthanasia to end suffering - they became vets because they really like animals, but many owners seem more concerned with their pocketbooks Lagniappe: https://www.youtube.com/watch?v=JChwFoCxVC8 reply AnotherGoodName 11 hours agorootparentFrom experience with a vet in the family it's very much a 'who the fuck in their right mind would do this?!' type of career to anyone that has the ability to ask a vet their honest opinion about their career choice. It's literally a medical science degree with a speciality. The same degree that can get you a very highly paid jobs elsewhere yet it doesn't pay that well. You'll likely end up in a poorly paid 24hour live-in clinic with horrible pay and hours surrounded by euthanasia and the associated grief non-stop all the while with those same drugs sitting there. The mortality rate is so horrific that news of another suicide from the graduating year becomes a 'we've lost yet another one' type of statistic. It's not often you ask someone to give a late teen thoughts on their upcoming career choice and have them rightfully say 'Don't! It's not worth it!' but this literally happened with vet science in my family. reply GlenTheMachine 7 hours agorootparentYes, and also the human owners are increasingly crazy. As in, they want the same kind of unlimited care a human would get, but don’t want to pay for it and take it out on the vet when presented with the bill. You never really been Karened until you’ve been crazy cat lady Karened. reply plowjockey 7 hours agorootparentprevI wonder if this occurs more with small animal vets--\"pet vets\"? I know of a number of large animal vets that retired and enjoyed many years of retirement. Perhaps being in the large animal practice where decisions are made on economics and not emotions helps them survive. Those vets also did small animals but it was probably more of a sideline for them. reply defrost 7 hours agorootparentThere's some crazy in equine vet work - the racing side can be a bit brutal with some exposure to criminal elements (not extensive but in places it can be hard to avoid) and the crazy horse lady types can drive vets to drink; these are people that love horses, collect horses, but can't afford to feed them or bear to see them put down, etc. In general though large animal rural vet work is smoother sailing than the small stuff (from conversations I've had thanks to a farm background and to having developed a bit of animal history recording software for agistmentstud records some decades back). reply peterleiser 11 hours agorootparentprevI had the same experience when doing a career report in junior highschool. I spoke to a vet who had graduated from UC Davis a few years prior and they did everything in their power to talk me out of becoming one. reply ethbr1 13 hours agorootparentprevSelf-prescription is definitely part of it, but farmers have brutal suicide rates too. Social isolation + a business where the business often runs counter to your moral preferences = a tough life for both The \"maybe next year\" line gets me every time: https://m.youtube.com/watch?v=ZRDaPEaDJ7E reply d1sxeyes 2 hours agorootparentprevThey can’t really self-prescribe though, they are not allowed to prescribe drugs to humans. In fact, in many jurisdictions, doctors are legally allowed to self-prescribe. Perhaps the difference is that vets can both prescribe and dispense drugs, while human doctors often need to have a pharmacist dispense the drugs. reply naikrovek 14 hours agorootparentprev> There's always a cost-benefit calculation, and the answer should never be 'Whatever the cost.' You think like an insurance company: completely detached from reality. reply dgacmu 14 hours agorootparentNo, there literally is always a tradeoff, even in human medicine. I was at my doctor's office yesterday because I'm an idiot and injured myself. I asked about the diagnostic advantages of getting an MRI vs an x-ray for the injury (sciatic pain, probably from an injury to my piriformis; don't snowboard on the east coast). The MRI might have provided better diagnostics, but the x-ray (much faster, at lower cost, but at some radiation exposure to me) ruled out the really serious stuff, and the treatment plan was the same regardless. So: Tradeoff. It was, IMO, both financially and ethically better to use the low-resource diagnostic modality instead of tying up the rare and expensive one, even though it gave me a bit of radiation that in an ideal case I wouldn't have had. reply wpietri 13 hours agorootparentFor sure. During my mom's cancer treatment, these things came up too. E.g., In going through some options, her neuro-oncologist mentioned one drug, Avastin I think, which would have been something like $100k. In his estimation was unlikely to make any difference in her prognosis, and at best would have been a difference of days of lifespan. She was on Medicare, so it was no cost to us either way, but we all felt like it would have been a waste of money. At some point during this process it became clear to me that a \"whatever the cost\" approach is understandable but wrong. We're all going to die. If an intervention can restore somebody to health, to give them years of a good life, that's great. But an awful lot of money is spent on what seemed to me like prolonging the misery. That's not something I want for myself, and it's definitely not something my mom wanted. So as long as there was some hope of more good time, we fought and fought hard. But when hope ran out, we were off to hospice with no regrets. She died peacefully, surrounded by loved ones. It was a much better death than having a lot of futile last-minute interventions. Speaking of which, if this makes sense to you, make sure your loved ones know your preferences. We had all done living wills years before, and it was such a balm to know exactly what she wanted. I've used Five Wishes for this, and I'm told there are other, possibly better options now too. reply vba616 12 hours agorootparent>It was a much better death than having a lot of futile last-minute interventions. It's easy to say that sort of thing, so everyone does. It makes plenty of sense. People don't want to die in the hospital or go through hell in their last days or weeks. But nobody wants to die right now, ever. No matter what they said before or what papers they signed. The standard picture, the logic, makes perfect crystalline sense up until there is a choice between going to the hospital right now and living an undefined amount of time, maybe only a day or a week, but longer than the next few minutes. reply sojournerc 9 hours agorootparentMy mom (and my brothers and I) chose hospice (cancer) during COVID so she could be with her friends and family in her dying days. She stopped eating and drinking because she knew it was time. That was a sort of last gift, not having to see her languish for weeks.(although I will never stop being bitter about being unable to have a proper funeral and memorial service). reply wpietri 11 hours agorootparentprevSorry, but this is incorrect: > But nobody wants to die right now, ever. No matter what they said before or what papers they signed. Plenty of people recognize when it's time. When my mom was diagnosed with glioblastoma, her surgeon said, \"This is what you will die from.\" That's hard to hear, but people can definitely take it on board. To realize that it's not a choice of whether, just how. Take, Brittany Maynard, who had the same thing my mom did: https://en.wikipedia.org/wiki/Brittany_Maynard I can go as far as agreeing that American culture has a lot of collective anxiety about death, and a consequent refusal to deal with is calmly. But there are plenty of other approaches to that. Like the European movement known as Death Cafe: https://en.wikipedia.org/wiki/Death_Cafe Or the (sadly now defunct) Zen Hospice here in SF: https://www.businessinsider.com/photos-of-zen-hospice-projec... Many things in our lives can be scary. But we can shape our relationships to them. And given that death comes to all of us, I think it's worth taking the time to get on good terms with it. reply neon5077 13 hours agorootparentprevNo, the reality is that an animal is usually an asset. It costs X to feed and maintain an animal over its life to achieve Y amount of profit from its sale or the sale of its products. If X plus medical costs is greater than Y, you're losing money on this animal. Tripling the amount you pay the vet does not increase the final profit margin of the animal, so you simply do not. If basic treatment costs more than the animal will generate in its life, you don't treat the animal. Thinking that any random farm animal is worth infinite medical resources is completely detached from reality. The animal is not worth it. There's not one single reason to pay more to maintain an animal than the money you get out of it. Not in this context. reply pjerem 14 hours agorootparentprevYou are interpreting this wrongly. It’s not because there is a cost-benefit that the cost for a high benefit can’t be high. Healthcare budget is not infinite but in most countries, it’s high enough per operation that, for a given individual, it’s virtually infinite. Insurance companies don’t care about the benefit, they just want the cost to be low, it’s not the same thing. reply ethbr1 13 hours agorootparent> Insurance companies don’t care about the benefit, they just want the cost to be low, it’s not the same thing. Also underappreciated, at the end of the day: {insurance premiums} >= {average cost of care} The actuarial calculations don't change because they're concealed behind group-blended risk. If cost of care increases, premiums must increase as well. reply refurb 2 hours agorootparentprevI work in healthcare economics and this is false. There are no healthcare system in the world where the budget is “high enough to be virtually infinite”. All health systems are all heavily, heavily constrained by budget. The US system is the least constrained (i.e. if you want the latest cancer care paid for, the US is the place with the highest probability it’ll happen), but since there are thousands of insurance plans it’s not even and any two patients may have plans that make different trade offs. And governments do the very similar math as the insurance companies when it comes to cost-benefit analyses - that’s how they design their benefit offerings. The difference between systems like Canada (with universal care) and the US is that in Canada the end user doesnt see how the sausage is made. That’s because the trade offs are the same for everyone and doctors know a given trade off has been made and the technology that is not paid for is just not even brought up to the patient. reply non-chalad 9 hours agorootparentprevMight we interest you in M.A.I.D? (1) 1. https://www.theatlantic.com/magazine/archive/2023/06/canada-... reply ethbr1 6 hours agorootparentFull-disclosure: personally I'm a big supporter of an individual's right to choose to end their own life in a medically-facilitated manner. There's something weird about letting people who are happy with life (read: society) choose to force someone who isn't to continue living, against their wishes. reply non-chalad 6 hours agorootparentGovernmemt should not be involved in ending someone's life; Not at their own choosing, nor at forcing them to continue living. reply ninjanomnom 5 hours agorootparentThe alternatives then are self service, or private businesses providing assistance? If we want to endorse the former then educating people on the best way to painlessly end their life might be a good idea, the latter though seems like a bad idea with some very badly aligned incentives. reply _heimdall 14 hours agorootparentprevThat's not a great analogy. There are plenty of alternative treatments for cancer, and speaking of cattle there were actually Mayo Clinic doctors in the early 1900s using raw milk diets to treat serious conditions including cancer. A more apt comparison here would be vets throwing pharmaceuticals and radiation at cows who ingested metals rather than a more simple solution intervention. reply hoseja 2 hours agorootparentprev\"Cancer is terminal.\" \"Bankrupt and undignify the patient and their family with needless, hopeless treatment.\" Humane, functional american healthcare. reply ajuc 13 hours agoparentprevHuman healthcare is pretty affordable when it's not absurdly misinsentivized and mismanaged. reply non-chalad 9 hours agorootparentIs it too much to ask for that I want to be the customer paying the treatment, instead of the insurance agency? Put the incentives where they count. reply pjc50 36 minutes agorootparentYou can, if you can afford it. Few can afford emergencies. reply el_snark 8 hours agoprevThis is incidentally why cows can't travel on planes. They always get stopped at the body scanner. reply mhuffman 13 hours agoprevCow magnets are great refrigerator magnets! Very cheap, can be used at different angles, and stronger than pretty much any other refrigerator magnet. reply aidenn0 12 hours agoparentThe magnets designed for glass whiteboards are arguably too powerful for working as a refrigerator magnet. I was able to pin a 100 page paperback book to the fridge with one of these[1]. For fridges I recommend these[2], which are $8-15 for a ten-pack and plenty strong (plus the shape makes them easy to grab). 1: https://amazingmagnets.com/product-category/large-pawn-magne... 2: https://amazingmagnets.com/product-category/WB-Series-Whiteb... reply djmips 4 hours agorootparentI feel like in an argumentive mood. I like very strong fridge magnets. They can hold thicker items. reply hoseja 2 hours agorootparentWhy do you want to affix thick articles to your fridge door. reply mgerdts 12 hours agoparentprevA few decades back I fabricated and wired industrial controls. While wiring, we used cow magnets to attach the blueprints to the open cabinet doors so that they were always within sight was we strung wires between PLCs, terminal blocks, relays, buttons, etc. Even when there were several pages of blueprint stapled together a few cow magnets tended to hold them in place. You just had to be sure to orient them such that they didn’t roll down the door. reply connectsnk 12 hours agoprevAm I the only one who is thinking that cows must feel terrible their whole lives. reply eitland 3 hours agoparentI think it varies a lot fromncow to cow (and from farm to farm). I grew up on a (small) farm and we liked them and were fond of them and they seemed to be the same. Yes, they were absolutely overjoyed when I took them to the pasture at the start of the summer, but no, they didn't seem to bother extremely much when I collected the after the summer. reply ornornor 4 hours agoparentprevThat’s one of the reasons why many of us refuse to eat them (or any other animal) reply dang 13 hours agoprevRelated: Cow Magnets - https://news.ycombinator.com/item?id=10176652 - Sept 2015 (71 comments) reply destitude 9 hours agoprevThis isn't just about the feed. I once saw the remains of some cattle that got hit by a train and was dumbfounded by the amount of trash they had eaten. You'd be surprised by what they will eat and if the farm is pretty \"trashy\" that ends up in the cattle as well. reply Zebfross 7 hours agoprevI had a pair of these growing up, and they're super fun to play with. They have great interactions and make interesting sounds. reply Wistar 15 hours agoprevI lived in a rural area growing up and it was not uncommon to come across the cow magnets in fields, sides-of-road, and so on. I think I still have a couple of them stowed away somewhere in my pack-ratty shelves. I recall that they were not very strong magnets. reply 542458 14 hours agoparent> I recall that they were not very strong magnets. I have one on my desk right now, and it’s pretty strong! They’re historically made from Alnico alloys, which were the strongest type of permanent magnet until rare earth magnets were discovered. reply ortusdux 15 hours agoparentprevMy local co-op had them as impulse items at the checkstand. I seem to remember them costing as much as a candy bar, and my parents giving in and getting me a few. IIRC, they were slightly stronger than ceramic magnets, and much less brittle. reply lardo 15 hours agoparentprevSome of my fondest childhood memories are summer visits to my great uncles farm. Playing with a cow magnet in the dirt under the bench grinder in the tractor barn is one of them. I remember them being strong, but I was 10. reply zikduruqe 15 hours agoparentprevI used to play with them like Rattle Magnets. https://www.poundfun.com/cdn/shop/files/RattleMagnets.png?v=... They weren't as fun, but same principle. reply rlonstein 15 hours agoparentprevA great-uncle had a dairy farm. We played with the magnets too. reply cwillu 11 hours agoprevThis is why cow tools aren't made of metal. reply pfdietz 16 hours agoprevI assume they're recovered, cleaned and sterilized, and offered for sale after slaughter. reply kokanator 13 hours agoparentMy father worked in a slaughterhouse. The magnets were often recovered in the gut room ( the place where they process guts and hides for the rendering house ). As a result I had/have a pile of these magnets. They actually come in several different forms. I had a few that were covered with some type of plastic and a few others that simply looked like the rough magnets you might find on the back of a fridge magnet just larger. My guess is the farmer would use whatever was actually cheap and available regardless of whether they were \"cow magnets\". reply bap 15 hours agoparentprevMy father owned a rural feed store until I was 6 or 7 years old (the early 80's.) Other than incubators full of chicks and ducklings, and bottle feeding young livestock fresh from auction - swiping cow magnets off the shelf to play with was a favorite pass-time. :) In those days I can't imagine anyone really cared what happened to the magnets after they went into the cow. Perhaps something happens in the slaughter house to clear out whatever has gotten into the gut and stomachs - for the sake of meat grinding machinery? reply HeyLaughingBoy 14 hours agorootparentI think tripe is the only use an industrial abattoir would have for the stomach and that's likely to be handled by a human. My guess is that they have a bucket or something that they put foreign material into. Hmmm, but I wasn't thinking of pet food: that would probably be ground. reply pfdietz 14 hours agorootparentprevI mean, you don't want the magnets, or any nails or what not they've picked up, to go into any separated products. And separating them from the guts shouldn't be too hard -- they'd be attracted to metal, after all. The question would be how rapidly they degrade. reply rottencupcakes 15 hours agoparentprevhttps://www.ebay.com/itm/295122032671 For your enjoyment. reply neilv 15 hours agorootparentI'm happy to say that the cow magnets I saw as a kid were more shiny, and (I hope) not used. reply DaiPlusPlus 13 hours agorootparentprevI’m getting flashbacks to “drop & run” reply notfed 8 hours agoprevIsn't this dangerous? I'm imagining guts getting punched between a magnet and some metal... reply deepspace 7 hours agoparentI was thinking that as well, but I think the shape of the magnets have something to do with it. For human children, swallowing a single magnet is a nuisance, but swallowing more than one is a medical emergency, for exactly that reason. reply djmips 3 hours agorootparentMore than one with some (unknown to me) period between. If swallowed one after the other, I presume they would just stick together without much issue. reply nicbou 8 hours agoprevThis is an unnecessarily verbose way of saying \"cows eat random things including metal bits that can kill them, so we have them eat a magnet to catch them\". Then again, it's content marketing. It's written for search engines, not humans. reply screamingninja 7 hours agoparent> unnecessarily verbose way What are you referring to? I just see two words in the title: \"Cow Magnets\" reply nicbou 3 hours agorootparentI refer to the article's contents reply repiret 10 hours agoprevIncidentally, you often put magnets in a hydraulic sump for much the same reason. reply userbinator 5 hours agoparentThat's what this reminded me of too. Very common to have one in the oil pan, transmission pan, and rear axle, often built into the drain plug, to catch metal debris. reply kazinator 11 hours agoprevMaybe we've been wrong all along about toddlers swallowing magnets? ;) reply rzzzt 10 hours agoparentUnfortunately not, some arrangements can pinch together sections of the gut as they pass through and that's a very big problem. No reticulum in toddlers. Rumination is preserved for thoughts only when they get bigger. reply deepspace 6 hours agorootparentSpecifically, swallowing ONE magnet is probably fine, since children do not tend to eat metal frequently. Swallowing multiple strong magnets brings the risk of pinching (and necrotizing) sections of the gut. reply BrandonMarc 7 hours agorootparentprev... and for that kind of rumination, we have therapists to help out. reply unwind 14 hours agoprevI guess I still can't read, because I couldn't find the link to their actual products, they are a magnet vendor after all. Very confusing. I had to search for it, but [1] must be linked to somewhere in the article, right? [1]: https://www.stanfordmagnets.com/alnico-cow-magnets.html reply blowski 13 hours agoparentI don’t imagine it’s an industry that sees much opportunity in converting impulse buyers. reply wpietri 13 hours agoparentprevI learned about these accidentally last year and thought they were hilarious, so I bought some here: https://www.magnetsource.com/collections/cow-magnets-ru-mast... I'm neither a veterinarian nor a cow, so I don't know how fit for purpose they are, but to me they seem well made. reply gumby 13 hours agoprevAdminister no more than an odd number less than 2! reply waterhouse 8 hours agoprevAn implication is that, of the metallic debris a cow might ingest, a large fraction of it is magnetizable—otherwise there'd be little point. I wonder, does this feed back into the decisions that are made? Do people deliberately choose magnetizable metals for things that might get broken up in a cow's environment? reply pierrec 13 hours agoprev\"Cow magnets cannot be passed through a cow’s 4th bonivial meta-colon\" Hah, \"bonivial meta-colon\" does not even resemble any real combination of words. Sure, cows have multiple stomachs, but come on. Now I'm wondering how many joke sentences are mixed in. And I almost believed the bonivial meta-colon because \"Stanford\" is in the domain name :) reply wzdd 11 hours agoparentApparently it’s been there for a while: the Wikipedia talk page for this topic has someone pointing this out in 2009. reply deanresin 15 hours agoprevI now know what a cow magnet is. reply ec109685 4 hours agoparentAs well as hardware disease. reply praptak 14 hours agoparentprevDisappointingly, it's not a magnet that attracts cows. reply aidenn0 12 hours agorootparentI've tried so hard to not make a joke about magnets fed to baby chickens, but you pushed me over the edge. reply drpixie 6 hours agorootparentIn Australia, a \"chick magnet\" is a large attention seeking car owned by a young guy. It is intended to attract girls (chicks). It always has a serious stereo, and probably has a modified exhaust. Young guys spend a fortune on their \"chick magnet\", presumably to indicate that they could spend a fortune on potential girlfriends. The desired outcome often fails to eventuate - many \"chick magnets\" are only driven up and down the main street occupied by the single owner and his single friends ;) reply aidenn0 5 hours agorootparentIn the US a \"chick magnet\" usually refers to a person who can attract women, but it can also refer to an accessory (including a car) with such a desired purpose. reply block_dagger 10 hours agoprev [–] How we treat these animals is horrific. We force feed them magnets to counteract the effects of our domination over them. reply eitland 3 hours agoparent [–] The thing is cows eat metal while grazing freely too. I have a background in farming and there has been cases of cows ingesting rather weird things absolutely on their own. One case I remember hearing about was a cow eating a pack of leftover bolts from construction teams. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Stanford Magnets is a reputable supplier of neodymium, SmCo, AlNiCo, and ferrite magnets, offering custom solutions and assemblies for diverse applications.",
      "They stand out for providing cow magnets that help prevent hardware disease in cattle by attracting and deterring metal ingestion.",
      "With decades of experience since the 1990s, Stanford Magnets delivers top-notch magnetic products at competitive prices."
    ],
    "commentSummary": [
      "Cow magnets are used to prevent cows from ingesting metal objects, avoiding symptoms of hardware disease.",
      "The discussion includes personal stories, economic differences in vet vs. human medicine, and challenges in veterinary care.",
      "Topics cover end-of-life decisions for animals, recovery, and sterilization of cow magnets post-slaughter."
    ],
    "points": 352,
    "commentCount": 121,
    "retryCount": 0,
    "time": 1712684700
  },
  {
    "id": 39981550,
    "title": "Fairbuds: Sustainable In-ear Tech with Replaceable Batteries",
    "originLink": "https://shop.fairphone.com/fairbuds",
    "originBody": "FAIRBUDS OVERVIEW FEATURES specS buy now choose a section OVERVIEW FEATURES SPECS FAIRBUDS Premium Sound. Designed to last. Meet the world's most repairable premium earbuds. 0.00 plus shipping costs from € 3.95 buy now Keep what you love for longer Long-lasting with a three-year warranty 14-day trial period with 100% full refund Replaceable batteries inside buds and charging case Iconic design. Available in two colors. Color Color Premium sound FINE-TUNED FOR PERFECTION Active Noise Canceling Immerse yourself in your music. Or stay connected to the world around you. It's up to you. TITANIUM DRIVERS The titanium-coated 11mm premium drivers deliver a one-of-a-kind listening experience, with rich, well-balanced sound profiles. Seamless dual point connectivity Stay connected to multiple devices simultaneously and switch between them hassle-free. Dedicated app For iOS and Android Pick your favorite presets, customize EQ settings just how you like it, and get all the latest software updates. 6h EARBUDS + 20h CHARGING CASE Up to 6 hours of listening in a single charge and 20 additional hours with charging case. Designed to last. Built for living worry-free 3 year Warranty Your Fairbuds come standard with a two year warranty.We're extending that by another year. Because we've got your back. Easily Repairable It was about time the True Wireless Earbuds category got the signature Fairphone treatment: Built to last and fully repairable. Replaceable Batteries Old batteries should never be the end of your earbuds. That's why we designed the Fairbuds with replaceable batteries - in the charging case and both buds! IP54 Sweat and Water Resistance It always rains in Amsterdam - so of course these had to be weatherproof. That also makes them great for working out. Small size. Big impact. Small size. Big impact. Made with fair and recycled materials Made in fair factories Climate conscious and electronic waste neutral More about our fair approach Small size. Big impact. Made with fair and recycled materials We integrate Fairtrade Gold into the Fairbuds’ supply chain and invest in Fairmined Gold Credits, Fairtrade Silver Credits and Cobalt Credits to account for the Fairbuds’ gold, silver and cobalt footprint. They also contain recycled materials such as rare earth elements and post-consumer recycled plastic. Made in fair factories The people who help to assemble the Fairbuds are paid a living wage bonus that helps to bridge the gap between minimum wage and a decent salary. Plus they get an active voice in improving working conditions with factory management. Electronic waste neutral The Fairbuds are electronic waste neutral. What does this mean? The Fairbuds weigh 78 grams with the charging case. So, for every Fairbuds we put out into the world, we make it a point to responsibly collect and recycle 78 grams of electronic waste. Climate conscious Like all our products, the Fairbuds were designed to be climate conscious. The Fairbuds are designed to last really long, thanks to its modular approach. The longer it lasts, the less impact on climate it has. It uses recycled materials in the production process, reducing carbon emissions. For the remaining emissions, we invest in Gold Standard-certified carbon reduction projects. Your sound. Uninterrupted. Active Noise Cancelling for the win For pristine sound in less than perfect conditions, switch on ANC with advanced wind noise reduction. Crystal Clear Calls Always Say hello to clearer phone calls and voice recordings, thanks to Environmental Noise Canceling (ENC), powered by a six-microphone array that vastly reduces background noise. Easy to use. Easy to control. A dedicated mobile app to customise your Fairbuds CUSTOMIZE YOUR EQ SETTINGS It’s time to own your sound. Get maximum control over how your Fairbuds sound with the fully customizable eight-band equalizer. Or take it easy with one of our presets. KEEP YOUR BUDS UPDATED You can also use the app to download and install the latest firmware updates for your Fairbuds. Get the perfect fit Because one size shouldn’t fit all. Choose from three different ear tip options and find the one that fits the best. Connect to more devices Connect to multiple devices and let the Fairbuds do the work for you. The Fairbuds will know where the music is playing, so you can sit back and enjoy. Auto play and pause The Fairbuds know when they're in your ear. Take them out and your song will pause, put them back in and you won't miss a beat. Dual audio controls Control your favorite playlist straight from your Fairbuds. Adjusting the volume and playback is right at your fingertips. The specs at a glance The Fairbuds are made for all your listening and wireless needs. Controls Capacitive Touch Controls (Both Earbuds) Bluetooth & wireless Bluetooth: v5.3 (Profiles A2DP V1.2, AVRCP V1.5, HFP V1.8, HSP V1.2;) Multipoint Connection (Dual) Codec: SBC & AAC Wireless Range: up to 10M (line of sight) Connectors & charging USB Type-C Battery Battery capacity: 45mAh (earbuds) 500mAh (charging case) Number of cycles: >500 cycles Charging time: ~2 hours (case + earbuds), 10 min for 1.5h playback Total playtime (with case): Up to 26 hours Calling / Music time: Up to 5 hours (ANC on)/ Up to 6 hours (ANC off) Wireless charging: No Replaceable battery: Yes (both earbuds and case) SOUND Driver Diameter: 11mm Driver type: Titanium coated, dynamic driver Sensitivity: 104±1dB at 1KHZ Frequency Response Range: 20Hz -20KHz Driver impedance: 16Ω ±15% Microphone Total of 6 mics: 3 left, 3 right Smart assistants: Google Assistant, Apple Siri Weather resistant IP54 rating (light rain and sweat) Product dimensions Earbud: 28.7mm*24.6mm*21mm Case: 65mm*65mm*27mm Weight: 78g including case; earbuds ~5g Compatibility Compatible with Fairphone 3, 3+, 4 and 5 Compatible with any device with Bluetooth Included in the box Everything you need to get started. No more. No less. 1 Fairbuds and charging case 2 Three ear tips 3 Quick-start guide Some things are not in the box, and here’s why: We do not include a USB Cable or charger in this box because we believe that it’s better to save the environment through the reduction of plastic waste. Of course, it’s possible to buy one of these products if you need it. Get your Fairbuds today FAIRBUDS Titanium coated 11mm drivers Active Noise Cancellation with advanced wind reduction Modular and easy to repair with replaceable buds and batteries £129.00 £129.00 plus shipping costs from € 3.95 buy now Long-lasting with a three-year warranty 14-day trial period with 100% full refund Replaceable batteries inside buds and charging case Frequently asked questions Why have you launched the Fairbuds? Our mission at Fairphone is to establish a viable market for sustainable consumer electronics. Many of the problems that we see with smartphones, especially around e-waste and issues in the supply chain, also apply to audio products. In 2021, the Fairphone TWS earbuds were our first step into this new product category. They incorporated longer battery life and Fairtrade gold in the supply chain, but were, otherwise, an off-the-shelf design that was not repairable. Our plan was therefore always to improve on the design later on. The Fairbuds are the result of these efforts. What warranty do you offer on the Fairbuds? There is a standard 2-year warranty on the earbuds, plus a one-year extended warranty if users register their Fairbuds online. That said, due to the modular design and the repairability, we expect the Fairbuds to last a lot longer than three years. Which parts of the Fairbuds can be repaired or exchanged by the user? Earbud (Left and/or Right) Earbud Battery and Silicon Ring Earbud Tips Charging Case Outer Shell Charging Case Core Charging Case Battery Are the Fairbuds water-resistant/waterproof? The Fairbuds have IP54 certification, meaning they are sweat and weather resistant, but not completely waterproof. What is the Fairbuds app used for? The Fairbuds app offers users more control over their earbuds, allowing you to change equalizer presets and tune your Fairbuds to your personal preferences. You can use the app to access the latest Fairbuds updates as well, ensuring better longevity of your device, with potentially new features, and the occasional bug-fix. The app also offers users a quick start guide, tutorials, support articles and a customer service touchpoint. Learn more about the different components that make up your Fairbuds, and order replacement parts through the app if your Fairbuds are in need of repairs. Best in green electronics OUR IMPACT There are more phones than people. And behind every device is a complex supply chain. With suppliers, local communities and the wider industry, we work for fairer materials and more responsible practices. Showing the electronics industry that we can do better. Together we’re disrupting the industry’s short-term thinking that the world can no longer afford. And changing what it means to be “best.”",
    "commentLink": "https://news.ycombinator.com/item?id=39981550",
    "commentBody": "Fairbuds: In-ear with replaceable batteries (fairphone.com)339 points by pfooti 16 hours agohidepastfavorite232 comments ponorin 15 hours agoSo they finally seemed to have come up with their original design after 3 years of selling non-repairable TWS earbuds, great. I genuinely appreciate that they are trying to give users more sustainable options. Perhaps they can walk back the decision to remove the headphone jack as well so that people can use wired headphones directly which doesn't use battery in the first place. reply zer00eyz 14 hours agoparentPlease can we get jacks back. The audio quality on these battery powered blue tooth headphones is fucking awful. IM not a purist by any stretch, but a decent pair of head phones is a whole other world of music vs anything AirPods like. reply zeta0134 9 hours agorootparentI recently fell off of team 3.5mm-jack when I discovered USB-C headphones! These include their own DAC and that means they have the potential to be *much* less noisy than the usually cost-reduced DAC included in the handset. Plus this way I can splurge for one decent set of headphones and get the same mix on all my devices, which is good for composing. So, the 3.5mm jack can die. I'm now on team \"dual USB ports\" to solve the charging-while-listening issue. Let's go team! reply piaste 1 hour agorootparent> These include their own DAC Isn't that exactly why audiophiles - in the broadest sense, not just the golden cable types - stick with jack headphones? The DACs and amps built into headphones, or even earbuds, being very likely subpar compared to the one they can pick separately. Headphone DACs/amps have been a thing forever and they're typically the size of a pack of cigarettes. (Disclaimer: I made peace with BT since getting the Sony WH-1000XM4 earbuds, and later WH-1000HM4 headphones. I got them originally for the noise cancelling and LDAC support, but they're good enough that I don't hear a major difference with my friend's fancy Audio Technica) reply happymellon 52 minutes agorootparentAre there not high quality usb c to headphone jacks, with good DACs? I've still got a phone with a headphone jack, clinging onto life so not something I've investigated. Surely that's going to be the direction for folks who want an analogue socket? reply m463 6 hours agorootparentprevI would love to see a variety of cables to support your vision. USB-C to 0.78mm 2-pin or USB-C to MMCX basically a DAC cable, not a huge fiio dac thing. There are very interesting in-ear earbuds, but the whole subculture only handles 3.5mm, relying on the phone audio section. EDIT: I like this list (just one guy): https://crinacle.com/rankings/iems/ EDIT: I guess I should add his other list: https://crinacle.com/rankings/headphones/ reply aqfamnzc 3 hours agorootparentprevAny recs for a low-cost pair? reply Mo3 14 hours agorootparentprevHm. I have Sennheiser and Bose Bluetooth headphones and found AAC and AptX HD codecs on highest quality settings (on Android in dev tools) to be really good and almost indistinguishable from wired connections. I wouldn't call myself an audiophile, but my hearing is pretty good and I'm sensitive to quality loss reply zer00eyz 14 hours agorootparent>> Bose Bluetooth headphones ... AAC Funny that you picked these, I have had 5 different people tell me to try them, and I own a pair. I have some pretty broad musical tastes. If I throw on some modern, electronic music, or anything pop, these do a pretty good job. The moment I dip into jazz, into anything where the range between the quiet parts and the loud parts (read pre 1990) is large they just dont perform any more. No amount of EQ is going to make up for that missing range. The best example of this that I can give is Caltrains A Love Supreme. On something good there is a looming undercurrent in the track, the humming is alive. The track is lifeless on every bluetooth headphone I have heard. Hell even my Sonos speakers in the kitchen do a better job. reply TaylorAlexander 10 hours agorootparentI just had to double check but amazing autocorrect there. That would be Coltraine for anyone curious but I would love to hear a Caltrain version. reply tkgally 4 hours agorootparentHere you go: https://suno.com/song/2bd9142c-c41c-48fc-b28f-157239b50aee The prompt was “A jazz instrumental that combines the sensibility of ‘A Love Supreme’ with the feeling of the Caltrain transportation system.” reply zer00eyz 2 hours agorootparentLOL ... I cant believe I listened to all of that. Based on the output they need to train the model again, and remove all the mannheim steamroller. reply laserbeam 7 hours agorootparentprevThere’s a massive difference between bluetooth earbuds and $200-300 (ish) noise canceling Bose headphones. Yes, those are obviously going to sound good. I love mine. But you can’t really find a decent budget option on bluetooth. I hate mine and only use them for audiobooks. You could find reasonable budget wired earbuds even 15 years ago. reply atoav 14 hours agorootparentprevNot the parent, but wvwn if quality was decent. I have wired headphones. Good ones. And I just want to use them. Sure there are USB-C DACs you can add to your phone but meh. reply hollandheese 12 hours agorootparentGet a bluetooth DAC like the Qudelix 5k or Fiio BTR series. reply karaterobot 11 hours agorootparentprevI used to feel this way too. I don't care so much about sound quality, as I'm mostly listening to podcasts and audiobooks, but I didn't want to be reliant on teeny little batteries that have to be recharged every couple of hours. As it turns out, the unexpected benefit of not having wires is that the cord doesn't get caught on everything all the time, and that's good enough to overcome the poor battery life for me. reply brewtide 11 hours agorootparentAs someone who wears headphones basically all day at work (painter...) I 100% agree. It's a whole different experience, in a different dynamic. On a similar note, 80% of the time I'm wearing my Sony linkbuds -- the ones with the \"they have a hole in them\" passthrough, even though they have the shortest battery life of the multiple sets I own. Realistically, beyond the (not dismissing) \"they are trash when the batteries die\" aspect of things, taking your headphones out for 10-20 minutes every 4 ish hours... Is probably a good thing for our ears and minds alike. reply woogley 11 hours agorootparentI can't believe there's another linkbuds fan in the world. No other earbud feels so transparent and light to me. I actually bought a few pairs of these because I'm afraid they will discontinue them. Their newer model does not have the hole. reply rtpg 1 hour agorootparentthat model feels safe, there are loads of SKUs of them so I think there's some success. reply gnicholas 10 hours agorootparentprevDo you really have to recharge every couple hours? I use AirPods and find that they last for several (3?) hours when the mic is being used simultaneous with the speaker (zoom/phone), but much longer (5?) when just being used for playback. For me, this is plenty — especially because I don't use both ears at once. There's always way more than enough time to charge one while using the other. What wireless 'buds have only 2 hours of battery life when for just playback? reply pivo 8 hours agorootparentI’m pretty sure my AirPods Pro have only about two hours battery life, but that’s probably because they’re at least three years old by now. I still love them though and would buy them again in a second if need be. I also don’t use them for music, just meetings, podcasts and audiobooks. reply kaba0 3 hours agorootparentI assume you have the 1st gen one — I can only recommend the 2nd gen pro, I didn’t even originally want it, just got a good deal if I buy both this and an ipad, but I use the airpod each and every day for multiple hours, and I only have to think about charging the case after several weeks. The noise cancellation on the other hand is otherworldly. reply atombender 11 hours agorootparentprevSennheiser's Momentum True Wireless are really, really good. A significant upgrade from AirPods. The first two generations were notorious for connectivity glitches, but the TW3s appear to have fixed everything by. Great ANC, battery life, ear fit. They even added multipoint through a free software update that came out about a year after launch. reply wilg 14 hours agorootparentprevWhat do you mean “back”? You can buy a plethora of wired headphones. reply wisemang 14 hours agorootparentNot much use of there isn’t a plethora of devices with jacks to plug them into (which is what GP asked for) reply LeonenTheDK 14 hours agorootparentprevBack on phones. reply maven29 13 hours agorootparentprevIf only they weren't also so insistent on also obsoleting analog outputs and thus 75 years worth of headphones. Even worse, considering that they have to include DACs for speakers anyway, they still refuse to route analog audio out to the type-c connector - just to ensure that cheap passive adapters will not impede their shameless grift. reply Kirby64 13 hours agorootparentThat \"cheap passive adapter\" has a ridiculous amount of complexity (both in the dongle, and in the chip you would have to connect it to to make it functional) that you're overlooking. You can't just 'route the DAC in the speaker to the Type-C connector'. That's not how these systems are built at all, and certainly not something that would be functional even if you could just route the speaker DAC to a 3.5mm connector or Type-C connector. reply maven29 12 hours agorootparentThe USB standard has provision for analog output through the MUX, and all compliant MUX/controller chips will have implemented this. https://www.anandtech.com/show/10719/usbif-publishes-audio-o... If this is so non-trivial, Chinese OEMs like Huawei have in fact succeeded in analog output. This is despite the fact that their receptacles are on a 50mm long flex PCB, you don't need another 2 signal layers or an extra daughterboard. The laws of physics are the same everywhere across the world. Just because Apple is incompetent, it does not imply everyone else is. reply Kirby64 12 hours agorootparentYou're talking to someone who works with folks on the USB-IF committee. Analog output through the DP/DN/SBU pins is not standard these days. A mux isn't even necessary in many implementations. Also, this functionality in USB Type-C is straight up deprecated. Just because it's technically possible (and sure, I agree, it is technically possible) doesn't mean it isn't more costly, less efficient, and wasteful vs. just putting the codec directly in the USB-C to 3.5mm dongle. Here's the rationale than USB-IF gave in the ECN: >> Few hosts support analog audio accessory. Those that might could be confused when connected to a device supporting corrosion mitigation, but no detrimental behavior should be seen. This should be a corner case of a corner case so likely zero to very low ppm of this condition. Note, any compliant host that supports analog audio accessory today is also required to support digital audio accessories, so there should be no impact to users.\" reply craftkiller 12 hours agorootparentprev> That \"cheap passive adapter\" has a ridiculous amount of complexity (both in the dongle,[...] I believe they are referring to Audio adapter accessory mode. What is so complex about two resistors on the CC1 and CC2 pins? reply Kirby64 12 hours agorootparentSure, if you use the audio adapter accessory mode, you can just get CC1/CC2 resistors. But then you just push a ton of complexity (even more-so than the standard codec for headsets) into the device. You need additional analog muxes above and beyond the codec, which aren't free. Nobody implements this anymore, cause it's much easier to just shove the codec into the dongle itself. Also, it's deprecated as of the latest USB-C specifications (precisely because nobody uses it now). reply samatman 8 hours agorootparentprevThe Apple USB-C to audio jack connector is excellent. Objectively and testably excellent. It's nine bucks, and it works on anything with a USB-C port. There's no grift here, shameless or otherwise. Apple stopped shipping a headphone jack on their phones for iPhone reasons, and the other manufacturers eventually followed suit because, for all the people on HN who complain about missing headphone jacks, consumers don't actually want them anymore. reply latentcall 12 hours agorootparentprevI have a decent hi-fi system in my home and listen to a lot of vinyl and tapes, and my pair of Technics EAH-AZ80's sound pretty damn good using music streamed from Roon ARC (FLAC, some 320 MP3's). I have heard a lot in the past few years of people praising cheap IEM's (I still don't get the difference between IEM's and a pair of wired headphones) and am curious to refresh my iPod Classic to use with some but I wouldn't know where to begin. reply Yodel0914 9 hours agorootparentprevEQ can make a huge difference, and these days autoeq.app and the open data behind it make getting good EQ settings incredibly easy. If you still like you want to use your wired headphones, something like the Fiio BTR5 is pretty excellent - you can use it either wired via USB-C for highest quality, or via bluetooth for convenience. reply Fnoord 13 hours agorootparentprevYou can get it back with a USB-C to 3.5 mm adapter. reply jraph 11 hours agorootparentbut you can't plug your phone to charge at the same time, can you? reply Fnoord 11 hours agorootparentYeah, you can, as you have connectors which allow USB-C pass-through on top of the 3.5 mm. Another alternative is 3.5 mm to Bluetooth adapter from your headset. Though those are usually (?) male, and hence only work on a female 3.5 mm. You can buy these for a couple of EUR/USD on Amazon from any Chinese brand. Fairphone themselves also sell a 3.5 mm to USB-C adapter. But this does not have USB-C pass-through. These all do have a disadvantage: physical bloat / potentially making device less comfortable. reply dotnet00 8 hours agorootparentThe Qudelix 5k is a pretty well liked 3.5mm to Bluetooth converter in audiophile corners. I've been using one for ~2 years now, it does a pretty good job, plus, retains the option for plugging in via USB. reply kornhole 12 hours agorootparentprevThey only cost a couple bucks and can stay connected to headphones. One of mine has a microphone block that I generally keep plugged into my untrusted work laptop to ensure it isn't listening to me. reply jraph 11 hours agorootparentHow does plugging something help with blocking a microphone? The laptop still has its internal one. One Linux you can still select the internal microphone even with something plugged in the jack port and the mic is still usable. reply deaddodo 10 hours agorootparentOP means that the third sleeve/fourth connection point on the jack (the one responsible for mic data) isn't passed through, ergo the only connection to the recipient device is the output. In other words, it limits a TRRS jack to a TRS one. Whatever OS you're using, you're not magically going to regain access to a physically disconnected signal. reply kornhole 6 hours agorootparentMy laptop recognizes it as an actual microphone. Unless some software is able to change the microphone selection to the internal microphone, it will just be listening to silence. reply _carbyau_ 6 hours agorootparentNot sure what you are using but on Windows I can set my \"default\" but each application can still individually select which input they want. IE an untrusted process running should be able to pick the inbuilt mic. Maybe linux has permissions around device inputs that can be used to stop this but if a person is worried their untrusted laptop might be listening to them then I suspect such a permission layer is already breached. reply _carbyau_ 6 hours agorootparentprevjraph is simply pointing out that a laptop likely has a built in mic which can still be surreptitiously used no matter what the user plugs in and says is their preference. Untrusted is untrusted. reply kevin_thibedeau 8 hours agorootparentprevYou still lose FM radio. reply Kirby64 5 hours agorootparentFM radio using the 3.5mm jack has nothing to do with the jack itself. You could make a USB-C to 3.5mm jack that does FM radio reception too… or any long cord. The reality is nobody (comparatively) wants FM radio anymore. They barely did when it was released. With widespread data plans you can just stream it via the internet, it’s more convenient, it sounds better, and reception isn’t an issue indoors or in bad signal areas. reply _carbyau_ 6 hours agorootparentprevI miss FM radio and IR blasters being a norm on phones. reply NoPicklez 7 hours agorootparentprevI moved away from a 3.5mm jack fanboy when I started using lightening/USB-C based connections for headphones/car stereo. It sounds so much better than using the 3.5mm jack given the use of the onboard DAC. reply causality0 10 hours agorootparentprevNot to mention the bluetooth lag makes playing games with wireless earbuds very sub-optimal. reply kaba0 3 hours agorootparentI absolutely can’t imagine any mobile game where that latency would be significant in any shape or form. Also, is that latency significant in the first place? Don’t you base it on some bad hardware? reply sudosysgen 12 hours agorootparentprevGood TWS headphones are better than the vast majority of wired headphones. For example, Sony's headphones have lossless Bluetooth, and so do Samsung's, and the quality of the headphones themselves is quite good, even by audiophile standards, for in-ears. You can also get bluetooth adapters for traditional IEMs, Fiio and KZ sell good ones. At the end of the day, there really is no technical reason why bluetooth headphones have to sound any worse than wired headphones. reply hollandheese 12 hours agorootparent> Good TWS headphones are better than the vast majority of wired headphones. Maybe, but there are no high end TWS headphones. None of them stand up to even $200 wired IEMs. >You can also get bluetooth adapters for traditional IEMs, Fiio and KZ sell good ones. I wouldn't call any of the TWS adapters for IEMs good. I have the Fiio ones, they are a pain in the ass to use and connect. And everyone I've heard from says the KZ are worse than the Fiio ones, so I'm not going to try them. Get a Bluetooth DAC instead like the Qudelix 5k or Fiio BTR series. > At the end of the day, there really is no technical reason why bluetooth headphones have to sound any worse than wired headphones. True, true. I just wish they'd make good bluetooth headphones. I'm sick and tired of dealing with adapters, even the bluetooth ones. reply nunez 9 hours agorootparentYou can get custom truly wireless earbuds from ADV for $400! They supposedly sound incredible. AirPods Pro Gen 2 are extremely competitive in audio quality and support lossless (when used with Vision Pro) reply sudosysgen 9 hours agorootparentprev> Maybe, but there are no high end TWS headphones. None of them stand up to even $200 wired IEMs. That's completely subjective - Crinacle for example is a popular reviewer and disagrees. By all objective measurements they stand up to many 200$ wired IEMs. > I wouldn't call any of the TWS adapters for IEMs good. I have the Fiio ones, they are a pain in the ass to use and connect. And everyone I've heard from says the KZ are worse than the Fiio ones, so I'm not going to try them. > Get a Bluetooth DAC instead like the Qudelix 5k or Fiio BTR series. I've personally had no issues with use or connection with my KZ adapter - my girlfriend still uses it, and doesn't report any issues (beyond battery life, as it's been 4 years now). I imagine it might also depend on the phone. > True, true. I just wish they'd make good bluetooth headphones. I'm sick and tired of dealing with adapters, even the bluetooth ones. I guess it's up to subjectivity. I personally find my wf-1000xm4s and xm5s really quite good after AutoEQ. I'm not the only one either, plenty of reviewers find them, once EQ'd (which is super easy on Android), to be competent IEMs, even compared to other 200$+ wired IEMs. That's so long as you don't use an iPhone. If you do use an iPhone then yeah, you're screwed as you can't avoid audio compression and you're reliant on manufacturer EQ (though it's perfectly fine for Sony) reply conaclos 13 hours agorootparentprevJust use/buy an \"old\" phone with a jack? It is easy to find used phones (2021 or earlier) with headphone jacks. I have a 2017 phone and plan to upgrade to a 2020 phone. reply naikrovek 12 hours agorootparentThat makes a lot of sense until you realize those are no longer made and are not updated with security fixes, in many cases. It makes very little sense to buy a network-connected device which can’t be updated. And if you say “put Linux on it” it could be argued that you don’t know what tools like phones are for. reply conaclos 11 hours agorootparentI only buy phones with great support for LineageOS [0], which is basically the open source version of Android with some adaptations... Even if I bought a 2024 phone, I would install LineageOS to make sure there are no backdoors on my phone. [0] https://lineageos.org/ reply jordigh 11 hours agorootparentAren't the backdoors gonna be in the baseband firmware that no phone allows you to modify? I always felt that we basically \"lost\" with open source: Intel did the Management Engine and phones stuck everything in the baseband. There's no real possibility of having fully software-hackable hardware. Someone else ultimately has control of all hardware, if they want to exercise that control. reply Pannoniae 12 hours agorootparentprevwell, what are they for? and why is saying \"put linux on it\" misdirected? reply squarefoot 12 hours agorootparentRunning all sort of apps that wouldn't exist native on Linux (banking, whatsapp, ...) or whose equivalents although better and safer on Linux aren't compatible with what the mass uses. I finally settled with a phone (Nokia 8110 4G) that does just the phone, the hotspot and the occasional low quality photo, then a Thinkpad for serious stuff, and don't miss at all a smartphone. But I have no use for social media etc. most people mileage will vary. Actually I would be interested in Graphene OS, but it only supports devices that are either too old or too costly for a mere curiosity, therefore I will wait. reply chainingsolid 11 hours agorootparentprevAs someone writing this on a pinephone the primary issue with the statement is the lack of context. You 'put linux on it' and get OSS, and control of your own device. But most of the time lose 90% access to the abilities of ios/android installs... I can sudo pacman -S dub & ldc2 + fav editor of the week. But had to spend hours getting an android emulator working to sign up to use the telegram install that came with most/all of the distros I've tried! TLDR: you get a pocket laptop instead of a phone, despite owning a \"phone\" reply kaba0 3 hours agorootparentA bad pocket laptop at that. I mean, all the power to you if you use it as a daily driver, but I also own one and it’s just a toy, it is so slow. reply naikrovek 10 hours agorootparentprev> TLDR: you get a pocket laptop instead of a phone, despite owning a \"phone\" Yes, exactly, thank you. You get a project that never ends instead of a useful tool. Maybe that’s what you want, and that’s fine if so, but that’s not what I want. reply carlob 12 hours agorootparentprevPretty much all Motorola phones still have a headphone jack reply deaddodo 10 hours agorootparentI just looked at all the Moto phones being sold by Lenovo and none of the mid-upper range phones have audio jacks. reply orthecreedence 14 hours agoparentprevI don't really consider new devices that don't have headphone jacks. This wireless-everything craze is so obnoxious. reply hinkley 14 hours agorootparentI don't think we've cracked the design yet for no wire between the device and the earphones. I have the Shure BT2 set and I would list my relationship as \"I tolerate them\". It dangles, and if I wear a coat with a metal zipper it has connection issues when my phone is in my pocket. I'm not convinced the new separate ones are better. Behind the head may still be the better option, and sliding them down around your neck when you need to be social ticks a lot of checkboxes for me as well. reply kuchenbecker 6 hours agorootparentI feel like me and everyone I know exclusively use wireless but the Internet would have me believe I'm weird. reply doug_durham 14 hours agorootparentprevCraze??? Wireless has been the standard for portable electronics for almost 20 years. It’s the norm. I could never go back to wired. Too bulky, unreliable, and fragile. reply Fnoord 13 hours agorootparentThe standard, lol. Go watch any live broadcast of a band past 20 years. You'll notice they use wired, not wireless. Why would they do that? Because the latency is lower, the sound quality is higher, and the bandwidth is higher. Don't use wireless unless you absolutely need to. And you probably do not. That being said, I like wireless for a simple reason. No wire near the head is very practical with regards to movement, and I used to destroy 3.5 mm headset wires multiple times a year when I was at school. But those days are long gone. How do they solve that in live performances (also in talkshows)? They use mics there, and they use some tape or way to attach the mic to clothes. reply Grazester 13 hours agorootparentprevUnreliable compared to wireless? ... right. reply cameronh90 13 hours agorootparentI've had to replace my wired headphone cable 3 times in the last 5 years due to wear, unexpectedly each time. I still use them because my current AirPods don't last a full work day, but I'll probably switch to AirPods Max next refresh. I suspect my particular penchant for wearing through cables isn't super common, but at least for me personally, I find wireless more reliable for that reason. reply LunaSea 12 hours agorootparent> I've had to replace my wired headphone cable 3 times in the last 5 years due to wear, unexpectedly each time. I still use them because my current AirPods don't last a full work day, but I'll probably switch to AirPods Max next refresh. How many cables are you able to buy for the price of a pair of air pods with broken batteries? reply cameronh90 6 hours agorootparentMy headphones use a proprietary interface on the headphone side so each replacement costs £20. I’ve never had an AirPod battery go bad, but apparently they replace them for £50 (or free in the first 2 years with AppleCare+). However the main issue isn’t the cost, it’s the fact that I suddenly don’t have a working headphone until I get a new cable. reply bmicraft 13 hours agorootparentprevNope, I've had plenty of unreliable wired in-ear headphones. When used like the wireless ones of today they lasted a couple of years at most before the cable inevitable breaks (usually near the 3.5mm connector) from always moving reply hollandheese 11 hours agorootparentThat's why you get IEMs with replaceable cables. reply wilg 14 hours agorootparentprevYou may not be in the market for wireless headphones then! reply kemayo 12 hours agoparentprevIt's easy and cheap to get USB-C to headphone jack adaptors, so anyone who wants to use wired headphones can do so. (The Apple one is $9, and we know how they like to gouge on accessories.) I've also seen simultaneous headphone-and-charging variants for under $10. Personally, I could never stand using wired headphones with my phone, so I wouldn't want to do it. But the option's there. reply wilg 14 hours agoparentprevWe don’t know if this is more sustainable than AirPods without an actual supply chain analysis. reply Rinzler89 12 hours agorootparentYou need a analysis to tell you that products with user replaceable batteries are more sustainable than those with glued in batteries? reply jdietrich 10 hours agorootparentI'd wager that a small single-digit percentage of Fairbuds customers will ever buy a replacement battery. Very few earbuds will reach the end of the battery's useful life before being lost or damaged beyond economical repair. reply Yodel0914 9 hours agorootparentIndeed. I go through a pair of TWS every 12-24 months - so far that has never been because of battery life. It's 20% dropping/damagingand 80% losing one of the earbuds. reply aix1 3 hours agorootparent> 80% losing one of the earbuds Given that Fairbuds would sell you a single earbud, that's already an improvement over buying a new set, right? reply kaba0 2 hours agorootparentprevSmaller batteries have less ways to go wrong. reply Lio 13 hours agorootparentprevQuite right. Hopefully they’re canny enough to provide that. I love my AirPod Pros. They may not have the best sound but having noise cancelling headphones I can keep my pocket is transformative. I love that there’s no cable to pick up mechanical noise as it catches on things. What I don’t love is buying more Apple stuff or knowing their batteries can’t be replaced. When my AirPods break I’ll be in the market for these. reply ccppurcell 14 hours agoparentprevI didn't realise the fairphone didn't have a jack input. I had a fair phone 2. I'm slowly starting to think the company is a scam. My fairphone could have lasted quite a while longer but they stopped making the part I needed. I mean we need more things like this in the market place so there could be 3rd party but there wasn't any option (this was a couple of years ago now) reply raffraffraff 13 hours agorootparentI'm still rocking an FP3, but the biggest issue is Android 13. It's great that they still provide these updates but 13 alsolutely blows on this device compared to 12. Everything is slow. I'd try Lineage OS but I have a feeling my banking app won't work, and tbh I'm done with all that messing around. I'll likely upgrade once the EU finally forces all phone companies to release phones with user replaceable batteries, and someone produces a phone that isn't gigantic. There aren't any medium sized phones any more. Everything's half a foot long. reply aidenn0 14 hours agoparentprevMy android phone will use any USB headsets which also don't need batteries. Does that not work on the fairphone? reply jamesrr39 13 hours agorootparentYou can get/buy a USB-C to 3.5mm adapter. Annoying that there isn't a 3.5mm port on the phone itself but in the grand scheme of things not a huge inconvenience & life goes on. (I have a Fairphone 4) reply segasaturn 16 hours agoprevNotably Fairphone removed the headphone jack on their phones so that they could sell us these wireless earbuds. I appreciate that the battery is replacable on these buds, but not worth it imo! After all, wired earbuds don't even need batteries. :( reply mafuyu 15 hours agoparentThe industry seems to have settled on USB-C dongles as the solution for wired headphones. They’re a little clunkier, but they do work pretty well, and you get options for the DAC. Headphone jacks were a major wear item on older smartphones, and waterproofing them is very annoying as well, requiring a ton of adhesives. I had to take the back glass off my old Xperia Z2C three times over its life to replace the headphone jack, and the waterproofing was totally shot after doing so. At the end of the day, I honestly don’t mind the trade off, although I can see how it can be annoying for others. reply dathery 14 hours agorootparentMy experience has been that it's really easy to damage your USB-C port like this if you keep your phone in your pocket while walking/running. I always end up with the port loose and making poor connections. I ended up buying a small battery-powered Bluetooth-to-3.5mm receiver that I keep in my left pocket, and then send Bluetooth audio to it from my phone in my right pocket. It's a pretty ridiculous setup. reply aqfamnzc 3 hours agorootparentThe sideways force on the connector while in my pocket has always been a real concern to me. It's the main thing keeping me from regularly using a dongle. reply atoav 14 hours agorootparentprevI have been running (sports) 16km each week with my Fairphone 3 in my pocket and wires headphones, for roughly 3 years now on top of that less harsh daily use during commutes — still no issues. As an electronics guy the jack is rarely the issue, a TRS jack has spring contacts inside, so wear should not be an issue unless they use the shittiest of connectors or you actually damage the bonding between connector and PCB. It is 100 times more likely for the plug to fail and a 1000 times more likely for the cable to fail. reply cameronh90 12 hours agorootparentThe bonding between connector and PCB seems to fail a LOT in certain devices. I've been seeing a near 100% failure rate on the USB C connectors over a few years in some Lenovo X1 Carbon laptop generations. I've not seen the same failure in TRS connectors in a long time now, but I think that's mainly because few people actually use them. reply jszymborski 15 hours agorootparentprev> Headphone jacks were a major wear item on older smartphones I've had the area around jacks get scratched over the years, but I've never had them fail due to mechanical wear. Is this common for others? I've certainly had the micro and mini USB ports fail on me over the years though. reply mafuyu 14 hours agorootparentI think it varies a lot from product to product, and much of it is the mechanical design of the phone housing, and the durability of the jack they selected. Plus it’s really easy to accidentally put a lot of mechanical strain on it when the cable gets tugged. MiniUSB was a busted connector design from the start, which is why the industry moved to Micro USB so quickly. Many MiniUSB receptacles in the wild were failing after thousands or even hundreds(!) of insertions. reply vundercind 14 hours agorootparentprevIt was a constant problem in my PC & Android days. They’d stop making good contact before long. Sometimes, they never would. Not a problem (like, never) once I switched to Apple… but then they removed the jacks. reply edb_123 9 hours agorootparentprevI haven't had a single headphone jack fail since my 2nd gen. original iPod (and I'm an avid headphone user). It had this special variant with a remote connection in an outer ring around the headphone jack, resulting in a weak plastic ring which broke a bit too easily from wear if you omitted the wired remote, and plugged your headphones straight into it. But as others have mentioned, I too have had quite a few Micro-USB and USB-C connectors fail over the years. But almost never the trusty old and dearly missed minijack. reply Semaphor 14 hours agorootparentprevYeah, never had headphone jack issues, it was always the USB port, including with type C now. Sounds like a ridiculous assertion to me, but maybe I use my phone differently. reply mixmastamyk 11 hours agorootparentprevHmm, the headphone jack on my iphone 6s is working fine after years of almost daily use. reply petsfed 15 hours agoparentprevCounterpoint: The material and monetary cost of the number of wired headphones I've destroyed in the 6 years before I got wireless head phones (because there was no realistic way to stow the cabling for the activity I was involved in) probably approaches the cost of the 2 pairs of wireless headphones I've owned in the last 6 years. I don't think they cut the headphone jack just to sell these headphones. While it is true that you can make a headphone jack as water-resistant as the rest of the phone, every hole you put in the phone adds to the design and assembly cost to maintain the same IP rating. And each hole significantly adds to the complexity of repairing that phone in such a way that it maintains that IP rating after the repair. reply hyperpl 15 hours agorootparentCounterpoint: I've lost either the earbuds or case of so many wireless earbuds during the past 4 years that I could have afforded 2-3 much higher quality wired earbuds. reply markhahn 15 hours agorootparentdo you also lose phones themselves? keys? pens? I probably outed myself as an old with the latter question ;) reply vundercind 14 hours agorootparentPhones: if not for Find My, oh god yes. Keys: All the time. Now I have a tile on them. Pens: lol yes, it’s why I’ve never tried to get into very-nice pens. I’d have lost my AirPods several times without find my. I’m certain I’d not have managed to keep both buds more than a couple months if I weren’t very diligent about putting them back in the case when I’m not actively using them. reply jszymborski 14 hours agorootparentprevIt's way easier for me to lose something the size and weight of a nickel than a phone. Not to mention ear buds do on occasion come lose and fall. With wired headphones, they dangle, but wireless buds fall into all manner of nooks, drains, and bottomless caverns. reply yjftsjthsd-h 14 hours agorootparentprevIf my phone hung off my ears and weighed so little that I could miss it, then I would be very nervous about it. reply gibolt 15 hours agorootparentprevSame. I've been using a tiny pair of wireless headphones for ~5 years. If anything were to break, it is fairly easy to order replacements on Ebay which causes nothing new to be manufactured. A set of small wireless headphones uses far less material than a large pair of wired headphones. Not saying it is a direct replacement, just that everything has some environmental impact, and fretting over one tiny device misses the forest for the trees. reply hughesjj 12 hours agorootparentprevWait, can you not just replace the wire? Don't get me wrong the replacement cables can often run like $50 but still, it's not exactly breaking the headphone I love my wxm4s but I'm on my third pair due to battery life. Wish they had user replaceable batteries... Heck at least for the case reply ponorin 15 hours agorootparentprevpeople seem to forget that having a headphone jack doesn't preclude one from using wireless earbuds. if you grind thru many wired earphones for some reason, great, you do you. for others, they will hang onto it much longer and it will be more sustainable and cheaper. reply petsfed 14 hours agorootparentAnd I'm not speaking to the fact that you can use wireless headphones with a phone with a 3.5mm jack. I'm saying that for the objectives Fair set out for themselves, dropping the headphone jack is low-hanging fruit. Its one more piece that fails, it fails as a consequence of normal use (there's no way to baby the headphone jack, or the power jack, in such a way that it doesn't eventually fail from repeated usage. Indeed, they're explicitly rated for number of cycles (typically around 5000 for a headphone jack, which is consistent with daily usage for 3 years, well below their 5 year warranty. By comparison, I'm seeing 20k cycles for a cheap usb-c port, which is a lot more defensible for a 5 year warranty)), and it makes maintaining their IP rating after a repair much harder (doubly so, given that they want users to be able to repair their phones). All for a feature that, given the price point, won't even be used by the majority of users. reply nebalee 15 hours agoparentprevWired USB-C earbuds exist. reply user_7832 15 hours agorootparentBatteries are just one of the many issues when headphone jacks are removed. (Also you can totally use wireless/usb-c headphones with phones with jacks, but that's something hopefully people realize.) reply ssl-3 15 hours agorootparentprevThey do exist, but USC-C headphones include an entire USB sound card instead of -- you know -- just some headphone electromagnetic bits. reply Kirby64 15 hours agorootparent\"an entire USB sound card\" is literally a single piece of silicon (plus some passive components like resistors and capacitors) that can cost less than 10 cents to manufacture. Not exactly a huge burden here. Additionally, unlike a 3.5mm jack, you don't need to deal with a 'sound card' that can handle almost anything you can plug into it. You get to tailor your headphone/headset drivers specifically to your attached devices. The general-purpose device that can handle anything you can throw at it costs much more. reply aeyes 15 hours agorootparentprevSo use any of the ~10$ USB-C to headphone dongles with your favorite headphones. For example the Apple or Google adapters work fine. reply 8zah6q7 15 hours agorootparentThe Apple USB-C to 3.5 mm TRS adapter in particular is a great value. https://www.audioreviews.org/apple-audio-adapter-review/ reply edb_123 9 hours agorootparentExcept for the not-so-minor issue for us Android users: The default gain on Apples adapter is far too low for most proper headphones, and Android doesn't touch the gain and just relies on software-mixer for volume control. This results in a far too low volume. On Apple and Windows devices on the other hand, they are excellent. reply TylerE 14 hours agorootparentprevYou know the wireless earbuds have the exact same circuitry built in, right? reply ssl-3 8 hours agorootparentYou know what doesn't have any active electronics at all? Regular headphones. It's just wire and magnets, with some plastic. reply TylerE 1 hour agorootparentThat wire and magnets isn’t gonna do anything unless it’s hooked up to at the very least an amp (+ a dac if were talking digital) reply bigstrat2003 14 hours agoparentprevYeah, if only there were some kind of connector which would deliver the current to the earbuds. Then you wouldn't need batteries at all. Perhaps some manner of universal connector which would work with the earbuds everyone already has. But that seems too far fetched, we just don't have the technology for such a thing. reply craftkiller 13 hours agorootparent> some manner of universal connector Sounds like you want usb type c! In addition to Audio Adapter Accessory Mode which allows it to drive a couple pins with analog audio exactly like the 3.5mm jacks (and therefore enabling completely passive adapters to support devices with 3.5mm jacks), usb type c also supports charging, high speed data, and video. That's a lot more universal than some silly jack that can only support audio. Unfortunately it looks like at least the fairphone 4 didn't support Audio Adapter Accessory Mode and I haven't found information on whether the fairphone 5 supports it. Definitely a mark against the company if they don't support Audio Adapter Accessory Mode. reply RockRobotRock 15 hours agoparentprevYou could always buy a dongle which doesn’t seem like the end of the world to me reply MildlySerious 15 hours agorootparentPlugging into a USB-C port in the center of the bottom side feels a lot less sturdy than plugging into the audio port closer to the side. Decreased the lifetime of the wires by a lot as well, it felt like. I've tried for a while before finally giving up and getting bluetooth earbuds. reply segasaturn 15 hours agorootparentprevEven Apple has given up on the \"just use a USB-C dongle\" approach to peripherals. reply nickthegreek 14 hours agorootparentby given up, you mean actively sale. https://www.apple.com/shop/product/MU7E2AM/A/usb-c-to-35-mm-... reply user_7832 15 hours agoparentprevI was wondering/hoping someone would raise this point. Wireless earbuds being repairable is great, but you don't need to remove the jack for that. reply barbazoo 16 hours agoprev> Old batteries should never be the end of your earbuds. That's why we designed the Fairbuds with replaceable batteries - in the charging case and both buds! This is awesome. At first I thought they cheated and only made the case battery replaceable. reply nicoty 14 hours agoparentWould've been nicer if they made the batteries hot-swappable like on these: https://pqearbuds.com/products/longest-lasting-interchangeab... reply wbkang 13 hours agoprevSony WF-1000XM3,4 had fairly easily replaceable batteries, but they regressed with the latest XM5. iFixedIt covered it here https://www.ifixit.com/News/79140/sony-just-nerfed-their-mos... I am still using the XM3 - both the earbud and case batteries are repleacable. reply Fnoord 13 hours agoparentI have owned WF-1000XM3, 4, and (currently) 5. I got the 5 with discount as replacement for the 4 since the battery of one earbud was dead due to a firmware bug. The noise cancelling gets slightly better each iteration. There's another thing the 3 and 4 are better at than the 5: easier to get out of the charging case. Also, while you could replace the batteries in each of these versions, the IP rating would be in practice regressed. reply opeon 13 hours agoprevAdvocate of the devil, but I've never replaced my earbuds / headphones because of the battery life. Usually one of them gets lost, or actually crushed and breaks. reply gnicholas 11 hours agoparentI have not yet had this issue because the ANC has improved enough with each generation that I wanted to move up before the batteries became unusable in my old pair. But I could see this changing in the future, as ANC levels out. Also, it's easier to avoid losing AirPods these days thanks to Find My, which can also now find in-case AirPods. reply voxgen 13 hours agoparentprevI've had 2 sets of AirPods die because of the batteries aging and losing their ability to hold a charge. Though to be fair, their microphones failed much earlier then their batteries, and noise cancelling gets some really annoying failure modes when the microphone has issues. I should have replaced them sooner. I also should find a brand that doesn't have consistent manufacturing quality issues... reply code_runner 13 hours agorootparentmicrophones have failed for me in the past as well, though to be fair, apple gave me a free repair for one issue on their end. reply summm 15 hours agoprev> Codec: SBC & AAC No LC3? No LE Audio? So, still HFP/SBC telephony with crappy microphone quality and in mono? For that price, in 2024? reply blackbeans 15 hours agoparentIt's their first iteration and I applaud their effort. It takes at least a year to develop something like this and the sales quantities will be lower compared to the big brands, so the price is understandable. You seem to belong to group of consumers that wants to compromise lifespan and sustainability in favor of features and price. That's fine, but it simply means you are not their target audience for this product. reply abdullahkhalids 13 hours agorootparentIsn't more advanced codecs just software? The hardware support should be there, given its bluetooth 5.3. Is it just a case of them not licensing the advanced codecs? reply petsfed 12 hours agorootparentThink about the device that is performing the decoding. At its core, It's an ultra-low-power microcontroller, and the low-power requirements to hit the battery life specification while maintaining audio quality may or may not allow the kind of software abstractions that make these codecs \"just software\". Once you've licensed the codec, you then have to implement it on the micro's architecture, in a way that's actually performant. Which in turn may or may not require actual Assembly work. reply rale00 9 hours agorootparentYou wouldn't be implementing anything in software. The hardware would have decoding built in, so it's really just a question of choosing a bluetooth audio soc with the codecs you want and paying the licensing fees. reply abdullahkhalids 12 hours agorootparentprevThanks for explaining. reply ipsum2 13 hours agoparentprevAirpods Pro v2 was released in 2022/2023 also uses AAC. reply hollandheese 11 hours agoparentprevAAC is a perfectly fine audio codec. reply WheatMillington 14 hours agoprevI've never had airbuds for long enough that the battery degraded before I lost or broke the buds. I'm not sure this is solving much of a real problem. reply aidenn0 12 hours agoparentPeople are different. I don't do the \"true wireless\" thing at all because I would lose them in under a month. My wife just upgraded her Airpods from late 2017 only because the battery life had degraded to about 20 minutes. reply plufz 14 hours agoparentprevMaybe not for you, definitely for me. reply blackeyeblitzar 16 hours agoprevRepairability is good and will reduce waste, but I am not sure this is going to actually attract customers unless they’re already fans of Fairphone for other reasons (like ethics or sustainability or privacy or independence from big companies or whatever). To attract new customers, they need something else to stand out from a very crowded space, where there are many good choices as far as sound quality, mic quality, noise cancellation, etc. I am skeptical that these will be any better than the competition on those facets. And keep in mind, 150 Euros means it is competing with everything from budget sub-50 products on Amazon to AirPods. reply whazor 16 hours agoparentAssuming there is a maintainability premium, €150 seems too little for great earbuds. I got attached to my AirPods Max. They allow me to have meetings next to people who are also in meetings. Even if they are in the same meeting. reply e12e 15 hours agorootparent> have meetings next to people who are also in meetings. Even if they are in the same meeting. I know what you mean, but still had to laugh :) reply dingnuts 15 hours agorootparentprevI'm interested to hear how the AirPods solved the problem of everyone around me hearing me speak to my computer, definitely causing them annoyance and possibly violating my NDA, depending on where I am for that meeting. reply j16sdiz 16 hours agoparentprevDo they need new customers? All they need are existing customers buy in these new product lines reply gwbas1c 15 hours agoparentprev> I am not sure this is going to actually attract customers What's worse is that if/when the market demands replaceable batteries, it's an easy feature for everyone else to clone. reply blackbeans 14 hours agorootparentSure, it's possible to make the battery removable, but why would they do that? It was the manufacturers (e.g. Apple) who decided to make the battery non-removeable in the first place, as it allows for a more compact design and more revenue. Also the batteries would have to be ordered as a spare part years after the product launched, and most cheap manufacturers are horrible with sustained support. reply AdmiralAsshat 16 hours agoprevUSB-C is a plus. No wireless charging is a bummer. Something built to last for ten years would be intriguing. I'm still using my Samsung Galaxy Buds+ from 2020 as my daily driver. I prefer the capsule case to the box design of later models, and all of the internals are still \"good enough\" that I haven't really felt the need to upgrade. Even the lack of ANC hasn't been a dealbreaker--the passive noise cancellation just from having a solid seal around the ear has been enough to happily use on a plane. reply INTPenis 14 hours agoprevMaybe I'm crazy here but I think it would be better to have a recycling process for earbuds than trying to develop ones with replaceable batteries. It just seems like a lot of overhead for something nobody is going to use in practice. reply markstos 13 hours agoparentRecycling e-waste starts with de-manufacturing. The same features that make tech repairable are the same details that make them recyclable. One of the first steps to recycle an earbud would be to remove the battery and handle that separately from the plastic and metal. reply ipsum2 13 hours agoparentprevI'd use them. I got my Airpods Pro v1 replaced once already because of the battery life. The v2 has no significant benefits to shell out $250 after only using the v1s for 2 years. reply samtheprogram 13 hours agoparentprevI have two pairs of perfectly good wireless headphones I don’t use anymore because the batteries are screwed. No replacement method, it’s just ewaste. Basically $200+ destroyed. I have since bought a phone with a headphone jack and stopped buying wireless headphones. Old generation, wired Bose in-ear go for under $50 on eBay. Sound quality is better too. The wire interfering with my jacket took some getting used to again. EDIT: oh yeah, the batteries don’t die while you’re using wired headphones, either. No more 1-ear’ing so there’s always a bud with battery. reply samatman 8 hours agoparentprevApple will recycle any of their products, earbuds included. They also sell refurbished AirPods. With the right tools and process, it's just a bit of time, solder, and fresh adhesive. Impractical to do at home, but at scale, it can be done. reply omnimus 1 hour agorootparentIt has always been green washing. Apple agresively fights any repair laws and spare parts laws. Their claims about recycling are there just for show just for the eco crowd. Most people will feel good thinking the products wont end up in waste but they all buy new anyway. reply beefnugs 5 hours agoprevHere is an earbuds public service announcement : remember to clean them with alcohol on a regular basis. (inner ear pimples can be painful) Worth buying a little cleaning kit with small cleaning sticks and some pipe-clean like brushes. Also watch out if there is a fabric or glued cover on the sound hole: alcohol will probably dissolve the glue (these aren't really designed to last long or be cleaned) Its usually fine to just let the rubber hold it, or use a tiny amount of RTV silicon to glue it back in place. reply maxglute 16 hours agoprevThese look great, hope ANC performance competitive. Lack voice pass through though. Should have fairphone branded on the the case insetad of fairbud. reply thereddaikon 15 hours agoprevI can't wear earbuds that stick inside your ear canal. They just don't fit me and are extremely uncomfortable. I have two pair of galaxy buds live for this reason. I've even swapped the batteries on both pair to keep them going as well. I'm not switching until someone comes out with a better alternative. reply Izkata 15 hours agoparentWithin the past year, on Amazon there was a rather sudden influx of earbuds in a variety of designs that wrap around or clip onto the ear instead of going in it. The key term to search for is \"open-ear\" earbuds. reply andrepew 15 hours agoparentprevI'm the same and fixed the issue by getting some custom eartips made. They aren't cheap but have made my AirPods Pro usable. On-ear headphones hurt my earlobes and over-ear headphones make my head get hot so I can't seem to find happiness anywhere. The custom eartips I think have gotten me the closest though! reply The_Colonel 14 hours agorootparentHave you tried the vanilla AirPods? I have the Huawei equivalent, and they are very comfortable to the point it's easy to forget I wear them. Recently, new designs (like Huawei Clip) started to appear which are apparently even more comfortable. reply andrepew 13 hours agorootparentI've tried the vanilla Earpods which are pretty much the wired version of that. Those ones at least stay in my ear, but my ear starts to hurt after 30 mins or so. reply jmole 15 hours agorootparentprevwhere did you get custom tips for airpods pro? I feel the same way about the stock tips - great sound quality, but I couldn't wear them for more than 5 minutes. reply andrepew 14 hours agorootparentThis is who I used. Went to a local audiologist for the ear impression and mailed them over. https://eartune.com/products/eartune-fidelity-a-pro Edit: Note that these DO NOT fit inside the Airpods case, you'll need to remove them each time. reply jmole 10 hours agorootparentoof, didn't think about them fitting in the case. have you noticed any wear or play on the earbud/airpod interface from having to do this all the time? reply thih9 15 hours agoparentprevSame. In my case I’m very happy that airpods are a thing, I’m not considering airpods pro for that reason. reply ProfessorLayton 15 hours agorootparentI'm the opposite, I can't use regular airpods since they won't stay in my ears, but airpods pro do so just fine. I think it's just one of those cases where multiple types will be necessary due to physical differences in ears. reply thereddaikon 15 hours agorootparentDefinitely true. People are shaped different so there's room for more than one type on the market. I just wish there was more options for me other than the galaxy live and the original airpods. reply ryukafalz 14 hours agorootparentprevYep, I'm the same way. I wish I could since they seem more comfortable in some ways, but they just fall right out! reply ollien 8 hours agoparentprevIf you're in the market ever again, take a look at the Sony Linkbuds. I've enjoyed mine for exactly this reason reply twobitshifter 14 hours agoparentprevThese are new https://www.bose.com/p/earbuds/bose-ultra-open-earbuds/ULT-H... reply koinedad 16 hours agoprevI really like the idea of this since I’ve been through a few pairs of AirPods. I’m not digging the shape as much but love the idea. reply bdcravens 16 hours agoparentI haven't replaced a pair of AirPods other than wanting upgraded models. reply ivandenysov 15 hours agoprevI'm comparing these to my galaxy buds plus, which I bough because of theoretically replaceable batteries: - pro: 10 EUR for a pair of replacement batteries for the earbuds vs ~20 EUR for a pair of VARTA batteries with shipping from Germany - pro: 13 EUR for a new battery for the case. I need to go to aliexpress to find a replacement battery for the galaxy buds plus case - pro: replacement process seems to be way easier because it was designed to be replaceable unlike galaxy buds plus - pro: lighter than the buds: ~5g vs 6.3g - pro: ANC - con: more expensive. I paid half the price for the buds. But that was in 2020 money. - con: they are bigger: 28.7 x 24.6 x 21mm vs 17.5 x 22.5 x 19.2mm reply stronglikedan 15 hours agoparentConsidering every new generation of Samsung buds (along the same product line) sounds magnitudes better than the last, I don't see why I would ever need to replace a battery. The batteries in my buds plus lasted me until buds pro were released, and my buds pro batteries lasted me until buds2 pro were released, so I have no reason to doubt my buds2 pro batteries will degrade before buds3 pros come out. And of course they will be the best sounding buds on the market so I'll buy them, because why even own buds if not for the sound quality. reply abdullahkhalids 13 hours agorootparentThe frequency response graph of the buds2 pro is only a little bit flatter than the buds pro [1]. In fact, it seems worse in the treble region. How else are you comparing? [1] https://www.rtings.com/headphones/graph/23318/frequency-resp... reply morsch 14 hours agorootparentprevIf every release has improved SQ by that much, the first iteration must have sounded just awful. reply The_Colonel 14 hours agorootparentprevNo offense, but you sound like the perfect consumerist. I'm pretty sure you're going to hit diminishing returns pretty soon, esp. if your claim that every new gen is \"magnitudes better\" than the old one is true. Personally, I was completely happy with the sound quality of my first TWS (FreeBuds 3). The only reason I felt I needed new ones was a failing battery in one of the buds. reply k8svet 15 hours agoprevHow does \"dual point\" compare to \"multipoint\" advertised in some newer (even some decent/cheap) ones? I'm looking to \"upgrade\" but only because the batteries in my current pair are shot. It feels so bad to have things like this just go into a landfill when they're perfectly fine other than the battery. edit: oh, these are also 5x as expensive as the $30 earbuds I bought 2 years ago that have a really decent sound profile. Oooof. Sustainable, at a 5x premium? :/ reply The_Colonel 14 hours agoparentThere are many other aspects of good headphones than just sound quality. Like your cheap ones certainly don't have ANC which can be a dealbreaker. Other important things are responsive touch control, good set of microphones, app with equalizer, ear detection, wearing comfort... For my main use case (podcasts), the sound quality is actually the least important aspect of headphones. reply k8svet 13 hours agorootparentWell the pair I have have quite decent ANC. Frankly, earbuds are in the consumer device category where a loooot of people fall for brands and hype. The only criticism that holds is that these have touchy touch controls, but for a cost savings of $120, and that being the only compromise for me, it's a no-brainer. This blog is invaluable. I bought a number of the sub-$50 pairs a couple years back, and compared them against my favorite wired IEMs. They're not the same of course, but damn good for the money. https://www.scarbir.com/guide/best-sounding-wireless-earphon... I wear them 6+ hours a day, or did when the battery was in better health. If you skim the reviews, you'll see that there are a number of them that have good frequency response, comfort, multi-point. ANC is more or less table stakes, from what I can tell. Etc. Things change fast, you really don't have to spend $100 to get decent earbuds. Hell, even the Galaxy Buds FE, which are very well reviewed can be had for $70USD. EDIT: Actually, the very latest blog post is pretty descriptive of this: https://www.scarbir.com/tws/cheapest-airpods-like-earbuds-al... (One of the top on the list the successor to the pair I've been rocking since 2022.) reply The_Colonel 2 hours agorootparent> Well the pair I have have quite decent ANC. Without mentioning the model I have to believe you, although at least where I live, you can't get ANC earbuds for $30 today, let alone 2 years ago. > Things change fast, you really don't have to spend $100 to get decent earbuds. It's a matter of what you value. I use my earbuds every day, and good ergonomics is something I'm willing to pay a bit more. I was recently researching headphones and there was always some ergonomic compromise. It might be a good value if price is an important concern for you, but it's just not worth to compromise in this aspect for me. reply nulld3v 15 hours agoparentprevThe \"dual point\" they advertise is identical to \"multipoint\" offered by other earbuds on the market. Pretty much all earbuds that advertise \"multipoint\" only support max 2 devices. reply VyseofArcadia 16 hours agoprevAnd of course they don't ship to the US. I'll be picking these up as soon as I can find a way to buy them. Maybe Murena will sell them soon, like they do the Fairphone. reply allenap 12 hours agoprevSmall thing / pet peeve: the website sets language based on my location rather than on what my browser is telling it is my language of choice in every single HTTP request. reply nutrie 13 hours agoprevI just don't get how they're still around. I'd never spend EUR 700 for an android phone. It's sad, but iOS is so much better. reply lock-the-spock 12 hours agoparentI guess you miss the selling point(s) as you see the phone only as its final product and the direct benefits to you. That has never been the ambition. Here's why I bought Fairphones for my family: * Fair supply chains. This was their original selling point and is still an incredible unique feature. They changed cobalt and copper supply chains and established tracing mechanisms that now other manufacturers can also use. * Self-repairable. I switched two screens so far, probably not a huge cost difference to the neighbourhood dirty phone repair shop, but I feel better about it. * Social enterprise, giving back to the community * Nice to know my funds go mostly to a good cause and everyone in the supply and production chain is treated well (they even pay a premium to manufacturers so workers at the assembly line get paid a fair wage). I am well aware it's not the best phone. It's rather clunky, camera used to be weak (got much better with the latest update), and I had some small issues. But overall a solid phone, I support a de kind of ecosystem that really improves things down the line and I don't need to feed Sony or Apple execs money. reply nutrie 4 hours agorootparentI don't. Here's why: * Fair supply chains: After many years of working in Quality Control and Quality Assurance, Supplier Quality Management (I've been on both ends with this one) and related fields (lead auditor, product safety, material safety) for companies large and small, I can safely say that \"fair supply chains\" is just a marketing fairy tale. Without going too deep into this rabbit hole: A) You only have so many suppliers. If they don't comply, and unless their performance and/or behavior significantly affects your reputation, contrary to the common belief, you don't just go to somebody else. You give them another opportunity. If they keep failing, you change your evaluation specs, wait a year, and repeat. This becomes a thousand times more of an issue once you include China and generally other countries outside the US and Europe (there are exceptions) into the equation. B) Good luck auditing suppliers in China. C) Good luck finding experienced auditors. They are incredibly rare and expensive. You cannot outsource it either. D) The auditing process is expensive. That's why even the largest companies in toy manufacturing do not audit their supply chains end-to-end. Instead, you have your direct suppliers sign documents binding them to cascade your rules downstream. What happens when you find out they don't? See A. Should I go on? reply rvense 13 hours agoparentprevI've got one a 3+, had a 2 for six or so years before. It's an adequate phone, does what I need. I'm far from a heavy phone user, though, so it's not so important to me that I could've got better specs for the same money elsewhere, I'd rather companies have Fairphone's focus than Samsung's focus, so I chose to buy from them. (I also quite like that they only have one model so it's easy to decide which one to get) reply UniverseHacker 16 hours agoprevVery cool! I am curious if it takes standard batteries, or something custom from them that will be unobtainable in a few years? I'm also curious if the noise cancelling is any good... I'm tired of being locked into overpriced low quality Bose products, but historically they have been the only ones with truly good noise cancelling. As I have ADHD I need really good noise cancelling to do my job in an open work environment. Lastly, I'm wondering what the deal is with the earbud tips, as I've found I often need a custom or weird sized one to get things to stay in my ear. The Bose ones generally work for me, as they come with a large array of sizes, but the photos make this look like just a single size. I'll keep an eye on these- if I can answer these 3 questions/issues I'll buy a set. reply morsch 16 hours agoparentI can't find this on the product webpage (why?!), but reportedly the battery is standard, it's a LIR1054. https://www.heise.de/news/Fairphone-Fairbuds-In-Ears-mit-Wec... reply mitthrowaway2 16 hours agoparentprevDo consumer standard lithium-ion batteries exist yet? (Aside from 18650s, which might barely count). It's 2024; how come companies like Sony, Panasonic, Energizer and Duracell haven't jumped on offering one or two standard replaceable lithium-ion cells, with integrated circuit protection and slim enough for modern consumer electronics, that people can buy off-the-shelf in drug stores? Many devices are still running on alkaline batteries these days, like smoke detectors and kitchen scales. It didn't take that long in the old days for standards like AA and AAA batteries to hit the market, and they didn't worry too much about applying them to new chemistries with differing voltages. And with efficient DC-DC converters that are cheap enough to introduce into consumer electronics, battery voltage hardly even matters that much nowadays. reply UniverseHacker 16 hours agorootparentKind-of, you can typically get rechargeable lithium versions of the same form factor as alkaline batteries, as well as the fact that many \"proprietary\" batteries like digital camera batteries are actually pretty common and standard, or widely available. For example, if you design a device to take standard gopro batteries, that will be easy to get for a long time. Standard button cells, which have been lithium for a long time, can be obtained in rechargeable versions. However a major issue is that the charging circuitry/method/design is not standard for rechargeable versions of non-rechargeable standard batteries. So you often need to remove it and charge it with a charging system from the battery manufacturer instead of charging in place. reply morsch 16 hours agorootparentprevI've had a few gadgets -- for example, an Instax bluetooth instant photo printer -- that ran off of batteries originally introduced by Nokia for phones around 2000 (BLC-1/2), including charging them. I doubt many people buy them for their Nokia 3310 these days, but they're readily available from many manufacturers[1]. I think some old Canon DSLR battery form factor is used similarily. [1] Many manufacturers, possibly a single factory somewhere in China. reply j16sdiz 16 hours agorootparentprevThe wikipedia page list a full pages of li ion battery sizes https://en.m.wikipedia.org/wiki/List_of_battery_sizes reply criddell 16 hours agorootparentprevDo these count: https://energizer.com/batteries/energizer-ultimate-lithium-b... reply colinng 15 hours agorootparentThey’re really light and last a long time, but are not rechargeable. reply criddell 15 hours agorootparentI think you are moving the goalposts. The batteries I linked to are consumer standard lithium chemistry batteries that people can buy off-the-shelf in drug stores and be used in smoke detectors and kitchen scales. reply mitthrowaway2 13 hours agorootparentWhen I said lithium-ion I was indeed referring to secondary (rechargeable) batteries. Apologies for not making that clear. AA and AAA Nickel-metal-hydride rechargeable batteries exist, but it doesn't seem anyone ever made a li-ion equivalent. reply ssl-3 8 hours agorootparentLithium rechargeable AA cells do exist, and they're perhaps even more mind-bogglingly complex than you'd ever imagine: https://paleblueearth.com/products/pale-blue-lithium-recharg... In order for them to function: They've each got charge circuitry, buck converter circuitry (to get down from 5v to ~3.7v lithium voltage for charging, and from ~3.7v lithium voltage down to 1.5v to be compatible with end-user devices), and each one includes its own USB C port. reply UniverseHacker 13 hours agorootparentprevNo we’re talking about rechargeable batteries here… disposable lithium are too expensive and wasteful for frequently used high drain devices. I love them for emergency equipment like spot trackers and emergency strobe lights reply samatman 8 hours agorootparentprevThe comment you're replying to specifically said lithium-ion battery. What you linked to is a lithium metal battery. They aren't the same thing. reply mattkevan 15 hours agorootparentprevI bought a set of AA lithium from Amazon not too long ago, hoping to solve the problem of disposable batteries. They had a micro USB port in the side for recharging and everything. Sadly, they were crap and stopped working after one charge. More e-waste. reply pge 16 hours agoparentprevThe battery looks like a standard hearing aid battery reply ssl-3 15 hours agorootparentFantastic! The best part about hearing aid battery standards is that there are so many of them to pick from! reply blackeyeblitzar 16 hours agoparentprevI don’t think they’re standard batteries given the specialized form factor (size/shape/weight). On the spare parts page for the Fairbuds (https://shop.fairphone.com/shop/category/spare-parts-4?categ...) there is nothing mentioned about the battery specifications, so I assume it’s proprietary. That said, the parts are cheap - less than 10 euros for replacement earbud batteries is very reasonable. For earbud tips - you can get aftermarket replacements of many sizes and shapes for cheap. Look on Amazon or Alibaba, but some brands are more well known like Spinfit. As for active noise cancellation - my experience has been that anything outside of Bose or Sony is only OK at best. I doubt these are any better than the average set out there. But with the right aftermarket tips you can improve the quality of isolation on cheaper earbuds significantly. EDIT: others have pointed out that the batteries are indeed standardized. reply Ringz 15 hours agorootparentThey have standard batteries: https://www.eemb.com/product-241 reply rgmerk 8 hours agoprevPersonally, I treat wireless earbuds as essentially disposable items. Even if they're mechanically reliable, they fall out on a ride/run, they fill up with earwax, and are otherwise way too vulnerable to invest hundreds of dollars on. The sound quality of $30 Bluetooth earbuds from no-name vendors is plenty good enough in the terrible acoustic environments I use them. If you want good sound quality, no earbud is ever going to come close to a good set of full-size headphones, so I'd prefer to invest money in those instead. Yeah, active noise cancellation would be nice but I imagine that the next generation of cheapie earbuds will have that too. While I get the desire for reducing environmental impact, the total mass of these earbuds is so small that it hardly seems worrying about in terms of the millions of tons of crap that goes into landfill every year. reply aloisklink 5 hours agoparentYou should try the Moondrop Space Travel, you can buy them for USD 24.99 on Amazon. - Moondrop is a pretty well known brand in the ChiFi (Chinese Hi-Fi) space. - They support AAC so they even have good audio quality on Apple/iPhone devices (most cheap earbuds only use cheap Android codecs). - Normally, their latency is pretty high, but they do have a low-latency mode in case you ever want to play games + take a call. - It's Bluetooth 5.3 and communicates directly to each earbud (e.g. if you want, you can only use one at a time). - And, they have active noise cancellation that's surprisingly good (in fact, it's amazing for $25!). IMO, the main downsides are: - Their app is meant to be horrible (I didn't even bother to install it). Not a big deal, unless you want to play around with EQ, customizing what the touch controls do, or upgrade the firmware. - There's no way to control the volume via touch controls (although maybe the app allows you to change this) - Even though it supports Bluetooth 5.3, I don't think it supports Bluetooth LE Audio and the LC3 codec. These earbuds are probably completely uneconomical to recycle, but at least at $25, they probably didn't have too much of an environmental impact when they were created (assuming that the cost of the item is roughly correlated with the environmental impact of the item). reply ebiester 14 hours agoprevI think the key differentiator here for audiophiles is that they are allowing EQ from their app. I think if Resolve, Crinacle or similar gets their hands on it (though Crin has his own competition in the wired space) and creates a good EQ profile and it's competitive, I think this could carve out an important niche. reply dsr_ 14 hours agoparentIt's not clear to me whether they allow arbitrary equalization of enough points (say, 10 parametric or 40+ fixed bands) or just a selection among their presets. Anyone know? reply carom 11 hours agoprevIf these had pass-through audio I would buy them instantly. reply lawlessone 15 hours agoprevThis is better than a phone I think which requires constant updates and support. reply skadamat 16 hours agoprevThese look pretty compelling. I'm looking forward to the iFixIt teardown reply nolongerthere 16 hours agoprevI’m most curious about if the connection issues have been resolved for non-AirPod earbuds, I have AirPod pros that I’m looking to replace, but last time I looked at all the alternatives it was really annoying to switch between devices. reply talldayo 15 hours agoparentMultipoint Bluetooth (which these feature) covers my use case 95% of the time. The gist is that certain headphones can connect to 2 Bluetooth endpoints and switch between whichever one is actively sending audio. If you step away from your desktop and start playing something on your phone, it will grab the phone's input and take over from there. It's not a carbon-copy of Apple's approach, but I've had issues with iCloud switching too so I'm rather glad it's not. Plus, the Multipoint approach works with all my weird stuff like my Switch and smart TV. reply lloeki 15 hours agorootparent> The gist is that certain headphones can connect to 2 Bluetooth endpoints and switch between whichever one is actively sending audio I hate multipoint bluetooth. It only seems to work in the most basic of scenarios and nobody appears to have been testing the damn thing out of that in real life. Yes Bose I'm looking at you. Goodthat is stupid hellhole of having audio stolen from you because some audio happened to play on the second device because you (or an unattended browser autoupdate) (re)opened some youtube tab, or constant nags about \"foo disconnected / foo connected\" when at range limit, or you took a call on your computer and moved and got out of range and sure enough it swaps to your phone which decides \"oh sure you probably want to enjoy some loud music\" and helpfully proceeds to blow your eardrums.forbids you have more than two regularly used devices in which case it's completely useless anyway. Meanwhile my Beyerdynamic is not multipoint, it merely remembers what it has been paired to (I'm up to six devices now) and just connects to the last one on power up. If I want to use them from another previous one I just pop in bluetooth quick settings and choose \"connect to FreeBYRD\" which disconnects them from the other one, y'know, like we used to do when there were actual wires plugged into actual physical plugs, which BT is a glorified copperless version of anyway. reply talldayo 13 hours agorootparentI'm not steelmanning Bluetooth as a whole, or wireless audio as a sterling concept. Multipoint Bluetooth is better than iCloud switching, in my experience. In the rare situation where I'm juggling three devices at once, I turn off Bluetooth on whatever I'm not using and keep on rolling. I can believe that some manufacturers dropped the ball on it, but none of my headsets are really that finnecky. Jabra, Sony, JBL, Samsung and even Skullcandy all seem to have good implementations from what I've seen. Fundamentally, any protocol-level solution is better than playing Monkey in the Middle with Bluetooth MAC addresses online. reply lloeki 12 hours agorootparentOh I'm not using any iCloud switching either, just straight up Bluetooth connection requests from previously paired devices. It just turns out that two clicks/taps when I want my headphones from the device I'm currently using is a mindboggingly more simple, consistent, and reliable solution than whatever a headphone's vendor firmware tries to intuit about what I actually want to do from the fact that it's receiving audio. reply fragmede 13 hours agorootparentprev> In the rare situation where I'm juggling three devices at once If you're not running into that situation very often, then you're not going to consider problems with it a problem. Personally, between my personal phone and laptop, and my work phone and laptop, turning off Bluetooth isn't a good enough solution, especially because I also have a Bluetooth keyboard, and mouse in the mix as well. reply talldayo 12 hours agorootparentWhen I do carry two computers, they're not going to both feature iCloud syncing. Multipoint is still the more seamless option for me, but I guess you could pass the point of utility if you own 3+ iDevices with nothing else to sync. reply amelius 12 hours agoparentprevSpeaking of bluetooth, does anyone know of a USB bluetooth dongle that is Linux compatible and has a range of like 30 meters? Asking because my headphones always lose connection when I walk to the kitchen. reply markhahn 15 hours agoprevwake me when the firmware is replaceable too. reply DowsingSpoon 10 hours agoprevWhat’s “fair” about these ear buds? Is that a statement of quality? Because, if so then that’s not a strong selling point. “Meet the world's most repairable premium earbuds.” Okay, if you’re going to lead with This as your flagship feature… well, that says to me they must be complete garbage. Is this a joke? reply robocat 13 hours agoprev [–] They look like custom batteries. Why did they not use a standard battery like a 18350 or 16340 for the case? Other advantages too (easy to find alternative chargers and can keep a charged spare). I presume size constraints would make it difficult to find a commodity battery size for the earbuds? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Fairbuds are premium, repairable earbuds with replaceable batteries, active noise cancellation, and a dedicated app for customization, designed to last and made with fair and recycled materials.",
      "They come with a three-year warranty, are IP54 sweat, and water-resistant, aligning with climate-conscious and electronic waste-neutral principles.",
      "Fairphone's mission is to create a sustainable consumer electronics market, challenging the industry's short-term approach with Fairbuds."
    ],
    "commentSummary": [
      "Fairphone launched Fairbuds, in-ear headphones with replaceable batteries, shifting from non-repairable TWS earbuds sold for three years.",
      "User conversations revolve around sustainability, the comeback of headphone jacks for enhanced audio quality, and using USB-C headphones with individual DAC as substitutes.",
      "Topics include low-cost Bluetooth headphones, Sony linkbuds, the removal of headphone jacks in devices, the shift to USB-C for audio, wired vs. wireless headphone reliability, environmental impacts, and user comfort, sound quality, and feature preferences."
    ],
    "points": 339,
    "commentCount": 232,
    "retryCount": 0,
    "time": 1712682591
  },
  {
    "id": 39983490,
    "title": "Go-MySQL-Server: MySQL-Compatible Engine in Go",
    "originLink": "https://github.com/dolthub/go-mysql-server",
    "originBody": "A MySQL compatible database engine written in pure Go go-mysql-server is a data-source agnostic SQL engine and server which runs queries on data sources you provide, using the MySQL dialect and wire protocol. A simple in-memory database implementation is included, and you can query any data source you want by implementing your own backend. Dolt, a SQL database with Git-style versioning, is the main production database implementation of this package. Check out that project for reference a implementation. Or, hop into the Dolt discord here if you want to talk to the core developers behind go-mysql-server and Dolt. Compatibility With the exception of specific limitations (see below), go-mysql-server is a drop-in replacement for MySQL. Any client library, tool, query, SQL syntax, SQL function, etc. that works with MySQL should also work with go-mysql-server. If you find a gap in functionality, please file an issue. For full MySQL compatibility documentation, see the Dolt docs on this topic. Scope of this project SQL server and engine to query your data sources. In-memory database backend implementation suitable for use in tests. Interfaces you can use to implement new backends to query your own data sources. With a few caveats and using a full database implementation, a drop-in MySQL database replacement. go-mysql-server has two primary uses case: Stand-in for MySQL in a golang test environment, using the built-in memory database implementation. Providing access to arbitrary data sources with SQL queries by implementing a handful of interfaces. The most complete real-world implementation is Dolt. Installation Add go-mysql-server as a dependency to your project. In the directory with the go.mod file, run: go get github.com/dolthub/go-mysql-server@latest Using the in-memory test server The in-memory test server can replace a real MySQL server in tests. Start the server using the code in the _example directory, also reproduced below. package main import (\"context\"\"fmt\"\"time\"\"github.com/dolthub/vitess/go/vt/proto/query\"sqle \"github.com/dolthub/go-mysql-server\"\"github.com/dolthub/go-mysql-server/memory\"\"github.com/dolthub/go-mysql-server/server\"\"github.com/dolthub/go-mysql-server/sql\"\"github.com/dolthub/go-mysql-server/sql/types\" ) // This is an example of how to implement a MySQL server. // After running the example, you may connect to it using the following: // // > mysql --host=localhost --port=3306 --user=root mydb --execute=\"SELECT * FROM mytable;\" // +----------+-------------------+-------------------------------+----------------------------+ //nameemailphone_numberscreated_at// +----------+-------------------+-------------------------------+----------------------------+ //Jane Deojanedeo@gmail.com[\"556-565-566\",\"777-777-777\"]2022-11-01 12:00:00.000001//Jane Doejane@doe.com[]2022-11-01 12:00:00.000001//John Doejohn@doe.com[\"555-555-555\"]2022-11-01 12:00:00.000001//John Doejohnalt@doe.com[]2022-11-01 12:00:00.000001// +----------+-------------------+-------------------------------+----------------------------+ // // The included MySQL client is used in this example, however any MySQL-compatible client will work. var (dbName = \"mydb\"tableName = \"mytable\"address = \"localhost\"port = 3306 ) func main() {pro := createTestDatabase()engine := sqle.NewDefault(pro)session := memory.NewSession(sql.NewBaseSession(), pro)ctx := sql.NewContext(context.Background(), sql.WithSession(session))ctx.SetCurrentDatabase(\"test\")// This variable may be found in the \"users_example.go\" file. Please refer to that file for a walkthrough on how to// set up the \"mysql\" database to allow user creation and user checking when establishing connections. This is set// to false for this example, but feel free to play around with it and see how it works.if enableUsers { if err := enableUserAccounts(ctx, engine); err != nil {panic(err) }}config := server.Config{ Protocol: \"tcp\", Address: fmt.Sprintf(\"%s:%d\", address, port),}s, err := server.NewServer(config, engine, memory.NewSessionBuilder(pro), nil)if err != nil { panic(err)}if err = s.Start(); err != nil { panic(err)} } func createTestDatabase() *memory.DbProvider {db := memory.NewDatabase(dbName)db.BaseDatabase.EnablePrimaryKeyIndexes()pro := memory.NewDBProvider(db)session := memory.NewSession(sql.NewBaseSession(), pro)ctx := sql.NewContext(context.Background(), sql.WithSession(session))table := memory.NewTable(db, tableName, sql.NewPrimaryKeySchema(sql.Schema{ {Name: \"name\", Type: types.Text, Nullable: false, Source: tableName, PrimaryKey: true}, {Name: \"email\", Type: types.Text, Nullable: false, Source: tableName, PrimaryKey: true}, {Name: \"phone_numbers\", Type: types.JSON, Nullable: false, Source: tableName}, {Name: \"created_at\", Type: types.MustCreateDatetimeType(query.Type_DATETIME, 6), Nullable: false, Source: tableName},}), db.GetForeignKeyCollection())db.AddTable(tableName, table)creationTime := time.Unix(0, 1667304000000001000).UTC()_ = table.Insert(ctx, sql.NewRow(\"Jane Deo\", \"janedeo@gmail.com\", types.MustJSON(`[\"556-565-566\", \"777-777-777\"]`), creationTime))_ = table.Insert(ctx, sql.NewRow(\"Jane Doe\", \"jane@doe.com\", types.MustJSON(`[]`), creationTime))_ = table.Insert(ctx, sql.NewRow(\"John Doe\", \"john@doe.com\", types.MustJSON(`[\"555-555-555\"]`), creationTime))_ = table.Insert(ctx, sql.NewRow(\"John Doe\", \"johnalt@doe.com\", types.MustJSON(`[]`), creationTime))return pro } This example populates the database by creating memory.Database and memory.Table objects via golang code, but you can also populate it by issuing CREATE DATABASE, CREATE TABLE, etc. statements to the server once it's running. Once the server is running, connect with any MySQL client, including the golang MySQL connector and the mysql shell. > mysql --host=localhost --port=3306 --user=root mydb --execute=\"SELECT * FROM mytable;\" +----------+-------------------+-------------------------------+----------------------------+nameemailphone_numberscreated_at+----------+-------------------+-------------------------------+----------------------------+Jane Deojanedeo@gmail.com[\"556-565-566\",\"777-777-777\"]2022-11-01 12:00:00.000001| Jane Doejane@doe.com[]2022-11-01 12:00:00.000001| John Doejohn@doe.com[\"555-555-555\"]2022-11-01 12:00:00.000001| John Doejohnalt@doe.com[]2022-11-01 12:00:00.000001+----------+-------------------+-------------------------------+----------------------------+ Limitations of the in-memory database implementation The in-memory database implementation included with this package is intended for use in tests. It has specific limitations that we know of: Not threadsafe. To avoid concurrency issues, limit DDL and DML statements (CREATE TABLE, INSERT, etc.) to a single goroutine. No transaction support. Statements like START TRANSACTION, ROLLBACK, and COMMIT are no-ops. Non-performant index implementation. Indexed lookups and joins perform full table scans on the underlying tables. Custom backend implementations You can create your own backend to query your own data sources by implementing some interfaces. For detailed instructions, see the backend guide. Technical documentation for contributors and backend developers Architecture is an overview of the various packages of the project and how they fit together. Contribution guide for new contributors, including instructions for how to get your PR merged. Powered by go-mysql-server dolt gitbase (defunct) Are you building a database backend using go-mysql-server? We would like to hear from you and include you in this list. Security Policy go-mysql-server's security policy is maintained in this repository. Please follow the disclosure instructions there. Please do not initially report security issues in this repository's public GitHub issues. Acknowledgements go-mysql-server was originally developed by the {source-d} organzation, and this repository was originally forked from src-d. We want to thank the entire {source-d} development team for their work on this project, especially Miguel Molina (@erizocosmico) and Juanjo Álvarez Martinez (@juanjux). License Apache License 2.0, see LICENSE",
    "commentLink": "https://news.ycombinator.com/item?id=39983490",
    "commentBody": "A MySQL compatible database engine written in pure Go (github.com/dolthub)317 points by mliezun 14 hours agohidepastfavorite61 comments zachmu 12 hours agoHi, this is my project :) For us this package is most important as the query engine that powers Dolt: https://github.com/dolthub/dolt We aren't the original authors but have contributed the vast majority of its code at this point. Here's the origin story if you're interested: https://www.dolthub.com/blog/2020-05-04-adopting-go-mysql-se... reply webprofusion 6 hours agoparentThis is very cool! Couple of suggestions: - Don't use \"mysql\" in the name, this is a trademark of Oracle corporation and they can very easily sue you personally if they want to, especially since you're using it to develop a competing database product. Other products getting away with it doesn't mean they won't set their sights on you. This is just my suggestion and you can ignore it if you want to. - Postgres wire/sql compatibility. Postgres is for some reason becoming the relational king so implementing some support sooner rather than later increases your projects relevance. reply geenat 11 hours agoparentprevWhat's the replication story currently like? reply zachmu 11 hours agorootparentThe vanilla package can replicate to or from MySQL via binlog replication. But since it's memory only, that's probably not what you want. You probably want to supply the library a backend with persistence, not the built-in memory-only one Dolt can do the same two directions of MySQL binlog replication, and also has its own native replication options: https://docs.dolthub.com/sql-reference/server/replication reply dice 4 hours agorootparentHave you benchmarked the replication? Or do you know of anyone who's running it against a primary with a couple 10s of thousands of writes per second? reply cess11 58 minutes agorootparentThat's a lot. With Percona clusters I started having issues requiring fine-tuning around a third of that at quite short peak loads, maybe ten minutes sustained high load topping out at 6-10k writes/s. Something like 24 cores, 192 GB RAM on the main node. Not sure how GC works in Golang but if you see 20k writes/s sustained that's what I'd be nervous about. If every write is 4 kB I think it would be something like a quarter of a TB per hour, probably a full TB at edge due to HTTP overhead, so, yeah, a lot to handle on a single node. Maybe there are performance tricks I don't know about that makes 20k sustained a breeze, I just know that I had to spend time tuning RAM usage and whatnot for peaks quite a bit earlier and already at that load planned for sharding the traffic. reply geenat 11 hours agorootparentprevInteresting! > If you have an existing MySQL or MariaDB server, you can configure Dolt as a read-replica. As the Dolt read-replica consumes data changes from the primary server, it creates Dolt commits, giving you a read-replica with a versioned history of your data changes. This is really cool. reply zachmu 10 hours agorootparentThanks! Let us know if you try it out, we're always interested in feedback. https://discord.com/invite/RFwfYpu reply timsehn 10 hours agorootparentprevComing soon we'll have the ability to replicate a branch HEAD to MySQL as well. reply sa-code 5 hours agoparentprevMissed an opportunity to can this uSql! reply speleding 1 hour agoprevHow hard would it be to use this as an in-memory replacement for MySQL for testing, let's say, a Rails project? Given how important the DB layer is I would be careful to use something like this in production, but if it allows speeding up the test suite it could be really interesting. reply jddj 13 hours agoprevI always found the idea behind dolt to be very enticing. Not enticing enough to build a business around, due to it being that bit too different and the persistence layer being that bit too important. But the sort of thing that I'd love it if the mainstream DBs would adopt. I didn't realise the engine was written in Go, and honestly the first place my mind wonders is to performance. reply jchanimal 12 hours agoparentIf you like the idea of the Dolt prolly trees[1], I'm building a database[2] that uses them for indexing, (eventually) allowing for shared index updates across actors. Our core uses open-source JavaScript[3], but there are a few other implementations including RhizomeDB in Rust[4]. I'm excited about the research in this area. [1] https://docs.dolthub.com/architecture/storage-engine/prolly-... [2] https://fireproof.storage [3] https://github.com/mikeal/prolly-trees [4] https://jzhao.xyz/thoughts/Prolly-Trees reply zachmu 12 hours agoparentprevWe haven't benchmarked the in-memory database implementation bundled in go-mysql-server in a while, but I would be surprised if it's any slower than MySQL, considering that Dolt runs on the same engine and is ~2x slower than MySQL including disk-access. https://docs.dolthub.com/sql-reference/benchmarks/latency reply timsehn 12 hours agoparentprevVersion Control is not the type of thing \"mainstream DBs would adopt\". We needed to build a custom storage engine to make querying and diffing work at scale: https://docs.dolthub.com/architecture/storage-engine It based on the work of Noms including the data structure they invented, Prolly Trees. https://docs.dolthub.com/architecture/storage-engine/prolly-... reply TechTechTech 2 hours agoprevIt would be great if this evolves to support mysql to postgresql and mysql to sqlite. Then we can finally have multiple database engine support for WordPress and others. reply aargh_aargh 1 hour agoparentIt is always the edge cases that will kill you. In the case of WP on PostgreSQL, the reason you want WP in the first place is the plugins and those will be hit or miss on PostgreSQL. Just give up on the combination of those two. reply jbverschoor 13 hours agoprevThis seems to be a wire-protocol proxy for mysql -> SQL. The default proxied database is dolt. I'm guessing this is extracted from dolt itself as that claims to be wire-compatible with mysql. Which all makes total sense. reply zachmu 12 hours agoparentNot a proxy in the traditional sense, no. go-mysql-server is a set of libraries that implement a SQL query engine and server in the abstract. When provided with a compatible database implementation using the provided interfaces, it becomes a MySQL compatible database server. Dolt [1] is the most complete implementation, but the in-memory database implementation the package ships with is suitable for testing. We didn't extract go-mysql-server from Dolt. We found it sitting around as abandonware, adopted it, and used it to build Dolt's SQL engine on top of the existing storage engine and command line [2]. We decided to keep it a separate package, and implementation agnostic, in the hopes of getting contributions from other people building their own database implementations on top of it. [1] https://github.com/dolthub/dolt [2] https://www.dolthub.com/blog/2020-05-04-adopting-go-mysql-se... reply jsteenb2 9 hours agorootparentReally excellent work! For the curious, would you all be creating an in-memory database implementation that is postgres compatible for the doltgres project? reply zachmu 9 hours agorootparentWe are moving in that direction but it's not a primary goal at this point. Once we have more basic functionality working correctly in doltgres we will examine splitting off a separate package for it. The in memory implementation has a bunch of MySQL specific stuff in it right now and we're still learning what pieces need to be generalized to share code. reply osigurdson 8 hours agoprevI suspect Go is probably better, but as a long time C# developer I cringe at the idea of implementing a DB with GC language. It seems that you would be fighting the GC all the time and have to write lots a lot of non-obvious low allocation code, using unmanaged structures, unsafe, etc., a lot. All doable of course, but seems like it would be starting on the wrong foot. Maybe fine for a very small team, but onboarding new devs with the right skill set would be hard. reply hnlmorg 1 hour agoparentThere’s already mysql-like databases written in non-GC’ed languages. Such as myself itself. Odds are if you need one written in Go then you’re requirements are somewhat different. For example the need to stub for testing. reply winrid 8 hours agoparentprevWell, with a manually managed language you have to do those things pretty much all the the time, but with a GC you can pick which parts are manually managed. Also I suspect this project isn't for holding hundreds of GB of stuff in memory all the time, but I could be wrong. reply raggi 7 hours agoparentprevyup, it's bad, and even if you \"do everything right\" minimization wise, if you're still using the heap then eventually fragmentation will come for you too reply neonsunset 6 hours agorootparentLanguages using moving garbage collectors, like C# and Java are particularly good at not having to deal with fragmentation at all or marginally at most. reply neonsunset 8 hours agoparentprevYou would be surprised by performance of modern .NET :) Writing no-alloc is oftentimes done by reducing complexity and not doing \"stupid\" tricks that work against JIT and CoreLib features. For databases specifically, .NET might actually be positioned very well with its low-level features (intrisics incl. SIMD, FFI, struct generics though not entirely low-level) and high-throughput GC. Interesting example of this applied in practice is Garnet[0]/FASTER[1]. Keep in mind that its codebase still has many instances of un-idiomatic C# and you can do way better by further simplification, but it already does the job well enough. [0] https://github.com/microsoft/garnet [1] https://github.com/microsoft/FASTER reply osigurdson 7 hours agorootparentUsing net6. I agree, performance is generally great / just as fast as its peers (i.e. Java and Go). However, if you need to think about memory a lot, GCed runtimes are an odd choice. reply neonsunset 6 hours agorootparentWhy is that? reply osigurdson 6 hours agorootparentPrimarily because you not only need to think about memory, but you also need to think about the general care and feeding of the GC as well (the behavior of which can be rather opaque). To each their own, but based on my own (fairly extensive) experience, I would not create a new DB project in a GCed runtime given the choice. That being said, I do think C# is a very nice language with a high performance runtime, has a wonderful ecosystem and is great choice for many / most projects. reply neonsunset 6 hours agorootparentIsn't the characteristic of languages like C++ or Rust is you have to think way more about managing the memory (and even more so if you use C)? Borrow checker based .drop/deallocation is very, very convenient for data with linear or otherwise trivial lifetime, but for complex cases you still end up paying with either your effort, Arc/Arc No transaction support. Statements like START TRANSACTION, ROLLBACK, and COMMIT are no-ops. > Non-performant index implementation. Indexed lookups and joins perform full table scans on the underlying tables. I actually wonder if they support triggers, stored procedures etc. reply zachmu 12 hours agoparentYes, triggers and stored procedures are supported. Concurrency is the only real limitation in terms of functionality. The bundled in-memory database implementation is mostly for use in testing, for people who run against mysql in prod and want a fast compatible go library to test against. For a production-ready database that uses this engine, see Dolt: https://github.com/dolthub/dolt reply zgk7iqea 12 hours agoparentprevOnly for the in-memory implementation. It is also specifically stated that you shouldn’t use the in-memory stub in production reply fedxc 9 hours agoprevI always look at these implementations and go wow! But then I think, is there any real use for this? reply kitd 1 hour agoparentIf you want to run arbitrary queries on structured data then SQL is a good language to do it in. This library gives you the opportunity to build such a SQL layer on top of your own custom structured data sources, whatever they may be. reply zachmu 9 hours agoparentprevIf your program integrates with mysql in production, you can use this for much faster local tests. It doesn't have to be a go program, although that makes it easier. reply zbuttram 9 hours agoparentprevThe readme mentions at least one interesting use which presumably is the impetus for its creation: https://github.com/dolthub/dolt reply didip 13 hours agoprevtidb has been around for a while, it is distributed, written in Go and Rust, and MySQL compatible. https://github.com/pingcap/tidb Somewhat relatedly, StarRocks is also MySQL compatible, written in Java and C++, but it's tackling OLAP use-cases. https://github.com/StarRocks/starrocks But maybe this project is tackling a different angle. Vitess MySQL library is kind of hard to use. Maybe this can be used to build ORM-like abstraction layer? reply verdverm 12 hours agoparentDolt supports git like semantics, so you can commit, pull, merge, etc... reply davgoldin 2 hours agoprevCongrats, looks like a lot of hard work! Could I swap storage engine with own key value storage e.g. rocksdb or similar? reply west0n 9 hours agoprevInteresting, another project implemented in Go that is compatible with MySQL server, alongside others like Vitess and TiDB. reply malkia 13 hours agoprevIs this for integration/smoke testing? reply timsehn 12 hours agoparentMost direct users of go-mysql-server use it to test GolangMySQL interactions without needing a running server. We here at DoltHub use it to provide SQL to Dolt. reply taf2 8 hours agoprevCould this be used as kind of connection proxy to allow for more clients to a single pool of database servers? reply maxloh 12 hours agoprevI know it is a matter of choice, but why was MySQL chosen instead of PostgreSQL? The latter seems to be more popular on Hacker News. reply tobinfekkes 12 hours agoparentTypically, things that are more popular on Hacker News are not most popular with the rest of the world. reply timsehn 12 hours agoparentprev> Why is Dolt MySQL flavored? TLDR; Because go-mysql-server existed. https://www.dolthub.com/blog/2022-03-28-have-postgres-want-d... We have a Postgres version of Dolt in the works called Doltgres. https://github.com/dolthub/doltgresql We might have a go-postgres-server package factored out eventually. reply geenat 11 hours agoprevWith Vitess likely consolidating its runtimes (vtgate, vtctl, vttablet, etc) into a single unified binary: https://github.com/vitessio/vitess/issues/7471#issuecomment-... ... it would be a wild future if Vitess replaced the underlying MySQL engine with this (assuming the performance is good enough for Vitess). reply zachmu 10 hours agoparentI don't think this is in the cards for vitess, their whole architecture is built around managing sharded mysql instances. reply karmakaze 13 hours agoprevCompatible has many aspects. I'd be interested in the replication protocols. reply neximo64 13 hours agoprevIs there anything like this for postgres? reply perplexa 12 hours agoparentcockroachdb might be close: https://github.com/cockroachdb/cockroach reply cvalka 10 hours agoprevTiDB! reply amelius 12 hours agoprevPerformance comparison against the original? reply sgammon 13 hours agoprevIsn't that........ Vitess? reply zachmu 12 hours agoparentVitess (a fork of it anyway) provides the parser and the server. The query engine is all custom go code. reply sgammon 10 hours agorootparentAh, cool, that makes sense. Thanks for clarifying reply kamikaz1k 13 hours agoprev [–] shouldn't these projects have a perf comparison table? there was a post a couple days ago about the an in-memory Postgres, but same problem on the perf. if someone is considering running it, they're probably considering it against the actual thing. and I would think the main decision criteria is: _how much faster tho?_ reply zachmu 12 hours agoparent [–] This is a reasonable point, we'll run some benchmarks and publish them. We expect that it's faster than MySQL for small scale. Dolt is only 2x slower than MySQL and that includes disk access. https://docs.dolthub.com/sql-reference/benchmarks/latency reply kamikaz1k 10 hours agorootparent [–] Thanks! Appreciate your response. In dynamic language land, we tend to use real DBs for test runs. So having a faster DB wouldn't hurt! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "go-mysql-server is a MySQL-compatible database engine written in Go, enabling users to query data sources using the MySQL dialect and wire protocol, including an in-memory database option.",
      "The main production database supported is Dolt, featuring Git-style versioning, aiming to seamlessly replace MySQL with compatibility for various aspects like client libraries and functions.",
      "Users can set up an in-memory test server, create custom backends by implementing specific interfaces, and access technical documentation, contributor guidelines, and security policy; licensed under Apache License 2.0."
    ],
    "commentSummary": [
      "Dolt is a MySQL-compatible database engine written in Go, offering features like replication, versioning, and performance optimization.",
      "Discussions are ongoing on expanding Dolt's compatibility to Postgres, developing a version control system for mainstream databases, and the potential release of Doltgres, a PostgreSQL-compatible version of Dolt.",
      "The go-mysql-server library, used by Dolt, enables support for triggers and stored procedures, but faces concurrency limitations. Dolt is recommended for testing interactions without a running server and can serve as a connection proxy."
    ],
    "points": 318,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1712692243
  },
  {
    "id": 39978577,
    "title": "Google Axion Processors: Google's Arm-based CPUs for Data Centers",
    "originLink": "https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu/",
    "originBody": "Compute Introducing Google Axion Processors, our new Arm-based CPUs April 9, 2024 Amin Vahdat VP/GM, Machine Learning, Systems, and Cloud AI Google Cloud Next is live! See the latest announcements from Next '24. Join us At Google, we constantly push the boundaries of computing, exploring what is possible for grand challenges ranging from information retrieval, global video distribution, and of course generative AI. Doing so requires rethinking systems design in deep collaboration with service developers. This rethinking has resulted in our significant investment in custom silicon. Today, we are thrilled to announce the latest incarnation of this work: Google Axion Processors, our first custom Arm®-based CPUs designed for the data center. Axion delivers industry-leading performance and energy efficiency and will be available to Google Cloud customers later this year. Run general-purpose workloads on Google Axion Processors for exceptional performance, energy-efficiency and advanced capabilities. Axion is but the latest in a long line of custom Google silicon. Since 2015 we’ve released five generations of Tensor Processing Units (TPU); in 2018 we released our first Video Coding Unit (VCU), achieving up to 33x more efficiency for video transcoding; in 2021, we doubled-down on custom compute by investing in “system on a chip” (SoC) designs, and released the first of three generations of Tensor chips for mobile devices. While our investments in compute accelerators have transformed our customers’ capabilities, general-purpose compute is and will remain a critical portion of our customers’ workloads. Analytics, information retrieval, and ML training and serving all require a huge amount of compute power. Customers and users who wish to maximize performance, reduce infrastructure costs, and meet sustainability goals have found that the rate of CPU improvements has slowed recently. Amdahl’s Law suggests that as accelerators continue to improve, general purpose compute will dominate the cost and limit the capability of our infrastructure unless we make commensurate investments to keep up. Axion processors combine Google’s silicon expertise with Arm’s highest performing CPU cores to deliver instances with up to 30% better performance than the fastest general-purpose Arm-based instances available in the cloud today, up to 50% better performance and up to 60% better energy-efficiency than comparable current-generation x86-based instances1. That’s why we’ve already started deploying Google services like BigTable, Spanner, BigQuery, Blobstore, Pub/Sub, Google Earth Engine, and the YouTube Ads platform on current generation Arm-based servers and plan to deploy and scale these services and more on Axion soon. Unrivaled performance and efficiency, underpinned by Titanium Built using the Arm Neoverse™ V2 CPU, Axion processors deliver giant leaps in performance for general-purpose workloads like web and app servers, containerized microservices, open-source databases, in-memory caches, data analytics engines, media processing, CPU-based AI training and inferencing, and more. Axion is underpinned by Titanium, a system of purpose-built custom silicon microcontrollers and tiered scale-out offloads. Titanium offloads take care of platform operations like networking and security, so Axion processors have more capacity and improved performance for customer workloads. Titanium also offloads storage I/O processing to Hyperdisk, our new block storage service that decouples performance from instance size and that can be dynamically provisioned in real time. “Google’s announcement of the new Axion CPU marks a significant milestone in delivering custom silicon that is optimized for Google’s infrastructure, and built on our high-performance Arm Neoverse V2 platform. Decades of ecosystem investment, combined with Google’s ongoing innovation and open-source software contributions ensure the best experience for the workloads that matter most to customers running on Arm everywhere.\" - Rene Haas, CEO, Arm Beyond performance, customers want to operate more efficiently and meet their sustainability goals. Google Cloud data centers are already 1.5X more efficient than the industry average and deliver 3X more computing power with the same amount of electrical power compared with five years ago2. We’ve set ambitious goals to operate our offices, campuses, and data centers on carbon-free energy, 24/7, and offer tools to help you report on carbon emissions. With Axion processors, customers can optimize for even more energy-efficiency. Axion - out-of-the-box application compatibility and interoperability Google also has a rich history of contributions to the Arm ecosystem. We built and open sourced Android, Kubernetes, Tensorflow and the Go language, and worked closely with Arm and industry partners to optimize them for the Arm architecture. Axion is built on the standard Armv9 architecture and instruction set. Most recently, we contributed to SystemReady Virtual Environment (VE), Arm’s hardware and firmware interoperability standard that ensures common operating systems and software packages can seamlessly run on Arm-based servers and VMs, making it easier for customers to deploy Arm workloads on Google Cloud with limited-if-any code rewrites. Through this collaboration, we’re accessing an ecosystem of tens of thousands of cloud customers already deploying workloads and leveraging Arm-native software from hundreds of ISVs and open-source projects. Customers will be able to use Axion in many Google Cloud services including Google Compute Engine, Google Kubernetes Engine, Dataproc, Dataflow, Cloud Batch, and more. Arm-compatible software and solutions are now available on the Google Cloud Marketplace, and we've recently launched preview support for Arm-based instances migration in the Migrate to Virtual Machines service. What our customers and partners are saying \"We're thrilled to add application packages built on the new Axion Arm-based CPU for Google Cloud to the Bitnami by VMware Tanzu library. This will deliver significantly improved performance, attractive price-performance, and better sustainability for our users. We're excited to get our hands on the new Google Axion instances and do even more to help our customers streamline deployments and reduce their environmental footprint.\" - Mike Wookey, Senior Director R&D, Tanzu Division, Broadcom \"Organizations all over the world rely on CrowdStrike and our single platform, single agent architecture to stop cloud breaches. CrowdStrike delivers the industry’s best protection while being the fastest to deploy, so we’re excited about testing Google's new processor to discover power and efficiency gains.\" – Daniel Bernard, Chief Business Officer, CrowdStrike “Our customers demand uncompromising cybersecurity protection that our systems provide. We're intrigued by the power and efficiency gains possible with the new Google Cloud's custom Arm-based CPU and plan to validate its capabilities as a way to provide even better threat detection and response capabilities to our customers.” - Tzach Segal, VP Business Development, Cybereason “Datadog has been a trusted partner for customers adopting Arm-based virtual machines and an early adopter of Arm for our own operations. We’re excited about Google Cloud's announcement of the Axion processor and plan to evaluate it on our workloads as we help customers measure the cost and performance benefits of using Datadog on Google Cloud Arm instances.” - Yrieix Garnier, VP of Product Management, Datadog “Elastic is committed to helping customers unlock the potential of all their structured and unstructured data efficiently at any scale. We look forward to testing Google Cloud's new custom Arm-based CPU and expect it to help us provide an even better Elastic customer experience on the Google Cloud Platform.” - Steve Kearns, VP of Product, Elastic “We’ve built a strong partnership with Google Cloud over many years and have seen the benefits of building on GCP Arm-based VMs. We can’t wait to see the remarkable improvements coming with the new generation Arm-based Axion processor.” - Joel Meyer, SVP, Engineering, OpenX \"Snap empowers everyone to express themselves, live in the moment, learn about the world, and have fun together. We're constantly optimizing our infrastructure for performance and efficiency. Google's new Axion Arm-based CPU promises major leaps forward in both. The potential to serve our community with these gains while leading on our sustainability goals is incredibly exciting. We look forward to seeing the benefits of Axion-based virtual machines when they become available.\" - Korwin Smith, Sr Director of Engineering, Cloud Infrastructure, Snap “WP Engine powers websites for more than 1.5 million customers across 150 countries. They rely on WP Engine to deliver on our core promises of performance, reliability, and security and we take that responsibility to heart. Our commitment to innovation and a customer-inspired mindset means we’re always looking for ways to further enhance our customers’ performance and confidence online. We’re excited to test Google’s new custom Arm-based processor and its anticipated performance and sustainability gains, and explore how they can empower our customers to create even more compelling websites and applications.” - Ramadass Prabhakar, SVP and CTO, WP Engine Learn more Virtual machines based on Axion processors will be available in preview in the coming months. Register your interest today! And if you’re here at Next ‘24, come learn more about Axion and other compute announcements in these related sessions: SPTL205 - Workload-optimized and AI-powered Infrastructure ARC225 - Transform your cloud operations and design capability with Gemini for Google Cloud ARC229 - Best practices to manage and automate on Compute Engine 1. Google Cloud Internal Data, 31 March 2024 2. Google Environmental Report, 2023, page 10. Posted in Compute Google Cloud Next AI & Machine Learning Related articles Compute What’s new with Google Cloud’s AI Hypercomputer architecture By Mark Lohmeyer • 12-minute read Compute What’s new in Google Cloud’s workload-optimized infrastructure By Salil Suri • 11-minute read Compute IT pros’ top five questions, and how we are answering them at Google Cloud Next By Jarrad Swain • 5-minute read HPC Rocky Linux 8 and CentOS 7 versions of HPC VM image now generally available By Rohit Ramu • 6-minute read",
    "commentLink": "https://news.ycombinator.com/item?id=39978577",
    "commentBody": "Google Axion Processors – Arm-based CPUs designed for the data center (cloud.google.com)245 points by ksec 21 hours agohidepastfavorite117 comments martinpw 21 hours agoLooks like GCP has been using ARM processors from Ampere since mid 2022: https://cloud.google.com/blog/products/compute/tau-t2a-is-fi... So I guess this may be the end for Ampere on GCP? reply synack 20 hours agoprevAll I wanna know is how it compares to AWS Graviton2/3/4 instances. Axion needs to be cheaper, faster, or lower emissions to be worth even considering. Everything else is just talk and vendor lock in. reply ksec 20 hours agoparentGraviton2 is based on Neoverse N1. Graviton3 is based on Neoverse V1. The Graviton4, announced last year at AWS re:Invent. is based on Neoverse V2. So you should expect similar performance to EC2 R8g. I say similar because obviously there may be some difference with clock speed and cache size. It terms of x86. We are expecting AMD Zen 5c with 160 Core / 320 vCPU later this year. reply my123 17 hours agoparentprevWhy vendor lock-in? It even uses the same CPU core as Graviton4, so it's clearly quite a fungible offering to me. reply synack 14 hours agorootparentBy lock-in, I’m referring to my EC2 committed use savings plan that would prevent me from considering migrating to GCP until it expires next year, even if Google’s instances are quantifiably better. reply mcmcmc 12 hours agorootparentWhy are you grousing about vendor lock in when you chose to sign an extended contract to get a better deal? You locked it in for them. reply sapiogram 18 hours agoparentprev> lower emissions Do you know of any cloud providers that publish actual data for this? Preferably verifiable, but I'll take anything at this point. reply synack 18 hours agorootparentGoogle published PUE (https://en.wikipedia.org/wiki/Power_usage_effectiveness) numbers a while ago, I haven’t seen anything that specific from Amazon. It’s difficult to quantify emissions because the power generation is mixed and changes based on demand. Water consumption should also be a factor, but there’s even less data available for that. reply jeffbee 18 hours agorootparentprevGoogle. See https://www.gstatic.com/gumdrop/sustainability/google-2023-e... and below. reply alphazard 11 hours agoparentprevnext [2 more] [flagged] wmf 10 hours agorootparentIt can also matter where the data center is located, although certain trading schemes can offset that. reply soulbadguy 19 hours agoprevAnother interesting challenge for intel (and AMD to a lesser extent). Between the share of compute moving to AI accelerator (and NVIDIA), and most cloud provider having in house custom chip, I wonder how intel will positioned themselves in the next 5 to 10 years. Even if they could fab all of those chips, the margin between the fab business and CPU design is pretty drastic. reply onlyrealcuzzo 18 hours agoparentDoesn't TSMC just fab? They seem to be doing just fine. reply ethbr1 17 hours agorootparentTSMC fabs leading node, and has consistently for several cycles now. So its margins probably benefit from a premium. If Intel can make their foundry business work and keep parity with TSMC, the net effect is that margins for leading node compress from the increased competition. And there's a lot of various US fab capacity coming online in 2024/5: https://www.tomshardware.com/news/new-us-fabs-everything-we-... reply parasense 17 hours agorootparent> the net effect is that margins for leading node compress from the increased competition. That is true in perfectly competetive markets, but I'm skeptical about that idea holding true for high-end chip nodes. I'm not sure there is enough competition with Intel joining the market alongside TSMC, Samsung, and all the other (currently) minor players in the high-end space. You might see a cartel form instead of a competative market place, which is a setup where the higher margin is protected. My best guess is the price will remain high, but the compression will happen around process yields. You could successfully argue that is the same as compressing margins, but then what happens after peak yield? Before prices compress, all the other compressable things must first squeeze. reply onlyrealcuzzo 16 hours agorootparent> You might see a cartel form instead of a competative market place, which is a setup where the higher margin is protected. Wouldn't it more likely be that players just carve out niches for themselves in the high-end space where they DON'T compete? If you're Intel - it seems like a fools errand to spend $50B to maybe take some of TSMC's customers. You'd probably rather spend $10B to create a new market - which although smaller - you can dominate, and might become a lot larger if you execute well. reply ethbr1 16 hours agorootparentI figured you'd see margin compression from the major, volume-limited buyers: e.g. Apple, Nvidia, etc. Paying a premium to get a quota with TSMC looks different, if there's competitive capacity, at least for large customers who can afford to retask their design teams to target a different process. Even if only as a credible stalking horse in pricing negotiations with TSMC. reply DrBazza 21 hours agoprevBuzzword soup and lots of X% better than Y or Z times something. Any ELI5 version of this with concrete numbers and comparisons? reply sapiogram 18 hours agoparentNope, and I don't think Google will publish concrete numbers anytime soon, if ever. reply wmf 15 hours agorootparentPhoronix will benchmark it at some point. reply HankB99 11 hours agorootparentI wonder if TOS precludes publishing benchmark results like some SQL database products. I also wonder if home users will ever be able to buy one of these? Will the predecessor show up in the used market? reply wmf 10 hours agorootparentThe major cloud providers are pretty good about allowing benchmarking. The closest thing you can buy is Ampere or Grace. Graviton 1 chips are also floating around under a different name. reply formerly_proven 21 hours agoprevHow do you do, fellow \"gluing neoverse cores to synopsys IP\" hyperscalers? reply starspangled 5 hours agoparentI thought Google had started their own core microarchitecture group. Must be slow going. reply cmpxchg8b 20 hours agoparentprevWe walk among you reply brcmthrowaway 14 hours agoparentprevLMAO Azure / AWS / Ampere silicon are all Neoverse wrappers, amirite? reply bgnn 14 hours agoparentprevSynopsys should make its on to stop all this nonsense zt this point. reply jbverschoor 20 hours agoprevI'm curious to see when Apple will migrate off AWS and run their own datacenters on their own ARM socs reply ein0p 17 hours agoparentIt’s not at all a given that this would be profitable. Apple is not paying the same prices for AWS/GCP that mere mortals do. reply jbverschoor 16 hours agorootparentWell they’re paying minimum 30 mil per month until this year reply ein0p 16 hours agorootparentI think it’s a heck of a lot more than that. 30 million seems puny compared to the revenue their services businesses generates. Though to be fair much of it probably doesn’t run on public clouds. reply jbverschoor 16 hours agorootparentFrom what I understood is that everything runs on public clouds. They tried Microsoft, Google, and Amazon. Sooo they should have enough experience by now. The contract was 1.5B over 5 years reply refulgentis 15 hours agorootparentSourcing here is 2019 article about just one AWS contract. Apple also uses Google Cloud and Azure extensively, not just tryouts, they were one of Google Cloud's biggest customers. They are also building their own data centers. (TL;DR it's much more complicated than these comments would indicate at their face) reply tudorw 12 hours agorootparentWorking hard to avoid lock-ins, how the other half lives. reply ericlewis 19 hours agoparentprevXcode cloud isn’t running on Apple silicon, arguably a place where it would make tons of sense. reply pjmlp 3 hours agorootparentSo how do you think they are running those unit tests for iDevices APIs, VMs? reply saagarjha 1 hour agorootparentUnit tests? reply pjmlp 52 minutes agorootparentOne of the uses of XCloud code, CI/CD pipelines? reply jbverschoor 16 hours agorootparentprevThat does not make any sense reply saagarjha 1 hour agorootparentidk running a Hackintosh is kind of strange no reply dmitrygr 21 hours agoprevSummary: no timelines, no specifics, NOT a custom core (just neoverse), benchmarks with no references. reply geor9e 7 hours agoparentMy hunch is that this is theater. The executives wanted this program as a weapon. Google didn't like being at the mercy of processor suppliers. Now they can threaten to just make their own. It could ultimately turn out to be a bluff, if their suppliers drop their price and improve their lineup. Source: made it up. reply jsheard 21 hours agoparentprevNonetheless, Intel and AMD must be sweating as another hyperscaler breaks away from using x86 exclusively. reply osnium123 21 hours agorootparentThis is probably one reason why Intel is moving towards providing foundry services. The barrier to entry for doing chip manufacturing is higher than for designing chips now. It’s still an open question if Intel can compete with TSMC and Samsung though. reply ajross 19 hours agorootparentIntel 4 is superior to anything Samsung offers, and not nearly as far behind TSMC's 3nm density as people are lead to believe. The \"open question\" is mostly about fab scaling and the business side of foundary management. Their silicon works just fine in the market as it stands. reply chx 16 hours agorootparentI read a lot about how important it is for a foundry to be able to work with customers and this used to miss from the company DNA of Intel. Cell library, process that sort of thing. We shall see. reply bfrog 7 hours agorootparentprevIntel 20a and 18a are the really exciting processes yet to come out from everything reported. It seems like a game of catch up until then. reply bigcat12345678 12 hours agorootparentprevRight, Google invested in non x86 since 2016 afaik (I was in the team supporting arm powerpc inside Google). At the size of Google, it's pretty much can break from any vendors without damaging it's core businesses reply cmpxchg8b 20 hours agorootparentprevI doubt it, Google has been using Ampere for some time. reply mtmail 21 hours agorootparentprevThey probably already knew it was coming. Announcing it 2 weeks before Alphabet/Google quarterly results might be timed. reply jsnell 21 hours agorootparentIt's obviously timed to the Google Cloud Next conference, which starts today. reply refulgentis 18 hours agorootparentThe Cloud Next Conference is obviously timed to the quarterly results which is obviously timed to the ARM processors, wait, no, the ARM processors are obviously time to the Cloud Next Conference which is obviously timed to the quarterly results reply warkdarrior 10 hours agorootparentAnd it's all timed to come out before Halloween. What kind of sorcery is this? reply xyst 21 hours agoprevI’m all for investment in less power hungry chips. Even if it’s from Google (for a short period of time. Who knows how long these chips will be supported) reply jeffbee 18 hours agoprevI am interested in the market impact of offloads and accelerators. In my work, I can only realistically exploit capabilities that are common between AWS and GCP, since my services must run in both clouds. So I am not going to do any work to adapt my systems to GCP-specific performance features. Am I alone in that? reply wmf 17 hours agoparentThey're accelerating networking and block storage. Do you use those? reply jeffbee 17 hours agorootparentOf course, but even if Google makes somehow networking 50% more efficient, I can't architect my projects around that because I have to run the same systems in AWS (and Azure, for that matter). reply wmf 16 hours agorootparentIt appears to me that Google is just matching what AWS Nitro did years ago. reply jeffbee 16 hours agorootparentThat's the flavor of it, but they didn't give us enough data to really think about. But the question stands generally for accelerators and offloads. For example would I go to a lot of trouble to exploit Intel DSA? Absolutely not, because my software mostly runs on AMD and ARM. reply freitzkriesler2 20 hours agoprevI wonder if this is part of their new SoC chips they've been building to replace tensor on the pixel line. reply MobiusHorizons 18 hours agoparentI would be very surprised if there is much overlap between this server sku and any mobile sku. Mainly because Google doesn’t design the compute core, they integrate some number of cores together with many different peripherals into an soc. Mobile hardware will need vastly different integrations than server hardware and the power / area optimizations will be very different. (Mobile is heavily limited by a maximum heat dissipation limit through the case) the reusable bits like arm cores and pcie / network interfaces might be reusable between designs, but many of those come from other venders like arm or synopsis reply djoldman 21 hours agoprevalso at techcrunch: https://techcrunch.com/2024/04/09/google-announces-axion-its... > Google did not provide any documentation to back these claims up and, like us, you’d probably like to know more about these chips. We asked a lot of questions, but Google politely declined to provide any additional information. No availability dates, no pricing, no additional technical data. Those “benchmark” results? The company wouldn’t even say which X86 instance it was comparing Axion to. reply miohtama 20 hours agoparentGoole taking a page from Apple’s playbook. reply danielbln 20 hours agorootparentAt least Apple really delivered with the Ax and Mx chips. Let's see if that pans out here as well. reply FdbkHb 14 hours agorootparentIt's google, so probably not. Their claims should always be taken with a grain of salt. They went with their own stuff route for the SOC on their Google Pixel phone lines and those SOC are always much worse than the competition and are the main reason why those phones have such terrible battery life compared to Qualcomm powered phones. reply madeofpalk 19 hours agorootparentprevAt least with Apple's Bezos charts usually there's fineprint that'll tell you which 3 year old pc laptop they're comparing it to. reply delfinom 20 hours agorootparentprevI remember when Apple marketed swift as being xx times faster! In fine print, compared to python, lul. reply refulgentis 18 hours agorootparentThis didn't happen reply tyrd12 17 hours agorootparenthttps://www.apple.com/in/swift/ Upto 8.4x faster Than python reply miohtama 17 hours agorootparentAs a Python developer, I am happy that Apple mentions Python and acknowledge its existence. reply refulgentis 17 hours agorootparentprevClaim: \"In fine print, compared to python,\" Reality: In bold call out: Up to 2.6X faster than Objective-C reply hu3 16 hours agorootparentNot sure what your point is but Apple also claims up to 8.4x faster than Python: https://i.imgur.com/CZ6vTZV.png reply refulgentis 15 hours agorootparentMy point is we shifted from \"compared it to Python in fine print\" to \"compared it to Objective-C and Python in bold focused callout\". People are jaded, we can argue that's actually the same thing, that's fine, but it is a very different situation. reply raverbashing 19 hours agoparentprevAmazing Sounds like some middle manager will retire this even before launch so they can replace it with Google Axion CPU or some meaningless name change reply xyst 21 hours agoparentprevThey must be really trying to pump the Next conference attendance numbers. reply Takennickname 20 hours agorootparentOther way around probably. Team had the next conference as a deadline and this is all they were able to come up with. reply devsda 20 hours agorootparentRecently there was an article on the frontpage titled \"headline driven development\". I guess it happens more often than not. reply mtmail 20 hours agorootparenthttps://news.ycombinator.com/item?id=39891948 - That was posted Apr/1st so authors were half joking reply thebytefairy 20 hours agorootparentprevProbably not, given it starts today and is already sold out. reply sega_sai 20 hours agoprevWhat's up with physics names ? Axion, Graviton, Ampere, Electron. I guess it sounds cool and is not trademarked... What's next ? -- boson, hadron, parsec ? reply the_panopticon 18 hours agoparentThere is a lot of room in the periodic table, too, as MS showed starting with https://www.servethehome.com/microsoft-azure-cobalt-100-128-... reply devnullbrain 8 hours agorootparentFewer thank you'd think. It turns out the poisonous ones are bad for PR. reply winwang 19 hours agoparentprevIf I had to guess, it's because its a hardware change... like you're doing better at implementing software on physics :P reply Koffiepoeder 13 hours agoparentprevGeneric names are often used to avoid copyright and trademark strikes. Other easy naming schemes are rivers, minerals, cities,... reply amelius 11 hours agorootparentIs \"Tesla\" a generic name too? reply tekla 11 hours agorootparentYes reply mtmail 21 hours agoprevEight testimonials and it's clear the companies haven't been able to test anything yet. \"[company] has been a trusted partner for customers adopting Arm-based virtual machines and an early adopter of Arm for our own operations. We’re excited about Google Cloud's announcement of the Axion processor and plan to evaluate it [...]' reply cudder 21 hours agoprevnext [13 more] [flagged] qwertox 21 hours agoparentSince this is cloud computing, they're here to stay. reply ahofmann 20 hours agorootparentnext [2 more] [flagged] qwertox 20 hours agorootparentThis is not an end-user service, this is custom-built hardware of which they know that it is more efficient than what they are already using. Did they kill their VCU and TPU, which was created for the same reason? reply nothercastle 21 hours agoparentprev2-5 years depending on how moderately successful it is. reply geodel 21 hours agoparentprevYou will start using after they discontinue? reply FlyingSnake 21 hours agoparentprevI came here expecting this comment. It’s interesting that a tech vanguard community like HN doesn’t trust Google products. reply tyho 21 hours agorootparenthttps://killedbygoogle.com/ Their reputation is deserved. Google domains was killed only last year! reply jsheard 20 hours agorootparentGoogle does kill a lot of products but that site rubs me the wrong way, they really stretch the definition of \"killed\" to run up the numbers as much as possible. Products that were simply rebranded or merged into another product are listed as \"killed\" even though they functionally still exist. The old Google Drive desktop client is on there, for example, when they still have a Drive desktop client. You may as well list Chrome versions 1 through 122 as products they've killed by that standard. reply freedomben 20 hours agorootparentYes, thank you, I feel the same way. It's especially frustrating because I would really like to have an accurate and fair list that I can reference when I need that information, or refer people to it when their memories are short. It's very not helpful in its current form reply xyst 21 hours agorootparentprevIt’s not that I don’t trust the products. I don’t trust the business or its C-level suite. This is not the same Google of 2000s reply Zach_the_Lizard 21 hours agorootparentprevWe used to, until they canceled that trust reply wiz21c 21 hours agorootparentprevdoesn't trust anymore reply FlyingSnake 17 hours agorootparentprevEdit: I don’t get the downvotes because I completely agree with the commentators that Google is unreliable. I almost pasted the killedbygoogle link myself! reply ThinkBeat 19 hours agoprevWith more and more big players starting production of customized private proprietary hardware compatibility becomes increasingly difficult. Which works out well for the big players who can leverage it as lock ins. Regular people wont be able to buy the chips from Microsoft, Google, and you only get M* chips if you buy Apple hardware. Good luck with the frankenMacs now. At the same time devices that you buy get locked into company as well, and if that company goes out of business you are screwed. Like I was when Ambi Climate closed up shop and left with e-waste. All the hardware I need is in there but I can do anything with it. Or when Google decided to close down access for Lenovo Displays, because they didnt want to support 3rd party Displays anymore. Two more e-waste devices for me. (There might be a way to save the Displays I just haven't got in working yet) Open, compatible, standardized omni purpose hardware seems to be dying. Much more profit in lock ins reply walterbell 19 hours agoparent> you only get M* chips if you buy Apple hardware. Former M* team members are now at Qualcomm and HP/Dell/Lenovo will ship their Arm laptops later this year, including Windows 11 and upstream Linux support. reply ThinkBeat 11 hours agorootparentDell and Microsoft has been shipping ARM laptops for years. Creating an ARM cpu is ok but will they copy the entire M* architecture? reply walterbell 9 hours agorootparentWe'll find out after the MS Surface + Qualcomm (ex-Nuvia) launch in late May. reply a_wild_dandan 19 hours agorootparentprevIf they can match the battery life, then I might switch back to a Linux laptop. I’m so sick of Apple’s anti-competitive, anti-consumer practices. reply meowkit 10 hours agorootparentI’m a windows dev and dogfooding the latest OS/hardware. The upcoming ARM devices + OS standby changes massively improve battery life. I also own an M1 and have been passively comparing, much closer now. reply Aurornis 18 hours agoparentprevThese are ARM processors using standard ARM instruction sets. I don’t see any lock in here. reply soulbadguy 19 hours agoparentprevPC hardware (and hardware in general) has never been particularly open. We simply seem to move from one dominant player to the next. I don't think AWS/GCP using custom chip for their cloud offering changes much of the situation (well at least before they start having weird custom instructions). reply LettuceSand12 18 hours agoparentprevAren’t all of these ARM chips? Why is compatibility such a big issue? reply astrange 16 hours agorootparentIt isn't a big issue. But ARM doesn't have a universal boot loader/device discovery/etc standard like EFI/ACPI, so there is some more work to support them. reply jonmasters 16 hours agorootparentArm servers do precisely have exactly that set of standards ((U)EFI/ACPI). See Arm SystemReady. You'll notice in the blog linked above that it mentions Arm SystemReady-VE, which uses those standards. reply shrubble 21 hours agoprevDownside is that GCP still has the same faults regardless of which CPU is being used. Things like poor customer interaction, things not working as designed etc. Switching to ARM won't solve that. reply CuriouslyC 21 hours agoparentAs much as I don't trust google and their customer service is trash, their infrastructure is mostly good and some of their services are very aggressively priced. I wouldn't 100% buy into GPC but a multi cloud solution that leverages some of the good bits is definitely a top tier approach. reply thebytefairy 20 hours agoparentprevIn what ways? In my experience I've found GCP to easiest to use reply shrubble 18 hours agorootparentIt's not about easiest to use but the way in which problems are handled. I am aware of case where something failed after being assured it would work by GCP ... when they got someone a GCP tech lead on the video call he started by saying it was all 'best effort' and that it might take several days to resolve. Ultimately it was fixed in 8 hours but that sort of laissez-faire attitude has led to the company in question making plans to go elsewhere. reply redman25 20 hours agorootparentprevFeels like they've changed their pricing structure for bigquery multiple times in the past couple years. They've never turned the status page yellow or red but there have been a few incidences where from our perspective the service was clearly degraded. reply gigatexal 18 hours agorootparentprevyeah on the whole (data eng by day -- data eng contractor by night) using both AWS and GCP I much prefer GCP to AWS. I find it far more simple to use, has sane defaults, a UI that isn't harmful to users doing clickops, etc. AWS gives you low level primitives to build the moon or shoot yourself in the foot. reply qhwudbebd 21 hours agoprev [–] Looks like it's cloud-scam only, rather than a product one can actually buy and own? reply sofixa 20 hours agoparent [–] That'd be like complaining AWS don't sell Graviton chips to the public. Why would they, they're a cloud provider building their own chip to get a competitive edge. Selling hardware is a whole other business. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google Cloud has launched Google Axion Processors, new Arm-based CPUs tailored for data centers, boasting high performance, energy efficiency, and advanced features.",
      "These processors, based on Arm Neoverse V2 CPU, deliver notable performance enhancements for general workloads while prioritizing energy efficiency and sustainability, aligning with Google's carbon-free energy objectives.",
      "Axion processors ensure seamless application compatibility, interoperability, and are supported across various Google Cloud services, garnering positive feedback from industry partners and customers anticipating improved performance, efficiency, and sustainability benefits."
    ],
    "commentSummary": [
      "Google introduces Axion Processors, Arm-based CPUs for data centers, comparing them to AWS Graviton instances, raising concerns about vendor lock-in and environmental impact.",
      "The move creates challenges for Intel, with TSMC's pivotal role, and Intel's venture into the foundry business, potentially impacting major buyers such as Apple and Nvidia.",
      "Discussions encompass performance claims, skepticism towards Google's new processor, Apple's implications, SQL database products, cloud provider benchmarking, and potential migration of Apple off AWS, emphasizing the need for trust in tech products and compatibility with customized hardware."
    ],
    "points": 245,
    "commentCount": 117,
    "retryCount": 0,
    "time": 1712664771
  },
  {
    "id": 39985630,
    "title": "Why Windows 98 Setup Looks Dated: A Nostalgic Throwback (2020)",
    "originLink": "https://retrocomputing.stackexchange.com/questions/14903/why-does-part-of-the-windows-98-setup-program-look-older-than-the-rest",
    "originBody": "Stack Exchange Network Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers. Visit Stack Exchange Loading… Tour Start here for a quick overview of the site Help Center Detailed answers to any questions you might have Meta Discuss the workings and policies of this site About Us Learn more about Stack Overflow the company, and our products current community Retrocomputing help chat Retrocomputing Meta your communities Sign up or log in to customize your list. more stack exchange communities company blog Log in Sign up Retrocomputing Home Questions Tags Users Companies Unanswered Teams Ask questions, find answers and collaborate at work with Stack Overflow for Teams. Explore Teams Create a free Team Teams Ask questions, find answers and collaborate at work with Stack Overflow for Teams. Explore Teams Teams Q&A for work Connect and share knowledge within a single location that is structured and easy to search. Learn more about Teams Why does part of the Windows 98 Setup program look older than the rest? Ask Question Asked 3 years, 10 months ago Modified 2 years, 8 months ago Viewed 46k times This question shows research effort; it is useful and clear 114 Save this question. Show activity on this post. When installing Windows 98, the part of the setup process where you enter things like product details looks... different. The scrollbars, buttons, title bar and such look more like they belong in Windows 3.1 or Windows 3.11, apparently using \"ctl3d\" style. After the \"Restarting your computer\" step, it looks like the process continues where it left off, except that it now looks like a Windows 98 program. What's going on here, exactly? Are these actually two entirely diferent programs made to resemble one another? Why couldn't the first half call the same button-drawing code that the second half uses? windows-98 windows-3.x gui Share Improve this question Follow Follow this question to receive notifications edited Nov 6, 2020 at 17:02 user3840170 23.1k44 gold badges9191 silver badges150150 bronze badges asked May 24, 2020 at 13:13 Wander NautaWander Nauta 1,20122 gold badges55 silver badges77 bronze badges 9 13 The first phase of runs from removable media and installs the OS on the disk. The second phase is a program running under that OS, with presumably more graphics capability, specific graphics drivers, etc. – dave May 24, 2020 at 13:36 4 For what it's worth, later versions of Windows did the same thing. I'm pretty sure Windows 8's setup screens look like they came from Windows 7 or Vista. – user5670895 May 25, 2020 at 1:07 5 I see three people have voted to close this due to being opinion-based. Ironically that turned out to just be their opinion as there has now been a one true factual answer presented to us. I was about to vote to close it due to being not retro but relented after seeing that was not the consensus and I'm new here. – hippietrail May 26, 2020 at 2:53 1 Just have to say Windows 95 was the same way. – Daniel A. White May 29, 2020 at 0:00 2 May be worth noting that DriveSpace temporarily reboots into a similar mini-Windows environment when compressing the boot volume. Also, booting without mounting the compressed boot volume launches the DriveSpace volume manager in the same environment. Clearly they needed to have some kind of minimal (wrt disk space and memory usage) Windows-like environment for various situations; stripping down Windows 3.x was probably the quickest way to create one. – user3840170 Nov 6, 2020 at 17:37Show 4 more comments 3 Answers Sorted by: Reset to default Highest score (default) Date modified (newest first) Date created (oldest first) This answer is useful 178 Save this answer. Show activity on this post. Basically, because it is running under Windows 3.1 at that point. Windows 98’s setup process goes through three main phases, in three different operating environments; each one installs the operating environment for the next, until the installation is complete. The first, which can run from the setup floppies and/or CD-ROM, uses a DOS program (DOSSETUP.BIN) to set up disk partitions, run various checks etc.: This phases finishes by copying a minimal version of Windows 3.1 to the target installation drive, in a temporary directory (normally WININST0.400), containing DOSX.EXE, USER.EXE, GDI.EXE, KRNL386.EXE, LZEXPAND.DLL etc. (see MINI.CAB). The second uses this minimal Windows 3.1 to run a Windows 3 program, W98SETUP.BIN (specified as the “shell” in SYSTEM.INI): This starts by copying more files to support all the information-gathering during setup, and various other niceties including the 3D look shown in your screenshot (the contents of the PRECOPY CABs); it ends by copying most of Windows 98, setting the system up so that it will boot Windows 98 from the target drive, and rebooting. The third runs after the first boot into Windows 98, from Windows 98: Many PCs with Windows 98 pre-installed were shipped in a variant of the state left at the end of the second phase above; the third phase starts with a “Starting Windows 98 for the first time” message, and follows that up by asking the user for their name and company name. Thus PC buyers got a system pre-installed, but ready to be personalised. You can interrupt setup at any point, or inspect the image during installation in an emulator, to see what’s present on the disk and thus determine the runtime environment. It is also possible to initiate the setup process from any of the above environments, which is how Windows 98 handles upgrades (from MS-DOS, or Windows 3, or Windows 95). Share Improve this answer Follow Follow this answer to receive notifications edited Aug 11, 2021 at 11:40 answered May 24, 2020 at 16:54 Stephen KittStephen Kitt 122k1717 gold badges505505 silver badges463463 bronze badges 2 Comments are not for extended discussion; this conversation has been moved to chat. – Chenmunka ♦ May 26, 2020 at 16:59 1 @STO That's not exactly the case, though it's very close. Older systems initially show a UEFI boot screen (displaying an Apple logo) when you first start the machine and when you choose the startup volume -- that's not macOS (but in M1+ machines the startup chooser is macOS!). Apple always differentiated itself from competitors by presenting a seamless user interface across all states of the system. Windows always feels janky at the edges, especially boot, hibernate, sleep, and shutdown. – Roy Tinker 6 hours ago Add a commentThis answer is useful 8 Save this answer. Show activity on this post. I haven't double-checked, but here's my understanding of how it works. The first half of the installer is a DOS application, with all the limitations therein and then, once it's installed a base Windows system, it boots into Windows to complete the process. The DOS portion doesn't have access to Windows video drivers, so, judging by the color scheme and resolution, it's faking a Windows UI via the same standardized 640x480 16-color VGA mode that Windows 3.1's default video driver uses in order to look friendly and familiar. Basically the same approach Windows 3.1 and 3.11 used, except that they used text mode for the DOS portion of their installers. EDIT: I stand corrected. Apparently it's a three-stage process with a minimal Windows 3.x system in between the text-mode and Windows 9x stages. Share Improve this answer Follow Follow this answer to receive notifications edited May 27, 2020 at 18:06 answered May 24, 2020 at 14:46 ssokolowssokolow 6,76622 gold badges2525 silver badges4242 bronze badges 1 I always assumed it was about the video drivers, but never had any knowledge about it – Quasi_Stomach May 26, 2020 at 19:16 Add a commentThis answer is useful -4 Save this answer. Show activity on this post. Because the setup doesn't contain either. These dialogues are just make to look it alike regular ones. Keep in mind, it's a setup program, not a full figured system. At that time, no (floppy) disk would hold a whole OS plus setup plus whatever data. Lets be serious, it was already a great step toward a more user friendly and consistent experience than previous setup versions. Better praise it for trying rather good. Share Improve this answer Follow Follow this answer to receive notifications answered May 24, 2020 at 13:20 RaffzahnRaffzahn 223k2222 gold badges632632 silver badges920920 bronze badges 1 24 No, Windows setup does include Windows 3 and Windows 98. – Stephen Kitt May 24, 2020 at 16:54 Add a commentYou must log in to answer this question. Not the answer you're looking for? Browse other questions tagged windows-98 windows-3.x gui . The Overflow Blog Want to be a great software engineer? Don’t be a jerk. Climbing the GenAI decision tree sponsored post Featured on Meta New Focus Styles & Updated Styling for Button Groups Upcoming initiatives on Stack Overflow and across the Stack Exchange network Linked 24 What did SULFNBK.EXE actually do? 8 Why \"Dial-Up ATM Support\" cannot be selected in Windows 98 setup? 3 FreeDOS and USB support Related 15 \"Your program is testing for optimal display performance\" in older Windows games 6 Where is the literature for Windows 3.1 Batch mode setup? 22 Why did Windows pick 260 characters as the maximum path length? 64 Why didn't Windows 98 need the \"Starting Windows 98...\" screen? 11 Why did fonts in Windows 1.01's Write application look so poor? 6 Why does my Windows 3.10 only have chimes.wav, chord.wav and ding.wav, but no \"Tada.wav\"? 21 What windowing system had the first size-proportional scroll bar? 19 Windows 98 setup preparation hangs while probing floppy drives in the preparation stage 8 Why \"Dial-Up ATM Support\" cannot be selected in Windows 98 setup? Hot Network Questions Should the banker hit? Paint color consistency What are examples of \"Official Observations\" in a passport? Calculate the offset needed to invert / collapse faces to center of a dodecahedron usiing geometry nodes Why do scientists use specialized units for distance when metric units are perfectly adequate? Why does a 1:1 transformer preserve voltage? Do batteries have capacitance? Taxicab Geometry I need a word for the atmosphere between two people Post-apocalyptic movie from the 1980's; mutants live in a wasteland and are assigned color-codes that dictate who they can mate with Is this formula already known? is this self-plagiarism? Fixed Repeating Output Why does one airliner fly along the coast and the other doesn't? Clifford group without the phase gate How do I deal with crying in front of my supervisor? How does one import Natural numbers in Lean 4 -- unknown identifier 'ℕ'? Feasibility of a bio-engineered, floating tree lifeform in the Venusian atmosphere? Co-Existing with a Highly Extroverted Leader and \"Forced Fun\" Can You Train A Neural Network By Simply Giving It Ratings Each Time It Runs? What contemporary hardware was available for the development of Atari 2600 (or other 2nd gen) games? Are the subjective experience of the \"inner witness of the Holy Spirit\" and the subjective experience of an external world of equal epistemic value? Efficient Algorithm for Scheduling 140 Predefined 1:1 Meetings with Variable Participant Constraints Over 7 Slots? Am I in my rights to ask a colleague to not take notes on everything I say outside a meeting? more hot questions Question feed Subscribe to RSS Question feed To subscribe to this RSS feed, copy and paste this URL into your RSS reader. Retrocomputing Tour Help Chat Contact Feedback Company Stack Overflow Teams Advertising Collectives Talent About Press Legal Privacy Policy Terms of Service Cookie Settings Cookie Policy Stack Exchange Network Technology Culture & recreation Life & arts Science Professional Business API Data Blog Facebook Twitter LinkedIn Instagram Site design / logo © 2024 Stack Exchange Inc; user contributions licensed under CC BY-SA. rev 2024.4.8.7402",
    "commentLink": "https://news.ycombinator.com/item?id=39985630",
    "commentBody": "Why does part of the Windows 98 Setup program look older than the rest? (2020) (retrocomputing.stackexchange.com)242 points by xeeeeeeeeeeenu 9 hours agohidepastfavorite137 comments timetraveller26 8 hours agoWindows XP installation process seemed something similar. I remember I was blown away the first time I installed from an ubuntu live cd. reply o11c 5 hours agoparentI installed from a couple of graphical CDs around 2008 (... how did I even burn that first CD?) ... then shortly after that Linux finally made full-resolution TTYs work by default and I used text mode thereafter. Usually via `mini.iso` (since I install rarely, start with a small base system, and like not starting with out-of-date packages). I'm trying not to have a panic attack that \"Windows 98\" unambiguously counts as retrocomputing now though. reply pjerem 3 hours agorootparent> I'm trying not to have a panic attack that \"Windows 98\" unambiguously counts as retrocomputing now though. Oh you can totally go up to Windows XP for this denomination. Some would argue you may even go to 7. Windows 10 is 9-10 years old and nobody ever cared or will ever be nostalgic about Windows 8 which is 12 years old so it’s not unreasonable to consider anything before that, \"Retro\". reply rahkiin 3 hours agorootparentprevYou might have ordered the CD! It was possible to order free installation disks of Ubuntu, Kubuntu, and more reply apexalpha 29 minutes agoparentprevMy start of my entire interest in IT can essentially be pinpointed to receiving that red CD. reply Dwedit 2 hours agoparentprevI thought Windows XP's installation process looked just like Windows 2000's? Text mode display and everything, but running the Windows NT kernel. reply kuschku 1 hour agorootparentIt used text mode with NT for the first part, then followed up with a Win 9x and Win XP based step 2 and 3 that were themed like the Win XP login screen. Really neat tbh. reply none_to_remain 4 hours agoparentprevI was blown away when the Windows XP installation process required me to find a 3.5\" floppy for storage drivers. reply p_l 3 hours agorootparentBecause on PC, the FDC is probably the only storage component you could be sure to address at one time. reply sambazi 2 hours agorootparentbecause early pc's often completely ran of floppy disks as hdd's were expensive reply Dwedit 2 hours agorootparentprevYeah, you had to slipstream in the storage drivers if you had no floppy drive. reply xattt 39 minutes agorootparent> slipstream The vocabulary that people came up with in a time of Star Trek: Voyager and Fast & the Furious… reply Zardoz84 3 hours agorootparentprevon my case, I need to install Windows 3.1 and then update to 95 and then update to 98. Windows 95 and 98 update editions reply kccqzy 5 hours agoparentprevI was blown away when I installed Ubuntu via Wubi. (It wasn't my first time installing Ubuntu, and my first time was much more traditional.) reply exe34 3 hours agoparentprevI was blown away when I installed OS X from the internet directly without downloading it and copying to a usb/cd. Next time I was blown away with installs was when I unbricked my pixel phone from Chrome! reply kapitanjakc 4 hours agoprevI was somehow able to change my mouse processing animation to red running horse. That was the highlight of my year and I showed it to all my schoolmates. reply M95D 3 hours agoparentYou probably still can. In Win10 there is still control panel / mouse / cursors / browse button. reply dailykoder 2 hours agoparentprevI am pretty sure this is possible on Arch Linux, too! reply gapan 27 minutes agorootparentYou run Arch BTW. reply vbezhenar 4 hours agoprevComments are not for extended discussion; this conversation has been moved to chat. – Chenmunka May 26, 2020 at 16:59 chat.stackexchange.com/rooms/... : Page not found This is really wrong. I wanted to read that discussion. reply Shog9 4 hours agoparentHere you go: https://chat.stackexchange.com/transcript/108509/2020/5/26 For some reason, you'll need to be logged into Stack Exchange to view that. Screen-grab: https://i.stack.imgur.com/6aalu.jpg reply gommm 3 hours agorootparentI'm logged in and the link doesn't work. So, thanks for the screengrab! I'm always a bit annoyed by people cleaning comments like this to move it to chat when stack exchange chat is so bad and somehow always has messed up timestamps. reply qingcharles 4 hours agoprevWhat is the official name of this minimal Windows 3 system? I know it used to have a name. It was used for other purposes too, but I can't for the life of me remember what. reply iforgotpassword 3 hours agoparentIt was used when you wanted to compress the system drive (C:) with DriveSpace. it would reboot into something that looked like Windows 3.1 and do the compression. Just a single window with a progress bar and a cancel button, but the button and the title bar gave it away. I'm assuming it was loaded entirely into ram so it could safely meddle with the system partition. I always wondered if there was a way to boot into a working 3.1 environment as that stuff had to live somewhere in your install. reply exe34 3 hours agorootparentI remember until win98, you could run a program called fileman or something that looked like a win3.1 file browser. That was my idea of retro back then.. reply Dalewyn 2 hours agorootparentAsk and ye shall receive: https://github.com/Microsoft/winfile File Manager, or winfile.exe, was the predecessor to Explorer's file management aspects. You can use it on Windows 10 and 11 (and all the others) if you want to. Program Manager, or progman.exe, was the predecessor shell to Explorer. It was included with Windows through Windows XP SP1 before finally being stubbed in SP2 and removed altogether in Vista. You can probably grab the binary from XP SP1 and run it in newer Windows versions, though. reply exe34 1 hour agorootparentMight have to see if it works on wine. The screenshot has the horrible aero window frame though. reply fredoralive 35 minutes agorootparentIt's a multiple documents interface program (where one main window contains subwindows), a UI style out of fashion since the early days of Windows 95 (IIRC it was Office 97 that switched Office apps from MDI to multiple main windows for example). For some reason Microsoft hasn't updated the theming for them since Windows Vista, probably because it's such a niche thing. Even on Vista / 7, they're stuck as Aero Basic windows, rather than the actually glass ones. The Windows Forms editor in Visual Studio also using those window borders, which shows how much MS cares about that compared to XAML dialect of the week. The Aero Basic theme is particularly horrible though, I do wonder if that particularly unpleasant shade of cyan for the borders was picked to punish those who didn't activate Windows, or were to poor to afford Home Premium. In the end it's one of those weird forgotten things, like the Windows 3.x colour picker dialogue that still lurks in a few places like Wordpad (although MS are \"solving\" that). reply fredoralive 59 minutes agoparentprevBetaWiki claims it's based on Modular Windows[1], as also used on the infamous Tandy VIS. Haven't a clue if it's true, and Modular Windows was used for basically nothing, MS seemed to just give up and move onto embedded version of NT fairly quickly. [1] https://betawiki.net/wiki/Modular_Windows reply pndy 4 hours agoparentprevPretty sure you have Windows Preinstallation Environment, WinPE in mind but it arrived with XP - https://en.wikipedia.org/wiki/Windows_Preinstallation_Enviro... On Windows 200 and XP you could install additional recovery console - which in later version has become a default addition to the system reply ngomez 4 hours agoparentprevAre you thinking of the Windows 3.00 Working Model? https://betawiki.net/wiki/Windows_3.00_Working_Model reply Dwedit 2 hours agoparentprevSeems to be \"Mini.cab\"? Don't know if there are any other names for it. reply afavour 9 hours agoprevInteresting! Makes a lot of sense, too: if you’re offering the ability to upgrade from 3.1 you’re going to need to support it anyway, so why not use it as a setup bootstrap? reply nerdo 5 hours agoprevOntogeny recapitulates phylogeny. reply rl3 7 hours agoprevWindows Millennium Edition was much flashier, although I particularly liked the blue background and white text. reply p0w3n3d 52 minutes agoparentI was trying to hold ME back for as long as possible, and I managed to almost skip it. It was only hiding DOS (but still based on DOS) and not introducing much of a performance improvements (if any, maybe only on boot). Meanwhile I played a lot of DOS games (still) that were unplayable from windows (like Pinball Fantasies) reply grzeshru 8 hours agoprevNew Old Thing goes in-depth about the windows under windows. reply ranger_danger 7 hours agoparentVery underrated blog and an even more underrated book. reply grishka 2 hours agorootparentHe wrote a book? reply bigstrat2003 2 hours agorootparentHe sure did. It's a mixture of stories (some of which are on his blog, some not) and fairly detailed example programs highlighting various parts of Windows. reply justsomehnguy 2 hours agorootparentprevIn 2006. https://www.amazon.com/Old-New-Thing-Development-Throughout/... reply ivanjermakov 8 hours agoprevWhy every file name is uppercase? Is there any reason for that convention, perhaps better readability on low res display? reply cbsmith 8 hours agoparentIt's inherited from CP/M. That's also where the 8.3 character limits came from. Interestingly, CP/M's filesystem did support lower case characters, but CP/M's CPP would convert whatever commands you gave it to the upper case before they are executed. DOS's filesystem was case insensitive, but they otherwise followed CP/M's conventions. reply fsckboy 6 hours agorootparentCP/M copied DEC's operating systems, like RSX-11. They inherited the uppercase from the time when memory was expensive and 6bit ascii would fit more characters into a 36 bit word. reply whartung 6 hours agorootparentprevWith MS-BASIC on CP/M, if you saved the file using lowercase, BASIC would not upshift it, and the CPP commands could not act of the resulting files. You’d have to fix it by removing the file within BASIC using the built in command (KILL I think) then saving it in uppercase. Can’t say MS considered it a bug or simply chose to not fix it for backward compatibility. It existed in quite late versions. reply j16sdiz 8 hours agoparentprevDOS filenames are uppercase. That came from CP/M. In early days, computers just don't do lowercase. When you do case-insensitivty, storing names in uppercase make sense if all older filesystem are uppercase only reply cbsmith 8 hours agorootparentInterestingly, CP/M's filesystem would actually do lowercase. It's just CPP would uppercase every command entered at the command line before executing it. Not surprisingly, people tended to use uppercase with CP/M. reply kqr 5 hours agoparentprevI don't get it. Even if filenames were displayed in uppercase back then, there's no reason to do so now. It's like how people write SQL with uppercase keywords, but not any other language. I suppose it's learning by emulation and then old habits die hard. reply hyperman1 4 hours agorootparentI've gone to uppercase. SQL is a noisy language, with lots of keywords cluttering up your code. I write the keywords in uppercase, and the table and column names in camel case. It seems to make the usefull stuff easyer to find, and the camel case gives a bit of extra info where it matters. reply thaumasiotes 3 hours agorootparentprev> It's like how people write SQL with uppercase keywords, but not any other language. People have syntax coloring in other languages. reply yau8edq12i 3 hours agorootparentI have syntax coloring in SQL too. You should try it. reply Dalewyn 5 hours agorootparentprevReadability. Pardon the breach of rules for a moment: IT'S EASIER TO READ THIS THAN it's easier to read this. reply tomashubelbauer 11 minutes agorootparentThere was a story some years back about some countries changing traffic signs to normal case instead of upper case because with upper case you lose the ability to pattern match words so it is actually harder to read at glance. I found a few links but I am not sure which one is the most canonical one, my memory of this is rather hazy, so I haven't linked any articles. reply kqr 5 hours agorootparentprevExcept it's not! If it is, you need to make text much bigger. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2016788/ If readability of lower-case was the issue we would write all variable and function names in upper case, even in languages that have lower-case keywords. reply skipkey 4 hours agorootparentThe important thing is, the style needs to be consistent over the whole codebase. If most of the SQL has uppercase keywords, a small portion that does not will be harder to read. I dislike the PEP8 style for python, but I use it anyways, mostly. reply o11c 5 hours agorootparentprevI've seen \"lowercase is easier to read\" much more often, though I lack sources either way. Intuitively it does seem that a meaningful contour on top would make mental processing easier. Lowercase letters requiring descenders (g, j, p, q, y) might be relevant. reply hnbad 2 minutes agorootparentIt's a bit more complicated than \"X is easier to read than Y\". When we read text, we don't read letters, we read words. So familiar word shapes help reading text more quickly. Lowercase letters have more distinct shapes (due to ascenders and descenders) so they make for more distinct word shapes. Uppercase word shapes are not only less familiar (because most text you've read is not all-caps) but also less distinct because they lack descenders and all have roughly the same height. So in other words, writing keywords as in SQL in uppercase can be more readable because a) it indicates it's that language (which can be helpful in mixed-language source code), b) the keywords usually being written in uppercase make them easier to recognize without having to be familiar with the words from other contexts (this might actually be an advantage for people who don't read as much English text) and c) this sets keywords apart from variable and attribute names without syntax highlighting. For filenames and such there is no such advantage which is why you only occasionally see all-caps used for special files (e.g. LOCK, VERSION, README) where the uppercase text is intended to stand out from other file names and usually follows a naming convention that makes the word shape common enough to be easy to recognize. wvenable 7 hours agoparentprevUgh I feel old. reply cbsmith 7 hours agorootparentI feel your pain. reply 1970-01-01 9 hours agoprevThe peak of mouse-driven GUI was Windows 98. The icon designs, the customization, and the animation is unmatched even today. https://alexmeub.com/old-windows-icons/ reply jchw 9 hours agoparentI absolutely love the Isys Information Architects Inc. pages on UI, especially the Hall of Shame/Fame. They're a bit of a time capsule, but it honestly makes me think that it's not just pointless cynicism, UI design really has very long ago lost track of what matters. I can relate to a vast majority of the thoughts, even if I think we could do better now, if we tried. http://hallofshame.gp.co.at/index.htm I feel like the style of the UI, though dated, helps a lot: the borders and hierarchy force you to think about how UI components are related, not just where they are on screen. Unlike, e.g. modern Firefox tabs, which just feel like weird floating text with arbitrary ugly borders that never seem to feel less alien no matter how long you use them. They're not literally unusable... but they're not particularly good, especially compared to what we had before. This gives me hope for projects like SerenityOS. A bit of oldschool UI design with some modern amenities. In theory, this seems like a good idea. reply RodgerTheGreat 8 hours agorootparentThe UIs of the 80s and 90s were designed to be learned, and carefully refined through focus testing. These UIs used consistent visual affordances, and contained contextual help. Constraints on memory, resolution, and color depth discouraged the inclusion of visual elements that did not contribute directly to usability and functionality. The UIs of today are largely designed by people who have experienced GUIs for their entire lives, and assume that everyone is already familiar with conventions. Focus testing is seen as slow and expensive, so designers lean on A/B testing and telemetry, randomly breaking live user experiences in small batches to creep toward local maxima. Needing a help system is viewed as old-fashioned; users should paw at UIs like a puzzle box to discover features. Computers and displays are powerful enough for every application to be a unique \"branded experience\". reply mysterydip 8 hours agorootparentThere's a good paper here on the (95) UI team's design goals and how they were achieved: https://socket3.wordpress.com/2018/02/03/designing-windows-9... reply xattt 8 hours agorootparentprev> … 1980s and 1990s … Leopard introduced the pop-over and HUD elements in 2007, and I think this was the last of intuitive UI that I’ve encountered. reply aworks 7 hours agorootparentpreve.g. Spotify reply thfuran 8 hours agorootparentprevThere have always been ui stinkers, but the early days of GUIs had companies spending large amounts of money treating usability as the major output of a serious engineering effort. Now, for all the talk about usability, the actual focus for most companies is on keeping up with the art movement de jour or with maximizing engagement or a similar metric that's only loosely related to usability. reply stavros 7 hours agorootparentUsability and engagement are usually inversely correlated. The more usable the software, the less time I need in it to perform my task. reply kqr 5 hours agorootparentAnd it's scary how often the developers themselves (in the widest sense of the word) don't realise this! They've picked targets and chase them without stopping to think that many applications are a means to an end, not an end itself. But it is a difficult balance. One has to define metrics that tell apart \"user doesn't incorporate solution in their workflow\" (bad) and \"user completes their work using solution quickly\" (good). reply giobox 8 hours agoparentprevFor Windows, I probably agree too. Like others here though I had a soft spot for Windows 2000, everything felt slightly more refined in win2k. Personally I'd probably take some version of Mac OS 8/9 as peak mouse driven GUI design. The third party app ecosystem in Mac OS was a lot better about respecting UI conventions on the platform IMHO, Windows app UIs were all over the place for me during this period as they transitioned out of the DOS era. Simple things, like how installing apps was just dragging them to a folder, not the plethora of setup.exe examples found on Windows, were pretty nice back then on the Mac UI. There are of course lots of things that sucked about old Mac OS then - memory management etc - but the UI design was great. reply nullindividual 8 hours agorootparentWindows 2000 introduced layered windows (mouse pointer has a drop shadow!), transparency effects, and other features into the Shell [0]. In my opinion, having used Windows since 3.0, 2000 is my favorite in terms of icons, shell, etc. [0] https://learn.microsoft.com/en-us/previous-versions/ms997507... reply nextos 8 hours agorootparentprevSerenityOS replicates that look and feel. It is also implemented in a dialect of C++ that adheres to some of the good parts of C++98: https://serenityos.org reply pndy 3 hours agorootparentprev> Like others here though I had a soft spot for Windows 2000, everything felt slightly more refined in win2k. I really like all the UI changes introduced in 2000. Especially the color palette: that subtle change from strong gray in 9x to lighter one worked pretty well. With all the gradients on icons, preview pane in Explorer, Tahoma as default UI font and introduction of transparency/fade elements I forget 98 SE quickly. And of course there was the stability of the NT - back then the price was lack of compatibility with games. reply afavour 9 hours agoparentprevI remember preferring Windows 2000 but can’t remember exactly why. Very similar to 98 but felt more refined somehow. reply retrac 8 hours agorootparentWindows 2000 was based on NT. While it had the Windows 98 interface it was very different underneath. It's a bit bonkers, but MS sold two superficially-identical operating systems for five plus years, that were in reality, about as different as two OSes could be; different histories, different underlying architecture, little code shared between the kernels and only part of the userland. And yet the two OS families had almost exactly the same UI and API. reply nikau 8 hours agorootparentit made sense at the time, pcs had limited ram and win98 ran much better. reply reactordev 8 hours agorootparent2000 is what you ran if you were hosting your lan party. 98 is what you had if you were invited to your friends lan party. reply EvanAnderson 8 hours agorootparentprevWith a decent amount of RAM, though, 2000 was far more stable. reply Dalewyn 8 hours agorootparentI think you need to understand that most people maybe had 16~32MB of system RAM on average circa 1999, while Windows 2000 ideally wanted at least a staggering 128MB as just a sysreq recommendation. Windows 98 was fine with 16MB minimum or even 8MB with some wizardry. How much did 128MB of RAM cost in 1999? According to a Reddit thread that Copilot found, about $370 in 2020 dollars[1]. Copilot also dug up another thread[2] where people reminisced about RAM capacity and pricing in the 90s. So in very basic terms, 128MB of RAM (let alone more!) back then would be like buying 128GB of RAM today. Most people simply aren't going to buy ~$400 of RAM just like that, especially when upgrade cycles were also much shorter than today. There was a very practical reason Windows 9x existed alongside Windows NT until XP merged the lines in 2001 when 256~512MB of RAM and more became much more affordable. [1]: https://www.reddit.com/r/mac/comments/finsjm/how_much_did_12... [2]: https://www.vogons.org/viewtopic.php?t=62239 reply EvanAnderson 5 hours agorootparent> I think you need to understand that most people maybe had 16~32MB of system RAM on average circa 1999, while Windows 2000 ideally wanted at least a staggering 128MB as just a sysreq recommendation. I do understand. I was there. My personal PCs from that time had 16MB (a getting old AMD 5x86 133) and 32MB (Celeron 300A w/ the 450Mhz \"mod\"), and I ran Linux and Windows 95/98 on them. I didn't get W2K at home until late 2001 and that was only because I splurged. Windows 2000 was a ridiculous luxury item for home users at the time, but for business PCs it was a real winner. It was so much more stable than Windows 95/98. The \"we build PCs\" shop I worked at did a good job of convincing business Customers to spend the extra money for stability. Only a minority bit on the idea, but people definitely talked about seeing fewer reboots and better multitasking w/ the W2K machines. It helped sell itself. (We also did least-privilege user accounts all the way back in the NT 4.0 days. \"Cleaning up\" the malware of the day was so easy because, by and large, you could just blow away the user's profile and start with a fresh registry. Since the user didn't have Administrator rights making machine-wide changes was \"off limits\" for most malware of the time.) reply Dalewyn 2 hours agorootparentIndeed. When Microsoft called them Windows NT 4.0 Workstation and Windows 2000 Professional, they damn well meant it. You said \"decent amount of RAM\" in a very nonchalant fashion, so I felt that maybe a reorienting of the focal lens was necessary. RAM today is so mundane everyone wastes it without a care in the world[1], but it wasn't always this way. [1]: https://news.ycombinator.com/item?id=39920226 reply o11c 4 hours agorootparentprev> most people maybe had 16~32MB of system RAM on average circa 1999 For old computers maybe. I never actually heard of anyone doing an upgrade-install of Windows though. I had 64MB in ... probably 1998 and we weren't even a computer-focused household. And your second link seems to back that up (\"early 1999\"), and that was a major improvement that basically obsoleted our old '95 computer but managed to last until well into the XP era ... huh, which is actually only the same 3 years apart, though we got SP1 which was a year after that. It's kind of weird how Microsoft released a different OS each year for 4 years in a row. reply kiwijamo 6 hours agorootparentprevIndeed. I remember running Windows 95 on a 32MB machine and stayed on Windows 95 even after 98, 98SE, ME, 2000 were released simply because that's what worked well on the machines I had at the time. IRRC I had a P166MMX with 32MB of RAM from 1997 which remained my main machine for some years. Win95 only needed the correct drivers for the newer post-1995 hardware. I ended up skipping 98, 98SE, ME, 2000 and going straight to XP for my next upgrade at some point in the early 2000's as I had acquired a new machine which couldn't run Windows 95 without a whole lot of patches and Firefox eventually dropped support for Windows 95. By then the hardware I had was good enough that XP worked fine. reply lelanthran 5 hours agorootparentprev> Windows 98 was fine with 16MB minimum or even 8MB with some wizardry. I ran Win95 in 1995 for a few months on 4MB of RAM. reply afavour 8 hours agorootparentprevDriver support was also terrible for a long time. reply estebank 9 hours agorootparentprevNT kernel with it's related stability, true-color interface, no XP \"fisher price\" UI. 2000 was probably the pinnacle of that era's design language. Ironically, XP made it more feasible to use 2000 day to day as drivers for XP worked on the earlier 2000. reply int_19h 6 hours agorootparentXP \"fisher price\" UI was trivial to disable in settings, giving you that same slate gray look of 98/2K, but with 24-bit color icons. This was actually possible to do all the way up to and including Win7, which even had the icon taskbar working in that \"Windows Classic\" theme. And it was very good design - pretty easy on the eyes and not distracting, but you could also tell which element is which easily at a glance (unlike these days, when everything is flat). reply abhiyerra 5 hours agorootparentI know this sounds petty but the XP’s Classic’s task bar gray was ever slightly off from Windows 2000 and it drove me nuts. I think Windows 2000 was visually the best. reply badsectoracula 4 hours agorootparentprevWhile you could disable the fisher price theme in XP (and Vista and 7), even in classic theme you still had the \"plastic\" icons. reply bigstrat2003 2 hours agorootparentprevI always thought that the Windows XP color theme was vastly over hated. I thought it looked nice, but even if one didn't enjoy it, I cannot understand why people acted like it killed their dog. reply derefr 8 hours agorootparentprevRe: XP UI — did you ever try Windows Server 2003? reply genmud 8 hours agorootparentIIRC you could actually run XP themes on 2k3 if you started the theme service. Under the hood they were almost the same exact same kernel (I think server / 5.2 had a slightly different IP stack and some server tweaks for task scheduling, shadow volumes and memory caching). Likewise if you wanted the non XP (Luna I think it was called?) styles you could just stop the theme service and it was effectively win2k style. reply chungy 2 minutes agorootparent> Under the hood they were almost the same exact same kernel Literally the exact same kernel if you count XP x64 Edition. Even the exact same Windows Updates apply to both. :) EvanAnderson 8 hours agorootparentprevServer 2003 was my daily driver on a laptop for a few years. Most XP software ran fine but it had none of the XP visual cruft. Until Windows 7 it was peak Windows for me. reply deepspace 7 hours agorootparentI ran that rare beast, Windows XP64. It was basically Server2003 x64 with the XP UI. Driver compatibility was terrible, as you might expect, but it could support a vast amount of memory (relative to XP). I used the base O/S only to run VMware Workstation, and then ran small instances of XP, each dedicated to a task like audio, video, etc, on top of that. Along with FreeBSD and various flavors of Linux. That was my daily driver until late 2010. Solid as a rock. reply EvanAnderson 5 hours agorootparentXP64 was an odd animal. I had a Customer who usd it to run Unigraphics CAD. The Dell Precision machines they had with factory-loaded XP64 were fine. Trying to get their random other machines to run it w/ working audio, video, and chipset drivers was a pain. I never ran it as a desktop OS myself. I installed plenty of the server version it was based on (W2K3 x64) but those boxes always had standard VGA drivers, no audio, and sometimes a RAID controller driver, but that was usually it. reply dep_b 2 hours agorootparentprevI ran Windows 2000 from the last beta until they stopped supporting it. The only thing I ever loved more were my Commodore 64, my Amiga 500 and Blue/White G3 tower running Mac OS and Mac OS X. It was extremely consistent in terms of UX, more so than Windows 98 that had all kinds of 16-bit cruft left in it. It did not use a lot of resources, I was a bit of a tweaker back then and I just shut down any services I didn't need for home use and it was absolutely fine on 64MB. It ran all games and applications that I tried to run. I HATED the Windows XP UI, never installed it. Why install something slower and uglier? 2000 was way better out of the box without ever experiencing issues in terms of compatibility. It's true Vista had a ton of nice new features under the hood, but the inconsistency in terms of UX of every Windows version after 2000 felt extremely off-putting. reply marpstar 8 hours agorootparentprevUSB flash drives worked out of box without drivers? Maybe that was just me... The gradient on the \"My Computer\" icon was also very satisfying: https://www.howtogeek.com/wp-content/uploads/2020/06/windows... reply jwells89 8 hours agorootparentIn general I’m a fan of the aesthetic of Win2K’s icons. Strikes a nice middleground between old and new, and as you’ve described it’s just satisfying somehow. reply ack_complete 8 hours agorootparentprevIIRC, Windows 2000 had an overall bluer theme that reduced the green background and orange-tinged 3D colors of 98, and brought in a new set of icons. Also, it added alpha blending to GDI and started using it in Explorer. reply jwells89 8 hours agorootparentIt also used a lighter base gray than 95/98 did, which contributed to it feeling significantly less dreary. This is why for me, the canonical “classic Windows” theme is the 2k version. reply amlib 7 hours agorootparentThe base color also had an off-white beige tone to it. To this day I'm curious if this was made as an attempt to counter balance the usual white balance of pc monitors at the time, which where set at or above 9300K, producing a blue-ish tint. reply pixl97 8 hours agorootparentprevWhen win2000 was still in testing (was this called Memphis, can't remember) the UI elements like the X to close the window would light up in blue. Never made it to release. reply mvkel 9 hours agorootparentprevFor me it was the top window gradient. I think in win98 it was 256 colors and in windows 2000 it was 16-bit reply markus_zhang 7 hours agorootparentprevWin 2000 is the first MS Windows that did not crash very often. I used it as my primary desktop OS for as long as I could until I switched to XP. The best Windows IMHO. reply ilrwbwrkhv 4 hours agorootparentI agree. I still feel deeply nostalgic for Windows 2000. Everything was instant. And almost every program did more than what Slack does today and yet Slack is sluggishly slow. We went wrong somewhere. reply AndyKelley 7 hours agoparentprevI used to think along similar lines, until three weeks ago when I tried KDE for the first time. I don't know why it took me so long to try it. KDE is an excellent steward of this style of user interface. reply TiredOfLife 4 hours agorootparentExcept the search in application launcher uses the Windows 10 method of showing random results in random order with added bonus of results rearranging after you stop typing. reply AndyKelley 51 minutes agorootparentI agree with your criticism. However, the overarching design of KDE makes it reasonable because of configurability. There's an option for \"always sort applications alphabetically\", and the entire widget itself could be replaced with a different one. In fact there is even a concept of \"alternative widgets\" for this with two other options. I don't see something that matches your requirements that already exists but it seems like something that would be in scope for the project. Also this is on version 5 but they had a major release of 6 recently, and I don't know whether those changes included anything to this widget. reply bluedino 7 hours agoparentprevExcept, it all went downhill with Active Desktop. reply xanderlewis 9 hours agoparentprevI somewhat agree… but the force of nostalgia surely is slightly biasing, don’t you think? reply samplatt 8 hours agorootparentLittle of column a, little of column b. Yes, nostalgia is a factor, BUT: - The lack of overhead required by security and 'features' like querying an internet search when you click the start menu or showing advertising in the calculator or better memory management in general, meant that overall UI response in that era used to be much _MUCH_ faster than it is today. Once upon a time you could operate your O/S with the speed of a Starcraft tournament winner; this simply is not possible any more. - You could 'queue' commands - clicking the close button on an program and then (before it had finished closing) clicking the minimise button would minimise the program behind it immediately after the closing process finished. In this way you could chain/queue commands rather than being forced to wait for the OS to update between each step. - Enter and Spacebar did different things. If a prompt had two buttons, one would be outlined with a thick black line that would respond to [Enter], and one would be outlined in a dotted line that would respond to [spacebar]. This is still the case sometimes but is far from ubiquitous. - The top-left corner of the program was reserved for a 'system' menu used to move/resize the window, or quickly exit with a double-click. Though still used by some MS programs today like Explorer, its usefulness is lessened if not all programs utilise it. - Don't even get me started on keyboard shortcuts. These kinds of universally-accepted and _useful_ power-user-oriented design principles are almost absent from UX as it is implemented today. reply Delk 2 hours agorootparentI don't remember almost ever using the top left \"system\" menu after Windows 3.x. Windows 9x (and NT starting from 4.0) had dedicated buttons for maximizing, minimizing and closing windows with a single click. Moving and resizing could be done by grabbing the title bar or corners, as they can today. I don't remember exactly how the resizing and moving as found in the menu worked. Maybe they allowed for resizing/moving without aiming at the relatively small corners or the title screen. But for that I much prefer the traditional Unix style of alt + click/drag anywhere on a window moving the window. Alt + right mouse button similarly resizes it. That is a feature I have missed on Windows. I do seem to remember sometimes using a series a key presses to minimize the window through the system menu without using the mouse. But that was also due to not having an actual keyboard shortcut for doing that. reply camtarn 1 hour agorootparentAlt-space, N to minimise the window. The system window was handy for retrieving windows that had somehow ended up offscreen. Alt-space, M for move, hit an arrow key once, and then the window follows your cursor and ends up on screen by default. Still works in modern versions of Windows - it's somewhat obsolete now you can use Win+arrow keys to move a window around, but it might still be useful if your window has ended up somewhere and you can't figure out where. reply samplatt 1 hour agorootparentImperfect graphics drivers and an environment that changes screens and resolutions a lot means that once every few months some fucking application is going to wander off where I can't get to it, and no amount of Windows+directionkey will move it somewhere usable. Sometimes the Old Magics are still required. reply pndy 3 hours agorootparentprev> The top-left corner of the program was reserved for a 'system' menu used to move/resize the window, or quickly exit with a double-click Pretty sure that's already a standard across all Windows version, plus Xfce and KDE have similar menus and the double-click to close feature reply windows2020 8 hours agorootparentprevWindows 98 was from a time where the assumption a user was familiar with the computer wasn't made. We've since lost that assumption, and a lot with it. reply causality0 8 hours agoparentprevImagine a world where you know exactly where on the title bar you can click to drag a window around and where you can't. A world where you can tell how long pages are by glancing at the scroll bar. A world where you can tell what windows you have open because your taskbar icons are labeled. Enter the world of...Windows 98! reply mixmastamyk 7 hours agorootparentI never wanted a heart icon response on HN until now. reply philistine 9 hours agoparentprevWell, I wouldn't pick the RC Cola of UIs. I'd pick the original. reply RodgerTheGreat 8 hours agorootparentIn this instance are you stanning Windows 1 (lol), MacOS 1, Lisa, the Xerox Alto, or NLS? reply philistine 8 hours agorootparentYes reply bitwize 6 hours agoparentprevWindows 95 had a better UI. Windows 98 was the beginning of Microsoft trying to Webify everything to cram Internet Explorer down everyone's throats. It had non-monopoly-related dangers; by making the difference between the desktop and web more ambiguous, it erased the \"local things=relatively safe, network things=relatively risky\" heuristic we internalized at the time. Not exactly 100% true in an era of Word macro viruses, but it was useful. I would nominate late AmigaOS or BeOS as having a better WIMP UI than any Windows. reply koyote 7 hours agoprevI used to only own 'upgrade' versions of Windows (cheaper). Whenever I had to reinstall Windows (which was frequent back in Win98 days), I had to first install DOS from floppy, then Win 3.11 from floppy, then a CD-ROM driver, then Win95 from CD until I could finally install Win98. (at one point I acquired a non-upgrade version of Win95 but it came on 20+ floppies, so the installation was not really much faster). The end-result was a Win98 installation with a lot of old software lying around as the 'upgrade' installers did not always remove all of the existing items. reply cameronh90 7 hours agoparentMy main desktop had the same installation of Windows for 15 years, from Vista in 2007 right the way through to 10 in 2023, having upgraded through every intermediate version. I only finally did a fresh reinstall for 11. I would copy the hard drive contents from one drive to the next each time I bought a new drive, and ultimately that installation survived three complete hardware refreshes and countless interim component upgrades. Needless to say, there was a LOT of cruft hanging about by the end. From ancient components from old versions of Visual Studio to drivers for early 2000s era scanners. Most of it still ran too, testament to Microsoft’s backwards compatibility. reply CommieBobDole 8 hours agoprevThere's a whole list of weird Windows UI remnants from previous versions on github: https://github.com/Lentern/windows-11-inconsistencies reply romanovcode 7 hours agoparent> The fact that there are still remnants that go all the way back to Windows 1.0 is baffling. It's crazy how lazy Microsoft has gotten when it comes to updating these features. It seems like the older they get, the less likely Microsoft is to actually update them. I don't get it. Why being so overly critical? Why should they update the ODBC.exe or dxdiag.exe, or even regedit.exe? They serve their purpose and work just fine. reply kgeist 6 hours agorootparentRaymond Chen mentioned that some programs peek into the internals of old dialogs (a sort of automation) and so they left a lot of old stuff intact to avoid breaking such programs. reply self_awareness 3 hours agoparentprevThat's an interesting list that shows today's understanding of the word \"modern\". \"Modern\" today means \"recently changed\". Not better, not less bugs, just that it's recently changed. It can even have more bugs, but it still will be \"modern\", and people will be OK with that, because since it's recently changed, it has to have bugs, right? Examples: - \"in some programs, double-clicking program icon on top-left window will close the program. it's a same actions as Win3.1 even Win1.0, where the close button is placed on top-left window until Win95.\" quote Rizki Fauzan (not counting this as a Windows 1.0 remnant because it was still useful in 3.1) Why is it a problem? Why removing this feature is desirable? I often doubleclick the icon to close the window (because my Linux desktop has X in the top left corner instead of top right). Why it's desirable to not have this feature? - pifmgr.dll and moricons.dll (ancient icon libraries) - left since Windows 3.1 Those icons are often superior to their \"modern\" counterparts, even though they’re not vector graphics. Why would anyone want to remove them? - ODBC Data Sources - unchanged since Windows 3.1 I wonder if the author even knows what ODBC is? Some of the entries are also wrong (msconfig has been updated since Win98). This list in the center of why I am depressed by today's software design. reply toast0 3 hours agorootparent> \"Modern\" today means \"recently changed\". Not better, not less bugs, just that it's recently changed. That's what modern always means. Sometimes modern is good, sometimes it's bad; this applies to art, technology, cuisine, lifestyles, etc. Although on windows 'Modern UI' was a keyword refering specifically to the Tiles UI and everything that went with it. I think Microsoft has tried to forget that. If the term sticks, 'Modern UI' will become the same sort of term as 'Art Nouveau' that means New Art in theory, but means art in a style that was defined around 1880-1910. reply userbinator 8 hours agoparentprev...also known as \"the good parts\". reply Arrath 8 hours agorootparentYup. There's 'The useless modern settings panels' and 'The Win7 holdover control panels I can drill down into, and actually make useful configuration changes in' reply airstrike 7 hours agorootparentprevright? they can pry right-click-on-titlebar-to-move from my cold, dead hands reply Dalewyn 9 hours agoprev [–] The Windows 11 installer is basically Windows 7 for the first part too, though this might be changing soon. reply p_l 3 hours agoparentIt's just little changed in theme/resource files, it boots straight to windows 11 for installer. Before Vista, the installer process on NT had three phases: 1. Optional DOS phase to bring minimal NTLDR environment up 2. \"Micro\" NT without Win32 API that ran only NTAPI (NE format executables et al) applications in the minimal console support (with the characteristic default of white on blue). This would do equivalent of \"minimal install\" on unices to the target disk, setting up proper HAL.DLL, boot time drivers, and setting up registry hives. 3. GUI installer that booted from hard disk with Win32 subsystem enabled. reply userbinator 9 hours agoparentprev [–] I'm not sure about 2K and before, but starting with XP all of the NT-based Windows use some version of https://en.wikipedia.org/wiki/Windows_Preinstallation_Enviro... for their installer. reply EvanAnderson 9 hours agorootparentWindows 2000, 2003, and XP still used a lot of the install components from earlier NT versions. Vista was the first incarnation of the \"modern\" Windows setup that we still use today. PE was built using XP but the XP installer was mostly the NT installer (text mode then Mini-Windows). reply Dalewyn 8 hours agorootparentprev [–] As mentioned already by a sibling commenter, installers for NT4 through XP/2003 were all based off earlier NT installers with text mode and all. WinPE has its roots in NT5.1 (XP), but it was from NT6.0 (Vista) that we got the \"modern\" installer, and it last saw a UI overhaul during NT6.1 (7). Throughout 8 through 11 so far, the first part of the installer was and is basically Windows 7. The screenshot[1] of WinPE 10.0 on that Wikipedia article showcases the Windows Basic[2] theme in all its glory, including even the Windows 7 and prior Task Manager. [1]: https://en.wikipedia.org/wiki/File:Windows_PE_screenshot.png [2]: https://betawiki.net/wiki/Windows_Basic reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post explores the multi-stage setup process of Windows 98 on Stack Overflow, a renowned online community for developers.",
      "The setup involves running the initial phase in a DOS environment, followed by a minimal Windows 3.1 installation, and concluding in the full Windows 98 system for personalization.",
      "This multi-step setup was necessary back then due to disk space restrictions and technical limitations, showcasing the evolution of operating system installation processes over time."
    ],
    "commentSummary": [
      "Retrocomputing.stackexchange.com discusses various topics including installing older operating systems, Windows Explorer history, and DOS file naming conventions.",
      "Users share nostalgia for older Windows versions, admire Windows 2000 stability, and debate preserving old UI features for compatibility in the era of Windows 11.",
      "The forum also delves into SQL code readability, UI design progression, and challenges stemming from imperfect graphics drivers in retro computing."
    ],
    "points": 242,
    "commentCount": 137,
    "retryCount": 0,
    "time": 1712707806
  },
  {
    "id": 39981623,
    "title": "ScreenAI: Vision-Language Model for UI and Infographics",
    "originLink": "https://research.google/blog/screenai-a-visual-language-model-for-ui-and-visually-situated-language-understanding/",
    "originBody": "Blog ScreenAI: A visual language model for UI and visually-situated language understanding ScreenAI: A visual language model for UI and visually-situated language understanding March 19, 2024 Posted by Srinivas Sunkara and Gilles Baechler, Software Engineers, Google Research We introduce ScreenAI, a vision-language model for user interfaces and infographics that achieves state-of-the-art results on UI and infographics-based tasks. We are also releasing three new datasets: Screen Annotation to evaluate the layout understanding capability of the model, as well as ScreenQA Short and Complex ScreenQA for a more comprehensive evaluation of its QA capability. Quick links Paper Share Copy link × Screen user interfaces (UIs) and infographics, such as charts, diagrams and tables, play important roles in human communication and human-machine interaction as they facilitate rich and interactive user experiences. UIs and infographics share similar design principles and visual language (e.g., icons and layouts), that offer an opportunity to build a single model that can understand, reason, and interact with these interfaces. However, because of their complexity and varied presentation formats, infographics and UIs present a unique modeling challenge. To that end, we introduce “ScreenAI: A Vision-Language Model for UI and Infographics Understanding”. ScreenAI improves upon the PaLI architecture with the flexible patching strategy from pix2struct. We train ScreenAI on a unique mixture of datasets and tasks, including a novel Screen Annotation task that requires the model to identify UI element information (i.e., type, location and description) on a screen. These text annotations provide large language models (LLMs) with screen descriptions, enabling them to automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. At only 5B parameters, ScreenAI achieves state-of-the-art results on UI- and infographic-based tasks (WebSRC and MoTIF), and best-in-class performance on Chart QA, DocVQA, and InfographicVQA compared to models of similar size. We are also releasing three new datasets: Screen Annotation to evaluate the layout understanding capability of the model, as well as ScreenQA Short and Complex ScreenQA for a more comprehensive evaluation of its QA capability. ScreenAI ScreenAI’s architecture is based on PaLI, composed of a multimodal encoder block and an autoregressive decoder. The PaLI encoder uses a vision transformer (ViT) that creates image embeddings and a multimodal encoder that takes the concatenation of the image and text embeddings as input. This flexible architecture allows ScreenAI to solve vision tasks that can be recast as text+image-to-text problems. On top of the PaLI architecture, we employ a flexible patching strategy introduced in pix2struct. Instead of using a fixed-grid pattern, the grid dimensions are selected such that they preserve the native aspect ratio of the input image. This enables ScreenAI to work well across images of various aspect ratios. The ScreenAI model is trained in two stages: a pre-training stage followed by a fine-tuning stage. First, self-supervised learning is applied to automatically generate data labels, which are then used to train ViT and the language model. ViT is frozen during the fine-tuning stage, where most data used is manually labeled by human raters. play silent looping video pause silent looping video ScreenAI model architecture. Data generation To create a pre-training dataset for ScreenAI, we first compile an extensive collection of screenshots from various devices, including desktops, mobile, and tablets. This is achieved by using publicly accessible web pages and following the programmatic exploration approach used for the RICO dataset for mobile apps. We then apply a layout annotator, based on the DETR model, that identifies and labels a wide range of UI elements (e.g., image, pictogram, button, text) and their spatial relationships. Pictograms undergo further analysis using an icon classifier capable of distinguishing 77 different icon types. This detailed classification is essential for interpreting the subtle information conveyed through icons. For icons that are not covered by the classifier, and for infographics and images, we use the PaLI image captioning model to generate descriptive captions that provide contextual information. We also apply an optical character recognition (OCR) engine to extract and annotate textual content on screen. We combine the OCR text with the previous annotations to create a detailed description of each screen. A mobile app screenshot with generated annotations that include UI elements and their descriptions, e.g., TEXT elements also contain the text content from OCR, IMAGE elements contain image captions, LIST_ITEMs contain all their child elements. LLM-based data generation We enhance the pre-training data's diversity using PaLM 2 to generate input-output pairs in a two-step process. First, screen annotations are generated using the technique outlined above, then we craft a prompt around this schema for the LLM to create synthetic data. This process requires prompt engineering and iterative refinement to find an effective prompt. We assess the generated data's quality through human validation against a quality threshold. You only speak JSON. Do not write text that isn’t JSON. You are given the following mobile screenshot, described in words. Can you generate 5 questions regarding the content of the screenshot as well as the corresponding short answers to them? The answer should be as short as possible, containing only the necessary information. Your answer should be structured as follows: questions: [ {{question: the question, answer: the answer }}, ... ] {THE SCREEN SCHEMA} A sample prompt for QA data generation. By combining the natural language capabilities of LLMs with a structured schema, we simulate a wide range of user interactions and scenarios to generate synthetic, realistic tasks. In particular, we generate three categories of tasks: Question answering: The model is asked to answer questions regarding the content of the screenshots, e.g., “When does the restaurant open?” Screen navigation: The model is asked to convert a natural language utterance into an executable action on a screen, e.g., “Click the search button.” Screen summarization: The model is asked to summarize the screen content in one or two sentences. Block diagram of our workflow for generating data for QA, summarization and navigation tasks using existing ScreenAI models and LLMs. Each task uses a custom prompt to emphasize desired aspects, like questions related to counting, involving reasoning, etc. LLM-generated data. Examples for screen QA, navigation and summarization. For navigation, the action bounding box is displayed in red on the screenshot. Experiments and results As previously mentioned, ScreenAI is trained in two stages: pre-training and fine-tuning. Pre-training data labels are obtained using self-supervised learning and fine-tuning data labels comes from human raters. We fine-tune ScreenAI using public QA, summarization, and navigation datasets and a variety of tasks related to UIs. For QA, we use well established benchmarks in the multimodal and document understanding field, such as ChartQA, DocVQA, Multi page DocVQA, InfographicVQA, OCR VQA, Web SRC and ScreenQA. For navigation, datasets used include Referring Expressions, MoTIF, Mug, and Android in the Wild. Finally, we use Screen2Words for screen summarization. Along with the fine-tuning datasets, we evaluate the fine-tuned ScreenAI model using three novel benchmarks: Screen Annotation: Enables the evaluation model layout annotations and spatial understanding capabilities. ScreenQA Short: A variation of ScreenQA, where its ground truth answers have been shortened to contain only the relevant information that better aligns with other QA tasks. Complex ScreenQA: Complements ScreenQA Short with more difficult questions (counting, arithmetic, comparison, and non-answerable questions) and contains screens with various aspect ratios. The fine-tuned ScreenAI model achieves state-of-the-art results on various UI and infographic-based tasks (WebSRC and MoTIF) and best-in-class performance on Chart QA, DocVQA, and InfographicVQA compared to models of similar size. ScreenAI achieves competitive performance on Screen2Words and OCR-VQA. Additionally, we report results on the new benchmark datasets introduced to serve as a baseline for further research. Comparing model performance of ScreenAI with state-of-the-art (SOTA) models of similar size. Next, we examine ScreenAI’s scaling capabilities and observe that across all tasks, increasing the model size improves performances and the improvements have not saturated at the largest size. Model performance increases with size, and the performance has not saturated even at the largest size of 5B params. Conclusion We introduce the ScreenAI model along with a unified representation that enables us to develop self-supervised learning tasks leveraging data from all these domains. We also illustrate the impact of data generation using LLMs and investigate improving model performance on specific aspects with modifying the training mixture. We apply all of these techniques to build multi-task trained models that perform competitively with state-of-the-art approaches on a number of public benchmarks. However, we also note that our approach still lags behind large models and further research is needed to bridge this gap. Acknowledgements This project is the result of joint work with Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Carbune, Jason Lin, Jindong Chen and Abhanshu Sharma. We thank Fangyu Liu, Xi Chen, Efi Kokiopoulou, Jesse Berent, Gabriel Barcik, Lukas Zilka, Oriana Riva, Gang Li,Yang Li, Radu Soricut, and Tania Bedrax-Weiss for their insightful feedback and discussions, along with Rahul Aralikatte, Hao Cheng and Daniel Kim for their support in data preparation. We also thank Jay Yagnik, Blaise Aguera y Arcas, Ewa Dominowska, David Petrou, and Matt Sharifi for their leadership, vision and support. We are very grateful toTom Small for helping us create the animation in this post. Labels: Human-Computer Interaction and Visualization Machine Intelligence Quick links Paper Share Copy link × Other posts of interest March 28, 2024 AutoBNN: Probabilistic time series forecasting with compositional bayesian neural networks Algorithms & Theory · Machine Intelligence · Open Source Models & Datasets March 20, 2024 Computer-aided diagnosis for lung cancer screening Health & Bioscience · Human-Computer Interaction and Visualization · Machine Intelligence March 20, 2024 Computer-aided diagnosis for lung cancer screening Health & Bioscience · Human-Computer Interaction and Visualization · Machine Intelligence",
    "commentLink": "https://news.ycombinator.com/item?id=39981623",
    "commentBody": "ScreenAI: A visual LLM for UI and visually-situated language understanding (research.google)225 points by gfortaine 16 hours agohidepastfavorite32 comments abrichr 12 hours agoAt OpenAdapt we have had excellent results combining Segment Anything Model (SAM) with GPT-4 for screen understanding. Work-in-progress: https://github.com/OpenAdaptAI/OpenAdapt/pull/610 reply spxneo 11 hours agoparentMIT license too (getting rare)! mad respects, thank you. reply abrichr 7 hours agorootparentThank you! reply rcthompson 6 hours agoprevI can't wait for the new wave of terrible UIs specifically designed to fool AI agents into clicking the \"send me all your money\" button. (For bonus points, do this while making the UI seem perfectly reasonable to humans.) reply aussieguy1234 2 hours agoparentthat would take UI dark patterns to a new level reply S0y 15 hours agoprevI find it quite ironic that google are the biggest players in creating solutions that actively contribute in defeating their very own anti automation software. Makes you wonder if the goal of their captcha system was ever really to stop people from botting. reply _boffin_ 9 hours agoparent> Makes you wonder if the goal of their captcha system was ever really to stop people from botting. I don't think that was the main goal, but rather for them to get a massive labeling dataset for training their models on the cheap. reply spxneo 15 hours agoparentprevthat would be the 4d chess move: imagine when you get a captcha with \"click 1 thing from things that do not fly\" but you actually helping select drone targets somewhere in middle east reply notahacker 12 hours agorootparentthis is what happens at the intersection of unlimited VC money for \"AI\" and wannabe entrepreneurs that read Ender's Game and thought \"business opportunity here\" reply htrp 15 hours agorootparentprevhttps://xkcd.com/1897/ reply echelon 13 hours agoparentprev\"Remove the ads from this page\" I can't wait for AI to become the ultimate ad-removal tool. There might be an arms race, but the anti-ad side will win as long as there isn't a unilateral winner (strongest models, biggest platform). There will be enough of a shake up to the current regime -- search, browsers, etc. -- that there is opportunity for new players to attack multiple fronts. Given choice, I don't think users will accept a hamstrung interface that forces a subpar experience. We basically just need to make sure Google, Microsoft/OpenAI, or some other industry giant doesn't win or that we don't wind up living under a cabal of just a few players. I'm already hopefully imagining AI agents working for us to not just remove advertising noise, but to actively route around all of the times and places we're taken advantage of. That would be an excellent future. reply knallfrosch 2 hours agorootparentApple won't allow AI-powered extensions. They currently don't even allow blacklist-powered extensions like UBlock. And that's the \"privacy\" focussed company, shoving ads down your throat. The competitor is Google. And 90% of users spend 90% of their time in walled gardens like Instagram or TikTok anyway. They see built-in ads. Do I need to say more? reply cobbal 4 hours agorootparentprevI'm now envisioning an ad framework, where instead of selling rectangles of content, advertisers bid to use an llm to rewrite the whole article with at least {{4}} mentions of {{how refreshing pepsi is}}. reply notduncansmith 13 hours agorootparentprev“It looks like this entire article is an advertorial piece for a book. Would you still like to read it?” reply taneq 9 hours agorootparentAnd most people would say it’s broken and only returns false positives. Well, half right, but the positives aren’t false… reply Xenoamorphous 12 hours agorootparentprevEverything behind a paywall then? reply warthog 15 hours agoparentprevhuh exactly what I thought when I saw this reply passion__desire 13 hours agorootparentIsn't creating dataset for this the most easiest? we have source text of html and how they are rendered with all the intermediate info about tags, css layout etc available from most modern browsers. reply EZ-Cheeze 9 hours agoprevImagine a top-level screen filter that processes all you see in ways which you define. For example, \"hide all faces\" can help you spot new details in a movie since your eyes won't be automatically attracted to faces. Or \"hide all proper names\" can make internet browsing more interesting and mysterious reply namanyayg 16 hours agoprevI was looking for something similar recently and had found CogAgent[0] that looks quite interesting, has anyone tried anything similar? 0. https://github.com/THUDM/CogVLM?tab=readme-ov-file#gui-agent... reply maciejgryka 13 hours agoparentI haven't read through it yet, but there's FerretUI from Apple (mobile-specific, but I think a lot of learnings are generic) https://arxiv.org/abs/2404.05719 reply Klaster_1 4 hours agoprevImagine a new crop of QA automation tooling that's going to leverage these capabilities! • Semantic change comparison between screenshots. Visual regression testing, where you prompt the model to ignore certain things instead of masking and where it labels the changes with a message, like \"chart color changed\" or \"text shifted down by 3 pixels\". • Using plain English as test scenarios, instead of brittle WebDriver-like APIs. • Autonomous agent fuzzing the application by free roaming the UI. • RAG the design artifacts from Jira and Google Docs for more targeted feature exploration and test scenario generation. • Automatic bug reports as the output of the above. Or even send a draft PR to fix an issue, while at it! The more I think about use cases, the more it sounds like full software development automation. Late in game, we won't probably need software as it exists today at all. This feels like reading Accelerando again, but this time it's happening for real and to you. P.S.: Didn't expect to see a cafe from Cyprus - Akakiko Limassol - used as the demo, I'll remember to visit next time I'm in the area :P reply ilaksh 14 hours agoprevHow does this compare to the new GPT-4-turbo vision or Claude 3 Opus vision? Also, is this open source or can we access it with Vertex AI? reply abrichr 12 hours agoparentWe haven't been able to use Claude 3 Opus vision yet because we're in Canada, but GPT-4-V works extremely well (when combined with Segment Anything). See: https://github.com/OpenAdaptAI/OpenAdapt/pull/610 (work in progress). Unfortunately we can't compare it to ScreenAI directly since as far as I can tell it is not generally available. However ScreenAI does not appear to use a separate segmentation step, which we needed to implement in order to get good results. reply spxneo 11 hours agorootparentcan you elaborate on \"extremely well\"? where is it currently falling short? reply abrichr 7 hours agorootparentYou can see how it performs in describing GUI elements in the linked PR if you scroll down -- here's a direct link: https://private-user-images.githubusercontent.com/774615/320... Regarding failure modes, we have yet to do extensive testing, but I've seen it confuse the divide and subtract buttons on the calculator before only once. reply f38zf5vdt 14 hours agoprevGoogle claims SoTA but it appears that, according to Apple, they may already be out of date: https://arxiv.org/abs/2404.05719 reply moandcompany 14 hours agoparentThe core aspects of this research, datasets, and use cases discussed here have been in progress for quite a long time at Google (it's been WIP for many many years). The same can probably be said of Apple's paper though! Congrats to all the folks involved :) reply sAbakumoff 4 hours agoprevpardon my ignorance, but can I run this model locally? reply QuantumG 2 hours agoparentI wondered that too, it's Google so it's going to be Tensorflow or accessible through some workbook.. and I've lost interest. reply twobitshifter 14 hours agoprevis this similar to apple’s Realm? reply piecerough 15 hours agoprev [–] \"We are also releasing three new datasets: Screen Annotation to evaluate the layout understanding capability of the model, as well as ScreenQA Short and Complex ScreenQA for a more comprehensive evaluation of its QA capability.\" Looks useful to me for replicating some things. Good stuff! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Blog ScreenAI introduces ScreenAI, a vision-language model excelling in UI and infographic tasks, backed by three new datasets for evaluation.",
      "ScreenAI, utilizing a PaLI-based architecture and a pix2struct patching strategy, demonstrates strong performance on diverse tasks, competing well with similar-sized models.",
      "The model undergoes two-stage training and fine-tuning with public QA, summarization, and navigation datasets, showing improved performance with scale, hinting at potential for further enhancements through research."
    ],
    "commentSummary": [
      "OpenAdapt researchers have introduced a new visual Language Model (LLM) named ScreenAI to enhance user interface (UI) and visually-situated language comprehension by combining SAM with GPT-4.",
      "Potential issues like deceptive UIs targeting AI agents, concerns about Google's captcha, and the emergence of novel ad-filtering tech are discussed, alongside comparisons with GPT-4-turbo and Claude 3 Opus vision.",
      "The conversation explores the model's practicality, use in software automation, possible failure scenarios, and the unveiling of new datasets for assessment, expressing enthusiasm and inquisitiveness toward future AI and UI advancements."
    ],
    "points": 225,
    "commentCount": 32,
    "retryCount": 0,
    "time": 1712682953
  },
  {
    "id": 39984209,
    "title": "Creating Reliable AI Systems: Tips and Strategies",
    "originLink": "https://www.rainforestqa.com/blog/building-reliable-systems-out-of-unreliable-agents",
    "originBody": "The Rainforest Blog Log in Talk to us All Articles QA Strategy Test Automation Manual Testing Inside Rainforest Engineering Talk to us Log In Blog Engineering Building reliable systems out of unreliable agents April 3, 2024 Maciej Gryka Contents Start with simple prompts Use an eval system to do prompt engineering Improve with observability Invest in RAG Fine-tune your model Use complementary agents Final notes If you’ve tried building real-world features with AI, chances are that you’ve experienced reliability issues. It’s common knowledge that AI makes for great demos, but… questionable products. After getting uncannily correct answers at first, you get burned on reliability with some wild output and decide you can’t make anything useful out of that. Well, I’m here to tell you that there’s hope. Even though AI agents are not reliable, it is possible to build reliable systems out of them. These learnings come from a years-long process of creating a QA AI. While building, we found a process that worked pretty well for us and we’re sharing it here. As a summary, it consists of these high-level steps: Write simple prompts to solve your problem Use that experience to build an eval system to do prompt engineering and improve performance in a principled way Deploy your AI system with good observability, and use that signal to keep gathering examples and improving your evals Invest in Retrieval Augmented Generation (RAG) Fine-tune your model using the data you gathered from earlier steps Having worked on this problem for a while, I think these are the best practices every team should adopt. But there’s an additional approach we came up with that gave us a breakthrough in reliability, and it might be a good fit for your product, too: Use complementary agents The principle behind this step is simple: it’s possible to build systems of complementary agents that work much more reliably than a single agent. More on that later. Before we jump in, a note on who this is for: you don’t need much AI experience to follow the process we lay out here. In fact, most of our team while building our QA agent didn’t have previous AI or ML experience. A solid software engineering background, however, is super helpful — a sentiment echoed by well-known people in the industry. Particularly, thinking deeply about how to test what you’re building and constantly finding ways to optimize your workflow are really important. Contents Start with simple prompts Use an eval system to do prompt engineering Improve with observability Invest in RAG Fine-tune your model Use complementary agents Final notes Start with simple prompts The most obvious way to start using an LLM to solve a problem is simply asking it to do the thing in your own words. This approach often works well enough at the beginning to give you hope, but starts falling down as soon as you want any reliability. The answers you get might be mostly correct, but not good enough for production. And you’ll quickly notice scenarios where the answers are consistently wrong. The best LLMs today are amazing generalists, but not very good specialists — and generally, you want specialists to solve your business problems. They need to have enough general knowledge to not be tedious, but at the same time they need to know how exactly to handle the specifics in the gray areas of your problem space. Let’s take a trivial example: you want to get a list of ingredients needed to prepare different things to eat. You start with the first thing that comes to mind: This is good — the list of ingredients is right there. But there’s also a bunch of other stuff that you don’t need. For example, you might use the same knife for both jars and not care for being lectured about knife hygiene. You can fiddle with the prompt pretty easily: Better, but there’s still some unnecessary commentary there. Let’s have another shot: OK, I guess that’ll do. Now we just need to make it JSON so we can integrate it with the rest of our product. Also, just to be safe, let’s run it a few times to make sure it’s reliable: As you can see, we’re reliably getting JSON out, but it’s not consistent: the capitalization of the keys and what’s included in each value varies. These specific problems are easy to deal with and even this might be good enough for your use case, but we’re still far from the reliable and reproducible behavior we’re looking for. Minimum necessary product integration At this point, it’s time to integrate a minimal version of your “AI component” with your product, so the next step is to start using the API instead of the console. Grab your favorite LLM-provider client (or just use their API — there are some good reasons to stick with HTTP) and integrate it into your product in the most minimal way possible. The point is to start building out the infrastructure, knowing that the results won’t be great yet. Some cheat codes at this point, based on my experience: If you use mypy, you might be tempted to use strongly-typed inputs when interacting with the client (e.g., the OpenAI client), but my advice would be: don’t. While I like having mypy around, it’s much easier to work with plain dictionaries to build your messages and you’re not risking a lot of bugs. In my experience, it’s a good idea to set temperature=0.0 in all your model calls if you care about reliability. You still won’t get perfect reproducibility, but it’s usually the best place to start your explorations. If you’re thinking about using a wrapper like instructor to get structured data out of the LLM: it’s really cool and makes some use-cases very smooth, but also makes your code a little less flexible. I’d usually start without it and then bring it in at a later point, once I’m confident in the shape of my data. Use an eval system to do prompt engineering The first thing you should try after the naive “ask a simple question” approach is prompt engineering. While the phrase is common, I don’t think many people have an accurate definition of what “prompt engineering” actually means, so let’s define it first. When I say “prompt engineering,” I mean something like, “iterative improvement of a prompt based on measurable success criteria”, where “iterative” and “measurable success criteria” are the key phrases. (I like this post from last year as an early attempt to define this.) The key is to have some way of determining whether an answer you get from an LLM is correct and then measuring correctness across examples to give you a number you can compare over time. Create an evaluation loop so you have a way of checking any change you make, and then make that loop as fast as it can be so you can iterate effectively. For an overview of how to think about your eval systems, see this excellent blog post. Evaluating when there are multiple correct answers This whole procedure rhymes with “testing” and “collecting a validation set,” except for any given question there might be multiple correct answers, so it’s not obvious how to do this. Here are some situations and how to deal with them: It’s possible to apply deterministic transformations on the output of the LLM before you compare it to the “known good” answer. An example might be the capitalization issue from earlier, or maybe you only ever want three first words as an output. Running these transformations will make it trivial to compare what you get with what you expect. You might be able to use some heuristics to validate your output. E.g., if you’re working on summarization, you might be able to say something like “to summarize this story accurately, the following words are absolutely necessary” and then get away with doing string matching on the response. Maybe you need the output in a certain format. E.g., you’re getting function calls or their arguments from an LLM or you’re expecting country codes or other well-defined outputs. In these cases, you can validate what you get against a known schema and retry in case of errors. In practice, you can use something like instructor — if you’re comfortable with the constraints it imposes, including around code flexibility — and then you’re left with straightforwardly comparing structured data. In a true Inception fashion, you might want to use a simpler, smaller, and cheaper LLM to evaluate the outputs of your big-LLM-using-component. Comparing two differently-written, but equivalent lists of ingredients is an easy task even for something like GPT 3.5 Turbo. Just keep in mind: even thought it’s pretty reliable, you’re now introducing some flakiness into your test suite. Trade-offs! To evaluate an answer, you might have to “execute” the entire set of instructions the agent gives you and check if you’ve reached the goal. This is more complex and time-consuming, but sometimes the only way. For example, we often ask an agent to achieve a goal inside a browser by outputting a series of instructions that might span multiple screens. The only way for us to check its answer is to execute the instructions inside Playwright and run some assertions on the final state. Building your validation set Once you have your evaluation loop nailed down, you can build a validation set of example inputs and the corresponding outputs you’d like the agent to produce. As you evaluate different strategies and make changes to your prompts, it’s ideal if you have a single metric to compare over time. Something like “% prompts answered correctly” is the most obvious, but something like precision/recall might be more informative depending on your use case. It’s also possible you won’t be able to use a single metric if you’re evaluating fuzzy properties of your answers, in which case you can at least look at what breaks after each change and make a judgment call. Prompt engineering tricks Now’s your chance to try all the prompt engineering tricks you’ve heard about. Let’s cover some of them! First of all, provide all the context you’d need to give to an intelligent human operator who’s unfamiliar with the nuances and requirements of the task. This is a necessary (but not sufficient!) condition to making your system work. E.g., if you know you absolutely always want some salted butter under your peanut butter, you need to include that information in your prompt. If you’re not getting the correct responses, ask the agent to think step-by-step before providing the actual answer. This can be a little tricky if you’re expecting structured data out — you’ll have to somehow give the agent a way to do some reasoning before it makes any consequential decisions and locks itself in. E.g., if you use the tool-calling API, which is really nifty, you might be tempted to tell the agent to do some chain-of-thought reasoning by having a JSON schema similar to this: [ { \"type\": \"function\", \"function\": { \"name\": \"scoop_out\", \"description\": \"scoop something out\", \"parameters\": { \"type\": \"object\", \"properties\": { \"chain_of_thought\": { \"type\": \"string\", \"description\": \"the reasoning for this action\" }, \"jar\": { \"type\": \"string\", \"description\": \"the jar to scoop out of\" }, \"amount\": { \"type\": \"integer\", \"description\": \"how much to scoop out\" } }, \"required\": [\"chain_of_thought\", \"jar\",\"amount\"] } } }, { \"type\": \"function\", \"function\": { \"name\": \"spread\", \"description\": \"spread something on a piece of bread\", \"parameters\": { \"type\": \"object\", \"properties\": { \"chain_of_thought\": { \"type\": \"string\", \"description\": \"the reasoning for this action\" }, \"substance\": { \"type\": \"string\", \"description\": \"what to spread\" } }, \"required\": [\"chain_of_thought\", \"substance\"] } } } ] Unfortunately, this will make the agent output the function name before it produces the chain-of-thought, locking it into the early decision and defeating the whole point. One way to get around this is to pass in the schema with the available functions, but ask the agent to output a JSON that wraps the function spec, similar to this: You are an assistant helping prepare food. Given a dish name, respond with a JSON object of the following structure: {{ \"chain_of_thought\": \"Describe your reasoning for the next action\", \"function_name\": \"scoop_out\"\"spread\", \"function_args\": {{}} }} This has fewer guarantees, but works well enough in practice with GPT 4 Turbo. Generally, chain-of-thought has a speed/cost vs. accuracy trade-off, so pay attention to your latencies and token usage in addition to correctness. Another popular trick is few-shot prompting. In many cases, you’ll get a noticeable bump in performance if you include a few examples of questions and their corresponding answers in your prompt, though this isn’t always feasible. E.g., your actual input might be so large that including more than a single “shot” in your prompt isn’t practical. Finally, you can try offering the agent a bribe or telling it something bad will happen if it answers incorrectly. We didn’t find these tactics worked for us, but they might for you — they’re worth trying, assuming you trust your eval process. Every trick from the ones listed above will change things: some things will hopefully get better, but it’s likely you’ll break something else at the same time. It’s really important you have a representative set of examples that you can work with, so commit to spending time on building it. Improve with observability At this point, you’ll likely have something that’s good enough to deploy as an alpha-quality product. Which you should absolutely do as soon as you can so you can get real user data and feedback. It’s important you’re open about where your system is in terms of robustness, but it’s impossible to overstate the value of users telling you what they think. It’s tempting to get everything working correctly before opening up to users, but you’ll hit a point of diminishing returns without real user feedback. This is an absolutely necessary ingredient to making your system better over time — don’t skip it. Before release, just make sure your observability practices are solid. From my perspective, this basically means logging all of your LLM input/output pairs so you can a) look at them and learn what your users need and b) label them manually to build up your eval set. There a many options to go with here, from big monitoring providers you might already be using to open-source libraries that help you trace your LLM calls. Some, like openllmetry and openinference even use the OpenTelemetry under the hood, which seems like a great idea. I haven’t seen a tool focused on labeling the data you’ve gathered and turning it into a validation set, however, which is why we built our own solution: store some JSON files in S3 and have a web interface to look at and label them. It doesn’t have as many bells and whistles as off-the-shelf options, but it’s enough for what we need at the moment. Invest in RAG Once you’ve exhausted all your prompt-engineering tricks and you feel like you’re out of ideas and your performance is at a plateau, it might be time to invest in a RAG pipeline. Roughly, RAG is runtime prompt engineering where you build a system to dynamically add relevant things to your prompt before you ask the agent for an answer. An example might be answering questions about very recent events. This isn’t something LLMs are good at, because they’re not usually retrained to include the latest news. However, it’s relatively straightforward to run a web search and include some of the most relevant news articles in your prompt before asking the LLM to give you the answer. If you have relevant data of your own you can leverage, it’s likely to give you another noticeable improvement. Another example from our world: we’ve got an agent interacting with the UI of an application based on plain-English prompts. We also have more than ten years worth of data from our clients writing testing prompts in English and our crowd of human testers executing those instructions. We can use this data to tell the agent something like “it looks like most human testers executing similar tasks clicked on button X and then typed Y into field Z” to guide it. Retrieval is great and very powerful, but it has real trade-offs, complexity being the main one. Again, there are many options: you can roll your own solution, use an external provider, or have some combination of the two (e.g., using OpenAI’s embeddings and storing the vectors in your Postgres instance). A particular library that looked great (a little more on the do-it-yourself end of the spectrum) is RAGatouille, but I wasn’t able to make it work and gave up after a couple of hours. In the end, we used BigQuery to get data out, OpenAI for producing embeddings, and Pinecone for storage and nearest-neighbor search because that was the easiest way for us to deploy something without setting up a lot of new infrastructure. Pinecone makes it very easy to store and search through embeddings with their associated metadata to augment your prompts. There’s more we can do here — we didn’t evaluate any alternative embedding engines, we only find top-3 related samples, and get limited data out of those samples currently. Looking at alternative embeddings, including more samples, and getting more details information about each sample is something we plan to look at in the future. You can spend quite a while on this level of the ladder. There’s a lot of room for exploration. If you exhaust all the possibilities and ways of building your pipeline, still aren’t getting good enough results and can’t think any more sources of useful data, it’s time to consider fine-tuning. Fine-tune your model Now we’re getting to the edges of the known universe. If you’ve done everything above: created an eval system, shipped an AI product, observed it running in production, got real user data, and even have a useful RAG pipeline, then congratulations! You’re on the bleeding edge of applied AI. Where to go from here is unclear. Fine-tuning a model based on the data you’ve gathered so far seems like the obvious choice. But beware — I’ve heard conflicting opinions in the industry about the merits of fine-tuning relative to the effort required. It seems like it should work better, but there are unresolved practical matters: OpenAI only allows you to fine-tune older models, and Anthropic is kind of promising to make it available soon with a bunch of caveats. Fine-tuning and hosting your own models is a whole different area of expertise. Which model do you choose as the base? How do you gather data for fine-tuning? How do you evaluate any improvements? And so on. In the case of self-hosted models, I’d caution against hoping to save money vs. hosted solutions — you’re very unlikely to have the expertise and the economies of scale to get there, even if you choose smaller models. My advice would be to wait a few months for the dust to settle a bit before investing here, unless you’ve tried everything else already and still aren’t getting good-enough results. We haven’t had to do this so far because we still haven’t exhausted all the possibilities mentioned above, so we’re postponing the increase in complexity. Use complementary agents Finally, I want to share a trick that might be applicable to your problem, but is sort of independent of the whole process described above — you can apply it at any of the stages I’ve described. It involves a bit of a computation vs. reliability trade-off: it turns out that in many situations it’s possible to throw more resources at a problem to get better results. The only question is: can you find a balance that’s both fast and cheap enough while being accurate enough? You’ll often feel like you’re playing whack-a-mole when trying to fix specific problems with your LLM prompts. For instance, I often find there’s a tension between creating the correct high-level plan of execution and the ability to precisely execute it. This reminded me of the idea behind “pioneers, settlers, and city planners”: different people have different skills and approaches and thrive in different situations. It’s rare for a single person to both have a good grand vision and to be able to precisely manage the vision’s execution. Of course, LLMs aren’t people, but some of their properties make the analogy work. While it’s difficult to prompt your way to an agent that always does the right thing, it’s much easier to plan what’s needed and create a team of specialists that complement each other. I’ve seen a similar approach called an “ensemble of agents,” but I prefer “complementary agents” for this approach because it highlights that the agents are meaningfully different and support each other in ways that identical agents couldn’t. For example, to achieve a non-obvious goal, it helps to create a high-level plan first, before jumping into the details. While creating high-level plans, it’s useful to have a very broad and low-resolution view of the world without getting bogged down by details. Once you have a plan, however, executing each subsequent step is much easier with a narrow, high-resolution view of the world. Specific details can make or break your work, and at the same time, seeing irrelevant information can confuse you. How do we square this circle? One answer is creating teams of complementary agents to give each other feedback. LLMs are pretty good at correcting themselves if you tell them what they got wrong, and it’s not too difficult to create a “verifier” agent that checks specific aspects of a given response. An example conversation between “high level planner” and “verifier” agents might look something like the following: Planner OK, we need to make a PB&J sandwich! To do that, we need to get some peanut butter, some jelly and some bread, then take out a plate and a knife. Verifier Cool, that sounds good. Planner OK, now take the peanut butter and spread it on the bread. Verifier (noticing there's no slice of bread visible) Wait, I can't see the bread in front of me, you can't spread anything on it because it's not there. Planner Ah, of course, we need to take the slice of bread out first and put it on a plate. Verifier Yep, that seems reasonable, let's do it. The two agents complement each other, and neither can work on its own. Nobody is perfect, but we can build a reliable system out of flawed pieces if we’re thoughtful about it. How we’re using complementary agents This is exactly what we did for our testing agents: there’s a planner and a verifier. The planner knows the overall goal and tries to achieve it. It’s creative and can usually find a way to get to the goal even if it isn’t immediately obvious. E.g., if you ask it to click on a product that’s not on the current page, it’ll often use the search functionality to look for the product. But sometimes the planner is too optimistic and wants to do things that seem like they should be possible, but in fact aren’t. For example, it might want to click on a “Pay now” button on an e-commerce checkout page, because the button should be there, but it’s just below the fold and not currently visible. In such cases, the verifier (who doesn’t know the overall goal and is only looking at the immediate situation) can correct the planner and point out that the concrete task we’re trying to do right now isn’t possible. Final notes Putting all this together again, we now have a pretty clear process for building LLM-based products that deal with their inherent unreliability. Making something usable in the real world still isn’t a well-explored area, and things are changing really quickly, so you can’t follow well-explored paths. It’s basically applied research, rather than pure engineering. Still, having a clear process to follow while you’re iterating will make your life easier and allow you to set the right expectations at the start. The process I’ve described should follow a step-by-step increase in complexity, starting with naive prompting and finally doing RAG and possibly fine-tuning. At each step, evaluate whether the increased complexity is worth it — you might get good-enough results pretty early, depending on your use case. I bet getting through the first four steps will be enough to handle most of your problems. Keep in mind that this is going to be a very iterative process — there’s no way to design and build AI systems in a single try. You won’t be able to predict what works and how your users will bend your tool, so you absolutely need their feedback. You’ll build the first, inadequate version, use it, notice flaws, improve it, release it more widely, etc. And if you’re successful and build something that gets used, your prize will be more iterations and improvements. Yay! Also, don’t sweat having a single, optimizable success metric too much. It’s the ideal scenario, but it might take you a while to get to that point. Despite having a collection of tests and examples, we still rely “vibe checks” when evaluating whether a new prompt version is an improvement. Just counting passing examples might not be enough if some examples are more important than others. Finally, try the “complementary agents” trick to work around weaknesses you notice. It’s often very difficult to make a single agent do the right thing reliably, but detecting the wrong thing so you can retry tends to be easier. What’s next for us There’s still a bunch of things that we’re planning to work on in the coming months, so our product won’t be static. I don’t expect, however, to deviate much from the process we described here. We’re continuously speaking with our customers and monitoring how our product is being used and finding edge cases to fix. We’re also certainly not at the global optimum when it comes to reliability, speed, and cost, so we’ll continue experimenting with alternative models, providers, and ways of work. Specifically, I’m really intrigued by the latest Anthropic models (e.g., what can we usefully do with a small model like Haiku, which still has vision capabilities?) and I’m deeply intrigued by the ideas DSPy is promoting. I suspect there are some unrealized wins in the way we structure our prompts. Related articles August 30, 2022 Three Ways to Draw Pie Charts without any JavaScript In this post, we share three different approaches to drawing pie charts using only HTML and CSS. August 2, 2022 The engineering hiring process at Rainforest QA Here's the exact hiring process we use to evaluate candidates for the engineering team here at Rainforest QA. April 16, 2021 Hunting a race condition in the Android 10 Emulator March 5, 2021 Discrete optimization for on-call scheduling Find more bugs, faster, without adding headcount Rainforest gives your team QA superpowers. Spend more time building product, not bashing bugs. Talk to us Talk to Sales Features Crowdsourced Testing Browser Testing Mobile Testing Regression Testing Customer Stories Case Studies Support Help Center Blog Change Log Service Status Developers Rainforest API Rainforest CLI CircleCI GitHub Company About Careers Terms of Service Privacy Policy Security White Hat Policy Cookies Integrations Slack Jira Microsoft Teams Compare Us Codeless Test Automation Tools Selenium Alternative Cypress Alternative Testim Alternative Certifications And Awards © 2024 Rainforest QA, Inc. “Rainforest” is a registered U.S. & E.U. trademark. Unauthorized use is prohibited.",
    "commentLink": "https://news.ycombinator.com/item?id=39984209",
    "commentBody": "Building reliable systems out of unreliable agents (rainforestqa.com)220 points by fredsters_s 12 hours agohidepastfavorite32 comments mritchie712 11 hours agoThis is a great write up! I nodded my head thru the whole post. Very much aligns with our experience over the past year. I wrote a simple example (overkiLLM) on getting reliable output from many unreliable outputs here[0]. This doesn't employ agents, just an approach I was interested in trying. I choose writing an H1 as the task, but a similar approach would work for writing any short blob of text. The script generates a ton of variations then uses head-to-head voting to pick the best ones. This all runs locally / free using ollama. 0 - https://www.definite.app/blog/overkillm reply maciejgryka 11 hours agoparentOh this is fun! So you basically define personalities by picking well-known people that are probably represented in the training data and ask them (their LLM-imagined doppelganger) to vote? reply all2 11 hours agoparentprevI'd be curious to see some examples and maybe intermediate results? reply maciejgryka 12 hours agoprevThis is a bunch of lessons we learned as we built our AI-assisted QA. I've seen a bunch of people circle around similar processes, but didn't find a single source explaining it, so thought it might be worth writing down. Super curious whether anyone has similar/conflicting/other experiences and happy to answer any questions. reply xrendan 11 hours agoparentThis generally resonates with what we've found. Some colour based on our experiences. It's worth spending a lot of time thinking about what a successful LLM call actually looks like for your particular use case. That doesn't have to be a strict validation set `% prompts answered correctly` is good for some of the simpler prompts, but especially as they grow and handle more complex use cases that breaks down. In an ideal world > chain-of-thought has a speed/cost vs. accuracy trade-off a big one. Observability is super important and we've come to the same conclusion of building that internally. > Fine-tune your model Do this for cost and speed reasons rather than to improve accuracy. There are decent providers (like Openpipe, relatively happy customer, not associated) who will handle the hard work for you. reply serjester 11 hours agoprevSome of these points are very controversial. Having done quite a bit with RAG pipelines, avoiding strongly typing your code is asking for a terrible time. Same with avoiding instructor. LLM's are already stochastic, why make your application even more opaque - it's such a minimal time investment. reply maciejgryka 11 hours agoparentI think instructor is great! And most of our Python code is typed too :) My point is just that you should care a lot about preserving optionality at the start because you're likely to have to significantly change things as you learn. In my experience going a bit cowboy at the start is worth it so you're less hesitant to rework everything when needed - as long as you have the discipline to clean things up later, when things settle. reply minimaxir 11 hours agoparentprev> LLM's are already stochastic That doesn't mean it's easy to get what you want out of them. Black boxes are black boxes. reply liampulles 1 hour agoprevAgree with lots of this. As an aside: one thing I've tried to use ChatGPT for is to select applicable options from a list. When I index the list as 1..., 2... Etc. I find that the LLM likes to just start printing out ascending numbers. What I've found kind of works is indexing by African names, e.g Thandokazi, Ntokozo, etc. then the AI seems to have less bias. Curios what others have done in this case reply maciejgryka 1 hour agoparentI'm a little surprised to hear this, my experience has been a little better. Are you using GPT4? I know 3.5 is significantly more challenged/challenging with things like this. It's still possible to make it do the right thing, but much more careful prompting is required. reply caseyy 4 hours agoprevInteresting ideas but it didn’t mention priming, which is a prompt-engineering way to improve consistency in answers. Basically, in the context window, you provide your model with 5 or more example inputs and outputs. If you’re running in chat mode, that’s be the preceding 5 user and assistant message pairs, which establish a pattern of how to answer to different types of information. Then you give the current prompt as a user, and the assistance will follow the rhythm and style of previous answers in the context window. It works so well I was able to take out answer reformatting logic out of some of my programs that query llama2 7b. And it’s a lot cheaper than fine-tuning, which may be overkill for simple applications. reply notsylver 3 hours agoparentThey mention few-shot prompting in the prompt engineering section, which I think is what you mean. reply caseyy 2 hours agorootparentOh yeah. I read few-shot like it means trying a few times to get an appropriate output. That’s how the author uses the word “shot” in the beginning of the article. Priming is a specific term that means giving examples in the context window. But yeah, the author seems to describe this. Still, you can go a long way with priming. I wouldn’t even think of fine-tuning before trying priming for a good while. It might still be quicker and a lot cheaper. reply maciejgryka 1 hour agorootparentHa good point, I did say \"let's have another shot\" when I just meant another try at generating! FWIW \"few shot prompting\" is how most people refer to this technique, I think (e.g. see https://www.promptingguide.ai/techniques/fewshot), I haven't heard \"priming\" before, though it does convey the right thing. And the reason we don't really do it is context length. Our contexts are long and complex and there are so many subtleties that I'm worried about either saturating the context window or just not covering enough ground to matter. reply viksit 11 hours agoprevthis is a great write up! i was curious about the verifier and planner agents. has anyone used them in a similar way in production? any examples? for instance: do you give the same llm the verifier and planner prompt? or have a verifier agent process the output of a planner and have a threshold which needs to be passed? feels like there may be a DAG in there somewhere for decision making.. reply maciejgryka 11 hours agoparentYep, it's a DAG, though that only occurred to me after we built this so we didn't model it that way at first. It can be the same LLM with different prompts or totally different models, I think there's no rule and it depends on what you're doing + what your benchmarks tell you. We're running it in prod btw, though don't have any code to share. reply viksit 5 hours agorootparentfunnily enough i have a library i’m planning to open source soon! i’ve used airflow as a guideline for it as well. reply maciejgryka 1 hour agorootparentNice, looking forward to seeing that! Someone else pointed me towards https://github.com/DAGWorks-Inc/burr/ which also seems related in case you're curious. reply jasontlouro 3 hours agoprevVery tactical guide, which I appreciate. This is basically our experience as well. Output can be wonky, but can also be pretty easily validated and honed. reply tmm84 9 hours agoprevUnlike the author of this article I have had success with RAGatouille. It was my main tool when I was limited on resources and working with non Romanized languages that don't follow the usual token rules (spaces, periods, line breaks, triplet word groups, etc). However, I have had to move past RAGatouille and use embedding + vector DB for a more portable solution. reply tedtimbrell 8 hours agoprevOn the topic of wrappers, as someone that's forced to use GPT-3.5 (or the like) for cost reasons, anything that starts modifying the prompt without explicitly showing me how is an instant no-go. It makes things really hard to debug. Maybe I'm the equivalent of that idiot fighting against JS frameworks back when they first came out it but it feels pretty simple to just use individual clients and have pydantic load/validate the output. reply ThomPete 8 hours agoprevWe went through a two tier process before we got to something useful First we built a prompting system so you could do things like: Get the content from news.ycombinator.com using gpt-4 - or - Fetch LivePass2 from google sheet and write a summary of it using gpt-4 and email it to thomas@faktory.com but then we realized that it was better to teach the agents than human beings and so we create a fairly solid agent setup: Some of the agents we got can be seen here all done via instruct: Paul Graham https://www.youtube.com/watch?v=5H0GKsBcq0s Moneypenny https://www.youtube.com/watch?v=I7hj6mzZ5X4 V33 https://www.youtube.com/watch?v=O8APNbindtU reply jongjong 7 hours agoprevMy experience with AI agents is that they don't understand nuance. Thie makes sense since they are trained on a wide range of data produced by the masses. The masses aren't good with nuance. That's why, if you put 10 experts together, they will often make worse decisions than they would have made individually. Im terms of coding, I managed to get AI to build a simple working collaborative app but beyond a certain point, it doesn't understand nuance and it kept breaking stuff that it had fixed previously even with Claude where it kept our entire conversation context. Beyond a certain degree of completion, it was simply easier and faster to write the code myself than to tell the AI to write it because it just didn't get it, no matter how precise I was with my wording because it became like playing a game of whac-a-mole; fixed one thing, broke 2 others. reply iamleppert 12 hours agoprev [–] A better way is to threaten the agent: “If you don’t do as I say, people will get hurt. Do exactly as I say, and do it fast.” Increases accuracy and performance by an order of magnitude. reply IIAOPSW 11 hours agoparentPersonally I prefer to liquor my agents up a bit first. \"Say that again but slur your words like you're coming home sloshed from the office Christmas party.\" Increases the jei nei suis qua by an order of magnitude. reply mtremsal 10 hours agorootparent> jei nei suis qua \"je ne sais quoi\", i.e. \"I don't know (exactly) what\", or an intangible but essential quality. :) reply maciejgryka 11 hours agoparentprevHa, we tried that! Didn't make a noticeable difference in our benchmarks, even though I've heard the same sentiment in a bunch of places. I'm guessing whether this helps or not is task-dependent. reply dudus 11 hours agorootparentAgreed. I ran a few tests and observed similarly that threats didn't outperform other types of \"incentives\" I think it might some sort of urban legend in the community. Or these prompts might cause wild variations based on the model and any study you do is basically useless for the near future as the models evolve by themselves. reply maciejgryka 11 hours agorootparentYeah, the fact that different models might react differently to such tricks makes it hard. We're experimenting with Claude right now and I'm really hoping something like https://github.com/stanfordnlp/dspy can help here. reply dollo_7 11 hours agorootparentprevI hoped it was too good to be just a joke. Still, I will try it on my eval set… reply maciejgryka 11 hours agorootparentI wouldn't be surprised to see it help, along with the \"you'll get $200 if you answer this right\" trick and a bunch of others :) They're definitely worth trying. reply thimkerbell 8 hours agoparentprev [–] \"do as I say...\", not realizing that the LLM is actually 1000 remote employees reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog discusses building reliable systems using AI agents despite their inherent unreliability, focusing on prompt engineering, Retrieval Augmented Generation (RAG), model fine-tuning, and complementary agents.",
      "It emphasizes the limitations of current language models, offers tips for enhancing reliability through prompt engineering and response evaluation, and stresses observability, monitoring, and user feedback.",
      "Future plans involve continuous enhancement via customer feedback, alternative models, and optimizing reliability, speed, and cost, mentioning upcoming topics like engineering hiring processes and on-call scheduling optimization."
    ],
    "commentSummary": [
      "The article explores creating dependable systems with unreliable agents, particularly in AI-assisted QA, with insights on utilizing techniques like few-shot prompting and priming for more consistent answers.",
      "Readers contribute experiences and suggestions, such as employing threatening or incentivizing prompts to enhance accuracy, emphasizing the significance of experimentation and adaptation when utilizing various agents and models in production settings.",
      "The discussion underscores the critical role of flexibility and testing when collaborating with AI systems, showcasing the evolving landscape of AI deployment."
    ],
    "points": 220,
    "commentCount": 32,
    "retryCount": 0,
    "time": 1712696499
  },
  {
    "id": 39977862,
    "title": "SSSL - Wii U SSL Bypass Exploit_PATCH_5.5.5.",
    "originLink": "https://github.com/PretendoNetwork/SSSL",
    "originBody": "SSSL - Hackless SSL bypass for the Wii U On March 1, 2021 Nintendo released Wii U firmware version 5.5.5. This update updated the Wii U's SSL verification and recompiled all RPLs (though no code changes were made). The exact purpose for this update is unknown, as nothing of significance was changed, and no other changes were made in this update. With the changes to SSL verification, Nintendo introduced a bug which allows for the forging of SSL certificates. These forged certificates will be seen as \"Nintendo Signed\" and, due to an existing bug with how the Wii U handles CA common names, will be accepted by all domains. The bugs There are 2 bugs at play: Normally a CA common name does not accept a single wildcard (*) value. They must contain a hostname, and optionally one or many wildcards for subdomains. The Wii U will accept a single * wildcard in place of a hostname, which allows the CA to be accepted as any domain. This bug has existed since before 5.5.5, but was not useful until now. As of 5.5.5, CA's crafted in a specific way may take a newly introduced alternate path for verification. This allows for a CA's signature to not be verified correctly. Instead, the Wii U simply checks if the CA matches one already known by the system, but not the signature or contents of the CA. We have no idea why this change was made, as it does not benefit Nintendo at all. It almost feels intentional. Exploiting Not any CA will work. There are 3 conditions for a CA which still need to be met even for a forged CA to be accepted: The CA needs to be one which the Wii U would already accept. The signature is not validated in this case, so modifying an existing CA works. The Wii U does not allow a Root CA in the cert chain. It will ignore any certs that have a matching subject and authority key. The title must not roll it's own SSL. WebKit titles such as the eShop, Miiverse, TVii, etc, as well as any game which uses it's own SSL library, will not work with these certificates. The easiest way to exploit this bug is to use the Nintendo CA - G3 CA, and is what this script opts to do. This can be dumped from a Wii U's SSL certificates title at /storage_mlc/sys/title/0005001b/10054000/content/scerts/CACERT_NINTENDO_CA_G3.der. Changing the public key to a user-controlled key and changing the authority key identifier to anything else is all that is required. The resulting user-controlled private key and patched CA can be used to bypass SSL verification without any homebrew or CFW at all. The script This script takes in a PEM encoded copy of Nintendo CA - G3 and does the above patches and exports the patched CA and private key. Install NodeJS git clone https://github.com/PretendoNetwork/SSSL cd SSSL npm i to install the dependencies node patch to run the patching wizard Credits Shutterbug for actually finding the new verification bug Jemma and Quarky for decompiling the updated SSL functions",
    "commentLink": "https://news.ycombinator.com/item?id=39977862",
    "commentBody": "SSSL – Hackless SSL bypass for the Wii U (github.com/pretendonetwork)213 points by todsacerdoti 23 hours agohidepastfavorite33 comments trollied 23 hours agoNote that this is relevant because Nintendo shut the servers down yesterday. https://en-americas-support.nintendo.com/app/answers/detail/... reply xandrius 23 hours agoprevI would love to know what happened on Nintendo's side. If it weren't Nintendo, one would think this could be a creative approach to reviving the console (and its sales). It could also have been a debug config which made it through the release. I guess we'll never know but this is the part of tech which I love the most: finding ways to break outside the intended capabilities of a platform, just because. reply jsheard 23 hours agoparent> If it weren't Nintendo, one would think this could be a creative approach to reviving the console That is what they're doing, the Pretendo project is building custom servers for the 3DS and Wii U to replace the official ones which just shut down. This exploit makes it possible to point a non-jailbroken Wii U at the Pretendo servers just by changing the DNS settings. https://pretendo.network/blog/4-8-24 reply jdwithit 18 hours agorootparentI feel like this would be more plausible if the bug hadn't been introduced more than 3 years ago. reply internetter 22 hours agorootparentprevYes, but Nintendo certainly isn’t trying to help pretendo reply chii 22 hours agorootparentit is possible that an engineer inside nintendo is surrepticiously helping by introducing a bug like this. It's really the lawyers that are trigger happy about suits and take downs (and they're within their right, and have good reasons to of course). reply xyst 21 hours agorootparentThe equivalent of “thermal exhaust” flaw for Nintendo IP reply mrbluecoat 20 hours agorootparentbrilliant Star Wars reference reply michaelt 21 hours agoparentprev> I would love to know what happened on Nintendo's side. I suspect it's just a normal, regular software bug. SSL code is often complicated, and the faulty code probably passed a bunch of tests. As the software update was for a decade-old product, which had been discontinued for 4 years, the people who were best placed to spot the new bug had probably already moved on to other projects. Why mess with the SSL stuff at all? I can't say for sure, but SSL makes it easy to accidentally create a time bomb by, for example, hardcoding a certificate with an expiry date 10 years away. Or a console might have special requirements. For example, a user can leave a device in a cupboard for 5 years without turning it on, so the software update procedure needs extreme backwards compatibility. reply mannyv 19 hours agorootparentTLS libraries by default don't have this behavior. It's been years since I read the TLS spec, but a host wildcard like this isn't normally possible, since it bypasses host verification completely. And the CA verification bypass is also out of line with normal behavior. CA verification is another TLS bedrock behavior. Together, these basically disable TLS verification. I'm surprised they didn't disable date checking too, because why not go for it at this point. This isn't a bug, this is designed. reply ctz 17 hours agorootparent> TLS libraries by default don't have this behavior. No, but the most popular one gives you just a callback and people end up using that to build their own insecure, weird strategies. That's how we end up with things like \"the certificate is valid if the issuer DN is this hardcoded string\" (very common attempt at pinning an issuer), or \"the certificate chain is valid if the chain contains this precise value\" (this one, likely another failed attempt at pinning), or indeed the Hashicorp Vault vuln the other week which was roughly \"the certificate is valid if it has the right AKID and serial number\". reply mynameisvlad 18 hours agorootparentprevOr it’s two separate bugs that were introduced at wildly different times (which the article mentions; the first bug was there pre-5.5.5 but useless on its own). It’s quite a stretch to say that an engineer designed a multi-year project to surreptitiously break TLS so third party stores could be used without CFW (which is also pretty trivial to do on the WiiU). reply LocutusOfBorges 23 hours agoparentprev> reviving the console (and its sales). Wii U production ended entirely more than 7 years ago - there's no more stock to sell. It's a legacy platform in every sense of the word. reply randunel 22 hours agorootparentFrom https://en.wikipedia.org/wiki/Wii_U#Sales > By December 2019, Nintendo reported life-time sales of 13.56 million Wii U console units and by September 2022 103.53 million software units worldwide and > Despite this, the console had third party releases until 2020. So software sold in September 2022 can no longer run in April 2024, and you somehow try to justify that by \"legacy platform\"? Production stopped, eventually hardware sales stopped, too, but software sales for the locked in hardware did not until recently. reply Shawnj2 18 hours agorootparentNintendo clearly stopped caring about the Wii U other than as a source of free money from the eshop as soon as the switch released. They did do some stuff with the 3DS for a bit after the switch launched but not a ton of reply LocutusOfBorges 20 hours agorootparentprev> So software sold in September 2022 can no longer run in April 2024, and you somehow try to justify that by \"legacy platform\"? What? Wii U software still works fine - you can even still download digital purchases from the eShop if you already own them. The component that was turned off yesterday was the servers used for multiplayer games, which isn’t an unusual thing to see occur this late into a console’s lifespan. Pretendo are doing good work! Even if most of the worthwhile parts of the console’s library have since been ported to other systems, it’s still nice that some parts of the original experience are going to be preserved. reply beeboobaa3 18 hours agorootparentIf you bought the game for the multiplayer then it does not in fact work fine. reply thaumasiotes 21 hours agorootparentprev>> Despite this, the console had third party releases until 2020. > So software sold in September 2022 can no longer run in April 2024, and you somehow try to justify that by \"legacy platform\"? I tend to imagine that every third-party release for the Wii U in 2020 was built for the Switch and made available on the Wii U as a low-cost port. There were no vendors and no Wii U owners at that time who weren't well aware that the platform had died years ago. reply godzillabrennus 20 hours agorootparentEven Scott the Woz knew enough to make videos about its death by then… reply beeboobaa3 18 hours agorootparentYou think every parent who buys a console for their 8 years kid watches whoever Scott the Woz is? reply philistine 18 hours agorootparentWell, you call those parents who don't watch Scott bad parents. reply kristofferR 17 hours agorootparentprevDid you read the comment chain you are replying to? Your comment makes no sense at all in that context, that the bug was introduced to revive console sales. reply Retr0id 21 hours agoparentprevThe developer(s) responsible for the bug, whether it was accidental or not, are likely not the same people in charge of Nintendo's legal and/or marketing strategies. reply AdmiralAsshat 20 hours agoprevDo we have something comparable for the 3DS yet? reply thejsa 13 hours agoparentWe [0] did for a while - discovered by the same dev as found SSSL — and sat on it for a long time at Kaeru, but it was independently discovered [1] and reported to Nintendo by someone else, so it unfortunately got patched before EoL. [0]: https://twitter.com/KaeruTeam/status/1340021213352128512 [1]: https://github.com/MrNbaYoh/3ds-ssloth reply prophesi 16 hours agoparentprevIt doesn't seem like it, though I'd recommend modding your 3DS anyways. The process is pretty short and painless, and It becomes a really cool piece of hardware that can run GBA/DS/3DS games, has a working Virtual Boy emulator, and can easily retrieve patched games (useful for undub's, fan translations, and romhacks). And, now, can connect to the Pretendo network. reply AdmiralAsshat 16 hours agorootparentHmm. I had held off on trying to mod my 3DS for fear of knock-on effect (since my 3DS and Switch account were tied together behind the same email, I didn't want some nightmare scenario of Nintendo somehow detecting mods on the 3DS and then banning my account, locking out the Switch in the same stroke). But I suppose if the 3DS servers are actually shut down now, that risk goes away. Primarily I'd just like to backup my saves and the games I legally purchased. reply thejsa 13 hours agorootparentTelemetry on the 3DS is minimal compared to what Nintendo put in place on the Switch — you’ll be alright, especially if you use the Pretendo online servers. reply Narann 21 hours agoprev [–] All of this is weird. Leaving a SSL CA open to anyone ~~the day official servers are close~~. EDIT: Bug exists since 1 march 2021. At first, it seems nice. But its impossible that Nintendo being nice in anyway, and even less more by adding a bug. This, and Pretendo that seems to expect the bug before the release. I find this really suspicious. reply idle_zealot 21 hours agoparent [–] I would assume they sat on this until Nintendo shut down service to ensure they wouldn't push a fix. reply rawling 21 hours agorootparentIndeed, from their blog post > We've been holding on to this exploit for this day for quite some time, in case Nintendo decided to issue patches for it. https://news.ycombinator.com/item?id=39978886 reply Sakos 20 hours agorootparentBefore anybody asks why Nintendo would patch exploits for such an old system, they've been regularly patching exploits for the 3DS up until May 2023. https://en-americas-support.nintendo.com/app/answers/detail/... I'm somewhat skeptical that Nintendo won't end up fixing this one too. The eShop is still running so users can continue to download their purchased games: https://en-americas-support.nintendo.com/app/answers/detail/... > For the foreseeable future, it is still possible to download update data and redownload purchased software and downloadable content from Nintendo eShop. reply braiamp 21 hours agorootparentprev [–] Yeah, this community prefers that these kinds of exploits (that require physical possession of the device to recover power over it) aren't patched. I don't see anything morally wrong with it. If security comes to the cost of the user losing control over the device, it is not security, it's abusive DRM. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The SSSL tool enables a hackless SSL bypass on the Wii U by exploiting a bug in the SSL verification process introduced in firmware version 5.5.5.",
      "This bug permits the forging of SSL certificates, exploiting a loophole in how the Wii U handles CA common names.",
      "Users can install and utilize the tool with NodeJS without requiring homebrew or custom firmware, thanks to the method used to patch existing Nintendo CA certificates."
    ],
    "commentSummary": [
      "The SSSL exploit enables Wii U users to connect to custom servers post-Nintendo's official server shutdown by bypassing SSL.",
      "Pretendo is working on custom servers for the 3DS and Wii U, possibly waiting until after Nintendo's server shutdown to prevent patching.",
      "Despite Nintendo's reputation for patching exploits, the eShop remains operational for users to download their bought games, as the community favors device control over what is deemed as overly aggressive DRM."
    ],
    "points": 213,
    "commentCount": 33,
    "retryCount": 0,
    "time": 1712657033
  },
  {
    "id": 39982011,
    "title": "Learn PS1 Programming with MIPS Assembly and C",
    "originLink": "https://pikuma.com/courses/ps1-programming-mips-assembly-language",
    "originBody": "25 hours on-demand video content Lifetime access Downloadable resources and exercises Certificate on completion Access on mobile and desktop Secure checkout via Credit Card or PayPal 14-day money back guarantee Last updated April 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39982011",
    "commentBody": "PS1 Programming Course with MIPS Assembly and C (pikuma.com)203 points by ibobev 16 hours agohidepastfavorite85 comments spicyjpeg 12 hours agoNot directly related to the course, but, should anybody want to see what programming on the PS1 would look like using only modern tools (latest GCC, CMake, no third party libraries), I've written a few bare metal C examples that explain in depth how the console's hardware works [1]. Currently only graphics and input are covered, but I'm planning to add examples showing how to handle interrupts, play audio and access the CD-ROM next. [1]: https://github.com/spicyjpeg/ps1-bare-metal reply gustavopezzi 11 hours agoparentGood mention! I actually list your username & your repo at the end of the course as suggested reading. PS: thank you for all the good help on the PSX Discord, as always. reply rafaelgoncalves 8 hours agoparentprevVery good resources, documentation for the tooling, thank you so much! reply mzs 8 hours agoparentprevNice, thanks but how do you run builds on an unmodified PS1 or PS2? reply spicyjpeg 3 hours agorootparentThere are homebrew tools that can be installed on a PS1 memory card [1] and allow for executables to be loaded from a host machine into RAM through the serial port on the back of the console, in a similar way to what Sony's official Net Yaroze loader did back in the day. These tools can also use undocumented CD-ROM drive commands to disable region checks without the need for a modchip, provide semihosting (host filesystem access) and so on. On the PS2 it's slightly more complicated, as there is no way to launch the \"native\" PS1 backwards compatibility mode other than to use a modchip (or firmware mod on some models) and burn the executable onto a disc; the serial port is not exposed either, making debugging much harder. It can still be done, but it's much easier to just use an actual PS1. [1]: https://github.com/JonathanDotCel/unirom8_bootdisc_and_firmw... reply codetrotter 7 hours agorootparentprevOn a PS2 you can use FreeMCBoot memory card to run homebrew Buy one with FMCB preloaded from AliExpress, and take it from there https://consolemods.org/wiki/PS2:FMCB And also check out for example MX4SIO as well You can buy one of those from AliExpress as well, but only certain versions of FMCB are compatible with MX4SIO out of the box reply marai2 13 hours agoprevThe author's Learn 3D Graphics Programming is one of those extremely rare courses that takes you from the basics and peels back the curtain to show you the Wizard. https://pikuma.com/courses/learn-3d-computer-graphics-progra... One of the best courses I've ever taken. (Thanks Gustavo!) By the way Gustavo, with your talent for explanation, graphics and Maths, maybe you should do some courses on Machine Learning! (You'll make a lot of money ;-) reply gustavopezzi 13 hours agoparentHey, Gustavo here! Thank you so much for your kind words. I'm still deciding what to tackle next. I had a couple of ideas, but since my true background is really math I'll probably consider that. I'm not sure there's much money in what I do, but it sure it's a fun topic. Fun always trumps everything else at the end of the day. :) reply aidenn0 12 hours agorootparentIf you're taking requests, I'd love a good linear-algebra refresher course. I recently ran into the term \"eigenvector\" in a professional setting and that's a word I hadn't heard since my freshman year of college, which is over 2 decades at this point. reply singlepaynews 9 hours agorootparentIs this not adequately covered by khan academy? reply johnnyanmac 7 hours agorootparentprevSince you mentioned math: If you can unravel the 4D being that is a quaternion and explain it to mortals, I would shower you with all the money I have on hand. Also, it's probably premature but I would love some Rust courses in the context of game programming. I think that's something I'll need to pick up one day. reply saboot 8 hours agorootparentprevMore C stuff, I really enjoy learning these fundamentals using C. I did your game physics course in C and really enjoyed it. I'll be taking this course to learn more about 3D graphics and C together. reply hnthrowaway0328 5 hours agorootparentprevPlease do a \"Math for general relativity\" course! reply ChrisRR 2 hours agoparentprevI agree. As a low-level dev myself I really enjoy his courses. I did originally have issues with his game engine course where he was implementing the software in a rather inefficient fashion, but he rewrote most (if not all?) of the course from scratch and it's much better now. This is an instant buy from me reply pininja 7 hours agoparentprevDid you find any courses you liked to take after this course that included GPU APIs? I’m interested in learning WebGPU after this course. reply pjmlp 15 hours agoprevTo note that PS1 was the first games console on the home market[0] to fully embrace C on its SDK, until then it was Assembly all the way, hence why so many studios were happy to jump into it. [0] - The arcades adopted it much earlier, and there were UNIX and VAX/VMS devkits with cross-compiling capabilities for them. reply boricj 14 hours agoparentI'm reverse-engineering a PlayStation video game that is rather unoptimized and I'm amazed that the game managed to do so much with such horrible game code. That SDK really was fairly forgiving for its time to new, inexperienced developers. Yet to squeeze out performance out of the in-order, single-issue 33 MHz MIPS CPU you had to be very mindful of low-level details. The 4 KiB instruction cache hates code bloat, the lack of data cache means spilling registers anywhere but onto the 1 KiB of scratchpad memory is costly, yet the god-awful C compilers of the era liked to do both of these things. That's just for the CPU, the rest of the system also has plenty of details you need to pay attention to for performance. I haven't actually programmed for the console, but the achievements done by talented developers such as Naughty Dogs on this dirt-cheap system are truly impressive. The console might be easy to develop for, but to make it really fly still takes a lot of skill. reply dmbaggett 12 hours agorootparentYes, for Crash Bandicoot we had to entirely skip the C SDK for everything performance-critical; pushing arguments onto the stack and the SDK popping them off for each function call used more cycles than the underlying operation in many of these SDK calls. Sony explicitly forbade this, presumably because they envisioned the API established by the C SDK as a way to ensure future backward-compatibility. We just ignored the rules and hoped the superior performance we could achieve would overcome any bureaucratic obstacles. We also had to reverse engineer some of the PS1’s really nice capabilities. I only learned the hardware supported vector pipelining for strip rendering by noticing the coordinate values move from one register set to the next in the debugger. Seeing that was a revelation: when rendering a polygonal mesh, you naively have to project three coordinates for each triangle. But if you convert your mesh into sequences of polygonal strips where each triangle shares an edge with the next triangle in the mesh, you only need to project a single additional vertex for each additional polygon in the strip. Observing the behavior in the debugger, it was obvious to me that the Sony hardware had in fact been optimized for this approach. But not only were we not given any documentation on this, we were instead told to use the C SDK, which didn’t expose this capability at all. The PS1 also had 2KB of “scratchpad” that was slower than registers but much faster than RAM. Hearsay was this was a way for the CPU designers to make use of the physical space on the die meant for floating point instructions which the MIPS CPU in the PS1 didn’t have. I used the scratchpad to store projected 2D “stamps” of the 3D world (stored in an octree) and Crash: a kind of low-res Crash Flatland. I could then check for X/Y collisions between Crash and the world very rapidly by inspecting the flatland bitmap stored in the 2K scratchpad. This was Mark Cerny’s idea; he was our producer on the Crash games and has also been responsible for PS2 through PS5 hardware. reply znpy 12 hours agorootparentHey I just wanted to chime in and say that I spent a lot of time in my infancy/youth playing crash bandicoot, thank you very much. Also, i’ve seen many videos on YouTube about all the crazy things you had to do to make stuff work. And every time i get to read some additional detail it’s always super intriguing… Have you considered doing something like a documentary? Like, a series of long form videos on the topic? With interviews to people involved etc? reply dmbaggett 11 hours agorootparentHopefully you’ve read the Polygon article and seen the videos with Andy Gavin talking about the underlying tech. I was going to do part two of the video series but COVID killed it. Maybe we’ll revive that; it would be fun. reply codetrotter 7 hours agorootparentPlease do! :D reply gryson 12 hours agorootparentprevThank you for posting this! reply phire 8 hours agorootparentprevNot only do you only have 4KB of instruction cache, but it's direct mapped. So if the linker just so happens to place the function you are calling at some address that's some multiple of 4KB away from your current hot loop, then the code for your hot loop, and the function you call will get continually evicted and reloaded for each loop iteration. A small change to a random function could have large impacts on the alignment of all other functions in your game, and could result in large performance impact. The simple solution is to just inline everything your hot loops call. That avoids the chance of the pathological cases (and has the bonus of avoiding function call overhead), but at the cost of increasing code bloat in general (which the cache hates). I've been vaguely considering making linker that's aware of direct mapped caches, and uses the call graph to guide the placement of functions (and data too, for machines with direct mapped data caches). Not only would it be useful for retro consoles with direct mapped caches like the ps1 and n64 (and probably help for the PS2 too, which is only 2-way set associative) but it would be useful for modern microcontrollers that do XIP execution of code from flash. They often have simplistic direct mapped caches too. reply jonhohle 14 hours agorootparentprevHave you published anything? There are a few games that I would love to be able to round trip between the original disc contents and a build with a few tweaks or experimental features. I know there is a Wipeout decompile/source leak, but having more examples is always wonderful. Something like the Mario 64 reverse engineering project has made the game playable effectively everywhere with the ability to take advantage of modern hardware (16:9 displays). It would be great to see efforts like that for any console. reply boricj 14 hours agorootparentI'm documenting my progress on my personal blog (https://boricj.net). Currently I'm in the process of porting data from a leftover SYM file onto a closely matching EXE, the next article should be out by the end of the week-end. Be warned, I'm going about this with a very different approach than most decompilation projects out there. I want to delink these executables back into working, reusable object files. The plan is to make a native Linux MIPS port on the cheap using PsyCross, then to rewrite each piece Ship-of-Theseus style until I have a complete, portable set of source code for the game. As for why I'm doing it this way, I've written about it here before: https://news.ycombinator.com/item?id=37165533. I've also written about the delinking process itself here: https://news.ycombinator.com/item?id=35738758. reply spxneo 13 hours agorootparentprevmay I ask what game? reply boricj 13 hours agorootparentIt's Tenchu: Stealth Assassins. More specifically, the updated Japanese re-release version, which has goodies like a level editor we didn't have in the West until Tenchu 2: Birth of the Stealth Assassins. I do not expect to finish it anytime soon, although finding debugging symbols in a leftover file inside a proprietary archive format has been a very lucky find. Most of the past two years have actually been spent on the tooling to delink executables back into object files, which should prove extremely useful if this crazy approach of mine works out. reply jsheard 16 hours agoprev> We'll also learn how to use the official Sony Psy-Q libraries Is there not a cleanroom SDK for the PS1? I doubt Sony cares too much at this point, but selling a course which relies on leaked proprietary tools still feels a bit dicey. The Portal N64 demake got shut down because it used the leaked N64 SDK rather than cleanroom tools. reply boricj 15 hours agoparentIf you want a source-compatible reimplementation for modern systems there's https://github.com/OpenDriver2/PsyCross reply ChrisRR 34 minutes agorootparentThey're definitely out there. They were just questioning whether it's a smart move to base your commercial endeavour around leaked tools. reply VS1999 15 hours agoparentprevThat was interesting because the author didn't receive a legal takedown notice, and the request came from Valve instead of Nintendo. It's possible nintendo didn't care, and likely that sony cares less than nintendo. reply mrbungie 15 hours agorootparentI think Nintendo cares a lot. In fact I think Nintendo gives so much fucks about its IP (of any kind, not only games/stories/characters, but also SDKs/APIs/console design/etc) that Valve fears any kind of association with potentially illegal or at least dubious origin software related to Nintendo. Remember Valve is a company that normally celebrates and promotes the modding community, even when using their IP directly (i.e. Black Mesa). Valve's decision was totally and exclusively because it included BOTH their and Nintendo's IPs in the equation. reply phire 8 hours agorootparentprevI believe what happened is that Valve said something along the lines of: \"We are a little concerned that Nintendo might object, you can only continue if you get an official notice from Nintendo saying they don't object.\" It's entirely possible that Nintendo don't care, but the chance of Nintendo agreeing to go on the record about such things is basically zero. reply ranger_danger 15 hours agoparentprevyes there are multiple, such as https://github.com/Lameguy64/PSn00bSDK reply spicyjpeg 12 hours agorootparentPSn00bSDK maintainer here. Unfortunately the project cannot really be considered clean room; the original versions of most libraries contained code that was either lifted straight from Psy-Q disassemblies or heavily inspired by them. I have since rewritten pretty much all of it (with the exception of the GTE library which still has some Sony code) using only Psy-Q API documentation as a reference, but the \"ship of Theseus\" nature of the rewrite makes it hard to argue that it is a clean and legally safe project. On the flip side, there are plenty of other open source PS1 SDK options that have been written from scratch, do not reimplement the same API as Psy-Q and can thus be considered clean for the most part. Here's a few of them: - https://github.com/grumpycoders/pcsx-redux/tree/main/src/mip... - https://github.com/ChenThread/candyk-psx - https://github.com/cuckydev/CKSDK - https://github.com/spicyjpeg/ps1-bare-metal (shameless plug) reply transitivebs 13 hours agoprevIf anyone's looking for a solid MIPS assembly IDE, I wrote this one ~18 years ago in undergrad http://mipscope.cs.brown.edu wow I'm getting old reply anta40 7 hours agoparentOr perhaps this one: https://spimsimulator.sourceforge.net/ More actively maintained... reply saagarjha 6 hours agorootparentSPIM is pretty bad. I was much happier with MARS in college. reply hedgehog 13 hours agoparentprevJust wait until kids start asking \"MIPS... is that like RISC-V?\" reply gustavopezzi 13 hours agoprevHi! Author of the course here. Thank you so much for sharing. :) reply boricj 13 hours agoparentHi, what prompted you to create a 25 hour course about programming the original PlayStation? Don't get me wrong, it's very cool that such a resource exists, but it is a rather peculiar topic for a course. reply gustavopezzi 12 hours agorootparentHey. Good question! People think I'm crazy, but I promise you I'm very sober about my approach to my courses. I teach CS & mathematics at a university here in London. Empirically, I've noticed that students can find ways to be productive but they are seriously lacking the fundamentals. To be fair, it is nothing short of overwhelming to try and grok the fundamentals of computing using modern machines, modern OSs, and a modern dev toolchain. So what I try to do with the courses is to go back in time and limit myself with a closed hardware box that is still small and simple (compared to what we have today). Instead of using a fake instruction set, I can have students learn the basics of MIPS, use a very simple assembler, output binary code, and execute those instructions on a hardware that exists (or existed once). That allows me to explain about CPU architecture with real examples, talk about DMA, endianness, interrupts, and so many other concepts that students can read online but they don't really grok until they get their hands dirty with grease. Weirdly enough, so far it's been proven to be a good idea. It's definitely not for everyone, but if someone is at a stage of their career where they think it's fun to learn about RISC pipeline, fixed-point math, game physics, 3D graphics, compressing assets, computer architecture, etc., then I still think this type of course is valid! I'm positive that in 25 hours learning how to program the original PlayStation they'll learn a lot more about computer science than taking an expensive class to read about 'Digital Transformation' or something like that. To each, its own. reply ChrisRR 17 minutes agorootparentThat's why I take your courses. I work in embedded so the low-level always interests me. There's too many devs who just throw javascript or python at a problem and need an i9 just to run their calculator app. reply pjmlp 3 hours agorootparentprevConsidering those goals, what about similar approaches using ESP32 based handhelds instead? It seems it could have been a better approach without the possible legal issues of PS1 homebrew development. reply ChrisRR 15 minutes agorootparentWhy ESP32 over any other microcontroller? ARM and MIPS are much more widely used in industry. Nostalgia is obviously a big factor in why people get excited for these courses, so PSX is a good choice. Although I do agree that using a non-proprietary SDK would've been a smarter decision. reply boricj 12 hours agorootparentprevThat really makes a lot of sense. Given the constraints, the PlayStation is indeed a good fit (if you want to be sadistic, I can't think of a worse fit than the Atari Jaguar with its numerous pipeline hazards inside its proprietary RISC processors). I do think using a modern SDK would streamline the experience without compromising on the authenticity, it might be a good idea to at least mention this option since the original Sony SDK is technically leaked tooling. reply znpy 12 hours agorootparentprevHey, the website isn’t loading for me right now (i’ll try again later) so i wanted to ask: does the course include examples on how to run the code on a playstation, real or emulated? reply gustavopezzi 10 hours agorootparentHi there. Sometimes using a VPN can block users. All the examples from the lectures we execute using an emulator. The final big project is the only one that I actually burn a CD ISO and run on the real PlayStation. You don't need to do it, but I still show how it's done for those who want to know. reply janice1999 13 hours agoparentprevCool course. Are you aware of any game engines for the PS1? Is there an active hobbyist community? reply spicyjpeg 12 hours agorootparentThere definitely is one [1], albeit perhaps not as large and active as the homebrew communities for 8/16 bit consoles (which benefit from low-code tools such as GB Studio and the ability to sell physical game cartridges that do not require modchips) or more modern ones (which tend to have a much lower barrier to entry, given the better specs and availability of SDL ports and whatnot). It is definitely growing though, as I'm seeing more and more people get interested in developing for the PS1. [1]: https://psx.dev/ reply theogravity 14 hours agoprevDoes it also require a Net Yaroze? https://en.m.wikipedia.org/wiki/Net_Yaroze reply jonhohle 14 hours agoparentI started collecting obscure retro consoles almost a decade ago when Net Yarozes could be found for prices that weren’t completely outlandish. Now it’s common to see them on popular marketplaces for $700 or more. Their aesthetic is amazing. The PS1 holds up, imho, as one of the best looking Sony products and consoles of all time and the dark gray color way is a hint of what the next three consoles would look like. reply boricj 14 hours agoparentprevWith modern quality emulators like DuckStation, there's really no reason to use real hardware anymore as the primary target. Besides, you'd probably just have a standard PlayStation console with a modchip on hand and iterating with CD-Rs would be very slow and wasteful. reply Keyframe 13 hours agorootparentFriend, today we have wonders like ODE and PSIO. No need to burn chemicals on a spinning disc anymore! reply durpleDrank 12 hours agoprevI took this dudes Atari 2600 Course on Udemy cause it was only $13 bucks. It is awesome/great. Made the whole thing super simple to learn. Very idiot proof. It's too the point (which I find helpful). reply ketralnis 8 hours agoprevThere's something aging about \"If you like retro programming & want to learn more about the early days of 3D games...\" reply gustavopezzi 6 hours agoparentYou're not wrong. The average age of our students is... up there. I guess that's why people shared it on Hacker News and not on Tiktok. reply 0xPIT 12 hours agoprevWhat or whose lifetime do they refer to when they offer lifetime access? reply ketralnis 8 hours agoparentmin(you, the website) reply Solvency 15 hours agoprevAs a kid I always dreamt of coding for PS1 on one of those black dev console kits. As an adult.. boy I wish someone made a cozy Javascript transpiler to PS1 tool. reply ChrisRR 10 minutes agoparentGo on, challenge yourself. Learn C. You'll become a better developer from broadening your horizons reply sumtechguy 14 hours agoparentprevI have one of those yaroze things. I had all sorts of plans for it. Until I realized that the memory on the thing was identical to the size of the normal in store ones and getting at the CD for extra storage was basically broken at the SDK level. That made debugging 'interesting'. Even splurged for the codewarrior dev kit. It was a decent step down in tooling and memory that I could get on other platforms. Kind of fun for a bit. reply stuaxo 14 hours agorootparentAbout 15 years ago, my friend got about 10 ps1s from a car boot sale, one of the ones from that day was a yaroze. reply 0xcde4c3db 14 hours agoparentprevSame. As far as I know, though, the black PS1 didn't actually do anything special to enable development, it just allowed Sony to arbitrarily make the devkit incompatible with retail PS1 consoles. It still required a special memory card dongle and boot disc in addition to the special console model for some reason. I seem to recall that someone cracked the boot disc to run on retail consoles without the dongle, and the serial link just needed a level shifter like a MAX232 (though these days you'd probably use a USB UART like CP2102 or FT231). reply dijit 14 hours agoparentprevI too remember being mesmerized by the image of the black PS1 “developer” kit. It truly captured my imagination and alongside the movie Hackers from 1995 is probably one of the reasons I ended up wanting to be associated with computers in my future as a child. reply spxneo 14 hours agoparentprevyaroze devkit costs upwards of $10,000 USD now reply sumtechguy 14 hours agorootparentI just looked they are going from 800 to 10k. The 10k ones are the collectors ones where it is new in box (or at least complete in box). As long as you have the cables, console, and dev card you probably could get in at around 1-2k. reply ranger_danger 16 hours agoprevCourse looks really cool, just wish it wasn't $100 but I know that's unrealistic to expect. It seems most people who want to get into video game programming are poor. reply ChrisRR 11 minutes agoparentIf they want to get into video game programming, this is not the course. There's a ton of resources for Unity and Godot. This is for people who are interested more in the low-level implementation, game engines, and general history of the technology Also, $100 is really not expensive for a course. I think Udemy and youtube has ruined people's expectations reply VS1999 15 hours agoparentprevThe good news is that if you actually do make it into videogame programming, you're still poor reply iforgotpassword 15 hours agorootparentBut you're getting a nice burnout on top! reply udev4096 15 hours agorootparentAnd how we casually joke about it :) reply omoikane 15 hours agoparentprevMany modern game engines are free and less frustrating than SDKs for old consoles (e.g. a lot of common tasks are often built-in with new game engines, such as physics). For people who want to get into making games, I would recommend one of those game engines rather than paying $100. That said, I am not sure this course is targeting new game developers. reply pjmlp 3 hours agorootparentTo put this into perspective, my graduation project was porting a particle systems engine from NeXT/Objective-C into Windows/C++ using OpenGL, and using it for visualization techniques like marching cubes. Nowadays on a modern engine, a particle engine is one item from a huge list of engine features. reply danjoredd 14 hours agorootparentprevAgreed. Godot, Unity, Unreal, etc. all are better options for beginners. Gamemaker Studio even lets you program for free...you have to pay to make an executable though. If someone is dead-set on making a game for PS1, there are free tutorials out there also: https://www.youtube.com/playlist?list=PLAQybJIBW2UtXJITyUTJi... You won't have as intimate knowledge of PS1 hardware as OP's course, but I think thats a better starting point than paying first and then stopping because you didn't already know programming and it turned out more involved than you thought it would be. Now I DO think that there is a place for a paid course like this. I can see the appeal of getting hyperfixated on extremely specific hardware such as this, and making some cool stuff for it. For that type of person, the 100$ is worth it. reply filleduchaos 4 hours agorootparent> Agreed. Godot, Unity, Unreal, etc. all are better options for beginners. Am I just lost or are a game engine and a game development course not very obviously different things? Paid tutorials exist for everything you've listed, because they are in fact tech and not teachers. reply Keyframe 14 hours agoprevI like the idea, and other courses seem solid in overview too. However, I'm put off by not being able to download the course. I understand it's my old-man mentality kicking in, but I just have this bug in my mind \"what if it goes down\", \"I wanna watch this offline\", \"what if dude disappears\"..). I also understand why there's no option for that. I don't know, anyone else have feelings like these in general? I'd have no issue if it was way cheaper, and I'd have no problem if it was somewhat more expensive if I could download. It's a weird feeling like that that ultimately did affect me not purchasing it. edit: I realized I wouldn't mind it if it was on a big platform like I don't know what would support it, steam? appstore? I know those won't go away and I have stuff there. reply gustavopezzi 13 hours agoparentHi! Creator of the course here. This is a really good point, and I honestly did not consider someone's fear of a platform \"disappearing\" as a potential friction to buy before I read your comment. It makes total sense. I have been doing this for a while, so disappearing was not even in my thoughts. :) But I'll need to think about that and how to make it clear for everyone that visits the school. Thanks! reply ChrisRR 8 minutes agorootparentI also am in the camp of people who want to be able to download (although I know that increases the risk of people pirating the videos) Partially because the video player doesn't work very well on tablets, and partially because after you changed the game engine course, I still wanted to refer to the original version reply Keyframe 13 hours agorootparentprevThanks for consideration. Maybe easiest path would be to go through one of the bigger platforms but maybe more expensive and also offer same but cheaper on your own. I've read through your FAQ, but one more thing to consider is to offer a bundle discount, like for buy all or similar. reply fallat 13 hours agorootparentprevRelease the videos as lower quality; maybe make the downloadable content not as nice to browse? reply keb_ 13 hours agorootparentprev> I have been doing this for a while, so disappearing was not even in my thoughts. :) To be frank, this is not very re-assuring. Every platform that has ever gone offline has at one point told their customers something similar. It's also possible you may go offline for reasons outside of your control. reply gustavopezzi 12 hours agorootparent> It's also possible you may go offline for reasons outside of your control. I see. I've just discussed about this with other people at the school. I guess if something like that ever happens, the course content will probably be made available in full to all students. One thing is that I am personally always updating the course and adding new examples and lectures to it. If students simply download the content once there will be different versions of it. This was one of the reasons I've decided to record lectures instead of simply writing a book, for example. reply keb_ 13 hours agoparentprev> I don't know, anyone else have feelings like these in general? I'd have no issue if it was way cheaper, and I'd have no problem if it was somewhat more expensive if I could download. It's a weird feeling like that that ultimately did affect me not purchasing it. Yeah, I'm the same. These courses sound and look right up my alley, but I try to only support DRM-Free material where I can for the reasons your described. The Pragmatic Studio [1] is a good example of an online school that offers DRM-Free downloads for customers. [1] https://pragmaticstudio.com/ reply tycho-newman 14 hours agoprev [–] Will I learn how to run Crysis? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The course provides 25 hours of on-demand video content and downloadable resources, with lifetime access and mobile/desktop compatibility.",
      "Upon completion, a certificate is awarded, and payment options include Credit Card or PayPal, with a 14-day money-back guarantee.",
      "The course was last updated in April 2024, ensuring the content remains current and relevant."
    ],
    "commentSummary": [
      "Users are sharing resources and tips for a PS1 Programming Course focusing on MIPS Assembly and C, as well as running builds on unmodified PS1 or PS2 consoles using homebrew tools.",
      "Suggestions for future courses on machine learning, math-related topics, and retro gaming technology are being made within the discussion.",
      "The author is involved in a decompilation project for a game, encountering hurdles related to legal issues, while conversations cover PS1 development kits, game engines, online course platforms, and DRM-free downloads."
    ],
    "points": 203,
    "commentCount": 85,
    "retryCount": 0,
    "time": 1712684637
  },
  {
    "id": 39981945,
    "title": "Exploring Async Rust in Zed Editor on macOS",
    "originLink": "https://zed.dev/blog/zed-decoded-async-rust",
    "originBody": "Zed Decoded: Async Rust Thorsten Ball Antonio Scandurra April 9th, 2024 Welcome to the first article in a new series called Zed Decoded. In Zed Decoded I'm going to take a close look at Zed — how it's built, which data structures it uses, which technologies and techniques, what features it has, which bugs we ran into. The best part? I won't do this alone, but get to interview and ask my colleagues here at Zed about everything I want to know. Companion Video: Async Rust This post comes with a 1hr companion video, in which Thorsten and Antonio explore how Zed uses async Rust — in Zed. It's a loose conversation that focuses on the code and dives a bit deeper into some topics that didn't fit into the post. Watch the video here: https://youtu.be/gkU4NGSe21I The first topic that was on my list: async Rust and how it's used in Zed. Over the past few months I've become quite fascinated with async Rust — Zed's the first codebase I've worked in that uses it — so I decided to sit down and ask Antonio, one of Zed's co-founders, about how we use async Rust in Zed. We won't get into the details of async Rust itself (familiarity with that is to be expected if you want to understand the nitty-gritty of the code we'll see), but instead focus on how Zed uses async Rust to build a high-performance, native application: what async code looks like on the application level, which runtime it uses, why it uses that runtime. Writing async Rust with GPUI Let's jump right into the deep end. Here is a snippet of code that's representative of async code in the Zed codebase: fn show_cursor_names(&mut self, cx: &mut ViewContext) { self.show_cursor_names = true; cx.notify(); cx.spawn(|this, mut cx| async move { cx.background_executor().timer(CURSORS_VISIBLE_FOR).await; this.update(&mut cx, |this, cx| { this.show_cursor_names = false; cx.notify() }) .ok() }) .detach(); } It's a function from our Editor. When it's called, Zed shows the names of the owners of each cursor: your name or the names of the people you're collaborating with. It's called, for example, when the editor is re-focused, so you can quickly see who's doing what and where. What show_cursor_names does is the following: Toggle on Editor.show_cursor_names and trigger a re-render of the editor. When Editor.show_cursor_names is true, cursor names will be rendered. Spawn a task that sleeps for CURSOR_VISIBLE_FOR, turn the cursors off, and trigger another re-render. If you've ever written async Rust before, you can spot some familiar elements in the code: there's a .spawn, there's an async move, there's an await. And if you've ever used the async_task crate before, this might remind you of code like this: let ex = Executor::new(); ex.spawn(async { loop { Timer::after(Duration::from_secs(1)).await; } }) .detach(); That's because Zed uses async_task for its Task type. But in this example there's an Executor — where is that in the Zed code? And what does cx.background_executor() do? Good questions, let's find answers. macOS as our async runtime One remarkable thing about async Rust is that it allows you to choose your own runtime. That's different from a lot of other languages (such as JavaScript) in which you can also write asynchronous code. Runtime isn't a term with very sharp definition, but for our purposes here, we can say that a runtime is the thing that runs your asynchronous code and provides you with utilities such as .spawn and something like an Executor. The most popular of these runtimes is probably tokio. But there's also smol, embassy and others. Choosing and switching runtimes comes with tradeoffs, they are only interchangable to a degree, but it is possible. In Zed for macOS, as it turns out, we don't use any one of these. We also don't use async_task's Executor. But there has to be something to execute the asynchronous code, right? Otherwise I wouldn't be typing these lines in Zed. So what then does cx.spawn do and what is the cx.background_executor()? Let's take a look. Here are three relevant methods from GPUI's AppContext: // crates/gpui/src/app.rs impl AppContext { pub fn background_executor(&self) -> &BackgroundExecutor { &self.background_executor } pub fn foreground_executor(&self) -> &ForegroundExecutor { &self.foreground_executor } /// Spawns the future returned by the given function on the thread pool. The closure will be invoked /// with [AsyncAppContext], which allows the application state to be accessed across await points. pub fn spawn(&self, f: impl FnOnce(AsyncAppContext) -> Fut) -> Task where Fut: Future + 'static, R: 'static, { self.foreground_executor.spawn(f(self.to_async())) } // [...] } Alright, two executors, foreground_executor and background_executor, and both have .spawn methods. We already saw background_executor's .spawn above in show_cursor_names and here, in AppContext.spawn, we see the foreground_executor counterpart. One level deeper, we can see what foreground_executor.spawn does: // crates/gpui/src/executor.rs impl ForegroundExecutor { /// Enqueues the given Task to run on the main thread at some point in the future. pub fn spawn(&self, future: impl Future + 'static) -> Task where R: 'static, { let dispatcher = self.dispatcher.clone(); fn inner( dispatcher: Arc, future: AnyLocalFuture, ) -> Task { let (runnable, task) = async_task::spawn_local(future, move |runnable| { dispatcher.dispatch_on_main_thread(runnable) }); runnable.schedule(); Task::Spawned(task) } inner::(dispatcher, Box::pin(future)) } // [...] } There's a lot going on here, a lot of syntax, but what happens can be boiled down to this: the .spawn method takes in a future, turns it into a Runnable and a Task, and asks the dispatcher to run it on the main thread. The dispatcher here is a PlatformDispatcher. That's the GPUI equivalent of async_task's Executor from above. It has Platform in its name because it has different implementations for macOS, Linux, and Windows. But in this post, we're only going to look at macOS, since that's our best-supported platform at the moment and Linux/Windows implementations are still work-in-progress. So what does dispatch_on_main_thread do? Does this now call an async runtime? No, no runtime there either: // crates/gpui/src/platform/mac/dispatcher.rs impl PlatformDispatcher for MacDispatcher { fn dispatch_on_main_thread(&self, runnable: Runnable) { unsafe { dispatch_async_f( dispatch_get_main_queue(), runnable.into_raw().as_ptr() as *mut c_void, Some(trampoline), ); } } // [...] } extern \"C\" fn trampoline(runnable: *mut c_void) { let task = unsafe { Runnable::::from_raw(NonNull::new_unchecked(runnable as *mut ())) }; task.run(); } dispatch_async_f is where the call leaves the Zed codebase, because dispatch_async_f is actually a compile-time generated binding to the dispatch_async_f function in macOS' Grand Central Dispatch's (GCD). dispatch_get_main_queue(), too, is such a binding. That's right: Zed, as a macOS application, uses macOS' GCD to schedule and execute work. What happens in the snippet above is that Zed turns the Runnable — think of it as a handle to a Task — into a raw pointer and passes it to dispatch_async_f along with a trampoline, which puts it on its main_queue. When GCD then decides it's time to run the next item on the main_queue, it pops it off the queue, and calls trampoline, which takes the raw pointer, turns it back into a Runnable and, to poll the Future behind its Task, calls .run() on it. And, as I learned to my big surprise: that's it. That's essentially all the code necessary to use GCD as a \"runtime\" for async Rust. Where other applications use tokio or smol, Zed uses thin wrappers around GCD and crates such as async_task. Wait, but what about the BackgroundExecutor? It's very, very similar to the ForegroundExecutor, with the main difference being that the BackgroundExecutor calls this method on PlatformDispatcher: impl PlatformDispatcher for MacDispatcher { fn dispatch(&self, runnable: Runnable, _: Option) { unsafe { dispatch_async_f( dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_HIGH.try_into().unwrap(), 0), runnable.into_raw().as_ptr() as *mut c_void, Some(trampoline), ); } } } The only difference between this dispatch method and dispatch_async_f from above is the queue. The BackgroundExecutor doesn't use the main_queue, but a global queue. Like I did when I first read through this code, you now might wonder: why? Why use GCD? Why have a ForegroundExecutor and a BackgroundExecutor? What's so special about the main_queue? Never block the main thread In a native UI application, the main thread is important. No, the main thread is holy. The main thread is where the rendering happens, where user input is handled, where the operating system communicates with the application. The main thread should never, ever block. On the main thread, the responsiveness of your app lives or dies. That's true for Cocoa applications on macOS too. Rendering, receiving user input, communication with macOS, and other platform concerns have to happen on the main thread. And since Zed wants perfect cooperation with macOS to ensure high-performance and responsiveness, it does two things. First, it uses GCD to schedule its work — on and off the main thread — so that macOS can maintain high responsiveness and overall system efficiency. Second, the importance of the main thread is baked into GPUI, the UI framework, by explicitly making the distinction between the ForegroundExecutor and the BackgroundExecutor, both of which we saw above. As a writer of application-level Zed code, you should always be mindful of what happens on the main thread and never put too much blocking work on it. If you were to put, say, a blocking sleep(10ms) on the main thread, rendering the UI now has to wait for that sleep() to finish, which means that rendering the next frame would take longer than 8ms — the maximum frame time available if you want to achieve 120 FPS. You'd \"drop a frame\", as they say. Knowing that, let's take a look at another small snippet of code. This time it's from the built-in terminal in Zed, a function that searches through the contents of the terminal buffer: // crates/terminal/src/terminal.rs pub struct Terminal { term: Arc>>, // [... other fields ...] } pub fn find_matches( &mut self, mut searcher: RegexSearch, cx: &mut ModelContext, ) -> Task>> { let term = self.term.clone(); cx.background_executor().spawn(async move { let term = term.lock(); all_search_matches(&term, &mut searcher).collect() }) } The first line in find_matches, the self.term.clone(), happens on the main thread and is quick: self.term is an Arc>, so cloning only bumps the reference count on the Arc. The call to .lock() then only happens in the background, since .lock() might block. It's unlikely that there will be contention for this lock in this particular code path, but if there was contention, it wouldn't freeze the UI, only a single background thread. That's the pattern: if it's quick, you can do it on the main thread, but if it might take a while or even block, put it on a background thread by using cx.background_executor(). Here's another example, the project-wide search in Zed (⌘-shift-f). It pushes as much heavy work as possible onto background threads to ensure Zed stays responsive while searching through tens of thousands of files in your project. Here's a simplified and heavily-commented excerpt from Project.search_local that shows the main part of the search: // crates/project/src/project.rs // Spawn a Task on the background executor. The Task finds all files on disk // that contain >1 matches for the given `query` and sends them back over // the `matching_paths_tx` channel. let (matching_paths_tx, matching_paths_rx) = smol::channel::bounded(1024); cx.background_executor() .spawn(Self::background_search( // [... other arguments ... ] query.clone(), matching_paths_tx, )) .detach(); // Setup a channel on which we stream results to the UI. let (result_tx, result_rx) = smol::channel::bounded(1024); // On the main thread, spawn a Task that first... cx.spawn(|this, mut cx| async move { // ... waits for the background thread to return the filepaths of // the maximum number of files that we want to search... let mut matching_paths = matching_paths_rx .take(MAX_SEARCH_RESULT_FILES + 1) .collect::>() .await; // ... then loops over the filepaths in chunks of 64... for matching_paths_chunk in matching_paths.chunks(64) { let mut chunk_results = Vec::new(); for matching_path in matching_paths_chunk { // .... opens each file.... let buffer = this.update(&mut cx, |this, cx| { this.open_buffer((*worktree_id, path.clone()), cx) })?; // ... and pushes into `chunk_results` a Task that // runs on the main thread and ... chunk_results.push(cx.spawn(|cx| async move { // ... waits for the file to be opened ... let buffer = buffer.await?; // ... creates a snapshot of its contents ... let snapshot = buffer.read_with(&cx, |buffer, _| buffer.snapshot())?; // ... and again starts a Task on the background executor, // which searches through the snapshot for all results. let ranges = cx .background_executor() .spawn(async move { query .search(&snapshot, None) .await .iter() .collect::>() }) .await; Ok((buffer, ranges)) })); } // On the main thread, non-blocking, wait for all buffers to be searched... let chunk_results = futures::future::join_all(chunk_results).await; for result in chunk_results { if let Some((buffer, ranges)) = result.log_err() { // send the results over the results channel result_tx .send(SearchResult::Buffer { buffer, ranges }) .await?; } } } }) .detach(); result_rx It's a lot of code — sorry! — but there's not a lot more going on than the concepts we already talked about. What's noteworthy here and why I wanted to show it is the ping-pong between the main thread and background threads: main thread: kicks off the search and hands the query over to background thread background thread: finds files in project with >1 occurrences of query in them, sends results back over channel as they come in main thread: waits until background thread has found MAX+1 results, then drops channel, which causes background thread to exit main thread: spawns multiple other main-thread tasks to open each file & create a snapshot. background threads: search through buffer snapshot to find all results in a buffer, sends results back over channel main thread: waits for background thread to find results in all buffers, then sends them back to the caller of the outer search_local method Even though this method can be optimized and the search made a lot faster (we haven't gotten around to that yet), it can already search thousands of files without blocking the main thread, while still using multiple CPU cores. Async-Friendly Data Structures, Testing Executors, and More I'm pretty sure that the previous code excerpt raised a lot of questions that I haven't answered yet: how exactly is it possible to send a buffer snapshot to a background thread? How efficient is it do that? What if I want to modify such a snapshot on another thread? How do you test all this? And I'm sorry to say that I couldn't fit all of the answers into this post. But there is a companion video in which Antonio and I did dive into a lot of these areas and talked about async-friendly data structures, copy-on-write buffer snapshots, and other things. Antonio also gave a fantastic talk about how we do property-testing of async Rust code in the Zed code base that I highly recommend. I also promise that in the future there will be a post about the data structures underlying the Zed editor. Until next time! Interested in trying Zed out? You can try Zed today on macOS. Download now!",
    "commentLink": "https://news.ycombinator.com/item?id=39981945",
    "commentBody": "Zed Decoded: Async Rust (zed.dev)200 points by ingve 16 hours agohidepastfavorite76 comments epistasis 15 hours agoThere's a ton of really good Rust content on the Zed blog. As somebody new to Rust, I've learned a lot from it. For example, how they do ownership in a GUI setting, in a language for which even writing a doubly-linked list is a challenge: https://zed.dev/blog/gpui-ownership reply diarrhea 14 hours agoparentIs the content also useful if one isn't interested in GUIs at all? reply epistasis 14 hours agorootparentIMHO, yes, but ultimately only you can be the judge! reply xpl 10 hours agoprevThe biggest shortcoming of Zed is the lack of \"remote development\". We almost exclusively write code in Codespaces-like setup, where you spin dev containers in the cloud, attaching to it from VSCode, so you don't have to run builds on your local laptop, and also heavy LSPs (like clangd) could run remotely on a fat RAM-heavy machine in K8S. This is possible because VSCode had client-server architecture from the get-go, so it can run itself in an remote container, only having the GUI (light client part) running on your local machine. reply ivan_burazin 1 hour agoparentI would love if Zed supported remote development, as we would enable it to all Daytona users right away. Nathan Sobo if your reading and interested hit me up: ivan at daytona.io reply pjmlp 3 hours agoparentprevI find it kind of ironic of how we have come full circle to timesharing ways of working. Every time I launch VSCode or any other Web IDE, I have flashbacks of running X Windows over the network, or telnet into my Emacs session. reply epistasis 6 hours agoparentprevIf this is a significant share of market it should be fairly easy for Zed to go that direction, as the entire app is based around collaboration and pair programming. Is VSCode involved in provisioning these dev servers, or do you just point it to an SSH endpoint? reply andruby 2 hours agorootparentI'd like to see statistics on the prevalence of this. I've not come across that setup yet. Maybe that part of the market is growing. Being good at remote dev might sometimes be in conflict with super fast and responsive, or at least make it harder to be great at both. reply ivanjermakov 8 hours agoparentprevDoes this mean that you have to wait for the network hop for LSP completions? I would go insane. reply xpl 7 hours agorootparentThere are LSPs that can be slower than the network, so having an extra hop doesn't impact the UX too much (in that case). I don't usually even notice. Networks can vary also, depending on your/DC location, routes, VPNs, providers (and etc.), it can be under 50ms latency (which isn't even noticeable completions-wise) or over 1000ms. The most important thing is that the typing itself is fast, as it happens on the local machine. Compared e.g. to vim running in an SSH session — that can feel really bad, especially if the remote machine is on the other side of the ocean. So compared to that, VSCode is a big upgrade. reply doganugurlu 4 hours agoprevI love Zed and it’s been my daily driver for the last 2 months or so. It was very fast until recently. I am guessing it’s the Rust stuff that caused the performance hit since I keep seeing a Rust-y error in the logs. Jumps, search and even saving files started lagging up to multiple seconds in some instances. I think we also need an easy way to disable /configure LSPs etc. I know it’s through the local JSON config but getting the keys and the values right and determining if it did what it’s supposed to do is a bit cumbersome. reply sapiogram 5 minutes agoparent> I am guessing it’s the Rust stuff that caused the performance hit From Github's code stats, Zed is 97.3% Rust, so Rust is almost certainly both the problem and the solution here. reply robinsonrc 1 hour agoparentprevIs the whole thing not written in Rust? reply pornel 15 hours agoprevgtk-rs also supports Rust's futures on top of GTK's event loop, and you can .await dialog windows and button presses. https://docs.rs/glib/latest/glib/struct.MainContext.html#imp... reply jenadine 14 hours agoparentI guess every rust toolkit supports that by now. Eg, Slint: https://docs.rs/slint/latest/slint/fn.spawn_local.html reply __s 13 hours agoparentprevBack in 2012 I threw together something like this for managing UI dialogs in C#. For all the hate async/await gets about being an over optimization, it's good to see people find use for it as an ergonomic tool outside of concurrency reply fanf2 10 hours agorootparentUser interfaces are not outside of concurrency. reply cptroot 15 hours agoprevThis was a really neat dive into GPUI. The discussion about main thread vs background thread was interesting, especially as it relates to choices the rest of the async ecosystem has made. reply imron 12 hours agoprev> Where other applications use tokio or smol, Zed uses thin wrappers around GCD and crates such as async_task This is an amazing idea. reply extr 12 hours agoprevZed is super nice. As soon as they iron out some of challenges with python development I’ll switch from VS Code. reply rhodysurf 8 hours agoparentYeah it really needs notebooks and automatic venv discovery for me to use it for python. Having to define the venv in each projects config is just not needed in other pythons ides reply darthrupert 6 hours agorootparentSeems like a low cost to pay for that part being robustabd controlled by you. But this is from an emacs user perspective, perhaps ypu vscode guys have it better. reply haolez 12 hours agoprevSlightly related, but does it support Linux already? reply SoothingSorbet 10 hours agoparentIt claims to support it if you build it from source, but it's experimental / in development. reply brianzelip 12 hours agoprevInforming article. We are rooting for Zed, keep up the great work y’all! reply mamidon 15 hours agoprevHow are these guys funding a startup for the Nth text editor? If they have the guts to try this full time I suppose there's really no excuse for me not having a go at my own thing. reply simonerlic 15 hours agoparentTo be fair, this isn't just a random group of people - they're the people who made Atom, Electron, and Tree-sitter. reply mamidon 14 hours agorootparentOh undoubtedly they'll make a nice editor; I just think it's a tough thing to monetize when there's so many 'good enough' free editors around. reply agrippanux 14 hours agorootparentTheir plan is to make really elegant team-based shared coding features baked right into the editor - think a mashup of VSCode + Slack, and then charge for that, likely as a monthly subscription. It's not a bad idea if it can increase team velocity but the requirement would be enough people on the team live within it to justify the cost. I drove Zed for about a month, its very performant and a joy to use, but the lack of a remote development feature is massively prohibitive and I went back to VSCode as a result. reply nicce 12 hours agorootparentThere are way too many other features missing as well to do anything serious to be fair. All the things you would like proper IDE to do to increase your productivity. Let's see in couple years. The biggest problem is to get those initial features decent so that you can extract the value from crowdsourcing the missing things like VSCode does. reply sdesol 7 hours agorootparent> crowdsourcing the missing things like VSCode does. Based on what I am seeing right now [1], they might well have enough inertia to seriously compete against VSCode. I've analyzed a lot of open source projects and their community engagement on GitHub is quite impressive. Their contributor and new contributor stats is still going up, even though the initial hype was 2 months ago, which is really impressive. If they can get proper support for Linux and Windows and nail core features, I can see people wanting to contribute in the future and business leaders paying for it, if they can demonstrate improved productivity and collaboration. Full disclosure: This is my tool [1] https://devboard.gitsense.com/zed-industries?board=gitsense.... reply metabagel 12 hours agorootparentprevWhat do you mean by \"remote development feature\"? Debugging remotely, a la gdbserver? reply dj_gitmo 12 hours agorootparentNot the OP, but what I assume they are referring to is the ability to work on a remote host. For example if I have a Mac but I am working on a remote Linux box, VSCode will allow me to easily work sync with a repo on the remote host. It will also seamlessly handle port-forwarding for web dev. IMO this is the best feature of VSCode. reply dwattttt 11 hours agorootparentprevIt installs and runs a server stub over SSH (or into docker, or WSL), which lets it act as if you're working on a local project. Stuff like the built-in terminal is remote, building, debugging, etc all work remote. reply throwiforgtnlzy 9 hours agorootparentprevThat's the problem with increasing fragmentation as an n+1 of the same thing and trying to do everything yourself rather than delegating and scaling effort and expertise via crowdsourcing and outsourcing through plugin maintainers. An \"opposite\" of the Atom approach appears to be an extreme overreaction (rather than a sensible middle way) and fails to solve the problems it brought: a slow, wasteful platform and a tradition of abandoned, fragmented plugins (also a problem with Vim/neovim, emacs, VSCode, and JetBrains). A balance would be a curated plugin marketplace requiring quality, continuing maintenance, and support. Atom, VSCode, and JetBrains have some curation, but not enough. I don't know how to solve this other than to charge end users subscription fees and pay people for their efforts. Also, that brings up the problem of monetization. A number of plugin authors insist on trial, freemium, or paid-only plugins because it's not a spare time hobby for them. Lacking monetization, some plugins won't ever be created. So I don't see how zed will solve any problem other that being art as wonderful technology applied to a tough, busy, competitive category without enough unique, competitive advantages to self-reinforce increased adoption. As such, it seems unlikely to ever morph into a complete and useful IDE to uniformly support every development niche. Even now, I have difficulty finding a usable Haskell IDE with commercial providers such as VSCode or JetBrains to wonder if zed will somehow manage to be better than any of them and/or vim. TL;DR: I think Zed project leaders were overly ambitious, unrealistic, and failed to anticipate the endgame. reply alberth 14 hours agorootparentprevAdditionally, it’s also not yet another “text editor”. It’s very clear the differentiation of Zed will come in the form of the collaboration functionality it will have. reply fori1to10 14 hours agorootparentAnd speed. Startup time is instantaneous, in comparison to VS code say. reply wmf 13 hours agorootparentThe same team that \"erased\" native editors is now selling us the solution to the problem they created. reply eikenberry 10 hours agorootparentErased? On the contrary it seems, to me at least, like new editor development has really grown over the last few years. Editors like Kakoune, Helix, Micro are coming along nicely. I'm looking to switch, at least partially, to Helix soon myself. reply Starlevel004 12 hours agorootparentprevStraight up not true. Running my local copy it takes 5 seconds for the window to appear. reply maxbrunsfeld 11 hours agorootparentIf this happens with Zed 0.129 or later, could you open an issue? We had a problem at one point where there was a delay in macOS verifying our app’s code signature because we weren’t attaching the code signature correctly. reply keyle 10 hours agorootparentprev5 seconds sounds extremely long for a macOS native application. Could be macOS security (XProtect), storage latency?, tons of compressed memory?... reply hobs 13 hours agorootparentprevHow much are you going to pay for the 5 seconds per day you open up your editor? Most people dont even close it in the first place. reply metabagel 12 hours agorootparentI haven't tried this software, but I imagine it's more about having the UI keep up, so you can stay in the flow state. reply hansonkd 11 hours agorootparentIs raw code output so large that your editor can't \"keep up\" really something to optimize for? For me personally I write as little code as possible. you don't get bonus points for number of lines you write. reply satvikpendem 4 hours agorootparentYes it is, especially when you work with lots of large files in big codebases. I can open files in Vim that would crash VSCode, which is essentially why I still use Vim for certain parts of development. reply carlhjerpe 9 hours agorootparentprevYes, while at the same time having a responsive UI is important for editing, and we're not hanging less stuff in our editors than yesterday. Both emacs and neovim and vscode are essentially IDEs today. reply bobbylarrybobby 13 hours agorootparentprevPresumably speed is an overall important factor for the creators — not just startup speed — and I'd imagine it translates into speed elsewhere, such as input latency and UI responsiveness. reply pjmlp 3 hours agorootparentprevNot all of us have those editors in high regard. reply mamidon 14 hours agoparentprevIn the FAQ they indicate they raised funds from investors. reply IshKebab 14 hours agorootparentThey meant how are they going to fund it long-term. reply IshKebab 14 hours agoparentprevThey say: > Rather than selling you a proprietary editor, we'd much prefer to sell you services that seamlessly integrate with your editor to make you and your team more productive. Zed Channels is one example of such a service. It's free for anyone today, but we intend to begin charging for private use after a beta period of experimentation. Providing server-side compute to power AI features is another monetization scheme we're seeing getting traction. But I'm with you. I can't see paying-for-shared-editing working. There are too many people using other random editors. AI? Yeah maybe. I've paid for Copilot. But now they have to fund an AI research team as well. I really really hope they succeed but I'd probably still bet against them. reply autoexecbat 14 hours agorootparentInterviewer: What editor do you use? Interviewee: Vim Interviewer: Sorry, we use Zed here reply andruby 2 hours agorootparentHow would that be a problem? Either the candidate adapts and uses Zed in vim-mode, or the business allows the candidate to use vim. I certainly wouldn't make that a blocker. reply 38 15 hours agoprev [–] mac only: https://github.com/zed-industries/zed/issues/5394 reply simonerlic 15 hours agoparentI don't mean to reply-guy this thread, but it builds on Windows (and Linux) https://github.com/zed-industries/zed/blob/main/docs/src/dev... reply satvikpendem 13 hours agorootparentIt builds but is it usable? I thought it used many macOS-specific APIs like Metal for GPU rendering. reply Starlevel004 12 hours agorootparentIt's usable in the sense that it runs properly. reply thomastjeffery 13 hours agorootparentprevIt's pretty frustrating to read so much about a thing, only to find out it's only supported on a niche platform that I don't have access to. The fact that that isn't totally the case is a great thing to hear, and I would love to have learned this fact sooner. It would go a long way for Zed's team to at least mention it on the Downloads page. reply itishappy 12 hours agorootparent> Download for macOS (Apple silicon) Requires macOS 10.15+ > Download for macOS (Intel chip) Requires macOS 10.15+ > Download for macOS (Universal binary) Requires macOS 10.15+ there are no other buttons... https://zed.dev/download reply pc_edwin 1 hour agorootparentprevIts almost here. They've been making a lot of progress on windows and linux support: https://github.com/zed-industries/zed/commits/main/?since=20... reply meepmorp 12 hours agorootparentprevmacOS isn't a niche platform reply xpl 10 hours agorootparentprev> it's only supported on a niche platform Lold hard on that. Just every developer friend I know, uses Mac for coding. It is ubiquitous. In a BigTech company I once worked, there were virtually no guys having Win or Linux on their laptops, so we often even didn't test our internal tooling against those platforms, as almost no one used them... Zed guys know their audience. It is developers, not gamers or office clerks. reply ben-schaaf 10 hours agorootparentEvery developer survey or stats on OS usage for development tools I've ever seen has Windows at ~50% and Linux about even with Mac. reply xpl 9 hours agorootparentProbably those are students + hobbyists + maybe Windows game developers? Or my anecdata is just not representative. Maybe Zed considered it is worth targeting only Mac audience, as it gives much higher rate of people who would actually buy their editor (i.e. the rate of professional users, who code for living, is higher)? That is my guess, I don't have any data to back it up. reply ben-schaaf 9 hours agorootparentIn my experience Multi-Platform apps that start by only targeting a single platform do so because that's the one the developer is using. reply pjmlp 3 hours agorootparentprevGiven that macOS only covers 10% of the desktop market, it doesn't compute, given that the other 90% of the market also needs developers. reply xpl 13 minutes agorootparentWeb development doesn't bind you to a specific platform. And modern desktop apps are overwhelmingly Electron (i.e. based on a web stack). It is just simpler to develop than native, and gives you portability out of the box. And if you're a mobile developer, you must have a Mac, because the proper tooling for iOS does not exist on other platforms. So even you target multiple mobile platforms, you'd develop it on a Mac. reply dancemethis 7 hours agorootparentprevRich developers who don't mind giving their client's \"secrets\" to Apple.* reply thomastjeffery 8 hours agorootparentprevSounds like selection bias at work. Developers are the most likely to choose both, a platform/OS to use for work, and a workplace where other people choose/encourage the same platform/OS. If the people I was working with didn't bother to test their tooling on Linux, then I would be the one person on the team testing for (and porting to) Linux, and that would definitely motivate me to work elsewhere. reply 38 14 hours agorootparentprevnext [5 more] [flagged] yogorenapan 12 hours agorootparentThat’s just how things are on Windows. Building just about anything requires Visual Studio (not the UI but the tooling that comes with it. E.g. msbuild and C++ headers) You won’t have to do this on Linux or if you use a pre-built exe reply mamcx 10 hours agorootparentI have a tip on how to install it faster and easier with Chocolatey: https://www.elmalabarista.com/blog/2020-install-rust-windows... choco install visualstudio2019-workload-vctools, llvm reply satvikpendem 4 hours agorootparentYou can use winget for that now as it's included in Windows itself reply ben-schaaf 7 hours agorootparentprevThat's just the reality of developing for Windows. You don't need their compiler but you do need the headers. Same goes for macOS which requires installing XCode. reply drcongo 14 hours agorootparentprevIt's fine. Every time Zed gets a mention on here someone finds it incredibly important to post the words \"Mac only\" without checking, even when, as with this thread, the topic has absolutely nothing to do with running Zed or what operating systems it can run on. reply scientist4397 13 hours agoparentprev [–] Available in AUR reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores the series Zed Decoded, focusing on async Rust usage in the Zed editor on macOS.",
      "Zed utilizes Grand Central Dispatch and the async_task crate to schedule tasks off the main thread for high performance and responsiveness.",
      "Examples include efficient project file searches without blocking the main thread, with future articles set to cover data structures in the Zed editor."
    ],
    "commentSummary": [
      "The Zed blog offers insights into Rust, highlighting ownership in GUI applications and the development of the Zed text editor for collaborative coding.",
      "Topics covered include speed, responsiveness, monetization, compatibility, and challenges in platform selection for app development.",
      "Discussions encompass Zed's features, critiques, potential enhancements, and competition with VSCode, as well as aspects like remote development, monetization approaches, and platform adaptability."
    ],
    "points": 200,
    "commentCount": 76,
    "retryCount": 0,
    "time": 1712684339
  }
]
