[
  {
    "id": 37967126,
    "title": "What every developer should know about GPU computing",
    "originLink": "https://codeconfessions.substack.com/p/gpu-computing",
    "originBody": "Subscribe Sign in Discover more from Confessions of a Code Addict Unlocking modern algorithms and research through practical coding. Welcome to the space where hard computer science becomes accessible. Over 1,000 subscribers Subscribe Continue reading Sign in What Every Developer Should Know About GPU Computing A primer on GPU architecture and computing ABHINAV UPADHYAY OCT 18, 2023 47 3 Share Most programmers have an intimate understanding of CPUs and sequential programming because they grow up writing code for the CPU, but many are less familiar with the inner workings of GPUs and what makes them so special. Over the past decade, GPUs have become incredibly important because of their pervasive use in deep learning. Today, it is essential for every software engineer to possess a basic understanding of how they work. My goal with this article is to give you that background. Much of this article is based on the book ‚ÄúProgramming Massively Parallel Processors‚Äù, 4th edition by Hwu et al. As the book covers Nvidia GPUs, I will also be talking about Nvidia GPUs and using Nvidia specific terminology. However, the fundamental concepts and approach to GPU programming apply to other vendors as well. Photo by Thomas Foster on Unsplash Confessions of a Code Addict is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber. Subscribe Comparing CPU and GPU We will start by doing a comparison between CPU and GPU which will give us a better vantage point of the GPU landscape. However, this is a topic of its own and we cannot possibly squeeze everything in one section. So, we will stick to a few key points. The major difference between CPUs and GPUs is in their design goals. CPUs were designed to execute sequential instructions1. To improve their sequential execution performance, many features have been introduced in the CPU design over the years. The emphasis has been on reducing the instruction execution latency so that CPUs can execute a sequence of instructions as fast as possible. This includes features like instruction pipelining, out of order execution, speculative execution and multilevel caches (just to list a few). GPUs on the other hand have been designed for massive levels of parallelism and high throughput, at the cost of medium to high instruction latency. This design direction has been influenced by their use in video games, graphics, numerical computing, and now deep learning. All of these applications need to perform a ton of linear algebra and numerical computations at a very fast rate, because of which a lot of attention has gone into improving the throughput of these devices. Let‚Äôs consider a concrete example. A CPU can add two numbers much faster than the GPU because of its low instruction latency. They will be able to do several of such computations in a sequence faster than a GPU. However, when it comes to doing millions or billions of such computations, a GPU will do those computations much much faster than a CPU because of its sheer massive parallelism. If you like numbers, let‚Äôs talk about numbers. The performance of hardware for numerical computations is measured in terms of how many floating point operations it can do per second (FLOPS). The Nvidia Ampere A100 offers a throughput of 19.5 TFLOPS for 32-bit precision. In comparison, the throughput of an Intel 24-core processor is 0.66 TFLOPS for 32-bit precision (these numbers are from 2021). And, this gap in the throughput performance between GPUs and CPUs has been growing wider with each passing year. The following figure compares the architectures of CPUs and GPUs. Figure 1: A comparison of the CPU and GPU chip design. Figure from the Nvidia CUDA C++ Programming Guide As you may see, CPUs dedicate a significant amount of chip area towards features which will reduce instruction latency, such as large caches, less ALUs and more control units. In contrast, GPUs use a large number of ALUs to maximize their computation power and throughput. They use a very small amount of the chip area for caches and control units, the things which reduce the latency for CPUs. Latency Tolerance, High Throughput and Little‚Äôs Law You might wonder, how do the GPUs tolerate high latencies and yet provide high performance. We can understand this with the help of Little‚Äôs law from queuing theory. It states that the average number of requests in the system (Qd for queue depth) is equal to the average arrival rate of requests (throughput T) multiplied by the average amount of time to serve a request (latency L). ùëÑ ùëë = ùëá √ó ùêø In the context of GPUs, this basically means one can tolerate a given level of latency in the system to achieve a target throughput by maintaining a queue of instructions which are either in execution or waiting. The large number of compute units in the GPU and efficient thread scheduling enables the GPUs to maintain this queue over kernel execution time and achieve a high throughput despite these long memory latencies. GPU Architecture So we understand GPUs favor high throughput but what does their architecture look like which enables them to achieve this, let‚Äôs discuss in this section. GPU Compute Architecture A GPU consists of an array of streaming multiprocessors (SM). Each of these SMs in turn consists of several streaming processors or cores or threads. For instance, the Nvidia H100 GPU has 132 SMs with 64 cores per SM, totalling a whopping 8448 cores. Each SM has a limited amount of on-chip memory, often referred to as shared memory or a scratchpad, which is shared among all the cores. Likewise, the control unit resources on the SM are shared by all the cores. Additionally, each SM is equipped with hardware-based thread schedulers for executing threads. Apart from these, each SM also has several functional units or other accelerated compute units such as tensor cores, or ray tracing units to serve specific compute demands of the workload that the GPU caters to. Figure 2: The GPU Compute Architecture Next, let‚Äôs breakdown the GPU memory and look inside. GPU Memory Architecture The GPU has several layers of different kinds of memories, with each having their specific use case. The following figure shows the memory hierarchy for one SM in the GPU. Figure 3: The GPU Memory Architecture from the Cornell Virtual Workshop on Understanding GPUs Let‚Äôs break it down. Registers: We will start with the registers. Each SM in the GPU has a large number of registers. For instance, the Nvidia A100, and H100 models have 65,536 registers per SM. These registers are shared between the cores, and are allocated to them dynamically depending on the requirement of the threads. During execution the registers allocated to a thread are private to it, i.e., other threads cannot read/write those registers. Constant Caches: Next, we have constant caches on the chip. These are used to cache constant data used by the code executing on the SM. To utilize these caches, programmers have to explicitly declare objects as constants in the code so that the GPU may cache and keep them in the constant cache. Shared Memory: Each SM also has a shared memory or scratchpad which is a small amount of fast and low latency on-chip programmable SRAM memory. It is designed to be shared by a block of threads running on the SM. The idea behind shared memory is that if multiple threads need to work with the same piece of data, only one of them should load it from the global memory, while others will share it. Careful usage of shared memory can cut down redundant load operations from global memory, and improve the kernel execution performance. Another usage of the shared memory is as a synchronization mechanism between threads executing within a block. L1 Cache: Each SM also has an L1 cache which can cache frequently accessed data from L2 cache. L2 Cache: There is an L2 cache which is shared by all SMs. It caches the frequently accessed data from the global memory to cut down the latency. Note that both L1 and L2 caches are transparent to the SM, i.e., the SM doesn‚Äôt know it is getting data from L1 or L2. As far as the SM is concerned, it is getting data from the global memory. This is similar to how L1/L2/L3 caches work in CPUs. Global Memory: The GPU also has an off-chip global memory, which is a high capacity and high bandwidth DRAM. For instance, the Nvidia H100 has 80 GB high bandwidth memory (HBM) with bandwidth of 3000 GB/second. Due to being far away from the SMs, the latency of global memory is quite high. However, the several additional layers of on-chip memories, and high number of compute units help hide this latency (see Little's law discussion in the CPU vs GPU section). Finding the article insightful? Consider becoming a free or paid subscriber to support my writing. Subscribe Now that we know about the key components of the GPU hardware, let‚Äôs go one step deeper and understand how these components come into picture when executing code. Understanding the Execution Model of the GPU To understand how the GPU executes a kernel, we first need to understand what a kernel is and what its configurations are. Let‚Äôs start there. A Brief Introduction to CUDA Kernels and Thread Blocks CUDA is the programming interface provided by Nvidia for writing programs for their GPUs. In CUDA you express a computation that you want to run on the GPU in the form similar to a C/C++ function and this function is called a kernel. The kernel operates on vectors of numbers in parallel which are provided to it as function parameters. A simple example would be a kernel to perform vector addition, i.e., a kernel that takes two vectors of numbers as inputs, adds them element-wise and writes the result to a third vector. To execute a kernel on the GPU, we need to launch a number of threads which is collectively referred to as a grid. But there is more structure to the grid. A grid consists of one or more thread blocks (sometimes simply called as blocks) and each block consists of one or more threads. The number of blocks and threads depends on the size of the data and the amount of parallelism we want. For instance, in our vector addition example, if we are adding vectors of dimension 256, then we may decide to configure a single thread block of 256 threads so that each thread operates on one element of the vector. For bigger problems, we may not have enough threads available on the GPU and we might want each thread to handle multiple data points. Figure 4: Grid of thread blocks (figure from Nvidia CUDA C++ Programming Guide) As far as implementation goes, writing a kernel requires two parts. One is the host code which executes on the CPU. This is where we load the data, allocate memory on the GPU, and launch the kernel with a configured grid of threads. The 2nd part is writing the device (GPU) code which executes on the GPU. For our vector addition example, the following figure shows the host code. Figure 5: Host code for the CUDA kernel for adding two vectors And the following is the device code, which defines the actual kernel function. Figure 6: Device code containing the definition of the vector addition kernel As the focus of this article is not teaching CUDA, we will not be going any deeper in this code. Now, let‚Äôs look at the exact steps behind the execution of a kernel on the GPU. Steps Behind Execution of a Kernel on the GPU 1. Copying Data from Host to Device Before the kernel can be scheduled for execution, all the data that it needs has to be copied from the memory of the host (the CPU) onto the global memory of the GPU (the device). Although, in latest GPU hardware one can also read directly from host memory using unified virtual memory (see section 2.2 of the paper: ‚ÄúEMOGI: Efficient Memory-access for Out-of-memory Graph-traversal in GPUs‚Äù). 2. Scheduling of the Thread Blocks on SMs After the GPU has all the necessary data in its memory, it assigns the thread blocks to the SMs. All threads within a block are processed by the same SM at the same time. To make this happen, the GPU must set aside resources on the SM for those threads before it can start executing them. In practice, multiple thread blocks can be assigned to the same SM for simultaneous execution. Figure 7: Assignment of a thread block to an SM As there are a limited number of SMs and large kernels can have a very large number of blocks, not all the blocks may get assigned for execution immediately. The GPU maintains a list of blocks which are waitlisted for assignment and execution. As and when any block finishes execution, the GPU assigns one of the waitlisted blocks for execution. 3. Single Instruction Multiple Threads (SIMT) and Warps We know that all threads of a block are assigned to the same SM. But there‚Äôs another level of division of threads after this. These threads are further grouped into sizes of 32, which is called a warp (called a warp2), and assigned together for execution on a set of cores called a processing block. The SM executes all the threads within a warp together by fetching and issuing the same instruction to all of them. These threads then execute that instruction simultaneously, but on different parts of the data. In our vector addition example, all the threads in a warp might be executing the add instruction, but they would be operating on different indices of the vectors. This execution model of the warp is also called single instruction multiple threads (SIMT) because multiple threads are executing the same instruction. It is similar to the single instruction multiple data (SIMD) instructions in CPUs. There is an alternative instruction scheduling mechanism available in newer generations of GPUs, starting from Volta and onwards, known as independent thread scheduling. It allows full concurrency between threads, regardless of warp. It can be used to make better use of the execution resources, or as a synchronization mechanism between threads. We will not cover independent thread scheduling here, but you can read about it in the CUDA programming guide. 4. Warp Scheduling and Latency Tolerance There are some interesting details about how warps work, that are worth discussing. Even if all the processing blocks (groups of cores) within an SM are handling warps, only a few of them are actively executing instructions at any given moment. This happens because there are a limited number of execution units available in the SM. But some instructions take longer to complete, causing a warp to wait for the result. In such cases, the SM puts that waiting warp to sleep and starts executing another warp that doesn't need to wait for anything. This enables the GPUs to maximally utilize all the available compute and deliver high throughput (Little‚Äôs law again in action here). Zero-overhead Scheduling: As each thread in each warp has its own set of registers, there is no overhead for the SM to switch from executing one warp to another. This is in contrast to how context-switching between processes happens on the CPU. If a process is waiting for a long running operation, the CPU schedules another process on that core in the meanwhile. However, context switching in CPU is expensive because the CPU needs to save the registers into main memory, and restore the state of the other process. 5. Copying of Result Data From Device to Host Memory Finally, when all the threads of the kernel have finished executing, the final step is to copy the result back to the host memory. Although we covered everything about a typical kernel execution but there is one more thing that requires its own section: dynamic resource partitioning. Resource Partitioning and the Concept of Occupancy We measure the utilization of the GPU resources through a metric called ‚Äúoccupancy‚Äù, which represents the ratio of the number of warps assigned to an SM to the maximum number it can support. To achieve maximum throughput, we would want to have 100% occupancy. However, in practice it is not always possible due to various constraints. So, why can't we always reach 100% occupancy? The SM has a fixed set of execution resources, including registers, shared memory, thread block slots, and thread slots. These resources are dynamically divided among threads based on their requirements and the GPU's limits. For example, on the Nvidia H100, each SM can handle 32 blocks, 64 warps (i.e., 2048 threads), and 1024 threads per block. If we launch a grid with a block size of 1024 threads, the GPU will split the 2048 available thread slots into 2 blocks. Dynamic vs Fixed partitioning: Dynamic partitioning allows for more effective usage of the computation resources in the GPU. If we compare this with a fixed partitioning scheme where each thread block receives a fixed amount of execution resources it might not always be the most efficient. In some cases the threads might be assigned more resources than they need, leading to wastage of resources and reduced throughput. Now, let's look at an example to see how resource allocation can affect the occupancy of an SM. If we use a block size of 32 threads and need a total of 2048 threads, we'll have 64 of these blocks. However, each SM can only handle 32 blocks at once. So, even though the SM can run 2048 threads, it will only be running 1024 threads at a time, resulting in a 50% occupancy rate. Similarly, each SM has 65536 registers. To execute 2048 threads simultaneously, each thread can have a maximum of 32 registers (65536/2048 = 32). If a kernel needs 64 registers per thread, we can only run 1024 threads per SM, again resulting in 50% occupancy. The challenge with suboptimal occupancy is that it may not provide the necessary tolerance for latency or the required compute throughput to reach the hardware‚Äôs peak performance. Efficiently creating GPU kernels is a complex task. We must allocate resources wisely to maintain high occupancy while minimizing latency. For example, having many registers can make code run quickly but might reduce occupancy, so careful code optimization is important. Summary I understand that wrapping your head around so many new terms and concepts is daunting. Let‚Äôs summarize the key points for a quick review. A GPU consists of several streaming multiprocessors (SM), where each SM has several processing cores. There is an off chip global memory, which is a HBM or DRAM. It is far from the SMs on the chip and has high latency. There is an off chip L2 cache and an on chip L1 cache. These L1 and L2 caches operate similarly to how L1/L2 caches operate in CPUs. There is a small amount of configurable shared memory on each SM. This is shared between the cores. Typically, threads within a thread block load a piece of data into the shared memory and then reuse it instead of loading it again from global memory. Each SM has a large number of registers, which are partitioned between threads depending on their requirement. The Nvidia H100 has 65,536 registers per SM. To execute a kernel on the GPU, we launch a grid of threads. A grid consists of one or more thread blocks and each thread block consists of one or more threads. The GPU assigns one or more blocks for execution on an SM depending on resource availability. All threads of one block are assigned and executed on the same SM. This is for leveraging data locality and for synchronization between threads. The threads assigned to an SM are further grouped into sizes of 32, which is called a warp. All the threads within a warp execute the same instruction at the same time, but on different parts of the data (SIMT). (Although newer generations of GPUs also support independent thread scheduling.) The GPU performs dynamic resource partitioning between the threads based on each threads requirements and the limits of the SM. The programmer needs to carefully optimize code to ensure the highest level of SM occupancy during execution. Wrapping Up GPUs are in pervasive use today, but their architecture and execution model is fundamentally very different from CPUs. In this article we covered various aspects of GPUs, including their architecture and their execution model. If you're curious about what makes GPUs so sought after and how they operate, I hope this article has provided some valuable insights. If you have any questions about what we discussed here, feel free to ask them in the comments, or reach out to me on Twitter/X. I would also like to give a shout out to our community. This topic was requested by many of you and I had to cover it. You can also request me to write about a topic of your choice by becoming a paid subscriber. Subscribe Acknowledgement I would like to thank Vikram Sharma Mailthody, who is a Senior Research Scientist at Nvidia for reviewing and offering insights on various parts of the article. His feedback helped improve the quality of the article significantly. I am very grateful. Vikram is very interested in increasing awareness about GPU programming, so if you are interested in learning more about this area, reach out to him on Twitter or LinkedIn. Further Resources If you want to dive deeper into GPUs, here are few resources you could refer to: Programming Massively Parallel Processors: 4th edition is the most up-to-date reference, but earlier editions are fine, too. Programming Massively Parallel Processors: online course by Prof. Hwu Nvidia‚Äôs CUDA C++ Programming Guide How GPU Computing Works (YouTube) GPU Programming: When, Why and How? For Little‚Äôs law refer to section 2.2 of the paper ‚ÄúGPU-Initiated On-Demand High-Throughput Storage Access in the BaM System Architecture‚Äù Share 1 Yes, CPUs can do things in parallel too, thanks to hyper-threading and multicore. However, for a long time, a lot of effort went into improving the performance of sequential execution. 2 On the current generation of Nvidia GPUs, the warp size is 32. But it may change in future iterations of the hardware. 47 Likes ¬∑ 8 Restacks 47 3 Share Previous 3 Comments Gautam Kumar 6 hrs ago Liked by Abhinav Upadhyay Nice article, refreshed my 2016 memory of CUDA programming. LIKE (1) REPLY SHARE Nat Writes The AI Observer Oct 20 Liked by Abhinav Upadhyay Keep up the great job üëè LIKE (1) REPLY SHARE 1 reply by Abhinav Upadhyay 1 more comment... Top New Community A Tutorial Guide to Using The Function Call Feature of OpenAI's ChatGPT API Learn How to Use OpenAI's ChatGPT API's Function Call Feature to Build Powerful ChatGPT Plugins JUN 15 ‚Ä¢ ABHINAV UPADHYAY 20 13 How CPython Implements and Uses Bloom Filters for String Processing Inside CPython's Clever Use of Bloom Filters for Efficient String Processing SEP 14 ‚Ä¢ ABHINAV UPADHYAY 24 4 Understanding Immortal Objects in Python 3.12: A Deep Dive into Python Internals A detailed examination of Python 3.12's internal changes featuring the concept of 'immortal' objects, for performance enhancements AUG 25 ‚Ä¢ ABHINAV UPADHYAY 19 See all Ready for more? Subscribe ¬© 2023 Abhinav Upadhyay Privacy ‚àô Terms ‚àô Collection notice Start Writing Get the app Substack is the home for great writing",
    "commentLink": "https://news.ycombinator.com/item?id=37967126",
    "commentBody": "What every developer should know about GPU computingHacker NewspastloginWhat every developer should know about GPU computing (codeconfessions.substack.com) 433 points by Anon84 19 hours ago| hidepastfavorite157 comments dang 5 hours agoSomeone emailed to complain about this:https:&#x2F;&#x2F;twitter.com&#x2F;abhi9u&#x2F;status&#x2F;1715753871564476597That is against HN&#x27;s rules. In fact, it&#x27;s the one thing that&#x27;s important enough to be in both the site guidelines and FAQ. HN users feel extremely strongly about this.Q: Can I ask people to upvote my submission?A: No. Users should vote for a story because they personally find it intellectually interesting, not because someone has content to promote. We penalize or ban submissions, accounts, and sites that break this rule, so please don&#x27;t.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsfaq.htmlDon&#x27;t solicit upvotes, comments, or submissions. Users should vote and comment when they run across something they personally find interesting‚Äînot for promotion.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html reply abhi9u 3 hours agoparentSorry, I was not aware of this rule. Although it was submitted by someone I don&#x27;t know.But I won&#x27;t do it again now that I know. reply 01100011 16 hours agoprev> Copying Data from Host to DeviceSurprised there&#x27;s no mention of async copies here. If you want to get the most out of the GPU, you don&#x27;t want it idle when copying data between the host and the GPU. Many frameworks provide for a mechanism to schedule async copies which can execute along side async work submission.The post is sort of GPU 101 but there&#x27;s a whole world of tricks and techniques beyond that once you start doing real-world GPU programming where you want to squeeze as much out of the expensive GPU as possible. Profiling tools help a lot here because, like much of optimizing now, there are hidden cliffs and non-linearities all over that you have to be aware of. reply nine_k 12 hours agoparentSince you likely use 64-bit (double) floats, not every GPU would help much, especially compared to a beefy CPU.But if you use a GPU with a large number of FP64 units, it may speed things up a lot. These are generally not gaming GPUs, but if you have a 4060 sitting around anyway, it has about 300 GFLOPS FP64 performance, likely more than your CPU. Modern CPUs are mighty in this regard though, able to issue many FP64 operations per clock per core. reply 01100011 7 hours agorootparentDid you reply to the wrong comment? reply permo-w 15 hours agoprev>Most programmers have an intimate understanding of CPUsmaybe this article is brilliant, but when the first line is something so blatantly untrue it really makes it hard to take the rest seriously reply PTOB 15 hours agoparentTry this on: \"A non-trivial number of Computer Scientists, Computer Engineers, Electrical Engineers, and hobbyists have ...\"Took some philosophy courses for fun in college. I developed a reading skill there that lets me forgive certain statements by improving them instead of dismissing them. My brain now automatically translates over-generalizations and even outright falsehoods into rationally-nearby true statements. As the argument unfolds, those ideas are reconfigured until the entire piece can be evaluated as logically coherent.The upshot is that any time I read a crappy article, I&#x27;m left with a new batch of true and false premises or claims about topics I&#x27;m interested in. And thus my mental world expands. reply smokel 14 hours agorootparentThat&#x27;s a refreshing take.I tried real hard to understand what some continental philosophers, such as Latour, Deleuze, and ≈Ωi≈æek are on about, giving some of their texts quite some benefit of the doubt. After about ten years of doing so, I am more and more returning to my previous opinion that some of these just like to talk, and even though they have lots to say, they say so much, that I still have to do all the actual philosophizing myself. reply Karrot_Kream 10 hours agorootparentPhilosophy is often called the \"Great Conversation\" because, as you say, a lot of it is just people \"saying things\". Some of these things are incredibly insightful others are just babbling, the ones that make it into books generally have a higher ratio of insight to babble. You need to read philosophy, especially continental philosophy, with a critical eye. If you aren&#x27;t chewing on and contemplating (and calling \"bullshit\") on philosophy as you read it, you aren&#x27;t getting out of it what you should. reply Dylan16807 4 hours agorootparentIt&#x27;s one thing to disagree, but if there&#x27;s a bunch of content-free words that&#x27;s just bad writing. reply salawat 14 hours agorootparentprevThe Ur subject of philosophy is not the writing and reading of philosophy, but to exercise the mind in coming to grips with that which we have no conception of yet. Language is it&#x27;s medium, and the IO is those texts. However the nugget at the center is figuring out that magic blackbox that lets you take all the dead ends in those texts and divine a reasonable guiding principle or recognition of an end to the space of tread discourse.To that end, you have to approach philosophical reading with a degree of hypothetical detachment that not everyone is immediately comfortable with. reply crooked-v 13 hours agorootparentThe Tao Te Ching opens with almost exactly that subject... though, of course, it also makes you think about it a while first to understand what it&#x27;s getting at. reply runlaszlorun 12 hours agorootparentAha, nice connection! I&#x27;m a fan of the Tao Te Ching but never would&#x27;ve caught that one my own. Thx! reply heyoni 11 hours agorootparentprevI feel like that would slow down my reading so hard‚Ä¶and I‚Äôm already a slow reader :( reply anonporridge 15 hours agoparentprevDefinitely not true about most programmers, but maybe the author meant CS educated engineers. Going through a formal CS program will give you an intimate understanding of CPUs, especially when compared to the very light coverage of GPUs. reply p1esk 8 hours agorootparentGoing through a formal CS program will give you an intimate understanding of CPUsPlease tell me you forgot the &#x2F;s.I have a PhD in computer engineering from a top-20 school in US. Took a bunch of grad level classes, passed the quals (my specialty was ML accelerators).I do NOT have an ‚Äúintimate understanding of CPUs‚Äù. I probably know a little bit more about CPUs than an average programmer. Which is very little.Modern CPUs are extremely complex. Almost as much of impenetrable black boxes as modern neural networks. reply gmadsen 5 hours agorootparentI think the intention of the phrase, is that most cs programs include a systems course. you will learn von neumann arch, with cpu including {registers, clock, ALUs, etc}. Obviously modern CPUs have a lot more complexity, but even the basic coverage of CPUs is more coverage than GPUS. reply p1esk 4 hours agorootparentEverything you learn about CPUs in your CS undergrad program is applicable to GPUs. Both are examples of von Neumann architecture, both have registers, ALUs, ISA, instruction schedulers, cache hierarchy, external DRAM, etc. The main difference is how they are being used - GPU, while a general purpose computer, is typically used as an accelerator for specific workloads, and it needs CPU to function. The distinction between the two is getting blurred as GPUs get better at executing control flows, and CPUs get more cores and wider vector operations. reply tester756 11 hours agorootparentprev\"Going through a formal CS program will give you an intimate understanding of CPUsand 101 other hilarious jokes you can tell yourself!\" reply chpatrick 11 hours agorootparentDepends on the university!In my school to pass the computer architecture course you had to read and present a recent paper on CPU design. reply pezezin 1 hour agorootparentIn my school we had to implement a toy 16-bit CPU in VHDL. reply PeterisP 8 hours agorootparentprevWhich most of these students will have entirely forgotten a few years after graduation - use it or lose it. reply tantalor 7 hours agorootparentprevThat&#x27;s a lot to ask of an undergrad reply latency-guy2 6 hours agorootparentMaybe a modern one for a degree mill that cares not for breadth or depth. It really shouldn&#x27;t be otherwise.I&#x27;m very confident in people having the ability to read by the time they are in college. And considering that summarizing a research paper doesn&#x27;t even have to be perfect, plus very little need to scrutinize the experiment itself, undergrads should be able to do that.Otherwise they don&#x27;t belong in college. reply croes 15 hours agorootparentprevMaybe a generation thing, I remember reading Michael Abrash&#x27;s Graphics Programming Black Book and learning a lot about cycle eaters in the CPU. reply jandrewrogers 8 hours agorootparentprevFor anyone that needs to know how a CPU works for performance engineering similar to a GPU, the details of the microarchitecture matter a lot even within the same ISA. I am not aware of any formal CS program that teaches anyone the nuanced internals of various microarchitecture designs. Everyone I know with this knowledge appears to be self-taught regardless of where they went to school.I think the descriptor ‚Äúintimate‚Äù is overstating the case if you don‚Äôt know how to optimize code on different implementations of the same ISA. Most formal CS programs give you a generic understanding of CPUs, more like a survey course, not enough information to do serious optimization. reply pca006132 5 hours agorootparentI am really curious about what kind of optimizations are enabled when you know the microarchitectural design of various CPUs, but without writing assembly by hand. I only know the basics such as optimizing data structure for better cache locality, add some fast paths, manual unrolling, but have no idea about how to work around things like pipeline stalls. It would be really helpful if you can point to some materials about this! reply usea 14 hours agorootparentprev> Going through a formal CS program will give you an intimate understanding of CPUsI did an undergrad in CS, where I did well. I don&#x27;t feel like I understand CPUs very well. Certainly not anywhere in the realm of \"intimate.\" reply galangalalgol 13 hours agorootparentprevAre most programmers self trained now? I can see if someone self trains for fe or even full stack with an eye on compensation, they wouldn&#x27;t understand program counters and the like. But so many people seem motivated by video games to get into the industry that I&#x27;d expect them to be reading about things like the fast inverse square root or similar. reply crooked-v 13 hours agorootparentA lot of game dev now happens in high-level engines, often with their own extra layer of scripting engine. Even working on the nitty-gritty of spatial logic rather than using engine-based colliders&#x2F;detection would be pretty rare. reply galangalalgol 12 hours agorootparentSo are we really losing knowledge or just specializing ever more down the fractal? reply hombre_fatal 12 hours agorootparentI think it&#x27;s just that you don&#x27;t need to be intimate with lower level details to deliver more value at higher levels like game and application dev.If your day to day involves \"intimate knowledge with the CPU\" and bumping program counters, I can rule out a lot of things that you probably aren&#x27;t building, like a compelling iOS app or forum HTTP server, for example.Maybe you&#x27;re doing impressive work on emulators or something though. But it&#x27;s nothing to get pompous about just because other people don&#x27;t share that interest.We too easily go off careening into a circlejerk. reply c2lsZW50 12 hours agorootparentprevI believe the answer is depends, but there were some interesting discussions today under an article which relates to this.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37965142 replytwixfel 11 hours agoparentprevI don&#x27;t understand why every other submission on the internet has to have at least one \"stopped reading at X\" comment relating to it. It adds absolutely nothing. reply permo-w 7 hours agorootparentI regret the way I phrased it. it&#x27;s snarky and not in the spirit of curiosity, but I do think it&#x27;s worth discussing that the author thinks that most programmers know how a CPU works reply abadpoli 10 hours agorootparentprevIt provides feedback to the author, and it also has generated quite a significant discussion here about the topic.It could have been worded more constructively sure, but given that the entire point of this website is to have discussions about the material posted, I think it‚Äôs added quite a bit. reply squeaky-clean 9 hours agorootparentIt hasn&#x27;t really generated any discussion about the topic at hand though. Just people discussing what they learned or didn&#x27;t learn about CPUs in school. reply njacobs5074 2 hours agoparentprevAgree that saying \"intimate understanding\" is a bit off the mark. Had the author written \"intuitive understanding\", it would have made a bit more sense.However, given the prevalence of the von Neumann computing architecture, I don&#x27;t think it&#x27;s completely off the mark - even if people don&#x27;t know von Neumann&#x27;s name :) reply moritzwarhier 9 hours agoparentprevI think at least 50% of the answers of this would depend on how one defines \"intimate understanding\"...I learned basic facts about CPU architectures at university, know in a very basic way the landscape of things and occasionaly stumble upon updates to my limited knowledge... but by no means would I say that, rather like \"a basic understanding of how CPUs work &#x2F; are designed &#x2F; are to be used\" (?)If I were proficient in assembler, maybe I&#x27;d claim to have an \"intimate understanding\" of how to use CPUs at a low level (still sounds a bit braggy)It still is not the same though as being an expert in CPU&#x2F;GPU design.So yeah I agree.Article is interesting though, esp. the diagram! reply rjh29 10 hours agoparentprevI learned it both in my degree and the Structure and Interpretation of Computer Programs course (which I recommend to anyone interested in low level computing) reply kalak 15 hours agoparentprevfeels very much like https:&#x2F;&#x2F;xkcd.com&#x2F;2501&#x2F; reply ashu 13 hours agoparentprevAnd this is the most insightful thing you had to say about this?! Pfft. reply Const-me 16 hours agoprev> During execution the registers allocated to a thread are private to it, i.e., other threads cannot read&#x2F;write those registers.Wave intrinsics in HLSL, and similar CUDA things can read registers from different threads within the current wavefront.Also, in the paragraph about memory architecture, I would mention the caches provide no coherency guarantees across threads of the same dispatch&#x2F;grid, but there‚Äôs a special functional block global to the complete chips which implements atomics on global memory. reply paulddraper 15 hours agoprevSIMD programming is f---ing wild.Want to run a calculation for every pixel on your screen? No problem.Want to have a branching condition? Ouchie. reply eimrine 15 hours agoparentWant to have eval? Stop everything. reply touisteur 14 hours agorootparentVectorized emulation is very interesting and fun, Brandon Falk (gamozolabs) has this series of hours-long streams and rust projects, you can start here: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=18222729 reply toppy 16 hours agoprevWhy are they still called GPU? PPU (Parallel Processing Unit) sounds like a better name. reply Salgat 11 hours agoparentBecause they&#x27;re filled with graphics specific silicon in addition to the GPGPU stuff. reply dist-epoch 12 hours agoparentprevBecause everybody understands what you mean when you say GPU.Same with drone versus quad-copter, etc... reply palata 11 hours agorootparentTo be pedantic, a quadcopter is a drone, but a drone is not necessarily a quadcopter. reply dragonwriter 11 hours agorootparentTo be even more pedantic, \"quadcopter\" is a statement about airframe layout, and \"drone\" is a statement about control, and neither necessarily implies the other. You can have a non-drone quadcopter [0] or a non-quadcopter drone [1].[0] e.g., https:&#x2F;&#x2F;www.jetsonaero.com&#x2F;jetson-one[1] e.g., https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;HESA_Shahed_136 reply croes 15 hours agoparentprevCPUs are PPUs too reply rational_indian 11 hours agorootparentA better name for GPUs would then be \"Massively parallel processing units\" or MPPUs. reply toppy 10 hours agorootparentprevTouche! reply speed_spread 15 hours agoparentprevVector Processing Unit would be more appropriate. reply KeplerBoy 15 hours agorootparentWhy stop there? Matrix Processing Unit or Tensor Processing Unit if you want to trigger the physicists or are a fan of the google cloud. reply dragonwriter 11 hours agorootparentThose are a different thing (which may be part of a GPU, e.g., Nvidia&#x27;s Tensor Cores, or separate, e.g., Apple Neural Engine.) reply nologic01 14 hours agorootparentprevDifferential Geometric Processing Unit would do it for me. reply speed_spread 13 hours agorootparentprevThose things actually exists and are called NPU (Neural Processing Unit). Many recent ARM CPU have them (Apple, Qualcomm, etc.) and even new RISC-V CPU. They&#x27;re different from GPU in being even more constrained in programming requiring usage of a fixed function vendor-supplied library. reply duckmysick 14 hours agoparentprevGeneral Processing Unit reply guidedlight 8 hours agoprevOne thing I don‚Äôt understand is how the architecture of Apple Silicon is different from NVidia‚Äôs.Looking at this quote:> the Nvidia H100 GPU has 132 SMs with 64 cores per SM, totalling a whopping 8448 cores.8448 cores sure sounds impressive. But the Apple M2 Ultra only has 76 cores?!How can the NVidia H100 GPU have over 110x more cores? Clearly it doesn‚Äôt have 110x more performance over the M2 Ultra, so what is going on here? reply subharmonicon 8 hours agoparentNVIDIA is intentionally being obtuse and frankly dishonest calling what‚Äôs effectively a vector lane a ‚Äúcore‚Äù and similarly using ‚Äúthread‚Äù in ‚ÄúSIMT‚Äù to mean the execution of one of those vector lanes.Yes, their architecture is different from many in that they support a separate program counter per lane (which is why they feel justified in calling this a ‚Äúthread‚Äù), but ultimately it‚Äôs the rate and throughput of ALUs that matter. reply freeone3000 8 hours agorootparentIt‚Äôs not even a seperate PC per lane, you only get that per block ‚Äî lane level execution goes to an execution mask lut per insn. Branchy code that‚Äôs not branchy uniformly in the block executes a lot of noops. reply sakras 7 hours agorootparentAs of Volta, they have independent PCs with a warp optimizer that dynamically groups threads with the same program counter, so branches aren‚Äôt nearly as bad as they used to be. reply subharmonicon 6 hours agorootparentCan you cite a reference explaining this ability to re-form new warps from existing threads?I ask because I‚Äôve seen posts from NVIDIA support saying that divergence is still very expensive and I‚Äôve also seen benchmarks that force divergence in each warp by evenly splitting the warp, and the benchmarks result in 2x runtime when that happens vs. when the control-flow is dynamically uniform.One thing to keep in mind is that even if you were to dynamically reform-warps, there‚Äôs still a potential expense because you then lose the advantage of doing things like accessing adjacent elements of data in adjacent threads. You‚Äôre bound to now have more bank conflicts, fewer memory accesses being coalesced, etc. Perhaps they do actually do this warp re-formation, but that itself does have this additional cost. reply mr_toad 7 hours agoparentprevFor one thing you can use the H100 to heat a room - it uses more than 10x the power of an M2 Ultra. reply kevingadd 8 hours agoparentprevNVIDIA&#x27;s SMs are most comparable to the &#x27;CUs&#x27; on AMD GPUs or cores on Apple GPUs, generally speaking. The \"cores\" are subsets of the SM that perform individual operations, IIRC.See this diagram from an nvidia blog post: https:&#x2F;&#x2F;developer-blogs.nvidia.com&#x2F;wp-content&#x2F;uploads&#x2F;2021&#x2F;g...( https:&#x2F;&#x2F;developer.nvidia.com&#x2F;blog&#x2F;nvidia-ampere-architecture... ) reply zackmorris 15 hours agoprevThis is a great writeup. And GPUs are more advanced&#x2F;performant for what they do than anything I could ever come up with.But I put SIMD in the category of something that isn&#x27;t necessary once one has learned other (more flexible) paradigms. I prefer MIMD and clusters&#x2F;transputers, which seem to have died out by the 2000s. Today&#x27;s status quo puts the onus on developers to move data manually, write shaders under arbitrary limitations on how many memory locations can be accessed simultaneously, duplicate their work with separate languages for GPU&#x2F;CPU, know if various hardware is available for stuff like ray tracing, get locked into opinionated frameworks like OpenGL&#x2F;Metal&#x2F;Vulkan, etc etc etc. GPUs are on a side tangent that can never get me to where I want to go, so my experience over the last 25 years has been of a person living on the wrong timeline. I&#x27;ve commented about it extensively but it just feels like yelling into the void now.Loosely, a scalable general purpose CPU working within the limitations of the end of Moore&#x27;s law is multicore with local memories, sharing data through a copy-on-write content-addressable memory or other caching scheme which presents a single unified address space to allow the user to freely explore all methods of computation in a desktop computing setting. It uses standard assembly language but is usually programmed with something higher level like Erlang&#x2F;Go, Octave&#x2F;MATLAB or ideally a functional programming language like Julia. 3D rendering and AI libraries are written as a layer above that, they aren&#x27;t fundamental.It&#x27;s interesting that GPUs have arrived at roughly the multicore configuration that I spoke of, but with drivers that separate the user from the bare-metal access needed to do general purpose MIMD. I had thought that FPGAs were the only way to topple GPU dominance, but maybe there is an opportunity here to write a driver that presents GPU hardware as MIMD with a unified memory. I don&#x27;t know how well GPU cores handle integer math, but that could be approximated with the 32 bit int portion of a 64 bit float. Those sorts of tradeoffs may result in a MIMD machine running 10-100 times slower than a GPU, but still 10-100 times faster than a CPU. But scalable without the over-reliance on large caches and fast busses which stagnated CPUs since around 2007 when affordability and power efficiency took priority over performance due to the mobile market taking over. And MIMD machines can be clustered and form distributed compute networks like SETI@home with no changes to the code. To get a sense of how empowering that could be to the average user: it&#x27;s like comparing BitTorrent to FTP, but for compute instead of data. reply mannyv 16 hours agoprevNow I understand why ML uses floats for precision. It wasn&#x27;t a choice, it was because graphics code uses them.Another piece in the \"why is ML so inefficient\" puzzle!I wonder what that memory copying overhead is IRL. If it&#x27;s like normal stuff it&#x27;ll be brutal. I mean, they offload tcp processing into hardware to avoid that. This is way more data, though it is done in bigger chunks. reply oivey 14 hours agoparentI don‚Äôt think the choice of using floating point numbers is particularly inefficient. If frameworks were fixed point by default it would be tricky to get the dynamic ranges right all the way through a network. The math in the training assumes the numbers are continuous, too. reply Dylan16807 4 hours agoparentprevWhat&#x27;s inefficient about floats? ML seems to get a big benefit out of having access to multiple&#x2F;many orders of magnitude. reply choppaface 16 hours agoparentprevFor a lot of larger modern networks the GPU compute time in computing gradients and doing the backwards pass is so slow that copying float data over the pcie bus is no bottleneck. I.e. copying a minibatch of float images is still plenty fast because the gradients &#x2F; SGD iteration is so slow and requires so much compute (even with mixed precision).For shallower networks, there can be an advantage to copying just the original compressed data to GPU memory and then doing decompression etc there. But modern GPUs haven‚Äôt adopted pcie 5 yet because the raw compute is more important.Lastly tensor cores have had a big impact, depending on the network they can be fast enough to be very under-utilized. reply touisteur 14 hours agorootparentI would say that some one modern GPU has adopted PCIe 5.0, the NVIDIA H100 - if you want more (FP32) &#x27;compute&#x27; (and no HBM...) in one GPU your sourceable choice is the L40 which didn&#x27;t get PCIe 5.0. This feels like another market&#x2F;product segmentation (that and L40 didn&#x27;t get nvlink). reply nologic01 14 hours agoprevIt is anybody&#x27;s guess what the future will bring, but on past form GPU programming will remain a niche for specific highly-tuned (HPC) applications and mere mortals can focus on somewhat easier multi-core CPU programming instead.The main reason GPU gets so much attention these days is that CPU manufacturers (Intel in particular) simply cannot get their act together. Intel had promised significant breakthroughs with Xeon Phi like a decade ago.In the meantime people have invented more and more applications that need significant computational power. But it will eventually get there. E.g., AMD&#x27; latest epyc features 96 cores. Importantly, that computational power is available in principle with simpler &#x2F; more familiar programming models. reply Jeff_Brown 18 hours agoprevNot every developer.I&#x27;m not trying to be snarky. I think there&#x27;s an unhelpful compunction to want to know everything about everything among STEM types like programmers (of which I am one). Specialization is fundamental to the success, not just of whole economies, but of the individuals in them. It can feel like a paintful sacrifice to admit that you&#x27;ll never (have time to) learn, say, the entire Python language specification, or how type inference works, or any number of other things someone might tell you is critical knowledge. But it&#x27;s often liberating, and more often than that, mandatory.(Maybe I was trying to be a little snarky initially, but I&#x27;m not any more.) reply yodsanklai 17 hours agoparent> It can feel like a paintful sacrifice to admit that you&#x27;ll never (have time to) learnAnd even if you do have time, you&#x27;ll probably forget most of it if you don&#x27;t make use of it in your daily job. Also if you do want to learn about a new topic, it&#x27;ll take commitment. I remember going through an MIT OS project, it took me something like 50 hours of hours to complete the project. Pretty much impossible when you have a full time job (I didn&#x27;t at the time). And despite that, I still consider myself a newbie in OS development.That being said, a little extra knowledge can come handy and make a difference in an interview for instance, or reduce ramp-up time when changing teams.This is also what school is for. Give you full time and a structured program to pick the fundamentals. There&#x27;s only so much you can learn once you have a demanding job. It&#x27;s actually pretty sad we don&#x27;t get to go back to school in the middle of our career.Edit: still an interesting article ;) reply owl57 17 hours agorootparent> I remember going through an MIT OS project, it took me something like 50 hours of hours to complete the project. Pretty much impossible when you have a full time job (I didn&#x27;t at the time). And despite that, I still consider myself a newbie in OS development.I took an OS course loosely based on MIT one, and of course wouldn&#x27;t consider myself an experienced OS developer as well, but I think the residual knowledge is sometimes really helpful when debugging bad application performance at work. reply abhi9u 17 hours agorootparentprevThank you :) reply loeg 17 hours agoparentprevAt this point, I think the running shtick &#x2F; inside joke of \"Every Developer Should Know ...\" headlines is that of course every developer doesn&#x27;t need to know the contents of the article that follows. reply wongarsu 17 hours agorootparent> of course every developer doesn&#x27;t need to know the contents of the article that follows.Things every developer should know about English, part 1: why word order matters in negation &#x2F;sObviously you meant \"not every developer needs to know\" instead of \"every developer doesn&#x27;t need to know\", but I see this switch so often lately that I&#x27;m beginning to wonder if it&#x27;s a dialect thing (similar to double negative implying a positive to some and an emphasis of the negative to others) reply a1369209993 16 hours agorootparentActually, that seems to be a legitimate precedence ambiguity. The probably more common way to parse&#x2F;interpret it is:> [Foreach developer D, D] needs to know this.> [Foreach developer D, D] doesn&#x27;t need to know this. (No developers.)But you could also have:> [The logical conjuction of all developers[0]] needs to know this.> [The logical conjuction of all developers] doesn&#x27;t need to know this. (Not all developers.)The former convention is clearly better unless I&#x27;ve missed something rather significant, but inherently broken features in a number of programming languages (eg perl 6 &#x27;junctive&#x27; operators) suggest the latter is well-established as a thing that exists.0: ie, a linguistic fiction that has only those properties &#x2F; needs only those things that are common to every developer. reply bachmeier 17 hours agorootparentprevNot true. Anyone experienced in programming discussions knows that \"software development\" overlaps exactly with the programming done by the commenter, and everything else is solving toy problems with tools that don&#x27;t scale. reply arp242 15 hours agorootparentprevI&#x27;m okay if \"every\" just means \"most\"; for example Spolsky&#x27;s old \"what every developers needs to know about Unicode\" (which is the origin of this, AFAIK) is fine because most (but not all) developers will need to deal with text sooner or later.But in this case it doesn&#x27;t even mean that: all of this applies to a substantial minority of programmers, but definitely a minority. reply LouisSayers 9 hours agorootparentprevThere are at most 10 things every developer should know.The first being number base systems. reply Agingcoder 16 hours agoparentprevI‚Äôd say very few actually ( and I say this from the perspective of someone who used to work in hpc ). If they need to know about hw, most devs need to know about their primary platform, ie cpu. Gpus for general purpose computing ( I‚Äôm deliberately excluding games here, and even then it‚Äôs not obvious ) and programmed by people who don‚Äôt write ml&#x2F;hpc libraries are far from ubiquitous.Yes, you want to know as much as possible ( helps debugging&#x2F;zooming in on issues since you don‚Äôt need to introduce an outsider to your problem, helps avoid errors), yes you need to specialize somewhere, no you can‚Äôt know everything and often don‚Äôt need to reply nabla9 18 hours agoparentprevThis is just minimum for those don&#x27;t need to know details.Even general knowledge has depth. Even if you are generalist, or specialist in different area you should deepen your knowledge at every area gradually. reply abhi9u 17 hours agorootparentTrue. This is probably what first few lectures of a first course on GPU computing would cover. The nitty gritty of how to write parallel algorithms for maximum throughput on the GPU is where things become too specific. :) reply pjmlp 17 hours agoparentprevIndeed, one thing that most seniors learn is humility and being able to say I don&#x27;t know, without caring about consequences. reply bobwaycott 17 hours agorootparentAn invaluable life skill, as well. reply nativeit 16 hours agoparentprevI can appreciate what you‚Äôre getting at in mourning the absence of greater opportunities for in-depth learning, but I personally value and appreciate the learning process such that I am overwhelmed by gratitude that I will likely never be without something to hold my interest. I have a deep understanding of the things I use in my daily work, but I think holding a breadth of knowledge is also useful in that you have a higher appreciation for what other specialists know and do, and in the event you need something novel, you may have a head start in getting to the knowledge you need at the time. I view it as unreservedly positive to audit many subjects even if you cannot engage with them further. reply cj 17 hours agoparentprevOne thing I find interesting about the software industry is our lack of descriptive job titles.At big companies you have SWE L1, L2, L3, senior, staff, principal, etc. You also have SRE and maybe some devops or architect roles.Smaller companies you have lots of people generic ‚Äúengineer‚Äù titles or ‚Äúfull stack engineers‚Äù, etc.Why don‚Äôt we encode people‚Äôs specialties in their titles if most engineers are working on narrow sections of software?E.g. ‚ÄúReact & Node Developer‚Äù instead of ‚ÄúFull stack engineer‚Äù ‚Ä¶ etcI suppose the easiest rationale is generic titles allow for easier mobility between disciplines. reply cratermoon 17 hours agoparentprevI dunno. The section \"Latency Tolerance, High Throughput and Little‚Äôs Law\" has many applications in programming. Ever need to scale your cache, or size a connection pool? Little&#x27;s Law. reply abhi9u 17 hours agorootparentThank you for reading! :) reply cratermoon 17 hours agorootparentThanks for writing this. I note you mention that GPUs are massively parallel, and I think it would strengthen the article to add a paragraph or two discussing what kinds of loads lend themselves to GPU computing. The sort of \"embarrassingly parallel\" things like linear algebra vs inherently serial example of numerical analysis. reply abhi9u 16 hours agorootparentYes, that&#x27;s a good feedback. Few other people also mentioned this. I was trying to save space, to spend time more on the \"how things work\" part. But I see your point that some motivation behind parallelization would make it a more interesting read. reply Jiro 18 hours agoparentprevOver-specialization is hard on your ability to find another job if you lose your current one. reply andrewSC 17 hours agorootparentAnd so it‚Äôs a balance :^) I personally take the approach of specializing in what I believe to be my natural talents (i.e. seemingly able to pick up X much easier than whoever) while just being generally aware of new technologies or other spaces&#x2F;sectors. Also tinkering and&#x2F;or having a general interest in tech helps lol.. IME doing this has helped me pivot into a new tech or other areas where I may not be specialized in, but may need to eventually be for a new role. Also worth mentioning is taking a pragmatic approach to problem solving in general.. I‚Äôve personally found that if you‚Äôre able to demonstrate solid reasoning and&#x2F;or problem solving, generally learning a new lang or specializing in something different than what you‚Äôre used to, isn‚Äôt too too bad. I‚Äôm not really sure how specialists outside of tech&#x2F;in other sectors can transition into other roles ‚Äúeasily‚Äù though‚Ä¶ hm.Edit: I wanted to mention that IME in STEM, most? More often than not? are goal oriented‚Ä¶ It is _completely_ okay to not have any ‚Äúworking‚Äù thing at the end of your tinkering&#x2F;learning‚Ä¶ The journey into that ‚Äúthing‚Äù can be a learning experience in and of itself. I‚Äôve often started to learn things but after a certain point have told myself ‚ÄúI‚Äôm gonna stop here and that‚Äôs okay. I don‚Äôt need to have a solid understanding of this, at this time‚Äù. YMMV‚Ä¶ reply Jeff_Brown 17 hours agorootparentprevCompletely true. The analogy to hydration holds, I think -- too much and too little can both be problematic. But I see seem to run into much more under-specialization than over-specialization. More than 90% of the wildest successes I know went really deep on something pretty narrow. reply HPsquared 17 hours agorootparentSpecialization is like putting everything on one number in roulette. It&#x27;s great if you win (i.e. your specialty is in demand), but you&#x27;re more likely to lose everything you invested. reply crabbone 16 hours agoparentprevYou are interpreting the title literally, while all this article is trying to do is to give an introduction to anyone who wants to get into GPU programming.Now, about every developer. Ideally, developers should know something, in general, about all fields related to programming. Similar to how in medicine you need to learn about different branches of medicine, even though you&#x27;ll specialize in one, and so it is in mathematics, physics and so on.Presently, the demand for quality professionals in programming is very low. There aren&#x27;t any good testing or certification programs that can tell a good programmer from a bad one. The industry is generally happy with \"specialists\" who perhaps only know to do one thing, somewhat. So, presently you don&#x27;t need to know anything about GPU or any other field that&#x27;s not directly related to your job description.----Now, about the article itself. While it gives a lot of valuable factual information, it&#x27;s missing the forest for the trees. It&#x27;s very dedicated to how CUDA works or some other particular aspects of NVidia&#x27;s GPUs. The part that&#x27;s missing is the part that could make it, potentially, a candidate for the kind of introduction to GPU programming that would make it worth reading to expand your general understanding of how computers work.If you ever paid attention to how encyclopedic articles are written: the structure of a definition given by encyclopedia often has two components. First puts the object of the definition into the more general category, second explains how the object of the definition is different from other elements of the category. What the GPU article is missing is the first component: putting GPU programming into the more general category. This, in practical terms, means that questions like \"is DPU programming anything like GPU programming?\" or \"can `smart&#x27; SSDs (with FPGAs) be treated similar to GPUs?\" unanswered. reply qwertox 13 hours agoprevLet&#x27;s assume I have an array of 10.000 lat&#x2F;lng-pairs. I want to compute the length of the track. I duplicate the array and remove the first item in the duplicated array, append the last entry of the original array to the duplicate in order for them to be equal in length.Then I use a vectorized haversine algorithm on these arrays to obtain a third one with the distances between each \"row\" of the two arrays.With NumPy this is fast, but I guess that a GPU could also perform this and likely do it much faster. Would it be worth it if one has to consider that the data needs to be moved to the GPU and the result be retrieved? reply kylebarron 12 hours agoparent10,000 coordinates are certainly not enough to see the difference, but at some scale this would be faster on the GPU.This is implemented in an nvidia geospatial library call cuspatial: https:&#x2F;&#x2F;docs.rapids.ai&#x2F;api&#x2F;cuspatial&#x2F;legacy&#x2F;api_docs&#x2F;spatial... reply rational_indian 13 hours agoparentprevYou need to try it and see if it is any faster. NVIDIA has a drop-in replacement for numpy: https:&#x2F;&#x2F;developer.nvidia.com&#x2F;cunumeric. reply tlb 13 hours agoparentprevProbably not. The computation is only a few trig instructions per array element, so most of the time is moving data on either CPU or GPU. reply dataflow 12 hours agoparentprevNot the answer to your question, but:> I duplicate the array and remove the first item in the duplicated array, append the last entry of the original array to the duplicate in order for them to be equal in length.I assume&#x2F;hope this is only what you&#x27;re doing logically, not physically?Otherwise you might as well just compute the length using n - 1 points from the existing array, then do the remaining portion manually and add it to the existing sum. That would avoid the copying of the whole array. reply dist-epoch 13 hours agoparentprev> Would it be worth it if one has to consider that the data needs to be moved to the GPU and the result be retrievedDepends on your batch size. If the computation on the CPU is less than lets say 200 ms it&#x27;s probably not worth it.Also consider that integrated GPUs don&#x27;t have separate memory, I&#x27;m not sure, but they might not have a high cost of moving data to memory. reply rakkhi 9 hours agoprevImagine with NVIDIA banned in china, how well the Chinese local companies will do in GPU&#x27;s for AI: https:&#x2F;&#x2F;x.com&#x2F;BeijingDai&#x2F;status&#x2F;1715861773495279743?s=20 reply kevingadd 8 hours agoparentIt&#x27;s going to take them a while to catch up to CUDA, though. Even with stolen IP it&#x27;s going to be tough to make 1:1 drop-in replacements. The amount of existing investment into AMD and NVIDIA architectures is huge, as evidenced by how bad the Moore Threads GPUs are - it simply isn&#x27;t easy to enter the market as a serious competitor. Even Intel is struggling. reply jokoon 14 hours agoprevI wish it was easier to program a GPU...I&#x27;ve already refrained myself to learn vulkan because it scares me, but similarly, opengl and cuda are a bit mysterious to me, and I don&#x27;t really know how I could take advantage of it, since most computing tasks cannot be made parallel.I&#x27;ve read there are data structures that are somehow able to take advantage of a GPU as an alternative to the CPU (for example a database running on a GPU), but it seems a very new domain, and I don&#x27;t have the skill to explore it. reply HalfCrimp 8 hours agoparentIf you are comfortable with C++ already then look at Thrust. It&#x27;s nvidia&#x27;s analogue to the standard library arrived at GPU computing.Writing and launching raw cuda kernels is too low level for me, but writing with Thrust makes it feel pretty similar to writing regular C++ code. You still need to deal with moving data from host to device and back, but that&#x27;s as simple as assigning a `thrust::device_vector` to a `thrust::host_vector` reply rnrn 6 hours agorootparentor look at using standard C++ with an implementation that uses the GPU: https:&#x2F;&#x2F;docs.nvidia.com&#x2F;hpc-sdk&#x2F;compilers&#x2F;c++-parallel-algor... reply kdwikzncba 9 hours agoprev> We can understand this with the help of Little‚Äôs law from queuing theory. It states that the average number of requests in the system (Qd for queue depth) is equal to the average arrival rate of requests (throughput T) multiplied by the average amount of time to serve a request (latency L).First off, this is obviously false. If you can serve 9req&#x2F;s and you&#x27;re getting 10req&#x2F;s the size of the queue depth is growing at a rate of 1req&#x2F;s. It&#x27;s not stationary.Second, what&#x27;s the connection between this and gpus? What&#x27;s the queue? What&#x27;s the queue depth? What are the requests?Seems to me that the article focuses more on being smart than actually learning. reply minitoar 9 hours agoparentThe scenario is that you‚Äôre calculating Qd given a static average latency. Absent that, this formula doesn‚Äôt give you a way to compute Qd. What is the average amount of time to service a request in a system where the queue depth is growing without bound? reply ssivark 9 hours agoparentprev> First off, this is obviously false. If you can serve 9req&#x2F;s and you&#x27;re getting 10req&#x2F;s the size of the queue depth is growing at a rate of 1req&#x2F;s. It&#x27;s not stationary.I haven‚Äôt formally studied any queuing theory, but I think:1. The rule assumes you have enough processing power to service the average load (otherwise it fails catastrophically like you mentioned)2. The rule is trying to model the fluctuations in the pending load (which might determine wait time or whatever else). reply RevEng 10 hours agoprevThis is one of the better explanations I&#x27;ve seen of how GPU programming works. I&#x27;ll be using this for my mentees in the future. Well done! reply osigurdson 13 hours agoprev>> Most programmers have an intimate understanding of CPUsI&#x27;d say the mental model for most programmers is: lines of text, in their language of choice, zipping by really fast. reply jacquesm 11 hours agoparentBy the time you&#x27;re looking at using GPUs my assumption would be that you&#x27;ve left that particular mental model behind long ago. reply quickthrower2 13 hours agoparentprevLike an interpreted language I suppose reply markhahn 7 hours agoprevit&#x27;s not just nvidia-specific, but nvidia-biased. the misuse of \"core\", pretending that CPUs are backwards, etc.every developer should know about more than nvidia&#x27;s spin. reply fatih-erikli 12 hours agoprevI think GPU computing should not be done in application layer. It&#x27;s way too low-level. reply jacquesm 11 hours agoparentThat&#x27;s the only layer where it makes sense because that&#x27;s where you know what it is that you are trying to achieve. The overhead in GPU programming is such that if you make one small assumption that doesn&#x27;t hold true in practice you may end up sinking your performance in a terrible way. So you need a lot of control over where and how things are laid out. For more generic stuff there are libraries, but those too run at the behest of your application. As this technology matures you&#x27;ll see more and more abstraction and automation of the parts that squeeze out the most performance. But for now that&#x27;s where you can make the biggest gains, just like any other kind of special purpose co-processor. reply diimdeep 12 hours agoprevAlso check out this talk and slides from few years ago about CPU and GPU nitpicksAlexander Titov ‚Äî Know your hardware: CPU memory hierarchy https:&#x2F;&#x2F;youtu.be&#x2F;QOJ2hsop6hMhttps:&#x2F;&#x2F;github.com&#x2F;alexander-titov&#x2F;public&#x2F;blob&#x2F;master&#x2F;confer...Know Your Hardware - CPU Memory Hierarchy -- Alexander Titov -- C%2B%2B Moscow Meetup March 2019.pdfhttps:&#x2F;&#x2F;github.com&#x2F;alexander-titov&#x2F;public&#x2F;blob&#x2F;master&#x2F;confer...GPGPU - what it is and why you should care -- Alexander Titov -- CoreHard 2019.pdf reply godelski 13 hours agoprevI thought I&#x27;d share something with my experience with HPC that applies to many areas, especially in the rise of GPUs.The main bottleneck isn&#x27;t compute, it is memory. If you go to talks you&#x27;re gonna see lots of figures like this one[0] (typically also showing disk speeds, which are crazy small).Compute is increasing so fast that at this point we finish our operations long faster than it takes to save those simulations or even create the visualizations and put on disk. There&#x27;s a lot of research going into this, with a lot of things like in situ computing (asynchronous operations, often pushing to a different machine, but needing many things like flash buffers. See ADIOS[1] as an example software).What I&#x27;m getting at here is that we&#x27;re at a point where we have to think about that IO bottleneck, even for non-high performance systems. I work in ML now, which we typically think of as compute bound, but being in the generative space there are still many things where the IO bottlenecks. This can be loading batches into memory, writing results to disk, or communication between distributed processes. It&#x27;s one beg reason we typically want to maximize memory usage (large batches).There&#x27;s a lot of low hanging fruit in these areas that aren&#x27;t going to be generally publishable works but are going to have lots of high impact. Just look at things like LLaMA CPP[2], where in the process they&#x27;ve really decreased the compute time and memory load. There&#x27;s also projects like TinyLLaMa[3] who are exploring training a 1B model and doing so on limited compute, and are getting pretty good results. But I&#x27;ll tell you from personal experience, small models and limited compute experience doesn&#x27;t make for good papers (my most cited work did this and has never been published, gotten many rejections for not competing with models 100x it&#x27;s size, but is also quite popular in the general scientific community who work with limited compute). Wfiw, companies that are working on applications do value these things, but it is also noise in the community that&#x27;s hard to parse. Idk how we can do better as a community to not get trapped in these hype cycles, because real engineering has a lot of these aspects too, and they should be (but aren&#x27;t) really good areas for academics to be working in. Scale isn&#x27;t everything in research, and there&#x27;s a lot of different problems out there that are extremely important but many are blind to.And one final comment, there&#x27;s lots of code that is used over and over that are not remotely optimized and can be >100x faster. Just gotta slow down and write good code. The move fast and break things method is great for getting moving but the debt compounds. It&#x27;s just debt is less visible, but there&#x27;s so much money being wasted from writing bad code (and LLMs are only going to amplify this. They were trained on bad code after all). And no, pytorch isn&#x27;t going to optimize everything for you automagically (no one can. Optimization is often situationally dependent). You&#x27;re still gonna need to understand the distributed package and it is worth learning MPI.[0] https:&#x2F;&#x2F;drivenets.com&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;05&#x2F;blog-networ...[1] https:&#x2F;&#x2F;github.com&#x2F;ornladios&#x2F;ADIOS2[2] https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp[3] https:&#x2F;&#x2F;github.com&#x2F;jzhang38&#x2F;TinyLlama reply jacquesm 11 hours agoparentAgreed, the memory bottle-neck is the biggest restriction. But even there there is quite a bit of progress. NV in particular is milking that for as much and as long as they can though. I&#x27;d be happy with a slower GPU able to address far more memory. reply fancyfredbot 12 hours agoprevThis article is pretty good but, looking at comments so far, nobody seems to have made the obvious&#x2F;predictable point that it&#x27;s terribly Nvidia specific. That would be understandable perhaps a few years ago. But the era where there was no reasonable alternative is over. Sycl is a good language which performs well across multiple hardware vendors. Sapphire rapids is very good hardware. AMD&#x27;s MI300 looks amazing. Nvidia has run out of GPUs. It&#x27;s finally time for GPU development to stop meaning CUDA, but I guess it&#x27;s going to take the world a while to realise this. reply jacquesm 11 hours agoparentAt this point the massive investment in software is what drives this, hardware differences may no longer be dominant but the only way to unseat NV at this point is drop in replacements and&#x2F;or drop in replacement libraries. And that&#x27;s getting there. Which is good because GPUs are too expensive and have too little memory, some competition might help move things along rather than to give NV more time to milk their precious digital cows. reply tormeh 10 hours agorootparentI really wish the ML researchers would have stayed with Vulkan or OpenCL instead of standardising on CUDA. Everyone must have known how it would end. reply jacquesm 9 hours agorootparentSometimes people just have a job to do rather than to play politics and in this case doing it with OpenCL or Vulkan would have ceded the market for their application to the competition due the difference in speed and money spent for a given amount of compute. That&#x27;s starting to change but the competition has reacted much too late. NVidia figured out that software is almost as important as the hardware early enough that betting on that with a sizeable dedicated team gave them a tremendous lead. I&#x27;ve used CUDA and have tried to use OpenCL but the number of hurdles I had to jump through with OpenCL made it a non-starter and that was before factoring in the cost. NV made it easy and made it work right out of the box. Quite often the most visible constraint is developer time and reducing developer friction is a great way to capture a larger slice of the market as well as to actually grow that market. reply bartwr 5 hours agorootparentprevYou have clearly not programmed in any of those. :)OpenCL was almost unusable due to how difficult it was to set up, plus limited support. Vulcan is terribly ugly and verbose and provides no benefits for pure compute workloads. Writing a \"hello word\" is a few thousands lines of code, debugging impossible.Working with CUDA is pure pleasure - immediate setup, super easy CPU&#x2F;GPU interop and code sharing, all the modern (at the time!) C++ features, super pleasant debugging with stepping, profilers, everything. :)I totally wish others invested as much into tools as NVIDIA did - but they didn&#x27;t and made the whole experience miserable. :( If your velocity is 1&#x2F;10 because of terrible tooling, you need to expect some insane benefits to pick it. reply PeterisP 8 hours agorootparentprevIf AMD really wished ML researchers to use their hardware, they could have put in the effort to make OpenCL or Vulkan competitive with CUDA with respect to ease of developing ML systems. nVidia did put in that software effort, AMD didn&#x27;t so now they both reap the consequences. reply tormeh 8 hours agorootparentNo argument there. AMD really shat the bed. But if you go with a single supplier, it‚Äôs pretty weird to make the pricing agreement ‚Äúwe‚Äôll pay whatever you charge‚Äù. reply nomel 6 hours agorootparentprevRelated, I‚Äôm putting some money into AMD. They can only improve, at this point. reply behnamoh 9 hours agorootparentprevThose ml researchers wanted to get hired‚Ä¶ reply p1esk 8 hours agorootparentThose ML researchers (myself included) wanted to get the job done. reply markhahn 7 hours agorootparentprevhuh? don&#x27;t ML researchers use pytorch and tensorflow? reply nanidin 7 hours agorootparentI‚Äôm guessing we‚Äôre talking about the pioneers here. reply rnrn 6 hours agoparentprevSapphire Rapids is a CPU.AMD&#x27;s primary focus for a GPU software ecosystem these days seems to be implementing CUDA with s&#x2F;cuda&#x2F;hip reply cebert 17 hours agoprev> Most programmers have an intimate understanding of CPUs and sequential programming because they grow up writing code for the CPUMaybe it‚Äôs just where I work, but I feel like even this isn‚Äôt true. A lot of the newer&#x2F;young employees don‚Äôt even seem to have a lot of OS&#x2F;system understanding. A lot of the higher-level languages abstract the immediate need of having any ‚Äúintimate understanding‚Äù of CPUs. reply softfalcon 16 hours agoparentYup, I was coming up under a senior at my work one day and he tried to very incorrectly hand wave how a browser uses a GPU to accelerate draw calls.I had done work in the C&#x2F;C++ code modifying Chromium directly and went, ‚Äúthat‚Äôs not how the browser talks to the OS to use the GPU for acceleration‚Äù. They immediately tried to argue I was wrong and made some more nonsense up.This person is a very competent coder, but we got into further discussions and it was made very clear they had lived in a JavaScript landscape for either far too long, or had only barely gone over basic computing concepts. Their understanding of computers largely ended at ‚Äúand then v8 does some stuff‚Äù.It was weird cause this person had an honours level computer science degree from a very prestigious university and they were not from some rich family that could have paid their way through schooling.They also had over a decade of experience shipping numerous projects. It really threw me for a loop as to what a software engineer needs to know to succeed at very high levels professionally. reply robertlagrant 14 hours agorootparent> and they were not from some rich family that could have paid their way through schoolingHuh? If they&#x27;d got private tutoring, that wouldn&#x27;t make them understand things any less? reply harry8 12 hours agorootparentThe suggestion is that they got there on merit, intelligence, hard work. Not family influence, money, low key corruption in support of the academic career of an idiot. reply yieldcrv 16 hours agorootparentprev> I was coming up under a senior at my work one day> It really threw me for a loop as to what a software engineer needs to know to succeed at very high levels professionallyhe is arguably more successful while you have prematurely optimizeddo things that make money, since that‚Äôs clearly the priority and context here reply eropple 12 hours agorootparent> he is arguably more successful while you have prematurely optimizedSure--until he isn&#x27;t. And that happens north of \"senior\".I&#x27;ve had a lot of I-shaped people reporting to and taking direction from me in my career, and I&#x27;m not as deep at any one thing as many of them. But I&#x27;m deeper at a lot of things than most of those folks, and I can synthesize solutions to more complex problems because of them. Heck, things I learned writing games in my teens still come up on occasion twenty years later. reply ghosty141 12 hours agorootparentprevSenior seems to be lacking knowledge that OP has and acts like it. No matter his backstory he shouldve acknowledged his defieceny. Since OP seems to be newer in the industry comparing success isnt really fair, he might make it out on top in the future, we dont know. reply dzign 16 hours agoparentprevEalier today, a post titled \"We have used too many levels of abstraction\" was on HN top...https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37965142 reply dr_kiszonka 17 hours agoparentprevIt&#x27;s not just where you work, in my experience. reply abhi9u 17 hours agoparentprevTrue. It&#x27;s becoming a lost art. Although, I think anyone who takes a CS program probably learns enough about CPUs in a Computer Organization or Computer Architecture course. Even if they never have to use that knowledge ever again. :)On the other hand I agree that people who take the non-CS route may not be that intimately familiar with all the details. reply galangalalgol 16 hours agorootparentWhat is the non-cs route? reply i_am_a_peasant 16 hours agoparentprevMeh, I was lucky enough to start my career in embedded and stay in it for a decade before I started delving into higher level stuff. Not everyone is that lucky to be at the right place right time receive guidance from the right people. reply Rizu 17 hours agoparentprevDo you have any recommendations to learn CPUs and Systems well ?, thanks reply United857 17 hours agorootparentThe Patterson Hennessy computer architecture textbook is the classic and still the best IMO. reply mavelikara 17 hours agorootparentprevNot the GP, but I recommend reading (or watching lectures based on) the book ‚ÄúComputer Systems: A Programer‚Äôs Perspective.‚Äù reply jocaal 15 hours agorootparentprevOnur Mutlu puts his computer architecture lectures on youtube, their really good. reply mackrevinack 16 hours agorootparentprevnot sure if this is what you have in mind but Code by Charles Petzold might be worth checking out reply einpoklum 13 hours agoprev> Most programmers have an intimate understanding of CPUsNot at all. Very few programmers have an intimate understanding of CPUs. I&#x27;m not one of them, for example. And that&#x27;s without mentioning how most programmers actually write Excel scripts, and most of the rest write interpreted code like Javascript or Python which is somewhat removed from the CPU.> because they grow up writing code for the CPUThere is not really such thing as \"The CPU\". The design space of CPUs is large enough for it to contain quite a variation of beasts. For example,> CPUs were designed to execute sequential instructionsOriginally, yes, but - one cannot really make this argument about CPUs since Dennard scaling hit the wall, and scale-out began in earnest.-----While the mental model is indeed different, hardware-wise, you should think of GPUs as CPUs, except that...* You only ever use SIMD registers: 32 lanes x 32bit* Large register file (up to 256 SIMD registers)* No branch prediction* Predicated execution supported, but no if-then-else (sort of).* Very little stack to speak of* ~10x L1 cache latency* A piece of L1 \"cache\" which doesn&#x27;t cache anything and you can just write to it like regular memory.* ~100x L2 cache latency, no coherence, and it&#x27;s smaller* ~100x main memory latency (and no L3)* A bunch of special registers for implementing geometric thread & lane indexing.* ~10x memory bandwidth* ~5x-10x cores than on a CPU(numbers are for NVIDIA cards, AMD are slightly different; and of course individual cards differ etc.)-----Now take all of those hardware differences, and conceive a programming model. What you get is the fanciful tale of a \"not-like-a-CPU\" processor.Programming for the CPU, we often do the opposite, i.e. ignore the cores we have, ignore things like latency hiding, careful management of what&#x27;s in registers and what&#x27;s in the cache, and just write naive serial programs. reply abhi9u 19 hours agoprevThank you for sharing! reply m3kw9 13 hours agoprev [‚Äì] Never really needed to know this stuff because compiler does all the translation, but low level programming like training AI would be useful. Is that a correct assumption? replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "This article details the distinctions between CPU (Central Processing Unit) and GPU (Graphics Processing Unit), their architectures, design objectives, and their significance in applications like deep learning.",
      "The discussion also covers Little's law‚Äîan important principle in queuing theory‚Äîand the increasing performance discrepancy between GPUs and CPUs.",
      "The complex architecture of GPUs and the need for code optimization are stressed, along with the essentials of effective resource allocation for optimal performance. Key terminologies such as CUDA kernels, thread blocks, and data copying are clarified."
    ],
    "commentSummary": [
      "The article discusses the complexity of GPU computing, stressing the importance of understanding CPUs, memory usage, and efficient code optimization in this field.",
      "Concepts such as differences between quadcopters and drones, Apple Silicon vs Nvidia GPU architectures, and the use of floats in machine learning are also explored.",
      "It emphasizes the benefits of continuous learning in the software industry, the growing impact of Chinese companies in the GPU market, and the importance of considering alternative hardware options for machine learning and GPU computing."
    ],
    "points": 433,
    "commentCount": 157,
    "retryCount": 0,
    "time": 1697899007
  },
  {
    "id": 37967936,
    "title": "FPGA N64",
    "originLink": "http://www.ultrafp64.com/",
    "originBody": "Skip to content ULTRA FP64 The Worlds First FPGA N64 Home Many things are done, and the core itself is almost done ‚Äì 5 Years in the making by probing everything in the real hardware. Everything is now done!!! Just working on some upgrades!!!! CPU design Full pipeline completed for standard MIPS instructions. Interlocks and Bypass functions were tested and confirmed to be working. Up to 150mhz without TLB, FPU and Cache cores. (The Goal of speed is 120Mhz for other cores) CP0 Core completed sending internal registers. 64-bit pipeline With 64 Bit regs. 64bit loads and stores fully working. TLB Core completed and used a duel clocked system to speed up FPU Core completed and tested working The FPU ALU will be separate from the main ALU pipeline to simplify the FPGA logic and do fault finding much easier. Cache memory with duel clocks. This is so we can start overclocking the CPU independent of the RCP core clock. The instruction cache is completed and working. With this, we can now run the CPU interface with a 64-bit wide data bus, so no bottlenecks will happen here anymore. Duel bus access. one for the instruction cache and one for the data cache. This allows the CPU able to get data for instructions and code without affecting each other. No more 32 bit muxed bus that the OG CPU uses for only 250Mbyte transfers. So almost a full 500mbyte access x2. Bus Design 128-bit Ram access / DMA Channel (64 bit for normal data and 64 bits for the Z-buffer and coverage/alpha bits and always render 32bits ‚Äì no more dither) 32-bit address and register access to devices 64-bit extended bit access for RDP/VI Z-buffer and Color alpha extended bits Job system where many cores can ask for ram at the same time and can be queued in the ram to keep data flow happening (This is a major issue with DDR3 ram and refreshing) All cores can read and write independently, thus giving an internal memory throughput of 500M/Bytes full duplex (500Mbytes read and 500Mbytes Write at the same time) Added direct RDP access to memory at 133mhz MIPS Interface Standard access to registers and local memories (IMEM/DMEM and Rom access) Ram access is via the DMA Channel as this would be able to byte-mask data Interface for an original N64 CPU via the Nexys Video FMS Port Confirmed all block and subblock access done (This is important for the cache memory access. This will be later removed from the core as we will run the CPU directly to the internal buses allowing for up to 500mbyte/s and not the 250 it is with this bus This is being removed as we no longer require the real CPU, but a simple switch between the two cores via a boot setup via a reg for debugging games. Thanks To ElectronAsh, we have the original CPU on a board that I can use to get the main RCP core and other external accesses done without knowing if the CPU crashed due to my own bugs ‚Äì Cannot wait to fix this. PIF/SI interface Have working controller inputs -Done Works to be done on the memory paks and rumble pak -Done BRAM Interface with DMA controller -Done An internal CPU that would make the PIF ram look like the CIC seed is correct -Done A new PIF controller using a 6502 core accesses the BRAM and external devices. -Done DMA controller that both reads and writes 64bytes at a time (not 64bits) This would be an improvement -Done Custom BIOS for booting. -Done EEPROM access ‚Äì Done Rom Reader 32 and 64-bit reads and writes via the Register bus and DMA controller. -Done Changeable timing for rom reads -Done We need to make this independent of the master clock (62.5mhz), so we can run higher speeds and keep the read latency time calculations. -Done We have found some unaligned transfers that can happen. Ram controller Full register setup ‚Äì Both the ram regs and the ram module regs are accessible be able to read and write at the same time to the MIG7 ram controller Full 2.2 gbit throughput ‚Äì 500mbyte read and 500mbyte write for normal N64 systems with more access for the RDP Multiple cores to read and write at the same time. Jobs are queued up so the latency of the DDR3 ram can be mitigated. We will work on a more extensive caching system on the memory controller of about 128kbyte for the RDP to run on, where half will be for the Z-buffer and the other half for the colour buffer. RSP Core -completed Entire DMA, Imem and Dmem are completed and working -Done We can get this core up to 90mhz ‚Äì Overclocking I hear -Done SU core tested and working -Done Interlock working in the pipeline process. This includes the VU core as well -Done the bypass also sorted for the EXE and WB stages in the main core The main CPU core has been completely tested for ALU and load and stores -Done CP0 is fully working ‚Äì Even the DMA -Done VU core has been built, and some normal ALU ops are tested and confirmed, Just some special ops that need to be checked. -Done Just the divide core to complete. -Done Duel Opcodes are implemented, except for loads/Stores and MTC/MFC/CFC/CTC as we cannot write to the reg file at the same time a VU opcode is al to be processed. -Done Bug testing to be completed as well. -Done RDP Core ‚Äì completed The pipeline has been designed and tested. But does it work ‚Ä¶. Will find out(UPDATE). We do have fill commands working, But there is an issue with masking. -Done Need to build the memory interface that can do both Z-buffer, Color image transfer and reads, Texture load and stores, and Masked stores that are all byte aligned. This has been done with some bugs in the TULT calls -Done Fully working 18bit Z-buffer and coverage checks Copy and Fill needs to be done directly from the memory controller -Done The memory controller will be done after the SU and VU in the RSP are tested and done.-Done Some Fill commands have been tested and confirmed working-Done Some caching stuff to do to help with DDR3 latency ‚Äì working on Full 8x 4K buffers for texture, image and Zbuffer line buffers Video Core Have a scaling unit working ‚Äì some bugs on some interlaced games Got HDMI core working DMA fully working Need to work on some AA stuff Dithering can be selectable with 16bit colour outputs Audio Core Audio going through the HDMI port and a DAC controller to check Some work on a custom frequency that the audio core makes, but all output is 48khz, just polyphase, so there is no popping on the audio output 32bit output for the HDMI core Got this running via HDMI, so no more DAC is needed Please also check N64Brew for updates on the hardware details I have updated. I support these guys and the Discord channel ‚ÄúN64Brew‚Äù This has been Five years in the making and written fully by myself. No leaks were used for the production of this core. Only emulators, reverse engineering and a lot of reading of patents. Coffee was overused as well Many thanks to the N64Brew team and Decompiler teams as well for testing and source code access to find all the bugs. https://www.twitch.tv/ultrafp64 https://www.youtube.com/user/mazamars312/videos Where am I up to right now‚Ä¶.. Well, let‚Äôs just say I‚Äôm almost done Finally!!!! I have no issues with buying myself a coffee to help my coffee addiction Buy Me a Coffee Search‚Ä¶ ULTRA FP64 Proudly powered by WordPress.",
    "commentLink": "https://news.ycombinator.com/item?id=37967936",
    "commentBody": "FPGA N64Hacker NewspastloginFPGA N64 (ultrafp64.com) 351 points by AndrewDucker 18 hours ago| hidepastfavorite73 comments rdlw 15 hours agoIf anyone is interested in new projects for N64 hardware, Kaze Emanuar has an amazing series about optimizing Super Mario 64 to run incredibly complex romhacks on original hardware.Interestingly, SM64 is usually throttled by memory speed. This means that using &#x27;inline&#x27; is detrimental to performance, since making the program larger can cause another read from memory.Here&#x27;s an overview video, he has many more specific videos about specific topics:https:&#x2F;&#x2F;youtu.be&#x2F;t_rzYnXEQlE?si=6yNnsTxOr7M5v4ub reply phoboslab 14 hours agoparentRomhacks are typically modifying the compiled binary ROM image. Kaze&#x27; work is based on the painstakingly disassembled code from the n64decomp project[1]. He&#x27;s working in C, modifying the game and compiling it again for the original hardware. Not sure I&#x27;d call that a \"romhack\".Great videos though![1] https:&#x2F;&#x2F;github.com&#x2F;n64decomp&#x2F;sm64 reply rdlw 14 hours agorootparentFrom a player&#x27;s perspective, I do think they tend to be called romhacks if they are based on an existing game, although I agree that&#x27;s selling the project short as it&#x27;s based on the decompiled source with heavy changes made.It&#x27;s sort of been a smooth transition from literal ROM hacks, to hacks with custom assembly, to hacks that are compiled from source, but the name has stuck. reply nubinetwork 12 hours agorootparentprevDevelopers for Nintendo consoles have been using C the whole time, even moreso after the SNES. I can&#x27;t imagine how long it would take to make a N64 game if I had to do it all in assembly. reply rdlw 10 hours agorootparentA decompilation of Super Mario 64 was only completed a few years ago; before that romhackers were indeed using assembly. The homebrew scene makes fully original games from scratch in C, but romhacks are alterations of existing games (although decomp blurs the lines between the two communities). reply epcoa 6 hours agorootparentprevI highly doubt there was a readily available C compiler for the 6502 in the 1980s. In any case the idea that developers were widely using C pre N64 is bull. Even in the SNES era it was the minority. reply nubinetwork 4 hours agorootparentI believe Hudson had a C compiler back then... if not, go check out a hexdump of some Koei games (Nobunaga&#x27;s Ambition 2 comes to mind), you&#x27;ll see text strings with the %s token... pretty sure those would only come from C. reply epcoa 3 hours agorootparentSo a handful of non graphically intense RPGs from a relatively obscure&#x2F;cult publisher doesn&#x27;t really contradict minority though. That particular game was released in early 1990, relatively late into the life of the NES and not exactly a first party title. By 80s, I meant more coincident with the release of the system.> you&#x27;ll see text strings with the %s token... pretty sure those would only come from C.There is nothing inherent in % escaped substitutions that unequivocally implies C. You could pretty easily write an asm port of a basic printf function if one wanted to reuse previously translated strings. Not saying C wasn&#x27;t used for the NES port of this title, but this by itself isn&#x27;t as strong a clue as implied. reply xcv123 4 hours agorootparentprev> Developers for Nintendo consoles have been using C the whole timeIt was only done with assembly on the NES. reply psyclobe 10 hours agorootparentprevSo rust should solve all their problems right? reply kibwen 15 hours agoparentprev> new projects for N64 hardwareThere&#x27;s an entire community doing N64 homebrew: https:&#x2F;&#x2F;n64brew.dev&#x2F;wiki&#x2F;Main_Page , including a faction who are working on a Rust SDK (basic demo: https:&#x2F;&#x2F;youtu.be&#x2F;XmlmvRrRFqs). reply rdlw 10 hours agorootparentI know a lot less about the homebrew community but it&#x27;s also very cool! Do you know of a big project that would be an interesting entry point for getting into the community? reply modeless 9 hours agoparentprevI love what he did, but it seems like his recent stuff is only running his romhack. Did he ever make a build of the original Mario 64 with all his peformance fixes, so the original game content can run at 60 FPS on a real N64? reply worewood 9 hours agorootparentNo, and he made a video about it. Basically he worked on the optimizations together with his modifications. He should&#x27;ve done a separate branch on the codebase with only optimizations on the original code reply Rarebox 37 minutes agorootparentHe says he&#x27;ll port his performance optimizations to the original game once he&#x27;s done with his game &#x2F; romhack. Otherwise he&#x27;d have to always update two codebases when he finds a new optimization. reply crtified 13 hours agoprevFor the thousands of people in the MiSTer FPGA ecosystem, some of whom bought the hardware [0] as long ago as 2020, for well under $200 (it&#x27;s more, now [1]), a featureful FPGA N64 core - developed by Robert Peip, who is paid, most would say very deservedly, thousands per month via P_treon) - will be coming soon as a free update, once the already very advanced work is complete. The betas are playable now, and there are near daily updates.[0] https:&#x2F;&#x2F;mister-devel.github.io&#x2F;MkDocs_MiSTer&#x2F;setup&#x2F;requireme...[1] https:&#x2F;&#x2F;www.terasic.com.tw&#x2F;cgi-bin&#x2F;page&#x2F;archive.pl?Language=... reply nix0n 13 hours agoparentWhy did you censor \"P_treon\"? Or are \"a\" and \"_\" close on your keyboard? reply crtified 12 hours agorootparentNo great reason, other than a vague unease at the tacit promotion (to people, and&#x2F;or to future spiders&#x2F;bots) involved in citing the name of a specific corporate - it being a largely irrelevant side element of the greater point I was here to make. reply LeoPanthera 13 hours agoparentprevI never got a MiSTer but I was always vaguely interested. Has the hardware been updated in all that time? Or is an updated one likely in the future? reply lukeck 9 hours agorootparentThere have been a few minor revisions of some of the optional add on boards and things like SNAC adaptors for using original controllers from different systems.There have also been a few different takes on making a mister in more of a console form factor, and boards that integrate into arcade cabinets or fit in ITX PC cases.The software has come a hell of a long way. In the army days, the most advanced cores were 8 bit systems like the NES. Now we‚Äôve got PlayStation 1, Sega Saturn and N64 in the works. Similar progress has been made supporting old computer and arcade systems too.There is an upcoming sort of successor called Marsfpga that‚Äôs recently started being teased. From the little that‚Äôs been shared so far, the community seems hopeful that this will mean the Dreamcast and support for arcade games through the early 2000s are possibilities.Once you get much beyond that era, an FPGA-based approach makes less sense - the systems become much, much more complex and also largely standardise around with x86 or ARM where there are always to run them without emulation at all. A notable exception to that is the PS3 with its cell architecture. Then again, people said for a long time that PS1 and Saturn were not possible. reply crtified 12 hours agorootparentprevThe MiSTer, which imo is still by far the best bang-for-buck system of it&#x27;s type either on the market now, or likely to hit the market anytime soon, is still based upon the original hardware. Several peripherals and form factors have been added or updated as options, but the central processors are the same.Software updates are a different matter entirely. The system has evolved greatly, with each year so far having resulted in an impressive list of new additions and improvements.The cogs and wheels of the niche entrepreneur industry have been slowly turning, and the next 1 - 5 years promises several likely newer generation products. People highly invested in some of the 32- and 64- bit systems not covered by MiSTer, and or people who want more modern display options, will be very interested in those. The price projections seem to be aiming at the $500-$1000 range.But as of today it&#x27;s still hard to look past the MiSTer, as a multi-year already proven system, which has for years offered elite level 8- and 16- bit recreations, recently added an amazing Playstation 1 core, is close to adding an N64 core, and can be had right now all-inclusive starting in the $300 segment. reply mistyvales 11 hours agorootparentThere&#x27;s a good Sega Saturn core in the official Downloader now too. reply deelowe 12 hours agorootparentprevThey still use the de10 board as the base but there&#x27;s been improvements to the io boards and memory support. Faster&#x2F;higher density memory is what has allowed for the more modern systems to be supported.There&#x27;s a project called marsfpga that will support newer hardware. reply isoprophlex 17 hours agoprevRelated: Analogue have a N64 FPGA clone \"coming in 2024\" https:&#x2F;&#x2F;www.analogue.co&#x2F;3dNot much info on the page, also I&#x27;m not in any way affiliated. Just think it&#x27;s cool that this is getting commercialized. reply badlucklottery 15 hours agoparentRobertPeip (FPGAzumSpass, the developer of the Mister PS1 core) also an open source N64 FPGA core in development: https:&#x2F;&#x2F;github.com&#x2F;RobertPeip&#x2F;Mister64 reply bluescrn 10 hours agoparentprevAnalogue make cool stuff, it&#x27;s just a shame that their business model seems so strangely focused on enriching scalpers (extremely limited supply and numerous &#x27;limited edition&#x27; releases even when the standard model is unobtanium - and most of them end up straight on eBay for silly prices) reply pcchristie 5 hours agorootparentI&#x27;ve bought everything they&#x27;ve made, but they don&#x27;t have great customer support.The only shipping they do is extremely expensive express shipping internationally. They had some adapters that were previously OOS come available, but the Pocket system was OOS. I emailed them, asking them if I could buy the adapters, have them put them aside, and reserve the Pocket, and have them ship it all at once whenever the Pocket was in stock. I even offered to pay it all up front so they weren&#x27;t put out. They basically (politely) told me to suck it up, and buy both, and pay ~ 100 AUD each time to ship them express, separately. reply philistine 10 hours agorootparentprevEither they&#x27;re their own resellers and don&#x27;t want to change, or they can&#x27;t get enough FPGA cores so they turn to silliness to market their very limited inventory. reply rlabrecque 9 hours agorootparentprevSimilarly they offer very little support, almost no communication, and yeah, just kind of weird. reply ericlewis 7 hours agorootparentprevThey‚Äôre a really small company and lean heavily on their sibling company 8bitdo for some bits. Getting the FPGAs is the problem generally, and the same reason why it‚Äôs hard to find a mister. They usually similar chips too.The size of the company also explains the support issues. reply MegaDeKay 12 hours agoprevIt doesn&#x27;t mention the FPGA platform used for this so I did a bit of digging. Looks like Digilent&#x27;s \"Nexys Video Artix-7 FPGA: Trainer Board for Multimedia Applications\". This goes for $550 and of course doesn&#x27;t count all of the PMOD&#x27;s and other hardware hanging off the board.https:&#x2F;&#x2F;digilent.com&#x2F;shop&#x2F;nexys-video-artix-7-fpga-trainer-b... reply grobibi 17 hours agoprevSee also: https:&#x2F;&#x2F;github.com&#x2F;RobertPeip&#x2F;Mister64 reply garciansmith 16 hours agoparentReally impressing the progress he&#x27;s made with that MiSTer core, especially since for years people said there was absolutely no way an N64 core could work on the MiSTer&#x27;s DE10 nano FPGA board. reply christoph 16 hours agorootparentAh, so this is the same as the Mister core? I thought it was a different codebase from the Mister core as I couldn‚Äôt see any mention of Mister, DE-10 or Nano on the page, so blindly assumed it was developed for a much larger FPGA (making my earlier comment below redundant). reply InvaderFizz 15 hours agorootparentNo. The GitHub link is to Mister64. Which is an impressive WIP, but is not related to the main article linked here.I&#x27;m most excited for the next generation of FPGA retro consoles coming in the next 6ish months. Specifically the MARSFPGA. All the existing Mister cores, plus a bunch of new stuff enabled by a better and newer FPGA. The estimated price is $700, that&#x27;s reasonable for those of us with large disposable income that want the retro kit with no hassle, easy access for the kids, and high Wife Acceptance Factor.I&#x27;m also looking forward to all the upcoming arcade cores for the MARSFPGA. reply naikrovek 4 hours agorootparentprevit&#x27;s much more about the community knowing for sure that something was impossible while also not knowing what they were talking about. each person said it was impossible because they read someone else say it was impossible, which itself started way back early in the project when someone said that any 3D console would be impossible.this was obviously always possible, but it required someone who wasn&#x27;t indoctrinated into the community. the community behind a project is always both the very best and very worst thing about any project. reply blitz_skull 17 hours agoprevWhat is this? Besides apparently a hardware compatability... thingy.. for N64? reply assimpleaspossi 9 hours agoparentThat&#x27;s the million dollar question for so many hardware and software projects where the information is linked to some page that says nothing about what it is or what it does including the github link to its creator here. reply a_t48 17 hours agoparentprevHardware emulation for an N64 with HDMI out reply asveikau 14 hours agoparentprevI haven&#x27;t been following the scene too closely, but there&#x27;s been a huge ecosystem of fpga clones of 90s video game consoles in the last few years. The \"mister\" project is one such endeavor.The market for the original hardware, old cartridges and mods for things like better video signals or Bluetooth controllers is also popping. reply nemo1618 17 hours agoprevLooks like the most recent youtube video was 3 years ago, and the most recent twitch stream was 2 years ago. Anyone know the actual current status of this? reply speps 17 hours agoparentMaybe he was hired for this: https:&#x2F;&#x2F;www.analogue.co&#x2F;3dThe timeline matches, 2 years between hire and announcement.Discussed a few days ago: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37901025 reply placesalt 16 hours agoprevRelated, any recommendations on low-cost ways to start playing around with FPGAs? Boards, chips, projects? reply ooterness 16 hours agoparentDiligent makes several boards for the educational market, prices in the sub-$200 range. (And the devices are small enough they can be used with the no-cost version of the AMD&#x2F;Xilinx toolchain.)https:&#x2F;&#x2F;digilent.com&#x2F;shop&#x2F;fpga-boards&#x2F;development-boards&#x2F;int...For online courses, I&#x27;ve heard good things about Nand2Tetris but have not tried it myself.https:&#x2F;&#x2F;www.nand2tetris.org&#x2F; reply arein2 13 hours agorootparentI looked at nand2tetris a while ago briefly and I have the impression that it focuses too much on making complex circuits.The target audience for nand2tetris are people that want to understand how the CPU works, and nand2tetris focuses too much on how to make logic using nand gates leaving other areas uncovered.I recommend the Ben Eater youtube chamnel, he has a series of videos about building an 8bit computer, that in my opinion is much more informative for a curious person. reply drivers99 12 hours agorootparentI‚Äôve gone through the nand2tetris course (parts 1 and 2) and I‚Äôve also physically built Ben Eater‚Äôs projects.nand2tetris covers many levels of abstraction; it is both lower and (much) higher level than Ben Eater‚Äôs projects, which is one of its main points: you can create something in one level of abstraction and then build on it in the next level of abstraction. It only starts with logic gates (a single one to start with, ‚Äúnand‚Äù obviously) and goes through the other logic gates that can be built from nand gates, then various digital logic built on that, registers and other synchronous logic, ALU, full CPU, full computer. Hardware is only the first half of the course. Then you build a stack machine, assembler, objected oriented language compiler, a software library I think, and finally a game you write in that (doesn‚Äôt have to be Tetris). So the whole second half is layer on layer of software abstractions starting from the hardware. I would say it handwaves over video output and keyboard input because it‚Äôs not what they‚Äôre teaching about, while Ben Eater shows how to physically build such things.Ben‚Äôs is purely hardware up through 2 main registers, an alu with only add and subtract, an 8 bit digital readout, no real input (besides updating RAM to input the code & data using switches), 16 bytes of RAM (4-bit address), and control unit to implement machine language with a few opcodes. His 6502 project starts with a CPU which is already far beyond where the 8-bit computer ends and builds a computer architecture around it (RAM, ROM, I&#x2F;O, and peripherals: video, keyboard, serial).nand2tetris is normally simulated and Ben‚Äôs covers physically building things which has its own set of lessons&#x2F;skills to learn. Coincidentally I ran across a crossover of the two projects today: a video of someone who built a breadboard version of the nand2tetris 16-bit cpu (called Hack)! https:&#x2F;&#x2F;youtu.be&#x2F;L-azf9ecvfo reply progbits 15 hours agoparentprevI like the iCEBreaker board, relatively cheap (80 euro), uses the digilent Pmod interface for add-on boards and the Lattice FPGA works well with OSS tooling (Yosys, Icestorm, ... for me this is a deal-breaker as I hate the proprietary IDEs).Here are some project tutorials for it (should be easy to adapt to other hardware too):https:&#x2F;&#x2F;github.com&#x2F;icebreaker-fpga&#x2F;icebreaker-workshop https:&#x2F;&#x2F;github.com&#x2F;icebreaker-fpga&#x2F;WTFpga reply dyselon 7 hours agoparentprevThere&#x27;s a ton of of dev boards out there, but I would say to be sure to get something with hardware buttons and LEDs, as it really helps with some of the Hello World level of things, and many of the cheapest options won&#x27;t have those.I started messing with FPGAs with the DE0-Nano, but eventually got so frustrated with the tiny buttons that I upgraded to a DE0-CV, which I really enjoyed my time with. It has some 7 segment LEDs, physical switches, and buttons, and it also has a VGA port, PS&#x2F;2 port, and Micro SD card slot, so you can build a pretty snazzy little PC if you want to. reply Calloutman 14 hours agoparentprevAltera MAX 10 evaluation board is probably one of the cheapest options. You use quartus prime lite to configure it which is free, and better than the equivalent Lattice tools. reply MaKey 16 hours agoparentprevYou might be interested in this new book: https:&#x2F;&#x2F;nostarch.com&#x2F;gettingstartedwithfpgas reply derefnull 16 hours agoparentpreveducational developer kits https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;articles&#x2F;t...there are a few different major brands reply ge96 5 hours agoparentprevorange crab another option reply tombert 10 hours agoprevI wonder if we&#x27;re going to hit a wall on cycle-accurate software emulation; Higan is pretty cool, but even that requires a fairly beefy CPU considering it&#x27;s emulating something reasonably primitive like the SNES.I know nothing about electronics, but the recent uptick in FPGA game systems like the MiSTer getting Playstation and N64 support, I am hoping that FPGAs can pick up where Higan left off; it would be great to get cycle-accurate emulation of the Xbox 360 some day since I never fixed mine after getting a red ring. reply phire 5 hours agoparentCycle-accurate software emulation is a topic that I&#x27;ve been looking into on and off for years.The Higan concurrency architecture hit a wall because it needs to sync between modules every single cycle, what gets very expensive as the clock rates go up.But you don&#x27;t need to sync every single cycle, especially once CPUs started getting caches, so we just need a new concurrency architecture that&#x27;s flexible enough to only sync when needed.I&#x27;ve been experimenting with an actor based concurrency model which seems to be fast enough for a cycle-accurate N64 emulator (though I won&#x27;t know until I have the RSP and RDP implemented). I&#x27;m hoping it might even be fast enough for 6th gen consoles, perhaps when combined with other techniques like a cycle-accurate jit and fine-grained memory locks.I&#x27;ll have to post an article if it ever gets to a workable state.------As for cycle-accurate FPGA emulators, I don&#x27;t think they are cost-effective even for something like the N64, yet alone a 360. You can get 90% of the way there with a cycle accurate CPU, shared bus, cycle accurate RSP&#x2F;RDP. But you have to map the RDRAM onto DDR3 memory and the timings don&#x27;t really line up. The linked UltraFP64 project doesn&#x27;t even try for accurate RAM timings and will preform better than a real N64. I suspect the WIP n64 core for MiSTer is much the same.You could build a custom board with memory that actually matches the timings, but that gets expensive. reply philistine 10 hours agoparentprevI do not believe it is possible to run an Xbox 360 game without copyrighted code from Microsoft. So we might see it from projects just like this one, but I wouldn&#x27;t hold my breath for Analogue to ever release anything past the PS2. reply tombert 10 hours agorootparentConceivably we could get some kind of Wine&#x2F;ReactOS thing couldn&#x27;t we? Something that recreates the APIs well enough to play most games?That said, I acknowledge that that would take a lot more work, and it might not be economically worth it for Analogue or the MiSTer people to bother with it. reply amelius 11 hours agoprevThat&#x27;s some plumbing even Mario and Luigi would be impressed by. reply ChuckMcM 13 hours agoprevThat is quite an achievement! It extends the range of fully realizable game consoles that are realizable in inexpensive FPGAs. At one time I saw a web article on FPGA versions of arcade game architectures (looking it up it seems that became the &#x27;mistercade&#x27; project for JAMMA compliant cabinets). I&#x27;m always amazed at the creativity here, both in the original designer&#x27;s part and in recreator&#x27;s part of reproducing that same vision. reply snvzz 10 hours agoprevI see a donation button, but not a link to the source code.Not sure this thing is open source hardware, or available to anyone in any form, or even real. reply christoph 17 hours agoprevLooks like great work! I feel it‚Äôs worth mentioning that there is also an N64 core for MisterFPGA now - https:&#x2F;&#x2F;misterfpga.org&#x2F;viewtopic.php?t=6947 reply thehias 15 hours agoprevThe project UltraFP64 will be released as Analogue 3D only. reply sbarre 12 hours agoparentSource? reply pjmlp 17 hours agoprevIt looks incredible, great amount of work invested into it. reply trollied 15 hours agoprevHN has hugged it too hard. archive mirror link: https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;*&#x2F;http:&#x2F;&#x2F;www.ultrafp64.com reply make3 15 hours agoprevmirror: https:&#x2F;&#x2F;archive.ph&#x2F;7KXgz reply exabrial 17 hours agoprev> Cache memory with duel clocks:) Just a nitpick, don&#x27;t mind me. This is a neat projectDual = There are two clocksDueling = The clocks are swinging swords at each other reply fb03 16 hours agoparenteach clock has 2 swords wielded in their &#x27;hands&#x27; ? reply djbusby 16 hours agorootparentShaving time off my development estimates hoping for sunshine. reply reactordev 16 hours agorootparentprevPerpetually slashing each other in turn. To keep that cache fresh? To ‚Äúcut down‚Äù on cache size? To conquer cache daemons? reply jonhohle 14 hours agorootparentprevA dual wielding duel? reply semiquaver 15 hours agoprev(Obligatory ‚Äúis this website hosted on the N64?‚Äù joke) reply alephnerd 16 hours agoprev [‚Äì] Take that Sony!(The PS2 is also implemented on a FPGA. Idk whether the PS1 is or isn&#x27;t though) reply monocasa 16 hours agoparent [‚Äì] The PS2 is not implemented on an FPGA. There might have been some internal dev hardware with some FPGAs, but nothing released. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The ULTRA FP64 FPGA N64 home console, which has been under development for five years, is nearing completion. Key components like CPU, TLB, FPU, cache and bus designs, among others, have been designed and tested successfully.",
      "The console features enhancements like increased memory access via the bus design, and improved data flow enabled by a RAM controller with a 2.2 gbit throughput. The video core boasts a scaling unit and HDMI output, with audio also delivered through the HDMI port.",
      "The project has enjoyed support from N64Brew and various decompiler teams, highlighting an active and engaged community behind the console's development. Some minor issues, particularly with the RDP core, are still being addressed."
    ],
    "commentSummary": [
      "The primary focus of the conversation is on projects optimizing Super Mario 64 to run romhacks on the original Nintendo 64 (N64) hardware.",
      "Discussions also cover other retro gaming systems, including the MiSTer system, upcoming projects like Analogue N64 FPGA clone, MARSFPGA console, and FPGA emulation.",
      "Users discuss game development language (C) for Nintendo consoles, share suggestions for FPGA development boards, and touch on educational resources such as nand2tetris and Ben Eater's projects."
    ],
    "points": 350,
    "commentCount": 73,
    "retryCount": 0,
    "time": 1697904105
  },
  {
    "id": 37971795,
    "title": "Startup CTO's Handbook",
    "originLink": "https://github.com/ZachGoldberg/Startup-CTO-Handbook",
    "originBody": "Skip to content Product Solutions Open Source Pricing Search or jump to... Sign in Sign up ZachGoldberg / Startup-CTO-Handbook Public Notifications Fork 26 Star 607 Code Issues 1 Pull requests Actions Projects Security Insights ZachGoldberg/Startup-CTO-Handbook main 1 branch 0 tags Go to file Code Latest commit zachequi feat: add the binary files that make up the actual published book cbe5e71 Git stats 31 commits Files Type Name Latest commit message Commit time published_files feat: add the binary files that make up the actual published book ACKNOWLEDGMENTS extra files LICENSE Initial Commit - auto conversion from .doc README.md another missing \"Th\" case StartupCTOHandbook.md fix: that README.md NOTE: As of October 2023 I'm still working on porting the book content into markdown. Everything is in there (via a .doc to .md auto-converter) but the formatting is all over the place and needs a lot of cleanup still, apologies for my mess in the interrum! The Book You can view the latest content of the book in markdown here You can buy the book on amazon (Coming Soon) Link of the latest version of the markdown rendered to PDF The original manuscript (now outdated) can be found as a google doc Welcome Hi, thanks for checking out the Startup CTO's Handbook! This repository has the latest version of the content of the book. You're welcome and encouraged to contribute issues or pull requests for additions / changes / suggestions / criticisms to be included in future editions. Please feel free to add your name to ACKNOWLEDGEMENTS if you do so. The Author Linkedin / Website / Email Licensing See the LICENSE file, but tl;dr - you're welcome to make copies, changes, redistribute etc. so long as you're not reselling, you keep my name/attribution attached, and you keep future versions open under a similar/the same license.. About The Startup CTO's Handbook, a book covering leadership, management and technical topics for leaders of software engineering teams ctohb.com Resources Readme License View license Activity Stars 607 stars Watchers 3 watching Forks 26 forks Report repository Releases No releases published Packages No packages published Contributors 3 zachequi Zach Goldberg ZachGoldberg Zach Goldberg jensenbox Christian Jensen Footer ¬© 2023 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
    "commentLink": "https://news.ycombinator.com/item?id=37971795",
    "commentBody": "Startup CTO&#x27;s HandbookHacker NewspastloginStartup CTO&#x27;s Handbook (github.com/zachgoldberg) 238 points by mooreds 9 hours ago| hidepastfavorite54 comments serial_dev 1 hour agoIt might be just me, but I like having meetings recorded, and not for having ammunition in my hands when the team starts the blame game, the he said she said. I joined a team where some meetings are recorded and I really like being able to re-watch meetings and discussions. Let&#x27;s assume there is an important discussion, an alignment where I really stretch my capabilities. I can&#x27;t pay attention to the topic and take notes&#x2F;memorize everything at the same time. If I know there is a recording, I can fully dedicate the meeting (where other people are present) to paying attention, asking good questions, challenge their assumptions and so on.Then, the next day, I can listen to the call, pause, increase speed 2x,3x, take notes for myself, write minutes, write summaries for internal use (casual slack message), notice inconsistencies in our thinking, or run an AI note taker all on my own schedule.And let&#x27;s not forget when someone is sick, need to pick up a mother in law from the airport, have a parallel meeting, or joined a week after an important alignment.If the meeting wasn&#x27;t that important? No problem, just ignore it. Set up a sane auto delete rule, and that&#x27;s it.In practice, I don&#x27;t think people spend time snooping around where they shouldn&#x27;t, so that isn&#x27;t really an issue. reply dijit 54 minutes agoparentThis, entirely this.I moved to google workspace precisely because of the ease in which you can record and retrieve recordings of meetings. (IE; it&#x27;s embedded into the calendar event).If a meeting is not important enough to be recorded then it probably wasn&#x27;t a meeting worth having.My personal position is that I put more hours into the day than other people, but it&#x27;s no synchronous, I might be in a more important meeting when there&#x27;s an important meeting; a lot of nuance gets taken out if you just read meeting notes (which people seldom do also) - but even there it&#x27;s so helpful in revising meetings notes quickly.I really push quite hard for having meetings recorded. reply abdussamit 34 minutes agoparentprevI&#x27;m sure there are various tools out there that can help with that, saving time is crucial nowadays.Shameless plug as excitingly I&#x27;m working on a tool https:&#x2F;&#x2F;designpro.ai which converts input from various sources into insights and tasks. I&#x27;ve used it to generate insights from my call transcripts, so I do know that it does work. reply sgt 39 minutes agoparentprevJust make sure the other party is aware and consents to it being recorded. reply bcantrill 6 hours agoprevThere is stuff in here I definitely agree with (record every meeting!), but there is also tons in here that I personally disagree with (e.g., on performance management[0]). This isn&#x27;t necessarily a criticism -- different problem domains likely demand different approaches and leadership styles -- just to say that I wouldn&#x27;t take any of this guide as sacrosanct: if anyone reading something in here that doesn&#x27;t comport with your personal experience, I would recommend deferring to your own experience.[0] https:&#x2F;&#x2F;twitter.com&#x2F;bcantrill&#x2F;status&#x2F;1216491216356823040 reply Galanwe 35 minutes agoparentI don&#x27;t think you&#x27;re comparing apple to apple here.You are talking about _forward looking_ performance. That is, how to improve performance from here.Performance reviews in most corporations is _backward looking_, it&#x27;s about ranking your past performance in the process of allocating compensation and promotions.You propose a wonderful fw looking process, but at the end of the day, you need some bw performance assessment as well. reply Hnrobert42 51 minutes agoparentprevPersonally, I would never consent to be recorded at every meeting. Further, I would be mortified asking others for their consent. And how could I, as CTO, ask without coercion? reply sgt 37 minutes agorootparentAgreed. I would probably consent now and then but largely I find it a bit inappropriate to be recorded, especially when it&#x27;s a collaborative and group think meeting. reply user_named 5 hours agoparentprevWhy record every meeting? Are you actually going to listen back to it? Waste of time and energy to me. reply esafak 5 hours agorootparentIt settles debates about what was discussed, informs people that did not attend, increases discoverability, and promotes transparency. Especially useful in interactions with customers. It&#x27;s not for your daily stand-up. To quote the inimitable \"Yes, Minister\" (https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=85fx0LrSMsE):It is characteristic of all committee discussions and decisions that every member has a vivid recollection of them and that every member&#x27;s recollection of them differs violently from every other member&#x27;s recollection. Consequently we accept the convention that the official decisions are those and only those which have officially recorded in the minutes by the officials, from which it emerges with an elegant inevitability that any decision which has been officially reached will have been officially recorded in the minutes by the officials and any decision which is not recorded in the minutes has not been officially reached even if one or more members believe they can recollect it, so in this particular case if the decision had been officially reached it would have been officially recorded in the minutes by the officials. And it isn&#x27;t so it wasn&#x27;t. reply appplication 5 hours agorootparentSo there was a laugh track behind that but honestly it seems more like good policy than comedy. If a little blunt. reply littlestymaar 2 hours agorootparentprev> It settles debates about what was discussed, informs people that did not attend, increases discoverability, and promotes transparency. Especially useful in interactions with customers.Oh, so we&#x27;re talking about ‚Äúrecording‚Äù as in writing down a record? Not ‚Äúrecording‚Äù as in ‚Äúaudio recording‚Äù of the meeting. Then I agree with that.Edit: it looks like bcantrill was actually talking about audio recording [1], and then I&#x27;m much more skeptical.[1]: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37972900 reply bcantrill 5 hours agorootparentprevThis probably merits a long piece at some point, but recording every meeting has been transformative for us: it&#x27;s zero time and energy (you just need someone to hit the record button!) and it gives engineers the freedom to not be in a meeting for fear of what will be done in their absence (a surprisingly common motive for attending a meeting!). As for going back through meetings: yes, we definitely do (for a variety of reasons), though the distribution is lumpy (and most meetings are likely never listened to again). reply netjiro 1 hour agorootparentRecordings are a great utility:Having a record allows all kinds of automated followup, post analysis, etc. This level of business intelligence and actual understanding is helpful in improving organisation function and individual training long term.Asynchronous is _a_ great next step beyond that:Moving discussions and decisions async has been a massive improvement in the organisations I have helped. Goes against dogma and habit (with all the huge obstacles that come with it) but pays off quickly.Discussions are more factual and evidence based.Decisions are thought through and transparent.Collective (and individual) memory of what has been discussed, decided, and why is much better.Training people to be clear, succinct, transparent also help them not get bogged down in gut feeling, habit, dogma, ego, posturing, pestering, politics, etc.And of course the immediate bonus of individually flexible scheduling and not killing efficiency by slicing everything to administration&#x2F;manager-mode-time. reply laserlight 4 hours agorootparentprev> it gives engineers the freedom to not be in a meeting for fear of what will be done in their absenceRecording the meeting won&#x27;t prevent what will be done in engineers&#x27; absence, will it? Do you mean that engineers have peace of mind that they could listen to the recording and learn about everything discussed in the meeting? reply PoignardAzur 1 hour agorootparentprevAs someone who recently joined a company that recorded every meeting it did since its creation: yup, this is immensely valuable.Having access to past meetings is super useful for onboarding. You can get a picture of what everyone is working on super quickly. reply px1999 3 hours agorootparentprevManually, probably not (exceptional circumstances aside).Recording generally means transcription and the ability to do automatic summarisation and to query action items, which _are_ usefulAlso it means folks can be away or if someone didn&#x27;t quite catch&#x2F;understand something relevant to them they might want to rewatch some of the meeting. reply zgoldberg 6 hours agoparentprevJust read through your twitter thread, I really appreciate this perspective and I completely agree, a manager&#x27;s job includes ensuring the right circumstances to foster intrinsic motivation. I think where I might quibble is there are often circumstances where there are genuine skill gaps and, a manager acting as coach, can accelerate someones growth and performance. reply bcantrill 5 hours agorootparentSure -- but why drag the entire organization through a perf process to accommodate those folks? Especially when they are tied to comp, performance review processes are (in my experience) about justifying decisions that have already been made rather than actually earnestly trying to improve an employee&#x27;s growth: decoupling review from comp and then additionally not having a one-size-fits-all review process allows that coaching and mentoring to go where it&#x27;s most needed -- and it similarly allows for positive feedback to happen much more quickly, broadly, and earnestly. reply Oras 2 hours agoparentprevThis advice should apply to any book&#x2F;blog&#x2F;video not just this book.Every written text is a state of mind of the author in a point of time. Should never been taken as the only to do things.People learn and grow, so does their advice and context.Read to learn perspectives and ideas, not to just follow. reply travissteffen 5 hours agoprevI had the privilege of working with the author across two companies, and I can‚Äôt tell you how big of a difference it makes when everything he‚Äôs written about here gets implemented. The impact is felt across the organization - not just within product or engineering. Like with any system or set of recommendations, eventually you‚Äôll need to make them your own in some way, but I‚Äôd definitely recommend utilizing what Zach teaches as much as possible. reply cientifico 3 hours agoprevI recently became CTO, and one of the biggest struggles I have is communication with the CEO. (Sorry if covered by the book, just read the index)On 8 months so far, the CEO: * Don&#x27;t want to have any alignment meeting. * Don&#x27;t want to plan anything. * Don&#x27;t want to drive vision. * Want to focus only in the killer feature. * Don&#x27;t want to build mockups or prototypes to test with users. * If we do, they have to be beautiful. * Don&#x27;t want to work on anything that takes more than a week. * Rejected to have any effective meeting.What I want to say in this message is that I miss exactly the things that are not in the book.Edit: we are just 3 cofounders! reply carschno 2 hours agoparentI&#x27;ve seen that before. At least in my context, this happened because the CEO did not see the CTO as anywhere near equal at all. Instead, the CTO was the CEO&#x27;s \"tech guy\". And because he was considered the best or most important tech guy, the CTO got his title.Sorry about that, but it sounds like you should consider the possibility that the CEO sees you as just another employee (with a fancy job title). In that mental state, it would mean a waste for the CEO to include you into his decisions. reply TeeWEE 1 hour agorootparentIndeed. Recommendation is to make this clear to the CEO and if he doesn‚Äôt understand (likely outcome) just leave. reply yowlingcat 3 hours agoparentprevI&#x27;ll relay a quote from my mother -- \"If you&#x27;re feeling bad about this. Don&#x27;t worry. It will only get worse! Hahahaha! (phone click)\"In all seriousness -- what you&#x27;re running into is exactly what makes the job challenging. Here are a couple places to start, having been both in your shoes, and the shoes of those on the other end:1. Executive 1:1s -- who are the other executives you work with outside of the CEO if any? Try to get 1:1s scheduled so you can get some more context about what you&#x27;re walking into. Get a sense of what their needs are, which will give you a sense of what the broader organization&#x27;s needs are. Remember, you&#x27;re being hired to make the CEO&#x27;s problems go away &#x2F;without&#x2F; their asking you. Being able to integrate into their executive team is one of the best ways to prove you know what you&#x27;re doing, and all it takes is some consistency, diligence, an open mind, and a desire to earnestly ask helpful questions.2. Executive alignment meetings -- Is your CEO and executive team meeting regularly? If so, ask to join. If not, take the initiative to plan it -- ideally after you&#x27;ve aligned with the other executives beyond the CEO. Even if the CEO cannot make every (or even most) of their meetings, they&#x27;ll greatly appreciate the initiative and it will build credibility. I know you mentioned they don&#x27;t want to have these meetings, so it may be required to work indirectly through other executives first, based on who has the CEO&#x27;s ear.3. CEO 1:1 -- if you&#x27;re the CTO, your CEO likely wants to have some degree of a regular 1 on 1 catchup, if at least to make sure you don&#x27;t leave. You can figure out a cadence to make this work on said CEO&#x27;s calendar -- whether it&#x27;s once a week, biweekly, monthly, etc. The purpose of this meeting is to ensure that you are able to align with the CEO and collect feedback from them about what areas you&#x27;re doing well on and around what areas you can improve. If you can&#x27;t get this scheduled, I don&#x27;t think you&#x27;re in a place where you&#x27;re set up to succeed and I&#x27;d advise you to leave. If it&#x27;s hard to get this scheduled, try pushing on the first two approaches first to get the rapport to get here.Most importantly -- if you can&#x27;t get any of these scheduled, you should really re-evaluate whether this is the right role and whether you shouldn&#x27;t move. If you are the CTO and you cannot get some kind of regular time, no matter how infrequent, with the CEO and other executives, you are not really the CTO and you should consider leaving. Just my 2c. reply tptacek 6 hours agoprevI don&#x27;t know any of the companies or people mentioned in this. Does anyone else here? Before I sink any time into reading this: I have an innate suspicion of \"chief technology officers\" to begin with; is this worth reading? reply CSMastermind 3 hours agoparentI don&#x27;t see anything in it that will be new to anyone with industry experience who has made it to the level of entry level engineering management.The target audience seems to be \"CTO\"s who inherit the title by being technical co-founders at startups with little to no professional experience beforehand. reply mixmastamyk 6 hours agoparentprevWon&#x27;t know &#x27;till we read it! Or at least a significant portion. reply dilyevsky 4 hours agoprev> Wikipedia describes DevOps as a set of practices that combines software development and IT operations‚Ä¶> I translate that as follows: DevOps is all the work that goes into making sure the business software runs in places other than your developers machines. Unless you&#x27;ve got a DevOps specialist on your team, you&#x27;re probably deprioritizing DevOps to some degree‚Ä¶That seems like a weird interpretation of the original definition. Especially the ‚Äúdevops specialist‚Äù part. reply zgoldberg 4 hours agoparentGood feedback! I was trying to get at the idea that DevOps is broad, its often invisible and its therefore often undervalued&#x2F;underprioritized. I&#x27;ll have another go trying to convey that. reply dilyevsky 4 hours agorootparentThanks for putting this together! I read the rest of this section and it seems solid, the into however does read like you‚Äôre suggesting to compartmentalize ‚Äúdevops tasks‚Äù which I think is antithetical to the original idea reply dijit 50 minutes agorootparentthe problem with the \"original idea\" is nobody can decide what it actually means.The interpretation that I see banded about comes from the 10+ deploys a day talk from flickr, where they said having a team of diverse backgrounds was better than having an individual.Somehow in that zeitgeist sysadmins got rebranded devops, so in that case it makes sense to have a \"devops engineer\". reply camel_gopher 7 hours agoprev‚Äú and proactively paying down debt is a necessary investment in overall engineering health.‚ÄùSometimes it‚Äôs cheaper overall to default on certain tech debt rather than pay it down. As long as the product managers don‚Äôt try to collect. reply wlll 6 hours agoparentThe analogy only goes so far and it&#x27;s not clear to me what you mean, what do you mean \"default on tech debt\"? Re-write? Give up and just accept the penalty when changing systems? reply camel_gopher 6 hours agorootparentBuild quickly with abandon to see if you get product market fit. Throw one away. reply wiradikusuma 5 hours agoprevCongrats on launching the book!I&#x27;m writing Opinionated Launch (https:&#x2F;&#x2F;opinionatedlaunch.com) to share practical ideas based on my experience as CTO for a small startup and VPoE in a publicly listed company. When I started writing, I realized there are two distinct topics: management&#x2F;team&#x2F;people side of things, and tech side. I ended up focusing on the tech side since that&#x27;s what I&#x27;m more passionate about. I&#x27;m glad someone did the other part! reply gumby 6 hours agoprevShould be named \"Software Startup CTO&#x27;s Handbook\"There&#x27;s a lot of good management advice to be had that applies to tech startups in which software is a very small part of the technical product or infrastructure.Such a book can&#x27;t be written by one person alone, but this could be the kernel of such a book. I&#x27;ve worked in the last 30 years exclusively in tech startups (or as they are called these days, \"deep tech\", bletch) and there are important model commonalities across software, cloud, networking, pharma, mechanical, consumer electronics, and chemistry as much as there are of course significant differences. The key is the word \"startup\" which implies \"fast moving &#x2F; high growth\" compared to \"normal\" companies in those sectors, even when it takes five years to get to product. reply zgoldberg 6 hours agoparentAuthor here!Such a book can&#x27;t be written by one person alone, but this could be the kernel of such a bookCompletely agree, and that&#x27;s my dream for the book.Should be named \"Software Startup CTO&#x27;s Handbook\"I struggled a lot with the title -- good point that it lacks the specific software reference, that should be perhaps be incorporated somehow. \"Startup\" as well as also important to me, as I agree, size matters and changes how you evaluate trade-offs. Implicit in the word \"startup\" is that time is your most valuable resource, and much of the advice in the book is based on that assumption. reply gumby 4 hours agorootparentThese days the language has become so debased that any new business or small business is called a ‚Äústartup‚Äù (and of course they have to be ‚Äútech‚Äù startups even if they just sell organic oats online ‚Äî real example).I‚Äôd set the ground rules up front so people who don‚Äôt want a high-growth business (and I see complains about that on this site which is fine) can stop reading. Likewise if your business doesn‚Äôt need the CTO function (even if they are still a four-person startup) then you can stop reading.If your dream for your book is to be about tech startups (in the actual meaning of the two words) I‚Äôd partner up with someone else from a different domain relatively soon, then expand the scope again after you‚Äôve got edition 1 done. And call it the ‚ÄúTech Startup CTO‚Äôs Handbook‚Äù. What about the folks making semiconductors? reply zgoldberg 4 hours agorootparentAppreciate the feedback. I think you&#x27;ve nailed something with the phrase \"Tech Startup\", feels more precise and universally understood&#x2F;aligned with what the book is about. reply ronnmico 5 hours agoprevReading this book and so far really enjoying it! Easy to follow and very concise. Will update as I finish the book~ reply monero-xmr 4 hours agoprevBoring technology is so crucial. As a private sector startup company you are already living a fraught existence - why risk it even more with unproven tech?As an example, mongodb inexplicably became a de facto DB for a bit between 2013 and 2016. It seemed like 1&#x2F;3 of startup companies switched from MySQL or Postgres to Mongodb. What an absolute unmitigated shitshow clusterfuck moronic disaster reply kaptainscarlet 4 hours agoparentI remember the days Mongo or NoSQL in general. It&#x27;s all in memes these days reply SMAAART 6 hours agoprevInteresting open source (freemium) publishing business model. reply TruthWillHurt 1 hour agoprevSo little about tech, and what that&#x27;s there is mostly how to annoy your devs with trivial corp-like control nonsense.Too many people in startups try to \"play house\", pretending to be big shot managers&#x2F;company rather than develop the product and actually moving the needle (i.e hiring an HR manager as employee #3-5).How about you do the work, keep to a pizza size team in a garage until you actually can&#x27;t? reply TeeWEE 1 hour agoparentAgreed, if the company is small keep the process light and focus on good people and a good product. This includes a good DevOps ‚Äúproduct‚Äù side of things.Personally I do like having a documenting culture so things can easily by found.In the current startup it‚Äôs often said: code is the documentation. But to me that‚Äôs just lazy. Especially if the code doesn‚Äôt have a minimal set of inline docs. reply amdolan 7 hours agoprevStartup CTO‚Äôs Cheatsheet. reply paulddraper 6 hours agoprevVery good material.(3 time CTO) reply strbean 6 hours agoprev [‚Äì] s&#x2F;Troughout&#x2F;Throughout reply zgoldberg 6 hours agoparent [‚Äì] Thanks! Fixed. reply appplication 4 hours agorootparent [‚Äì] If you‚Äôre looking for that sort of feedback: leadershp->leadership in the repo description reply zgoldberg 4 hours agorootparent [‚Äì] I&#x27;ll never turn down a pointer to a typo; fixed, thanks! reply dazzaji 3 hours agorootparenthttps:&#x2F;&#x2F;github.com&#x2F;ZachGoldberg&#x2F;Startup-CTO-Handbook&#x2F;issues&#x2F;... reply dazzaji 4 hours agorootparentprev [‚Äì] Ok, I‚Äôll offer some format and spelling fixes. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"The Startup CTO's Handbook\" is an open-source book tackling leadership, management, and software engineering topics for leaders in tech startups.",
      "The book is being converted into markdown format and is available for view and edits on GitHub, increasing accessibility and collaboration potential.",
      "The author invites contributions and provides rules for content licensing and usage, promoting an open and inclusive creation process."
    ],
    "commentSummary": [
      "The discourse primarily debates the pros and cons of recording meetings in a startup environment, with supporters citing the potential for improved productivity and transparency, while opponents express concerns about consent and appropriateness.",
      "The role and challenges of a Chief Technology Officer (CTO) and the common skepticism towards it, as well as the definition and interpretation of DevOps, a practice that combines software development and IT operations, are other significant points of discussion.",
      "Other relevant topics include tech debt, a term that represents the implied cost of additional rework caused by choosing the easy solution now instead of using a better approach that would take longer, startup management advice, and the importance of comprehensive documentation."
    ],
    "points": 238,
    "commentCount": 54,
    "retryCount": 0,
    "time": 1697934587
  },
  {
    "id": 37969248,
    "title": "Adtech surveillance and government surveillance are often the same",
    "originLink": "https://www.eff.org/deeplinks/2023/10/adtech-surveillance-and-government-surveillance-are-often-same-surveillance",
    "originBody": "Skip to main content Electronic Frontier Foundation About Issues Our Work Take Action Tools Donate SEARCH Adtech Surveillance and Government Surveillance are Often the Same Surveillance BY MATTHEW GUARIGLIA OCTOBER 18, 2023 In the absence of comprehensive federal privacy legislation in the United States, the targeted advertising industry, fueled by personal information harvested from our cell phone applications, has run roughshod over our privacy. Worse, the boundaries between corporate surveillance and government surveillance are eroding. Unless your data is fully encrypted or stored locally by you, the government often can get it from a communications or computing company. Traditionally, that required a court order. But increasingly, the government just buys it from data brokers who bought it from the adtech industry. An investigation from the Wall Street Journal identified a company called Near Intelligence that purchased data about individuals and their devices from brokers who usually sell to advertisers. The company had contracts with government contractors that passed this data along to federal military and intelligence agencies. The company says it purchased data on over a billion devices. The government, in turn, can buy access to geolocation data on all those devices, when generally they‚Äôd have to show probable cause and get a warrant to get that same data. Many smartphone application developers, to make a quick buck, are all too eager to sell your data to the highest bidder‚Äìand that often includes the government. Courts should hold that the Fourth Amendment requires police to get a warrant before tracking a person this way, but unfortunately, this corporate-government surveillance partnership has mostly evaded judicial review. With the click of a mouse, police can use such surveillance tools to see the devices of people who attended a protest, follow them home to where they sleep, and target them for more surveillance, harassment, and retribution. Police can also track people whose devices have been inside an immigration attorney‚Äôs office, a reproductive health clinic, or a mental health facility. Police could easily use this tool to watch a secret rendezvous between a journalist and their whistleblowing source. Not to mention that law enforcement officials have often abused surveillance technologies for malicious personal reasons. This type of surveillance also makes people who live and work in heavily-policed areas more vulnerable to falling under police suspicion. If you happened to be next door to a pizza shop that got robbed, or took a coffee break near graffiti, police could easily see your device located near the crime and target you for more surveillance. News about Near Intelligence comes just a year after an EFF investigation revealed Fog Data Science, a previously unknown company that provides state and local law enforcement with easy and often warrantless access to the precise and continuous geolocation of hundreds of millions of unsuspecting Americans, collected through their smartphone apps and then aggregated by shadowy data brokers. In light of the Journal‚Äôs recent expose, Congress must close this databroker loophole once and for all. The Fourth Amendment is Not For Sale Act is bipartisan, commonsense law that would ban the U.S. government from purchasing data it would otherwise need a warrant to acquire. Moreover, with the invasive surveillance law Section 702 of the Foreign Intelligence Surveillance Act set to expire in December 2023, Congress has a chance to include a databroker limits in any bill that seeks to renew it. Further, Congress and the states must enact comprehensive consumer data privacy legislation. If companies harvest less of our data, then there will be less data for the government to buy from those companies. It‚Äôs up to us to keep agitating to prevent the government from continuing to buy information about us that it would otherwise need a warrant for. Discover more. Email updates on news, actions, events in your area, and more. Anti-spam question: Enter the three-letter abbreviation for Electronic Frontier Foundation: Share on Twitter Share on Facebook Copy link FOLLOW EFF: x facebook instagram youtube flicker linkedin mastodon tiktok Check out our 4-star rating on Charity Navigator. CONTACT General Legal Security Membership Press ABOUT Calendar Volunteer Victories History Internships Jobs Staff Diversity & Inclusion ISSUES Free Speech Privacy Creativity & Innovation Transparency International Security UPDATES Blog Press Releases Events Legal Cases Whitepapers EFFector Newsletter PRESS Press Contact DONATE Join or Renew Membership Online One-Time Donation Online Giving Societies Shop Other Ways to Give COPYRIGHT (CC BY) TRADEMARK PRIVACY POLICY THANKS",
    "commentLink": "https://news.ycombinator.com/item?id=37969248",
    "commentBody": "Adtech surveillance and government surveillance are often the sameHacker NewspastloginAdtech surveillance and government surveillance are often the same (eff.org) 237 points by gslin 15 hours ago| hidepastfavorite39 comments CAPSLOCKSSTUCK 15 hours agoI agree with the points raised by the article, but shouldn&#x27;t there also be a push to stop cellular service providers from selling your location data? It&#x27;s worthwhile to call out smartphone app developers, but ISPs can get your location even if you don&#x27;t have a smartphone, and in fact are required to be able to report on this by law (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Communications_Assistance_for_... etc.). Does the EFF just think that it&#x27;s moot to try to legislate against the latter? reply daoboy 15 hours agoparentNot at all. The EFF does a lot of good work along those lines also. https:&#x2F;&#x2F;www.eff.org&#x2F;issues&#x2F;cell-tracking reply jbmsf 9 hours agoparentprevI built one of the first location aggregation systems. Maybe the first, hard to be sure. My company had close relationships with all of the US cellular companies and built white label apps for them, mostly under the auspices of family safety (though many users were more concerned about the location of their partners than of their kids).Around the same time OAuth 1a came out and it seemed like a great ideal to offer a platform that leveraged these relationships to sell location to app developers, subject to consent and with a bunch of features to protect privacy.In the end, we had one app developer who was even remotely successful and the less privacy-focused aggregators partnered with the next generation of app developers and the product had no future.Nostalgia is fun. reply colinsane 12 hours agoparentprev> In the years since CALEA was passed it has been greatly expanded to include all VoIP and broadband Internet traffic.the article makes it sound like LE is freely able to tap any VoIP call. how would that work?i thought the usual setup for a VoIP call is SIP to ring the other party (which is partially secured), followed by ICE&#x2F;STUN&#x2F;TURN for the parties to find the most direct route between them, followed by the actual audio stream. do clients just not encrypt the audio stream? do carriers MITM everything downstream of that first SIP hop? (that would be inconsistent with the read-only probing suggested: ‚Äúhardware taps or switch&#x2F;router mirror-ports are employed to deliver copies of all of a network&#x27;s data to dedicated IP probes.‚Äù). reply Nextgrid 12 hours agorootparent> ICE&#x2F;STUN&#x2F;TURN for the parties to find the most direct route between themFrom my experience, using SIP to interact with the PSTN usually means your first point of entry into the PSTN also proxies the media.> do clients just not encrypt the audio stream?SIP encryption is very uncommon, and unless you use ZRTP (which is end-to-end), I believe any SIP server within the path mediates the key exchange and can thus capture the key or downgrade&#x2F;disable the encryption.Given the above points, a packet capture on a switch port would give you both signalling and media traffic and is most likely how SIP lawful intercept is implemented. reply 2snakes 7 hours agorootparentAlthough this may be due to my own lack of technical understanding, Microsoft Skype&#x2F;Teams traffic is SIP-encrypted. reply worik 14 hours agoparentprev> ...shouldn&#x27;t there also be a push to stop cellular service providers from selling your location data?YesThere should be rules stopping the collection of that dataThere should be rules against locking down devices people \"own\" that prevent countermeasuresPrivate use of this data is as worrying as government use. More so. States are often democratically accountable reply chaps 13 hours agorootparentWe also have public records laws where we can hold gov agencies accountable to their abuse of, eg information. Doesn&#x27;t really exist in private orgs, and it&#x27;s one of the reasons gov agencies love third party vendors. reply j45 13 hours agorootparentInteresting. Could the laws be changed without the population reply chaps 13 hours agorootparentNot 100% sure what you mean, but yes we need something similar to GDPR which allows requests for our own information. It&#x27;s a mostly a matter of legislative interest, which can stem from demands from the public. reply hedora 8 hours agorootparentIn the US, with stuff like qualified immunity, having public access to records of crimes doesn‚Äôt allow anyone to be held accountable.Also, based on repeated leaks, our public records laws are completely inadequate.At least when an ad tech company gets caught breaking the law, they get fined a couple seconds of revenue, and there is a theoretical chance someone might get sent out for a few days of community service or probation. reply chaps 5 hours agorootparentPreaching to the choir there! While public records laws stink, a lot of the work by orgs involved in transparency, and others have made it better. By no means is it going to be \"good\" anytime soon, but progress is certainly being made. Whether the work is being done fast enough is a different story, though I&#x27;ll say that any amount of help makes a larger difference than it might seem. Just gotta be persistent.One of my proudest moments is (through reporting) getting the Chicago police department to admit they made an enormous mistake in their analysis, which a colleague says was the first time he&#x27;s seen them admit to such a mistake in the years he&#x27;s done this work. So there are signs that it&#x27;s not all lost. Happy to chat about ways to contribute towards this work if you&#x27;re interested.One thing I&#x27;ll say about leaks is that they occasionally have the effect of revitalizing interest in these issues. But again, it takes persistence. replySoftTalker 12 hours agoparentprevAlso we need better education about the risks of carrying around an always-on GPS receiver that continually transmits your location to third parties.Victim-blaming? Maybe. But you can choose to turn your phone off. reply walterbell 11 hours agorootparentHow many iPhone owners know their phones cannot be turned off, i.e. a faraday bag is required to stop radio transmission? reply irq 8 hours agorootparent> How many iPhone owners know their phones cannot be turned off, i.e. a faraday bag is required to stop radio transmission?This can be fully disabled in Settings. reply SoftTalker 11 hours agorootparentprevWhat transmission is still active when the phone is off? reply colinsane 9 hours agorootparentcan&#x27;t say exactly, but the \"find my\" feature explicitly remains active even while \"powered off\" so it&#x27;s evidently enough transmission that Apple can track location. reply SoftTalker 9 hours agorootparentApparently this uses the phone NFC capability and other nearby active phones to determine location. So yes, either leave the phone at home if you don&#x27;t want to be tracked, or use a faraday bag. replythrow3121 14 hours agoprevSee also: the concept of AdInt, mentioned in https:&#x2F;&#x2F;www.theregister.com&#x2F;2023&#x2F;09&#x2F;16&#x2F;insanet_spyware&#x2F;, more on its workings in the original investigation by Israeli newspaper Haaretz at https:&#x2F;&#x2F;archive.ph&#x2F;7dbaV(previously submitted with 2 comments only, https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37542097) reply newscracker 5 hours agoprev> Unless your data is fully encrypted or stored locally by you, the government often can get it from a communications or computing company.> Traditionally, that required a court order. But increasingly, the government just buys it from data brokers who bought it from the adtech industry.I presume the adtech industry includes Meta and Google. But some people (even here) get indignant and say ‚Äúno, Meta doesn‚Äôt really sell your data‚Äù or ‚Äúno, Google doesn‚Äôt really sell your data‚Äù. If these companies are participating in this ecosystem and buying from data brokers, they‚Äôre just as evil as the data brokers. Let‚Äôs not pretend that Meta and Google are saints who should not be criticized for making most of their money from these relationships and enabling surveillance. reply Podgajski 14 hours agoprevI that this is going to end much more badly than people realize.I mean it isn‚Äôt the definition of fascism when corporations collude with governments? So isn‚Äôt this what we‚Äôre seeing here? reply ben_w 13 hours agoparent> it isn‚Äôt the definition of fascism when corporations collude with governments(As this question only makes sense to ask if you meant \"isn&#x27;t it\" rather than \"it isn&#x27;t\", I&#x27;m going to make the obvious assumption)That&#x27;s the first time I&#x27;ve heard this definition for fascism.Normally it&#x27;s defined as authoritarian nationalism, having a centralised autocratic dictatorial leader, militarism, forcible suppression of opposition, subordination of individual interests for the perceived good of the nation or race, and propaganda to support the belief in some natural social hierarchy to maintain all that. reply anigbrowl 12 hours agorootparentBenito Mussolini famously equated fascism with corporatism, although the latter term has a somewhat different meaning in Europe from how Americans would typically read it.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Corporatism reply peyton 5 hours agorootparentprevThe corporate state is a pretty big piece of the puzzle you‚Äôre missing. I‚Äôm generally confused when people today throw around the word ‚Äúfascist.‚Äù reply arrosenberg 11 hours agorootparentprevEh, Fascism is a specific version of authoritarian nationalism, but you can have the latter without it being the former (e.g. Napoleonic France, the Kaiserreich). We never refer to modern China as fascist and it has all of those qualities.I can&#x27;t think of an authoritarian nationalist leader who didn&#x27;t also have a significant measure of control over industry as well, so I think authoritarianism in general usually will involve a high degree of state control. reply Krasnol 12 hours agorootparentprevI have the feeling that \"isn‚Äôt the definition of\" became a hollow phrase meant to raise the drama on the word which follows it. reply motohagiography 13 hours agoparentprevThere was a time I would have agreed abuses like these would end badly, but after seeing the effects of the Snowden and Wikileaks revelations about how nihilistic and evil our institutions can be, I doubt the reaction to these or other surveillance realities will come to much. The main effect of those disclosures was a rise in populism, but even with the loss of the White House for a term, it had no lasting impact and that populist reaction has been effectively quashed.If you want to effect change, get in earlier when it&#x27;s still just the crazy fringe talking about it, like 25 years ago, or 25 years from now, and maybe you can steer the course away from the next one. reply WarOnPrivacy 12 hours agorootparent> There was a time I would have agreed abuses like these would end badly, but after seeing the effects of the Snowden and Wikileaks revelations about how nihilistic and evil our institutions can be, I doubt the reaction to these or other surveillance realities will come to much.A lot happened in a very short time after the Snowden revelations. HTTPS became expected and Let&#x27;s encrypt launched. Data providers started encrypting backbone traffic in earnest.Countries that had long condoned US Gov surveillance - or were largely compliant&#x2F;complicit - began to push back.Did that momentum wind down? Yes.When did it wind down? When most major news orgs lost most of their interest.Snowden dragged the press into reporting on surveillance abuses. Eventually that pressure eased and major news orgs went back to doing what they did before - giving most surveillance abuse a pass.And here we are.The only way this gets better is if the press gives a crap in a meaningful way. Otherwise, invasive snooping will continue expanding into every nook+cranny of our lives.The same way US political corruption became normalized after decades of underreporting, surveillance will become equally entrenched - and for most of the same reasons. reply Nextgrid 12 hours agorootparent> A lot happened in a very short time after the Snowden revelations. HTTPS became expected and Let&#x27;s encrypt launchedAnd yet the proliferation of the attention economy and \"growth & engagement\" means that in practice, you no longer need to care about breaking HTTPS if you can just tap the analytics&#x2F;marketing&#x2F;adtech providers and can get data from 90% of the internet since these trackers litter pretty much every single webpage nowadays.As a bonus, you don&#x27;t even need to fund or maintain the interception infrastructure, since the adtech provider is doing that for you for free. reply WarOnPrivacy 11 hours agorootparent> you no longer need to care about breaking HTTPS if you can just tap the analytics&#x2F;marketing&#x2F;adtech providersRight. It&#x27;s why the LEO&#x27;s \"Going Dark\" FUD was complete and utter crap. LEO has more info on us than at any time in history.However, none of that negates how real progress followed Snowden and that bad actors lost their forward momentum. At least until the press lost interest and then bad actors resumed their progress against us.News orgs not caring is 98% of how we got here. As far as we let press off the hook, we&#x27;re earning what we get. reply smoldesu 14 hours agoparentprevI wouldn&#x27;t say it&#x27;s characteristically fascist, but your instinct that all this will end poorly is probably right. Ever since the Snowden leaks, we&#x27;ve known that multiple nations built up a digital power disparity that can stockpile exploits and apply them for petty purposes. NSO Group proved that it can be done practically silently, to any phone, with no extra software. Situations like the Canada&#x2F;India murder probe also recently suggested that supposedly-secure channels can be intercepted anyways. One can only imagine what China or the NSA is fully capable of today.So far we&#x27;ve avoided catastrophe, but it does feel increasingly like the surveillance and internet control lines have already been drawn. I&#x27;d hear out anyone who wants to steelman a nicer interpretation of things, though! reply akira2501 12 hours agoparentprevIt&#x27;s the inverse. In a fascist government I&#x27;d expect the administration to effectively control the markets and corporations; however, I think this move is the corporations effectively controlling and attempting to supplant the government. It&#x27;s the perverted idea of \"stakeholder capitalism\" taken to it&#x27;s natural conclusion. reply ramesh31 13 hours agoparentprev>I that this is going to end much more badly than people realize.The state of web surveillance today will seem quaint compared to what&#x27;s coming. Things like Quest 3 and Apple Vision are going to be just the beginning of it, but once everyone is walking around with AR devices, you will have no option of opting out of the point cloud. These companies will have a realtime graph of where and what everyone in public is doing at all times, even those who&#x27;ve never used any services. It&#x27;s terrifying. reply raarts 11 hours agoprevSomeone once said: \"A democracy with advanced tech is less free than a dictatorship with primitive technology\". reply JumpCrisscross 10 hours agoparent> Someone once said: \"A democracy with advanced tech is less free than a dictatorship with primitive technology\"Someone was an idiot. They should read about life in ancient Egypt, Inquisition-era Spain, Nazi Germany or the GDR, each of which was less technologically advanced than Switzerland, New Zealand, Denmark, Estonia or Ireland today [1].[1] https:&#x2F;&#x2F;worldpopulationreview.com&#x2F;country-rankings&#x2F;freedom-i... reply amelius 9 hours agorootparentIt can become true at some point, though. reply Ygg2 9 hours agorootparentprevPossiblity for abuse is greater with greater tech. For all his purges Stalin never had ability to track and measure contents of your thoughts, we might not be there yet but a democracy with ability to detect and alter thoughts is way more sinister than all those mentioned.Imagine tomorrow&#x27;s Holocausts but enjoyable, and then quickly forgotten. reply JumpCrisscross 5 hours agorootparentSure. That doesn‚Äôt invalidate democracy on the basis of technological advancement. reply shaycommacarli 9 hours agoprev [‚Äì] My personal data should not be ‚Äúprotected‚Äù or ‚Äúregulated‚Äù by the FTC, then. The only legislation that would have any teeth would be a complete outlaw on all points of sale of personal user data without explicit consent of said user (i.e. cookies confirmation), but that will never happen.Also, if this is so blatant, why hasn‚Äôt there been a lawsuit yet? The U.S. government is already bound by the constitution and the 4th Amendment. I don‚Äôt think we need half assed legislation from people who don‚Äôt know what they‚Äôre doing replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The line between corporate and governmental surveillance is fading with governments purchasing personal data from data brokers; this data is originally sourced from the targeted advertising industry.",
      "This practice allows government bodies to access information on billions of devices without requiring a warrant; smartphone app developers often sell user data, even to the government, which makes people more susceptible to surveillance and targeting.",
      "Measures are underway to close the data broker loophole and enact comprehensive consumer data privacy laws that will ensure the privacy of individuals is protected."
    ],
    "commentSummary": [
      "The article discusses the similarities between adtech and government surveillance, raising concerns about personal data exploitation and calling for regulatory measures against location data sales by cellular carriers.",
      "Emphasizing the importance of encryption, accountability, user education about risk, and improved privacy measures, the article introduces \"AdInt,\" an example of surveillance practices, to criticize tech giants Meta and Google for their part in adtech and profiting from user data sales.",
      "It also sheds light on potential adverse effects of surveillance, misuse of advanced technology, the necessity for personal data regulation, explicit user consent, and collaboration between tech firms and data brokers."
    ],
    "points": 237,
    "commentCount": 39,
    "retryCount": 0,
    "time": 1697912552
  },
  {
    "id": 37969387,
    "title": "Reddit mods dumped tokens hours before blockchain program termination",
    "originLink": "https://cointelegraph.com/news/reddit-mods-dumped-tokens-hours-before-blockchain-program-termination",
    "originBody": "X BTC $29,907 +0.06% ETH $1,632 +0.83% BNB $214 +0.48% XRP $0.52 -0.18% ADA $0.258 +0.56% SOL $28 -1.45% ENGLISH ADVERTISE ABOUT News Markets Magazine People Cryptopedia Research Video Podcasts Markets Pro Ad PRASHANT JHA OCT 20, 2023 Reddit mods dumped tokens hours before blockchain program termination Analysis suggests at least three Reddit moderators dumped thousands of dollars worth of Moons just minutes before the actual announcement. 13049 Total views 648 Total shares Listen to article 2:54 NEWS Join us on social networks Popular social network platform Reddit announced the winddown of its blockchain-based community points program on Oct. 17, citing scalability issues. The announcement created controversy in the crypto community, with many calling it a rug-pull, as the price of the native token on different subreddits tanked after the news was revealed. Each subreddit had its own native token. For example, the Moons (MOON) token was the native crypto asset of r/CryptoCurrency, while Bricks (BRICK) was for r/FortNiteBR. Users could spend these points on badges and exclusive items for their avatars. At the time of the announcement, the moderators of most of the subreddits involved with the community points program claimed to be unaware of the decision. However, this is now being called into question as new on-chain data suggests that at least a couple of moderators holding Moons may have been linked to three wallets that dumped millions of the tokens shortly before the announcement. Related: Reddit community tokens soar on Kraken listing On-chain analysts such as Pledditor were the first to draw attention to the actions of subreddit moderator u/Mcgillby. On-chain data reveals that this moderator transferred more than 100,000 MOON over two different transactions on the Arbitrum Nova blockchain, turning it into more than $23,000 in Ether ETH $1,632 . The user subsequently deleted all earlier Reddit posts. Advertisement eToro - Join a secure, trusted platform and get $10 of free crypto Ad In another incident, just 17 minutes before Reddit‚Äôs statement, moderator u/Rider_of_the_storm allegedly shifted 345,422 MOON, worth over $69,000 at the time, to an exchange address. The Reddit account in question has since been deactivated. According to Lookonchain, on-chain data showed that at least three of the administrators overseeing the cryptocurrency subreddit liquidated tokens some 20 to 30 minutes before the announcement went public. A statement from the Reddit moderators clarified that they received the notice about the termination of the community points program an hour prior, suggesting that the three moderators could have used the information to dump their token holdings. Magazine: Blockchain detectives ‚Äî Mt. Gox collapse saw birth of Chainalysis #Blockchain #Cryptocurrencies #Reddit #Business #Pump and dump 12 5 3 3 READ MORE Why the 2024 Bitcoin halving may play out differently than in the past Ad This play-to-earn game offers NFTs linked to physical items like gold Crypto lending invalidated by Chinese court in second landmark ruling Ad HELEN PARTZ OCT 20, 2023 Tether works on real-time reserve report system: Bloomberg Paolo Ardoino, Tether‚Äôs chief technical officer and incoming CEO, said the firm is working on a system of real-time publishing reserve data, according to a report. 10405 Total views 6 Total shares Listen to article 1:19 NEWS Join us on social networks Tether Holdings, the issuer of the world‚Äôs largest stablecoin by market capitalization, Tether USDT $1.00 , is reportedly working on instant updates of its reserve data reports. Paolo Ardoino, Tether‚Äôs chief technical officer and also upcoming CEO, disclosed that the firm plans to start publishing reserve data in real time in 2024, Bloomberg reported on Oct. 20. Soon after the article was published, Tether said that it had no ‚Äúhard-and-fast deadline by which to achieve this goal,‚Äù according to the updated article. Tether did not immediately respond to Cointelegraph‚Äôs request for comment. According to the Tether transparency page, the stablecoin issuer currently publishes and updates its reserves data at least once per day. Tether also issues monthly reserve reports in addition to quarterly reserve reviews. Tether transparency page update as of Oct. 19, 2023 at 11:30 pm UTC. Source: Tether Despite many cryptocurrency markets seeing some slump in 2023, the USDT issuing company has gained momentum over the past year. According to Tether's Q2 update, the company‚Äôs assets rose 5.7% to $86.5 billion. The firm made more than $1 billion in \"operational profit,\" which is a 30% increase from the previous quarter. Tether has also seen a increase in its stablecoin lending in 2023, which came despite the firm having cut such loans down to zero in December 2022. Related: Tether stablecoin loans rise in 2023 despite downsizing announcement in 2022 According to Tether‚Äôs Q2 attestation from accounting firm BDO, the firm increased its excess reserves by $850 million, bringing total excess reserves to $3.3 billion. The company also disclosed that it had $72 billion worth of indirect exposure to United States Treasurys held by money market funds, as well as U.S. Treasurys collateralizing its overnight repo. Update Oct. 21, 11:00 AM UTC: The article was updated to reflect that Tether had no ‚Äúhard-and-fast deadline\" for publishing reserve reports in real time. Magazine: Beyond crypto: Zero-knowledge proofs show potential from voting to finance #Dollar #Business #Fiat Money #Tether #Stablecoin #Regulation Add reaction READ MORE Central banks want to look under crypto‚Äôs hood ‚Äî Is this a positive sign? Ad This play-to-earn game offers NFTs linked to physical items like gold Tether stablecoin firm appoints CTO Paolo Ardoino as CEO Ad EDITOR‚ÄôS CHOICE FTX clients face deceptive priority withdrawal scam BTC price hits 2-month high amid bet Bitcoin will break $32K ‚Äòsoon‚Äô Sam Bankman-Fried asked FTX attorney to ‚Äòcome up‚Äô with legal argument for $8B hole ‚ÄòThis will be our last post‚Äô ‚Äî LBRY throws in towel against the SEC EtherHiding: Why hackers may prefer Binance‚Äôs BNB Smart Chain COINTELEGRAPH YOUTUBE Subscribe Ad Ad Ad Advertise with us Ad TAKE BACK YOUR SAFETY IN WEB3 Ad YOUR BANKING ALTERNATIVE Ad JOIN ETORO, GET $10 CRYPTO NEWS Bitcoin Blockchain Ethereum Altcoins Business Policy & Regulations AI NFTs DeFi Adoption FEATURES Top 100 2023 Top 100 2022 Top 100 2021 Top 100 2020 Magazine Analysis Opinion Hodler‚Äôs Digest Expert Take Top 10 Cryptocurrencies Market Analysis Interview Historical MARKET TOOLS Market News Markets Pro Bitcoin Price Index Ethereum Price Index Bitcoin Cash Price Index Litecoin Price Index Ripple Price Index Monero Price Index Heatmap PARTNER Calculator CRYPTOPEDIA Explained Learn How to Glossary INDUSTRY Research Podcasts Press Releases Events Cointelegraph Accelerator Changelly PARTNER Scalable PARTNER ABOUT US Franchise Advertise About Widgets Newsletters Are you a journalist or an editor? Join us COINTELEGRAPH NEWSLETTER Email Address Subscribe FOLLOW US Terms of services and Privacy policy ¬© Cointelegraph 2013 - 2023 Cointelegraph covers fintech, blockchain and Bitcoin bringing you the latest crypto news and analyses on the future of money. Cointelegraph.com uses Cookies to ensure the best experience for you. ACCEPT",
    "commentLink": "https://news.ycombinator.com/item?id=37969387",
    "commentBody": "Reddit mods dumped tokens hours before blockchain program terminationHacker NewspastloginReddit mods dumped tokens hours before blockchain program termination (cointelegraph.com) 232 points by paulpauper 15 hours ago| hidepastfavorite138 comments ehsankia 12 hours agoAlways a bit confusing since reddit has admins and moderators, and they are very different. Admins are reddit employees, moderators are just random people who run a specific subreddit.So if I understand correctly, this was a site level feature, and admins gave a 1h heads up to the moderators of the specific subreddits using that feature, allowing the moderators to sell early.Wouldn&#x27;t the blame here be more on the admins? Is there a reason this \"heads up\" was secretly only given to mods, and not just announced publicly right away? reply chris_wot 11 hours agoparentIn a regulated securities market, this would be considered insider information, and this could lead to jail time. reply erik_seaberg 10 hours agorootparentIt&#x27;s orders of magnitude too expensive to build anything like this in a regulated securities market. Even IPOs are being delayed until established companies can sustain today&#x27;s enormous compliance costs. reply pas 5 hours agorootparentit&#x27;s definitely not free, but unlikely to be prohibitively expensivehttps:&#x2F;&#x2F;www.startupblog.com&#x2F;sec-approves-token-sale-through-... reply JumpCrisscross 10 hours agorootparentprev> In a regulated securities market, this would be considered insider information, and this could lead to jail timePer the SEC&#x27;s definition (and thus prevailing law of the land), this is already illegal insider trading. That said, it&#x27;s a low enforcement priority, largely on account of the victims being unsympathetic. reply bsamuels 10 hours agorootparent> largely on account of the victims being unsympathetic.the crass bullshit that people on HN say astounds me every day. the victims here are literally the people who need protecting the most reply JumpCrisscross 10 hours agorootparent> the victims here are literally the people who need protecting the mostIf you look at my comment history, you&#x27;ll see I&#x27;m for enforcement. When I&#x27;ve advocated for it, this is what I&#x27;ve been told: crypto enthusiasts aren&#x27;t a sympathetic victim block. That leads to reduced enforcement when they&#x27;re the victims. When it&#x27;s regular folk, like those depositing with Gemini Earn, we get enforcement.Same reason we don&#x27;t see enforcement against dealers of adulterated illegal drugs in respect of their adulteration. The victims aren&#x27;t sympathetic. reply fallingknife 10 hours agorootparentprevThe SEC&#x27;s definition has been taking major Ls in court and is in no way \"the law of the land.\" reply JumpCrisscross 7 hours agorootparent> is in no way \"the law of the land.\"Yes, it is. Some parties are trying to have it struck down. That hasn‚Äôt happened yet. reply fallingknife 3 hours agorootparentYou are incorrect. The only time that the SEC has been to court on the issue of that definition, the court ruled against them. The SEC does not have the power to determine what is a security anyway. That rests with statutory law and court precedent. replyI_Am_Nous 13 hours agoprevReddit mods who are historically very power hungry and abusive acted in power hungry ways which abused their power to make money.However they aren&#x27;t paid so I could see how someone could easily justify doing this as \"payment\" for their hard Reddit mod work they have put in. reply qingcharles 11 hours agoparentIt&#x27;s not just Reddit mods. It happens to almost all mods. Except dang. reply null0pointer 9 hours agorootparentThe difference is that dang is one of the rare mods that‚Äôs actually paid for the work. I assume this means it‚Äôs a job to him, rather than a means to satisfy some desire for power and control. reply I_Am_Nous 8 hours agorootparentI have a feeling that some of the \"mods abusing power\" phenomenon is that they may be subconsciously afraid of losing their mod position and all the power and authority that comes with. They have no method to hold that power tight except to wield it against any perceived threat.So paying mods might reduce that behavior since their power is tied to them doing their job properly and there is an authority to appeal to if they suck and abuse power.Not to say dang isn&#x27;t doing a great job only because he&#x27;s paid to! But I agree with you, paid mods have different rewards and goals than volunteer mods. reply I_Am_Nous 11 hours agorootparentprevI guess it just shows how few of us can actually be trusted with power then :( reply fullshark 13 hours agoparentprevI doubt even that level of shame &#x2F; rationalizing of it took place. reply stavros 9 hours agoparentprevI was a reddit mod back in the day, and, from what I&#x27;ve seen, the \"power hungry\" meme is because people don&#x27;t like having rules applied against them.Also, in this case, mods didn&#x27;t use their power, they used their information. reply I_Am_Nous 9 hours agorootparentThey used their privileged information. Privilege is a kind of power, and in this case they acted in ways they could only do if they were in those privileged positions.I have a feeling you are correct in some ways (depending on which sub you were a mod of, r&#x2F;the_donald was a very different crowd with different mod practices than something like r&#x2F;pics) but a lot of the complaints also come from disagreements about particular interpretations of the rules and lack of recourse to an adjudicator with higher authority. I&#x27;m sure there are plenty of petty mod actions, especially on Reddit when posting in one sub can automatically get you banned in another if a mod checks your history.Reddit is a special rodeo that I spent way longer than 8 seconds on. Positives came out of my time there but my, my was it too much time overall lol reply fallingknife 9 hours agorootparentprevYeah really strange that people don&#x27;t like other people with the power to write, interpret, and enforce riles against them, and who they have no power to remove from their position if they abuse it. reply pityJuke 14 hours agoprevI see no discussion on &#x2F;r&#x2F;cryptocurrency, but I do see some on &#x2F;r&#x2F;buttcoin [0]. If I haven&#x27;t missed anything, supression of the story (predictable)?[0]: https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;Buttcoin&#x2F;comments&#x2F;17cikr2&#x2F;reddit_mo... reply gurchik 13 hours agoparentIt looks like most discussion has been deleted by the mods. They&#x27;ve allowed this one to stay: https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;CryptoCurrencyMeta&#x2F;comments&#x2F;17b8lqe...Most of the mods involved are no longer mods. At least one (TNGSystems) is still a mod since they waited until the official announcement before dumping their position. Here is their explanation: https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;CryptoCurrencyMeta&#x2F;comments&#x2F;17b8lqe... In this argument you can only see TNGSystems&#x27;s responses because all the comments from the other side were deleted by moderators.Copied for posterity:> If I‚Äôm an asshole for selling my moons in order to provide a guaranteed safety blanket while my wife is off on maternity leave and I‚Äôve got to cover all bills & upkeep & all the new shit we have to buy for a baby, then so be it.> I hadn‚Äôt made my mind up on selling, even when the announcement was posted. Otherwise I would‚Äôve done it after 10 seconds, not 3 and 6 mins after it was posted.> At the end of the day I decided to take a guaranteed chunk of cash in return for my contributions, time and energy rather than wondering if the system can be built back up by mods.> Crypto is a zero sum game. reply bozhark 13 hours agorootparentHe really typed out and said ‚Äúit‚Äôs a pyramid scheme‚Äù and ‚Äúit‚Äôs not insider trading because 2 minutes 50 seconds more than 10 seconds matters‚Äù reply MightyBuzzard 12 hours agorootparentIf it was after the announcement, it was after the announcement. One second is plenty for him to be perfectly morally justified. reply smcin 12 hours agorootparentWhat is the primary URL for where the announcement was actually posted? (on Reddit itself? in a press release? somewhere else?)How long is the time lag between posting it there and when the typical holder of Moons would have been notified?(Think: Efficient Market Hypothesis viz. time lag of disclosure) reply Aeolun 11 hours agorootparentHe doesn‚Äôt have to wait for the typical holders of moons. Only for those very dedicated. And 3 minutes should be plenty fof those. reply bozhark 12 hours agorootparentprevRight, agreed, so why the attempt to explain the difference between 10 seconds and 3 minutes as a value statement? replyredox99 13 hours agorootparentprevWow. Those comments. In case anyone had any doubt what your typical reddit mod is like. reply Aeolun 11 hours agorootparentIt‚Äôs also a mod of cryptocurrency, I imagine that makes a difference. reply pcrh 10 hours agorootparentprevThat&#x27;s an intensely bizarre discussion.These \"moons\", or whatever, don&#x27;t seem to have any connection with real world economic activity, and so are intrinsically worthless beyond the value that those involved in trading these notions give to them.I wonder if there are any economic studies into the value that people place on assets and goods according to their direct connections with human wellbeing, e.g. food, housing, and clothing, vs capital, vs derivatives of capital such as shares etc, vs the crazy world of crypto. reply secondcoming 11 hours agorootparentprev\"Hey wife, I&#x27;ve banked our future into something called a Moon. Yes, a Moon. It&#x27;s crypto, you wouldn&#x27;t understand\" reply matteoraso 10 hours agorootparent\"Look, you said you wanted me to get a job, now I work 12 hours a day powertripping on an online forum for imaginary money. What more do you want?\" reply cowsup 14 hours agoparentprevIt&#x27;s from a few days ago, and (predictably) got a lot of downvotes, but the post from the Reddit admin is still up:https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;CryptoCurrency&#x2F;comments&#x2F;17a33ql&#x2F;ser... reply pityJuke 13 hours agorootparentThat&#x27;s the post about sunsetting the tokens mentioned in the article, I&#x27;m talking about a post discussion the mods&#x27; dumping the tokens. reply mminer237 13 hours agorootparentThere was one, but it was downvoted and the mods nuked it. reply qingcharles 11 hours agoprevI&#x27;ve been on Reddit since it started and never heard of Moons, so I had to look them up:https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;CryptoCurrency&#x2F;wiki&#x2F;moons_wiki&#x2F; reply sexy_seedbox 9 hours agoparentDid you hear about &#x2F;r&#x2F;FortNiteBR Bricks?https:&#x2F;&#x2F;blog.kraken.com&#x2F;product&#x2F;asset-listings&#x2F;brick-and-moo... reply runnr_az 10 hours agoparentprevHaha. Same‚Ä¶ but that happened to me with the whole third party client thing too. reply mardifoufs 14 hours agoprevI&#x27;m having a hard time imagining who they dumped it on. Like, who bought all of those tokens? Imagine buying Reddit coins, it reminds me of the \"bingus token\" meme. reply omneity 12 hours agoparentUsually there&#x27;s a liquidity pool which acts like a self-service market maker (see Uniswap for details on how it works).Unless everyone pulls their stake from the liquidity pool (it&#x27;s the same \"pull\" in rugpull), there is always a counterparty to any sell&#x2F;buy position. reply mertd 13 hours agoparentprevThey are no different than the people who bought before them. They just couldn&#x27;t find a greater fool this time. reply kylebenzle 6 hours agorootparentDifference is the Mods were able to control the source of Moons they got for free then sold to people paying cash.Its a typical crypto scam bu with the extra step of reddi in between. reply jrockway 11 hours agoprevWhat was Reddit&#x27;s angle, anyway? It feels like they saw crypto was hot in 2020, and organizational inertia made it impossible to launch a coin until 2022. By then, crypto&#x27;s unbridled enthusiasm phase was over. I don&#x27;t think Reddit even needed to be involved in crypto (they&#x27;re an ad company), but if you want to get something launched while there&#x27;s hype, you have to move a lot faster. reply rootsudo 10 hours agoparentAngle? To make money. What else? They are adtech. reply haunter 14 hours agoprevAs I said in the previous thread I thought one of the idea behind the killing of the 3rd party apps was to force everyone to use the official app > it has the crypto wallet so Reddit can promote it. Yet 2 months later they kill that too. Very strange (guess it didn&#x27;t fit the IPO plans) reply JumpCrisscross 11 hours agoparent> guess it didn&#x27;t fit the IPO plansFrom what I heard, this is exactly it. Going public as a crypto-tainted company would have reduced the pool of buyers and lowered the stock price. reply seydor 13 hours agoprevReddit is destroyed, out of lack of maintenance and leaving everything to moderators who have grown to become figurative monsters.Lemmy is growing and it s at this \"not big enough\" phase that makes things seem great . It doesnt scale, but it will be interesting to see how popular it will get before hitting the limit. Hopefully it will find equilibrium at a level that makes it still usable. reply kodapoda 9 hours agoparentLemmy&#x27;s active user base has declined recently, while the number of posts and comments has grown. So the future looks mixed.On the topic of its limit: when Digg&#x2F;Reddit communities came about, the overall feeling was that people wanted to share and discuss things. There was far less polarization at early stages, which allowed for a rapid growth. Lemmy is already extremely polarized, with instances being defederated, never-ending hostile bickering in comments. This does not look like a welcoming community, and therefore is unlikely to grow into something that can compete with Reddit. reply seydor 5 hours agorootparentThat&#x27;s because the culture of the internet has changed. The innocence is lost from people, but i think reddit has swung completely to the other way, where people are no longer interested to debate, they are mostly looking for ways to report&#x2F;ban you. It&#x27;s exteremely cynical and lost its humour (unless you use &#x2F;s you re most likely to be reported).It will be interesting to see how lemmy goes . For me it is imnportant that it has an infinite Overton window even if it only happens on defederated edges. Popularity and reach is not everything, but the mere existence of all opinions is important reply redox99 13 hours agoprevI like the idea of fake reddit points being crypto tokens.But I find it pretty dumb that some people consider it an \"investment\", thus what should just be a silly thing with almost null value, becomes a multi million dollar market (with it&#x27;s inevitable insider trading, pump and dumps, etc). reply fudged71 11 hours agoprevI don&#x27;t see anything on &#x2F;r&#x2F;modnewsI know a bunch of mods are close with the admins so maybe there was a private discussion somewhere. reply sexy_seedbox 9 hours agoparentIt&#x27;s on &#x2F;r&#x2F;CryptoCurrencyMeta reply 29athrowaway 13 hours agoprevThat site is malicious. It tries to access your clipboard. reply bozhark 12 hours agoparentHow‚Äôd you catch that? reply 29athrowaway 12 hours agorootparentWhen the site requests permissions to access the clipboard. reply paulpauper 15 hours agoprevIt&#x27;s so obvious what happened here, all on the blockchain for the world to see. At least this is one benefit of the blockchain is it makes it easier to detect insider trading. reply Animats 14 hours agoparentYes, and it is insider trading under US law.[1] But for a few thousand dollars, the SEC probably won&#x27;t go after them. The SEC is busy dealing with the billion dollar scams. There&#x27;s about one crypto enforcement action every two weeks.[2] Not enough, but it&#x27;s having an effect.Meanwhile, 27 more crypto crashes&#x2F;scams&#x2F;thefts&#x2F;rug pulls just this month.[3][1] https:&#x2F;&#x2F;www.skadden.com&#x2F;insights&#x2F;publications&#x2F;2022&#x2F;07&#x2F;crypto...[2] https:&#x2F;&#x2F;www.sec.gov&#x2F;spotlight&#x2F;cybersecurity-enforcement-acti...[3] https:&#x2F;&#x2F;web3isgoinggreat.com&#x2F; reply araes 12 hours agorootparentAs a funny bit found in the 3rd link on \"really\" dumb crypto-bros:> The American supermarket chain and cookie butter paradise, Trader Joe&#x27;s, has filed a lawsuit against the popular Trader Joe decentralized exchange. According to the lawsuit, the supermarket believes the exchange is trying to benefit off the supermarket&#x27;s popularity.> This is actually the second such lawsuit by the supermarket against the exchange, after the first was thrown out when defendants claimed that they had simply named the project after the co-founder&#x27;s brother, Joe. However, shortly after the victory, a co-founder admitted on their blog that they \"just named it Trader Joe, after the supermarket\".> Trader Joe&#x27;s is seeking all profits made by the exchange, plus damages and compensation for the failed lawsuit last year. reply bozhark 12 hours agorootparentAll profits?Why not a licensing fee with retroactive pricing? reply JumpCrisscross 11 hours agorootparent> Why not a licensing fee with retroactive pricing?I doubt the supermarket wants to be affiliated with a crypto exchange. reply jrockway 11 hours agorootparentprevThis is probably a negotiating tactic. Ask for the world, compromise down to what you&#x27;re actually looking for. reply duskwuff 9 hours agorootparentprev> Why not a licensing fee with retroactive pricing?Because their goal is to get the exchange to stop using their name. Charging a licensing fee would be the opposite of that. reply toomuchtodo 14 hours agoparentprevReport to the SEC. Reddit should have users&#x27; IP logs.EDIT: Done. reply mvdtnz 14 hours agorootparentAre fake reddit bucks a real security? I&#x27;m not convinced anything illegal happened here. Even Reddit itself is not publicly traded so I don&#x27;t think there&#x27;s a case to be made for material non-public information. reply anonymouskimmer 12 hours agorootparent> Are fake reddit bucks a real security? I&#x27;m not convinced anything illegal happened here.Even if they aren&#x27;t securities, a rug-pull of any medium of exchange (\"reddit bucks\") would fall under wire fraud. https:&#x2F;&#x2F;www.justice.gov&#x2F;archives&#x2F;jm&#x2F;criminal-resource-manual...> the four essential elements of the crime of wire fraud are: (1) that the defendant voluntarily and intentionally devised or participated in a scheme to defraud another out of money; (2) that the defendant did so with the intent to defraud; (3) that it was reasonably foreseeable that interstate wire communications would be used; and (4) that interstate wire communications were in fact usedThe moment the seller knows that the tokens they are selling have taken a big hit to their value, they are engaging in parts 1 and 2 above when they attempt to sell the tokens to those ignorant of this hit in value. Parts 3 and 4 are guaranteed with internet architecture.If this doesn&#x27;t fall under the SEC&#x27;s authority, they can easily send it on to the FBI. reply dehrmann 11 hours agorootparentIf this is wire fraud, would Reddit be somewhat complicit in it since they notified moderators first? reply cassonmars 10 hours agorootparentYes reply toomuchtodo 10 hours agorootparentprevI reported it to the DOJ as well, just in case. reply toomuchtodo 14 hours agorootparentprevAlready reported, SEC can sort it out. I don‚Äôt care for potential securities fraud and insider trading. reply chaps 14 hours agorootparentNothing else to add, but I just want to say thank you for reporting it. reply kmeisthax 14 hours agoparentprevBitcoins are prosecution futures reply IvyMike 13 hours agoprevIs my butt-ugly NFT avatar safe? reply Tao3300 12 hours agoparentAll my snoos gone. reply pstuart 15 hours agoprevTell me again about how crypto is not a scam? reply fturst 13 hours agoparentThe Internet contains a lot of scams. Does that make the WWW, a scam? No. Same thing. reply alpaca128 12 hours agorootparentThe internet also contains countless legit websites & services and offers many ways to use it productively.I think if blockchain projects offered a comparable value, people would have better ways to defend it than assuring everyone it&#x27;s definitely not all a scam. reply smolder 47 minutes agorootparentI have seen good, dispassionate defenses of blockchain tech pop up in the comments here, but this argument has played out many times and it probably wears on people.Anyway, Namecoin seemed like a pretty nice use case, even if it didn&#x27;t take off. Smart contracts seem like a good idea to me. Behind all the hype and idiotic money chasing behavior and criminality there are some niche but proven-useful technical innovations. And despite people throwing blockchain-everything at the wall to see what sticks for some time, there&#x27;s still plenty of opportunity for new applications to emerge. reply rabbits_2002 15 hours agoparentprevThats the fun part, it is! reply dewey 14 hours agoparentprevSearching HN for \"insider trading\", around 80% of the stories are about insider trading happening on Wall Street. reply JumpCrisscross 11 hours agorootparent> Searching HN for \"insider trading\", around 80% of the stories are about insider trading happening on Wall StreetBecause most insider trading is trivial to catch. An insider trader has to act ex ante. That&#x27;s difficult on multiple levels: acquisition of the information, predicting its effect and executing the trades all while evading capture. Detection, on the other hand, is ex post facto: find unusually profitable trades after the information has occurred. That&#x27;s what the SEC does. To the extent insider traders aren&#x27;t caught, it&#x27;s because they failed to turn a profit.Look at the people being caught insider trading; they&#x27;re rarely professionals. Because professionals know how hard it is relative to the gains. Instead, it&#x27;s mostly numpties who heard about an acquisition from their spouse or whatever.With crypto, we have the tape. But linking the wallets to the numpties is hard. (Also fewer people care about the victims, let&#x27;s be honest.) Imagine if this chain had real names attached to it. We&#x27;d have a predictable list of offenders to prosecute. reply Tao3300 12 hours agorootparentprevTell me again about how crypto is not a scam? reply dragontamer 13 hours agorootparentprevInsider trading is when an insider knows the true price better than you do and takes advantage of that by making buys&#x2F;sells on the market without disclosing the insider information.------What happened here is a rugpull. Not insider trading. The coin was collapsing, but plenty of people figured out how to make money even in the collapse.Rugpulls don&#x27;t really happen in the regulated market. But they are a dime a dozen on cryptocoins. reply dewey 12 hours agorootparent> Analysis suggests at least three Reddit moderators dumped thousands of dollars worth of Moons just minutes before the actual announcementSounds like they had...insider information? reply AnonCoward42 14 hours agoparentprevCrypto (currencies) are not a scam. These are currencies and like any other currency are subject to devaluation and abolition. reply creatonez 14 hours agorootparentThey are very unlike other currencies, in that almost all of them are rigged for a period of rapid hype-based growth followed by a deflationary apocalypse where late adopters are squeezed out by those who perpetuated the ponzi scheme (which is to say, nearly the entire community aside from Satoshi and maybe Vitalik Buterin -- who are probably good people, but hold silly beliefs) reply Exoristos 14 hours agorootparentSounds like pre‚ÄìCivil War U.S. currency. reply creatonez 13 hours agorootparentYes.... and it was a terrible system, not having a money system with any sort of democratic political controls on its supply and demand. Through a modern lens, we should view the thousands of years of gold coin currency as a human system that was ripe to cause artificial limitations and excesses of resource availability for populations that participated in the system. With a time machine, we should advise people of the past that a much better (but still imperfect) system -- clay tablets with a list of creditors and debtors, with regular debt cancellations&#x2F;easements as needed to maintain stability -- had already been invented, possibly thousands of years before the invention of coins. reply AnonCoward42 14 hours agorootparentprev> They are very unlike other currencies, in that almost all of them are rigged [...]So you&#x27;re actually agreeing with me, that crypto currencies are not a scam. The number of currencies is also irrelevant even in a more colloquial sense. What&#x27;s more important is how much is really invested in a given currency and the bad tokens are a niche. Ponzi scheme (or just new) coins are usually for gamblers and not in broad use and are comparable to jettons in a casino. Are jettons a scam in that sense? reply creatonez 13 hours agorootparentWe should be alarmed about any money system that has the properties that give way to gambling and ponzi-like systems. I would not hesitate to criticize both the money system and the con artists who took advantage of it, when it&#x27;s so clear that the money system&#x27;s rules have such inevitable outcomes. I don&#x27;t mind abolishing any of the money systems that turn out to be harmful and are pushed without broad sociopolitical consent. reply AnonCoward42 3 hours agorootparentOur current money system has the properties that give way to gambling. You can&#x27;t pretend that gambling didn&#x27;t exist before crypto. reply Avshalom 11 hours agorootparentprevNo, the casino is the scam that&#x27;s why the house always wins and just like cryptocurrencies as soon as you build one the incentives are to make it an even more predatory scam. That&#x27;s why they&#x27;re variously illegal or heavily regulated reply mvdtnz 14 hours agorootparentprevFunny thing about crypto enthusiasts is that they&#x27;ll claim cryptocurrencies are currencies when it&#x27;s convenient then claim they are \"assets\" or \"a store of value\" when it&#x27;s not. reply yjftsjthsd-h 14 hours agorootparent> they&#x27;ll claim cryptocurrencies are currencies when it&#x27;s convenient then claim they are \"assets\" or \"a store of value\" when it&#x27;s not.It&#x27;s been a while since I took an econ class, but aren&#x27;t all currencies, by definition, assets &#x2F; stores of value?Edit: Oh, you mean they&#x27;re only assets but don&#x27;t meet other criteria to be currencies? reply JumpCrisscross 11 hours agorootparent> aren&#x27;t all currencies, by definition, assets &#x2F; stores of value?Yes and no. Modern currencies separate the medium of exchange and unit of account functions, on one hand, from the store of value and standard of deferred payment functions, on the other hand. Paper and deposit dollars are the former, Treasuries are the latter. Someone complaining about inflation eroding their cash under a mattress&#x27;s value is literally using their dollars wrong. reply mvdtnz 12 hours agorootparentprevYeah, your edit. It&#x27;s a common refrain among Bitcoiners when faced with the deficiencies of their scambucks. reply AnonCoward42 3 hours agorootparentHow is Bitcoin a scam for you. Tell me. reply injeolmi_love 12 hours agorootparentprevThere‚Äôs over 1.8 million crypto tokens. You can find one for any concept that‚Äôs been thought up. reply AnonCoward42 14 hours agorootparentprevObvious strawman. reply yanderekko 12 hours agoparentprevDemand for an asset fell so its price dropped. Nothing scammy about this. reply FireBeyond 12 hours agorootparentI&#x27;m not sure if you&#x27;re intentionally trying to miss the point - the scam is that people who knew that demand for the asset was about to crater - because they were about to crater it, by shutting it down, extracted their investment knowing that others would not be able to. reply yanderekko 11 hours agorootparentThe reddit moderators who cashed out were not the ones who were shutting down the service. They simply had insider information that allowed them to move first with this information. This was not a rugpull scenario. reply jedberg 14 hours agoprevIt&#x27;s a good thing crypto is a totally unregulated securities market with ultimate freedom, so that these folks won&#x27;t suffer any consequences for acting on their insider information, as it should be. &#x2F;sSarcasm aside, this is a great example of why a partially regulated market is a good thing. reply jsbisviewtiful 12 hours agoparentA former friend of mine dumped his remaining savings into a niche crypto coin. Dude also doesn‚Äôt have a 401k. It‚Äôs wild how blind and into the crypto propaganda he‚Äôs dug himself. The dude‚Äôs future is fucked. reply mbar84 12 hours agorootparentIt&#x27;s not as if crypto has a particularly good reputation or the scammy nature of crypto is a secret. I wonder if your former friend has the same kind of personality that just as well would have deluded themselves into thinking they have some amazing insights into horse racing or sports. Without crypto, would they have found some other way to gamble away their future? reply psychlops 11 hours agorootparentprevCrypto was probably the latest in a long list of ways he squandered money. reply g42gregory 10 hours agorootparentprevHow would this be different&#x2F;worse than going to say Las Vegas and losing all the money? How is this crypto‚Äôs fault? reply llbeansandrice 10 hours agorootparentVegas isn‚Äôt a market. You can‚Äôt have insider information to win roulette.It‚Äôs not about the possible outcomes but how the game&#x2F;market actually works. reply belltaco 10 hours agorootparentprevBecause crypto is still called and considered an investment in many circles, even in news articles, unlike casino gambling. reply pmarreck 11 hours agorootparentprevIt&#x27;s funny, because when I thought it might actually turn out to be a black swan event and thus a huge investment opportunity back in, oh, 2011-2012 or so, everyone thought I was crazy.*The thing is, you have to be crazy to see it early I guess. If you don&#x27;t get in at the beginning of these things, you will miss your hockey stick. AND, there has to be a fundamental advantage OTHER THAN hype to carry it forward.*(Don&#x27;t bother hacking me. There are none left, unfortunately, because I got disgusted by the scamminess of the people entering the market around 2016 or so and I don&#x27;t like money sitting in things I&#x27;m not watching. And I now of course wish I hadn&#x27;t&#x27;ve bought that Tesla with 2013-era bitcoins... A Tesla that still runs like a top. I&#x27;ve also survived multiple hacking attempts already. So, sellers&#x27; remorse, partly. Anyway. I believed in crypto, and survived crypto.) reply baz00 13 hours agoparentprevMarket regulation is only a worse idea for the people selling you something. reply yterdy 12 hours agoparentprevFWIW, insiders regularly get away with dumping regulated securities just before bad news becomes public. reply nativeit 12 hours agorootparentThat‚Äôs probably because they‚Äôre largely appointed (the directors anyway) from a pool of highly conflicted industry insiders who hold little to no intention of fulfilling their public responsibilities. reply yterdy 9 hours agorootparentThey&#x27;re also not held accountable in any meaningful way by regulators (again, because they&#x27;re highly conflicted). Bernie Madoff was the exception; most don&#x27;t even get the Martha Stewart treatment, even after causing orders of magnitude more damage. reply gdevenyi 12 hours agoparentprevLBRY and Odysee would like a word. reply SoftTalker 11 hours agoparentprevRegulation just moves where the corruption happens. I‚Äôm being a bit sarcastic but not completely. reply latchkey 12 hours agoparentprevI sold all of the MRIN I was holding at $25 thanks to Reddit, and that&#x27;s a fully regulated market. reply SuperNinKenDo 12 hours agoparentprevAh yes, insider trading, something that only ever occurs in unregulated markets. reply jedberg 12 hours agorootparentI think you missed the point. reply SuperNinKenDo 7 hours agorootparentWhat point do you believe I missed based on my comment? reply Havoc 14 hours agoprevReddit while still having good user generated content has become an utter shithole as platform.Just shows how powerful network effects are. reply webnrrd2k 11 hours agoparentReddit is now a cesspool. I tried to post something criticalof Uber to the Uberdriver forum and it was auto-blockef four times. Each attempt was milder and less critical than the previous, and the fourth attempt was almost pablum. Each was blocked within a second or so of posting, so they must have some sort of auto block or sentiment analysis going on. reply MichaelZuo 12 hours agoparentprevWhich makes it more strange that it has no competitors that are technically better as a platform.Even Mastodon bungles and messes up very simple things like uploading a PNG image, and it&#x27;s not trying to match even 80% of reddit&#x27;s functionality. reply seydor 12 hours agorootparentLemmy seems to be growing as an alternative. Tehcnical superiority is overrated - Reddit is held together by the network effect. The nagging about its bad technology turns out to be quite a talking point on reddit too. reply appplication 12 hours agorootparentSpeaking as someone who spent over a decade spending way too much time on Reddit‚Ä¶ the problem with fediverse apps like lemmy is yes, partially the experience of the fediverse is inherently confusing. But it also doesn‚Äôt feel much like the raw, unfiltered internet. The fediverse has a bit of a sterilized and utopian feeling to it, in a way that even corpo-internet sites like Instagram, Reddit, Tumbler, etc don‚Äôt.There is this hard to place aura about it, that the closest word I would use to describe it might be elitism. Despite being prescriptively and almost cloyingly inclusive at times, it‚Äôs still somehow not a place for all people and walks. Contrast that to a place like Facebook or Reddit, and even if you‚Äôre uneducated, technologically limited, and by all accounts living a most ordinary and unexamined life, you can find a community to welcome you. The fediverse really doesn‚Äôt have space for that sort of person, or really even folks who just empathize with such people and don‚Äôt mind them around.And yes I know the fediverse is beautifully principled in that it‚Äôs not one thing or platform or instance and anyone can make their own blah blah but that‚Äôs not really how it plays out.I don‚Äôt think the fediverse is a bad thing. But I do think it‚Äôs more of a home for the kind of people who love RSS feeds and classical music and vim and GPL and adblockers than it is for the huddled masses that comprise the majority of internet space. reply dotnet00 11 hours agorootparentWhat impression of the fediverse you get is highly dependent on who you federate and interact with. If you&#x27;re on one of the nodes that block everything the admins disagree with, you&#x27;re going to see the sterilized content you&#x27;re describing. If you go with a node that doesn&#x27;t block anything (except illegal content), you&#x27;re likely to see more of the raw, unfiltered\"old internet\" style content where it was ok to be offensive or disagreeable. reply armchairhacker 11 hours agorootparentprevI wouldn&#x27;t describe it as \"sterilized and utopian\" but otherwise I agree.Basically, 99% of the people on the Fediverse are tech-savvy, anti-corporate, chronically-online people*. Which also means that there&#x27;s a much smaller population, because it only consists of these types of people. Meanwhile, there are more \"normal\" people on Reddit so it has more&#x2F;more active communities and the viewpoints are more diverse.It&#x27;s ironic seeing Lemmy have even worse left-wing bias than Reddit, because there&#x27;s a lot of left-center people on Reddit, but mostly only radical leftists are motivated to participate in Lemmy.* Excluding the nearly-separate Fediverse of right-wing groups, who move to alternative platforms because they&#x27;re banned from centralized ones. Except it&#x27;s also an extreme echo-chamber, just a different one. reply Angostura 11 hours agorootparentprev> The fediverse has a bit of a sterilized and utopian feeling to itI mean if you want to join an instance with no rules that is a hive of horror and villainy, I&#x27;m pretty sure they are out there. They just won&#x27;t be widely federated. reply seydor 12 hours agorootparentprevit certinaly doesnt feel like that on lemmy , maybe you re talking about some other part of the &#x27;fediverse&#x27;?To be honest i don&#x27;t even care about that. As a user i want a place to store my thoughts, outside the impossibly juvenile corporate gardens, even if very few people read it. And if censorship becomes too much of a problem i can still create my own instance. reply RivieraKid 10 hours agoparentprevI wish there was a place for high quality discussion on any topic. By \"high-quality\" I don&#x27;t mean super serious and polite, just intellectually stimulating.Early Quora was a bit like that. I also used to visit a non-English website with a blog section that was amazing - wide variety of mostly smart people with memorable personalities and diverse set of opinions. There were heated debates on various topics, humor, technical posts about some niche field and a specific atmosphere, it felt a bit like a small-town pub. It was not an echo-chamber like most places today, probably because there was no comment voting. Little to no moderation. I still remember a lot of the people, which I can&#x27;t say about HN or Reddit. reply charcircuit 14 hours agoprevSelling when you learn reddit is abandoning it is expected. Not many people would want to keep holding after. reply anonymouskimmer 14 hours agoparentThat&#x27;s the point of most criminal and tort laws: Penalizing anti-social expected behavior in order to disincentivize it. reply thisgoesnowhere 13 hours agorootparentId bet all my Reddit tokens that the person you are responding to has absolutely no idea that torts are even a thing. reply bbarnett 13 hours agorootparentExcellent re-tort. reply charcircuit 12 hours agorootparentprevAntisocial? By selling tokens it lowers the price bringing it closer to the true market value. Preventing people from overpaying is a social good. reply anonymouskimmer 12 hours agorootparentI see you haven&#x27;t heard of a Veblen good.People can&#x27;t overpay if they can&#x27;t buy it at all. By putting it on the market the seller aids and abets their buying at higher than the true market price. This is a social bad. reply RugnirViking 14 hours agoparentprevIt&#x27;s also generally not allowed, just like you aren&#x27;t allowed to buy stock in your own company right before a merger agreement you negotiated, or sell shares right before you post quaterly earnings you know will be bad.Heck look at the scandal (which imo should have been bigger) around congresspeople trading around mask&#x2F;healthcare&#x2F;vaccine stocks early in 2020 based on their reports while publicly downplaying risks. reply charcircuit 12 hours agorootparent>It&#x27;s also generally not allowedThat&#x27;s because the US government at least hates competition. If people get too good at discovering the value of something those methods become illegal because it isn&#x27;t fair to those who don&#x27;t want to actually go after information and just want to read news headlines and earning reports. reply anonymouskimmer 12 hours agorootparentIf the US government \"hated competition\" it would nationalize everything.Though maybe you&#x27;re right. If it really loved competition it would be better at preventing monopolies, would bar patents, and do sundry other things that would guarantee an unregulated (by anyone, not just government) free-for-all. reply abigail95 14 hours agoprev [‚Äì] MNPI?https:&#x2F;&#x2F;www.penningtonslaw.com&#x2F;news-publications&#x2F;latest-news... replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Reddit has discontinued its blockchain-based community points program due to scalability issues.",
      "There are allegations that about three Reddit moderators sold their tokens just before the announcement, possibly using insider info.",
      "Despite a downturn in crypto markets, Tether, the organization behind the USDT stablecoin, has experienced a growth in its assets and lending in 2023. It plans to start sharing reserve data in real-time from 2024."
    ],
    "commentSummary": [
      "The primary focus of the text is a conversation on Reddit about moderators allegedly engaging in insider trading by selling tokens, sparking debates on the regulation of securities within the cryptocurrency market.",
      "The text also broaches other issues related to cryptocurrency, such as scams, thefts, and questions about its legitimacy as a currency.",
      "It includes critiques of various currency systems, discussions on corruption, and mentions of Reddit's perceived decline as a platform while highlighting the subject of illicit trading activities by congress members."
    ],
    "points": 231,
    "commentCount": 138,
    "retryCount": 0,
    "time": 1697913525
  },
  {
    "id": 37966569,
    "title": "The Pixel 8 Pro's Tensor G3 off-loads all generative AI tasks to the cloud",
    "originLink": "https://www.notebookcheck.net/MrWhosetheboss-video-reveals-Google-s-Pixel-8-Pro-Tensor-G3-off-loads-all-generative-AI-tasks-to-the-cloud.760215.0.html",
    "originBody": "‚ñº Reviews News Benchmarks / Tech Buyers Guide CheckMag Library Search Jobs Contact Apple iPhone 15 Pro review! Apple iPhone 15 Pro Max review - More camera power and titanium for Apple's biggest smartphone Google Pixel Fold in review These are the best smartphones of summer 2023 The Glyph is back: Nothing Phone (2) in review The best camera phones of 2023 reviewed Motorola Razr 40 in review Asus Zenfone 10 in review OnePlus Nord 3 Review: Gets a lot of things right Survey: Laptop & Phone Support Satisfaction @MrWhosetheboss video reveals Google's Pixel 8 Pro Tensor G3 off-loads all generative AI tasks to the cloud Google and its Pixel 8 Pro are at the center of another controversy. (Source: Notebookcheck) Popular YouTuber @Mrwhosetheboss has singled out the Google Tensor G3 as a major failing of the Pixel 8 Pro. According to the YouTuber, all of the new generative AI features found on the Pixel 8 Pro cannot be processed onboard the device, but need to be off-loaded to the cloud for processing, despite Google pitching the device as being ‚ÄúAI-first.‚Äù Sanjiv Sathiah, Published 10/20/2023 AI Android Google Pixel Smartphone 91% Apple iPhone 15 Pro smartphone in review - High-end phone as a replacement for the PlayStation? 82% Gigabyte G5 KF review: Gaming laptop with RTX 4060 for 969 euros 92% Lenovo ThinkPad P1 G6 laptop in review - Mobile workstation replaces the ThinkPad X1 Extreme 88% Ayaneo Air 1S Retro Power gaming handheld in test - Zen4 Power for your pocket Next Page ‚ü© The Google Pixel 8 Pro has had a rockier launch than has been typical for the flagship series. Last year‚Äôs Pixel 7 Pro won a few high-profile YouTuber ‚ÄúPhone of the Year‚Äù awards, including from Arun Maini, who is better known by his handle of @Mrwhosetheboss. While Maini awarded the Pixel 7 Pro his gong for ‚ÄúBest Smartphone of 2022‚Äù, he hasn‚Äôt been quite so effusive in his praise for the Pixel 8 Pro. There are many aspects of the Pixel 8 series that Maini praises in his latest video, including its design, the frosted glass finish on the rear of the Pixel 8 Pro, and its advanced software. He also praises the still photo capabilities of the devices, as well as new AI-powered features like Best Photo, Magic Editor and Audio Magic Eraser. However, Maini is less than complimentary about the janky way the Pixel 8 Pro handles zoom when transitioning between lenses, and its artificial-looking video stabilization. But his biggest criticism of the Pixel 8 series is reserved for the Tensor G3 processor. Maini highlights how the new generative AI features including AI wallpaper and Magic Editor, for example, need a permanent internet connection as these require more processing power than the Tensor G3 SoC can deliver. As Maini explains: For all of this generative AI stuff, for anything that actually has to use AI to create things like the AI Wallpaper making, the Magic Editor needs a permanent internet connection. [This is] because every action you take needs to be passed through Google‚Äôs servers‚Ä¶It feels so sluggish, that you are constantly reminded that it's not running on-device‚Ä¶It really makes you realize that the Tensor G3 chip inside this phone is not quite flagship level. Maini‚Äôs observations run contrary to Google‚Äôs marketing which touts the AI processing capabilities of the Tensor G3 as being ‚ÄúAI-first.‚Äù In an official blog, Monika Gupta, Google VP of Product Management makes the following claims about the Tensor G3: This past year we‚Äôve seen incredible AI breakthroughs and innovations ‚Äî but a lot of those are built on the kind of compute power only available in a data center. To bring the transformative power of AI to your everyday life, we need to make sure you can access it from the device you use every day. That's why we're so excited that the latest Pixel phone features our latest custom silicon chip: Tensor G3. Our third-generation Google Tensor G3 chip continues to push the boundaries of on-device machine learning, bringing the latest in Google AI research directly to our newest phones: Pixel 8 and Pixel 8 Pro. Our work with Tensor has never been about speeds and feeds, or traditional performance metrics. It‚Äôs about pushing the mobile computing experience forward. And in our new Tensor G3 chip, every major subsystem has been upgraded, paving the way for on-device generative AI. It includes the latest generation of Arm CPUs, an upgraded GPU, new ISP and Imaging DSP and our next-gen TPU, which was custom-designed to run Google‚Äôs AI models. Not only does Google‚Äôs account of the importance of both what the Tensor G3 claims to be able to accomplish apparently clash with the reality of how the Pixel 8 Pro is actually handling generative AI tasks, its claims about the unimportance of performance metrics do not stack up with Arm‚Äôs own account of the central its CPUs and GPUs play in processing AI tasks ‚Äì these are not solely handled by the TPU (Tensor Processing Unit), as Google infers in its blog. On its website, Arm explains why its CPUs and GPUs (such as those used in the Tensor G3) are critical in performing AI tasks on device: As AI compute moves from the cloud to where the data is gathered, Arm CPU and MCU (Micro Controller Unit) technologies are already handling the majority of AI and ML workloads at the edge and endpoints. The CPU is central to all AI systems, whether it‚Äôs handling the AI entirely or partnering with a co-processor, such as a GPU or an NPU for certain tasks. As we covered exclusively earlier this week, in an extraordinary move, Google went out of its way to block reviewers from being able to easily install popular benchmark apps through its Play Store during the review embargo period. This actually also extended into the post-launch period too, however Google lifted the ban after our article went live. Tests using Primate Labs‚Äô popular cross-platform benchmark Geekbench 6 showed that - despite having quite new CPU architecture - Tensor G3 performance is closer to the mid-range Qualcomm Snapdragon 7+ Gen 2 than it is to its current flagship chip the Snapdragon 8 Gen 2. Update: We've recorded a couple of short clips confirming that a persistent internet connection to enable cloud-based processing is indeed required for the new Pixel 8 Pro AI features including Magic Editor and AI Wallpaper. Purchase the Google Pixel 8 Pro 128GB from Amazon starting from $999. Working For Notebookcheck Are you a techie who knows how to write? Then join our Team! Wanted: - Specialist News Writer - Magazine Writer Details here Source(s) @Mrwhosetheboss via YouTube Own Top 10 Laptops Multimedia, Budget Multimedia, Gaming, Budget Gaming, Lightweight Gaming, Business, Budget Office, Workstation, Subnotebooks, Ultrabooks, Chromebooks under 300 USD/Euros, under 500 USD/Euros, 1,000 USD/Euros, for University Students, Best Displays Top 10 Smartphones Smartphones, Phablets, ‚â§6-inch, Camera Smartphones Related Articles Google Pixel 8 Pro falls short of Apple iPhone 15 Pro Max and Huawei P60 Pro in DxOMark camera tests 10/20/2023 OnePlus 12 rumoured to outshine Google Pixel 8 Pro with new ultra-bright display 10/16/2023 Google Pixel Watch recharges only half as fast after firmware update 10/15/2023 Google Pixel 8, Pixel 8 Pro benchmark block lifted 10/15/2023 Google Pixel 8 Pro and Tensor G3 showcase disappointing performance in GPU tests 10/15/2023 Google blocked Pixel 8, Pixel 8 Pro reviewers from using popular benchmarks to test the Tensor G3 chip, new owners too 10/14/2023 Google Pixel 8 Pro cuts a fine figure in scratch, burn and bend tests 10/13/2023 Benefits of Gorilla Glass Victus 2 upgrade shown in Google Pixel 8 Pro drop and scratch tests 10/09/2023 Pixel 8 Pro teardown reveals no vapor chamber, but thicker than usual graphite, copper layers instead 10/09/2023 Google Pixel 8 Pro impresses in early camera and video samples 10/06/2023 Read all 5 comments / answer Loading Comments Comment on this article Space startup with 3D printed rocke... Hisense C1 TriChroma Laser Mini Pro... Sanjiv Sathiah - Senior Tech Writer - 1413 articles published on Notebookcheck since 2017 I have been writing about consumer technology over the past ten years, previously with the former MacNN and Electronista, and now Notebookcheck since 2017. My first computer was an Apple ][c and this sparked a passion for Apple, but also technology in general. In the past decade, I‚Äôve become increasingly platform agnostic and love to get my hands on and explore as much technology as I can get my hand on. Whether it is Windows, Mac, iOS, Android, Linux, Nintendo, Xbox, or PlayStation, each has plenty to offer and has given me great joy exploring them all. I was drawn to writing about tech because I love learning about the latest devices and also sharing whatever insights my experience can bring to the site and its readership. contact me via: @t3mporarybl1p Please share our article, every link counts! > Notebook / Laptop Reviews and News > News > News Archive > Newsarchive 2023 10 > @MrWhosetheboss video reveals Google's Pixel 8 Pro Tensor G3 off-loads all generative AI tasks to the cloud Sanjiv Sathiah, 2023-10-20 (Update: 2023-10-21) Contact / ImprintData Privacy Declaration Languages: DeutschEnglishEspa√±olFran√ßaisItalianoNederlandsPolskiPortugu√™s–†—É—Å—Å–∫–∏–πT√ºrk√ßeSvenskaDo not share my personal information21.10.2023 10:56 * If you buy something via one of our affiliate links, Notebookcheck may earn a commission. Thank you for your support! dark / light / del",
    "commentLink": "https://news.ycombinator.com/item?id=37966569",
    "commentBody": "The Pixel 8 Pro&#x27;s Tensor G3 off-loads all generative AI tasks to the cloudHacker NewspastloginThe Pixel 8 Pro&#x27;s Tensor G3 off-loads all generative AI tasks to the cloud (notebookcheck.net) 226 points by redbell 20 hours ago| hidepastfavorite124 comments kelnos 12 hours agoGoogle said:> And in our new Tensor G3 chip, every major subsystem has been upgraded, paving the way for on-device generative AI.This is definitely weasel-worded, but note that this does not actually have to mean that all the generative AI stuff can be done on Tensor G3. They could (credibly, but, again, weasel-y) claim that the stuff they&#x27;ve done on G3 is prep work that is \"paving the way\" for a future chip to be able to do it all.But either way, I&#x27;m not terribly surprised? Generative AI on a mobile SoC (even with special-purpose hardware) still sounds like a bit of a stretch at this point, no? At least, doing it on-device with acceptable performance and power consumption seems a little unlikely? reply dagmx 12 hours agoparentThere are apps on iOS that do run stable diffusion like models locally on device.https:&#x2F;&#x2F;apps.apple.com&#x2F;ca&#x2F;app&#x2F;draw-things-ai-generation&#x2F;id64...I believe the Tensor chip is powerful enough to do the same but I suspect the efficiency is low enough that they don‚Äôt want to run it locally. reply hotstickyballs 12 hours agorootparentThat‚Äôs the difference between apple approach and google approach.Apple does a lot of computation on your own devices (in the name of privacy and what not) and ends up saving a lot of server compute.Googles strategy so far has been just burning cloud compute costs. reply disillusioned 8 hours agorootparentThis isn&#x27;t a fair characterization, though. At least not as reductively as you&#x27;re making it sound.Google ALSO performs transcription&#x2F;live captioning&#x2F;speaker diarization on-device, for privacy, for instance.The features being discussed here are the generative AI features, like Magic Editor and their heretofore not-yet-launch Video Boost mode. reply kyrra 8 hours agorootparentI would say it&#x27;s also done for copyright reasons. Google will not do assistant read aloud if the article is behind a paywall. But the new read aloud that is done on device allows for reading almost all any website. reply fooblaster 11 hours agorootparentprevGoogle has an enormous advantage over apple in data center compute. For one thing, they don&#x27;t have to pay the Nvidia monopoly tax to do inference. It&#x27;s silly to color this as a disadvantage. reply VectorLock 11 hours agorootparentprevNot that surprising really when you consider where the companies come from and what their other major successes are. reply valianteffort 11 hours agorootparentprevFrom what I understand google&#x27;s hardware doesn&#x27;t measure up to apple even when just comparing ML cores. reply hajile 7 hours agoparentprevwhy is this even the headliner complaint?Post-launch embargoes to deceive consumers about actual performance is a MUCH bigger deal. reply phatskat 1 hour agorootparentWhat is this in reference to? Sorry, OOTL on ‚Äúpost-launch embargoes‚Äù reply user_7832 18 hours agoprevQuote from Google:> Our work with Tensor has never been about speeds and feeds, or traditional performance metrics. It‚Äôs about pushing the mobile computing experience forward. And in our new Tensor G3 chip, every major subsystem has been upgraded, paving the way for on-device generative AI. It includes the latest generation of Arm CPUs, an upgraded GPU, new ISP and Imaging DSP and our next-gen TPU, which was custom-designed to run Google‚Äôs AI models.Then why the internet requirement? Were they trying something that failed last minute forcing them to ship it the way it is? Or was this always just advertisement material? reply dataflow 17 hours agoparent> Our work with Tensor has never been about speeds and feeds, or traditional performance metricsMeanwhile, on [1] they say:> Pixel 8 and Pixel 8 Pro are equipped with Google Tensor G3, Google‚Äôs fastest, most efficient, and most secure chip yet. Every major component of the chip has been upgraded for enhanced performance and efficiency. Not only has the number of machine learning models on device more than doubled since 2021, but their complexity and sophistication have increased as well. And Tensor is running many of these complex models at the same time.Sounds to me like it was very much about performance...[1] https:&#x2F;&#x2F;store.google.com&#x2F;intl&#x2F;en&#x2F;ideas&#x2F;articles&#x2F;google-tenso... reply ForkMeOnTinder 18 hours agoparentprev> Were they trying something that failed last minuteSeems likely. Your chip underperforms? Just block people from using benchmarking apps, and also tell the press you never cared about speed. Problem solved. reply josephcsible 16 hours agorootparentSee https:&#x2F;&#x2F;www.notebookcheck.net&#x2F;Google-blocked-Pixel-8-Pixel-8... for details about \"block people from using benchmarking apps\". reply AlexandrB 15 hours agorootparentWtf? How is this not an abuse of market power? reply fidotron 13 hours agorootparentIt&#x27;s rife across the whole benchmarking industry. Lies, damn lies, and benchmarks.The massive problem, which has particularly afflicted Qualcomm and Intel in the past, is the difference between peak and sustained performance being absolutely huge, since running at peak would trigger thermal conditions necessitating dynamic down clocking to below normal speeds, which would then persist for a surprisingly long time before normal performance resumes. Certain groups would detect known benchmarking code and alter the acceptable thermal parameters for the execution of the benchmark tests. (i.e. allow the device to become unusually hot).As others have mentioned this has all the hallmarks of an execution failure, either by Google, Samsung, both, or some unknown third party. Google have a cultural problem of believing way too much in theoretical untested potential solutions instead of aiming for the boring known to work but only 95% as theoretically good, only to find when building it reality is different enough to more than nullify the advantages. It has bitten them in this area before, and it more than likely will again. reply hypercube33 3 hours agorootparentNow that you mention it...I have only seen one review about workstations that benchmarked machines for 24 hours to make sure they didn&#x27;t thermally saturate and throttle and I&#x27;m wondering why that isn&#x27;t more of a metric for non mobile stuff. Heck even mobile workstations may be put under heavy load for durations and this would show if their cooling keeps up or falls short reply canucker2016 14 hours agorootparentprevthe notebookcheck.net article states: \"In a highly unusual move, it has been revealed that Google has blocked reviewers of the Pixel 8 and Pixel 8 Pro from installing popular benchmarks including Geekbench and 3D Mark.\"there&#x27;s an embedded URL linking to a YouTube video, which shows that the Google Play store won&#x27;t allow the user to install Geekbench or 3dMark on the pre-released Pixel 8 review devices.see https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=bk4ZUmKqRm0&t=330 reply charcircuit 9 hours agorootparentThose app are notorious for leaking performance. At least with geekbench you have to buy a license to even have the choice to not upload results. reply numpad0 3 hours agorootparentprevsaw some phone geeks suspecting last minute addition for thermal dispersion under LCD as well as frequency limiter to manage heat. not sure where the suspicion came from but sounds plausible reply intelVISA 12 hours agorootparentprevthey can&#x27;t keep getting away with this (Copy)(2) reply danpalmer 14 hours agoparentprevThere&#x27;s a bit of a difference between an on-device chip for some AI tasks (like all modern smartphones and some computers have), and high end server-class graphics cards&#x2F;GPUs&#x2F;accelerators than take hundreds of watts to run for generative AI.State of the art for low end generative AI is still running on some sort of desktop class GPU with optimised models and getting not-so-great results. A phone can&#x27;t do that.Lots of ML happens on device, more is moving to the device. Generative AI is not ready for that yet from what I can tell (based on external experience, not Google specific). reply canucker2016 13 hours agorootparentfrom Feb 2023 blog post, https:&#x2F;&#x2F;www.qualcomm.com&#x2F;news&#x2F;onq&#x2F;2023&#x2F;02&#x2F;worlds-first-on-de...:\"...The result of this full-stack optimization is running Stable Diffusion on a smartphone under 15 seconds for 20 inference steps to generate a 512x512 pixel image ‚Äî this is the fastest inference on a smartphone and comparable to cloud latency. User text input is completely unconstrained.\"Hasn&#x27;t shown up in a shipping cellphone yet AFAIK. reply hypercube33 3 hours agorootparentThe problem is all of these mobiles have custom proprietary AI chips with custom SDKs and are usually not even market share (looking at you Qualcomm Microsoft and Google) so beyond the OEM is it worth any third party to invest time in development for them? Seems like general purpose gpu would be a better pick reply danpalmer 13 hours agorootparentprevThere&#x27;s a big difference between tech demos and production software for a general audience. Generative AI is only barely production software at the best of times, so almost any compromises mean it&#x27;s not shippable.This blog post is interesting, but there are so many caveats to it. It&#x27;s a 1B parameter model which is tiny. Inference takes 20 seconds, which sucks for UX, but also that will be 20s of sustained extremely high load on the device, which means battery drain that would probably make this unshippable as well. It&#x27;s also worth noting that the images in the blog post were not generated by this process as far as I can tell, they&#x27;re just stable diffusion examples.It&#x27;s good this research is being done of course, but I think we&#x27;re a few years away from this being shipped in any real form on device. reply canucker2016 12 hours agorootparentYou stated, \"State of the art for low end generative AI is still running on some sort of desktop class GPU with optimised models and getting not-so-great results. A phone can&#x27;t do that.\"The Qualcomm article states, \"Read on to learn how Qualcomm AI Research performed full-stack AI optimizations using the Qualcomm AI Stack to deploy Stable Diffusion on an Android smartphone for the very first time. \" reply danpalmer 12 hours agorootparentOk I guess it depends what you think of state of the art as being. I was thinking in terms of the best option that is somewhat generally available, like the best thing I could do with tools and techniques available. Qualcomm&#x27;s research is better, but at an earlier stage. Maybe that is state of the art! Not quite as useful as I was thinking about though. replywccrawford 16 hours agoparentprevThat&#x27;s exactly how it went with Stadia and \"negative latency\", and that was never actually implemented before they shuttered Stadia. reply qwertox 17 hours agoparentprevpaving the way for on-device generative AI seems to indicate that they&#x27;re not there yet. reply antifa 5 hours agorootparentTo me, that choice of words indicates Google is intending to claim the hardware is ready. reply troupo 17 hours agorootparentprevI wonder if they can get there. Apple did bet on the on-device ML, with varied results. I doubt Google can get around their \"we run everything in the cloud\" mindset.As an anecdotal data point: they recently silently defaulted storing photos in Google Photos instead of on device (tweet in Russian: https:&#x2F;&#x2F;twitter.com&#x2F;igrekde&#x2F;status&#x2F;1715456025594134690) reply jpalawaga 16 hours agorootparentgoogle has tons of on-device ml. live audio transcriptions, for example. always-on shazam.generative ai is very computationally expensive, even for inference. I too wonder if they can get there, but nobody has really done it. reply kortilla 13 hours agorootparentIs Shazam considered AI? I thought fingerprinting songs was a relatively straight-forward algo that didn‚Äôt require any training. reply ShamelessC 8 hours agorootparentSort of. They were early to use vector databases for semantic search. Now we use embeddings from a neural net as features for semantic search, I think they used lots of labels and analyzed features like beats per minute, chord progression, etc. happy to be corrected of course. replybarkingcat 17 hours agoparentprevAll of that text you quoted is pure advertising nonsense. It&#x27;s like reading buzzword after buzzword. reply SoftTalker 16 hours agoparentprev> Then why the internet requirement?So they can include it in their advertising profile about you, and later insert ads into the generated content. reply hef19898 15 hours agorootparentLater? reply l33t7332273 5 hours agorootparentDo they currently? reply IshKebab 16 hours agoparentprevIt&#x27;s pretty obvious - some ML tasks are easy enough to do on the phone (e.g. speech recognition); some are not (e.g. inpainting).Yeah they are being a bit slippery about it, but I don&#x27;t think it&#x27;s really a surprise that ML models that require a GTX 4090 to run aren&#x27;t going to work on a phone. reply izacus 16 hours agorootparentThe fact that those tasks are running in cloud was called out in Pixel 8 keynote and mentioned in their blog post as well - https:&#x2F;&#x2F;blog.google&#x2F;products&#x2F;pixel&#x2F;google-pixel-8-pro-camera...This seems yet another nothingburger news to generate clicks and HN rants. reply canucker2016 14 hours agorootparentFrom the keynote, the only thing I remember being called out as offloaded to the cloud was the HDR video - a feature that will show up eventually on the phone.All of the AI photo-related tasks that the Google employees demoed, i.e. inpainting et al, happened in realtime.Rewatching the keynote just now, I see some of the Magic Editor-related scenes have the following disclaimer in tiny white text, \"Features simulated and sequences shortened throughout ad. Magic Editor coming soon\" - see https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=pxlaUCJZ27E&t=2416s reply IshKebab 14 hours agorootparentprevIt&#x27;s not mentioned in that blog post except for Video Boost. reply user_7832 16 hours agorootparentprevWhile I mostly agree with this, apparently some AI features run fine offline on the Pixel 7 Pro but not on the 8&#x2F;Pro (as per a comment on the article). And I&#x27;m not sure if the computation for AI wallpapers is that high to need to be offloaded too. But then again, I think this was more of a bug than an intended feature. reply nunez 17 hours agoparentprevMaybe Google intends on pushing those activities entirely on device later? reply IAmGraydon 17 hours agorootparentI was thinking this. It may not be ready yet and they had to launch before it was done. reply fotta 16 hours agoprevHas anyone looked at network traffic while using the Magic Editor to see if it‚Äôs actually offloading the processing? Or is this just an inferred conclusion based on needing an internet connection? Is it possible that it requires an internet connection to fetch new models or something but the processing is still on device? reply privacyking 13 hours agoparentThe magic editor (rainbow button on Google photos) only works if you enable cloud backup, so yes. reply hedora 9 hours agorootparentAll sorts of stuff in photos breaks in degoogled phones.It wouldn‚Äôt surprise me at all if they decided to disable on device features unless you sent them your data so they could monetize it, share with oppressive regimes, etc. reply partiallypro 16 hours agoparentprevThis seems like the most likely answer to me, but getting little fanfare here. reply baby_souffle 15 hours agoprevThis makes some amount of sense: generative AI is _very_ expensive.But generative is only one class of AI workload; predictive&#x2F;inference is probably what the tensor is used for most. I would not be shocked to find out that the \"find object in this photo\" part of magic eraser worked on device but the \"figure out what to put in picture after object removed\" bit is done server side. reply randyrand 14 hours agoparentThere is inference & training. generative is inference. reply astrange 7 hours agorootparentMany generative tasks like inversion&#x2F;dreambooth&#x2F;finetuning aren&#x27;t inference. reply jan_Sate 18 hours agoprevIf they&#x27;re offloading to the cloud, what&#x27;s the AI processor in Tensor G3 even used for? reply coder543 18 hours agoparentIt is used for a few non-generative tasks like speech recognition, but other chips are also capable of these tasks.The only thing the Tensor G3 seems to be better at is avoiding Qualcomm&#x27;s habit of EoLing consumer smartphone chips. reply fidotron 18 hours agorootparentI think the truth is the Tensor is a barely modified Samsung Exynos. reply coder543 17 hours agorootparentThe Tensor G1 was probably pretty close to that, but I think subsequent generations have been diverging more. They are still made at Samsung&#x27;s fabs, and I wouldn&#x27;t be surprised if Samsung is still helping to design the chips.Either way, I haven&#x27;t seen anything to be impressed by when it comes to the Tensor SoCs themselves. The performance is lackluster, and in spite of Google&#x27;s marketing, the AI acceleration isn&#x27;t clearly better than what Qualcomm or Apple are offering. reply kcb 14 hours agorootparentprevAll the tensor chips so far have had different CPU core configurations than available Exynos chips. So it isn&#x27;t just a rebadge. reply fh9302 18 hours agorootparentprevIf that&#x27;s the case, why do phones with the Tensor G1 and G2 only receive 3 major updates? That even applies to the Pixel Fold, a $1800 phone released a couple months ago. There&#x27;s no Qualcomm to blame here, it appears to be purely a business decision by Google. reply coder543 18 hours agorootparentWhere did I mention the Tensor G1 and G2? I didn&#x27;t. I only said \"Tensor G3\". Yes, feel free to blame Google for not supporting the G1 and G2 better.None of that changes that Qualcomm does not support consumer smartphone chips beyond a couple of years. reply pjmlp 16 hours agorootparentGoogle is not a naive poor company that isn&#x27;t capable to have a lawyer company setting up proper support contracts.In fact, Microsoft has no issues imposing exactly that for Windows ARM devices since the Windows CE&#x2F;Pocket PC days.It seems it needs to be called for every time this is used as excuse. reply coder543 16 hours agorootparent> Google is not a naive poor company that isn&#x27;t capable to have a lawyer company setting up proper support contracts.Set up contracts with whom, exactly? Google doesn‚Äôt control the other OEMs, and the other OEMs are the ones negotiating with Qualcomm, not Google. Unless you‚Äôre specifically talking about the Pixel line, which doesn‚Äôt have enough market share to force Qualcomm to agree to anything.Are you perhaps suggesting Google should suddenly and unilaterally prevent Android from working on Qualcomm devices until Qualcomm agrees to support it for X years? Which, if it were even possible, would hurt the other OEMs more than it would hurt Google?Or are you suggesting that Google stop working with the other OEMs until they agree to negotiate with Qualcomm to support their devices longer?Google can‚Äôt prevent Qualcomm or the other OEMs from compiling Android for their devices; it‚Äôs not closed source like Windows. Your comment is confusing.If Android were closed source, and if Google were negotiating this from the beginning, your comment would make sense. With an open source product, Qualcomm can just do whatever they need to in order to bring Android to Qualcomm devices. The cat is already out of the bag. Google has no say in the matter.Google could always break the Play Store &#x2F; Google Play Services on Qualcomm devices, I suppose, but that would directly hurt Google, Google‚Äôs OEM partners, and the developers who build for the Google Play Store. More importantly, a move like this would make Samsung push much harder on their own App Store, and Google would attract the scrutiny of unhappy regulators. Google has no incentive to do any of that. reply ClumsyPilot 16 hours agorootparent> Are you perhaps suggesting Google should suddenly and unilaterally prevent Android from working on Qualcomm devices until Qualcomm agrees to support it for X years?Yes, literally> Which, if it were even possible, would hurt the other OEMs more than it would hurt Google?Seriously? Whats the revenue share of Qualcomm&#x27;s sales from Android vendors, 99.9%? The negotiation would last one meeting.It would only hurt the head of the Qualcomm CEO when doing a quarterly report. reply izacus 15 hours agorootparentThat would be a massive violation of Android Open Source license - something this site constantly rants about when Google moves a new component behind closed doors. People are screaming at Google for \"monopoly abuse\" when they move the SMS app into closed source and now you want them to do what exactly? Ban OEMs from using Apache licensed Android until they bow down to their business? How do you expect that to work?Like, do you people even think for a second when you write this stufF? reply wmf 12 hours agorootparentGoogle Android diverged from AOSP long ago and already has hundreds of pages of requirements. https:&#x2F;&#x2F;source.android.com&#x2F;docs&#x2F;compatibility&#x2F;14&#x2F;android-14-... Likewise Google abused their monopoly for years, so why not use that power for good? https:&#x2F;&#x2F;www.theverge.com&#x2F;2022&#x2F;9&#x2F;14&#x2F;23341207&#x2F;google-eu-androi... reply izacus 2 hours agorootparentBecause both US and EU are suing them to prevent that. reply pjmlp 16 hours agorootparentprevMany questions that lawyers in international trade law have certainly an answer for.If Google actually cared about making it happen. reply coder543 16 hours agorootparentThanks for those ‚Äúinsights‚Äù into trade law‚Ä¶Google isn‚Äôt above the law, despite what some people in this thread apparently wish. reply pjmlp 16 hours agorootparentI bet Google can afford to pay for that insight.After all they had enough money for lawyers, to avoid becoming Microsoft on the second lawsuit of Java history After screwing up Sun. reply coder543 16 hours agorootparentMoney can‚Äôt just make anything happen for any reason. Please re-read my first comment to you. Google has no apparent legal authority to stand on to do what you‚Äôre suggesting. You‚Äôre not even pretending to explain where they would get the legal authority instead of getting sued into oblivion.You‚Äôre just shouting at Google to go stomp over to Qualcomm and throw a tantrum since they have no authority to more. It makes no sense.Good luck with that. reply jsnell 15 hours agorootparentprevSo they&#x27;ve paid for that insight, and then proceeded to not do the thing you think they&#x27;re able to and should do. How exactly is this supposed to support your position? Isn&#x27;t it much more a sign that any lawyer would tell them \"hell no\" when presented with your plan. reply pjmlp 15 hours agorootparentWe know what we don&#x27;t know. replyfh9302 18 hours agorootparentprevCan you show me evidence that Qualcomm refuses to support chipsets even in the case where they would get paid to extend support? reply wmf 17 hours agorootparentI wonder if there&#x27;s a coordination problem where no individual Qualcomm customer wants to pay the entire cost of extended support but if they all shared that cost it would be reasonable. reply coder543 18 hours agorootparentprevFairphone made some claims about this when they launched the Fairphone 5. That&#x27;s the sole reason it&#x27;s not using a normal consumer-facing Snapdragon chip, and instead using some kind of industrial Qualcomm chip. Fairphone pointing this out wasn&#x27;t exactly news to anyone who has been paying attention to this stuff.I also wouldn&#x27;t be too surprised if Qualcomm starts to change their tune under pressure from other manufacturers who want to stay competitive with Google. reply fh9302 18 hours agorootparentMishaal Rahman claims it&#x27;s purely a business decision and not an artifical limitation set by Qualcomm: https:&#x2F;&#x2F;twitter.com&#x2F;MishaalRahman&#x2F;status&#x2F;1710442249778086260 reply coder543 18 hours agorootparent\"Business decision\" and \"artificial limitation\" are frequently synonymous. I think @MishaalRahman was correct to say \"If OEMs keep buying a particular chipset [...] then it&#x27;ll continue being supported\", especially with Qualcomm, if the sales volume remains high enough. I also agree it is generally correct to say you can pay an SoC vendor to continue supporting a chipset, but that doesn&#x27;t seem to have been correct for Qualcomm.If a particular mid-range chip was still selling well years after release, then Qualcomm may have seen fit to continue supporting it so that those new phones could continue selling well. Qualcomm primarily cares about selling chips. Supporting old chips that have already sold (and aren&#x27;t continuing to sell well) does not help Qualcomm sell more chips, so they haven&#x27;t wanted to do it. I think Qualcomm may come around to supporting chips better (maybe even at their next announcement in a few days), but their historical behavior in this regard has been less than ideal.In fact, Qualcomm cares so much about selling chips that they&#x27;ve allegedly nearly sunk the Oryon SoC in the process.[0] They apparently saw an opportunity to force-bundle more chips, in this case the PMICs, which were so unsuitable that the manufacturers wanted to buy them and literally throw them in the garbage just to keep Qualcomm happy, while using alternative PMICs instead. But, Qualcomm supposedly baked the decision into Oryon so deeply that only Qualcomm&#x27;s PMICs are compatible. Apparently, this is causing manufacturers to consider abandoning Oryon entirely. Most people would logically think that Qualcomm cares primarily about making money, but instead their first priority actually seems to be selling chips, even if it means leaving money on the table, regardless of whether that is logical or not.Why did Fairphone choose to use an industrial SoC if they could have just paid Qualcomm a little more money to extend the life of the Snapdragon 8 Gen 2?[0]: https:&#x2F;&#x2F;semiaccurate.com&#x2F;2023&#x2F;09&#x2F;26&#x2F;whats-going-on-with-qual... reply tadfisher 11 hours agorootparentQualcomm is happy to enter a support contract for the BSP they provide. It will be frozen at that already-outdated LTS kernel with whatever mainline backports they feel the need to apply, and the drivers will be the same binary blobs shipped on day one because Android HAL.This, understandably, runs into Google&#x27;s CTS policies regarding minimum kernel versions and others, which is why 2 OS releases + 1 year security became the norm.Still, Projects Treble and Mainline just reached the point where you can stick with 5.x kernels and have an extended support schedule. This involved revving the HAL interfaces among other changes that are just not (economically) feasible to backport. OEM BSPs that are derived from Android Common Kernel releases can feasibly be supported for many years; for example, 5.10 shipped to AOSP in 2021 and will be supported through 2026 [1].1: https:&#x2F;&#x2F;source.android.com&#x2F;docs&#x2F;core&#x2F;architecture&#x2F;kernel&#x2F;and... reply my123 16 hours agorootparentprevExtended security update support by Qualcomm is very much possible if you&#x27;re a big OEM, but can be quite expensive.If you don&#x27;t get money after the sale of the device, even $1 can be too much to pay for on that front. replyimjonse 17 hours agoparentprevMaybe it has hw acceleration support for HTTP operations :) reply baz00 13 hours agoparentprevMarketing street cred. reply izacus 16 hours agoprev\"Tensor\" isn&#x27;t offloading anything, it&#x27;s the Android apps that are doing that to Google cloud. Not sure what the SoC has to do with that? reply jrflowers 15 hours agoparentWho wrote the Android apps you‚Äôre talking about? reply wmf 16 hours agoparentprevBut why don&#x27;t those apps use the local AI engine? reply izacus 15 hours agorootparentEr, have you seen the hardware requirements for latest high quality AI models? They won&#x27;t even run on your MacBook, much less on a mobile SoC.This really shoulnd&#x27;t be a hard thing to check on a HACKER news forum which constantly talks about modern AI. reply astrange 7 hours agorootparentWhich latest high quality AI models? MacBooks have unified memory which means they are well suited for running many of them. (As long as it&#x27;s the 16GB+ models.) reply izacus 2 hours agorootparentAnd Pixel 8 doesn&#x27;t have 16GB+ of RAM. reply abeyer 15 hours agorootparentprevBecause ads aren&#x27;t local reply kibwen 15 hours agorootparentThe mothership needs to inform the phone whether it should inject a subliminal Pepsi logo into the generated image, or a subliminal Nike logo. reply kristjansson 9 hours agoprevDuh? High quality generative features on-device would be a shocking leap. reply dicriseg 16 hours agoprevThe headline immediately reminded me of the Juicero.How can they advertise on-device if they‚Äôre immediately sending your data elsewhere? reply whartung 12 hours agoprevIt&#x27;s going to be an interesting conundrum for Apple, with their drive to keep everything on phone.Both the computing requirements and the data requirement on modern AI make it a challenging fit. So it will be a curiosity about how Apple tries to balance a feature set that they can power on the phone, net take advantage of the modern AI capabilities.Or, just bail, and funnel it to the net. reply wallaBBB 12 hours agoparentMy initial reaction was completely opposite to yours: - Oh this is going to burn google&#x27;s money, especially if the phone becomes popular.Unless you want to charge a subscription for a feature it really isn&#x27;t smart for Apple to try something like this. reply foota 12 hours agoparentprevAren&#x27;t the M1 and friends relatively capable for LLM tasks? reply brucethemoose2 11 hours agorootparentSort of.Apple is super stingy about RAM capacity on their devices, which is a problem for GenAIAlso, the M1 NPU (the equivalent to Google&#x27;s offloading) is not very fast in Apple&#x27;s own Stable Diffusion port, and I&#x27;m not aware of any community LLM frameworks that even use the NPU. They all run Metal implementations on the M1&#x27;s GPU. reply ActorNightly 10 hours agoparentprevApple doesn&#x27;t care about on or off phone, its only goal is to make trendy devices that sell because of the badge. reply astrange 7 hours agorootparentApple spends over $20 billion a year on R&D. reply 5cott0 11 hours agoprevOn-device is already possible on mobile?https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;tarot-raven&#x2F;id1539747669 reply lopkeny12ko 14 hours agoprevEverything old is new again. After delivering technology over the last 20 years to seamlessly offload arbitrary computation to a distributed cluster of dedicated hardware, managed entirely transparently by people you&#x27;ve never met, yes let&#x27;s just go back to slow, local compute because that&#x27;s \"cooler\" now or something. reply liuliu 7 hours agoparentIt is a lie that we tell people \"your compute is offloaded to the cloud\" until now. User-centric heavy compute in the past 20 years simply don&#x27;t happen in \"the cloud\". Monetization and engagement tricks yes (ads bidding, behavior analysis, friend &#x2F; content recommendations, content recompression), but not user-centric ones. reply randyrand 14 hours agoparentprevcooler, fully reliable, and privacy preserving (ideally). reply nmstoker 16 hours agoprevIt&#x27;s strange as the Magic Editing feature, whilst showing promise turned out to be massively underwhelming... It&#x27;s kind of slow, so you can&#x27;t really experiment freely and the results usually aren&#x27;t that great (maybe with practice it would be easier to get good results but that&#x27;s not happening due to the clunky speed) reply quitit 16 hours agoparentGoogle&#x27;s smartphone&#x2F;AI announcements are used to portray them as being massively ahead of their smartphone competitors. The actual product on the other hand is not what they advertise.Take Duplex for example. It was meant to be this flexible and natural AI that even the store wouldn&#x27;t notice. In 2023 it&#x27;s a clunky chatbot experience, it has a patchwork of features depending on the user&#x27;s location and sounds so much like a robocall that there is a problem with how many stores hang up on it. reply annaaa 15 hours agorootparentIf I remember correctly, Duplex was intentionally modified this way after controversy from the initial demo, when, during a call, it didn&#x27;t obviously identify itself as an automated system. reply quitit 14 hours agorootparentHaving to identify as a chatbot and provide a call back number was a requirement under some states&#x27; laws - while this seems like something they should have looked into before advertising it, my comment was with a wider scope to how it performs: it&#x27;s not flexible and it doesn&#x27;t sound natural. reply avipars 17 hours agoprevWhat is the point of hyping up the G3 then? reply clnq 12 hours agoparentWeasel words marketing is still marketing. reply causality0 16 hours agoprevMan I&#x27;d kill for a Pixel 8 Pro with a snapdragon. reply sergiotapia 10 hours agoparentThe days of pining for the highest geekbench score are over.It&#x27;s all about utility and how much I can do with my phone. I don&#x27;t care about how many cpu megarams my phone has. Can I fix a picture I took of my wife and kids as they passed by on the Hulk rollercoaster? reply wkat4242 9 hours agorootparentThere&#x27;s a huge difference in battery life between Snapdragon and Exynos devices though. It&#x27;s so clear because Samsung makes the same device in both configurations.For tensors we don&#x27;t have such an exact comparison but it does seem that Samsung&#x27;s fab plays a role in this.And battery life is still very much a hot topic. reply sva_ 9 hours agorootparentprevI think the Snapdragon 8 Gen 2 does just fine with doing some matmuls, just as the Pixel tensor chip does.Moreover, the Snapdragon uses less battery overall and stays cooler. But the differences seem marginal. reply causality0 9 hours agorootparentprevFor me it&#x27;s not about performance, it&#x27;s about the Mali GPU cores. They seriously gimp game console emulation. reply jauntywundrkind 18 hours agoprev> For all of this generative AI stuff, for anything that actually has to use AI to create things like the AI Wallpaper making, the Magic Editor needs a permanent internet connection... It feels so sluggish, that you are constantly reminded that it&#x27;s not running on-device...This is fine.Generative AI has gotten good by being beastly huge, consuming gobs of resources. It&#x27;s not particularly fast even on >$1000 consumer GPUs.It feels like it would be an enormous waste of time and effort to try to scale these generative tasks down to small on-device footprints, where they can fit and run in reasonable time, knowing how much worse the results would be.I don&#x27;t see what the alternative is here. But it certainly does leave the question open of what exactly are the great tensor tasks for the edge. With chipmaker like AMD and Qualcomm and ARM all putting on sizable neural&#x2F;tensor cores too, the question is a somewhat concerning one. reply ip26 14 hours agoparentLiterally anything latency sensitive is appropriate for edge. Off-device voice recognition is a miserable experience anytime you have less than a perfect connection. Imagine clicking an object in photoshop, using AI to determine the edges‚Ä¶ and every time you click, photoshop bundles the image, uploads it to the cloud, and waits five seconds for every select clicking to be scheduled, dispatched, and returned. Imagine running the camera app with ML object recognition focusing, except the cloud is focusing your camera and tracking objects in the viewfinder, with a three second lag. reply Moldoteck 18 hours agoparentprevIt&#x27;s fine, but then this means you have a poor processor that is not even capable of doing what it was advertised for. G3 is worse than snap gen2 in terms of performance, maybe close to gen 1(and I&#x27;m not sure battery consumption is comparable) and snap is about to release gen3 reply hiddencost 15 hours agoparentprevSpeech recognition, wakeword detection, object recognition, auto complete, ...Wakeword especially is a continuously running process that needs a relatively small foot print CNN over a fixed window, where power is extremely important. reply jessfyi 16 hours agoparentprevIt&#x27;s not fine when this \"magic\" is being advertised as on-device.After reading (and attempting to quickly implement the models ensembles within) both the RealFill[0] and Break-A-Scene[1] papers published from Google researchers just prior to the Pixel 8 launch I was expecting either a leap in their G3 tensor core akin to 2013 Moto X NLP+contextual awareness cores[2] (which provided better implementations of Active Display, gesture recognition, and voice recognition in loud environs than 95% of current mobile devices) or the Coral[3], the edge TPU they developed that got shockingly amazing inference performance from (though HW production handed off to ASUS in 2022--thanks to the chip shortage, the general arbitrary nature of the company, and their wholesale divestment from IoT) I expected more.All that to say this: your assumptions of inference performance on >$1000 hardware are fundamentally flawed (the fact that you reach for the buzzy \"generative\" prefix suggests they&#x27;re erroneously informed by twitter influencers and attempting to deploy current LLMs.)Custom hardware can and has been developed in the past (on mobile devices) that could&#x27;ve been tailored to the task at hand. If they failed to meet performance, power draw, or processing time requirements, they should&#x27;ve reframed their pitch instead of exposing themselves to what is likely going to be yet another class action suit focusing on their hardware.[0] https:&#x2F;&#x2F;realfill.github.io&#x2F; [1] https:&#x2F;&#x2F;omriavrahami.com&#x2F;break-a-scene&#x2F;static&#x2F;paper&#x2F;Break-A-... [2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Moto_X_(1st_generation)#Hardwa... [3] https:&#x2F;&#x2F;coral.ai&#x2F; reply jsnell 15 hours agorootparent> It&#x27;s not fine when this \"magic\" is being advertised as on-device.I can&#x27;t help but notice that you included a lot of references, but none for this claim.Neither of the features mentioned in the article is claimed to be on-device in the official Pixel 8 Pro announcement blog[0]. The only feature that the blog post claims is on-device is the Best Take feature, which the article does not say requires an internet connection.But of course that&#x27;s just one bit of marketing material, and I&#x27;m sure you&#x27;ve seen these features advertised as happening on-device. Maybe you could post a link?[0] https:&#x2F;&#x2F;blog.google&#x2F;products&#x2F;pixel&#x2F;google-pixel-8-pro&#x2F; reply jpalawaga 16 hours agorootparentprev> Custom hardware can and has been developed in the past (on mobile devices) that could&#x27;ve been tailored to the task at hand.You think google doesn&#x27;t know that? are you aware of what&#x27;s inside of Google&#x27;s phones?I&#x27;m not sure what performance benefit you expect out of custom hardware. How many orders of magnitude? You&#x27;re going to probably need at least a few, probably more, to make generative AI work well in the palm of your hand.Oh, and if you&#x27;ve figured that out, Apple, Google, OpenAI, and other AI companies would like a word. reply jessfyi 13 hours agorootparentI&#x27;m aware that what&#x27;s in Google&#x27;s phones aren&#x27;t capable of doing the on-device ML inference they claim. You might want to actually read what both I and the article are addressing in particular beyond the broad \"generative AI\" umbrella that you and other philistines new to the field are imagining aren&#x27;t capable of being performed on device. reply Topfi 15 hours agorootparentprev> But of course, on-device generative AI is really complex: 150 times more complex than the most complex model on Pixel 7 just a year ago. Tensor G3 is up for the task, with its efficient architecture co-designed with Google Research.This is a direct quote from an official press release [0]. They claimed Tensor G3 is \"up for the task\" that is \"on-device generative AI\".I&#x27;d say \"if you can&#x27;t do it, simply don&#x27;t promise it\", but the fact is, this is the third time Tensor has been outright incapable of what was promised. People pointing that out are more than justified.Prior to the launch of the Pixel 6, with their first generation of Tensor SOC, they made big promises concerning HDR video performance [1], implying heavily or outright stating (depending on whom generous you want to be) that they&#x27;d finally manage to be on par with Apple. They weren&#x27;t, by a lot. Pixel 6 video performance was neither on par with Apple nor did it exceed the Pixel 5 on an upper-mid SD765G. Still, first-gen and a bit of overhyping happen to the best of us.During the Pixel 7 launch [2], they claimed Tensor G2 enabled users to finally get computational photography for high-quality videos. Spoiler alert: It didn&#x27;t. Fool me once...Now, on the Pixel 8 with their third generation of Tensor, they finally have a solution that gets their nighttime video processing results competitive with the current iPhone in the form of Video Boost. Instead of doing that processing on their amazing Tensor SOC though, they offload that to the cloud [3]. At least they didn&#x27;t promise on-device processing improvements to video with the G3, only a tone of GenAI capabilities...I have followed Tensor extensively, and I am happy to see that they are at least utilizing their control over the silicon to provide a longer update cycle. But few of their local processing promises have held water, and even fewer appear to be impossible on contemporary SOCs from competitors such as Qualcomm (who are by no means angles and need all the competition the market can provide).If the Pixel team were more honest about their SOCs capabilities and proactively transparent on what they run locally vs off-load to datacenters, that&#x27;d be appreciated. With Video Boost they did just that, though I fear that was mainly because of the upload times...[0] https:&#x2F;&#x2F;blog.google&#x2F;products&#x2F;pixel&#x2F;google-tensor-g3-pixel-8&#x2F;[1] https:&#x2F;&#x2F;9to5google.com&#x2F;2021&#x2F;08&#x2F;02&#x2F;google-pixel-6-video-hdr-t...[2] https:&#x2F;&#x2F;www.youtube.com&#x2F;live&#x2F;2NGjNQVbydc?si=2Gg1mPrdOkmu1L44...[3] https:&#x2F;&#x2F;blog.google&#x2F;products&#x2F;pixel&#x2F;google-pixel-8-pro&#x2F; reply retskrad 18 hours agoprevGoogle is not a hardware company. Their hardware can never be trusted. reply edandersen 9 hours agoprev [‚Äì] Now we know where the ‚Äú7 years of support‚Äù came from - that‚Äôs when they will&#x2F;can turn the servers off, remotely removing advertised features. reply astrange 7 hours agoparent [‚Äì] It&#x27;s because that&#x27;s the storage writes and repair parts availability lifespan. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "YouTuber @Mrwhosetheboss criticizes the Google Pixel 8 Pro's Tensor G3 processor, arguing that contrary to Google's marketing, the device requires a constant internet connection for certain AI functions, implying that it off-loads AI tasks to the cloud.",
      "The YouTuber points out that the Tensor G3 chip's performance is slower than expected and comparable to that of mid-range processors, questioning Google's flagship claims.",
      "Google is additionally criticized for preventing reviewers from using benchmark apps during the review embargo period, which could have allowed for objective performance assessment."
    ],
    "commentSummary": [
      "The key discussion is about the readiness and capabilities of generative AI on mobile devices, especially concentrating on Google's Pixel 8 Pro and its Tensor G3 chip.",
      "There are doubts regarding the chip's efficiency, Google's claims about its performance, and concerns about offloading AI tasks to the cloud impacting user privacy.",
      "It also mentions Qualcomm's discontinued support for older chipsets and the limitations of on-device AI processing, while criticizing Google for not fulfilling promises and demanding more transparency and long-term support."
    ],
    "points": 226,
    "commentCount": 124,
    "retryCount": 0,
    "time": 1697894055
  },
  {
    "id": 37970484,
    "title": "Canada has fewer entrepreneurs today than it did 20 years ago",
    "originLink": "https://www.cbc.ca/news/business/canada-entrepreneurs-shortage-solutions-1.7002171",
    "originBody": "Content Skip to Main Content Accessibility Help MENU Search Sign In news Top Stories Local Climate World Canada Politics Indigenous Opinion The National More Canada has fewer entrepreneurs today than it did 20 years ago ‚Äî and that's a big problem for everyoneCBC News Loaded Business Canada has fewer entrepreneurs today than it did 20 years ago ‚Äî and that's a big problem for everyone With 100,000 fewer entrepreneurs than in 2000, BDC recommends skills training James Dunne ¬∑ CBC News ¬∑ Posted: Oct 20, 2023 4:00 AM EDTLast Updated: October 20 James Lynn, the founder of independent pet food-maker Kal≈´ in Montreal, is shown at work with his dog, Sunshine. Lynn is among a declining number of Canadians getting into entrepreneurship. (Dan Lynn) Social Sharing Facebook LinkedIn X Email Reddit 192 comments When James Lynn lost his job early in the COVID-19 pandemic, he decided to pursue his dream of becoming an entrepreneur. It hasn't been easy. \"You know, entrepreneurship is like being in a maze, where you know where you want to go but you're not exactly sure how to get there,\" he said. \"And there's a ton of pitfalls.\" Lynn is the founder of Kal≈´, an independent pet food-maker in Montreal. He leads a team of three, selling online and in 20 shops across Quebec, with plans to expand into Ontario next year. He makes cat and dog food, but as an entrepreneur, Lynn's an increasingly rare bird. In a new report, the Business Development Bank of Canada (BDC) examined numbers from Statistics Canada and found that the country has 100,000 fewer entrepreneurs than it did 20 years ago ‚Äî despite the fact that the population has grown by more than 10 million over the same period. Government to extend pandemic small business loan repayment deadlines New business owners pursue passion projects despite growing cost of living The Crown corporation says the decline puts the economy in danger, and it worked with researchers from the Universit√© de Montr√©al to analyze the problem and what's causing it, and to consider solutions. Its recommendation: Some of the difficulties of entrepreneurship can be overcome by helping business owners develop \"soft skills,\" such as grit, marketing and how to interact with people. BDC, a financial institution for entrepreneurs that provides loans for small and medium-sized businesses, released the report for Small Business Week. Why entrepreneurship is down For its report, BDC defined an entrepreneur as a self-employed worker who hires employees to support their venture. \"Twenty years ago, there was three Canadians in 1,000 every year becoming an entrepreneur,\" Pierre Cl√©roux, BDC's chief economist, told CBC News. \"Now we're down to about one for every 1,000 people.\" The actual figure is 1.3 people for every 1,000, less than half the rate of two decades ago. Pierre Cl√©roux, chief economist at the Business Development Bank of Canada, speaks to entrepreneurs this week at the Universit√© de Montr√©al. He says the entrepreneurship decline 'simply can't be ignored,' because new businesses are responsible for 'almost all net new job creation in this country.' (Universit√© de Montr√©al) It's also far below the apparent public interest in entrepreneurship. The popularity of TV shows like Dragons' Den, the growth of college and university entrepreneurship programs, and the creation of business incubators for entrepreneurs across Canada all suggest the appeal of starting a business. Small B.C. businesses get creative trying to survive inflation A report by Global Entrepreneurship Monitor says 14 per cent of Canada's population has entrepreneurial aspirations; so there seems to be plenty of dreamers. Yet BDC says few of those dreamers are becoming doers for several reasons: People in their late 20s to early 40s are the most likely to start businesses, but that demographic is shrinking with Canada's aging population, leaving a smaller pool of candidates as potential founders. Low unemployment and high wages mean fewer people feel the need to start a new business. Business owners and would-be entrepreneurs face a barrage of discouraging factors, such as labour shortages, inflation, technological change and the increasing domination of large companies. Why it matters Cl√©roux said the entrepreneurship decline \"simply can't be ignored,\" because new businesses are responsible for \"almost all net new job creation in this country.\" He said that \"in any healthy economy, you need a good percentage of startups\" to bring new products and services to market and \"force mature businesses to do better.\" Sarah Lubik, director of entrepreneurship at the Beedie School of Business at Simon Fraser University in Vancouver, agrees. Sarah Lubik, director of entrepreneurship at the Beedie School of Business at Simon Fraser University, says she would like to see scholarships for entrepreneurship at colleges and universities as a way to cultivate young business founders. (Beedie School of Business, Simon Fraser University ) \"There's so much at stake,\" Lubik said, focusing especially on entrepreneurs' role in creating breakthrough technologies. \"If you're not having entrepreneurship to take those ideas forward, then we're not going to get those ideas that we need to save the world.\" A 'soft skills' solution to overcome tough problems Another problem identified in the report is that a third of entrepreneurial ventures that do open close within just five years. So what can be done to help more people get started as entrepreneurs and succeed? BDC recommends a focus on developing \"soft skills\" to help entrepreneurs through every stage of business. Based on a survey of 1,250 entrepreneurs, it says marketing and finance skills are key to help a business get started, while administration and operations skills are needed to keep it going, and leadership and people skills are essential to create growth. Soar program aims to lift Indigenous entrepreneurs to new heights With tech incubators bringing investment and inspiration, Black entrepreneurs feel they can 'change the world' Relationship skills and grit ‚Äî the ability to deal with stress, handle change and overcome setbacks ‚Äî stood out as important in every stage of business. The BDC report stresses that key skills are not innate personality characteristics, but traits and behaviours that can be learned through coaching and mentorship, reading, formal classes and engagement with peers. Dominic Lim, an associate professor of entrepreneurship at Western University's Ivey Business School in London, Ont., says successful entrepreneurs evolve and learn to cover for weaknesses. Dominic Lim, an associate professor of entrepreneurship at Western University‚Äôs Ivey Business School, says he hopes that entrepreneurs from older demographics will emerge and contribute their skills and resources. (Ivey Business School, Western University ) \"You almost see how these people can reinvent themselves throughout the process,\" he said, \"or sometimes they involve other people who can complement them.\" But James Lynn, the founder of Kal≈´, said he isn't so sure the all-important grit can be taught. \"It needs to be within you,\" he said. \"There's too many points where it's easy and tempting to give up.\" What else might help? At an event in Toronto for Small Business Week on Thursday, Rechie Valdez, the federal minister of small business, said a \"full solution\" is needed to ensure entrepreneurs feel there's \"an actual possibility for them to grow and thrive.\" To help cultivate young business founders, Simon Fraser University's Lubik said she would like to see scholarships for entrepreneurship at colleges and universities ‚Äî just as there are for academics and athletics. Rechie Valdez, right, the federal minister of small business, speaks with Shopify president Harley Finkelstein at an event in Toronto on Thursday marking Small Business Week. (Shopify ) Lim agrees education is crucial. He also points out that \"hyper growth\" startups have a new role to play and that the scale of companies like Wealthsimple, Shopify and Aritzia can offset some of the decline in entrepreneur numbers. Lim said he hopes that entrepreneurs from older demographics will emerge as an offset, because they have skills and resources that could be \"a blessing for the entrepreneurial ecosystem.\" The latest millennial trend? Buying a franchise business ANALYSISBusiness quest to do more with fewer workers confronts productivity puzzle But enticing some of those workers to become entrepreneurs like Lynn will be a challenge so long as the labour market is strong. \"I wish sometimes that I would have chosen to just taken a regular job and gotten a regular paycheque,\" he said, \"but I snap out of it after five minutes and I keep going.\" WATCHIndigenous entrepreneurs get support for businesses: Indigenous entrepreneurs get a boost at Soar conference 1 month ago Duration 2:01 Almost 1,500 Indigenous entrepreneurs are attending the Soar conference to network and get support for their fledgling businesses. ABOUT THE AUTHOR James Dunne Producer, CBC News Business James Dunne researches, produces and writes stories for the CBC News business unit. Based in Toronto, he's covered business for about 15 years starting with local news, before moving on to the show Venture and co-creating the series Fortune Hunters. His work for those programs won awards at the New York Festivals and Columbus International Film and Animation Festival. James has a master's degree in public policy and administration and has also worked on special projects as well as the World at Six on CBC Radio One. Contact James at james.dunne@cbc.ca James Dunne on Linkedin James Dunne on Twitter With files from Philippe de Montigny CBC's Journalistic Standards and Practices|About CBC News Corrections and clarifications|Submit a news tip|Report error RELATED STORIES New business owners pursue passion projects despite growing cost of living Soar program aims to lift Indigenous entrepreneurs to new heights Government to extend pandemic small business loan repayment deadlines Small B.C. businesses get creative trying to survive inflation With tech incubators bringing investment and inspiration, Black entrepreneurs feel they can 'change the world' ANALYSIS Business quest to do more with fewer workers confronts productivity puzzle The latest millennial trend? Buying a franchise business ANALYSIS Canada reaches for critical mass in creating new companies: Don Pittis Your weekly look at what‚Äôs happening in the worlds of economics, business and finance. Senior business correspondent Peter Armstrong untangles what it means for you, in your inbox Monday mornings. Email address: Subscribe Current TimeIgnore this field. If any data is entered for this field, you will not be subscribed to this newsletter. COMMENTS To encourage thoughtful and respectful conversations, first and last names will appear with each submission to CBC/Radio-Canada's online communities (except in children and youth-oriented communities). Comments on this story are moderated according to our Submission Guidelines. To learn more about how to participate, visit our Getting Started guide. Please note that CBC does not endorse the opinions expressed in comments. Become a CBC Account Holder Join the conversation Create account Already have an account?Log in Loading... Footer Links My Account Profile CBC Gem Newsletters About CBC Accounts Connect with CBC Facebook Twitter YouTube Instagram Mobile RSS Podcasts Contact CBC Submit Feedback Help Centre Audience Relations, CBC P.O. Box 500 Station A Toronto, ON Canada, M5W 1E6 Toll-free (Canada only): 1-866-306-4636 About CBC Corporate Info Sitemap Reuse & Permission Terms of Use Privacy Jobs Our Unions Independent Producers Political Ads Registry AdChoices Services Ombudsman Corrections and Clarifications Public Appearances Commercial Services CBC Shop Doing Business with Us Renting Facilities Radio Canada International CBC Lite Accessibility It is a priority for CBC to create products that are accessible to all in Canada including people with visual, hearing, motor and cognitive challenges. Closed Captioning and Described Video is available for many CBC shows offered on CBC Gem. About CBC Accessibility Accessibility Feedback ¬©2023 CBC/Radio-Canada. All rights reserved. Visitez Radio-Canada.ca Information about cookies Please know that cookies are required to operate and enhance our services as well as for advertising purposes. We value your privacy. If you are not comfortable with us using this information, please review your settings before continuing your visit. Learn more Close",
    "commentLink": "https://news.ycombinator.com/item?id=37970484",
    "commentBody": "Canada has fewer entrepreneurs today than it did 20 years agoHacker NewspastloginCanada has fewer entrepreneurs today than it did 20 years ago (cbc.ca) 174 points by amichail 12 hours ago| hidepastfavorite240 comments zaptheimpaler 10 hours agoThis article itself is a great case study. The implicit structure of this article is that Lynn got fired and became an entrepreneur because he was forced into it. They never asked him anything about what it would take to get more entrepreneurs or make his life as an entrepreneur easier.Instead they asked 4 professors of entrepreneurship & govt. ministers who probably weren&#x27;t entrepreneurs. These people mentioned twice that one of the reasons for low entrepreneurship is because unemployment is too low and jobs pay too well... the whole push for this is coming from callous bureaucrats who&#x27;ve never been anywhere close to entrepreneurship who apparently believe its so bad that the only reason someone would do it is if they have no other options..What a joke.. it&#x27;s hard to believe this level of stupidity , it might just be malice (a pretext to justify raising interest rates and increase unemployment) reply opportune 6 hours agoparentThis is what happens in a world where ‚Äúexperts‚Äù are merely those with ‚Äúcertifications at the expert level‚Äù and not people with actual knowledge and experience.That kind of world is an inevitable outcome of large bureaucracies and over-reliance on qualifications. You don‚Äôt just get idiocracy, you get Untruths and policies oriented around expanding the bureaucracy rather than addressing problems. See also: Lysenkoism in the USSR, EU‚Äôs model of funding startups through grants.I honestly think this one of the things the US does way, way better than other developed countries. We pay a price in cronyism and corruption, but we actually let industry experts with real experience both advise on policy and take key government positions. We don‚Äôt pretend you need a PhD in X to be knowledgeable about X to nearly the same extent as Canada and the Eu.You can‚Äôt just stick a midwit in a university for 10 years and expect them to be anything other than a midwit, and you can‚Äôt expert an organization that values compliance and internal politics more than performance and effectiveness to produce performant or effective leaders. You can‚Äôt bureaucracy your way into a free market either. reply vasco 42 minutes agorootparentA bunch of people with the wrong assumptions who don&#x27;t understand the core mechanics of a situation surely won&#x27;t improve things I agree with you on that. Calling them midwits with no prospects, on the other hand, says more about you than them. reply dleslie 7 hours agoparentprevAye, I like to think of entrepreneurs as extreme investors. They are investing their time and effort in the expectation that there will be significant returns in the market.The Canadian entrepreneur problem is the TSX problem: it&#x27;s a struggle to convince oneself to invest in Canada, because generally the returns are lackluster. Rail, Banks, Oil and Gas are the consistent winners, all well-connected and deeply rooted, and rare is the startup that becomes a star on the TSX.It&#x27;s not that there isn&#x27;t enough starving and unemployed talent, it&#x27;s that there&#x27;s something about Canada that suffocates and debilitates small and medium companies. reply sidkshatriya 3 hours agorootparent> it&#x27;s that there&#x27;s something about Canada that suffocates and debilitates small and medium companies.On the face of it, Canada should be a hotbed of entrepreneurship. The country has full access to the US market, has a good standard of living, has capitalism with good social nets, has great rule of law, well educated residents, allows qualified immigrants...Every major economy is aging. Surely this cannot be the answer why entrepreurship in Canada is down.Also, I object to the metric of entrepreneurs as a percentage of population used in the article. It is about how many successful companies are being born. You could have a very high entrepreurship rate with a high failure rate or you could have a low entrepreneurship rate with a high success rate. At the end it is the product of the two i.e. sustainable, successful businesses born per year that is a better metric. We also need to take future profitability and future-impact into account. Giving birth to one OpenAI per year is arguably better than getting 100 new dry-cleaner businesses per-year.The article could have been better. reply raverbashing 1 hour agorootparentYes but, thinking about this:Canada sells and sees itself as the good boys. The conformists. The least likely to rock the boat. The opposite of that Apple ad essentiallyYou can see that by how often they will go with proprietary solution, even for their startups&#x2F;SMEs. \"Oh why don&#x27;t we do this in .NET\" even when there are plenty of other solutionsThey will only risk the water only when they see other people way down. I&#x27;d say even Europe is not that bad in that sense. reply logicchains 1 hour agorootparent>\"Oh why don&#x27;t we do this in .NET\" even when there are plenty of other solutionsThat&#x27;s an interesting idea for a metric: how many programming languages has Canada created relative to similarly sized countries? reply atgreen 17 minutes agorootparentCanada and Canadians are doing well here. You can start with Rust, PHP, and Java. replytrevelyan 5 hours agoparentprevPerhaps you are being a bit cynical? There are certainly no problems that can&#x27;t be solved by giving more money to government initiatives like the MaRS Centre in Ontario. These institutions also create jobs and are businesses of a certain sort, so funding them is supporting business directly.And who can argue with the results? Several promising entrepreneurs each year take advantage of these critical programs to consult with pro-business advisers who assist in job creation by recommending they apply for 3k tax breaks. reply onlyrealcuzzo 9 hours agoparentprevEspecially when wages in Canada for skilled labor are horrendous. reply ta8645 5 hours agoparentprevThe CBC is little more than crass government propaganda at this point. Canada in general should be viewed as a cautionary tale for anyone who thinks more government intervention in our lives, is a good thing. reply michaelje 2 minutes agorootparentOr maybe the issue is regulatory capture of government, not government itself?‚ÄòGovernment intervention‚Äô isn‚Äôt the same as a government who works to support the needs of citizens. reply kredd 2 hours agorootparentprevAs a person who has very close familial ties to other countries (US and Europe), and been around a decent bit, I still think life in Canada is better than in other places (for me). Sure we have problems, some of them are getting even worse than before, but comparatively we‚Äôre doing okay. It‚Äôs easy to say everything sucks here, but it also sucks in other places for different reasons.Full disclaimer, I consider myself politically moderate&#x2F;centrist and still think CBC is very valuable. And again, I understand everyone has different priorities in life and Canada might not be suitable for them. But implying ‚Äúwe have it the worst and nobody would want to live here if they had a way to get out‚Äù is also wrong. reply someone13 4 hours agorootparentprevAs someone that has lived in several countries, and currently in Canada, I will respectfully have to disagree both about your opinion of the CBC and beliefs about government intervention. Some parts of Canadian local&#x2F;provincial&#x2F;federal government seem deeply dysfunctional, but some are extremely helpful and better than I‚Äôve seen elsewhere. reply ta8645 4 hours agorootparentYou&#x27;re seeing what remains of a strong and healthy period of Canadian history. But almost every facet is currently under threat and significant stress.You can&#x27;t fund a \"news\" organization with public money and expect them to be an independent agency that holds the government&#x27;s feet to the fire. They don&#x27;t do much more than add a thin veneer of objectivity and independent analysis for any significant government initiative.The article posted above is just a small piece of evidence for the broken nature of the media. There is a torrent more spewing out daily. reply ornornor 4 hours agorootparentprevI don‚Äôt see people becoming homeless because of medical bills in Canada.I don‚Äôt see people getting shot nearly as much in Canada.I don‚Äôt see as many exploited illegal immigrants in Canada.To fight corporate greed and capitalistic cynicism we do need government. reply faeriechangling 1 hour agorootparent>I don‚Äôt see as many exploited illegal immigrants in Canada.There is not a large land border with Mexico in Canada, but Canadians are happy enough to exploit what they call temporary foreign workers who do jobs they don&#x27;t want to do and have rights which fall short of a citizens on paper and in practice.This difference seems less due to Canada&#x27;s superior nature than it seems due to Canada being surrounded by oceans on three sides and by a wealthier country on the fourth side. In fact I think Canadians would do well to keep in mind that they don&#x27;t face the same issues that other countries do before passing judgement on immigration issues. Almost all immigrants to Canada are immigrants Canada explicitly welcomed to the country who met every qualification. reply ulucs 3 hours agorootparentprevI don&#x27;t know why they are homeless, but there are quite a bit of homeless there; along with 15-people illegal boarding homes. Canadian real estate market is also quite a miracle.Compared to US, I suppose. Canadian homicide rate is comparable to Balkan countries, each famous in their own way of dysfunction and corruption.Drop the word \"illegal\" and you will see a lot. How do you expect illegal immigrants to get to Canada when they only border US?The final point is quite amusing since it&#x27;s the Canadian Government and the regulatory capture it creates that screws up a lot of things in Canada and pushes prices to insane levels (mobile phone plans, housing as two examples that came to my mind) while also depressing wages via immigration. reply rrr_oh_man 3 hours agorootparent> Canadian homicide rate is comparable to Balkan countriesAt 2.1&#x2F;100,000 it‚Äôs a hair above the 2021 Balkans average, which is, frankly, quite low and just a little higher than the Northern Europe average.> each famous in their own way of dysfunction and corruption.Yes, but not for high homicide rates.What‚Äôs the point you are trying to make? reply ulucs 42 minutes agorootparentIf being Balkan-like is good enough, then yeah I understand why you wouldn&#x27;t complain. After reading the other comments on this page it seems that \"Well, it&#x27;s good* enough.\" should be the national Canadian slogan or something.[*] For some low standard of good reply david-gpu 1 hour agorootparentprev> Drop the word \"illegal\" and you will see a lot.There are major differences between legal immigrants, who have to pass some pretty strict tests of suitability, and uncontrolled immigration. Legal immigrants have most of the rights and obligations of citizens, including access to healthcare and paying taxes.> How do you expect illegal immigrants to get to Canada when they only border US?Canada has thankfully developed enough by now to have airports.> I don&#x27;t know why they [sic] are homelessYou said it yourself: expensive real estate. reply ulucs 31 minutes agorootparent> Canada has thankfully developed enough by now to have airports.Truly spoken like someone who has no information about illegal immigration. I suppose the border control just lets everyone in? Or flights to Canada just let everyone board without checking their visas? Or Canadian embassies just throw around visas without checking for illegal immigration risk? For those who don&#x27;t need visas, anyone interested in illegally immigrating from this list of countries (https:&#x2F;&#x2F;www.canada.ca&#x2F;en&#x2F;immigration-refugees-citizenship&#x2F;se...) are much better served by buying a bus ticket to Germany. reply david-gpu 0 minutes agorootparent> Truly spoken like someone who has no information about illegal immigrationQuote from this [0] article: \"But in the past 10 years, visa overstays in the United States have outnumbered border crossings by a ratio of about 2 to 1, according to Robert Warren, who was for a decade the director of the statistics division at the agency that has since been renamed U.S. Citizenship and Immigration Services\".[0] https:&#x2F;&#x2F;www.theatlantic.com&#x2F;international&#x2F;archive&#x2F;2019&#x2F;04&#x2F;re... avtolik 37 minutes agorootparentprevThe homicide rate in my poor corrupted Balkan country is 4 times lower than the US. reply ulucs 27 minutes agorootparentOne of the few things we can take solace in as Balkanites: at least it&#x27;s not as bad as it could be. Thank you US for showing that money does not necessarily fix all your problems. reply wesapien 3 hours agorootparentprevYea sure sure but it&#x27;s significantly worse now that before. As more people become more desperate, they will turn to crime. Canada needs an embodiment of Theseus to defeat the Procrustes like progressives. reply fooster 5 hours agorootparentprevNonsense. Don‚Äôt spread misinformation. reply ta8645 5 hours agorootparentSpoken with the same tone and language that the nanny state and media use. It&#x27;s inappropriate and shameful. I hope more people start to see through it before civil society is completely lost. reply FpUser 5 hours agorootparentprevI am Canadian and am in agreement with the point to which you replied. There is no misinformation about Canada moving backwards comparatively to what it once was. reply bbarnett 7 hours agoparentprevWhat a joke.. it&#x27;s hard to believe this level of stupidity , it might just be malice (a pretext to justify raising interest rates and increase unemployment)The political branch of the government does not control interest rates, nor have any say in them. The Prime Minister cannot raise or lower interest rates.This is the same in most western democracies, as such things cannot be left to political control. reply Scoundreller 6 hours agorootparentI mean, they admit that they only have \"considerable independence\", but not \"total independence\":\"The Bank of Canada is a special type of Crown corporation, owned by the federal government, but with considerable independence to carry out its responsibilities.\"https:&#x2F;&#x2F;www.bankofcanada.ca&#x2F;aboutThey also add: \"The Governor and Senior Deputy Governor are appointed by the Bank&#x27;s Board of Directors (with the approval of Cabinet), not by the federal government.\"... and the BoD are appointed by cabinet.Does the political branch make the decisions? No. But they choose the people that do! reply bbarnett 4 hours agorootparentDoes the political branch make the decisions? No. But they choose the people that do!This by no means implies any control over interest rates. At all.People are appointed, and that&#x27;s where political input ends. And those appointed stay appointed, are not chosen lightly, markets do not react well to poor choice.By your metric, you&#x27;d think our Supreme court judges were puppets too. They aren&#x27;t.People may have political leanings, but understand, that is not the same as allowing input where it is not allowed.The government cannot interfere with the judicial branch, nor with fiscal policy. To do so, renders it a banana republic, or dictatorship, any attempts would be disaster, for those appointed take this very seriously.A core part of the job is to ignore ministers, the PMO, and so on.It is entirely independent. reply logicchains 3 hours agorootparent>By your metric, you&#x27;d think our Supreme court judges were puppets tooIn the US supreme court judges almost always agree with whatever position is favoured by the party that appointed them, even if nominally independent. reply barsonme 1 hour agorootparentIf this were true, SCOTUS would almost always be split 5-4. But, it is not. In fact, it typically has significantly more 9-0 decisions than 5-4.For example: https:&#x2F;&#x2F;www.politifact.com&#x2F;factchecks&#x2F;2021&#x2F;jun&#x2F;23&#x2F;trey-wingo... reply logicchains 1 hour agorootparent>If this were true, SCOTUS would almost always be split 5-4That&#x27;d only be the case if there were no policies with bi-partisan support. How many of those 9-0 are for politically contentious issues? reply bbarnett 1 hour agorootparentprevYou say that as if that&#x27;s relevant, when discussing Canada.And you&#x27;re confusing political leanings, with influence. Political leanings which can, and have changed over careers. Not to mention, some of them outlive everyone who appointed them. reply Scoundreller 3 hours agorootparentprevBank of Canada directors are on 3-year (renewable) terms.You don&#x27;t need someone that will take in your input when you install people that are going to do what you want in the first place. reply bbarnett 3 hours agorootparentThe governor and deputy are 7 years, appointed by directors. They control fiscal policy.Look, I know it&#x27;s fashionable to claim that everyone just ignores their roles via legislation (such as the bank of canada act), ignores change and what&#x27;s happening in the world around them, to instead act as puppets, but that&#x27;s just not how it works. reply csomar 5 hours agorootparentprevThis is wrong. They can fire the director (which sets the policy). As the director is not that independent (or there is no strong safe guard that she is) then you indirectly set the interest rate. reply bbarnett 4 hours agorootparentNo. Just no.The Governor(not director) os not appointed by the government. Nor fired.You need to read up on this. Please. You&#x27;re just making stuff up. reply throwawaysleep 7 hours agoparentprevUnless you want a yacht or find every other option so miserable&#x2F;impossible, why would you be an entrepreneur?It‚Äôs a lot of work and there‚Äôs plenty of money to be made as a corporate drone. reply ornornor 3 hours agorootparentIt‚Äôs not only about material wealth, it‚Äôs about agency.If you‚Äôre a corporate drone, someone else owns your time. Want to take some time off? You need permission from your employer. Want to do something differently? Need permission. Want to think for yourself or take initiative? Frowned upon at best, forbidden at worst.Nothing wrong with one or the other, but it doesn‚Äôt suit everyone. reply faeriechangling 1 hour agorootparentA union Canadian civil servant can work 35 hour weeks and be unfirable even if their actions are frowned upon. Whereas an entrepreneur often gets to work a liberating 80 hours a week and stepping a toe out of line can draw the ire of clients and bankrupt them.Of all the criticisms I would have of Canadian work culture, lack of autonomy is not one of them. reply arvinsim 3 hours agorootparentprevIt takes time for a business to be at the level where you can do the above. At the start, you are essentially a slave to your business. reply dgudkov 6 hours agorootparentprevSome people just hate to be a corporate drone. reply johnnyanmac 7 hours agoparentprev>ell... the whole push for this is coming from callous bureaucrats who&#x27;ve never been anywhere close to entrepreneurship who apparently believe its so bad that the only reason someone would do it is if they have no other options..In all fairness, isn&#x27;t that a legitimate reason? If you could make a very comfy life and retire early I think many would be entrepreneurs would take the easy way out. Of course, some want the big cash out or strongly believe in their mission, but many people are there for the money.Of course, I can&#x27;t verify if wages in Canada are indeed \"too well. Seems like an outdated statement if the economy up there is anything close to the US&#x27;s. Legitimate reason with incorrect data is simply a misguided take. reply deanCommie 7 hours agorootparentIt&#x27;s definitely not the case.Canada has a cost of living crisis, and an entire generation has been priced out of the possibility of owning a home.One can survive better than in the US at the extremely low end, and being sick won&#x27;t kill or bankrupt you.But nobody is living a \"comfy\" life without busting and hustling. reply amrocha 3 hours agorootparentAll my late 20s accountant friends are buying houses right now. I know a chemical engineer doing the same. Likewise for everyone in tech. And a lot of people live with their parents and will inherit a family house at some point too.Plenty of people living comfy. reply jq-r 1 hour agorootparentAt least in the US, tech is currently in a wage supression war, and I‚Äôm afraid this will ripple out all the world.But for ‚Äúa lot of people live with their parents‚Äù that‚Äôs actually a sign that things are not going well at all. This can be seen all over Europe, and just means that people cannot afford to move out, not that they like living with their parents. If&#x2F;when they inherit their parent‚Äôs house they might be in their fifties or sixties, and most of their prime life is gone by then. reply danielmarkbruce 6 hours agorootparentprevEngineers at big tech are. reply dgudkov 5 hours agoprevI know the cost of living is now a standard excuse for everything unfortunate in Canada, but I have an untypical view on this topic:Canadians strike me as a very undemanding nation. I mean, they are ready to go through whatever inconvenience a big corp may force them into, and still be fine about it. To Canadians, it&#x27;s just the way things are, inevitable evil. At the same time, an entrepreneur frequently starts by targeting a market niche and offering better quality or better service or a novel way of doing things. However, a market niche assumes that there are consumers with a niche demand. But if consumers are undemanding and are OK with keeping paying for something subpar, this leaves us with not so many market niches and thus opportunities.At the same time, doing business is great in Canada. The tax system is simple (just 3 taxes). Reduced tax rates for small businesses. Registering a business can be done online for a small fee. The banks are OK. Abundance of decent business-related services - accountants, designers, etc. It&#x27;s relatively easy to sell to the government. Finding a good lawyer is a tough problem, though. But I suspect it&#x27;s a problem everywhere.PS. I run a technology company based in Canada. reply muunbo 3 hours agoparentI totally agree with the ‚Äúundemanding Canadians‚Äù view‚Ä¶so true! I moved here 12 years ago from Dubai and the demands&#x2F;standards of business and service are way lower here. Pros are that it‚Äôs laid back and chill, cons are that we are unlikely to lead and innovate because we are satisfied with the status quo reply amrocha 3 hours agorootparentIf your idea of leading and innovating means being an asshole to service workers then go back to Dubai we don&#x27;t need that in Canada reply fastball 43 minutes agorootparentWhoa where did this come from? reply martin_drapeau 9 hours agoprevCanadian tech startup entrepreneur here.Coziness is the biggest hurdle. As a programmer, I can make a really good salary at a tech giant. Even after 4 years of grinding and finally achieving product market fit and growth, my wife doesn&#x27;t understand why I&#x27;m doing a startup. Life could be so much easier working for Google.Heck everyone has a job here. The 2&#x2F;1,000 missing entrepreneurs don&#x27;t have an incentive to start. I&#x27;m from Ottawa and all my friends work for the federal government with big fat pensions and doing boring work (my perspective mind you). But they have great family lives, lots of vacation and great outside of work experiences. Why the hell would they jump into entrepreneurship? Worst of all, the government workforce is growing faster than that of the private sector!What worries me most is the future - a country cannot get wealthier nor innovate without entrepreneurs. It is s a real concern indeed. reply bettercallsalad 7 hours agoparentWhat are you talking about? Coziness? Are you one of those people you bought a house in the 2010s and are now well positioned to cash in and live a life of comfort with 500$ mortgages while homes out there are near a million dollar. Because then it would make sense.Problem with Canada is the cost of living is too high. And there is more money if you move south. Easy. A job in US would easily pay you twice if you are in tech. Why would you want to live in Canada? Oh did I say the taxes. For a millennial,ife is way more harder. reply seabrookmx 6 hours agorootparent- 2010 mortgages are not that low, unless you bought a travel trailer - $1million actually doesn&#x27;t buy you much in Toronto, Vancouver, or even a smaller city like Victoria - a millennial working at big tech ($200k+) can still afford one of these absurdly priced homesYes housing in Canada is messed up. But very well paid tech workers are actually some of the few young people that can still make it work.Heck, I know some millennials that managed to buy in the Bay Area where it&#x27;s even worse (they work at Tesla). reply bettercallsalad 5 hours agorootparentOh so now the millions in Canada all they need to do is to work in big tech. I see. Problem solved. Thanks for enlightening me. I am so glad I left that messed up country filled with ‚Äúwe made it our own and you can too‚Äù people.Just to give you context, median software Engineer salary in Canada is 95200. An average townhome in any of the cities now is over 600k easily, forget about Toronto and Vancouver where most likely those tech jobs are. Mortgage for a place like this in the current rate is easily around 2500-3000. Add in car, insurance, electricity, cost of daycare. And then see how much of after tax income can you afford to bring home. reply ffgjgf1 1 hour agorootparentprev> to buy in the Bay Area where it&#x27;s even worseIs it? If adjusted by median income Vancouver and Toronto are both more expensive than San Francisco or San Jose. reply danielmarkbruce 6 hours agorootparentprevThey said working in big tech as an engineer. It&#x27;s cozy. It pays well. Ask anyone who&#x27;s done it. It&#x27;s a fair description. reply ipaddr 6 hours agorootparentprevPlenty of affordable homes in Canada at the $500,000 mortgage + down payment range. Go find a smaller city and grow with it because you have the same affordability problem in San Francisco or New York. Your tech salary isn&#x27;t getting you a house there either. The time to buy a house in Toronto was when they cost $5,000 after the war or was the time to buy in 1975 when the property cost $30,000 or in the 90s when you could have gotten a house for $200,000. In 2012 for $500,000 you could get a house in 2021 it was 1.3 million now it&#x27;s 1.1 million. In 2015 you could have purchased a home in New Brunswick for $25,000..Your 1.1 million dollar home you buy today will be worth 5 million in 50 years at least 2 million in 20 years. Still as good of an investment as ever.It&#x27;s like bitcoin sure it would have been great buying coins for 5 cents and today and you would be a billionaire but you weren&#x27;t able to make that bet. If you want to buy at the 5 cent level you need to find another coin because that opportunity passed. You weren&#x27;t able to buy a home in 2010 or 1947 so you are never getting those prices again. You can buy today but don&#x27;t expect the same prices on the same properties as if you lived in the past reply zinodaur 5 hours agorootparentyeah, but in 1975 the average household income was 50k. Now its 92k. The cost of a house in Toronto (and everywhere in Ontario) went from being 60k (120% of annual average family income) to 1.1m (1200% annual average family income).\"Go somewhere else\" - They did that - and Halifax housing prices tripled in 3 years, from 310% annual average family income to 840% annual average family income.We need to build more houses or have fewer people, but instead we&#x27;re slowing down on building and importing new people as fast as we can reply ipaddr 3 hours agorootparentHalifax is a premier city. Think smaller. reply light_hue_1 16 minutes agorootparentThere&#x27;s always one in every thread.Someone who says the solution to the housing problem is to move somewhere horrible with no jobs, no culture, no services, horrible schools so that my children can waste their future, never mind if I want to go out and do something one in a while. These places are so awesome that the depression and suicide rates are far higher than in real cities.Maybe you like living in such places. But the solution to the housing crisis cannot be to give up on having a reasonable life for most people. reply NicolBolas 3 hours agorootparentprevCOL is even higher in China relative to their domestic wages. Yet, entrepreneurship is much more dynamic in China than Canada.It&#x27;s a cultural problem. Hustle culture and entrepreneurship is respected in China. That&#x27;s not the case in Canada. reply harimau777 5 hours agorootparentprevSmall cities usually don&#x27;t become cosmopolitan cultural centers in the span of a lifetime. So growing with a small city with the plan of living there long term isn&#x27;t viable.Growing with a small city as an investment vehicle seems like it would run into problems predicting which small city is going to see significant growth. reply ipaddr 3 hours agorootparentToronto did. Other cities can too. reply WarOnPrivacy 4 hours agorootparentprev> Plenty of affordable homes in Canada at the $500,000 mortgage + down payment range.Canadian humor is weird.Using the most typical US salary as a guide, it would require the combined salaries of 6.3 people to afford a $500k home. After adding insurance, maintenance, fees, property taxes, etc - I&#x27;m thinking more like 8 people (10 in FL).ref: https:&#x2F;&#x2F;www.marketwatch.com&#x2F;story&#x2F;how-much-do-you-need-to-ea...ref: https:&#x2F;&#x2F;www.indeed.com&#x2F;career-advice&#x2F;finding-a-job&#x2F;common-jo... reply ipaddr 3 hours agorootparentThe parent mentioned $500,000 mortgages are the thing of the past.The $500,000 is usually carried by two. $250,000 mortgage while expensive is still much cheaper than current rental values. reply snovv_crash 2 hours agorootparentThe rent would also be carried by 2. replyjacquesm 8 hours agoparentprevThe inefficiency of the Canadian government is nothing to be proud of though. It&#x27;s insane how many people are employed there compared to doing something productive. reply helloooooooo 8 hours agorootparentImplying the employees of the federal government aren‚Äôt doing something productive? And the Canadian federal government isn‚Äôt even that different in size to the American government, accounting for population different.By what metric are you making your claims? reply lcssthrowaway 7 hours agorootparentJust going to toss in my own anecdotes but not so much productive but surprisingly inefficient. My current employer is currently contracted for the federal government and it&#x27;s painful to do anything.Just an example, I put in a request to order a $40 network module to replace a failed one. On our side it&#x27;s already painful enough, I had to get approvals from 3 different managers that have to align all of the budgeting before bringing it forward to the federal government&#x27;s representative for approval. The rep in tern has to bring it to his bosses for approval as well.Typically this takes 12 weeks but this was put in as a critical priority because this was impacting about a hundred people. Probably costing the federal government (cause they&#x27;re footing the bill) about $12,500 a week in lost productivity because our teams can&#x27;t do their job properly. Thus their director got to it in a blazing fast 4 weeks.$40 part. $50,000 in lost productivity because they couldn&#x27;t get a $40 approval for done faster then 4 weeks.The other anecdote I&#x27;ve had is with the Canadian Firearms Center in Miramichi. For those that are unaware, Canadians are required to be licensed under a federal program to own a firearm privately. Miramichi handles much of the paperwork for firearms registration. This was a while ago but last time I saw, they were still manually entering in data from paperwork but also from electronic submission as well. Stuff that could be readily automated and something I demo&#x27;d in an afternoon.As it turns out, the inefficiency is intentional because there&#x27;s a very real fear among the line staff of job reductions if they get too productive per person. And there&#x27;s not many jobs in Miramichi that pay as well as the federal government (or at least that was the case when I was visiting there probably 20 years ago). So unfortunately there is justification to be as inefficient as possible. But it&#x27;s always left me wondering how much how much further this attitude extends to other government departments. reply transcriptase 6 hours agorootparentIt does extend to other departments. I‚Äôve worked alongside entire teams of longtime federal employees who would spend 10 minutes cleaning up and changing out of work gear, drive 5 minutes to the building with a cafeteria, take a 15 minute union mandated break, then spend 5 minutes driving back to a work site, 10 minutes getting back into their gear and readying tools, only to work for 20 more minutes to finish the job at hand. Then get cleaned up, change, put tools away, and drive back to the building just about in time for lunch.Overall nearly a thousand dollars in salary being wasted in a single morning because the idea of not taking a break at exactly 10:30am instead of finishing the job and taking a break at 11:00am instead is inconceivable.There is absolutely zero incentive for efficiency and motivated individuals will actually burn out from being in a culture where trying to improve anything or making your colleagues look bad by being too productive creates a toxic work environment.That‚Äôs not to mention the absurd ‚Äúuse it or lose it‚Äù budgeting system, whereby if you need to replace a million dollar piece of equipment every 5 years, you need to piss away over a million dollars every year to ensure you‚Äôll have the budget when it‚Äôs actually required. reply BeefWellington 3 hours agorootparentIf you think this kind of stuff doesn&#x27;t happen in private sector tech companies, even after all the layoffs, I&#x27;ve got some bad news for you.Whenever I see people ragging on governments as though they&#x27;re automatically inefficient, I laugh because the implication that \"private sector is always more efficient\" is laughably stupid and ill-informed. People will take their liberties wherever they can get them. reply jq-r 1 hour agorootparentThe big difference is that the private sector is burning their own money. If a private company burns their own or their investors money thats on them and they‚Äôll probably go bankrupt sooner or later. Whomever was involved in this will naturally wise up next time.On the other hand, when public sector burns through money that money isn‚Äôt theirs. It was mostly taken away by force through taxes from private citizens. So even of they burn through any money with inefficiency, corruption, bad decision making or anything, they can demand the same or usually more of the same money next year. And next year. All the while that bureaucracy is growing and private sector is shrinking. reply BeefWellington 2 hours agorootparentprevI&#x27;ll two-up you:I&#x27;ve worked at two very large tech companies in the past. At both shops I have experiences that make that $50k in lost productivity look like a joke.First company invested tens if not hundreds of millions chasing the idea of scaled clustered deployment using Spark. I&#x27;m sure they saw some kind of technology return but the actual investment was never returned nearly a decade on and turned out to be a blunder.Second company had a far worse procurement issue than what you describe in your post. $50k in lost productivity would&#x27;ve been welcomed. They had a hardware failure in one of their datacentres affecting the cooling systems. Some customer systems impacted but mostly internal stuff. It took about seven weeks to replace during which time hundreds of employees basically got to twiddle their thumbs.Yeah, it shouldn&#x27;t take four weeks to get approval for a $40 part but let&#x27;s not pretend these kind of problems don&#x27;t also happen in the private sector too. It&#x27;s one of the reasons I left and won&#x27;t go back -- I shouldn&#x27;t need paperwork signed in triplicate to get a new monitor. reply rdm_blackhole 1 hour agorootparentI agree with your point that things may not be more efficient in certain big companies of the private sector but there is a crucial difference between the two.The difference is that the government&#x27;s employees are paid from taxes collected from productive companies, aka companies that create jobs and achieve a profit big enough to warrant paying taxes in the first place.A private sector company, unless it is subsidized in some way or has a monopoly on a certain market will eventually try to rectify these inefficiencies because its makes it more resilient in the fight against it&#x27;s competitors.On the other hand, there is no incentive on the government side to make things more efficient.If a government looses money, they can just borrow more, increase the national debt and sweep any kind of reform under the rug. Because of this lack of incentives, some state and government agencies act as de facto subsidized job&#x27;s programs.It&#x27;s the same problem in many western nation, just look at how many people are employed in France by the government.Many basic services are actually decaying but the government keeps hiring more people.If we remove the employees who work as teachers, cops, in the military, and all other essential services, the rest of them, I am just not sure what the hell they do with their time.My point is that it should be a duty of the government to be as efficient as possible. To do more with less, should be a priority unless the tasks require more people.So I can understand when people complain of the inefficiency of a government agency. reply Scoundreller 6 hours agorootparentprevuhhhh, I&#x27;d say that inefficiency is intentional for firearms because they just want it to be difficult&#x2F;time-consuming&#x2F;delayed to acquire&#x2F;possess firearms. Then they can justify increasing the fees because \"they need to recover the costs of program operation\".Meanwhile my Canadian tax return is pretty straightforward: open my tax app, click some download function, it pulls in stuff my bank&#x2F;employer&#x2F;etc has already electronically submitted to government.The most annoying part is calculating \"Adjusted Cost Basis\" for cap gains&#x2F;losses on stock sales. It&#x27;s technically fairer than a simpler FIFO approach, but brokers&#x2F;gov refuse to understand that most people do everything through 1 broker and calculate these figures for us. reply all2 6 hours agorootparentIt&#x27;s to the benefit of the parasite to make it easier to feed on its host. It is also to the benefit of the parasite to ensure that its host cannot get rid of it.Bureaucracy will always seek its own survival, even at the cost of its chartered mission. reply transcriptase 6 hours agorootparentprevFirearms license renewals were at a standstill for over a year because apparently the single license card printer for the entire country was out of order. reply lcssthrowaway 5 hours agorootparentThat was COVID lockdown caused I thought.https:&#x2F;&#x2F;www.publicsafety.gc.ca&#x2F;cnt&#x2F;trnsprnc&#x2F;brfng-mtrls&#x2F;prlm... reply lcssthrowaway 5 hours agorootparentprev> uhhhh, I&#x27;d say that inefficiency is intentional for firearms because they just want it to be difficult&#x2F;time-consuming&#x2F;delayed to acquire&#x2F;possess firearms. Then they can justify increasing the fees because \"they need to recover the costs of program operation\".Kinda doubt it, there&#x27;s already a mandatory wait time for getting your license. If anything Ottawa wanted them to process the paper work faster, I just don&#x27;t know if they could have done so.Circa early 2000&#x27;s the Canadians were in the process of trying to get long guns registered (handguns and some other types of firearms always had to be registered), but it turned into a debacle. The cost to build the program should&#x27;ve been $119 million, with fees covering $117 million. The cost that the auditor general found was $140 million registration fees trying to cover $1 billion in costs that she could account for at the time. Didn&#x27;t help that her team didn&#x27;t have time to crunch the numbers. CBC went in and estimated that it was closer to $2 billion dollars[2].The program was already controversial in the first place, but the problems just kept getting problem after problem thrown at it. In protest a man named Brian Richard Buckley sent in firearms registration for a Black and Decker soldering heat gun. There were already huge error rate in the registry at the time; something like 70% error rate of licenses and 90% error rate for registrations but that little stunt he pulled revealed a much deeper problem occurring. I can&#x27;t find the article anymore but the backlog had become so severe that the staff were instructed to no longer validate any of the information they were getting, even if it didn&#x27;t make sense.All that to say that say, if things were being run inefficiently intentionally, that probably would&#x27;ve just given even more opposition even more ammunition to embarrass the government at a time when they were already taking a beating about the mismanagement of the new firearms program. So I sincerely doubt it was a mandate.[1] https:&#x2F;&#x2F;www.cbc.ca&#x2F;news&#x2F;canada&#x2F;auditor-general-takes-aim-at-...[2] https:&#x2F;&#x2F;www.cbc.ca&#x2F;news&#x2F;canada&#x2F;gun-registry-cost-soars-to-2-... reply jacquesm 8 hours agorootparentprevBy having lived there for almost a decade and interacting frequently with the government, many different departments (taxes, immigration, law enforcement, municipalities, various ministries). I don&#x27;t see what America has to do with it. reply dghlsakjg 7 hours agorootparentWhat other governments have you interacted with recently?My experience after living here for 5 years is that the Canadian and BC government deliver an incredible level of service compared to other jurisdictions I‚Äôve lived in. reply qlkjwenf 4 hours agoparentprevThe efficiency of the Canadian government is significantly greater than that of Germany, often considered the world&#x27;s capital of bureaucracy. reply preommr 6 hours agoparentprev> Coziness is the biggest hurdle. As a programmer, I can make a really good salary at a tech giant. Even after 4 years of grinding and finally achieving product market fit and growth, my wife doesn&#x27;t understand why I&#x27;m doing a startup. Life could be so much easier working for Google.If this was a major factor then it would also apply to entrepreneurs in the US, yet it doesn&#x27;t. reply throwawaysleep 9 hours agoparentprevCanada is very much a \"let someone else do it\" kind of place. You see that in politics, you see that in business, you see that in government.A lot of that though is simply not wanting to be rich. Americans truly want lambo type wealth. I can&#x27;t say I have met someone born here who wanted that. We are content to own a house and go to Mexico. reply mvncleaninst 7 hours agorootparent> We are content to own a house and go to Mexico.But isn&#x27;t housing ridiculously expensive in the parts of Canada where jobs are concentrated (which I&#x27;m pretty sure are around Toronto and Vancouver)? Pardon me if wrong, I&#x27;m American ;)For the record, I want those things too, it&#x27;s just that housing in California is ridiculously unaffordable reply throwawaysleep 7 hours agorootparentDepends on who you are.For most people, yes it is quite expensive. If you are looking at techies specifically, it is still affordable.For typical knowledge jobs or skilled labour jobs, it is affordable as well if you pair up with someone. reply WalterBright 6 hours agorootparentprev> I can&#x27;t say I have met someone born here who wanted thatPossibly because the ambitious people moved to the US. reply billy99k 8 hours agorootparentprevI frequently travel to Toronto and the number of people driving Teslas there has increased 10X in the last 5 years. This could be a result of new Chinese money though and not native Canadians. reply dleslie 7 hours agorootparentMany Canadians became wealthy via the housing market. Effectively drawing wealth from the mortgaged future potential income of Canada&#x27;s young first time home buyers.Or, just running STRs in the big cities. reply Scoundreller 6 hours agorootparentDunno about other cities, but City of Toronto proper didn&#x27;t add any net new hotel rooms from 2000 - 2015. And only up 22% in the \"Greater Toronto Area\", which is way less than that area&#x27;s pop&#x27;n growth. I doubt things improved as airbnb became a thing.p. 10 here: https:&#x2F;&#x2F;www.toronto.ca&#x2F;legdocs&#x2F;mmis&#x2F;2016&#x2F;ed&#x2F;bgrd&#x2F;backgroundf...When your residential construction market is so on fire that you don&#x27;t even build hotel space, you end up with STRs as a symptom. reply helloooooooo 8 hours agorootparentprevUSA also has a significant amount of generational wealth to go around. Canadas is isolated to a few dozen families in Montreal and Toronto. Canada is at its core, a resource extraction economy. Generational wealth doesn‚Äôt really come from that these days. reply ChumpGPT 7 hours agoparentprev>Heck everyone has a government job here...ftfy1 in every 4 people work for the Gov in Ontario and it is increasing. An elementary school teacher, fireman, Police officer and even some postal workers make more than most programmers. HS teachers retired with million dollar pension at 57 and collect 60-70k a year. There are regular cops making 120k a year. There are more STEM jobs in some US cities than there are in entire provinces. There is no work in Canada unless you find a Gov job. Private industry pays like shit in Canada, it&#x27;s embarrassing. Everyone I know flees to the US first chance they get. Canadian private industry pays like shit and most folks are either slaves or under employed if they are lucky enough to find work.It is an incredibly bad situation across the entire country. reply faeriechangling 11 hours agoprevOne thing I‚Äôll highlight is that public sector growth has outpaced private sector during that whole time, and entrepreneurs indirectly have to pay to prop up public services only to get from a business perspective to get a worse business ecosystem than America in pretty much every way except not having to pay for employee health insurance and lower labour costs. If the idea is that the public sector‚Äôs wonderful services are supposed to help the private sector flourish, well, that whole idea doesn‚Äôt seem to be working.The other is rents have increased that whole time, who can afford to be an entrepreneur when your lease will eat you alive before you ever get off the ground. If the argument is ‚Äúaha - but Canadians could start an innovative company that relies on remote work and thus avoid such expensive leases‚Äù what is the appeal of starting such a business in Canada compared to starting one in the United States where success is far more likely due to a larger market and more pro-business environment?Finally inequality and monopolization is just much worse than 20 years ago internationally. Why be David fighting Goliath when the deck is stacked against you? I‚Äôd be kind of surprised if the trend in the article isn‚Äôt happening internationally.The articles suggestion that Canadians just don‚Äôt have good enough soft skills to be entrepreneurs is just insulting blaming of non-existent individual deficits that ignores systemic issues. reply 4death4 10 hours agoparentWhat evidence do you have that there is a causal inverse relationship between public sector growth and private sector growth? Isn‚Äôt it entirely possible that private sector growth has lagged because some third process has produced fewer entrepreneurs and fewer entrepreneurs results in less private sector growth? It‚Äôs possible that the relative difference in public and private sector growth is caused by fewer entrepreneurs, not the other way around. Basically, your argument is a classic case of assuming causality from a correlation. reply personjerry 9 hours agorootparent> It‚Äôs possible that the relative difference in public and private sector growth is caused by fewer entrepreneursYou&#x27;re saying the government saw there are no entrepreneurs so they increased government jobs??? reply faeriechangling 9 hours agorootparentA lack of private sector jobs can be caused by a lack of entrepreneur&#x27;s. A lack of private sector jobs can cause a lot of people to be eligible for welfare. The government can then be incentivised to react to this by increasing public sector jobs, which at least makes people somewhat productive in theory while also giving them the means to provide for themselves.I&#x27;ll point out that none of the beliefs expressed so far actually contradict each-other. Less entrepreneurs could mean more public sector jobs. More public sector jobs could mean less entrepreneurs. Third order effects could mean less entrepreneurs as well. reply rospaya 10 hours agoparentprev> If the idea is that the public sector‚Äôs wonderful services are supposed to help the private sector flourish, well, that whole idea doesn‚Äôt seem to be working.Are you implying that social programs are here to help to private sector? reply umvi 10 hours agorootparent\"once you have universal health care and every possible social safety net, then people will be able to take risks without fear of failing and innovate like crazy\"...so the logic goes reply zdragnar 9 hours agorootparentThat&#x27;s a pretty decent paraphrasing of arguments for UBI that I&#x27;ve seen just on this site, actually. reply HWR_14 9 hours agorootparentprevAnd there&#x27;s no evidence that logic doesn&#x27;t hold. It&#x27;s not like Canada introduced universal health care 20 years ago so we can see the effects of just that. reply hartator 10 hours agorootparentprevIt‚Äôs what people say when you complain about a very heavy social bureaucracy. reply geepytee 11 hours agoparentprev>public sector growth has outpaced private sector during that whole timeInteresting, do you have a source for this? Would love to read more about it reply motohagiography 10 hours agorootparentThis example is for the prior decade, but the difference is so stark over 2003-2013 (22% public sector growth vs. 10% private) that even if the comment was conjecture, it would be a pretty good one. https:&#x2F;&#x2F;www.fraserinstitute.org&#x2F;article&#x2F;times-have-changed-p...I&#x27;d suggest a more constructive comment might be to find an example source and ask whether it is consistent with the reasoning. These \"source?\" comments are low effort and degrade the quality of conversation. reply II2II 9 hours agorootparentIt is also worth noting that the Fraiser Institute is a conservative think tank and the article does not explain their methodology. In some ways the article is contradicting itself, by making assertions that imply causation near the top of the article then admiting that correlation does not imply causation near the end of the article.Understanding whether we are looking at causation or correlation is important here. Canada is neighbours with the largest economy in the world, with the Canadian economy being heavily influenced by the American economy. Quite often public programs are introduced simply to be competitive because the private sector is not stepping up. (At least that is the perception. That perception is important polticially.) reply motohagiography 9 hours agorootparentIt&#x27;s an example that is better than a comment that says, \"source?\" Pretty sure public sector growth outpaces private sector growth in Canada.However, what is \"stepping up,\" in the private sector, as if it involves putting down our cigars and caviar and having our butlers open the jobs lever a bit, I&#x27;d suggest there is a broken ontology at play. reply faeriechangling 9 hours agorootparentprevI found that Fraser Institute article first and went through the effort of finding the actual Statscan data because I&#x27;m well aware that a significant slice of Canadians will disregard anything coming from the Fraser Institute. I&#x27;ve been hearing about they&#x27;re an evil organisation funded by American Corporations since I was a wee lad hearing my socialist family complain about the latest Fraser Institute study published in our local Postmedia paper. They implored me never to trust them!In any case, their claims seem to line up with the Statscan data to me, and I don&#x27;t see where else they would have gotten this data from. reply fooster 5 hours agorootparentThey may have just made it up to fit their narrative. reply faeriechangling 10 hours agorootparentprevhttps:&#x2F;&#x2F;www150.statcan.gc.ca&#x2F;t1&#x2F;tbl1&#x2F;en&#x2F;tv.action?pid=141002...Sept 2023 - Public sector employees: 4,291.7; Private sector employees: 13,293.4; Self-employed: 2,685.1Sept 2003 - Public sector employees: 2,945.8; Private sector employees: 10,298.9; Self-employed: 2,430.0Difference - Public sector employees: 1,345.9; Private sector employees: 2994.5; Self-employed: 255.1% growth - Public sector employees: 45.7%; Private sector employees: 29.1%; Self-employed: 10.5% reply Scoundreller 10 hours agorootparentPopulation change over the same time period is +23%. 31.6m to 38.8mWhere did all the jobs come from? Are people not retiring? Spending less time in schooling?It&#x27;s wasn&#x27;t a massive decrease in unemployment that could explain that.https:&#x2F;&#x2F;www.macrotrends.net&#x2F;countries&#x2F;CAN&#x2F;canada&#x2F;population reply BeefWellington 2 hours agorootparentThat would make 23% roughly the floor then for public sector growth, assuming that the increase in population requires a corresponding increase in jobs like teachers, police, firefighters, healthcare, and all the other civil servants.The remaining 26% is the real question - who did huge hiring over the past 20 years? reply abdullahkhalids 10 hours agorootparentprevA large part of the population growth is from adult working-age immigrants. So the employable proportion of the population has gone up. reply tormeh 10 hours agorootparentprevAn easy way to explain this is increased health care and old-age care costs due to an ageing population. I haven&#x27;t verified it for Canada in particular, but I&#x27;d be surprised if this wasn&#x27;t the cause. reply orwin 10 hours agoparentprevThe reason seems to be the same than France. I think in the OECD only the US managed to avoid it yet (probably a mix of net immigration, retirement age being really weird, and life expectancy 10year lower than the average ), but most of the increase is in Healthcare, specifically elderly care, and in the administration, also caused by the population being older: the overall population grow, but the number of &#x27;active&#x27; do not, or do it slower.Education, research and policing growth slower than all other ministry (although police funding seems to be catching up).There is also administrative bloat, mostly due to more rules, but it seems quite contained here to ecology and urbanism (except in the education ministry, which employ now more administrators than teachers) reply throwawaysleep 10 hours agoparentprev> If the idea is that the public sector‚Äôs wonderful services are supposed to help the private sector flourishThat isn&#x27;t the idea. The services are for the people, not to help you line your pockets. reply Tiktaalik 10 hours agoprevThe severe rent seeking behaviors throughout the Canadian economy are making things too burdensome to try entrepreneurship.For a Main St type business, rents, costs and regulations are too high.Rents again come into play for the entrepreneur, being so insanely high that they discourage risk taking and encourage people to stay at their current relatively high paying company.As with so many things in Canada it all comes back to housing lol.We will not have more entrepreneurship until it is dramatically cheaper to live in Canada, and that means lowering one of the biggest costs that people have, which is housing. reply cmrdporcupine 6 hours agoparentIt&#x27;s been this way since colonial times. It&#x27;s either rip-it-and-ship-it extraction industries... or \"I got there first, so give me $$\" rent seeking & monopoly.I don&#x27;t want to be a part of the US, I hate their culture and politics. But the local elites here benefit too much from having their own little closed market to monopolize and control. reply monero-xmr 6 hours agorootparentThis comment, and the whole thread really, is such a mind fuck for me - someone who lives in a rich left-wing American city. Canada is idolized and worshipped here as the paragon of lifestyle - the perfect balance of capitalism, civil discourse, and welfare state support (in addition to Norway). I literally have friends making comfortable $500k+ family incomes musing about how much better it would be to move to Canada.The grass is always greener! reply bugglebeetle 6 hours agorootparentGraz in Austria with its Marxist mayor is an example of a wealthy, left-wing city. No such cities exist in the US. Just look at the recent scandals with the LA city council, the place people would probably describe as the ‚Äúfar left‚Äù of the USA. The leading Democrats were recorded spewing racism, plotting to screw over renters, and destroy the ‚Äúleft wing‚Äù council members who are in favor of what would at best be centrist policies in Europe. People idolize Canada because it seems to have a slightly more pragmatic baseline than the US, although in reality both places are pretty far to the right. reply cmrdporcupine 6 hours agorootparent\"Left wing\" in the US (and TBH much of Canada) is just reduced to being about a bunch of cultural issues. Nobody on the so-called left in North America actually espouses any real socialist economic program. It&#x27;s blasphemy, and \"going too far\" even among people who wear the t-shirt. reply cmrdporcupine 6 hours agorootparentprevYea, American liberals don&#x27;t really get Canada -- even when they come here, my friends from the US... they wear rose tinted glasses. Most of it is fantasy. Canada&#x27;s social service programmes are really not great when compared to European social democracies; welfare and disability payments are actually quite low, and we have pretty bad homelessness in our major cities. Canada is no Scandinavian social democracy. It&#x27;s really just a fusion of aspects of both the UK and the US.Universal healthcare is good but it&#x27;s been underfunded for decades, slowly coming apart, and inconsistent across the country.Public education is definitely in a better state than the US, though. And I&#x27;d argue our higher education &#x2F; university system is superior in both cost and quality. So far.Culturally, Canada might be in a better state. The culture wars not as pronounced (depending on where you are). Abortion rights etc. mostly a given. Gay marriage and rights pretty much unassailable at this point. Legal cannabis, etc. etc.But in terms of bread and butter supports for regular working people, it&#x27;s not that great. Though of course, again, the health care thing is a big caveat. US healthcare is great if you&#x27;re upper middle class, wayyyyy better than anything you&#x27;d get here. But the average working class person is definitely better off in Canada compared to the US, on the healthcare front.Anyways, housing prices here are way worse (relative to earning potential) than any American city other than I guess Manhattan, at this point. People in the Bay Area complain, but they really have nothing on the GTA or Vancouver lower mainland. reply monero-xmr 5 hours agorootparentI appreciate your perspective. I think another crucial aspect is that the Canadian population is only 10% of the US, and the variation among states here is just like Canadian provinces or even countries in the EU. The US is very decentralized, and one of its secrets is that during times of great internal division the federal government becomes paralyzed while the states becomes empowered, and conversely during times of national unity the federal government can move mountains while the states take a back seat. Currently it‚Äôs a paralyzation moment, but that doesn‚Äôt really prevent anything from moving forwards as the system is pretty decentralized and individual-focused already.On the healthcare aspect, there is no doubt the poorest people in Canada are the EU are treated better than the poorest in the US. I do have a suspicion that as the universal systems reach a breaking point they start to look a bit more like the US - where people pay more if they use more - to encourage better economization of scarce healthcare resources. I have heard some really fucked stories of cancer treatment delays leading to death which I don‚Äôt think is as common here (they will treat you in the US, but you might go bankrupt). It‚Äôs like, would you rather be rationed and die or get treatment and be bankrupt? I‚Äôd prefer to live myself. reply cmrdporcupine 5 hours agorootparentCanada&#x27;s population only appears low if you&#x27;re not looking at its geographical distribution. Toronto is as big as Chicago, and the population density in southern Ontario is as high as any of the US eastern states, and mostly contiguous with them and part of the same shipping routes, etc. etc..It&#x27;s a population clustered near the border and really should be seen as one that is culturally and economically contiguous with the US, just not politically. In terms of metropolitan regions, much of Canada is really much the same as the US.On healthcare &#x2F; public services, I will give you examples of where the US liberal biases about Canada just don&#x27;t line up.At the tail end of the COVID-19 crisis, spring 2021 I went across to go skiing with a friend from Albany. I needed a test to get back across the border to get home. I was able to just go to a US pharmacy and get a molecular test for free, with almost instant results. I was told that if I tested positive I could get anti-virals, for free, immediately, without even showing citizenship. Meanwhile in Ontario all public testing for COVID-19 was cancelled before the end of 2020. My elderly parents in Alberta just tested positive but had to fight to get anti-virals because despite being 1 year shy of 80 they didn&#x27;t qualify unless they had pronounced comorbidities. So... a country without a universal public health care system was able to offer much better universal coverage (where it mattered) than here.Another example: say you wanted a blood test for your kid to test blood lead levels. No doctor will offer that here, really. You&#x27;d have to really lobby for it and have a serious acute reason for it-- despite all the older cities here being full of lead pipes. I was down in the Finger Lakes a few years ago and there were signs for public clinics all over offering this as a free service.It is by no means a black and white situation. There are many aspects in which many liberal US states outdo Canadian provinces on certain social programs. reply monero-xmr 5 hours agorootparentI just think it‚Äôs expected that 330 million people would have more variation than 40 million people. The density of Canada is certainly in comparison to dense US areas but the absolute population just puts thing in another order of magnitude - the single US market is insane entrepreneurially speaking. Companies get started and immediately have access to the biggest economy on earth, the biggest pools of investment capital, the richest consumers, the global military superpower, lobbyists who get access to politicians that can change other countries‚Äô laws and regulations, and on and on.I can‚Äôt even remember what we are discussing. In summation, I love the US, but this thread has really given me ammo to counteract the cocktail party liberals who worship Canada. replySashaSirotkin 11 hours agoprevThe opportunity cost of starting a business in Canada is too high. People who have the capital to start their own business are more likely to see better returns by investing in real estate which has much lower risk and requires less effort. The federal and provincial governments would need to stop artificially propping up the real estate sector to make entrepreneurship competitive which is unlikely to happen willingly. reply applied_heat 6 hours agoparentHow are the federal and provincial governments propping up the real estate market? reply SashaSirotkin 5 hours agorootparentMany of the programs to help with housing affordability is making it easier for first-time homebuyers to get enough money for a downpayment. There is tax break for first-time buyers, the ability to borrow money from your own RRSP and most recently the FHSA (First Home Savings Account). Long term this increases home prices by injecting more money into the system. reply martin_drapeau 9 hours agoprevI run a Canadian tech startup here. Funding to get going is of course an issue. The BDC (referenced in this article) is just another bank and have zero means to help a SaaS getting started.We turned to the US to get funding (besides our own money). TinySeed specializes in boostrapped B2B SaaS. They have an incubator program that is really good and got us going.We&#x27;re at the point of getting a line of credit and again have to deal with Canadian banks. Again the BDC cannot help more than any other bank.We also looked at government tech subsidies and grants. Honestly, there is so much paperwork that takes time away from gaining customers and growing the business. We did not pursue.Maybe when you reach 20+ staff these things will come to help you. Especially if you have someone to manage it. But when you get started, you&#x27;re on your own. reply raverbashing 12 minutes agoparent> We also looked at government tech subsidies and grants. Honestly, there is so much paperwork that takes time away from gaining customers and growing the business.I wonder how that compares with, for example, joining an YC batch, or talking to multiple VCs in search of financing reply cmrdporcupine 6 hours agoparentprevBack when I worked at a Canadian startup and had to do SRED paperwork I got the vibe that it was more about companies that could afford to hire SRED consultants than it was actual research and dev. Heard stories about web dev &#x2F; media shops milking SRED, while I saw people doing actual foundational engineering struggle (nor could they waste the time filling in the paperwork).This was over a decade ago. Did not come away with a positive impression of the tech startup scene in Canada. Yours and other comments since have only fed that. reply clwg 4 minutes agorootparentIn my experience, this is absolutely the case. I&#x27;ve observed straightforward tasks being claimed, audited, and approved as R&D under SRED. Take someone non-technical and ask them to do something in Tableau, you can probably make a SRED claim.As both a taxpayer and a technologist it‚Äôs quite appalling to witness. reply somerandomqaguy 3 hours agorootparentprev... I want to disagree with this, but it lines up with my own experience. My work was being presented for similar grants with very flimsy justifications. Not pure web work but it wasn&#x27;t exactly foundational research either. reply e-brake 2 hours agorootparentI too have seen this as a past entrepreneur in Canada. A lot of accountants are in on misrepresenting SR&ED tax credit applications and will attempt to find the \"research\" in regular IT work. They will even do the technical writing and filing work for free, while taking a commission of the take when the tax credits roll in (free money). There is a whole segment of startups propped up by SR&ED. I&#x27;ve seen many doing innovative work with a sound business plan, using that money to get to profitability, and it&#x27;s great they were able to get funding, but a lot of others milked the system fraudulently, using SR&ED consultants to continue to exist when they should have failed. reply chx 3 hours agoprev> and found that the country has 100,000 fewer entrepreneurs than it did 20 years ago ‚Äî despite the fact that the population has grown by more than 10 million over the same period.Canada is a bit below 39M people. In 2003 it was above 31M. The growth is decidedly less than 10M.But yes, support for entrepreneurs is nonexistent, my little company (not even reaching 1M CAD a year in revenue) is controlled by the same rules on paper as SNC-Lavalin (except of course if they break the rules the prime minister fires the minister of justice to avoid prosecution but if I am late with a report a few days I get fined). Many European countries have simple flat taxes with minimal administration. reply steve_adams_86 10 hours agoprevWell. I‚Äôve had a lot of ideas in my adult life that I felt passionate about, but doing business here is remarkably unrewarding. I can do what I enjoy and I‚Äôm good at with an American company for 2-3x what I‚Äôd realistically earn here working at least as hard on my own business.I‚Äôve consulted for several Canadian tech companies and their finances often seemed remarkably poor. I know some are doing well; I‚Äôm more so speaking about ‚Äúregular people‚Äù businesses. Small private companies with staff around 10-20 people.They work really, really hard for their incomes and the rewards really aren‚Äôt what you‚Äôd expect. This seems true outside of tech as well.My wife works for the federal government on the other hand. She earns much less than I do, yet still dramatically higher than the average for our city and the rest of the country. She also has generous time off, training, an excellent union, interesting work, etc. Why in the world would she start a business? She actually could, too. She‚Äôs an awesome hydrographer with cutting-edge skills and knowledge. But she‚Äôd have to work herself to the bone if she went private and she‚Äôd have to work in wildly different and less comfortable contexts. And for what? 1.5x the income? 2x? After that‚Äôs chewed up by taxes, she has traded her family life for a career that barely pays more when you do the quantitative and qualitative maths.Yet I think this is a huge problem. It isn‚Äôt really wise to work hard and innovate here, and I think it‚Äôs actually harming our workforce and economy quite seriously. Look to many countries and you can quickly point at many past and recent innovations and core competencies. But what do you see when you look at Canada? There‚Äôs certainly less, and seemingly less all the time. Our small population is a huge disadvantage here, but we have such immense opportunity for innovation.I do think we innovate in extractive industries. We do a lot of environmental research around our primary industries, and that‚Äôs very valuable. Economically though, I don‚Äôt know‚Ä¶ We aren‚Äôt much of a land of opportunity these days, and it doesn‚Äôt seem like much is happening to reverse this trend. reply jacquesm 9 hours agoparentThis isn&#x27;t Canada per-se but more of a SV versus the rest of the world (with a few other small exceptional pockets). reply logicchains 3 hours agorootparentOnly if you consider \"rest of the world\" to mean Europe + English speaking countries. There&#x27;s an insane amount of entrepreneurship coming out of China and India, and a similar hustle culture. Also in some parts of Africa, like Nigeria and Kenya. reply cmrdporcupine 6 hours agorootparentprevThank you. Was about to say. It&#x27;s a phenomenon of a few US coastal cities. And also very pronounced now, vs say 25 years ago. reply WalterBright 6 hours agoparentprev> After that‚Äôs chewed up by taxesFunny thing about high taxes being a disincentive. reply cmrdporcupine 6 hours agorootparentPlenty of US states (e.g. California) end up with total taxation up in the same range as Canadian provinces, in the higher income brackets anyways.And Canadian corporate taxes are very low. US and Canada basically the same on this front.https:&#x2F;&#x2F;www.canadian-accountant.com&#x2F;content&#x2F;taxation&#x2F;canada-...\"While the lowest five corporate tax rates worldwide are held by corporate tax havens (led by Ireland), the United States and Canada have ranked ninth and tenth respectively out of 33 major economies for the past two years. The corporate tax rate in Canada is lower than all the averages of regional jurisdictions, including the global and G7, according to the average tax rates of 33 UHY international firms, assuming companies have a profit of $1 million. \"Canada&#x27;s problem is not excessive taxation. It&#x27;s monopolies and lack of competition and over-reliance on commodity exports. reply WalterBright 5 hours agorootparentSomething is paying for Canada&#x27;s massive public sector. What do you think it is?As for California, a lot of companies moved to Washington to escape the high taxes. Washington used to be a low tax state, but no longer, and companies aren&#x27;t moving here anymore.(Washington recently added a capital gains tax, payroll taxes, a $.50 per gallon additional gas tax, and boosted the sales tax to 10.1%.) reply cmrdporcupine 5 hours agorootparentWhy don&#x27;t you try using facts and numbers instead of insinuations, then? Show me how Canadians are being screwed on taxes.Actually I know working class Canadians are -- Canada&#x27;s system is actually not very progressive compared to many US states, the bottom end of the tax bracket is fairly heavily weighted... but corporations here are sitting pretty. Best gig in Canada (after real estate) is to be a fake \"contractor\" paying corporate taxes instead of personal income taxes, and stashing your wealth in corporate assets and paying your spouse dividends... it&#x27;s ridiculous.In any case, to throw it back: Something is paying for the US&#x27;s massive defense sector. What do you think it is? reply WalterBright 3 hours agorootparent> Something is paying for the US&#x27;s massive defense sector. What do you think it is?Taxes and deficits. Both of which are extracted from the economy. reply user_named 4 hours agorootparentprevThe fake contractor scheme is exactly the same in Sweden, the dividends too and saving assets inside the business entity. replygumballindie 9 hours agoparentprevThis is true for tech work as well. Pay is higher in east european countries after a certain level of seniority. All in all Canada is noth worth it for the money. Everyone I know from the UK moving to Canada came back running as pay was easily higher. That means businesses can&#x27;t afford people. reply brailsafe 8 hours agorootparentFwiw, I&#x27;d still prefer living on the west coast of Canada than an overwhelming majority of other places like the U.K or U.S, even despite literally not having a job, any prospects, or the possibility of owning a home at any point in my life, even as a software developer. If I left, and it&#x27;s possible, it would be because all of those factors got to a point where I had no choice. reply applied_heat 6 hours agorootparentYou can get a 3 bedroom townhouse with a garage and a small yard 45 minutes from Vancouver for 900k. Is that really unachievable? Split it with a partner or a friend, rent out the 3rd room for $1k a month, and you need to save 90k for the Downpayment and the mortgage is 5k a month minus the rent, so you need to chip in $2000 a month to the mortgage. Rule of thumb for accommodations to be less than 30% of your income so your income is 6,666&#x2F;mo or 80000 a year, or $41&#x2F;hr.Any tradesperson can make that. People are charging $45&#x2F;hr to clean air bnbs.Doesn‚Äôt seem so hopeless to me. reply jholman 5 hours agorootparentGenerally I approve of this type of thinking, it&#x27;s how I got my start, too. So I&#x27;m only quibbling to improve the fidelity of the model for those who are like us.1) My impression is that it&#x27;s prettttty tough to get those 10% down mortgages these days. Though, again, that&#x27;s behind me now, I could be wrong.2) The bank will not consider your accom to be mortgage-minus-sublet-rent for the stress test, I promise you. They&#x27;ll count your rent as additional income, at some discount (fuck you, bank), and then count 30% of that against your mortgage payment (for the stress test). I guess if you know someone who actually got this past the bank stress test then let us know, and let us know which bank, because I&#x27;ll move the next time my mortgage comes up. But I have tried in the past. So you&#x27;d need a joint income of 200k (I trusted your estimate of mortgage at present rates on $810k), minus 10k&#x2F;a for the rent, is $95k each, $46&#x2F;h.Still. Although it is definitely difficult, it is, as you say doable. reply applied_heat 4 hours agorootparentI meant brailsafe‚Äôs share of the down payment is 90k, and brailsafe‚Äôs partner would also have to come up with 90k. 20% down. Agree 10% down is not plan A.Also agree on the banks being difficult to work with on including the future rental income when qualifying.It sucks if you are a single person - probably have to go apartment route or 1 bedroom condo. The most successful guy I know, who has a pretty sick motor yacht now, had a room mate until his now wife moved in. reply jholman 3 hours agorootparent> 20% down. Agree 10% down is not plan A.Right, okay. And, to be clear, back in my day 5% down (FIVE) was totally plan A, so that has gotten harder for the next generation. Thanks, fedgov!> difficult to work withHeh, notwithstanding the amusing savagery of my sibling comment, is \"difficult to work with\" a euphemism for \"impossible to work with\", or do you have reason to believe that this is even possible?> It sucks if you are a single personI would say that it&#x27;s getting closer to hopeless if you&#x27;re a single-income family, and being a single-income individual isn&#x27;t much better, because 1br condos are actually not that much cheaper than 1br.I&#x27;m in a condo, btw, and I could have done it on my income alone when I bought (laxer stress tests, I provided the down solo anyway, rates were lower, prices were lower), but I could not buy this condo solo today, and certainly not with the income I had then.I guess my overall message is: I think that the type of financial modelling you were doing is excellent to show that it is not yet impossible or hopeless, and if someone wants to make it happen, it&#x27;s still within reach... but... it&#x27;s genuinely really hard. For decades I&#x27;ve been scoffing at people whining about affordability, saying that they just spend too much on short-term pleasures, but in the last decade I&#x27;ve kinda stopped the scoffing and shut my mouth. reply user_named 4 hours agorootparentprevHe&#x27;s not saying banks are difficult to work with. He&#x27;s saying your entire calculation is funds mentally wrong. replyjacquesm 9 hours agorootparentprevThat and those never ending winters. reply dopidopHN 8 hours agorootparentYeah. But also those infinite boreal forest, 10 of thousand of pristine lake‚Ä¶ There is something about it. reply jacquesm 38 minutes agorootparentAgreed, nature is amazing there. I&#x27;d go out for solo hikes in the Sault-ste Marie, Wawa, Sudbury triangle and it was incredible. reply steve_adams_86 8 hours agorootparentprevOur winters here in Victoria are great. A decent amount of clear skies, never too cold, not as much rain as people think, heaps of stuff to do nearby or a day trip away. It‚Äôs hard to beat. reply silisili 7 hours agorootparentVictoria is also the most expensive city in Canada, which - reading the accounts of low pay in this thread, make it seem out of reach for most Canadians. reply cmrdporcupine 6 hours agorootparentprevWinter in Toronto (lattitude 43.6) isn&#x27;t really much different from Chicago (lattitude 41.8) and pretty much the same as most of the US northeast and not nearly as bad as a place like Minnesota (or Vermont or Maine, etc.) etc. Toronto sits at the same latitude as northern California, and while it has plenty of very cold days the total length of winter isn&#x27;t any longer than much of the northern half of the US.It ain&#x27;t California or the PNW, but it also doesn&#x27;t fit the stereotype of the Great White North, eh? As a person from a part of Canada with actual winters, calling this never-ending winter seems like a giant distortion. Most years we don&#x27;t really have proper snow on the ground until after Christmas, and it&#x27;s gone before April. Where I grew up in Alberta it&#x27;s snow from late October until April. reply jholman 2 hours agorootparent> Toronto sits at the same latitude as northern CaliforniaThis is(a) just actually not literally true (41N at the border vs 43N),(b) particularly off-base when you consider that \"northern California\" is typically a reference to the SF Bay Area(c) completely deceptive.Four months of the year with around two feet of snow, with an average low below freezing. San Francisco has barely ever gone below freezing in its entire historical record (literal record low of ‚àí3C a hundred years ago).Yes yes, you don&#x27;t have igloos in Toronto, fair point. But a person who drives on summer tires year round in Toronto is a homicidal maniac. In Vancouver that&#x27;s merely lazy, an excuse to call in sick a few days a year.No argument about Minnesota, though, nor about The Texas Of The North. And yes, it&#x27;s hardly never-ending winters. The summers in T.O. are brutal too. replytired_and_awake 11 hours agoprevCanada (in particular Vancouver), fix your absurd housing problem and we&#x27;ll come back. reply voisin 9 hours agoparentNo need for the particular call out to Vancouver. That‚Äôs a 2014 story. In 2023, every city in the country has an absurd housing problem. I‚Äôve lived coast to coast over the last five years and seen first hand how it doesn‚Äôt matter if you are in Vancouver or Halifax (or much smaller towns far away from either), the pricing has gone parabolic thanks to interest rates being ultra low for two decades and housing stock being artificially constricted, and god damn AirBNB ‚Äúinvestors‚Äù buying everything.Thankfully we now have a ministry of finance serious about inflation and moving rates high, and provincial and federal governments cracking down on Airbnb. reply bbarnett 6 hours agorootparentThankfully we now have a ministry of finance serious about inflation and moving rates high, and provincial and federal governments cracking down on Airbnb.No minister has anything to do with interest rates. Only the Bank of Canada does, and it takes no input or guidance from the government. reply cmrdporcupine 6 hours agorootparentUnfortunately I get the impression that this current BoC leadership is wayyy too sensitive to public backlash and input. And fear that the loud and problematic proclamations of the official opposition are going to lead to immense pressure to lower rates prematurely. reply bbarnett 4 hours agorootparentI don&#x27;t know where this impression comes from, although I do see excess political noise on this front.I am disappointed by this aspect of the OO, but that does not imply the BoC is more sensitive. replypbj1968 10 hours agoprevThey all move to America, take high paying jobs, and complain about everything. At least the ones I‚Äôve encountered. reply kahnclusions 6 hours agoprevDifficult and complicated to get funding, insanely high rent, Canadian banks and telcos are stuck in the Stone Age, and the government bureaucracy is impotent and moves at a snail‚Äôs pace.Or we move to America and earn twice as much money. reply langsoul-com 4 hours agoprev> The Crown corporation says the decline puts the economy in danger, and it worked with researchers from the Universit√© de Montr√©al to analyze the problem and what&#x27;s causing it, and to consider solutions. Its recommendation: Some of the difficulties of entrepreneurship can be overcome by helping business owners develop \"soft skills,\" such as grit, marketing and how to interact with peopleI wonder if they&#x27;re self aware about how dumb their suggestions sound. If all it took was some sort skills, then everyone would be starting businesses left and right.Or because they&#x27;re paid to say something. And that needs to be easily and individually actionable. reply adamiscool8 8 hours agoprevCanada is captured by real estate investors, bureaucrats, and the Century Initiative [0]. So rather than developing national infrastructure and tier 2&#x2F;3 markets across this giant landmass (like China) to create affordable places to incubate entrepreneurship, they ramp up immigration into \"mega-regions\" driving up living costs, making everyone risk averse and entrepreneurship untenable.[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Century_Initiative reply voisin 9 hours agoprevI would suggest that 20 years ago life was far more affordable for the average person, such that it was more conducive to risk taking for personal gain. Today, everything is so damn expensive (housing being the most obvious, but really everything) that taking a few years out of your life to try to start a business feels like a massively riskier proposition in that those are years you‚Äôd otherwise be making money to ideally save for a house down payment which keeps getting farther and farther away even while working (in some areas of the GTA, house prices were increasing on average by $80k&#x2F;month). reply thomassmith65 5 hours agoprevThe grand theories in this thread about Canada&#x27;s national character, its housing prices, its tax rates, etc etc. The descent of Canada&#x27;s tech industry has moved in lockstep with the ascent of the internet, and good internet access depends on good telecom companies, and Canada has none. Canada&#x27;s telecom monopoly stabbed the country&#x27;s future to death with a rusty awl. reply EVa5I7bHFq9mnYK 2 hours agoprevApparently it&#x27;s a Netherlands decease, as Canada&#x27;s main business is natural resources, and it makes little sense to invest in other things. reply pj_mukh 11 hours agoprev‚Äú People in their late 20s to early 40s are the most likely to start businesses, but that demographic is shrinking with Canada&#x27;s aging population, leaving a smaller pool of candidates as potential founders.‚ÄùCanada has an ace in the hole here (that America doesn‚Äôt) in the form of a relatively generous immigration program.It‚Äôs time it leans on immigrants to bolster its entrepreneurship. Immigrants are by definition self-selected risk takers it‚Äôs a match made in heaven. However to fix it Canada needs:a) less regulations on work requirements for immigrants (it‚Äôs less complicated than America but that‚Äôs not saying much)b) better funding environment (I‚Äôm not a funder&#x2F;investor so not qualified to speak to this, but I know it sucks)c) housing, housing and lots of cheap housing especially, in the big cities and college towns where most budding entrepreneurs build the networks required to succeed.Without c) esp Canada is DOA. But that‚Äôs a whole other post. reply faeriechangling 11 hours agoparentThe relatively generous immigration program is resulting in higher population growth than housing built spiking property prices. More and more of the Canadian population over the last 20 years has been recent immigrants and entrepreneurship has cratered. It‚Äôs not evident to me at all Canadas immigrant friendly stance is helping with entrepreneurship and it‚Äôs fairly plausible that it‚Äôs having an adverse effect, although it‚Äôs also plausible that it has prevented the situation from being worse than it otherwise would have been, and additionally possible that tweaks to immigration might result in immigration being more of an engine of entrepreneurship than it is right now.To me, it seems to me that immigration is mostly motivated by politicians desperate to cover up the cost of their unwise deficit spending and liabilities by diffusing the cost across more people, as well as enriching the landed gentry with more rents. They couldn‚Äôt give two craps about helping recent immigrants start successful businesses, they want them to benefit their interests immediately, not incubate some far off benefit twenty years in the future. reply KRAKRISMOTT 11 hours agorootparentThe talented ones just move south the moment they have the chance to. The bay area is full of grads from places like Waterloo and Toronto. reply graypegg 11 hours agorootparentprevBirth-right citizen Canadians just aren‚Äôt having enough children to make that work in a modern economy requiring a big base of young workers.Sure we have enough housing to support our slowing aging&#x2F;dying native-born population if we shut off immigration. That‚Äôs not actually an improvement at all.We agree that cost of living + housing costs are cratering entrepreneurship, and that the simple effect of supply (slow home building, restrictive zoning) and demand (more people&#x2F;immigration) is at play.However, I think it‚Äôs not an option to slow down on either immigration or keep the current slow trickle of residence building as-is. We need BOTH faster, because we let this problem fester. reply pj_mukh 11 hours agorootparentprevI hear this all the time.But everytime it‚Äôs complaints about too much demand, but never a complain about supply. Even though they are both similar political problems. I don‚Äôt want to speculate why.Immigrants have more than proven themselves to be good entrepreneurs once the shackles I mentioned above have been removed. reply faeriechangling 11 hours agorootparentNobody complains about supply? News to me. Trudeau has been boasting about how his sizeable immigration increases will solve the housing crisis for months now with a greater supply of workers to address the ‚Äúlabour shortage‚Äù. The ‚Äúlabour shortage‚Äù being the biggest buzzword to complain about lack of supply. Meanwhile construction has been basically stagnant and even backsliding slightly according to statscan data while he has basically given a big shot of adrenaline to demand.Whereas a five year old could understand, if we literally just stopped all immigration tomorrow (this would have extremely bad consequences like causing an immediate recession and huge unemployment but hypothetically) housing prices would crater because Canadians don‚Äôt reproduce at anything close to replacement. But people go oh no no, the REAL solution to the problem is the thing causing the problem.Immigration I accept is a lesser evil to the alternative, but it‚Äôs certainly not the solution to the housing crisis. It‚Äôs just not. reply pj_mukh 11 hours agorootparentThe federal government is (mostly) inconsequential on housing supply. Trudeau knows this and so has to do all kinds of other song and dances to distract.If we have a labor problem let‚Äôs import construction workers, fix zoning, build a large housing fund and construction department and mobilize it. Actually work the problem on a provincial and municipal level.As opposed to constantly kvetching about supply and trying to murder it (see: foreign buyers bans, airbnb bans and now the ever popular immigration stoppages). It‚Äôs all whack-a-mole band aids that are clearly not working.Solve the actual problem, and that‚Äôs supply, supply and supply. reply z0r 10 hours agorootparentI don&#x27;t see how increasing demand solves supply problems. Surely there are multiple problems here, not just supply. replyKRAKRISMOTT 11 hours agoparentprevI have friends who moved north and applied for their entrepreneurship visa. The whole program is a pay for play scam where local VCs and incubators either demand money up front to \"join\" their incubator or give ridiculous equity terms in the hope of cornering a desperate founder who won&#x27;t or can&#x27;t immigrate easily to the states. The whole thing is a money making grift designed to enrich the friends of their politicians.https:&#x2F;&#x2F;www.canada.ca&#x2F;en&#x2F;immigration-refugees-citizenship&#x2F;se...You are going to find very few if any brand name VCs participating in their dog and pony show.https:&#x2F;&#x2F;www.canada.ca&#x2F;en&#x2F;immigration-refugees-citizenship&#x2F;se... reply pj_mukh 11 hours agorootparentControversial opinion: that program is mostly inconsequential at scale.The real needle driver is the much maligned student visa program. While stuffing students into rando colleges in the far flung regions of the country (‚Äúdiploma mills‚Äù) isn‚Äôt great, the mainstay colleges&#x2F;universities should be encouraged to double&#x2F;triple their intake.There is an existing workflow that turns students into well-networked, educated individuals empowered to manouever the levers of power&#x2F;capital. Just lean on that more. reply silenced_trope 10 hours agorootparentprev> You are going to find very few if any brand name VCs participating in their dog and pony show.Maybe some grifters. Case in point, Jason Calacanis: https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;TheAllinPodcasts&#x2F;comments&#x2F;17bkk8a&#x2F;j... reply steve_adams_86 10 hours agoparentprevI live in Victoria, and while our situation is special (we‚Äôre on the tip of an island and can‚Äôt build in many directions because it‚Äôs ocean), a few developers I‚Äôve spoken to are very confident that the housing issue‚Äôs greatest barrier is labour.I asked if immigration could address this, and they each pointed out that a. Immigrants can‚Äôt afford to work in this industry in places where we need housing the most, b. While building requires unskilled labour, it also requires highly skilled labour that is at an unprecedented shortage which isn‚Äôt easily or rapidly corrected with immigration, c. Cities and neighborhoods are deeply attached to their reasons for resisting various densification projects, and breaking through arbitrary red tape here is an immense and intractably expensive undertaking.They painted a very bleak picture of the housing situation, making it seem as though huge changes need to occur in order for meaningful progress to be made.This might be less of an issue in other cities. I‚Äôm not sure. reply somerandomqaguy 10 hours agorootparentI doubt immigration is going to help in that regard. At least the raw numbers aren&#x27;t painting a nice picture.https:&#x2F;&#x2F;www.canada.ca&#x2F;en&#x2F;immigration-refugees-citizenship&#x2F;co...2019 saw 1000 skilled tradespeople get invites.2020 had 251 come in.2021 saw zero people under the federal skilled trades worker program.2022 also had zero people come in under the federal skilled workers program.I don&#x27;t know why we&#x27;re not seeing skilled trades people getting invited to come into Canada. reply pj_mukh 10 hours agorootparentprev\"Cities and neighborhoods are deeply attached to their reasons for resisting various densification projects, and breaking through arbitrary red tape here is an immense and intractably expensive undertaking.\"The large-scale tolerance of this is basically permanently resigning Canada to degrowth. So no, lets not accept this. It is what most cities are facing and the provincial governments need to put an end to it.What we need is massive training&#x2F;re-training + labor importation program for the construction industry (and probably also healthcare, but I digress), and WW2 level mobilization in construction, anything short of that and I&#x27;m betting the economic and entrepreneurial problems get entrenched for a century. reply faeriechangling 10 hours agorootparentI agree with this notion politically, it would be a huge benefit if the federal government:#1 - took in a bigger slice of taxes#2 - redistributed them to municipalities#3 - made this redistribution contingent on densification.What we have now is some tragedy of the commons crap where everybody wants to address the housing prices, but if you call to densify any neighbourhood the local busybodies will cry bloody murder. Yet everybody wants the government to do something about housing, just not in their backyard. The only way for densification to be fair, and not result in one municipality seeing a sharper decline in housing prices than another, is for EVERYBODY to be compelled to change and have each municipality work out the finer details.Alas, I am deeply pessimistic about change, in no small part because elected politicians are far more likely to be landlords than the general population putting them in a huge conflict of interest. reply jackcosgrove 10 hours agorootparentprevI think the shortage of skilled construction labor is a huge problem in the US too. I have heard from general contractors and tradespeople that the trades have a big shortfall in the talent pipeline that is getting worse.I can only imagine the situation is worse in Canada since the immigration system favors people with tertiary degrees.We need to figure out how to incentivize people to choose skilled manual labor as a career. As it is the rewards don&#x27;t compare with a career spent in an office even if the pay in the office is lower. That&#x27;s because the office work is more pleasant and the career likely longer due to less wear and tear on the body.Public safety employees (police and firefighters) have early retirements because their jobs are physically demanding. If we don&#x27;t figure out a way to guarantee early retirements for people with physically demanding jobs in the private sector, young workers with options will continue to avoid them. And the houses that everyone knows we need most of all won&#x27;t get built. reply lotsofpulp 8 hours agorootparent> We need to figure out how to incentivize people to choose skilled manual labor as a career.You just did with the rest of your comment.>As it is the rewards don&#x27;t compare with a career spent in an office even if the pay in the office is lower. That&#x27;s because the office work is more pleasant and the career likely longer due to less wear and tear on the body. reply zdragnar 9 hours agorootparentprevThe apprenticeship model I&#x27;m the US is especially difficult. Why pay for electrician schooling, then struggle for three to five years earning $30k at best per year, when you can get more straight out of school in any white collar job?I don&#x27;t know what a better option would look like, but it is a deal breaker that some people I know personally had to wrestle with. reply voisin 9 hours agorootparentprev> can‚Äôt build in many directions because it‚Äôs oceanUP! Why does everyone forget UP?! I‚Äôve followed some of the projects proposed for development in Victoria (I am in the industry and lived on Salt Spring Island for a while), and the city has been as backward as any other city in the nation by making developers jump through hoops to get even a tiny modicum of density. The NIMBY population in Victoria is also among the most aggressive in the country. reply steve_adams_86 8 hours agorootparentHaha, I couldn‚Äôt agree more. Up is the obvious solution and it‚Äôs proposed on a regular basis, but yeah, the NIMBY powers are strong here. reply ta_vf7xjd34cc 10 hours agoparentprev> It‚Äôs time it leans on immigrantsUncontrolled immigration without assimilation can have disastrous consequences. And assimilation is a polite way of saying \"give up your culture and adopt ours.\"Unfortunately, without assimilation, all the politics and problems of the homeland will follow the diaspora to the new host country.",
    "originSummary": [
      "The Business Development Bank of Canada (BDC) report shows a noticeable drop in entrepreneurship over the last 20 years, with 100,000 fewer entrepreneurs despite a growing population.",
      "The BDC suggests the necessity of developing \"soft skills\" like marketing, finance, and leadership to tackle the challenges in initiating and operating a business.",
      "Factors contributing to the decline include a decreasing demographic of potential founders, low unemployment with high wages, and several discouraging factors in the business milieu. Proposals for reversing the downwards trend include scholarships for entrepreneurship and engagement of older, experienced entrepreneurs."
    ],
    "commentSummary": [
      "The discourse centers on the decrease in entrepreneurship in Canada, due to factors like limited understanding and investment opportunities, an oppressive business environment, and risk aversion tendencies.",
      "High living expenses, specifically housing costs, are seen as an obstruction for young aspirants; government inefficiencies are discussed, along with their impact on productivity.",
      "Themes of increasing public sector growth, Canada's image as an ideal living location, and issues of immigration and taxation are raised, pointing towards a need for government intervention in housing and construction matters."
    ],
    "points": 173,
    "commentCount": 237,
    "retryCount": 0,
    "time": 1697922755
  },
  {
    "id": 37967748,
    "title": "Human microbiome myths and misconceptions",
    "originLink": "https://www.nature.com/articles/s41564-023-01426-7",
    "originBody": "Your Privacy We use cookies to make sure that our website works properly, as well as some optional cookies to personalise content and advertising, provide social media features and analyse how people use our site. By accepting some or all optional cookies you give consent to the processing of your personal data, including transfer to third parties, some in countries outside of the European Economic Area that do not offer the same data protection standards as the country where you live. You can decide which optional cookies to accept by clicking on \"Manage preferences\", where you can also find more information about how your personal data is processed. Further information can be found in our privacy policy. Accept all cookies Manage preferences Skip to main content Advertisement View all journals Search Log in Explore content About the journal Publish with us Sign up for alerts RSS feed nature nature microbiology perspectives article Perspective Published: 31 July 2023 Human microbiome myths and misconceptions Alan W. Walker & Lesley Hoyles Nature Microbiology volume 8, pages 1392‚Äì1396 (2023)Cite this article 76k Accesses 3 Citations 898 Altmetric Metrics details Abstract Over the past two decades, interest in human microbiome research has increased exponentially. Regrettably, this increased activity has brought with it a degree of hype and misinformation, which can undermine progress and public confidence in the research. Here we highlight selected human microbiome myths and misconceptions that lack a solid evidence base. By presenting these examples, we hope to draw increased attention to the implications of inaccurate dogma becoming embedded in the literature, and the importance of acknowledging nuance when describing the complex human microbiome. Main Human microbiome research has undergone rapid growth over the past two decades and thousands of research papers on this topic are now published every year. Huge sums of money have been spent investigating the human microbiome as a cause of, or potential therapeutic solution to, a wide range of diseases, including inflammatory bowel disease and cardiometabolic conditions. Although truly exciting, the increasing focus on microbiome research has unfortunately also brought with it hype and entrenched certain misconceptions. As a result, many unsupported, or undersupported, statements have become ‚Äòfact‚Äô by virtue of constant repetition. Some are more widespread than others and some are relatively trivial, but, cumulatively, they highlight that misinformation is pervasive in the human microbiome literature. Given the potential importance of human microbiomes for health, it is crucial that claims are based on evidence. In this Perspective, we shine a light on persistent or emerging microbiome myths and misconceptions, outlining factual inaccuracies. We begin with relatively minor, but illustrative, points and build towards issues with greater potential impacts. We have purposefully tried to avoid unnecessary finger pointing at original sources of erroneous information, and instead hope that our insights and critical assessment are helpful to the field. ‚ÄúMicrobiome research is a new field‚Äù The pace of human microbiome research has greatly accelerated over the past 15 years, but the field is not in its infancy. To state so does a disservice to the excellent research that preceded the advent of high-throughput DNA-sequencing approaches. Indeed, there has been a rich history of research into human-associated microorganisms since at least the late nineteenth century. Escherichia coli was first isolated in 18851, bifidobacteria were described in 18992 and Metchnikoff speculated on the importance of beneficial gut microorganisms in the early 1900s3. Similarly, concepts such as the gut‚Äìbrain axis have been researched for centuries4 and health impacts of key microbiome-associated metabolites, such as short-chain fatty acids, were first reported more than 40 years ago5. ‚ÄúJoshua Lederberg coined the term ‚Äòmicrobiome‚Äô‚Äù Although Nobel laureate Joshua Lederberg had many notable achievements in his career, he did not invent the word ‚Äòmicrobiome‚Äô. This oft-repeated claim has been thoroughly refuted elsewhere, with evidence provided that the word was used in its modern context more than a decade before Joshua Lederberg first used it in 20016. Although relatively trivial, this myth serves as an example of how easy it is for falsehoods to become embedded in the human microbiome literature. ‚ÄúThere are 1012 bacterial cells per gram of human faeces‚Äù This figure is commonly mentioned in the microbiome literature, but its source has been difficult to ascertain. It may, however, have originated from dry-weight rather than wet-weight faecal cell counts. Regardless, it is incorrect. The real figure, as ascertained using various methods such as direct cell counts, fluorescence in situ hybridization, flow cytometry and quantitative polymerase chain reaction (qPCR), is typically between 1010 and 1011 microbial cells per wet-weight gram of faeces7,8,9. ‚ÄúThe human microbiota weighs 1 to 2 kg‚Äù Although this is mentioned many times in the literature, it is often given without citation and we were unable to find an original source for this claim. Nonetheless, it is unlikely to be true in most cases. The majority of the human microbiota resides in the colon, and these microorganisms typically account for less than half of the weight of faecal solids10. The average human stool weighs less than 200‚Äâg (wet weight)11, with total colonic contents ranging from 83 to 421‚Äâg in a small study of sudden-death victims12. Therefore, aside perhaps from rare cases of severely constipated individuals with extremely compacted faecal matter in their colons, the total weight of the human microbiota is much more likely to be less than 500‚Äâg, and perhaps even considerably lower than this in some cases. ‚ÄúThe microbiota outnumbers human cells by 10:1‚Äù This myth is perhaps one of the most pervasive in the human microbiome literature and is one that we have also repeated uncritically in the past (we, sadly, are not immune to mythology). Excellent work10 has, however, shown that this myth seems to have originated from a ‚Äòback of the envelope‚Äô calculation in the 1970s. More detailed analyses indicate that the true figure, albeit still impressive, is probably closer to a ratio of 1:1. It should be noted that the ratio is likely to vary from person to person and is dependent on factors such as the host‚Äôs body size and the amount of faecal material they are carrying in their colon13. Current estimates are also largely based on observations from adult individuals living in urbanized high-income country settings. More comprehensive estimates will require study of individuals from lower income or rural settings, and also from across the life course. ‚ÄúThe microbiota is inherited from the mother at birth‚Äù Although variants of this statement are more often found in popular science articles than the scientific literature, it is an example of how nuance is extremely important when describing the human microbiome. Although some microorganisms are directly transferred from mother to baby during birth14,15, proportionally few microbiota species are truly ‚Äòheritable‚Äô and persist through from birth to adulthood in the offspring15,16. Indeed, most of the expansion in gut microbiota diversity occurs after birth, during the first few years of life, and increases most dramatically after weaning17 (Fig. 1). Every adult ends up with a unique microbiota configuration, even identical twins that are raised in the same household18. Therefore, although microbiota assembly is not yet fully understood, adult microbial communities seem to be predominantly shaped by prior stochastic environmental exposures, as well as multiple other factors such as diet, antibiotic therapy and host genetics, with direct ‚Äòinheritance‚Äô from the mother at birth playing a similarly lesser role. Fig. 1: Diversity of the human gut microbiota dramatically increases in the years after birth. Diversity (as assessed using number of observed operational taxonomic units) dramatically increases during the first few years of life, particularly after weaning, before beginning to plateau in childhood. This pattern is observed across individuals living in different geographical locations: Malawi (red), Venezuela (green) and the United States (blue). Figure adapted with permission from ref.‚Äâ17, Springer Nature Ltd. Full size image ‚ÄúMost diseases are characterized by a pathobiome‚Äù It has become increasingly common to read claims in the literature that most diseases are caused by a ‚Äòpathobiome‚Äô. This is loosely defined as deleterious interactions between microbial communities and their host that lead to disease. This term is unfortunately overly simplistic and inherently flawed. Microorganisms and their metabolites are neither ‚Äògood‚Äô nor ‚Äòbad‚Äô, they merely exist. Their impacts on us as hosts are heavily dependent on context. Microorganisms or metabolites that are deleterious in one context may cause no harm in another. As examples, Clostridioides difficile can be carried asymptomatically throughout life, and only cause problems in older age when the host is immunocompromised and treated with antibiotics19. Similarly, a strain of E.‚Äâcoli may be relatively harmless in the colon, but cause a urinary tract infection if it invades the urethra20. As a result, the term pathobiome remains vague and lacking in the precision required to be truly useful in clinical practice. It is true, however, that numerous human conditions have been shown to correlate with alterations in microbiota composition. This is sometimes referred to as ‚Äòdysbiosis‚Äô, which is also a vague term with limited clinical applicability21. It is very likely that this may contribute to disease progression in some conditions, including inflammatory bowel diseases22,23, however, such alterations are rarely consistent and the microbiota is hugely variable between individuals, both in health and disease. This makes it extremely difficult to identify gut microbiota configurations with the required specificity and reproducibility for clinical practice24. In addition, correlating gut microbiome changes with systemic markers or data is fraught with challenges. This often fails to account for confounders such as age, body mass index (BMI), sex and medication, factors such as microbial community interactions or for changes that occur as a result of immunological, metabolic or other functional changes in the host rather than being directly causal (Fig. 2). Attempts to define ‚Äòtipping points‚Äô at which changes in microbiome composition definitively influence disease progression have so far largely failed to generate a clear consensus due to a lack of consistency between different studies. It is, therefore, a leap that is not yet evidence based to conclude that a characteristic pathobiome has a role in ‚Äòmost‚Äô diseases. Fig. 2: Difficulties of establishing causality from correlation-based microbiota studies. Changes in faecal microbiota have been associated with a range of diseases in humans. Interestingly, despite the diverse nature of these conditions, and the organs they affect, there are some broadly common recurring microbiota features, such as reduced diversity and increases in facultative anaerobes like Enterobacteriaceae. One common theme amongst these conditions is that they often result in increased levels of inflammation, at local and systemic levels. Such inflammation can, in turn, deplete the gut microbiota (and consequently microbial gene diversity), and allow facultative anaerobes such as Enterobacteriaceae to proliferate. This directly impacts the metabolic output of the microbiota, and its interactions with the host. Additionally, there are other host factors that contribute to disease and gut microbiota composition, such as age, BMI and medication, as well as host metabolism and immune response. This makes it very difficult to distinguish cause from effect in correlation-based studies. Full size image ‚ÄúThe Firmicutes:Bacteroidetes ratio is altered in obesity‚Äù Related to the previous section, this commonly used but erroneous claim stems primarily from rodent-based research, and from findings in single, or under-powered, human studies. However, as with many other studies that report links between specific microbiota profiles and disease, reproducibility is poor. Indeed, there have now been at least three meta-analyses reporting that this finding is inconsistent between human studies, and that there are, in fact, no reproducible microbial taxonomic signatures of obesity in humans25,26,27. This misconception also reflects an unhelpful tendency to examine sequence-based microbiota profiles at very broad taxonomic levels, such as phyla. Although this is appealing from a data simplification point of view, it fails to incorporate the huge and inherent variability within individual phyla. To draw a crude analogy, humans, birds, fish, reptiles and even sea squirts are all members of the phylum Chordata, yet clearly have very different physiologies, lifestyles and impacts on their environments. Moreover, this claim was also based on relative-abundance-based DNA sequence data. Compositional data are still useful, and can correlate well with absolute quantifications obtained with techniques such as qPCR28,29. However, some studies have suggested that relative-abundance-based correlations can lose significance when absolute microbial abundances are also factored into analyses9. Moving forwards, increased incorporation of absolute quantification data may help to make conclusions based on compositional analyses more robust. ‚ÄúThe gut microbiome is functionally redundant‚Äù This claim derives from studies showing that, whereas the taxonomic composition of human metagenomes can vary hugely, functional gene prediction profiles remain remarkably consistent. We contend that this is at least partly artefactual, as these functional comparisons are typically carried out after discarding the large proportion of metagenomic data that does not map to reference databases30. Much of what does map to those databases is likely to be derived from common housekeeping and/or well-characterized genes, which are found across many different bacteria and are also relatively well represented in reference databases. These comparative analyses therefore fail to accurately capture specialist, or less well-characterized, functions. As such, the truth is more nuanced. Although there are important functionalities that are conserved across many different human microbiota species, such as short-chain fatty acid production29, there are many key functions that are only carried out by a relatively small number of microbiota species. Examples include oxalate31 and resistant-starch32 degradation. In the absence of key species, functionalities such as these may not necessarily be fully replaced by other members of the microbiota. ‚ÄúSequencing is unbiased‚Äù Although sequence-based methods have been transformative for microbiome research, they are not perfect. Biases can be introduced at every step of sequence-based studies, from sample collection and storage, through laboratory-based steps such as DNA extraction, to choice of bioinformatic pipelines and reference databases used to analyse the data33. Comparisons of sequence-based versus culture-based studies of the microbiota have shown that sequence-based approaches completely failed to detect some species that were only recovered using traditional culturing methods34. Modern sequence-based approaches are hugely powerful but, like all techniques, they are not unbiased. For optimal interpretation of results, it is important to be aware of the inherent limitations of any given method. ‚ÄúWe need standardized methodologies‚Äù This opinion is prevalent in the microbiome field and is sensibly grounded in a desire to make it easier and more robust to compare results from different studies. However, as outlined above, there are no methodologies that are perfect, and all are biased in some way. If everyone in the world is using the same method, then everyone is equally blind to the limitations of that particular approach. There is also the problem of deciding which protocol everyone should use. For example, comparisons of results from the Human Microbiome Project with the MetaHIT project showed stark differences in microbiome profiles and indicated that the Human Microbiome Project protocol was less effective at extracting DNA from eukaryotes and Gram-positive bacterial lineages35. The truth is that the ‚Äòbest‚Äô method fundamentally depends on the underlying structure of the microbial community in a given sample and this can vary hugely between individuals and between body sites. For these reasons we argue, as others have, that optimization and verification of sequence-based results with additional approaches are preferable to asking everyone to adopt the same method36. An additional advantage of multi-faceted studies using different methods and research platforms is that they can enable more mechanistic understandings of associations between microorganisms and host phenotypes37,38,39. Increased transparency when reporting methodology choices would be helpful for comparing results from different studies. The recently published STORMS (Strengthening the Organization and Reporting of Microbiome Studies) guidelines40, for example, could greatly aid this process if adopted widely. ‚ÄúMost of the human microbiota is ‚Äòunculturable‚Äô‚Äù The adoption of high-throughput sequence-based technologies has also been mirrored by claims from some quarters that these methods must be used because most human-associated microorganisms cannot be cultivated in the laboratory. In fact, a reasonably large proportion of the bacterial and archaeal component of our microbiota has already been cultured41 (viruses and fungi remain under-represented), with pioneering work from as early as the 1970s establishing the cultivability of a broad diversity of species from the human gut microbiota42. Many more species continue to be cultured as laboratory-based efforts have increased throughput43,44. This implies that current gaps in culture collections are at least in part attributable to a lack of previous effort rather than an inherent ‚Äòunculturability‚Äô. Although cultivation is undeniably labour intensive, has its own biases and often requires expensive specialist equipment and media, there are clear advantages to having microorganisms in culture. These include enabling mechanistic experiments, verifying genomic predictions, and developing them as novel therapeutics45. Given the importance of continued cultivation-based work for the progression of microbiome research, it is gratifying that this myth has become less prevalent in recent years following the publication of the aforementioned high-impact studies that demonstrate it to be false. However, it serves as an excellent example of how previously widely accepted dogma is sometimes simply not true. There are important lessons for many other myths and misconceptions that have yet to become as widely rejected. Conclusions The microbiome field is broad, and there are many other controversial topics that might also have been included here. However, knowledge is still evolving on many of these; consequently, we have largely focused on concepts where we believe there is a strong evidence base for rejecting myths and misconceptions. Although some of the points above may seem trivial, we argue that the accuracy of details such as these matters. If we are consistently repeating falsehoods about minor details, can our accuracy be relied upon when covering more important matters? We hope that, by illustrating just a few examples of microbiome myths and misconceptions, we can draw increased attention to the potential problems of over-simplification and insufficient critical assessment in the microbiome literature. Given the many potential health impacts, the huge amount of funding and the keen public interest in microbiomes, rejection of unfounded assertions is crucial if we wish to avoid expending finite resources researching unproductive avenues and undermining public confidence in our conclusions. References Hacker, J. & Blum-Oehler, G. In appreciation of Theodor Escherich. Nat. Rev. Microbiol. 5, 902 (2007). CAS Google Scholar Cruickshank, R. Bacillus bifidus: its characters and isolation from the intestine of infants. J. Hyg. 24, 241‚Äì254 (1925). CAS PubMed PubMed Central Google Scholar Metchnikoff, E. The Prolongation of Life: Optimistic Studies (William Heinemann, G. P. Putnam‚Äôs Sons, 1907). Miller, I. The gut‚Äìbrain axis: historical reflections. Microb. Ecol. Health Dis. 29, 1542921 (2018). PubMed PubMed Central Google Scholar Roediger, W. E. W. The colonic epithelium in ulcerative colitis: an energy-deficiency disease? Lancet 316, 712‚Äì715 (1980). Google Scholar Prescott, S. L. History of medicine: origin of the term microbiome and why it matters. Hum. Microb. J. 4, 24‚Äì25 (2017). Google Scholar Stephen, A. M. & Cummings, J. H. The microbial contribution to human faecal mass. J. Med. Microbiol. 13, 45‚Äì56 (1980). CAS PubMed Google Scholar Hoyles, L. & McCartney, A. L. What do we mean when we refer to Bacteroidetes populations in the human gastrointestinal microbiota? FEMS Microbiol. Lett. 299, 175‚Äì183 (2009). CAS PubMed Google Scholar Vandeputte, D. et al. Quantitative microbiome profiling links gut community variation to microbial load. Nature 551, 507‚Äì511 (2017). CAS PubMed Google Scholar Sender, R., Fuchs, S. & Milo, R. Revised estimates for the number of human and bacteria cells in the body. PLoS Biol. 14, e1002533 (2016). PubMed PubMed Central Google Scholar Cummings, J. H., Bingham, S. A., Heaton, K. W. & Eastwood, M. A. Fecal weight, colon cancer risk, and dietary intake of nonstarch polysaccharides (dietary fiber). Gastroenterology 103, 1783‚Äì1789 (1992). CAS PubMed Google Scholar Cummings, J. H., Pomare, E. W., Branch, W. J., Naylor, C. P. & Macfarlane, G. T. Short chain fatty acids in human large intestine, portal, hepatic and venous blood. Gut 28, 1221‚Äì1227 (1987). CAS PubMed PubMed Central Google Scholar Sender, R., Fuchs, S. & Milo, R. Are we really vastly outnumbered? Revisiting the ratio of bacterial to host cells in humans. Cell 164, 337‚Äì340 (2016). CAS PubMed Google Scholar Ferretti, P. et al. Mother-to-infant microbial transmission from different body sites shapes the developing infant gut microbiome. Cell Host Microbe 24, 133‚Äì145.e5 (2018). CAS PubMed PubMed Central Google Scholar Valles-Colomer, M. et al. The person-to-person transmission landscape of the gut and oral microbiomes. Nature 614, 125‚Äì135 (2023). CAS PubMed PubMed Central Google Scholar Rothschild, D. et al. Environment dominates over host genetics in shaping human gut microbiota. Nature 555, 210‚Äì215 (2018). CAS PubMed Google Scholar Yatsunenko, T. et al. Human gut microbiome viewed across age and geography. Nature 486, 222‚Äì227 (2012). Goodrich, J. K. et al. Human genetics shape the gut microbiome. Cell 159, 789‚Äì799 (2014). CAS PubMed PubMed Central Google Scholar Sch√§ffler, H. & Breitr√ºck, A. Clostridium difficile ‚Äì from colonization to infection. Front. Microbiol. 9, 646 (2018). PubMed PubMed Central Google Scholar Worby, C. J. et al. Longitudinal multi-omics analyses link gut microbiome dysbiosis with recurrent urinary tract infections in women. Nat. Microbiol. 7, 630‚Äì639 (2022). CAS PubMed PubMed Central Google Scholar Olesen, S. W. & Alm, E. J. Dysbiosis is not an answer. Nat. Microbiol. 1, 16228 (2016). CAS PubMed Google Scholar Ni, J., Wu, G. D., Albenberg, L. & Tomov, V. T. Gut microbiota and IBD: causation or correlation? Nat. Rev. Gastroenterol. Hepatol. 14, 573‚Äì584 (2017). PubMed PubMed Central Google Scholar Pammi, M. et al. Intestinal dysbiosis in preterm infants preceding necrotizing enterocolitis: a systematic review and meta-analysis. Microbiome 5, 31 (2017). PubMed PubMed Central Google Scholar Damhorst, G. L., Adelman, M. W., Woodworth, M. H. & Kraft, C. S. Current capabilities of gut microbiome-based diagnostics and the promise of clinical application. J. Infect. Dis. 223, S270‚ÄìS275 (2021). CAS PubMed Google Scholar Finucane, M. M., Sharpton, T. J., Laurent, T. J. & Pollard, K. S. A taxonomic signature of obesity in the microbiome? Getting to the guts of the matter. PLoS ONE 9, e84689 (2014). PubMed PubMed Central Google Scholar Walters, W. A., Xu, Z. & Knight, R. Meta-analyses of human gut microbes associated with obesity and IBD. FEBS Lett. 588, 4223‚Äì4233 (2014). CAS PubMed PubMed Central Google Scholar Sze, M. A. & Schloss, P. D. Looking for a signal in the noise: revisiting obesity and the microbiome. mBio 7, e01018-16 (2016). PubMed PubMed Central Google Scholar Tettamanti Boshier, F. A. et al. Complementing 16S rRNA gene amplicon sequencing with total bacterial load to infer absolute species concentrations in the vaginal microbiome. mSystems 5, e00777-19 (2020). PubMed PubMed Central Google Scholar Reichardt, N. et al. Specific substrate-driven changes in human faecal microbiota composition contrast with functional redundancy in short-chain fatty acid production. ISME J. 12, 610‚Äì622 (2018). CAS PubMed Google Scholar M√©ric, G., Wick, R. R., Watts, S. C., Holt, K. E. & Inouye, M. Correcting index databases improves metagenomic studies. Preprint at bioRxiv https://doi.org/10.1101/712166 (2019). Daniel, S. L. et al. Forty years of Oxalobacter formigenes, a gutsy oxalate-degrading specialist. Appl. Environ. Microbiol. 87, e0054421 (2021). PubMed Google Scholar Ze, X., Duncan, S. H., Louis, P. & Flint, H. J. Ruminococcus bromii is a keystone species for the degradation of resistant starch in the human colon. ISME J. 6, 1535‚Äì1543 (2012). CAS PubMed PubMed Central Google Scholar Quince, C., Walker, A. W., Simpson, J. T., Loman, N. J. & Segata, N. Shotgun metagenomics, from sampling to analysis. Nat. Biotechnol. 35, 833‚Äì844 (2017). CAS PubMed Google Scholar Rajiliƒá-Stojanoviƒá, M., Smidt, H. & de Vos, W. M. Diversity of the human gastrointestinal tract microbiota revisited. Environ. Microbiol. 9, 2125‚Äì2136 (2007). PubMed Google Scholar Wesolowska-Andersen, A. et al. Choice of bacterial DNA extraction method from fecal material influences community structure as evaluated by metagenomic analysis. Microbiome 2, 19 (2014). PubMed PubMed Central Google Scholar Munaf√≤, M. R. & Smith, G. D. Robust research needs many lines of evidence. Nature 553, 399‚Äì401 (2018). PubMed Google Scholar Hoyles, L. et al. Molecular phenomics and metagenomics of hepatic steatosis in non-diabetic obese women. Nat. Med. 24, 1070‚Äì1080 (2018). CAS PubMed PubMed Central Google Scholar Koh, A. et al. Microbially produced imidazole propionate impairs insulin signaling through mTORC1. Cell 175, 947‚Äì961.e17 (2018). CAS PubMed Google Scholar Belda, E. et al. Impairment of gut microbial biotin metabolism and host biotin status in severe obesity: effect of biotin and prebiotic supplementation on improved metabolism. Gut 71, 2463‚Äì2480 (2022). CAS PubMed Google Scholar Mirzayi, C. et al. Reporting guidelines for human microbiome research: the STORMS checklist. Nat. Med. 27, 1885‚Äì1892 (2021). CAS PubMed PubMed Central Google Scholar Lagkouvardos, I., Overmann, J. & Clavel, T. Cultured microbes represent a substantial fraction of the human and mouse gut microbiota. Gut Microbes 8, 493‚Äì503 (2017). PubMed PubMed Central Google Scholar Moore, W. E. C. & Holdeman, L. V. Human fecal flora: the normal flora of 20 Japanese-Hawaiians. Appl. Microbiol. 27, 961‚Äì979 (1974). CAS PubMed PubMed Central Google Scholar Lagier, J. et al. Culture of previously uncultured members of the human gut microbiota by culturomics. Nat. Microbiol. 1, 16203 (2016). CAS PubMed Google Scholar Browne, H. P. et al. Culturing of ‚Äòunculturable‚Äô human microbiota reveals novel taxa and extensive sporulation. Nature 533, 543‚Äì546 (2016). CAS PubMed PubMed Central Google Scholar Walker, A. W., Duncan, S. H., Louis, P. & Flint, H. J. Phylogeny, culturing, and metagenomics of the human gut microbiota. Trends Microbiol 22, 267‚Äì274 (2014). CAS PubMed Google Scholar Download references Acknowledgements A.W.W. would like to thank S.‚ÄâPatrick for the original invitation to give a talk on this topic, which inspired the subsequent writing of this article. A.W.W. and the Rowett Institute receive core funding support from the Scottish Government‚Äôs Rural and Environment Science and Analytical Services division. L.H. is funded by Alzheimer‚Äôs Research UK, Healthcare Infection Society, Diabetes UK, Cancer Research UK and the European Union‚Äôs Horizon 2020 research and innovation programme under grant agreement 874583. This publication reflects only the authors‚Äô views and the European Commission is not responsible for any use that may be made of the information it contains. Author information Authors and Affiliations Microbiome, Food Innovation and Food Security Research Theme, Rowett Institute, University of Aberdeen, Aberdeen, UK Alan W. Walker Department of Biosciences, Nottingham Trent University, Nottingham, UK Lesley Hoyles Corresponding author Correspondence to Alan W. Walker. Ethics declarations Competing interests The authors declare no competing interests. Peer review Peer review information Nature Microbiology thanks Ami Bhatt, Fergus Shanahan and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Additional information Publisher‚Äôs note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Rights and permissions Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law. Reprints and Permissions About this article Cite this article Walker, A.W., Hoyles, L. Human microbiome myths and misconceptions. Nat Microbiol 8, 1392‚Äì1396 (2023). https://doi.org/10.1038/s41564-023-01426-7 Download citation Received 31 August 2022 Accepted 15 June 2023 Published 31 July 2023 Issue Date August 2023 DOI https://doi.org/10.1038/s41564-023-01426-7 Share this article Anyone you share the following link with will be able to read this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing initiative Subjects Clinical microbiology Metagenomics Microbiome Download PDF Sections Figures References Abstract Main ‚ÄúMicrobiome research is a new field‚Äù ‚ÄúJoshua Lederberg coined the term ‚Äòmicrobiome‚Äô‚Äù ‚ÄúThere are 1012 bacterial cells per gram of human faeces‚Äù ‚ÄúThe human microbiota weighs 1 to 2 kg‚Äù ‚ÄúThe microbiota outnumbers human cells by 10:1‚Äù ‚ÄúThe microbiota is inherited from the mother at birth‚Äù ‚ÄúMost diseases are characterized by a pathobiome‚Äù ‚ÄúThe Firmicutes:Bacteroidetes ratio is altered in obesity‚Äù ‚ÄúThe gut microbiome is functionally redundant‚Äù ‚ÄúSequencing is unbiased‚Äù ‚ÄúWe need standardized methodologies‚Äù ‚ÄúMost of the human microbiota is ‚Äòunculturable‚Äô‚Äù Conclusions References Acknowledgements Author information Ethics declarations Peer review Additional information Rights and permissions About this article Advertisement Nature Microbiology (Nat Microbiol) ISSN 2058-5276 (online) nature.com sitemap About Nature Portfolio About us Press releases Press office Contact us Discover content Journals A-Z Articles by subject Nano Protocol Exchange Nature Index Publishing policies Nature portfolio policies Open access Author & Researcher services Reprints & permissions Research data Language editing Scientific editing Nature Masterclasses Live Expert Trainer-led workshops Research Solutions Libraries & institutions Librarian service & tools Librarian portal Open research Recommend to library Advertising & partnerships Advertising Partnerships & Services Media kits Branded content Career development Nature Careers Nature Conferences Nature events Regional websites Nature Africa Nature China Nature India Nature Italy Nature Japan Nature Korea Nature Middle East Privacy Policy Use of cookies Your privacy choices/Manage cookies Legal notice Accessibility statement Terms & Conditions Your US state privacy rights ¬© 2023 Springer Nature Limited",
    "commentLink": "https://news.ycombinator.com/item?id=37967748",
    "commentBody": "Human microbiome myths and misconceptionsHacker NewspastloginHuman microbiome myths and misconceptions (nature.com) 173 points by lxm 15 hours ago| hidepastfavorite58 comments vjk800 1 hour agoPseudoscience involving human microbiome seems to be very popular here in HN. Whatever health articles gets posted, someone starts touting their favourite internet theory on how it is all completely determined by the human microbiome, using getting a lot of upvotes. I would advice those people to carefully read the caption on Figure 2 in the article:> Changes in faecal microbiota have been associated with a range of diseases in humans. Interestingly, despite the diverse nature of these conditions, and the organs they affect, there are some broadly common recurring microbiota features, such as reduced diversity and increases in facultative anaerobes like Enterobacteriaceae. One common theme amongst these conditions is that they often result in increased levels of inflammation, at local and systemic levels. Such inflammation can, in turn, deplete the gut microbiota (and consequently microbial gene diversity), and allow facultative anaerobes such as Enterobacteriaceae to proliferate. This directly impacts the metabolic output of the microbiota, and its interactions with the host. Additionally, there are other host factors that contribute to disease and gut microbiota composition, such as age, BMI and medication, as well as host metabolism and immune response. This makes it very difficult to distinguish cause from effect in correlation-based studies.In short: it is all much more complicated than you think and separating the cause and effect is something no-one has managed to do convincingly yet. It&#x27;s possible that the microbiome health really is as important as some people think, but it&#x27;s also possible that the role is more minor and the changes in it associated with various diseases is just incidental. reply epgui 14 hours agoprevIt‚Äôs a good article, but I have the following criticism (i‚Äôm not the biggest expert, but I‚Äôm a biochemist):- many of these myths are things I‚Äôve never heard before. I‚Äôm wondering how prevalent some of these really are.- some of these seem to be targeted towards uncharitable interpretations of ideas that could be explained more carefully- there are much more prevalent popular myths that are not mentioned.- I disagree about their take on how new of a field of research this is. I think they‚Äôre not taking big enough of a step back. I would say without hesitation that the field of nutrition, as a science, is VERY young. And yet it predates the study of the microbiome by a generous margin. I think this is a matter of perspective. reply tremon 13 hours agoparentThese don&#x27;t appear to be general-public myths, but misapprehensions from fellow scientists working in the field. Most of them are too specific (\"Firmicutes:Bacteroidetes ratio\", really?) to be prevalent among non-scientists.Also, this one is curious:- There are 10^12 bacterial cells per gram of human faecesThey first say \"it is incorrect\", then continue with \"the real figure is typically between 10^10 and 10^11\" -- that&#x27;s just one order of magnitude off, on a 12-digit number. The pedanticism is strong on that one. It only makes sense if they&#x27;re addressing their fellow colleagues, who are supposed to know better. reply chefandy 3 hours agorootparent> It only makes sense if they&#x27;re addressing their fellow colleagues, who are supposed to know better.Well, this is an article written by a medical journal asking research paper authors to stop making these common errors. reply bumby 12 hours agorootparentprevHere‚Äôs what may be a more charitable interpretation:A prevalent layman‚Äôs myth is that the bacteria in our gut outnumber our human cells by a wide margin (the article quotes the myth as 10:1). That‚Äôs a striking figure to a layman and ‚Äúonly‚Äù an order of magnitude off from what they claim is the true 1:1 ratio. reply epgui 13 hours agorootparentprev> These don&#x27;t appear to be general-public myths, but misapprehensions from fellow scientists working in the field.I am a biochemist and read a few relevant journals regularly, and that‚Äôs precisely what I‚Äôm talking about. reply dtgriscom 10 hours agorootparentprev> just one order of magnitude off, on a 12-digit numberDoesn&#x27;t matter how many digits are involved; a factor of 10 is pretty significant. reply wolverine876 8 hours agorootparentI don&#x27;t know. The difference between 10^101 and 10^102 seems insignificant. reply Sebb767 7 hours agorootparentDepends on context. The difference between 10^34 and 10^35 is the difference between a baby and a full-grown adult in planck sizes. For a more relevant example, a terrabyte is an unit we regularly work with that is ~10^18 bytes and an order of magnitude difference is still quite impactful (this is fewer orders of magnitude than your example, to be fair, but more than the original correction). reply thereisnospork 10 hours agorootparentprevDepends on the scaling factors. 10x the sound energy is only twice as loud. reply hgomersall 2 hours agorootparentOnly sounds twice as loud to a human observer. reply mjan22640 10 hours agorootparentprevImagine you received a wage that was 1&#x2F;10 of what it should have been, you notified the appropriate department in the company, and they wondered why is your pedanticism so strong when it is just one order of magnitude. reply tuatoru 7 hours agorootparentIf my official wage was 10^12 dollars&#x2F;euros&#x2F;pounds sterling, I don&#x27;t think I&#x27;d be worried, really. Even yuan or yen. reply ToValueFunfetti 5 hours agorootparentIt&#x27;s the difference between rich and poor in Venezuelan Bolivares (290k USD vs 29k) reply tuatoru 3 hours agorootparent29K USD is still more than I make now. reply sydbarrett74 11 hours agorootparentprevScientists should be 100% pedantic. It&#x27;s baked into the discipline. reply dekhn 10 hours agorootparentprevThe one that follows shows what they call \"incorrect\" is just 2X larger.When I read the article I determined it was basically some folks on a hobby horse trying to promote their view and saying other people are wrong.I don&#x27;t have a problem with debunking absolutely made-up (no original source) numbers, but if you&#x27;re sying people are wrong with a factor of 2 or 10 difference, you&#x27;re just being overly pedantic. reply djtango 7 hours agorootparentit&#x27;s impossible to know where and how an assumption will be built upon common intuition in existing science.What if the estimate for an LD50 for a new drug going into a trial were an order of magnitude out due to one of the underlying inputs being an order of magnitude out. The worst case scenario is that things could die off that wrong number. If nor humans, at least a lot of rats... reply teekert 10 hours agoparentprevI‚Äôve heard and believed most of them (I am a biologist). Yesterday I came across the 1 kg of microbiome statement in Nemo in Amsterdam (a science&#x2F;do museum for kids). So I consider them prevalent fwiw.I actually thought it proven that the microbiota was seeded during vaginal birth and may lead to more issues for caesarean section delivered kids. I guess it all just sounded so logical. I know I know, that‚Äôs the most dangerous thing. reply Alpi 6 hours agoparentprevActually I can relate to those misconceptions as a person who enjoys popular science.I like articles like this since they hopefully help me to fine tune my intuition. reply MarkusWandel 13 hours agoprevOne thing I didn&#x27;t see in this. Probiotics. So you have an active culture yoghurt, for example, that has one or a small number of bacterial species in it. This is claimed to benefit your gut biome. But how? Surely [re]introducing these few species hardly matters, given the overall diversity? reply InSteady 13 hours agoparentSome studies show bacteria from fermented foods colonizing the colon and others do not, although regular consumption does tend to marginally increase microbiome diversity (a generic and in many ways crude measure of GI health). In theory, an optimally functioning GI tract would be killing a majority of the live bacteria long before they reach the colon via a combo of enzyme activity, stomach acid, bile, pancreatic enzymes, and immune activity on the way down. What has been more consistently shown is fermented foods causing decreases in GI inflammation and other positive impacts. Some researchers now believe that many if not most of the benefits come from all of the microbial action prior to consumption.Fermenters such as Lactobacteria do a lot of work for us ahead of time, such as breaking down so-called &#x27;anti-nutrients&#x27; and converting low-bioavailability nutrients into high bioavailability nutrients, producing anti-inflammatory metabolites, outcompeting pathogenic bacteria and fungi, and so forth. Bacterial fermentation has even been shown to break down pesticides [0], aflatoxins [1], and other potential contaminants present in the raw food. I wouldn&#x27;t be shocked if microplastics and&#x2F;or PFAS gets added to that list, at least under certain conditions, in the coming years.[0] https:&#x2F;&#x2F;www.tandfonline.com&#x2F;doi&#x2F;abs&#x2F;10.1080&#x2F;10408398.2012.67...[1] https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC3737493&#x2F; reply elcritch 12 hours agorootparent> Some researchers now believe that many if not most of the benefits come from all of the microbial action prior to consumption.That seems counter to the whole idea behind modern research on the importance of the gut microbiome and a throwback to the thinking of the previous century.Certainly some of the work of bacteria is done before but that‚Äôs well known as many foods are fermented but the cultures are killed or pasteurized afterwards. Classic American yoghurt was like this, but there‚Äôs clear benefits to yoghurt with live cultures. Even if many bacteria don‚Äôt colonize the gut their presence in the food or taken separately appears important.In my opinion the evidence is overwhelming that actual probiotic bacteria themselves are important, not just their by products in the food. Our immune system detects healthy bacteria both living and dead and lowers inflammation in response, etc. There‚Äôs a lot of research on the topic and good summary research (1, 2). The presence of the bacteria also change the gene regulation in the intestines.Fecal transplants can directly treat diseases, or trigger diseases with maladaptive bacteria (3).1: https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC8512487&#x2F; 2: https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S277250222... 3: https:&#x2F;&#x2F;www.frontiersin.org&#x2F;articles&#x2F;10.3389&#x2F;fimmu.2022.9917... reply slothtrop 10 hours agorootparentYeah but fecal transplant bacteria isn&#x27;t comprised of the sort of transient bacteria generally found in probiotic sources. It leads to a kind of hard-reset for gut flora. There are few sources that offer bacteria that will persist in the gut, I think I read kefir is one of them, but to what extent I&#x27;m not certain. reply mjan22640 10 hours agorootparentprevFungi will start feeding on plastic in the environment relatively soon. Digesting plastic in an animal digestive tract is a tougher nut to crack, similar in principle to digesting celulose. reply syndicatedjelly 10 hours agorootparentCitation? reply djtango 7 hours agorootparentprevyeah I always found it a little counter intuitive how eating bacteria would be good for my gut insofar as most the GI tract is meant to be major gauntlet for bacteria.But I get dodgy IBS-like issues eating spicy food but consuming fermented foods in proximity really helps settle the stomach. ie Kimchi affects me less than other spicy dishes while eating natto after having a lot of spice can really help. Ditto for yoghurt but I&#x27;m lactose intolerent and so many lactose-free yoghurts are full of crap reply jalk 1 hour agorootparentWhat crap is in lactose-free yoghurt? Afaik they add lactase which lactose tolerant ppl still ‚Äúproduce‚Äù after weaning. So the lactose is simply ‚Äúpre-digested‚Äù into glucose and galactose. reply iterateoften 13 hours agoparentprevFrom what I read and researched online is that probiotics aren‚Äôt the bacteria you need, but are sort of good neighbors to the needed bacteria and create an environment they need to thrive by balancing pH of the gut and outcompete the ‚Äúbad‚Äù bacteria and yeast. reply epgui 13 hours agorootparentThe only thing I have to say about that is that pH is probably not very clinically relevant. The rest more or less checks out. reply iterateoften 12 hours agorootparentYeasts are pretty sensitive to pH levels reply InSteady 12 hours agorootparentTo say nothing of viruses, which make up a huge (but not well understood) portion of our microbiome. Current estimates put the number at over 100,000 unique species and perhaps a few hundred trillion viruses residing in the human microbiome, far eclipsing endogenous and bacterial cells. reply t-3 13 hours agoparentprevAlmost all food can be considered \"probiotics\", as pretty much everything is covered in microbes and provides food for microbes when eaten. A diet that promotes a healthy microbiome is going to be much more effective than supplementing lactobacilli or whatever. reply InSteady 12 hours agorootparent>A diet that promotes a healthy microbiome is going to be much more effective than supplementing lactobacilli or whatever.This may be generally true, but there may be situations where it is significantly better to do both, if we are talking about probiotic pills. Needs more research before making definitive statements (other than the reasonable stance that careful refinement of diet should always be the starting point when possible).Fermented foods are on another level, though. It appears that increasing consumption of fermented foods is just plain good for us as a general rule. May someday be a standard corollary to \"eat your veggies\" for overall health and longevity. reply chjfkdn 8 hours agorootparentThat&#x27;s why so many ancient cultures made beer. Basically every culture that began eating grains immediately also started fermenting them into beer.It wasn&#x27;t for the alcohol, that was normally very low in c. 5000BC beer anyways. It was because people that consumed it were healthier than those that didn&#x27;t. Fermentation is nutritius and beneficial beyond alcohol&#x2F;calories&#x2F;potable water. reply nabla9 10 hours agoparentprev(heard from probiotics researcher)Probiotics can potentially be highly effective, but you must eat some specific probiotic that helps you. The science of figuring out what type of probiotic you need is not very advanced.Pharmasies sell probiotics for consumers and there are also foods that claim to have benefits, but they don&#x27;t usually work, or work only by accident.If you have been taking antibiotics, your biome is likely out of order. Fecal transplants can help. reply forgotmypw17 13 hours agoparentprevTwo thoughts:a) By introducing a large number of one or a couple of particular species, you&#x27;re influencing the demographics.b) The type of creatures you find in fermented foods are exactly the type of creatures who can help you digest those foods. reply aziaziazi 13 hours agoparentprevMy 2 cents hypothesis : as some food ‚Äúkills‚Äù many organism (garlic for exemple is an excellent gut cleaner), you also benefit to re-fill ‚Äúgood‚Äù organism like those in yoghurt. reply cmckn 13 hours agorootparentI think you‚Äôd have to eat a colossal amount of garlic for its anti-microbial properties to have any measurable impact on your gut bacteria. reply InSteady 13 hours agorootparentprevGarlic is both antimicrobial and a rich source of food (oligosaccharides) for many strains of gut bacteria. It happens to be a favorite additive to fermentation recipes for this reason. reply chasil 14 hours agoprevThis was something interesting that I read recently on (reducing) staph colonizations:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34713431This came well after the University of Iowa protocols for mitigating staph prior to surgical procedures (it&#x27;s all about the nose, a favorite place for staph to inhabit):https:&#x2F;&#x2F;now.uiowa.edu&#x2F;news&#x2F;2013&#x2F;06&#x2F;be-gone-bacteria reply iambateman 9 hours agoprevWould someone please do all of us a favor and give an educated overview of what gut bacteria actually is and does?I honestly have no idea and the article just assumed I do. reply HankB99 12 hours agoprev> The average human stool weighs less than 200 g.Guess I&#x27;m not average. But TBH, I&#x27;ve never weighed my poop, only occasionally estimated.I always figured if I took a healthy dump before heading out to run, that&#x27;s be a pound less crap I had to haul around. ;) reply JPLeRouzic 13 hours agoprev> \"‚ÄúThe human microbiota weighs 1 to 2 kg‚Äù --- The majority of the human microbiota resides in the colon,They give a reference which appears to not have been cited by anyone???Yet it&#x27;s easy to find references (second result in Ecosia search engine).https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC6313343&#x2F; reply aziaziazi 13 hours agoprevI used to be bloated before my doctor advised me to try eating more bifidus with the help of a popular yogurt brand. It works like a charm: almost no bloating anymore if I got my morning yogurt but gaz seems to come back in a day or two of bifidus abstinence. I‚Äôd love to find a diary-free alternative. reply miriam_catira 13 hours agoparentIf you have Forager where you live, I&#x27;d highly recommend it. It&#x27;s a cashew based probiotic.https:&#x2F;&#x2F;www.foragerproject.com&#x2F; reply aziaziazi 12 hours agorootparentThanks a lot! I‚Äôll give it a try. reply daoboy 13 hours agoprevSo do I still have to drink kombucha? reply epgui 13 hours agoparentWhy do you believe you have to drink kombucha? reply daoboy 13 hours agorootparentI don&#x27;t really. It was just a silly joke based on the notion that kombucha is supposed to help your gut microbiome but many people think it tastes gross. reply mikrl 12 hours agorootparentThe cool kids brew&#x2F;drink kefir now.If you don‚Äôt have middle eastern or Slavic tastes you‚Äôll probably think it tastes gross tho. reply slothtrop 10 hours agorootparentI think I read that the slavs don&#x27;t like sour things and preferred to drink kefir during a state it was early stages of fermenting &#x2F; bubbling, which is rather sweet, if you&#x27;ve ever made it yourself. reply mikrl 10 hours agorootparentYes, I do the secondary fermentation which involves decanting it into a mason jar when it starts to separate, with some fruit pieces and letting it go for another 2-3h before refrigeration. This takes off the tartness and makes it a little creamier.My coworker, who is Iranian, usually just drinks it straight (I don‚Äôt know about the fermentation time) or with a little mint and salt since this is typical for West and Central Asia: refreshing because of the hot climate and similar to drinks like doogh or ayran. reply h1bf1throwaway 9 hours agorootparentprevIt was a great joke, fwiw reply daoboy 7 hours agorootparentThank you! This is a tough crowd, got a lot of down votes on this one reply slothtrop 10 hours agorootparentprevI like the taste but find the amount of added sugar dubious, in products sold. reply refurb 5 hours agoprev [‚Äì] It‚Äôs a nature paper so it‚Äôs directed towards scientists not the general public. But also scientists not necessarily doing research in the space.I‚Äôm happy to see a bit of myth busting going on. The space is in its infancy but wow do I ever see university press releases touting ‚Äúmicrobiome is connected to disease X‚Äù a lot and it‚Äôs usually based on pretty flimsy evidence.The field will mature over time and no doubt find some interesting connections to human health, but it seems like there is a mad rush to ‚Äúkeep a healthy microbiome‚Äù when in fact we don‚Äôt even know that much about what a healthy microbiome looks like beyond a few obvious diseases like c. difficile. reply vjk800 1 hour agoparent [‚Äì] From the article:> As examples, Clostridioides difficile can be carried asymptomatically throughout life, and only cause problems in older age when the host is immunocompromised and treated with antibioticsIt seems that even c. difficile is not obviously \"bad\" . replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article targets common myths and misconceptions about human microbiome research, including the belief that it's a new field and the incorrect figures about bacterial cells in human feces.",
      "It discusses limitations in microbiome research, challenging popular notions about the microbiome's role in disease, and advocates for evidence-based claims.",
      "The end goal is to provide readers with a more accurate perspective on the human microbiome."
    ],
    "commentSummary": [
      "The article delves into the misconceptions concerning the human microbiome, stressing the multifaceted nature of its role in maintaining health.",
      "It highlights the importance of further research to explore the full potential impact of the microbiome and addresses criticisms of current microbiome research.",
      "The beneficial effects of fermented foods on gastrointestinal health, such as reducing inflammation, are emphasized, supported by personal experiences regarding dietary changes."
    ],
    "points": 173,
    "commentCount": 58,
    "retryCount": 0,
    "time": 1697903169
  },
  {
    "id": 37966485,
    "title": "Refactoring has a price, not refactoring has a cost",
    "originLink": "https://www.germanvelasco.com/blog/refactoring-is-a-habit",
    "originBody": "German Velasco Blog Testing LiveView TDD Phoenix Talks Refactoring has a price. Not refactoring has a cost. Either way, you pay. October 20, 2023 I recently heard this phrase: Good habits have a price. Bad habits have a cost. Either way, you pay. That is a great life lesson. But it‚Äôs also a great lesson for our team and codebase. Refactoring is a good habit. I‚Äôm talking about refactoring as test-driven development (TDD) defines it ‚Äì something you do with every test and feature. I‚Äôm not talking about pausing feature work for a month so we can do refactoring. In TDD, we refactor as the third step of the red-green-refactor cycle: Red: write a test that fails Green: get the test to pass Refactor: clean up the changes. Improve design. Many developers think our work is done when the code ‚Äúworks‚Äù (when we get to green). After all, the tests pass. So, we‚Äôre good, right? I think that misses an amazing opportunity to improve our codebase! The work is not done when the code works. The work is done when the code works and when we‚Äôve made sure the changes are designed well. And to do that, we need to refactor. Of course, refactoring after every feature has a price ‚Äì just like good habits. It takes time and discipline. But people erroneously think that the alternative is free ‚Äì that not refactoring has no cost. Yet, if we don‚Äôt refactor continuously, we eventually have to pay the cost. That cost is likely imperceptible at first. So, we carry on without refactoring. Our codebase atrophies day by day, feature by feature, every change seemingly not affecting the whole. But one day all the cruft calcifies, and the numerous incompatible changes and features grind our progress to a halt. It is suddenly impossible to add new features without breaking something else. Bugs come up faster than we can squash them. And every feature takes longer and longer to ship as we have to verify that nothing is breaking. The cost of our bad habit accumulates until we cannot foot the bill. That‚Äôs when we throw in the towel, declare code bankruptcy, and call for the ‚Äúgreat rewrite‚Äù. We either refactor continuously as we build features ‚Äì paying the price. Or we slowly accumulate cruft until our codebase is impossible to change ‚Äì paying the cost. Either way, you pay. Want my latest thoughts, posts, and projects in your inbox? Subscribe I will never send you spam. Unsubscribe any time. Home Blog Testing LiveView TDD Phoenix Talks Twitter GitHub Get in touch ¬© 2023 German Velasco",
    "commentLink": "https://news.ycombinator.com/item?id=37966485",
    "commentBody": "Refactoring has a price, not refactoring has a costHacker NewspastloginRefactoring has a price, not refactoring has a cost (germanvelasco.com) 172 points by todsacerdoti 21 hours ago| hidepastfavorite151 comments bloopernova 19 hours agoWe&#x27;re dealing with accumulated tech debt burying us right now.For years the team was told to deliver features, that the upgrades to node dependencies weren&#x27;t of a high enough priority. Now some of the dependencies are so old they&#x27;re complicating upgrading past node 12.Yet the cybersecurity people are demanding that vulnerabilities be fixed.Thankfully we have a stronger product owner now, and spent most of the last 6 months getting things updated. Yet there are still those in senior leadership who openly question why our team hasn&#x27;t delivered the features they want.We&#x27;re working on educating them. And something pithy and catchy is useful, so I&#x27;m definitely going to mention this in future: \"fixing tech debt has a price, not fixing it has a cost.\"It&#x27;s the kind of thing that if you can get a \"cool\" or influential senior leader to repeat, the rest will fall in line. reply rr808 19 hours agoparentNode 12? Holy s** that is only 4 years old. Meanwhile in the real world most applications are running on 10+ year old stacks - Java 8, Python 2, Angular.js etc etc reply Xorakios 11 hours agorootparentOh gosh darn you kids. During peak Covid I got calls about COBOL code I wrote in 1981 for Ranger Insurance company because it&#x27;s still used for some reason at Igloo for ice buckets after the companies merged 40 years ago.And don&#x27;t get me started on the Cold Fusion code I wrote in 1997 that probably still is buried somewhere in the Epic software that generates mycharts for HIPAA compliant doctor&#x2F;patient reporting. Last year I was almost arrested for hacking when my real name triggered a security flag that I might be doing something evil when I really just needed a colonoscopy.Think the internet is forever? Try being scowled at for innocent code you wrote as a kid.And stay off my lawn... reply orangepanda 9 hours agorootparentTell me more about your lawn reply deathanatos 13 hours agorootparentprev> the real world [‚Ä¶] Python 2I think what kills me most is when you encounter these codebases in the real world and you&#x27;re like \"wait ‚Ä¶ this entire startup is younger than Python 3 significantly\", and younger than the point where there was good support from the ecosystem‚Ä¶Double bonus points these days if younger than the Python 2 deprecation date‚Ä¶(And yet every security team lists some variant of \"keeping stuff updated\" in their process docs, somehow.) reply thfuran 18 hours agorootparentprevWe just last month upgraded away from a dependency that seems to have been abandoned since about 2008. reply baq 16 hours agorootparentprevOf the three, I‚Äôm somewhat surprised by myself thinking that Python 2 is actually easiest to upgrade! reply wiseowise 18 hours agorootparentprev> Meanwhile in the real world most applications are running on 10+ year old stacks - Java 8, Python 2, Angular.js etc etcSource: trust me, bro reply patmcc 12 hours agorootparentDo you think Oracle is supporting Java 8 until 2030 for funsies?If you work at tech companies led by tech people who think of their companies as tech companies, yah, you&#x27;ll probably be on new(ish) versions.Most software isn&#x27;t written at companies like that; it&#x27;s written at boring business companies (like banks and utilities) that are led by business people who say \"what do you mean to want to update java? Isn&#x27;t it working fine? That would cost us a huge amount of money and be super risky and it&#x27;s not even end-of-life. Go write more Java 8.\" reply rr808 10 hours agorootparentActually we have a mission critical application running Java 6. 8 remains a dream. reply wiseowise 1 hour agorootparentprevOur bank migrated to Java 17 year ago and plans to upgrade to Java 21 are in motion already. reply joshmanders 11 hours agorootparentprev> Do you think Oracle is supporting Java 8 until 2030 for funsies?> \"... it&#x27;s not even end-of-life. Go write more Java 8.\"Sounds like Oracle is enabling this response by not EOL&#x27;ing it. Just like IE didn&#x27;t die completely until Microsoft stopped supporting it. reply Scarblac 13 hours agorootparentprevWe retired our last Angular.js app literally last week, and have a project ongoing to replace the final Python 2 backend. And someone seriously suggested in a meeting last week to keep part of it around... reply AllegedAlec 13 hours agorootparentprevWe&#x27;re running .net 4.5.2Source: trust me, bro reply ikekkdcjkfke 4 hours agorootparentMany babies were thrown out with .net core though reply AllegedAlec 3 hours agorootparentWe develop for systems that at of this time are still stuck on XP&#x2F;Vista. We&#x27;d love to at least be able to progress to 4.8.1 but as of now we can&#x27;t. reply corethree 17 hours agorootparentprevThe sky is blue.Source: trust me, broThe earth is round.Source: trust me, bro reply kstrauser 12 hours agoparentprev> Yet the cybersecurity people are demanding that vulnerabilities be fixed.Talk to them. They may have your back. It was always a happy day when the security audit we were going through at that moment wanted us to attest that all our dependencies were sufficiently updated. That meant we had an external mandate to do the work we&#x27;d been wanting to do for ages anyway.When I was a CISO, I would&#x27;ve been glad to \"order\" one of my colleagues to do that if they asked me to. reply tomrod 18 hours agoparentprev> Yet there are still those in senior leadership who openly question why our team hasn&#x27;t delivered the features they want.Is this _really_ so hard for stakeholders to understand? I&#x27;ve found some limited success by dual-boating -- upgrading as features are created. \"We can&#x27;t build this feature without an upgrade\" carries a lot of weight -- but sometimes you just have to bite the bullet and demand an upgrade.Are stakeholders completely oblivious to the existential threat to their bonus if their company is hacked and driven to bankruptcy in lawsuits? Sure some bigs can avoid this, but not the mids and smalls. reply ozim 12 hours agorootparentSorry but do you have an example of a company that got hacked and driven to bankruptcy by lawsuits?Most of what people see in open are companies that get hacked, share price dips for a month or two if it is software company, if it is non software company share price does not even dip.Okta went down quite a lot since 2022 but still far away from bankrupt and latest \"hack\" did not move it that much really.Where in my opinion (as a tech person) they should be out of business already even after slightest \"hack\".Microsoft Azure latest hack did not really affect their stock... reply jusssi 2 hours agorootparenthttps:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Vastaamo_data_breachNot sure if the actual cause of bankruptcy was lawsuits, action by authorities, or just loss of business caused by the publicity. reply fzeroracer 11 hours agorootparentprevCode Spaces, for one. Larger companies can survive a hack yes, but many smaller companies get buried. Not always due to lawsuits but due to critical infrastructure failures. reply tessierashpool 14 hours agorootparentprevIs this _really_ so hard for stakeholders to understand?for decades now, every generation of programmers has been larger than the previous one.every generation of engineering managers has probably grown in a proportionate way.so, for decades now, every generation of engineering managers has been learning the same basic \"how software differs from manufacturing\" lessons over and over again.(partly because business schools don&#x27;t seem to have learned this lesson yet even once.) reply TeMPOraL 11 hours agorootparentWait, is this the \"amount of programmers in the industry doubles every ${not many} years, therefore software development is dominated by fresh juniors who know fuck all about anything\" argument, but applied to engineering management?Can&#x27;t find the sources for the OG argument about programmers, but it does sound plausible. reply earnesti 19 hours agoparentprevI was working for a company, where the engineering team was doing this kind of refactoring, test development and what not mainteinance coding with large team for about 2 years. Meanwhile not many new business features were being developed and the competition took over, and eventually everyone got fired. I think the leadership was just too lazy to actually push the development of the product, but let the engineers do this pussy work of endless refactoring was big part of the problem. Not only one of course. reply jacquesm 19 hours agorootparentThat&#x27;s exactly it: you need balance. Not so much focus on tech for tech&#x27;s sake that the competition gets to bury you but also not so little that you end up burying yourself. reply aero142 19 hours agorootparentWhen senior leadership is non-technical, they can&#x27;t tell when non-customer visible changes are useful or they just have mediocre engineering. They also can&#x27;t tell when their software stack is such a mess it&#x27;s beyond fixing. reply blackoil 19 hours agorootparentThat&#x27;s the most important job of a CTO. Communicate the cost and implications of any development done by tech team. This could be feature required by product&#x2F;sales&#x2F;CEO or internally by tech team. reply bloopernova 19 hours agorootparentprevThat&#x27;s a great point! We don&#x27;t have commercial competitors because we&#x27;re an inhouse project, but there&#x27;s still a balance to be made, you&#x27;re right. reply IshKebab 19 hours agorootparentprevIMO the best way is just to spend part of your time refactoring and part of your time adding features. It basically solves all the problems. You still deliver features so your company hopefully doesn&#x27;t go under. And you can reduce technical debt without having to awkwardly justify not delivering features.In my experience in the medium term (~2-5 years) it doesn&#x27;t even slow down feature delivery since high technical debt can have an absolutely catastrophic effect on development speed.At the previous company I worked in I&#x27;d say we delivered work at about 50% of the potential rate because the infrastructure they&#x27;d developed was so janky. A big Python&#x2F;YAML&#x2F;Makefile mess with zero static types. CI time was over 2 hours and used hundreds of CPU-hours, even for fixing a typo. reply godelski 12 hours agoparentprev> why our team hasn&#x27;t delivered the features they want.Good work takes time.But I think it is an optimization problem. Rather, that different people are optimizing different things. Your team is optimizing long term profits. The senior leadership who&#x27;s frustrated is optimizing short term profits. I think one way to handle these situations is to clarify which is the more important optimization problem. That makes points like \"opportunity costs vs marginal cost\" discussions. The same way we turn down immediate profits to go to school, because in the long run we make more money (and for other reasons). But our systems often push us to focus on short term goals because these are far easier to measure. reply arcbyte 8 hours agorootparentI agree clarity is needed but i would caution that it is not ethical to let executives that get to make that determination.I don&#x27;t think we as engineers can ethically ever let management optimize our work for the short term. They will ALWAYS want the short term improvement and by even giving them that decision we abdicate our own judgement. That abdication is totally unethical. reply preommr 19 hours agoparentprev> It&#x27;s the kind of thing that if you can get a \"cool\" or influential senior leader to repeat, the rest will fall in line.sympathetic-lol at the desperate copium. reply bloopernova 19 hours agorootparentIf I am honest with myself, it&#x27;s about 40% copium and 60% experience with the directors in question.There&#x27;s 2 \"cool\" directors who were tech people but are also influential in that peer group. You can&#x27;t trust them to represent you fairly, but you can trust that they&#x27;ll listen to your argument.The 40% copium stems from knowing that some of the \"only features have business value\" group are stubborn and think everyone should just work more. reply n0us 18 hours agoparentprevIt‚Äôs okay we can‚Äôt get past Node 8 reply kibibu 8 hours agoparentprevThe \"debt\" metaphor is a good one already though, and one that senior leaders should well understand. reply gardenhedge 11 hours agoparentprevA bottom up approach to tech this way obviously does not work long term. Startups will eat your lunch. reply Tainnor 10 hours agoparentprevNode 12? That&#x27;s cute.We have an application that runs on Java 6. reply MrStonedOne 9 hours agoparentprevThe issue isn&#x27;t tech debt, its tech interest.Looking at the historical maintenance cost of the stacks when choosing them and prioritizing ones with a better history of backwards and forwards compatibility in their ecosystem does far more to prevent tech debt pile up than trying to reach a strict habit of bending over backwards to update dependencies. reply nuancebydefault 14 hours agoprevI often see that code is being refactored and there you have it, refactored code. But usually the problem is not in the code but rather in the design &#x2F; architecture that has driven the code.The design was made without a diligent machine like a computer with compiler checking it. Design reviews (by people) are boring and it hurts when someone pokes holes in one&#x27;s design that has come to them after a few days and or nights of thinking. Also it is often based on unclear requirements.So that&#x27;s how code is born, which is based on suboptimal or even backward designs.As soon as there is need for refactoring, usually only the code itself (a whole of tiny design decisions) is revamped, keeping the initial design alive. reply godelski 12 hours agoparentI definitely think you&#x27;re right about that we can&#x27;t write perfect code upfront. Not to mention that the environment is changing, so left still software will rot.BUT I do think at times we need to slow down. Too much \"move fast and break things\" and sprints have many opportunity costs. Long term optimization is a hard task, so people typically focus on short term gains, which we should be aware can leave a lot of money on the table. So don&#x27;t confuse moving fast with maximizing profits (e.g. we invest in ourselves by going to school turning away immediate profits for higher future profits. But this is harder for businesses to do). A good manager&#x2F;CEO should be one that is learning and focusing on this optimization problem, not the next quarter or ill defined metric.> Design reviews (by people) are boring and it hurts when someone pokes holes in one&#x27;s designI think there&#x27;s a lot to be said here too. A not uncommon gauntlet many scientists have to go through is getting their work trashed on. With many lab groups purposefully holding mock presentations and being overly critical to help train the junior scientist&#x27;s confidence. We should do this in review too, making sure to do it with juniors. It&#x27;s slower, but pays high dividends.But review, in any form, has another problem. The reviewers themselves. Paid or unpaid, they often frame their job as being to find flaws in a work. Instead, their actual job is to make the work the highest quality it can be. If you frame the reviewer reviewee relationship as adversarial then you&#x27;re (in general) degrading the system. Rather this always needs to be framed as cooperative, where you&#x27;re working together to make the work better. This means that it is perfectly okay for a work to be accepted with no comments, and in fact that should be the goal of any QA system. That your job often is closer to a teacher and your success is reducing your work. But if we frame adversarially you&#x27;re going to use metrics like the number of QA comments and rejects to determine how good your reviewers are. In turn, this will be a cobra effect and people will purposefully create low hanging but easy to fix mistakes so that reviewers can point to something (demonstrating they did their job and their need) but reviewees can fix the problems and everyone is happy. But that&#x27;s just Goodhart&#x27;s law in action and isn&#x27;t optimizing the product&#x2F;profit&#x2F;work&#x2F;whatever. This does need to be kept in mind since many metrics are nowhere near aligned with the actual thing being desired to be optimized (and there&#x27;s environmental drifts too). reply RangerScience 17 hours agoprevBeen having this fight a lot lately. The phrase that seems to have struck a chord is ye olde \"first you make the change the easy, then you make the change\".As a further elaboration, I&#x27;m trying out that this gets you:- Better position. Code&#x27;s better, so product is better (more stable, with better features, etc) - Higher velocity. Each time you do this, the overall codebase gets easier to change, because you&#x27;re continually making it easier to change - High acceleration, because you&#x27;re giving your devs a way to stretch their capabilities and grow their skills.Definitely gonna see how \"Refactoring has a price. Not refactoring has a cost. Either way, you pay\" plays with folks!PS -AFAICT, future proofing is bad when you build stuff to aim it, mostly because you probably won&#x27;t end up with the problem you think you will, so you&#x27;re not actually able to build it correctly now.But!You can build in gaps, for where that new stuff can go. You can future proof pretty much only by writing your code today with eye towards making unknown changes later - ye olde \"the only architecture that matters is the one that lets you change\". reply karmakaze 16 hours agoparentYes! this quote captures my reasoning of factoring before rather than after. The other problem with factoring after is that it&#x27;s doing so without knowing the future new motivations. I would say that the factoring after is to suit making a working prototype first, then when a factor is identified aligning the design&#x2F;source along those lines.Note I say factor rather than refactor since in most cases the existing code wasn&#x27;t factored in the first place. It&#x27;s also a reminder that we should know what that factor is before making changes. In some cases there is a new reason that changes which to factor but is less common IMO. reply pacificmaelstrm 17 hours agoprevI&#x27;ve been at a company where we had two leading developers.One refactored everything endlessly to a fault, wrote tests for everything and emulators for every external piece of hardware.The other rarely&#x2F;never refactored and was focused on ends-justify-means functionality.They hated each other and progress was very slow because they could not work together at all.My two cents is that all products of engineering are temporary. Software (and hardware) are not eternal. What you write today you will eventually become obsolete.Your entire app will eventually be re-written. If not by you then by your competitor who takes you out.There is no such thing as perfect or optimal code because what is great for now won&#x27;t be great for tomorrow.With that in mind, good refactoring is like good math. Taking a long and complex system of code and replacing it with something functionally better and far simpler and easy to understand.Bad refactoring is just moving things around for \"understandability\" or in a lot of cases one developer rewriting something in their own idiosyncratic style because they don&#x27;t understand the style of the programmer before them.And the final point is don&#x27;t be afraid to rewrite the entire system if you have that ability.Since the development process is about defining functionality as much as enacting it, once you have a finished application, you can re-create it much faster and simpler than it was originally written.The Apollo missions got to the moon and back with 145000 lines of code you don&#x27;t need 20 million + for your web app. reply clumsysmurf 14 hours agoparent> They hated each other and progress was very slow because they could not work together at all.Been there. And often gets worse when the \"ends-justify-means\" camp (1) collects velocity points (that&#x27;s the most important thing, they decided to gain the system that way) and people who care about velocity (typically management) are happy but... (2) leave a path of destruction in their wake that slows everyone else down, decreasing the productivity of everyoneWith refactoring, consider 2 things (1) How bad &#x2F; ripe for simplification is the code (2) How much is it changingIf its bad stuff but nobody needs to deal with it, I say leave it unless it has dependencies (those sneak up on you too and prevent perhaps a library upgrade that would benefit the rest of the app) reply galoisscobi 10 hours agorootparentThere‚Äôs a fun phrase in ‚ÄúA philosophy of software design‚Äù describing such engineers, they‚Äôre called Tactical Torpedos. reply SoftTalker 17 hours agoparentprev> What you write today you will eventually become obsolete.True, but \"eventually\" can vary wildly.Web app front-end written in framework-du-jour? Yeah it will probably be gone in a few years.OS kernel functions? May still be running unchanged 30 years later. reply goalieca 14 hours agoparentprevThere two kinds of phases a product lives under: growing and maintenance.Maintenance mode products aren‚Äôt growing revenue. They might get a few new features but the main goal is to not mess with the cash cow unless you need to patch or make a special customer a new feature. It will fade into irrelevance with time as something new replaces it.Projects that are growing need to scale and adapt to new environments and use cases. If you treat these with disrespect then your business will suffer and you‚Äôre less likely to make that new feature or big pivot needed to become a cash cow. reply tstrimple 13 hours agoparentprev> And the final point is don&#x27;t be afraid to rewrite the entire system if you have that ability.No... you should absolutely be afraid of that.https:&#x2F;&#x2F;www.joelonsoftware.com&#x2F;2000&#x2F;04&#x2F;06&#x2F;things-you-should-...Rewriting your entire system is almost always the wrong thing to do and has been the end of plenty of products and companies. I think Joel is spot on here. It&#x27;s much more difficult to read code than to write code, so new developers always think existing code bases are junk and need to be scrapped while ignoring the value that platform has delivered over years if not decades. Developers aren&#x27;t paid to be craftsmen. They are paid to deliver a product that provides business value.Too often developers lose sight of the business value and want to write code for the sake of the craft. You&#x27;re working for Ikea and complain that you can&#x27;t use your fancy woodworking tools and techniques and have to stick to dowel based assembly instead of that fancy joinery you&#x27;d rather be working with. Those fancy joints are stronger and more stable after all! And what the fuck is up with all the particle board?! All real woodworkers know the stuff is junk and will fall apart as soon as it gets wet. We should be working with mahogany and walnut with cocobolo inlays!There are places where your fancy joinery skills are appreciated and none of those expenses are spared. But most developers are working at an Ikea or equivalent where the predictability of cost and process is well understood and a reliable product can be delivered at a reasonable price. If you want to be a craftsman, you need to work at the places who are looking for those skills and not just the places who want flat pack furniture quickly and cheaply to keep delivering value to their customers. reply crabbone 10 hours agoparentprevYou are forgetting to measure. You are saying something to the effect of \"if I fry potatoes for too long, I get coal, therefore it&#x27;s best to eat potatoes raw\".How much to invest in design vs how fast can things be done w&#x2F;o such an investment is a difficult problem. Very similar in nature to https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Multi-armed_bandit .Sometimes the program you are writing is so worthless, it&#x27;ll be thrown out tomorrow. Sometimes it may outlive you. The later is rare, but not impossible. Eg. my former boss has some of its code flying on an satellite, circling the Earth some 30 odd years.My personal oldest code that is still alive (that I&#x27;m aware of) is some minor stuff in cl-gd library (Common Lisp bindings to GD library). It&#x27;s about 15 years old now.For my day job, I work for a company whose codebase was started some time in late 90&#x27;s. While things have changed over time, some, and especially design decisions are still there. A lot of them are painfully wrong, and have been a burden on this company for the last 20+ years.Oh, and should I mention the 32-bit sized IPv4 addresses?----How much should one invest into design? how often should cornerstone ideas be revisited? -- Definitely not nothing and definitely not never. But how to know when it&#x27;s the right amount? -- I don&#x27;t believe anyone has a strategy for finding out. Good (or lucky) project managers will have a good guess about the timing. reply jacquesm 19 hours agoprevRefactoring has a fixed cost, not refactoring has an ever increasing cost. Clearly either way you will pay but if you refactor you will end up paying much less in total. The bigger problem is not the cost per-se but the resource allocation issue associated with refactoring: you can go bankrupt with very clean refactored code that gives your competition time enough to bury you with a crappy codebase and the features that the customer needs.All of these are balancing acts. It&#x27;s never &#x27;refactor or not&#x27;, it&#x27;s how much to refactor and what to let go? These are strategic issues and they go hand-in-hand with business goals and knowledge about the longer term roadmap. For instance it would be pointless to refactor a codebase that will end up being supplanted by another product or for which a strategic acquisition is planned of a company with a technically superior product. reply sokoloff 18 hours agoparent> Refactoring has a fixed cost, not refactoring has an ever increasing cost.That means that refactoring has an increasing cost as well (with increased frequency, the total money spent on refactoring increases). reply seadan83 16 hours agorootparentIf time is cost, then yes, everything has an increasing total cost.The question is more about complexity and interactions. Does a specific subsystem interact with N modules, or with 1. If N+1 module interacts with N, then the cost of maintaining that system will explode very quickly. OTOH, if the system is revisited after that test goes green to increase modularity, reduce dependencies, then complexity is now potentially growing sub-linearly to the number of modules rather than O(n!) reply jacquesm 17 hours agorootparentprevNo it doesn&#x27;t because you only write so much code. reply sokoloff 17 hours agorootparentI think you‚Äôre then arguing that refactoring doesn‚Äôt have a fixed cost per occurrence, but rather that the more code you write between refactorings, the cost of the refactoring increases (which I agree with). reply lionkor 19 hours agoparentprevIt&#x27;s also sometimes \"when to refactor\", because eventually you either have to refactor or rewrite (or retire). reply jacquesm 19 hours agorootparentYes, correct, sometimes you will have to do it anyway but simply not now because there are more pressing needs. This more often than not leads to runaway technical debt though, and sooner or later the problem becomes insurmountable. I&#x27;ve found over the years that refactoring is best done as a continuous background task with a certain percentage of capacity allocated to dealing with it and addressing the parts that you most want to avoid first. reply blackoil 18 hours agoprevTech debt is like all other debts, it need to be serviced. You keep accumulating it and it becomes toxic. Never take any debt and you are leaving easy money on table and may loose to competition because you have slower delivery. reply reactordev 16 hours agoparentI agree with this. I‚Äôd go further and say it‚Äôs a visibility issue. Non-tech folks don‚Äôt know that the stuff is out of date, vulnerable, no longer supported. They don‚Äôt know how much debt they have borrowed. So having leadership know the costs, the cogs, the debts, and the projections is vital. reply somewhereoutth 19 hours agoprevThe most foolproof way to avoid regressions is to leave the code alone.The best testing is battle testing - if it is working well after plenty of contact with the customer then leave the code alone.Bitter experience has taught me this.---I&#x27;m going to expand on my comment a bit - we need to be clearer on what (production) code actually is. We believe it is the definition of the runtime behavior of a given system (and of course it is exactly that).However, for any reasonably complex system, a complete understanding of the definition&#x2F;behavior is beyond any single person. Thus code is in fact a recording of the social and technical process of producing the system, and as such embeds decisions and solutions that may not be evident to anyone just reading the code.When viewed in this light, it is easy to see how dangerous refactoring can be. reply iopq 18 hours agoparentThat just leaves the code in a state where it&#x27;s not clear what&#x27;s wrong with it, because it works most of the time. There are just two types of code: obviously correct, or not obviously wrong.If you write some code and you don&#x27;t know how it works, it&#x27;s most likely not correct in some edge cases.Code that is written to be so simple it&#x27;s obviously doing the right thing is better than battle tested mess of if statements.I&#x27;ve given interviews of fizzbuzz and even though developers usually recognize they need a separate case for 15, they don&#x27;t always do the else ifs right (maybe test for the 15 case last when you&#x27;ve already hit the divisible by 3 case first, or similar silly mistakes). In that case, doing the string accumulation refactor makes it clear that the code has no mistakes because it for sure adds a Fizz first and a Buzz second in the 15 case. It&#x27;s also a good refactor for extending to Bazz (divisible by 7) as well, so it&#x27;s something you will want to do when you want to add that feature reply tuna74 15 hours agorootparent\"If you write some code and you don&#x27;t know how it works, it&#x27;s most likely not correct in some edge cases.Code that is written to be so simple it&#x27;s obviously doing the right thing is better than battle tested mess of if statements.\"Only if it actually do the right thing in ALL cases. In a lot of refactors I have seen (and done myself) that has not been true. reply iopq 13 hours agorootparentYes, but then the refactor value is dubious. To write simple code you actually need to understand the difficulty of the problem. Why is certain code easier to talk about than other type of code? Why is it better written this way and not another way?If you don&#x27;t understand the problem enough you may hack at it in another wrong way. reply MichaelZuo 15 hours agorootparentprevIf that&#x27;s the typical performance on fizzbuzz, I can&#x27;t even imagine what the performance would be like on more advanced questions. reply iopq 13 hours agorootparentHumans are very bad on whiteboard interviews. I like giving people a laptop better now reply MichaelZuo 13 hours agorootparentThe point is to make folks answer without the help of external tools, such as an IDE, on the basis that they will be able to reliable accomplish more advanced tasks with such tools in practice. Whether or not this theory is mostly reliable or barely, or not at all, depends on the viewpoint.If a more advanced question is posed, then the assumption is that the requirement is for the prospective employee to accomplish even more advanced tasks in practice. reply iopq 12 hours agorootparentWould you rather get someone who can do fizzbuzz correctly on a board the first time in 7 minutes or someone who can write it in 1 minute on a laptop incorrectly, test it, and fix it in 30 seconds?I see no value in the \"gotcha\" of you made a logic mistake since we all make them and there&#x27;s a reason why we test code reply MichaelZuo 11 hours agorootparentIf they make 1 &#x27;logic mistake&#x27; on fizzbuzz then it&#x27;s likely they&#x27;ll make 10x, or more, &#x27;logic mistakes&#x27; on their actual work. Even with a laptop and all the tooling that makes them 10x better, because the real work will be at least 100x more complex. reply iopq 2 hours agorootparentI made a mistake in fizzbuzz the first time I did it on a whiteboard because I wrote it in \"pseudocode\"I wrote if (n % 15) print \"fizzbuzz\" ...I got away with it because my interviewer assumed it was some kind of shell which means 0 is trueI should have written n % 15 == 0 to be absolutely clear what I&#x27;m doingnote that in Rust this won&#x27;t even compile: 3if n % 15 {^^^^^^ expected `bool`, found integerBut I don&#x27;t have the type checker to find mistakes for me when writing on a whiteboard which is not realistic. I can&#x27;t commit this code because CI wouldn&#x27;t merge a build failure anyway replydataflow 18 hours agoparentprev> The most foolproof way to avoid regressions is to leave the code alone.The most foolproof way to avoid car accidents is to avoid ever being around a car.Surely that&#x27;s not the only objective, right? reply iterateoften 18 hours agorootparentThat‚Äôs the opposite analogy. The OP is saying drive the car as much as possible and as long as it doesn‚Äôt have problems don‚Äôt mess around under the hood.Some people tinker with adding mods and things to their cars which increase the probability of the problem. reply koolba 17 hours agorootparentUsing the engine analogy, a plane is a better example than a car. Especially as the mental picture of not surviving a crash is more vivid.Would you rather fly on a plane that has been used daily for the same flight for the past 10 years? Or fly on a plane that‚Äôs either never flown, just got out of the shop, or hasn‚Äôt been flown in years? reply marcosdumay 17 hours agorootparentOh, planes are a great analogy. If you insist on flying them without changing anything, you can even face some criminal charges.But you also can&#x27;t go and redesign the entire thing. reply orwin 17 hours agorootparentprevIf _nothing_ has ever been changed on the plane, I&#x27;ll take the new one, thank you. I don&#x27;t know how it lasted that long, but I won&#x27;t take any chances. reply mathgeek 16 hours agorootparentRefactoring is analogous to changing the composition of the plane, rather than repairing newly broken parts with identical ones. reply Gibbon1 15 hours agorootparentprevI&#x27;ll fly on the 20 year old 737-700 and you can fly on the spanking new 737-MAX. reply fatherzine 13 hours agorootparentprevPlease, at a minimum, change the oil according to manufacturer specs. Or eventually blow up your engine. reply dataflow 17 hours agorootparentprev> The OP is saying drive the car as much as possible and as long as it doesn‚Äôt have problems don‚Äôt mess around under the hood.If that&#x27;s what you and the OP are saying, then you&#x27;re conflating the codebase with the end product (program). Yes, the end user (at least in an ideal world) shouldn&#x27;t need to modify the end product. That wasn&#x27;t ever disputed, though.The point (if I have to use your car-modding analogy instead) is the manufacturer&#x27;s engineering team very much does need to be able to evolve the design (read: source code) for future versions of the car. reply seadan83 16 hours agoparentprevTesting is a form of battle testing. Code that has been thoroughly tested is akin to a running system for a year. On the other hand, modify that legacy system without test, that \"battle test counter\" resets to zero and you get to go through the process again.Classically speaking, refactoring is only for code that has thorough coverage. Refactoring code that is not under test might be better worded as rewriting.Obviously, rewriting critical and complex code that has evolved over time with a variety of fixes and random features bolted on with random conditionals - obviously that is risky reply somewhereoutth 14 hours agorootparent> Code that has been thoroughly tested is akin to a running system for a yearExcept that isn&#x27;t even remotely true. Perhaps if you system has a purely machine interface, with a limited &#x27;dictionary&#x27; of interactions, then a test suite could replicate deployment to a very large degree, but otherwise at best it can give only an indication of how use-worthy the system is.Take a (reasonably complex, with human interface) production system, implement as much testing as you like, and I can guarantee any significant refactoring will introduce a regression, and quite possibly a serious one.The problem with testing is that you need to know what to test. So unless you have some method of capturing every possible way a system is actually used, you will always run into the &#x27;unknown unknowns&#x27; that you didn&#x27;t realize even needed testing. reply gt565k 18 hours agoparentprevSure, but a lot of poorly written spaghetti code that works is sometimes impossible to extend or add new features to without a major refactor or complete rewrite.Currently dealing with some 13 year old objective C code for a mobile app that‚Äôs gonna require a major rewrite cause it looks like it was written by someone who should have never been writing code in the first place reply Gibbon1 14 hours agorootparentThere is a mathematical technique called simulated annealing.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Simulated_annealingThe idea is code goes through development cycles. You have the most leeway to make changes in the first few development cycles. But less and less as time goes on. And the beginning of a development cycle you can make larger changes than at the end. Between cycles the code is frozen and you never touch it unless you can prove a change is needed. And development cycles are triggered by the need to for gross functional changes or interface changes. reply somewhereoutth 14 hours agorootparentGood analogy. Where I am we try to follow this process for each release cycle - so big changes early, then becoming increasingly conservative as to what gets altered&#x2F;added as the release date approaches. reply RangerScience 17 hours agoparentprevI mean, yes and no, and aside from that - the article isn&#x27;t even about regressions.> embeds decisions and solutions that may not be evident to anyone just reading the code.Strongly agree! Good code communicates this to other programmers (including yourself in 3-6 months). If the code doesn&#x27;t do a good job of this, that alone can not only be worth a refactor, and when you refactor while \"what it took to understand what this does\" is fresh in your mind, you can write the new version that much more effective at communicating those learnings. reply somewhereoutth 14 hours agorootparentHowever that requires your learnings to be complete (in respect of the code you are rewriting). My point is that for any reasonably complex code base of some age, that may not actually be possible even with the best of intentions. There will be a corner case you&#x27;ll miss, or a necessary optimization you&#x27;ll leave out. Of course this is over a spectrum, so refactoring code that only you wrote quite recently is probably ok and indeed a good thing! reply sorokod 18 hours agoparentprevLeaving the code alone, even one that works reasonably well, is not an option.Lehman&#x27;s laws of software evolution say so.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Lehman%27s_laws_of_software_... reply marcosdumay 18 hours agoparentprevYou can only ever leave your code alone if it&#x27;s modular and independent enough so that new changes on your system do not depend on changing it. reply joshspankit 17 hours agoparentprevIt‚Äôs fantasy, but if each battle was distilled in to a test and added to the test suite then it would be possible to touch production code without breaking business functionality. reply somewhereoutth 14 hours agorootparentIndeed it is a fantasy, however a fantasy that is all too easily and often indulged - which is how regressions end up back with the customer. reply mumblemumble 15 hours agoparentprevI don&#x27;t think it&#x27;s as simple as that.I agree that one should not take something like this lightly. Far, far too many \"refactorings\" really just boil down to hasty rage-coding at some predecessor who had differing stylistic opinions, or keeners who&#x27;ve never heard of Chesterton&#x27;s Fence getting frustrated with some annoying design element they don&#x27;t understand.But also, sometimes the code really is tangled and difficult to maintain, such that it&#x27;s difficult to make any modifications without introducing regression defects.Concrete example: I recently did a refactoring of a piece of code my team maintained that was so extensive that, by the final iteration, it was arguably a rewrite. The old code was sprawling and complex, could basically only be comprehended by the staff engineers, and tended to be ground zero for defects. The new code is much smaller and simpler, and easier to modify, and every single member of the team is comfortable reading it.- but -I only did this after months of careful observation, quietly building a case for the change and a plan for how to do it. This process included interviewing everyone I thought might know anything about this module, including folks who had moved on to different teams, to make sure I understood as much as possible of what was there and why it was built that way. I discussed my plans with them and sought (and, ultimately always got) their blessing. And then I spent more time bolstering the test coverage, including hunting for edge and corner cases that the existing test suite had missed, when necessary, adding additional tests that did not depend on this module&#x27;s implementation details so that they would not be broken by the refactor. And then, when I finally did do the actual refactor, I didn&#x27;t do it all at once. I broke the work into many small, carefully controlled iterations. reply somewhereoutth 14 hours agorootparentDefinitely agree! I suppose this is where the &#x27;art&#x27; of software engineering comes into play, to know what can and should be refactored, and how to go about it in a safe way. reply zeroonetwothree 18 hours agoparentprevRefactoring for no reason is not useful. But what happens in practice is you have a new requirement and you realize your hacked together ‚Äúbattle tested‚Äù system can‚Äôt possibly support it without breaking everything. So then you need to refactor first.And if you are actively developing then refactoring along the way is generally better than if you wait until the very end. reply AnimalMuppet 18 hours agoparentprevTrue of fixing bugs, too. The odds of introducing a bug when fixing a bug is between 20 and 50%. (Per one study from decades ago. If you can produce better numbers, please do.)But the problem is, even if code is battle-tested, someone always wants it to do something more. To get it to do more means changing it. And when you&#x27;ve changed it, they&#x27;ll want another change. And then another.So you either don&#x27;t make changes (even bug fixes!) or you do. If you do, you either refactor the code as you change it, or you don&#x27;t. If you don&#x27;t, the code becomes more and more brittle, that is, harder and harder to change without breaking something.So I think you are drawing wrong conclusions from true observations. You are correct that you can break code when you change it, even by refactoring it. Keep it in a state where the probability of that is minimized - where it&#x27;s clear what the code is doing, and how, and why. To do that, you need to keep refactoring. reply rizky05 18 hours agoparentprevit works when the requirements never change. If somebody demands change, you screwed. reply amelius 15 hours agorootparentOr if you suddenly run into a bug that causes the entire thing to collapse for unknown reasons. reply jillesvangurp 13 hours agoprevThe cost and price scale non linearly to the amount of change. So, waiting to refactor has an exponentially higher cost the longer you wait whereas doing lots of small refactoring as you go has a relatively low cost. To the point where it&#x27;s just negligent not to do it. You can spend five or ten minutes cleaning up a little or you skip it. Not doing it is sloppy. Just apply the boy scout rule.People have the wrong reflexes where being overly conservative and fearful to change a working system eventually leads to a system that becomes so hard to fix that people stop bothering to even try and just replace or retire it in the end. Which is very expensive and destructive of course. Usually such decisions become a reality when people leave or change teams and it turns out there&#x27;s nobody around that even has a clue where to begin fixing the old thing. Getting rid of the thing then becomes the logical choice.Calling it technical debt is a way of talking yourself into believing you can deal with it later. Quite often that doesn&#x27;t work out that way. Another fallacy is believing the debt is something with a fixed&#x2F;known price that you can pay off at the time of your choosing. You can&#x27;t; it&#x27;s literally risk that accumulates that you haven&#x27;t properly analyzed and estimated yet. Some of it might be fixable, some of it might not be. You won&#x27;t know for sure until you sit down and do the work. Deferring the process of finding out is what gets teams into trouble. The less you know what you can still fix the more likely it is that the cost has become stupendously high.Half the work is just knowing what needs doing and how it needs to be done. That doesn&#x27;t mean you do the work right away. But spending a lot of time chin stroking over stuff you could do without actually doing stuff (aka. analysis paralysis) is also not productive. Finding a balance between knowing what needs doing and then just balancing new stuff vs. making sure that the cost of that new stuff isn&#x27;t overly burdened by broken old stuff is the name of the game. Having a lot of technical debt usually translates into work that ought to be quick, simple, and easy being anything but those things. So, the technical debt metaphor means that you burden the cost of future essential work with that extra burden. Until you fix it, it only gets worse and worse. You pay the interest on every change until you fix the problem. And you won&#x27;t know how expensive that is until you do it.That&#x27;s why you should refactor a little bit all the time. Low cost, it lowers your risk, and it keeps you aware of all the pain points and weak points in your system. reply ttoinou 18 hours agoprevEither way you pay but the goal is to generate more revenue from your software than what you pay But one day all the cruft calcifies, and the numerous incompatible changes and features grind our progress to a halt. It is suddenly impossible to add new features without breaking something elseThat&#x27;s also the case if you keep refactoring every day, there&#x27;s no reason to believe the changes you&#x27;re doing right now are that good.Refactoring and paying constant attention supposed code perfection has a huge unknown price that you have to pay very soon, whereas not refactoring has a future cost. You should use the money saved not refactoring right now to make something better later once the tech debt is too high but you know you have a good working software.In any case, this kind of article needs tangible examples to illustrate... reply candiddevmike 19 hours agoprevAny sort of refactor needs to have agreed upon goals&#x2F;scope and folks need to stay within that scope. This is a really difficult skill IMO. Even when I start out with the best intentions of refactoring libraryA, suddenly I&#x27;m in a bad place refactoring libraryD because libraryA was refactored to use a new enum approach and damnit I want it to be consistent everywhere. reply karmakaze 16 hours agoprevI often find myself (re)factoring when beginning work if the codebase is not in the shape (design rather than quality) that it needs to be in for the particular new features to fit well within it. The reason I wouldn&#x27;t factor after implementation is that it should have followed a design that already identified the factors that are being separated.Whenever there&#x27;s continuous refactoring, the factors aren&#x27;t so much known and decided at a high level, it&#x27;s typically more an exercise of deduplication. If you can&#x27;t name the thing (or why) you&#x27;re factoring it&#x27;s blind deduplication, which still has some value but is much lower. If someone says they want to refactor, I&#x27;ll ask what&#x27;s the factor and why. A system separated along well chosen seams allows things to naturally and easily fit in its design. reply jandrese 19 hours agoprevAttempting to future proof your code so you never have to refactor: the most expensive choice of them all. reply blowski 19 hours agoparentYes, for most business applications, in the long run it‚Äôs cheapest to optimise for refactoring than believing you can avoid it. reply ttoinou 18 hours agorootparentThere isn&#x27;t any long run for most businesses. Especially if your developers are that blinded by theoretical perfection as the author of this article reply lionkor 19 hours agoprevOne way I&#x27;ve seen this enforced is this: Have your \"best\" devs, the people who put out 2x the amount of solid, testable, well documented code as anyone else, you know those that carry the sprint, spend half their time reviewing. Your overall sprint velocity may go down a bit, but your code quality will go up.There&#x27;s a magic to slowing things down, too - require reviews on every change, require tests to run and pass, run UI tests on each PR. Let people discuss good and bad code they see. Turn every little thing people mention, (like \"man, this view is difficult to reason about\", \"the entire xyz class is a mess\", \"does anyone know why we do X here?\") into a Refactor issue, discuss it, plan it, do it. You can do this during a big feature sprint, you dont need to wait until your product is done (then its too late).Refactor, let people say \"yes this issue isnt high prio, but its important to me\". Embrace when people want to make code better. reply mjr00 19 hours agoparent> One way I&#x27;ve seen this enforced is this: Have your \"best\" devs, the people who put out 2x the amount of solid, testable, well documented code as anyone else, you know those that carry the sprint, spend half their time reviewing. Your overall sprint velocity may go down a bit, but your code quality will go up.Problem with this is your best devs will soon become your best former devs.Becoming a \"mandatory reviewer\" sucks. Especially at scale. You&#x27;re a bottleneck, you get pressured by other devs and occasionally management to let code that you don&#x27;t like through, and if things go wrong you get assigned at least partial blame for not catching it during the review process.I&#x27;m speaking from a background of seeing this implemented at a major AWS service with designated \"component owners\", who were 1-3 developers with a final (and mandatory) review for all code merged into subcomponents. This was for a service with 400+ developers working on it.> There&#x27;s a magic to slowing things down, too - require reviews on every change, require tests to run and pass, run UI tests on each PR. Let people discuss good and bad code they see.Yeah this is the way to do it. Automate everything. Unfortunately there&#x27;s no real \"good code\" vs \"bad code\" easy metric, but you should at least have automated testing, linting, compiling, etc to ensure people aren&#x27;t merging total garbage to shared branches. reply bluGill 18 hours agorootparentThere are many easy to automate good code vs bad code things, and you should do that. Turn on all compiler warnings and refuse code with even one for example. Problem is this type of thing isn&#x27;t sufficient. (You should write your own checks for common problems in your project that could not apply to generic compilers as well ) reply rr808 19 hours agoparentprev> you know those that carry the sprint, spend half their time reviewingSounds like a good idea, but no one wants to do this. People would rather quit. reply nightpool 19 hours agorootparentYour experiences are not universal. Just because you hate doing code review doesn&#x27;t mean everybody does reply wnolens 18 hours agorootparentThey&#x27;ll burn out because that ask for 50% code review time is often 100 + 50% time.In the playoffs you don&#x27;t tell your star player to stop shooting and pass more. In my experience, software projects are always lead to accomplish a goal that is out of reach unless the stars bust their ass. reply baq 16 hours agorootparentprevThe problem is that doing a proper code review is actually hard and thankless work. It‚Äôs one thing to skim a medium sized change and another to actually go through every line and every condition, consider second order effects, check what‚Äôs missing in the change, etc.I‚Äôve seen estimates that it takes 1h to properly review 400 LOC and the reviewer then needs a long break after that time regardless of whether they‚Äôre done reviewing or not. reply briHass 13 hours agorootparentI&#x27;ve argued to management that it&#x27;s no less than 50% of the time it took to develop. If you&#x27;re actually reviewing functionality, you need to understand the original ticket, and at least loosely architect it in your mind.Anything less than that is basically just checking code style or egregious errors. reply wiseowise 18 hours agorootparentprevI hate it, two other top performers in our team hate it too. They might not be universal, but there‚Äôs sure lot of them. reply yowlingcat 11 hours agorootparentHave you considered whether you and your two colleagues are actually the top performers you think you are? reply wiseowise 1 hour agorootparentYes. And yes. reply docflabby 18 hours agorootparentprevThe reviewing isn&#x27;t problem it&#x27;s mostly because you have to become that asshole that everyone hates - developers who produce bad code also are the worst at taking feedback... reply lionkor 12 hours agorootparentprevnot sure I&#x27;d want those people on my team anyways in that case, as the shared vision should be good code, regardless of who wrote it reply TeMPOraL 11 hours agorootparentIt&#x27;s not about who owns the code. It&#x27;s about writing code. Your star performers, those \"people who put out 2x the amount of solid, testable, well documented code as anyone else\", are likely this effective because they like the coding part. Unless they also love reviewing someone else&#x27;s code, making them spend 50% time on reviews means taking away most of the nice parts of their work. Assuming they&#x27;re not robots, expect motivation to suffer - and with it, the quality their future output. reply wredue 19 hours agoparentprevThe problem here is that senior devs are not immune to being hypester developers, and if you let a small group have the final say, there‚Äôs a non-zero chance you‚Äôre going to end up listening to whatever‚Äôs the current hype on medium, and your code will end up even worse. reply nathan_compton 18 hours agoprevDoes this essay actually say anything non-obvious? Like everyone knows there are relative costs. That isn&#x27;t the hard problem. The hard problem is evaluating those costs correctly. reply zeroonetwothree 18 hours agoparentPerhaps it‚Äôs useful for very junior folks? reply myspeed 18 hours agoprevRefactoring is OCD, you can do it infinite times. Many times a working code is rearranged to match some one&#x27;s preceptive of business logic. reply iopq 18 hours agoparentEventually you reach the limit of expression of the language you&#x27;re working with.My fizzbuzz:https:&#x2F;&#x2F;bitbucket.org&#x2F;iopq&#x2F;fizzbuzz-in-rust&#x2F;src&#x2F;master&#x2F;src&#x2F;l...there&#x27;s just no more refactors I can do because of the limitations of Rust reply TeMPOraL 10 hours agorootparentYou eventually not only reach the limits of your language, but approach the limit of having plaintext code as your single source of truth and the thing you directly work on. At this point, it&#x27;s like GP says - you can keep refactoring stuff infinitely. You won&#x27;t be improving anything - you&#x27;ll only be moving along the surface of limits of plaintext code, only ever making some aspects of code more readable at the expense of others. reply zeroonetwothree 18 hours agoparentprevOCD is a really difficult mental health condition that millions suffer from. It destroys lives and relationships. Please don‚Äôt minimise it by likening it to arguably fun parts of a job. reply corethree 17 hours agorootparentI have ocd and he&#x27;s right. Don&#x27;t pretend you&#x27;re a spokesperson for people who suffer from my condition.The condition in reality actually lives on a gradient and it very much influences everyone to varying degrees. It&#x27;s only labeled ocd once it crosses a threshold. It no doubt exists in programming. Much of refactoring is done for the same mechanisms that cause ocd and zero practical reasoning.I have ocd but I&#x27;m able to differentiate my compulsions from rational reasoning. Thus I can see how irrational my actions are. reply majkinetor 18 hours agoprevEither way, somebody pays, not me necessarily. Clearly different thing then claimed. Current project owner pays for sure. But who will that be when collection happens, and what team will be directly involved, that&#x27;s entirely another thing. My bad habits may cost me nothing, there is also probability of gain. reply AnimalMuppet 17 hours agoparentIf I\"m the person who winds up inheriting the code you produce, I&#x27;m probably going to resent you.Do what you can, as if you were going to be the next person. For one reason, because anything else is irresponsible laziness. For another, you may in fact be the next person - you may not leave in time for someone else to inherit the mess. So do what you can now to minimize it. reply majkinetor 17 hours agorootparentWho cares.- I don&#x27;t even know you- &#x27;Irresponsible laziness&#x27; could also be replaced with &#x27;ducks in space&#x27;- I can leave any time. This is not slavery.So its a method of work - maximize the input, minimize the loss.You would be surprised. I actually know people like that, more then 1. reply SonicSoul 18 hours agoprevoh wow.. refactoring is an art form. just because your code \"works\" doesn&#x27;t mean you have to leave it alone. your working code will likely be copied into 5 other projects and will not scale well.ideally you&#x27;re investing into creating shareable code libraries and into infrastructure that makes the process of maintaining&#x2F;referencing shared components easy enough for engineers to want to dedicate their precious sprint time to it. reply hansvm 6 hours agoprevExpanding slightly on the article, there are at least a few places where incremental refactoring is (often) beneficial:(1) Before you work on a feature, if the existing code has some tradeoffs or outright deficiencies which make the new feature hard to add, start by fixing the work site, and then add the feature. The benefits from this snowball quickly on code of any age or complexity, and since the assumption is that we&#x27;re only doing \"extra\" work when a feature was going to be complicated and that the \"extra\" work makes the feature simple, it doesn&#x27;t actually add a meaningful amount of time to your tickets (i.e., throughput slows down a bit at the beginning but not enough for management to notice or care, and it ramps up over time).(2) A fair number of developers seem to \"think\" in code -- incrementally building out the solution in code to learn how to solve it and build appropriate mental models. If this describes you, throw away the first draft and start over. V2 takes a fraction of the time to write as V1 since you now know what you&#x27;re doing, and the second draft will be substantially better. If this gets pushback, never describe V1 as \"working\" or a \"prototype\" or anything vaguely implying that enough duct tape and unpaid overtime would allow it to hobble along in production. I find that \"brainstorming\" often elicits the correct response.(3) The article calls out that after building a feature \"correctly\" it might not fit perfectly into the existing code. I&#x27;m not necessarily convinced that the right time to refactor this is immediately (since it&#x27;s so hard to predict which abstractions you might need (ignoring rules of thumb like \"separating payments and entitlements\", which represent hard-won tribal knowledge)), but if you&#x27;re able to come up with a measurably better solution then the best time to implement it is definitely while everything is fresh in your mind and before you bug your coworkers for a review.(4) The article _also_ calls out that cruft has probably accumulated over time, which might need to be handled incrementally or with a rewrite. A lot of that is often addressable via (1), but partial rewrites _can_ be cheaper than you think. If you happen to notice a particularly thorny problem afflicting an organization, can confine it to under a few thousand lines of code, and have a decent idea for how you will make it better, spending a couple days to rip it out is time well spent. I&#x27;ve worked on a lot bigger projects before than those sorts of quick rewrites, but those small changes have had a comparatively outsized ROI. They&#x27;re fast projects that nobody else had any good ideas for fixing (or they would have done so rather than bitch about how much of a hassle the thing is) that can immediately save 1+ developer days per calendar day or 10 GPU years per calendar day or whatever. Your work pays for itself quickly, and since it only took a small fraction of your time you still have nearly identical throughput on whatever your main job is.(4a) No longer job-related, but a similar idea manifests itself in ordinary household functions as well. If you have a low-risk, low-cost method for achieving 10%+ annual returns, that&#x27;s often worth considering. Consider buying flour or soap or something in greater quantities than normal. It&#x27;s not uncommon to pay 2x the price for 5x the quantity. If you go through the item every 2.4 months (illustrative, to make the rest of the math easy), consider the relative effects on your wallet from buying in bulk. You spent 2p (for some unit price p) immediately (and couldn&#x27;t invest it or do anything else; it&#x27;s gone), but at the end of the year you have 3p more cash than you would have had you bought smaller units. I.e., your 2p investment grew 50% to 3p. That&#x27;s an amazing annual return that you couldn&#x27;t easily replicate elsewhere, even after accounting for tax-deferred benefits or other such effects. Similarly, paying some portion off a 20% interest loan (try to never get those...) has, ignoring taxes and bankruptcy and other such details, exactly the same effect on your net worth as getting 20% returns in some investment account. Moreover, it&#x27;s nearly \"risk-free\", whereas such a high yield investment account would not be.(4b) Individual circumstances vary. Try to interpret (4a) in a charitable light; if you don&#x27;t bake much, don&#x27;t buy 10yrs worth of flour. If you have to buy extra square footage to plan to buy in bulk that&#x27;s maybe a problem. If you require a ton of time to avoid scams or quasi-scams (like stores pricing bulk items with a higher per-unit cost than non-bulk items) then that&#x27;s potentially a cost that keeps it from being worth it. The point though is that on the scale of a single person going through a single life, there are lots of small things you can do which have outsized returns (education and learning fall into that as well IMO, but this comment is long enough), far beyond the strategies which work at the scale of a retirement fund. reply menotyou 19 hours agoprevFirst of all, how about using a programming approach which needs much less refactoring? Aka not using object orientation? reply ttyyzz 18 hours agoparentI&#x27;m not sure if that choice would help you much. What drives code into being a mess really? I think it has more to do with wildly changing or unclear requirements, badly adopted scrum, having to deliver at any cost, forcing devs to so stuff they don&#x27;t want to and so on. Using another programming approach wouldn&#x27;t fix any of these. reply porridgeraisin 18 hours agoparentprevYeah. Or any one paradigm basically. Libraries too, if they force you into a certain paradigm. reply menotyou 18 hours agorootparentI don&#x27;t program myself, but I do manage developers for 20yrs+. I normally don&#x27;t care about what paradigm programmers use, but what I do empirically observe is that time spend on (automated) testing, refactoring and bug fixing is much higher in an object oriented environment. It seems from outside that the ability of producing reasonably bug free code is much lower when using OOP, hence the need of faster deployment cycles.Other industries went the way of producing good quality products whereas IT seems to use the approach to test the quality into the products and to fix, quite often when product is already at customer. This normally turns out be more expensive and have less quality in the products at the same time. reply TeMPOraL 10 hours agorootparent> This normally turns out be more expensive and have less quality in the products at the same time.Hypothesis: software market has near zero barriers to entry, so while other industries are protected from would-be competitors by large up-front expenses, for most purely-software products anyone can rapidly build a half-assed clone of whatever you&#x27;re doing and start eating into your customer base - making software companies obsess over velocity &#x2F; feature delivery rate, and&#x2F;or seeking all kinds of non-software barriers to entry (e.g. network effects, or content deals - like, it wouldn&#x27;t be hard to make a better Spotify client, but good luck replicating the deals Spotify has with recording labels). reply j45 12 hours agoprevOne aspect of technical debt is simply accumulating less until the lightbulb moment arrives to include mandatory refactoring. Build the least number of features you need to. reply readthenotes1 16 hours agoprevPrice: what consumers pay to acquire Cost: what producers pay to supplyRefactoring has both reply corethree 17 hours agoprevThere&#x27;s no theory on good design. You organize your code one way versus another way nobody can really point to any mathematical proof or empirical evidence for anything. It&#x27;s just one baseless arrangement vs. another.This is why, often, when you refactor code it doesn&#x27;t make it better. You don&#x27;t fully know if it&#x27;s better. It&#x27;s just another way. So even when you do this refactor stuff often you find yourself in the SAME situation even if you didn&#x27;t touch it.Think about it. You follow best practices, you try to write code as beautifully as possible. That doesn&#x27;t actually prevent a new feature from having you to gut a huge portion of your code to accommodate it.But developers don&#x27;t realize this. They think the extra time spent refactoring puts them in a better place then before. But really they just can&#x27;t see the counter factual. Did it really?I&#x27;ve been in places that emphasize on good code quality and put lots of good practices in place but they end up with \"tech debt\" anyway. Then when clean up day comes they blame it on bad practices and shortcuts but they&#x27;re blind to the fact that the they were one of the most rigorous companies in terms of code quality. reply sz4kerto 14 hours agoparentRefactoring is generally not about the code style, or changing the design for its own sake.Refactoring happens because the model of the world that the software works with has changed since the software has been implemented. Maybe the way users and roles are represented was kinda OK 5 years ago, but the world (== business environment, understanding, requirements) has changed since. So the code that has been OK 5 years ago is painfully mismatched with the present.That&#x27;s when refactoring is needed.Also. Tech debt is usually (not always) the marginal cost of the next feature. If new features cost X, and engineers are convinced that features could cost much less if the software was organised differently, then it might be useful to calculate the cost of a refactor. reply corethree 13 hours agorootparent>Refactoring happens because the model of the world that the software works with has changed since the software has been implemented. Maybe the way users and roles are represented was kinda OK 5 years ago, but the world (== business environment, understanding, requirements) has changed since. So the code that has been OK 5 years ago is painfully mismatched with the present.Totally agreed. This is a practical reason for it. But much of refactoring doesn&#x27;t happen because of this. The article says you should refactor all the time for \"better\" code. It&#x27;s not talking about your use case where you have to update dependencies, security features and things like that. reply RangerScience 17 hours agoparentprev> There&#x27;s no theory on good design.This is absolutely false. This talk: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=3OgbQOsW61Y does an excellent job of introducing some basic theory on good design, with solid (heh) justification for each principle. reply corethree 15 hours agorootparentThat&#x27;s not theory. It&#x27;s just a bunch of opinions.You can see plenty of companies that follow those principles and still end up in bad places.Theory is like number theory. You need axioms and theorems. Just because you assign acronyms and big words to things doesn&#x27;t make it a \"theory\". You need proof or empirical justification.Solid is like scrum, someone made it up. It was pulled out of someone&#x27;s ass. Doesn&#x27;t mean it&#x27;s bad, but it&#x27;s not categorically correct, it&#x27;s more of someone&#x27;s opinion.Something that is categorically correct is complexity theory for algorithms. You can definitively say Algorithm A is better than algorithm B from a performance perspective. But for program design and organization? Is one design better than another? How Solid is one design vs. another design? Can you concretely even define Solid in formal terms? No.I say again there is no theory on software code organization. None. reply RangerScience 12 hours agorootparentAlright, what‚Äôs your criteria for a ‚Äútheory‚Äù? Similarly, what‚Äôs your criteria for ‚Äúprogram design‚Äù?I want to have a better idea of the goal posts that you‚Äôre setting. reply corethree 5 hours agorootparentformal theory is the full term.https:&#x2F;&#x2F;www.wikiwand.com&#x2F;en&#x2F;Formal_theoryThe first three definitions fit.Anything that&#x27;s called \"Design\" usually means there&#x27;s no \"theory\" around it. It&#x27;s all ad-hoc gut feelings and made up. Things like UI or art typically have a lot of \"design\" associated with it.Usually a \"theory\" is a very concrete thing. You don&#x27;t \"design\" the shortest distance between two points. That&#x27;s more of a calculation. A calculation is available because we have a formal theory called geometry.We can calculate which algorithm is better because we have a formal theory around algorithm speed and memory.But program organization? No. No theory. Just rules of thumbs and made up acronyms and big words masquerading as theory. We don&#x27;t truly even have a formal definition of \"good program design\" or \"technical debt\" It&#x27;s these nebulous terms and the culture shifts and changes with new fads all the time.OOP used to be big with OOP design patterns as a big thing. Now OOP is on it&#x27;s way out... Do you see that with geometry? Can the theory of geometry be a fad? You may not know it, but even SOLID is more religion than it is formal theory.The closest thing to a formal theory of program organization I&#x27;ve seen is within functional programming and category theory. To get a formal theory for program organization we have to come of with formal definitions. What is good design? What is modularity? How do we compare the modularity of one program to another? None of these things have been defined yet but it looks like category theory and functional programming has sort of the primitives to build upon to make these concepts concrete. We&#x27;re still far away from something formal but if we ever develop something along these lines it will likely arise from Functional programming or category theory.Check this resource if you&#x27;re interested: https:&#x2F;&#x2F;www4.di.uminho.pt&#x2F;~jno&#x2F;ps&#x2F;pdbc.pdfHe calls it \"program design by calculation\" but it&#x27;s really just program design using algebras. He can&#x27;t prove why it&#x27;s better because, again, lack of theory but it looks like this approach is generally the right direction. reply RangerScience 41 minutes agorootparentOh wow, thank you for this write up, this is great! I think you&#x27;ve convinced me a little bit, and I&#x27;m super down to get into this discussion.If I&#x27;m understanding all this correctly (and I was never all that good at this level of math), what you&#x27;re saying is that we don&#x27;t have a way to, essentially, calculate the \"goodness\" of code using math. Or, in a practical sense, your linter can&#x27;t tell you how much tech debt you&#x27;ve got.And - yeah, I think you&#x27;ve convinced me that we don&#x27;t have that. Not as a complete theory like geometry, anyway....but! I do think we&#x27;ve got some solid steps in that direction. There&#x27;s a lot of things we can calculate - LoC, symbol counts, \"cyclomatic complexity\" - so then as I see it, the gap is connecting those metrics to \"goodness\" in a solid enough way that it&#x27;d cross the threshold into \"formal theory\". Which might not be possible, if \"goodness\" itself can&#x27;t be nailed down.One of the big reasons I like the talk I linked above is that I think it takes a solid (heh) step towards making that connection. You can use the techniques within to measure \"how much change was used to accomplish something\" (although there&#x27;s issues with that measurement, from \"what&#x27;s the unit?\" to \"how do you normalize that against, say, the size of the something?\" and \"that&#x27;s how much was used, but could less have been used?\").I&#x27;d then propose that \"goodness\" is something like a weighted sum of a few metrics, one of which is that \"how much change was used\", and further, that there are measurable things that can tell us something about how much change might be needed - although to your point, none of it a degree of completeness that&#x27;d constitute a theory.I guess, at the end of the day - I think we do have enough to propose some basic theories about program \"goodness\". SOLID could be one such theory, if it were rendered into much more formalized language. replyswayvil 18 hours agoprevRefactoring has a price. Not refactoring has a cost. Either way, you payPRICE. COST. PAY.3 roughly synonymous terms used when one would do just fine. Why?For example : Refactoring has a price. Not refactoring has a price. Either way there&#x27;s a price.This \"clever writing\" is a pet peeve of mine. reply marcosdumay 16 hours agoparentPrice in what you pay to get it. Cost in what you keep paying to keep it.Price is pretty unambiguous, cost is very ambiguous. I&#x27;ve never seen anybody make this distinction, but IMO, it&#x27;s clear enough. reply TeMPOraL 10 hours agoparentprevIDK, isn&#x27;t this just \"writing 101\" you learn in primary school, that repeating the same regular word multiple times in close proximity (within couple sentences) is ugly and indicates bad writing? reply diatone 12 hours agoparentprevIt‚Äôs intentional, to distinguish between known unknowns and unknown unknowns. reply zeroonetwothree 18 hours agoparentprevI would say it‚Äôs anticlever. Repetition is a far more effective device reply awill88 19 hours agoprevAh yes, this short, click baiting article highlights the very predictable and widely known side effect of what we all know as the software development cycle. reply xyzzy4747 18 hours agoprev [‚Äì] There‚Äôs two kinds of people, those who like to build business value and those who are obsessed with refactoring and code perfectionism. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article emphasizes the essential role of refactoring in development, indicating that not refactoring correlates with increased maintenance costs.",
      "It argues that a lack of continuous refactoring causes a deteriorated and difficult-to-maintain codebase, making feature additions risky due to potential breaks.",
      "The article concludes with the notion that whether opting for regular refactoring or allowing cruft accumulation, both attract a cost - however, the cost of a complete rewrite is higher."
    ],
    "commentSummary": [
      "The article emphasizes the consequences of neglecting to update outdated tech, stressing the importance of dealing with tech debt and the role of senior leadership in understanding the need for prioritizing such updates.",
      "The involvement of cybersecurity teams is recommended to aid in addressing vulnerabilities, suggesting the need for a balance between delivering new business features and focusing on tech tasks.",
      "Discussions revolve around the value of design and refactoring in software development, including perspectives on employing external tools like an IDE, the implications of rewriting a system, and the potential need for a formal theory of program organization."
    ],
    "points": 172,
    "commentCount": 151,
    "retryCount": 0,
    "time": 1697893334
  },
  {
    "id": 37970800,
    "title": "OpenRefine",
    "originLink": "https://openrefine.org/",
    "originBody": "Skip to main content OpenRefine Download Documentation Community Blog Donate GitHub Search K OpenRefine OpenRefine is a powerful free, open source tool for working with messy data: cleaning it; transforming it from one format into another; and extending it with web services and external data. Download Main features Faceting Drill through large datasets using facets and apply operations on filtered views of your dataset. Clustering Fix inconsistencies by merging similar values thanks to powerful heuristics. Reconciliation Match your dataset to external databases via reconciliation services. Infinite undo/redo Rewind to any previous state of your dataset and replay your operation history on a new version of it. Privacy Your data is cleaned on your machine, not in some dubious data laundering cloud. Wikibase Contribute to Wikidata, the free knowledge base anyone can edit, and other Wikibase instances. And much more to discover in our documentation. Get OpenRefine Download What's new Extensions Other distributions GitHub repository Documentation User manual Contributing External resources Privacy notice Community Get involved Forum Gitter chat Twitter Mastodon OpenRefine's documentation is licensed under a Creative Commons Attribution 4.0 International License.",
    "commentLink": "https://news.ycombinator.com/item?id=37970800",
    "commentBody": "OpenRefineHacker NewspastloginOpenRefine (openrefine.org) 164 points by rbanffy 12 hours ago| hidepastfavorite11 comments cdcarter 10 hours agoFour or five years ago, this was a tool I was using almost every day for work. Doing data consolidation and migrations for small nonprofits, we were faced with so many loosely structured excel sheets and CSV exports from various mailing programs. OpenRefine was absolutely instrumental in cleaning up lots of disparate data when the data sources were too many and too variable to make a scripted solution valuable. Glad to see it lives on. reply layman51 7 hours agoparentI have been working at a nonprofit and have only recently started using this for cleaning up Excel or CSV files that we want to import. I am not as familiar with doing this with code, but I love that this tool gives me the steps I have taken in case I ever want to audit the changes I made to the data. The one disadvantage I see is that it seems like it‚Äôs only for a single user and it might be burdensome to collaborate since you have to share the project file.I‚Äôm still excited to learn more about OpenRefine, but I guess maybe something like Google Colab might be better in terms of sharing and having direct access to our G Drives. reply yawnxyz 9 hours agoparentprevWhat tool did you move on to using instead? This tool seems super powerful! reply a5seo 8 hours agorootparentCan‚Äôt speak for OP but I moved to Exploratory.io. And the beauty of it is, it‚Äôs a GUI for R so you can export your transformation steps to R if needed. reply codetrotter 7 hours agoprevThis is awesome!I had a look at how to use this. The video I watched is a couple years old, but probably mostly relevant still. https:&#x2F;&#x2F;youtu.be&#x2F;nORS7STbLykThe thing that really resonates with me here is the way they use faceting to find bad data.When I write pipelines on the command line, I sometimes find it necessary to filter and select data in various ways. Because of this I end up rerunning cli pipelines multiple times sometimes. If instead I dump it to csv I could see myself using OpenRefine with its faceting to pick out the relevant data for processing reply wg0 8 hours agoprevThis used to be Google Refine at some point. It seems it still uses GWT (Google Web Toolkit) which was an amazing idea for its time.Rewrite it in Rust+SQLite+Tauri+Typescript+Svelte? reply jillesvangurp 41 minutes agoparentYou could rewrite it but it doesn&#x27;t really solve a problem this thing has. Web assembly of course is an opportunity to bring in a lot of existing data processing frameworks from e.g. the python, julia, r, etc. worlds and run them in a browser. Refine did a lot of it&#x27;s processing with a Java based server approach. The goal should be to reuse, not to reinvent if you take on a project like this. Chat gpt integration is a no brainer for this stuff these days. It excels at cleaning things up and figuring out unstructured&#x2F;half structured data.The spread sheet ui is super useful and something that non technical people are much more comfortable dealing with. I&#x27;ve used Google sheets as an interface to business people over the years. Whether it is categorizations, place descriptions, addresses, etc. just put it in a spreadsheet.Instead of building complicated UIs and tools, you just build a csv&#x2F;tsv importer and let people do their thing in a spreadsheet, export, validate, import. Once you get it in people&#x27;s heads that the column names are off limits for editing, they kind of get it. The nice thing about this stuff is that it is low tech, easy, and effective. And easy to explain to an intern, product owner, or other person that needs to sit down and do the monkey work.Refine takes this to the next level. You can take any old data in tabular format and cluster it phonetically, minor spelling differences, or by other criteria, bulk edit some rows, and export it. It&#x27;s also easy to enrich things via some rest API or run some simple scripts. But even just the bulk editing and grouping is super useful. We used it when it was still Google Refine more than 12 years ago to clean up tens of thousands of POIs. Typically we&#x27;d be grouping things on e.g. the city name and find that there would be a few spelling variations of things like M√ºnchen, Munchen, Muenchen, Munich, etc. Toss in a few utf-8 encoding issues where the √º got garbled and it&#x27;s a perfect tool for cleaning that up.Tens of thousands of records is potentially a lot of work but still tiny data. We had a machine learning team that used machine learning as the hammer for the proverbial nail. Google Refine achieved more in 1 afternoon than that team did trying to machine learn their way out of that mess in half a year. reply xnx 10 hours agoprevCool tool that I&#x27;ve been looking for an excuse to use more. Glad to see that it has continued to updat and improve after evolving from Freebase Gridworks to Google Refine to OpenRefine. reply hodanli 8 hours agoprevthis is my go-to tool for text unification and database normalization. reply alflervag 1 hour agoparentI‚Äôm surprised to hear you using this for database normalization. Could you expand on how OpenRefine is helpful here? reply datadrivenangel 8 hours agoprev [‚Äì] Tools like this allow for easy transformations and data wrangling while also keeping project history in a way that helps preserve lineage. Very valuable. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "OpenRefine is a free, open-source tool for cleaning and transforming unorganized data, with features like faceting, clustering, reconciliation, and unlimited undo/redo.",
      "The tool prioritizes privacy by cleaning data locally and allows data extension with web services and other external sources.",
      "Users can additionally contribute to Wikidata and access extra resources and documents, enhancing its utility and collaborative nature."
    ],
    "commentSummary": [
      "OpenRefine is a tool praised for its data cleaning and consolidation capabilities, particularly handling disparate and loosely structured data sources while tracking the process.",
      "Users discuss alternatives tools like Exploratory.io and the possibility of redeveloping OpenRefine with different technologies.",
      "The evolution of OpenRefine is noted, from its origins as Freebase Gridworks, to Google Refine, and finally to its present state as OpenRefine."
    ],
    "points": 162,
    "commentCount": 11,
    "retryCount": 0,
    "time": 1697925118
  },
  {
    "id": 37970572,
    "title": "The decline of computers as a general-purpose technology (2021)",
    "originLink": "https://cacm.acm.org/magazines/2021/3/250710-the-decline-of-computers-as-a-general-purpose-technology/fulltext",
    "originBody": "ACM SIGN IN Go ACM.org Join ACM About Communications ACM Resources Alerts & Feeds Communications of the ACM HOME CURRENT ISSUE NEWS BLOGS OPINION RESEARCH PRACTICE CAREERS ARCHIVE VIDEOS Home/Magazine Archive/March 2021 (Vol. 64, No. 3)/The Decline of Computers as a General Purpose Technology/Full Text CONTRIBUTED ARTICLES The Decline of Computers as a General Purpose Technology By Neil C. Thompson, Svenja Spanuth Communications of the ACM, March 2021, Vol. 64 No. 3, Pages 64-72 10.1145/3430936 Comments VIEW AS: Print Mobile App ACM Digital Library Full Text (PDF) In the Digital Edition SHARE: Email Reddit Hacker News Facebook Twitter LinkedIn Share Credit: The Image Foundation Perhaps in no other technology has there been so many decades of large year-over-year improvements as in computing. It is estimated that a third of all productivity increases in the U.S. since 1974 have come from information technology,a,4 making it one of the largest contributors to national prosperity. Back to Top Key Insights The rise of computers is due to technical successes, but also to the economics forces that financed them. Bresnahan and Trajtenberg3 coined the term general purpose technology (GPT) for products, like computers, that have broad technical applicability and where product improvement and market growth could fuel each other for many decades. But, they also predicted that GPTs could run into challenges at the end of their life cycle: as progress slows, other technologies can displace the GPT in particular niches and undermine this economically reinforcing cycle. We are observing such a transition today as improvements in central processing units (CPUs) slow, and so applications move to specialized processors, for example, graphics processing units (GPUs), which can do fewer things than traditional universal processors, but perform those functions better. Many high profile applications are already following this trend, including deep learning (a form of machine learning) and Bitcoin mining. With this background, we can now be more precise about our thesis: \"The Decline of Computers as a General Purpose Technology.\" We do not mean that computers, taken together, will lose technical abilities and thus 'forget' how to do some calculations. We do mean that the economic cycle that has led to the usage of a common computing platform, underpinned by rapidly improving universal processors, is giving way to a fragmentary cycle, where economics push users toward divergent computing platforms driven by special purpose processors. This fragmentation means that parts of computing will progress at different rates. This will be fine for applications that move in the 'fast lane,' where improvements continue to be rapid, but bad for applications that no longer get to benefit from field-leaders pushing computing forward, and are thus consigned to a 'slow lane' of computing improvements. This transition may also slow the overall pace of computer improvement, jeopardizing this important source of economic prosperity. Back to Top Universal and Specialized Computing Early days‚Äîfrom specialized to universal. Early electronics were not universal computers that could perform many different calculations, but dedicated pieces of equipment, such as radios or televisions, designed to do one task, and only one task. This specialized approach has advantages: design complexity is manageable and the processor is efficient, working faster and using less power. But specialized processors are also 'narrower,' in that they can be used by fewer applications. Early electronic computers,b even those designed to be 'universal,' were in practice tailored for specific algorithms and were difficult to adapt for others. For example, although the 1946 ENIAC was a theoretically universal computer, it was primarily used to compute artillery range tables. If even a slightly different calculation was needed, the computer would have to be manually re-wired to implement a new hardware design. The key to resolving this problem was a new computer architecture that could store instructions.10 This architecture made the computer more flexible, making it possible to execute many different algorithms on universal hardware, rather than on specialized hardware. This 'von Neumann architecture' has been so successful that it continues to be the basis of virtually all universal processors today. The ascent of universal processors. Many technologies, when they are introduced into the market, experience a virtuous reinforcing cycle that helps them develop (Figure 1a). Early adopters buy the product, which finances investment to make the product better. As the product improves, more consumers buy it, which finances the next round of progress, and so on. For many products, this cycle winds down in the short-to-medium term as product improvement becomes too difficult or market growth stagnates. Figure 1. The historical virtuous cycle of universal processers (a) is turning into a fragmentation cycle (b). GPTs are defined by the ability to continue benefiting from this virtuous economic cycle as they grow‚Äîas universal processors have for decades. The market has grown from a few high-value applications in the military, space, and so on, to more than two billion PCs in use worldwide.38 This market growth has fueled ever-greater investments to improve processors. For example, Intel has spent $183 billion on R&D and new fabrication facilities over the last decade.c This has paid enormous dividends: by one estimate processor performance has improved about 400,000x since 1971.8 The alternative: Specialized processors. A universal processor must be able to do many different calculations well. This leads to design compromises that make many calculations fast, but none optimal. The performance penalty from this compromise is high for applications well suited to specialization, that is those where: substantial numbers of calculations can be parallelized the computations to be done are stable and arrive at regular intervals ('regularity') relatively few memory accesses are needed for a given amount of computation ('locality') calculations can be done with fewer significant digits of precision.15 In each of these cases, specialized processors (for example, Application-specific Integrated Circuits (ASICs)) or specialized parts of heterogeneous chips (for example, I.P. blocks) can perform better because custom hardware can be tailored to the calculation.24 The extent to which specialization leads to changes in processor design can be seen in the comparison of a typical CPU‚Äîthe dominant universal processor‚Äîand a typical GPU‚Äîthe most-common type of specialized processor (see the accompanying table). Table. Technical specifications of a CPU compared to a GPU. The GPU runs slower, at about a third of the CPU's frequency, but in each clock cycle it can perform ‚àº100x more calculations in parallel than the CPU. This makes it much quicker than a CPU for tasks with lots of parallelism, but slower for those with little parallelism.d GPUs often have 5x‚Äì10x more memory bandwidth (determining how much data can be moved at once), but with much longer lags in accessing that data (at least 6x as many clock cycles from the closest memory). This makes GPUs better at predictable calculations (where the data needed from memory can be anticipated and brought to the processor at the right time) and worse at unpredictable ones. For applications that are well-matched to specialized hardware (and where programming models, for example CUDA, are available to harness that hardware), the gains in performance can be substantial. For example, in 2017, NVIDIA, the leading manufacturer of GPUs, estimated that Deep Learning (AlexNet with Caffe) got a speed-up of 35x+ from being run on a GPU instead of a CPU.27 Today, this speed-up is even greater.26 Another important benefit of specialized processorse is that they use less power to do the same calculation. This is particularly valuable for applications limited by battery life (cell phones, Internet-of-things devices), and those that do computation at enormous scales (cloud computing/ datacenters, supercomputing). As of 2019, 9 out of the top 10 most power efficient supercomputers were using NVIDIA GPUs.37 Specialized processors also have important drawbacks: they can only run a limited range of programs, are hard to program, and often require a universal processor running an operating system to control (one or more of) them. Designing and creating specialized hardware can also be expensive. For universal processors, their fixed costs (also called non-recurring engineering costs (NRE)) are distributed over a large number of chips. In contrast, specialized processors often have much smaller markets, and thus higher per-chip fixed costs. To make this more concrete, the overall cost to manufacture a chip with specialized processors using leading-edge technology is about $80 millionf (as of 2018). Using an older generation of technology can bring this cost down to about $30 million.23 Despite the advantages that specialized processors have, their disadvantages were important enough that there was little adoption (except for GPUs) in the past decades. The adoption that did happen was in areas where the performance improvement was inordinately valuable, including military applications, gaming and cryptocurrency mining. But this is starting to change. The state of specialized processors today. All the major computing platforms, PCs, mobile, Internet-of-things (IoT), and cloud/supercomputing, are becoming more specialized. Of these, PCs remain the most universal. In contrast, energy efficiency is more important in mobile and IoT because of battery life, and thus, much of the circuitry on a smartphone chip,34 and sensors, such as RFID-tags, use specialized processors.5,7 Cloud/supercomputing has also become more specialized. For example, 2018 was the first time that new additions to the biggest 500 supercomputers derived more performance from specialized processors than from universal processors.11 Industry experts at the International Technology Roadmap for Semiconductors (ITRS), the group which coordinated the technology improvements needed to keep Moore's Law going, implicitly endorsed this shift toward specialization in their final report. They acknowledged the traditional one-solution-fits-all approach of shrinking transistors should no longer determine design requirements and instead these should be tailored to specific applications.16 The next section explores the effect that the movement of all of the major computing platforms toward specialized processors will have on the economics of producing universal processors. Back to Top The Fragmentation of a General Purpose Technology The virtuous cycle that underpins GPTs comes from a mutually reinforcing set of technical and economic forces. Unfortunately, this mutual reinforcement also applies in the reverse direction: if improvements slow in one part of the cycle, so will improvements in other parts of the cycle. We call this counterpoint a 'fragmenting cycle' because it has the potential to fragment computing into a set of loosely-related siloes that advance at different rates. As Figure 1(b) shows, the fragmenting cycle has three parts: Technology advances slow Fewer new users adopt Financing innovation is more difficult The intuition behind this cycle is straightforward: if technology advances slow, then fewer new users adopt. But, without the market growth provided by those users, the rising costs needed to improve the technology can become prohibitive, slowing advances. And thus each part of this synergistic reaction further reinforces the fragmentation. Here, we describe the state of each of these three parts of the cycle for computing and show that fragmentation has already begun. Technology advancements slow. To measure the rate of improvement of processors we consider two key metrics: performanceg and performance-per-dollar. Historically, both of these metrics improved rapidly, largely because miniaturizing transistors led to greater density of transistors per chip (Moore's Law) and to faster transistor switching speeds (via Dennard Scaling).24 Unfortunately, Dennard Scaling ended in 2004/2005 because of technical challenges and Moore's Law is coming to an end as manufacturers hit the physical limits of what existing materials and designs can do,33 and these limits take ever more effort to overcome.2 The loss of the benefits of miniaturization can be seen vividly in the slowdown of improvements to performance and performance-per-dollar. Figure 2(a), based Hennessy and Patterson's characterization of progress in SPECInt, as well as Figure 2(b) based on the U.S. Bureau of Labor Statistics' producer-price index, show how dramatic the slowdown in performance improvement in universal computers has been. To put these rates into perspective, if performance per dollar improves at 48% per year, then in 10 years it improves 50x. In contrast, if it only improves at 8% per year, then in 10 years it is only 2x better. Figure 2. Rate of improvement in microprocessors, as measured by (a) Annual performance improvement on the SPECint benchmark,7appx and (b) Annual quality-adjusted price decline.1appx Fewer new users adopt. As the pace of improvement in universal processors slows, fewer programs with new functionality will be created, and thus customers will have less incentive to replace their computing devices. Intel CEO Krzanich confirmed this in 2016, saying that the replacement rate of PCs had risen from every four years to every 5‚Äì6 years.22 Sometimes, customers even skip multiple generations of processor improvement before it is worth updating.28 This is also true on other platforms, for example U.S. smartphones were upgraded on average every 23 months in 2014, but by 2018 this had lengthened to 31 months.25 GPTs are defined by the ability to continue benefiting from this virtuous economic cycle as they grow‚Äîas universal processors have for decades. The movement of users from universal to specialized processors is central to our argument about the fragmentation of computing, and hence we discuss it in detail. Consider a user that could use either a universal processor or a specialized one, but who wants the one that will provide the best performance at the lowest cost.h Figures 3(a) and 3(b) present the intuition for our analysis. Each panel shows the performance over time of universal and specialized processors, but with different rates at which the universal processor improves. In all cases, we assume that the time, T, is chosen so the higher price of a specialized processor is exactly balanced out by the costs of a series of (improving) universal processors. This means that both curves are cost equivalent, and thus superior performance also implies superior performance-per-dollar. This is also why we depict the specialized processor as having constant performance over this period. (At the point where the specialized processor would be upgraded, it too would get the benefit of whatever process improvement had benefited the universal processor and the user would again repeat this same decision process.) Figure 3. Optimal processor choice depends on the performance speed-up that the specialized processor provides, as well as the rate of improvement of the universal technology. A specialized processor is more attractive if it provides a larger initial gain in performance. But, it also becomes more attractive if universal processor improvements go from a rapid rate, as in panel (a), to a slower one, as in panel (b). We model this formally by considering which of two time paths provides more benefit. That is, a specialized processor is more attractive if Where universal and specialized processors deliver performancei Pu, and Ps, over time T, while the universal processor improves at r.j We present our full derivation of this model in the online appendix (https://doi.org/10.1145/3430936). That derivation allows us to numerically estimate the volume needed for the advantages of specialization to outweigh the higher costs (shown in Figure 3(c) for a slowdown from 48% to 8% in the per-year improvement rate of CPUs). Not surprisingly, specialized processors are more attractive when they provide larger speedups or when their costs can be amortized over larger volumes. These cutoffs for when specialization becomes attractive change, however, based on the pace of improvement of the universal processors. Importantly, this effect does not arise because we are assuming different rates of progress between specialized and universal processors overall‚Äîall processors are assumed to be able to use whatever is the cutting-edge fabrication technology of the moment. Instead, it arises because the higher per-unit NRE of specialized processors must be amortized and how well this compares to upgrading universal processors over that period. A numerical example makes clear the importance of this change. At the peak of Moore's Law, when improvements were 48% per year, even if specialized processors were 100x faster than universal ones, that is, (a huge difference), then ‚àº83,000 would need to be built for the investment to pay off. At the other extreme, if the performance benefit were only 2x, ‚àº1,000,000 would need to be built to make specialization attractive. These results make it clear why, during the heyday of Moore's Law, it was so difficult for specialized processors to break into the market. However, if we repeat our processor choice calculations using an improvement rate of 8%, the rate from 2008‚Äì2013, these results change markedly: for applications with 100x speed-up, the number of processors needed falls from 83,000 to 15,000, and for those with 2x speed-up it drops from 1,000,000 to 81,000. Thus, after universal processor progress slows, many more applications became viable for specialization.k Financing innovation is harder. In 2017, the Semiconductor Industry Association estimated that the cost to build and equip a fabrication facility ('fab') for the next-generation of chips was roughly $7 billion.35 By \"next-generation,\" we mean the next miniaturization of chip components (or process 'node'). The costs invested in chip manufacturing facilities must be justified by the revenues that they produce. Perhaps as much as 30%l of the industry's $343 billion annual revenue (2016) comes from cutting-edge chips. So revenues are substantial, but costs are growing. In the past 25 years, the investment to build leading-edge fab (as shown in Figure 4a) rose 11% per year (!), driven overwhelmingly by lithography costs. Including process-development costs into this estimate further accelerates cost increases to 13% per year (as measured for 2001 to 2014 by Santhanam et al.32). This is well known by chipmakers who quip about Moore's \"second law\": the cost of a chip fab doubles every four years.9 Figure 4. Deteriorating economics of chip manufacturing. Historically, the implications of such a rapid increase in fixed cost on unit costs was only partially offset by strong overall semiconductor market growth (CAGR of 5% from 1996‚Äì2016m,35), which allowed semiconductor manufacturers to amortize fixed costs across greater volumes. The remainder of the large gap between fixed costs rising 13% annually and the market growing 5% annually, would be expected to lead to less-competitive players leaving the market and remaining players amortizing their fixed costs over a larger number of chips. As Figure 4(b) shows, there has indeed been enormous consolidation in the industry, with fewer and fewer companies producing leading-edge chips. From 2002/2003 to 2014/2015/2016, the number of semiconductor manufacturers with a leading-edge fab has fallen from 25 to just 4: Intel, Taiwan Semiconductor Manufacturing Company (TSMC), Samsung and GlobalFoundries). And GlobalFoundries recently announced that they would not pursue development of the next node.6 We find it very plausible this consolidation is caused by the worsening economics of rapidly rising fixed costs and only moderate market size growth. The extent to which market consolidation improves these economics can be seen through some back-of-the-envelope calculations. If the market were evenly partitioned amongst different companies, it would imply a growth in average market share from in 2002/2003 to in 2014/2015/2016. Expressed as a compound annual growth rate, this would be 14%. This means that producers could offset the worsening economics of fab construction through market growth and taking the market share of those exiting (13%<5%+14%). In practice, the market was not evenly divided, Intel had dominant share. As a result, Intel would not have been able to offset fixed cost growth this way.n And indeed, over the past decade, the ratio of Intel's fixed costs to its variable costs has risen from 60% to over 100%.o This is particularly striking because in recent years Intel has slowed the pace of their release of new node sizes, which would be expected to decrease the pace at which they would need to make fixed costs investments. The ability for market consolidation to offset fixed cost increases can only proceed for so long. If we project forward current trends, then by 2026 to 2032 (depending on market growth rates) leading-edge semiconductor manufacturing will only be able to support a single monopolist manufacturer, and yearly fixed costs to build a single new facility for each node size will be equal to yearly industry revenues (see endnote for detailsp). We make this point not to argue that in late 2020s this will be the reality, but precisely to argue that current trends cannot continue and that within only about 10 years(!) manufacturers will be forced to dramatically slow down the release of new technology nodes and find other ways to control costs, both of which will further slow progress on universal processors. The fragmentation cycle. With each of the three parts of the fragmentation cycle already reinforcing each other, we expect to see more and more users facing meager improvements to universal processors and thus becoming interested in switching to specialized ones. For those with sufficient demand and computations well-suited to specialization (for example, deep learning), this will mean orders of magnitude improvement. For others, specialization will not be an option and they will remain on universal processors improving ever-more slowly. Back to Top Implications Who will specialize. As shown in Figure 3(c), specialized processors will be adopted by those that get a large speedup from switching, and where enough processors would be demanded to justify fixed costs. Based on these criteria, it is perhaps not surprising that big tech companies have been amongst the first to invest in specialized processors, for example, Google,19 Microsoft,31 Baidu,14 and Alibaba.29 Unlike the specialization with GPUs, which still benefited a broad range of applications, or those in cryptographic circuits, which are valuable to most users, we expect narrower specialization going forward because only small numbers of processors will be needed to make the economics attractive. We also expect significant usage from those who were not the original designer of the specialized processor, but who re-design their algorithm to take advantage of new hardware, as deep learning users did with GPUs. It is expected the final benefits from miniaturization will come at a price premium, and are only likely to be paid for by important commercial applications. Who gets left behind. Applications that do not move to specialized processors will likely fail to do so because they: Get little performance benefit, Are not a sufficiently large market to justify the upfront fixed costs, or Cannot coordinate their demand. Earlier, we described four characteristics that make calculations amenable to speed-up using specialized processors. Absent these characteristics, there are only minimal performance gains, if any, to be had from specialization. An important example of this is databases. As one expert we interviewed told us: over the past decades, it has been clear that a specialized processor for databases could be very useful, but the calculations needed for databases are poorly-suited to being on a specialized processor. The second group that will not get specialized processors are those where there is insufficient demand to justify the upfront fixed costs. As we derived with our model, a market of many thousands of processors are needed to justify specialization. This could impact those doing intensive computing on a small scale (for example, research scientists doing rare calculations) or those whose calculations change rapidly over time and thus whose demand disappears quickly. A third group that is likely to get left behind are those where no individual user represents sufficient demand, and where coordination is difficult. For example, even if thousands of small users would collectively have enough demand, getting them to collectively contribute to producing a specialized processor would be prohibitively difficult. Cloud computing companies can play an important role in mitigating this effect by financing the creation of specialized processors and then renting these out.q Will technological improvement bail us out? To return us to a convergent cycle, where users switch back to universal processors, would require rapid improvement in performance and/or performance-per-dollar. But technological trends point in the opposite direction. For example on performance, it is expected that the final benefits from miniaturization will come at a price premium, and are only likely to be paid for by important commercial applications. There is even a question whether all of the remaining, technically-feasible, miniaturization will be done. Gartner predicts that more will be done, with 5nm node sizes being produced at scale by 2026,18 and TSMC recently announced plans for a $19.5B 3nm plant for 2022.17 But many of the interviewees that we contacted for this study doubt were skeptical about whether it would be worthwhile miniaturizing for much longer. Might another technological improvement restore the pace of universal processor improvements? Certainly, there is a lot of discussion of such technologies: quantum computing, carbon nanotubes, optical computing. Unfortunately, experts expect that it will be at least another decade before industry could engineer a quantum computer that is broader and thus could potentially substitute for classical universal computers.30 Other technologies that might hold broader promise will likely still need significantly more funding to develop and come to market.20 Back to Top Conclusion Traditionally, the economics of computing were driven by the general purpose technology model where universal processors grew ever-better and market growth fuels rising investments to refine and improve them. For decades, this virtuous GPT cycle made computing one of the most important drivers of economic growth. This article provides evidence that this GPT cycle is being replaced by a fragmenting cycle where these forces work to slow computing and divide users. We show each of the three parts of the fragmentation cycle are already underway: there has been a dramatic and ever-growing slowdown in the improvement rate of universal processors; the economic trade-off between buying universal and specialized processors has shifted dramatically toward specialized processors; and the rising fixed costs of building ever-better processors can no longer be covered by market growth rates. Collectively, these findings make it clear that the economics of processors has changed dramatically, pushing computing into specialized domains that are largely distinct and will provide fewer benefits to each other. Moreover, because this cycle is self-reinforcing, it will perpetuate itself, further fragmenting general purpose computing. As a result, more applications will split off and the rate of improvement of universal processors will further slow. Our article thus highlights a crucial shift in the direction that economics is pushing computing, and poses a challenge to those who want to resist the fragmentation of computing. Figure. Watch the authors discuss this work in the exclusive Communications video. https://cacm.acm.org/videos/the-decline-of-computers Back to Top References 1. Amazon Web Services: Elastic GPUs, 2017; https://aws.amazon.com/de/ec2/elastic-gpus/ 2. Bloom, N., Jones, C., Van Reenen, J. and Webb, M. Are Ideas Getting Harder to Find? Cambridge, MA, 2017; https://doi.org/10.3386/w23782 3. Bresnahan, T.F. and Trajtenberg, M. General purpose technologies 'Engines of growth'? J. Econom. 65, 1 (Jan. 1995), 83‚Äì108; https://doi.org/10.1016/0304-4076(94)01598-T 4. Byrne, D.M., Oliner, S.D. and Sichel, D.E. Is the information technology revolution Over? SSRN Electron. J. (2013), 20‚Äì36; https://doi.org/10.2139/ssrn.2303780 5. Cavin, R.K., Lugli, P. and Zhirnov, V. V. Science and engineering beyond Moore's Law. In Proceedings of the IEEE 100, Special Centennial Issue (May 2012), 1720‚Äì1749; https://doi.org/10.1109/JPROC.2012.2190155 6. Dent, S. Major AMD chip supplier will no longer make next-gen chips, 2018; https://www.engadget.com/2018/08/28/global-foundries-stops-7-nanometer-chip-production/ 7. Eastwood, G. How chip design is evolving in response to IoT development. Network World (2017); https://www.networkworld.com/article/3227786/internet-of-things/how-chip-design-is-evolving-in-response-to-iot-development.html 8. Economist. The future of computing‚ÄîAfter Moore's Law (2016); https://www.economist.com/news/leaders/21694528-era-predictable-improvement-computer-hardware-ending-what-comes-next-future 9. Economist. The chips are down: The semiconductor industry and the power of globalization (2018); https://www.economist.com/briefing/2018/12/01/the-semiconductor-industry-and-the-power-of-globalisation. 10. ENIAC Report. Moore School of Electrical Engineering, 1946. 11. Feldmann, M. New GPU-accelerated supercomputers change the balance of power on the TOP500, 2018; https://www.top500.org/news/new-gpu-accelerated-supercomputers-change-the-balance-of-power-on-the-top500/ 12. Gartner Group. Gartner Says Worldwide Semiconductor Revenue Grew 22.2 Percent in 2017. Samsung Takes Over No. 1 Position. Gartner. 2018; https://www.gartner.com/newsroom/id/3842666 13. Google Cloud. Google: Release Notes, 2018; https://cloud.google.com/tpu/docs/release-notes 14. Hemsoth, N. An Early Look at Baidu's Custom AI and Analytics Processor. The Next Platform; https://www.nextplatform.com/2017/08/22/first-look-baidus-custom-ai-analytics-processor/ 15. Hennessy, J. and Patterson, D. Computer Architecture: A Quantitative Approach (6th ed.). Morgan Kaufmann Publishers, Cambridge, MA, 2019.. 16. International Technology Roadmap for Semiconductors 2.0: Executive Report. International technology roadmap for semiconductors, 79, 2015; http://www.semiconductors.org/main/2015_international_technology_roadmap_for_semiconductors_itrs/ 17. Jao, N. Taiwanese chip maker TSMC to build the world's first 3nm chip factory. Technode, 2018; https://technode.com/2018/12/20/taiwanese-chip-maker-tsmc-to-build-the-worlds-first-3nm-chip-factory/ 18. Johnson, B., Tuan, S., Brady, W., Jim, W. and Jim, B. Gartner Predicts 2017: Semiconductor Technology in 2026. 19. Jouppi, N.P. et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th Annual Int. Symp. Comput. Archit. (2017), 1‚Äì12; https://doi.org/10.1145/3079856.3080246 20. Khan, H.N., Hounshell, D.A. and Fuchs, E.R.H. Science and research policy at the end of Moore's Law. Nat. Electron. 1, 1 (2018), 14‚Äì21; https://doi.org/10.1038/s41928-017-0005-9 21. Khazraee, M., Zhang, L., Vega, L. and Taylor, M.B. Moonwalk? NRE optimization in ASIC clouds or accelerators will use old silicon. In Proceedings of ACM ASPLOS 2017, 1‚Äì16; https://doi.org/http://dx.doi.org/10.1145/3037697.3037749 22. Krzanich, B. Intel Corporation Presentation at Sanford C Berstein Strategic Decisions Conference, 2016. 23. Lapedus, M. Foundry Challenges in 2018. Semiconductor Engineering, 2017; https://semiengineering.com/foundry-challenges-in-2018/ 24. Leiserson, C.E., Thompson, N., Emer, J., Kuszmaul, B.C., Lampson, B.W., Sanchez, D. and Schardl, T.B. There's plenty of room at the top: What will drive growth in computer performance after Moore's Law ends? Science (2020). 25. Martin, T.W. and Fitzgerald, D. Your love of your old smartphone is a problem for Apple and Samsung. WSJ (2018); https://www.wsj.com/articles/your-love-of-your-old-smartphone-is-a-problem-for-apple-and-samsung-1519822801 26. Mims, C. Huang's Law is the new Moore's Law, and explains why Nvidia wants arm. WSJ (2020); https://www.wsj.com/articles/huangs-law-is-the-new-moores-law-and-explains-why-nvidia-wants-arm-11600488001 27. NVIDIA Corporation. Tesla P100 Performance Guide. HPC and Deep Learning Applications, 2017. 28. Patton, G. Forging Intelligent Systems in the Digital Era. MTL Seminar, 2017; https://www-mtl.mit.edu/mtlseminar/garypatton.html#simple3 29. Pham, S. 2018. Who needs the US? Alibaba will make its own computer chips. CNN Business, 2018; https://edition.cnn.com/2018/10/01/tech/alibaba-chip-company/index.html 30. Prickett Morgan, T. Intel Takes First Steps To Universal Quantum Computing. Next Platform, 2017; https://www.nextplatform.com/2017/10/11/intel-takes-first-steps-universal-quantum-computing/ 31. Putnam, A. et al. A reconfigurable fabric for accelerating large-scale datacenter services. Commun. ACM 59, 11 (Oct. 2016), 114‚Äì122; https://doi.org/10.1145/2996868 32. Santhanam, N., Wiseman, B., Campbell, H., Gold, A. and Javetski, B. McKinsey on Semiconductors, 2015. 33. Shalf, J.M. and Leland, R. Computing beyond Moore's Law. Computer 48, 12 (Dec. 2015), 14‚Äì23; https://doi.org/10.1109/MC.2015.374 34. Shao, Y.S., Reagen, B., Wei, G.Y., and Brooks, D. Aladdin: A pre-RTL, power-performance accelerator simulator enabling large design space exploration of customized architectures. In Proceedings of the Int. Symp. Comput. Archit. (2014), 97‚Äì108; DOI:https://doi.org/10.1109/ISCA.2014.6853196 35. Semiconductor Industry Association: 2017 Factbook; http://go.semiconductors.org/2017-sia-factbook-0-0-0 36. Smith, S.J. Intel: Strategy Overview, 2017; https://doi.org/10.1016/B978-0-240-52168-8.10001-X 37. Top500. The Green500 List (June 2019); https://www.top500.org/green500/lists/2019/06/ 38. Worldometers. Computers sold in the world this year, 2018; http://www.worldometers.info/computers/ Back to Top Authors Neil C. Thompson (neil_t@mit.edu) is an innovation scholar in the Computer Science and Artificial Intelligence Lab and the Initiative on the Digital Economy at the Massachusetts Institute of Technology, Cambridge, MA, USA. Svenja Spanuth (sspanuth@ethz.ch) is a Ph.D. candidate in the Department of Management, Technology, and Economics at ETH Zurich, Switzerland. Back to Top Footnotes a. Their analysis excludes the farming sector. b. In this article, the term \"computer\" describes both, devices with solely a universal processor and those that also contain specialized functionality. c. Calculated as 2008‚Äì2017 R&D and additions to PPE spending. d. Of course, many tasks will have multiple parts, some parallelizable and some not. In which case, speed-ups will be constrained by Amdahl's Law. e. For brevity we will use the term \"specialized processors\" throughout this article to refer both to stand-alone processors as well as specialized functionality on heterogeneous chips (for example, I.P. blocks) f. This true for the 16/14nm node size. Lithography cost are by far the biggest cost component of the manufacturing NRE.21 Other costs include labor and design tools, as well as IP licensing. g. While we have in mind a measure of performance based on computational power/speed, this model is actually more general and could refer to other characteristics (for example, energy efficiency). h. Computing at larger scales (including the massive parallelism of current deep learning models) are scaled-up versions of this same problem, and the logic of our analysis (and thus our results) also carry over to them. i. Here we assume that the cost of the CPU running the OS and controlling the specialized processor(s) does not materially affect this calculation. Relaxing that assumption would not change our model but would require incorporating of these costs into the specialized processor parameter estimates. j. In practice, manufacturers do not update continuously, but in large steps when they release new designs. Users, however, may experience these jumps more continuously since they tend to constantly refresh some fraction of their computers. The continuous form is also more mathematically tractable. k. In the online appendix (https://doi.org/10.1145/3430936) we also consider how these values change with code development costs. l. $23 billion of Foundry revenue (TSMC and GlobalFoundries) can be attributed to leading-edge nodes.36 Assuming the majority (90%) of Intel's ($54 billion) and Samsung's ($40 billion) total revenues12 derives from leading-edge nodes, yields an upper bound of $108 billion/$343 billion‚âà30%. m. We implicitly assume that this is also the rate of growth for the leading-edge nodes. In practice it may be somewhat lower, which would only accentuate our point. n. Despite their rate of change being less favorable, Intel's large market share meant that they started from a lower base, so they remained highly competitive. o. Calculated from Intel financial statements, with fixed costs as R&D + Property, Plant and Equipment, and variable costs as cost of goods sold. p. Assumes new facilities are needed every two years; 30% of market sales go to leading edge chips; and 13% annual increase in fixed costs. 2026: 0% market growth / 2032: 5% market growth. We (conservatively) assume all market demand can be met with a single facility. If more than that is needed, the date moves earlier. q. Already, Google provides TPUs on its cloud,13 and Amazon Web Services (and others) provide GPUs.1 The authors contributed equally to the work. Copyright held by authors. Request permission to (re)publish from the owner/author The Digital Library is published by the Association for Computing Machinery. Copyright ¬© 2021 ACM, Inc. No entries found SIGN IN for Full Access User Name Password ¬ª Forgot Password? ¬ª Create an ACM Web Account SIGN IN ARTICLE CONTENTS: Introduction Key Insights Universal and Specialized Computing The Fragmentation of a General Purpose Technology Implications Conclusion References Authors Footnotes MORE NEWS & OPINIONS What Do Neurons, Fireflies, and Dancing the Nutbush Have in Common? University of Sydney (Australia) On Being a Computer Science Communicator Sheldon H. Jacobson Coaxing Performance from the Complexity of HPC Anton Demin, Oleg Shakhov, Andrei Sukhov For Authors For Advertisers Privacy Policy Help Contact Us Mobile Site Copyright ¬© 2023 by the ACM. All rights reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=37970572",
    "commentBody": "The decline of computers as a general-purpose technology (2021)Hacker NewspastloginThe decline of computers as a general-purpose technology (2021) (acm.org) 161 points by yeesian 12 hours ago| hidepastfavorite100 comments ethbr1 9 hours agoI understand this is ACM, but I would say the decline has strictly been a failure of software.Specifically, corporate IT&#x27;s failure to deliver general purpose development tools usable by anyone in a company to make computers do work for them (aka programming).There&#x27;s still a ton of general value that could be delivered by general purpose processors&#x2F;computers.But instead, most non-programmers live in a Kafkaesque reality where the only programs they have access to don&#x27;t do what they want, and so their work becomes \"working around the bad program.\"I helped a lady on Friday at a midsized bank whose job it was to take a set of dates in an Excel file, search each one individually into an internal search tool, then Ctrl+f the result report and verify that every code listed in the original document exists in the report.For 100+ entries.Every month.In 2023.What the fuck are we doing as a society, that this lady, with a machine that could compute the above in a fraction of a second, has to spend a decent chunk of her work time doing this?Example specifically chosen because I&#x27;ve seen innumerable similar \"internal development will never work on this, because it&#x27;s too small of an impact\" issues at every company.Computers should work for us. All of us. Not the other way around. reply delusional 1 hour agoparent>I helped a lady on Friday at a midsized bank whose job it was to take a set of dates in an Excel file, search each one individually into an internal search tool, then Ctrl+f the result report and verify that every code listed in the original document exists in the report.I happen to work in a midsized bank, and we have TONS of these sorts of manual processes. My favorite unintuitive fact is that 90% of them turn out to be some outdated compliance process that nobody remembered a person even did, and that is no longer necessary.That also usually the reason why something is \"too small of an impact\". Having 2 engineers come in to figure out that something is obviously just busywork, and then try to run some political process to convince leadership, is very expensive. reply omscs99 5 hours agoparentprevHypothetically, if she figured out how to automate it on her own, would she get any kind of reward for it? If not, why should she automate it?Another example, if there‚Äôs some repetitive task that you automate at a software dev job, would you get rewarded for figuring out how to automate it? The answer is obviously dependent on culture, at my current gig you just get brushed offSeems to me like you‚Äôre assuming that the economy incentivizes efficiency. Imo it doesn‚Äôt, it‚Äôs just a bunch of stupid rich people sloshing money around (for the most part). None of it makes any sense reply arvinsim 3 hours agorootparent> Hypothetically, if she figured out how to automate it on her own, would she get any kind of reward for it? If not, why should she automate.The reward is more time. Now where that time is spent is another story. reply HHC-Hunter 3 hours agorootparentSounds like you&#x27;re implying that that newly spared time would be hers to spend and not her employers? reply eru 3 hours agorootparentIf she keeps her mouth shut about it, definitely. reply TeMPOraL 2 hours agorootparentprevOf course. She was presumably hired for regular office work, not R&D. reply dingi 1 hour agorootparentprevwhy not? she needs to be rewarded for that automation somehow. reply atoav 27 minutes agorootparentRealistically that reward is going to be:1. more work2. nice words and then more work3. nice words, more work and then she is asked to do the same for another person&#x27;s task who is then firedSorry to be cynical here, but it is very rare for managment to reward the few people that do a better than good job with more free time or less work. reply Defletter 3 hours agorootparentprevMore time, yes, probably doing something else equally tedious, or looking for a job. reply atoav 29 minutes agorootparentprevI had a job like this as a student. I automated it and didn&#x27;t tell anyone and used the newly won time to do other stuff. reply dTal 8 hours agoparentprevWhile this is true and arguably of more importance, the thesis of the article was something quite specific: \"the economic cycle that has led to the usage of a common computing platform, underpinned by rapidly improving universal processors, is giving way to a fragmentary cycle, where economics push users toward divergent computing platforms driven by special purpose processors\". This is likely to exacerbate the issue you describe, but it is not caused by it.I don&#x27;t think your observation was any less true 20 years ago either. There hasn&#x27;t been a \"decline\", as such - more of a failure to realize potential. Office Space (1999) was full of people doing mindless busywork that could easily be automated. reply ethbr1 8 hours agorootparentFrom a hardware perspective, the ACM cycle discussed is tightly tied to continued general purpose CPU performance gains.There are many pure-performance classes of software, where more performance = more value. Those classes have been diverging since the 80s (media playback), 90s (graphics), 00s (mobile), and ~10s (gpgpu).But there are other classes that are functionality limited. E.g. electronic medical record or enterprise resource planning.If software functionality were more plastic or expanded in those, the same general purpose performance would then be more valuable, and investment would also be incentivized.Accepting the inevitability of divergent, harder to user-program platforms, when we still have a lot of value on the table feels premature.And like it bodes badly for further losses of user computing-sovereignty as hyper-optimized hardware makes cloud platforms more attractive and kills off user-owned processing. reply makeitdouble 8 hours agoparentprevThere&#x27;s many situations where manually dealing with 100+ records is a decent tradeoff. I feel people in our field overestimate the actual complexity of many dumb looking tasks.From your bank employee example, it looks like your solution would be to open the bank&#x27;s internal tool to her excel instance, have it somewhat find the right records and inject the right value, while checking the listed codes. That looks like a series of APIs if they care about data exposure, or huge CSV exports that they need to then track ?And as it seems to be important enough to warrant verification, it means your program is not some random batch, it needs significant validation.Then picture the internal tool changing its output format. Reports getting categorized differently because of organizational change. New fields being added to follow new legislations. The tool could break at any of these changes, and the lady&#x27;s left with a broken system except a lot of money has been invested, it&#x27;s supposed to have solved her problem, she might not have the same access to data she had before the integration, and overall the situation could be worse than before.That feels like a bleak scenario, but it happens all the time as well. reply bloak 1 hour agorootparentI think you&#x27;re right. I&#x27;ve sometimes been in the situation of having to maintain a script that depends on APIs or data formats that change from time to time without warning. A fair amount of skill and effort is required. (There is skill involved in writing the script so that it is maintainable and not too fragile but still detects when something has gone wrong.) If the script is doing a task that has to be completed on a particular day every month, rather than whenever the script hacker is next available, then you&#x27;d have to choose between paying the hacker a reasonable retainer or having someone around who you know can still do the task manually, which means you might have to get them to do the task manually from time to time to stay in practice.A dilemma I keep facing in practice is converting a text that someone else has produced in their own special way, like the ghastly cluttered HTML from a word processor. For example, do I attempt to use a Perl script to automatically convert some of the mark-up, or do I throw away all the mark-up and put back the italics by hand, with the risk of making a mistake? (If anyone knows of a good configurable or modifiable tool for converting cluttered HTML into sane HTML&#x2F;XML I&#x27;d be interested to hear about it. My Perl scripts for doing it are not great.) reply danielbln 57 minutes agorootparentSounds like a good usecase for an LLM. reply jiggawatts 8 hours agorootparentprevIt‚Äôs basically a JOIN, the most fundamental data manipulation operator. If you can‚Äôt automate this, then you don‚Äôt ‚Äúget‚Äù computers at all.I once had two guys arguing next to me about how it was impossible to produce a simple report about VM storage utilisation in under a month. I produced the report, printed it out, and shoved it in their hands just to shut them up while they were still busy arguing. A ‚Äúmonth‚Äù of work required five joins. reply johnmaguire 8 hours agorootparent> It&#x27;s basically a JOINThe comment you&#x27;re replying just explained why this is not \"basically a JOIN.\" There are two unrelated systems (not SQL databases) which don&#x27;t expose any APIs to the analyst. reply tuatoru 7 hours agorootparentIt&#x27;s still basically a join. If it&#x27;s on a computer, it can be converted to text files, and then it can be JOINed, one way or another. reply csomar 6 hours agorootparentIn these cases, the work is usually about connecting the systems together than the data modification itself. reply TeMPOraL 36 minutes agorootparentNo, that&#x27;s just made up work that&#x27;s not required to solve a problem. Systems are already connected enough by virtue of being accessible by the same person on the same computer at the same time. Turning this into software project and building Proper Integration is only creating a tight coupling between the systems, which will only keep creating bugs and requires ongoing maintenance. reply ethbr1 7 hours agorootparentprevMy sibling from another mother. That&#x27;s exactly what we did. reply c0pium 1 hour agorootparentprevNo they didn‚Äôt. They made up out of whole cloth a bunch of nonsense and then acted smug about how other people don‚Äôt get it. Then OP posted that it was in fact just a join.Stop being so helpless. reply nicbou 27 minutes agorootparentThis is a pointlessly mean and unpleasant comment. It was polite discussion up until that point. reply ethbr1 8 hours agorootparentprevWe were able to fix it up with some automation in a couple days.Critically, still running visually on her machine, and with internal resources for lifecycle support if the process changes. reply johnmaguire 7 hours agorootparentFinding internal resources to fix the tool when things change is very prescient.Growing up, my mom performed a job where she would essentially copy car crash data from one system into another, row by row. As a teenager learning to program, it was obvious to me this could be easily automated. She (perhaps rightfully) didn&#x27;t want to run anything I cooked up on her company&#x27;s computer!A year or so later she was laid off a long with many others in the company performing similar roles.And this is the problem, isn&#x27;t it? Once the job is automated, the analyst is no longer necessary. You really need to \"learn to fish.\" reply troupe 7 hours agorootparentprevDo you have any guess how long it will take to break even? Presumably you charged some amount to spend a few days automating it and hopefully it frees her up to do something more valuable for the company.Sometimes the reason things aren&#x27;t automated is because there isn&#x27;t anything more valuable for that person to do so automation doesn&#x27;t have a positive return on investment. reply concordDance 5 hours agorootparentCan&#x27;t speak for them, but I&#x27;ve done office automation work with a breakeven measured in days. reply ethbr1 7 hours agorootparentprevIt was an internal citizen developer initiative that I was supporting externally. Don&#x27;t know her salary, so couldn&#x27;t say.> because there isn&#x27;t anything more valuable for that person to doI&#x27;ve seen an interesting shift post-COVID where (remaining) ops workers are often more coordinators of &#x2F; responsible for their area of responsibility.Which is a distinction because in that sort of job, not having Task X leads directing into finding something else you can be doing to improve your area. reply makeitdouble 7 hours agorootparentprev> Critically, still running visually on her machine, and with internal resources for lifecycle support if the process changes.That&#x27;s such an elegant and perfect solution. She keeps track of what&#x27;s happening, it all runs through her GUI so worse* case scenario she does it herself again, and she can get help if&#x2F;when she needs it.* actual worse case would be the script going berserk, but as she&#x27;s not the one who wrote it, she should be shielded from the blame if that ever happens reply bee_rider 8 hours agoparentprevIt would be nice if there was a general expectation by now that people would have a sort of ‚Äúintro to scripting‚Äù type class in high school. Not everyone has to be a programmer, in the same way not everyone needs to be an author, but everybody ought to be able to write a memo, email, or shopping list. reply eru 2 hours agorootparentLook at all the other classes people are already sitting through for high school and not learning anything either. What makes you think this one would be any different?Already around the world high schools all have something highfalutin like &#x27;teaching critical thinking&#x27; in their curriculum. reply TeMPOraL 59 minutes agorootparent> What makes you think this one would be any different?Because unlike almost all other classes, that one would be immediately beneficial in daily life, in a direct and apparent way. reply oytis 39 minutes agorootparentprevTeaching people a scripting language when the language has been already chosen for them, platform has been prepared, data preformatted etc. might be easy - a non-trivial share of my classmates couldn&#x27;t comprehend a for loop though.To have real transferrable programming skill - I am not talking about one that will get you a software engineering job, just basic problem solving - you need to know an awful lot about how computers work. Otherwise you&#x27;ll quickly get overwhelmed by errors you can&#x27;t comprehend at all stages , not being able to get data in right format, not knowing what tools are suitable for what problems etc. reply rottencupcakes 3 hours agorootparentprevWhat I learnt from the emergence of chatGPT was that way more people were allergic to writing even a small note than I thought. reply asdff 3 hours agorootparentprevTake it a step further and teach actual data analysis and statistics that are not coin flip&#x2F;compound interest plug and chug into the calculator problems. No matter what sort of job you do in life, being able to pull a csv from all your different credit or debit accounts and draw some inferences from that would be so useful. reply eru 2 hours agorootparentJust because something fancy (or even basic) is on the curriculum doesn&#x27;t mean that anyone is learning anything.No matter how well meaning, high school barely teaches most people anything in practice. reply throwaway14356 4 hours agoparentprevI use to point at office buildings and tell people non of the jobs there are real. They are all software that didn&#x27;t get written. One logistics friend argued his well paid job was very important but with very little help he automated it in 2 weeks, it was just the idea never occurred to him. i told him to not tell anyone but he did and 300 people got fired reply glandium 8 hours agoparentprevKind of a counterpoint: I&#x27;ve heard countless stories (albeit, in Japan) of people doing things like \"manually\" doing additions of numbers they already have in a spreadsheet instead of just asking Excel to do the addition in what, two clicks, because they \"can&#x27;t trust the computer to do it right\" which sounds like a BS rationalization to justify that they wouldn&#x27;t have much of a job if the computer was doing it. reply RajT88 8 hours agorootparentBack in 2009, I visited Tokyo for work. What I learned from the folks in my local office is the deployment software my company made tooling for was not very popular.The reasons were mainly that most of that software was made by Western (English speaking, specifically) countries and they did not trust it. They were not exactly wrong - software tends to work best on English systems, unless it was developed by a local company. I heard of some really embarrassing Korean translation issues that even as a person just with Google as my Korean language skill, I could validate the translation issue. Like Korean 101 kinda mistakes.So Japanese companies were using vastly inferior deployment software, which would basically RDP into an interactive session and replay inputs, because that software worked fine in Japanese....Or also very common was sneakernet deployment with DVD&#x27;s and thumbdrives, making the lowest workers do the running around.*I do not know the current state of software deployment in the Japanese market. reply MichaelZuo 1 hour agorootparentIf I was a busy decision maker at a major Japanese firm with a dozen direct reports and a hundred balls in the air simultaneously, and if I could even spot translation issues in a few minutes on the polished demo presented to upper management, then there&#x27;s no way I would ever assume anything is correct with the software when I&#x27;m not looking.Let alone my subordinate&#x27;s subordinate&#x27;s subordinate who would actually be using the software day in day out with their job on the line.So it&#x27;s a very sensible heuristic. If it&#x27;s supposed to be a serious enterprise software product, offered for sale in Japan, then spending a few million dollars to hire expert technical translators to double check everything should not be an issue. reply SoftTalker 8 hours agorootparentprevI mean not exactly the same thing but just last night Apple Maps was directing me to turn down a street that had clearly been closed for some time. Computers \"make mistakes\" all the time in most peoples&#x27; experience and after you get burned a few times (especially if you get badly burned, like getting fired over the \"mistake\") you learn to not blindy trust computers. reply gavinhoward 7 hours agorootparentprevIIRC, Excel uses floating point for calculations, so they&#x27;re not wrong. reply cratermoon 8 hours agorootparentprevPerhaps some folks here are too young to remember the Pentium FDIV bug[1], but it&#x27;s not complete BS to distrust the calculations.1 https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pentium_FDIV_bug reply bee_rider 8 hours agorootparentI‚Äôm sure FDIV-like bugs are absolutely eclipsed by typos and misclicks from doing things manually. reply TeMPOraL 55 minutes agorootparentYeah, but typos and misclicks don&#x27;t scale. What was that saying? \"To err is human, but to really foul things up you need a computer.\" reply userbinator 8 hours agoparentprevWhat the fuck are we doing as a society, that this lady, with a machine that could compute the above in a fraction of a second, has to spend a decent chunk of her work time doing this?To put it bluntly: do you want her to be unemployed instead? reply NikolaNovak 7 hours agorootparentI was going to say that - I&#x27;m increasingly subscribing to \"bullshit jobs\" theory :* productivity for goods is high and availability is high (we can discuss inequality and distribution)* we just don&#x27;t need everybody to work 40hrs a week to obtain collectively good standard of living* but as a society we are uncomfortable with universal basic income* so instead we basically do UBI through inefficient, unnecessary jobsThat&#x27;s my current perspective at least. As I continue to grow older and slide into cynicism though, I&#x27;m likely to switch to \"everybody is idiots and everything is screwed up\" theory more and more :-D reply hn_throwaway_99 6 hours agorootparent> so instead we basically do UBI through inefficient, unnecessary jobs.Your first 3 bullet points make sense, but this last one is where I think the normal theory behind \"bullshit jobs\" really falls apart. Every individual business has a large, strong economic incentive to not hire these bullshit jobs if they don&#x27;t need to, so why should we think they would be so generous to engage in this \"bullshit jobs charity\"?I think what is really happening is that as society advances, lots of rules, regulations and processes just build up and up over time, and this complexity eventually becomes self-sustaining, even if it greatly diverges from the original intent of the original rules.Case in point (which is a common case I know) is the complete, total insanity of the US healthcare system, specifically how healthcare is paid for. Literally every single person I know that has ever gotten into some aspect of healthcare payments (potentially with an idea to improve the madness) eventually comes to the conclusion \"The whole system is completely fucked, it should all just be torn down and greatly simplified.\" The problem, though, is now there are a ton of entrenched interests who depend on that complexity for their livelihood (I heard it referred to as \"an abusive relationship\" - Company A exists to \"simplify some aspects of healthcare payments\", which means they depend on the underlying complexity in the first place to exist), so there are no real incentives from the people that control the levers to simplify. So a lot of those bullshit jobs come about to manage that complexity, but it&#x27;s not like some company thought \"let&#x27;s hire people to do busywork so they&#x27;ll have employment.\" reply ludston 5 hours agorootparentWhilst business owners have a financial incentive not to employ people, there are competing incentives. For example, non-owners are incentivised to have more people reporting to them because it increases their status and status is usually correlated with salary. In fact, when you are rich enough that money doesn&#x27;t mean anything any more, you may as well have a bunch of people employed to do nothing but make you feel important and powerful. reply mvncleaninst 2 hours agorootparentprev> I think what is really happening is that as society advances, lots of rules, regulations and processes just build up and up over time, and this complexity eventually becomes self-sustaining, even if it greatly diverges from the original intent of the original rules.This is one theory, I think a slightly different explanation could be that most corporations are too large for the people making decisions regarding things like layoffs to be able to have a clear picture of what each employee is doingAlso like a sibling comment said, there are also conflicting incentives like middle management engaging in empire building. Because of this, there isn&#x27;t any vertical enforcement or clarityReally interesting how much better complexity scales in software than it does in business reply zweifuss 3 hours agorootparentprevYes, that&#x27;s how it is. For every reform that would benefit society as a whole, there is now a tiny minority of certain losers with a deeply entrenched lobby against the new and for the old. Be it fossil fuels, health care, banking, peace in the Middle East, nuclear technology, the use of genetic engineering in plant breeding, electric vehicles, and so on.I don&#x27;t think UBI would change that, but UBI might have a chance to change the perception of one&#x27;s job as a bullshit job (they say that&#x27;s 40% of the workforce). reply tempodox 5 hours agorootparentprevGP&#x27;s last point isn&#x27;t to be taken literally. It&#x27;s just the short and snappy summary of what took you many words to describe. reply opportune 7 hours agorootparentprevWho&#x27;s saying that lady&#x27;s job is bullshit just because it can be automated? There is real value in paying some up front cost in detection&#x2F;monitoring&#x2F;validation of data to prevent mistakes. Just because software engineers would consider her job \"beneath\" them because they could do it faster or better does not make it not worth doing.There are mega-efficient factory farms in Nebraska that are immensely productive per-human involved, but that doesn&#x27;t mean what hobby farmers in Connecticut or subsistence farmers in Africa are doing is bullshit.I think \"bullshit jobs\" are less of a thing than people believe. It&#x27;s just that people devalue assembly-line or cog-in-machine work, even when they&#x27;re the ones doing it - especially in the US where we grow up wanting to be a unique success story like an entertainment celebrity or rich entrepreneur and so that kind of work is antithetical to our idea of a successful person. Fact of the matter is, machines needs cogs, and we don&#x27;t have an infinite capacity to replace human cogs with automated ones. reply concordDance 5 hours agorootparentDoes she find that task enjoyable and fulfilling? If not, it&#x27;s bullshit.And if bet money on it NOT being enjoyable and fulfilling. Humans almost universally hate being cogs doing the same repetitive trivial action over and over. reply oldsecondhand 36 minutes agorootparentDo janitors find their job enjoyable and fulfilling? Probably not. Their job is still important for the functioning of society, so I wouldn&#x27;t call it bullshit. reply tuatoru 7 hours agorootparentprevThis is the \"lump of labor\" fallacy.If she turns up to work reliably, and stays on-task, there are no end of other things she could be doing that increase general welfare more than this. reply johnmaguire 7 hours agorootparentMaybe, but I&#x27;m not sure everyone has the inclination to become a programmer.And the whole thread was about how computers should work for everyone, including this lady whose job could be automated.Which begs the question: do most people actually want general purpose computing? Or does the average human prefer \"apps\"? reply ethbr1 7 hours agorootparentIn this scenario, she had plenty of other high-value work to perform if she never had to do this again.Which is the other side of the coin of the ubiquity of this problem -- modern businesses are so optimized on the people side that roles are collapsed down to 1-2 people, and therefore those people inevitably catch \"oh, and also do this\" could-be-automated work. reply tuatoru 7 hours agorootparentprevWho said anything about her becoming a programmer?An interior decorator, or a gardener, or a nurse aide, or a yoga instructor, or a nail technician: they all add more to human welfare than this task.If she wants to become an architect, or a water system engineer, more power to her! reply TeMPOraL 52 minutes agorootparentWhatever you list here, she&#x27;ll be competing for an entry-level position with much younger people, who have more time, more energy, lower expenses and no obligations.The older one is, the more time one spent in one line of work before being forced to find something else to do, the harder it hits. So sure, maybe you can switch to landscaping in your 50s, but that also means you and your family suddenly being kicked down one or two economic classes. reply rrr_oh_man 4 hours agorootparentprevNot disagreeing with your point in general, but:Not everyone can become an architect or water systems engineer at 50, after having worked a \"general assistant\" type office job for many years.I think that (and its consequences) might be the biggest short term societal risk of automation in an aging society.How would you solve this problem? reply asdff 3 hours agorootparentWhy can&#x27;t you become these things at 50? Considering you can become them at 23, having worked perhaps in fast food for a few years prior? reply auggierose 58 minutes agorootparentGood question! Wait until you are 50, maybe you can answer it then ;-) reply tuatoru 3 hours agorootparentprevIf there are resources available, some entrepreneur will figure out a way to make use of them. reply asdff 3 hours agorootparentprevIf the average human were allowed to achieve its most base preferences, we&#x27;d all be a half ton in weight, floating around in chairs, stupefied at some screen, just as depicted in Wall-e. reply asdff 3 hours agorootparentprevShe will not be unemployed in this situation, nor will her peers in similar situations. Available labor is always gobbled up and not left idle for long. Case in point: we&#x27;ve obviated jobs like the horseshoe maker and the stable boy and yet unemployment rate today at 3.8% or so is half of what it was in the 1900s when we had all these horse and stable jobs. reply ffgjgf1 43 minutes agorootparentYou have to also take into account income not just the unemployment rate. Growth has been close to non-existent if not negative over the last 40-50 years or so if earn below the median. reply yard2010 1 hour agorootparentprevJobs do not disappear, they just change reply opportune 8 hours agoparentprevBut surely she had the tools to do that in excel itself, no? This is something I see a lot in software, where people devalue and trivialize the skills and abilities needed to make software simply because they know something is possible to do in software and because, as a software worker, they most likely had a natural predilection towards the kind of thought processes that led them toward pursuing software work.I think it&#x27;s fallacious to think everybody has the natural disposition and curiosity to want to write software. Writing has been around forever, and most people don&#x27;t want to do it beyond purely functional communication, and even those who do want to go beyond that are mostly terrible at it. That lady could have automated her job if she really wanted to and was motivated enough to do it; she simply preferred to be a ditch digger than to design a ditch digging machine.It doesn&#x27;t help that \"designing ditch digging machines\" is in such high demand that most companies are really only able to hire mediocre people to fill those roles. Leonardo da Vinci is not working on sox compliance at a fucking bank.There&#x27;s a good chance that the payoff for that bank automating \"take a set of dates in an Excel file, search each one individually into an internal search tool, then Ctrl+f the result report and verify that every code listed in the original document exists in the report\" is not even there, because the first attempt will have insufficient monitoring or verifications to catch silent failures and introduce an expensive issue caught months after the fact that leads to a huge remediation scramble and requires more development than initially expected to address, not to mention the constant need for maintenance in updating the software and making sure it continues to run (which also involves a rotating cast of programmers joining, needing to learn the software, and leaving). Leonardo might do it right the first time, but he&#x27;s not doing it for cheap and not going to make it his life&#x27;s work. Joe Schmoe can do it cheaper but it&#x27;s going to take a while, have an ongoing maintenace burden, and probably some mistakes made along the way. That lady can do it for even cheaper than Joe Schmoe can without many headaches. reply conception 7 hours agorootparentYou see this in manufacturing as well. You‚Äôd think robots would be everywhere but often, like way more than you‚Äôd imagine, it‚Äôs cheaper to hire a human to do it. reply asdff 3 hours agorootparentI think part of that is how these sorts of businesses choose to organize their money. A manufacturer might budget themselves to be quite lean, so lean that for any sort of improvement like automation with robots, they have to hire an expensive consultant to tell them they have to hire an expensive robot company to build them expensive robots with expensive service contracts. So of course in that model the human is cheaper. However, if the manufacturer instead took all that money they would have spent on the expensive consultant, and opened up their own \"internal\" automating wing that would just look for things within the company to replace with a robot they create and service internally, maybe the math would pencil out another way. reply opportune 7 hours agorootparentprevYeah, it&#x27;s easy to forget that humans themselves are pretty amazing technology. You can program them with relatively vague instructions using natural language, they come with built in general-intelligence so you can delegate things for them to figure it out and expect them to adapt their instructions according to their situation, and they come with amazingly precise and dextrous arms. They generally cost under $150k&#x2F;unit&#x2F;year for most manual applications and you pretty much know what you&#x27;re getting up front vs hardware&#x2F;software purchased off the shelf, not to mention the capital cost of acquiring one is pretty low because they already exist and just need to be hired. reply tuatoru 7 hours agorootparentprevThe way fertility rates are going, not for much longer. reply SanderNL 2 hours agoparentprevSomeone told me their job was mainly checking Word documents for discrepancies. Basically diffing them, but manually, by reading.I showed them how you could diff them automatically. This was not appreciated.‚ÄúCool, IT dude, now I lost my job.‚Äù And they were probably right if anybody cared enough to look at what they actually do. reply MrLeap 8 hours agoparentprevA large amount of inefficiency is actually a chain of accountability. Trust, and failing that assigning blame, and trusting those assignments, is the root of so. much. reply mnky9800n 3 hours agoparentprevAnd don&#x27;t forget that automating this process could also include a host of error checking she is simply unable to do because she is a human and not a computer. reply karmakaze 8 hours agoparentprevThis sounds like a perfect use-case for automation using recorded macros. The failure (or perhaps solutions exist) is lack of facilities in our desktop OSes. reply glandium 8 hours agorootparentAFAIK, macOS is the only OS to come with this sort of things out of the box (and that is supposed to work consistently across all applications; good luck with that on Linux).https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Automator_(macOS)https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;AppleScript reply karmakaze 8 hours agorootparentThat is what I had in mind as I wrote it. Sad that many macOS apps now are built with Electron, or we simply use web browsers to access public or internal apps. reply depressedpanda 1 hour agorootparentUsing web browsers to access apps is actually a boon in this regard: the whole UI is exposed via the DOM, and you can use JavaScript to interact with it for macros -- and it&#x27;s cross platform as it works on any OS that can run a web browser!In contrast Automator&#x2F;AppleScript only works on macOS afaict. reply dr_kiszonka 6 hours agorootparentprevThere is a free Power Automate Desktop for Windows. reply salawat 7 hours agorootparentprevMacros are generally locked down in enterprise network environments due to the risk of acting as a malware vector. reply dingi 1 hour agoparentprevThat lady could have automated that with just a little glue code. Let the program do the work and use that time to do nothing. Nobody&#x27;s complaining. reply waterheater 9 hours agoprevThis fragmentation was predicted well over a decade ago, but we&#x27;re finally seeing mass market realization of these predictions.The most relevant idea here is Koomey&#x27;s Law [1], which postulates that the \"number of computations per joule of energy dissipated doubled about every 1.57 years.\" This law is essentially a combination of Moore&#x27;s Law and Dennard scaling. As transistors get smaller, Moore&#x27;s Law predicts increases in the number of transistors on a chip, while Dennard scaling predicts decreasing power usage per transistor. For many years, Moore&#x27;s Law really just tracked with Dennard scaling; CPU on-die areas weren&#x27;t getting substantially larger (to my knowledge), but transistors were getting smaller and more efficient, so more could fit in a given area.However, Dennard scaling also says that the power density of a transistor remains constant. As transistors became smaller, more of the die became occupied by transistors, causing more heat dissipation per unit area. After exhausting cooling options (think back to the TDP of those CPUs back in the late-90s&#x2F;early 2000s), Dennard scaling died in the mid-to-late 2000s because of thermal issues.However, just because heat can&#x27;t be dissipated doesn&#x27;t mean CPU manufacturers won&#x27;t try to cram more and more transistors in a given area. As a result, a modern CPU will not use all its transistors at once because it would overheat, creating \"dark silicon\". Once in that paradigm, the next logical design approach was prioritizing application-specific transistor clusters, the extent of which we&#x27;re now seeing.While I generally agree with the authors, the key in these circumstances is to find ways to combine the fragments and make a better whole. Perhaps we&#x27;ll have a \"brain board\" attached to the motherboard, where sockets on the brain board allow user-specific processor cores to be installed. Not everyone needs an i9 or even an i7, and maybe not everyone needs advanced ML performance.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Koomey%27s_law reply Qwertious 5 hours agoparentThe dark silicon isn&#x27;t necessarily application-specific, it&#x27;s also used by duplicating a particular circuit and then rotating which duplicate does the computing, so that the other areas have time to cool down a bit before they&#x27;re used again. reply blueblimp 5 hours agoprevI question the article&#x27;s framing of CPUs as \"universal\" and GPUs as \"specialized\". In theory, they can both do any computation, so they differ only in their performance characteristics, and the deep learning revolution has shown that there is wide range of practical workloads that is non-viable on CPUs. The reason OpenAI runs GPT-4 on GPUs isn&#x27;t that it&#x27;s faster than running it on CPUs--they do it because they _can&#x27;t_ practically run GPT-4 on CPUs.So what&#x27;s going on is not a shift away from the universality of CPUs, but a realization that CPUs weren&#x27;t as universal as we thought. It would be nice though if a single processor could achieve the best of both worlds. reply tjoff 51 minutes agoparent> [...] differ only in their performance characteristics [...]... but that is exactly why CPUs are considered \"universal\" and GPUs as \"specialized\".The whole concept of specialized hardware is to do fewer things more efficient, and in tons of applications that means the problem suddenly becomes feasible. That has always been the case. Not sure what the deep learning revolution has shown in regards to this. reply benrutter 3 hours agoprevI don&#x27;t know if there&#x27;s good justification for this yet. The claim that computers are shifting from general (CPU) chips to more specialized (GPU) chips.I&#x27;m not sure classifying GPU as \"specialized\" is right- it&#x27;s essentially a chip that can do lots of small computations, rather than big ones like the CPU does. To me, the trend looks more like a shift from centralized to distributed models of computing. Which I think is also backed up by data processing tools like spark which distribute over multiple computers.I saw a \"SQL process unit\" announced recently[1] which I guess really is a move towards specialized compute. I haven&#x27;t heard of much uptake in it yet though, so I guess time will tell.[1] https:&#x2F;&#x2F;www.neuroblade.com&#x2F;product&#x2F; reply karmakaze 8 hours agoprevI largely don&#x27;t blame the specialized processors, they exist to fill a need. The transition to mobile&#x2F;tablet has been very much a producer-consumer relationship. In the old days, normal users would one day \"View source\" and dabble in HTML and making their own web pages and journey into making their own programs. There&#x27;s no parallel to this on mobile today. Perhaps the next generation that grew up on Roblox and using yet to successful end-user visual development tools will.Is there a web-app like MS Access or Visual Basic today, maybe Airtable, others? reply dang 7 hours agoprevDiscussed at the time (of the article):The decline of computers as a general-purpose technology - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=26238376 - Feb 2021 (218 comments) reply seydor 5 hours agoprevditch your phone , go back to your desktop, problem solve. Mobile has been a regression in pretty much everything except corporate profits.The focus of the article is wrong. PCs for example are not getting more specialized, they are getting new capabilities with the additional specialized gpus. As others have said the problem does not lie in processors, but on the software and UI, which has been steadily dumbing down since \"smart\"phones have been introduced. Phones have perfectly capable general computing processors but corporate decisions limit their usage. reply sinuhe69 3 hours agoprevThe tittle is misleading. It should read: The Decline of CPU As a General Computing Device.But then, I guess not many people would mind it.A computer? I consider all forms of computing units today as computer: micro-controller, single board computer, CPU +&#x2F;- GPU +&#x2F;-Neutral Engines etc.What about the math-coprocessor in the 90s? Did they cause a decline of computer as a general computing device? Of course not. reply RachelF 8 hours agoprevThe failure is primarily due to stagnation in single core speed improvements since 2003, compared to previous decades.Yes we have more cores now, and single cores have got faster, but nothing like the scale of the 1990&#x27;s when raw clock speeds increased 150x.Moore&#x27;s law is about the number of transistors, not the speed of the device. More cores are good, but software is hard to parallelize, despite decent efforts from the CS community.There have also been efforts to put FPGAs into general purpose CPUs. These have also not taken off.For most users, a single core CPU running at 30GHz would appear faster than 32 cores running at 2GHz. reply kaycebasques 4 hours agoprev> the economic cycle that has led to the usage of a common computing platform, underpinned by rapidly improving universal processors, is giving way to a fragmentary cycle, where economics push users toward divergent computing platforms driven by special purpose processorsWhere does RISC-V and all its extensions land on the GPTfragmentation spectrum? reply opportune 8 hours agoprevIt&#x27;s way too premature to say we are turning into fragmentation cycle regarding bespoke hardware. GPUs are still universal computers, they just use a different model with different performance characteristics (and applications, which stem from these performance characteristics and also inertia&#x2F;maturity of tooling) than CPUs. ASICs and specialized processors have been a thing for a long time and their application towards some \"new\" things like crypto, hft, and a handful of cloud optimizations is hardly a new trend when seen in the larger context of their being used all the time for now-boring, once also-new technologies like networking hardware and embedded applications.I&#x27;d argue that the last three decades have actually had massive shifts towards more generalized and ubiquitous computing that may continue even further. First, with the introduction of PCs the computing landscape shifted away from bespoke hardware and software to more standardized plug-and-play hardware and software. With the internet, everybody got a middleware called a \"web browser\" that standardized UI development on a single computing model. With mobile computing, we got a more restricted model of userspace that standardized what applications could do and even how (app review, software signing) you could do it. Cloud did the same things to various extents to server software. Except for at the height of Windows&#x27; PC dominance, it&#x27;s never been easier to write software usable by such a large portion of the market, and the absolute number of devices and users that applies to is probably an order of magnitude more than existed during peak-Windows.Everybody in tech knows single-core cpu performance gains are slowing down and that the remaining incremental improvements will eventually end too, so what comes next is on peoples&#x27; minds. IMO this article is jumping the gun though - it&#x27;s still way too big an undertaking to use specialized computers (and I don&#x27;t count GPUs as these) for all but the largest scale or most performance sensitive use cases, as it&#x27;s always been. reply 8note 7 hours agoparentThe last two decades at least have moved towards a purpose built consumer device, with a general purpose server.Gpus, CPUs, and Asics, are run by vendors rather than end customers. The end customer runs a full computer, but has no rights to it because it&#x27;s called a smart phone rather than a computer reply williamtrask 8 hours agoprevMachine learning is a massive counter example to this trend. Specialised deep learning hardware is just supporting an even more general GPT. reply deafpolygon 2 hours agoprevThe decline is due to vested interests. More specifically: to make money. reply Madmallard 3 hours agoprev [‚Äì] Is this because people in efforts to secure their long-standing jobs have made software more complex and less effective? replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article highlights the slowing progress in general purpose CPUs, leading to a shift towards specialized processors like GPUs for their performance and power efficiency benefits.",
      "However, this shift has drawbacks such as higher costs, programming complexity, and possible hindrances to the overall development in computing and universal processor production economics.",
      "Furthermore, the article examines the challenges and implications of this trend, including increased costs, fragmentation, potential effect on job markets, and competition, emphasizing the need for a new computation model."
    ],
    "commentSummary": [
      "The article discusses the decrease in the use of computers as a universal technology, citing the failure of software to create effective tools for those without programming skills.",
      "There's a focus on the potential shifts towards automation, enhanced education around scripting and data analysis, and the emerging trend towards specialized processors over universal Central Processing Units (CPUs).",
      "Also discussed is the complex nature of software, possible fragmentation of computing platforms, and challenges of employment in an increasingly automated industry."
    ],
    "points": 160,
    "commentCount": 100,
    "retryCount": 0,
    "time": 1697923447
  },
  {
    "id": 37967751,
    "title": "Keju, China‚Äôs difficult civil service test",
    "originLink": "https://aeon.co/essays/why-chinese-minds-still-bear-the-long-shadow-of-keju",
    "originBody": "√ó CLOSE Philosophy Science Psychology Society Culture Essays Videos Audio Popular About DONATE NEWSLETTER MENU DONATE NEWSLETTER SIGN IN The exam that broke society Keju, China‚Äôs incredibly difficult civil service test, strengthened the state at the cost of freedom and creativity Excerpt from the scroll Viewing the Pass Lists, traditionally attributed to Qiu Ying (1494-1552). National Palace Museum, Taipei/Wikipedia Yasheng Huang is Epoch Foundation professor of global economics and management at MIT Sloan School of Management. He is the author of many academic papers and 11 books in both English and Chinese, the latest of which is The Rise and Fall of the EAST (2023). Edited bySam Haselby 4,000 words SYNDICATE THIS ESSAY 15 Comments Email Save Tweet Share On 7 and 8 June 2023, close to 13 million high-school students in China sat for the world‚Äôs most gruelling college entrance exam. ‚ÄòImagine,‚Äô wrote a Singapore journalist, ‚Äòthe SAT, ACT, and all of your AP tests rolled into two days. That‚Äôs Gao Kao, or ‚Äúhigher education exam‚Äù.‚Äô In 2023, almost 2.6 million applied to sit China‚Äôs civil service exam to compete for only 37,100 slots. Gao Kao and China‚Äôs civil service exam trace their origin to, and are modelled on, an ancient Chinese institution, Keju, the imperial civil service exam established by the Sui Dynasty (581-618). It can be translated as ‚Äòsubject recommendation‚Äô. Toward the end of its reign, the Qing dynasty (1644-1911) abolished it in 1905 as part of its effort to reform and modernise the Chinese system. Until then, Keju had been the principal recruitment route for imperial bureaucracy. Keju reached its apex during the Ming dynasty (1368-1644). All the prime ministers but one came through the Keju route and many of them were ranked at the very top in their exam cohort. Keju was sheer memorisation. Testing was based primarily on the Confucian classics. And there was a lot to memorise. There were some 400,000 characters and phrases in the Confucian classics, according to Benjamin Elman‚Äôs book A Cultural History of Civil Examinations in Late Imperial China (2000). Preparation for the Keju began early. Boys aged as young as three to five began to practise their memorisation drills. After the immediate environs of their families, Keju was their first exposure to the world. Keju, which was open only to the male gender, was fiercely competitive. Using figures provided by Elman, during the Ming dynasty, 1 million regularly took the qualifying tests and, of these, eventually about 400 would make it to the final Jinshi round. Passing the first tier of Keju, known as the provincial exam, was a lot easier ‚Äì working out to be 4 per cent on average during the Ming. Still, this was more cut-throat than getting into Harvard in most years. Join over 250,000+ newsletter subscribers. Our content is 100 per cent free and you can unsubscribe anytime. Daily: A daily dose of essays, guides and videos from Aeon+Psyche Weekly: A week‚Äôs worth of big ideas from Aeon Subscribe for free I‚Äôm already subscribed Privacy policy T he prestige of Keju was such that even an emperor coveted its bona fides. According to a legend, an emperor in the late Tang dynasty (618-907) hung on the wall of an imperial palace a wooden tablet proudly displaying his Keju degree ‚Äì only it was fake. The emperor had it made for himself. This credentialism pervades officialdom today. Many Chinese government officials claim PhD degrees ‚Äì earned or otherwise ‚Äì on their r√©sum√©s. Much of the academic literature focuses on the meritocracy of Keju. The path-breaking book in this genre is Ping-ti Ho‚Äôs The Ladder of Success in Imperial China (1962). One of his observations is eye catching: more than half of those who obtained the Juren degree were first generation: ie, none of their ancestors had ever attained a Juren status. (Juren was, at the time, the first degree granted in the three-tiered hierarchy of Keju.) More recent literature demonstrates the political effects of Keju. In 1905, the Qing dynasty abolished Keju, dashing the aspirations of millions and sparking regional rebellions that eventually toppled China‚Äôs last imperial regime in 1911. Keju cultivated and imposed the values of deference to authority and collectivism The political dimension of Keju goes far beyond its meritocracy and its connection to the 1911 republican revolution. For an institution that had such deep penetration, both cross-sectionally in society and across time in history, Keju was all encompassing, laying claims to the time, effort and cognitive investment of a significant swathe of the male Chinese population. It was a state institution designed to augment the state‚Äôs own power and capabilities. Directly, the state monopolised the very best human capital; indirectly, the state deprived society of access to talent and pre-empted organised religion, commerce and the intelligentsia. Keju anchored Chinese autocracy. Candidates queue for the national civil service examination on 27 March 2021 in Taiyuan, Shanxi province, China. Photo by Wu Junjie/China News Service via Getty The impact of Keju is still felt today, not only in the form and practice of Gao Kao and the civil service exam but also because Keju incubated values and work ethics. Today, Chinese minds still bear its imprint. For one, Keju elevated the value of education and we see this effect today. A 2020 study shows that, for every doubling of successful Keju candidates per 10,000 of the population in the Ming-Qing period, there was a 6.9 per cent increase in years of schooling in 2010. The Keju exams loom as part of China‚Äôs human capital formation today, but they also cultivated and imposed the values of deference to authority and collectivism that the Chinese Communist Party has reaped richly for its rule and legitimacy. But isn‚Äôt it the case that the West ‚Äì Prussia, then the United Kingdom and the United States ‚Äì all had their own civil service exams? How is it possible that a strong bureaucracy complemented rather than supplanted political and religious pluralisms in the West? C hina and the West bureaucratised under an entirely different sequential order and under different contextual conditions, and these differences entail substantial implications for the subsequent political development. The civil service in the West was not a single-platform institution in the way that Keju was. There was a military civil service, a civil service for foreign affairs, for forestry, etc, etc. Multiple platforms of bureaucratic recruitment competed with one another and, collectively, they competed with other channels of mobility, such as the political parties and commerce. In the US, the Pendleton Act of 1883 removed the power of Congress and the political parties to control civil service appointments. Before the 1883 Act, federal appointees returned a portion of their salaries to the party that had appointed them. Civil service never replaced Congress or political parties in toto, as witnessed by the fact that Congress today wields enormous power over the bureaucracy, including the power of the purse that funds its operation. Another difference ‚Äì and this is a big one ‚Äì is timing. In the 19th century, the US introduced bureaucracy when ‚Äò[t]he two institutions of constraint, the rule of law and accountability, were the most highly developed,‚Äô as Francis Fukuyama writes in Political Order and Political Decay (2014). The state in the US and the UK was already ‚Äòa Shackled Leviathan‚Äô, to use the words of Daron Acemoglu and James A Robinson in their influential book, The Narrow Corridor (2020). The sequential order ran from politics to bureaucracy, not as in China from bureaucracy to politics. In the West, society was vibrant long before the state ramped up its administrative capacity. The rule of law, the principle of accountability, and the powers of the legislature and the political parties were already firmly entrenched. Yes, the Leviathan was shackled by society, but different parts of the Leviathan shackled each other. Bureaucracy in the US formed and gained power only under a myriad of constraints and contending forces, rather than the socioeconomic tabula rasa that greeted the arrival of Chinese bureaucracy. Vladimir Putin‚Äôs autocracy pales in comparison with that of China‚Äôs president Xi Jinping The civil service in the UK and the US was ensconced in pluralistic societies that enjoyed a degree of religious freedom and a modicum of emergent electoral democracy. A world of competing forces and constraints attended the arrival of bureaucracy, even helped to create it. Government bureaucracy competed in some situations or complemented in others with church, universities, commerce and other social groups for human capital, legitimacy and resources. For political development, birth order really matters. In his book Strong Societies and Weak States (1988), Joel S Migdal identifies a common problem in the developing world ‚Äì the struggle of the state to acquire autonomy and capabilities. China, through history and today, is exactly the opposite. The state dominates society. Vladimir Putin‚Äôs Russia is autocratic but his autocracy pales in comparison with that of China‚Äôs president Xi Jinping. Harassed and targeted by the state, opposition parties are still legal and tenuously legitimate in Russia and some of Putin‚Äôs critics command a sizeable following. Even the power to commit violence ‚Äì war fighting ‚Äì was outsourced to a private force, the mercenaries led by Yevgeny Prigozhin, an arrangement not even remotely conceivable in China. Last-minute revision before the 2010 civil service examination in Hefei, Anhui province, China. Photo AFP/Getty Since 2013, against the increasingly dictatorial Xi, there have been two prominent critics of the president and both were dispensed with summarily. Unlike Putin who has to rely on extra-legal means to silence his critics, suggesting some formal constraints on him, Xi directed the full apparatus of the Chinese state after his critics. The Chinese court sentenced the businessman Ren Zhiqiang to 18 years in prison, and Tsinghua University promptly fired Xu Zhangrun, a law professor who wrote an open letter criticising Xi. Standing forlornly by themselves, neither Ren nor Xu commanded any formal political organisations behind them. In 2022, the Chinese regime put almost 400 million people under some sort of COVID-19 lockdown, a feat that is unimaginable in any other country. A n ultimate autocracy is one that reigns without society. Society shackles the state in many ways. One is ex ante: it checks and balances the actions of the state. The other is ex post. A strong society provides an outside option to those inside the state. Sometimes, this is derisively described as ‚Äòa revolving door‚Äô, but it may also have the positive function of checking the power of the state. State functionaries can object to state actions by voting with their feet, as many US civil servants did during the Donald Trump administration, and thereby drain the state of the valuable human capital it needs to function and operate. A strong society raises the opportunity costs for the state to recruit human capital but such a receptor function of society has never existed at scale in imperial China nor today, thanks ‚Äì in large part, I would argue ‚Äì to Keju. Keju was so precocious that it pre-empted and displaced an emergent society. Meritocracy empowered the Chinese state at a time when society was still at an embryonic stage. Massive resources and administrative manpower were poured into Keju such that it completely eclipsed all other channels of upward mobility that could have emerged. In that sense, the celebration by many of Keju‚Äôs meritocracy misses the bigger picture of Chinese history. It is a view of a tree rather than of a forest. The crowding-out effect of Keju is captured succinctly in a book from the late 19th century: Since the introduction of the examination system ‚Ä¶ scholars have forsaken their studies, peasants their ploughs, artisans their crafts, and merchants their trades; all have turned their attention to but one thing ‚Äì government office. This is because the official has all the combined advantages of the four without requiring their necessary toil ‚Ä¶ This is the larger impact of Keju. Its impressive bureaucratic mobility demolished all other mobility channels and possibilities. Keju was an anti-mobility mobility channel. It packed all the upward mobility within one channel ‚Äì that of the state. Society was crowded out, and over time, due to its deficient access to quality human capital, it atrophied. This is the root of the power of Chinese autocracy and, I would argue, it is a historical development that is unique to China and explains the awesome power of Chinese autocracy. China has legions of intellectuals, but it is bereft of an intelligentsia Take intellectuals as an example. Keju inculcated literacy and helped create a vibrant book readership. Book ownership was widespread as early as the Ming dynasty. ‚ÄòMore books were available,‚Äô writes Timothy Brook in The Troubled Empire (2010), ‚Äòand more people read and owned more books, in the late Ming than at any earlier time in history, anywhere in the world.‚Äô Brook sums up the impressions of Jesuits visiting China: ‚ÄòMore surprising, perhaps, is that complete illiterates may well have been a minority in the late Ming.‚Äô But a striking fact is that no organised intelligentsia of any significant size and visibility ever emerged in imperial China. There were no Chinese equivalents of the Royal Society in Britain or the many learned societies in France. One that left a mark is the Donglin Academy, a private discussion forum founded in 1111 by intellectuals of the Song dynasty (960-1279). The academy lasted as long as its founders‚Äô lifespan and vanished into obscurity after their expiry. It was revived in 1604 during the reign of the Wanli emperor (1573-1620), but it operated as a political rather than an intellectual force. The scholar-officials formed a Donglin Faction, later brutally put down by the powerful eunuchs of the Ming court. The grand total of the second life of the Donglin Academy is 21 years, from 1604 to 1625. The term ‚Äòscholar official‚Äô is of Chinese coinage and it is evocative of China‚Äôs lacuna of intellectuals as an institutionalised establishment. Compare that situation with Tsarist Russia, another autocracy. Russians coined the term ‚Äòintelligentsia‚Äô ‚Äì intellectuals as a class ‚Äì and Russian intellectuals have a long tradition of standing apart from and defining their identity as separate to the state. China has legions of intellectuals, but it is bereft of an intelligentsia. Prior to Keju and even during the early centuries of Keju, China had a plurality of upward mobility. Within bureaucracy, officials were appointed through nepotism, family ties, heredity and recommendations. Commerce, while always curtailed, was a nascent force, promising to burst forward. The Song dynasty experienced a vibrant development of commerce and a market economy. Although Confucianism was always the first among equals, other ideologies, such as Legalism, Daoism and Buddhism, cohabitated with Confucianism and vied with one another for the Chinese population‚Äôs attention and adherence. But these societal forces were too nascent and too embryonic by the time Keju arrived and matured. They had yet to acquire their own unique identity, significant organisation and autonomous agency. In imperial China, there never was a level playing field between state and society, and over nearly 1,500 years, Keju further deprived the congenitally deficient society of its oxygen ‚Äì human capital. Fukuyama is right to assert that the Chinese state was precocious, but it was precocious in a particular fashion: its precocity contrasted sharply with the immaturity of Chinese society. T he most direct way Keju decimated Chinese society is through talent monopoly but there were others. Keju also monopolised the time and mental energy of its candidates. Keju was not a one-shot deal. A candidate could take the test multiple times. In a dataset that has information on the 11,706 Keju candidates during the Ming dynasty, the average age passing the final stage of Keju was 32, approaching middle age at a time when average life expectancy was much lower than today. The oldest in the dataset was was probably Gui Youguang (1506-1571). Before passing the provincial examination in 1540 at the youngish age of 34, Gui had already failed it on six occasions. He then proceeded to toil for more than 24 years of his life and finally attained his Jinshi degree in 1565, although ranking near the bottom of his class and at the ripe age of 59. Unfortunately, he did not bask in his exalted status for long, as he died aged 65. For him, and many others, Keju was a life-long endeavour. View of the examination cells in Canton. Library of Congress Examination hall in Canton. Library of Congress Jiangnan imperial examination centre, Nanjing, c1913. Courtesy Historical Photos of China 1 of 3 The Keju curriculum was formidable and required memorising close to 400,000 characters. Is there spare residual energy, capacity and curiosity left to pursue other mentally taxing activities, such as ideation of new thoughts, new politics, and discoveries of natural phenomena? In my book The Rise and Fall of the EAST (2023), I show that Chinese technology began to stagnate as Keju gained dominance. The brain power that ended up in the state did not flow to Chinese society, the economy or human creativity. Mental energy aside, the values drilled deeply into Keju candidates were pro-autocracy and authoritarian. Keju legitimates statism. Boys as young as three or four began to practise writing characters that were meant to instil admiration of, and devotion to, the ideas and teachings of the master ‚Äì Confucius ‚Äì which would eventually be tested on Keju. By the Ming dynasty, the initial plurality of the Keju subjects gave way to one subject only, Confucianism ‚Äì ‚Äòknowledge of classics, stereotyped theories of administration, and literary attainments‚Äô. Autocracy and Keju became ever more intimately intertwined Imagine repeated exposures to the statist values at that tender age, producing what psychologists call ‚Äòan imprinting effect‚Äô. The autocratic values were incubated in substance but also by the format of Keju; this was standardised testing par excellence. When Keju was first established, candidates were tested on a wide range of subject matters but, after the Song dynasty, the Keju curriculum became progressively stratified and exceedingly narrow. Candidates were required to fill in the blanks with missing words or phrases in excerpted texts from the Confucian classics. The Yuan dynasty (1271-1368) narrowed the Keju curriculum further. Only a streamlined version of annotations of Confucian classics was allowed, the so-called Neo-Confucianism, which was the brainchild of the great Confucian scholar Zhu Xi (1130-1200) of the Song dynasty. Neo-Confucianism is a pared-down version of classical Confucianism, and it strips away some of the moral veneer of its classical predecessor. Summarising a common view among historians, Peter K Bol observes in Neo-Confucianism in History (2010) that this version of Confucianism ‚Äòprovided a justification for seeking external authority in the ruler‚Äô and stipulated the responsibility for transforming the world as that of the emperor alone. The Neo-Confucianist Keju curriculum was rigid, narrow and absolutist, and was single-minded in its advocacy of a hierarchical order ‚Äì subordination to the ruler, to the elderly, and to the male gender. No scope for scepticism and ambiguity was allowed. Autocracy and Keju thus became ever more intimately intertwined. There was, however, a massive operational advantage to the Neo-Confucianist curriculum: it standardised everything. Standardisation abhors nuance and the evaluations became more straightforward as the baseline comparison was more clearly delineated. There was objectivity, even if the objectivity was a manufactured artefact. The Chinese invented the modern state and meritocracy, but above all the Chinese invented specialised standardised testing ‚Äì the memorisation, cognitive inclination and frame of references of an exceedingly narrow ideology. M ing standardised Keju further: it enforced a highly scripted essay format, known as the ‚Äòeight-legged essay‚Äô, or baguwen in Chinese (ÂÖ´ËÇ°Êñá), to which every Keju candidate had to adhere. A ‚Äòleg‚Äô here refers to each section of an essay, with a Keju essay requiring eight sections: 1) breaking open the topic; 2) receiving the topic; 3) beginning the discussion; 4) the initial leg; 5) the transition leg; 6) the middle leg; 7) the later leg; and 8) conclusion. The eight-legged essay fixed more than the aggregate structure of exposition. The specifications were granular and detailed. For example, the number of phrases was specified in each of the sections and the entire essay required expressions in paired sentences ‚Äì a minimum of six paired sentences, up to a maximum of 12. The key contribution of the eight-legged essay is that it packed information into a pre-set presentational format. Standardisation was designed to scale the Keju system and it succeeded brilliantly in that regard, but it had a devastating effect on expositional freedom and human creativity. All elements of subjectivity and judgment were taken out. In his book Traditional Government in Imperial China (1982), the historian Ch‚Äôien Mu describes the ‚Äòeight-legged essay‚Äô as ‚Äòthe greatest destroyer of human talent‚Äô. A bane to human creativity was a boon to autocracy. Standardised testing was conducive to authoritarianism. In his book Who‚Äôs Afraid of the Big Bad Dragon? (2014), Yong Zhao, professor at the School of Education of the University of Kansas, notes a natural compatibility between authoritarianism and standardised testing. Authoritarianism, he writes, ‚Äòsees education as a way to instil in all students the same knowledge and skills deemed valuable by the authority.‚Äô The standardised tests appeal to an authoritative body for correct answers; as Zhao said in an interview for the US National Education Policy Center, the tests ‚Äòforce students to comply with the answers or the way of thinking that the authority wants.‚Äô The direction of deference is automatically established: ‚ÄòThen you hold the students, the teachers and, to a lesser extent, the parents accountable for being able to get the answers that the authority wants and to show that they have mastered the skills and the knowledge and possibly even the beliefs that the authority wants.‚Äô Confucianism, thus, functioned as an equivalent of the abstruse and arcane vocabulary of the SAT In his book The WEIRDest People in the World (2020), Joseph Henrich posited that the West prospered because of its early lead in literacy. Yet the substantial Keju literacy produced none of the liberalising effects on Chinese ideas, economy or society. The literacy that Henrich had in mind was a particular kind of literacy ‚Äì Protestant literacy ‚Äì and the contrast with Keju literacy could not have been sharper. Keju literacy was drilled and practised in classical and highly stratified Chinese, the language of the imperial court rather than the language of the masses, in sharp contrast to Protestant literacy. Protestant literacy empowered personal agency by embracing and spreading vernaculars of the masses. Henrich‚Äôs liberalising ‚ÄòWEIRD‚Äô effect ‚Äì Western, educated, industrialised, rich and democratic ‚Äì was a byproduct of Protestant literacy. It is no accident that Keju literacy produced an opposite effect. Why was there such a close affinity between Keju and Confucianism? The answer is not obvious. Ancient China boasted other great ideologies and traditions, such as Daoism, Mohism and Legalism, but they were completely absent in the Keju curriculum. This ideological single-mindedness of Keju is puzzling and it is puzzling still considering the following: in my book, I document that several emperors who played an instrumental role in inventing and developing Keju were not Confucianists themselves. The answer may lie in an operational imperative of Keju. Standardised testing is necessary when you want to scale the evaluation. Subjective evaluations, such as relying on reputation, recommendations and interviews, are feasible when the number of candidates under evaluation is small. For example, the Big Three colleges in the US ‚Äì Harvard, Yale and Princeton ‚Äì began to embrace the SAT (the standardised test for college admissions) when they started recruiting beyond their traditional, narrow socioeconomic group ‚Äì the white Anglo-Saxon Protestants (WASPs) in the elite private schools of the east coast. The Chinese emperors made the same decision when they expanded bureaucratic recruitment beyond the nobility and wealthy elites. Standardising and constricting the Keju curriculum were not an optional luxury; it was a necessity to scale Keju. Confucianism offered an operational advantage. It is textually rich; the verbiage is massive, and the pontifications are incredibly involved, not unlike the verbal portion of the SAT. As noted before, there are approximately 400,000 characters and phrases in the Confucian classics. Using a website, Chinese Text Project, ‚Äòan online open-access digital library that makes pre-modern Chinese texts available to readers and researchers all around the world‚Äô, I found that among the classical texts created before the Han dynasty (206 BCE-220 CE) Confucianism is paragraphically the richest, with 11,184 paragraphs. No other ideologies come remotely close. Legalism has 1,783 paragraphs; Daoism has 1,161 paragraphs, and Mohism has 915 paragraphs. Confucianism, thus, functioned as an equivalent of the abstruse and arcane vocabulary of the SAT, and it was most suited for screening and selecting the desired human capital from a large pool of candidates. Is it at all possible that Keju successfully anchored and shaped the nature of the Chinese autocracy because of this accidental feature of Confucianism and on account of an operational technicality? Let‚Äôs pause, savour and ponder for a moment the momentous implications of this proposition. This essay is adapted from the book The Rise and Fall of the EAST: How Exam, Autocracy, Stability and Technology Brought China Success, and Why They Might Lead to its Decline (2023) by Yasheng Huang. Global history Nations and empires Education 19 October 2023 SYNDICATE THIS ESSAY At Aeon we provide original essays, curated videos, and beautiful imagery that reflects the diversity of ideas and thinking from around the globe. We are a charity dedicated to sharing knowledge with the world and we rely on reader support to keep going. Please support Aeon to help sustain us into the future. I would like to donate: Monthly Annually One-time Select amount (US dollars): $5 per month $10 per month Donate now Essay Religion Praying in shoes The Sunni movement of Salafism was born at the beginning of the 20th century, with the goal of modelling life on the 7th Aaron Rock-Singer Essay Mental health People not professionals Training individuals to support one another through difficult times is a profound step forward in our mental health crisis Arjun Kapoor & Jasmine Kalha Essay Economics Finance as alchemy Finance fraud is not a deviation from an essentially rational system but a window onto the reality-distortion of markets Aris Komporozos-Athanasiou Essay Nations and empires Settler colonialism Displacing and destroying peoples by colonisation is not just a historical Western evil but a global and contemporary one Lachlan McNamee Essay Bioethics Reproductive technologies Infertility treatments aim to improve women‚Äôs lives. But they risk tying womanhood to the toxic expectation of motherhood Gulzaar Barn Essay Political philosophy In the interests of all How Eugene V Debs turned American republicanism against the chiefs of capitalism ‚Äì and became a true crusader for freedom Tom O‚ÄôShea Essays Ideas Videos About Contact RSS Feed Donate Community Guidelines Follow Aeon Sign up to our newsletter Updates on everything new at Aeon. Daily Weekly Subscribe See our newsletter privacy policy. ¬© Aeon Media Group Ltd. 2012-2023. Privacy Policy. Terms of Use.",
    "commentLink": "https://news.ycombinator.com/item?id=37967751",
    "commentBody": "Keju, China‚Äôs difficult civil service testHacker NewspastloginKeju, China‚Äôs difficult civil service test (aeon.co) 136 points by laurex 18 hours ago| hidepastfavorite80 comments bane 16 hours agoNearby Korea, being strongly influenced by Chinese bureaucratic concepts and Confucianism, also used this system in antiquity and flavors of it have carried over into the modern age. There were some differences, the Chinese exam created a kind of possible path for social mobility if you could find a way to study for it. Lots of families would pool their resources to send one son to an academy to learn how to pass the exam on hopes that, if they passed it, would help pull the entire family out of poverty and potentially support sending other sons.In ancient Korea it had a different effect and tended to fix certain classes into the \"classes from which the bureaucracy could be reliably drawn from\". They still had the academies, but it tended to be the same class&#x2F;castes who attended and passed the exam. In theory it was open, but in practice it worked this way [1].Some of the preserved academies still exist and are major UNESCO sights. [2]Modern South Korea still uses an examination system for civil servants [3], but has lost the caste system so in an interesting way has ended up more like the ancient Chinese concept. It tests civics, history, data analysis, etc. and I believe there are different tests for different levels of seniority and are required for promotion. It&#x27;s common for test takers to have spent several years in dedicated study after university to prepare for the exam [4].A semi-equivalent might be like a bar exam in other countries.1 - https:&#x2F;&#x2F;www.amazon.com&#x2F;History-Korea-Antiquity-Present&#x2F;dp&#x2F;07...2 - https:&#x2F;&#x2F;whc.unesco.org&#x2F;en&#x2F;list&#x2F;1498&#x2F;3 - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Republic_of_Korea_public_servi...4 - https:&#x2F;&#x2F;youtu.be&#x2F;zt4d_jGLXMY reply chalcolithic 6 hours agoparentThank you very much! TIL something that completely changed my outlook on the subject matter reply mc32 13 hours agoparentprevDid this lead people to buy surnames? I know buying (into a) surname happened ‚Äîwhich is why like 80% of the pop has one of half a dozen surnames, rather than there being a longer tail of distribution. reply thaumasiotes 8 hours agorootparent> I know buying (into a) surname happened ‚Äîwhich is why like 80% of the pop has one of half a dozen surnames, rather than there being a longer tail of distribution.Huh? Why would you expect a flatter distribution? The concentration of surnames is easily explained by natural development over time. reply bozhark 13 hours agorootparentprevKim Lee SonThis explains a lot, neat reply resolutebat 13 hours agoprev> In 2022, the Chinese regime put almost 400 million people under some sort of COVID-19 lockdown, a feat that is unimaginable in any other country.India did the same. But, India being India, implementation and enforcement were quite haphazard and chaotic compared to China.Still, China is not quite as unique as the author thinks: India too has massive competition for civil service jobs, heavy emphasis on rote learning and passing incredibly difficult exams, and an increasingly authoritarian leader. reply ssnistfajen 13 hours agoprevYet Keju directly influenced civil service examinations in the West: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Imperial_examination#In_the_We...The thing about socio-political systems is that their innovativeness doesn&#x27;t last forever. Keju was innovative in the pre-industrial age because it provided a path to social mobility for those not born into hereditary nobility. Meanwhile European monarchs in the Early Modern era relied on nobility, landed gentry, and the clergy in administering their realms, none of which had academic competency as a requirement to entry.Of course, by the 19th century this has fallen dreafully behind as Keju curriculums were never updated to fit an evolving world. Confucian Classics were no better than the Bible or the Quran at guiding the development of elecrical generators, excavating coal, synthesizing chemicals, or smelting steel. At some point in the future, our current model of measuring academic competency will be seen as rigid and obsolete too.Standardized examinations aren&#x27;t unique to China. The cut-throatness is a byproduct of China&#x27;s large population base combined with limited possibilities of upward mobility due to its status as a developing economy not aligned with the most advanced economic bloc on the planet.>Keju was so precocious that it pre-empted and displaced an emergent society. Meritocracy empowered the Chinese state at a time when society was still at an embryonic stage. Massive resources and administrative manpower were poured into Keju such that it completely eclipsed all other channels of upward mobility that could have emerged.What pre-empted and displaced an emergent society was the creation and re-creation of unified imperial dynasties where emeperors were despots who were occasionally enlightened at best. Wealthy merchants existed in every Chinese Dynasty and \"Ten Great Merchant Guilds\" dominated the economic landscape of the Ming and Qing Dynasties. The upper bound of society was not limited by Keju, but by China&#x27;s position as an unchallenged hegemon in its sphere of influence until the industrial revolution. reply andy_ppp 4 hours agoprevBeing as close to government as possible is good for a) social credit scores b) financially c) corruption opportunities d) general status e) finding a partner in a very competitive dating market (fewer women [1]). So yeah, of course the exam is difficult![1] I‚Äôm pretty shocked about how bad the women situation is, especially in the 15-19 age group here https:&#x2F;&#x2F;www.statista.com&#x2F;statistics&#x2F;282119&#x2F;china-sex-ratio-b... :-&#x2F; reply Animats 11 hours agoprevNew York City jobs still have competitive examinations.\"The map below is a scheme of subway lines in the city. Each line is marked by a different color and is associated with a different letter. Stations are numbered by their location on the line(s). The numbering of the stations progresses from the beginning of the line to its end. Lines always begin at the northernmost station, except for lines G and M which begin at the westernmost station. Codes associated with some of the stations have been written below them. A station can have more than one code associated with it, as some stations have more than one line passing through them. Which of the following routes would be the least efficient (passes through the most stations)?\"[1]If only someone could devise a workable sociopath filter...[1] https:&#x2F;&#x2F;www.jobtestprep.com&#x2F;civil-service-exam-nyc-free-prac... reply est 7 hours agoprevon one hand, Keju serves as a social mobility mechanism. One the other hand, it&#x27;s hides an overall bigger problem: monarchism with too much centralized power. reply whoevercares 15 hours agoprevFor years, Gao Kao has been the only route for tens of thousands Chinese people in poverty to change their fate. It‚Äôs interesting if we can run an metaverse simulation one day to see whether the system in US will produce better or more fair results for poor people reply ssnistfajen 12 hours agoparentDoesn&#x27;t the US already have SAT which despite being non-mandatory, is required by 80% of degree-issuing US institutions? And there are already tons of studies showing family wealth has a positive correlation with higher SAT scores. reply otoburb 12 hours agorootparent>>[...] SAT [...] required by 80% of degree-issuing US institutions?I believe it used to be that way a few years ago, but since the pandemic the ratio inverted such that most US colleges are now test-optional. reply ilrwbwrkhv 15 hours agoparentprevIndia also has a similar tough civil service test called UPSC. I have a cousin married to one dude in India who passed the test and is some sort of high official there. Apparently people study for this and another stem exam called IIT which are for some OKish colleges but the entrance rate seem to be crazy because of how many people sit for these exams, not that the colleges are any good. reply thelastgallon 4 hours agorootparentThere is really nothing tough about these exams. They are all based on regurgitating some textbooks.Indian education system is built by the British, they needed some locals to do clerical work, who repeat what the masters said is knowledge (remember and and faithfully reproduce), follow orders. The white sahibs are replaced by brown sahibs, the education system is the same, what British called ICS is renamed to IAS (top govt official in a district), but its the same thing. reply pietro72ohboy 15 hours agorootparentprevIITs are prestigious institutions. They‚Äôre not OKish institutions in the slightest. reply thelastgallon 4 hours agorootparentLet&#x27;s look at one US University, UC Berkeley. 26 faculty Nobel Prize winners and 35 alumni winners. The current faculty includes 262 American Academy of Arts and Sciences Fellows, three Fields Medalists, 77 Fulbright Scholars, 139 Guggenheim Fellows, 90 members of the National Academy of Engineering, 144 members of the National Academy of Sciences,[81] ten Nobel Prize winners, four Pulitzer Prize winners, 125 Sloan Fellows, 8 Wolf Prize winners and 1 Pritzker Prize winner.Or, lets take CalTech with a similar size enrollment (in the thousands): As of October 2022, Caltech has 46 Nobel laureates to its name awarded to 30 alumni (26 graduates and 4 postdocs), including 5 Caltech professors who are also alumni.> IITs are prestigious institutions. They‚Äôre not OKish institutions in the slightest.They aren&#x27;t. They are a filter for a few thousand out of 1.4 billion people. Yet, all the IITs put together produce nothing compared to the output of one US University. > 99% end up writing Java code for Wall Street or work for ad-tech. I doubt they are pushing the boundaries of science. reply dragonwriter 4 hours agorootparent> Let&#x27;s look at one US University, UC Berkeley. [...]> Or, lets take CalTech with a similar size enrollment (in the thousands) [...]Caltech student population is a little over 2,200, mostly graduate students.UC Berkeley is over 45,000, about 3&#x2F;4 undergraduates.They don&#x27;t have \"similar enrollments\". reply ilrwbwrkhv 13 hours agorootparentprevThe are not even top 100 in the world. They are OKish. reply magnio 7 hours agorootparentThose university rankings are rubbish. Sure, they tell you MIT and Stanford are one of the best schools in the world; like anyone would need a ranking for that. Those rankings not only consider teaching quality to be the sole factor, but also infrastructure, research output, financial status, and degree of internationalization. It is as gameable as gacha game rankings.For more sobering examples: Universiti Malaya (UM) and City University of Hong Kong ranks higher than Brown University. University of Copenhagen, University of Waterloo, and Rice University are all outside of top 100, so I guess we can call them OKish (all according to QS 2024). reply phist_mcgee 8 hours agorootparentprevNot even top 100? Total waste of time then. reply p1esk 8 hours agorootparentI‚Äôd say if it‚Äôs not top-10 no one cares if you went there. I went to a top-20 school and it does not look impressive on my resume. replyCSSer 17 hours agoprevThis was an interesting, if not somewhat exhausting, read. The premise of this article can be succinctly summarized by one of its references to a 19th century text:‚ÄúSince the introduction of the examination system ‚Ä¶ scholars have forsaken their studies, peasants their ploughs, artisans their crafts, and merchants their trades; all have turned their attention to but one thing ‚Äì government office. This is because the official has all the combined advantages of the four without requiring their necessary toil ‚Ä¶‚ÄùIt refers to a very difficult exam that existed for a time in Chinese society that the author feels is analogous to systems re-emerging in Xi‚Äôs modern China. I hope someone will correct me if I‚Äôm wrong. To be honest, I normally dislike comments like this that just paraphrase the article, but I read most of it and he repeats himself a lot. He could really use an editor to help him be more succinct. reply boomboomsubban 17 hours agoparentThe article is ostensibly an advertisement for his book, and the idea of reading the book seems tedious after reading this article.I understand not wanting to spend the time detailing your research showing that China&#x27;s technological growth lagged after introduction of the Keju in a teaser article. Repeating \"I found this data, it supports my conclusion, and China&#x27;s doing the same thing today\" isn&#x27;t a good alternative.Maybe the book has a better editor. reply CSSer 16 hours agorootparentYeah, I think similarly the most difficult aspect for me was its lack of focused instrumentation for its points. He mentions Gao Kao briefly but never revisits it again. Towards the end of the article, he mentions the U.S. systems. Most of the article just harps on Keju to the point that I felt it would‚Äôve been better served if it were just a historical summary rather than a piece with an unclear agenda. I‚Äôm not crying propaganda either. I just mean I‚Äôm not sure there‚Äôs enough here to even know what he‚Äôs really trying to say. Perhaps he‚Äôs afraid of openly criticising these systems too? reply boomboomsubban 12 hours agorootparentHe&#x27;s pushing his book. It&#x27;s \"buy now for the full picture!\" Only his way of displaying a partial picture makes me hesitant to labor through the full picture. reply icegreentea2 16 hours agoparentprevWithin the excerpt, I don&#x27;t think its fair to say that the author is claiming that modern descendants of keju (Gao Kao and the modern civil service exam) are responsible for the authoritarian tendencies of the modern Chinese state. Rather, the author argues that the keju baked in centuries of authoritarian tendencies into Chinese... inertia? culture? I want to say society, but one of the author&#x27;s arguments is that keju effectively neutered the growth of Chinese society, by channeling nearly all the excess energy of the Chinese population into the state.I would add to your premise summary \"keju helped (allowed? permitted? enabled?) the Chinese state to dominate Chinese society\". reply CSSer 16 hours agorootparentIt‚Äôs funny because I see what you‚Äôre saying but I was caught on the same points you question here. I don‚Äôt really know. I suppose my takeaway is that there was interesting standardized test in Chinese history and leave it at that. reply seanmcdirmid 16 hours agorootparentprevThankfully the Chinese state would occasionally fall, creating turnover in entrenched practices. The Ming dynasty would be very different from the Qing. Unfortunately, both dynasties lasted a very long time. reply ssnistfajen 12 hours agoparentprevGao Kao in its present form hasn&#x27;t really changed much since 1978. Xi was still an university student in that year. So I don&#x27;t see the point of blaming Gao Kao for making the country slip back into incompetent autocracy when the glaring problem is the lack of checks and balances to prevent an incompetent person from obtaining unchallengable absolute power. Checks and balances can exist in non-democratic systems too, it&#x27;s about the design not the principle. reply visarga 17 hours agoparentprev> somewhat exhausting readI rely a lot on TTS screen readers, especially for longer articles. Listening and reading at the same time somehow makes my focus much sharper. Or I can just relax my eyes and listen. reply konschubert 17 hours agoparentprevWhat is the proposed mechanism? The exam being difficult in an itself surely isn‚Äôt the thing that‚Äôs creating the draw? reply boomboomsubban 16 hours agorootparentHuh? If you&#x27;re asking why people took the Keju, passing it got you a prestigious job in the Chinese bureaucracy. reply simonebrunozzi 17 hours agoparentprevA good use of current GPT-like tools could be to:1. transform a long and repetitive test into a shorter one, and&#x2F;or2. analyze an article or a webpage and give us a \"score\" that represents how well or not well it has been edited.We shouldn&#x27; rely on someone like you to read the entire article and tell others that it&#x27;s repetitive, and therefore boring to read in its entirety. reply FrankWilhoit 18 hours agoprevThe headline is misleading; it might better say \"...that broke Chinese society...\", but then the article asserts that the Ching regime imploded after and because the exams were stopped in 1905. reply canjobear 17 hours agoparentYasheng Huang‚Äôs broader thesis is that keju broke Chinese society in the sense that all intellectual energy was focused on the single hierarchy established by the test, instead of being focused on multiple endeavors and new ideas. He explains it better in his Tyler Cowen interview than in this article: https:&#x2F;&#x2F;youtu.be&#x2F;KcbwBqIyusc?si=6aiG8-qPyPPxa0Fv reply ssnistfajen 11 hours agorootparentThis is going into alt history territory TBH. Keju was an improvement over the previous system which largely consisted of staffing the civil service based on ancestry from large aristocratic families who nominated each other to strengthen their political power.Imperial China&#x27;s hegemony over its surrounding regions was not thoroughly challenged until the 19th century, and once it began to be challenged by nations with more advanced ideas and technologies, Keju did enter terminal decline along with the rest of the imperial system. Even without Keju, Early Modern China didn&#x27;t have the recipes for scientific or industrial revolution to start there. reply latexr 18 hours agoparentprevYou can surround text in asterisks for emphasis, *like so*. Use a backslash and an asterisk or two asterisks together to output a * literally, **like so**. reply foobarian 17 hours agorootparentYour mastery of HN comment meta-character rendering is impressive! :-) reply CSSer 17 hours agorootparentThe name checks out. Anyone who can understand LaTEX must have a natural disposition for markup mastery. reply philsnow 16 hours agorootparent(or markup masochism)I love the output that you can get with LaTeX, but I just can&#x27;t wrap my head around the syntax.For some things you run a \\command just inline with whatever you&#x27;re doing (and if you want that command to only apply for a small part, you can surround it in an implicit environment with {\\command whatever you were going to type}, for others you create an \\environment{and write things inside it}. I guess in general it&#x27;s weird to me that something like \\Large can affect everything that comes after it, it feels vaguely like overuse of global variables in C.Parameters&#x2F;options to commands and environment directives... I guess sometimes use square brackets and sometimes use curly brackets? I could never figure this out. reply digitalsushi 16 hours agorootparentprevCSSer, a passing master, subtly bows his head to latexr, a passing master, in the opposite direction. An oak creaks in the slightest breeze and a woodpecker bores for a beetle. replyesafak 16 hours agoprevHow useful is the stuff they memorize? Forcing people to study so much with no benefit other than being able to rank them in an exam would be a horrendous waste. And for what, becoming an appendage of the Leviathan? reply canjobear 16 hours agoparentIt was useful in that a lot of the texts they were studying purported to describe ideal government. Government officials often communicated with each other using obscure references to these texts, and their expectations about appropriate behavior were set by these texts. Anyone who got to the top of the tests could be guaranteed to be a good reader and writer with a huge body of shared literary knowledge with other government officials, not to mention being diligent and verbally intelligent.It was useless in that the texts were vague and fictionalized, and people weren&#x27;t asked to offer new interpretations of the texts or to apply them to modern circumstances. The tests were graded based on how elegantly students could reproduce the standard orthodox interpretations in elaborate essays. reply ssnistfajen 11 hours agoparentprevThese are the core study materials: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Four_Books_and_Five_ClassicsThey have some philosophical values that were progressive for the time they were written (before 300BC), but by the 1800&#x27;s they were as useless for adapting to an industrialized modern world as Abrahamic scripture. The disconnect between Keju study materials and realistic administrative responsibilities of bureaucrats have existed for as long as Keju itself. reply gnicholas 17 hours agoprev> Passing the first tier of Keju, known as the provincial exam, was a lot easier ‚Äì working out to be 4 per cent on average during the Ming. Still, this was more cut-throat than getting into Harvard in most years.While Harvard‚Äôs admit rate might be above 4% in some years, there is more selection bias in the applicant pool, it would seem. The vast majority of people don‚Äôt bother to apply. reply hnthrowaway0315 16 hours agoparentSame for keju, most peasants did not apply. reply gnicholas 15 hours agorootparentWhat percent of people took the test? My guess is that a very small percent of 18 year olds apply to Harvard, when you consider that not all graduate from high school, and only a small percent of students who graduate with high GPAs would even apply. reply CSSer 17 hours agoparentprevRight. Without an outright scholarship or wealthy background, how would you pay? I never bothered because I was an upper quartile student, not an outstanding one (I was distracted by the usual things). I needed in-state benefits and grants to afford college. This leads me to suspect there is no ‚Äúacademic middle class‚Äù at Harvard.Ironically, I also suspect that despite that, the quality distribution of accepted candidates is roughly similar to most other universities too. Much of this personal suspicion stems from empirically utilizing online courses of theirs when I was in undergrad and observing students ask questions in recordings of course lectures. Of course they got into Harvard, but I was surprised how often many of them struck me as very normal in terms of ability to grasp material. I suppose the rest of it comes from some of the silly stuff I occasionally read in HBR. Heh. reply jibe 16 hours agorootparentWithout an outright scholarship or wealthy background, how would you payHarvard and most top private schools are generous in financial support if you can‚Äôt afford it. Harvard is free for families making less the 85k. But you are right that might leave an empty middle where you don‚Äôt get much aid, and don‚Äôt have personal wealth.https:&#x2F;&#x2F;college.harvard.edu&#x2F;admissions&#x2F;why-harvard&#x2F;affordabi... reply ensignavenger 15 hours agorootparent\"And can&#x27;t afford it\"I do appreciate their efforts, but I bet that they decide who can and can&#x27;t afford it, probably based on rigid criteria that inflexibly denies at least some folks in the middle who really can&#x27;t afford it.Edit: It is possible I am wrong, the Harvard website linked above at least gives the impression they work with folks in varied circumstances. My experience helping folks with aid programs has perhaps coloured my perspective on Harvards unfairly. I have no experience dealing with them. If you want to attend Harvard, by all means, please apply and give them a chance! It is worth the rhe shot. reply tekla 13 hours agorootparentI personally know of 3 first generation immigrant families who all made well less than $60K a year and with good grades, all 5 kids from them went to Ivy League schools with full scholarships. reply wolverine876 15 hours agorootparentprev> Without an outright scholarship or wealthy background, how would you pay? I never bothered because I was an upper quartile student, not an outstanding one (I was distracted by the usual things). I needed in-state benefits and grants to afford college. This leads me to suspect there is no ‚Äúacademic middle class‚Äù at Harvard.Your suspicions are evidence of your suspicion? I&#x27;m not sure what you meant by \"academic middle class\": If you mean, in terms of academic skill, well of course - why would Harvard accept any but the best (and wealthiest and most powerful)?> the quality distribution of accepted candidates is roughly similar to most other universities tooRecent experience with Ivy League undergraduates leads me to a believe that it&#x27;s a pretty effing talented cohort. That statement may match the snobbery that many are (rightfully) suspicious of, but it&#x27;s not that; they were just very impressive. reply elijaht 15 hours agorootparentprevIvys have really generous financial aid across all income brackets in my experience. Came from a family that didn‚Äôt qualify for complete financial aid ($125k household income). My brother at Princeton pays very little out of pocket, maybe $5k? It was cheaper than any other school he applied to reply mannykannot 17 hours agoparentprevWomen could not participate, so it was definitely more cut-throat than Harvard for half the population.The article gives a figure of ~1M taking the provincial exam annually during the Ming dynasty, with the later Ming empire having a population in the range 160-200M. You are probably right about self-selection being greater for Harvard, then, given that just about anyone can, in principle, apply there. reply tgv 15 hours agoprevHow do you take these statements from the article?[1] More recent literature demonstrates the political effects of Keju. In 1905, the Qing dynasty abolished Keju, dashing the aspirations of millions and sparking regional rebellions that eventually toppled China‚Äôs last imperial regime in 1911.[2] A 2020 study shows that, for every doubling of successful Keju candidates per 10,000 of the population in the Ming-Qing period, there was a 6.9 per cent increase in years of schooling in 2010. [comparison shows a logarithm relation between Keju success and current schooling between regions.]Wrt to [2]: there were only 260 regions, that period ranged over more than 500 years, and the correlation is not particularly good. It seems extremely improbable that all other factors have been excluded (it&#x27;s just linear relations for a handful of numbers they could find). The logarithmic nature is counter-intuitive: since the exam was very, very selective, you would expect the reverse: a small increase in candidates passing requires many more students below that level, and thus a super-linear increase in schooling.[1] sounds very far-fetched. The linked article is paywalled, but the jump from reducing fairly small hopes of fortune to revolution is too big to accept from statistics that sound similar to those in [2].The author seems to use it as support for his argument, but in my view, it just turns it into a house of cards. I stopped reading after the second article. Do these things bother you? reply ssnistfajen 12 hours agoparentThis is the paper used to support the claim in [1]: https:&#x2F;&#x2F;www.jstor.org&#x2F;stable&#x2F;43866446. It can be read via an institution account or that website. The main claim is that regions with higher keju quotas had higher incidences of revolutionary activity in 1911, using what appears to be purely statistical methods. The paper&#x27;s conclusion did not state the abolition of Keju was the decisive factor in the fall of the imperial dynasty, and acknowledged by quoting Huntington&#x27;s Political Order in Changing Societies, ‚Äúnecessarily involved the alienation of many groups from the existing order‚Äù like all revolutions.So I think the author misappropriated the paper to support their own argument. It&#x27;s possible that the abolition of Keju increased regional unrest which contributed to higher incidences of rebellion in 1911, but itself did not \"spark\" rebellions on its own. The 1911 revolution began with the Wuchang uprising which was started by Westernized \"New Army\" divisions of the Qing Dynasty, an effort that began in 1895 and was not tied to Keju. It ended quickly because Yuan Shikai, the real commander of the remaining loyal New army in the North, realized he could make a powerplay by forcing the imperial court to abdicate the convince the revolutionaries to support him as president instead. reply rawgabbit 15 hours agoparentprevI will assume this is a bad English translation of a Chinese article.Nevertheless [1] appears to claim that ending the civil service test caused the fall of the Qing dynasty which is complete nonsense. I assume this is just an inept translation.[2] The claim (in English) is incomprehensible; I assume the author&#x27;s intention is that when more civil service jobs were awarded more Chinese studied in an attempt to pass the exam.Overall, I believe the English text of the article is poor. I believe he is arguing the civil service test was all pervasive because over a million people took the test annually in an effort to get a very small number of civil jobs. The test itself could be considered an intelligence test because it required intelligence to memorize and elegantly recite the orthodox answers based on Confucian classics.If we want to argue what is deficient about Chinese government or culture during the Ming&#x2F;Qing period, there are an infinite number of other topics. I would argue overall the Civil Service exam is a net positive for China, not a negative. As an aside, I watch some modern Chinese melodramas. These TV shows seem to argue China&#x27;s past was marked by corruption, infighting among officials, and poison poison poison. According to these TV shows, ancient Chinese politics consists of everyone trying to poison everyone else. There are TV plots that involve multi-factor poisons. By itself the poisons are benign, but when they are mixed via food & incense & medical prescriptions, they are deadly. That is they blame endless in-fighting for China&#x27;s predicament. The TV shows often feature characters spouting Confucian euphemisms but have no qualms poisoning or killing women&#x2F;children for political gain. In short, modern Chinese people think past Chinese politicians were unethical and downright evil. reply d0mine 12 hours agorootparent> politicians were unethical and downright evilAre there any other kind? any place&#x2F;any time reply tgv 2 hours agorootparentYes. Please cut the cynicism. There are quite a few that went into politics because of self-interest, but certainly not all of them. And they are human, too. They have their flaws. And if their ideological and pragmatical stances don&#x27;t overlap with yours, that doesn&#x27;t mean they&#x27;re unethical or evil. Be glad there are at least some who try. reply paulpauper 17 hours agoprev [‚Äì] In the US still, the path to success is a highly optimized one: start early, max out the extracurriculars, become an outlier among outliers. GitHub has become the new resume. No one cares that you worked at Petco at 17; they want to see your repos, your pull requests. Today&#x27;s elite and top earners are not born, like under the aristocracy or nobility, but rather forged, of course assuming the potential is there, like IQ. reply pc86 17 hours agoparentYou seem to be conflating two mutually exclusive paths to success. Nobody in the world of banking, or politics, or manufacturing, give a shit about repos and pull requests. And nobody in the tech world gives a shit about your extracurriculars. And \"become and outlier among outliers\" doesn&#x27;t give anyone any actual idea how to become successful and could be applied to literally any field or organization in any society at any point in history. reply oefrha 16 hours agorootparent> And nobody in the tech world gives a shit about your extracurricularsPeople in the (American, and I dare say elsewhere too) tech world do give a shit about the college you went to, and the elite colleges do give a shit about extracurriculars, especially if you‚Äôre Asian or something, unfortunate as it is. reply bigstrat2003 7 hours agorootparent> People in the (American, and I dare say elsewhere too) tech world do give a shit about the college you went to...Not really. Nobody gives a damn what college you went to for the vast majority of jobs. They care about whether you went to college or not, regardless of the school. And even then, it&#x27;s becoming more and more rare to find employers who are sticklers that applicants must have a degree. reply oefrha 6 hours agorootparentWhat&#x27;s this weird confusion between \"requirement\" and \"gives a shit\"? A Stanford graduate isn&#x27;t treated the same as a random state college graduate out of school, period, before you even consider relationships forged during college and alumni networks. reply bigstrat2003 2 hours agorootparent> A Stanford graduate isn&#x27;t treated the same as a random state college graduate out of school, period,At most jobs? Yeah, they are actually. reply pc86 15 hours agorootparentprevMaybe for your first job or two, but there are plenty of self-taught FAANG developers with completely unrelated degrees from completely unknown colleges.I&#x27;m really curious what the \"or something\" in your race statement means though. reply oefrha 9 hours agorootparentFirst job or two do affect your entire career, not to mention the lasting effects of networking. ‚ÄúThere are plenty of counter examples‚Äù doesn‚Äôt mean people don‚Äôt give a shit in general, it means it‚Äôs not the only factor at play.As for ‚Äúor something‚Äù, it is well known that the bar is very high for Asians and extracurriculars are needed to distinguish oneself (too many perfect SATs and GPAs), but maybe there are other demographics similarly affected that I don‚Äôt hear about much. reply paulpauper 17 hours agorootparentprevthanks for your constructive feedback reply tintedfireglass 17 hours agorootparentum., is this sarcasm? reply jstarfish 16 hours agoparentprevEverything you say is presumably true of FAANG or SV in general. Cult rules apply there, where you have to forsake anything that might serve you in outside communities so you risk economic death in trying to get into this one.Posers aside, the rest of the world does not function this way and advice like yours really fucked my professional development up.If you&#x27;re just a regular type of dude in most of the US, none of this applies. Your Petco job does matter, your employer won&#x27;t even look at your Github page, and talking about PRs will only invite clarification of whether you mean public relations or Puerto Ricans.The SV-optimized path is not the only path to success in America. This country is itself entirely optimized for entrepreneurs. Tradesmen crush it and don&#x27;t have to live in fear of AI obsolescence. The most successful people I know are all undereducated immigrants and ex-military types running boring businesses selling medical equipment, carpet and tile. Always boring shit people generally need, not drop-shipped dollar-store crap from Temu.To be the guy positioned to sell N95 masks in 2019 never required a Github profile or elite education. reply Spooky23 17 hours agoparentprevFor now.US Civil Service merit & fitness requirements were undermined by courts finding that because most examinations correlated to general intelligence as defined by IQ, they were discriminatory to various stakeholders who are either under-assessed by IQ due to cultural or disability related bias.The easy path for employers is to utilize the experience + education provided by the candidate, and clean up any mess later. (or not) reply Apocryphon 16 hours agoparentprevThe software focus of this comment aside, there is something to be said about how primary and secondary education has become hyper-optimized cram school grindfests among a certain strata. Students are expected to max out not only the SATs but their entire candidate profiles in a bid to get into increasingly competitive universities even as the worth of upper education credentials becomes increasingly deflated. In some areas, preschools become competitive for parents to vie for to begin their children‚Äôs educational careers off right. This applies to all manner of white collar professions, not only engineering. Albeit then again maybe this is a Silicon Valley-centric phenomenon, and only an elite striver subset (Palo Alto, Cupertino, Saratoga, Contra Costa County) of the region. reply streakfix 16 hours agoparentprevThere are professions other than software development reply satvikpendem 17 hours agoparentprev> they want to see your repos, your pull requestsI&#x27;ve never had any potential employer look at my GitHub. It was all about the resume, experience, and being able to explain that experience, as well as a technical coding assessment. reply arp242 17 hours agorootparentEveryone asks for it, but few actually look at it. And that&#x27;s fine too, because not everyone has the same hobbies and \"let me see your GitHub\" is essentially screening for a hobby. I say this as someone with lots of GitHub stars and such.If anything, \"grinding leetcode\" would be the closest analogy to Keju (although not exactly identical of course). reply ensignavenger 15 hours agorootparentOn the other hand, I have been interviewd and asked about a project on my GitHub, so there are those who do. reply saagarjha 17 hours agoparentprevBeing a software engineer is not the only way to achieve success. reply visarga 17 hours agoparentprevThat analogy would work if Github only had one single project. The problem was not the test, but the fact that there was just one single test, and the subject matter and manner of testing were supporting authoritarianism. There was no open competition for talent between multiple institutions. They outsourced brainwashing to parents and teachers. reply ssnistfajen 11 hours agoparentprev [‚Äì] Many of today&#x27;s elites and top earners are very much still born to previous generations of elites and top earners. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article focuses on the historical impact of the Keju, China's civil service exam, suggesting that it promoted authority deference and collectivism; its abolition in 1905 led to regional rebellions and the collapse of the Qing dynasty.",
      "The author compares the Keju system to civil service exams in the West and suggests that China's autocracy under President Xi Jinping is stronger than Russia's under Vladimir Putin.",
      "It is argued that the Keju system monopolized the time and energy of candidates, promoted pro-autocracy values, and suppressed creativity. The article surmises that the success of Chinese autocracy could be partially due to the usage of Confucian texts in the Keju system."
    ],
    "commentSummary": [
      "The article discusses the influence of China's civil service test, the Keju, on social mobility and family investments, and draws comparisons with similar exam systems in ancient and modern South Korea.",
      "It highlights the prestige and history of Indian exams, their likeness to the British education system, and discusses the difficulties of gaining admission into prestigious institutions like Harvard.",
      "The piece concludes with a debate on the importance of college and extracurricular activities in the tech industry, including issues of bias, networking, and unconventional paths to success."
    ],
    "points": 136,
    "commentCount": 80,
    "retryCount": 0,
    "time": 1697903200
  },
  {
    "id": 37966367,
    "title": "It takes 12 people to use the 18k Sphere camera",
    "originLink": "https://petapixel.com/2023/10/20/darren-aronofsky-says-it-takes-12-people-to-use-the-18k-sphere-camera/",
    "originBody": "News Reviews Guides Learn Equipment Glossary Podcast Newsletters Send a Tip Darren Aronofsky Says it Takes 12 People to Use the 18K Sphere Camera OCT 20, 2023 JEREMY GRAY The music and entertainment arena Sphere opened in September in Las Vegas and the striking venue is the largest spherical building in the world. It features the world‚Äôs biggest and highest-resolution screens and its content is captured using a state-of-the-art 18K cinema camera. Acclaimed filmmaker Darren Aronofsky used the remarkable camera to create his new movie, Postcard From Earth, which is now playing at Sphere. As Screen Rant describes, Aronofsky‚Äôs new film is a ‚Äútechnical marvel.‚Äù In a recent appearance on Late Night With Seth Meyers, Aronofsky discussed Postcard From Earth, including some remarkable behind-the-scenes information. Among the fascinating details Aronofsky discusses is that the 18K camera he used requires about a dozen people to operate, including moving it and keeping it from overheating. Top Stories 01:06 Smartphone vs Camera: The Best Camera is the One You Have With You Aronofsky‚Äôs 18K resolution film plays back at Sphere at a blistering-fast 60 frames per second, more than twice the speed of the typical 24p motion picture, and is about half a petabyte in size, or about 500 terabytes. Aronofsky explains that the movie is about 32GB of data per second, or nearly 2,000 GB a minute. Beyond the technical challenges, working with never-before-seen technology also presents artistic challenges. Aronofsky worked on a project for a wholly unique venue that he had not even been able to see in person. He didn‚Äôt see Sphere until August, the month before it opened. Seeing the 18K creation for the first time on the world‚Äôs most gargantuan screen was a spectacular experience. ‚ÄúIt was great. It was what we were hoping for. So, no one had ever seen it, so we didn‚Äôt really know what they would respond. And then this moment happened. I actually didn‚Äôt know that jaw-dropping was a real thing. You hear that word, but it‚Äôs fun to look around, and people are just like, their jaws are literally dropped, and they‚Äôre all pointing at different places on the screen. So it‚Äôs a joy to have audiences go along for that trip,‚Äù Aronofsky says to Meyers. ‚ÄúThe venue will house the world‚Äôs highest resolution LED screen: a 160,000 square-foot display plane that will wrap up, over, and behind the audience at a resolution over 80 times that of a high-definition television with approximately 17,500 seats and a scalable capacity up to 20,000 guests. While the facility for viewing these immersive experiences sounds impressive on its own, it leaves one wondering what kind of cameras and equipment are needed to capture the content that gets played there,‚Äù PetaPixel wrote about Sphere earlier this year. The 18K camera Aronofsky describes, dubbed Big Sky, features a 316-megapixel HDR image sensor three by three inches in size. That is a 40x resolution increase compared to 4K cameras. For a detailed dive into this remarkable camera, make sure to read PetaPixel‚Äòs exclusive coverage from earlier this year. Aronofsky will not be the only talented filmmaker to get their hands on the camera, and it will be incredible to see how visual artists take advantage of the camera and the one-of-a-kind venue, Sphere. Postcard From Earth will play exclusively at Sphere for two years. Image credits: Sphere Studios, MSG. Image of Darren Aronofsky licensed via Depositphotos. NEWS, SPOTLIGHT CAMERA, DARRENARONOFSKY, FILM, LASVEGAS, MOVIE, SPHERE, TECHNOLOGY RELATED ARTICLES Steven Spielberg to Direct Jennifer Lawrence in Biopic About War Photog Lynsey Addario Sphere Studios‚Äô Big Sky Cinema Camera Features an Insane 18K Sensor This is the First Photo Shot with Sony‚Äôs Star Sphere Satellite The ISS is About to Receive its Highest Resolution Camera Ever DISCUSSION LOAD COMMENTS TRENDING ARTICLES Photographer Finds $1,400 Lens for $7 at Thrift Store OCT 16, 2023 Historical New England Photos Destined for the Trash Saved by Photographer OCT 17, 2023 Epic Marine Photos Star in Ocean Photographer of the Year 2023 OCT 18, 2023 Photographer Unwittingly Captures Paraglider‚Äôs Demise in Alaskan Peaks OCT 19, 2023 Photographer Vanishes on Road Trip to Shoot ‚ÄòForgotten People‚Äô of US OCT 18, 2023 Reading mode: Light Dark Home About Contact Write for PP Advertise Privacy Policy Full Disclosure ¬© 2023 PetaPixel Inc. All rights reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=37966367",
    "commentBody": "It takes 12 people to use the 18k Sphere cameraHacker NewspastloginIt takes 12 people to use the 18k Sphere camera (petapixel.com) 134 points by belter 21 hours ago| hidepastfavorite59 comments ljoshua 20 hours agoSome mind-blowing technical details about the camera and recording system itself are in a secondary article linked from this one as well:https:&#x2F;&#x2F;petapixel.com&#x2F;2023&#x2F;06&#x2F;12&#x2F;sphere-studios-big-sky-cine...Bits like a fiber-to-media recording system and custom GPU-based processing systems, really awesome project reply jauntywundrkind 18 hours agoparentFor those also curious as I was, some numbers: 4k is ~8 megapixels. 8k is ~35MP. 12k is ~80MP. 18k is ~170MP. Except for Big Sky the resolution is not 2:1 aspect ratio like DCI or other formats, it&#x27;s 1:1 18k, so that&#x27;s 340MP.Big Sky is shooting 120FPS at 10-bit (or 60FPS at 12 bit). That&#x27;s 408Gbps&#x2F;51GBps.A RX 7700 XT has 432Gbps memory bandwidth and a 4070 504Gbps... maybe that could render this? Unless you need to both write to memory and then read from it to render, and now you are up to a flagship RX7900 XTX or 4090 (960&#x2F;1008Gbps). Which with their 24GB memory can fit a whopping 0.47s of video in memory.Now consider the NVMe storage array you ll need to get video at 408Gbps. PCIe 5.0 NVMe drives can supposedly hit 14GBps, so \"only\" 4 drives going full tilt should be adequate to let you play this back at realtime speeds. reply otteromkram 12 hours agorootparentOnes might wager that they aren&#x27;t going to use consumer-grade GPUs for this.Here&#x27;s a pro-level card, for instance: https:&#x2F;&#x2F;www.amd.com&#x2F;en&#x2F;products&#x2F;professional-graphics&#x2F;amd-ra...That says 864 GB&#x2F;s, which is pretty good. reply jauntywundrkind 11 hours agorootparentMy bad, I failed to read the correct units!! The throughput figures in the desktop cards was actually in GB&#x2F;s, not Gbps!!!So any of these would be more than fine. That makes a lot more sense, considering what it takes to make a complex 3d scene. reply nunez 20 hours agoparentprev0.5Tbps light to chip!!!!!That&#x27;s insane. reply mschuster91 19 hours agoparentprev> What‚Äôs even more impressive is the camera can run completely separate from this recording technology as long as it is connected through its cable system, this includes distances of up to a reported mile away.That&#x27;s not that impressive, Blackmagic cameras have been able to run on fiber since 2018 [1].The bandwidth for the Sphere cam however, that is insane. 500 Gbit&#x2F;sec, that used to be the domain of entire ISP&#x27;s core switches, and now that kind of performance is (effectively) handheld-capable.[1] https:&#x2F;&#x2F;www.blackmagicdesign.com&#x2F;media&#x2F;release&#x2F;20180201-02 reply mhandley 17 hours agorootparent> 500 Gbit&#x2F;sec, that used to be the domain of entire ISP&#x27;s core switchesAnd then you realize that the latest datacenter Ethernet switch chips have 51 Tbit&#x2F;sec agregate non-blocking capacity (128 ports of 400Gb&#x2F;s, or similar setups), and a single AI cluster can use hundreds of these switches. reply jandrese 19 hours agorootparentprevA consumer grade GeForce 4070 has that much memory bandwidth, and there are more expensive models with even more.It‚Äôs an almost unfathomable number but at the same time it is well within the realm of commodity hardware. We are living in the future. reply tgaj 18 hours agorootparentMemory bandwidth is easier to achieve because of q short distance between elements on the motherboard. reply kibwen 20 hours agoprev> Aronofsky‚Äôs 18K resolution film plays back at Sphere at a blistering-fast 60 frames per second, more than twice the speed of the typical 24p motion picture, and is about half a petabyte in size, or about 500 terabytes. Aronofsky explains that the movie is about 32GB of data per second, or nearly 2,000 GB a minute.I can already see the PC gamers demanding 18K 144hz monitors. reply bitwize 14 hours agoparentWe need to hurry up and switch to Wayland; once displays exceed 32k, they will literally be larger than X11&#x27;s coordinate space. reply clnq 5 hours agorootparentCertainly the Sphere will be quite difficult to use without fractional scaling as well. reply tmikaeld 20 hours agoparentprevThe human eye can only see at 60 frames per secondedit: Apparently, you don&#x27;t joke about frames per second. reply codetrotter 20 hours agorootparenthttps:&#x2F;&#x2F;www.pcgamer.com&#x2F;how-many-frames-per-second-can-the-h... reply icegreentea2 20 hours agorootparentprevWhat makes it a joke? reply diarrhea 19 hours agorootparenthttps:&#x2F;&#x2F;knowyourmeme.com&#x2F;memes&#x2F;human-eye-can-only-see-at-60-... reply fsckboy 14 hours agorootparent\"the claim that the eye can only see 60 FPS has been proven false in numerous online threads\" reply icegreentea2 19 hours agorootparentprevThanks lol. reply Our_Benefactors 17 hours agorootparentprevLet‚Äôs for a moment accept this as fact. Now consider you have a 60fps screen and 60fps natural vision. The periodicity of video frame updates and ‚Äúvision frames‚Äù are not in any way synchronized. Meaning the image appears less ‚Äúsmooth‚Äù than if you had a higher refresh screen.Lastly, have you ever tried a 144hz and 60hz display side by side? The difference is very obvious. reply sdenton4 16 hours agorootparentThere&#x27;s no synchronized refresh in the retina though.Imagine pixels being randomly updated continuously in time, but each individual pixel has a refresh rate of 60Hz.The overall perceived &#x27;frame rate&#x27; for this parallel random update will be much higher than 60Hz: Suppose 25% of the pixels in an object need to change to register a perceptible difference: then the effective framerate for noticing changes in the object will be ~240Hz. reply tmikaeld 14 hours agorootparentThe fastest proven seems to be 220hz but 240hz is the maximum theoretical for humans. reply fsckboy 14 hours agorootparentif you are referring to this:> The USAF, in testing their pilots for visual response time, used a simple test to see if the pilots could distinguish small changes in light. In their experiment a picture of an aircraft was flashed on a screen in a dark room at 1&#x2F;220th of a second. Pilots were consistently able to \"see\" the afterimage as well as identify the aircraft. This simple and specific situation not only proves the ability to percieve 1 image within 1&#x2F;220 of a second, but the ability to interpret higher FPS.that is not a test of FPS, it&#x27;s a test of a few photons in a dark room and the afterimage affect of the retina (or is it also the optic nerve and the visual cortex?), potentially aka \"blurring\".I&#x27;m not taking a position on framerates, I&#x27;m taking a position on interpreting the data. reply tmikaeld 3 hours agorootparentAh, you&#x27;re right, maybe it&#x27;s even higher than 240hz then, I mean, they&#x27;re making 540Hz monitors now so there must be something to it. replypdntspa 16 hours agorootparentprevDude gamers get super defensive over refresh rate. It&#x27;s like, don&#x27;t trash my large numbers! They are large numbers!It&#x27;s kind of stupid. reply Thorrez 14 hours agorootparenttmikaeld was joking. HN generally doesn&#x27;t approve of low-effort jokes. We don&#x27;t want HN to become reddit. reply xoa 20 hours agoprevObviously we&#x27;re a long ways (if ever) from having such theaters available widely enough to be more then location tourist attractions, but VR will almost certainly eventually make such video accessible for home viewing as well. And the march of tech will undoubtedly eventually bring down camera costs to \"very high\" vs \"stratospheric one of a kind high\", and in turn mean more works get made with it.So from an artistic point of view, I&#x27;m a little curious long term what film makers might come up with when they get a format where it&#x27;s impossible to see it all in one go. I mean yes, lots of the best films reward multiple viewings and as a practical matter it can be very hard to notice little details the first time around. Same thing for that matter with static images. But still, you can physically see everything there at once, if multiple people watch it everyone sees the same thing. Whereas in a sphere format with the work exceeds your physical field of view it&#x27;ll be truly impossible to experience all of the film in a single go no matter how closely you pay attention.While it&#x27;d be very challenging to use well and I&#x27;m sure a lot of the format will be documentary type of stuff, nevertheless it seems likely there will be works eventually that really lean into it and they&#x27;ll genuinely offer different experiences in the same work depending on where you choose to focus. A medium that has already long done this in a way is video games, where it&#x27;s not at all uncommon for there to be games that offer multiple mutually exclusive paths; picking one means you will not ever see certain content in that run. That can be cheap sometimes, but can also used by a creative producer&#x2F;writer in interesting ways. Will open a new dimension for film making perhaps. reply duckfruit 14 hours agoprevGreat article. The Sphere is such a ridiculous, hilarious and fantastic project. We should be build one in every city. reply ilaksh 9 hours agoprevIt&#x27;s not a sphere. It&#x27;s a dome. reply Apocryphon 8 hours agoparentNot with that attitude reply huytersd 20 hours agoprevHow does viewing a movie on the sphere work? From my understanding it‚Äôs a sphere that displays things on the outside. Wouldn‚Äôt this mean you would only be able to see only a small part of the screen at any given time? reply jffry 20 hours agoparent> From my understanding it‚Äôs a sphere that displays things on the outsideIt&#x27;s got two screens! One inside and one outside.The interior screen is 16k by 16k pixels (the 18k camera resolution affords some cropping for image stabilization etc).The exterior screen doesn&#x27;t have much in the way of specs other than area, but I&#x27;d expect it is comparably lower pixel density. reply nicoburns 20 hours agoparentprevI believe there&#x27;s another huge screen on the inside. reply nunez 20 hours agorootparentThe Sphere&#x27;s surround LED screen is 16K by 16K reply spencerflem 20 hours agoparentprevThere is a similar set of screens on the inside reply atum47 18 hours agoprevDon&#x27;t know if this is the right topic to be asking this but here it goes anyways: is there a 360¬∫ interactive version of the U2 show at the sphere? reply WatchDog 7 hours agoparentNot sure how ‚Äúinteractive‚Äù it is, but yeah U2 are putting on a show there for a while. reply fecal_henge 18 hours agoprevMain hardware comment: 4 way right angled Lemo cables. Must be fancy. reply RcouF1uZ4gsC 20 hours agoprev> The venue will house the world‚Äôs highest resolution LED screen: a 160,000 square-foot display plane that will wrap up, over, and behind the audienceIt would be great for watching horror movies. reply jauntywundrkind 18 hours agoprevKeep having these vibe that: simulating Sphere feels like a good goal that could show off VR well. reply spacecadet 19 hours agoprev [‚Äì] No one has seriously shot 24p in 2 decades but Aaron. Time to catch up man... It takes his crew 12 people lol, this isn&#x27;t much worse then shooting existing 8 and 12k which at most is a single well trained DIT. I looked at this setup, its not any different, hype article. reply roughly 16 hours agoparentI think as a general rule when you‚Äôre using technical specs to argue that one of the greatest practitioners of their generation of whatever art you‚Äôre describing is doing it wrong, it‚Äôs probably worth sitting back and contemplating what point you‚Äôre trying to make and whether you‚Äôve maybe missed something important along the way. reply arp242 15 hours agoparentprevYeah, like that stupid one-eared Dutch painter who just did these kind of low-res pixelated paintings, unlike more realistic work like the old masters such as Rembrandt. What&#x27;s his name again? I doubt anyone remembers, or if there&#x27;s anything named after this doze. reply CyberDildonics 18 hours agoparentprev [‚Äì] No one has seriously shot 24p in 2 decades but AaronWhat is this supposed to mean? Movies are 24fps. Here is a short list of the movies that are not 24fps.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;List_of_films_with_high_frame_...at most is a single well trained DITIs DIT supposed to mean &#x27;Document Image Transformer&#x27; here? This is photography, not upscaling. reply spacecadet 17 hours agorootparent [‚Äì] Haha, let me grab a random list too... yours is particularly bad because you didn&#x27;t understand my point. Im talking about physical limitations of film, vs digital, vs human perception and the trade offs of budget, crew, and equipment.Shooting on a camera locked at 24fps is not equal to shooting 24fps. Spend the next 20 years doing it and come back to me. reply crazygringo 16 hours agorootparent> Shooting on a camera locked at 24fps is not equal to shooting 24fps.Care to explain?I know quite a bit about camera technology and don&#x27;t have the slightest idea of what you mean with this sentence. reply ZiiS 15 hours agorootparentThe only generous interpretation I can see is that Aronofsky uses extremely innovative techniques yet has long publicly voiced that he prefers to use analog film. For instance some of his signature style is short shots taken with cameras mounted on actors in dark conditions which vastly easier to realize with digital yet he has repeatedly chosen to use film. Whilst it is perfectly consistent that what would change his mind is an extremely powerful digital camera; it is also possible that the resulting PR spin causes some frustration to a generation of professionals who have been working in digital for 20 years and not been given shiny 18K cameras to play with. reply spacecadet 15 hours agorootparentHe only did that in Pi. Maybe thats all he is known for, but I wouldn&#x27;t call it his style anymore. reply ZiiS 15 hours agorootparentAnd Requiem but yes that was only one example, he has an amazing and varied output. reply spacecadet 15 hours agorootparentprevIt has nothing to do with tech. The same reason Daren shoots film is why I still shoot film, at any speeds. Since almost no one chooses to shoot true 24fps anymore, everything is sampled down and so everything I watch looks like shit. Sharp, but jittery shit.Use the tool for the situation. I actually respect the tech behind the sphere and 18k cameras, because it&#x27;s purpose built and I imagine looks lovely in its native speed and fov.But Daren... this piece is gross man. reply crazygringo 13 hours agorootparentYou still haven&#x27;t defined what \"true 24fps\" is, and the idea that it \"has nothing to do with tech\" makes no sense. 24 fps is a technical term, no more and no less.And everyone here is left baffled because it seems to be some personal notion of yours, as opposed to what it is universally and clearly understood to mean.I literally have no idea what you mean when you talk about things being \"sampled down\" or \"sharp, jittery shit\".The only thing sampled down in most modern TV&#x2F;film production is a usually a 4K source down into 1080p or less for distribution. But I can&#x27;t possibly imagine you&#x27;re objecting to that. And frame rates aren&#x27;t \"sampled down\".Meanwhile, by \"jittery\" I can only guess you mean shutter speed, not frame rate? Normal shutter angle is 180¬∞, and that has been standard for many, many decades, identical between film and digital, and which has never been jittery. Sometimes action scenes use a smaller angle (faster speed), which you&#x27;re free to call jittery, but that&#x27;s not the norm. Generally just for action scenes, or for occasional other artistic effect.If you want to have any kind of productive conversation on HN, I suggest you start explaining what you actually mean rather than throwing around terms with idiosyncratic personal definitions you simply invent, that have nothing to do with how the terms are generally understood. reply clnq 4 hours agorootparentI never thought about this, but what if you shoot at 30 fps and want to produce 24? How do you map 30 onto 24 time-wise? In 30, each frame lasts 33.33ms. In 24 it‚Äôs 41.66. So if the first frame is synced, the second frame of something captured in 30 and transformed to 24 would show up 8.3ms too late. The third - 16.ms, and so on. It‚Äôs not possible to properly sync video to 24fps if it was shot at 30 without interpolation, right? reply Kirby64 4 hours agorootparentIt‚Äôs commonly done the other way, actually. Converting 24fps to 30fps for converting film (typically shot at 24fps) to TV broadcast ready, which is 29.97 fps in the US&#x2F;NTSC region.That said, you can go the other way. Basically, you interlace certain frames. Wikipedia has a great article on this.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Three-two_pull_down#2:3 replyCyberDildonics 17 hours agorootparentprevyou didn&#x27;t understand my pointI still don&#x27;t. You said something that wasn&#x27;t true in any way, so I&#x27;m still not sure what the point is. Your list doesn&#x27;t seem to make any sense or have any connection to what you&#x27;re saying either.Im talking about physical limitations of film, vs digital, vs human perception and the trade offs of budget, crew, and equipment.If you say so, but what your comment actually said was that no one has shot 24fps in 20 years, which is nonsense.Then you said something about 18k not being impressive because someone could shoot at 8k and scale it up, but even that wasn&#x27;t very clear. reply spacecadet 15 hours agorootparentI think you&#x27;re expecting 3 decades of cinema tech knowledge to be dumped on you in a comment.No one shoots true 24. Anymore. Nor does any modern studio want to.That doesnt mean content isnt finalized at 24...18k is impressive, thats a-lot of heat. 12 people running a standard high res RAID, is not impressive and puffed up for press. reply CyberDildonics 13 hours agorootparentNo one shoots true 24. Anymore. Nor does any modern studio want to.This is not true, where are you getting this information? Converting a high frame rate to a lower frame rate is not like resampling a high res image to a lower res image. You have to interpolate or throw out frames, both of which are terrible compared to just shooting in the frame rate you want. reply vardump 13 hours agorootparentUnless it‚Äôs an integer multiple. Shrug. reply CyberDildonics 10 hours agorootparentNope, if it&#x27;s an integer multiple you still have to throw out frames and those frames could have been part of the motion blur of other frames instead. It isn&#x27;t the same because instead of taking a single exposure for half the time the frame elapses you would have two frames, each exposed for a fourth of that same time. reply Kirby64 4 hours agorootparentOr you just interpolate between frames if you want the same motion blur. It‚Äôs just like downsampling from 4k to 1080p. The process is clean since you can directly map 2 frames to 1 frame (or, 4 pixels to 1 pixel for resolution conversion). replyspacecadet 17 hours agorootparentprev [‚Äì] Here ya go, random list, tho mine is better matched to my point.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;List_of_films_shot_on_digita... replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Renowned filmmaker Darren Aronofsky utilized an 18K cinema camera, called 'Big Sky', for his new film, 'Postcard From Earth'. The camera, which requires approximately 12 operators, offers a resolution 40 times greater than standard 4K cameras.",
      "The film plays at 60 frames per second, double the speed of typical motion pictures, with a data rate of approximately 32GB per second and total size half a petabyte.",
      "'Postcard From Earth' will be exclusively screened at the Sphere venue in Las Vegas for two years. Aronofsky reported his astonishment at the audience's reaction to the film on the venue's formidable screen."
    ],
    "commentSummary": [
      "The article highlights the advanced features of the 18k Sphere camera, such as its high resolution, frame rate, bandwidth, and data size.",
      "It explores how this camera could revolutionize film-making, virtual reality (VR), and home viewing experiences.",
      "The discussions also extend to the complexities and compromises associated with converting between different video frame rates."
    ],
    "points": 134,
    "commentCount": 59,
    "retryCount": 0,
    "time": 1697892312
  },
  {
    "id": 37966767,
    "title": "Cratering motor fuel sales in Norway show the death spiral that can end oil",
    "originLink": "https://electrek.co/2023/10/20/cratering-motor-fuel-sales-in-norway-show-the-death-spiral-that-can-end-oil/",
    "originBody": "Skip to main content Go to the Electrek home page Switch site Toggle social menu Toggle dark mode Toggle search form Exclusives Autos Alt. Transport Autonomy Energy Tesla Shop OIL AND GAS NORWAY Cratering motor fuel sales in Norway show the death spiral that can end oil Jameson DowOct 20 2023 - 3:18 pm PT 158 Comments It‚Äôs common knowledge that Norway is the land of electric cars and that the country keeps breaking EV sales records with virtually no new fossil vehicle sales. But what‚Äôs really important is the effect those EVs are having on oil sales, which are in steep decline in the country as a result ‚Äì and the same thing could happen elsewhere. Norwegian statistics agency SSB released its latest numbers on motor fuel sales today, showing a whopping 9% decline in motor fuel sales year-over-year for the month of September. This is a result of Norway‚Äôs world-leading EV sales, with over 90% of new vehicles in the country having some sort of plug and vanishingly few having no electrification at all. The country has exceeded its own high expectations, virtually ending fossil vehicle sales years ahead of schedule. However, there are still fossil vehicles on the road from previous years that are continuing to pollute and use fossil fuels throughout their lifecycle. But as they age and are replaced almost solely with EVs, the vehicle fleet cycles out from fossil to electric. If it takes 10-15 years for the vehicle fleet to cycle out, then that means Norway would remove ~6-10% of fossil cars from the road every year, replace them with electric cars, and thus reduce motor fuel usage by a similar amount every year. But this trend is nothing particularly new. While this big 9% drop is just a one-month snapshot, petrol/gasoline sales have been in decline for about two decades in the country, as diesel started to replace petrol in the mid-2000s. But diesel has also been in decline for the better part of a decade, as electricity has replaced it as a motor fuel. Stats from Robbie Andrew‚Äôs excellent Norway EV stats tracker at robbieandrew.github.io/EV/ To compare against other rapid declines, US coal usage has gone from a peak of 1,045 million tons in 2007 to 469 million tons in 2022, a decline of about 5% per year (and going from ~50% of the US electricity mix to ~20% now, and dropping). Many observers acknowledged, even near the beginning of this trend, that coal was a dead industry. Any subsequent attempts to expand it have been unserious political stunts that were doomed to fail from the start ‚Äì everyone (with a brain) knows the industry is dead. But in that context, Norway‚Äôs decline in motor fuel sales seems to be happening almost twice as fast on a percentage basis as the United States‚Äô decline in coal use, at least according to today‚Äôs data point. And the long-term trend may accelerate as the country now has virtually no gas vehicle sales. This is important because when we talk about electrifying the auto industry, the point is not just to get people into better cars with neat new technology. The point is to reduce oil consumption, such that carbon that belongs underground stays there ‚Äì permanently. This is vitally important because if we burned even a fraction of all the oil that is already discovered and owned by oil companies, the carbon released would cause catastrophic climate change. This was covered in Bill McKibben‚Äôs excellent 2012 article ‚ÄúGlobal Warming‚Äôs Terrifying New Math.‚Äù The only way we can avoid this fate is through one of the more wonderful phrases in the English language: ‚Äústranded assets.‚Äù In this context, the phrase refers to oil reserves owned by oil companies which get written off of those companies‚Äô books because they are uneconomical to extract and sell. In short, oil companies need to lose money, and lots of them need to go bankrupt. And while Norway is just one relatively small country, news like this shows how that could happen as EV sales (and better yet, even cleaner methods of transportation like e-bikes and public transit) grow rapidly worldwide. Oil demand -> oil prices -> oil supply There is an interplay between oil demand, oil prices, and oil supply that could lead to a death spiral for the oil industry. Lately, oil prices have been quite high around the world, nearing the historic highs of the 2010s and late 70s. This spike has largely been driven by pandemic-related supply (and demand) disruptions, the Russian invasion of Ukraine, and, as always, the decisions of Saudi Arabia (in this case, their decision to cut supply to buoy oil prices). But looking back to the last peak, we can see another interesting thing: a giant drop in oil prices in the mid-2010s, which was driven by a ‚Äúsupply glut.‚Äù This supply glut was at least partially related to increased usage of hybrid and electric cars, which led to a relatively small decrease in oil demand. However, that small decrease meant that more oil was being pumped than used, which led prices to drop by about two-thirds in a matter of months. The effect of oil prices on consumer demand is that as oil prices go up, usage (often) goes down, and interest in electric cars goes up. This stands to reason, as people start thinking about more efficient vehicles when the cost of fueling their vehicle becomes too much. But the effect on supply is less popularly examined. In this case, low oil prices can actually be environmentally advantageous because it means that oil companies are less incentivized to explore new methods of extraction and that more expensive methods (such as tar sands extraction, which is also much more environmentally costly) become uneconomical. If it costs more to extract the oil than the oil is worth, then the project won‚Äôt get started. And if the project doesn‚Äôt get started, then the oil stays in the ground to begin with, right where it belongs. So, in a way, low oil prices can actually be better for the environment than high oil prices. This means fewer projects get started, and more projects and companies go bankrupt due to high costs and low profits. And this is the spiral that we want to see. As the primary driver of oil demand (vehicles, specifically consumer vehicles) disappears, oil prices can drop because of this supply-demand imbalance. Then, there will be less reason for companies to extract oil in the first place, leading to the stranded assets we spoke of before. Some regions with low cost of extraction might even prefer it this way and work to ensure this happens. The Middle East can extract oil for cheaper than anywhere else, so it could be to their benefit to put high-cost extraction methods out of business. Norway itself is an oil country (primarily for export, at this point) and has middling oil-extraction costs, but it may benefit in the short term from a shakeout of higher-cost countries. But ideally, Norway‚Äôs extraction would soon become uneconomical ‚Äì and hopefully, so will Saudi Arabia‚Äôs. Top comment by jrharbort Liked by 10 people There's another way the supply and demand story can go, and is already heading that direction in the U.S. due to our much larger impact on the oil industry. Every day in the U.S. now there is enough EVs sold displace an entire gas station-worth of fuel. Per day. And this is rapidly accelerating. Gas stations already don't make much profit on selling just gas, so what can they do to continue making profit when less gas is being sold? They are forced to raise prices. Gas stations and their operating oil companies look at the short term profits, not the long term. They're not going to engage in a long term low price war in the hopes that people buy more gasoline cars. They know that is coming to an end. Additionally, lowering prices wont help in the short term with selling more gas as people already can't buy more than their tanks can hold. All they'll keep doing is raising prices to compensate for less gasoline sales, which will push more and more people to EVs. It's a vicious cycle where the oil industry will die by thousands of self-inflicted cuts. View all comments The one danger of this path is that if oil demand does drop low enough, low oil prices could jeopardize consumer decision-making to move to cleaner options. Oil is subsidized to the tune of trillions of dollars worldwide per year based on unpriced external costs that all of us are paying on the back end ‚Äì usually in the form of higher hospital bills or other environmental costs. This could be solved by finally properly pricing oil globally, as Norway already rightly does. Norway‚Äôs realistic pricing for carbon pollution has helped to ensure that the true price of oil is reflected in consumer pricing, making it more apparent to consumers that fossil vehicles are not an economical option for society or their pocketbooks. In contrast, the artificially low gasoline costs in the US (yes, US gasoline prices are still artificially low, even at today‚Äôs high prices) work to buoy consumer oil demand. Removing the ~$650 billion in implicit subsidies received by the fossil fuel industry in the US alone would help ensure that fair market conditions could prevail, and consumers would have a clear choice about what the better and cleaner option is. And if we finally let the market work freely, after more than a century of both direct and implicit oil subsidies that have coddled this lying, deadly industry, we could finally see it spiral into the oblivion it deserves. Add Electrek to your Google News feed. FTC: We use income earning auto affiliate links. More. Stay up to date with the latest content by subscribing to Electrek on Google News. You‚Äôre reading Electrek‚Äî experts who break news about Tesla, electric vehicles, and green energy, day after day. Be sure to check out our homepage for all the latest news, and follow Electrek on Twitter, Facebook, and LinkedIn to stay in the loop. Don‚Äôt know where to start? Check out our YouTube channel for the latest reviews. Featured from Electrek slide 1 to 2 of 4 Podcast: Tesla Cybertruck update, Elon Musk‚Äôs terrible call, TSLA earnings, and more Fred Lambert Oct 20 2023 Tesla shot a Tommy gun at a Cybertruck, here‚Äôs the result Fred Lambert Oct 20 2023 Aventon Adventure.2 e-bike now $700 off, Rad Power RadRover 6 $1,799, more Rikka Altland Oct 20 2023 Apple Maps EV routing now rolling out to F-150 Lightning trucks from 9to5Mac Oct 20 2023 Subscribe to Electrek on YouTube for exclusive videos and subscribe to the podcast. Comments EXPAND COMMENTS Guides oil and gas Norway Author Jameson Dow Jameson has been driving electric vehicles since 2009, and has been writing about them and about clean energy for electrek.co since 2016. You can contact him at jamie@electrek.co Jameson Dow's Favorite Gear Aptera Referral Code Use our Aptera Referral Code for $30 off a reservation for the upcoming Aptera electric vehicle.",
    "commentLink": "https://news.ycombinator.com/item?id=37966767",
    "commentBody": "Cratering motor fuel sales in Norway show the death spiral that can end oilHacker NewspastloginCratering motor fuel sales in Norway show the death spiral that can end oil (electrek.co) 121 points by cs702 20 hours ago| hidepastfavorite192 comments macNchz 19 hours agoA couple of comments on the article point out that if gas stations start closing due to reduced demand, it will accelerate the effect by making it more cumbersome to own a gas car. Recently, some of the industrial area near my apartment in NYC was rezoned for housing, and nearly all of the local gas stations were torn down (not because of low demand but because a 15 story building would be more profitable). The result is that even though I own a gas car, I now know of more‚Äìand more convenient‚Äìplaces in the area where I could charge an electric car than I do gas stations. Interesting shift! reply thelastgallon 17 hours agoparentOwning a gas car will become inconvenient and expensive very soon. The number of gas stations has gone down from 202K to 115K, ~50% down[1]. Gas stations make very little money on gas, most of their income is from people buying cigarette, lotteries, lotto, beer, beverages, and some trinkets in the store. All profits are at the margins. When foot traffic drops, even by 10 - 20%, gas stations are not viable. They can increase gas prices, but that will further reduce foot traffic, because most everyone uses gasbuddy or similar apps to find the cheapest gas. And Costco, Walmart, etc can be extremely competitive on gas and even make these loss leaders to drive foot traffic as well as EV charging spots. Gas stations have no move left to gain foot traffic.10% EV adoption is not far off, thats when we start seeing collapse of gas station. EVs charge at home or work, except when going on long rides. Gas stations lose this foot traffic forever. Adding EV spots at gas station won&#x27;t help, there is nothing for people to do to for 15 - 20 mins at a gas station, they&#x27;d rather go to Target, Walmart, Costco and do some grocery shopping.Most people don&#x27;t understand how quickly things change. We went from flip phones to smart phones in less than decade. Neil DeGrasse Tyson explains How The Horse Industry Vanished in less than 20 years: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Lvzw7NPmppYFolks buying new ICE cars will soon ( So, obviously other factors were driving that trendYes, gas sales are nearly zero in US, they are just starting. I agree other factors were driving that trend. Probably gas stations figuring out that its not a viable business. But now, EV sales have started. 25% of cars sold in California are EVs and growing. If gas stations have gone down 50% (202K to 115K) without any EVs, we can predict they will go down faster with EVs.> gas sales industry was continuing to meet consumer demandThere is no consumer demand for gas sales, any more than there is a demand for whale oil or kerosene. There is a consumer demand for lighting, and whale oil, then kerosene were used in lamps obsoleted by electric lamps. Similarly, there is a consumer demand for going from point A to point B, most people are not gas-holics, they&#x27;ll use whatever fuel is cheapest (first and foremost) and convenient. Well, EVs are most convenient, you can charge anywhere you park, it is hard to think of any building (home, work, shop) without electricity. If the biggest challenge that mankind is faced with is running a cable from a building to parking lot and we fail spectacularly at that, we are doomed. However, we have precedence that we can do this. Standard oil is built in consumer demand for lighting and kerosene lamps. When light bulb was invented, its not just about convincing people to buy electric lamps. It is also about doing the far harder things of producing electricity and distributing it. This grid was built and people switched to electric lamps. So, I am someone confident people will figure out how to run a cable from a building to parking spot. US has 2 billion parking spots and ~250 million cars, just converting a small fraction to EVs is enough.An EV is a bigger breakthrough than a smartphone (which is primarily a content&#x2F;addiction delivery mechanism). An EV solves energy storage, an unsolved problem of civilization. EVs can provide passive income. Folks who participated in a VPP pilot with powerwalls made $150&#x2F;day [1], this is the very definition of passive income. A powerwall has 13.5kWh capacity vs car batteries of 80kWh, there is nothing preventing cars from supplying power to the grid at peak prices and making bank. Most of our electricity bill is for peak power, which is from costly natural gas peaker plants. These will all be replaced.Next, there is significant curtailment of renewables, which overproduce and there is no demand. Power prices go negative (200 million times&#x2F;year [2]), which means an EV owner can be paid for the privilege of charging his car. I can think of at least a few people who&#x27;d be interested in getting paid for charging their car, or even charge for free.An EV is primarily an energy storage device (idle in parking lot 95% of the time), which provides elastic demand, as well as supply. Nothing else on the grid has this capability. This capability changes things permanently. Think of a time before information storage, youtube&#x2F;instagram&#x2F;tiktok -- all of this content and businesses were not possible before we figured out how to store information (hard disks, ssds). EVs bring that capability. This is a significant civilization upgrade that will enable many things in the future.[1] https:&#x2F;&#x2F;electrek.co&#x2F;2023&#x2F;07&#x2F;05&#x2F;tesla-electric-customers-repo... [2] Wholesale prices went negative about 200 million times across the seven US grids in 2021: https:&#x2F;&#x2F;www.bloomberg.com&#x2F;news&#x2F;articles&#x2F;2022-08-30&#x2F;trapped-r... reply fy20 11 hours agorootparentprevIf that&#x27;s the case, then why are they making it more convenient to pay?Where I am (in Europe) all the major petrol stations have mobile apps you can pay with. One even launched a system where you can register your license plate, and whatever you fill up is automatically charged to your account.If foot traffic is all that makes money, it seems they should want you to go into the store, not pay from your phone. reply PopAlongKid 14 hours agorootparentprev>Gas stations make very little money on gas, most of their income is from people buying cigarette, lotteries, lotto, beer, beverages, and some trinkets in the store.This omits a significant factor, which is that it used to be many gas stations also had several repair bays, employing a mechanic or two.Anecdata: between mid-1980s and 2000, four such gas station&#x2F;repair businesses within a mile of my house closed, leaving three gas&#x2F;convenience store businesses remaining within that same radius, some of which used to have repair service as well.This used to be a solid small business opportunity for a solo or partnership owner&#x2F;worker. I suspect that as cars became more computerized the repair business became too specialized, and also some of the stations which were leased from large oil companies were sold out from under the operator. reply jliptzin 17 hours agorootparentprevYea, exactly, I have been telling friends to think hard about buying a new ICE vehicle for this reason. Gas isn‚Äôt a utility like electric or water. Stations are all privately owned, as soon as it‚Äôs no longer profitable to operate, it‚Äôll shut down. If you‚Äôre in a small town and it‚Äôs the only one, sorry, you now have to drive extra distance to get to the next one. The whole thing will collapse like a house of cards relatively soon and the bag holders will be people with ICE cars that they can only sell for scrap metal.I guess if you have to, then leasing is the only reasonable way to go? reply throw_pm23 17 hours agorootparentprevGood observations (and relevant username).Just pointing out that people spending 15-20 mins at stations is not unrealistic. At least near motorways, gas stations already position themselves as a place where you can do your regular rest during a long drive, with diners, cafes, children&#x27;s playgrounds, etc., some drivers even catch a quick nap in the parking lot -- this is how it looks in Europe at least. reply prepend 16 hours agorootparent> Just pointing out that people spending 15-20 mins at stations is not unrealistic.This is super unrealistic due to volume.I filled my car at Costco this morning. I spent about 3 minutes pumping 10 gallons. At the same time, 25 other cars filled up at the 25 pumps. And there was a line backed up. To change refuel times from 3-5 minutes to 15-20 (rather optimistic based on my supercharging experience) won‚Äôt work unless you increase the charging infrastructure significantly.People will need to charge at home, not at gas stations. And motorway stops are going to have to have hundreds of chargers not the current dozen or two. reply lamontcg 16 hours agorootparentI can&#x27;t remember the last time I had to wait in line for gas that wasn&#x27;t at Costco. Costco is a pretty extreme outlier.But agreed that charging should shift to the home, the problem there is chargers for on-street parking though for people who don&#x27;t have driveways&#x2F;garages (of course this might eventually get solved by fully automated vehicles and lowering ownership of vehicles, but that&#x27;s still vaporware right now while EVs are clearly here right now). reply prepend 12 hours agorootparentIt‚Äôs not the waiting that‚Äôs the important part. It‚Äôs that lots of people want to get gas and they can‚Äôt all wait 20-30 minutes to recharge. reply robocat 11 hours agorootparentEspecially problematic during peak periods like longer distance holiday travel? reply fragmede 15 hours agorootparentprevIt&#x27;s not too hard to imagine parking meters having charging ports. reply mulmen 14 hours agorootparentPorts is right. This is only viable if the cars bring the cable.Seattle tried installing chargers at public street parking spots and the cables were stolen. reply lightbritefight 12 hours agorootparentThere are endless street level, public EV chargers in Seattle with cables.Some random chargers being vandalized isn&#x27;t really indicative of anything. reply erik_seaberg 11 hours agorootparentprevThat&#x27;s a first step, but I don&#x27;t see parking meters in US suburbs. Some barely have streetlights. reply fragmede 7 hours agorootparentthough, In the suburbs, you&#x27;re more likely to have a house with a garage where there can be a charging port. reply simfree 15 hours agorootparentprevRunning a 240v 50A power cable is orders of magnitude cheaper than building a gas pump. reply lightbritefight 12 hours agorootparentprevNow imagine if instead of those 25 pumps, each parking spot in the regular costco parking lot had a charging station. Suddenly 25 min isn&#x27;t a big deal, and you \"fuel\" just as many, if not more cars in the same time frame as the mega large gas station, which itself could be even more charging spots. reply badpun 16 hours agorootparentprevThe stations won&#x27;t \"collapse\", but merely the number of them will start declining proportionally to the gas vehicles losing popularity (assuming it actually happens). This way, each of the remaining gas stations will have a greater share of the (decreased) foot trafic, thus still maintaining profitability. reply Arnt 13 hours agorootparentThat&#x27;s not how it works in Norway. A lot of stations are so far from the closest neighbour that they have a quasi-monopoly in their area, and \"so far\" is a surprisingly low number, according to the hearsay I&#x27;ve heard.The hearsay also says that they close when they need expensive maintenance, not before. reply zdragnar 18 hours agoparentprevICE cars don&#x27;t need to fill up near home, because it is so quick. EVs ideally fill up where they&#x27;re going to be unattended, because it takes longer.It makes sense that you&#x27;d find more EV charging in residential areas and more gas stations near where people travel and work. reply TheRealPomax 18 hours agorootparentif you have to drive 15 minutes there and 15 minutes back, that&#x27;s not faster than a fast-charge top-up at the fast charge point down the street, though. There&#x27;s a critical density below which if you have a charge point nearby the nuisance of \"going out for gas\" and not being able to do anything else during that time isn&#x27;t worth it compared to plugging in your car, and then just walking away for an hour. Especially if it&#x27;s just down the road and you can just walk home, then walk back an hour later to pick up your car.Because that&#x27;s what&#x27;s really in competition: you have to be with your car the entire drive to and from the gas station, plus the few minutes it takes to fill up and pay, whereas you don&#x27;t have to be with your car when you&#x27;re plugging it in to charge: that time is now yours. reply daveFNbuck 18 hours agorootparentI don&#x27;t think most people make a special trip just to get gas. It&#x27;s usually a quick added stop along the way of another trip, which is less of a nuisance than having to make two small walks to move and plug&#x2F;unplug your car. reply chongli 18 hours agorootparentThe GP was responding to a point about the closure of gas stations accelerating the decline of ICE vehicles. If all the stations near where you normally travel close, then you are forced to make a special trip to fill up. reply mcpackieh 9 hours agorootparentThe last gas stations standing won&#x27;t be in remote areas far from everything else. They&#x27;ll be in heavily trafficked areas that everybody goes to regularly, particularly near grocery stores. reply vel0city 18 hours agorootparentprevBut if there&#x27;s no longer any gas stations on any of my usual trips, it&#x27;ll become a special trip.Also it&#x27;s not fun having to make your trip even longer stopping to get gas. I&#x27;d prefer to just leave the house with a full tank every time. reply ianferrel 17 hours agorootparentEven if like 80% of gas stations close, the remaining 20% will be situated along major transportation arteries. I expect there will be a gas station somewhere along someone&#x27;s regular weekly driving route for all but the most remote dwellers. reply TheRealPomax 18 hours agorootparentprevAnd in those cases plugging in your EV amounts to no net gain or loss, but there absolutely are people who already have to make a dedicated trip to the gas station because their own town where they do their shopping simply doesn&#x27;t have one. reply zdragnar 14 hours agorootparentSure, but this conversation was about an urban area being converted from industrial to residential.The people who will be living there with ICEs will certainly be driving them past gas stations elsewhere in the city. reply nucleardog 18 hours agorootparentprevI don‚Äôt know anyone that often drives somewhere specifically to get gas. As long as you occasionally drive past a gas station during your travels you just stop in for 3-5 minutes and fill the car up.Even if there isn‚Äôt one directly on your route, we‚Äôre talking a long ways off I think that you need to travel 15 or minutes from every point of your trip to each a gas station. Like, we&#x27;ve reached such a low density that the entire distribution network is barely even sustainable anymore. reply Arnt 13 hours agorootparentWhat you&#x27;re descriving is a lot of Norway. Low density, and employees cost a lot per hour. reply TheRealPomax 18 hours agorootparentprevTime to move to the country side. Discover a whole new aspect of \"...so you&#x27;re saying I can get basic groceries in town, but gas is 15 minutes further in the opposite direction of where I live?\". reply fbdab103 18 hours agorootparentprevFew people make a special trip to fill an ICE car. It is almost always a side trip on the way to another activity. reply jliptzin 17 hours agorootparentFew people today reply billfor 18 hours agoparentprevWhere do you see convenient charging stations? Tesla shows 4 superchargers in all of Manhattan and 3 in Brooklyn, not including 3rd party stations that can take 2 hours to charge the car. What is the population of Manhattan and Brooklyn? reply jliptzin 17 hours agorootparentI have had a tesla for 4 years, I have used super chargers maybe a dozen times total. And I do about 15,000 miles&#x2F;yr. reply macNchz 17 hours agorootparentprevThere are curbside charging locations popping up around my neighborhood and most parking garages seem to have some charging capacity now. There certainly isn‚Äôt enough infrastructure for the whole population of car owners to go electric tomorrow, but it‚Äôs interesting to see big changes happening in real time. reply thebruce87m 15 hours agorootparentprevI only use Superchargers for long journeys, and I don‚Äôt do many. Even when I do it‚Äôs only 20mins max and then off.I haven‚Äôt used any public charging in the last 3 months. reply jncfhnb 18 hours agorootparentprevYou don‚Äôt need the superchargers reply TheLoafOfBread 4 hours agorootparentOh yes, because people in Manhattan lives usually have driveway. Of course. reply rho138 18 hours agorootparentprevJust a friendly reminder that Tesla isn‚Äôt the only show in town. Chargepoint shows several thousand charging stations just in manhattan. reply goles 18 hours agoparentprevPerhaps not applicable to NYC, but isnt it kind of troubling that people who cant afford a luxury vehicle (55-100k) could be put in a position where they have to spend more money (gas) to go fill up out of their way, taxed for being poor, or shut up and take public transit (if it even exists where they live)?Surely no one driving an EV ever got to their position to buy one driving an ICE.Edit: happy this comment inspired lots of spirited discussion. Would like to stress the \"Perhaps not in NYC\" part and if this trend spreads to say Oklahoma. reply corbet 18 hours agorootparentNot in New York... But our Bolt, while not the cheapest car ever, certainly doesn&#x27;t match your description here. One doesn&#x27;t have to buy a Tesla to get an EV. reply goles 18 hours agorootparentWould you be willing to share what the car has cost in total (maint, elec, insur, ect) over the years? Genuinely curious. reply sokoloff 17 hours agorootparentI daily a 2015 LEAF, bought in Dec 2014.It had a warranty battery repair (to replace a failing module, free of charge to me but a possible concern long-term).As the guy who turns all the wrenches on the family cars, it‚Äôs been incredibly low maintenance. Wiper blades twice, cabin air filter three times, and I had to charge the 12V battery once in the peak of COVID shutdowns when the car sat for 8 weeks or so with a Bluetooth adapter plugged into the OBD2 port. It‚Äôs about due for tires all around.It was $32K before $10K in government cheese, so $22K for a new car was pretty good, but, like any then-new car, it has been a lot more expensive than buying a 2011 Honda CR-V would have been. I paid that premium in order to see if I&#x27;d enjoy having an EV. Overall, I have, but only because we have a 2005 Honda CR-V for long-distance travel&#x2F;road trips, etc. The LEAF is entirely impractical as an only car (though other EVs are better in this regard).Our electricity is expensive in MA, so energy costs about $0.06 per mile. That compares slightly favorably (about half) to a 35 mpg car buying $3.70&#x2F;gal gas and doing periodic oil changes.Around town or to neighboring towns, we choose to drive the LEAF almost 100% of the time. If it had a better CarPlay story, we&#x27;d probably choose it 100% of the time. (If I&#x27;m not sure where I&#x27;m going, the aftermarket Android head unit with CarPlay maps that I installed in the CR-V is easier and safer than the terrible factory nav in the LEAF.) reply lostlogin 16 hours agorootparentThe newer model Leaf has CarPlay, and as you say, it‚Äôs a deciding factor in which car we take. reply MostlyStable 17 hours agorootparentprevNot who you originally asked, but I bought one of the cheaper EVs earlier this year (Hyundai Kona, slightly more expensive than the Bolt but in a similar class). With fuel savings included, (which, for our use case was between $100-$130&#x2F;month), it was hard to find an ICE vehicle that met our needs for cheaper when we were shopping. We did have to compromise slightly on size (it&#x27;s smaller than we wanted) in a way that we wouldn&#x27;t have had to with an ICE vehicle, but so far, it&#x27;s absolutely been cheaper and easier than my previous ICE vehicles. As I said to some relatives not that long ago: even if I found out 100% conclusively that electric vehicles had exactly zero environmental benefit over ICE vehicles, I&#x27;m not sure I&#x27;d ever go back. I just like it more as a vehicle. reply semi-extrinsic 17 hours agorootparent> We did have to compromise slightly on sizeThis is my main trouble with EVs available so far. If I want something that&#x27;s not substantially smaller than what our current Ford Galaxy has, and I want to still have the tow bar and roof box, there are very few EVs available and they cost $$$.We&#x27;re talking stuff like the Nio ES8 at $70k+, or the Mercedes EQS at $105k+.Or you can go down the van route and get a Peugeot e-Traveller ($65k) or Mercedes EQV ($85k) or something like this, you get even more space but your range is abysmal at 150 to 200 miles.Meanwhile on the ICE side you can get the spacious Dacia Jogger for just $15k new! Then you have $40k+ left to spend on gas and maintenance... reply acdha 17 hours agorootparentprevEVs don‚Äôt have to cost 55k. Most of the people I know who drive them paid under $30k, almost half of the average gas-powered vehicle, and they‚Äôre so much cheaper than ICE vehicles to maintain and operate that they‚Äôre saving considerably more over the life of the vehicle.Transit is cheaper still but has the problem that it‚Äôs nowhere near as subsidized as private vehicle travel but expected to have high fare box recovery, and the transit experience is directly worsened by private vehicle use. Simply subsidizing car ownership less would help there by not masking the true costs and reducing congestion. reply jncfhnb 18 hours agorootparentprevI think it‚Äôs more troubling to view owning a car as a base right that everyone should have. It‚Äôs better as a luxury. We want fewer cars and for people to live in walkable areas where cars are not needed. reply guidoism 17 hours agorootparent100% agree. But‚Ä¶. how do we get there? A LOT of people in the US living in very (gross) spread-out areas where there is horrible pedestrian infrastructure. The infrastructure that does exist seems like it was put in place to mock those without cars (sidewalks on extremely loud stroads). I currently live in such a place.The end-point I want is for people to live in dense walkable areas by default. But people don‚Äôt live there and the few places that exist like that are disgustingly expensive because they are (surprise surprise) desirable patterns of building cities. So we need to build more of it. It will be a very big very energy intensive build-out program that will take decades. So the question is: Is it better or worse for the environment to move people to ICE so they can live in their existing buildings or is it better to build entirely new infrastructure so they can walk.Maybe my thinking is too black and white. I want the latter but I‚Äôm not sure the costs to the environment in the short term are justified. Maybe the long term benefit justifies it. I don‚Äôt know. reply acdha 17 hours agorootparentI think there are two key moves: one is removing the subsidies for private car usage so the true costs are more visible and people can make better choices (c.f. The High Cost of Free Parking), and another would be removing low-density requirements ‚Äì most expensive cities could increase density rapidly if, say, every homeowner had the ability by-right to add an ADU or convert a single house into a multi-family without decades of recreational lawsuits from the neighborhood NIMBYs. Anywhere built before 1950 probably has a bunch of neighborhoods with little business zones which are marginally profitable because there isn‚Äôt quite enough local business but would do well if the density went up by 20-30%, and the profits would probably encourage a virtuous cycle of other neighbors upgrading. reply Podgajski 15 hours agorootparentprevI am homeless and live in a 2001 Dodge grand Caravan. So they‚Äôre not even thinking about people like me. Nor the hundreds of people I see parked out in the deserts of Arizona because they can‚Äôt afford housing.If they want EV adoption, they also have to Provide housing. reply DrReachAround 8 hours agorootparentprevIn short, high density housing, apartments. Not everyone wants to rent a little apartment the rest of their lives, particularly with the incredible limitations they bring with what you can do at home. There&#x27;s a whole world out there that apartment dwellers can&#x27;t participate in, and most people would prefer not to live in an apartment if given the choice and the financial means. reply jncfhnb 8 hours agorootparentThat‚Äôs fine but it‚Äôs not a fantasy that we should be specifically subsidizing for people that cannot afford it. reply danaris 13 hours agorootparentprevThat&#x27;s a great position to have‚Äîanywhere in the developed world but the USA.Here, it&#x27;s a great aspirational position to have, but wholly impractical in the short or medium term.Perhaps, if the Republican Party genuinely collapses within the next couple of years, it might become possible to pass the kinds of massive infrastructure overhauls required to make owning a car anything but a necessity in the vast majority of this country. Until and unless something that drastic happens, their adamant resistance to that kind of measure makes it very clearly impossible. reply ben_w 18 hours agorootparentprev\"\"\"The reason that the rich were so rich, Vimes reasoned, was because they managed to spend less money. Take boots, for example. ... A man who could afford fifty dollars had a pair of boots that&#x27;d still be keeping his feet dry in ten years&#x27; time, while a poor man who could only afford cheap boots would have spent a hundred dollars on boots in the same time and would still have wet feet.\"\"\" - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Boots_theory reply jncfhnb 18 hours agorootparentThis effect is true but the thesis is dumb. The rich are rich because of the compounding effects of wealth. Not that small fraction spent on durable goods. reply throw0101c 17 hours agorootparent> The rich are rich because of the compounding effects of wealth. Not that small fraction spent on durable goods.The less you (have to) spend on goods and services the more you can keep invested to compound.If 100% (or more) of your income is taken up by paying for necessities (shelter, food, transportation), what do you have to compound?* https:&#x2F;&#x2F;ofdollarsanddata.com&#x2F;the-biggest-lie-in-personal-fin... reply jncfhnb 15 hours agorootparentNo, this is the misleading thing. Which your article is largely agreeing with me on.The compounding effect of wealth scales with the amount of wealth you have more than the incremental savings you‚Äôre likely to earn. Trimming your expenses won‚Äôt make a difference. If you have no wealth and low income then the compounding is irrelevant. Even if your income is high it‚Äôs very hard to acquire ‚Äúwealth‚Äù.Poor folks will be poor even if you chop their expenses by half. reply candiddevmike 18 hours agorootparentprevI think the OP demonstrates the compounding effects of wealth, just not in a direct money-creates-money way. reply jncfhnb 18 hours agorootparentShrug. It‚Äôs a tiny drop in the bucket. reply ben_w 17 hours agorootparentWhen you&#x27;re rich, it is.When you&#x27;re poor, you can&#x27;t afford the bucket. reply jncfhnb 15 hours agorootparentHaving the drop in the bucket is not, however, a path to being rich. reply ben_w 14 hours agorootparentIt is when your standard for \"rich\" is framed in terms of \"can afford to buy good boots\". reply jncfhnb 10 hours agorootparentWhich is not the definition implied by the original quote so great. We agree. reply ben_w 1 hour agorootparentWhat are you even talking about? This is literally called \"boots theory\" and is explicitly giving boots as an example. That&#x27;s what \"framing in terms of\" means. replyyazaddaruvala 17 hours agorootparentprevBasically all EV sales are new EVs and the used EV market is still very hot.There will be a lot more ‚Äúcheap‚Äù EVs in 5-10 years as the first EVs depreciate and sell for $3,000 type thing. reply ljf 1 hour agorootparentIt is crazy how fast that is happening here in the UK. Just over a year ago a friend bought an ev VW Golf and was convinced by the delay telling him the current resale value for a year old 2nd hand ev Golf was only a 10% drop.He took the car to tide him over to a great deal we were supposed to be getting at work on EV cars. Went back to sell the car to the dealer and the chap said best he could offer would be ¬£8k (well over a 50 percent drop), and he actually didn&#x27;t want it even at that price as he had 5 on his forecourt already.Last summer we wanted an automatic, and vaguely looked at EVs - and they were totally out of our budget. This year we totally could have bought one.I&#x27;d say it is possible to geta ¬£5k ev here in the uk now, and in the next year or two it will get even better for the 2nd hand market. reply Scoundreller 18 hours agorootparentprevIt‚Äôs NYC. There‚Äôs going to be an underground gas distribution that fills that gap. reply boringg 18 hours agorootparentprevChina has EVs that aren&#x27;t luxury and I&#x27;m pretty sure Europe does as well if I am reading your comment correct you are trying to insinuate that EVs are luxury. If not your comment needs some clarity. reply guidoism 18 hours agorootparentICE are to EV as HDs are to SSDs. My intuition is that because there‚Äôs a floor to the cost of HDs (I think at some point it was around $35) and no practical floor to SSDs, there might be a similar story with cars. I know it‚Äôs really a bad analogy.Cars still need brakes (for now) and tires and lots of other stuff, but there‚Äôs a huge amount of infrastructure built in a car to help it blow up million year old leaves. EVs are simpler so should be able to be made cheaper (eventually).Maybe that means that what we expect from a car changes. If it costs you $10,000 to do all the stuff for a gasoline engine then a $1,000 AC isn‚Äôt a huge addition and there are maybe a bunch of other thousand dollar decisions that get added to make the cars cost what they cost.Makes me think of stuff that used to be luxury items that came in expensive packaging and came with expensive accessories and now the actual item is so cheap the old packaging costs more than the actual item. reply cmrdporcupine 17 hours agorootparentSorta, but. It&#x27;s not really analogous. The \"floor\" on EV cost is the battery. It&#x27;s the hugest bottleneck and although prices &#x2F; kWh have come down drastically it&#x27;s still extremely high because supply is just not there.Meanwhile the floor on an ICE is more the complexity of manufacturing and labour. Some of which exists also for EVs. reply goles 18 hours agorootparentprevWhat is the least expensive EV one could buy in the middle of nowhere North America?I&#x27;m open to anyones knowledge in this area I dont have but our primary mode of transportation when we were struggling was a $2,000 USD 30 year old Toyota.If you don&#x27;t think EVs are a luxury than we both need some clarity. reply macNchz 17 hours agorootparentOn cars.com right now it looks like the electric price floor is roughly $5k, which gets you a 10 year old Nissan Leaf, though it looks like they mostly have lower mileage than comparable-age ICE cars.Used car prices have ballooned in recent years, so I‚Äôm not sure how much lower you can go without buying a complete heap. As a comparison I looked at the cheapest Nissan Sentras and Versas on the market, which are about $4k, a bit older, and have 200k miles. reply vGPU 17 hours agorootparentA car with a 40 mile effective range is not a realistic solution. reply mulmen 14 hours agorootparentWhy not? It would have done my commute when I was struggling in Idaho and would easily do my commute since moving to civilization. reply sannee 18 hours agorootparentprev> I&#x27;m pretty sure Europe does as wellDoes it? A cursory search seems to put an (used) EV at around 10k$ at best. Poor people definitely do not drive 10k$ cars here. Hell, even my tech-adjacent friends in their twenties rarely own anything more expensive than 3k$-ish. reply bryanlarsen 17 hours agorootparentThe scrap value of a car is now $3k, so cars that actually run are north of that price these days. reply cmrdporcupine 17 hours agorootparentprevThey may not drive a $10k used ICE, but if you add up the cost in fuel & oil changes, it&#x27;s likely equivalent.The real problem is that right now a $10k used EV is almost certainly an old Leaf or Kia Soul EV etc with extremely low range and degraded battery (because they lacked proper thermal mgmt, mainly). So they&#x27;re not really directly usable by most people. This will eventually change though; in 10 years there&#x27;ll be boatloads of Model 3s or Ioniqs or Bolts etc on the used market. And yes, they&#x27;ll be more expensive than a used ICE but they&#x27;ll also be cheaper to maintain. reply michaelmrose 18 hours agorootparentprevIt is simply unnecessary to own a car in New York city and my impression is that poor folks in the city already largely don&#x27;t own a car. The only way to maintain more gas stations than the market naturally demands is to subsidize them or otherwise distort the market. In NYC it would involve taxing everyone including the poor to subsidize the well off. reply mschuster91 17 hours agorootparentprev> Perhaps not applicable to NYC, but isnt it kind of troubling that people who cant afford a luxury vehicle (55-100k) could be put in a position where they have to spend more money (gas) to go fill up out of their way, taxed for being poor, or shut up and take public transit?Base electric vehicles like the Fiat 500e can be had for ~35k [1], if all you&#x27;re interested in is commuting without getting wet choose a Twizy for ~10k [2], and if you&#x27;re fine with whatever and just need something cheap, get a motorcycle for ~5k.[1] https:&#x2F;&#x2F;ev-database.org&#x2F;car&#x2F;1328&#x2F;Fiat-500e-3plus1-42-kWh[2] https:&#x2F;&#x2F;evcompare.io&#x2F;cars&#x2F;renault&#x2F;renault_twizy&#x2F; reply badpun 15 hours agorootparentTwizy&#x27;s doors don&#x27;t have glass windows, so you will get wet in it. Also, it doesn&#x27;t have heating (or cooling), so in terms of thermal and rain comfort, it&#x27;s closer to riding a motorcycle than driving a car. reply ForHackernews 18 hours agorootparentprevThis is a lazy line of thinking that argues we should always maintain the status quo, no matter how lousy, if the transition costs money: won&#x27;t somebody think of the poor!\"If we outlaw child labor, isn&#x27;t it kind of troubling that families who particularly rely on their 8-year old&#x27;s wages will be the ones losing the most...\"The fact of the matter is that EVs have gotten cheaper, they are getting cheaper, and there are government subsidies to help people buy them. Maybe those subsidies could be even larger, or better targeted, or means-tested? Or, yes, in fact we could improve public transit!Anyway, I&#x27;d give this argument more credence if the same partisans for sticking with fossil fuels had demonstrated any kind of consistent care for the world&#x27;s poor in any other context, but I haven&#x27;t seen that. reply seltzered_ 17 hours agoprevIt may be worth considering that while Norway&#x27;s &#x27;motor fuel sales&#x27; went down supposedly their &#x27;oil consumption&#x27; use per EIA has remained flat (and supposedly gone up recently) - see this chart by Nate Hagens in https:&#x2F;&#x2F;twitter.com&#x2F;NJHagens&#x2F;status&#x2F;1669072120939159553 which is figure 2 in https:&#x2F;&#x2F;fictitiouscapital.substack.com&#x2F;p&#x2F;a-sticky-situation-... which says \"But look at the interesting case study of Norway, where oil consumption has remained flat (it has one of the highest per capita oil consumption levels globally ‚Äî 3x China & 10x India) despite EVs being 90%+ of new car sales and 98% electricity coming from hydropower.\" reply lostlogin 16 hours agoparentI can see the list of things they use it for, but why is oil consumption flat lining? What is increasing their usage (as ICE car usage declines)? reply semi-extrinsic 17 hours agoprevThe headline doesn&#x27;t follow from the actual data (par for the course with electrek I guess).Here is a graph from the source used in TFA. Now look at the sum of gasoline, diesel and biofuels, and tell me that&#x27;s \"cratering\".https:&#x2F;&#x2F;robbieandrew.github.io&#x2F;EV&#x2F;img&#x2F;NORenergy_road.png reply conjecTech 17 hours agoparentThe table they pull that data from is annual and only has info through 2022[1]. It&#x27;s not useful for saying anything about the year-over-year monthly number being discussed. The chart showing monthly sales of road petroleum show a consistent 5% annual drawdown, which is proportional to how quickly the fleet is transitioning[2].[1] https:&#x2F;&#x2F;www.ssb.no&#x2F;en&#x2F;statbank&#x2F;table&#x2F;11561 [2] https:&#x2F;&#x2F;robbieandrew.github.io&#x2F;EV&#x2F;img&#x2F;fuelsales.png reply seltzered_ 17 hours agoparentprevSeconding your point & The headline is on &#x27;motor fuel oil&#x27;, not overall oil consumption for Norway reply exar0815 19 hours agoprevWell, in the larger metropolitan regions (Oslo, Bergen, Trondheim), where probably 2&#x2F;3 of Norwegians live, you see a majority of electrics or at least hybrids.When you get further North, that diminishes. From Lofoten to Troms√∏, you see more and more petrol or diesel. And heavier vehicles.The automotive culture there starkly reminded me of Iceland. reply ajsnigrutin 19 hours agoparentIt&#x27;s not so much the automotive culture, but backbreaking taxes on ICE vehicles, and basically no alternatives if you live in rural areas and have to drive long distances.Maybe someone from norway can calculate&#x2F;check how much a mid-range petrol&#x2F;diesel car (eg. VW golf, renault megane,...) costs there after all the taxes, and compare to an electric one, but it&#x27;s a LOT more than in eg. germany. reply dromtrund 18 hours agorootparentA VW Golf starts at 35800‚Ç¨, and a VW ID.3 starts at 32800‚Ç¨, taxes included. Overall, electric cars are a bit cheaper than their counterparts, but not a lot. However, today, petrol is 2.15‚Ç¨&#x2F;l and electricity is 0.013‚Ç¨&#x2F;kWh. That&#x27;s 11.20‚Ç¨&#x2F;100km versus 0.185‚Ç¨&#x2F;100km for the ID.3, literally two orders of magnitude cheaper. Everyone charges at home, and there&#x27;s also a 40% discount in toll roads, which saves me about 8‚Ç¨ per day I&#x27;m commuting.With the 3k‚Ç¨ lower price, a range of 430km, and fast charging stations every 30km along the major highways, there&#x27;s just no reason to pick the Golf.We still have a PHEV in addition to our electric car, but there&#x27;s no reason to. If we&#x27;re going on a 500km road trip, we&#x27;ll take the electric BMW i4, no question. reply aftbit 17 hours agorootparentWhat if you were going on a 2000km road trip? reply ctroein89 16 hours agorootparentThat distance is London to Stockholm by road, which is going to be unfathomably far for most Europeans. Maybe that could be a road trip across the full length of Norway or Sweden, but generally that‚Äôs a once-in-a-lifetime multi-country road trip that one will want to enjoy.I‚Äôve travelled London to Stockholm before, and the only reason to do it is to travel with a family pet. Otherwise flying or taking the train will be faster, and cheaper. reply dromtrund 16 hours agorootparentprevYes, actually. I have to charge for 30-40 minutes every 5 hours or so. That fits well with my meal schedule, and most charging stations are at or close to restaurants or shops. Charging at these stations is almost as expensive as gasoline though.I wouldn&#x27;t normally drive more than 1000km in one trip though, and that&#x27;s at most once per year. reply Scoundreller 18 hours agorootparentprevAssuming the taxes are just on first-hand sales, the subsidies on new EVs probably flooded the rural regions with cheaper-than-expected 2nd-hand ICE vehicles.Obvs the market for newer 2nd-hand ICE vehicles is going to wash away. reply manxman 18 hours agoprevFishy click bait title. The reason electric is so popular is that ICE vehicles get taxed 100% and electrics are not. Remove the incentives and I suspect the market would be different. reply bryanlarsen 17 hours agoparentIt&#x27;s a case study on what could happen if EV&#x27;s become cheaper than gasoline vehicles. It doesn&#x27;t really matter if the shift happens due to subsidies or market forces. reply TheLoafOfBread 4 hours agorootparentBasically this study has figured out that if you subsidize Ferraris, people are going to ride in Ferraris. But the moment you will stop subsidizing them people will return back to normal cars.Absolute majority of countries does not have a capital of Norway to subsidize on such a large scale. reply landosaari 18 hours agoprevIn 2025 bans of diesel will begin for some large populations Athens, Madrid, Mexico City, Paris, etc. Electric buses for Cape Town, Milan, Quito, etc. [0]If these occur with no difficulties, it will be easier for governments to uphold their 2030 and 2035 agreements.[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Phase-out_of_fossil_fuel_vehic... reply mahidol 18 hours agoprevMotor fuel sales cannot be the sole main indicator for a \"death spiral that can end oil\".Oil&#x2F;Petroleum products are massively employed in the manufacture of daily life items (pretty much maybe everything, not just plastics) and cutting the usage of petroleum products in that area will be the real death spiral that can end oil. reply ben_w 18 hours agoparentI think we only really care about the oil being burned, not merely used as chemical feedstock. In this regard we do also have to care about oil fuel heating, oil fuelled power stations, and the transport that uses oil derivatives but not normal petrol stations (flying, shipping, non-electrified rail, probably some others too), so I would still say this is a good start rather than the actual end of oil, but for different reasons to you.Peak oil is still a thing, and we will still need to shift the feedstocks to something renewable, but the schedule doesn&#x27;t need to be the same. reply SoftTalker 17 hours agoprevNorway leads the way in EV adoption while getting (and staying) rich selling oil to the rest of the world. reply jl6 17 hours agoparentDon‚Äôt get high on your own supply. reply tanseydavid 9 hours agorootparentThey&#x27;re bathing in it! reply arwhatever 18 hours agoprevI wonder if electric cars were to approach the maximum range that most people would ever want to drive in a day, about double the stated range of the current longer-ranged cars, if that might trigger a rather profound and sudden tipping point.Who cares how quickly they charge if you‚Äôre stopping to sleep for the night anyway? reply DoingIsLearning 18 hours agoparentYou are looking at it from an American viewpoint.If you are in US, Canada, Australia, Brazil, Argentina, you might reasonably need to cover huge distances in a day.However there are a _lot_ of people in countries across Europe, Indian sub-continent, SE Asia for whom the current EV range is actually enough. reply guidoism 17 hours agorootparentThe US‚Äôs cars also emit a huge percentage of the world‚Äôs greenhouse gases so solving that problem is kinda important. reply applied_heat 17 hours agorootparentUS is 14% of global co2 emissions, vehicles are 29% of US emmissions, so US vehicles are 4% of global. I don‚Äôt know if I would call 4% a huge percentage but it is certainly significant. reply DoingIsLearning 17 hours agorootparentprevMy point is for a lot of people around the world the technology has reached their non-anxiety range. Which doesn&#x27;t mean we should stop battery and motor innovation. reply mtlmtlmtlmtl 18 hours agoparentprevIt would be nice if EV batteries were standardised and modularised enough that one could just swap out the battery for a charged one at a gas&#x2F;charging station in a couple minutes. reply williamdclt 17 hours agorootparentI saw that in Taipei for electric mopeds. Spread through the city, there‚Äôs racks of batteries ready to go: just grab one, put your empty one in the slot where it‚Äôll charge for the next person and be on your way.Probably more complex for cars (the weight alone might make usability a challenge), but a cool concept reply wcoenen 13 hours agorootparentprevTesla explored this and found it to be unpractical. A battery swapping station can cost millions to build, compare that to something on the order of $50K per high power rapid charger.https:&#x2F;&#x2F;www.powersystemsdesign.com&#x2F;articles&#x2F;why-did-tesla-gi... reply bigtex 17 hours agoprevHave they found a replacement for plastic yet? reply harimau777 16 hours agoparentDepending on the application, aluminum, steel, wood, or cotton are all equal or superior to plastics. The only fairly common application that I know of where there&#x27;s not a good non-plastic alternative is vegan cold weather clothing.Personally, I think that plastic is mostly used for throwaway consumerism. We&#x27;d probably be better off if we went back to making things out of materials that could last. reply Forbo 14 hours agoparentprevA quick search tells me that plastics account for around 4-5% of oil production, so I don&#x27;t think this is going to be the safety net for the industry that it might look like at first. reply pfdietz 11 hours agorootparentAnd a great deal of plastic comes from natural gas, not oil. reply ImHereToVote 15 hours agoparentprevHow much CO2 does plastic production release compared to other materials? reply soniman 16 hours agoprevWhat happens when the electricity goes out? And the food in the fridge goes bad and nobody can get to the store because the electric car isn&#x27;t charged? Have we really thought this through? Redundant energy sources makes sense. reply speedgoose 13 hours agoparentYou always have some battery left in the car. There is no need to go to 0% as it can be very inconvenient and it can also damage the battery on some types of chemistry. Most people never reached home with 0%.So if electricity goes out, you most likely have more than enough range to drive to a nearby store or fast charger.If electricity goes out for an extended amount of time in a large radius around you, that sucks and you may have to find alternative solutions such as walking, running, skiing, biking‚Ä¶ It‚Äôs not scary or considered as big issues in Norway. Many many people enjoy that kind of activities even with electricity.Electricity goes out rarely too. I think I had two short incidents in the last 10 years. reply erik_seaberg 11 hours agoparentprevI remember a concern during Y2K prep that an extended power outage would cause problems pumping gas (and Diesel, which would be the real hurdle for rebooting logistics). reply randerson 15 hours agoparentprevWith many cars now you get V2L charging, so you can use the giant battery in your car to power your fridge. You have to keep your car charged, but that is easy to do if you have a charger at home. reply soniman 15 hours agorootparentSo you&#x27;re saying that something like Powerwall will be the backup power solution? An additional $15,000? reply ljf 1 hour agorootparentA power wall 3 is 13.5kWh, many evs are 40 to 100kWh reply ZeroGravitas 14 hours agorootparentprevNo, they&#x27;re saying the EV can act like a powerwall, a free bonus capability. reply thebruce87m 15 hours agoparentprevMy car rarely goes lower than 70% charged. I charge it to 80% every night (recommended) and use less than 10% every day. On average, my EV is fuller than my diesel was at any random point. reply pppp 15 hours agoprevWhere I live (in the USA), they are still building new gas stations. Are they expecting the government to bail them out? What kind of business person sees the impending end of fossil-fuel cars and builds a gas station? reply gohigh54 18 hours agoprev30-40+&#x2F;-% of refined oil is gasoline. So if you need to refine oil for jet fuel, heating oil, plastics, pavement and&#x2F;or the 100s of other products we use everyday, then you are stuck with gasoline. Need to solve the 100s of other equally significant challenges as well. reply WJW 18 hours agoparentIt&#x27;s 30-40% gasoline because that is what the refining process has been optimized to produce that. You can get other hydrocarbon mixes by choosing different process parameters.Also, a huge part of oil refining is already shortening the chain lengths by adding hydrogen and \"cracking\" the hydrocarbon molecules [1]. They don&#x27;t do that to gasoline at the moment because it has such a huge available market that it doesn&#x27;t make economic sense to process it further. If that market disappears however then we absolutely would be able to convert \"waste\" gasoline further into jet fuel or plastics or whatever.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cracking_(chemistry) reply titzer 18 hours agoparentprevSo it&#x27;s surplus and we have a problem if we don&#x27;t use it now? That can&#x27;t be a serious argument. They&#x27;d have to start paying us to use this annoying byproduct of throwaway materials? Something sounds fishy with that argument. reply lazide 18 hours agorootparentIn most places, Gasoline is already cheaper than milk, for instance.Most people just don‚Äôt buy 20+ gallons of milk at a time.Propane has gotten down to $3.30&#x2F;gal in the US West. reply icegreentea2 18 hours agoparentprevWell, you could at least burn them in centralized plants and get back a bunch of thermal efficiency. You can probably gain a bunch of processing efficiency too that way. reply jgalt212 18 hours agoprevNorway is like a drug dealer that never gets high on its own supply. reply blondie9x 19 hours agoprevOil is still being used for plastic products. It will still be mined and drilled or fracked until all uses of oil diminish. reply jonnat 18 hours agoparentWhile it&#x27;t true that oil is used for plastics and chemicals today, it&#x27;s absolutely not the case that it will be drilled into the future when demand for gasoline disappears.Oil is not strictly a necessary precursor to much of the chemical industry, it is rather a convenient raw material because the high value of gasoline makes lights virtually free. In the early days of oil exploration, lights were constantly burned as they had no economic use at the scale they were produced.If oil exploration would continue just for the sake of chemicals, the cost structure of these precursors would fundamentally change and it would become more attractive to replace them with renewable sources. reply ric2b 19 hours agoparentprevIt will still be less if demand is lower.And if plastics become more expensive due to no longer being a \"free\" byproduct of oil consumption their usage will also go down. reply mechagodzilla 18 hours agorootparentRight - as the biggest consumer of petroleum, gasoline subsidizes a lot of other uses that would be much less attractive otherwise. reply barbazoo 18 hours agorootparentprevWhat a beautiful spiral this would be. reply phantom784 11 hours agoparentprevIf oil is only being used in a product that won&#x27;t eventually be burned and produce CO2, then that&#x27;s a big win. reply zotbar1234 18 hours agoprev\"Death spiral\", you say.Norway is an outlier in many areas related to cars, but \"end oil\" conclusion is a bit premature.* The ICE cars themselves are disproportionately taxed (compared to the rest of the world) while EVs enjoy a number of subsidies. Denmark presents an interesting case of what happens when that taxed advantage is removed (https:&#x2F;&#x2F;electrek.co&#x2F;2017&#x2F;04&#x2F;19&#x2F;denmark-electric-vehicle-ince.... TL;DR nobody buys EVs unless an enormous tax advantage is there). So that \"death spiral\" relies heavily on disproportionate taxation.* -9% Y&#x2F;Y in petrol consumption? Okey, that&#x27;s fair. But there is 29.4% drop in new vehicles year-to-year as well (from here: https:&#x2F;&#x2F;ofv.no&#x2F;bilsalget&#x2F;bilsalget-i-september-2023). That does not necessarily translate to an overall lower consumption (because of the existing ICE vehicles), but the overall demand drop (new EVs are down 20.9% y&#x2F;y). So, how much of the overall decrease in petrol consumption is due to a lower demand for newer ICEs ?* -9% Y&#x2F;Y petrol compsumption? Okey, that is still fair. But... https:&#x2F;&#x2F;www.ssb.no&#x2F;statbank&#x2F;table&#x2F;09654&#x2F;tableViewLayout1&#x2F; tells us that the petrol price increased 21.40 -> 22.90. That&#x27;s +7%. It WOULD be strange, if the change in petrol price did not at all alter the consumers&#x27; behaviour, would it not ? Granted, 7 != 9, but surely a higher price is a contributing factor?* The inflation Y&#x2F;Y is 3.3% (from https:&#x2F;&#x2F;www.ssb.no&#x2F;priser-og-prisindekser&#x2F;konsumpriser&#x2F;stati...) and the central bank&#x27;s interest rate went from 2.25% to 4.25%. Could it possibly have affected the people&#x27;s ability to spend on petrol? On average people with mortgage are at around 3.5x their gross yearly income (from here https:&#x2F;&#x2F;www.finanstilsynet.no&#x2F;publikasjoner-og-analyser&#x2F;boli...). Surely +2% on that mortage is actually noticeable.All of this is to say that \"cratering fuel sales\" are a function of many variables, not necessarily translatable to any other country. reply kieranmaine 12 hours agoparentThe article is based on data from https:&#x2F;&#x2F;robbieandrew.github.io&#x2F;EV&#x2F;.You make fair points on how other variables can influence fuel consumption. However, if you look at the \"Distance travelled\" section there&#x27;s a clear trend over the last 7-8 years of EV miles displacing ICE miles travelled. With EVs approaching 90% of sales this trend will continue, regardless of fluctuations in the variables you mention. reply cde-v 18 hours agoprevnext [2 more] [flagged] robk 17 hours agoparentRacist reply ada1981 18 hours agoprevSomeone remind me again on the stats of battery toxicity and pollution vs fossil fuels?Also, is all of Norway energy produced by wind or solar? Or does this just move the tail pipe and introduce more toxic battery waste? reply x86x87 18 hours agoparentThe irony is that Norway is making money (and to their credit pumping it back to the people) by exploiting fossil fuel. reply speedgoose 18 hours agoparentprevIt&#x27;s mostly hydro. Solar isn&#x27;t worth it unless you like the concept because hydro is so cheap and there isn&#x27;t much sun on average. When it rains a lot or there is windstorm in North Europe you can get paid to use electricity.You can check the data on this website: https:&#x2F;&#x2F;app.electricitymaps.com&#x2F;map reply shortcake27 16 hours agoparentprev> Someone remind me again on the stats of battery toxicity and pollution vs fossil fuels?Can do! Fossil fuels cause more harm than the waste products of batteries. The myth of batteries being worse for the environment has been _thoroughly_ debunked. Feel free to do your own research. reply mtlmtlmtlmtl 18 hours agoparentprevThe vast majority of Norway&#x27;s power is hydro power with some wind and solar. There&#x27;s a single coal plant on Svalbard which is about to be shut down, and oil&#x2F;gas is used mainly by the oil industry to generate power for their own use on-site. Though this is also being phased out in the future. reply lb1lf 17 hours agorootparent-Actually, I believe the Svalbard coal power plant was shut down earlier this week; the plant in question now runs diesel generators instead. While not very green, it is slightly better than burning coal. reply mtlmtlmtlmtl 17 hours agorootparentAh, nice! I knew I remember reading about it in the news just recently, but I guess I misremembered the timing. Shows you how fast information decays I guess. reply juhanakristian 18 hours agoparentprevInterestingly 90% of energy in Norway is produced with hydro and the rest is mostly wind power reply zotbar1234 18 hours agoparentprev98% of the domestic needs are from renewable sources. reply otabdeveloper4 19 hours agoprev> electricity comes from the wall socketWould that it were so! reply daoboy 19 hours agoprevCan anyone recommend a good article comparing environmental lifecycle costs of EV vs combustion engine that include lithium and cobalt mining? reply pkulak 18 hours agoparenthttps:&#x2F;&#x2F;www.motortrend.com&#x2F;features&#x2F;truth-about-electric-car...The takeaway is that any analysis that suggests gas cars are better, at the very least, assumes that gasoline exists in pools just under gas stations. But gas is mined, just like lithium, and gas cars require 100s of tonnes, instead of 100s of pounds. reply burkaman 18 hours agorootparentAlso, gas is single use, but lithium and cobalt are infinitely recyclable. When you calculate the environmental cost of producing your EV&#x27;s battery, it&#x27;s not really appropriate to assign all of that to just one car, because a lot of it will get recycled.This is the best argument I can think of when people start to talk about how awful lithium mining is for the planet. Yes, it does genuinely suck that we have to dig these giant holes in the ground, but at least when we find what we&#x27;re looking for we don&#x27;t ship it across the world and then literally set it on fire. reply pfdietz 11 hours agorootparentprevThe CO2 effects of gasoline use will far outweigh any effects of mining for EV materials. reply burkaman 18 hours agoparentprevThis is a good one from the head of research at Our World in Data: https:&#x2F;&#x2F;www.sustainabilitybynumbers.com&#x2F;p&#x2F;ev-fossil-cars-cli...Also a followup about air pollution, but this doesn&#x27;t include production: https:&#x2F;&#x2F;www.sustainabilitybynumbers.com&#x2F;p&#x2F;electric-vehicles-... reply daoboy 18 hours agorootparentThank you! That was exactly what I was looking for. reply imjonse 18 hours agoparentprevBTW, besides the impact on climate, it&#x27;s worth taking into account the effect of tailpipe emissions on health. This is where EVs have a big advantage especially in crowded areas. reply barbazoo 18 hours agoparentprevI‚Äôd love that too. What I want to know is the total impact of me keeping my 2016 ICE or if I should switch to a new EV, new because there are no used EVs really where I am. reply mechagodzilla 18 hours agorootparentIn terms of impact, think of it this way - As long as you sell your ICE car to someone else, it&#x27;s still being driven and maybe it&#x27;s displacing the purchase of a new ICE car, which is what we really want to prevent. Ideally we would just stop producing new ICE vehicles completely and repair&#x2F;maintain&#x2F;sell the existing fleet until it&#x27;s no longer needed. Hopefully new EVs will also be driven for a very long time, and their on-going environmental impact will lessen as power production decarbonizes going forward. People focus a lot on &#x27;personal&#x27; impact, but as long as the car is still being driven, it doesn&#x27;t really matter who is doing the driving. reply PopAlongKid 17 hours agorootparent>Hopefully new EVs will also be driven for a very long timeCurrently, the average age of cars on the road is 12.5 years. When someone tries to sell their 10-yr old EV, the buyer will know that a huge battery replacement bill looms, discouraging sales.\"EV batteries typically cost at least $5,000. [...] an EV battery lasts 10 years on average before needing a replacement. In addition to requiring expensive batteries, today‚Äôs electric cars run into repair problems after collisions. EVs are still relatively new to the market, so finding replacement body and battery housing parts can be difficult. Additionally, EVs typically have more expensive technology (such as Tesla‚Äôs Autopilot) that requires more expertise to repair.\"[0][0]https:&#x2F;&#x2F;www.motorbiscuit.com&#x2F;do-electric-cars-last-longer-th... reply x86x87 18 hours agorootparentprevIt kind-of does matter. Buying something new always has a net negative impact. It&#x27;s not because the old thing is not used, it&#x27;s because you are incentivizing production of new things.(This is true even if you own an EV and you sell it and buy a new one) reply speedgoose 18 hours agorootparentprevIt‚Äôs not a trivial exercise.I think it‚Äôs important to look at the future pollution and compare without going into the sunk cost fallacy. The pollution from your existing car cannot be taken back and I think there is no point to make this pollution \"worth it\".Producing a new vehicle, that will generate a lot of pollution too, may be better over time. It will depend on your electricity grid and how much you drive.You old vehicle may also go on the used market and replace a much more polluting vehicle. It‚Äôs difficult to take into account all parameters. reply x86x87 18 hours agorootparentProducing the vehicle itself leads to pollution. The question is: is it better to buy a new vehicle (and polute a nonsignificant ammont) or is the polution of my current vehicle going to be less for its remaining life? reply speedgoose 18 hours agorootparentIn my opinion, unless you destroy the new EV in its early life and the battery cannot be reused or recycled for some reasons, it&#x27;s likely much better to sell your ICE now so it replaces a more polluting car.You could also compare the pollution per km in the remaining lifetime of your current car, and the pollution per km in the remaining lifetime of a new EV. Your local electricity grid may improve over the years, but you could ignore that. reply x86x87 12 hours agorootparentI think that&#x27;s directionality how people think but is this based in reality or is this mostly wishful thinking triggere by the \"EVs are good\" propaganda? replyxbmcuser 18 hours agoparentprevhttps:&#x2F;&#x2F;elements.visualcapitalist.com&#x2F;life-cycle-emissions-o... reply roenxi 19 hours agoprev [‚Äì] Norway is more of a city than a country, and a rather wealthy one that is difficult to draw conclusions from when it comes to consumer spending patterns. I&#x27;d brush this off as not that interesting.However, if we&#x27;re looking for oil death spirals in Norway, I recommend: https:&#x2F;&#x2F;crudeoilpeak.info&#x2F;global-peak&#x2F;norway-peak-oil reply travisgriggs 18 hours agoparentNot Norwegian, but lived there for 2 years, in multiple places from Narvik to Stavanager to Oslo to Trondheim.Population wise, Norway slots between Minnesota and South Carolina and is ~7.5x the mighty \"city\" of Alaska. It has a mainland latitude sweep of nearly 24 degrees, California covers 16 degrees of latitude. It&#x27;s coastline is a mind boggling 51,748 miles. I can&#x27;t find a good stat on total road network size, but neighboring Sweden of similar foot print has only a little less roads than the Spain.Some city I guess.As a Washington State resident, my biggest primary holdback with going all electric is the range reduction that occurs in winter months due to cold. Canadian friends have confirmed that it&#x27;s a real thing. I would love to see what effect this has in Norway. I also would love to see which makes and models are trending there. reply roenxi 11 hours agorootparent> Population wise, Norway slots between Minnesota and South Carolina and is ~7.5x the mighty \"city\" of Alaska. It has a mainland latitude sweep of nearly 24 degrees, California covers 16 degrees of latitudeAnd yet the first 4 things you choose to compare Norway to are, conspicuously, not countries. If we want to compare Norway to a country, by wealth and population it makes about as much sense to talk about Liechtenstein and Luxembourg than Saudi Arabia, Australia or Canada (other resource rich countries, in case you&#x27;re wondering). Qatar too, which is ... basically a big city. reply travisgriggs 10 hours agorootparentSorry, as a US resident, I figured states worked better as a point of reference. They&#x27;re, supposedly, like a bunch of little countries, similar to the EU. reply nordsieck 18 hours agorootparentprev> As a Washington State resident, my biggest primary holdback with going all electric is the range reduction that occurs in winter months due to cold.From what I can tell, a substantial mitigating factor is if you can garage your vehicle. reply jgilias 18 hours agorootparentKeeping it in the garage, or just somewhere you can plug it in overnight helps somewhat because it gets heated up before your drive using the grid electricity.The consumption is still higher though. Mine goes from 14 kWh&#x2F;100km when it‚Äôs 20 Celsius to 24 kWh&#x2F;100km when it‚Äôs -10 Celsius. Highway consumption changes less dramatically as going faster you spend a larger share of the energy on moving as opposed to heating. So 18 kWh&#x2F;100km to 20 kWh&#x2F;100km.Range wise that‚Äôs a 25% reduction in usable range. I do 400 km comfortably during the summer and 300 km during the winter. Mine has a heat pump though, which should make it more efficient when it comes to heating. reply speedgoose 18 hours agorootparentIn addition to the mandatory heat-pump, and starting a trip with a warm car and a warm battery, the better EVs also extract heat from the battery to heat-up the cabin. Long trips with one or two short fast charging stop aren&#x27;t so much worse than summer. In both cases the car cools down the battery after a charging stop (the battery heats up a lot when fast charging). In winter it moves the heat inside the car, and outside the car during summer (in addition to running AC). reply Scoundreller 18 hours agorootparentprevJust do what electric buses in cold climates do: have a diesel&#x2F;kerosene auxiliary heater.Nice part is that you (usually) don‚Äôt have to pay road tax on it. reply tyfon 19 hours agoparentprevBeing Norwegian, that is one of the funniest description I have heard of the country which in reality is the exact opposite. reply mytailorisrich 18 hours agorootparentI suspect that what the poster is getting at is that Norway&#x27;s population is small and very wealthy, and it is concentrated around Oslo&#x2F;Bergen&#x2F;Trondheim so that it&#x27;s risky to generalise based on what&#x27;s happening there because, on global scale, it&#x27;s a bit like a large-ish wealthy city (there are 1 million more people in Hongkong than in Norway).I don&#x27;t think he meant that it wasn&#x27;t an actual country. It obviously is.On top of that, Norway has a lot of hydro power, I believe. So not only there are wealthy with a small population, but they get effectively free electricity... reply joe__f 18 hours agorootparentI got the impression they are a firm believer in the spherical cow model reply roenxi 11 hours agorootparentmytailorisrich&#x27;s reading is much more accurate than yours then. He&#x27;s picking up what I&#x27;m putting down. reply Broken_Hippo 17 hours agorootparentprevbut they get effectively free electricity...Tell that to anyone paying an electric bill, especially if they live in Southern Norway. reply mytailorisrich 17 hours agorootparentPrices have only increased over the past 2 years and partly because their skyrocketed in the EU so market forces sent electricity to exportation and pushed domestic prices up. reply Angostura 19 hours agoparentprevYou made me look up the area of Norway, and that‚Äôs a fairly ludicrous assertion.Whatever reason you want to use to brush it off - that‚Äôs a daft one. reply kiba 19 hours agoparentprevIt&#x27;s a country of five million spread over a significant amount of land area?My metropolitan area is like 6 million people, but only 500K lives within the city of Atlanta. reply flavius29663 18 hours agorootparentso the poster is right, your city&#x2F;metro is larger than Norway, population wise. And they are super wealthy to boot.It makes it less interesting in the context of switching to EVs. reply mrweasel 18 hours agorootparentDoesn&#x27;t it make it more interesting, or at least interesting from a different perspective.Vast cities are interesting because it&#x27;s challenging to provide enough charging capacity for a \"small\" area, but also because there are a myriad of transportation option that could effectively complement cars.Countries like Norway is interesting because the cars are need because low population density makes other options to expensive and because now the cars need a much longer range. It&#x27;s also interesting because, regardless of peoples opinion about the size of Norway, it is a country. Imagine the changes in government policies that are because possible when gas powered cars are no longer a thing to consider. Norway also taxes fuel, that money will be missing from the government budget in the future. It&#x27;s going to be harder to explain that you&#x27;re taxing electricity in a country that gets a large amount of it&#x27;s power from hydro and windmills. Or how about the fact that Norway is a huge oil producer... is it morally right to ban gas powered cars at home, but sell oil abroad?It&#x27;s hugely interesting precisely because Norway is an entire country. reply joe__f 19 hours agoparentprevSomewhat larger and with more fjords than your average city reply x86x87 18 hours agorootparentHow many fjords does the average city have? reply tokai 18 hours agorootparentNorwegian cities? Around ‚â≤1. reply joe__f 18 hours agorootparentAh yeah I was going to go for &#x27;0 or epsilon&#x27;, globally reply yread 18 hours agoparentprev [‚Äì] here you can see population density map of Norway Sweden and Finland. It is quite spread outhttps:&#x2F;&#x2F;mapsontheweb.zoom-maps.com&#x2F;post&#x2F;58777074043&#x2F;populati... replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Norway's significant drop in motor fuel sales is attributed to the high sales of electric vehicles (EVs), with over 90% of new vehicle purchases being electric.",
      "The decrease in oil consumption illustrates a potential downfall for the oil industry as EV sales increase globally and could provide environmental benefits by dissuading oil companies from exploring new extraction techniques.",
      "Proper oil pricing, as implemented in Norway, could encourage consumers towards more financially savvy and eco-friendly decisions, possibly leading the oil industry toward obsoleteness if the market continues to evolve organically."
    ],
    "commentSummary": [
      "The key points discussed include the increase of electric vehicles (EVs) usage in Norway, leading to a decrease in motor fuel sales and gas station closures, potentially impacting the oil industry.",
      "There were debates over affordable EVs, the availability and feasibility of EV charging stations, particularly in Manhattan and Brooklyn, achieving sustainable urban environments, and benefits of increasing housing density.",
      "Topics such as the environmental effects of EVs, their recycling potential, comparison to combustion engines, and the role of subsidies and tax incentives in popularizing EVs in Norway are also discussed."
    ],
    "points": 121,
    "commentCount": 192,
    "retryCount": 0,
    "time": 1697896136
  },
  {
    "id": 37970325,
    "title": "Google owes executive $1M after losing gender bias lawsuit",
    "originLink": "https://www.theverge.com/2023/10/21/23926501/google-cloud-lawsuit-ulku-rowe-verdict",
    "originBody": "Skip to main content The Verge homepage / Tech/ Reviews/ Science/ Entertainment/ More Menu TECH/LAW/POLICY Google owes executive $1 million after losing gender bias lawsuit / Ulku Rowe, a female Google Cloud executive, accused the company of giving higher pay to less-experienced male counterparts. By Wes Davis, a weekend editor who covers the latest in tech and entertainment. He has written news, reviews, and more as a tech journalist since 2020. Oct 21, 2023, 11:05 AM PDT|5 Comments / 5 New Share this story Illustration: The Verge A jury found Google guilty of sexual discrimination and awarded female Google Cloud executive Ulku Rowe, who filed the complaint, $1.1 million. Rowe‚Äôs lawsuit alleged that the company gave higher pay to less-experienced male cohorts and that it later denied her promotions in retaliation for her complaints, both internal and later in court. Bloomberg Law reports the jury ruled that Google owes Rowe for both punitive damages and pain and suffering. Rowe said before the trial started that her time at Google was ‚Äúovershadowed by what I believe are unfair compensation and treatment due to my gender.‚Äù She alleged that, after the company passed her over for a promotion to vice president that was given to a man who had neither applied nor was qualified and she filed her lawsuit, the company again denied her another similar promotion. Related Over 20,000 Google employees participated in yesterday‚Äôs mass walkout In an email sent to The Verge, Attorney Cara Green of Outten & Golden said that the ‚Äúunanimous verdict not only validates Ms. Rowe‚Äôs allegations of mistreatment by Google,‚Äù but that it sends a message that ‚Äúdiscrimination and retaliation will not be tolerated in the workplace.‚Äù Green credited ‚Äúthe efforts of thousands of Googlers who walked out in 2018 and demanded reforms.‚Äù Over 20,000 Google employees and contractors staged a walk-out protest that year after a New York Times investigation revealed that the company had given Android co-founder $90 million as he left the company over sexual assault allegations. 5 COMMENTS5 NEW Most Popular Frying pan company sued for claiming temperatures that rival the Sun So long, small phones Xbox and PlayStation battle it out at The Sphere ‚ÄòReddit can survive without search‚Äô: company reportedly threatens to block Google Jon Stewart‚Äôs Apple TV Plus show ends, reportedly over coverage of AI and China Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily. Email (required) SIGN UP By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. More from Tech You can make YouTube Music your default service on the Apple HomePod Microsoft and Nintendo are slashing prices on plenty of spooky titles for Halloween Xbox and PlayStation battle it out at The Sphere So long, small phones SPONSORED CONTENT Here Are 23 Of The Coolest Gifts For 2023 Top Consumer Guide The 5 stocks Set to Double - Not the ones you expect Zacks Research Learn More Game shows what the world without US military interventions would look like History Strategy Game Play game The new electric BMW IX (Take a look at the prices) faqarena.com Introducing the Tenuto 2: A Wearable ED Device mysteryvibe.com Learn More Beckham Marries The Richest Heiress In The World Street Insider TERMS OF USE PRIVACY NOTICE COOKIE POLICY DO NOT SELL OR SHARE MY PERSONAL INFO LICENSING FAQ ACCESSIBILITY PLATFORM STATUS HOW WE RATE AND REVIEW PRODUCTS CONTACT TIP US COMMUNITY GUIDELINES ABOUT ETHICS STATEMENT THE VERGE IS A VOX MEDIA NETWORK ADVERTISE WITH US JOBS @ VOX MEDIA ¬© 2023 VOX MEDIA, LLC. ALL RIGHTS RESERVED",
    "commentLink": "https://news.ycombinator.com/item?id=37970325",
    "commentBody": "Google owes executive $1M after losing gender bias lawsuitHacker NewspastloginGoogle owes executive $1M after losing gender bias lawsuit (theverge.com) 120 points by cratermoon 13 hours ago| hidepastfavorite74 comments az226 8 minutes agoDoes anyone at Google want to chime in how easy it is to get from L8 to L10 in two years? And is it reasonable to file a lawsuit over not having been promoted from L8 to L10 in two years and claiming gender based discrimination because you didn&#x27;t get those promotions? reply roenxi 9 hours agoprevI&#x27;m stunned and pleased - bloomberglaw.com includes the raw case files&#x2F;useful links to the primary sources! Might be the first time I&#x27;ve seen that in a news article. The complaint doesn&#x27;t look that explosive.The amount of politics in these sort of situations is intense. I don&#x27;t think it is reasonable for executives - male or female - to sue their employers over promotion decisions. I&#x27;m going to point at \"Another male colleague‚Äôs background was in physics, not engineering, and he did not satisfy the requirement that candidates have an MS in Computer Engineering\" - in and of itself that suggests the male was better at the job than she was. We know from Berkson&#x27;s Paradox in statistics that people without formal qualifications who get a job anyway tend to be good at what they are doing. Amongst people actively working in a field, qualifications are a negative signal of quality.Female executives using an Equal Pay Act to fight executive skirmishes undermines the look of this sort of legislation. Maybe some people want to pretend that Google executives are delicate daisies that need special legal protections but it just isn&#x27;t true. This executive shouldn&#x27;t be getting special treatment because of her gender. reply matsemann 9 hours agoparentSo you want to detract from a case about bias (that was won), because you have another bias about this male having been better because of their lack of correct qualifications?> This executive shouldn&#x27;t be getting special treatment because of her gender.But you&#x27;re perfectly fine with her getting worse treatment because of her gender? Since you don&#x27;t believe in her right to sue to remedy this.. I&#x27;m sorry, but your post reeks of \"these thing&#x27;s don&#x27;t affect me and therefore don&#x27;t need fixing\". reply roenxi 8 hours agorootparentI literally haven&#x27;t talked to anyone this month who has as materially successful a life as her. Probably this year. Maybe longer. Yeah I&#x27;m OK if she feels hard done by, although I&#x27;m also OK with her feeling good.I&#x27;ve not read the specifics of the case, but based on her complaint? I&#x27;m completely OK with her losing a HR struggle for stupid reasons [0]. Judges don&#x27;t need to be involved. I just wish we could all be mistreated with that sort of compensation and power.> So you want to detract from a case about bias (that was won), because you have another bias about this male having been better because of their lack of correct qualifications?I&#x27;m happy to admit to being biased in favour of basic statistics. Berkson&#x27;s paradox isn&#x27;t some weird edge case, it turns up all the time. If you don&#x27;t like it there isn&#x27;t much you can do, even moving to a different universe isn&#x27;t enough to escape math.Based purely on her complaint, Google could quite reasonably have been chosing someone with a signal of higher quality. In the same paragraph she complains about 19 vs 22 years of experience as though that matters too, which keeps undermining my outrage.[0] I&#x27;m not even taking a position on whether the reasons are gender related. I&#x27;ve seen this sort of thing play out among male executives based on height, tone of voice, random luck, coincidence, nepotism and all sorts of silly factors. It could even be legitimately good reasons. I don&#x27;t see why, especially legally speaking, anyone needs to care. Nobody has a solid metric to measure executive quality, or we&#x27;d just use that. reply arp242 5 hours agorootparentIn the end, I think it&#x27;s almost impossible to say anything meaningful about individual cases. Fact is that in a lot of companies salary varies, sometimes quite wildly, based on all sorts of factors: how well you negotiated, how badly the position was needed at the time of hiring, the mood of the hiring manager that day, how much of the budget was used that year, how well the company is doing at the time of hiring, stellar alignment, etc. It&#x27;s very hard to say which factor played a part. This is true for most roles and not limited to the executive level.The law states that (from the judge&#x27;s opinion): \"Rowe must demonstrate that Google failed to pay her equally to a man in the same geographic area for (a) equal work on a job the performance of which requires equal skill, effort and responsibility, and which is performed under similar working conditions, or (b) substantially similar work, when viewed as a composite of skill, effort, and responsibility, and performed under similar working conditions.\", but this is kind of an unreasonable standard to really prove gender discrimination IMHO.So any male getting paid more than any female can only be explained by discrimination? In some cases it&#x27;s probably a factor. In others: probably not? Hard to distinguish the two however.All we can really do is statistics over a large group, and then say something about that group. But that says nothing about individuals. There are short Dutch people, tall Indonesians, and Germans with a sense of humour. reply IshKebab 3 hours agorootparentYeah I think this is going to be really hard to prove on an individual level. Not impossible though. It depends on the job. For instance in programming the work and skill levels vary so wildly it would be very hard. But think about something like sales.If a woman&#x2F;man was bringing in more sales than men&#x2F;women that were promoted over her?Or in rare cases you might get written&#x2F;recorded damning evidence. reply arp242 1 hour agorootparent> If a woman&#x2F;man was bringing in more sales than men&#x2F;women that were promoted over her?It depends; being better in sales does not automatically make a person a good team lead, or whatever else they&#x27;re being promoted to.And even \"bring in more sales\" is not that simple because if you&#x27;re a curmudgeon that makes everyone around you miserable then that&#x27;s probably going to be factor. Stuff like that. reply matsemann 5 hours agorootparentprevSo principles doesn&#x27;t matter when you feel jealousy? That&#x27;s a weird stance to have. It reads as since you don&#x27;t want this specific woman to sue or win (without even having read the documents), no woman should ever be able to, even in other positions.Why not view it as good that an executive risks much to remove the glass ceiling, and perhaps prove biases in the processes affecting the whole chain? It reads as since you don&#x27;t want this specific woman to sue or win (without even having read the documents), no woman should ever be able to, even in other positions. reply jrflowers 9 hours agoparentprev> I don&#x27;t think it is reasonable for executives - male or female - to sue their employers over promotion decisions.I agree, this might set a dangerous expectation that employers can be sued for breaking the law. If we don‚Äôt hold organizations to consistent standards, that is a good thing and the intention of the law.> Female executives using an Equal Pay Act to fight executive skirmishes undermines the look of this sort of legislation.This is a good point, despite what the law says sex-based discrimination clearly stops being a problem at the word ‚Äúexecutive‚Äù regardless of what that means in real terms of compensation or organizational responsibility. The success of the Equal Pay Act is measured not in jobs or compensation, but the overall feelings of casual observers of company culture. The fact that this woman displayed something other than gratitude for being passed over is a problem. reply threeseed 9 hours agoparentprev> Berkson&#x27;s Paradox in statistics that people without formal qualifications who get a job anyway tend to be good at what they are doing. Amongst people actively working in a field, qualifications are a negative signal of quality.Source. Because on the surface this looks like a pretty ridiculous take.And you seem to have missed the whole point of the lawsuit which is that she wanted the company to simply follow their normal processes. reply Kamq 9 hours agorootparent> ...on the surface this looks like a pretty ridiculous take.Is it really? If you put up a significant barrier to a group of people, and only the exceptional can cross those barriers, it&#x27;s shouldn&#x27;t be that surprising when the pool that make it past those barriers is filled with a higher percentage of exceptional people.Sort of like how a surprising percentage of women in the sciences before 1960 were of Ada Lovelace quality. The women who were merely competent weren&#x27;t allowed to work in the sciences. reply cmeacham98 7 hours agorootparentThis assumes that the same barriers are being put up in front of all candidates, no?Which the lawsuit we are talking about explicitly alleges is not the case. reply Kamq 5 hours agorootparent> This assumes that the same barriers are being put up in front of all candidates, no?No, my example very much does not. It&#x27;s about the exact the opposite, where extra barriers are put in front of one group, but not the other. reply roenxi 8 hours agorootparentprevhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Berkson&#x27;s_paradox& that isn&#x27;t the point of the lawsuit, I think the point was more that Google misrepresented to her that she&#x27;d get paid more than she did after some period of employment and she also believes that there is a gender aspect to that. It isn&#x27;t reasonable to tell a company that your interpretation of their policies is more correct than theirs.IMO someone is born yesterday if they buy the \"we&#x27;re not going to agree to that wage now but we will in a while\" line from a corporation, but whatever. reply DonsDiscountGas 8 hours agoparentprev>I don&#x27;t think it is reasonable for executives - male or female - to sue their employers over promotion decisionsWhy? We have all kinds of rules about discrimination, they can and do include promotion decisions.>We know from Berkson&#x27;s Paradox in statistics that people without formal qualifications who get a job anyway tend to be good at what they are doing.This would only be true if you assume that interviews are consistently perfect judges of job performance, which is well-known to not be the case.>Maybe some people want to pretend that Google executives are delicate daisies that need special legal protections but it just isn&#x27;t true.It isn&#x27;t special legal protection. It&#x27;s very regular legal protection. It&#x27;s the kind of legal protection which should be provided to everybody (and in practice is provided to everybody who can afford a lawyer). reply dchftcs 5 hours agoparentprevIt&#x27;s tough to see the arguments, I can&#x27;t sympathize with her because doesn&#x27;t seem to be offering arguments in good faith.To her credit, gender discrimination is hard to prove and perhaps she felt she really had a strong case (based on the risk she took - this action may close some doors for her), but allowing these arguments to go through really erodes what facade of meritocracy we have left in the industry.The claims seem to be have gone as far as saying she should have been hired as VP. Her background was senior tech management at a bank, which no doubt is a significant appointment, but I don&#x27;t think it&#x27;s easy for anyone to argue that bank tech management easily transfers to big tech.Giving L8 and presumably a sizeable raise from her bank role means Google already valued her experience very highly. Much more highly than I would have tbh; if someone was top level management in an bank, effectively at least partially responsible for the poor culture of bank tech, and then hired as senior tech management at my company, us grunts would be running for our lives. As she didn&#x27;t get consistently stellar performance reviews as L8 (which Google uses as argument), the only thing one can argue for is that the performance reviews were biased - which again to her credit tends to be hard to prove - but there&#x27;s no reason to believe that say a third party looking at those reviews would have given her a promotion.I also can&#x27;t fathom someone is arguing in good faith that at this level MS in CS should be a barrier. It should be clear to any Google L8 that MS in CS means absolute jack shit for anyone who manages to get to L6, if not L5. It should only be a factor in junior-ish hiring. For one, Google&#x27;s technical work even at L6 level can literally shape curriculums at universities. It may just be a legal tactic but I don&#x27;t want US courts to lend any credibility to this argument, and anyone who wants the court to sanctify this point is basically someone that damages my profession and I don&#x27;t want near my jurisdiction. reply tetsuhamu 8 hours agoparentprevLol all these people salty at you.I think you&#x27;re right that the world of executives is less like \"employment\" and it&#x27;s kind of insulting to pretend it is.But I also think it&#x27;s fine for executives to sue the companies they represent. I think the government should have a role in corporate governance. I would even support having a government appointed representative like the Federal Reserve has.But I&#x27;m just a communist :p reply dontlaugh 41 minutes agorootparentThe replies are missing the point, I agree. ‚ÄúExecutives‚Äù are part of the bourgeoisie, employment laws bitterly won by the organised proletariat aren‚Äôt really relevant.At the same time, people can sue each other based on laws, why not? If you don‚Äôt like that, you should join those working to overthrow the current system.I think it‚Äôs also potentially useful, at least rhetorically, for gender discrimination to be an issue even among the ruling class. reply choppaface 1 hour agoparentprev> I&#x27;m going to point at \"Another male colleague‚Äôs background was in physics, not engineering, and he did not satisfy the requirement that candidates have an MS in Computer Engineering\" - in and of itself that suggests the male was better at the job than she was.A true milestone in YC‚Äôs and HN mods transition to the alt right that the top comment currently on this article is so brazenly sexist. There is nothing curious about unzipping a thesis like this and airing it out. Battle of the Sexes gets won by hard work, team work, and leaving behind the game (‚Äúbattle‚Äù) which was pointless anyways. reply krsrhe 8 hours agoparentprevRich people have the power to fight. Poorer people get the precedent for free.I do believe that rich parties should pay for their use of precious court time, reply fny 11 hours agoprevNot intending to detract from this article or the merits here. Congratulations to her for pushing forward.Can a man sue on similar grounds? I have a friend who‚Äôs been passed up for promotion several times while women were given greater titles. The company actually gave him an equivalent raise to assuage him, but he still doesn‚Äôt have a fancy title despite doing equivalent work. It‚Äôs basically an open secret they‚Äôre doing this for gender parity. This is at a Fortune 500.Does he have grounds to sue? reply justin66 8 hours agoparent> Does he have grounds to sue?I imagine the women who were promoted above him, but are nevertheless earning the same amount of money as someone below them on the org chart (and presumably, less than some of their peers) might have a more compelling case in court.An \"I&#x27;m being overpaid for my job title\" situation is going to require a pretty nuanced approach when it comes to showing damages, compared to a lot of other job discrimination lawsuits, I would think.(this seems like something where a lawyer would say \"you want to do what?\") reply anon373839 7 hours agorootparentThere could be a discrimination claim, because being denied a promotion is an adverse employment action irrespective of pay. Damages can be shown from the loss of career opportunities that a fancier title would carry. That can be established with different types of evidence, one being expert testimony.The difficulty with this sort of case is showing causation, because the company will always produce some alternate reasons other than gender to justify its decision. It‚Äôs a surmountable hurdle, but not every case is strong enough to even make it to a jury.Of course, on a practical level, you have all of the politics surrounding this. Depending on your judge and jury pool, you may or may not have a fighting chance. reply justin66 6 hours agorootparentI&#x27;m not a lawyer, and I&#x27;m aware that the law might be infinitely more ridiculous than what I&#x27;ve seen as a mere observer working in business, but:> Damages can be shown from the loss of career opportunities that a fancier title would carry.The guy is going to sue his employer because, while he&#x27;s arguably overpaid in the way that&#x27;s easy to measure (pay for a given job title) he&#x27;s arguably underpaid in the way that&#x27;s difficult to quantify (was he provably more worthy of a promotion than the person who got it?).And not even he claims that he is not receiving equal pay for equal work?Honest question - has anyone (a person that can be named, not a message board hypothetical) ever won a lawsuit like that? Did the victory involve anything more than a court ordering that they be awarded a \"fancier title?\" reply TylerE 5 hours agorootparentTitles don‚Äôt have to be a zero sum game. No reason can‚Äôt have 2 ‚ÄòSenior&#x2F;Executive‚Äù Whatevers reply aksss 7 hours agorootparentprevTitles have value. Two people with same pay, their rank in the company and relative progression in their career is determined by title. Arguably titles can even trump pay. Would you take the title of CxO even if we leave your total comp the same? Many people would say yes, if only to get it on the resume, spend a couple years with it, and jump laterally to shore up the comp aspect. I use title CxO as extreme example but same behavior applies to any senior leadership position at least. reply ugh123 10 hours agoparentprevWhat you&#x27;re describing doesn&#x27;t sound like a \"pay equity\" issue since he&#x27;s getting equal pay and he&#x27;s doing equivalent work. If he was paid less for equivalent work then he&#x27;d have a stronger case. He&#x27;d have to claim the lesser title is damaging to his career prospects. reply Aeolun 9 hours agorootparentSo the reverse is true? Because the guy with the lesser title is getting paid as much as his female counterparts with a higher one? reply connicpu 9 hours agorootparentIf they&#x27;re doing equivalent work for equivalent pay I don&#x27;t think anyone involved has much of a chance for any legal claims, regardless of titles. The value of a title is far too subjective. reply sokoloff 8 hours agorootparentIt does seem to create a pay inequity for the job at the lower level whereby the man in this story is being paid more than the typical woman who is at that same job level as him. reply dcow 8 hours agorootparentAdjacent bands overlap all the time. replykyleyeats 7 hours agoparentprevIt&#x27;s okay to be racist and sexist as long as it&#x27;s in the \"right\" direction. This is standard-issue morality in California and it applies to absolutely everything including hiring and the courts. reply gremlinunderway 7 hours agorootparentSexism and racism refer to institutional bigotry and discrimination, not just discrimination in general.The reason why women (or a person of color in a racism-based case) have a claim and men don&#x27;t is because sexism imposes institutional biases, frameworks and power structures that favor men. Certainly people can discriminate men, but its why most people scoff at the concept of being sexist to a man because it misses the point of the word. reply lovich 7 hours agorootparentNah man, discrimination on immutable characteristics is bad no matter what.If we want equality we need to aim for equality, not just turning the wheel so the next group of people get to be king shit of oppression mountain reply TylerE 5 hours agorootparentI wouldn‚Äôt go quite that far. Are 5‚Äô 5‚Äù people going to be able to sue the NBA? Height is h immutable characteristic. reply Manuel_D 2 hours agorootparentThat&#x27;s because in sports physical attributes are a bona fide occupational qualification. In the same vein actors can be hired based on their appearance.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Bona_fide_occupational_quali... reply TylerE 1 hour agorootparentGP was speaking in no-exception absolutes, though. reply richbell 7 hours agorootparentprev> Sexism and racism refer to institutional bigotry and discrimination, not just discrimination in general.This is a definition that a small, zealous, subset of people try to push. It is not a definition that the vast majority of people believe in or intend to use.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=6nbnGUUdQTU reply gustavus 7 hours agorootparentprevThat&#x27;s funny I always thought that prejudice and discrimination was something someone could be regardless of who you are. That is interesting to learn that as long as you are part of certain groups it is impossible for one to be racist, sexist, or homophobic.I love this morality, next thing you&#x27;ll tell me it is not wrong for one group of people to attempt to perform ethnic cleansing because they were \"disadvantaged\" so it&#x27;s okay.I mean if that is the price we have to pay to keep Nazis out of society though clearly it is worth it. I mean how horrible would it be if we had students at prominent ivy League universities chanting things like. \"There is only one solution infantada revolution.\"But I am sure this relative standards of ethic and morality will lead us to a new golden age and not just engender resentment and divide. reply abraae 7 hours agorootparentNever go full Godwin. reply moron4hire 6 hours agorootparentprevI&#x27;m not going to try to wade into this discussion and argue one way or another (mostly because I&#x27;m too tired to put that kind of effort in), but I do want to point out a case of varying word usage that I think often muddies the waters in these kinds of situations.It&#x27;s a misclassification of things. Prejudice is a feeling against a group. Racism&#x2F;Sexism&#x2F;etc., is an action against individuals based on prejudice. There is an old saying, \"racism = prejudice + power\". Basically, people can be prejudiced all day long, but if they lack the power structure to be able to do anything about it, then they can&#x27;t be whatever-ist.It&#x27;s also where the notion of \"systemic racism\" comes from. If a law or policy disproportionately impacts a certain group of people, that is prima facia a power structure that enforces a prejudice.In a way, it&#x27;s almost like a kind of mathematics. There are operands and operators. Depending on the operands, the operations are one-way. Like the issues of zero getting anywhere near division. In the pursuit of a mathematics that is consistent, we discard a mathematics that is complete.This distinction is important because people educated in traditionally liberal institutions have this taught to them, as well as what \"counts\" as power structures capable of weaponizing prejudice to become an -ism. Just as we accept in STEM fields that \"division by 0 is an undefined operation\", people in social justice fields accept \"traditionally underrepresented groups can&#x27;t be -ist\". Kind of like how lawyers have their own language for things like \"reasonable\" and all kinds of Latin phrases that don&#x27;t follow or completely befuddle the intuitive understanding that lay-people have.For example, you&#x27;ll see people talk about how certain standup comics appear to \"get away\" with saying things that would cause screaming fits if a person of a different background said similar things. E.g., an African American comic making comments about white people vs. a white comic making comments about African Americans. You&#x27;re likely to hear apologia like, \"it&#x27;s impossible for a black person to be racist\". What they mean is that black people historically lack the power structure to be able to activate prejudice against white people. In other words, Kevin Hart&#x27;s prejudicial comments about white people aren&#x27;t \"racist comments\" because it&#x27;s unlikely to have any major impact on how white people are perceived or treated. Whereas Michael Richards using the N-word reinforces and normalizes existing systemic problems in American culture.Again, I&#x27;m not here to argue that prejudice from one group is OK as long as that group has been historically (and let&#x27;s be honest, contemporaneously) mistreated. But if you want to understand how people can make such confusing statements as \"black people can&#x27;t be racist\" or \"women can&#x27;t be sexist\", this is the worldview under which they are operating.NB: I&#x27;m a big believer in Sun Tzu&#x27;s exhortation to \"know your enemy and know yourself\". To be clear, the enemy here is \"people who overly-parse words to post hoc rationalize saying mean things about other people.\" reply endisneigh 9 hours agoparentprevIsn‚Äôt your situation literally evidence of the woman being discriminated against here?The woman in your situation with the greater title is being paid no more than your friend with a lower one, or women with the same title are being underpaid. If and when the man is promoted to the same title as the woman with the greater one they will be paid more than the woman despite doing ‚Äúequivalent work‚Äù.Unless your friend is required to do the work above their title it seems like a non issue and literally your friend is the one with the advantage‚Ä¶ reply Kamq 9 hours agorootparent> Unless your friend is required to do the work above their titleThis is what the OP is implying is happening (I&#x27;m not going to rule on fact). Or at the very least, the person is voluntarily doing work above their title. reply endisneigh 9 hours agorootparentIn either case the friend, the man, still is in the superior position.In the first case they are paid the same as someone with a higher title, and when promoted inevitably they will be paid more than someone with the same title.In the second case they are paid more than people with their same title who may be performing all of the duties required, and this is not to say that there are not others also performing equivalently who did not received this equivalent wage.This sort of situation is exactly why people need to talk to their manager, to avoid over performing and being resentful about it. The fact the person was able to get a raise without a promotion is literally the best case scenario. If their work is actually the same they‚Äôll be promoted and get another raise, resulting in higher pay than those with the same title‚Ä¶ reply Kamq 5 hours agorootparent> In the first case they are paid the same as someone with a higher title, and when promoted inevitably they will be paid more than someone with the same title.You act like promotions are automatic, but you&#x27;re ignoring case where they never get promoted, and can&#x27;t leverage their resume nearly as effectively because of the lower title.In which case, all they got was a temporary pay bump in exchange for doing extra work, but didn&#x27;t get any credit for that work. reply clnq 7 hours agoparentprevAsk a lawyer and your friend.Grounds to sue mean at least wanting to sue, feeling that an injustice was made, having a legal avenue of suing, and predicting good probability of a favorable outcome.Whether it‚Äôs likely he would get a favorable outcome depends on the legal system as well as its components. In case law countries, this might be a strong precedent-setting lawsuit, where the defendant will have an advantage due to the very lack of precedent. In civil law countries, it might be easier as no precedent is needed and discrimination would almost definitely not involve a jury. So if the laws are unbiased, a man and a woman would be treated exactly the same in that case. reply DonsDiscountGas 8 hours agoparentprevIn a general sense yes. For example, if a man was unjustly denied a promotion and some random unqualified woman got the job instead (gender swapped version of the article) they might have a case. In your friends case...I dunno. Seems like if they&#x27;re being paid the same it&#x27;s hard to argue discrimination. reply todd3834 10 hours agoparentprevMaybe but I imagine his future at the company will be strained after this. Is he willing to do this on exit? Since there is no financial issue I‚Äôm not sure what he would win.Not a lawyer‚Ä¶ etc‚Ä¶ reply t-writescode 10 hours agorootparentAlso not a lawyer.If they do ‚Äústrain his future‚Äù, wouldn‚Äôt that be retaliation for a complaint, and wouldn‚Äôt that then be illegal? Again, not a lawyer. reply soup10 8 hours agorootparentif someone files litigation, that&#x27;s obviously gonna sour the relationship, even if \"retaliation\" is legal or not reply anon373839 6 hours agorootparentSuing while continuing to work is an absolute baller move, but it has to be carefully coordinated. When done well, it puts the company in a terrible predicament, and they will be eager to settle in most cases.And no, you definitely won‚Äôt have a future there. reply IshKebab 2 hours agorootparentprevIt&#x27;s probably even harder to prove that sort of retaliation than that he has been the victim of sexism. reply VanTheBrand 10 hours agorootparentprevYes reply jrflowers 7 hours agoparentprev> Does he have grounds to sue?This is a good question because ‚Äúsue‚Äù and ‚Äúconvince a jury‚Äù are used interchangeably. It is a good thought experiment because if we treat the act of filing a lawsuit and the act of winning one as the same thing, this does make the outcome of this trial proof positive of reverse sexism.Some kooks might say ‚Äúin America you can sue anyone for anything and your chance of winning depends on myriad factors‚Äù but that is just a distraction. One woman won one case, so it stands to reason that we must ask if the system is set up so no man can win any case. reply Rebelgecko 5 hours agoparentprevFull disclosure, work for Alphabet but my opinions are my own etc etcIn 2019 Google found that some male SWEs were being underpaid and issued extra raises that IIRC came out to around $10m&#x2F;year reply liquidpele 10 hours agoparentprevOf course he can sue, whether he has evidence enough to make it worth it he should ask a lawyer. reply seanmcdirmid 7 hours agoparentprevI‚Äôm pretty sure a guy has won a case based on gender discrimination before, but it doesn‚Äôt happen as often as the other way around. It would be a really awkward case to push in any case, which is probably why it doesn‚Äôt happen often? reply mellosouls 4 hours agoparentprevAsk James Damore. reply yieldcrv 10 hours agoparentprevYou just have to convince a jury, which I would not consider gender impartial. Interestingly, she brought this case under a New York law.It didn‚Äôt use a state labor commission or the federal EOCCso the answer is probably, but its a much greater gamble, as the commissions use public funding to strong arm companies to settle, while providing a path for representation in the courts alongside your own counselIt takes a big place of privilege to circumvent that and use a different state law and go straight to courtWhereas the labor commissions would absolutely take your case and try to get you compensated reply SoftTalker 10 hours agoparentprevHonestly, no. Legally, maybe but it won&#x27;t go anywhere and he&#x27;ll just come off looking like a crybaby. reply raincole 4 hours agorootparentThe interesting part is when a woman does the exact same thing, she will be considered strong, brave and independent.Actually, it&#x27;s right! Speaking up against inequality is strong and brave. We really need to see men who do that the same way, instead of \"what a crybaby\". reply zeroonetwothree 10 hours agoparentprevAsking for legal advice on the internet always goes well reply sva_ 10 hours agorootparentOP isn&#x27;t asking for legal advice but trying to start a discussion, I believe reply threeseed 9 hours agorootparentIt&#x27;s not really a solid base for a discussion though.We don&#x27;t have any objective evidence e.g. when they say \"equivalent work\" on what basis is that being determined. reply cratermoon 9 hours agoparentprevI would suggest looking first at the legal definition of \"protected group\" as well as \"historically underrepresented&#x2F;marginalized groups\". If your friend still believes he has a case, then consult a lawyer. Good luck going up against a Fortune 500 if he does move forward. reply jetrink 8 hours agorootparentIn the United States, the law says that employers can&#x27;t discriminate against any individual on the basis of sex. It doesn&#x27;t matter what the sex of the person is.1. https:&#x2F;&#x2F;www.eeoc.gov&#x2F;statutes&#x2F;title-vii-civil-rights-act-196... reply krsrhe 8 hours agoprev [‚Äì] Ripped off from https:&#x2F;&#x2F;news.bloomberglaw.com&#x2F;us-law-week&#x2F;google-must-pay-fe...Mods please fix link. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In a lawsuit filed by Ulku Rowe, a female executive, Google has been found guilty of sexual discrimination, having favored less-experienced male colleagues with higher pay and denied her promotions for voicing complaints.",
      "The jury awarded Rowe $1.1 million in damages, heralding the verdict as a validation of mistreatment allegations against Google, signaling that discrimination and retaliation in the workplace will not be tolerated.",
      "This lawsuit comes after a mass protest in 2018 when over 20,000 Google employees walked out demanding changes in the wake of sexual assault allegations against an executive."
    ],
    "commentSummary": [
      "Google was mandated to pay $1 million to a former female executive who alleged gender bias, stating she was overlooked for promotions.",
      "This lawsuit has provoked discourse about gender prejudice in work settings, the potential for executives to challenge promotional decisions, and the difficulties of substantiating damages in such instances.",
      "The debate has also interrogated the definitions of sexism and racism, the possibility of men bringing similar lawsuits, voiced concerns about individuals speaking up for equality being labeled as overly sensitive, and stressed the necessity for objective proof in inequality discussions."
    ],
    "points": 120,
    "commentCount": 73,
    "retryCount": 0,
    "time": 1697921507
  }
]
