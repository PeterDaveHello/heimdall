[
  {
    "id": 40260035,
    "title": "Revived Dillo Project Releases Version 3.1.0",
    "originLink": "https://dillo-browser.github.io/latest.html",
    "originBody": "As commented before[1], I&#x27;ve been working on the past months to get the Dillo back to life and today I&#x27;m happy to release the 3.1.0 version, after almost 9 years since the last one.[1]: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38847613During this time:- A new mailing list was created[2] which is beginning to get some messages and patches. It is available in gmane via NNTP at gmane.comp.web.dillo.devel.[2]: https:&#x2F;&#x2F;lists.mailman3.com&#x2F;hyperkitty&#x2F;list&#x2F;dillo-dev@mailman...- A LiberaPay page[3] which received the first donations (thanks!).[3]: https:&#x2F;&#x2F;liberapay.com&#x2F;dillo&#x2F;- Some more bugs where fixed and new features where added (details in the release page and&#x2F;or changelog).Thanks to all the people that contributed with patches and tests. Now let&#x27;s see if we can make it land in some distros!",
    "commentLink": "https://news.ycombinator.com/item?id=40260035",
    "commentBody": "Dillo 3.1.0 released after 9 years (dillo-browser.github.io)353 points by rodarima 13 hours agohidepastfavorite77 comments As commented before[1], I've been working on the past months to get the Dillo back to life and today I'm happy to release the 3.1.0 version, after almost 9 years since the last one. [1]: https://news.ycombinator.com/item?id=38847613 During this time: - A new mailing list was created[2] which is beginning to get some messages and patches. It is available in gmane via NNTP at gmane.comp.web.dillo.devel. [2]: https://lists.mailman3.com/hyperkitty/list/dillo-dev@mailman... - A LiberaPay page[3] which received the first donations (thanks!). [3]: https://liberapay.com/dillo/ - Some more bugs where fixed and new features where added (details in the release page and/or changelog). Thanks to all the people that contributed with patches and tests. Now let's see if we can make it land in some distros! niutech 51 minutes ago2024 is a breakthrough year, first Netsurf 3.11 released after many years (https://news.ycombinator.com/item?id=38804205), Servo picked up from the dust, Ladybird getting better every month and now Dillo 3.1 resurrected after 9 years! reply rodarima 12 hours agoprevForgot to mention that we are also on Fedi: https://fosstodon.org/@dillo Here are some cherry picks: - Dillo on the Kindle: https://fosstodon.org/@dillo/112181258739093008 - Dillo on an old Samsung phone: https://fosstodon.org/@dillo/112327798958777998 reply mikae1 11 hours agoparentStupid (and unthankful) question perhaps, but have you considered working on https://ladybird.dev instead? Kling and the small team seem to be making great progress. reply rodarima 11 hours agorootparentAFAIK, Ladybird goal is to build an independent web browser from scratch that can render the \"modern\" web. Dillo original goal[1] is to provide access to the web to places with very bad internet speed or latency as well as old or resource constrained computers. [1]: https://dillo-browser.github.io/old/interview.html I don't think we will ever implement JS support, as that would increase a lot the minimum requirements to run Dillo and make the attack surface on the browser much bigger. reply niutech 43 minutes agorootparentRather than joining Ladybird, it would be great to see a merge of Netsurf and Dillo as a very lightweight alternative to Blink/Webkit/Gecko/Goanna-based browsers. For low bandwidth or slow computers, also try Carbonyl Terminal (https://github.com/niutech/carbonyl-terminal). reply mikae1 53 minutes agorootparentprevOK, thanks for clarifying the goal. reply goosedragons 13 hours agoprevBrings back memories. I used to use Dillo on Damn Small Linux on my hand me down laptop with 32MB RAM which even then was a pitiful amount for web browsing. reply rodarima 13 hours agoparentCheck out the gallery: https://dillo-browser.github.io/gallery/index.html reply anthk 1 hour agorootparentDillo is my daily browser among Links under an N270 netbook. I might submit a photo with the device running Dillo. reply idatum 9 hours agoparentprevYes, same -- for me a '98 Toshiba laptop running NetBSD. Nicer to use compared to lynx but not bloatware. reply cookiengineer 3 hours agoprevDo you test the engine against acid3? I think what we need would also be something like acid4 as a test suite, because CSS4 with its flow-root rework is kinda hard to implement for browser engines from the CSS2.1 age. Basically everything has been changed when it comes to the parser's grammar and lexical approach. Things like nested conditions in CSS through media queries or supports queries are impossible to implement without rewriting the parser from scratch. Do you plan on implementing all the event/media/supports related stuff from an isolated CSS perspective, including pointer events, transitions and animations? Because having the @supports queries unified across the board would help a lot supporting older browsers without animations or without a tweening engine. reply rodarima 13 hours agoprevBy the way, there is a CSS bug in HackerNews that we discovered today[1] that is causing most of the rules to be wrongly parsed. I'm not sure if hn@ycombinator.com is the proper place to report it. https://lists.mailman3.com/hyperkitty/list/dillo-dev@mailman... Here is the bad rule: input[type=\\\"submit\\\"] { font-family:Verdana, Geneva, sans-serif; } reply dang 13 hours agoparentShould be fixed now. Thanks! reply fallingsquirrel 12 hours agorootparent5 minute bugfix, on a Saturday no less. Incredible reply Sabinus 12 hours agorootparentDang is a treasure. reply rodarima 12 hours agorootparentprevThanks! reply philipkglass 13 hours agoparentprevYes, email hn@ycombinator.com. Dang is very responsive. reply forgotmypw17 7 hours agoprevYes! Thank you so much. I love this browser, and I'm really grateful to you for maintaining it. I test my websites with Dillo diligently, so I have some re-testing to do now. :) reply rogeliodh 13 hours agoprevbtw, \"Dillo is a fast and small graphical web browser\" reply h4sh 12 hours agoprevthe best dillo experience on macos is probably with https://github.com/crossbowerbt/dillo-plus/ ( I couldn't get dillo to compile easily on macos, it doesn't seem to detect ssl libraries installed on the system) you can do this to compile it on macos (tested on M1): install https://www.xquartz.org/ to have X11 brew install fltk libjpeg #you might also need openssl@3 but unsure git clone https://github.com/crossbowerbt/dillo-plus/; cd dillo-plus # update the 1.3.8_1 fltk version sed 's/1.3.8_1/1.3.9/g' Makefile.options.MacOS > Makefile.options make -j8 # find binary in ./src/dillo maybe someone should make a brew package (for both this and dillo-plus)..? reply LAC-Tech 12 hours agoprevReally cool. Will grab it when it hits the arch repos. Might make a comfy set up where I bookmark some low resource websites and use dillo to browse them. Fond memories from tiny linux distros that ran entirely in a few MB of ram using dillo. reply rodarima 12 hours agoparentIt is orphan in Arch, but I will check tomorrow if I can apply for maintainer. In the meanwhile I have the AUR package[1] dillo-git: [1]: https://aur.archlinux.org/packages/dillo-git reply marttt 8 hours agoparentprevI think it is still the default browser for Tiny Core Linux. reply Palomides 11 hours agoprevhow does dillo compare with netsurf? I've been considering porting netsurf to an unusual platform, but if dillo has substantially better handling of modern pages, I might use it instead reply marttt 8 hours agoparentI think Netsurf renders pages closer to what they \"should\" look like in a mainstream browser. However, a huge bonus for Dillo is that it is really easy to switch off CSS, images, etc if the page doesn't look \"good\", and only render the text (IMO this is also very much encouraged by the UI). I recall having some rendering annoyances with Netsurf, but this was at least 5 years ago. reply rodarima 11 hours agoparentprevWhat prevents you from doing some tests? We have seen Dillo running in a lot of old and not very common machines (Atari), so you may have some work already done. reply Palomides 10 hours agorootparentnothing! I look forward to testing it out, just haven't had the time yet reply SahAssar 11 hours agoparentprevAre you also considering ladybird? reply Palomides 10 hours agorootparentI hope to, though I'm a little concerned its design and dependencies will be harder to handle reply niutech 39 minutes agorootparentLadybird is much more heavyweight at this point, it uses hundreds MB of RAM, not tens MB like Dillo. reply khimaros 7 hours agoprevis there a document or code reference that describes the subset of HTML/JS/CSS which is supported by dillo? i've seen a number of people wishing for realistically achievable \"HTML-lite\" that could be targeted as an alternative to separate protocols like Gemini and dillo's baseline feels like it would be a good starting point. reply 627467 12 hours agoprevTIL of Spartan Protocol. I love Dillon, how is it handling modern security features like new versions of TLS or SSL? reply rodarima 11 hours agoparent> TIL of Spartan Protocol You can browse spartan pages with a Dillo plugin[1], as well as Gopher, Gemini and others. [1]: https://dillo-browser.github.io/#plugins > how is it handling modern security features like new versions of TLS or SSL We support OpenSSL 1.1 and 3 (and any other API-compatible), LibreSSL and mbedTLS 2 and 3. You can choose the one you like/trust more at link time. reply Whitespace 11 hours agoparentprevIt's in the changelog summary: > Add support for OpenSSL, LibreSSL and mbed TLS for HTTPS, which is now enabled by default. reply vouaobrasil 12 hours agoprevI love these minimal browsers. Much better than the modern-style web for sure. reply ptek 8 hours agoparentugh tell me about it. Especially with all the frame works added :(. And the CSS wizards who remove the underline on the link, so when I'm helping people over the phone they can't find the link :'-( And the damn trackers :( reply zem 11 hours agoprevawesome work! dillo used to be my standard documentation browser when projects started using html for their docs, it was snappy and opened files almost instantly reply rodarima 11 hours agoparentYou may like this plugin[1] to read local man pages as properly formated HTML pages. [1]: https://github.com/dillo-browser/dillo-plugin-man More plugins: https://dillo-browser.github.io/index.html#plugins reply anthk 37 minutes agorootparentThe gopher one it's broken upon using a URL with no selector: gopher://sdf.org for instance. it defaults to a 0 one so it doesn't show up right, you need to state gopher://sdf.org/1/ by hand. Otherwise, it works great. On the man plugin, for the users with mandoc/mdoc, I found a bug, '-T html' should be '-Thtml' without spaces. Then works fine. reply zem 11 hours agorootparentprevnice :) thanks for the pointer reply marttt 8 hours agoprevMade my day, massive thanks! Dillo must be my all time favorite browser by far. Responsiveness, modular UI and excellent config file -- everything about it is pure joy, really. reply thriftwy 13 hours agoprevDillo just shows how fast the web without css, js and fonts is. It loads before you even lift your finger from the keyboard. reply agumonkey 13 hours agoparent20 tabs for less than 1MB of RAM. At one point I used it for programming docs browsing. reply cxr 11 hours agorootparentPeople working on developer docs would do well to make them work well in terminal-based browsers, too. This should be the norm. reply anthk 13 hours agoparentprevDillo supports a minimal amount of CSS. The issue it's mainly JS. reply kristopolous 12 hours agorootparentIt'd be interesting to make a \"less ambitious\" JS in the same way that embedded chips have these less ambitious \"dialects\" of c and c++. Like a js-mini that doesn't have the bells and whistles like opengl, bluetooth and midi support reply cxr 11 hours agorootparentThat's a category error. JS is a programming language. All the bells and whistles you mention are part of Web platform machinery with APIs through JS-level bindings accessible to programs written by website authors. The language/dialect has little to do with it; you're envisioning a different stack that would require a new browser engine, not a language. reply anthk 11 hours agorootparentprevIt exists, Duktape and Quickjs, but even on bigger browsers the support it's subpar. Shockingly, edbrowse parses JS pages well enough to login and post in some sites. reply krylon 13 hours agoprevThank you very much!!! reply sylware 8 hours agoprevIf you are interested in dillo, have a look at netsurf, another css renderer, but this one has the big advantage to require only a plain an simple C compiler, not a gigantically complex c++ compiler. That said, more noscript/basic (x)html browsers, the better, even written with computer languages with an extremely complex syntax (rust/java/etc). reply kome 12 hours agoprevI loved Dillo! great to see new activity reply ptek 8 hours agoprevThanks. Will try this on NetBSD :) reply hanniabu 13 hours agoprevscreenshots? reply rodarima 13 hours agoparentIn the main page: https://dillo-browser.github.io/ And here are more: https://dillo-browser.github.io/gallery/index.html reply iamthejuan 13 hours agoparentprevClick Home. reply iamthejuan 13 hours agoparentprevChuck Home. reply anthk 13 hours agoprevThe coverage on CJK fonts it's still bad, even on supported fonts. Also, a tip:andtags should be able to be redirected into a media player, or xdg-open. reply rodarima 13 hours agoparentThere are a lot of features waiting to be included in Dillo. We focused on solving bugs first and tried to get as few features as posible to get the 3.1.0 out as soon as posible. Apart from CJK, there are other issues with fonts that we have to solve too. Feel free to open more issues: https://github.com/dillo-browser/dillo/issues reply throwaway984393 11 hours agoprevI can still vaguely remember when the web was mostly just HTML, images, css, and the occasional plugin content like Flash, Java, and RealPlayer. You could really do a lot of stuff in a tiny package. Now you have to run a pseudo-operating system to deliver bizarre bloated applications via text. Technology didn't used to make me sad... I once shipped Dillo in a tiny memory resident distro, it was fantastic. Happy to see its return. reply floxy 12 hours agoprevAnyone ever compile a browser with emscripten to wasm? So that you could try out a new browser without having to download and install it? I think that could be genuinely useful in instances like this (try-before-you-buy-browser-in-your-browser), instead of just a recursive curiosity. reply bmacho 12 hours agoparentYou probably can't try out a browser in the browser, because of the CORS rules. reply crznp 10 hours agorootparentYou could if the host domain acted as a VPN/proxy reply genewitch 5 hours agorootparentthe cloudpilotemu project can use this host browser as a proxy thing so you can sync your emulated (in the browser) to your PC as well as give the emulator internet access, likely one way to have a browser in a browser. I didn't try it out, though, i just read it in the documentation. reply littlestymaar 12 hours agoparentprevPlease bring a new step of “The birth and death of JavaScript” [1] to the real world (but still not the nuclear war part of it, thanks). [1]: https://www.destroyallsoftware.com/talks/the-birth-and-death... reply rodarima 12 hours agoparentprevThis was already asked the last time[1], but I think it would miss the point of Dillo. [1]: https://news.ycombinator.com/item?id=38853053 reply yjftsjthsd-h 12 hours agorootparentIn some ways it would miss the main use of dillo, but at the same time if you were going to embed a browser like that surely dillo is a great option precisely because of how light it is. reply cxr 11 hours agoprevFrom> we have setup a new website based on GitHub pages (so hopefully it won't dissapear soon) Why cede control of your namespace to GitHub, though? You can host stuff on GitHub Pages without having to go through github.io to get there. Just grab another (cheap) domain and set up a CNAME record to point to GitHub Pages. reply rodarima 11 hours agoparentBecause I considered it a good starting point as I can place a redirect notice there if we move to another place, without worrying we will lose the DNS again. If you want to help, feel free to do so :-) reply cxr 9 hours agorootparentdilloproject.org This cost me $17.32 for two years—so this time next year, even if there's not $11 or whatever available, there'll be a whole other year to come up with it. NearlyFreeSpeech.NET (which this isn't using right now; it's just on a free static host) allows anyone to deposit funds into an account for sites hosted there, not just the person behind the site, which means it can in theory stay up indefinitely, even if the person in charge is incapacitated and/or stops putting their own money into it (so long as they don't actively take an interest against its continued operation). reply fn-mote 11 hours agoparentprev> Just grab another (cheap) domain The post explains how they lost their long-held domain name. At least GitHub won't dry up and blow away if you miss a payment. They're all cheap at the start... and do not necessarily stay that way. The very end of the post makes it clear this is a temporary situation. reply cxr 10 hours agorootparentwat They didn't lose dillo.org because it suddenly became not cheap. They didn't \"lose\" it at all—they were never in control of it. The post explains that it lapsed because whoever had it didn't renew, and it wouldn't matter much, because the person who was in charge is no longer involved with Dillo, anyway, which would have made dillo.org a true zombie site. > The very end of the post makes it clear this is a temporary situation There's no such thing when it comes to namespaces. Which is my whole point. If they had grabbed dilloproject.org (or whatever) today, they could continue using that no matter where they're actually hosting the pages in the future. Once you're in somebody else's namespace, though, the only thing you can do is abandon it after adopting a new one and hope that a redirect suffices. reply riffic 11 hours ago [flagged]prev [–] Again I love how posts like these have no context what a Dillo is. While it's not hard to find that info out on my own, it could have also been done by the poster. kudos on shipping code I guess. reply rodarima 11 hours agoparentTLDR: Dillo is a fast and small graphical web browser. It kind of died in 2017 and this is the first release after 9 years from the last one. You can read the main website for more details[1]. [1]: https://dillo-browser.github.io/ And the release page[2], which explains a bit of the history of the project and the current state. [2]: https://dillo-browser.github.io/latest.html reply bckr 11 hours agoparentprev [–] You could have used this comment to tell us what it is. It’s a minimalistic web browser that works on low powered machines https://dillo.org/ reply rodarima 11 hours agorootparentPlease don't link to dillo dot org, see: https://dillo-browser.github.io/dillo.org.html reply bckr 9 hours agorootparentThanks reply riffic 10 hours agorootparentprev [–] nah someone else in the thread already did that. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Dillo project, dormant for almost 9 years, has been revived by the author who released version 3.1.0 after several months of work.",
      "The revival includes creating a new mailing list, receiving donations via LiberaPay, bug fixes, and the addition of new features.",
      "The author appreciates the contributions made and aims for the new version to be adopted by different distributions."
    ],
    "commentSummary": [
      "Dillo 3.1.0 has been released after 9 years, introducing new features and bug fixes to the lightweight browser, favored for its accessibility on slow internet connections and old hardware.",
      "Users are sharing their experiences with Dillo, appreciating its speed, minimalist design, compatibility with plugins, and support for HTTPS encryption, while also discussing the potential development of brew packages for Dillo and Dillo-plus.",
      "The community shows gratitude for Dillo's efficiency and the developer's quick support, emphasizing its value for users with limited resources."
    ],
    "points": 353,
    "commentCount": 77,
    "retryCount": 0,
    "time": 1714854174
  },
  {
    "id": 40256868,
    "title": "Don't Make Me Fill Out Your Contact Form",
    "originLink": "https://adamjones.me/blog/dont-use-contact-forms/",
    "originBody": "No, I don’t want to fill out your contact form Adam Jones Published 28 April 2024 · Updated 4 May 2024 This article is written solely in my personal capacity, and does not represent the views of any organisations I am affiliated with. Contact forms are almost always worse for users than just putting an email on your website. I explore why they’re terrible, why you’ve done it anyway, and what to do about it. Why your contact form sucks Your contact form is completely broken It’s remarkable how many contact forms are just straight-up broken. A WordPress upgrade here, a change to your CRM there, and your contact form silently breaks. At time of writing, B&Q’s contact form just plainly doesn’t work1. I am fairly amazed that a retailer with revenues in the billions doesn’t notice written queries have stopped coming in. (as a side note: your form should have better error messages. The error above gives the user no way out, given that trying again would not help) This is not an isolated incident. I’ve been stung by broken contact forms on all kinds of websites, from many other big and small organisations. AWS’s abuse contact form has been broken for months. And here’s a particularly gnarly example from Elastoplast, one of the largest plaster brands in the UK: Your contact form is broken for some users Axa, an insurer with almost a trillion euros of assets under management, offers a contact form that looks like it’s time travelled from the early 2000s. Unfortunately, it doesn’t seem like that time has made it work better: it incorrectly rejects some valid emails2 and doesn’t show all the relevant Axa products. Vodafone, a major UK mobile network, has a form to report devices lost and stolen. However, their site is frequently unavailable for maintenance overnight. I can’t imagine how frustrating having a phone stolen, only to try to report it and be presented with this: Finally, a lot of contact forms do not meet basic web accessibility standards, making it hard for assistive technology users to properly complete them. For example, Virgin Mobile’s complaint page is nearly unnavigable by keyboard due to hiding any kind of selection indicators. Your contact form takes ages to fill out I’m not sure how it’s been achieved, but the UK’s Charity Commission’s complaints form takes about 6-7 seconds to load every single page! It offers no indication of progress, and you have to get through 11 pages to complete it. At this rate, you spend 75 seconds just waiting for pages in the form to load. And that’s not including the time to answer the questions asked - or even processing some of them. Don’t worry, you’ll get to review each one carefully because none of them have autocomplete attributes set. You’ll also have to read through all the options in this monstrosity of a dropdown - which is helpfully not grouped or sorted in any coherent way: (Additionally, this question doesn’t make sense to ask in the first place. The complaint is about a service by the Charity Commission, after answering ‘No’ to the question ‘Does your question or query relate to a specific registered charity?’) Your contact form asks for irrelevant personal information National Grid has contact and feedback forms.3 I might want to give them some minor feedback like ‘hey this link on your site is broken’, however I then need to figure out whether I really want to provide all my personal details to do this - like my home address and phone number (or at least make up something that looks plausible). I’ve also seen some complaints forms asking for more sensitive data, like date of birth or copies of ID documents. Sometimes this is despite them not holding this information in the first place, so they’re not comparing it to what they have on record as a security measure.4 You do need to collect some contact details, but do you really need three different contact methods to respond to some off-hand feedback? Other things you probably don’t actually need on a general contact form include title, job function, company size, and landline number. This unnecessarily increases the risk of my data being stolen and used for identity fraud. For less trustworthy organisations, this also directly increases the chance I get spam - so I either don’t submit the form, or make up data. Your contact form is less useful for customers Email allows customers to create drafts, schedule messages, and easily keep a record of what’s been said. It also often integrates into their workflows more easily, for example the ability to snooze emails to chase up on them later. Your form likely doesn’t have any of these features. In fact, even if it’s fully working users often don’t get any form of confirmation they can refer back to later, let alone a confirmation that contains all the details they submitted. I’m not certain there is malicious intent behind it, but I have experienced several companies ‘accidentally’ losing form submissions, only to mysteriously find them again once I pull up copies of the form I saved a screenshot of. If you’re forcing your customers to do this, you’re doing it wrong. Your contact form is wasting your resources It takes time and energy to build and maintain a contact form properly. It almost always gets routed to an email behind the scenes (or a system that accepts email), so skip the middleman and put up the email address! It also can mean that customers contacting you can’t remember what their original message was about. I’ve experienced getting a response from an organisation and then having to ask them what the original query was about - had this been an email in the first place this could have been avoided. Your contact form is irritating in other ways There are probably hundreds of ways you could set up your contact form to be maximally irritating, and I can’t cover them all here. I’ll leave you with one final example that doesn’t fit into the categories above though. Sainsbury’s has a contact form that forces you to navigate down a long options tree. Only to open a modal and ask you to select the topic again. They then present the tiniest form window ever, which makes it near impossible to fill out: Why you’ve done this, and what to do instead Beyond just ‘it’s what we’ve always done’, there are a few reasons you might think a contact form is a good idea. Here we’ll go through some of them and what you might do instead. To collect structured data that speeds up resolving problems You might want to use a contact form to collect structured data, so it saves you time processing requests. For example, making sure the customer provides the right identifiers for you to find them on your systems quickly, or automatically assigning queries to the right teams. This can be okay in some circumstances. Amazon does this well: it shows you the recent items you’ve purchased and also allows you to completely self-serve common customer service workflows (like requesting a return) - while still allowing you to speak to a human fairly easily. However, unless you’ve got an excellent customer service portal (you probably don’t), you should just stick with an email for general contact purposes. To make sure you have the right details, just set up a sensible auto-reply. Here’s a good example from GP at hand:5 You could also encourage users to provide the right details upfront by using mailto links with appropriate parameters, such as support@example.com.6 Next, before automating assigning queries to the right teams, consider whether you need to do this: I’ve seen very small teams try to justify ‘a contact form will help us categorise emails’ when they receive two emails a day, or they all go to the same person anyways. If you really do need to automate assigning emails, take a look at platforms like Zendesk and their AI systems - this kind of thing is exactly what LLMs thrive at (or often just regular expressions will get you far enough). You want people to contact you, and think your form will lower the barrier Empirically, this doesn’t hold up. I previously thought this was true, but I’ve since seen at 3 different organisations who’ve made the switch that this doesn’t seem to be the case - i.e. they get the same number of genuine contacts by listing an email as having a contact form. I’d love to rigorously analyse and publish the data on this to help convince others. One category where this might not be true by default is for very ‘low-effort’ feedback forms. I think this can probably be mitigated by making it clear that you’re friendly and would be happy to receive informal emails. The other category where I think this might be more likely to lower the barrier is where people prefer to be anonymous. For example, someone who might not be public about their sexual orientation might not want to have related comments so easily linked back to them, but still want to give feedback on some article. The UK Government website, GOV.UK, has a delightfully simple example that works well for collecting anonymous feedback: You’re worried you’ll get spam if you list an email online Surprisingly, not really. I found we often ended up getting less spam compared to WordPress forms - there might be something about it being easier to submit anonymous forms with rubbish, compared to actually sending emails with decent enough reputation to get past today's spam filters. If you really worry about this, you can obfuscate your email from spam bots. But I think this is usually wasted time given just how rare this kind of email spam is, and how good spam filters have gotten. Plus this risks you messing this up and breaking something. You want to manage messages as a team You can do this with email! Hook up a shared mailbox, collaborative inbox, or one of the many off-the-shelf customer service solutions like Zendesk, Zoho Desk, Freshdesk, Request Tracker7, Zammad, osTicket or FreeScout to your email. You think email isn’t secure enough Email used to be very insecure because it wasn’t encrypted.8 This meant that people between mail servers could read the contents of messages. However, unless you’ve done something weird with your email setup, it’ll probably be encrypted securely like 99% of other email traffic is in 2024. In theory, your customer might be using an email provider that doesn’t support encryption - which could lead them to sending something to you insecurely and putting them at risk. I think most organisations can accept this risk given how rare this is, given that this is on the customer’s end. For what it’s worth, the UK’s National Cyber Security Centre is happy to list a contact email for themselves. Another question to ask yourself if you don’t list an email for this reason: do you respond via email after people contact you? You think email isn't accessible to assistive technology users I've heard the claim that you need a contact form to be compliant with some common accessibility standard - this is simply not true. If you don't beleive me, try and actually find this rule.9 Listing an email is almost certainly easier to parse and understand than your contact form. You don't want people to fill in your form Yeah - this is one I'm not really here to help you with. There are some organisations that intentionally want their form to be difficult to complete - perhaps it’s a regulatory requirement that you don’t really want to comply with. If you’re doing this, you should probably feel bad. It’s unclear how intentional this is, but in this vein Meta (Facebook) makes it very difficult to contact them about data protection matters. Some charities have already highlighted that Meta’s forms seem to be particularly difficult to find, and difficult to complete. My own experiences with this process have been complex and drawn out, having been referred to several different broken forms. Regulators should be doing a better job ensuring they are setting the bar appropriately in these cases, in a way that rewards actors who are complying with both the letter and spirit of laws or regulations. Conclusion Contact forms are hard to get right, and often just a worse experience for everyone involved. Go forth and remove your contact form and list your email on your website now! Footnotes As in, it’s impossible to submit from the website - the error appears when submitting. Inspecting the network requests suggests this error is because a ‘title’ property is missing: likely some mismatch between the backend and the frontend (there is no title field on the frontend). Yes, these problems happen in software development from time to time: but there’s no need to incur this if email already ‘just works’. ↩ test+test@example.com, tést@example.com and test@éxample.com are all valid emails as per RFC 6532, but are all rejected by this form ↩ As a fun side bonus, if you make a single mistake on this form it deletes everything and forces you to start from scratch. For example, if you format your phone number ‘wrong’, your entire message you typed up is wiped. ↩ I don't think people's birthdays should be a security measure, but unfortunately it seems to be the world we live in. I'll have to explore this in another rant. ↩ It could probably place this a bit more prominently by cutting some of the other stuff, but on the whole the message is fairly clear and not an unreasonable length. ↩ Thanks to foreigner for the recommendation. ↩ Thanks to amluto for the recommendation. ↩ There are also other email security risks, such as email spoofing (solvable with DMARC!). But on the whole, the email security risk seems overblown (especially compared to 'crappy WordPress form' security risk). ↩ Further examples that would seem to suggest that this is not necessary is that the following organisations list an email to contact them: Equality and Human Rights Commission (the public body responsible for disability rights), Scope (a major disability equality charity) and AbilityNet (a charity building assistive technologies). ↩",
    "commentLink": "https://news.ycombinator.com/item?id=40256868",
    "commentBody": "I don't want to fill out your contact form (adamjones.me)288 points by domdomegg 22 hours agohidepastfavorite169 comments akudha 18 hours agoAs I become older (and grumpier) I have realized that not engaging is the best thing to do, wherever possible. Dirty restaurant / rude staff? Don't go there. Don't like dark patterns on a website? Contact forms don't work? Don't use those sites. And so on. Two reasons - first, most of the time, these businesses know their shit is broken or they're doing low quality work etc. They just don't care. Second, it is good for our own stress levels to avoid dealing with shitty stuff. Obviously this can't be applied to essential services like healthcare etc. reply masfuerte 18 hours agoparentI've been doing this for years. Unfortunately I'm looking for a job now and there's no escape from it. The job sites, the agencies, the employers' sites - they are all awful. The best one was an employer's site that described itself as \"Easy Apply\"! You had to give it a resume, which it parsed, badly, and sprayed randomly into about a thousand text boxes. I thought maybe the problem was starting with a pdf, so I began again with a Word document. The results were exactly the same, suggesting they exported to pdf and used the same shitty parser. Having to rearrange all this text into the correct boxes was annoying enough, but they weren't just vanilla text boxes. They were janky javascript abominations that responded to input really slowly. And employers moan that they have trouble finding good staff. reply rrr_oh_man 18 hours agorootparent> And employers moan that they have trouble finding good staff. Employers have trouble finding good staff that they can pay peanuts. A shitty application form is a great filter for people who are desparate and will put up with low pay and toxic corporate idiosyncrasies. reply bboygravity 17 hours agorootparentReminds me of ASML's yearly whining (they form cartels with other tech businesses in the region to keep max compensation down and are then acting surprised that they can't find local engineers who deliberately avoid the company). reply fHr 17 hours agorootparentNice, I can't understand their growth potential could be like infinite they operate in the right space and have the right tech and fill in a nice niche. But C-suite and shareholders dividends need to be maximalized and engineeres enslaved I guess. Man I love latestage capitalism /s. reply rrr_oh_man 8 hours agorootparentI bought capitalism.boo last night. wanna do something with it? reply WalterBright 8 hours agorootparentprev> engineers who deliberately avoid the company .. engineeres enslaved Not sure how you equated the two. reply hackable_sand 11 hours agorootparentprevRecently I almost completed one that was equal parts data harvesting form and job application. The original link was through a third-party job board. The job board tried to trick me into signing up in order to jump to the posting. The job \"app\" itself was actually two applications. One was an automated resume parser that was just... incorrect. The second was a manual-entry form that asked for the same information. :D Funny enough, I got to the \"Why do you want to work for A Shady Company with Questionable Morals?\" series of questions I was actually given a chance to stop and sober up to the idea: A human being (allegedly) put together the most byzantine hiring process to conceal something, and if they actually do hire someone, it will be a self-selected fanatic who needs the cash more than the indulgence. reply wuj 12 hours agorootparentprevGreenhouse and Lever have the most convenient job application interface IMO. The application area is one page, which means you can navigate using tab. There's also no need to create an account and verify email address (Though I understand why some portals do that to prevent spams). reply trentnix 15 hours agorootparentprevFor sure, job application submission is an awful mess. As a rule, I won’t apply anywhere that uses Workday, considering they require you to create an account to submit your application. Truth is, any company that makes getting a job an awful experience (despite every incentive to the contrary) won’t be any better once you’re an employee. reply Terr_ 50 minutes agorootparent> As a rule, I won’t apply anywhere that uses Workday, considering they require you to create an account to submit your application. At work, we've made chat interface product that takes data from (account-less) visiting applicants and makes Workday job-applications on their behalf. So maybe that makes the world just a slightly better place... Or it's maybe it's the opposite because it enables Workday? Hard to say. reply otteromkram 5 hours agorootparentprevOn the flipside, there's Oracle HCM which doesn't let you create an account and makes you verify your email address with each subsequent job app. They rely on cookies for all of this. No thanks. I'll take workday over that. I like using passwords and don't like tracking cookies, so I guess I'm weird. reply Voultapher 12 hours agorootparentprevCulture permeates reply akudha 18 hours agorootparentprevOne recruiter asked for my high school scores, from 20 years ago. For a 3 month contract job. Another recruiter wanted to know my salary expectations first, before giving me a single detail about the job. When I refused politely, she yelled at me. There are lots of adults who never grew up, never learned words like please or thank you, feel super entitled etc. It is not our job to help these people (unless they happen to be friends or family, even then we can only try). Best thing to do is avoid, and look for good people to talk to, do business with. Life is too short to waste on shitty stuff - people or otherwise reply prepend 15 hours agorootparentIt’s actually good that they are so stupid so soon. It’s nice to filter those people early than to spend time on the application to learn how stupid the organization is. reply xcdzvyn 18 hours agorootparentprevYeah, Canonical asks you how you performed in your high school English class as if that's something you're supposed to know. Is that really a valuable metric for a software engineer? reply jasonpeacock 16 hours agorootparentTell them you graduated with honors from AP English and your teacher called you \"the next Faulkner\". Or tell them that your high school didn't offer English class, learning was student-led and project-based. Or you took the GED at 12yrs to skip high school and study puffin colonies in Alaska with your aunt. How are they going to fact-check any of that? reply thaumasiotes 15 hours agorootparentOne memorable part of getting paid coaching for interviews was the admonishment \"There is no place for honesty in a behavioral interview. No one is going to check on your story.\" reply Karellen 11 hours agorootparentA coach that you paid money to advised you to lie during a behavioural interview? Well, that could certainly give the prospective employers plenty of information about the way you behave. I wonder how many lies the coach told you about themselves and their qualifications, on the belief that you'd never check on their story. reply Thiez 2 hours agorootparent> Well, that could certainly give the prospective employers plenty of information about the way you behave. That's the beauty; how would they know? The information is completely unverifiable so all such an interview does is find the person best at telling you what you want to hear. reply deprecative 10 hours agorootparentprevIt makes sense though. The employer will lie constantly in one of those interviews. It's best to shore up your chances. This is the system employers wanted so give it to them. It's not like you'll be working there in three years anyway. reply thaumasiotes 6 hours agorootparentprev> Well, that could certainly give the prospective employers plenty of information about the way you behave. How? Do you think he was wrong about them checking? > I wonder how many lies the coach told you about themselves and their qualifications, on the belief that you'd never check on their story. None; he was randomly assigned to me by the platform. reply akudha 16 hours agorootparentprevMy guess is that they are trying to vacuum as much information as possible. It is easy to do (\"I can do nothing, the client is asking for the high school scores, not me!\"). Who knows what they are doing with that data reply pydry 15 hours agorootparentNo, the CEO is just fucking weird and doesn't seem to mind that he puts people off with his low wages and idiosyncratic, drawn out hiring practices. reply quaddo 11 hours agorootparentprevIs it possible they’re trying to separate out candidates who studied English literature as a matter of typical high school education vs those who studied ESL back in their home countries? reply kmoser 10 hours agorootparentprevKnowing your ability to communicate in English is a useful metric, but asking for your high school English grade is definitely not the right way to go about it. reply baby_souffle 8 hours agorootparentprev> The best one was an employer's site that described itself as \"Easy Apply\"! You had to give it a resume, which it parsed, badly, and sprayed randomly into about a thousand text boxes. I thought maybe the problem was starting with a pdf, so I began again with a Word document. The results were exactly the same, suggesting they exported to pdf and used the same shitty parser. Ohh, let me guess. Workday? There are a few application systems that offer this functionality but workday is _consistently_ the worst at parsing whatever I give it (text, markdown, html, pdf...). reply chefkd 8 hours agorootparentprevIs it me or has there been an up tick in 3rd party job application websites? When I first applied for jobs it used to be directly on the companies website but nowadays it just redirects to a weird subdmain with weird tracking reply ykonstant 15 hours agorootparentprevMore than one university in the UK had exactly this procedure. I eventually gave up on applying, but even before that I was 90% sure no meaningful information would reach the hiring committee. reply david38 10 hours agorootparentprevA few things for when applying for jobs- * use a dedicated gmail * use a dedicated google voice number * have your PDF resume up to date, maybe a few versions of it for different types of jobs * keep a formatted text version of it as well for those horrible text boxes reply desro 6 hours agorootparenti have found the recent crop of SotA LLMs to be extremely useful for the latter few tasks you mentioned. Give it my full, comprehensive CV, as well as a prospective job description, then ask \"tailor a condensed resume from the info in my CV to match this job description.\" Of course you'll want to review and edit, but it's taken a huge amount of drudgery out of the process, for which I am grateful. reply tomjen3 5 hours agorootparentI hate writing job applications, so even if LLMs aren't good enough yet, at least they are a start. reply CM30 14 hours agorootparentprevOh god job application forms are a travesty. Every single company seems to do it differently, about half of them seem to like making the form ten times longer than necessary and good luck figuring out whether your submission will actually get checked by anyone or thrown straight into the trash by an automated system. And I definitely emphasise with the 'easy apply' auto fill crap. Those are incredibly unreliable at the best of times, and a waste of time all around. But the worst ones to me have to be the incredibly lengthy 'ask everything' forms that way too many large companies and government agencies like too much. The ones which feel less like a job application, and more like filing your taxes. Way too often you'll go for something on LinkedIn, see a form, then notice it says something like 'part 1 of 20' at the top of the page because someone at Microsoft thought letting companies add a ton of unique questions was a 'great' feature. reply justinclift 15 hours agorootparentprevNah. If you see that kind of thing, just nope out from that place and move on to the next one. There's no shortage of places looking for people. :) reply fHr 17 hours agorootparentprevfr easy aply is so dogshit, most of these parsers for your cv can't handle the most basic shit, with my limited knowledge dealing with headless browsing with phantomjs I could come up with a better solution in an afternoon easily. Sorry but using a parser that can't even read experiences or education section to easy in a >10k+ tech company that does software is just not bearable. reply WalterBright 8 hours agorootparent> I could come up with a better solution in an afternoon easily I once did some contract work to write a parser for that. It wasn't long before I realized the variety of resumes made it completely impractical, and had to abandon the project. (I didn't charge for my time on it.) If you could do a better one in an afternoon, you can make good money doing that. reply throwaway598 9 hours agorootparentprevNot a core competence so vendor solution is used. That's financialisation. That's maximising value. Right? reply WalterBright 8 hours agorootparentI do my own mechanical work, but am a disaster at painting. So I hire that out. It's not \"financialization\". It's the economic Law of Comparative Advantage. https://en.wikipedia.org/wiki/Comparative_advantage It only makes sense to do in business what one has an aptitude for, and pay others to do the other stuff. I bet you don't grow your own food, or make your own soap, either. reply eddd-ddde 16 hours agorootparentprevHopefully one of the areas that LLMs can actually improve. I expect an LLM to fairly accurate parse content from resumes. Maybe we even start using plain text resumes. reply nox101 16 hours agorootparentI'd expect LLM parsers to enforce a monoculture in that small variations from the norm will mess it up and it will downrate/discard lots of edge cases reply dotnet00 8 hours agorootparentThat's already the case with existing resume parsers and evaluators. LLMs might at least broaden the monoculture somewhat. reply mistrial9 15 hours agorootparentprevI expect that LLMs will be used aggressively by a subset of employers for exactly all the lazy and asymmetric power reasons that an employee can think of.. being automation, the footprint of that employer subset will be much larger on the whole, and often be the first or only resort for the desperate, uninformed etc applicants reply MrJohz 11 hours agoparentprevOne of the wisest pieces of advice my mother gave me growing up was always to think about what you want out of an interaction. Before you send a text, prepare an email, write a comment, argue a point, etc: what is your goal, and what's the best thing you're going to get out of it? It's really good advice, because it makes it so much easier to just let things go. Yes, the website I'm using is awful and could easily be done better. Yes, the person I'm taking to is obviously wrong. But I'm not going to get anything out of getting involved - at best some mild catharsis - and I'll just waste everyone's time doing so. So let it go. reply forgotmypw17 18 hours agoparentprevI agree. I’ve noticed a strong correlation between friction (such as newsletter modals, cookie consent modals, register to read, etc.) and low-quality content that is just a waste of my time to read. Since I realized this, I’ve saved a lot of time and effort by closing a tab as soon as I see one of these tells and not looking back. I'm really grateful to the low-quality content creators for making it so easy to recognize. reply wuj 12 hours agorootparentThat moment when you want to read a tweet just to be prompted to login. reply deprecative 10 hours agorootparentIf someone uses Twitter that's a pretty good indicator that I don't need to interact with them nor their content. reply dylan604 14 hours agoparentprevHear hear! Seconded! And all of the other similar phrases of support. However...I was recently at my neighborhood tavern, and the group down the bar from me got my attention in a way that moved me to action by wanting to donate to their cause. I asked how, and they provided me a URL that took me to a payment portal. It should have been that easy. Instead, it wanted full account creation with username, email, phone number, and password with specific requirements. After 3 attempts of not being able to generate a valid password, they decided I had too many at the pub and decided to \"help\". After multiple attempts, they were also unable to generate a password to create the account to take my money. Their own website and all off the unnecessary account creation policies actively prevented a successful conversion. I laughed and laughed at their folly. Of course, the individuals receiving the laughter were not the ones that mattered regarding this, so I stifled my smugness in this victory and suggested they tell their coworkers. reply WalterBright 8 hours agorootparentA principle I learned from Eric Engstrom is \"if you want to succeed in business, make it easy for people to give you money\". It's true, too. By making payment easier, sales volume doubled. reply dylan604 7 hours agorootparentThe fact that it's a \"shut up and take my money\" is a meme says enough as well. Probably reach more younger people that don't read and only speak in images /s reply jjice 17 hours agoparentprevCompletely agree. It may be considered a \"loss\" if you encounter rude staff, but fighting back won't make you \"win\", even if you technically do. Your stress and mental state is much better if you just let some stuff go. Obviously not everything, but I'd say that probably goes for the majority of small annoyances in your life. reply anonymoushn 16 hours agoparentprevUnfortunately companies like Anthropic like to provide web sites that work long enough to obtain your credit card information, then break them in a way that prevents you from unsubscribing. reply CM30 14 hours agorootparentOr have it so it's trivial to sign up online, but cancelling requires contacting them via phone/post/whatever. 'Funny' how well these systems seem to work when people are giving you money, but how much of an unusable mess they turn into when it's the other way around. reply Repulsion9513 12 hours agorootparentState Farm wanted me to speak to my agent directly (who they had never bothered to change from one down in Texas when I told them I moved to Montana), so I gave them written notice through the contact form on their website and then had to file a chargeback when they charged me again. Got a check from them in the mail a couple weeks later. (For $13.83, I'm not sure exactly what that's supposed to represent) reply Modified3019 14 hours agorootparentprevVirtual credit cards and masked email addresses have been amazing. reply diarrhea 13 hours agorootparentI’ve been using catchall email for everything for well over 5 years now, and it hasn’t been useful once since. Regular spam filters from my provider and occasionally hitting “Unsubscribe” once seem to do the trick. reply LoganDark 13 hours agorootparentprevSome banks offer virtual cards directly, but there's also Privacy (.com). For masked email I find Firefox Relay works pretty well. I like Privacy because they let me switch banks easily, as well as place spending limits, pause or close cards, etc., just like Firefox Relay would allow me to switch emails easily. I believe I use Firefox Relay for everything email-related now (I pay $1/mo for my own subdomain), and also use Privacy for everything money-related (given they accept Privacy cards). https://emkei.cz is a good \"fake mailer\" for getting outbound email from a Firefox Relay address, for those companies that want you to send them an email from the address on your account. Basically you send something from the company's email to your relay address, then reply to it and Firefox Relay will send it to the real company's email, but from your relay address. (You know, this sounds like it could make for a great phishing exploit because Firefox Relay doesn't check or notify you if SPF/DKIM/DMARC fails on an incoming email, and the forward that it does to your personal email will be entirely lacking those indicators. So aside from email content itself looking suspicious, it could be possible to perfectly spoof a real email because the relay step strips all the original authenticating information.) reply stevenae 18 hours agoparentprevI co-sign. If I could expand: When you must engage with sub-par experiences, look to redirect in a positive manner. Don't try to brute-force a solution, rather, suggest an alternative that the counterpart may not have considered. reply candiddevmike 18 hours agorootparentAs I've grown older, I've learned unsolicited advice is almost universally despised (and you have a > 50% chance of making an ass of yourself due to lack of context/armchair general). Therefore, start with a complaint, and if they truly want your feedback, offer it. reply teaearlgraycold 17 hours agorootparentIt’s such a shame. I think I can feel what others do when I get unsolicited advice, but I’m able to regulate my emotions and either take the advice or explain why it’s missing context. If only others could have more humility and analysis ability. But you can’t do too much to change others. reply amne 18 hours agorootparentprevYes. I browse, I see banner (slide, popup, sidebar), I close tab. I have friends that ask me if enjoyed the paragraph about something they shared a link to only for me to have to come back and say I closed it after reading two words and getting interrupted by ”Put your email here to read more\". Nope. I tell them that is not a good web experience for me. I just hope that more people start doing this. reply al_borland 4 hours agoparentprevI’m getting close to completely abandoning Amazon over this. I recently unsubscribed from Prime and the number of dark patterns in every single checkout, attempting to get users to sign up for Prime, is unconscionable. No wonder so many people are subscribed, the user has to actively fight against it. reply scrubs 15 hours agoparentprevTotally agree - I will just add that 1 or 2 times a year I do the opposite. I call out stupid when I see it and spare nothing. Clear, straight criticism leveled at management if I can find them. Your right: they know it's screwed up most of the time. But the front line is not at root responsible ... and I get a bit of extra satisfaction flushing management out into the open so they can't hide out. reply trvz 14 hours agoparentprev> Don't use those sites. Go harder on that – block those sites in your network outright (such as in Pihole). Else you may end up on them in future, either by accident (clicking a link) or temptation. reply mihaic 11 hours agoparentprev>these businesses know their shit is broken or they're doing low quality work I'm honestly sure about this often, since bad staff might not be obvious to the manager without someone pointing it out. As a general rule though, just giving up on something bad is not a terrible strategy. reply dheera 9 hours agoparentprevI can't remember the number of times that I've selected \"Afghanistan\" from the contact form list as my country of residence because it was the first option in the dropdown. reply kkfx 14 hours agoparentprev> Obviously this can't be applied to essential services like healthcare etc. And that's the reason why you have to engage. Allow things to go that way one time, you'll get them all the time. Protest, politely but effectively, all the time and things will change. We all suffer crappy services, if almost nobody protest they'll not change, because much of them are used by people with no choice, starting from fiscal stuff. reply whamlastxmas 17 hours agoparentprevMy favorite person on twitter (plinz) had advice that I loved and try really hard to follow that’s similar to this. If you see a discussion or comment online that bothers you or is frustrating, the best thing to do is not engage with it. Engagement causes that person to post more and effectively creates more of the content that you dislike. E.g. if no one on the internet ever responded to pro Trump stuff, pro Trump people would get tired of yelling into the void with no reaction. reply jascination 7 hours agorootparentThis is basically that Treehouse of Horror episode of the Simpsons where the ads go on a crazy rampage and to defeat them all you have to do is \"just don't look\" reply rambambram 14 hours agorootparentprevSo true! And then you jinx it when you start talking about ... reply akudha 16 hours agorootparentprevYup, Trump would have likely not won in 2016 if he wasn't pushed by the media. I remember the media would not cover Bernie, even when he got big crowds. But the media would wait for Trump to take the stage, showing empty podium live. ALL media (mainstream or social or legacy) know to push controversy, negativity etc because it gets them engagement which gets them dollars. It is also easy and lazy thing to do. There is a reason politicians push \"the other guy is bad\" rather than \"I am good\" narrative in their ads. It works short term at least while doing long term damage reply qzw 16 hours agorootparentThere’s a meme that we are where we are today in the U.S. starting with the Reagan presidency, and there’s certainly a lot of truth to that. But personally I believe American politics began a long downward trend once television became the primary medium. We’ve all heard how JFK outsmarted Nixon during their televised debate by wearing a blue shirt, because blue showed up as white whereas Nixon’s white shirt showed up as gray. The visuals of candidates began to dominate politics, and people made their judgements based that. Would Reagan have even become president if he wasn’t so good in front of a camera? reply tmm 15 hours agorootparent> We’ve all heard how JFK outsmarted Nixon during their televised debate by wearing a blue shirt, because blue showed up as white whereas Nixon’s white shirt showed up as gray. Huh. I always heard that Nixon refused makeup and JFK didn’t, with the result being about the same: Kennedy looked healthy and Nixon looked like a sweaty corpse. But considering the quality of TV screens and broadcasts in 1960, the shirt thing sounds more realistic. reply drewcoo 15 hours agorootparentprevJFK didn't just wear shirts on TV. He managed to convince the dead in Cook county, Illinois to vote for him, often several times each! I'd consider that a much more impressive first. It eclipses hanging chads and Russiagate in more recent elections. JFK won the election because Nixon conceded even though there were plenty of suspicious circumstances - more than enough to justify challenging the results of a very close election. Today the people have so little faith in the system and in the candidates that almost half of them don't vote. reply smegger001 15 hours agorootparentI have heard this accusation made about every election my entire life the only change.is the canidate and district. but when you look into it the numbers of voter fraud cases found have been in the low double digits. I think its an urban legend at this point. reply drewcoo 14 hours agorootparentAs I stated above, the Daley machine rigged the 1960 election. 3 people did prison time for it. That absolutely happened. https://en.wikipedia.org/wiki/1960_United_States_presidentia... It doesn't actually matter whether or not election fraud is \"an urban legend.\" Faith in the system has been lost. That was Nixon's fear and the reason he didn't contest the election results. reply deprecative 10 hours agorootparentThen Nixon destroyed faith in the system and set us down this path. Maybe the treason guy isn't who you should prop up as some vanguard of democracy. reply deprecative 10 hours agorootparentprevThen Nixon destroyed faith in the system and set us down this path. Maybe the treason guy isn't who you should prop up as some vanguard of democracy. reply LVB 4 hours agorootparentprevThat’s what I figured, too, and then I joined Truth Social to that a peek behind that curtain. It really is just all pro Trump people posting pro Trump stuff and upvoting everyone else’s stuff. There’s little discussion, debate, or internet-typical arguing. It’s everyone just +1’ing each other in the weirdest, most boring echo chamber I’ve seen online. reply ardme 17 hours agoparentprevnext [3 more] [flagged] stirfish 17 hours agorootparent>Avoiding dirty restaurants? You might have low testosterone. Try getting absolutely juiced and demanding to clean the kitchen. The world can be better, but it's also okay to pick your battles. reply bequanna 14 hours agorootparentprevNot necessarily true. Higher T levels are associated with more risk taking and aggression, sure, but also less neurotic and irritable behavior. reply delish 19 hours agoprev>I don't want to fill out your contact form Yes -- and companies or governments don't want to be _contacted_ by you. It's a cost to them. The median \"contact us for a sales quote\" form is clearer and has less friction than the median \"file a complaint / ask a question\" form. One reason not in the article people might use forms instead of email is the \"set and setting\" of being a guest on a website and filling out their form. When in \"your\" email inbox as opposed to on \"someone else's\" site, you may conduct yourself differently. An example of this is the sometimes-onerous Github issue template questions. I'm not arguing they're not necessary, but they do two things: mandate required information and _imply_ that you are a guest and you must hold yourself to someone else's communication norms. reply Aurornis 12 hours agoparent> but they do two things: mandate required information and _imply_ that you are a guest and you must hold yourself to someone else's communication norms. To be honest, tools like this do quite a good job of filtering out people who want you to bend over backward for them. If someone is so stubborn that they refuse to take a couple minutes to fill out someone’s form, they’re likely to be very demanding and uncooperative with every future engagement. Of course, these people never see themselves as such. reply carlosjobim 16 hours agoparentprev> Yes -- and companies or governments don't want to be _contacted_ by you. For a company to make a sale, there needs to be a way for the client to contact them, a way to make a purchase. Having crap contact forms makes as much sense as having restaurant waiters spit clients in the face to greet them. With the world we're living in that might become the norm in a few years or months. reply qingcharles 12 hours agorootparentI've worked for client who spend a lot of money optimizing form fill-out rates down to the nth degree. I once worked on a mortgage form. It had a pic of a call center person next to it. I persuaded my boss to try a pic of a dog with a tie, glasses and headset I found instead. It increased the conversion rate by 17%. People are weird. reply ztetranz 16 hours agoprevIf you build a contact form, please at least make it respond automatically with a \"we've received your message\" email. That at least gives me some confidence that the back end received it and it hopefully went somewhere useful. Without the auto-response I always have doubts if it worked or not. reply wuj 12 hours agoparentAll contact forms should have a feature similar to Google Form's \"Send me a copy of my response\" for recordkeeping. reply knallfrosch 13 hours agoparentprevPlus give back all submitted information. reply ozim 12 hours agorootparentThat's not gonna happen. I can put your e-mail in and type out all kind of swear words or put in phishing link in contact for of a company and you would never know it came from me and you would blame that company. I had spammers trying that all the time, multiple times they had some confirmation for buyer of their services - well only me got that info because from any public form we always sent out confirmation and \"was it you? if not disregard, please\" where content went to our special place so it would be safe, like our sales person not clicking some bs link from such contact form. reply amluto 19 hours agoprev> Hook up a shared mailbox, collaborative inbox, or one of the many off-the-shelf customer service solutions like Zendesk, Zoho Desk, Freshdesk, Zammad, osTicket, or FreeScout to your email. This list is missing the classic, and still excellent, Request Tracker. https://bestpractical.com/request-tracker https://github.com/bestpractical/rt (I have no affiliation, and I’ve only ever interacted with it from the request-submitting side. But it’s always been straightforward and rock-solid, and it’s worth something that the same system has worked continuously for apparently 20 years. And it’s flexible enough that I once worked with a small institution that wired up Request Tracker for submitting jobs to a large-format printer.) reply dijit 16 hours agoparentI used it at one company, that company switched to Jira. I miss RT, it was ugly but good. reply Repulsion9513 12 hours agoparentprevIt's funny how easy it is for something to just keep working when they aren't badly combining it with other services they purchased (hi ZenDesk, I still can't delete users in it because that's part of \"Support\" which we don't use, but I can create users just fine because that's still part of \"Chat\"). reply domdomegg 13 hours agoparentprevThanks for the recommendation, will add! reply SoftTalker 11 hours agoparentprevLast time I worked with RT, to do anything custom you had to script it in perl. That's a language that hasn't been popular for a couple of decades (yes I know it's still running a lot of stuff, but try to hire a Perl dev in 2024), good luck. Maybe a case where AI could help. But not sure even AI knows about RT \"scriptlets\" reply nottorp 19 hours agoprevContact forms are dead. These days you type some text in a box and a LLM gives you a few answers that are totally unrelated to your problem. If you want to contact support, you have to threaten to cancel. If they even care about that. reply ryandrake 18 hours agoparent> If they even care about that. Yea, something I've noticed lately is that companies are beginning to be OK with letting go of customers they can't just silently and passively milk forever. It used to be, you could call up your cable company and threaten to cancel, and they'd pass you over to a \"customer retention\" specialist who will give you a deal that lowers your cost to what it was a few years ago. Last time I tried that trick, they put me on a brief hold and then came back to the phone with \"OK, sir, your service is canceled as of today. Is there anything else I can do for you?\" Whoops! reply wrboyce 17 hours agorootparentI complained to Amazon a few months ago as one of my subscription orders was a few days late noting in the complaint that the service “didn’t feel very prime”. The CSR responded by cancelling my prime subscription despite my not even nearly suggesting I wanted this! (So, naturally, I opened a new complaint about this and received an apology and a few months credit added to my reinstated prime subscription). reply anonzzzies 16 hours agorootparentSounds like ‘AI’ interpreting your email as a cancellation? Although I have now had LLMs parsing my intent for a support question better than the human employee that was appointed to me. reply MaxBarraclough 13 hours agorootparentprevI've heard of the same thing happening with people trying to negotiate a better phone contract. I don't think there's an alternative though. The credible threat of losing you as a customer is, of course, the whole point. Reminds me of an old quote: If you can’t walk away from a negotiation, you aren’t negotiating. reply marcosdumay 13 hours agorootparentThe real alternative is that you negotiate with their competitor and go back to cancel after you get better terms there. Threatening to leave was never a good tactic. reply fHr 16 hours agorootparentprevThey realy are to rich if shit lile this flys, but I guess they realy just made way to much money in the high economical times. reply toast0 15 hours agorootparentI, for one, apprechiate a company that will cancel their service easily and doesn't have a secret price list only available to people that complain. reply WarOnPrivacy 8 hours agorootparentprev> Last time I tried that trick, they put me on a brief hold and then came back to the phone with \"OK, sir, your service is canceled as of today. I canceled Spectrum to jump to a fiber provider. I turned in my equip to a shop and said I was moving out of the country to avoid the retention ordeal. 3 weeks later the plastic cards started coming, every day. Two days ago a Spectrum rep showed up at my house and asked why I quit their service. I explained I needed 1Gb/s upload which ended that part of the conversation. He next offered me free mobile service which I declined. I closed the door before he could pull out a mix tape. reply CM30 14 hours agoparentprevNah, the 2024 solution for getting support is through social media. Tag the company account with your complaints, maybe with a few extra tags for large media outlets or popular internet creators that can amplify it. For example, almost every instance of a YouTube creator retrieving their hacked account in the last few years has been from tagging Team YouTube on Twitter or what not. Seems the possibility of a social media PR nightmare is the only thing that moves the needle nowadays. reply YurgenJurgensen 12 hours agorootparent...and that's terrible. Don't want to sell your soul to Xitter and the Zuckerverse? No customer service for you. And it gives companies an easy metric to prioritise tickets: Follower count. reply CM30 10 hours agorootparentYeah, that's a huge issue for sure. If you don't use any of these services, then getting support is incredibly difficult. The only possible alternative might arguably be Hacker News if the company is a tech one, since there are Alphabet/Meta/Apple/Amazon employees that use this site, and having someone on the inside champion your cause seems to also help things get resolved more quickly. reply MyFedora 11 hours agorootparentprev2024 solution? I don't know about that. Companies typically use software to manage these complaints across multiple social media platforms. Ever since Twitter began charging an obscene amount of money for their API, companies just shrugged and said goodbye. reply CM30 10 hours agorootparentThat's a fair point, Twitter's certainly not what it used to be. That said, there are still a surprisingly number of large companies using it, and a fair few them still run ads there. Not sure when they'll move to Mastodon/Threads/BlueSky/whatever, but it hasn't seemingly happened quite yet. Either way, I'd say the best advice in any case would be \"be very difficult to ignore, to the point the company's reputation takes a hit if they don't resolve the issue\". reply xingped 19 hours agoparentprevSo far I've somehow only run across one LLM support chatbot. And it was actually mostly helpful. Not 100% but decent enough. Better than the old support chatbots that just go \"does this FAQ entry solve your problem?\" Which it never ever does. reply tuetuopay 18 hours agorootparentIn my experience LLM chatbots are a net improvement because they understand \"put me in relation with a human\" as opposed to the scripted ones that only spit out the FAQ. reply nottorp 17 hours agorootparentWhat? Where are those? I thought support forms, LLM based or not, are designed to never put you in touch with a human. reply tuetuopay 16 hours agorootparentI guess that an LLM is harder to scope and retain, and does what it is instructed to. While traditional chatbots can only do what they are programmed for (e.g. follow a scenario), LLMs are easier to sidetrack. Thus you as a client may have an easier time escaping their context, especially when it's a generic LLM integration done by underpaid contractors. It really depends, but at least here in france many companies start having support over whatsapp/messenger/the likes. They used to suck hard a few years ago, but my recent experience with sncf connect (french railway company) was surprisingly good given my issue was working around an idiosyncracy of their system. YMMV as we are talking about chatbots and not plain old forms, thus the interactivity is better and feedback loop to escape the LLM's context is faster. reply jmkni 17 hours agoparentprevMy new hobby is trying to jailbreak these AI customer support chatbots reply pbhjpbhj 15 hours agorootparentAny examples? reply smegger001 15 hours agoparentprevOr send a letter to the legal department. Lawyer generally take complaints seriously. reply ocrow 16 hours agoprevThe rate of spam to a form is roughly constant over time, whereas the rate of spam to a published email address goes up over time as the site is repeatedly scanned by spam robots and the address added to more and more spammer lists. While spam detection is good, it isn't perfect. As your total volume of spam goes up, so does the amount that sneaks through the filters. Additionally, at a certain point it becomes impossible to look in your spam filter for misclassified real email. Eventually you're overwhelmed and have to change emails. If you're going to publish an email address you have to consider it a burnable resource that you will replace once the volume of spam is too high. If the author hasn't experienced this, I think it must be because they haven't done the exercise of leaving a live email address on a public website for years. reply squirrel 16 hours agoparentMy email address has been on my public website for at least 15 years, and my spam level is constant and manageable. That may simply indicate that I’m not popular enough to encounter the problem, of course. reply seabass-labrax 14 hours agorootparentSame for me - I do get spam, and it's frustrating, but the level seems to be broadly constant over time. I wonder if it's deliberate co-operation between scammers and other spam senders, perhaps in order to keep the total amount of spam just under the threshold that would cause people to actually crack down on the issue. Certainly, receiving only single-digit numbers of spam emails each day keeps it just slightly away from being my personal number one priority to get some proper filters installed. reply jltsiren 12 hours agoparentprevIn my experience, peak email spam was 15-20 years ago. At some point, I got ~500 messages/day delivered to my spam folder. Today the average for the same address is maybe 2 messages/day. Spam filters flagging legitimate emails as spam or not delivering them at all has long been a much bigger issue than any spam that gets through. reply domdomegg 12 hours agoparentprevThanks for reading the article! I agree it doesn't consider this point, and I actually hadn't thought of that. Semi-empirically, I've run some websites with emails and contact forms sitting on them for 5+ years and I haven't noticed this effect. Although I must admit I haven't studied it quantitively well enough to determine this for certain - I'd love to look over the data to see if this is true. Unfortunately on all these inboxes spam is deleted automatically after some time so I no longer have records. If you do have data here, it'd be great to see someone publish this and would happily add a link to this analysis! And theoretically, would a contact form link not also be a thing that gets added to more and more lists over time and have the same problem? (Although I also didn't notice this pattern on contact forms, so I'm not claiming this does happen - just a thought experiment on this logic!) reply remram 8 hours agoprevInteresting related tidbit: you can put initial values for \"subject\" and \"body\" in a mailto URL like so: mailto:support@example.com?subject=feedback&body=What+you+were+doing%3A%0AWhat+happened%3A This lets you provide a template to your users as they prepare to send you an email. edit: Oh that's been added to the article reply hgs3 18 hours agoprevOof I just built a new client contact form for my company. The reason I used a form over email is because (1) a form feels more impersonal so I feel less guilty about not responding to potential clients I have no interest in conducting business with and (2) the form works without JavaScript and I figured displaying an email without JS obfuscation would attract more spam. reply lxgr 15 hours agoprevOne of my favorite EU (or German? not sure) regulations is that every company doing business online has to have an email address they actually monitor for customer contact. It’s often the only way to get a written answer in a reliable, persistent medium from a company. Corporate support chats are usually horrible; phone calls leave no proof in case of disputes. reply SushiHippie 9 hours agoparent> One of my favorite EU (or German? not sure) Maybe there is also an EU regulation now, but at least in Germany the \"Telemediengesetz\" was introduced in 2007, where §5 \"Allgemeine Informationspflichten\" is responsible for the need of an Impressum. https://www.gesetze-im-internet.de/tmg/__5.html reply wobfan 7 hours agorootparentThat doesn't include a customer support email though. Most of the times, if not all at bigger companies, there's only an email address in the imprint for legal communication, but specifically not for customer support. reply bilater 15 hours agoprevI'll give you a caveat here. To be clear I think any big company should have easy to fill contact forms since they have the resources but as a solo dev building projects I get so much crap from contact forms. It's not even spam - a significant number of users think of the contact form as a chatbot and will ask personal/lazy questions. Sometimes its not even questions. Just one off statements like 'I want to use your product'. OK...then use my product? Its very weird and made me try to hide the form more and add extra fields just to add a little friction. reply bradleyjg 14 hours agoprevAlso don’t send me an email from noreply@. If you don’t want my email, I don’t want yours. reply foreigner 18 hours agoprevTip for \"collecting structured data\": you can prepopulate the email body using a mailto: link with a \"prompt\" to the sender to fill out the data you need. reply junto 15 hours agoparentGood tip. Also, many customers will classify their enquiry as “other” anyway. reply domdomegg 12 hours agoparentprevGreat idea! Have added to the article. reply bennyp101 15 hours agoprevI'm sure I've said this before on here, but I used to work for a company that provided a SaaS solution to my countries biggest Telecoms provider, and I remember them saying that they actively did NOT want people contacting them - I wouldn't be surprised if at some point actively \"hostile\" interfaces cost less than meat answering the phones/emails reply mrbluecoat 19 hours agoprev> There are some organisations that intentionally want their form to be difficult to complete - perhaps it’s a regulatory requirement that you don’t really want to comply with. If you’re doing this, you should probably feel bad. Bad contact forms I encounter usually fall into this category. reply Animats 15 hours agoprev> You don't want people to fill in your form The real reason. reply sexy_seedbox 10 hours agoparentThe public-facing contact form (and stupid chatbot) are for people who didn't bother to read the FAQs, shipping information, return policies... basically the rest of the website in a very structured format with numerous links and in proper hierarchy with breadcrumbs. That's why the public-facing contact form is long and have friction. If you're in the 80% who had self-checked out and paid, you probably won't ever see the contact form and have our customer service email address in a proper \"purchase successful\" email and you simply reply to that for any follow-up or help. reply junto 15 hours agoparentprevFor many companies it’s a core part of their operating procedure. Cost to acquire, cost to serve and churn reduction being the core elements that they watch like hawks. Answering customer queries eats away at that cost to serve variable. reply dewey 19 hours agoprevThe rant should be: „Don’t break your contact form“, forms that are done well are good for both sides. Better triage on the receiver end, quicker response for the sender. reply wslh 18 hours agoprevNo, my contact form, but just rephrasing the author: why do you put a contact form and don't perform QA and best practices on it? Also, the contact form is just another channel (that should work) but not the center of contacts. For example WhatsApp business replaces a lot of these forms in many regions. reply domdomegg 12 hours agoparentYep - strongly agree that if you are going ahead with a contact form, you better QA it. I'm not a huge fan of organisations shifting to proprietary social networks being a primary contact method (especially if it's that or a broken contact form). But hoping that the Digital Markets Act's interoperability rules might make this a better experience. reply makingstuffs 6 hours agoprevWhen I was working at one of the UK’s high street fashion brands we used to regularly get told to make the contact form non functional during periods of high traffic, just an FYI. reply miniwark 17 hours agoprevI perfectly understand why a contact form would reject a mail with the domain \"example.com\". This is obviously not a valid email domain (and may be the default domain used as greyed example in the contact form). reply domdomegg 12 hours agoparentThe contact form is linked in the article [1], and it rejects genuinely valid emails. You can try it yourself and see it doesn't have greyed examples, and that the problem is not example.com but the use of symbols. For example, greffe_acces@montréal.ca is a valid in-use email [2] that is rejected. example.com was only used to take an example screenshot. [1] https://adamjones.me/blog/dont-use-contact-forms/#:~:text=ma... [2] https://montreal.ca/sujets/politique-de-confidentialite#:~:t... reply imaginarypedro 17 hours agoparentprevtest+test@example.com is a perfectly cromulent email address. reply Rygian 16 hours agorootparentThat's explicitly a fake email address, as it's a special-use domain name reserved by IANA to be used as example/placeholder. https://en.m.wikipedia.org/wiki/Example.com reply WalterBright 8 hours agoprevMy favorite is one for a local utility. Their payment page has a box for the account number. The account number has dashes in it. The box has a headline \"enter your account number, WITHOUT THE DASHES!\" reply ofrzeta 14 hours agoprevThere are a lot of public services and a big insurance company quoted as examples in the blog post. From my consulting experience I can easily imagine how much work, meetings and ceremony got poured in each of these contact forms. Surely they all got enterprisey backends with Spring Boot and whatnot. In another world a junior dev could create better, faster, more ergonomic forms in less time with a more pragmatic approach. reply vzaliva 16 hours agoprevAnother issue with these forms is that the communication method is asymmetric. You fill out the form to contact the business, but their only way to respond is by sending you an email. reply junto 15 hours agoparentThat’s good for companies though. When dealing with large volumes of customers asymmetric is good. Telephone lines get quickly overloaded. reply knallfrosch 13 hours agorootparentAsymmetric, not asynchronous. reply junto 12 hours agorootparentAutocorrect. Sorry. reply chrismorgan 19 hours agoprev> In theory, your customer might be using an email provider that doesn’t support encryption - which could lead them to sending something to you insecurely and putting them at risk. I think most organisations can accept this risk given how rare this is, given that this is on the customer’s end. I think we’ve reached the stage where major providers are rejecting messages over cleartext, right? Requiring either explicit TLS or STARTTLS? reply lambdaxyzw 17 hours agoparentI seriously doubt it. I've tried to set up my (tiny) company email TLS-only, and had to backtrack two days later when two different customers complained that their emails were bounced. One of them was representing a major national bank. I've lost the last bit of hope for e-mail I had. reply neilv 16 hours agoprev> At time of writing, B&Q’s contact form just plainly doesn’t work1. I am fairly amazed that a retailer with revenues in the billions doesn’t notice written queries have stopped coming in. I've noticed customer service forms on brand Web sites are often broken, most commonly by some Web backend error at submission time, but there are other ways, too. For some brands, a broken contact form may be incompetence or corporate dysfunction. But for some of them, it could be a a lazy dark pattern, to reduce customer support costs. (Dark pattern similar to how, when waiting in a holding pattern for telephone customer service, they barge in every 30 seconds, to jolt you into thinking they might be picking up, but then blare, \"Your call is important to us! Please remain on hold, and the next available customer service representative will assist you.\" I assume they know they're making the on-hold experience so much worse.) Of the forms that do work, I'd say at least half the time they trigger an automated email response to call customer service on the telephone. The exact thing you were trying to avoid by opting for a Web form, where you could avoid telephone hell, and also concisely capture the pertinent information in a way that wouldn't get garbled by a CSR (or later by a manager trying to hide a problem). When I have gotten a non-automated email response, it's often someone ignoring the message and latching onto a keyword to send a boilerplate response. Maybe that's good for a poorly treated CSR's metrics, and maybe it also suits someone else's metrics/KPIs/OKRs. Or it's an entirely new boilerplate form, to be done in email, since apparently they asked the wrong things in their Web form. Maybe that one is mostly just ordinary corporate dysfunction, and it also ends up working for some people. Separately, for companies that provide a contact email address... there's the email bounce messages, when the contact address was an email alias that forwards to someone no longer there. Clearly, making sure customer service is covered is When I'm contacting a company, it's usually about a problem they should want to know about, such as if they care about safety. Though I assume that's not the majority of the kinds of complaints they hear. I have sympathy for anyone doing support for large numbers of retail customers/users, but if you chose to do a business that involves that, you can't be disingenuous or negligent about it. reply syradar 15 hours agoprevI have started searching for “@company-url.tld” to find a public email instead of using contact forms reply egypturnash 18 hours agoprevHoly crap the example of the Sanisbury’s contact form is a brilliant piece of passive-aggressive design. It really screams “We do not want to hear from you over the Internet, peon”. reply domdomegg 12 hours agoparentI think it is an awful design at present, no doubt about it. It might not be fully intentional though [1]. I suspect it probably started as a more sensible form, with the top text being just the first sentence. Then they realised they got loads of refund contacts that they preferred to deal with by phone so just added the text - without realising how terrible this made the UI. [1]: https://en.wikipedia.org/wiki/Hanlon%27s_razor reply dazc 19 hours agoprevOn the other side, a typical contact form message reads something like... 'Of course I have read the FAQs but just wanted to ask the exact same question which happens to be the very first FAQ on the list... Also, publishing an email address seldom works out well. reply hnbad 19 hours agoparentSure but that message would be no worse if sent by email instead of contact form. And he does address the \"risk\" of publishing an email address. I can second that I've seen more spam from WordPress forms than published email addresses. reply npilk 19 hours agoprev> You might want to use a contact form to collect structured data, so it saves you time processing requests. For example, making sure the customer provides the right identifiers for you to find them on your systems quickly, or automatically assigning queries to the right teams. As the author alludes to, I think collecting structured data from unstructured input (e.g. inbound support emails) is a very promising real-world use case for LLMs. The goal would be to make it as easy as possible for users to send you information, and then use AI to parse what you need out of it. This would lead to less frustrated users and even increased response rates (for reasons the author mentions). I've been playing around with this idea at https://www.semiform.ai for anyone interested. reply domdomegg 12 hours agoparentThis does seem pretty cool. I have not spent too long looking at this but a few bits of initial feedback: 1. Appreciation for an informative site: I like that your website actually explains what your product does in simple terms. So many marketing sites are impossible to parse, so it's cool that this one gets to the point quickly. Bonus points for having a live demo without a sign up. 2. Dealing with uncertainty / edge cases: One worry I'd have is that this might miss things that are relevant, or doesn't capture uncertainty well. I'd probably want a default of a 'flag for a human because this doesn't fit in the boxes well' marker by default on all forms. For example, if someone responds to the conference example with 'Sure, I'll be there on Monday and I wear a size M. Also I am in a wheelchair so will need the venue to be accessible - please let me know if it's not.', I'd want to make sure this gets flagged rather than the automated system ignoring the last part (especially as people might expect humans to read the response to an email). reply trvz 14 hours agoprevContact forms are a way for companies to offload work onto you, so the data you're sending them is structured in their preferred manner. That argument goes away with LLMs, which can be decently used to process free-form mail. But then, a company with an already awful contact form will hardly do the work to implement that. I guess there's space for a startup doing that and selling it as a service. reply YetAnotherNick 19 hours agoprevThe biggest complaint of the author seems to be needing to give out his data to fill a form. This is very much intentional. In my company, almost all of the contact form submissions tried to sell us something, sometimes in very deceiving way(e.g. I found something broken in your front page which leads to poor search engine ranking, and I can help fix it), if not outright spam. reply parpfish 17 hours agoparentBut this is so frustrating when you need to contact a company and you don’t have the info they want. A while back we wanted to have the physical landlines from the phone company removed from our house. I found the company, but every contact form asked for my account number. If I didn’t provide one, Id get redirected to their sales team because obviously I was trying to create a new account. I don’t remember how I got it fixed other than a lot of time on hold. reply YetAnotherNick 15 hours agorootparentSupport and contact are two different things. Companies shouldn't have existing customer to fill the generic contact us form. reply rglullis 15 hours agoprevIncredible, just last week there were people ranting about email as a communication tool, and I was genuinely surprised because my experience is that people still prefer to just send an email to support@communick over asking a question on my support Discourse instance. reply tomjen3 5 hours agoprevIt costs about 10 dollars to have a certified letter sent through an online service - that includes printing and being put into an envelope. It costs a lot less if you don't need to get it certified. So if your time is worth more than 10 dollars an hour, its is quicker for you to send a normal letter than waste time with their form. reply smallerfish 15 hours agoprevWe're sitting here blaming companies, but lazy, inexperienced or insufficiently skilled developers can and should be blamed for a significant number of the issues he mentions. The state of software on the web in aggregate is generally abysmal. There's dozens of factors that could lead to apathy about quality, but if you care about UX, you could start by ensuring that what you personally produce has excellent UX, and pushing back on peers or higher-ups where what they specify for you to build is bad (and if you don't know what is good or bad, educate yourself.) reply kkfx 14 hours agoprevI'm one of the few I know off that anytime (almost) I discover a crappy service/UI/* I take time to document and protest. Unfortunately most people protest only with their friends instead of barking at those who made the crap... If we all take time to write USEFUL feedback than even org run by ....... have to take the cry into account. reply LightBug1 15 hours agoprevYeah, this rubs me up the wrong way. The matter will have to be very important to get me to engage. I'll enage maybe 5% of the time with contact form reply paulcole 14 hours agoprev> it incorrectly rejects some valid emails Stuff like this is a good way to reject people who are likely to be annoying to hear from. And these articles always miss the point that the company doesn’t necessarily want to hear from you. reply notatoad 19 hours agoprev [–] And I don’t want to read your email… reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article critiques contact forms on websites for being problematic, citing issues like malfunctioning forms, poor user experience, and excessive personal data requests.",
      "It recommends using email as a streamlined and user-centric option for communication and gathering feedback, emphasizing clear communication and accessibility for users.",
      "Some entities deliberately complicate forms to sidestep regulations, underscoring the significance of facilitating user contact via clear communication channels like email."
    ],
    "commentSummary": [
      "The debate encompasses frustrations with flawed job application processes and customer service encounters, highlighting the significance of steering clear of adverse online interactions.",
      "Discussions include the influence of TV on politics, tactics for enhancing customer engagements, issues surrounding email spam, the efficiency of website contact forms, and the integration of LLMs and AI in customer support.",
      "The spotlight is on the necessity of employing transparent communication channels and fostering positive interactions in personal and professional environments."
    ],
    "points": 288,
    "commentCount": 169,
    "retryCount": 0,
    "time": 1714822608
  },
  {
    "id": 40258037,
    "title": "How to Develop a $20B Semiconductor Fab",
    "originLink": "https://www.construction-physics.com/p/how-to-build-a-20-billion-semiconductor",
    "originBody": "Share this post How to Build a $20 Billion Semiconductor Fab www.construction-physics.com Copy link Facebook Email Note Other How to Build a $20 Billion Semiconductor Fab Brian Potter May 03, 2024 101 Share this post How to Build a $20 Billion Semiconductor Fab www.construction-physics.com Copy link Facebook Email Note Other 11 Share Intel fab under construction in Ireland, via Intel Ireland For the last several decades, one avenue of technological progress has towered over nearly everything else: semiconductors. Semiconductors are materials that can have their conductivity varied by many orders of magnitude, which makes it possible to selectively block and allow the flow of electrons. This property makes it possible to manufacture all sorts of electronic devices, not least of which is the digital computer. As semiconductor technology has advanced over the past several decades, the cost and size of electronic computation has steadily fallen, making the PC, the internet, and mobile phones all possible. Today, semiconductors in the form of powerful GPUs that can perform enormous numbers of matrix multiplications are the keystone for advancing AI technology. Increasingly available computation to do enormous amounts of search and learning drives progress in things like game-playing AI, computer vision, and large language models (LLMs). As Moore’s Law has marched forward, transistors (an electronic switch which is the fundamental building block of integrated circuits) have gotten smaller and cheaper. In 1954, the first transistor radio, the TR-1 had 4 transistors which cost $2.50 apiece ($29.03 in 2024 dollars). Today, an AMD Ryzen processor with 9.9 billion transistors is on sale for $650, or about $0.000000066 per transistor; in other words, since the 1950s the cost of a transistor has fallen by about a factor of 300 million. The same shrinking and cheapening has happened for every semiconductor component. But at the same time, the facilities to manufacture them have become increasingly expensive. In the late 60s and early 70s, a semiconductor fabrication facility (or ‘fab’) cost on the order of $4 million (~$31 million in 2024 dollars). Today, a modern fab can cost $10-$20 billion or more. Intel is building a pair of fabs in Arizona which are projected to cost $15 billion apiece, and Samsung’s fab in Taylor, Texas is projected to cost $25 billion. These enormous costs are ultimately due to the same factor that has steadily driven down the cost of semiconductors: Moore’s Law, the observation that the number of components on an integrated circuit tends to double every two years. (There is a Moore’s Second Law, also known as Rock’s Law, which posits that the cost of a semiconductor fab doubles every four years.) The smaller semiconductor components get, the more difficult it is to create the conditions to manufacture them. A modern microchip has features on the order of 50 nanometers in width, or around 1/2000th the width of a human hair.1 Materials are placed in layers a few atoms thin. Creating objects this small requires ultra-precise manufacturing equipment, and a production environment that can screen out as many sources of interference as possible; every rogue speck of dust or tiny fluctuation in electrical voltage. And these conditions must be maintained not in the rarefied conditions of an experimental lab, but in a mass production facility that is producing hundreds of millions of microchips every year. The combination of ultra-precision and high-volume production results in some of the most complex, expensive factories in the world. The semiconductor fabrication process To understand why a modern fab is so expensive, we first need to understand the process for manufacturing semiconductors. If we sliced a computer chip down the middle, and looked very, very closely, we would see that it is composed of a series of layers. Here is a cross section of a chip made with Intel’s current manufacturing process: And here’s an illustrated cross section of a generic integrated circuit, showing the different layers: Via Wikipedia. At the bottom of the chip, the area marked FEOL (front end of line) on the graphic above are the semiconductor components themselves: transistors, capacitors, resistors, diodes, and all the other parts created out of silicon to make a microchip.2 These individual components will be connected together using microscopic metal wires, the layers in the area marked BEOL (back end of line). Because of the enormous number of components in a modern electronic chip, this wiring must be routed on separate layers, which are separated by electrical insulators known as dielectrics. Intel’s current process, for instance, consists of 15 layers of metal wiring. This wiring gets connected together and to the semiconductor components through holes in the layers known as ‘vias.’ Chips are made by building up this complex structure one layer at a time. Starting with a wafer of extremely pure silicon, layers of material are added, portions of the layer are removed, more material is added (or existing material is modified), and so on, until the entire structure is complete. This fabrication method is known as the planar process, and it was invented at Fairchild Semiconductor in 1959 by Jean Hoerni. It’s what makes integrated circuits, and modern computer technology, possible. A simple electronic voltage divider and the fabrication steps to produce it. Via Fabrication Engineering at the Micro- and Nanoscale The process for building up the structure of a microchip can be broken down into four basic operations, repeatedly applied: layering, patterning, doping, and heat-treating. Layering Layering is used to add an extremely thin layer of material — as little as a nanometer or less, 1/100,000th the thickness of a human hair — to the surface of the wafer. These layers might be insulators (such as layers of silicon dioxide used to separate components), conductors (such as layers of copper wiring used to connect components) or semiconductors (such as epitaxial thin films of silicon which form part of the semiconductor components). Depending on the step in the process and the material being used, different layering techniques might be applied. Deposition methods have also evolved over time as features have shrunk. Current common layering methods include thermal oxidation (exposing wafers to oxygen in a furnace, which creates a thin layer of silicon dioxide on the wafer surface), chemical vapor deposition (CVD) (where gaseous chemicals are brought together, depositing their reactants on the surface of the wafer) and sputtering (where a solid material is bombarded with plasma ions, causing atoms to break free and be deposited on the wafer’s surface). The small size of modern semiconductor components demands that these layering methods are capable of extreme precision; modern atomic layer deposition methods, for instance, are capable of creating a single layer of atoms at a time. Patterning Patterning is the process of inscribing specific patterns on the wafer, selectively removing material as needed to create electronic components. On modern semiconductors this is done using photolithography. A wafer is first coated with a thin layer of light-sensitive material called photoresist. Bright light of a specific wavelength is shone through a glass or quartz plate called a mask that has the pattern to be created on the wafer inscribed on it. A mask contains the patterns necessary for a single chip, but a wafer will contain hundreds of individual chips. To expose each one, the mask is moved back and forth over the surface of the wafer using a stepper or scanner. The light that isn’t blocked by the mask passes through and induces a chemical reaction in the photoresist; with positive photoresist, the area exposed to light weakens and becomes soluble; with negative photoresist, the exposed area hardens while the rest of the photoresist remains soluble. In either case, the wafer will be washed after it’s exposed to light, leaving behind a hardened layer of photoresist in the pattern that needs to be applied. From here, the wafer will be etched: a corrosive chemical will be applied to the wafer, eating away the material exposed by the removed photoresist and inscribing the pattern onto the wafer itself. Etching can be “wet” (by exposing the wafer to liquid chemicals such as hydrofluoric acid) or “dry” (by exposing the wafer to gasses like fluorine which have been energized into a plasma). After etching, the remaining photoresist is removed. Doping Doping is the process of introducing very small amounts of impurities into a semiconductor material to change its electrical conductivity. By doping silicon with a small amount of a group V element, such as phosphorus or arsenic, an n-type semiconductor with an excess of free electrons is created. By doping it with a small amount of a group III element, such as boron, a p-type semiconductor with an excess of electron holes is created. With the proper arrangement of p-type and n-type silicon, components such as transistors can be created. p and n-type semiconductors in a MOSFET, via MKS. Early on, the primary method of introducing impurities in semiconductor manufacturing was diffusion: by heating the wafer up in the presence of a gas, atoms of the gas would diffuse into the wafer surface. But today doping is primarily done via ion implantation: a beam of ions (atoms with an excess or deficit of electrons, giving them an electrical charge) is fired at the wafer, depositing the atoms below the surface. Ion implantation device, via Xiao 2012. Heating The last primary operation in semiconductor manufacturing is heating. There are many steps in the process where the wafer is heated or cooled to obtain a specific result. Ion implantation, for instance, results in a damaged silicon crystal structure. This damage is repaired with rapid thermal annealing; thermal lamps heat the wafer to over 1,000 degrees in a matter of seconds, and then the wafer is slowly cooled, repairing the crystal structure. Heating is also used during the lithography process to “bake” and harden the liquid photoresist. Via Xiao 2012. In addition to these four basic processes, semiconductor manufacturing involves many other supporting processes. One key supporting process is polishing: because a microchip is built up from dozens of individual layers, variations in layer thickness will propagate over time as more layers are added, causing problems if left unaddressed. To combat this, wafers are frequently polished during the fabrication process using chemical mechanical polishing (CMP) to smooth out the wafer surface. CMP is also used to fill holes created by etching, by applying a layer of material to the entire wafer and then polishing away the material above the hole. Wafer surface with and without CMP, via Chris Mack. CMP used in the process of filling a trench with silicon dioxide, via Wikipedia. Another key supporting process is cleaning. Because the tiniest stray particle can cause a microchip to malfunction, wafers must be constantly cleaned with solvents and extremely pure water. In a modern fab, a wafer might get cleaned 200 times or more during the production process. And to ensure that processes are working correctly, fabs make extensive use of metrology — measuring the wafer at various points in the process to determine if there have been any manufacturing mistakes or defects. Process flow diagram of a semiconductor fab, via Xiao 2012. By applying these four basic processes over and over again, along with the various support processes, the structure of a microchip is slowly built up. And as more transistors have been crammed onto an integrated circuit, this structure (and the process for creating it) has become increasingly complex. Early integrated circuits could be made with just five to ten different masks and dozens of process steps, but a modern leading-edge microchip might require 80 or more masks and thousands of separate process steps. Process flow for a simple nine mask CMOS chip circa the 1980s, via Embedded Related. Today, leading-edge chips require 80 or so masks, and have much more complex process flows. Once the wafer has gone through all these steps and the structure of the circuit is complete, it proceeds to assembly and packaging. This is where the wafers are cut apart into individual chips, each chip is connected to wires (and to any other chips, as with advanced packaging) and is encased in a protective coating. Packaging might be done at the semiconductor fab, or at another facility entirely. The difficulty and expense of semiconductor manufacturing On its own, a large number of process steps wouldn’t require $20 billion manufacturing facilities. After all, many complex manufactured goods require as many or more steps to produce. An early 20th century watch, for instance, consisted of 150 parts that required over 3700 operations to produce. But when these process steps are being used to make components whose size is measured in the billionths of a meter, manufacturing complexity is enormously magnified. With most manufacturing processes, even those using precision methods to produce interchangeable parts, there is a fair degree of tolerance in the process. If a part is a fraction of a millimeter too long or too short, it will still fit. If the impurity content of a metal is a tiny bit too high, the metal can still be used. If a process runs slightly too fast or too slow, the output is still usable. In semiconductor manufacturing, allowable tolerances are whittled away to almost nothing. Making transistors a few nanometers across requires processes that are hundreds of thousands of times more accurate than conventional manufacturing. The tiniest rogue particle can short out a connection and destroy an entire chip. A few atoms in the wrong place can cause a process step to fail. Imperceptibly small amounts of impurities can irreparably damage materials. The history of semiconductor manufacturing is a chronicle of an endless war against these minute effects and their catastrophic impacts. Even getting semiconductor devices to work at all required paying extremely close attention to chemical concentrations and rogue impurities. When semiconductors were being researched at Bell Labs in the 1940s, mysterious component failures were eventually traced to researchers who had touched copper door knobs; the tiny number of copper atoms that migrated from the door to the workers hands was enough to ruin their work material. Early semiconductor manufacturers found that their processes were influenced by, among other things, the phase of the moon, whether workers had recently visited the bathroom, and female workers’ menstrual cycles. And as semiconductor features have gotten smaller, the problem has only gotten more difficult. As transistors shrank, Intel found that even the most innocuous equipment change — using a slightly longer pipe or cable, for instance — could cause process disruptions to new fabs and cause months or years of lower yields. To combat this, Intel instituted a process known as Copy EXACTLY! New fabs would be identical to existing fabs to the extent possible, right down to the color and brand of the paint on the walls. A modern semiconductor fab must thus create a world of incredible precision and predictability. Every possible effect that might disrupt the manufacturing process, no matter how small, must be screened off, any subtle deviation hunted down and eliminated. And this control must be maintained in a mass production environment, where hundreds of thousands of wafers and millions of individual chips (each one with billions of transistors) are produced each year. The structure of a fab To create this environment, a modern semiconductor fab typically consists of four levels. The heart of the fab is the cleanroom level; the factory floor where the fabrication process actually takes place. Below the cleanroom is the sub-fab, one or more levels (typically two) that contain the ducts, piping, wiring, and equipment needed to support the cleanroom operations. And above the cleanroom level is an interstitial space with fans and filters used for recirculating air into the cleanroom below. Cross section of a fab, via Intel. The cleanroom level contains the process tools: the individual pieces of equipment that perform the various operations discussed above. Tools range from lithography machines (such as ASML’s EUV machines), to chemical vapor deposition machines, to ion implanters, to “wet benches” for cleaning and etching, and so on. These machines are made by a small handful of specialty manufacturers such as ASML, Lam Research, Applied Materials, and Tokyo Electron, and are incredibly expensive. Major process machines can cost $5-$10 million, and some can cost upwards of $100 million. ASML’s cutting edge photolithography machines cost nearly $400 million. These tools might perform one specific process step (such as furnaces for wafer heating) or integrate several individual process steps. Applied Materials, for instance, makes machines which incorporate multiple layering and surface preparation steps. To produce a large number of wafers (a modern microprocessor or “logic” fab might produce 40 to 50,000 wafers a month; a fab producing memory might produce 120,000), a large number of tools are required, 1,000 or more. Process tools will be clustered together by type; this allows the tools to share requirements for things like chemical and gas lines (it’s easier to run piping if all the demand for a certain chemical is in one place), and it makes it possible to isolate certain contaminants. Since copper impurities can have catastrophic effects on semiconductor behavior, parts of the process that use copper (such as the tools depositing microscopic copper wiring) might be isolated from other parts of the fab. HVAC systems will similarly be isolated between different process zones. To minimize interference from things like columns or load-bearing walls, the roof of the fab is typically supported by large, long-spanning trusses which allow the cleanroom space to be as open as possible. Process tools on a cleanroom floor in a Micron fab, via TaiwanPlus Docs. Intel fab under construction, showing the large trusses spanning over the cleanroom floor, via Intel Newsroom. The cleanroom is designed to minimize contamination. Semiconductor fabs are typically built with Class 10 or Class 100 cleanrooms, meaning there can be a maximum of 10 or 100 particles 0.5 microns or larger in each cubic foot of air. By comparison, an ordinary house has on the order of 500,000 particles per cubic foot of air, and a surgical operating room will have about 100,000.) To achieve this, large air handling units force air down through HEPA or ULPA filters in the cleanroom ceiling. The air is pulled down through the floor into the sub-fab, then recirculated up through the ceiling over and over again. Cross section of a fab, via MKS. To prevent particles outside the cleanroom from entering, the cleanroom is kept at positive pressure relative to the outside. Keeping the air at the level of cleanliness required means the air is being changed hundreds of times per hour, compared to the 5-10 times per hour in a typical office building. This volume of air flow, combined with the size of semiconductor cleanrooms (which might be 500,000-1M square feet or more on large fabs), means that fab HVAC systems are enormous. In addition to these large HVAC systems, the materials and process tools used in the cleanroom need to be specially designed not to emit particles. To minimize contamination from people inside the cleanroom, workers don bunny suits in a special gowning area before entering, and go through a special cleaning procedure. As semiconductor features shrank, cleanroom requirements became more stringent. In the 1980s fab cleanrooms were being built to Class 1000 standards, but by the 1990s, some manufacturers were building incredibly clean Class 1 cleanrooms (just one 0.5 micron particle per cubic foot of air). Because achieving this level of cleanliness is costly, manufacturers have adopted a strategy of isolating the wafers from the rest of the cleanroom. Wafers are transported between process tools in sealed pods called FOUPs (front opening unified pods), and the process tool itself is enclosed and sealed off. This TSMC fab, for instance, was built with a Class 100 cleanroom, but the wafers themselves are handled inside Class 0.1 “mini-environments.” By adopting the mini-environment strategy, fabs have been able remove even more impurities from the air without having to purify the millions of cubic feet of air in the cleanroom. Drawing showing FOUPs with a tool mini-environment. FOUPs are moved between process tools using an automated material handling system. In most modern fabs, this consists of a ceiling-mounted rail system (though some fabs, particularly older ones, may use floor-based automated guided vehicles instead). Specially designed vehicles pick up FOUPs at one process tool, and drop them off at the next one. The large number of manufacturing steps means that it can take months for a wafer to go through the entire process. This, combined with the large number of wafers a fab produces a month, means that at any given time there are tens of thousands of wafers at various points in the production process, traveling back and forth between tools or being stored waiting for their turn. Over the course of the production process a wafer will travel many miles back and forth between different production tools. Because of this, the rail transport systems must be extensive, with miles of track and hundreds or thousands of transport vehicles. And they must be carefully designed to avoid bottlenecks and allow the “traffic” to flow smoothly. FOUP AMHS, via Intel. When it arrives at the process tool, the FOUP can be connected to a special loading point, and wafers can be handled automatically within the controlled environment of the machine. After processing, the wafer can be loaded into another FOUP and moved on to the next tool. Since a process tool may not be available immediately, FOUPs are kept in storage until a spot opens up and they can be moved. FOUPs in storage will be occasionally flushed with nitrogen to ensure no contaminants reach the wafers inside.3 But rogue particles aren’t the only thing that can disrupt the manufacturing process, and every part of the fab, from the cleanroom down to the foundation, must be designed to minimize outside interference. The extreme precision required means process tools are extremely sensitive to vibration (even loud noises can negatively affect the manufacturing process), and fabs are designed to minimize it. Fabs are typically built away from airports, rail lines, busy highways, and any other significant outside source of vibrations, and the fab supporting facilities themselves must also be designed to eliminate vibrations. (In one case, unacceptable cleanroom floor vibrations were being caused by an exhaust vent 400 feet away from the fab building.) This extreme vibration sensitivity is exacerbated by the enormous amount of potentially vibration-generating machinery and equipment in a fab such as motors, pumps, HVAC systems, and even fluid flow in pipes. Fabs must limit vibrations to several orders of magnitude below the threshold of perception, while simultaneously absorbing 100 times the mechanical energy and 50 times the air flow as a conventional building. Vibration requirements for different building types. The most sensitive areas in modern semiconductor fabs are built to VC-D or E requirements, or even higher. Via Bayat et al 2012. To minimize vibrations, the floor of the cleanroom is typically built as a deep concrete waffle slab two to four feet thick, supported by closely spaced columns to make it as stiff as possible. Above the slab is a raised metal flooring which allows pipes and cables to be routed below it, and allows process tools to be placed on separate supporting pedestals to prevent worker footsteps from causing vibrations. Some extremely sensitive equipment, such as lithography tools, might require even more stringent measures like active vibration dampers that can sense and cancel out any rogue vibrations. In some cases a fab might be built with a structural isolation break to keep the cleanroom floor physically separated from the rest of the building and any vibrations it might induce. Semiconductor tool pedestal with active vibration damping, separate from the raised cleanroom floor, via TMC Concept for a structural isolation break in a semiconductor fab, via Bayat et al 2012. Preventing vibration also means, ideally, placing your fab in a seismically inactive area. When this isn’t possible (such as in Taiwan or Japan), other measures might be taken, such as adding earthquake dampers or using special foundations that isolate the building from the surrounding soil. In addition to particles and vibrations, there are numerous other sources of interference a fab is designed to eliminate. To prevent light from accidently exposing photoresist, lithography areas often use special yellow lights that won’t expose the chemicals. Anti-static materials must be used for things like flooring to prevent the buildup of static electricity. Tools are sensitive to electromagnetic interference (even the fields from nearby power lines might cause equipment disruption), and tools must be shielded and EMF sources minimized. Fabs have backup generators and uninterruptible power supplies in case of power outages, and equipment must be designed to handle the voltage variations in utility electricity supply. (Prior to design standards that required this, it was apparently common for utility voltage variations to cause semiconductor manufacturing issues.) Temperature and humidity in the cleanroom must be maintained in a narrow range, which places further burden on the HVAC system. A fab also may be designed to be radiofrequency (RF) shielded for security purposes. Beneath the carefully controlled conditions of the cleanroom lies the sub-fab: one or more levels of equipment needed to support the operations of the cleanroom. An EUV lithography machine, for instance, is a complex piece of equipment the size of a truck, but the cleanroom tool is only a portion of the total equipment required. Beneath the cleanroom lies the enormous CO2 laser that drives the EUV system, and the pumps required to create the vacuum within the process chamber. Many other process tools, such as ion implanters and sputtering machines, also require a vacuum, and a large fab may have thousands of vacuum pumps in the sub-fab. Rendering of a process tool with support equipment in the sub-fab below, via Crystal5D Technologies. The sub-fab is also where many of the chemicals required for process tools are stored and routed (although some chemicals, particularly highly toxic ones, will be stored within the process tools to minimize the risk of leaks, while others will be stored outside the fab building). A semiconductor fab uses a wide variety of chemicals, ranging from nitrogen (used for purging and cleaning FOUPS and process tools), oxygen (used in oxidation furnaces and abatement equipment), argon (used in plasma reactions), hydrogen (used in EUV machine cleaning) and others. The volume of chemicals and gasses used, and the amount of exhaust generated, requires an enormous amount of piping, with some pipes reaching up to ten feet in diameter. And these chemicals must be extremely pure, in some cases 99.9999999% pure. Semiconductor gasses, via link. Piping in the sub-fab, via link. Many of the chemicals used in the fab, such as phosphine and arsine for doping, are highly toxic. Others, such as the silane used in some CVD processes, are pyrophoric (meaning they ignite spontaneously with air.) One chemical, the chlorine trifluoride used for cleaning CVD chambers, is so toxic and so prone to spontaneous ignition (it's capable of setting wet sand on fire) that some chemists refuse to work with it. These chemicals require special handling and leak detection systems, backup power systems, and specially designed fire protection systems due to the hazard they represent. The sub-fab also contains the exhaust systems for handling the various byproducts generated by the process tools. To prevent byproducts (particularly ammonia) from reacting with each other, several separate exhaust systems must be used. Many of the processes require abatement equipment (which will burn off any harmful byproducts) or scrubbers to remove hazardous material. This equipment might be mounted to the process tool itself, or be part of a centralized exhaust system. In addition to chemical handling and exhaust equipment, the sub-fab contains electrical boxes, transformers, fans, air handlers, chillers, RF generators, heat exchangers, and all the other equipment needed to keep the fab tools operating. Sub-fabs are often divided into a “clean” sub-fab (where the air from the cleanroom recirculates), and a separate “dirty” or utility sub-fab below that. And while the level of control can be relaxed somewhat in the sub-fab (workers don’t need to wear bunny suits in it), variations and potential disruptions must still be minimized. Even slight changes in voltage, pressure, or vibration, or the smallest particle emitted by the piping, can negatively affect the manufacturing process. Achieving this means that sub-fab equipment must be manufactured with much more stringent requirements and much tighter tolerances than in conventional manufacturing. Things like pipes, ducts, pumps, and other material handling equipment are stainless steel and teflon-coated. The interior of pipes must be electropolished to prevent particles from being emitted or providing places for contaminants to accumulate, and pipes must be joined using special orbital welding methods to prevent leaks or contamination. Often sub-fab equipment and material must itself be manufactured in a cleanroom using specialized manufacturing procedures, and transported to the jobsite double-wrapped in plastic bags to prevent contamination during transport. All chemical and gas piping and handling systems must be designed to deliver a smooth, uninterrupted flow of material; even slight variations in pressure can have “disastrous” effects on the production process. Teflon coating uses in a fab. Outside of the sub-fab, many other facilities are required to support cleanroom operations. Moving heavy process tools into the cleanroom requires industrial elevators that can lift tens of thousands of pounds. A fab uses nitrogen and oxygen in such large amounts (a large logic fab might use 50,000 cubic meters of nitrogen every hour) that fabs will often have air separation plants on-site that produce gasses like nitrogen, oxygen and argon. Similarly, a fab will use very large amounts of ultrapure water for wafer cleaning and CMP, along with the regular water for things like chillers for process cooling. A large fab can use millions of gallons of ultrapure water a day, as much as a town of 50,000 people, and producing it requires its own specialized plant. As with other aspects of the fab, the requirements for ultrapure water have gotten more stringent as features have shrunk. Other fab support equipment includes boilers, chillers, emergency generators, and wastewater treatment. Ultrapure water production process, via MKS. All this equipment and processes consume large amounts of energy. A large fab might demand 100 megawatts of energy, or 10% of the capacity of a large nuclear reactor. Most of this energy is used by the process tools, the HVAC system and other heating/cooling systems. The demands for power and water are severe enough that some fabs have been canceled or relocated when local utilities can’t guarantee supply. And to ensure the proper conditions are maintained in the fab, tens of thousands of sensors are used for monitoring things like particle levels, pressures, and impurity levels. Constructing a fab A large fab will have hundreds of thousands of square feet of cleanroom, and the facility might be spread over hundreds of acres. Building it requires tens of thousands of tons of structural steel, and hundreds of thousands of yards of concrete. Intel boasts that its fabs use twice the concrete as the Burj Khalifa, and five times the metal used in the Eiffel Tower. Putting this material in place at the necessary level of precision requires thousands of specially trained construction workers. Intel’s new fab in Magdeburg is expected to require over 9,300 workers at its peak, and TSMC’s new fab being built in Arizona is using 12,000. Workers must follow specially-designed “clean construction” protocols to keep materials clean, minimize particle intrusion and to ensure the cleanroom can operate successfully when it's completed. In some cases this has meant things like using equipment to “eat” welding smoke, and painting the edges of anything cut on-site with epoxy paint to prevent particle emissions. To meet the level of cleanliness and precision required, piping and mechanical equipment might be prefabricated off-site and then delivered and installed. To help clarify requirements, SEMI, a semiconductor industry association, publishes numerous standards and design guides on various aspects of facility design and equipment production. Once the fab is complete to the point where positive pressure can be maintained in the cleanroom (known as “blow down”), the process tooling can be installed. Equipment might arrive in many separate pieces and need a long and careful assembly process — one of ASML’s advanced EUV machines “ships in 40 freight containers, spread over 20 trucks and three cargo planes.” Tools must be handled carefully: the sensitivity of production tools means that a dropped or bumped piece of equipment can result in delays and millions of dollars of repair. And once tooling is installed, it might take six months to a year of ramp up before the fab is hitting acceptable process yields. Despite their size and complexity, fabs are built surprisingly quickly, around two to four years on average. This is not all that different from other large commercial building projects, and far faster than some other tightly controlled process facilities, like nuclear power plants. In the US, however, fabs are built slower than elsewhere in the world. Fab construction time in the US has increased from just over 650 days on average in the 1990s to over 900 days on average in the 2010s, compared to around 600-700 days in Asian countries, in part because of increasingly stringent environmental review processes. US fabs are also more expensive to build than in other parts of the world, with estimates ranging from 30% more expensive (per Intel) to up to four times as expensive (per TSMC).4 Cost breakdown of a fab It's not surprising that building fabs is so expensive: they’re large, complicated, and have extremely stringent performance requirements. But the fab exists to provide the necessary environment for the thousands of process tools to function, and it's these tools that are by far the most expensive part of building a new fab. Roughly 70-80% of the cost of a new fab will be the process tools that go in it. (One side effect of this is that upgrading an existing fab to use a more advanced process node can cost an appreciable fraction of the cost of a totally new fab.) The equipment proportion of fab cost has risen over time. DRAM fabs in the mid-1980s, for instance, were roughly equally split between facilities and equipment cost, but by the late 1990s equipment made up the vast majority of the cost. Construction costs vs equipment costs in DRAM fabs, via Art et al 1994. For the cost of construction, we see a somewhat similar breakdown as the cost of single family homes, with line items for the structure, architectural finishes, sitework and landscaping, and services and mechanical systems. The main difference is that a fab has a much higher fraction of mechanical, electrical and other services. Things like ultrapure water facilities, multiple exhaust systems, and enormous HVAC needs mean that services make up close to 2/3rds of the cost of a new fab, compared to less than 20% of the cost in a single family home. For the cost of equipment, the largest expense will typically be the lithography machines followed by equipment for deposition and cleaning and etching. The cost of lithography machines is often estimated as 20% of the cost of a new fab, which means that lithography tools can cost as much as the entire fab facility itself. As time has gone on and transistors have shrunk, the cost to build a fab has risen. For modern semiconductor fabs, each new process node increases fab cost by about 30%. There are two main drivers of this increase. One is that more advanced process nodes require more expensive equipment. ASML’s EUV lithography machines, for instance, are far more expensive than the deep ultraviolet machines that they replaced. The second main cost driver is that as transistors continue to shrink, more masks and process steps are required to manufacture them. Connecting transistors together requires more layers of metal wires, and FinFETS (transistors made from “fins” that project up from the wafer’s surface) require more layering steps than the simpler transistors they replaced. (EUV, however, temporarily reversed this trend, as it made it possible to do in one mask what previously took two or more, and thus reduced the number of process steps.) More layers and more process steps means more equipment: if product A has twice the manufacturing steps as product B, it will require twice as much equipment to produce if the output level is to remain constant. Via Practices of Wafer Fab Operations But there are also other cost drivers beyond these two factors. As semiconductor features have gotten smaller, the silicon wafers used to produce them have gotten larger. Chips were produced on 50 mm wafers in the 1970s, but today’s leading-edge fabs use much larger 300 mm wafers (a transition to 450mm wafers was planned but never executed), and larger wafers tend to require more expensive equipment. The switch to 300 mm wafers, for instance, necessitated much greater use of automated material handling equipment, as the wafers were too heavy to carry around in FOUPs by hand. These handling systems, in turn, required larger structures with taller cleanroom ceilings. The ever-increasing cost of fabs has created a shift in the structure of the semiconductor industry. When fabs were cheaper to build, any chip producer could afford to have their own fabs. But as fab costs increased, it became more and more burdensome for manufacturers to operate cutting edge manufacturing facilities due to the high costs, and fewer manufacturers had the production volume to spread those costs over. The “efficient scale” of a 150 mm wafer fab is around 10,000 wafers a month, but for a 300 mm logic wafer fab, that jumps to 40,000 wafers. (And memory wafer fabs will be even larger, at 120,000 wafers per month. Thus only a very small number of companies (currently TSMC, Samsung, and Intel) attempt to operate leading-edge nodes, and the industry has shifted to a “fabless” model where companies like Apple and Nvidia design their chips but have them manufactured by “foundries” like TSMC. By pooling the orders of many different chip companies, the foundries can achieve the scale necessary to afford cutting edge fabs. Conclusion The enormous expense of a modern semiconductor fab boils down to the intersection of two things. One is that semiconductor fabs are mass production factories, with modern “gigafabs” producing hundreds of millions of chips per year, each chip containing billions of transistors.5 The second is that producing semiconductors requires almost unfathomable levels of precision. Manipulating huge volumes of matter on the atomic level, repeatedly and reliably, 24 hours a day 365 days a year, is an enormously expensive undertaking. For those interested in reading more about semiconductor manufacturing, a reading list of the best books and other sources I found on the topic is available here for paid subscribers. 1 Determining the size of transistor features is complicated by the fact that process node descriptions don’t match the actual size of features. TSMC’s 5-nanometer node, for instance, is just a name and a marketing node, and doesn’t correspond to actual feature size. 2 On a microprocessor, these components will overwhelmingly consist of transistors, but things like DRAM will make heavy use of semiconductor capacitors. 3 Prior to FOUPs, wafers were often carried in similar containers called SMIFS. Because they were used for smaller wafers, SMIFs were light enough to be carried by an individual person, and required less material handling automation. 4 It's possible the 4x cost is referring to just the facility, not the equipment. This would place US total fab costs at around 60-90% higher. 5 If we considered things in “capital cost per component” terms, and considered transistors as individual components, semiconductor fabs are actually probably among the cheapest manufacturing facilities. Subscribe to Construction Physics By Brian Potter · Hundreds of paid subscribers Essays about buildings, infrastructure, and industrial technology. Subscribe Error 101 Share this post How to Build a $20 Billion Semiconductor Fab www.construction-physics.com Copy link Facebook Email Note Other 11 Share Previous",
    "commentLink": "https://news.ycombinator.com/item?id=40258037",
    "commentBody": "How to build a $20B semiconductor fab (construction-physics.com)242 points by spenrose 19 hours agohidepastfavorite65 comments gjkood 14 hours agoI have been listening to Kai Ryssdal's Marketplace on NPR/KQED the last few days on my commute home from work. The topic for the last several days was on the CHIPS And Science Act and the new Semiconductor Fabs being built by TSMC and Intel in Phoenx, AZ. It will be several years before the plants already being constructed will go into production but there is a whole ecosystem of current construction, education of the future workforce that will need to be hired in the future. Not to mention all the ancilliary companies that are needed to support these gigantic plants in the area. The dollars from CHIPS Act are not only bringing in the manufacturing plants but will be essential to bring this lost capability back to the US in the scale needed both from an economic and national security perspective. It was great listening to the show and the impact the CHIPS Act on people's lives already happening now and in the future. For anyone interested the links to the specific shows are available as podcasts here. https://www.npr.org/podcasts/381444600/marketplace reply wuj 13 hours agoparentTSMC has moved many engineers from Taiwan to Phoenix. Entire towns were built with accomodation, schools, ethnic grocery stores from scratch. It would be interesting to see this initiative's cultural and economic impact on Phoenix in the years to come. reply navigate8310 1 hour agorootparentHere's another fascinating article about the cultural struggles faced by TSMC in order to expand in Phoenix: https://restofworld.org/2024/tsmc-arizona-expansion/ reply jonhohle 4 hours agorootparentprevI’m not sure where “towns” (plural) are being built. The first plant is across the highway from a master planned community that was built 20 years ago, long before any fabs were being discussed in that area. There certainly is a lot of development there, but it’s not like a factory town or anything. There’s an outdoor recreational shooting facility across the street. I can only assume that is a huge culture shock for anyone coming over from Taiwan. reply psaux 12 hours agorootparentprevI did not know this, I have family in Tempe close by. Do you have any references I can pass along to them? reply gjkood 12 hours agorootparentI believe the new term for Phoenix, AZ and other major cities in AZ is \"Silicon Desert\". You can see a map of the many companies in the high tech space in AZ at the following site. https://siliconmaps.com/silicon-desert/ reply davidgay 9 hours agorootparentMajor semiconductor manufacturing in Arizona is not exactly new: Motorola had fabs there in the 70s (or earlier?), Intel's presence dates to 1980, etc. The article [1] below from 2001 says that Arizona was \"3rd in chipmaking\"... [1] https://go.gale.com/ps/i.do?id=GALE%7CA79561583&sid=googleSc... reply zdw 7 hours agorootparentprevBeing an AZ resident, this map is hilarious as it skips out the ~100 mile distance between Chandler and Tucson. Tucson is more well known for optics, but more focused on space science than on use in silicon fabs. reply wuj 12 hours agorootparentprevHere is an article from Phoenix Business Journal: https://archive.ph/dYCAF > With Taiwanese transplants moving to north Phoenix in droves, Arizona officials — and a local baker — are working behind the scenes to make them feel welcome reply trwm 9 hours agoparentprevYes this is called a positive externality. It is why outsourcing produced much more devastation than was promised and why onshoring will create much more work than expected. The end result currently will likely be stagflation since like always politiciand do the wrong thing even when doing the right thing. reply VHRanger 6 hours agorootparentThat's not why outsourcing created devastation. The issue without outsourcing is that the benefits are widespread (lower prices!) but the drawbacks are concentrated (factory town is now a hellhole). And our political system is incapable of redistributing correctly even though the net effect is highly positive. The seminal study on the topic is the \"China shock\" paper from Autor et Al.: https://www.nber.org/papers/w21906 reply TeMPOraL 1 hour agorootparentStrange, because put this way, it should be entirely positive - widespread benefits and concentrated drawbacks are what we want to happen, as it benefits more people and concentrated problems are much easier to manage. What's very bad is when benefits are concentrated (often in the hands of a small group), and drawbacks are widespread, and therefore near-impossible to manage. See e.g. pollution, emissions... ... and outsourcing. The benefits are concentrated: profits captured by the companies doing the outsourcing. Sure, they may sometimes trickle down to the consumer, but the costs - the distributed drawbacks - are inferior quality of goods, elimination of local jobs, high ecological footprint, abusive business practices, lack of effective customer support. And the extra magic here is, it spreads direct responsibility over national borders, so it's near-impossible to hold anyone to account. reply trwm 6 hours agorootparentprevThe multiplier effect is well known and understood. Nit sure why you're arguing against it. reply philosopher1234 3 hours agorootparentI don’t know or understand this multiplier effect you’re referring to. If you’d like to persuade me (and I assume other readers) explaining your argument might be more effective. Instead I get a sense of “don’t argue against me” as opposed to “this is why I’m right” reply moneywoes 6 hours agoparentprev> ancilliary companies any pertinent examples? r.g. schools for workers families ? reply gjkood 4 hours agorootparentThe show was talking about chip packaging companies to create the end usable chips from the silicon produced. Just for the construction work alone, they mentioned that the pipefitters local union membership has doubled since 2020. Refinery level complexity on the specialized piping needs for the plants. Special training programs geared towards the semiconductor industry being offered in the local Community and Trade schools training people to be the skilled and semi-skilled workforce for these companies. People who were teachers now making four times the income working on the construction project. reply ijidak 6 hours agoparentprevSo, is Intel buying ASML EUV machines for their build out in Arizona? It seems like Intel is skipping ASML EUV entirely. If that's the case, I'm trying to understand how Intel ever gets decent yields at 7nm to 5nm. It's definitely not coming from High-NA, which seems like a short-term distraction. reply ac29 4 hours agorootparentIntel is using EUV for Intel 4, which has been shipping in volume since the beginning of the year. reply jc_811 2 hours agoprevWhat a fascinating article. It truly blows my mind just how advanced humans have become. When playing angry birds on an iPhone we tend to forget the insane amount of research, resources, and specialized expertise that has made all the modern tech world possible. I’d also highly recommend the book Chip Wars by Chris Miller for anyone looking for a deeper dive into the history and current dilemmas of the semiconductor industry reply rmu09 31 minutes agoprevThe stated tolerances of typical manufacturing processes seem to be off by at least an order of magnitude. Not that it does make sense to give a tolerance number without some magnitude it applies to, nonetheless, 0.125mm stated tolerance of CNC machining is ridiculous. reply ghghgfdfgh 14 hours agoprevSomewhat related: a high school student creating his own semiconductor fav in his parent’s garage on a \"budget\". https://youtube.com/watch?v=IS5ycm7VfXg It’s a thoroughly interesting video, but I’m a bit disappointed he never took his idea any further than he did. I’d really love to see something like a 6502 being made at home. reply krishadi 13 hours agoparentSam is now building Atomic Semi [0] with Jim Keller (ex AMD) [0] https://atomicsemi.com/about/#:~:text=general%20fabrication%... reply pests 7 hours agorootparentYou say ex AMD like that's all he's known for. Jim Keller is like some fairy who flies around companies designing sota chips. From DEC, to AMD, to ARM and Broadcom, his own firm, hops over to Apple which then buys his old firm, heads back to ARM and then over to Tesla and finally one last stop at Intel before going into startup land again. Worked on the K7/K8, MIPs for networking, did the A4 and A5 for Apple, on the Zen/K12, and the Tesla TPU. reply amelius 48 minutes agorootparentBut building a fab is less about designing chips (which is a lot like programming) and more about chemistry and fundamental physics. So that fairy is completely useless in this context. reply ghghgfdfgh 12 hours agorootparentprevThat's good for him, but it means we'll never find out in the near future whether it's possible for the average person to create useful ICs in their garage. reply nwiswell 9 hours agorootparentIt's very similar to creating clothes by hand at home. Can you do it, given the right tools, training, and patience? Yes. Will they be any good? No. Will they be cost-competitive? No. Getting chips made on a shuttle run for an old node is very affordable. There's really no need for it. (According to one MPW supplier, 10mm2 of 340nm, up to 10 dies, costs 6400 euros, and it's unlikely 340nm is achievable in a garage anyway) reply ghghgfdfgh 6 hours agorootparentFrom a business perspective, I agree that it’s a fool’s errand. But imagine being able to design and tangibly build your own computer, at home. 6400 euros is pennies for a business, but exorbitant for an individual. I believe the way Sam Zeloof circumvents the enormous amount of capital needed for a chip fab by relying on modern technology to create 1970’s technology. He simply mounts a cheap digital projector onto a cheap microscope - they didn’t have that advantage in the 70s, and thus it cost millions to start a chip fab. My point is that it could conceivably be doable for an individual to create old computing technology with the advantages of living in the modern world. I certainly don’t have the drive to do it, but I wish someone did. reply nwiswell 4 hours agorootparentYou'd spend far more than 6400 euros to do it at home. If you did it often and didn't count your own labor costs, then maybe the average cost would be less, but that's an incredibly specific situation. > I believe the way Sam Zeloof circumvents the enormous amount of capital needed for a chip fab by relying on modern technology to create 1970’s technology Yes, exactly. Old lithographic technology is so crude that you can even use modern high resolution laserjets to print masks (10000 dpi is less than 3 microns). Even so, 1970s-era CVD, PVD, and plasma etch is still quite complicated, and CMP is impossible (it hadn't even been invented yet). So the devices you can create are significantly integration-constrained. reply ghaff 1 hour agorootparentprevMaybe a little different. For narrow enough definitions of \"clothing,\" homemade clothing can be good. And there are other artisanal homemade crafts (e.g. woodworking) that can be good. But I agree in general. reply abdullahkhalids 6 hours agorootparentprevWill it give a 100% guarantee that there are no backdoors in your device? Yes. This yes can be priceless in some circumstances. reply xeonmc 6 hours agorootparentprevreally the main dealbreaker is HF at home, the rest of chipmaking really isn't that complicated on the process level for a crude design. reply ghghgfdfgh 4 hours agorootparentMost rust cleaner that you buy at the store is HF solution. For example, the one that teen used was 1.5% HF. reply nwiswell 4 hours agorootparentI can't believe that worked. Industrially (by which I mean how it was done circa 1970), silicon oxide and silicon nitride was etched using a buffered HF solution known as BOE (buffered oxide etchant). The buffer was typically ammonium fluoride; because of the presence of the buffer, the concentration of fluorine ions in solution stays constant even as some of the fluorine attacks the substrate to form e.g. hexafluorosilicic acid. Since the concentration of fluorine stays constant, so does the etch rate. If you just pull some rust cleaner off the shelf at home depot, the etch rate will crash as the concentration of fluorine ions decreases. That's compounded by the fact that the HF concentration isn't very high in the first place. As a result it would be very difficult to determine how long your wafer should remain in the etch bath. Underetching could easily cause \"opens\" in the circuits from unremoved insulator, and overetching and/or undercut can destroy the patterns you're trying to produce. Either way it can ruin the chip. reply trueismywork 10 hours agorootparentprevIt's possible yes, but not really.. reply etaioinshrdlu 7 hours agoprevOne part really jumped out at me: Thus only a very small number of companies (currently TSMC, Samsung, and Intel) attempt to operate leading-edge nodes, and the industry has shifted to a “fabless” model where companies like Apple and Nvidia design their chips but have them manufactured by “foundries” like TSMC. By pooling the orders of many different chip companies, the foundries can achieve the scale necessary to afford cutting edge fabs. I wonder if AI training will end up being similar in the long-term (it's already partially true today). reply langsoul-com 1 hour agoparentIt is already true. The cost of training gpt jumped exponentially per model, in the realm of millions upon millions. There's a good reason why the latest gpt-4 competitor is meta or Google. reply kibwen 8 hours agoprevFascinating article, thanks. > (There is a Moore’s Second Law, also known as Rock’s Law, which posits that the cost of a semiconductor fab doubles every four years.) If this were to hold, then in under 30 years a single fab would cost more than three trillion dollars, which itself implies a hard upper bound on node improvements by way of economic considerations. reply rossant 1 hour agoprevExcellent and fascinating article. I knew semiconductor fabs were incredibly complex but this article still blew my mind. reply louthy 14 hours agoprevStep 1. Acquire $20b Step 2. Build the $20b semiconductor fab https://knowyourmeme.com/memes/how-to-draw-an-owl reply schmorptron 10 hours agoparentwe can collaborate on this, you handle step 1, i'll do step 2? reply narrator 12 hours agoprevI've been to a number of big construction sites for business purposes. When I hear, \"this is a $40 million dollar project\" and when I see all these workmen, equipment and materials moving around, I think, so this is what $40 million dollars in motion looks like. reply tnmom 12 hours agoparentMuch of that money goes to permitting, admin, and insurance. What you’re seeing is probably the $10-20 million that wasn’t successfully siphoned off by bureaucrats. reply duxup 9 hours agorootparent> Much of that money goes to permitting, How much? reply SECProto 8 hours agorootparentTheir comment suggests 50 to 75%, which is hilariously high. reply duxup 7 hours agorootparentYeah that would be an absurd claim. reply moneywoes 6 hours agorootparentprevis this flow of dollars tracked anywhere reply bmiller2 11 hours agorootparentprevI don't know why you're being downvoted. There is a reason why development volume across states is asymmetric. reply BryantD 9 hours agorootparentBecause “successfully siphoned off by bureaucrats” has an implicit value judgement in it. If someone a) thinks the cost of regulation provides value and b) believes that downvoting is a reasonable response to opinions they disagree with, they’re likely to downvote in this case. Personally I only believe one of those things. I also believe the point about additional cost of development could have been made without the value judgement. reply tnmom 10 hours agorootparentprevWho knows - it’s just fake internet points. Hard to get worked up about. reply zarathustreal 9 hours agorootparentKinda easy to guess, there’s really only one possibility and answer: people felt a certain way after reading and voted accordingly. The idea that anyone votes based on the content of what’s been said is false. For example, you’re not gonna upvote a true statement of a statistical fact if you think there’s a terrible reason for it. You’re gonna downvote because you feel bad about what you perceive as the reason regardless reply stevofolife 5 hours agorootparentprevDownvotes help people skip the noise and focus on the signal. Ain’t nothing wrong with that. reply genericone 4 hours agoprevFirst, attempt to build a $10 billion dollar fab. reply ijidak 10 hours agoprevSo, semi-related question. Is Intel buying any EUV machines from ASML. Not asking about High-NA machines, but regular EUV machines? I can't find the answer online although I do see that the New Ireland Fab seems to be in the regular EUV range. reply langsoul-com 1 hour agoparentEveryone has to buy from ASML, and even if someone wants one, there's a wait list. The sheer complexity and costs means there's no real possibility of a competitor. Nor would a billion dollar fab want to experiment with a new company. reply Aloisius 7 hours agoparentprevThere is no one else to buy EUV machines from. ASML is the sole supplier in the world. Fab 34 in Ireland got its first ASML EUV machine two years ago. I don't believe High NA EUV machines will end up in this generation of fabs. The first one was shipped to Intel's dev fab a few months back. reply ijidak 6 hours agorootparentBut, did Intel skip EUV entirely, or did it finally start buying EUV machines? reply Aloisius 6 hours agorootparentThey use ASML EUV machines today for all their Intel 4 process chips (Meteor Lake so far). They plan to use it for Intel 3, Intel 20A and 18A as well. Intel 14A will use high-NA EUV. reply deanmen 12 hours agoprevV reply imperialdrive 14 hours agoprevAbsolutely fascinating, thank you for sharing! I just want to order new phones computers laptops and servers left and right now. reply xyst 12 hours agoprev [–] At what point will current fabrication plants reach their limit? Is it a supply issue? Or maybe a resource constraint? > Similarly, a fab will use very large amounts of ultrapure water for wafer cleaning and CMP, along with the regular water for things like chillers for process cooling. A large fab can use millions of gallons of ultrapure water a day, as much as a town of 50,000 people, and producing it requires its own specialized plant. This is wild. All of this water so companies can create chips that will ultimately be used to … pump out “advanced” chat bots. I really hope all of this sacrifice is worth it in the end. Climate change is accelerating the loss of drinkable water around the planet. If the best we could do is a slightly better chat bot, then we are doomed. reply patmcc 11 hours agoparent\"...as much as a town of 50,000 people\" This is always so funny to me. Oh, something that can make enough chips for millions/billions of devices also uses as much water as a small town? That sounds perfectly reasonable. Don't build it in the middle of the desert, I guess, but otherwise it's not a problem. reply kortilla 9 hours agorootparentThe desert is fine. That’s like a couple of idiotic farms worth of water for a much larger benefit. reply jiggawatts 12 hours agoparentprevThe water is not used up, and it doesn’t even evaporate, except for the small amount used in evaporative chillers. The big scary sounding number at the input has a big number at the output. Not to mention that up to 98 percent of the water is reused on site. It just gets cleaned and goes back into facility. It’s a big loop, not an input disappearing into a parallel universe. reply konfusinomicon 10 hours agorootparentscience be damned, why oh why is all this water pouring into our universe. luckily its not 2% more or we'd all be doomed reply mptest 10 hours agoparentprev [–] >chat bots Ignoring Alphafold, moderna partnering with openai, the new class of antibiotics, etc etc. There's a lot more to ai than chat bots. That's a disingenuous reduction. I agree that it's a bold bet though, burning how many ever billions a year on the hope that ai can help us solve medicine and fusion and climate change. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Semiconductor manufacturing is complex and precise, involving costly facilities, equipment, and safety measures to produce microchips.",
      "Leading companies like Intel and Samsung invest heavily in modern fabs to meet the demand for advanced technology.",
      "The semiconductor industry faces challenges due to the high costs of manufacturing, leading to a trend of companies adopting a fabless model by outsourcing chip production to foundries."
    ],
    "commentSummary": [
      "TSMC and Intel are constructing new semiconductor fabs in Phoenix, AZ under the CHIPS Act, impacting economic and national security.",
      "The discussion includes TSMC's cultural challenges, Arizona's semiconductor manufacturing history, and the pros and cons of outsourcing.",
      "Topics range from chip design, fabrication processes, EUV machines, water usage concerns, sustainability, and AI's potential in addressing global issues."
    ],
    "points": 242,
    "commentCount": 65,
    "retryCount": 0,
    "time": 1714833764
  },
  {
    "id": 40257677,
    "title": "In Praise of Leisure: Bertrand Russell's Vision for a Balanced Society",
    "originLink": "https://harpers.org/archive/1932/10/in-praise-of-idleness/",
    "originBody": "October 1932 Issue [Article] In Praise of Idleness Download PDF Adjust Share by Bertrand Russell, LIKE most of my generation, I was brought up on the saying “Satan finds some mischief still for idle hands to do.” Being a highly virtuous child, I believed all that I was told and acquired a conscience which has kept me working hard down to the present moment. But although my conscience has controlled my actions, my opinions have undergone a revolution. I think that there is far too much work done in the world, that immense harm is caused by the belief that work is virtuous, and that what needs to be preached in modern industrial countries is quite different from what always has been preached. Every one knows the story of the traveler in Naples who saw twelve beggars lying in the sun (it was before the days of Mussolini), and offered a lira to the laziest of them. Eleven of them jumped up to claim it, so he gave it to the twelfth. This traveler was on the right lines. But in countries which do not enjoy Mediterranean sunshine idleness is more difficult, and a great public propaganda will be required to inaugurate it. I hope that after reading the following pages the leaders of the Y. M. C. A. will start a campaign to induce good young men to do nothing. If so, I shall not have lived in vain. Before advancing my own arguments for laziness, I must dispose of one which I cannot accept. Whenever a person who already has enough to live on proposes to engage in some everyday kind of job, such as school-teaching or typing, he or she is told that such conduct takes the bread out of other people’s mouths, and is, therefore, wicked. If this argument were valid, it would only be necessary for us all to be idle in order that we should all have our mouths full of bread. What people who say such things forget is that what a man earns he usually spends, and in spending he gives employment. As long as a man spends his income he puts just as much bread into people’s mouths in spending as he takes out of other people’s mouths in earning. The real villain, from this point of view, is the man who saves. If he merely puts his savings in a stocking, like the proverbial French peasant, it is obvious that they do not give employment. If he invests his savings the matter is less obvious, and different cases arise. One of the commonest things to do with savings is to lend them to some government. In view of the fact that the bulk of the expenditure of most civilized governments consists in payments for past wars and preparation for future wars, the man who lends his money to a government is in the same position as the bad men in Shakespeare who hire murderers. The net result of the man’s economical habits is to increase the armed forces of the State to which he lends his savings. Obviously it would be better if he spent the money, even if he spent it on drink or gambling. But, I shall be told, the case is quite different when savings are invested in industrial enterprises. When such enterprises succeed and produce something useful this may be conceded. In these days, however, no one will deny that most enterprises fail. That means that a large amount of human labor, which might have been devoted to producing something which could be enjoyed, was expended on producing machines which, when produced, lay idle and did no good to anyone. The man who invests his savings in a concern that goes bankrupt is, therefore, injuring others as well as himself. If he spent his money, say, in giving parties for his friends, they (we may hope) would get pleasure, and so would all those on whom he spent money, such as the butcher, the baker, and the bootlegger. But if he spends it (let us say) upon laying down rails for surface cars in some place where surface cars turn out to be not wanted, he has diverted a mass of labor into channels where it gives pleasure to no one. Nevertheless, when he becomes poor through the failure of his investment he will be regarded as a victim of undeserved misfortune, whereas the gay spendthrift, who has spent his money philanthropically, will be despised as a fool and a frivolous person. All this is only preliminary. I want to say, in all seriousness, that a great deal of harm is being done in the modern world by the belief in the virtuousness of work, and that the road to happiness and prosperity lies in an organized diminution of work. [inline_ad ad=2]First of all: what is work? Work is of two kinds: first, altering the position of matter at or near the earth’s surface relatively to other such matter; second, telling other people to do so. The first kind is unpleasant and ill paid; the second is pleasant and highly paid. The second kind is capable of indefinite extension: there are not only those who give orders but those who give advice as to what orders should be given. Usually two opposite kinds of advice are given simultaneously by two different bodies of men; this is called politics. The skill required for this kind of work is not knowledge of the subjects as to which advice is given, but knowledge of the art of persuasive speaking and writing, i.e. of advertising. Throughout Europe, though not in America, there is a third class of men, more respected than either of the classes of workers. These are men who, through ownership of land, are able to make others pay for the privilege of being allowed to exist and to work. These landowners are idle, and I might, therefore, be expected to praise them. Unfortunately, their idleness is rendered possible only by the industry of others; indeed their desire for comfortable idleness is historically the source of the whole gospel of work. The last thing they have ever wished is that others should follow their example. From the beginning of civilization until the industrial revolution a man could, as a rule, produce by hard work little more than was required for the subsistence of himself and his family, although his wife worked at least as hard and his children added their labor as soon as they were old enough to do so. The small surplus above bare necessaries was not left to those who produced it, but was appropriated by priests and warriors. In times of famine there was no surplus; the warriors and priests, however, still secured as much as at other times, with the result that many of the workers died of hunger. This system persisted in Russia until 1917, and still persists in the East; in England, in spite of the Industrial Revolution, it remained in full force throughout the Napoleonic wars, and until a hundred years ago, when the new class of manufacturers acquired power. In America the system came to an end with the Revolution, except in the South, where it persisted until the Civil War. A system which lasted so long and ended so recently has naturally left a profound impression upon men’s thoughts and opinions. Much that we take for granted about the desirability of work is derived from this system and, being pre-industrial, is not adapted to the modern world. Modern technic has made it possible for leisure, within limits, to be not the prerogative of small privileged classes, but a right evenly distributed throughout the community. The morality of work is the morality of slaves, and the modern world has no need of slavery. It is obvious that, in primitive communities, peasants, left to themselves, would not have parted with the slender surplus upon which the warriors and priests subsisted, but would have either produced less or consumed more. At first sheer force compelled them to produce and part with the surplus. Gradually, however, it was found possible to induce many of them to accept an ethic according to which it was their duty to work hard, although part of their work went to support others in idleness. By this means the amount of compulsion required was lessened, and the expenses were diminished. To this day ninety-nine per cent of British wage-earners would be genuinely shocked if it were proposed that the King should not have a larger income than a working man. The conception of duty, speaking historically, has been a means used by the holders of power to induce others to live for the interests of their masters rather than their own. Of course the holders of power conceal this fact from themselves by managing to believe that their interests are identical with the larger interests of humanity. Sometimes this is true; Athenian slave-owners, for instance, employed part of their leisure in making a permanent contribution to civilization which would have been impossible under a just economic system. Leisure is essential to civilization, and in former times leisure for the few was rendered possible only by the labors of the many. But their labors were valuable, not because work is good, but because leisure is good. And with modern technic it would be possible to distribute leisure justly without injury to civilization. Modern technic has made it possible to diminish enormously the amount of labor necessary to produce the necessaries of life for every one. This was made obvious during the War. At that time all the men in the armed forces, all the men and women engaged in the production of munitions, all the men and women engaged in spying, war propaganda, or government offices connected with the War were withdrawn from productive occupations. In spite of this, the general level of physical well-being among wage-earners on the side of the Allies was higher than before or since. The significance of this fact was concealed by finance; borrowing made it appear as if the future was nourishing the present. But that, of course, would have been impossible; a man cannot eat a loaf of bread that does not yet exist. The War showed conclusively that by the scientific organization of production it is possible to keep modern populations in fair comfort on a small part of the working capacity of the modern world. If at the end of the War the scientific organization which had been created in order to liberate men for fighting and munition work had been preserved, and the hours of work had been cut down to four, all would have been well. Instead of that, the old chaos was restored, those whose work was demanded were made to work long hours, and the rest were left to starve as unemployed. Why? Because work is a duty, and a man should not receive wages in proportion to what he has produced, but in proportion to his virtue as exemplified by his industry. This is the morality of the Slave State, applied in circumstances totally unlike those in which it arose. No wonder the result has been disastrous. Let us take an illustration. Suppose that at a given moment a certain number of people are engaged in the manufacture of pins. They make as many pins as the world needs, working (say) eight hours a day. Someone makes an invention by which the same number of men can make twice as many pins as before. But the world does not need twice as many pins: pins are already so cheap that hardly any more will be bought at a lower price. In a sensible world everybody concerned in the manufacture of pins would take to working four hours instead of eight, and everything else would go on as before. But in the actual world this would be thought demoralizing. The men still work eight hours, there are too many pins, some employers go bankrupt, and half the men previously concerned in making pins are thrown out of work. There is, in the end, just as much leisure as on the other plan, but half the men are totally idle while half are still overworked. In this way it is insured that the unavoidable leisure shall cause misery all round instead of being a universal source of happiness. Can anything more insane be imagined? The idea that the poor should have leisure has always been shocking to the rich. In England in the early nineteenth century fifteen hours was the ordinary day’s work for a man; children sometimes did as much, and very commonly did twelve hours a day. When meddlesome busy-bodies suggested that perhaps these hours were rather long, they were told that work kept adults from drink and children from mischief. When I was a child, shortly after urban working men had acquired the vote, certain public holidays were established by law, to the great indignation of the upper classes. I remember hearing an old Duchess say, “What do the poor want with holidays? they ought to work.” People nowadays are less frank, but the sentiment persists, and is the source of much economic confusion. II Let us, for a moment, consider the ethics of work frankly, without superstition. Every human being, of necessity, consumes in the course of his life a certain amount of produce of human labor. Assuming, as we may, that labor is on the whole disagreeable, it is unjust that a man should consume more than he produces. Of course he may provide services rather than commodities, like a medical man, for example; but he should provide something in return for his board and lodging. To this extent, the duty of work must be admitted, but to this extent only. I shall not develop the fact that in all modern societies outside the U. S. S. R. many people escape even this minimum of work, namely all those who inherit money and all those who marry money. I do not think the fact that these people are allowed to be idle is nearly so harmful as the fact that wage-earners are expected to overwork or starve. If the ordinary wage-earner worked four hours a day there would be enough for everybody, and no unemployment — assuming a certain very moderate amount of sensible organization. This idea shocks the well-to-do, because they are convinced that the poor would not know how to use so much leisure. In America men often work long hours even when they are already well-off; such men, naturally, are indignant at the idea of leisure for wage-earners except as the grim punishment of unemployment, in fact, they dislike leisure even for their sons. Oddly enough, while they wish their sons to work so hard as to have no time to be civilized, they do not mind their wives and daughters having no work at all. The snobbish admiration of uselessness, which, in an aristocratic society, extends to both sexes, is under a plutocracy confined to women; this, however, does not make it any more in agreement with common sense. The wise use of leisure, it must be conceded, is a product of civilization and education. A man who has worked long hours all his life will be bored if he becomes suddenly idle. But without a considerable amount of leisure a man is cut off from many of the best things. There is no longer any reason why the bulk of the population should suffer this deprivation; only a foolish asceticism, usually vicarious, makes us insist on work in excessive quantities now that the need no longer exists. In the new creed which controls the government of Russia, while there is much that is very different from the traditional teaching of the West, there are some things that are quite unchanged. The attitude of the governing classes, and especially of those who control educational propaganda, on the subject of the dignity of labor is almost exactly that which the governing classes of the world have always preached to what were called the “honest poor.” Industry, sobriety, willingness to work long hours for distant advantages, even submissiveness to authority, all these reappear; moreover, authority still represents the will of the Ruler of the Universe, Who, however, is now called by a new name, Dialectical Materialism. The victory of the proletariat in Russia has some points in common with the victory of the feminists in some other countries. For ages men had conceded the superior saintliness of women and had consoled women for their inferiority by maintaining that saintliness is more desirable than power. At last the feminists decided that they would have both, since the pioneers among them believed all that the men had told them about the desirability of virtue but not what they had told them about the worthlessness of political power. A similar thing has happened in Russia as regards manual work. For ages the rich and their sycophants have written in praise of “honest toil,” have praised the simple life, have professed a religion which teaches that the poor are much more likely to go to heaven than the rich, and in general have tried to make manual workers believe that there is some special nobility about altering the position of matter in space, just as men tried to make women believe that they derived some special nobility from their sexual enslavement. In Russia all this teaching about the excellence of manual work has been taken seriously, with the result that the manual worker is more honored than anyone else. What are, in essence, revivalist appeals are made to secure shock workers for special tasks. Manual work is the ideal which is held before the young, and is the basis of all ethical teaching. For the present this is all to the good. A large country, full of natural resources, awaits development and has to be developed with very little use of credit. In these circumstances hard work is necessary and is likely to bring a great reward. But what will happen when the point has been reached where everybody could be comfortable without working long hours? In the West we have various ways of dealing with this problem. We have no attempt at economic justice, so that a large proportion of the total produce goes to a small minority of the population, many of whom do no work at all. Owing to the absence of any central control over production, we produce hosts of things that are not wanted. We keep a large percentage of the working population idle because we can dispense with their labor by making others overwork. When all these methods prove inadequate we have a war: we cause a number of people to manufacture high explosives, and a number of others to explode them, as if we were children who had just discovered fireworks. By a combination of all these devices we manage, though with difficulty, to keep alive the notion that a great deal of manual work must be the lot of the average man. In Russia, owing to economic justice and central control over production, the problem will have to be differently solved. The rational solution would be as soon as the necessaries and elementary comforts can be provided for all to reduce the hours of labor gradually, allowing a popular vote to decide, at each stage, whether more leisure or more goods were to be preferred. But, having taught the supreme virtue of hard work, it is difficult to see how the authorities can aim at a paradise in which there will be much leisure and little work. It seems more likely that they will find continually fresh schemes by which present leisure is to be sacrificed to future productivity. I read recently of an ingenious scheme put forward by Russian engineers for making the White Sea and the northern coasts of Siberia warm by putting a dam across the Kara Straits. An admirable plan, but liable to postpone proletarian comfort for a generation, while the nobility of toil is being displayed amid the ice-fields and snowstorms of the Arctic Ocean. This sort of thing, if it happens, will be the result of regarding the virtue of hard work as an end in itself, rather than as a means to a state of affairs in which it is no longer needed. III The fact is that moving matter about, while a certain amount of it is necessary to our existence, is emphatically not one of the ends of human life. If it were, we should have to consider every navvy superior to Shakespeare. We have been misled in this matter by two causes. One is the necessity of keeping the poor contented, which has led the rich for thousands of years to preach the dignity of labor, while taking care themselves to remain undignified in this respect. The other is the new pleasure in mechanism, which makes us delight in the astonishingly clever changes that we can produce on the earth’s surface. Neither of these motives makes any great appeal to the actual worker. If you ask him what he thinks the best part of his life, he is not likely to say, “I enjoy manual work because it makes me feel that I am fulfilling man’s noblest task, and because I like to think how much man can transform his planet. It is true that my body demands periods of rest, which I have to fill in as best I may, but I am never so happy as when the morning comes and I can return to the toil from which my contentment springs.” I have never heard working men say this sort of thing. They consider work, as it should be considered, as a necessary means to a livelihood, and it is from their leisure hours that they derive whatever happiness they may enjoy. It will be said that while a little leisure is pleasant, men would not know how to fill their days if they had only four hours’ work out of the twenty-four. In so far as this is true in the modern world it is a condemnation of our civilization; it would not have been true at any earlier period. There was formerly a capacity for light-heartedness and play which has been to some extent inhibited by the cult of efficiency. The modern man thinks that everything ought to be done for the sake of something else, and never for its own sake. Serious-minded persons, for example, are continually condemning the habit of going to the cinema, and telling us that it leads the young into crime. But all the work that goes to producing a cinema is respectable, because it is work, and because it brings a money profit. The notion that the desirable activities are those that bring a profit has made everything topsy-turvy. The butcher who provides you with meat and the baker who provides you with bread are praiseworthy because they are making money but when you enjoy the food they have provided you are merely frivolous, unless you eat only to get strength for your work. Broadly speaking, it is held that getting money is good and spending money is bad. Seeing that they are two sides of one transaction, this is absurd; one might as well maintain that keys are good but keyholes are bad. The individual, in our society, works for profit; but the social purpose of his work lies in the consumption of what he produces. It is this divorce between the individual and the social purpose of production that makes it so difficult for men to think clearly in a world in which profitmaking is the incentive to industry. We think too much of production and too little of consumption. One result is that we attach too little importance to enjoyment and simple happiness, and that we do not judge production by the pleasure that it gives to the consumer. When I suggest that working hours should be reduced to four, I am not meaning to imply that all the remaining time should necessarily be spent in pure frivolity. I mean that four hours’ work a day should entitle a man to the necessities and elementary comforts of life, and that the rest of his time should be his to use as he might see fit. It is an essential part of any such social system that education should be carried farther than it usually is at present, and should aim, in part, at providing tastes which would enable a man to use leisure intelligently. I am not thinking mainly of the sort of things that would be considered “high-brow.” Peasant dances have died out except in remote rural areas, but the impulses which caused them to be cultivated must still exist in human nature. The pleasures of urban populations have become mainly passive: seeing cinemas, watching football matches, listening to the radio, and so on. This results from the fact that their active energies are fully taken up with work; if they had more leisure they would again enjoy pleasures in which they took an active part. In the past there was a small leisure class and a large working class. The leisure class enjoyed advantages for which there was no basis in social justice; this necessarily made it oppressive, limited its sympathies, and caused it to invent theories by which to justify its privileges. These facts greatly diminished its excellence, but in spite of this drawback it contributed nearly the whole of what we call civilization. It cultivated the arts and discovered the sciences; it wrote the books, invented the philosophies, and refined social relations. Even the liberation of the oppressed has usually been inaugurated from above. Without the leisure class mankind would never have emerged from barbarism. The method of a hereditary leisure class without duties was, however, extraordinarily wasteful. None of the members of the class had been taught to be industrious, and the class as a whole was not exceptionally intelligent. It might produce one Darwin, but against him had to be set tens of thousands of country gentlemen who never thought of anything more intelligent than fox-hunting and punishing poachers. At present, the universities are supposed to provide, in a more systematic way, what the leisure class provided accidentally and as a byproduct. This is a great improvement, but it has certain drawbacks. University life is so different from life in the world at large that men who live in an academic milieu tend to be unaware of the pre-occupations of ordinary men and women; moreover, their ways of expressing themselves are usually such as to rob their opinions of the influence that they ought to have upon the general public. Another disadvantage is that in universities studies are organized, and the man who thinks of some original line of research is likely to be discouraged. Academic institutions, therefore, useful as they are, are not adequate guardians of the interests of civilization in a world where every one outside their walls is too busy for unutilitarian pursuits. In a world where no one is compelled to work more than four hours a day every person possessed of scientific curiosity will be able to indulge it, and every painter will be able to paint without starving, however excellent his pictures may be. Young writers will not be obliged to draw attention to themselves by sensational pot-boilers, with a view to acquiring the economic independence needed for monumental works, for which, when the time at last comes, they will have lost the taste and the capacity. Men who in their professional work have become interested in some phase of economics or government will be able to develop their ideas without the academic detachment that makes the work of university economists lacking in reality. Medical men will have time to learn about the progress of medicine. Teachers will not be exasperatedly struggling to teach by routine things which they learned in their youth, which may, in the interval, have been proved to be untrue. Above all, there will be happiness and joy of life, instead of frayed nerves, weariness, and dyspepsia. The work exacted will be enough to make leisure delightful, but not enough to produce exhaustion. Since men will not be tired in their spare time, they will not demand only such amusements as are passive and vapid. At least one per cent will probably devote the time not spent in professional work to pursuits of some public importance, and, since they will not depend upon these pursuits for their livelihood, their originality will be unhampered, and there will be no need to conform to the standards set by elderly pundits. But it is not only in these exceptional cases that the advantages of leisure will appear. Ordinary men and women, having the opportunity of a happy life, will become more kindly and less persecuting and less inclined to view others with suspicion. The taste for war will die out, partly for this reason, and partly because it will involve long and severe work for all. Good nature is, of all moral qualities, the one that the world needs most, and good nature is the result of ease and security, not of a life of arduous struggle. Modern methods of production have given us the possibility of ease and security for all; we have chosen instead to have overwork for some and starvation for others. Hitherto we have continued to be as energetic as we were before there were machines. In this we have been foolish, but there is no reason to go on being foolish for ever. Tags 20th century Hours of labor Leisure Social classes Work ethic More from Bertrand Russell Wraparound Wraparound Reports From the October 1932 issue Download PDF From the Archive Timeless stories from our 173-year archive handpicked to speak to the news of the day. Email address Sign Up Got it! Thanks for signing up! Related [Report] The Life and Death of Hollywood Film and television writers face an existential threat by Daniel Bessner, [Letter from the Campaign Trail] The Race for Second Place The Republican primaries as farce by Kyle Paoletta, [Letter from Big Sky] Slippery Slope How private equity shapes a ski town by Nick Bowlin, Adjust Share",
    "commentLink": "https://news.ycombinator.com/item?id=40257677",
    "commentBody": "In Praise of Idleness (1932) (harpers.org)228 points by TotalCrackpot 20 hours agohidepastfavorite89 comments wenc 14 hours agoI read In Praise of Idleness when I was 13-14 because an older person recommended it to me. Although the essay was written in the 1935, he convinced me this was the future, and that it foretold what European life was going to be. And indeed, modern Western Europe lives this way -- where leisure is accorded importance, hard work is not the highest virtue, and citizens were free to create culture and invent new ideas. As evidence, he pointed to all the discoveries made by medieval monks and people with idle time to play with ideas, as opposed to the proletariat who worked but did not have the luxury to think higher thoughts. Idleness was thus the pre-condition for great ideas. In a sense, this is the vision of UBI -- where basic needs were met, and people were free to self-actualize. This is also the happy version of tenure in academia -- where you didn't have to worry about \"publish or perish\" but instead you get to work on really important ideas without showing results for years (multi year grants or being in a place like the IAS helps). Google in some ways used to operate like this before the current pivot -- many googlers lived a life of \"resting and vesting\" while wandering about for years looking for a big idea with little pressure to deliver anything. I definitely found this vision attractive, but as I grew older, I realized that it was not entirely tenable in it purest form. Yes, the best ideas certain came from having time to wander and work on different things (you get more creative working on multiple decorrelated ideas at the same time rather than one big idea), but in my experience, complete idleness without pressure to deliver anything does not work. I don't know if I believe the premise of In Praise of Idleness any more. We no longer live in a simple world. In a complex world, great ideas come from incrementalism, and keeping busy and making progress seems to be necessary in many domains in order get to the big idea because all the low hanging fruit have been plucked. reply advael 13 hours agoparentI think Russel, like many scientists, feels the need to dangle \"the next big idea\" as a tantalizing reason to allow more idleness. I do think that in an important sense, he is right and you are wrong, that \"pressure to deliver\" is not as necessary as you think, and that the world has not changed so much since his time so as for him to have once been right but now be wrong However, I think this takes for granted the primacy of \"big ideas\" as the sole organizing principle we should arrange the world to efficiently produce, by force if necessary. I think the real argument for UBI is that self-determination is a core value, that the negotiations we make to better society neither need nor should involve a gun to the head of every person not born into wealth. I also think that overvaluing efficiency, expediency, and generally speaking impatience is pushing our species off a cliff reply DenisM 13 hours agoparentprevLow-hanging fruit depletion is a problem, but there are ways of dealing with it: - A larger number of people searching for the next Newtonian apple all over the place. - A smaller number of larger groups that pursue a narrow area with intense focus. That’s more like “work” though. - New areas. Especially software. Google, Facebook, Microsoft, Apple were created by tinkerers with the privilege of free time. This process did not yet stop. To your other point, not every idle person will pursue new knowledge. And that's ok. A larger number of idle people will contain a larger number of tinkerers. reply bee_rider 13 hours agoparentprevI do wonder if the problem is not enough, or too much idleness. Isaac Newton is regarded as a genius, but he worked on really basic stuff like calculus and Newtonian mechanics. Of course it was harder, when he and his competitors were inventing it. Sure, we handle more complex stuff now. But modern highschool material used to be really complex. Eventually theory, frameworks, language, and pedagogy, develop around a field that make it look deceptively simple. That’s still incrementalism but incrementation comes from the next generation that grew up in an environment where our discoveries are table stakes. Is it possible that you no longer live in a simple world, because you’ve become an expert, and moved on from ingesting the refined model from the previous generation, to either applying the current unrefined model to the hardest problems it can handle, or to building the model for the next generation? reply galdosdi 8 hours agorootparentTo your point, I took a History of Math class in college, and one week we had to actually do calculus the way Leibniz and Newton would have done it. Some exercise that was easy in our modern notation was actually very painful using period accurate tools. I forget the details, but remember the feeling. reply mycologos 9 hours agoparentprevAnother complication is that boredom was easier to come by in 1932. There are far more things competing with far more sophistication for our attention today. Boredom is underrated! reply deepsun 6 hours agoparentprevMy friend professional musician says that it's a common myth that muse comes to artists randomly. He says the best inspiration comes from invoices, and deadlines definitely help. reply willsoon 5 hours agorootparentThat's true. Ideas comes from total idleness or come from being under total pression. Maybe ideas just... comes. reply hyperthesis 7 hours agoparentprev\"[A]ll the low hanging fruit have been plucked\" from these trees. In a complex, highly multi-dimensional space, yes, there's always a hill to incrementally climb. There are other hills. Incrementalism gets you necessary data; but not necessarily insight. EDIT Of course, following up an idea is work. reply ip26 6 hours agorootparentClimbing this hill is the only way you’ll discover the far more tantalizing peak, hidden in the clouds above it. reply user_7832 13 hours agoparentprevRe: your last paragraph I would say there's a mid-point somewhere between overworked 60-hour-a-week employees burning out and folks doing absolutely nothing productive. The nature and environment of work is important too, if you're tilling a farm there's an upper limit on your output, but with the internet anyone can be made to feel inadequate for not \"hustling\"/\"grinding\" enough. reply eszed 3 hours agorootparentRussell's proposed mid-point is 20 (or maybe 24?) hours of work in a week. That seems sensible to me! reply nextaccountic 4 hours agoparentprev> Google in some ways used to operate like this before the current pivot -- many googlers lived a life of \"resting and vesting\" while wandering about for years looking for a big idea with little pressure to deliver anything. I think Valve works like this too (not sure if they stopped) reply apples_oranges 11 hours agoparentprevMaybe low hanging fruit only hang low in hindsight. There’s a kind of control-certainty associated with some things, we got this, and there are some things where we at least know what we need to work on. Other things are more vague. So we now have more of the first category (in some areas) and this makes the apples appear to be lower than they were and most likely still are. reply ip26 6 hours agoparentprevIt might be a low hanging fruit problem, but just look to all the great ideas that took the tireless efforts of tens of thousands of people to bring to life. Few modern marvels are the product of an solitary, idle daydreamer. To connect with modern dialogue, we’re talking about the “idea guy”. reply jimsimmons 13 hours agoparentprevIt comes down to honesty. Are you idle because you want to be idle or are you just being lazy and not going after the opportunity right before you? Are you not pursuing the low-hanging fruit out of principle or is your ego holding you back from doing humbler things? If you can be honest with such questions then the duality goes away and everything boils down to doing the right thing reply t43562 12 hours agorootparentThis is not about idleness or laziness at all. It's about working for the man versus doing what you want to do. reply jimsimmons 12 hours agorootparentRight, how do you know what you want to do is not sleeping 15 hrs a day versus getting inspired by some idea reply t43562 12 hours agorootparentWhy shouldn't you sleep more if you want to? Why should you feel under pressure to not sleep because you have to add 0s to some billionaire's account - that's exactly what my daily work is about BTW. reply mantas 2 hours agorootparentWho will pay for food on your table and roof above your head? Billionaires pay little tax so that’s not the answer. reply hackable_sand 11 hours agorootparentprevYou can do both at the same time. reply immibis 7 hours agorootparentprevIf you're actually tired enough to sleep for 15 hours a day then you need it and you'll be much more productive in the remaining 9 hours if you do it. If this condition persists more than say a couple of weeks (if you have some severe sleep debt) then see a doctor. If you mean lying in bed awake but not really doing anything, then (a) thinking is doing something, (ii) if you're not thinking and just moping then there's a reason why you are depressed and it's not simply that you are just lazy. Right now, the political climate is a big one for a lot of people. And (3) maybe you just haven't stumbled on the right inspiration yet. It's random. Give it time. Go and do some stuff that seems kinda interesting, in the meantime. I've been in a similar state for several months; I've done some stuff but nothing that exciting. Attending FOSDEM for the first time this year inspired me to join a hackerspace, and to read the code of some open source projects to learn how they work. Some days I stay in bed most of the day or just graze YouTube whether in bed or not (and I hate that after the fact). Some days I want to do something so I do it. reply t43562 14 hours agoparentprevKeep yourself busy on one thing if you want to. If you don't then why are you doing it? reply t43562 12 hours agoprevI was thinking recently that there's enough food to feed everyone but some people cannot afford to buy it. Hence there are food banks in an apparently \"first world\" country like the UK. Just to annoy some Americans...what's with the worlds richest country letting people be homeless? I remember a beautiful park in San Jose that I went to was full of rough sleepers and wondering how that happened in the middle of all that tech wealth. Why? Why are there men who demand 50-whatever billion payouts in a country where not everyone has a place to sleep? It seems the same issue to me. We're not on the earth to \"do great things\" or \"achieve progress\" or any of that crap. That's for people who have some special enthusiasm which the rest of us need not share, or, as Bertrand Russell says, for the elite who want us to labor for them. If one builds one's morality or sense of virtue on doing stuff one is a self-whipping slave and probably ready to become a slave driver for other people too. reply latentcall 10 hours agoparent> Why? Why are there men who demand 50-whatever billion payouts in a country where not everyone has a place to sleep? It is my firm belief the US is only good if you’re rich. If you aren’t rich, well, you slave away to make other people rich. The rich however need the poor to make their food, fix their cars, serve them lattes, deliver their groceries. The only thing that will really effect change is if the poor (in vastly greater numbers) stands up and takes the boot off their necks. reply Aunche 12 hours agoparentprevOne can argue that the California homelessness crisis is caused by a certain type of idleness. NIMBYs refuse to adapt to their changing cities and instead try to force it to remain unchanged. You also have progressives who think that they can simply throw billions of dollars in tech money at homelessness, which is another form of idleness. As a result, you get nonprofits that receive millions of dollars a year that don't accomplish anything. reply blargey 10 hours agoparentprev> Why? If you're actually interested in learning why the homelessness situation in a particular area is how it is, you can pore over the countless (politicized) discussions of the issue to start piecing together the concrete issues and factors that contribute to it, beyond the vague notion of \"rich country has homeless\". Alternatively, take a specific example of one of those \"50-whatever billion payouts\", and your idea for converting it into the right combination of land-near-San-Jose, housing construction, mental health care, actual improvements to mental health, caretakers, government workers, outreach, policing, jobs programs, public infrastructure, etc, etc that's supposed to solve homelessness there. The internet commentariat is bound to have people eager to impolitely point out the issues in that plan. reply anon7725 11 hours agoparentprev> We're not on the earth to \"do great things\" or \"achieve progress\" or any of that crap. That's for people who have some special enthusiasm which the rest of us need not share, or, as Bertrand Russell says, for the elite who want us to labor for them. If one builds one's morality or sense of virtue on doing stuff one is a self-whipping slave and probably ready to become a slave driver for other people too. Thank goodness there are people who do build their morality on \"doing stuff\": surgeons, cancer researchers, etc. Some people innovate, and some (most) maintain. A healthy society needs a spectrum of endeavour. I think our fundamental problem is that many of us do work that does not feel meaningful, and it seems that a lot of meaningful work is not respected and compensated as it should be. reply t43562 1 hour agorootparentA person researching cancer doesn't need to do it because they're desperate to pay off the mortgage. They can do it because it's interesting, or a personal challenge. There are a lot of people with advanced degrees now - we don't need to put enormous pressure on a few of them. reply silverquiet 11 hours agorootparentprevI agree strongly with the last two paragraphs. I think the surgeon/medical example is nice (though not they certainly don’t work for free), but a lot of the ambitious people I’ve known all sought middle management or higher in some big company, which hardly seems like a good application of human potential. reply chadcmulligan 7 hours agoparentprevIsn't it a corollary of lassez faire capitalism? - there has to be a winner and a loser. If there's no social security net, then abject poverty is the ground state. reply BirAdam 6 hours agorootparentNo. Poverty is the default no matter what. From there, there are really only two methods of human interaction. The economic or the political. The economic means are voluntary and the political means are force. So far, all attempts at creating a third means either result in tyranny or in societal collapse. reply chadcmulligan 5 hours agorootparentThe europeans seem to have succeeded in the middle state would you say? they have sacrificed some competitiveness for a more stable social framework. Not sure I agree that poverty is the default state no matter what. reply mantas 2 hours agorootparentThe european way right now is to be a freerider. Get military coverage from US and sell assets to China/Russia/etc to get some money for now. It will be interesting times once easy money dry up and US tells us to pay for our own defense. reply immibis 7 hours agoparentprevThe USA is the richest country precisely because many people have been made poor in a zero-sum way. reply BirAdam 6 hours agorootparentThe USA became the richest country despite many people having been made wealthy via corruption, and despite the establishment of a nearly all-powerful oligarchy. reply r00fus 6 hours agorootparentprevStarting with the Native Americans. reply pbj1968 12 hours agoparentprevBecause some people want to sleep outside. Are you proposing we round them up into camps against their will? Very European of you. reply t43562 12 hours agorootparentI'm from Zimbabwe - so insults like \"very European\" aren't your get out of jail free card. The third world has an excuse for its problems - where's yours? reply mft_ 11 hours agorootparentprevYou seem to be suggesting that homelessness is simply an expression of people's wish to sleep al fresco? reply drsopp 15 hours agoprevIn 1998 I found some articles online by Russell and put them here: http://trondal.com/russell/russell.html reply blueridge 5 hours agoparentAmazing, thank you! reply gavmor 14 hours agoparentprevExcellent grist for a Saturday afternoon RAG. Thanks! reply CobaltFire 14 hours agoprevI, through a combination of fortune and misfortune, was able to retire at 40. I am trying to figure out how to employ my hands in ways I enjoy and that may be of net benefit to society at this time, and this essay touches on a lot of the concepts I've been thinking over. I'm actually surprised I haven't run across it; I should probably start my search by reading a little deeper into some philosophy. reply purple-leafy 2 hours agoparentYou could try first doing that which is a net benefit to yourself, and your family (non-monetary) then extend that onto your immediate community Congrats on retirement :) reply tracerbulletx 13 hours agoprevI think every discussion about this should focus on what exactly are the things that need to happen that no one would do if not coerced by either force or threat of destitution. How do we make sure those things get done and done well is a constraint of any economic changes we make to make things more fair. Also keeping in mind some of those things need to organize 1000s of people, how do you mine lithium in one place, and ship it across the world to 200 other places without financial incentives, how would you fuel the ships, load and unload them, track their location, repair and maintain them. I think it's pretty obvious things could be MUCH better and our current solution is very sub-optimal, but also that the problem being solved is very complex and the solution we have mostly works, but also there is a lot of work that is not particularly fulfilling or attractive to do, and especially not going to self incentivize anyone to do it well. Also it's clear markets are the best way to signal demand. So you really want to keep all of that, while getting rid of some of the biggest exploitative extractive inefficiencies in the current system. reply gorbo42 13 hours agoparentIf everyone had to perform an equal portion of the labor that's needed but nobody wants to do, the need for that labor would be minimized. If Zuckerberg or Musk (and everyone else) had to pick up trash one day a month they'd spend their capital solving those problems (automating them away) instead of whatever the hell they think is so important right now. reply kubb 13 hours agoprev> In America men often work long hours even when they are well off; such men, naturally, are indignant at the idea of leisure for wage-earners, except as the grim punishment of unemployment; in fact, they dislike leisure even for their sons. There will be hundreds of people here that fit this perfectly, or am I wrong about the demographics of HN? reply resource_waste 13 hours agoparent\"The strong do what they can, the weak suffer as they must\" There is a reason we work constantly and always. Nature is brutal, and if I am not among the best, I'm going to experience pain. Or at least that is what my 'trauma' pushes me to do. I'm literally afraid not to be a 1%er. And if you lived my life, have my experiences, you'd probably come to the same conclusion. reply dambi0 12 hours agorootparentWhat are you scared of? Why 1%? Why not the 0.5% or the 2%? It all seems rather arbitrary. reply j7ake 10 hours agorootparentOP didn’t mean literally 1 percent. They meant as close to number 1 as possible, and never stopping even if they are number one. They are afraid that they would be discarded by society if they fall too far behind. reply matwood 9 hours agorootparentI don't care about being discarded, but I do care about financial security. I grew up with near nothing, and I'll fight and claw to never end up there again. reply sameoldtune 13 hours agorootparentprevTrading pain for fear is one approach I guess. I don’t know a lot of 1%ers without coin sickness. reply programjames 4 hours agorootparentprev\"The strong do what they can and the weak suffer what they must.\" Source: Thuc. 5.89, justification for Athen's invasion of Melos, 416 BC. The full quote is even more brutal: \"We shall not trouble you with specious pretences—either of how we have a right to our empire because we overthrew the Mede, or are now attacking you because of wrong that you have done us—and make a long speech which would not be believed; and in return we hope that you, instead of thinking to influence us by saying that you did not join the Lacedaemonians, although their colonists, or that you have done us no wrong, will aim at what is feasible, holding in view the real sentiments of us both; since you know as well as we do that right, as the world goes, is only in question between equals in power, while the strong do what they can and the weak suffer what they must.\" reply badpun 2 hours agorootparentprev> Nature is brutal, and if I am not among the best, I'm going to experience pain. Working so hard to get into the 1% is also very painful. Looks like, in your philosophy, pain is inevitable, and you only get to choose which form of it you take. reply TacticalCoder 11 hours agoparentprev> There will be hundreds of people here that fit this perfectly, or am I wrong about the demographics of HN? Why would there be people on HN not wanting people earning less than them to have leisure? People are free to bicycle, hike, rock-climb, and do whatever the fuck they want. The only thing I don't want is wage-earner longing for my savings through taxes so they can buy Luis Vitton man purses. And I want to be free to buy whatever the fuck I want with my hard-earned money: be it luxury cars or audiophile (audiofool, I don't care) audio gear. But I don't give a crap what others do. reply bogdart 11 hours agoprevI understand everything and maybe even agree with the main thesis. But the positive example that he gives is Soviet Russia, which that time (1932-1933) went through massive famine in literally the most fertile land in the world which was caused by completely artificial reasons. And Soviet workers were forced to work more than ever without any payment. So the solutions in the article cannot be taken seriously. reply Fezzik 14 hours agoprevOne of my favorite bands, TTNG, has a wicked good song inspired by this essay. If you like math-rock at all I reckon you’d enjoy it: https://m.youtube.com/watch?v=dCKXg2scb_s&pp=ygUaaW4gcHJhaXN... reply itchyjunk 13 hours agoparentI have been bingeing on ttng for a year now. I thought of the song when I saw the title and didn't realize maybe it was inspired from the essay. Saw the mention of TTNG and it got me excited enough to log in and comment after a long time, hurrah! reply Fezzik 50 minutes agorootparentI am thrilled to cross-paths with another TTNG fan online; I know a grand total of 1 fan outside of the internet. They are criminally underrated and I am praying that despite their years-long-silence we’ll get some new material and another tour in the near future. reply mitchbob 13 hours agoprevPrevious discussion: https://news.ycombinator.com/item?id=6513765 (120 comments) reply dang 12 hours agoparentThanks! Macroexpanded: In Praise of Idleness (1932) - https://news.ycombinator.com/item?id=29338666 - Nov 2021 (170 comments) In Praise of Idleness (1932) - https://news.ycombinator.com/item?id=21509144 - Nov 2019 (82 comments) In Praise of Idleness, by Bertrand Russell - https://news.ycombinator.com/item?id=10876730 - Jan 2016 (25 comments) In Praise of Idleness (1932) - https://news.ycombinator.com/item?id=10310846 - Oct 2015 (24 comments) In Praise of Idleness by Bertrand Russell (1932) [pdf] - https://news.ycombinator.com/item?id=9015092 - Feb 2015 (50 comments) Bertrand Russell: In Praise of Idleness (1932) - https://news.ycombinator.com/item?id=6513765 - Oct 2013 (120 comments) In Praise of Idleness - Bertrand Russell - https://news.ycombinator.com/item?id=1396167 - June 2010 (5 comments) In Praise of Idleness - https://news.ycombinator.com/item?id=1187681 - March 2010 (4 comments) In Praise of Idleness - Bertrand Russell - https://news.ycombinator.com/item?id=85325 - Dec 2007 (1 comment) reply cko 15 hours agoprevI like this essay but I've found it hard to be idle. I jumped aboard the FIRE train so I could coast the rest of my life, and quit my job once when I was 31 and then when I was 33 after working for a year. Now I'm working the same dead end job again and I don't mind it. When I'm not working I'm consuming YouTube, HN and reddit. Not sure how to love idleness again. I used to be able to sit still for an hour at a time (meditation). reply lolinder 14 hours agoparentYou seem to have really misunderstood the essay, then. He uses the word \"idleness\" to be provocative, but he's not actually saying that people find fulfillment in sitting and staring at the wall. He explicitly calls for efforts to be made to help everyone to learn enough about a wide enough variety of topics that they can choose their own interests to pursue in their leisure time. From the essay: > It will be said that, while a little leisure is pleasant, men would not know how to fill their days if they had only four hours of work out of the twenty-four. In so far as this is true in the modern world, it is a condemnation of our civilization; it would not have been true at any earlier period. There was formerly a capacity for light-heartedness and play which has been to some extent inhibited by the cult of efficiency. > ... > When I suggest that working hours should be reduced to four, I am not meaning to imply that all the remaining time should necessarily be spent in pure frivolity. I mean that four hours' work a day should entitle a man to the necessities and elementary comforts of life, and that the rest of his time should be his to use as he might see fit. It is an essential part of any such social system that education should be carried further than it usually is at present, and should aim, in part, at providing tastes which would enable a man to use leisure intelligently. I am not thinking mainly of the sort of things that would be considered 'highbrow'. Peasant dances have died out except in remote rural areas, but the impulses which caused them to be cultivated must still exist in human nature. The pleasures of urban populations have become mainly passive: seeing cinemas, watching football matches, listening to the radio, and so on. This results from the fact that their active energies are fully taken up with work; if they had more leisure, they would again enjoy pleasures in which they took an active part. reply cko 7 hours agorootparentThis is wonderful but I guess what I'm saying is: whether I'm jobbing 24 hours a week like I do now, or 55 hours like I used to, or zero hours, I spend all the remaining time in pure frivolity (what he calls passive pleasures). Modern civilization's cult of efficiency combined with the ease of frivolous entertainment must have beaten the leisure out of me. reply paulpauper 14 hours agorootparentprevPeople seldom read beyond the title before commenting reply drak0n1c 14 hours agoparentprevIn my experience, having kids has had the dual benefits of making my days purposeful and fulfilling, and also more appreciative of idle time. The mind does not do well with the extremes of unlimited idleness or unlimited work. Turns out the rhythm of child-raising can fit the perfect middle ground between work and idleness, and has the advantage of creating a more meaningful cumulative product than either realm. That balance of time is especially true if you're financially independent or have family willing to help during the first year or two of a new baby. reply dyauspitr 13 hours agoparentprevIdleness is not just sitting on your couch watching TV. It’s basically the freedom to pursue whatever’s interesting you in the moment. reply theropost 13 hours agoparentprevSame, I don't think we can lump all different personas and natures into one bucket. Some people thrive, and enjoy doing, building, learning, and not staying in one place. Others enjoy sitting, relaxing, doing little, introspection, etc. It really depends on the individual, each has their own areas that give them meaning and joy. reply 082349872349872 11 hours agorootparent> each has their own areas that give them meaning and joy on that note, see https://news.ycombinator.com/item?id=40256243 reply hnthrowaway0328 7 hours agoprevIndeed I can never enjoy Mediterranean sunshine idleness. To me the sin is not to get the most out of life and especially not devoted most of life to a passion. To me the sin is to stop \"evolving\". Getting sunshine is mostly for health benefit. I keep asking myself: Did I learn more about the universe? Did I improve myself today? Of course most of the time the answer is no, but I still try to get some yes occasionally. A bit of idleness for sure is an antidote for burnout, but the Mediterranean sunshine idleness is way way too much for my taste. reply malingo 12 hours agoprevReminds me a little of hammock-driven development [1] > the background mind is good at synthesizing things. It's good about strategy [1] https://github.com/matthiasn/talk-transcripts/blob/master/Hi... reply kzz102 13 hours agoprevIf one works in a field where there is already an issue of abundance, which is (nowadays) basically any field that produce information, it's better for the society to produce less, but higher quality, more meaningful work. Of course, it is hard to do so because the incentives are against it. reply tim333 10 hours agoprevFun fact - if you search your file system for idleness.txt you'll probably find it includes quite a few copies as npm/node likes including it in its packages. I seem to have 30 on my macbook. reply quercusa 13 hours agoprevIf the ordinary wage-earner worked four hours a day, there would be enough for everybody and no unemployment -- assuming a certain very moderate amount of sensible organization. True sensible organization has never been tried! reply BirAdam 6 hours agoparentDid you mean this ironically, cuz uh… it’s been tried many times and often led to death on very large scales. reply 48864w6ui 12 hours agoparentprevIt's been found difficult, and left untried reply leetrout 15 hours agoprevThis made me think of this passage from \"Happy to Work Here: understanding and improving the culture at work\" which is on the web[0]: The business of busyness is a contradiction in terms. The more politics forces you to look busy, the less time there is for real business. An old joke to set the tone for this section: A group of excited young curates crashes into the office of the Archbishop at St. Patrick’s cathedral. “Your Eminence!” one of them cries. “Jesus Christ has just appeared in lower Manhattan!” “What?!” “He walked across the water and came ashore in Battery Park.” “Oh my.” “And now he’s headed up Fifth Avenue toward St. Patrick’s. He could be here any minute!” “I see.” “So, tell us, Eminence, what do we do?” The Archbishop thinks that over for a moment and finally says, “Look busy.” An apparent busyness can be a sign of deep and very professional engagement in an important task, vital to the long-term interests of the organization. Or it might be a sign of something else entirely. In a fearful organization it most likely implies a worry that it’s downright unsafe to seem unbusy. The unspoken rule that governs people in this case is: Look busy. Of course, the fear itself has already done damage to the organization’s culture. But obedience to the unspoken rule makes the matter worse. The consequences of everyone trying to look busy include: • No time for reflection • No time to confer with colleagues (which might be interpreted as “chatting”) • No time for lunch • No time for training • Nobody willing to be away from his/her desk • No off-site activities • A general uneasiness with activities that might seem “passive” like reading, and research. Most of the things that the rule makes impossible are culture positive. That is, they help the culture heal and improve itself. The more you find yourself and your co-workers compelled to look busy, the surer you can be that your working culture is damaged. [0] https://systemsguild.eu/ reply fastasucan 15 hours agoparentThis is parts of what I enjoy by working as an academic. I can get into a comfortable position with a book, take a walk or go to the gym during my workday without any judgement. I find that it is very helpful to be able to do something physical while thinking about a hard problem, and conversely being able to leave my desk and do something else for a short while if I am struggling to get into my work. I am a lot more productive than when I was a consultant and was busy trying to look busy. reply rlhf 8 hours agoprevThat's true, maybe it's the reason why short videos burst into exploding. reply lo_zamoyski 12 hours agoprevI would recommend Josef Pieper’s “Leisure: The Basis of Leisure” [0]. Leisure is not recreation. Indeed, the word “school” is derived from the Greek word for leisure, and the state of having to work was defined in terms of the lack of leisure, a negation of leisure. The leisure/work distinction is also reflected in the classical division of the liberal arts and the servile arts. (The liberal arts were what free men pursued, for the sake of wisdom, virtue, etc. The servile arts were for the sake of practical ends.) Work was understood as something you did for the sake of leisure (but again, not leisure as we understand it today which is at best recreation), not as work for work’s sake. [0] https://www.amazon.com/Leisure-Basis-Culture-Josef-Pieper/dp... reply user_7832 13 hours agoprevA point that one of the comments here addressed but I think is worth re-emphasising - Russell isn't talking about not working, but rather to not work in a way that's not productive - which often occurs if you need to be in office till 5pm because I told you so!\" reply lurking15 10 hours agoprevI read Intellectuals by Paul Johnson and it occurs to me that even generously doubting the book, Bertrand Russell comes off as a real scumbag, complete womanizer, generally untethered person and despite being prodigiously intelligent, simply unwise. Also defending idleness seems like the sort of opinion the well-off espouse, Russell carried around accounting of every cent he made, just to cheer him up whenever he felt down. reply azureumbra 10 hours agoprevThanks for the awesome read! Sharing it with my UBI group now reply readthenotes1 10 hours agoprevHe sure put a lot of work into this reply alexashka 7 hours agoprevA shorter version with much greater bite and wit by Nietzsche published in 1882: 329 Leisure and idleness. - There is something of the American lndian, something of the savagery peculiar to the Indian blood, in the way the Americans strive for gold; and their breathless haste in working - the true vice of the new world - is already starting to spread to old Europe, making it savage and covering it with a most odd mindlessness. Already one is ashamed of keeping still; long reflection almost gives people a bad conscience. One thinks with a watch in hand, as one eats lunch with an eye on the financial pages - one lives like someone who might always 'miss out on something'. 'Rather do anything than nothing' - even this principle is a cord to strangle all culture and all higher taste. Just as all forms are visibly being destroyed by the haste of the workers, so, too, is the feeling for form itself, the ear and eye for the melody of movements. The proof of this lies in the crude obviousness which is universally demanded in all situations in which people want for once to be honest with others - in their relations with friends, women, relatives, children, teachers, students, leaders, and princes: one no longer has time and energy for ceremony, for civility with detours, for esprit in conversation, and in general for any otium 2 ° For life in a hunt for profit constantly forces people to expend their spirit to the point of exhaustion in continual pretence or out-smarting or forestalling others: the true virtue today is doing something in less time than someone else. And thus hours in which honesty is allowed are rare; during them, however, one is tired and wants not only to 'let oneself go' but also to lay oneself down and stretch oneself out unceremoniously to one's full length and breadth. This is the way people now write !etters, the style and spirit of which will always be the true 'sign of the times'. If sociability and the arts still offer any delight, it is the kind of delight that overworked slaves make for themselves. How frugal our educated and uneducated have become concerning 'joy'! How they are becoming increasingly suspicious of all joy! More and more, work gets all good conscience on its side; the desire for joy already calls itself a 'need to recuperate' and is starting to be ashamed of itself. 'One owes it to one's health' - that is what one says when caught on an excursion in the countryside. Soon we may well reach the point where one can't give in to the desire for a vita contemplativa21 (that is, taking a walk with ideas and friends) without self-contempt and a bad conscience. Well, formerly it was the other way around: work was afflicted with a bad conscience. A person of good family concealed the faet that he worked if need compelled him to work. The slave worked under the pressure of the feeling that he was doing something contemptible: 'doing' was itself contemptible. 'Nobility and honour are attached solely to otium and bellum'22 - that was the ancient prejudice! The Gay Science Bertrand Russell did a great disservice to philosophy by reducing great minds and talent to what he understood of them. Philosophy departments haven't recovered since and now we seem to be in full swing going back to... fucking religion for ethics and morality spearheaded by the likes of Jordan Peterson. reply dang 12 hours agoprevWe've changed the URL from https://libcom.org/article/praise-idleness-bertrand-russell to the original source. Submitters: \"Please submit the original source. If a post reports on something found on another site, submit the latter.\" - https://news.ycombinator.com/newsguidelines.html reply firtoz 15 hours agoprev [–] The essay comes up every now and then in HN, 5 pages of it in search. I'm unsure if it reached the status of most shared essay yet. Does anyone know? reply pvg 14 hours agoparent [–] Looks about the same as other evergreens like Politics and the English Language and The Story of Mel to me. Greenest (i.e. earliest) evergreen is almost certainly Story of Mel, ever-est (most posted), I'm not sure but I want to say I've seen bigger ones than either of those. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bertrand Russell proposes a shift in societal values from glorifying work to valuing leisure, suggesting modern technology can maintain productivity with fewer working hours.",
      "Russell criticizes the unequal distribution of wealth and power and questions the historical foundations of the obligation to work, advocating for a society where all can enjoy leisure and pursue their interests for a happier life.",
      "His argument aims for a society where individuals have more leisure time, promoting a more content and tranquil way of living."
    ],
    "commentSummary": [
      "Bertrand Russell's essay \"In Praise of Idleness\" emphasizes the significance of leisure time in nurturing creativity and innovation.",
      "The essay discusses Universal Basic Income, the diminishing easy opportunities in society, and the interplay of idleness with creativity, productivity, and wealth inequality.",
      "It explores various viewpoints on work, leisure, wealth, success, societal norms, advocating for a harmonious balance between work and idleness amidst historical foundations and evolving societal attitudes."
    ],
    "points": 228,
    "commentCount": 89,
    "retryCount": 0,
    "time": 1714830718
  },
  {
    "id": 40259196,
    "title": "ADA Lawsuit Raises Concerns Over Corporate Ties",
    "originLink": "https://www.theguardian.com/global/commentisfree/2024/may/02/american-diabetes-association-lawsuit",
    "originBody": "View image in fullscreen ‘Over 100 million Americans have diabetes or prediabetes, and 100,000 die from the condition annually.’ Photograph: Andreas Nageli/Alamy Death by diabetes: America's preventable epidemic Diabetes Does the American Diabetes Association work for patients or companies? A lawsuit dared to ask The ADA just settled an explosive legal case accusing the organization of betraying people with diabetes Neil Barsky Thu 2 May 2024 06.01 EDT Share A cloak of silence has descended over the recent whistleblower lawsuit claiming that the American Diabetes Association, or ADA, accepted corporate money in return for recommending recipes that threatened the health of people with diabetes. Study: ‘gamechanger’ diabetes drugs cost up to 400 times more than needed Read more Elizabeth Hanna, the ex-ADA chief nutritionist who alleged that her former employer fired her over her refusal to endorse Splenda-filled salads, has quietly settled her case. In a statement to the Guardian, Hanna’s attorney, Lauren Davis, said that “the matter has been resolved”. No details were provided either by Davis or by a spokesperson for the ADA, which declined to comment. For Hanna, accepting a settlement from the ADA was no doubt a simpler and less stressful and risky alternative to a trial, but for me and the country’s other 38 million people with diabetes, it is a letdown. What a great opportunity a trial would have been to expose the inner workings of the ADA, the patient advocacy organization up to its eyeballs in big-business funding. I recently wrote about Hanna’s lawsuit as part of our series Death by Diabetes: America’s Preventable Epidemic. My view is that diabetes is an urgent national scandal. Over 100 million Americans have diabetes or prediabetes, and 100,000 die from the condition annually. In addition, every year hundreds of thousands of people with diabetes have limbs amputated or suffer blindness or kidney disease. Diabetes costs our country $400bn annually to treat. And although type-2 diabetes is often reversible through a low-carbohydrate diet, the ADA and the pharmaceutical industry don’t seem very interested in acknowledging that. Instead, they promote a laundry list of corporate deals and pharmaceutical treatments that have failed to stem the disease’s lethal and expensive impact on American life. Hanna’s complaint, filed last year in a New Jersey court, alleged a litany of wrongdoings by one of the country’s most powerful patient advocacy organizations. Hanna said that ADA higher-ups pressured her to approve recipes that included generous helpings of the artificial sweetener Splenda, despite research published in the ADA’s own scientific journal finding that artificial sweeteners may raise consumers’ risk of type-2 diabetes. Hanna's account depicts the ADA as a fund-raising machine, anxious to please its corporate overlords at the expense of people with diabetes Hanna further alleged that the pressure was the result of a $1m contribution made to the ADA by Heartland Food Products Group. Finally, she said the ADA was a revolving door of nutrition directors, with seven leaders coming and going over the past four years – largely because, she asserted, her predecessors “were either terminated by the ADA when they refused to comply with the ADA’s unethical and unlawful practices or were constructively terminated by the ADA by the abusive and hostile work environment they faced for refusing to comply”. The ADA rejects the allegations in the lawsuit. The Heartland Food Products Group, which was not named in the lawsuit, has also said that it rejects any allegations of wrongdoing, and indicated it would continue working with the ADA. “Heartland will continue to support the ADA and honorably provide recipes and educational information to help people successfully reduce sugar levels and live happier and healthier lives,” Heartland said in a statement. By settling, the ADA manages to avoid the discovery process, and the potentially embarrassing revelations that might have come with it. Hanna’s lawyers appear to have grasped the implications of her lawsuit, and originally suggested her quest could take on heroic dimensions. “Hanna’s story could be the next movie that Americans need to see to understand what is going on behind closed doors between major for-profit corporations and the not-for-profit health sector,” they wrote in their legal complaint. Although she chose to settle with the ADA, in reality Hanna has already performed a giant public service. Her legal complaint is a public document open for the ADA’s board of directors, clinicians across the US and the world, and members of Congress to read. Her meticulous account depicts the world’s most important diabetes patient advocacy organization as a cynical fund-raising machine, anxious to please its corporate overlords at the expense of the millions of people with diabetes it is supposed to be trying to help. “The defendant’s conduct shows that they were party to a scheme to defraud the American people by approving and endorsing recipes submitted by Splenda to be lauded by the ADA as a healthy choice for people with diabetes, when the ADA knew that those recipes were contrary to the ADA’s guidelines and well-established and emerging scientific principles,” the complaint reads. In case you’re curious, the ADA and Splenda appear to be still at it. As I write this, the ADA’s Diabetes Food Hub web page still features no fewer than 203 recipes – some marked “sponsored”, some not – that include Splenda, whose parent company’s $1m contribution has brought to light the utter insanity of our diabetes epidemic. Isn’t it long overdue for the ADA’s board to act? Neil Barsky, a former Wall Street Journal reporter and investment manager, is the founder of the Marshall Project Explore more on these topics Diabetes Death by diabetes: America's preventable epidemic Health Health policy features Share Reuse this content",
    "commentLink": "https://news.ycombinator.com/item?id=40259196",
    "commentBody": "Does the American Diabetes Association work for patients or companies? (theguardian.com)202 points by rokkitmensch 16 hours agohidepastfavorite182 comments guerby 15 hours agoDavid Unwin is a real hero, things are starting to move hopefully for diabetics Recommanded readings https://nutrition.bmj.com/content/early/2020/11/02/bmjnph-20... https://nutrition.bmj.com/content/early/2023/01/02/bmjnph-20... https://nutrition.bmj.com/content/early/2023/12/14/bmjnph-20... Those papers are the most read papers of BMJ nutrition whole history, and this is the top journal of the field reply stephenbez 4 hours agoparentI’m curious about this as I have loved ones with T2D. What makes this work groundbreaking? It seems like one doctor talked about how at his practice he started telling people to go low carb and noticed they were improving. But his data doesn’t compare the 27% of his patients who opted in to the low carb diet to those who chose another option. The low carb diet has been around for a while? Has there not been a RCT looking at how it works for treating diabetes? He makes an argument on why RCTs aren’t so good that is unconvincing to me. > However, when it comes to actual clinical implementation, RCTs have a problem. So many variables have been removed that the studies no longer represent real people leading ordinary lives outside of a tightly controlled trial. For example, in our National Health Service (NHS) clinic, very few patients with hypertension do not have a weight problem. Many of them are also on drugs for joint pain, reflux or depression. This is why the results seen in RCTs often do not roll out into real-world, clinical practice. For this to occur we need interventions that have a high degree of ‘external validity’—approaches for that better represent the ‘ordinary’ people who populate our clinics. Away from the carefully controlled conditions of clinical trials, results can be very different in the messy, complex world of everyday general practice. I noticed that he mentioned a RCT on the low carb diet showing good results and read it, but it seems to be p-hacked. They don’t even mention the mean change in weight between the two groups, but only the percent that achieved a 5 or 10% reduction in weight at some point during the study. The also said that 55% of patients lost weight and 45% gained weight. https://nutrition.bmj.com/content/bmjnph/6/2/326.full.pdf reply UncleSlacky 11 hours agoparentprevAlso check out the Newcastle Protocol by Prof. Roy Taylor, now adopted by the NHS: https://www.ncl.ac.uk/magres/research/diabetes/reversal/#pub... reply david-gpu 7 hours agorootparentConsuming fewer calories, especially fewer carbohydrates, will help lower your blood insulin, which is the necessary condition that allows adipose tissue to finally release its excess stored energy. Even though calorie restriction works great in lab rats as the researcher can control what the rat eats and the rat can't do anything about it regardless of how hungry it becomes, it's not that simple with humans, particularly after a year or two, once the initial willpower is lost. So, rather than fixating on lowering calories without further considerations, a more effective approach is maximizing the satiety obtained per calorie consumed. It's harder than it sounds! One might be tempted to simply consume large quantities of leafy vegetables, but that will only lead to them learning how hunger is a multidimensional experience. What do I mean by that? You can feel your stomach stretched out and full, and still feel hungry, because satiety isn't only triggered by how far your stomach has stretched, nor by the precise number of calories you have consumed. In practice, in order to maximize satiety one must strike a balance between the volume of the food consumed, its protein content, and the amount of insulin-raising carbohydrates it contains. That's the reason so many people find success in low-carbohydrate diets, which emphasize these three satiety factors rather than the excessively simplistic calorie count approach. reply Fire-Dragon-DoL 6 hours agorootparentI'm a master of hunger, I'm always hungry. With the low calories diet, it's like you said: alk veggies won't make you feel full, it comes from multiple sides. The core problem is when you are on very low calories, sometimes it's hard to find that food you are needing. Sometimes it's a glass of milk, sometimes an apple, sometimes something with fat reply theendisney 7 hours agorootparentprevFrom what i understand the amount you eat might not be all that relevant. The differnce between thin people who eat huge amounts and fat people who eat almost nothing is that the former eat lots of fat and protine in a kind of frenzy with long breaks between meals, the later eat primarly carbs and sugar very regularly. reply voisin 12 hours agoparentprevThe insight being low carb can send T2 diabetes into remission? reply throwawaymaths 10 hours agorootparentIsn't it known that 800 kcal diet (brutal) will reverse T2D? Iirc, 90% remission, 12 patient study, nejm article reply lambdaba 6 hours agorootparentYeah but low carb is sustainable. High carb diets are a misuse of human biology. reply not2b 15 hours agoprevMy diabetic wife has long been horrified at the dietary recommendations coming from the ADA, they are worse than useless. I don't blame the whistleblower for settling but it's a shame those people won't take consequences for actively harming people in exchange for corporate contributions. Their nonprofit status doesn't keep them from paying the top officers fat salaries out of those donations. reply user_7832 13 hours agoprev> In case you’re curious, the ADA and Splenda appear to be still at it. As I write this, the ADA’s Diabetes Food Hub web page still features no fewer than 203 recipes – some marked “sponsored”, some not – that include Splenda, whose parent company’s $1m contribution has brought to light the utter insanity of our diabetes epidemic. If you call yourself the \"American Diabetes Association\", (why) aren't there regulations against such stuff? For eg in some countries you cannot use the name of the city/state/country unless you're a government entity. reply johnfernow 13 hours agoparentI agree, it seems extremely misleading, especially for medical organizations. The US government doesn't make consumer clothes, so I don't think anyone's too confused with American Eagle Outfitters not being government-ran, but the US does have several government-ran Health and Human Services divisions and U.S. Public Health Service agencies (e.g. the Centers for Disease Control and Prevention (CDC), the National Institutes of Health (NIH), etc.) I wonder what percent of Americans think that the American Diabetes Association is a government-ran organization. My guess is that it's quite high. You'd have to design a poll correctly (e.g. \"From the following list, select which ones are government-ran organizations\") to not give the answer away, but I imagine it's a high percentage. reply cjensen 7 hours agoparentprevWhy can they call it the \"American Diabetes Association\" is a pretty easy question. The First Amendment. Part of the First is that the government cannot ban you from using a word unless there is a very good reason do so. \"American\" is a word that is not associated exclusively with the government, nor was it ever used that way historically. In fact, the word \"American\" predates the US Government. Since the word does not now, nor has it ever, implied governmental affiliation, it would be illegal for the US Government to attempt to obtain a monopoly on the word by banning others from using it. reply newshackr 4 hours agorootparentIt doesn't seem much different from trademarks, where you can't use a word if it causes confusion reply deegles 12 hours agoparentprevmaybe someone should start the American Anti-Diabetes Association. reply crazygringo 12 hours agoparentprevI've been baffled by this for the longest time. Why does \"Bank of America\" get to call itself that? Growing up there was a \"USA Federal\" credit union [1] too, with a stylized American flag logo. Similarly there's a historic \"New Yorker\" hotel in New York City, and the totally separate \"New Yorker\" magazine, neither of which have any official affiliation with the city. The short answer is that no, there don't seem to be regulations against it. Why any business seems to be able to take the name of a country or state or city, and therefore gain an aura of authenticity or approval, and then prevent any other from doing the same (since the name got taken, you can't have two Bank of Americas) -- I've never understood why the government allows these things. [1] https://en.wikipedia.org/wiki/USA_Federal_Credit_Union reply verisimi 12 hours agorootparentEven the federal reserve is privately owned though. reply ianburrell 10 hours agorootparentThe Federal Reserve is not privately owned. Parts of the Federal Reserve System are privately owned. The most important part, the Federal Reserve Board, is an independent federal agency like the FAA and SEC. The board members are appointed by President and confirmed by Congress. The Federal Open Market Committee controls monetary policy. It is composed of seven Board members and five regional bank presidents. The regional Federal Reserve Banks implemented monetary policy. They are privately owned by member banks, but legally considered both government and private. The important thing is that the Board is at the top and controls the lower levels. reply throwawaymaths 10 hours agorootparentprevSort of. It's actually worse. It gets to claim privately owned when being government is inconvenient; it gets to claim delegated authority from government when that's convenient. reply nradov 12 hours agorootparentprevAmerica is a pair of continents, not the name of a country. The US federal government doesn't hold a trademark. The notion that companies shouldn't be allowed to name themselves after geographic features or political regions is absurd. reply crazygringo 12 hours agorootparentIn the US, if you ask 100 people what \"America\" is, they'll tell you it's the name of the country they live in. And since language is defined by use, yes -- America is the name of a country. The shortened version, of course, just like \"Mexico\" is the shortened version of the \"United Mexican States\". Surely you're not going to claim that Mexico isn't the name of a country either? And no, \"America\" is not a pair of continents in standard usage -- that's \"the Americas\" you're probably thinking of (plural and with the definite article). Which is a rare term to come across, essentially unused in regular conversation, reserved for some highly specific contexts. And I'm saying that companies being able to name themselves after political regions is absurd. How is that fair? I don't get to name my company \"Google Bank\" (and make it seem falsely associated with Google), so why should I get to name my company \"New York Bank\" (and make it seem falsely associated with New York)? reply bee_rider 10 hours agorootparentAsking people in the US seems like a sort of skewed sample; surely everyone in the Americas at least should get a vote on whether or not they want to distinguish more clearly between the continents and this one large country some of us happen to live in. I do agree though that it is not so ambiguous really in the case of America, the Americas, North America, and South America—because there isn’t a singular “America” continent to refer to (unless we want to dip into the nightmare that is counting and naming the continents; Europe is a subcontinent, I’ll happily die on that hill). But “American” seems a little ambiguous, what else should we call somebody from the Americas, if we want to refer to them in that context? Nobody would say “An Americasian.” reply devilbunny 9 hours agorootparentThis is one of those places where in some languages - Spanish, of course, and I would assume Portuguese - \"americano\" (or equivalent) basically means \"Western Hemisphere\". But in others, like English, \"America\" means the United States of America, just like \"Mexico\" means the United Mexican States. If you're using English, \"North American\", \"Central American\", and \"South American\" cover most of the situations you're talking about. \"American\" without further qualification in English means the USA. And it is justifiable: there is exactly one country on the planet with \"America\" in its name. reply mthoms 10 hours agorootparentprevWe Canadians use “America” and “The States” interchangeably to describe the United States and we’re not the least bit bothered by it. If we want to refer to the continent we’ll just say North America. reply BobbyTables2 9 hours agorootparentprevI still never understood how we appropriated two whole continents. If my name were “Bob of Idaho”, nobody would think Idaho meant only me. They would simply know I lived there. Yes the United States are in the Americas, but it isn’t the entirety of them! We have no more right to call ourselves “American” than Chileans do! reply sgift 9 hours agorootparentFrom my outsider perspective it's by being by far the most important country on the continent in terms of size, influence, economy and so on. Also, it's (afaik? cannot think of a counter-example) the only country which has America in its name and usually people talk about countries, not continents. reply vundercind 9 hours agorootparentprevIt helps that a demonym for “person who lives in any part of the Americas” is something we almost never want to use. If “American” only meant that, we’d practically never use it. So in American English, at least, using it the way we do causes no real trouble. reply jodrellblank 11 hours agorootparentprevWhy would it be “falsely” associated if you actually are in New York? What if you change your name to John New York then named the company after yourself, is that bad? reply Dylan16807 10 hours agorootparent> Why would it be “falsely” associated if you actually are in New York? Because it equally associates it with the other millions of people there. > What if you change your name to John New York then named the company after yourself, is that bad? Yes. And it's not like you can change your name to John Disney and then start branding all sorts of things Disney. reply shkkmo 10 hours agorootparentprevTravel to some other american countries and ask them what they think of the appropriation of the word. Not everyone is a fan. I don't really get how you think a company using the name of the country is wrong but somehow a country appropriating the name of the continent is just fine? reply DiggyJohnson 9 hours agorootparentI think you’re getting offended on behalf of a cohort of people that do not exist. Let them speak for themselves if they do. reply shkkmo 8 hours agorootparentDo some traveling and broaden your horizons. It isn't a particularly novel or even rare opinion. \"American\" is one of the idiocies of the English language, and you'll notice it is the only language that presumes that that word refers only to US citizens. reply EasyMark 5 hours agorootparentprevIn common parlance, in both the US and abroad, it's commonly used as an agnomen for the USA, and that's okay. When people need to be more specific, they can say North America, South America, or \"the Americas\". It's fine, it works and isn't confusing. reply lupusreal 11 hours agorootparentprevIn America, the pair of continents are know as \"the Americas\". Every American knows singular 'America' refers to America. > But in other countries we say- Other countries exist? reply ianburrell 10 hours agorootparentMore importantly, in English, America is the US and the Americas are the continents. It is sort of irrelevant what other countries call America or if they divide up the continents differently. Other languages name things differently. reply II2II 9 hours agoparentprevI would be surprised if there is a lack of regulation. To give you an example from my country, the government has trademarks that identify government agencies. That includes the Canada workmark (with the flag over the final \"a\") and the flag to the left of the words \"Government of Canada / Government du Canada\". As trademarks, their use is protected. Yet words like Canada or Canadian don't convey any association with the government or even the country. reply citizen_friend 12 hours agoparentprevNobody has your interest in my mind, except in so far as aids their interests. Doesn’t matter how many regulations or labels or non profit signals there are. reply callalex 10 hours agoparentprevLet’s start with the Chamber of Commerce and the Better Business Bureau. reply BobbyTables2 9 hours agoparentprevAmazing. One million is barely the cost of office supplies at many places… Too often it feels like bribing politicians is cheaper than paying employees! reply im3w1l 12 hours agoparentprevAssociations aren't typically governmental entities are they? reply riknos314 15 hours agoprevWorth noting that the only requirements to be recognized as an Association by the IRS is that there must be a dated, written document showing its creation, signed by at least two people. [1] [1] https://www.irs.gov/charities-non-profits/definition-of-an-a... reply bhelkey 13 hours agoprevFrom the linked study, \"These findings of positive associations between artificial sweetener intakes and increased T2D risk strengthen the evidence that these additives may not be safe sugar alternatives.\" This is news to me. I was under the impression that: 1) artificial sweeteners were a safe substitute for sugar for people with diabetes and 2) diabetes came from excess sugar consumption which wasn't a problem with artificial sweeteners. reply McP 11 hours agoparentYes it is new. The WHO only changed their guidance about a year ago [1]. Still, as far as I know the evidence is only associational. From the paper the article links to: \"Potential for reverse causality cannot be eliminated\". [1] https://www.who.int/news/item/15-05-2023-who-advises-not-to-... reply shepherdjerred 10 hours agorootparentSo it sounds like artificial sugars can actually cause diabetes? That's unfortunate. Occasionally I drink Coke Zero as an alternative to Coke, but perhaps I should start replacing that with unsweetened seltzer water like La Croix. reply amanaplanacanal 57 minutes agorootparentOr, just as likely, diabetics are more likely to use artificial sweeteners. reply kevinmchugh 5 hours agorootparentprevAn occasional pop, sugar or no, isn't going to cause diabetes. reply worik 6 hours agorootparentprevWater. On its own. It's what I drink reply kwhitefoot 12 hours agoparentprevSugar does more than one thing. It's mere presence triggers hormone related actions in the body and its energy content is what the reaction is supposed to deal with. If a sweetener behaves like sugar as far as taste is concerned, that is it fools one part of your body into reacting as though it is sugar, it seems plausible that it might also fool other parts of the system. reply m463 9 hours agoparentprevThere are some studies showing some artificial sweeteners raise blood glucose, just like sugar. reply ayakang31415 5 hours agoparentprevIs the association correlation or causation? Because that is very important. People with higher BMI (thus higher risk of diabetes) tend to be self conscious about their diet, they might consume more artificial sweeteners than healthy individuals. They can be already diabetic to begin with before artificial sweetener was consumed frequently. reply Der_Einzige 4 hours agoparentprevI don’t buy this at all! Zero calorie sweeteners by definition cannot cause insulin responses. I can easily verify this by using my families blood glucose monitors before and after I’ve had a diet soda. I’ve done this several times and I have zero increase in blood glucose levels from a Diet Coke. It actually goes down since time passes between me starting the drink and finishing it. The obsession from even purportedly good doctors with trying to find reasons to vilify anything that might taste good while not spiking glycemic loads is horrifying to witness. I’m extremely upset, especially as someone with two diabetic parents and a history of diabetes. Both parents are diabetic because they’re fat as hell. reply Jensson 4 hours agorootparent> It actually goes down That is what you expect if you raise your insulin, insulin reduce blood glucose, so if the sweetener increased insulin without adding sugar then it reduces blood glucose. > goes down since time passes between me starting the drink and finishing it. Blood sugar levels are stable unless you eat sugar, they don't go down over time, unless you do something to add insulin to make it go down or add sugar to make it go up. Here it sounds like you adding insulin when drinking that and thus making it go down. reply Der_Einzige 3 hours agorootparentDiet soda using a zero calorie sweetener has an insulin index and glycemic index of zero. You’re just wrong. reply Jensson 3 hours agorootparentYou are making the strong statements here, not me, I'm just saying your evidence there wasn't enough to support your strong statements. If you have more evidence such as the insulin index being zero, you should have brought that up. If the insulin index is 0 then per definition it doesn't increase insulin, yes, but the glycemic index being 0 doesn't say it about insulin, as your post suggest, so your post was definitely wrong. I was not wrong pointing out that your post was wrong. reply watwut 12 hours agoparentprevThere is type of diabetes that is simply hereditary and that is all there is to it. It is not rare. Then there is also pregnancy diabetes that comes and goes with pregnancy (they make routine test for it). Not all diabetes is from sugar. reply epcoa 10 hours agorootparent> There is type of diabetes that is simply hereditary and that is all there is to it. This is a common misconception. Type 1 diabetes cases are more often acquired with no family history, there are some genetic associations but it is actually less strong than Type 2 (insulin resistance/diet related diabetes) where family history is a substantial risk factor. Pregnancy (gestational) diabetes isn’t just pregnancy related either as it is more likely in those with metabolic syndrome/obesity and is associated with type 2 as well, so it is also partially diet related. > Not all diabetes is from sugar. I would go further and just say no diabetes is “from sugar”, it’s just a completely uselessly simplistic way of looking at it (I know this was in response to GP comment). It’s not like low carb diets even prevent some people from getting type 2. Type 2 diabetes is a disease of acquired insulin resistance for which excessive consumption of high glycemic index food (eg processed food with high sugar content) is a major risk factor, but not the only one. reply shepherdjerred 10 hours agorootparentDo we know if type 2 diabetes itself is actually hereditary, or is it possible that the genetic component makes individuals eat more sugar which causes type 2 diabetes? reply worik 6 hours agorootparentOr it is possible that dietary habits are taken up from your cultural environment I see that, but I have not made a study reply throwawaymaths 10 hours agorootparentprevIirc diabetes is technically \"sugar in the pee\" so all diabetes must be linked to some sugar metabolism at some point. reply epcoa 10 hours agorootparentIt’s an interesting bit of ancient word and medicine trivia but it isn’t terribly elucidating about the mechanism or course of diabetes in modern times. Your body always has some glucose metabolism going, even if you’re in ketosis, etc there’s always “sugar metabolism” - your blood glucose doesn’t drop to 0 unless you’re dead. It’s simply not a useful abstraction to link sugar consumption to diabetes as implied by the GP. And ultimately diabetes is defined as a condition of dysregulation of sugar in the blood. In fact, these days most well cared for people with controlled diabetes don’t have sugar in the pee unless they’re taking a medication specifically to put it there. Also if you want your mind blown and why you can’t diagnose based on etymology alone (at least not without a sense of humor) - lookup “diabetes insipidus”. Literally means “sweet pee that isn’t sweet”. The historic reason is that excessive urination is common to both. reply zadokshi 9 hours agorootparentprevYes, but from an education perspective it’s probably hurting more people than it helps to use this terminology. It’s better to label the true source of the problem, rather than reduce the blame to “sugar” reply gumby 14 hours agoprevI was shocked by the ADA's nutritional training (required by my insurance company) but I figured that the actuaries had decided they'd delay amputation better by encouraging people to eat a small slice of cake rather than no cake at all. reply akira2501 13 hours agoparentSo the actuaries decided to unironically \"sugar coat\" the truth to gain a small measure of compliance? reply siliconc0w 12 hours agoprevEndless capture of our intuitions by entrenched incumbents. I'm pretty convinced that unless you're an athlete, everyone should be on a low carb diet ( The more complete and histocally evidenced rule is that all institutions eventually just work to continue and/or expand their existence in itself. This isn’t more complete and historically evidenced. This is a Law Named After Person/Dilbert Quip, which is the pit of cliches that a lot of HN comments fall into on sociology. What, other than just cynicism,[1] have these Stated Truisms contributed to? These rules are so rigid (so they can be pithy, snappy) that they sound immutable. Is the point only to, say, feel smug about how the manager directly above you has been promoted to his level of incompetence? [1] Cynicism is fine and good. But just-cynicism has no way of moving beyond itself to a better state. The difference between critique and throwing your arms up. > They can get founded in the genuine interests of some cause (and often are), but each transition in leadership tends to find itself more professionalized in some way and more divorced from the founding cause, with process (and/or corruption) becoming their effective mandate instead. Nothing in history is ever just a downward spiral of corruption and rigidity. Outside things happen, revolts happen, things are replaced, systems are overturned. reply watwut 12 hours agorootparentprevThat quip is a cynic joke, not a statement of historical fact. There are many organizations that simply kill themselves and die out via people leaving as the original purpose don't matter anymore. reply swatcoder 12 hours agorootparentQuip? Joke? I don't know what you mean. It's an extensively treated paradigm in sociology. With sociology being a \"soft\" science and not having access to a methodology as rigorous as physics, it's certainly contestable and there are of course many sociologists who have made arguments against it or that simply don't consider it convincing, but it's not just some casual insight and certainly not someone's \"joke\". The reason I mentioned it, in any case, was to relate it to previous commentor's supposition that unions were excluded from their \"rule\" (which was a casual insight). You needn't take either this perspective nor theirs as true yourself, but there's not much case to exempt unions if you're going to start looking through the world from that lens in the first place. reply mistrial9 14 hours agorootparentprevthis has lots of \"real\" in it but details matter. At a formative time, American politics specifically substituted \"safe\" leadership in union upper management.. either connected to party politics or just directly from old-Right Europe who had lots of experience dealing with workers and systems. Yes, there were real Mafia families in the Teamsters, in other words. The fiery and violent revolutions across the world did have their impacts on America. Since the 1980s, evolution via bureaucracy and golf clubs, court cases and election results seem to have been more the driving force.. people can only get so fat before their eyes start to glaze over and trivial concerns take the airtime. reply newshackr 15 hours agoparentprevUnions also don't always work in the interest of everyone they represent. For instance, they tend to be protective of older and longer working members in exchange for limiting upward mobility of younger members. Or building policies that encourage the growth of the union as an organization, despite potential costs. reply CSSer 14 hours agorootparentIt’s true. My first job was as a cashier at Kroger. I remember looking at the pay tables and being shocked. You could make impressive money as a cashier at Kroger… if you worked there for 30 years. Virtually no one I worked with had even worked there for more than a few years either. Meanwhile, I earned just above the federal minimum wage and multiple hours of my weekly earnings went to little more than protecting my “right” to “no-call, no-show” for six straight days in a row before being fired. I never did this. It seemed unfair. Others did and I had to work that much harder on those days. One day a union rep stopped by. He was very well-dressed, and he had this beaming smile. He gave me a t-shirt. That pretty much summed up the benefits I experienced. I remain pro-union, but every time the subject comes up I think there’s a lot more nuance there than people would like to admit. My Dad, for example, has worked at a union job for over 30 years. Ironically, he’s a Republican. He makes decent money now, but the job is very labor intensive, and the healthcare sucks. He’s repeatedly turned down a promotion into management because he’d be out of the union and earning a salary that is not that much higher than what he currently earns per hour. He’s also told me repeatedly that kids just don’t want to work anymore because the turnover rate is high. Many starting out, especially those with dependents, complain that it’s not worth it for what they earn. He holds that although it’s not great it’s enough. Recently, in an attempt to persuade him to take a promotion, he learned that the healthcare plans offered to management are 2-3x cheaper for better benefits than what the union has negotiated. It’s been pretty crazy to watch his opinion slowly begin to shift. reply ipaddr 13 hours agorootparentHe is staying in for protection. The extra money doesn't make up for having your experience count as added protection (they can't fire him, they can reduce those jobs and employees with more service time will keep the job first). If a company can fire quickly and replace with cheaper options no one is going to last more than a few years. Many software shops do this (meta, Amazon, generic local company, etc. I share your experiences working in a union environment when younger and having family have live long jobs. In one case I felt the union was against me or my class of worker (student employee) because we took away from regular union jobs. Still had to pay dues. The other union job just took a few dollars from my paycheck but gave me a wage I couldn't earn elsewhere. reply crmd 14 hours agorootparentprevWhen you become an older worker, and experience firsthand the vulnerability of seniority, you realize why your union was always so protective of older workers. I would think many tech workers over ~35 can relate. reply akira2501 12 hours agorootparentprevThe fundamental idea of most unions seems to be, that once labor recognizes that it is working for a monopoly, instead of working to break up that monopoly, they decide to form one of their own in order to gain some power of negotiation with their employer. Typically to the detriment of the consumer market and the labor market. For new entrants into the labor market, as you've flagged, now they have to successfully negotiate between two overly large entities with predictably unfortunate results. Labor is best served as a competitive market and unions should only be used in the few limited circumstances where they are otherwise unavoidable. reply hawkice 15 hours agoparentprevUnions aren't acting in the interest of the people generically, often not even the members of the union. Otherwise they wouldn't go to court to force people who don't even want to be in the union to pay dues, even when the dues go to political campaigns unrelated to the purpose of the union, as in Janus v. AFSCME (where the support for political causes by the union meant it violated public sector workers freedom of speech if they are compelling fees from non-members, a relatively narrow ruling not impacting most unions). reply beryilma 13 hours agorootparent> Otherwise they wouldn't go to court to force people who don't even want to be in the union... Even worse, they might request your dismissal if you don't join. Here is a direct quote from the Union agreement of a major university, where I teach part-time: \"The Union may request that a Part-time Faculty Member who fails to join the Union, maintain Union membership, pay an agency fee, or make a charitable contribution in lieu of an agency fee shall be dismissed. If the Union makes such a request, the Employer shall comply... If the Part-time Faculty Member fails to pay within that time period, and the Union so verifies, the Part-time Faculty Member’s employment will be terminated at the Union’s request\". If anything, unions are only acting in their own interest. reply sircastor 12 hours agorootparentArguably, a union has to hold the position of requiring membership, and against those who don’t want to join. Collective bargaining only works when your position represents the group to the point where it can’t be dismissed. But yeah, It’s challenging for a union to remain exclusively dedicated to serving its membership. I think it’s increasingly complicated with national unions which exist for the sake of unions as a concept, but not necessarily any union members. It’s weird. reply Jensson 4 hours agorootparentUnions doesn't work that way in Europe, it is not something unions must do. Collective bargaining works as long as you are a collective that bargains together, you don't need to have every single person be a art of that collective. reply elicash 13 hours agorootparentprevI strongly disagree with you about Janus. The point of that decision by a right-wing court was to make it harder for public sector workers to stand together in unions. Unions are democratic institutions, with dues and leadership decided by the membership, and the idea that some people pay and some don't even though the entire unit is represented doesn't make a ton of sense. Just like it wouldn't make sense if 2 people in a unit of 1000 wanted to be in a particular union to just say those 2 can bargain collectively. The weird \"speech\" argument was basically that their worksite issues are inherently political. I disagree. Unions have separate political funds from the worksite stuff that are optional. reply devilbunny 9 hours agorootparent> public sector workers A group for whom even FDR, the great co-opter of unions, had to pretend to be opposed to unionization. Seriously, civil service protections are there for you and don't apply to private-sector employees. But you claim the right to strike against your fellow citizens and deprive them of government services? This isn't Andrew Carnegie; it's your fellow citizens. reply elicash 7 hours agorootparent1. I’m not a public sector worker, weird assumption. 2. I advocate for private sector to unionize as well. I’m not the one who brought up Janus/private sector. 3. You’re telling me people with important jobs (private or public) shouldn’t be able to strike to improve the very services we all depend on? Hospital workers striking for better patient ratios? Teachers for their students? Taking away those freedoms is wrong and dangerous. reply hn_version_0023 15 hours agoparentprevThere’s a movie clip I’ve seen with Brad Pitt of all people, explaining how America isn’t a nation, it’s a business. A business you don’t own and can’t escape. And you’re on your own. Something to that effect anyway. I’d love to know the movie or show! Its such a perfect example of a truth we all know but that never gets spoken aloud. The Founding Fathers would be deeply ashamed of us, I think. reply sircastor 12 hours agorootparentI think the founding fathers would be confounded by us more than anything else. Probably by federalism, the focus on the presidency, Senate elections by the general populace, the role and power of the Supreme court, and a pile of other things. reply rawling 14 hours agorootparentprevKilling them softly? reply hn_version_0023 5 hours agorootparentThank you kindly :) The irony is I had never seen the entire scene, and Jefferson is called out hard! reply AmVess 14 hours agorootparentprevYep. Quote is at the end. reply adhamsalama 14 hours agorootparentprevFight Club? reply 7speter 14 hours agorootparentprev>The Founding Fathers would be deeply ashamed of us, I think. Sorry, but the founding fathers who held other humans in bondage for all sorts of free labor? Those founding fathers, or did I wake up in a parallel universe this morning? Let me check wikipedia… reply mistrial9 14 hours agorootparent> the founding fathers who held other humans in bondage for all sorts of free labor Actually, slavery was a bitter dividing line among the founders of the USA. It is intellectually lazy to ascribe slavery to all of the founders of the USA. It also insults those who were vocally and politically against slavery, from the very earliest days. You can find many examples with any effort at all. reply 7speter 9 hours agorootparentYes it was so divisive in the late 18th century in what became the U.S. that the founding father punted the issue so that the 13 states would ratify the new constitution. The issue was swept under the rug until that rabble rouser Abe Lincoln got elected, and all the slave states seceded between the time he got elected and inaugurated, sone 80 years after the signing of the declaration of independence. Feel free to downvote because I’m not gonna accept your revisionist history. reply beepbooptheory 13 hours agorootparentprevIs there anything at all to be said about the fact that slavery ended up being permissible anyway? Or is that just lazy? Like, doesn't the sheer magnitude of the inhumanity that actually existed in these times kind of overshadow whatever armchair-enlightment some guys voiced? How could you be aware of what they did in those times even the slightest bit, and yet still be concerned that one might \"insult\" guys who have been dead 200 years? How can that even make sense? reply mixmastamyk 12 hours agorootparentSlavery wasn't invented in America. It was common during the Roman Empire and thousands of years before that. Blaming 18th century folks for not righting every wrong up to their time is lazy, as it would be to attribute full-responsibility to you today for something improved and looked down upon in the future. (None of us are fully independent but gain and suffer inertia from history and society at large.) Still, we can learn from Ancient Greece, American Founders, as well as folks today. reply beepbooptheory 11 hours agorootparentI apologize, this is such a strange way to be positioned to all this. The point of bringing up something like the practice of slavery in early America is not about \"blaming\" people about anything. They are already dead! Many generations over. This conceit that the bare acknowledgement of history itself should merely serve to assign blame or culpability to certain people or another feels just so wrong. The point is that it happened. It was determined and sustained by countless totally mechanical and impersonal conditions and tendencies. Just as the operative ideologies in play in the minds of all our fave founding fathers can only be viewed from our purview as some composite of factors, not as some collection of good guys and bad guys. To point out that maybe they should not be a moral compass to us today is not scapegoating them in some grand moral court of human existence! Its just making a point, and urging historical context as a tool to maybe be a little more rational about our world today. There is nothing at stake but that. reply mixmastamyk 10 hours agorootparentThis whole thread is a bit weird now rereading it. My post was pushing back on the narrower idea that past folks' ideas in subject area A should be disregarded because they may have participated in now unacceptable subject area B. Also that Americans tend lack context around the subject. reply sapphicsnail 14 hours agorootparentprevAlmost all of the founding fathers either owned slaves or weren't willing to stick their necks out to stop it. We absolutely should question their judgement. reply devilbunny 9 hours agorootparentPolitics is the art of the possible. Given what post-slavery America looked like for the former slaves (not much different), the question becomes: do you care so much about slavery, which is an ancient institution, that you give up the idea of forming a united front? Was it good? No, it was a bad idea. But that's something we say comfortably from our homes in a large, powerful country. In 1787... this was a remote and weak place. If you wanted the slave-dependent colonies to join, you had to either buy their slaves from them or allow it to continue. And there wasn't enough money to buy them. reply mistrial9 13 hours agorootparentprevless than ten seconds of research: $googlesearch \"how many signers of the US Constitution were slave owners?\" 25 Of the 55 delegates to the Constitutional Convention, about 25 owned slaves. Many of the framers harbored moral qualms about slavery. Historical Context: The Constitution and Slavery reply yongjik 10 hours agoparentprevI'm not sure why you're complaining about instagram/tiktok, because \"everything aside from unions do not work in the interest of the people\" is the kind of facile, black-and-white statement that is perfectly suited for the modern ragebait industry. reply rpmisms 12 hours agoparentprevUnions work in their own favor, not necessarily the workers'. That's the real rule: everything tries to benefit itself. reply goto11 15 hours agoparentprevThere is no \"the interest of the people\". There are lots of different people with different and often conflicting interests. reply citizen_friend 12 hours agoparentprevWhat property do unions possess that makes them different? reply Nasrudith 5 hours agorootparentThe standard rhetorical sleight of hand that is standard any time \"the people\" are cited. To be fair unions are slightly better than most users in that department as they have actual votes. reply aardvarkr 15 hours agoparentprevThat’s… honestly not a bad take. If we’re not paying for it then we’re the product, not the customer. Need to clarify that unions aren’t for ALL PEOPLE, just the people who pay for them reply umvi 10 hours agoprevI feel like diabetes will be difficult to cure because it's like tobacco: you need to buy expensive consumables at regular intervals (needles, cgms, insulin, etc). Stem cell therapy to restore beta cell functionality (for T1) would threaten and disrupt that sweet regular cash flow. So I imagine companies will fight hard to keep diabetics purchasing their regular consumables until the end if time if possible reply gumby 10 hours agoparentDoing this would be worth so much money that if you can do it, you should. You’ll make a mint. Currently, islet cell transplants are possible, but barely available. The insurance companies that pay for a lot of the supplies you talk about would prefer a one stop solution if it costs less. The big expense due to diabetics for insurance companies is not those consumables, it’s the consequences of dialysis and amputations. reply m463 10 hours agoparentprevwith respect to type 2 diabetes - which most americans are at risk for - you're arguing about something too far into the process. If you don't get T2 diabetes in the first place, because you got good advice, you will never get to the needles and insulin stage at all. reply GiorgioG 15 hours agoprevThe ADA has for a very long time told diabetics to keep eating carbs. Their tune is finally changing, but no, they don't give a shit about anyone other than themselves. reply code_biologist 15 hours agoparentOne drug advertisement I’ve seen on Pinterest lately says “diet and exercise can’t treat diabetes” which may be true of type 1 diabetes, but last time I checked 90% of the diabetes in the US is type 2 and diet improvements should absolutely be the first line intervention. reply connicpu 14 hours agorootparentIt would probably be more accurate to say that diet and exercise alone are not enough for most people with Type 2 diabetes in the US. They are still an important part of intervention. But as a practical matter, telling people who've developed Type 2 to just suddenly gain the willpower to do both of those things really isn't going to be enough to stop progress of the disease. reply nradov 13 hours agorootparentprevDiet and exercise are extremely effective in treating type 1 diabetes. But some exogenous insulin (and other medications) are usually still required. For type 2 diabetes, research had shown that many patients can put the condition into remission through lifestyle changes alone. Nutritional ketosis is very effective in reversing insulin resistance. https://www.virtahealth.com/research reply UncleSlacky 13 hours agorootparentAgreed, I've just successfully reversed mine using the \"Newcastle Protocol\" (calorie restriction for ~12 weeks): https://www.ncl.ac.uk/magres/research/diabetes/reversal/#pub... https://www.diabetes.co.uk/news/2017/dec/newcastle-diet-achi... reply GiorgioG 6 hours agorootparentprevYou’re talking out of your ass about type 1 diabetes. Type 1 diabetics stop producing insulin entirely and are 100% dependent on exogenous insulin. My 12 year old son is a type 1 diabetic. He plays ice hockey, and his blood sugar will have a tendency to spike during games without having eaten anything for hours prior simply due to emotions/Adrenalin, whereas at practice he’s more likely to experience low blood sugars. Edit: Since I can't reply to another one of your posts below, I'm going to respond here. There's a \"honeymoon period\" after the initial symptoms of type 1 appear that yes you will still produce some insulin, for most patients it's about a year, but after that, no insulin is produced. Zero-carb isn't a realistic long term solution. My son was diagnosed at 18 months of age. Feeding him nothing but bacon, steak and chicken is not particularly good idea for a toddler or growing child. reply ksaun 11 hours agorootparentprev> Diet and exercise are extremely effective in treating type 1 diabetes. But some exogenous insulin (and other medications) are usually still required. I think you are misinformed about Type 1 diabetes. Maybe you are thinking of some other condition? In Type 1 diabetes, the pancreas stops producing any insulin. Insulin must be administered regularly (always, not usually) or the person will die. \"Other medications\" aren't required. One's diet and exercise are relevant to Type 1 diabetes treatment, but are not a treatment method. Source: my daughter has Type 1 diabetes reply GiorgioG 6 hours agorootparentYou put it much more diplomatically than I did in my response (my son has T1D) reply nradov 6 hours agorootparentprevI'm sorry for your daughter's condition, but I think you are misinformed about type 1 diabetes. Most patients still product some endogenous insulin, it's just usually not enough. Like any chronic condition, it's a spectrum and there are outliers at both ends. I've learned to be very cautious about using words like \"always\" or \"never\" because of the odd cases that occasionally come up. https://doi.org/10.1007%2Fs00125-013-3067-x Diet and exercise are very effective at treating type 1 diabetes, just usually not sufficient. With a minimal or zero carb diet plus the right exercise regime, some patients find that they can go much longer between insulin doses while still maintaining good blood glucose control. In particular, I think a lot of people don't appreciate the necessity of doing resistance training to build enough skeletal muscle mass as a glucose sink. https://doi.org/10.1542/peds.2017-3349 reply addicted 15 hours agoparentprevThere is nothing about carbs themselves that is bad for diabetics. Carbs are necessary macronutrient for everybody. If you mean certain foods that also have a high percentage of carbs then sure but carbs are not the problem. Take this line from the article itself, for example. > It can be challenging for many people with diabetes to forgo the breads, sweets, pastas and starches that form the basis of many diets. What’s common about these foods isn’t that they’re high in carbs. It’s that they’re nutritionally deficient foods that have been stripped of all their fibers and basically reduced to sugars. Further, the diets promoted as low carb popularly are not good diets either for diabetics or for non diabetics. reply kamens 14 hours agorootparent\"nothing about carbs themselves that is bad for diabetics\" is like saying \"nothing about waves themselves that are bad for boats\" kinda sorta true...but turns out sailing in calm waters is a hell of a lot safer than heading into rough seas (type 1 for 28 years) reply Sargos 14 hours agorootparentprev>Carbs are necessary macronutrient for everybody. This is a wild claim to make and doesn't seem to hold true scientifically. Humans require fat, protein, and various vitamins usually received from vegetables. There is no requirement that comes from carbohydrates as they effectively are just empty calories. reply adrian_b 13 hours agorootparentCarbohydrates are not necessary for survival, but there are circumstances when they are necessary, for instance when a very intense effort is required to be sustained for a long time it is impossible to achieve a maximum performance without eating carbohydrates, because they can be absorbed and used for energy production faster than the alternatives. Moreover, there is not enough data to decide whether a diet lacking almost completely carbohydrates results in optimal health in the long term, even if it may have favorable effects when replacing a worse previous diet. Carbohydrates are also the cheapest kind of food. While eating them in excess is bad, obtaining less than 50% of the energy intake from carbohydrates still results in a much lower cost of the food than replacing all of them with expensive fats and proteins. For diabetes prevention, it is likely that it is more important to avoid sugar than it is to avoid starch, because in many traditional societies where starch was a big fraction of their food, diabetes was nevertheless uncommon. reply keybored 13 hours agorootparent> Carbohydrates are not necessary for survival, but there are circumstances when they are necessary, for instance when a very intense effort is required to be sustained for a long time it is impossible to achieve a maximum performance without eating carbohydrates, because they can be absorbed and used for energy production faster than the alternatives. And some people require an intake of 8000+ kcal a day. Not relevant to anything. reply watwut 12 hours agorootparentprevWhat is know for fact that this \"carbs are evil\" messaging absolutely sux for anyone having to deal with eating disorder - both sic people and their close ones. And if I had to choose between diabetes and eating disorder, I would go for diabetes. Eff the demonizing of whole food groups of food. reply meroes 14 hours agorootparentprevNothing gives me quick, accessible energy like carbs though. Energy to think and love my body. Yes I’ve seen documentaries on low carb high fat ultra athletes, but they still have their fats with a bowl of pasta. God only knows their saturated fat intake too. A carbless diet does not seem well rounded, and is thus unhealthy. So carbs are essential. reply cempaka 14 hours agorootparentIt does not follow from you personally deriving psychological benefits or quick energy from eating carbs that a diet which excludes them is not well rounded or is unhealthy. reply toast0 14 hours agorootparentprevKeto isn't no carbs, but it's a pretty small number compared to typical diets. It's not for everyone, but it can help some people manage blood sugar. reply keybored 13 hours agorootparentprev> A carbless diet does not seem well rounded, and is thus unhealthy. Why “thus”. Just saying that lacking something is not well rounded thus unhealthy just looks some sort of middle of the road fallacy. Diets that are not about calorie restrictions are all about excluding certain things. And they all claim to be better than the potentially more versatile middle of the road diet. reply kjksf 14 hours agorootparentprevSo you go from \"does not seem well rounded\" to \"therefore is essential\"? Here's some facts. There are nine essential amino acids that our body needs to function properly and cannot produce by itself from something else. Meat provides those amino acids and carbs do not. Carbs are a source of energy but so is fat, proteins, ketones and alcohol. In addition to reversing diabetes, people on low carb / high fat diets, including carnivore, often report increase in overall energy levels and lifting of a mental fog that they experienced on standard carb and sugar heavy diet. That energy spike that carbs and sugar give you is a glucose spike in blood and the downside of it is that often it goes in the other direction (i.e. lethargy) when you come off of it. That's the \"nap after heavy meal\" effect. This is not an anti-carb just anti what you do, elevating carbs into some unquestionably good, unique energy source necessary for you to think or love your body. Consuming carbs in moderation is fine. The problem is that our modern diet and what is available in grocery stores or restaurants make it almost impossible to consume carbs in moderation. And apparently plenty people like you don't even understand that carbs are, in fact, bad for most people. U.S. stats on this are shocking: 73.6% americans over 20 are overweight and 42.4% obese (all obese people are overweight but not all overweight people are obese) reply adaptbrian 13 hours agorootparentThis is exactly how I felt when I went keto/low inflammation foods. Would have never expected it. reply mlhpdx 13 hours agorootparentprev> There is nothing about carbs themselves that is bad for diabetics. That is untrue. A Type 1 diabetic requires insulin proportional to the amount of carbohydrates they eat. The larger the insulin dose, the higher the potential error in the dose compared to the carbs (it’s inexact). If the error is on the “too much” side it can drive blood sugar fatally low. This happens, unfortunately often. reply pimeys 12 hours agorootparentNowadays the artifical pancreas software can quite nicely counteract the carb spikes. It's not very hard, just take all your basals for the next two hours and then have no insulin delivery for that time. Especially if you eat fast carbs, this is the right strategy. More complex is to dose for fat, protein and carb mixture. You basically need almost no insulin first, but in the next four hours you need 1.3-1.4x your basal to cope with the raising sugar. reply kjksf 14 hours agorootparentprevThis is shocking misinformation. > Meat is considered a complete protein source, meaning it contains all nine essential amino acids that your body cannot produce on its own Do carbs? > Carbohydrates primarily provide energy for your body and are not a significant source of amino acids. Essential amino acids, which your body cannot produce on its own, are primarily found in protein-rich foods such as meat, dairy, eggs, and some plant-based sources like quinoa and soy So you're 100% wrong: carbs do not provide essential amino acids and meat does. The mechanism behind type 2 diabetes is well known and it's all about eating too much carbs. Sugars and carbs are converted to glucose in your blood. You can't have too much (or too little) glucose in blood. Some of it is burned as fuel but the rest has to be removed somehow so body starts producing insulin to push glucose into cells where it's get converted into fat. If you can't produce insulin, you have type 1 diabetes and need insulin injections. If you overeat carbs you store more and more fat. You become over-weight and insulin gets worse at moving glucose from your bloodstream. The diagnostic test for diabetes is literally: do you have too much glucose in your blood (compared to what is healthy range) or related test a1c which tests for elevated levels of insulin in your blood. If you know the above, then how in the world can you claim that carbs are not the problem? It is literally the thing that causes diabetes. The simple solution to reversing type 2 diabetes is therefore to stop eating carbs. That's low carb diets like keto or carnivore. You make very wrong assertions (carbs are not the problem; carbs have all macronutrients; low carb diets are bad) without a single supporting argument or reference. reply UncleSlacky 13 hours agorootparent> The mechanism behind type 2 diabetes is well known and it's all about eating too much carbs. No it's not. From https://www.ncl.ac.uk/magres/research/diabetes/reversal/#bac... \"We now know that type 2 diabetes is caused by excess fat inside liver and pancreas. ... The Twin Cycle Hypothesis described how it might be possible to explain the cause of type 2 diabetes in a very simple way. ... It was clear from that time onwards that type 2 diabetes is caused by too much fat building up within the liver, then overspilling to the rest of the body - including the pancreas. This starts up a second vicious cycle inside the pancreas, with the fat actually switching off normal insulin production...One of the most important discoveries is that of the Personal Fat Threshold. Type 2 diabetes is not caused by ‘obesity’. Different people have different levels of tolerance of fat within liver and pancreas. But if you have type 2 diabetes, you have crossed your ‘personal fat threshold’. reply themgt 6 hours agorootparentThe explanation for T2D from the PDF paper from your own link explicitly singles out carbs and explains why they are critical to causing the disease process you describe. > During any one period of time, if more calories are ingested than metabolized then any fat excess is stored either subcutaneously, viscerally or in the liver. But any excess carbohydrate cannot be stored once the glycogen depots are full. If more glucose is ingested than can be oxidized for energy or stored as glycogen, it has to be turned into fat by the process of de novo lipogenesis. This process only happens in the liver in humans, and triglyceride synthesized in situ is particularly likely to be stored in hepatocytes rather than exported for safe storage in subcutaneous adipose tissue. > The newly synthesized fat has three possible fates: it can be oxidized for energy; exported as VLDL in the plasma to be delivered to other tissues or it can be stored in a rather full liver. As de novo lipogenesis is stimulated by insulin, those people who are relatively insulin resistant in muscle—and who therefore have a raised plasma insulin level—are especially likely to accumulate fat in the liver. This could explain the reason why muscle insulin resistance is the first detectable signal of risk for Type 2 diabetes. https://www.ncl.ac.uk/media/wwwnclacuk/newcastlemagneticreso... reply User23 13 hours agoprevThe entire American medical system manages to act against the interests of both patients and doctors, the two essential classes of participants. It’s a rather remarkable achievement. reply fransje26 11 hours agoparentI'll leave this little perl here: https://arstechnica.com/tech-policy/2018/04/curing-disease-n... reply Nasrudith 5 hours agorootparentSince when do publically traded corporations care about sustainability of business models? They'll cheerfully destroy a business for short term gain and then leverage the funding elsewhere. reply marcosdumay 13 hours agoparentprevIf you look around, that happens often on several different industries. Somehow both people consuming and producing stuff are powerless nowadays. reply johnfernow 12 hours agoprevHere's a great PDF I found on the ADA's website from fall 2023: https://professional.diabetes.org/sites/default/files/media/... > This content is brought to you by Splenda, a proud supporter of the American > Diabetes Association > A Message from Splenda > Splenda® is committed to helping people achieve their health goals by making it > easier for people to reduce the amount of added sugar in their diet. You likely know > Splenda Original Sweetener (“the yellow packet”), but did you know we also make > Splenda Stevia? Splenda Stevia is a plant-based sweetener made from the > sweetest part of the leaves of the stevia plant. And just like Splenda Original, > Splenda Stevia contains zero calories and zero sugar. The people you see can use > Splenda Stevia to make a variety of delicious recipes from appetizers and drinks to > entrées and desserts. > Check out the ADA's Diabetes Food Hub® for recipes that use Splenda Stevia, like > these Slow-Cooker Sweet & Spicy Turkey Meatballs, which are perfect for a football > party, and these Gluten-Free Mini Eggnog Cupcakes which add cheer to any holiday > gathering! WHY??!!! I get upset enough with tech YouTubers making misleading claims about VPN sponsors, but the American Diabetes Association allowing sponsors that sell products that several studies link with causing and worsening diabetes to write parts of their newsletters is an entirely different degree of unethical behavior. No, the link between artificial sweeteners and diabetes has not been firmly established and more research is needed, but it's a likely enough connection that the CDC and WHO[1] have expressed concern and have noted the potential links. [1.] https://www.who.int/news/item/15-05-2023-who-advises-not-to-... I get it, organizations have to get funding somehow. But if the American Lung Association started allowing vape companies to write part of their newsletters, I think people would rightfully be outraged. Sure, vaping is less bad for you than cigarettes, and may even be a helpful way of quitting for some, but allowing them to be a sponsor is a major conflict of interest and causes you to lose credibility. reply gloryjulio 8 hours agoparentI thought Splenda has almost nonexistent calorie and I am not sure what makes it different from aspartame functionally. Any reputable research on the direct damages caused by Splenda? reply underseacables 14 hours agoprevLike most associations and trade groups, they are beholden to whatever gives them money. It's like when the Academy of nutrition and dietetics was sponsored by Pepsi and Mars. reply 0xcde4c3db 11 hours agoparentThat group has rubbed me the wrong way for a long time, ever since I saw one of their publications (ca. mid-2000s, I think?) and found it full of glossy food stylist photos, magazine-like layouts, and stock vacuous advice like eating \"smaller portions\" and a \"varied diet\". For all of its shortcomings and the undeniable political swamp it has to ford through editing and publication, I bet you'd very much get more specific, actionable, and evidence-based dietary advice from the Dietary Guidelines for Americans than from whatever that rag was. There's a chance that I'm misremembering this or confabulating details from multiple publications from different organizations, but skimming the current website (\"eatright.org\", lol) has done nothing to make me doubt the general vibe. reply underseacables 9 hours agorootparentIt's an awful organization. They push dietetics licensure preventing people from getting nutrition information from a provider of their choice, because the organization depends on dietitians paying yearly for the Registered Dietitian credential. You can't get a license without being an RD. It is the most useless form of government regulation, but thankfully they have lost a lot of states. Now you only need a license in about 14 states. If you want to see something really hilarious, look up Pepin Tuma, their former government affairs guy. Total jerk. There are countless videos of him threatening and berating government committees over their skepticism of the license. He was finally fired a couple of years ago. reply blackeyeblitzar 10 hours agoprevThe best guidance on avoiding diabetes or living with it is found on YouTube and not the ADA. The ADA guidance and standards of care are often incorrect or outdated or very conservative. For example standard care says you can’t reverse insulin resistance, which is just plain wrong. reply johnfernow 12 hours agoprev> My view is that diabetes is an urgent national scandal. Over 100 million Americans have diabetes or prediabetes, and 100,000 die from the condition annually. In addition, every year hundreds of thousands of people with diabetes have limbs amputated or suffer blindness or kidney disease. Diabetes costs our country $400bn annually to treat. So about 4x as many deaths per year as homicide, twice as many as suicide, twice as many as car crashes, twice as many as accidental falls, and a multiple of plenty of other preventable causes of death. Obviously some of these other types of deaths take far more years off people's lives than diabetes, but I think the author is right to call it an urgent national scandal. > Her meticulous account depicts the world’s most important diabetes patient advocacy organization as a cynical fund-raising machine, anxious to please its corporate overlords at the expense of the millions of people with diabetes it is supposed to be trying to help. “The defendant’s conduct shows that they were party to a scheme to defraud the American people by approving and endorsing recipes submitted by Splenda to be lauded by the ADA as a healthy choice for people with diabetes, when the ADA knew that those recipes were contrary to the ADA’s guidelines and well-established and emerging scientific principles,” the complaint reads. In case you’re curious, the ADA and Splenda appear to be still at it. As I write this, the ADA’s Diabetes Food Hub web page still features no fewer than 203 recipes – some marked “sponsored”, some not – that include Splenda, whose parent company’s $1m contribution has brought to light the utter insanity of our diabetes epidemic. I really wish this could have gone to the discovery phase. Hopefully there will be investigations. > And although type-2 diabetes is often reversible through a low-carbohydrate diet, the ADA and the pharmaceutical industry don’t seem very interested in acknowledging that. Instead, they promote a laundry list of corporate deals and pharmaceutical treatments that have failed to stem the disease’s lethal and expensive impact on American life. I'm not naïve enough to think that all (or even most) of the tens of thousands of people dying from type 2 diabetes each year would be saved if the ADA didn't give poor advice, but I also think it's wrong to think that it'd have had no impact: insulin has been infamously expensive for many years in the US, and insulin pumps are thousands of dollars. I certainly think some of the millions of people who ended up dying from type 2 diabetes would have made lifestyle changes had it been made clear that it's often reversible with diet changes, if for no other reason than to save money. If you don't have a pump, having to take insulin shots throughout the day is a pain the ass, so again, I think some people would have made lifestyle changes to avoid that had it been made clear that it was an option. reply Animats 13 hours agoprevCures are nice, but the money is in chronic conditions. reply kevinmchugh 5 hours agoparentHealth insurers, who buy quite a lot of diabetes treatment, would much rather buy a cure. And they really don't want to pay for untreated diabetes. I've heard from an insurance CEO who wishes their plan could pay the electric bill for diabetes patients to ensure they don't lose power and refrigeration, which would make their insulin get too warm reply someonehere 14 hours agoprevnext [2 more] [flagged] awful 12 hours agoparentactually, a Wall Street industry analyst said and wrote approximately that, in what, the 1980s. reply renewiltord 15 hours agoprevClassic principal-agent problem. It's why you can't usually trust experts who aren't aligned with you. One way is skin in the game. But I suppose \"if you aren't paying, you're not the customer\" applies as well to nutrition and medicine as it does to free webmail. reply cempaka 14 hours agoparentIn light of this news, how should we reevaluate some of the recent admonishments to \"trust the experts\" and \"don't do your own research\"? reply Dylan16807 10 hours agorootparent> \"don't do your own research\" Does anybody say that? I see a lot of \"some guy making a convincing-seeming argument on youtube is not doing your own research unless you fact check it by legitimately looking for omitted or incorrect statements, because there's a million crazy echo chambers out there\". \"Do your own research\" can be a red flag, but only because so many people are bad at research, not because research is bad. It's important to look for comprehensive information and not just search for what you already think is correct. other comment> The \"trust the authorities\" perspective currently en vogue is certainly not that \"doing your own research works.\" It's that one needs an MD or PhD in a medical field in order to evaluate research, and hence should defer judgment on personal decisions to such qualified individuals (or more often, in practice, to institutions which purport to speak for them like the AMA, AAP, or American Diabetes Association). It's not that you need it, it's that you'd better put serious effort in to make up for it, and you need to take scientific consensus very seriously (Which is not the same as listening to the most prominent voices. But if almost every expert top to bottom is saying the same thing, they're almost certainly right.) And for 95% of things, you're not going to put in enough research to understand from base principles. So if you're outsourcing, outsource to someone competent. reply marcosdumay 13 hours agorootparentprevAre you talking about doing your own research or sheepishly believing on what some random person you like preaches to you? One of those work, the other is what people that say they \"did their own research\" usually do. reply cempaka 13 hours agorootparentThe \"trust the authorities\" perspective currently en vogue is certainly not that \"doing your own research works.\" It's that one needs an MD or PhD in a medical field in order to evaluate research, and hence should defer judgment on personal decisions to such qualified individuals (or more often, in practice, to institutions which purport to speak for them like the AMA, AAP, or American Diabetes Association). reply AlbertCory 14 hours agoprev [–] > Over 100 million Americans have diabetes or prediabetes These two are not the same. It is dishonest to combine the two. https://www.chicagotribune.com/2016/07/29/prediabetes-the-ep... I'm not quite ready to publish my hypothesis that prediabetes is a scam. I need to research it a little more. But a statistician friend of mine responded: A handful of years ago, I looked into the National Health and Nutrition Evaluation Survey, NHANES. To first order, the 1AC level defining pre-diabetes, 6.5, is rather close to the median level. So that's scam-adjacent. Every once in a while, my doctor thanks me for giving him this NHANES table. https://docs.google.com/spreadsheets/d/1g3Icgu0ixLtYCjscYoiC... As soon as you say this, someone will respond \"My father has diabetes and he had his foot amputated!\" This is not minimizing diabetes; it's questioning whether prediabetes is a thing. How is it different from saying \"Men over 60 have pre-prostate cancer?\" Or, \"we all have pre-death?\" You should have a regular blood test and your doctor should be monitoring a lot of things, including blood sugar. If the level is close to diabetes, he or she should warn you. But that's different from saying, \"you have a disease.\" reply thefifthsetpin 14 hours agoparentI have no credentials here, but I'm often working in this space and your take is the polar opposite of what I normally hear from the endos and diabetes researchers that I've worked with. More accurate terms might be diabetes and morbid diabetes. That the cutoff for prediabetes is close to the median level is a statement that much of the population is actually unhealthy in this regard. reply AlbertCory 14 hours agorootparentNo, it's a statement that the \"cutoff\" was chosen on questionable grounds. Defend it. reply guerby 14 hours agorootparentDid you check the litterature? https://pubmed.ncbi.nlm.nih.gov/20697688/ Hba1c 6.0 to 6.5 (so called prediabetic range) has 2.5 times the hasard ratio of below 6.0. Increasing Hba1c is the leading indicator of future health issues and by far. reply AlbertCory 13 hours agorootparentThis comes closer to a cogent answer, which is what I was after. > future health issues like? As I said, I'm not ready to publish this, but I wanted to hear some arguments that made sense. So thanks. To me, \"probability of progression from pre-diabetes to diabetes\" is the only reason to say it's a real condition rather than just a risk factor. Furthermore, it would be the derivative of the level, rather than the level itself. And finally, it would have to be separated from other unhealthy conditions. Meaning, if they drink too much AND have pre-diabetic A1C, ERROR! Otherwise, how do you know it's not the alcohol? reply mlhpdx 13 hours agorootparentGet peer review and publish; let’s see what happens. reply mlhpdx 13 hours agorootparentprevAccording to what I understand from researches in this area is that even a slight increase in A1C is highly correlated with later progression. You may not like the term, but it effectively communicates the situation. reply AlbertCory 13 hours agorootparent> You may not like the term I like numbers. Where are yours? reply tptacek 12 hours agorootparentConverse curiously; don't cross-examine https://news.ycombinator.com/newsguidelines.html reply AlbertCory 11 hours agorootparentthat begins with \"Be kind. Don't be snarky. \" I don't think that was snarky. You made some non-quantitative statements. reply pvg 11 hours agorootparentThose also end at their full stops, it's not 'you can act like an interrogator as long as it's not a snarky interrogator'. reply AlbertCory 10 hours agorootparent> even a slight increase in A1C is highly correlated with later progression. asking for the numbers behind those subjective statements is totally in line with HN guidelines and traditions. It isn't \"cross-examination.\" reply tptacek 9 hours agorootparentYou didn’t so much “ask”, though, right? You attempted to impeach an argument by appealing to a lack of citations. reply LiquidPolymer 14 hours agoparentprevI switched health providers and got a physical recently. One of my labs came back with \"pre-diabetic\" check marked. I asked my doctor about this and I was .1% into the metric. It could have been either side of the line. I'm very fit because I enjoy hiking and exploring. I already avoid sugar and bread. I work out nearly every day. I'm very active and my BMI is on point. My blood sugar was fine. It was another metric that I can't recall at the moment - but it seemed weird. Regardless, a very obese nurse sat me down and gave me tips for changing my diet. reply borski 14 hours agorootparentThis was likely your A1C, or a metric of the last three months or so of your sugar levels. Just because you feel healthy doesn’t necessarily mean you are healthy. You may have, for example, a pancreas condition. Or it may be nothing at all, and just something to track, which is why you never base any medical decisions off a single test. Your last sentence is nonsense. People can provide exceptional (and correct) advice without necessarily following it themselves, and for all you know this nurse has spent the last year actively trying to improve their health. reply AlbertCory 13 hours agorootparentIt's strictly a question of his likelihood P for developing diabetes. If his P is the same as that of other people with lower A1C, then \"prediabetes\" is nonsense. reply borski 13 hours agorootparent> Or it may be nothing at all, and just something to track, which is why you never base any medical decisions off a single test. reply Hello71 14 hours agoparentprev> pre-diabetes [...] is rather close to the median level but according to the NIH, 30.7% of Americans are overweight, and 42.4% are obese. as the median American is overweight, it doesn't seem a stretch to claim that the median American is also pre-diabetic? I don't know whether it's true or not, but your evidence seems a bit thin. reply strken 8 hours agorootparentAccording to the CDC, 38.0% of the adult US population is pre-diabetic. 11.6% of the total population have actual diabetes. It's very close to half. https://www.cdc.gov/diabetes/data/statistics-report/index.ht... reply AlbertCory 13 hours agorootparentprev> it doesn't seem a stretch to claim that the median American is also pre-diabetic No, it's your evidence that's thin. You seem to have started from the premise (\"Americans are unhealthy\") and derived a pre-diabetes level from that. reply shkkmo 10 hours agorootparentYou've literally provided zero evidence beyond hearsay from an alleged statistician friend. The prediabetes level is drawn from where you see a huge uptick in the risks for developing diabetes mellitus. You want data? There is plenty of it out there. If you're gonna try to dispute the established understanding, you need to bring evidence or you are just wasting people's time. reply nradov 12 hours agoparentprev [–] If you're going to try an appeal to authority, then at least come up with a plausible authority instead of some random statistician who likely doesn't know the basics of physiology. The first thing you need to understand is that normal ≠ healthy. For example, a resting heart rate of up to 100 bpm is considered \"normal\" in the sense that it doesn't require urgent medical intervention. But of course anything over 60 bpm for an adult usually indicates some underlying pathology with a risk of premature morbidity and mortality. The essence of type 2 diabetes is insulin resistance. Like many medical conditions it exists on a spectrum. The specific HbA1c thresholds of 5.7% for pre-diabetes and 6.5% for diabetes are inherently arbitrary and serve mainly to make communication easier. But there is a clear correlation between elevated HbA1c levels and higher all-cause mortality. https://doi.org/10.1136/bmjopen-2017-015949 Also note that HbA1c tests aren't perfect for diagnosing type 2 diabetes and can have false positives or false negatives. If there is reason to suspect diabetes then it would be wise to conduct additional tests to get a better understanding of the patient's metabolic condition. https://peterattiamd.com/ama15/ reply monkburger 10 hours agorootparent> Also note that HbA1c tests aren't perfect for diagnosing type 2 diabetes and can > have false positives or false negatives. When we do workups on pts suspected of diabetes, we use the following criteria. - Iron deficiency anemia workup to confirm accuracy of HbA1C - HbA1C ≥6.5% - FPG ≥126mg/dL (7.0 mmol/L). Fasting is defined as no caloric intake for at least eight hours - Two-hour plasma glucose ≥200mg/dL (11.1 mmol/L) during an oral glucose tolerance test - In a patient with classic symptoms of hyperglycemia or hyperglycemic crisis, a random plasma glucose ≥200mg/dL (11.1 mmol/L) I would like to redo the diagnostic criteria to include their BMI. reply AlbertCory 10 hours agorootparentExcellent. Thanks. Is an A1C level sufficient, all by itself, sufficient to classify them as \"pre-diabetic\" or are other symptoms required? Or is that even a thing? reply monkburger 4 hours agorootparentUsually, yes. However, some PCPs / endo will order up A1c as well as glucose challenge, depending on certain factors (eg, family history, BMI, symptoms). If I were you, order up the following: - A1C - 12 hour Fasting blood glucose levels - Two-hour glucose tolerance test: this test will measure blood glucose levels before and after ingestion of 75 g of glucose solution (if the results show blood levels that fall between 140 mg/dL to 199 mg/dL, it is diagnostic of prediabetes. - A random plasma glucose test Personally, if you have a family history of diabetes/obesity, you should get checked often. Some providers have been writing rx's of GLP-1's to aid in weight loss, which reduces the risk of developing T2DM. reply AlbertCory 11 hours agorootparentprev [–] > some random statistician who likely doesn't know the basics of physiology. since you know nothing about him, that's hardly called for. I could doxx him but he didn't consent to that. And he said his doctor thanked him for it. And you have fuzzed the difference between \"a disease\" and \"something to watch for.\" If ~50% of the US population needs to be watched, the doctor learns nothing by having a label put on their HbA1c level. The word added nothing to their understanding. > If there is reason to suspect diabetes then it would be wise to conduct additional tests to get a better understanding of the patient's metabolic condition. And finally, you just confirmed what I said. It's not \"a disease\" -- it's a risk factor. Like smoking, drinking, obesity, or sedentariness. The more everyone objects without any logical argument, the more it's confirmed: \"if that biomarker, all by itself, predicts type 2 diabetes better than random chance, in the absence of any other risk factor, we're entitled to call it a disease.\" reply nradov 6 hours agorootparent [–] Labeling insulin resistance as a \"risk factor\" is correct only in a narrow, technical sense. That's not helpful for patients and clinicians who need to make treatment decisions in the real world. The terminology is irrelevant. Instead of getting hung up on semantics you need to spend some time learning the basics of human metabolism and the endocrine system. Insulin resistance is a risk factor in the same way that a growing malignant tumor is a risk factor: unless you expect to die soon from something else you should take urgent measures to fix it. While a HbA1c test isn't perfect, it's pretty good as a cheap and easy initial screening for insulin resistance. If you want to believe that it's some kind of \"scam\" then I don't know how to help you. The average US person today has at least some level of metabolic problems. Playing games with statistics won't change that reality or the negative long-term outcomes. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The American Diabetes Association settled a lawsuit for allegedly accepting corporate funds in return for promoting recipes that could be harmful to individuals with diabetes.",
      "The ex-chief nutritionist, Elizabeth Hanna, filed the lawsuit, claiming pressure to endorse recipes with artificial sweeteners despite the associated risks.",
      "The settlement enabled the ADA to sidestep additional investigation and potentially damaging disclosures, revealing insights into the organization's conduct and connections to corporate interests."
    ],
    "commentSummary": [
      "The discussion explores Type 2 Diabetes, low-carb diets, and the American Diabetes Association's strategies, emphasizing the significance of lifestyle changes in managing the condition.",
      "It addresses the impact of artificial sweeteners on health, the complexities of diabetes management, and historical perspectives, discussing corporate influences on health organizations.",
      "Emphasizes the importance of accurate information, personalized treatment plans, and critical evaluation of prevailing medical beliefs in managing diabetes effectively."
    ],
    "points": 202,
    "commentCount": 182,
    "retryCount": 0,
    "time": 1714845173
  },
  {
    "id": 40259185,
    "title": "Verus: Verify Rust Code with Advanced Specifications",
    "originLink": "https://github.com/verus-lang/verus",
    "originBody": "Verus is a tool for verifying the correctness of code written in Rust. Developers write specifications of what their code should do, and Verus statically checks that the executable Rust code will always satisfy the specifications for all possible executions of the code. Rather than adding run-time checks, Verus instead relies on powerful solvers to prove the code is correct. Verus currently supports a subset of Rust (which we are working to expand), and in some cases, it allows developers to go beyond the standard Rust type system and statically check the correctness of code that, for example, manipulates raw pointers. Status Verus is under active development. Features may be broken and/or missing, and the documentation is still incomplete. If you want to try Verus, please be prepared to ask for help in the 💬 Zulip. Try Verus To try Verus in your browser, please visit the Verus Playground. For more involved development, please follow our installation instructions. Then you can dive into the documentation below, starting with the 📖 Tutorial and reference. Documentation Our (work-in-progress) documentation resources include: 📖 Tutorial and reference 📖 API documentation for Verus's standard library 📖 Guide for verifying concurrent code Project Goals Contributing to Verus License Getting in touch, reporting issues, and starting discussions Please report issues or start discussions here on GitHub, or join us on 💬 Zulip for more realtime discussions and if you need help. Thank you for using and contributing to Verus! We use GitHub discussions for feature requests and more open-ended conversations about upcoming features, and we reserve GitHub issues for actionable issues (bugs) with existing features. Don't worry though: if we think an issue should be a discussion (or vice versa) we can always move it later. We welcome contributions! If you'd like to contribute code, have a look at the tips in Contributing to Verus. Zulip sponsors free hosting for Verus. Zulip is an open-source modern team chat app designed to keep both live and asynchronous conversations organized.",
    "commentLink": "https://news.ycombinator.com/item?id=40259185",
    "commentBody": "Verified Rust for low-level systems code (github.com/verus-lang)200 points by gz09 16 hours agohidepastfavorite69 comments lsuresh 8 hours agoWe've used Verus to write formally verified Kubernetes controllers. Basically, we can prove liveness properties of the form \"eventually, the controller will reconcile the cluster to the requested desired state\". As you can imagine, there is a lot of subtlety and nuance to even specifying correctness here (think rapid changes to the desired state requirement, asynchrony, failures and what not). Code: https://github.com/vmware-research/verifiable-controllers/, with a corresponding paper due to appear at OSDI 2024. reply jph 9 hours agoprevIf you want a small stepping stone toward Versus, you can add Rust debug_assert for preconditions and postconditions; the Rust compiler strips these out of production builds by default. Example from the Versus tutorial with verification: fn octuple(x1: i8) -> (x8: i8) requires -16i8 { debug_assert(-16verifying the correctness of code What is the difference between \"verifying\" the correctness of code, as they say here, vs \"proving\" the correctness of code, as I sometimes see said elsewhere? Also, is there a good learning resource on \"proving\" things about code for working programmers without a strong CS / math background? Edit: I'm also very curious why \"zero knowledge\" proofs are so significant, and why this is so relevant. Eg I heard people talking about this and don't really understand why it's so cool: x.com/ZorpZK reply opnitro 12 hours agoparentA very good resource for both verifying code and functional programming is Software Foundations (https://softwarefoundations.cis.upenn.edu). One note though: Verus and the tool Software Foundations works with (Coq) take different approaches to proving things. Verus attempts to prove properties automatically using something called an SMT solver, which is an automated system for solving constraints. Coq on the other hand, requires you to manually prove much more, offering a more limited set of automations for proving things. Both have their advantages and disadvantages, namely that automation is great when it works and annoying when it doesn't. (Another side note: Zero Knowledge Proofs (ZKPs) are kind of something different. A great many people who work in formal verification/proof don't touch ZKPs at all (ex: me). They are better thought about as a cryptography primitive) reply dumbo-octopus 12 hours agoparentprevVerifying and proving are used synonymously, as made clear later in the opening paragraph. As for zero knowledge proofs, there is little practical use, significance, or relevance to them due to the overhead involved and the lack of a \"killer app\", so to speak. But they're conceptually interesting. reply vlovich123 11 hours agorootparentWhenever I hear people talk about the lack of practicality of some mathematical construct, I always remember G H Hardy who worked on number theory at the turn of the century. One of his famous quotes I love is: > I have never done anything 'useful.' No discovery of mine has made, or is likely to make, directly or indirectly, for good or ill, the least difference to the amenity of the world. Despite his self-proclaimed focus on pure mathematics, Hardy's work, particularly in number theory, has had profound applications in cryptography and other fields. I agree about the overhead. The costs have come down significantly already but they still remain a few orders of magnitude too large. That being said, it’s killer app is cloud compute. Right now the only way to amortize the cost of HW is to run it on someone else’s computer, which brings along with it all sorts of security and privacy risks. Well-performing ZK proofs (which we don’t know if it exists / it may be a long time before we figure out how to do it) would let you do cloud computing securely without worrying about vulnerabilities in your cloud provider’s network. Like all cryptography, it’s a double-edged sword because the same techniques would let websites deliver code for your machine to execute that you have no knowledge of what it’s doing. reply dumbo-octopus 9 hours agorootparentDo you ever think about the many more folks who worked on stuff that really was completely useless and nobody remembers? \"Whenever I hear people talking about how the lottery is a waste of money with net negative expected returns, I think about the person I saw on TV who did win.\" Cloud compute might be a good use case. But the overhead would have to come down dramatically for it to ever beat simply doing the compute in-house and incurring the overhead of managing a physical rack. Which is (and always has been, and likely always will be) an extremely viable option for anyone who is seriously interested in data access control. reply cyberax 4 hours agorootparent> But the overhead would have to come down dramatically for it to ever beat simply doing the compute in-house and incurring the overhead of managing a physical rack. A physical rack needs several physical people to look after it. Imagine it going down in the middle of the night while your sysadmin is on a vacation on Hawaii. And cloud computing is also not expensive. If you don't have a large load, a t4g.micro instance on AWS costs 0.5 cents per hour ($1.2 per day, $438 per year). It will run most small workloads just fine, and you can easily scale it up with a couple of clicks. reply dumbo-octopus 2 hours agorootparentZKP's are a large load, so this hypothetical is invalid. Also ZKPs require you to already have some amount of in house compute on an in house rack, or else who are you even protecting your data from? So the question is do you provision slightly more compute into the rack you're already managing, or incur orders of magnitude more CPU time expense and a complete refactoring of your business logic to use ZKP's in order to... have slightly fewer servers in your rack? The benefits are hard to even comprehend. reply vlovich123 3 hours agorootparentprevI think you may be overestimating the complexity & costs involved. Hyperscalars run millions of machines and have extracted impressive efficiencies from that that you’re not going to be able to compete with by buying and maintaining your own rack. This starts to become clear when you consider how ancient, outdated and expensive servers for hospital systems are. reply fweimer 50 minutes agorootparentThe hyperscalers are only efficient when compared to typical corporate IT departments, and what they can buy on the open market. For example, some analysts suggest that AWS EC2 has an operating margin of around 30%. If we take what off-brand cloud providers charge (who compete mostly on price) and compare that to the EC2 pricing, AWS must have some tremendous inefficiencies if they only have 30% margins. Even when considering network bandwidth (where hyperscalers are famously expensive, and orders of magnitude above wholesale pricing), hyperscalers still be might be the cheaper option for larger public companies because of the structural issues they face internally (e.g., the inability to make it official that they are already running an ISP anyway, and start buying much cheaper carrier services from other ISPs). reply inkyoto 6 hours agorootparentprev> But the overhead would have to come down dramatically for it to ever beat simply doing the compute in-house and incurring the overhead of managing a physical rack. Out of sheer curiosity, by which metric deploying the pre-packaged code (which takes seconds) into the cloud incurs a higher overhead compared to maintaining an in-house physical rack? reply dumbo-octopus 2 hours agorootparentWhen the code has to use ZKP's, there is an intrinsic overhead measured in orders of magnitude of CPU time. By avoiding the use of third parties, that overhead is eliminated. So the question is, does this hypothetical entity with very precious data incur the extra orders of magnitude of compute time in oder to not maintain a rack in house, or do they just... maintain a rack. Keeping in mind, of course, that they already must maintain a rack in order for ZKP's to make any sense at all. So... do they add a server to their existing rack to handle the workload, or refactor their entire computation to use ZKP's, and incur several extra orders of magnitude of compute time, and outsource the work to a cloud service? reply phanimahesh 5 hours agorootparentprevUsually when you need to do it many times at scale, self managed rack can be cheaper than cloud. The downside is requiring expertise to manage and maintain it. reply inkyoto 4 hours agorootparentThe cost of deployment into the cloud is $0 at a scale that is unfathomable to any rack with the overhead being a near zero as well. Initial capital investment for the cloud is $0 as well. The self-managed rack: 1. Has to exist somewhere. 2. Therefore it required the initial capital investment at some point, which is a mid five figure amount or more. 3. Scales by its weight, height, depth and hardware specifications. 4. Does not scale beyond (3) – hardware constraints of blades deployed in the rack are very rigid. 5. Has to have available processing capacity to support the new workload. 6. Has a habit of running of capacity at the most inconvenient moment requiring, well, a new rack. A new/extra rack: 1. Has to be paid for from a budget. The budget may or may not exist when the new rack is required, therefore potentially incurring further, potentially lengthy, delays (i.e. «no money in the budget until the next financial year». Boom.). 2. Has to be taken through the procurement. Depending on the organisation and its size, procurement can take anywhere in between 3 and 12 months. 3. Has to be in stock. 4. Has to be installed, deployed and configured. 5. Requires technicians and engineers to be available within a designated time slot to complete (4). The technicians and the engineers (all of them or a single/few) may be unavailable due to X, Y or Z. Bonus points: 1. The DC may not have enough room / power / cooling / etc. You and your rack are now stuck for an non-deterministic amount time. 2. Adding a new rack to your rack HA setup requires a new network interconnect due to the network capacity reaching a saturation point. It is called «an expensive network switch». Add further delays, repeat all steps required to procure a new rack, add new/unforeseen delays. With the above in mind, I fail to see how the overhead of a poorly scalable, self-managed rack is lower compared to a $0, software driven code deployment into the cloud at a scale that is limited by the size of one's wallet. reply dumbo-octopus 1 hour agorootparentAh yes, the cloud, where network interconnect issues simply do not exist, and extra capacity is always available, and the budget never runs dry, and configuration is not required, and technicians and engineers are always available. Can I have a puff of whatever it is you smoked to reach this cloud? reply dns_snek 32 minutes agorootparentYour comment is inflammatory but you're not wrong. Selling the cloud as $0 cost to set up is at best, a fairy tale and at worst, an ad. If \"the cloud\" was so simple, there wouldn't be 6-7 figure job positions dedicated to just setting up and managing their complexity and they definitely wouldn't come with certification requirements. Somehow the requirement of having a small team of employees \"who can manage our own infra\" is a deal-breaker but having a team of AWS Solution Architects is not. reply Nursie 5 hours agorootparentprev> by which metric deploying the pre-packaged code (which takes seconds) into the cloud incurs a higher overhead compared to maintaining an in-house physical rack? In terms of time? Probably not much. In terms of cost, we’ll it’s about scale. I worked on a project for a big financial firm about 8 years ago, to implement a burst-capacity overflow to cloud strategy for their compute needs. About one week a month or even one week a quarter, they would need a few tens of thousands of extra cores. At this scale the cloud providers were not really able to react or provision fast enough, and required us to reserve the capacity full time. In the end the costs for “buy and manage more racks” internally worked out cheaper. reply inkyoto 4 hours agorootparent> […] to implement a burst-capacity overflow to cloud strategy for their compute needs. 8 years ago the cloud did not effectively exist yet. Today the available capacity surpasses the needs of the prevailing majority of cloud customers, with a caveat – see below. > At this scale the cloud providers were not really able to react or provision fast enough, and required us to reserve the capacity full time. A few things have happened since then: 1. Hardware capacity has been ramped up at a very large scale. 2. Most importantly, advances in the workload distribution and optimisation: the cloud control plane distributes workloads to the available capacity across cloud data centres. The caveat. For most demanding customers, cloud providers have a solution called «the cloud appliance». A truck rolls in with the hardware that gets deployed as an extension of your own private cloud within a few days. Deployed software does not notice a difference and gets automatically distributed to the new processing capacity. If that is not enough, another truck rolls in with another «cloud appliance». It is presumed that if one operates at such a scale, they also have the money to pay for it. reply Nursie 2 hours agorootparent> 8 years ago the cloud did not effectively exist yet Yeah it really did. I’ve been around a while. “Cloud” was becoming a buzzword around the tech industry 16 years ago. 8 years ago we had AWS, IBM, and Microsoft all vying for our business on this project. > For most demanding customers, cloud providers have a solution called «the cloud appliance». A truck rolls in with the hardware that gets deployed as an extension of your own private cloud within a few days None of which was as cost effective as adding more capacity to your own data centres, if you’re already running them at scale, because fundamentally someone is profiting from renting out those cloud appliances. If you have the in-house capabilities already, you can cut out the middlemen. reply Ar-Curunir 10 hours agorootparentprev> As for zero knowledge proofs, there is little practical use, significance, or relevance to them due to the overhead involved and the lack of a \"killer app\", so to speak. But they're conceptually interesting. This is just plain wrong. There are numerous applications in the blockchain space where the overhead is tolerable because they provide properties (privacy, succinct verification) that other techniques cannot provide. Their relevance to blockchains has led to massive reductions in the overhead, making it quite plausible that ZKPs will find use in settings where integrity is a must. For example, you could prove that a particular binary is the resulting of compiling source code that satisfies particular properties, such as memory safety. This would allow you to skip unnecessary safety checks. reply dumbo-octopus 8 hours agorootparentFor a tool to be of \"practical use, significance, or relevance\" requires that the work it produces also is of \"practical use, significance, or relevance\". Blockchains are not. And the best way to verify a compiled binary is correctly compiled is to do the compilation. That's a trivial amount of work. Constructing a ZKP of the same is certainly more cost intensive. reply A1kmm 2 hours agorootparent> Constructing a ZKP of the same is certainly more cost intensive. That is certainly true for the compilation phase (i.e. constructing the ZKP). But for the verifier, that isn't so obviously true: - Firstly, the verifier might not even have access to the formal proof that the program meets the memory correctness guarantees. It might have access to a binary code, plus a proof that the binary code compiled from a completely memory safe language. The verifier would need zero knowledge of the code (which might be proprietary). - Secondly, proofs aren't necessarily that big (especially for Groth16 circuits), and can be applied recursively (you prove that you had a proof that something was true), and aren't that expensive to verify. If verifying a program once when you download it means it can be flagged as being safe to run without expensive bounds and MMU checks, it could open up new types of more performant CPUs, and potentially save far more than it costs to verify. reply sabas123 2 hours agorootparentprev> And the best way to verify a compiled binary is correctly compiled is to do the compilation. That's a trivial amount of work. Constructing a ZKP of the same is certainly more cost intensive. This makes the assumption that you both have the source code and that the compiler is deterministic. With the latter one being not the case for (most?) modern compilers. reply OJFord 3 minutes agorootparentIt doesn't assume the compiler is deterministic, because you don't need to use the result for verification, you can just use the result. dumbo-octopus 1 hour agorootparentprevAh yes, the totally-valid use case of ensuring that a third party has properly compiled source code that you don't have access to. I'm sure this comes up in industry all the time. reply yodsanklai 10 hours agoparentprev> \"proving\" things about code for working programmers I'd argue that this is antinomic. Proving things about code isn't something working programmers do yet. I'd say that Hoare logic is a good starting point as it is sometimes taught in introductory CS classes. Coq has a steep learning curve, especially if you're not familiar with OCaml or similar languages. Maybe Why3 is more beginner friendly https://www.why3.org Proving vs verifying: could mean the same thing. Proving seems to me as something more interactive in nature, while verifying could be automatized (model checking, SMT-solving of annotated programs). reply teaearlgraycold 10 hours agorootparentWorking programmers write proofs in a limited sense. Any time you write types you're writing a proof. Maybe it's a stretch to say \"const a: int = b;\" is a proof, but when you get into higher-order types in TypeScript it's appropriate. reply samatman 7 hours agorootparentA trivial proof is still a proof. There's nothing trivial about \"const a: int = foo();\" though. Compilers disprove the claim by contradiction all the time. reply wk_end 12 hours agoparentprev> Also, is there a good learning resource on \"proving\" things about code for working programmers without a strong CS / math background? I don’t know how difficult Software Foundations is for someone with a limited background but it’s probably worth trying: https://softwarefoundations.cis.upenn.edu/ reply A1kmm 2 hours agoparentprev> What is the difference between \"verifying\" the > correctness of code, as they say here, vs \"proving\" > the correctness of code, as I sometimes see said > elsewhere? There is not much difference, except that verification usually includes identifying a formal logical proposition about the behaviour of the code. In other words, formally verified code is code that has been proven to meet at least one formal proposition about its behaviour - for example, a proposition about a function f might be: if variable x is greater than 0, then `f(x) = 1`. There is no such thing as proving something 'correct', you need someone to define what exactly correct means, and then someone proves it meets that definition. So the proving is only a subset of the overall formal verification task. > Also, is there a good learning resource on \"proving\" things about code for working > programmers without a strong CS / math background? Most will be specific to a particular technology and type of verification. There are some courses online that provide a high level overview, e.g. https://anton-trunov.github.io/csclub-coq-course-spring-2021.... If you want to get into specifics, you might need to pick an approach. You could learn a dependently typed language, for example there are some good resources out there on proving things in Agda or Idris. Or perhaps play around with one of the formal verification systems that can be bolted on to C or Rust. > Edit: I'm also very curious why \"zero knowledge\" proofs are so significant, and why this > is so relevant. Eg I heard people talking about this and don't really understand why it's > so cool: x.com/ZorpZK ZK is an exciting area of cryptology because breakthroughs in that area power new applications that people wouldn't have thought possible before. Applications to cryptocurrencies in particular can solve some of their scaling and privacy problems. For example, one of the biggest problems with cryptocurrencies is that every single transaction ever needs to be recorded in a ledger that is distributed to every node participating in the network. That simply won't scale to microtransactions. Let's say that 1000 people each start with 1 coin, and do 100,000 small transactions averaging 0.001 coins amongst themselves (maybe they bought a coffee, or paid for information on a per-view basis, or whatever). Storing those 100,000 transactions forever will have an ongoing cost for every one of thousands of nodes on the network long after the transaction has happened. Now that could be solved with centralisation - the parties send their transactions to a trusted company, who maintains balances for each of them without sending transactions to the network, but lets them withdraw their balance to the network if they ever want to. But centralisation is a risk - what if the company betrays their trust? Zero-knowledge cryptography allows for the parties to roll up the signed transactions into a cryptographic proof saying, given these were the balances at the start of the 100,000 transactions, the person creating the roll-up has access to the signed transactions proving that the balances of each of the 1,000 parties at the end are this. Notably, the proof can be much smaller than the size of the 100,000 transactions. So that enables applications where people work off in 'side chains', and but can merge the side chain back into the main chain by submitting proof about the effects (but not all the detail of) the side chain into the main chain. reply binary132 11 hours agoparentprevVerification is proving specific things about specific properties of the program. reply LoganDark 12 hours agoparentprevAFAIK, zero-knowledge proofs allow you to prove that you know something without revealing what it is you know. For example, verifying you know a password, but without having to send the password to the server, so a malicious server or MitM wouldn't be able to sniff it. It also might provide better options for identity verification, i.e. proving you have a certain government-issued ID but without actually leaking the document to the server (for it to be stored \"for a maximum of 2 years / 3 years / 6 months / etc.\" but then leaked in a data breach anyway). reply dumbo-octopus 12 hours agorootparentSecure password verification does not require anything close to what we mean when we refer to modern \"zero knowledge proofs\". A better example would be verifying you know the answer to a SAT problem without sharing the answer. Which is of... limited.. practical application. reply vlovich123 11 hours agorootparentZK proofs could be used to implement secure cloud computing where a total compromise of a vendor’s machines / network wouldn’t in any way compromise the customers running workloads on that network. reply dumbo-octopus 9 hours agorootparentPerhaps. But if someone cares that much about data access control, they can very easily manage a rack in house (they probably already do), and do the compute on that. Paying someone else to do orders of magnitude more work than strictly needed when you have private enough data to justify just buying a server (again, you already have one) is a very hard sell. It could maybe, possibly be justified if the workload is highly burstable, but... even then, it's hard to come up with numbers that make sense. reply LoganDark 8 hours agorootparentprevIs homomorphic encryption a type of ZKP or is it a different thing entirely? reply vlovich123 3 hours agorootparentGood question. I found [1] but I’m not a cryptographer so I may have made mistake because they’re so closely related. It sounds like homomorphic encryption is the strong aspect. ZKP are used in securing chats in Signal for example: https://eprint.iacr.org/2019/1416.pdf [1] https://crypto.stackexchange.com/questions/57747/what-is-the... reply LoganDark 12 hours agorootparentprevI never said it required ZKP. reply dumbo-octopus 12 hours agorootparentYou implied it would be a helpful tool for soling that particular problem, which is to dramatically misrepresent their utility to the parent asking for concrete examples of their relevance. reply IshKebab 12 hours agoparentprev> What is the difference between \"verifying\" the correctness of code, as they say here, vs \"proving\" the correctness of code, as I sometimes see said elsewhere? In this context it is the same. > Also, is there a good learning resource on \"proving\" things about code for working programmers without a strong CS / math background? I wish. The Dafny docs are pretty good but IMO formal software verification is not really at the point where it is usable for normal programmers like us who don't have a PhD in CS / maths. The examples make it look relatively easy, but you will quickly run into \"nope, couldn't prove it\" and the answer as to why is some hardcore implementation detail that only the authors would know. reply Animats 7 hours agorootparent> IMO formal software verification is not really at the point where it is usable for normal programmers like us who don't have a PhD in CS / maths. I know. Four decades ago I headed a project to build a system amazingly similar to this one, intended for real-time automobile engine control code.[1][2] This new system for Rust looks practical. It seems to be intended for people who need bug-free code. Most verification systems come from people in love with formalism. Those involve too much human labor. Hints: - The assertions and invariants need to be part of the language. Not something in comments, and not different syntax. They should be syntax and type checked during compiles, even if the compiler doesn't do much with them. - It's useful to work off the compiler's intermediate representation rather than the raw source code. Then you're sure the compiler and verifier interpret the syntax the same way. - SAT solvers aren't powerful enough to do the whole job, and systems like Coq are too manual. You need two provers. A SAT solver is enough to knock off 90% of the proofs automatically. Then, the programmer focuses the problem by adding assertions, until you get the point where you have assert(a); assert(b); and just need to prove that a implies b as an abstraction mostly separate from the code. Then you go to the more elaborate prover. We used the Boyer-Moore prover for that. After proving a implies b, that became a theorem/rule the fast prover could use when it matched. So if the same situation came up again in code, the rule would be re-used automatically. I notice that the examples for this verified Rust system don't seem to include a termination check for loops. You prove that loops terminate by demonstrating that some nonnegative integer expression decreases on each iteration and never goes negative. If you can't prove that easily, the code has no place in mission-critical code. Microsoft's F* is probably the biggest success in this area.[3] [1] https://archive.org/details/manualzilla-id-5928072/page/n3/m... [2] https://github.com/John-Nagle/pasv [3] https://www.microsoft.com/en-us/research/video/programming-w... reply dumbo-octopus 12 hours agorootparentprevFormal software verification encompasses a wide set of what we refer to as advanced mathematics. The process of taking some thing you want the code to do, correctly abstracting it out into component lemmas, and generating proofs for those lemmas, is itself advanced mathematics. I don't really see a way that this could be simplified. reply fweimer 31 minutes agorootparentIs it really that advanced? Do you actually need more logic tools than what was used in the 19th century to formalize real analysis? Things get complicated if you want to put the proofs into the types, but I don't think this is the main approach the industry uses for certifying software for high-integrity applications (in cases there is a requirement for formal verification, which seems to be less and less the case). I don't think it's the mathematics, it's the tools. If the proof is separate and needs to be kept aligned with the specification and implementation manually, that's not really useful except for niche applications. Integrated approaches (like Verus here, or SPARK in the Ada context) should solve the implementation/proof gap, but such tools are not widely available for languages people actually use. reply screcth 7 hours agorootparentprevFormal hardware verification has been much more successful in industry. Are there fundamental differences in the types of problems that they solve? EDA companies have been able to create tools that don't require a PhD in Math to be used effectively. I think that the explanation for such different levels of success is that economic incentives are different. The cost of a hardware bug is much much higher than the cost of the average software bug; this means that hardware companies are willing to spend a lot of money in getting designs right the first time, versus software companies that know they can always make bug fixes in new versions of their products. Additionally, hardware companies are used to paying millions of dollars in software licenses, which is not common in the software world. reply IshKebab 3 hours agorootparentYeah I agree formal hardware verification is an order of magnitude easier to use. My guess is it's because the problem is so much simpler. No variables, no loops, no recursion, etc. reply dumbo-octopus 1 hour agorootparentYes, the challenge in any formal software verification is dealing with unbounded inputs and compute durations, as enabled by recursive programs. If the code under analysis is straight-line non-reentrant basic logical blocks, the verification is quite trivial indeed. This is vaguely the case for hardware verification, though of course there are complexities introduced by the physical nature of the embedding. reply opnitro 10 hours agorootparentprevThis is why I think interactive proof assistants (as opposed to \"automatic\" ones like Dafny), are a better starting point for learning. You're still gonna need to learn some higher level concepts, but you won't have the frustrating experience of the automation just failing and leaving you shrugging. Software Foundations is great: https://softwarefoundations.cis.upenn.edu If you stick with it long enough, you'll even build up to Hoare logic which is the underpinning the tools like dafny use to generate the equations they throw to the solver. reply algorithmsRcool 7 hours agorootparentI've found the DX of Dafny to be very approachable. The VSCode extension is pretty great, and you get feedback as you type. Also, its ability to give you counter examples in the IDE is really nice. reply ComputerGuru 10 hours agoprevFor those that are interested but perhaps not aware of this similar project, Dafny is a \"verification-aware programming language\" that can compile to rust: https://github.com/dafny-lang/dafny reply algorithmsRcool 9 hours agoparentAlways cool to see Dafny mentioned! Shameless plug: I just wrote a beginner's introduction to Dafny a few days ago. https://www.linkedin.com/pulse/getting-started-dafny-your-fi... reply camkego 2 hours agoprevCould someone familiar with Verus comment on the power and expressiveness of Verus vs. Lean4 I understand Verus is an SMT based verification tool, and Lean is both an interactive prover and SMT based tool. But my understanding in the area of formal verification is limited, and it would be good to get an opinion from someone well versed in formal methods for software. reply dist1ll 11 hours agoprevOne of the main contributors gave an excellent talk [0] on Verus at the Rust meetup in Zürich. I was really impressed how clean this \"ghost\" code fits into programs (reminded me a bit of Ada). [0] https://www.youtube.com/watch?v=ZZTk-zS4ZCY reply lifeinthevoid 2 hours agoprevIs there some way to implement this so that the resulting code is still valid Rust code that can be compiled using vanilla Rust tools? reply IshKebab 2 hours agoparentIt is valid Rust... but only because it wraps everything in a proc macro. Creusot does it in a different way using attributes, which IMO is a better approach because it means normal tooling works, though it does have much worse syntax. reply TachyonicBytes 11 hours agoprevIs there any relationship between this and Kani[1]? Do they work differently? [1] https://github.com/model-checking/kani reply mmoskal 11 hours agoparentModel checkers typically only explore a bounded number of states which is efficient at bug finding and often doesn't require additional annotations in the program. Automatic (SMT-based) verifiers like Verus, Dafny, F* (and my VCC :) require you to annotate most every function and loop but give you broad guarantees about the correctness of the program. Tools based on interactive provers (like Coq or Lean) typically require more guidance from the user but can guarantee even more complex properties. reply im3w1l 11 hours agoprevThis looks really cool. One thing I think would be really useful for people is some instructions / examples of how to add proofs for an existing codebase. So maybe an example could be a bare-bones gui app with a single textbox, that does an http request to some resource (having data that is unknown at compile-time and potentially untrusted is a very common thing) and fetches an array, which is bubble-sorted and displayed in the box. The bubble sort has some intentional bug (maybe due to some off by one error, the last element remains untouched). There are unit-tests that somehow don't trigger the bug (worrying that your tests are incomplete would be a primary motivator to go for proofs). It could then show how to replace the unit tests with a proof, in the process discovering the bug and fixing it. The example wouldn't need to go into huge detail about the proof code itself as it is potentially advanced, instead it would focus on the nitty-gritty details of adding the proof, like how the interface between proved mathematical code and non-proved io code works, what command line to run to prove&build, and finally a zip archive with all of that, that you can play around with. Edit: Actually just reading from stdin and writing to stdout is probably good enough. reply nullorempty 7 hours agoprevHm, so you write the code twice :) reply PhilipRoman 2 hours agoparentYou're not wrong, but formal verification is still useful for multiple reasons: 1. Cases where specification is much less complex than implementation, like proving a sorting algorithm - the spec is very simple, forall integer i,j : i result[i]<=result[j] plus the requirement that elements may not be removed or added 2. Ability to eliminate checks for improved performance (not sure if this applies to Rust yet, but it works great with Frama-C). 3. \"Unit tests\" for entire classes of behavior, not just specific inputs. Even if you cannot write a formal specification for a huge complex protocol, you can incrementally add asserts which cover much more area than simple unit tests. reply jpc0 3 hours agoparentprevYou make implicit assumptions you had during development explicit through code or comments which doesn't actually effect runtime execution speed since it only runs in debug/compile time. There's a place for formal verification, usually in places where a bug causes death or significant financial loss. reply IshKebab 12 hours agoprevInteresting! Looks most similar to Creusot. The syntax is definitely nicer but wrapping your entire code in a macro surely is going to upset rust-analyzer? reply jaybosamiya 11 hours agoparentA fork of rust-analyzer, called verus-analyzer, provides support for Verus syntax and actions (including new proof-specific actions) https://github.com/verus-lang/verus-analyzer/ reply tsujamin 12 hours agoparentprevI’m not sure how rust-analyser works, but presumably you’d make the macro a no-op and just return the original tokens in debug builds reply jimsimmons 12 hours agoprev [–] What exactly do SMT systems \"solve\" in cases like this? If I wrote a simple BFS or DFS and enumerated the search space how far would I get.. Is that not what TLA+ does in principle. I am surprised people prefer having a dependency of something like Z3 at compiler level. reply tkz1312 57 minutes agoparentSMT solvers use a decision procedure known as CDCL(T). This uses a SAT solver at the core, which operates only on the core propositional structure of the input formula, and dispatches higher level constructs (e.g. functions, arrays, arithmetic) to specialized theory specific solvers. This is an extension of the CDCL (conflict driven clause learning) approach to SAT solving, which is a heuristic approach that uses information discovered about the structure of the problem to reduce the search space as it progresses. At a high level: 1. assign true or false to a random value 2. propagate all implications 3. if a conflict is discovered (i.e. a variable is implied to be both true and false): 1. analyze the implication graph and find the assignments that implied the conflict 2. Add a new constraint with the negation of the assignment that caused the conflict 3. backtrack until before the first assignment involved in the conflict was made The theory specific solvers use a diverse set of decision procedures specialized to their domain. The “Decision Procedures” book is an excellent overview: http://www.decision-procedures.org/ reply sunshowers 10 hours agoparentprevSAT is an NP-complete problem. Doing an exhaustive search is very time-consuming. An SMT or SAT solver uses heuristics to make that process quicker for practical problems. It looks like there are some TLA+ implementations that do use SMT solvers under the hood. reply IshKebab 12 hours agoparentprev [–] They try to find a counter-example to the constraints you have set up, or tell you that no such counter-example exists, in which case your program is correct. The counter-example is in the form of inputs to your program or function. It looks like the TLA+ Proof System does the same thing, but I believe you can also use TLA+ in \"brute force all the states\" mode. I haven't actually used it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Verus is a tool that verifies Rust code correctness by enabling developers to write specifications checked by solvers, supporting a subset of Rust and extending beyond the standard type system.",
      "The project is actively developed, with incomplete documentation; users are advised to seek help on Zulip, aiming to verify concurrent code and welcoming contributions.",
      "Users can experiment with Verus in-browser or pursue installation instructions for in-depth development, communicating and reporting issues on GitHub or Zulip."
    ],
    "commentSummary": [
      "The text examines Verus for formally verifying Kubernetes controllers, distinguishing between code verification and proof of correctness, and the importance of zero-knowledge proofs in cloud security.",
      "It analyzes the cost-effectiveness of cloud services versus physical infrastructure and the practical applications of zero-knowledge proofs in blockchain and password security.",
      "The discussion covers formal software verification, leveraging tools like Dafny and Verus, and utilizing interactive provers such as Coq and Lean for ensuring code accuracy, alongside tools like verus-analyzer, SMT solvers, and TLA+ for tackling intricate problems."
    ],
    "points": 200,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1714845053
  },
  {
    "id": 40260259,
    "title": "GPUDeploy.com: Revolutionizing GPU Access and Efficiency",
    "originLink": "https://www.gpudeploy.com",
    "originBody": "Hi HN,YC w24 company here. We just pivoted from drone delivery to build gpudeploy.com, a website that routes on-demand traffic for GPU instances to idle compute resources.The experience is similar to lambda labs, which we’ve really enjoyed for training our robotics models, but their GPUs are never available for on-demand. We are also trying to make it more no-nonsense (no hidden fees, no H100 behind “contact sales”, etc.).The tech to make this work is actually kind of nifty, we may do an in-depth HN post on that soon.Right now, we have H100s, a few RTX 4090s and a GTX 1080 Ti online. Feel free to try it out!Also, if you’ve got compute sitting around (a GPU cluster, a crypto mining operation or just a GPU) or if you’re an AI company with idle compute (hopefully not in a Stability AI way) and want to see some ROI, it’s very simple and flexible to hook it up to our site and you’ll maybe get a few researchers using your compute.Nice rest of the week!",
    "commentLink": "https://news.ycombinator.com/item?id=40260259",
    "commentBody": "gpudeploy.com – \"Airbnb\" for GPUs (gpudeploy.com)182 points by nicowaltz 12 hours agohidepastfavorite116 comments Hi HN, YC w24 company here. We just pivoted from drone delivery to build gpudeploy.com, a website that routes on-demand traffic for GPU instances to idle compute resources. The experience is similar to lambda labs, which we’ve really enjoyed for training our robotics models, but their GPUs are never available for on-demand. We are also trying to make it more no-nonsense (no hidden fees, no H100 behind “contact sales”, etc.). The tech to make this work is actually kind of nifty, we may do an in-depth HN post on that soon. Right now, we have H100s, a few RTX 4090s and a GTX 1080 Ti online. Feel free to try it out! Also, if you’ve got compute sitting around (a GPU cluster, a crypto mining operation or just a GPU) or if you’re an AI company with idle compute (hopefully not in a Stability AI way) and want to see some ROI, it’s very simple and flexible to hook it up to our site and you’ll maybe get a few researchers using your compute. Nice rest of the week! ezoe 1 hour agoIf you let third party stranger to offer GPU resource, how do you deal with: 1. Privacy. An attacker can set up a GPU honey pot and and sell the data they got. 2. Fake GPU computation. An attacker can fake GPU and send back dummy data sometimes to reduce the computation. 3. Corrupt GPU. Practically same with 2. But not malicious intent. It's just the faulty GPU. reply mnahkies 1 hour agoparentI know Azure has a confidential computing offering for GPUs, which I'm hoping will get broader uptake soon. It seems like the best way to address these concerns to me. https://azure.microsoft.com/en-us/blog/azure-confidential-co... reply TazeTSchnitzel 57 minutes agoparentprevThere's a privacy issue in both directions too: the tenant will be afraid their workload will be stolen, and the host will be afraid other data in their system will be stolen (if the sandboxing is imperfect). reply courseofaction 6 minutes agoparentprevReputation reply yeldarb 11 hours agoprevLove the concept. I've used vast.ai (similar \"Airbnb for GPUs\" pitch) for years to spin up cheap test machines with GPUs you can't really find in the cloud (and especially consumer-grade GPUs like 4090s). Any insight into how this is different/better? reply nicowaltz 8 hours agoparentMain difference is that we are more opinionated (in terms of configurations) and sort of do the scrolling and sorting out for you – hopefully a bit smoother as a user experience. We sort out bad machines immediately. We're also directly working on making compute from unknown high-end data centers available, there's a lot of unused compute out there! See gpulist.ai Also, don't know if vast.ai does this, but with us you can have 6 user sessions on your machine if you have six GPUs, so granular utilization is possible. reply bradfox2 6 hours agorootparentWe own and operate 40+ data center GPUs (v100s, a100s, and ax000s) in a private cluster and use vast to rent unused capacity. What would make you better than vast is extremely easy spot leasing and job prioritization. I want to be able to have one of our training jobs finish, and then have the capacity immediately transition to a lease. With vast, we are renting in week long blocks. reply nicowaltz 6 hours agorootparentExactly, that's the idea reply bradfox2 3 hours agorootparentIs it implemented? reply bigcat12345678 5 hours agorootparentprevYou can do that on llm.sxwl.ai Shoot me an email at z@sxwl.ai for instructions, the web site is pretty outdated, the main UI is through restful API (which we don't have time to write doc yet) reply mkl 34 minutes agorootparentIf you have time to answer emails with instructions, you have time to update your site and documentation. Why not just do that? reply ganoushoreilly 11 hours agoparentprevIm also interested in what the differing factor is. Would also like to see more documentation for onboarding rather than just \"Ubuntu and root available\". reply icelancer 8 hours agorootparentvast you have to choose specific machines. gpudeploy routes to whatever resources are available. vast has a lot of bad machines with terrible PCIe lanes and architecture you have to learn the hard way. Someone on HN wrote a script to run a test docker image on every machine and auto-tagged the machines' quality using their API, which is what I'd do if I was going to use vast seriously for compute. reply firloop 11 hours agoparentprevI use vast.ai somewhat often. It's great! reply tehsauce 8 hours agoparentprev+1 for vast. they usually are the cheapest and have the most supply. some instances can be less reliable at the low end though reply nirvanatikku 10 hours agoprevGreat idea and wishing you the best of luck! Dropping a note that I've found https://akash.network/ ~ https://akash.network/gpus/ to be impressive, as typically with crypto projects it's all scams, however in this instance there's demand and legit usage. https://stats.akash.network/ Something to consider! reply lordofgibbons 6 hours agoparentI was excited to see their offering, but unfortunately they don't take fiat - only some specific cryptos. reply nirvanatikku 5 hours agorootparentFiat proxied through USDC - a stablecoin offered by [Circle](https://www.circle.com/en/usdc). reply cdchn 2 hours agoprevIf I got an RTX 4090 sitting around idle, how much can I get for it? reply qeternity 29 minutes agoparentYou should be able to get 40-50 cents/hour. reply ant6n 59 minutes agoparentprevThat’s what I wondered, too. I feel the pricing page should be like a currency trader, showing a list of sell at/buy at. reply keefle 2 hours agoprevNeat Idea! I was wondering, are there any security guarantees for the providers? Assume I have a small GPU cluster at home, if I rent out my GPUs, what sort of access should I accept the renters to have? Only GPU kernels would be sent to machine? Or will they have a limited permission user access on my cluster? Also instead of having the operatos openning ports in their routers, were there any considers of adding them to a private network for a more seamless experience? (Nebula/headscale and the likes) reply bllchmbrs 11 hours agoprevFor those that wants prices checkout: https://gpumonger.com/ This seems much more in depth, and a true service, but for those that just want to compare prices check out gpumonger. reply latchkey 9 hours agoparentThere is no 'contact us' page, which is a little concerning. reply Ocha 6 hours agorootparentThey are not providing actual service - just price comparisons across different providers. Why is contact us page missing a concern here? reply latchkey 6 hours agorootparentI'm a provider, I'd like to list my service. How do I do that? Since there is nothing obvious, then how do you know the providers they list are a valid representation of what is out there? Maybe there are other providers who don't want to pay to be listed? But how do you even pay? reply mike_d 3 hours agorootparentThey list over 40 vendors. If you want to be listed, maybe get bigger? reply LoganDark 1 hour agorootparentWho says they're even looking? Being able to contact them would tell you right away whether they simply overlooked you or whether you don't yet meet some criteria (which they might actually tell you). reply CaptainFever 2 hours agoprevWhat about privacy? Can I run sensitive data on these strangers' GPUs? reply EE84M3i 11 hours agoprevWhat's the security story here? reply lschneider 8 hours agoparentAt the moment, we manually verify operators and are currently onboarding some tier-4 operators. Down the line, we'll have a 2-tier system where you can choose whether you want a verified machine or not. From the operator's perspective, everything runs inside Docker, configured with security best-practices. reply lolinder 8 hours agorootparentI've always understood that containers are not proper sandboxes and shouldn't be used for containing untrusted code, no matter the best practices used. Has this changed in recent years? Do you have documentation for what sorts of best practices you're using and why they are sufficient for executing untrusted code? reply gavindean90 7 hours agorootparentYou are correct from my knowledge. I would expect that if the container is set to not run as root you might be able to enforce fine meaningful security but I’d still run it in a VM if feasible. reply remram 9 hours agoparentprevWhat's preventing GPU providers from sending wrong results instead of actually running the computation? For example, send the last computed result? Is this something that the renter has to handle by adding their own checks? In addition to the problem of the renter crashing your machine or reading your password through DMA, of course. reply greenish_shores 1 hour agorootparentLinux supports IOMMU on most platforms. reply latchkey 8 hours agorootparentprevWhat incentive would a GPU provider have to spend time figuring out what result to send for some custom application? reply greenish_shores 1 hour agorootparentRun 1/10,000 - 1/100,000 of computations locally, and also send them as tasks to be send remotely. If compare yields difference, repeat both. After, say, 10 tries, blacklist the provider. Of course it will take a lot more nuances to implement that, but that's the general idea. It's a no-brainer. reply remram 8 hours agorootparentprevThe incentive is huge, if I spend 2 milliseconds sending you your previous results instead of 2 hours running your new computation, I can (pretend to) run way more computations on the same hardware and collect hundreds of time more money. reply latchkey 8 hours agorootparentAt the risk of being exiled off the platform and earning nothing. Don't forget, there is a bit of KYC with Stripe. reply krapht 8 hours agorootparentprevID verification before you can host and random audits from gpudeploy. reply greenish_shores 1 hour agorootparentNO. That's the worst way to do almost anything on the Internet, and should be considered a last-line defense, if nothing else can be done. Here, it can be. See my comment above. reply remram 6 hours agorootparentprevThat's my whole question, do they do random audits, or is it the job of customers to double-check their results for possible attack or compute-theft and report. reply greenish_shores 1 hour agorootparentIt seems wrong to call it a \"job of customers\". It's like you wrote a Bitcoin client which didn't verified hashes of transactions, \"trusting\" everything. Or like serving a website with login feature supporting only HTTP, not HTTPS. It is a very basic feature of whatever software would connect to such services. reply htrp 10 hours agoparentprevprobably very basic... so don't run it on anything that has your own data on it (if you're an AI startup, definitely don't run it on your research cluster). reply Traubenfuchs 8 hours agorootparent> definitely don't run it on your research cluster ...what‘s the threat, actually? GPU time sellers stealing your secret sauce? reply lolinder 8 hours agorootparentI think they mean don't lease out your research team's GPUs and allow random people to run untrusted code on your cluster, lest they figure out a way to break out of any sandboxing the software has in place and get loose in your network. The company's current answer to that concern is \"everything runs inside Docker, configured with security best-practices\", which is less than inspiring. https://news.ycombinator.com/item?id=40261591 reply latchkey 10 hours agoprevInteresting, these companies are springing up left and right. Software solutions to a hardware problem. It looks like most of the available compute is allocated, so they are going to have to get a bunch more providers onboarded. I wonder why y-combinator is stuffing their investments with multiples of these similar companies... https://www.shadeform.ai/ is another one. A few quick comments: Reading the source of their install script: https://gpudeploy-public.s3.us-west-1.amazonaws.com/join_clu... It doesn't start off with set -e, which could result in an incomplete install, yet appear to finish. It also installs some binary \"instance-server\"... who knows what it does... would you trust this on your server on your network? It is nvidia specific... sadly, don't expect AMD gpus anytime soon. Feels like a MVP, let's see how this grows over time. reply tptacek 8 hours agoparentAs the post points out, this is a pivot from a company that was a consumer of this kind of service in its original incarnation; YC had presumably nothing to do with it. reply latchkey 8 hours agorootparentThat an interesting presumption. If a startup is \"Backed by Y Combinator\", and it pivots, you don't think that Y Combinator would have some sort of opinion on that? I would expect some sort of conversation like: \"Oh hey, we have another company, in our portfolio, that is far ahead of you, doing exactly the same thing. Maybe you should do something else?\" But of course, this is AI... plenty of space for gpu marketplaces. reply dang 6 hours agorootparentYC partners might have an opinion and might offer advice but they're always going to support what the founders want to do—that's kind of the core principle of YC. As for \"other company in portfolio\", that's unavoidable when funding thousands of startups and almost always turns out to be a non-issue. reply latchkey 6 hours agorootparentAwesome, makes perfect sense. Thanks for the clarity @dang. I'm curious now, if you can say. Was advice offered in this case? If so, what was it? reply tptacek 4 hours agorootparentprevThat's not how YC works. We're YC W20. We have a GPU offering. Guess how many conversations we had with YC about it. What's YC going to do? They have a tiny stake in your firm. That's the point; that's what \"founder-friendliness\" means. They're not your board. reply dheera 10 hours agoparentprevIs there any way to monitor for sales of hardware if/when any of these companies dies so that I can get 1xH100 for personal use at a deep discount? reply latchkey 9 hours agorootparentEbay? reply dheera 9 hours agorootparentIs that where they really go? I feel like buying anything for several thousand dollars on Ebay is sketch. What if they ship me an empty box? reply mcmcmc 9 hours agorootparenthttps://www.ebay.com/help/buying/returns-refunds/returns-ref... reply latchkey 9 hours agorootparentprevBack in my crypto asic mining days, I'd hear stories of people receiving boxes of cement blocks. Looks like you can get a MI210 on ebay for $6500, with 30 day warranty. heh. Oh, there are a bunch of H100's there... reply 15155 8 hours agorootparentprevFile a dispute with PayPal? And then with your credit card? reply moneywoes 6 hours agoparentprevwhy does ycombinator invest in companies that will compete against each other, isn’t that a conflict of interest reply scottfits 9 hours agoprevI have a cluster on RunPod and it's great, but there's definitely some opportunity in the space, for example if you can focus on transparency (utilization rates, more info about end user and their purpose, reporting) that would be helpful. I think RunPod and Vast have most the market share but t's still early to the game reply idiotsecant 11 hours agoprevThis is an interesting idea but it would be cool if it were more granular, like I pay while my payload is executing only, and an API abstracts away which GPUs I'm running on, the execution environment, etc and just let's me push in code and get out data when it's done. Maybe that's what this is, not sure. I hit an account sign up before I could figure it out. reply dbish 9 hours agoparentI think products like this have to evolve into pay by the hour otherwise you find alternatives that have it. reply callalex 10 hours agoprevBack in my day we cooperated to advance medicine and science with folding@home. How times change. https://en.m.wikipedia.org/wiki/Folding@home reply cntrmmbrpsswrd 10 hours agoparentAnd https://en.wikipedia.org/wiki/SETI@home. I suppose this is different, since I'd wager it'll end up being used primarily by for-profit projects. reply agumonkey 9 hours agorootparentIt's strange how society psychology changes so much. SETI and Folding were huge in scale but not in hype. reply EduardoBautista 10 hours agoparentprevfolding@home was used in the “console wars” to justify one’s choice of the PS3 over the Xbox 360. Was the cell processor in the PS3 really that efficient for this purpose? reply TkTech 9 hours agorootparentIt was a fairly unique architecture that had some pros and cons. In reality what made it a winner was the steep subsidy by Sony (units were sold at a loss, making the assumption you would buy a few games) and the availability (later removed) of an official Linux distribution. reply luigi23 5 hours agorootparentvery few cared about linux. it was mostly hd-dvd (later dvd) vs blu-ray. reply kuschku 9 hours agorootparentprevIt was basically an all-purpose vector computing monster focused on SIMD. You could use it for physics simulations, animations, tesselation, etc. Basically everything you'd use a compute shader for nowadays. That's why emulators need AVX512 support to match the PS3. It was incredibly powerful. Obviously, in that era's single-threaded world no engine could make use of that functionality and few knew how to program for it. It was ahead of its time, by quite a while. reply azinman2 4 hours agoprevSeems like you have to rent for specific dates… that doesn’t feel very on-demand to me. It also means you have to know how much time you need. I’d love to be able to run a job with a remote gpu as if it were local, spinning up instances as needed. Then if things crash and I need some hours to figure it out, let me downsize it all. reply lreeves 10 hours agoprevIt'd be nice if there was any documentation on how the sharing side of things worked pre-account creation. Is it a VM image you distribute? Docker container? reply lschneider 10 hours agoparentThere's a post how to list your GPU here: https://blog.gpudeploy.com/earn-passive-income-renting-out-y... reply oefrha 9 hours agoprevI don’t get “final rate is usually lower”. Why is it lower, and by how much? And since it’s “usually”, what about the unusual case? Equal to the sticker price, or higher? But since you use “ or if you’re an AI company with idle compute (hopefully not in a Stability AI way) What does that mean? I’m likely missing some context, could anyone explain? reply burkaman 10 hours agoparentI think Stability is not doing well and ran out of money to pay for all their compute, so they should probably use some Airbnb guests right now. reply omneity 9 hours agorootparentMy understanding is that Stability was renting GPUs from AWS rather than buying them[0]. So they have no capacity per se. [0]: https://www.theregister.com/2024/04/03/stability_ai_bills/ reply a2128 11 hours agoprevWhat's so unique about this compared to just using RunPod? reply malfist 11 hours agoprevI'm curious how a business pivots from drone delivery to GPU Airbnb? reply exe34 11 hours agoparentI got it from the line about how they trained their robot models - think of Amazon pivoting to aws, the substrate becomes the commodity. Unless you're asking if it makes sense, or what went wrong? Then I don't know. I imagine startups that survive usually pivot from something else? reply MikeTheGreat 11 hours agorootparentI've never run a start-up (successful or otherwise :) ) but I've also heard that being able to pivot can be really useful - Flickr is often cited as an example. Also, \"GPU on demand\" sounds _a_lot_ easier than \"drone-based delivery\". Between Seti@Home/Folding@Home/etc, various grid-computing/clustering/orchestration stacks that already exist, etc it seems reasonably doable to implement in a year or so. \"drone-based delivery\" sounds capital-intensive, sounds like you'll need to spend a lot of time building a professional network of business people who might use the service (so there's a 'cultural friction' between techie founders and business folks, potentially), plus the ever-looming threat of Amazon/etc figuring this out first. tl;dr: I agree it's weird pivot, and good on the founders for being able to make the change! :) reply nicowaltz 5 hours agorootparentWhile most people think it's cool and some think it's scary, drone delivery is ultimately something that people (that is: companies with money) don't need. Turns out, using drones to deliver is also not that competitive either. Delivery vans are very cost-efficient, and for food delivery / on-demand delivery, drones are not able to carry most orders. So it's not even the regulatory pains that make this difficult, which are unbearable in their own right. It was a lot of fun to work on and we would have definitely stuck with it if there was any interest. There was none, so we had to admit that to ourselves. This is a hard pivot, but it's been very stimulating to work on. reply qeternity 24 minutes agorootparentBtw thanks for this honest insight. When I read the bit about your pivot, I sort of rolled my eyes. But this is a really nice, sober reflection that actually builds credibility with people like me who might make snap judgements at first. I hope you guys find traction. reply jayyhu 5 hours agorootparentprevTo add to \"capital-intensive\": the regulatory framework right now (in the US) for drone delivery is uncertain at best. While most of the rules & regs to enable drone delivery is already finalized & published, a few very important ones (such as BVLOS - beyond visual line-of-sight) have yet been finalized, and thus need one-off waivers from the FAA to allow. And getting those one-off waivers is what eats a lot of time & money. reply kristopolous 11 hours agoparentprevThey're chasing money. It's a tactic reply bigcat12345678 10 hours agoprevWe have a similar product at https://llm.sxwl.ai/ModelRepository, also routing container workloads to GPU clusters (managed by Kubernetes). The difference is that ours sell GPU compute hours, not machines. reply lordofgibbons 6 hours agoprevHello, I'm currently in the market for an on-demand GPU host so this is perfect timing. What kind of privacy can I expect from this service? reply rushingcreek 7 hours agoprevThis is really great! I hope you succeed. At Phind, we’re customers of Voltage Park and SF Compute for something similar. Let me know if you’d like to chat! reply overgard 11 hours agoprevPretty neat, but is there any Windows or MacOS support planned? I wouldn't mind renting out my GPU when it's idle, but I don't really want to go through the process of dual booting etc. reply nicowaltz 8 hours agoparentWindows support is definitely something we are thinking about. reply bongodongobob 10 hours agoparentprevWhat are you doing GPUs that requires Windows or Mac support? reply zephyrthenoble 10 hours agorootparentVideo games reply otterley 10 hours agorootparentHave you looked at Nvidia GeForce NOW? It's like $10/mo for a pretty decent streaming gaming rig. I'm very happy with it - I don't have to deal with Windows and can play AAA games on my Macbook Pro at 60Hz (1080p). reply erhaetherth 10 hours agorootparent> on my Macbook Pro at 60Hz (1080p). I think you just answered yourself. Some of us like to play games at 4K at 80Hz+, with no subscription fees, no internet bandwidth requirements, no added latency, and ability to mod. reply hhh 10 hours agorootparentYeah, but that has nothing to do with the context of the question. Someone is specifically asking about accessing a GPU over the internet for video games. reply freedomben 9 hours agorootparentprevNowadays it's rare that a Windows-only game that I want to run doesn't run flawlessly on my Linux machine (through Proton/Wine). I wouldn't recommend going outside of Steam though unless you're willing to do some troubleshooting. reply 1992spacemovie 9 hours agorootparentTry running COD Warzone on Linux and not get quarantined to cheater-suspect lobbies. reply bongodongobob 6 hours agorootparentprevSo what are you looking for, an RDP session with an attached GPU? reply CtrlAltDelete51 8 hours agoprevWhat kind of utilization percentages do you expect to be able to provide? Do you have any existing usage/ROI data? reply nicowaltz 6 hours agoparentJust launched but will put a panel on the landing page for that, once the data comes in. reply animex 9 hours agoprevI wonder if I could get strangers to heat my home with GPUs in the winter... reply remram 9 hours agoparentYes: https://techcrunch.com/2020/03/31/qarnot-raises-6-5-million-... > French startup Qarnot (...) manufactures heaters and boilers with a special trick — they pack computers as computers tend to generate a lot of heat. Qarnot then lets companies leverage that computing power by running tasks on those unusual servers. reply nicowaltz 8 hours agoparentprevYes, great use-case. reply CtrlAltDelete51 8 hours agoprevSide note: you contact form doesn’t work - do you have an email I send some questions over to? reply nicowaltz 7 hours agoparentSure, nico@gpudeploy.com reply modeless 9 hours agoprevHow are you different from vast.ai? What's your payout rate? reply mattxxx 10 hours agoprevDoesn't this just raise the price of gpu's, and further encourage people to hoard them? reply densh 10 hours agoprevAre you planning on letting hobbyists rent their single machine with multiple GPUs? reply nicowaltz 8 hours agoparentDefinitely, but we will sort out machines that are unreliable quite quickly. reply moneywoes 6 hours agoprevwhat is a Stability AI way? reply epa 10 hours agoprevDDoSaaS reply krasin 11 hours agoprev [–] Related: https://vast.ai is good and cheap. Just don't put any sensitive data on these GPU machines. I've been a happy user of vast.ai for some time now. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A startup transitioned from drone delivery to launch gpudeploy.com, a platform redirecting on-demand GPU instance traffic to available compute resources.",
      "The website offers straightforward service without concealed charges, granting users access to GPUs such as H100s, RTX 4090s, and GTX 1080 Ti.",
      "Seeking compute resources to link with their platform for potential return on investment."
    ],
    "commentSummary": [
      "Gpudeploy.com functions as an \"Airbnb\" for GPUs, enabling users to rent out GPU instances and route on-demand traffic to idle compute resources.",
      "The platform provides a straightforward user experience with multiple GPU options and transparent pricing, sparking discussions on security, potential computation errors, and the importance of rigorous operator verification.",
      "Users discuss the shifting business models towards GPU marketplaces, opportunities in cluster computing, and the market effects of GPU pricing fluctuations and hoarding."
    ],
    "points": 182,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1714856635
  },
  {
    "id": 40261681,
    "title": "New Automated Hash Functions Outperform MurmurHash3",
    "originLink": "https://github.com/skeeto/hash-prospector",
    "originBody": "Hash Function Prospector This is a little tool for automated integer hash function discovery. It generates billions of integer hash functions at random from a selection of nine reversible operations (also). The generated functions are JIT compiled and their avalanche behavior is evaluated. The current best function is printed out in C syntax. The avalanche score is the number of output bits that remain \"fixed\" on average when a single input bit is flipped. Lower scores are better. Ideally the score is 0 — e.g. every output bit flips with a 50% chance when a single input bit is flipped. Prospector can generate both 32-bit and 64-bit integer hash functions. Check the usage (-h) for the full selection of options. Due to the JIT compiler, only x86-64 is supported, though the functions it discovers can, of course, be used anywhere. Article: Prospecting for Hash Functions Discovered Hash Functions There are two useful classes of hash functions discovered by the prospector and the other helper utilities here. Both use an xorshift-multiply-xorshift construction, but with a different number of rounds. Two round functions Update: TheIronBorn has used combinatorial optimization to discover the best known parameters for this construction: [16 21f0aaad 15 d35a2d97 15] = 0.10760229515479501 This 32-bit, two-round permutation has a particularly low bias and even beats the venerable MurmurHash3 32-bit finalizer by a tiny margin. The hash function construction was discovered by the prospector, then the parameters were tuned using hill climbing and a genetic algorithm. // exact bias: 0.17353355999581582 uint32_t lowbias32(uint32_t x) { x ^= x >> 16; x *= 0x7feb352d; x ^= x >> 15; x *= 0x846ca68b; x ^= x >> 16; return x; } // inverse uint32_t lowbias32_r(uint32_t x) { x ^= x >> 16; x *= 0x43021123; x ^= x >> 15 ^ x >> 30; x *= 0x1d69e2a5; x ^= x >> 16; return x; } More 2-round constants with low bias, some even better than lowbias32: [15 d168aaad 15 af723597 15] = 0.15983776156606694 [17 9e485565 16 ef1d6b47 16] = 0.16143129787074881 [16 604baa5d 15 43d6ce97 15] = 0.16491052655811722 [16 a812d533 15 b278e4ad 17] = 0.16540778981744320 [16 9c8f2d35 15 5d1346b5 17] = 0.16835348823718840 [16 88c0a94b 14 9d06da59 17] = 0.16898511658356749 [16 a52fb2cd 15 551e4d49 16] = 0.17162579707098322 [16 b237694b 15 eb5b4593 15] = 0.17274184020173433 [16 7feb352d 15 846ca68b 16] = 0.17353355999581582 [16 4bdc9aa5 15 2729b469 16] = 0.17355424787865850 [16 dc63b4d3 15 2c32b9a9 15] = 0.17368589564800074 [16 e02bd533 15 0364c8ad 17] = 0.17447893149410759 [16 603a32a7 15 5a522677 15] = 0.17514135907753242 [16 ac10d4eb 15 9d51b169 16] = 0.17676510450127819 [15 f15f5959 14 7db29359 16] = 0.18103205436627479 [16 83747333 14 aa256573 16] = 0.18105722344231542 [16 be8b6ca7 14 6dd624b5 16] = 0.18223928664971270 [17 7186cd35 15 fe6bba73 15] = 0.18312741727971640 [16 93f2552b 15 959b4a4d 15] = 0.18360629205797341 [16 df892d4b 15 3c2da6b3 16] = 0.18368195486921446 [15 49c34cd3 13 e7418ca7 16] = 0.18400092964673831 [15 4811acab 15 5591acd7 16] = 0.18522661033580071 [16 dc85aaa7 15 6658a5cb 15] = 0.18577280285788791 [16 1ec9b4db 15 3224d38d 17] = 0.18631684392389897 [16 8ee0d535 15 5dc6b5af 15] = 0.18664478683752250 [16 462daaad 15 0a36c95d 16] = 0.18674876992866513 [16 17cdd657 15 a426cb25 15] = 0.18995262675473334 [16 ab39aacb 15 a1b5d19b 15] = 0.19045785238099658 [17 cd8512ad 15 b95c5a73 15] = 0.19050717016846502 [16 aecc96b5 15 f64dcd47 15] = 0.19077817816874504 [15 2548acd5 15 0b39d397 16] = 0.19121161052714156 [15 7f19c559 15 b356358d 16] = 0.19198007174447981 [16 4ffcab35 15 e98db28b 16] = 0.19423994132339928 [15 1216ccb5 15 3abcdca9 15] = 0.19426091938816648 [16 97219aad 15 ab46b735 15] = 0.19536391240344408 [16 c845a997 15 f214db9b 17] = 0.19553179377831409 [15 3a7ba96b 13 5e919299 16] = 0.19563436462680908 [16 c3d9a965 16 362e4b47 15] = 0.19575424692659107 [17 179cd515 15 4c495d47 15] = 0.19608530402798924 [16 5dce3553 15 a655d8e9 15] = 0.19621753012889542 [17 88a5ad35 16 96338b27 16] = 0.19653922266398804 [17 0364d657 15 ac2a34c5 15] = 0.19665754791333651 [16 3c9aa9ab 16 051369d7 16] = 0.19687211117412906 [17 0ee6d967 15 9c8a4a33 16] = 0.19722490309575344 [16 b921a6cb 14 30b5a6d1 16] = 0.19745192295417058 [18 a136aaad 16 9f6d62d7 17] = 0.19768193144773874 [16 0ae84d3b 15 3b9d4e5b 17] = 0.19776257374279985 [17 24f4d2cd 15 1ba3b969 16] = 0.19789489706453650 [16 418fb5b3 15 8cf3539b 16] = 0.19817117175199098 [16 f0ae2ad7 15 8965d939 16] = 0.19881758420284917 [17 9bde596b 16 1c9e9647 16] = 0.19882570872036193 [16 bd10754b 14 35a29b0d 16] = 0.19885203058591913 [17 78d31553 15 c547ac65 15] = 0.19918133404528665 [15 81aab34d 15 18e746a3 15] = 0.19938572052445763 [16 054335ab 15 146da68b 16] = 0.19943843016872725 [17 a1c76a55 16 5ca46b97 16] = 0.19959562213253398 [15 c62f4d53 14 62b8a46b 16] = 0.19973996656987172 [16 6872cd2d 15 f4a0d975 17] = 0.19992260539370590 This next function was discovered using only the prospector. It has a bit more bias than the previous function. // exact bias: 0.34968228323361017 uint32_t prospector32(uint32_t x) { x ^= x >> 15; x *= 0x2c1b3c6d; x ^= x >> 12; x *= 0x297a2d39; x ^= x >> 15; return x; } To use the prospector search randomly for alternative multiplication constants, run it like so: $ ./prospector -p xorr:15,mul,xorr:12,mul,xorr:15 Three round functions Another round of multiply-xorshift in this construction allows functions with carefully chosen parameters to reach the theoretical bias limit (bias = ~0.021). For example, this hash function is indistinguishable from a perfect PRF (e.g. a random permutation of all 32-bit integers): // exact bias: 0.020888578919738908 uint32_t triple32(uint32_t x) { x ^= x >> 17; x *= 0xed5ad4bb; x ^= x >> 11; x *= 0xac4c1b51; x ^= x >> 15; x *= 0x31848bab; x ^= x >> 14; return x; } // inverse uint32_t triple32_r(uint32_t x) { x ^= x >> 14 ^ x >> 28; x *= 0x32b21703; x ^= x >> 15 ^ x >> 30; x *= 0x469e0db1; x ^= x >> 11 ^ x >> 22; x *= 0x79a85073; x ^= x >> 17; return x; } More 3-round constants with low bias: [17 ed5ad4bb 11 ac4c1b51 15 31848bab 14] = 0.020888578919738908 [16 aeccedab 14 ac613e37 16 19c89935 17] = 0.021246568167078764 [16 236f7153 12 33cd8663 15 3e06b66b 16] = 0.021280991798512679 [18 4260bb47 13 27e8e1ed 15 9d48a33b 15] = 0.021576730651802156 [17 3f6cde45 12 51d608ef 16 6e93639d 17] = 0.021772288363808408 [15 5dfa224b 14 4bee7e4b 17 930ee371 15] = 0.02184521628884813 [17 3964f363 14 9ac3751d 16 4e8772cb 17] = 0.021883292578109576 [16 66046c65 14 d3f0865b 16 f9999193 16] = 0.0219446068365007 [16 b1a89b33 14 09136aaf 16 5f2a44a7 15] = 0.021998624107282542 [16 24767aad 12 daa18229 16 e9e53beb 16] = 0.022043911220395354 [15 42f91d8d 14 61355a85 15 dcf2a949 14] = 0.022052539152635078 [15 4df8395b 15 466b428b 16 b4b2868b 16] = 0.022140187420461286 [16 2bbed51b 14 cd09896b 16 38d4c587 15] = 0.022159936298777144 [16 0ab694cd 14 4c139e47 16 11a42c3b 16] = 0.02220928191220355 [17 7f1e072b 12 8750a507 16 ecbb5b5f 16] = 0.022283743052847804 [16 f1be7bad 14 73a54099 15 3b85b963 15] = 0.022316544125749647 [16 66e756d5 14 b5f5a9cd 16 84e56b11 16] = 0.022372957847491555 [15 233354bb 15 ce1247bd 16 855089bb 17] = 0.022406591070966285 [16 eb6805ab 15 d2c7b7a7 16 7645a32b 16] = 0.022427060650927547 [16 8288ab57 14 0d1bfe57 16 131631e5 16] = 0.022431656871313443 [16 45109e55 14 3b94759d 16 adf31ea5 17] = 0.022436433678417977 [15 26cd1933 14 e3da1d59 16 5a17445d 16] = 0.022460520416491526 [16 7001e6eb 14 bb8e7313 16 3aa8c523 15] = 0.022491767264054854 [16 49ed0a13 14 83588f29 15 658f258d 15] = 0.022500668856510898 [16 6cdb9705 14 4d58d2ed 14 c8642b37 16] = 0.022504626537729222 [16 a986846b 14 bdd5372d 15 ad44de6b 17] = 0.022528238323120016 [16 c9575725 15 9448f4c5 16 3b7a5443 16] = 0.022586511310042686 [15 fc54c453 13 08213789 15 669f96eb 16] = 0.022591114646032095 [16 d47ef17b 14 642fa58f 16 a8b65b9b 16] = 0.022600633971701509 [15 00bfaa73 14 8799c69b 16 731985b1 16] = 0.022645866629596379 [16 953a55e9 15 8523822b 17 56e7aa63 15] = 0.022667180032713324 [16 a3d7345b 15 7f41c9c7 16 308bd62d 17] = 0.022688845770122031 [16 195565c7 14 16064d6f 16 0f9ec575 15] = 0.022697810688752193 [16 13566dbb 14 59369a03 15 990f9d1b 16] = 0.022712430070797596 [16 8430cc4b 15 a7831cbd 15 c6ccbd33 15] = 0.022734765033419774 [16 699f272b 14 09c01023 16 39bd48c3 15] = 0.022854175321846512 [15 336536c3 13 4f0e38b1 16 15d229f7 16] = 0.022884125170795171 [16 221f686d 12 d8948a07 16 ed8a8345 16] = 0.022902500408830236 [16 d7ca8cbb 13 eb4e259f 15 34ab1143 16] = 0.022905955538176669 [16 7cb04f65 14 9b96da73 16 83625687 15] = 0.022906573700088178 [15 5156196b 14 940d8869 15 0086f473 17] = 0.022984943828687553 Prepending an increment to triple32 breaks the hash(0) = 0 issue while also lowering the bias a tiny bit further: // exact bias: 0.020829410544597495 uint32_t triple32inc(uint32_t x) { x++; x ^= x >> 17; x *= 0xed5ad4bb; x ^= x >> 11; x *= 0xac4c1b51; x ^= x >> 15; x *= 0x31848bab; x ^= x >> 14; return x; } // inverse uint32_t triple32inc_r(uint32_t x) { x ^= x >> 14 ^ x >> 28; x *= 0x32b21703; x ^= x >> 15 ^ x >> 30; x *= 0x469e0db1; x ^= x >> 11 ^ x >> 22; x *= 0x79a85073; x ^= x >> 17; x--; return x; } Measuring exact bias The -E mode evaluates the bias of a given hash function (-p or -l). By default the prospector uses an estimate to quickly evaluate a function's bias, but it's non-deterministic and there's a lot of noise in the result. To exhaustively measure the exact bias, use the -e option. The function to be checked can be defined using -p and a pattern or -l and a shared library containing a function named hash(). For example, to measure the exact bias of the best hash function above: $ ./prospector -Eep xorr:16,mul:e2d0d4cb,xorr:15,mul:3c6ad939,xorr:15 Or drop the function in a C file named hash.c, and name the function hash(). This lets you test hash functions that can't be represented using the prospector's limited notion of hash functions. $ cc -O3 -shared -fPIC -l hash.so hash.c $ ./prospector -Eel ./hash.so By default it treats its input as a 32-bit hash function. Use the -8 switch to test (by estimation) 64-bit functions. There is no exact, exhaustive test for 64-bit hash functions since that would take far too long. Reversible operation selection x = ~x; x ^= constant; x *= constant1; // e.g. only odd constants x += constant; x ^= x >> constant; x ^= x > 8; x *= 0x88b5U; x ^= x >> 7; x *= 0xdb2dU; x ^= x >> 9; return x; } // 3-round xorshift-multiply (-Xn3) // bias = 0.0045976709018820602 uint16_t hash16_xm3(uint16_t x) { x ^= x >> 7; x *= 0x2993U; x ^= x >> 5; x *= 0xe877U; x ^= x >> 9; x *= 0x0235U; x ^= x >> 10; return x; } // No multiplication (-Imn6) // bias = 0.023840118344741465 uint16_t hash16_s6(uint16_t x) { x += x > 8; x += x > 2; x += x > 8; return x; } // Which is identical to this xorshift-multiply uint16_t hash16_s6(uint16_t x) { x *= 0x0081U; x ^= x >> 8; x *= 0x0009U; x ^= x >> 2; x *= 0x0011U; x ^= x >> 8; return x; } A good 3-round xorshift hash (a short search via hp16 -Xn3) is a close approximation of a good s-box (i.e. hp16 -S). Be mindful of C integer promotion rules when doing 16-bit operations. For instance, on 32-bit implementations unsigned 16-bit operands will be promoted to signed 32-bit integers, leading to incorrect results in certain cases. The C programs printed by this program are careful to promote 16-bit operations to \"unsigned int\" where needed.",
    "commentLink": "https://news.ycombinator.com/item?id=40261681",
    "commentBody": "Automated integer hash function discovery (github.com/skeeto)173 points by danny00 8 hours agohidepastfavorite25 comments skissane 5 hours agoI don't know this guy personally but I like his code. I particularly like his JSON library [0], option parsing libraries [1], branchless UTF-8 decoder [2], lock-free stack [3], and trie library [4]. I also like his taste in licensing (all of the above is released under The Unlicense) [0] https://github.com/skeeto/pdjson [1] https://github.com/skeeto/optparse and https://github.com/skeeto/getopt [2] https://github.com/skeeto/branchless-utf8 [3] https://github.com/skeeto/lstack [4] https://github.com/skeeto/trie reply cyfex 20 minutes agoparentHe's also the author of elfeed [0] \"An Emacs web feeds client\". I've found his minimalist implementation very inspiring. [0] https://github.com/skeeto/elfeed reply aappleby 3 hours agoprevHi, I'm the MurmurHash guy. Neat stuff, and I'm amused that multiply-shift-xor has held up so well for all this time. reply edflsafoiewq 3 hours agoparentXor-shift counteracts the two weaknesses of a multiply: the high bits have nothing above them to influence, and the low bits have nothing below them to be influenced by. reply keepamovin 5 hours agoprevI've often thought about the idea of auto hash finding based on my experiences developing good hash functions^0 Cool to see this! I think it would be cool to hook it up with SMHasher3^1 (a much improved, and faster variant of the venerable hash test suite developed by Frank J. T. Wojcik) to automatically evaluate its output. You could use a subset of tests and fail fast for speed. It would also be cool to expand it to 64 and 128 bit hashes (tho obviously the search space is larger). Somewhat related I created some NodeJS code to measure avalanche under multiplication for 64-bit primes in order to select the values for Rain. [Rain]: https://github.com/dosyago/rain [SMHasher3]: https://gitlab.com/fwojcik/smhasher3 reply infogulch 2 hours agoprevI implemented the 1brc in go a few weeks back [1] and this repo was inspiration to try to find a custom perfect hash function that put each station into its own bucket with no collisions. Then I noticed the rule that you can't customize the hash function to the data before the program starts and dropped the idea. I made a test harness to check random constants (start values, multiplication constants, shift/rotate amounts, etc) and print out the best constants found so far based on the number of colliding buckets and how many collisions. I think I got it down to 1 bucket with just two colliding values with a ~40% fill rate. Interestingly, I found that the best performing constants included similar values for the number of positions to shift regardless of the other constants, so I ended up hard coding them. [1]: https://github.com/infogulch/1brc-go reply throwaway_1237 4 hours agoprevCan someone explain why this is cool and what it’s used for? reply tommiegannert 3 hours agoparentIt seems to be a tool that generates instructions for building hash functions, then evaluates the hash functions to see how good they are. The goal metric has been chosen to be that as many output bits as possible should change as randomly as possible with every changed input bit. It outputs C code for the best hash function it generated. So it's useful if you want a hash function, and don't think any of the existing ones are good enough. Or if you're researching hash functions, and want new ideas for structures. Cool? Well, generating code is cool. Doing it randomly is a first step towards genetic programming, which would be even cooler. ... and making computers burn CPU cycles computing (mostly unused) hashes seems to be something we humans want to do since around 15 years ago. reply eru 1 hour agorootparent> [...] making computers burn CPU cycles computing (mostly unused) hashes seems to be something we humans want to do since around 15 years ago. Keep in mind that there's a big difference between cryptographic hash functions and the kind of hash functions investigated here. reply masklinn 2 hours agoparentprevHash functions on integers, so if you want a fast integer hash for a set or map. If the functions diverge sufficiently it also provides fast hashes for a bloom filter. reply alegeaa 3 hours agoparentprevI am asking myself the same question. reply thomasmg 1 hour agoprevI wonder if using the same constant for both multiplications would reduce code size and so could (slightly) speed up computation? I have updated my StackOverflow answer at https://stackoverflow.com/questions/664014/what-integer-hash... reply Retr0id 58 minutes agoprev> Three round functions: [...] this hash function is indistinguishable from a perfect PRF I assume this means \"indistinguishable through naive statistical tests\", because any more literal meaning would surprise me reply evujumenuk 47 minutes agoparentMaybe I'm misinterpreting something, but… isn't this really about https://en.wikipedia.org/wiki/Perfect_hash_function? \"Perfect\" doesn't represent a statement of quality here, if that's what you're alluding to. reply Retr0id 9 minutes agorootparentNo, I mean in the sense given on the page (I should've included the parenthetical in my quote) > (e.g. a random permutation of all 32-bit integers) Since they only use bijective primitives, it's trivially \"perfect\" in the \"perfect hash function\" sense. reply o11c 4 hours agoprevLimiting itself to reversible operations gives some mathematical niceties, but also locks out a lot of things. When I did something similar, I was thinking about perfect hashing, where the set of inputs is known ahead of time. The usual approach uses a constant array, but I wanted to - in particular, if the inputs are already small integers, if I could compact them further (obviously this is possible: hash -= hash > > gap_index). I sat down and wrote out a list of ... probably 100? ... primitive operations (some of which are admittedly redundant with each other, but which are useful thinking of separately). And then I got bored and never did anything with the project. reply mshroyer 2 hours agoparent> Limiting itself to reversible operations gives some mathematical niceties What are those niceties, why is a reversible operation desirable in this context? reply pyinstallwoes 24 minutes agoparentprevXOR xor xor reversibility through cyclical permutations ? reply renonce 2 hours agoprevI know I’m comparing oranges to apples here as these functions are not well suited for cryptographic operations, but how does the measured “bias” affect cryptanalysis? Can someone familiar with differential cryptography explain if a hash function with a lower bias defeats cryptanalysis with lower rounds or with less compute? Will this thing help find better cryptographic hash functions? reply eru 1 hour agoparent> Will this thing help find better cryptographic hash functions? Very, very unlikely. reply rurban 4 hours agoprevI also have something similar here, with a slower generator (interpreted, not jitted), but better quality functions: https://github.com/rurban/fast-hash/tree/master/hashgen But I found nothing better than the old favorites. reply adgjlsfhk1 4 hours agoprevthis is a very cool library, but it needs someone to figure out a good way of validating 64 bit hash functions. reply pcranaway 1 hour agoprevIs this something like a superoptimizer? looks cool! reply rowanG077 5 hours agoprevWhat would make this really interesting is if you can give it a generator for input data yourself. Very often you don't have random binary data but structured in some way. The structure might imply you can get a very nice hash function for it. reply nullc 6 hours agoprev [–] It would be interesting to generalize this with the operations available in the riscv bitmanip extension-- it might discover some strong functions for use in the future when those instructions are more widely available. carryless multiply would also allow extending the set of reversible operations and are fast on some existent hardware (CRC too, somewhat a wider set of hardware but should be a strict subset of what CLMUL could find). Many applications of hashes only care about the LSB or MSB of the hashes so evaluating the bias on MSB or LSB windows would be interesting (or mod varrious numbers), and some overall unbiased functions will look better or worse for metrics that don't gauge the entire output. (or for non-uniform inputs, for that matter, e.g. ascii text). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The text introduces the Hash Function Prospector tool, focusing on automated discovery of integer hash functions, particularly two-round permutations with low bias.",
      "New hash functions like triple32, with bias measurements approaching the theoretical limit, are presented, outperforming established algorithms like MurmurHash3 on x86-64 architecture.",
      "Code examples for implementing hash functions are offered, highlighting the significance of reversible operations and advising caution with C integer promotion rules."
    ],
    "commentSummary": [
      "GitHub user skeeto has created an automated tool for discovering integer hash functions, praised for his code quality and previous library developments.",
      "The tool focuses on producing integer hash functions with high output randomness and reversibility, deemed beneficial for quick integer hashing and applications in perfect hashing, performance optimization, and cryptography.",
      "Users are discussing the potential applications of this tool in various fields due to its capabilities in generating efficient integer hashes."
    ],
    "points": 173,
    "commentCount": 25,
    "retryCount": 0,
    "time": 1714873145
  },
  {
    "id": 40257064,
    "title": "Vulture Decline Imperils Zoroastrian Burial Rites",
    "originLink": "https://www.theguardian.com/world/article/2024/may/04/vulture-shortage-threatens-zoroastrian-burial-rites-india-iran-pakistan",
    "originBody": "View image in fullscreen A funeral procession for a member of the Parsi community at a prayer hall in Mumbai. Photograph: Indranil Mukherjee/AFP/Getty Images Pakistan ‘Our culture is dying’: vulture shortage threatens Zoroastrian burial rites Inadvertent poisoning of scavengers across Indian subcontinent is forcing some communities to give up ancient custom Sonia Gulzeb Sat 4 May 2024 01.00 EDT Share Traditional Zoroastrian burial rites are becoming increasingly impossible to perform because of the precipitous decline of vultures in India, Iran and Pakistan. For millennia, Parsi communities have traditionally disposed of their dead in structures called dakhma, or “towers of silence”. These circular, elevated edifices are designed to prevent the soil, and the sacred elements of earth, fire and water, from being contaminated by corpses. Bodies are placed on top of the towers, where they decompose, while vultures and other scavengers eat the flesh on the bones. After being bleached by the sun and wind for up to a year, the bones are collected in an ossuary pit at the centre of the tower. Lime hastens their gradual disintegration, and the remaining material, along with rainwater runoff, filters through coal and sand before it is washed out to sea. “We are no longer able to fulfil our traditions,” Hoshang Kapadia, a Karachi resident in his 80s, said. “We’ve lost a way of life, our culture.” Kapadia explained that the purpose behind the Parsi burial customs was to “take less and give more” to the world. “The whole idea is not to pollute the earth,” he said View image in fullscreen Vultures gather on a Parsi ‘tower of silence’, circa 1880. Offering one’s deceased body to the birds is regarded as the devout Zoroastrian’s ultimate act of charity. Photograph: Sean Sexton/Getty Images Karachi, which is built upon a river ecosystem on the western bank of the Indus River delta, is home to only 800 Parsis out of a population of 20 million people. The city has just two remaining towers of silence, both barely functional. Another Karachi Parsi, Shirin, said: “The vulture’s mystical eye is believed to aid the soul’s cosmic transition, and offering one’s deceased body to the birds is regarded as the devout Zoroastrian’s ultimate act of charity.” “The massive urbanisation and environmental changes in Karachi have led us to revisit our burial rites, as dakhmas were usually built on top of hills in locations distant from urban areas. “Our tradition is dying. Our culture is dying in a time of increasing environmental change.” Unlike many scavengers, vultures are classified as “obligate”. This means they do not opportunistically switch between predation and scavenging, as their mammalian counterparts do, but rely solely on locating and feeding on animal carcasses. In recent decades, vultures have been dying in large numbers across the Indian subcontinent, primarily due to inadvertent poisoning with the anti-inflammatory drug diclofenac, which is extensively administered to cattle in India and Pakistan. When these cattle die, vultures feed on their carcasses and ingest the drug, which causes painful swelling, inflammation, and ultimately kidney failure and death in vultures. Research in 2007 estimated that about 97% of the three main vulture species in India and the surrounding region had disappeared. View image in fullscreen Bombay, the Parsee Repository for their Dead, an illustration from 1722. Photograph: CPA Media Pte Ltd/Alamy The Parsi community in India is exploring captive vulture breeding and the use of “solar concentrators\" to expedite the decomposition of bodies. As the solar concentrators only work in clear weather, some have been forced to opt for burial instead. Kapadia said: “Parsis in Karachi [are forced to] opt for alternative methods of disposal, such as cremation or burial in designated Parsi cemeteries, as the two towers of silence in Karachi are barely functional”. He added that when vulture numbers declined at the towers of silence, some community members suggested creating a small captive group of vultures in an aviary to continue the traditional practice. To prevent the extinction of vulture species, scientists have recommended banning the use of diclofenac in livestock, a move so far taken by India, Pakistan and Nepal. Captive-bred vultures have also been released into the wild in India in a bid to boost the threatened populations. Explore more on these topics Pakistan India Iran South and central Asia Middle East and north Africa Birds Pharmaceuticals industry news Share Reuse this content",
    "commentLink": "https://news.ycombinator.com/item?id=40257064",
    "commentBody": "Vulture shortage threatens Zoroastrian burial rites (theguardian.com)172 points by YeGoblynQueenne 21 hours agohidepastfavorite112 comments KineticLensman 17 hours agoA factor that devastates African vulture populations is when poachers lace carcasses (e.g. rhinos killed for their horns) with poison, because vultures circling a kill reveal the presence of the poachers to wardens. A single big poisoned carcass can kill dozens of vultures at a time, of multiple species. reply jncfhnb 16 hours agoparentIsn’t it a myth that vultures circle kills reply swatcoder 15 hours agorootparentThey circle and kettle in other circumstances as well, but no, they definitely do circle iver carcasses (and what may soon become so). It's a familiar site if you live around them. Do you remember where you heard it's a myth? I'd be curious to see how that argument was made. reply jncfhnb 15 hours agorootparentThe circling behavior is, as I read, almost always them waiting for a warm current to take them elsewhere. reply swatcoder 14 hours agorootparentThat's something they seem to do, for sure. It's just not the only thing. They're not very vocal and are very social, so circling over carcasses might be a way to signal to others that they've found something. They also form kettles as a big communal thing, too, for neither of those purposes. I've seen as many as 50 or 60 circling together for well over an hour, with no food nearby and no clearly no interest in going anywhere else. (This is all turkey vultures in the US. These African vultures might have different behaviors, but I wouldn't be surprised if they are mostly comparable in terms of variety and sophistication) reply throwaway598 7 hours agorootparentPerhaps an insight into vulture humour: George the turkey vulture: It's a nice day to hang out circling. Rambo the turkey vulture: Hey George, let's hang. Pappias the turkey vulture: Where's the food? No food. LOL. You guys! All: LOL (but silently). Sydney the turkey vulture: Where's the food? No food. LOL. You guys! All: LOL (but silently). Violet the turkey vulture: Where's the food? No food. LOL. You guys! All: LOL (but silently). Sydney the turkey vulture: Where's the food? No food. LOL. You guys! All: LOL (but silently). Seven the turkey vulture: Where's the food? No food. LOL. You guys! All: LOL (but silently). Arnold the turkey vulture: Where's the food? No food. LOL. You guys! All: LOL (but silently). Angel the turkey vulture: Where's the food? No food. LOL. You guys! All: LOL (but silently). ... 60 vultures later someone observes 60 vultures circling and concludes: A. There's definitely no carcass, as that's a myth. B. There's definitely a carcass, as there's 60 vultures. C. I'm going to find some water to drink. reply pelagicAustral 9 hours agorootparentprevI live in a place where Vultures are protected, so there's tons of them, and I have seen them circling in both cases, clearly hovering a carcass, but also randomly circling in numbers when no food is around... reply deepsun 6 hours agorootparentprevAs a soaring pilot I can confidently say that we circle when we _found_ the updraft. There's no \"waiting\", there's \" searching\" for it. reply thebeardisred 14 hours agorootparentprevWhere did you read this? I would love to learn more. reply jncfhnb 13 hours agorootparenthttps://www.forbes.com/sites/quora/2017/10/25/why-do-buzzard... https://www.quora.com/Why-do-buzzards-circle-around-their-pr... https://www.livescience.com/32202-how-do-vultures-find-dead-... https://www.adirondackalmanack.com/2020/07/turkey-vultures-c.... reply swatcoder 13 hours agorootparentThanks for sharing these! And yeah, on close read, you'll see that they're all trying to communicate that you can't assume that the vultures are circling a carcass whenever you see them circling because they have other behaviors that involve circling as well. But it's an easy detail to miss in some of them for sure! reply jncfhnb 10 hours agorootparentI feel that’s not quite right. The writing appears to indicate that circling a carcass is the small minority. They’re finding the carcasses by smell of decay. I’ve read elsewhere vultures are actually good signals of gas leaks for this reason. They don’t have much of a reason to circle a carcass they intend to eat unless perhaps it’s a lagging indicator and they’re leaving the carcass. They don’t circle dying things because they don’t have the means to detect dying things. I’m not convinced circling vultures is a good sign that there’s a carcass below them. reply DontchaKnowit 15 hours agorootparentprevNo. They'll even circle live animals if they can tell they are sick or dying. I personally witnessed this in the city trash dump of Tegucigalpa. Emaciated cattle grazing in trash, with cultures circling above them. reply fakedang 11 hours agorootparentAlso vultures circling the battlefields of every Attila, Genghis Khan and Tamerlane, before swords were joined.;) reply KineticLensman 13 hours agorootparentprevMyth or not, the poachers believe it and murder the vultures as a result. I’ve worked with vultures at a raptor conservancy. They are social, curious and intelligent birds. I have very fond memories of an Egyptian vulture named Boe who would unfailingly undoe my shoelaces when I entered her aviary. I love these birds reply spookie 13 hours agorootparentprevAs someone who lived near Egyptian Vultures and Eurasian Black Vultures (Aegypius monachus), I can assure you they do. But that's not the only reason for them to do so. reply standeven 16 hours agorootparentprevI’ve personally seen turkey vultures circle carcasses on several occasions. reply el_benhameen 15 hours agorootparentprevDifferent area and I assume different vultures, but head on out to the east east bay in the summer and you’ll see plenty of vulture circles. reply optimalsolver 15 hours agorootparentprevThey absolutely do on the East African savannah. reply soulofmischief 15 hours agorootparentprevI've witnessed it many times myself. reply celias 21 hours agoprevRadiolab and 99% Invisible have podcasts about this https://99percentinvisible.org/episode/towers-of-silence/ https://radiolab.org/podcast/corpse-demon reply miiiiiike 13 hours agoparentIf you're only going to listen to one, listen to the Radiolab episode. The Radiolab episode is crisp and well reported, the 99% Invisible episode is muddled and rambling by comparison. reply ethbr1 6 hours agorootparentSeconded. That Radiolab episode is one of the better ones I've listen to. Mostly because it's laid out as \"But that begs yet another question...\", to which the answer is equally fascinating. Also, it was just satisfying having a story with an ending: cause, found. reply boomboomsubban 19 hours agoprevIf it's consumption of NSAID's that are killing the vultures, are the towers themselves not a risk? Are the dosages/drugs given to humans not a problem? Or is there some sort of preparation that would remove the NSAID's? I tried to research the Zoroastrian procedure, but it got gory pretty fast. reply bakul 18 hours agoparentDiclofenac has been banned in India for veterinary use since 2006. Another NSAID that is not deadly to vultures is recommended for such use. Not sure why Guardian ran this story now as this is old news. The real crisis with plunging vulture population goes far beyond Zoroastrian burial rites. See https://en.wikipedia.org/wiki/Indian_vulture_crisis reply boomboomsubban 18 hours agorootparentThat Wikipedia article is what made made curious, as it says \"drugs like diclofenac...\" From that small bit, it seems like the many NSAID's given to humans such as ibuprofen or asprin could be issues. Even diclofenac seems to have some human use. reply bakul 18 hours agorootparentI suspect stronger drugs such as codeine etc are used for palliative care of humans in their last stages. Not sure of their effect on vultures. Parsi numbers have never been high and gradually decreasing (now about 70,000 in India), also they mainly live in relatively few places, most in Mumbai and nearby. The vulture population dropped from 40 million to a few hundred so the primary cause must be from their non-human diet. reply boomboomsubban 11 hours agorootparentI assume NSAID's are still common, though opiates are stronger pain killers, antiinflammatory and fever reducers would still be helpful. That said, I'm not arguing the official explanation is wrong, just curious if the same thing would happen at the towers. reply dyauspitr 13 hours agorootparentprevAnd by decreasing you mean they are finally being assimilated into the general Indian population. reply bakul 12 hours agorootparentLow fertility rates (like in most well to do communities), interfaith marriages (if you marry a non-parsi and your kids are not considered parsi), aging population, migration out of India. reply quonn 18 hours agorootparentprev> Even diclofenac seems to have some human use. Sure. Voltaren. reply bitwize 17 hours agorootparentIt's also administered orally. reply the__alchemist 19 hours agoprevI'm in South East, USA, vice the Indian subcontinent. Did something happen here within the last year? (Maybe the same thing?) I haven't seen vultures (Turkey vultures) in maybe 3/4 of a year. They are usually ubiquitous and highly visible due to large wingspan, staying airborne for long periods etc, using thermals from roads/concrete etc that make them common in human-populated areas. reply hedora 18 hours agoparentWe have a lot of turkey vultures here in California. A few years ago, we found a few fresh dead (poisoned) rats in our barn. The turkey vulture population immediately dropped to zero at our house, and the rodent population skyrocketed for the next year. Please use traps (electrocution or old-fashioned wood and metal spring work best. The electrocution ones are more pet-friendly), and not poison to deal with your vermin. On a related note, we also have started to get Peregrine Falcons and Bald Eagles again. Hopefully those populations will continue to recover too. Poisoned rodents often wander around confused and screaming before they die, and if they do that in a field, they can take out a bird of prey instead of just the vultures. Seriously, just use the traps. They are way more humane. Also, you won’t have to fish dead animals out of your vents and walls. reply pfdietz 18 hours agorootparentI went birding this morning near Ithaca, NY. Five turkey vultures, a bald eagle, and a kestrel (which caught and ate a mouse while we were watching) were among the species seen (and so many kinds of warblers. Really, why are there so many kinds of warblers?) I often see turkey vultures near our house (once a swooping flock of a dozen of them riding the wind coming up from the lake). Black vultures are starting to appear in this area also, most notably at the Cornell U. compost piles. reply bitwize 16 hours agorootparentprevRatX, MouseX, and RatRid baits by EcoClear use a combination of gluten and salt to dehydrate the rodent, rather than poisoning it with toxins. Supposedly, they work well against rats and mice while being safe for larger animals, including birds of prey, to consume. People may wish to consider using these baits as well for rodent control. https://ecoclearproducts.com/ One of my favorite pest-control measures is furry and purrs when I stroke him. Currently we don't have rodents, but he likes to catch insects as well. reply jncfhnb 16 hours agorootparentCats generally don’t like hunting rats reply tomrod 15 hours agorootparentFunny, our barn cats growing up loved hunting everything, including rats. Maybe indoor cats might not be so inclined though. reply jncfhnb 10 hours agorootparenthttps://crittercontroloftampa.com/blog/do-cats-get-rid-of-ra... reply p3rls 7 hours agorootparentThe presence of the cat alone will deter rats from nesting nearby. The cat doesn't have to kill the rat just has to make it uncomfortable enough to leave. I work in fire protection in NYC (in the area referenced by the website and their experiment even) and see it all the time https://pixeldrain.com/u/hocUQxjk proof :) reply tomrod 9 hours agorootparentprevThis link speaks to a different point with regard to effectiveness, not apparent enjoyment. (And to be fair to the barn cats, they did pretty well keeping not only rats and mice down, but also snakes and scorpions) reply jncfhnb 8 hours agorootparentThey don’t hunt rats because the rats are large enough to threaten the cat. They probably would win 95% of those fights cleanly but it’s risky enough to not be worthwhile. Enjoyment is certainly not a differentiating thing. These are urban rats specifically which are larger and may not apply to country rats reply tomrod 7 hours agorootparentI'll grant some cats don't hunt rats. In our case it was more suburban rats, sometimes they did get big, yes. reply shkkmo 10 hours agorootparentprevThat seems like a biased source given they nake money killing rats. reply dyauspitr 13 hours agorootparentprevThey will if you don’t feed them. reply NPC82 1 hour agoparentprevHere's a link to the general region you describe ordered by most recent sightings. You can zoom in on your specific location and look deeper or draw a new boundary. Hope this helps! https://www.inaturalist.org/observations?nelat=40.7014779017... reply nukeman 16 hours agoparentprevOld World Vultures are not closely related to New World Vultures (which are closer to storks). The mechanism which kills vultures in India and Africa does not present the same issue in vultures native to North America. reply mkl 7 hours agorootparentThe bit about being closer to storks seems disputed, and contradicted by more recent DNA evidence: https://en.wikipedia.org/wiki/New_World_vulture reply LAC-Tech 13 hours agorootparentprevAll my schoolboy taxonomy feels useless now! Crazy that it was convergent evolution. reply codezero 17 hours agoparentprevI am in the South East too, but I see lots of vultures, but one weird? thing I noticed in the past 4 years is that I see them a lot more often in suburban areas/on sidewalks eating roadkill than I had seen them in the past (usually seeing them circling freeways or rural areas) reply Shawnj2 17 hours agoparentprevIIRC the issue is that new common drugs for humans contain toxins that are deadly to vultures in high doses. One issue with just creating a vulture sanctuary around towers of silence is that it’s nearly impossible to tell if a person has had any of those drugs and if so how much. Plus in that culture cremation/other forms of disposing of bodies are essentially sacrilege because the idea is that having the body touch the earth, fire, or water would be a bad thing which is why they do sky burials in the first place so it’s not like they can just refuse burials. reply devilbunny 13 hours agoparentprevTurkey vultures still going strong in my part of the SE USA. reply Ichthypresbyter 11 hours agoparentprevMaybe they flew north? I've seen plenty of them in Maryland this spring. reply bbarnett 18 hours agoparentprevIt's a well established fact that animal populations are cyclictic, booming then starving, then booming again. Predator finds prey aplenty, and so is fruitful and multiplies. Then prey becomes so numerous, it eats prey until there are few left. Predator then has a population crash, and the prey rebound without predation. Over and over this happens. Is that what is happening here? Perhaps, as carrion eaters are susceptible too, when this happens to other populations. After all, during this cycle prey and predator both crash... leaving less carrion. And then of course carrion eaters can overpopulate too.. So I wonder, is this just another clickbait headline? reply nothercastle 18 hours agorootparentNah it’s probably rat poison here reply tdb7893 9 hours agorootparentprevNo it's not a click bait headline. Birds that eat carcasses have been in steep decline in many places because of very specific human generated pollutants in the dead animals (in India's case a pain reliever, in California it was lead, etc). reply EasyMark 14 hours agorootparentprevthat is probably true in the absence of humans, but modern humans ruin all these cycles when they become part of them, often kill apex predators just because they're bored or their activities inadvertently kill them (poisoning everything in site because you don't like mice/rats) reply totalconfusion 15 hours agorootparentprevThat's interesting, I wonder if this accelerates evolution from environmental pressure massively. reply yawpitch 17 hours agorootparentprev> So I wonder, is this just another clickbait headline? No. As the article makes abundantly clear there’s plenty of carrion, it’s just poisoned. Nothing natural about it. reply bbarnett 14 hours agorootparentI see that's not even traditional poison, but instead something meant to help the cattle. Unfortunate. reply yawpitch 5 hours agorootparentIt’s not meant to help the cattle, it’s meant to preserve profits by treating the cattle for a disease they only have because they’re kept in unsanitary and overcrowded conditions. The loss of the towers of silence is a turducken of human incompetence. reply rangerelf 17 hours agorootparentprevNice try Mr. Rat Poison Salesman. reply odyssey7 18 hours agoprevThere’s a global epidemic of grievous inflammatory disease in cattle caused by the paratuberculosis bacterium. The challenge can be solved entirely by vaccines, it’s just not prioritized. You can simply “cull” a sick cow and sell the diseased meat in the grocery store. The article doesn’t say what the inflammatory diseases in the cows are in India, but I wonder if it’s the same. Edit: Yes, researchers have identified widespread Paratuberculosis infections in India. “Our research on screening of over 26,000 domestic livestock for MAP infection using 4 different diagnostic tests (microscopy, culture, ELISA and PCR), during last 31 years has shown that the average bio-load of MAP in the livestock population of India is very high (cattle 43%, buffaloes 36%, goats 23% and sheep 41%).” [1] [1] https://pubmed.ncbi.nlm.nih.gov/29090657/ reply mehulashah 16 hours agoprevIt’s interesting how burial rituals change over time. Now, we’re talking about composting our bodies, which in some sense returns the remaining minerals back to where they came from in the earth. reply dyauspitr 13 hours agoparentWhich is what burial also accomplishes. What’s composting a human body outside of a burial? reply mos_basik 6 hours agorootparentI think the difference is that if you go to a funeral home in the US and say \"I'd like one burial, plain, no sugar\" you will end up dressed in fancy clothes, filled with formaldehyde and a decent amount of sawdust, lying on soft polyester cushions and frills in a fancy varnished hardwood box with a lacquer finish and nearly airtight seal. This goes into the ground and then (I'm less to believe) largely doesn't change for decades. Whereas if you go to a human composting place and ask to be composted, you get packed into a tube with enough wood chips and straw to create the right carbon to nitrogen ratio and a few scoops of microbes and fungi and held at an optimum temperature and oxygen content until you turn into soil. Then you get delivered in the back of a pickup truck and hopefully someone uses you to plant a tree or something. Not to say that burial is always mutually exclusive with returning your nutrients to the earth - just that the default approach to burial these days isn't so good at it. reply defrost 6 hours agorootparentprevOddly enough, from 7 days ago: What is human composting and should it be legalised? https://www.abc.net.au/news/2024-04-27/calls-to-legalise-hum... mos_basik mostly has it aside from bone grinding at some point, human skulls turning over in the market garden can be disconcerting and a nuisance for law enforcement so it's considered polite to remove and|or grind them. reply not_your_mentat 8 hours agoprevI get sky burials. I'm still waiting for someone to explain sky births to me. reply chefkd 8 hours agoparentIn my head its giving birth wile skydiving or on reentry from orbit but I thought the same thing about sky burials until I googled it haha :) reply jajko 18 hours agoprevEven in Iran, the place for zoroastrianism, the towers of silence I've seen are completely unused for decades. I've visited ie one in Yazd just outside the city some 8 years ago when international relationships were at highest point (if I knew I would go there ten times, absolutely amazing country and people, completely unspoiled by mass tourism). Zoroastrianism is fascinating and probably first major monotheistic religion and later ie judaism took a heavy inspiration from it. So much for everybody yelling how holiest their truth is and how chosen they specifically are, a lot of folks and mankind overall would benefit massively if they traveled more and more remote. I know I did. They have concrete building not far where they put their dead instead, forgot the exact procedure unfortunately but clearly its still ok within their religion. If in holiest city of whole religion which houses 'eternal flame' they can manage this, maybe other places should take a note too? reply psunavy03 16 hours agoparent> Even in Iran, the place for Zoroastrianism, the Towers of Silence I've seen are completely unused for decades. The fact that Iran is for all intents and purposes an Islamic theocracy with a veneer of democracy on top probably has something to do with this. reply LAC-Tech 13 hours agorootparentYou could have spent 5 minutes looking it up, rather than speculating. reply xhevahir 15 hours agorootparentprevYeah, no kidding. Iran hasn't been \"the place for Zoroastrianism\" in a very long time. reply jajko 14 hours agorootparentIf you think that Zoroastrianism there is heavily punished there by authorities, you are severely mistaken. Officially their number is roughly the same as in India, but in reality its much much more and they face no repressions that I could anyhow see or gather from talking to locals. But its not hard to understand why official numbers would not represent reality, unlike say in India which is cca democracy and this group is quite powerful there. Or do you have more of personal experiences from Iran that contradict mine? I've also been to beautifully restored old christian church in the heart of the big city (forgot which one, maybe Isfahan), no issues I just went in, there were masses happening there regularly. No hassle anywhere. reply moneywoes 10 hours agorootparentis the decline due to the restriction around who can convert into the religion reply vundercind 8 hours agorootparentI think it’s also one of those doesn’t-seek-converts plus you-leave-if-you-marry-outside religions. Those two things plus lots of secrecy around actual beliefs and practices (that one’s less of a problem with Zoroastrianism iirc) seemed to always be factors in a book I once read about vanishing near-East religions. Basically mimetic-evolutionary action, if the religion has the wrong properties it’ll tend to decline. reply rustcleaner 17 hours agoparentprevTake with a grain of salt, but if Robert Sepehr's videos have any truth to them then the history and traditions which interweave from that time and place are rich and far reaching. Sepehr: https://www.youtube.com/watch?v=6xf4FOpS1rU https://www.youtube.com/watch?v=OBkgI34mMH0 NOT Sepehr: https://www.youtube.com/watch?v=6fI8rXPalMA https://www.youtube.com/watch?v=kcMUBpC4leM (Do your due dilligence!) reply rustcleaner 14 hours agorootparentOUCH link 3 went private! It is titled \"Sex The Secret Gate to Eden Gnostic Teachings\" by Thelema Press and should be found elsewhere (if I notice it seemingly fully disappear, I can make it re-appear so seekers may find it and imbibe). reply kortex 9 hours agorootparentIt's not every day you come across Gnostic stuff on HN, but those links are super fascinating, I can't wait to check them out! Zoroastrianism is such a fascinating topic. reply tazu 8 hours agorootparentprevSepehr is by far my favorite anthropologist, his videos are great. reply moneywoes 10 hours agoparentprevwhat did you learn? very interested reply kortilla 16 hours agoparentprev> So much for everybody yelling how holiest their truth is and how chosen they specifically are This is a non-sequitur and travel definitely will not solve the issue. People are very well aware of the many other religions even now currently being practiced and are still convinced theirs is correct. reply vundercind 8 hours agorootparentI assure you a lot of Christians don’t even have a basic understanding of the Mediterranean and Levantine political situation at the time of the gospels (which kind of features heavily in them) let alone anything about contemporary local events in Messianic Judaism and anti-Roman sentiment. Other religions, currently-active or ancient (or both)? Nah. Zero or near-zero knowledge for most. That’s nerd shit. reply jajko 14 hours agorootparentprevI am not claiming it will automatically click with every single simpleton who has been spoon fed one single truth since birth, thats extremely hard place to ever get from. But seeing culture, people, history, and religions too puts a massive perspective change. I could see that ie my wife traveling across India backpacking had her eyes opened quite brutally compared to her strict catholic upbringing, and it was definitely this cultural/religious shock. Seeing other religions as equal to yours - how many folks do you know that actually have that? Seeing refugees not as bothersome scum coming to rape and steal and take social payments but same people as you, with same type of dreams, fears etc... again this ain't something you will ever get to from watching news, and that's how most people get all their relevant info, thats the folks likes of trump feed from, from their fears and hate for things they don't actually know, only heard about. Being treated the nicest from poorest people of the world (dalits in India in my case), complete stranger yet they shared the very little they had with me, and helped me tremendously, repeatedly. There is tons of islamophobia in Europe, especially in eastern part but I can see it literally everywhere. Most folks like that I talked to have absolutely 0 clue, they just pick few worst news about terrorism attacks, some 'alternate' media talking same stuff for 2 decades. Yes going to all-inclusive package tour to say Egypt and staying in absolute tourist bubble for 2 weeks ain't going to change anybody. I wasn't talking about that sort of traveling, but exact opposite of it. reply LAC-Tech 13 hours agoprevI'm always a little jealous that there still exists an active, continuously practised Indo-European religion in Iran, even if a minor one. They have something that Europeans and their diaspora have lost. reply dyauspitr 13 hours agoparentHinduism. It’s 1/7th of humanity. reply LAC-Tech 12 hours agorootparentSure, but that's further away from Europe :) reply dyauspitr 11 hours agorootparentI feel you. Unfortunately once it’s gone, it’s gone forever. Any attempt at revival seems corny and contrived and lacks any gravitas. reply moneywoes 10 hours agoparentprevIndoEuropean meaning? reply mkl 6 hours agorootparentIt's a language family [1] deriving from a hypothetical shared root language and population, with hypothesised religion [2]. Abrahamic religions' origins, like the Semitic languages they're associated with, are not Indo-European. [1] https://en.wikipedia.org/wiki/Indo-European_languages [2] https://en.wikipedia.org/wiki/Proto-Indo-European_mythology reply herogayab 16 hours agoprevnext [2 more] [flagged] thuuuomas 16 hours agoparentWhy reply dctoedt 19 hours agoprev [–] \"Our culture\" really means \"the ways in which we've become accustomed to doing things (because reasons), and which we believe everyone else should recognize as an entitlement for us.\" Likewise, \"our culture is dying\" seems like an overly-dramatic catch phrase for, \"we don't like it that circumstances are changing and reducing our ability to do things in the manner to which we've become accustomed and are now entitled.\" EDIT: As another commenter correctly says, it's not a great idea to drive vultures into extinction. My comment is about people whinging about it in a way that suggests it's all about them. reply amenhotep 19 hours agoparentPeople in valuing intangible things for not obviously rational reasons that can be easily dismissed if you approach the entire world with an attitude of smug cynicism shock reply dctoedt 18 hours agorootparentValuing intangible things ≠ whinging when those intangibles are no longer adaptive to circumstances. reply bbarnett 18 hours agorootparentOne thing that really bothers me, is a frozen culture. For example, Native Canadians would have a changed culture by now, 500 years after we showed up. Before our arrival, there was war between Native nations, cultural change due to trade and new inventions, and so on. Unless people believe that Natives didn't have new ideas, this is clearly so. And Native history shows change! To imagine that 500 years later, no music, language, culture, technology would have changed, is a massive disservice to both the intellect and capabilities of those peoples. Yet we enable things such as unrestrained hunting, and even whale hunting, for cultural reasons. As if a healthy Native nation wouldn't stop hunting endangered species! Come on! So yes, static \"this is the way it was\" is a load of hurtful outcomes. For all we know, Natives might have invented our tech by now, had we not intervened. reply pbj1968 13 hours agorootparentWell, given as they couldn’t come up with a functional wheel on their own… highly doubtful. reply bbarnett 9 hours agorootparentYou need a few things to care about wheels. Some sort of idle class with free time, domesticated draft animals, and an industrial base. Basically, people need to stop moving around and hunting to live, which means crops and domesticated food/work animals. And large cities for a strong industrial base. There were few places where all that came together. Maybe the Incans? But they also has waterways and slaves. But I agree, the timing was short. reply bobthepanda 15 hours agorootparentprev> As if a healthy Native nation wouldn’t stop hunting endangered species! The North American megafauna didn’t disappear all by themselves. More recently, the Maori certainly made some species go extinct in New Zealand. https://en.wikipedia.org/wiki/List_of_New_Zealand_species_ex... If you’re going to recognize that natives/first nations/etc. have agency then we have to accept the good and the bad instead of infantilizing them according to some fantasy ideal. reply bbarnett 15 hours agorootparentThat wasn't my meaning. We have hunting quotas, and attempt to mitigate over fishing, and so on. Yes, we aren't perfect, but we try. My point was, if a contempary Native nation existed, had modern agriculture methods, and was able to subsist without hunting? I suspect they would self regulate as we do. reply geraneum 10 hours agoparentprev> \"Our culture\" really means \"the ways in which we've become accustomed to doing things (because reasons), and which we believe everyone else should recognize as an entitlement for us.\" This could be the laziest definition of culture that I’ve read so far. reply dctoedt 10 hours agorootparentIt's a translation of the word as used in context. I suppose I could have added \"Here,\" to be more clear that it's not a definition. reply geraneum 8 hours agorootparentRight, this could be the laziest translation (to English?) of the word culture in any context, that I’ve read so far. There, I fixed it. reply dctoedt 7 hours agorootparent\"Laziest\" — you misspelled concise, right? Or I guess you meant succinct. reply sungho_ 19 hours agoparentprevYou seem to think that the only value of something is its practicality reply oivey 18 hours agorootparentI doubt that. You’d need to test taking away one of this person’s non-essential luxuries to find out. reply dctoedt 18 hours agorootparentprevnext [3 more] [flagged] sungho_ 18 hours agorootparentCheck this: https://news.ycombinator.com/item?id=40258051 reply dctoedt 18 hours agorootparent\"Objection, your honor, non-responsive.\" reply ericd 18 hours agoparentprevIf you read the article itself, it's probably also just generally not a great idea to drive vultures into extinction. reply dctoedt 18 hours agorootparent> it's probably also just generally not a great idea to drive vultures into extinction. On that we definitely agree! My comment is about people making it all about them. reply Boogie_Man 18 hours agoparentprevCultural and religious traditions bind us to the past and the future. reply drawkward 19 hours agoparentprevMonocultures are fragile. reply exe34 18 hours agoparentprev [–] This point of view isn't common, get used to it! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The decline of vultures in the Indian subcontinent is jeopardizing Zoroastrian burial customs, notably for the Parsi community that relies on vultures for body disposal at towers of silence.",
      "Vulture population decrease is mainly attributed to poisoning from diclofenac given to cattle, prompting initiatives like banning the drug and captive vulture breeding as solutions.",
      "Efforts such as prohibiting diclofenac usage and reintroducing captive-bred vultures into the wild aim to avert the extinction of vulture species."
    ],
    "commentSummary": [
      "The decline in vulture populations, mainly due to poaching, poisoning, and consumption of harmful substances, endangers Zoroastrian burial rituals.",
      "Vultures, known for their social nature, intelligence, and unique behaviors like sharing food and soaring on updrafts, are facing threats from NSAIDs and pollutants in their food sources.",
      "The conversation emphasizes the challenge of balancing personal beliefs with broader societal and environmental issues, impacting cultural preservation and shaping perspectives through travel experiences."
    ],
    "points": 172,
    "commentCount": 112,
    "retryCount": 0,
    "time": 1714824799
  },
  {
    "id": 40260210,
    "title": "Remembering Atari's Iconic Designer, Mike Jang",
    "originLink": "https://arcadeblogger.com/2024/02/13/ataris-mike-jang/",
    "originBody": "Article Atari’s Mike Jang Posted by Tony on February 13, 2024February 15, 2024 Some sad news to report on the blog this week. I got word that long-time Industrial Designer at Atari coin-operated division, Mike Jang, has passed away. I’ve known Mike since 2016, and whilst we never met, he was always on the other end of an email answering my questions and providing really valuable input into my articles here on the blog, and especially so for my book Missile Commander. Mike has always been approachable and happy to share his knowledge and deep insight into the industrial design processes at Atari. Learning his craft at San Jose State University, Mike joined Atari’s Industrial Design team during the mid 70s and remained there through the eighties. This meant he was fortunate enough to witness the explosion of Atari’s fortunes right through the Golden Age of arcade videogaming. He was right there in the engine room of Atari contributing to the coin-op division’s game output. As an Industrial Designer, his main role was to refine the ergonomics of arcade cabinets – he was a big advocate of making sure that the physical interface players used to interact with Atari’s game cabinets was just right; and if that meant bespoke controls, parts and mouldings were required, then he would push the company to make it happen. He brought his own artistic flair to his design work, creating some of the most iconic cabinets from the era. Mike Jang at work here. I believe that was an unreleased kiosk of some kind, intended for installation at MacDonald’s outlets across the USA. The project never came to fruition It is very easy to take for granted some of Atari’s cabinet design choices, but all were deeply thought through by Mike and his team, extensively tested and then produced – mostly in-house – resulting in the most varied portfolio of arcade cabinets ever produced by one manufacturer. Atari were the industry leaders in this regard by a country mile. Mike’s work was extensive and he was responsible for many of Atari’s patents, made in his name. This picture was taken in 1981. Mike is captured here discussing the design of Atari’s Asteroids Deluxe cabinet with colleague Dave Cook: Mike is on the right, discussing Asteroids Deluxe with fellow designer Dave Cook at Atari’s offices Mike had the opportunity to fly to Atari Ireland to oversee the initial production of one his cabinets, Roadblasters. Whilst there, he shared these photographs with me that he personally took at the Tipperary facility: Mike’s photographs of Asteroids Deluxe cabinets on the factory floor at Atari Ireland. Click for larger images I documented in my book the extensive work that Mike put into the original design of the Missile Command prototype cabinet. He had this to say: Arcade games were pre-CAD so we drew up the cabinet with drafting tables, pencil, and paper. Each piece of wood or plastic was drawn and sent to the engineering woodshop to be built. Metal parts went to another in house shop. Everything was done inside our building so as to keep the game secret from the competition. Extract from interview with Mike Jang discussing the industrial design process at Atari The workspace at Atari Industrial Design where Mike Jang worked in the 70s and 80s Mike sketched out the initial ideas for Missile Command, based on US military nuclear consoles – keeping the vibe relevant to the game’s subject matter: Mike Jang’s initial sketch of his vision for the Missile Command cabinet The first main goal is to build a game that is put out for testing. Usually only one was built since one never knew if it will make mega bucks or no bucks. If the first test goes good, we might build and test more. That was a sales and marketing call. If it tested good, there would be a big push to go into production as soon as possible. Reason being, unscrupulous companies would spot a game on test and start making their own illegal copies. It’s better to get the game out and sell as many as possible before the copiers showed up. That was more cost effective than going to court and sue. Extract from interview with Mike Jang discussing the industrial design process at Atari Missile Command prototype cabinet When the original Missile Command cabinet came back from testing with negative feedback about the large marquee, he went back to his design sketch and made a pretty dramatic change: The large marquee had to go! Notice the new red lines on this cabinet draft, outlining the proposed shape of the production version of Missile Command that we are familiar with today As you can see, a key part of Mikes job, was to provide solutions to problems arising from creating arcade cabinets, testing hardware, either internally or out in the field, inevitably creates issues that will need fixing, requiring the industrial design team to go back to the drawing board to amend their initial creations. This would be a pretty thankless task as you can imagine, but Mike was happy to oblige most requests. I didn’t interact with Mike Jang a great deal, but when I was doing temperature testing of the games and found the temperature inside was too high Mike didn’t have a problem putting in a vent, sometimes an extra vent, and sometimes a fan. Jed Margolin, Atari engineer and colleague Here are some more cabinet designs that Mike was responsible for during his time at Atari: One of Mike’s earliest patented cabinet designs was Starship 1, released in 1976. You can click these images for larger versions The United States patent for the Hard Drivin’ video game cabinet design Another of Jang’s creations was the Star Wars cockpit design I spoke to Mike about his work on the Star Wars cabinet, and he had this to say: I started the concepts for the Star Wars cabinet and later another designer did more detailed work on the plastic part in front of the monitor. One of the main elements I sketched up were the hydraulic ram shapes on the plastic parts. Those rams were often seen in the movie, especially the ramp to the Millennium Falcon. Also I wanted to continue the mechanical theme by adding that truss style design to the sides of the roof. I was concerned because that was a particle board part that was cut with an angled router bit. Then the bare particle board was just painted black. I was worried about the wood texture appearance but nobody noticed after everything else was put in place. Mike Jang discussing Star Wars cabinet design Atari’s Star Wars production cockpit cabinet Atari Stun Runner. Click for bigger pics Atari’s ultimately doomed Starfighter cabinet. Designed and partially completed in 1984, the game was never released, but made it into the film of the same name. Click for larger versions of these images Here’s Mike’s concept drawing of Atari’s RoadBlasters cabinet. The final production cabinet remained pretty true to this sketch. This was the cabinet that Mike travelled to Atari Ireland to oversee during early production for the European market. “I went with Dave Cook to Ireland, we both had a project starting in the Ireland factory. So we were sent to solve any problems that might come up. They were sent the paper printouts of the cabinet parts to be made. We visited the wood shop that was building the first proto RoadBlaster. The cabinet looked great so we had no problems there” A roofless cockpit driving cabinet – intended for use in an eventually shelved laser disc game called Malibu Grand Prix Another two games that spring to mind are Firefox, where Mike was responsible for the control panel design, and Hydra which used one of Mike’s full cabinet sketches. His full discography of arcade cabinet design is too extensive to cover here, but his work spanned almost two decades and was extremely varied as you can see. Post Atari, Mike was big on Hot Rodding and took his design skills to mock up car designs. Here is his 1951 Mercury coupe that he acquired in 1989 and spent many hours meticulously customising. He still owned this car right up to his passing: Mike’s passion project – his customised 1951 Mercury coupe Arcadeblogger.com is a richer experience thanks to Mike’s input over the years, and my book would be a lesser tome were it not for Mike’s unique perspective and detail on the events leading up to, and after, the creation of the prototype Missile Command cabinet. Here’s a great shot of Mike, captured in 1979 demonstrating the size of Atari’s Hercules pinball cabinet Although Mike is no longer with us, his design legacy does live on in some of the fantastic looking arcade hardware he created – have a look out for these machines next time you’re at an arcade, and raise a glass when you play. One of the things that Mike did recently, was to donate all of his old Atari documents for archiving at Stanford University. The Michael Jang collection of Atari materials, 1978-1991 can be accessed and viewed by researchers for years to come, which is great news. Mike I’m sure will be deeply missed by his Atari colleagues, who speak very highly of him as a designer, but also a friend and person; and based on my interactions with him, I would absolutely echo those sentiments here and send condolences to his family and friends. I hope in a small way, this tribute highlights some of Mike’s superb work. See you next time. Tony Type your email… Subscribe Share this article! Tweet WhatsApp Email Like Loading... arcadearcade historyatariAtari coin-opAtari Industrial DesignIndustrial DesignMichael JangMike JangMissile CommandNintendoplaystationretrogamingvideo games",
    "commentLink": "https://news.ycombinator.com/item?id=40260210",
    "commentBody": "Atari's Mike Jang (arcadeblogger.com)164 points by speckx 13 hours agohidepastfavorite6 comments zuluonezero 11 hours agoThe design of the Star Wars cabinet stands out as the single most immersive game experience in my life. It fired my imagination. I was always bad at that game but while playingit I was a Starship Pilot! But I could clock Missile Command so I learned to play it with my feet sitting on a stool. It's not sad news for me that Mike has died, we all do, and seemed to have lived a full life. Thanks for giving some insight into it. reply 082349872349872 2 hours agoparent> I could clock Missile Command I saw reentry vehicles doing their thing once in my life, and all I could think was that the flipping parallel lines in the sky looked way too much like melonfarming Missile Command. reply DonHopkins 8 hours agoparentprevI vividly remember once visiting Atari Cambridge Research Labs with my friend Devon, and seeing a Star Wars cabinet with a huge bus of ribbon cables coming out of it and going to a Lisp Machine. It left a huge indelible impression on me (as a committed “Star Wars Freak” who collected all the trading cards in elementary school and can still perform a great Chewbacca impression), although I never got to actually play that. I did later have the console version of Star Wars in my garage in Mountain View though, but never owned a cabinet like the one at Atari Cambridge Research Labs, or a Lisp Machine to go with it! I just found Bill St. Clair’s LinkedIn account and resume that mentions it: https://www.linkedin.com/in/wwstc/details/experience/ Lisp hacker, Atari Cambridge Research Lab 1984 - 1984 · Less than a year Symbolics Lisp Machine code to use a Star Wars arcade game as a graphics output device, 6805 code loader, 3D turtles, maze game (jsMaze.com was my second try at this). https://web.archive.org/web/20160315014835/https://lisplog.o... https://news.ycombinator.com/user?id=bill-stclair https://billstclair.com/ That sounds like the most amazing job in the world! And it confirms that I was not just dreaming. He also hacked lots of Lisp at places like Computer Interactive Services, Thinking Machines, Apple Computer, Digitool, Shaker Computer & Management Services, ITA Software, and Closure Associates. Anybody remember what was up with that? This document mentions lots of other cool stuff with Lisp Machines at Atari Cambridge Research Lab, but not the Star Wars machine: https://tcm.computerhistory.org/CHMfiles/Atari%20Research%20... But I can’t manage to find any more information about it. https://news.ycombinator.com/item?id=18803966 DonHopkins on Jan 2, 2019parentcontextfavoriteon: How Atari created the Star Wars arcade game (2017) I saw an Atari Star Wars cockpit at Atari Cambridge Research Labs, with a huge mess of cables draping out of it, hooking it up to some other equipment. Here are is a video playlist from Cynthia Solomon with demos of other cool stuff they did at Atari Cambridge Research, with Alan Kay, Margaret Minsky, David Levitt, Gumby, and other amazing people: https://www.youtube.com/watch?v=CR2CwKculBU&list=PL850B65ECB... https://groups.google.com/g/alt.games.video.classic/c/WQ62u7... Don Hopkins, Dec 8, 1992, 6:23:20 AM en...@cs.montana.edu (Jacob Cormier) writes: >Hasn't anyone here played Star Wars, the arcade game?!? It's the best! If I ever see that at an arcade again, I will whip out my credit card and buy it on the spot! I had a version for my Commidor64, but it was garbage. My housemate brought home a Star Wars machine one day, and it became an important part of our lives. A very well designed game! After you blow up the Death Star, there's just enough time for bong hit. -Don Richard Stueven, Dec 8, 1992, 10:31:28 PM In (an earlier article), (someone) wrote (something like) \"In Star Wars, after you blew up the Death Star, there was just enough time for a bong hit.\" In article 75...@ultb.isc.rit.edu, tjg...@ritvax.isc.rit.edu () writes: >What is the bong hit in Star Wars? Heh, heh. :-) gak Richard Stueven g...@wrs.com attmail!gakhaus!gak 107/H/3&4 reply cmsonger 10 hours agoprevRest In Peace. I was a pre-teen when I first learned to program on a TRS-80 Model 1. I was, at the time, an expert in BASIC and Z80 assembly. At the same time I was asking my parents for quarters to play space invaders. And that's where I feel this article. These are the shoulders I stood on. There were people who made that first generation of video games and personal computers. I benefited from their work. Their work launched me into what has been a great career. But they are aging out and dying. Their work was foundational. And, at least to me, inspirational. reply videotopia 7 hours agoparentYou'll probably enjoy the Ted Dabney Experience Podcast: https://www.teddabneyexperience.com/ reply jnaina 7 hours agoprev [–] Rest in Peace. Atari was the singular inspiration in my teen years, both the video games and the home computers. Ad Astra, Mike. Thanks for all your great work. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mike Jang, a renowned Industrial Designer at Atari's coin-operated division, has passed away, leaving behind a legacy of iconic arcade cabinet designs like Missile Command and Star Wars.",
      "Jang was known for his meticulous attention to detail, focus on refining ergonomics, and creating immersive physical interfaces for players, overseeing the production of games like Roadblasters.",
      "His design heritage endures in the arcade hardware he crafted, and his work documents are archived at Stanford University for future research, marking his significant impact on the gaming sector."
    ],
    "commentSummary": [
      "The article explores how Atari's Star Wars arcade game influenced people and mourns the recent loss of Mike Jang, prompting users to reminisce about their nostalgic Atari gaming moments.",
      "Users acknowledge the developers for shaping the gaming industry and express gratitude towards them for their contributions.",
      "A podcast honoring Ted Dabney, a video game industry trailblazer, is also highlighted in the discussion."
    ],
    "points": 164,
    "commentCount": 6,
    "retryCount": 0,
    "time": 1714856162
  },
  {
    "id": 40261001,
    "title": "Vera Rubin Observatory: Major Milestone as Mirror Receives First Reflective Coating",
    "originLink": "https://www.universetoday.com/166842/vera-rubins-primary-mirror-gets-its-first-reflective-coating/",
    "originBody": "Posted on May 3, 2024May 3, 2024 by Evan Gough Vera Rubin’s Primary Mirror Gets its First Reflective Coating First light for the Vera Rubin Observatory (VRO) is quickly approaching and the telescope is reaching milestone after milestone. A few weeks ago, the observatory announced that its digital camera, the largest one ever made, is complete. Now the observatory has announced that its unique primary/tertiary mirror has its first reflective coating. The Rubin’s massive digital camera has an important job and garners a lot of attention. But it’s powerless without the telescope’s innovative primary/tertiary mirror. Primary mirrors are always the most critical and time-consuming part of modern observatories. The VRO’s primary/tertiary mirror took seven years to make. The mirror is called a primary/tertiary mirror because it comprises two optical surfaces with different curvatures. The primary mirror is 8.4 meters, while the tertiary mirror is 5 meters in diameter. The pair of surfaces are combined into one large structure. The unique design reduces the telescope’s engineering complexity without reducing its impressive light-gathering capability. It can be rotated quickly and also settles quickly. The VRO’s unique primary/tertiary mirror is two mirrors in one. It’s mounted on lightweight honeycomb material for strength. Image Credit: VRO The outer surface forms the primary mirror. It captures light from space first, then that light reflects upwards to the 3.4-meter secondary mirror. After that, it’s reflected back down to the inner 5.0-meter surface that forms the tertiary mirror. Then, the light is sent to the camera. The primary mirror’s size is critical because it determines how much light the telescope can collect. More light means astronomers can study very faint or distant objects. The VRO’s design allows the camera to capture a large area of sky the size of 7 full moons across in a single image. via GIPHY Only meticulous engineering and construction can build a telescope like this. One of the stages is putting the reflective and protective coatings on the mirrors. The VRO announced that the primary/tertiary mirror has its first coating. “This was a very well-conducted project from every angle, thanks to a combination of careful planning and the technical skills of our excellent team.” Tomislav Vucina, Senior Coating Engineer, VRO The VRO has a special onsite coating chamber built just for this purpose. It’s a 128-ton chamber on the observatory’s maintenance floor. It uses a process called magnetron sputtering to apply coatings. The chamber will be reused during the telescope’s lifetime whenever the mirror needs re-coating. The chamber can apply coatings of different reflective materials alone or in combinations. It took a lot of work to determine the perfect coating for reflectivity and durability. Researchers tested different coatings on a steel stand-in mirror. The first layer was an adhesive layer of nickel-chromium. Next came an incredibly thin layer of silver weighing only 64 grams spread over the 8.4-meter mirror. On top of that, another nickel-chromium adhesive layer, then a protective layer of silicon nitride to shield the reflective layer. The person in charge of these precision coatings is Tomislav Vucina, the Senior Coating Engineer. Vucina describes the coatings as a balancing act. “This outer layer needs to be thick enough that it’s not worn off by cleaning,” said Vucina, “but not so thick that it absorbs too many photons and prevents the mirror from meeting Rubin’s scientific requirements.” This image shows the Rubin Observatory’s 8.4-meter combined primary/tertiary mirror after being coated with protected silver in April 2024. The reflective coating was applied using the observatory’s onsite coating chamber, which will also be used to re-coat the mirror as necessary during Rubin’s 10-year Legacy Survey of Space and Time. Image Credit: RubinObs/NOIRLab/NSF/AURA Until these coatings were applied, the glass was just glass. Highly specialized glass, but glass nonetheless. Now that the glass has received its reflective silver coating, it’s truly a mirror. The application process took only 4.5 hours, nothing compared to the 7 years required to build the primary/tertiary mirror. Vucina and his team subjected the mirror to a battery of tests: reflectivity, adhesion, pinhole, and cosmetic. According to Vucina, the application process was successful. “This was a very well-conducted project from every angle,” said Vucina, “thanks to a combination of careful planning and the technical skills of our excellent team.” It’s been a long road to completion for the VRO. But after a long wait, first light is rapidly approaching. Excitement and anticipation for the observatory’s unique and powerful scientific contribution is growing. Its main output is the decade-long Legacy Survey of Space and Time. “We’re extremely excited that both mirrors are now coated and will be installed on the telescope very soon,” said Sandrine Thomas, Deputy Director for Rubin Construction. “The combined reflectivity of these mirrors will enable Rubin to detect very faint and far-away objects, leading to great science!” Share this: Click to share on Facebook (Opens in new window) Click to share on Twitter (Opens in new window) Click to share on Reddit (Opens in new window) Like this: Like Loading... CategoriesAstronomy",
    "commentLink": "https://news.ycombinator.com/item?id=40261001",
    "commentBody": "Vera Rubin's primary mirror gets its first reflective coating (universetoday.com)118 points by bikenaga 11 hours agohidepastfavorite28 comments npunt 7 hours agoVera Rubin was quite an astronomer [1] both in her discoveries like the rotation of galaxies (confirming dark matter), and for being a trailblazer for women in astronomy. I love reading bios of people like this. Glad to see her get recognized with such a great observatory [2] continuing her legacy of studying dark matter. [1] https://en.wikipedia.org/wiki/Vera_Rubin [2] https://en.wikipedia.org/wiki/Vera_C._Rubin_Observatory reply jpizagno 1 hour agoparentFritz Zwicky discovered Dark Matter. Rubin thought she discovered Dark Matter, because she wasn't aware of Zwicky's paper written in German many years before Rubin's paper. edit: I have a PhD in Astronomy, with a focus on Dark Matter, and I measured rotation curves of galaxies myself. reply dylan604 9 hours agoprev\"The VRO’s design allows the camera to capture a large area of sky the size of 7 full moons across in a single image.\" Wow, that's a lot of sky for a telescope. Around 3.5° at once. I purchased a telescope specifically to view DSO, and it is ~3 full moons. When attaching my older DSLR, a full moon isn't even 50% of the captured image. Of course, mine's only ~150mm instead of 8.4m. Just putting size in perspective of my own use instead of using school buses or football pitches or basketball type nonsense. reply mfranc42 8 hours agoparentThat angle of view corresponds to a lens with 588mm focal length on a full frame camera (horizontally), and 392mm (vertically). Funny that astronomers and photographers mean very different things by saying wide angle. ;-) reply dylan604 7 hours agorootparentPeople get stuck into trying to apply A to B when B != A. Just because the same words are used does not mean they have the same use and meaning as applied. Also, \"space is big. You just won't believe how vastly, hugely, mind-bogglingly big it is.\" reply bingbingbing777 0 minutes agorootparentIt's a full frame equivalent. I don't think anyone is trying to use this on a full frame camera. nabla9 43 minutes agoparentprevVRO is a survey telescope. It will survey the all sky visible in the location in just few nights. reply nullc 8 hours agoparentprevIt's about the same angle of view that a 400mm lens on a full frame camera has vertically. (wide field astrophotography can be fun: https://nt4tn.net/astro/horse4.jpg ) VRO has a 10313mm focal length (vs 8.4m diameter, so f/1.234-- though with a huge central obstruction) illuminating a 630mm focal plane. reply ramijames 8 hours agoprevIt's so, so cool that humanity builds these kinds of things. I'm so grateful that there are people who dedicate their lives to this. I wish that I had. reply dylan604 4 hours agoparentyou can always switch careers. you'll just have to switch the lifestyle to which you have become accustomed as well. I'm guessing the average bit banger around these parts earns a much higher salary than the average person in astronomy. You do however have the potential of actually helping humanity understand things rather than droning away on ad tech and social platforms which is definitely not pushing humanity in any thing resembling a positive direction. reply cududa 9 hours agoprevThat video was pretty wild. I didn’t understand at first what had happened when the silver layer got laid down and it was suddenly reflective reply Loughla 8 hours agoparentGod the work that went into that one shot. Not just camera work, but all the engineering for the mirror. It's astounding. reply prpl 7 hours agorootparentThe mirror is the easy part. In fact, it was finished years ago - just not coated. reply dylan604 4 hours agorootparentlulz. Easy. Okay. It might be the easiest part of this build (I have my doubts to the accuracy of that), but let's not get carried away with slinging words like \"easy\" around things that are measured to microns accuracy and took years to build. reply dylan604 9 hours agoparentprevThe video was very cool. The arm swings around and hello! Just like that, it's a shiny mirror. Very impressive effect to be sure reply keitmo 6 hours agorootparentThat was breathtaking. reply mrbluecoat 8 hours agoprevVery cool! HD video: https://rubin.canto.com/v/gallery/album/HDSNU?display=curate... reply euroderf 2 hours agoprevWatching all that monster-size equipment makes me think of the Olympics: is a lot of money being poured into a single use ? Was this equipment pre-existing or purpose-built ? If the latter, will any get re-used/repurposed for other endeavors ? reply ccgreg 1 hour agoparentThere are a bunch of Vera Rubin-sized mirrors in orbit, that's why this one is the size that it is. reply JohnMunsch 7 hours agoprevThis telescope and its mission to catch changes in the sky over time is going to help discover so many things. reply dylan604 4 hours agoparentI love that we're also remembering to take less scoped in to see a wider view. I wonder how many times they will complete the full sky survey. Will there be enough to provide a timelapse from multiple surveys, or will it take the entire scope to complete the full sky? I find some of the timelapse of various objects quite fascinating now that we've been recording observations long enough. Seeing the movement around SagA* is amazing. Just the other day, we saw Cassiopeia A over a couple of decades. Some part of my brain knows things are constantly changing in the universe, but at the time scale it seems strange things are noticeably different within our own lifetimes. reply TheBlight 5 hours agoparentprevThe reaction will be interesting. reply unfamiliar 4 hours agoprevDumb question, but why don’t they just use a single more highly-curved mirror rather than a series of 3 bounces? reply Sharlin 4 hours agoparentMore curved means more optical aberrations. The Rubin is a so-called three-mirror anastigmat design that minimizes astigmatism, coma, and spherical aberration. (Chromatic aberration is not a problem in reflectors because dispersion only occurs when light is refracted.) A two-mirror design couldn’t be used in such a wide-field telescope without severe image quality issues, at least comparatively speaking. https://en.wikipedia.org/wiki/Three-mirror_anastigmat reply nullc 9 hours agoprevThe AAT has a pretty good video on aluminising with a lot of exposition, https://www.youtube.com/watch?v=fztRqJerfOk VRO's process is considerably more sophisticated, as they do the silicon nitride protective coating. I believe the specific approach on VRO is similar to the one pioneered for gemini: https://www.gemini.edu/files/docman/websplash/websplash2004-... Fancier protected metal (and dialectic enhanced) coatings have been common in smaller reflectors... but it's quite a big difference doing a fancy sputter coating on a 150mm object vs a 8m one! :P reply airstrike 7 hours agoprev [–] as much as I'd like to read this, the mobile website is hot garbage due to the number of ads.. if anyone has a better source, I'm all ears reply Larrikin 5 hours agoparent [–] https://www.firefox.com https://github.com/dhowe/AdNauseam reply renjimen 4 hours agorootparent [–] Use Brave if you're on iOS. Firefox on iOS doesn't block ads IIRC reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Vera Rubin Observatory's primary/tertiary mirror has been coated with silver, a critical advancement in the telescope's construction.",
      "The mirror's innovative design, featuring two optical surfaces with distinct curvatures, enables exceptional light collection and wide-field imaging capacity.",
      "The successful 4.5-hour coating process signifies a major achievement in the observatory's journey towards achieving first light."
    ],
    "commentSummary": [
      "The Vera C. Rubin Observatory applies the first reflective coating to Vera Rubin's primary mirror, commemorating her contributions to astronomy focusing on dark matter and galaxy rotation.",
      "With a design enabling capturing extensive sky areas in one image, the observatory aims to survey the sky broadly and uncover groundbreaking findings.",
      "Through substantial construction and engineering work, the observatory is poised to enhance our comprehension of the universe by monitoring temporal variations."
    ],
    "points": 118,
    "commentCount": 27,
    "retryCount": 0,
    "time": 1714863750
  },
  {
    "id": 40258315,
    "title": "Spotify's Backstage: Enterprise IT Tool Expansion",
    "originLink": "https://techcrunch.com/2024/04/30/spotifys-getting-serious-about-its-enterprise-and-dev-tools-business-play/",
    "originBody": "(opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window) Link Copied Featured Article With Backstage, Spotify’s getting serious about its enterprise and dev tools business play Music-streaming giant takes a cue from Red Hat's open source playbook with enterprise support and services Paul Sawers@psawers / 2:00 PM UTC•April 30, 2024 Comment Image Credits: Spotify You know that mildly jarring experience whenever that well-known celebrity shows up in an entirely different context — e.g. a musician making a horror flick cameo; an NFL player rearing their head in a comedy series; or a Hollywood movie icon selling mobile phone plans on TV? Well, it’s starting to feel like that with Spotify’s foray into the enterprise and developer tooling space — nothing wrong with it per se, but it makes you flinch just a little due to its divergence from the norm. What we’re talking about is Backstage, a platform and framework Spotify introduced internally in 2016 to bring order to its developer infrastructure. Backstage powers customizable “developer portals” that combine tooling, apps, data, services, APIs and documents in a single interface. Want to monitor Kubernetes, check your CI/CD status or track security incidents? Backstage to the rescue. Lots of companies construct their own internal systems to help developers work more efficiently. And lots of companies release such systems to the public via an open source license to spur wider adoption, as Spotify did with Backstage in 2020. But it’s highly unusual for a consumer technology company to actively monetize this side of its business, which Spotify has been doing since 2022. Now, Spotify is leaning even further into this play with the launch of a new suite of products and services designed to make Backstage the de facto developer portal platform for the software development industry. Modular Backstage is built on a modular, plug-in based architecture that allows engineers to layer-up their developer portal to meet their own needs. There is already a thriving marketplace for Backstage plug-ins, some developed by Spotify itself and some by the wider community, including developers from Red Hat and Amazon Web Services (AWS) — AWS, for example, has developed a plug-in to make data from Amazon Elastic Container Service (ECS) available in Backstage. Since late 2022, Spotify has been selling a handful of premium plug-ins as a subscription, such as Backstage Insights, which serves up data related to Backstage usage across an organization, including which plugins they’re engaging with most. Backstage Insights plug-in. Image Credits: Spotify The open source Backstage project has been adopted internally by some of the world’s most well-known companies, including LinkedIn, Twilio, American Airlines, Unity, Splunk, Ikea, HP and more than 3,000 organizations. But as with just about any open source project, the main issue with Backstage is the complexity involved in getting set up — lots of integrations, configurations and figuring out how it all glues together. Thus, Spotify is now introducing an out-the-box version of the open source project called Spotify Portal, available in beta from today, which is pitched as a “full-featured, low-/no-code internal developer portal (IDP)” built atop Backstage. Spotify Portal. Image Credits: Spotify Spotify Portal ships with quick-start tools for connecting all their internal services and libraries, replete with setup wizard for installing Portal and connecting it with a company’s GitHub and cloud provider. “When you set up your IDP, typically you need to ingest a lot of software into that, because the point of the IDP is to capture your full software catalogue and map that to the user base, and there’s potentially a lot of integrations involved in,” Tyson Singer, Spotify’s head of technology and platforms, explained to TechCrunch. “And so with Spotify Portal for Backstage, we’ve basically given folks a no-code way to do that.” Spotify Portal: Ingesting software catalog. Image Credits: Spotify Getting SaaS-y? On the surface, this seems like some sort of SaaS play, similar to how a commercial company might offer a fully managed, hosted version of a popular open source product. But that isn’t quite what’s happening here — there is no hosted element to this, though that might change in the future. It’s what Singer calls “Backstage in a box,” one which is deployed within the customer’s own ecosystem either on-premises, or in their own cloud. “It’s the customer who manages it,” Singer said. “What’s important from our perspective is that we’ve really focused on both reducing the startup time and the maintenance time. So that means not only is the setup and the onboarding ‘no-code,’ it’s also the maintenance where we’re reducing code. That really makes it quite easy to manage in your own particular context.” However, in a follow-up question, a Spotify spokesperson clarified that Spotify Portal for Backstage is its “first step towards a managed product,” which means that it more than likely will be offered more like a SaaS service in the future. “We’ve seen a growing appetite for a more managed product that would allow us to share our expertise more directly with companies, and we want to be able to offer more in support of that need,” the spokesperson said. “Portal is our first step on that journey, but in the future, we’re going to expand our offerings as managed.” In addition, Spotify is adding various enterprise support and services to the mix, which it says it has already been providing since last summer but hasn’t disclosed this until now. This includes one-on-one tech support from dedicated Backstage personnel at Spotify, and includes service-level agreements (SLAs), security reviews and incident notifications. And for those wanting to get up-and-running with Backstage in the first instance, Spotify is also offering consulting services. Spooling up In essence, Spotify is now catering to three broad category of users: the core open source project for those with the resources and technical nous to self-deploy everything; the “hybrid adopters,” which is what Spotify calls those that have some of the necessary skills but need some support along the way; and the businesses that need something a bit more oven-baked — which is where Spotify Portal enters the fray. Similar to the pricing structure for its existing plugin subscriptions, which are charged based on “individual customer parameters” such as usage and capacity, the new Portal and enterprise services don’t come with up-front costs. “For pricing, we are referring customers back to our sales organization,” Singer said. “It’s custom pricing.” Given this transition to an enterprise-focused developer tools company, Spotify is also having to staff-up accordingly, though Singer wouldn’t share how many people it would be hiring or allocating to these new support roles. “We are changing how we go forward with both our sales organization and support,” Singer said. “So we’re shifting more focus towards how can we support customers in their initial journey and then also, once they’ve got it set up, their ongoing journey because we do want to be able to support them to get to value as quickly as possible.” All this, it seems, is just the tip of the iceberg as far as Spotify’s developer tooling shift is concerned. The company is adding new features to some of its existing premium plug-ins, and it’s adding more plug-ins to the mix, too. One of these is the “data experience” plug-in, which makes it easier to add individual data entities to a software catalog — this includes built-in “ingestors” to scoop metadata from external data platforms, and make this available across Backstage. Last year, Spotify also teased a totally separate product for software development teams called Confidence, which is like an A/B experimentation platform based on one of its own internal tools. For now, that remains a beta product, but Singer says that it’s “all systems go” as it readies things for prime-time in the future. “We are super happy with the feedback that we’ve been getting from our [Confidence] beta customers so far,” Singer said. “We built out an experimentation platform that is broad and deep, covering a tremendous amount of use cases covering everything from your typical A/B testing on a user surface, to being able to do that across all of our ML [machine learning] use cases. And I think that really sets it aside, as more and more companies are using ML in the same sorts of ways that we are to optimize things.“ Please login to comment Login / Create Account TechCrunch Disrupt 2024 Innovation For Every Stage LEARN MORE Sign up for Newsletters See all newsletters(opens in a new window) By subscribing, you are agreeing to Yahoo's Terms and Privacy Policy. Email Subscribe (opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window) Copy Tags backstage open source Spotify",
    "commentLink": "https://news.ycombinator.com/item?id=40258315",
    "commentBody": "Spotify getting into enterprise IT tools (techcrunch.com)111 points by iamzycon 18 hours agohidepastfavorite62 comments parpfish 17 hours agoI’m not sure who the customer is for Backstage. For a small startup, poking around in the gcp/aws console works well enough. Maintaining another console layer on top of that feels like one more thing that could break. If you’re a big company with a team that could support developer platform tools, you probably have some sort of portal system hacked together and you’ll never be able to change because somebody has a critical dependency on the old system reply arnvald 15 hours agoparentCompanies grow - today you have 50 engineers and a few microservices, in 2 years you might have 200 engineers and tons of APIs and web applications. I think if old orgs could have bought off the shelf developer portals years ago, they would have. Now companies that grow have this option. It might not be a massive market, but there are a few interesting companies building this kind of tools and they sell it reply sbarre 14 hours agoparentprevWe were that big company (50,000 employees, probably about 10,000 or so in technical roles across the enterprise) that had a bunch of custom portals and internal sites for all our dev stuff, and also Stoplight (Spotlight?) for APIs etc.. We were early adopters of Backstage, back in 2020. We've migrated almost all of our portals and \"meta-infrastructure\" to Backstage, and it's certainly not perfect, and not everyone has moved over, but the majority of our teams are in one service catalogue and people can much more easily find service owners, dependencies and more. It's been a great tool and a pretty big internal success story for us. But this move from Spotify tells me that they are no longer stewards of an open source project, they now use open source as a loss-leader for their paid offerings. Which is totally fine, but it will change how we look at that relationship. reply acedTrex 17 hours agoparentprevThe big company use case is the ideal backstage use. Backstage is just a framework. you can use it to interface with the old system behind the scenes and provide a nicer layer on top for your application devs. reply mrbluecoat 17 hours agoparentprevGlad I'm not the only one confused. Even the product purpose seems vague... > Libraries, websites, ML models — you name it, Backstage knows all about it reply frompdx 12 hours agorootparentThis is in reference to the software catalogue feature. reply moltar 13 hours agoparentprevYes, and it’s not just poking. You have to actually code lots of stuff from scratch and maintain it. Feels like you almost need a full time person maintaining it. reply asmor 17 hours agoparentprevBackstage is great at slowly migrating dysfunctional organizations into slightly less dysfunctional organizations. I deal with 30 years of random tech debt in a company with immense shadow IT (domains randomly registered, vendors randomly hired to do X, often nobody knows things are still needed, our zonefiles are a mess, unclear responsibilities with our team of 4 being the default fallback), and Backstage at least puts some semblance of insight into that entire mess and provides a way forward to formalize such processes. Also, putting documentation where developers commit to every day instead of a Wiki where it's out of sight / out of mind is very underrated. reply parpfish 17 hours agorootparentBut doesn’t that mean you have a long migration period with two systems to maintain and reconcile? reply eszed 15 hours agorootparentYes. I've seen this go well, and I've seen this go terribly. It works (in my experience) when the migration work is pushed down to the directly responsible teams - with, of course, affordance made for the time it will take them. (Sidenote: that has the benefit that they learn / re-learn their dependencies for themselves, and discover efficiencies they never would have otherwise.) An overarching \"migration team\", or whatever you choose to call it, only adds friction, confusion, and more kludges. Management has to be fully committed, and enforce deadlines all the way down the line. Running two systems can (usually) be done short-term, but not long. (Another thought: \"short\" and \"long\" terms are variable for different teams and different types of services. Scheduling should take that into account.) Admittedly, however, I've not been through it at a particularly large organization. I'm sure there is exponentially metastasizing complexity (human perhaps even more than technical) once the process involves more people than can fit in a room. reply p_l 15 hours agorootparentprevIn such dysfunctional case the truth is that you do not have an old system to reconcile with - there's no real system of record reply siva7 14 hours agoparentprevThe customers for Backstage aren't startups but mostly big corporations with hundreds of developer teams. Those corporations want to get rid of these hacky internal solutions and many don't even have one. reply jakozaur 17 hours agoprevIt is tough to keep two widely different businesses under one roof. One is a mature, public B2C, and the other is a dev tooling startup. Spotify should spin off Backstage as a separate company, providing the initial IP and funding and being the first client. Many companies, like Uber, created a lot of useful infra, though they failed to monetize it. They could take a cut in the Chronosphere (M3) and Temporal (Cadence). Much better than laying off people, though origin companies would need to be smart, without being too greedy. Though the model is unproven. reply anonzzzies 17 hours agoparent> One is a mature, public B2C, and the other is a dev tooling startup. Amazon/AWS? reply jakozaur 16 hours agorootparentAmazon is an outlier, where each business runs its profit/loss, and higher-ups act as VCs. Wish more companies were like that, but you need to take a lot of bets to make that work. AWS worked, but the Fire phone turned out to be a write-off. reply gghffguhvc 16 hours agorootparentFire Phone was largely outsourced I’ve been told. Was big regrets all the way to the top about that decision. Lesson learned. reply theturtletalks 15 hours agorootparentIf anything, AWS looks like the outlier compared to their other projects. It only survived because of the sheer size of Amazon.com and was built to accommodate that traffic and deployment. If they set out to create AWS without the need for it, don’t think it would’ve made it this far. Even after all these years, AWS is the breadwinner, while the website it was made for can barely turn a profit and resorts to increase the price of prime every year to show revenue growth. reply vasco 16 hours agoparentprevI think you are describing spin-in's. Made famous by Cisco. Didn't catch on much. reply financetechbro 16 hours agoparentprevMany companies, like uber, use their other more profitable business units to subsidize other business units. It doesn’t always make sense to spin out a company / product reply frompdx 12 hours agoparentprevBackstage is an open source CNCF project. It needs a corporate sponsor to keep it alive. reply claytonjy 17 hours agoprevHas anyone successfully implemented Backstage at their company? How'd it go? We're looking at it for an internal platform serving ~50 engineers today, spanning mobile, backend, enterprise integrations, and ML teams. The hope is it'll make it easier to spin up new things \"the right way\" without e.g. ML PhDs having to learn Helm and K8s, plus other goodies like nice docs. reply frompdx 15 hours agoparentMy company uses it. Our team is about the same size. I have mixed feelings. It has been useful for things like making sure a new repository is created with all of the CI goodies every project should have. You can also use it to automate things like opening PRs against your gitops repo to get apps deployed from a template. Creating software templates can be a large investment. It would be great if there was a software template marketplace. The backstage contributors took a stab at this by creating a template repo that has not been updated in about a year. https://github.com/backstage/software-templates If you do invest in creating a template, be sure to get plenty of stakeholder feedback so the thing you make is something that actually gets used. reply stackskipton 16 hours agoparentprevWe piloted two jobs ago and aborted. Wouldn't recommend at your size because it seemed to do effectively, you need a \"Backstage Team\" which is not Ops Team who occasionally does Backstage work. Probably why Spotify is considering it because sweet consulting money. While I don't know your exact situation, I'm currently at company of your size engineers and stop torturing your Developers with Helm is off the shelf advice I would give. See if Kustomize would get the job done and be honest. I've found Devs can handle Kustomize a ton easier since kubectl kustomize > testoutput.yaml is alot easier for them to parse. Helm is great if you have a bunch of different things to launch with a ton of different options but most companies don't do that. They need HPA/Deployment/Service/Ingress which is easily handled by Kustomize. reply claytonjy 14 hours agorootparentI'm somewhat new to helm, after a year I feel like I'm pretty good at it, but struggle to teach other ML folks what I've learned the hard way over the last year. Some of that is helm, plenty of it is Kubernetes. Most of our charts are ingress, service, deployment, service account, scaled object (keda), but there's plenty of small variation even across ML applications, plus the different needs of other teams. Any good resources you can point me to on how Kustomize might be easier for us? reply stackskipton 14 hours agorootparentDocumentation is best: https://kubectl.docs.kubernetes.io/references/kustomize/ However, biggest is it's just YAML patching. So you don't have this {{if such}} littered throughout your project that people can't easily pick up. Values is biggest reason I see people want Helm but if it's all env variables, kustomize lets you put in .env file it turns into config map and injects into deployments. It's possible Kustomize doesn't fix your problem esp if Developers have decided it's not their problem. I just find Devs find Kustomize much easier. reply frompdx 12 hours agorootparentprevHelm is what you should use as your cluster \"package manager\" to install all of your favorite off the shelf tools. Kustomize is probably a better choice for shipping your internal projects due to the variations you mentioned, but also because the learning curve isn't much steeper than learning about how to work with plain Kubernetes manifests. reply arnvald 15 hours agoparentprevI’ve introduced it at a company with ~3,000 engineers and it’s been quite successful, though we do not yet leverage all of its power and we do have a dedicated team to run it and build on top of it We have a lot of in-house tooling and Backstage allows us to move big chunk of UIs into one place - it helps us keep UIs consistent, and makes it easier for everyone to find the tools we have. However with a lot of older, more mature in-house tooling it’s been a struggle to migrate to their data model, introduce their software templates etc. For an org with ~50 engineers I’d recommend something more off-the-shelf like Cortex, Port, or OpsLevel reply badgersnake 17 hours agoparentprevWe use it, and apparently it’s great. It didn’t help with the silos though because I don’t have access. reply junto 15 hours agoparentprevWe are trialing it. Setting up VPN connections, firewall rules, automated secret rotation, API key creation for centralized external services, DNS entries for private endpoints, new team onboarding, new developer onboarding, anything time consuming repeatable task that usually takes our ops or sec teams used to deliver manually in support of the various tech teams to deliver their apps into our clusters. The various “plugins” create PR’s for the ops/sec team to double check and approve to keep the auditors happy. Servicing ~500 developers. It’s definitely a platform that needs a dedicated team though. reply moltar 13 hours agoparentprevLook into projen. It’s not as heavy and doesn’t require any services. But makes it easy to template and spin up projects. But the key is that templates are living and updatable. If you want an intro and a free consultation reach out via email in my profile. reply dilyevsky 16 hours agoparentprevIve watched a couple of botched attempts at implementing it and just like with other spotify fads you need to be all in or not at all. Also im highly sceptical it is actually going to “break your silos” or whatever they are promising because silos usually come from the management and the culture not the type of idp management happened to cargo cult on you reply ravivyas 16 hours agoparentprevIt depends how you define successful. I work in a large org, and it work well for governance, and as a console, but it has its limitation. Pushing too many plugins on it makes management of the console hard reply whirlwin 16 hours agoparentprevWe're 30-40 developers, and we have just stopped using backstage. We started using it for APIM behind Okta SSO, but all the custom frontend and functionality tweaking needed to make it look nice was daunting. If we start looking at it again, I'd try a simpler use-case, but definitely interested in trying it again. reply arccy 15 hours agoparentprevwe use it through roadie.so but it doesn't feel very polished compared to other developer portals... reply siliconc0w 16 hours agoprevThe 'single pane of glass' is pretty seductive and there are a lot of tools that try to sell that vision. Backstage looks a pretty good swing with their modular/flexible approach, though I haven't used it. The problem you end up having is that (for example) Google docs is really good for writing and commenting on docs, and a general purpose tool is never going to be as good as Google docs for docs. You could write a \"Google Docs\" plugin and just link out to a project's google doc but then you're just a link aggregator which offers limited usefulness. reply frompdx 15 hours agoprevBackstage is built on a modular, plug-in based architecture that allows engineers to layer-up their developer portal to meet their own needs. Frankly, this is just incorrect. Just read the configuration guide for \"installing\" a plugin to see what I mean. You have to modify the source code of your own deployment in several places. That's not a plugin based architecture. https://backstage.io/docs/getting-started/configure-app-with... Additionally, check out what it takes to stand up a standalone backstage server. You used to have to clone their repo if I remember right, but now they give you a starter template via npx and leave it up to you to put all of the pieces together. They don't even ship a docker image to get you started. https://backstage.io/docs/getting-started/ Frankly, the architecture of Backstage is ill-conceived and a hassle to operate. If you have the budget their are vendors that will deal with all of this for you. I think the concept of Backstage is fine, but the execution leaves a lot to be desired. reply MyFedora 14 hours agoparentI did not expect that. So Backstage asks you to manually insert plugin components into the app source code? Yeah... that really doesn't feel like a plugin architecture to me. More like patch the source code so it does what you need it to do. Nothing wrong with that, but not a plugin architecture. reply frompdx 12 hours agorootparentThe trouble with the way Backstage is designed is that it makes keeping everything up to date difficult. It is also harder for them to ship updates for their core and plugins due to the labor involved in incorporating any updates. A plugin architecture like Wordpress or Grafana is much more manageable for an operator. In fact, a set of plugins for Wordpress could probably get you an equivalent solution, but it's PHP, so you know, ew. I'd love to see an alternative to backstage that gets it right. reply rglullis 16 hours agoprevThe \"Featured Article\" at the top means it's a sponsored post, right? reply deagle50 15 hours agoparentand the account that submitted here has zero comments. reply rglullis 15 hours agorootparentSomewhat related: the dev behind replyguy said he is working to add AI astroturfers to HN comments as well. I'm curious to know what type of mechanisms (if any) will be adopted to detect and neutralize them. reply morkalork 14 hours agorootparentBut how do we know you aren't just one of replyguy's bots being used to promote his service right now, eh? reply MichaelZuo 14 hours agorootparentTo take this seriously, probably by account age, there just aren't that many 2007 accounts so eventually astroturfers will have to resort to newer and newer accounts. reply immibis 15 hours agorootparentprevMaking them buy immunity. reply muhehe 15 hours agoprevI saw link to backstage some time ago, but I could not figure out what it's supposed to provide. Can someone explain what it does / what do you use it for? reply arnvald 15 hours agoparentBy default it offers you a few tools to better understand and manage your software landscape: - you have all your micro services catalogues in one place, with a fairly decent data model (services are grouped into systems, they are owned by teams, they have declared dependencies on each other) - you have documentation for each service that you can easily find - you have a template catalogue that allows you to easily create new services. You need a full-stack app? Here, recommend option is Java + React, use this template. You need a new ML system? Here’s what you should use, click “create” and we’ll bootstrap it for you - plus a few more, like customizable home page If you are looking for a way to start cataloguing and standardizing services across your org, the developer portal (whether Backstage or other solution) is a good thing to have reply azemetre 15 hours agoparentprevIn a past job of a fairly decent sized org (around 80 engineers, maybe 10 different active projects, a few dozen microservices, component library, template generators, and some custom CLTs) it was very useful as a way of documentation for your engineers. I'm sure you used stuff like confluence but it was always a terrible way to search across code bases and projects to understand how they worked. Backstage was interactive documentation. I can see what projects we have, I can click on one, see who is maintaining it, what their communication is, what their various perf/benchmark scores were. It was nice. Apparently it's even grown beyond documentation needs from what I can tell and it provides platform tooling now. reply shanemhansen 13 hours agoparentprevTo me it looks like a SharePoint for engineering teams. With plugins focused around projects/deployments/metrics. Which sounds actually interesting. reply ravivyas 16 hours agoprevI think one of the largest problems they will face is their consumer not having the same culture as them, and what worked wonderfully for them failing in other orgs. reply tuyguntn 17 hours agoprevpeople who are using DataDog, how does it compare to DataDog service catalogs? If I want to use Backstage what kind of value add it provides over observability tools with cataloging tools? reply frompdx 12 hours agoparentThe scaffolder is the part Datadog won't get you, and in my experience it is the most useful feature. reply lloydatkinson 17 hours agoprevI’ve always thought Backstage seemed like an interesting solution to existing god awful “solutions” many enterprise type companies subject their devs to. For example, PostHog, Swagger. reply dilyevsky 16 hours agoparentWhat’s wrong with PostHog? Do you mean Postman? reply namanyayg 16 hours agoparentprevWhat's bad about PostHog? I haven't tried but I thought it looks good reply esafak 16 hours agoparentprevI think you have the wrong product; PostHog is for startups, not enterprises. reply betimsl 14 hours agoprevYes. The world needs another one of these... reply gibbitz 17 hours agoprevMy employer is considering using backstage for our data infrastructure tooling. This makes me nervous because Spotify seems unable to program a reliable shuffle feature on their player. Maybe it's because they put all the good devs on backstage? reply btown 17 hours agoparentSpotify's shuffle algorithm can't be changed easily because too many people rely on its existing quirks. Which means that Spotify's ability to keep it at a reliably stable level of unreliability for so long, through so many other platform changes, makes it a prime example of why Spotify is a perfect choice for enterprise! ;) reply saghm 16 hours agorootparentIt's true; they make UI changes that no one wants (like pushing the queue off to a sidebar that can't be expanded to the way it used to be), constantly try to push features I don't want (some weird \"DJ\" feature that has an actual programmed voice between songs, \"collaborative\" playlists when I pretty much exclusively listen alone with headphones), and refuse to implement extraordinarily simple features that would be universally popular (like being able to mark certain songs/artists as \"don't ever play this for me please\"), and yet the few other players in the space all have similarly mediocre products that give me no real choice. This is peak enterprise UX. reply jessetemp 17 hours agoprevnext [3 more] [flagged] toyg 17 hours agoparentWhat do you mean by \"sensible\"? Spotify has playlists, but most listeners simply don't bother. Plus, it's obviously convenient for the platform to actually control what you're going to listen to next, so obviously they'd rather emphasize the \"magic\" auto-queueing (which, tbh, kinda works well-enough). reply jessetemp 16 hours agorootparentWell to me, sensible would be where I can play an album, and then select a couple more albums to start playing in sequence after the first album. Spotify's queue doesn't do that, at least on windows. Instead, playing the first album adds it to a queue but not the queue. Selecting more albums inserts them after the current track, not after the album. If I decide I want to remove the remaining tracks of the first album from the ghost queue and only listen to the queue I have to remove them one by one. And the most bonkers part, is when removing the last remaining track from the first album's ghost queue, Spotify replaces it with a different track from the first album! There's no way to remove the ghost queue. The workaround is to simply not use the play button and only add albums to the queue by clicking into the ellipsis menu, but it's a bit of a kludge and not what I'd call sensible. Plus for some reason I always go for the green play button first, then decide later that I want to add a few more albums to the queue reply threecheese 17 hours agoprev [–] Please don’t use “IDP”, we really already have that (for chrissakes we have like four of them in our “enterprise IT”). They can have SAML also, “Spotify: Acronym’s Mine, Losers”. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Spotify has ventured into the enterprise and developer tools domain with its platform Backstage, offering customizable developer portals, initially launched as an open-source project in 2020.",
      "The platform is now monetized through premium plug-ins and services, alongside the introduction of Spotify Portal, a low-/no-code internal developer portal, complemented by enterprise support and services.",
      "The shift signifies Spotify's transformation into an enterprise-oriented developer tools firm, with upcoming additions of features and products to its portfolio."
    ],
    "commentSummary": [
      "Spotify has launched Backstage, an IT platform for streamlining services for companies with many engineers, receiving positive feedback from larger firms with complex systems.",
      "Discussions address technical debt, shadow IT challenges, and project comparisons within companies like Amazon, emphasizing Helm and Kustomize for managing Kubernetes deployments.",
      "Backstage is acknowledged as a valuable tool for standardizing services and enhancing documentation in enterprises, though criticisms focus on its plugin architecture and complexity, alongside concerns about Spotify's reliability and functionality, notably its queue system."
    ],
    "points": 111,
    "commentCount": 62,
    "retryCount": 0,
    "time": 1714836134
  },
  {
    "id": 40257843,
    "title": "Budget Cuts Halt Mirror Fusion Test Facility",
    "originLink": "https://www.beautifulpublicdata.com/the-mirror-fusion-test-facility/",
    "originBody": "DOE The Mirror Fusion Test Facility A decade-long effort to build a machine to unlock the promise of nuclear fusion fell victim to budget constraints and competing science, and was shut down the day it was dedicated. It was never turned on. Jon Keegan Mar 27, 2023 • 7 min read Photos: Lawrence Livermore National Laboratory. On Friday, February 21, 1986 a group of 300 scientists, engineers, contractors and government officials gathered for a dedication ceremony at Lawrence Livermore National Laboratory. After final diagnostic tests, the \"Mirror Fusion Test Facility-B\" (MFTF-B) completion was celebrated, and a letter from John Herrington, Ronald Regan's Secretary of Energy was presented to program director T. Kenneth Fowler extending his congratulations on a job well done. On the very same day after nearly a decade of development and nearly a billion dollars* of funding, the project was shut down, the massive machine having never been turned on. \"I want all of you to know how much I regret the fact that, just as you complete this remarkable new facility, the budget pressures dictate that we must put it into standby and not operate it as you might have hoped. This is frustrating, and perhaps not the best use of our national talent and resources, but we must bring the deficit under control,\" wrote Herrington in the letter to Fowler. Photos of the 400-ton \"yin-yang\" magnet mirrors used in the MFTF-B. Source: Lawrence Livermore National Laboratory. Photo 1, Photo 2. I came across photos from the construction of the components of the MFTF-B on Lawrence Livermore National Laboratory’s website and was captivated by a photo from 1980 showing a strange twisting mass of metal that at the time was the largest superconducting magnet in the world. This 350 ton magnet was encased in stainless steel built in a distinct “yin-yang” shape, and its interior was cooled to temperatures of 425º F below zero, by pumping liquid helium through the vessel. Thirty miles of copper and niobium-titanium wire were wound over the course of a year to make the magnet’s conductor. The magnet was capable of generating magnetic fields 150,000 times that of Earth’s that could contain the 500-million Kelvin degree plasma generated by the fusion device. A photo of the MFTF-B under construction in 1978. Source: LLNL The Race for Fusion “According to Greek mythology, fusion energy, the fire of the Sun is a gift hard won”, goes the first line of the T. Kenneth Fowler’s 1997 book, “The Fusion Quest”. Fowler continued, “Today, irresistibly drawn to the challenge of bringing fusion energy down to Earth from the stars, scientists tempt Zeus still.” The promise of mastering this elemental force is a clean, safe and near limitless energy source, which could literally save the planet. While the more widely known process of nuclear fission has been producing energy at commercial scale in power plants since the 1950’s, harnessing nuclear fusion – which Fowler’s poetic description accurately captures as the same reaction taking place in the heart of the Sun – has been the subject of an international race for decades. Only in December of 2022 did scientists at the National Ignition Facility at the Lawrence Livermore National Laboratory announce that they had achieved the first recorded fusion reaction with a net energy gain – meaning that it released more energy than was put in, which was a crucial milestone for the whole field, though it could be decades before this is put into practical use. The energy crisis of the 1970s motivated the U.S. government to throw lots of money at alternative energy sources, and fusion was one of the big areas of interest. According to the Department of Energy, the basic principle of nuclear fusion is the fusion of two lighter nuclei (such as the commonly used combo of deuterium and tritium) to form a heavier one (helium) which releases energy (and subatomic particles, such as neutrons). To do this, super-hot plasma is created in a vacuum to create the fusion reaction, and either lasers or powerful magnets are used to control and contain the plasma. During this period of frenzied investment in the 70’s, two major directions for fusion research emerged: the torus or donut shaped “Tokamak” design utilized by Princeton Plasma Physics Laboratory, and MFTF-B's “magnetic mirror” based approach, with a linear vessel housing the superheated plasma bouncing the plasma off two opposing magnetic “mirrors” at either end of the chamber. 1975 photo of the Princeton Large Torus at the Princeton Plasma Physics Laboratory, which is an example of a \"tokamak\" fusion reactor design. Source: Princeton Plasma Physics Laboratory, Public domain, via Wikimedia Commons Research at Lawrence Livermore National Laboratory developing the mirror based approach showed promise in smaller scale tests, which led to the decision to go all in on a large scale device. The fact that other major labs were going all in on the tokamak approach, left an opening for hedging the big fusion bet on this potential alternate path. The question of whether there was enough sound evidence to ramp up to the scale of the MFTF-B was subject to debate at the time, and the final decision seemed to partly come down to ideology and gut instinct. In a really thorough 1987 Science magazine story on the MFTF-B Edwin Kintner, who was the associate director of the Department of Energy’s Office of Energy Research at the time said as much. Looking back on the decision to go all in on the MFTF-B, Kintner is quoted as saying “Everybody was concentrating on tokamaks. I thought it was necessary for these tokamak guys to have to look over their shoulders.” In the same article, MFTF-B program director Fowler is quoted as saying, “You could debate the decision, but it wasn’t illogical. Building big machines is a mixture of lead times, resources, prudence and gambling.” Below: \"Mirror Fusion Test Facility magnet system— Final design report. Sept. 3, 1980. Source: Lawrence Livermore National Laboratory.\" \"Just-completed and never-used\" The Reagan administration’s decision to mothball the machine came as a gut punch to the researchers. Lawmakers tried to fight for extra funds to throw a lifeline to the program or to salvage parts of the project to perform some science in a more limited scope. The money and time spent on the project was weighing heavy in the comments recorded in hearings held by the House Subcommittee on Energy Research and Production in February of 1986. Congressman Fortney (\"Pete\") Stark of California made a particularly impassioned plea to extend the life of the project. Citing the Reagan Administration's proposed budget, Stark wrote \"This proposal would mothball the just-completed and never-used Mirror Fusion Test Facility-B (MFTF-B) at the Lawrence Livermore National Laboratory.\" Stark continued, \"A lot of hard work and money has been invested in the world's largest superconducting, tandem mirror fusion experiment. For close to 8 years some of the finest scientists and engineers in America have dedicated their time and energy to the project. $350 million dollars have been invested in MFTF-B.\" The MFTF-B under construction in 1983. Source: Lawrence Livermore National Laboratories. Stark included some of the photos of the facility for the record, showing its impressive scale. \"As you see the facility is truly an incredible accomplishment. Even to those who have not had years of scientific training, the enormous complexity of the project can be appreciated. Please let me repeat: 8 years of dedicated manpower and $350 million dollars have been pumped into the MFTF-B.\" In the years following the shutdown of the program, parts of the machine were scavenged for other projects, and the rest was scrapped in 1998. Building 431 at Lawrence Livermore National Laboratory sat empty for a number years and was eventually demolished around 2005 after determining that the site did not meet the threshold of historical significance to be protected on the National Register of Historic Places. The installation of a magnet into the MFTF-B in 1981. Source: LLNL Recent breakthroughs reignite hopes for fusion With the recent news of Lawrence Livermore National Laboratory's National Ignition Facility achieving the first net energy gain nuclear fusion reaction, the outlook for prioritizing fusion research looks bright. Having achieved \"ignition\" for the first time on December 5, 2022 by creating a reaction where more energy is released than consumed, it turns out a third approach was the key to success: containing the plasma within high powered lasers. In fact, the successful ignition employed another huge, expensive machine – this one equipped with 192 massive lasers all focused on a tiny pellet, pounding it with 2 million joules of energy, creating a fusion reaction that only lasted for 100 trillionths of a second. \"Crossing this threshold is the vision that has driven 60 years of dedicated pursuit — a continual process of learning, building, expanding knowledge and capability, and then finding ways to overcome the new challenges that emerged. These are the problems that the U.S. national laboratories were created to solve.” Another huge, expensive fusion machine. But this one worked. The target chamber of the National Ignition Facility at Lawrence Livermore National Labs (LLNL). Source: LLNL * In 1986, the total cost of the project was described in Congressional testimony as costing $350 million, which would equal $965.4 million according to the BLS. 🙏🏻 Did you enjoy this post? You can subscribe to our newsletter to get future posts delivered to your inbox for free. 👉🏻 📫 Subscribe now. Sharing is caring 📣 If you think your followers or friends may like it, please consider sharing it. 🙋🏻♀ If you have any suggestions, comments or requests, please email them to beautifulpublicdata@gmail.com Thanks for reading! - Jon Keegan (@jonkeegan)",
    "commentLink": "https://news.ycombinator.com/item?id=40257843",
    "commentBody": "The Mirror Fusion Test Facility (2023) (beautifulpublicdata.com)111 points by not_a_boat 19 hours agohidepastfavorite64 comments willis936 19 hours agoI worked at a fusion lab at UW Madison for a few years. There is a statue of these mirror magnets (with a poetically broken water display). The way MFTF funding cut as soon as it was complete was a generational shock to fusion researchers. The taste still hasn't left peoples' mouths. reply jethkl 16 hours agoparentThe engineering hall statue! Thank you for posting your comment. I thought it was an abstract sculpture trying look \"engineery\". But it's a realistic representation of a cultural wound, and it is a warning. https://engineering.wisc.edu/wp-content/uploads/2022/10/Kids... reply actionfromafar 16 hours agorootparent“No deed of honor is commemorated here” reply pfdietz 15 hours agoprevIf I understand correctly (and I'm not sure I am), what killed MFTF was they pushed ahead too fast. They didn't find all the instabilities before they started building it. Since then, the instability that killed it (DCLC) has been understood and they've found a way to design the system to avoid it. So mirrors are being worked on still, they just don't look like MFTF. https://plasma.physics.swarthmore.edu/brownpapers/WHAMmirror... https://www.researchgate.net/publication/374070078_Physics_b... https://realtafusion.com/ reply willis936 15 hours agoparentWouldn't that have come to light within the first few campaigns? It's difficult to justify spending the capital on an experiment that is never run, regardless of how wise it was to make compared to other experiments. reply FiatLuxDave 17 hours agoprevBeautiful pictures, but I felt that the post was lacking in mention of Post: https://en.wikipedia.org/wiki/Richard_F._Post reply pfdietz 15 hours agoparentThe father of actress Markie Post from \"Night Court\". reply api 19 hours agoprevThis field is so underfunded, but progress is still being made. Check out Commonwealth Fusion and their progress with insanely powerful compact energy efficient magnets. They recently clocked a stable 20T magnetic field. reply cycomanic 15 hours agoparentCalling fusion research underfunded is rich(no pun intended). Costs for ITER are estimated to 45 to 65 billion USD it's one of the most ambitious and costly science projects in the world to date. That's not even accounting for all the other fusion projects. There are many areas in science that receive significantly less funding. I would also argue that the prevelant perception that fusion would lead to to essentially unlimited free energy is wrong. The extremely high capital costs would likely mean that fusion is unlikely to ever match solar or wind in energy production costs. I'm not opposed to fusion/plasma research, we have much insights into nonlinear dynamics and chaotic behaviour due to this research, I disagree with the notion that this is a field that achieves incredible outcomes with small budgets, that's far from reality. reply baq 11 hours agorootparentApple is buying back $110B of stock. They could build an Apple iFusion Reactor instead and still buy back $50B. reply willis936 10 hours agorootparentArguably for much less than that. The current fusion startups are targeting burning plasma and altogether they've raised a tenth as much as ITER's (speculated) 6.5 Bn USD. Granted, very few companies are targeting burning plasma with their current level of funding. reply nabla9 14 hours agorootparentprevLet's read again what you just wrote and think: \"ITER are estimated to 45 to 65 billion USD\" That's not the most costly costly project, not even close (prices in current dollars and total costs) Apollo project: $257 billion ISS: $150 billion. STS: $196 billion. Fusion energy has been constantly underfunded. reply pfdietz 12 hours agorootparentNone of those projects was worth the cost. reply Dylan16807 9 hours agorootparent\"Estimates of the return on investment in the space program range from $7 for every $1 spent on the Apollo Program to $40 for every $1 spent on space development today.\" I guess you take issue with estimates like this? reply pfdietz 9 hours agorootparentYes, because they are basically exercises in bullshit. You might think they went and catalogued in detail what the benefits were, but they're actually just based on macroeconomic assumptions about what the return on R&D is. I mean, think: for spinoff arguments to work, you need to have some notion of how technology would have progressed without NASA. How could one possibly figure out that contrafactual? reply j-krieger 11 hours agorootparentprevthe ISS was definitely worth the cost. reply pfdietz 11 hours agorootparentReally? What result did ISS deliver that justifies its construction? It's an exercise in orbital flagpole sitting. reply shepherdjerred 11 hours agorootparentWhat's the point of being alive without building monuments? reply pfdietz 10 hours agorootparentI find that justification not only not convincing, but utterly repulsive. Governments shouldn't be in the business of praising themselves. reply api 10 hours agorootparentprevIt’s an engineering test platform for long duration space flight. Along with the development of reusable rockets we are now almost to the point of being ready to actually go somewhere and do more than just plant a flagpole. Apollo was ridiculously ahead of its time and was not sustainable. The entire stack was disposable and it’s pure luck that we never lost a crew. We needed a project like the ISS to do the real work to figure out how to live in space and build long lasting space hardware. The biggest shortcoming of the project is that we never tried centrifugal gravity. There were plans for a module but it never got there. reply pfdietz 10 hours agorootparentThat's great, except (1) the thermal environment is all wrong, (2) ditto for the radiation environment, (3) that just pushes back the need for justification to long distance spaceflight. Justifications for space activities that amount to \"enables other space activities\" are an example of what's called a \"self-licking ice cream cone\". There were plenty of non-self-referential justifications for ISS (like growing protein crystals, or space manufacturing) that never really went anywhere. Ironically, maybe the best justification was it shows large space structures can be assembled from small units, so large launch vehicles aren't needed. NASA has of course totally ignored this lesson with SLS. reply lukeschlather 7 hours agorootparentIt's really hard to put a dollar value on what the value of having something like SpaceX Starship available is, and it's hard to evaluate what the Starship project would look like if we hadn't put the money into the Apollo/ISS/STS/SLS before. With the current state of things, I would bet that with a fully reusable orbiter like Starship, space-based solar with microwave transmitters will probably be a cheaper option than fusion. But we should work on both because we can't predict it until both are tried. It's easy to say no to a project when we don't know what the outcome will be, but we need to take on a lot of big risky projects to see which ones give outsized returns. reply Dylan16807 6 hours agorootparentSLS only started in 2011. If SpaceX learned anything from it, it was not much and it was after they'd proven themselves. Starship has been quite from-scratch outside of the engines. reply lukeschlather 2 hours agorootparentI mean, SLS was definitely a waste of money, and arguably so was STS. But that's easier to say in hindsight, and it's also easy to say in hindsight that SpaceX had an obviously better approach, but that was harder to support in 2011. reply api 14 hours agorootparentprevThe new Vogtle reactor in Georgia (the US state) cost about $17B. A single offshore oil platform can cost upwards of $1B and we build hundreds of these. When weighed against the potential payoff fusion is very underfunded. If we make it work we basically have infinite clean energy until the heat death of the universe, not to mention a source of energy that could jet us around the solar system and beyond at speeds far beyond what anything we currently have can achieve. The cost of ITER is not crazy for the energy industry in general. Huge energy projects are very expensive. We spend multiples of that on oil drilling every year. reply jiggawatts 10 hours agorootparentprevITER is like electric cars. There’s no point in spending millions to design a new EV until the battery technology exists. That’s the key limiter, not the “rest of the car”. For fusion, this key technology limiter is the superconducting magnetic tape. Even small improvements in field strength have a dramatic non-linear improvement in fusion power gain. ITER started their design work decades ago and will continue construction for decades more. Meanwhile, SC magnetic tape technology has moved on, making the entire enterprise a dead-end waste of time and money. They got bogged down in the paperwork and bureaucracy of the “rest of the facility” with no hope of making actual working reactor. There are many instances of this issue in industry, and in my experience large government bureaucracies are pathologically incapable of planning ahead for where the puck will be instead of where the puck was. I like to imagine what would happen if a rich industrialist like Bezos or Musk approached the problem: First, spend a few billion on superconductor tape research and then start building many small reactors that are minimal test beds while in parallel improving the magnets. Only “go big” once the reactor can produce net power. See also: NASA spending billions per launch on the SLS with a couple of test launches per decade while SpaceX spends millions per Starship and plans half a dozen test launches this year! reply XorNot 8 hours agorootparentSo how's the Lockheed Martin skunkworks project going? That was the last poster child for \"REBCO tape will quickly solve all the problems\". Oh right - incepted in 2010, cancelled in 2021. 11 years - over a decade - and no reactor in sight. reply willis936 7 hours agorootparentThey never mentioned REBCO or HTS. It was obvious bullshit from the beginning. It was more likely a cover for a black project, like the SR-72. reply xcv123 9 hours agorootparentprevThat's a ~12 year construction project. 5.4 Billion USD per year is nothing these days. Only 0.006% of annual GDP for a project that has the potential to transform our civilization. reply Zardoz84 14 hours agoprevI was always asking what happened with the magnetic mirror experiments about fusion. I only know about, what was in a book that I read when I was young. I think that was the \"The fusion quest\" reply SaberTail 19 hours agoprev> Only in December of 2022 did scientists at the National Ignition Facility at the Lawrence Livermore National Laboratory announce that they had achieved the first recorded fusion reaction with a net energy gain We've been achieving fusion reactions with net energy gain since the 1950s, when hydrogen bombs were developed. And, on top of that, the achievement they're talking about was more energy out than light energy put in from lasers. The NIF lasers are not anywhere close to 100% efficient at converting energy to light, and so it was not actually net energy positive. NIF is not going to lead to fusion power. That's not the point. Fusion bombs are primarily an engineering problem. There's lots of plasma physics going on that is very difficult to simulate with a computer, and you need real world data to inform the simulations to make sure they're working properly. That's what NIF is for. Instead of having to blow up a nuclear bomb, and violating test ban treaties, NIF can produce that data. And with the PR shine of tying it to fusion energy. reply rqtwteye 18 hours agoparentIt’s pretty crazy that they are still working on improving nuclear bombs. If there is one area where the current state is “good enough” it should be that area. I can see the need for maintaining what we have already but why try to advance the technology? A pure fusion bomb would be cleaner as far as I know but do we really want to make nuclear war more feasible? reply segfaultbuserr 18 hours agorootparent> I can see the need for maintaining what we have already [...] The officially-stated goal of these labs (pulsed power, fusion, and hydrodynamic test facilities [0]) is indeed for maintaining existing nuclear weapons, not to design new ones (and also for doing basic research during free time). This was called the Science Based Stockpile Stewardship program [1] - ensure that existing nuclear weapons would remain functional in the foreseeable future. (Interestingly, the lesser-known hydrodynamic test facilities such as the Dual-Axis Radiographic Hydrodynamic Test Facility are more useful for weapon designs than fusion facilities). The idea is to test materials under extreme lab conditions to help computer modeling, so that it would still be possible to do minor design changes to replace obsolete or end-of-life parts (the FOGBANK incident [1] came to mind). Understanding long-term aging is also a stated goal. [0] http://www.wslfweb.org/docs/agex.htm [1] https://en.wikipedia.org/wiki/Stockpile_stewardship [2] https://en.wikipedia.org/wiki/Fogbank reply dgacmu 18 hours agorootparentprevIn their defense, some of the questions and that they're asking are about how our stockpile degrades over time - which hopefully means keeping weapons on ice longer instead of having to reprocess and build as many new ones. reply sbierwagen 14 hours agorootparentprevPure fusion bombs are very destabilizing, yes. The current nuclear arms control regime depends on uranium enrichment/plutonium production being heavy industry, requiring large facilities with unusual equipment. A research program for EPFCG pure fusion weapons could be quite small, only requiring a few hundred people and little equipment that couldn't be manufactured indigenously. Test explosions could be done at the kilogram scale, producing no radiation detectable from orbit or seismic effects, then easily scaled to kiloton yields. reply jtriangle 17 hours agorootparentprevIt depends on your answer to the question, is nuclear war inevitable? If it is, making weapons that are as clean as possible is a reasonable goal. If you get some knowledge that's applicable elsewhere, that's a nice bonus. If you don't think it's inevitable, you can likely justify making them cleaner because it will likely have applications elsewhere by the logic of fusion weapons being the only place we've been able to harvest usable energy thusfar. reply SaberTail 17 hours agorootparentThe difficulty with \"clean fusion bombs\" is that bomb makers can always increase the yield of a fusion bomb by making it dirty. Fusion releases neutrons, and these neutrons have enough energy to fission the common 238 isotope of uranium, which releases roughly 100 times more energy than than the neutron started with. reply jtriangle 7 hours agorootparentYes, but if you're getting the desired yield from fusion alone, you don't need to. Remember, most of the world is focused on relatively small nuclear arms, not world ending polymegaton devices reply doubloon 17 hours agorootparentprevif we do not maintain expertise in nuclear weapons then we will end up like Ukraine. reply rqtwteye 14 hours agorootparentWe already know everything that’s to know about nuclear weapons if used for defense and deterrence. A pure fusion bomb would be relatively clean so it would be much more tempting to use as a regular weapon. What if Putin had such bombs right now? reply rbanffy 1 hour agorootparentA pure fusion bomb is also very hard to detect compared to a fission-fusion device. reply XorNot 8 hours agorootparentprevNuclear weapons aren't a deterrent because of radiation. They're a deterrent because they enable an ICBM to destroy a city. reply rqtwteye 7 hours agorootparentThey are not very useful as attack weapons because they contaminate an area and produce fallout. If you can get the explosive power of a nuke without the fallout it's much more tempting to use one. reply tsimionescu 3 hours agorootparentThat fallout doesn't seem as huge a problem as it is often made out to be. Hiroshima and Nagasaki are not wastelands, they are relatively large cities, and they basically never stopped being ones, even immediately after the bombs fell. The pure destruction of the explosion was a worse problem than the fallout overall - and with today's much higher yields and much more densely populated cities, that seems likely to continue to be the case. reply XorNot 2 hours agorootparentprevExcept this isn't true: the fallout of a nuclear detonation is extremely limited - their physically isn't enough radiological material in the warhead for it to be greater. Troops with chemical suits and respirators could safely operate in a zone which was recently nuked - in fact ground zero is likely to be less contaminated then surrounding areas, since the effect of the blast is to disperse material. The problem, like all WMDs (weapons of mass destruction), is that they're of limited value against military forces. The distances a military force fights a war over are generally large, and they're mobile - or dug in. The effect of bombarding someone's lines with nuclear weapons though is that you might produce a few holes, but if you tried to advance through them you'd be immediately surrounded since you didn't completely obliterate them. But they're of devastating effect against civilians, and civilian assets like cities which can be military objectives. Leveling a city rather then taking it is certainly an option, but if that was your plan all along then you could also just drop an ICBM on it - and at that point we're back to \"the primary effect of nuclear weapons is making ICBMs useful weapons\". The casualties from a full-scale nuclear war between the United States and Russia were always estimated as \"only\" being in the hundreds of millions at the top end. But the subsequent famines from the destruction transport and distribution infrastructure, would be in the billions within 6-12 months. Basically the issue is \"tactical\" and \"strategic\" nuclear weapons don't make much a distinction: if it's worth hitting someone's frontlines with a tactical nuke, then why not hit the military base supplying those lines with it? And that base is probably in a city with industry, so why not hit that instead etc. etc. etc. I suppose another way to put it would be, there's a \"hidden\" escalation threshold we don't really talk about because no one's been stupid enough to do it: destruction of arable land. You would find that international opinion and weaponry would turn on any country very quickly if they were found to be deliberately targeting and destroying arable land as a policy of invading and denying resources to a neighbor (think literally \"salting the earth\") on a large, deliberate scale. Which is all a way of saying, the issue with WMDs is that they're WMDs and WMDs all have broadly similar issues - namely that they will do far more damage to civilians then military targets, even without special preparations, and that they don't allow taking and holding territory. \"Clean\" nuclear weapons wouldn't change that. reply daedrdev 18 hours agoparentprevWe already have enough nukes to glass the world several times over. reply dekhn 16 hours agorootparentThat may have been true in the past but the major nuclear arsenals are much smaller, and the yields much lower, with the intent of incapacitating an enemy, not destroying the entire world. reply jvanderbot 18 hours agorootparentprevI don't understand your comment at all. It doesn't seem to me that GP was suggesting more bombs, just suggesting more testing and letting bomb makers use all their available tools and funding on it. reply pfdietz 17 hours agorootparentprevThis is not true. reply Terr_ 1 hour agorootparentprevI doubt you meant that literally, but just to illustrate the gap, let's consider what it would require for just one glassing. World land area is ~149,000,000 km². Acknowledged nuclear warheads are ~13,000. So the average single warhead would somehow need to \"glass\"--however, thoroughly or deeply you choose to define that--over 11,000 km². For context, that's about one US state of Connecticut or the nation of Qatar. ... So you might say I'm a \"glass is nowhere even close to half-full\" kinda guy. reply CalRobert 19 hours agoprevIf you're the type to read comments before the article itself, do yourself a favour and click over if only for the fantastic, sci-fi level design of this device. Incredibly cool-looking. reply exochrono 16 hours agoparentI love the juxtaposition in one of the first pictures of this super advanced contraption being rolled into place using plywood and logs. It totally looks like it could be a still from the a movie where medieval humans discover some sort of crazy alien artifact and drag it back home to put in the middle of their town as a trophy. reply GordonS 19 hours agoparentprevWow. It almost looks alien - insanely cool, and thanks for the push, it's quite an interesting article. reply jcims 19 hours agoparentprevGave me Contact vibes. Love it! reply relaxing 15 hours agoparentprevThese Big Science projects give me Akira vibes. reply fnordpiglet 17 hours agoprev“This is frustrating, and perhaps not the best use of our national talent and resources, but we must bring the deficit under control,” so said all accountants when in charge of anything. reply keepamovin 18 hours agoprevThis is such a tragedy. So ridiculous it just wreaks of corruption. Energy industry threatened by new tech. To not even turn it on? And collect the data? Like the Darkstar Mach 10 test day shutdown scene by the “Drone Ranger” in Top Gun: Maverick Oh well perhaps if the magnet mirrors still exist they can be repurposed by the Guggenheim for an installation. Beautiful sculpture!! Hehe :) reply airstrike 17 hours agoprevlooks straight out of Control (the game)reply doubloon 17 hours agoprev\"we must bring the deficit under control,\" that's funny because Reaganomics was the beginning of the \"deficits dont matter\" wing of the Republican party. reply api 15 hours agoparentIt was also the start of politically weaponized Keynesianism. When Republicans are in power they spend massively to stoke the economy while using deficit hawk rhetoric. When they lose power they blame the ensuing deficits on Democrats. Works incredibly well because people don’t look beneath the headlines. reply 2OEH8eoCRo0 11 hours agorootparentThe two Santas theory. https://en.wikipedia.org/wiki/Jude_Wanniski#The_Two_Santa_Cl... reply willis936 10 hours agorootparentNot to be confused with the Futurama episode named after this theory, written by Bob Odenkirk, and that definitely lives rent free in my mind. https://en.wikipedia.org/wiki/A_Tale_of_Two_Santas reply pstuart 16 hours agoparentprevaka The Two Santas. reply mensetmanusman 19 hours agoprev [–] These are good stories to keep in mind when your/our much smaller passion projects get killed by management. Life moves on. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Mirror Fusion Test Facility, a project focusing on nuclear fusion, was closed on its dedication day due to financial limitations, never being operational.",
      "This project, using a magnetic mirror method to confine plasma for fusion reactions, was dismantled in 1998 after the Reagan administration's decision.",
      "Recent advances like achieving the first net energy gain fusion reaction at the National Ignition Facility have sparked new optimism for the future of fusion research."
    ],
    "commentSummary": [
      "The Mirror Fusion Test Facility (MFTF) was a government-funded fusion research project completed in 2023, but funding was cut shortly after due to progressing too quickly without identifying all instabilities.",
      "Fusion research, such as ITER, incurred billions in costs, with ongoing exploration of different mirror magnet designs despite past failures.",
      "The debate over fusion's clean energy potential versus the cost-effectiveness of renewables like solar and wind extends to discussions on large-scale projects' value, including space programs, nuclear weapons development, and concerns over WMDs' limited effectiveness in warfare."
    ],
    "points": 111,
    "commentCount": 64,
    "retryCount": 0,
    "time": 1714832221
  },
  {
    "id": 40261965,
    "title": "Sequoia: Accelerating LLM Serving on Consumer GPUs",
    "originLink": "https://infini-ai-lab.github.io/Sequoia-Page/",
    "originBody": "SEQUOIA: Serving exact Llama2-70B on an RTX4090 with half-second per token latency Zhuoming Chen*1, Avner May*2, Ruslan Svirschevski*3 Yuhsun Huang1, Max Ryabinin2, Zhihao Jia1, Beidi Chen1,4 1Carnegie Mellon University 2Together AI 3Yandex 4Meta AI *Indicates Equal Contribution Code arXiv Introduction We introduce Sequoia, a scalable, robust and hardware-aware speculative decoding framework that enables serving LLMs (70B, 33B...) with a reasonable latency on consumer GPUs without any approximation (using 16bit precision and maintaining the original output distribution). Addressing the problems of robustness and scalability of previous works on speculative decoding, we show below that Sequoia, with a large speculation budget, can serve a Llama2-70B on a single RTX-4090 with an average time between tokens (TBT) as low as 0.57s, which is 8X faster than a highly optimized offloading serving system, 9X faster than DeepSpeed-Zero Offloading. On a single 2080Ti GPU (only 11GB memory), Vicuna-33B can be served with a TBT of 0.87s. Serving Solutions by Sequoia GPU Bandwidth(GB/s) Target Model Draft Model TBT(s) Baseline(s) 4090 31.5 Llama2-70B Llama2-7B 0.57 4.54 4090 31.5 Vicuna-33B TinyVicuna-1B 0.35 1.78 4090 31.5 Llama2-22B TinyLlama-1.1B 0.17 0.95 4090 31.5 InternLM-20B InternLM-7B 0.17 0.77 4090 31.5 Llama2-13B TinyLlama-1.1B 0.09 0.27 2080Ti 15.8 Vicuna-33B TinyVicuna-1B 0.87 4.81 2080Ti 15.8 Llama2-22B TinyLlama-1.1B 0.53 3.04 2080Ti 15.8 Llama2-13B TinyLlama-1.1B 0.34 1.53 Sequoia can speed up LLM inference for a variety of model sizes and types of hardware. We evaluate Sequoia with LLMs of various sizes (including Llama2-70B-chat, Vicuna-33B, Llama2-22B, InternLM-20B and Llama2-13B-chat), on 4090 and 2080Ti, prompted by MT-Bench. The hardware platforms have different GPUs, CPU RAMs and CPU-GPU bandwidth. The evaluation results are listed above. Here we show a demo for Llama2-70B inference on a single RTX-4090 (with and without Sequoia. Video plays at 4X speed). Why Sequoia Benefiting from two key advantages, Sequoia significantly accelerates LLM serving with offloading. Firstly, Sequoia is more scalable with a large speculation budget. For a given draft / target model pairs, Sequoia leverages a dynamic programming algorithm to search for the optimal tree structure, which enables a much faster growth in terms of accepted tokens with a certain budget (i.e. the size of the speculation tree). Secondly, thanks to sampling without replacement algorithm, Sequoia is robust in terms of generating temperatures, compared to top-k sampling and sampling with replacement. Apart from offloading, Sequoia provides a hardware-aware solution to adjust the size and depth of speculation trees to adapt to different hardware platforms. Sequoia can also speed up LLM inference on data-center GPUs like A100 and L40, which is discussed in detail in our paper. Left (Scalability): Handcrafted tree structures do not scale well with large speculation budget. Right (Robustness): The total acceptance rate of 5 speculation tokens. Sampling with replacement (SpecTr) fails when temperature is low and Top-k sampling fails with high temperature. Sequoia, leveraging sampling without replacement, attains the highest acceptance rate. Below we show two examples of tree structures in Sequoia. The left one has 64 nodes which is suitable for on-chip inference and the right one has 768 nodes, suitable for offloading settings. We append more budget to nodes in previous layers with a higher probability to get accepted. Conclusion and Future Work Leveraging a large speculation budget, everyone can use RTX 4090 or other consumer (low-cost) GPU, e.g., AMD RX7900 with Sequoia to host very strong LLMs like 70B model without approximation, boosting the applications of AI generated content. In addition, we believe Sequoia will perform particularly well on future hardware, because it’s performance scales well with the compute/bandwidth ratio of the hardware, which has been increasing over time (e.g., V100, A100 and H100). Moreover, Sequoia, as a speculative decoding framework which mitigates the gap in the memory hierarchy, adapts to any draft/target pairs and any AI accelerators. We will stay tuned with hardware community. BibTeX @article{chen2024sequoia, title={Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding}, author={Chen, Zhuoming and May, Avner and Svirschevski, Ruslan and Huang, Yuhsun and Ryabinin, Max and Jia, Zhihao and Chen, Beidi}, journal={arXiv preprint arXiv:2402.12374}, year={2024} } This page was built using the Academic Project Page Template which was adopted from the Nerfies project page. You are free to borrow the of this website, we just ask that you link back to this page in the footer. This website is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. The icons are created by GPT4.",
    "commentLink": "https://news.ycombinator.com/item?id=40261965",
    "commentBody": "SEQUOIA: Exact Llama2-70B on an RTX4090 with half-second per-token latency (infini-ai-lab.github.io)108 points by zinccat 6 hours agohidepastfavorite43 comments spxneo 6 hours agothis is quite worrying for OpenAI as the rate token prices have been plummeting thanks to Meta and its going to have to keep cutting its prices while capex remains flat. whatever Sam says in interviews just think the opposite and the whole picture comes together. It's almost a mathematical certainty that people who invested in OpenAI will need to reincarnate in multiple universes to ever see that money again but no bother many are probably NVIDIA stock holders to even out the damage. reply jstummbillig 4 hours agoparentI disagree. a) A year after GPT-4 set the bar, it's still the best model, despite everyone else not having to do it first. Just copy, and just software. And that's not for lack of trying by every other viable prime player on the planet with unprecedented acceleration. Imagine any other piece of software, where the incumbent has a mere 2-3 year head start, in which they had to work out the entire product that everyone else, despite just having to copy and pressing the pedal through the floor is struggling just trying to catch up with. b) The current models including GPT-4 are so bad. The few billions can be made by just by continue playing this game of improvements for a few years and getting better each year. I think people are wildly confused about how big this market is going to be when that happens. They are not squeezing hosting or compute. They are squeezing intelligence. Intelligence is the entire economy. The notion that there would ever not be room for multiple things here, maybe through size or specialisation or cost (as with all other intelligence), and that a few billion dollar are a big deal, is so strange to me. c) The game will at some point, be mostly about infra and optimization. People come to the conclusion that's a problem for the incumbents, when our entire industry is mostly about infra and optimization. AWS is infra and optimization. I think even the average hn tinkerer understands that therein lies a proposition that's not exactly equivalent to \"just rent a few servers and do it yourself\". reply anon373839 4 hours agorootparent> A year after GPT-4 set the bar, it's still the best model Debatable. Many people find Claude Opus superior, and I know I've found it consistently better for challenging coding questions. More importantly, the delta between GPT-4 and everything else is getting smaller and smaller. Llama 3 is basically interchangeable with GPT-4 for a huge number of tasks, despite its smaller size. reply ashu1461 59 minutes agorootparentAgree, the delta is getting smaller. And for majority of the tasks you can use the Claude Sonet which is better than 3.5 and also fast. But at the same time when you actually want to solve a complicated problem, deep down you know that only GPT 4 can crack it. reply jstummbillig 4 minutes agorootparentEven more important, you know that GPT-4 will probably also not crack it. Which is why the SOTA is not terribly interesting. The delta between GPT-4 and the competition has been closing but why anyone would assume that this is a trend and that it would continue with GPT-4.5 to competition, or GPT-5 to competition instead of the other way around is a mystery to me. I am not saying it could not be true. But extrapolating from differences between current bad models to a future with better models is weird, specially when everyone seems to pretty much agree that scale is the difference between the two and scale is hard and exclusive. reply threeseed 3 hours agorootparentprevGPT-4 was released in March 2023. Which means the research that went into it would've been finalised quite some time prior. Meaning that you're getting close to a 2 year head start. reply acheong08 3 hours agorootparentWhile they still call it GPT-4, the one topping the rankings are newer iterations of it despite still retaining the same name. The latest one is from 2024-04-09. Sure that one probably finished training a few months ago but it is by no means a 2 year head start. reply j-bos 6 hours agoparentprevIsn't that why he's making rounds to lock down the biggest AI's? reply hackerlight 14 minutes agoparentprevDepends how good their next model is, and if they prevent leaks and departures so they can prolong the lead for an undetermined amount of time. reply michelsedgh 5 hours agoparentprevI agree with you somewhat. You are correct unless they have a much better GPT model that have not released for whatever reason. They are a year ahead than competitors and GPT4 is pretty old now. I find it hard to believe they don’t have much more capable models now. We Will see though reply easygenes 5 hours agorootparentThere's wide speculation that what will be branded as either GPT-4.5 or GPT-5 has finished pretraining now and is undergoing internal testing for a fairly near-term release. reply michelsedgh 5 hours agorootparentMy speculation is that internally they have much stronger models like Q* but they won’t be able to release them to public even if they want to for lack of compute and safety and other reasons they see probably… reply kaliqt 5 hours agorootparentThey don't actually care about safety, that's a lie, so compute and business strategy is the only thing stopping them. SoRA is the same. It's not ready and it's too slow. reply whimsicalism 3 hours agorootparentprevI am curious whether this is true - OAI at least has the reputation in the industry of caring the least about safety of the major labs reply hhh 2 hours agorootparentIf they don’t care about safety (or perceived safety), why do they spend so much time lobotomizing models for safety reasons? reply torginus 1 hour agorootparentprevHonestly I'm pretty puzzled by this mystical fog that hangs over OpenAIs skunkworks projects - don't people leave for other jobs/go to conferences etc.? I'm surprised that nobody call tell what they infact do or do not have. reply j45 4 hours agorootparentprevThe polish of OpenAI stuff when released has been quite mature since gpt4 or even 3.5. They are no doubt sitting on ultra polished stuff. When you are the tip of the arrow though and the cutting edge itself it might not be as efficient but does it ever show you things you can’t unsee. When OpenAI can launch a video thing a day after because it’s ready to go. I am less and less skeptical e dry time they ship because the quality of the first version isn’t sliding back wards even in different areas like video. Maybe releasing it is strategic, or releasing it also requires supporting it infrastructure wise and then some. That might be a challenge. My feeling is the next model of an k between may have massive efficiency and performance improvements without having to go quantum with brute forcing it. Meanwhile others who are following what OpenAI has done seem to be able to optimize it and make it more efficient whether it’s open source or otherwise. Both are doing important work and I'm not sure I want to see it as a one winner take all game. The way AI vendors are responding suddenly to another’s launch feels like they are always ready to launch and continue to add functionality to it that could also ship. It reminds me of when Google spent a billion dollars advertising bing had a billion pages indexed. Google stayed quiet. Then when the money was spent by Microsoft, Google simply added a zero or two to their search page, when they used to list how many pages they have indexed. They were just sitting on it already done, announcing it when it’s to their benefit. reply imtringued 2 hours agorootparentprevI'm not saying Claude 3 and Gemini are better than GPT4 in every aspect, but those two models can at least perform addition on arbitrarily long numbers, meanwhile GPT4 struggles. reply jiggawatts 5 hours agoparentprevThere’s a Pareto frontier where Meta is pushing out the boundaries along the “private” and “cheap” axes. Open AI can release GPT 4.5 or 5 and push out the boundary in the direction of “correctness” and “multimodality”. Either way, we win as customers while the the level of competition remains this hot. I personally want a smart AI much more than a cheap or fast one. Your mileage may vary. reply mft_ 59 minutes agorootparentWell, Pareto is about optimisation, not either/or. I want a model that’s smart enough, while also being locally-executable. I don’t know whether/when we’ll get there, and whether it will be improvements in models, or underlying model technology, or GPU/TPUs with larger memory at a consumer price point, or something else, that will deliver it. reply 14u2c 5 hours agoparentprevMost of the big \"investments\" in OpenAI are in the form of compute credits. I fail to see the downside of that. reply hiddencost 6 hours agoparentprevI suspect that when it costs 0.5c per 100 million generated token, and you can generate 1000 tokens per second, they'll be very happy. reply moralestapia 5 hours agoparentprevDisclaimer: not a fan of \"Open\"AI Everyone could say anything about open source models, but they're comparing themselves to what OpenAI released a year ago. They haven't shown all of their cards yet and they have a decent moat already in place; some say they have no moat, I disagree, they have one of the best moats possible which is brand awareness. Sora on its own could bring in billions in revenue; an open-source Sora will take at least another year, if not two, to come out. Then more time until it can run on commodity hardware. An open source model that only runs in a dedicated H100 is actually less useful than a closed model behind an API call; not to detract from open source, I think it's the way to go but I'm just being pragmatic and realistic. There's a reason why MS Office is still the top productivity app in the world, even though dozens of open source alternatives exist. reply Hendrikto 1 hour agorootparent> they have one of the best moats possible which is brand awareness. Do they though? If you talk to \"regular people\", everybody knows ChatGPT, but nobody knows or cares about OpenAI. And most of them don‘t even really know that name. They call it ChatUuuuhm, ChatThingy, Chad Gippity, or similar. I think they will just switch, when something better comes along. reply CapeTheory 12 minutes agorootparentGood old Chatty Jeeps reply poslathian 4 hours agorootparentprevMS had yet to fully stabilize that lead a full decade after they had won the os platform standard for ibm compatible pcs. A platform standard moat goes way way beyond a brand advantage. Azure, while significant, has no similar monopoly to support OpenAI. Do you really see a structural advantage to openAI beyond the Microsoft products integrating it? reply modeless 4 hours agoprevI don't need exact results. FP8 quantization is almost lossless and even 6-bit quantization is usually acceptable. Can this be combined with quantization? reply dimask 2 hours agoparent> Can this be combined with quantization? It is in their TODO part in https://github.com/Infini-AI-Lab/Sequoia/tree/main reply mmoskal 3 hours agoparentprevYes. It's speculative decoding but instead of generating just a few sequential tokens with the draft model they generate a whole tree of some sort of optimal shape with hundreds of possible sequences. It ends up being somewhat faster than regular speculative decoding in normal setting (GPU only). If you are doing CPU offloading it's massively faster. Edit typo reply freeqaz 6 hours agoprevSo this is 8x faster for serving these models than before? Or is this about it being more deterministic? I can't quite tell from reading it. reply maccam912 6 hours agoparentThe idea is to serve models that would normally be considered too large for GPU memory (70 billion parameters at 16 bytes each for 140 GB of memory required). Some people figured out you can offload the model and only have parts of it loaded so a 24 GB GPU like the 4090 can still serve the model, but it goes a lot slower. They have a new way to serve the same model on the same GPU but 8x better throughput. Something about decoding tokens on a smaller model maybe, then just checking multiple tokens on the larger model in a single batch. Magic, but ultimately its the same model, same GPU, same output as before, but much better throughout. reply aussieguy1234 6 hours agoprevI'm looking at buying 2 X RTX 3060s to run LLama 70b for my new PC I just purchased. Will this work, or do I need a Tesla P40 or two? reply tarruda 49 minutes agoparentNote that 2 RTX 3060 will probably be significantly slower than RTX4090. Even with RTX 4090, 2 tokens per second is very slow and likely not ideal for most tasks. It is impressive (much faster than previous solutions), but still very slow for real time use. If you want to run Llama 3 70b, might be better to purchase a mac studio with 64gb RAM (more for longer contexts) and run with 4-bit quantization. My 2 cents: For most common tasks Llama 3 8b will be more than enough, and you can run that with full precision using a single rtx 3090. At a much lower cost, you can also run Llama 3 8b with 8-bit quantization in a single RTX 3060, if it has 12GB RAM. reply dannyw 5 hours agoparentprevTheoretically there's no reason why this shouldn't work, but you likely will find the software isn't designed for multi-GPU and have to reimplement/fix things yourself. You will also be getting about 720GB/s of memory bandwidth with 2x3060; instead of 1TB/s with the 4090; so expect lower performance. reply zwaps 2 hours agoprevIs it me or is this paper basically missing all technical information? I get that Therese proprietary technology, but if so, can we please not put this on arxiv and pretend it’s a scientific contribution? reply qrios 23 minutes agoparentThe linked github repo [1] seems to have the code available and well documented. [1] https://github.com/Infini-AI-Lab/Sequoia/tree/main/Engine reply halyconWays 6 hours agoprevSomeone get this into koboldcpp reply thelittleone 5 hours agoprev [–] Other than portability and privacy, are there any benefits to running a local model with a 4090, versus running the same model on-demand on a cloud service with the same or more powerful card? reply razodactyl 5 hours agoparentThere are always going to be pros and cons. That's why solutions like managed databases are reality. From an expert perspective it seems like there's more to lose but from the perspective of a company with employee turn over, possible data loss, security etc. the benefits start to far outweigh the costs. This reasoning can mostly be applied here. If you want to learn about and pull the LLM apart. Perhaps fine-tune and tinker then 100% go ahead running locally. You however won't be able to scale this up easily for a consumer base and the electricity use and heat output starts to become a problem. At some point it's more beneficial to pay the provider for inference, this includes upkeep, latest models, faster generation, stability, hosting etc. Pros and cons! Choice is important and Meta is doing the right thing by the AI community and tech community in general by being realistic with these programs. The ecosystem is giving back by being able to access these high quality models. reply j45 4 hours agorootparentWhat Meta is doing is very nice and differentiates them. I also hope that it ought not change if it became more palatable to not be open. reply kaliqt 5 hours agoparentprevGuaranteed uptime. you are the guarantor but that's good enough. reply imtringued 2 hours agoparentprevIf you have a robot or self driving car, you're going to want on device inference for your vision language models. For video games, being locked to a cloud service means the feature will disappear when the servers are shut down. reply choppaface 4 hours agoparentprev [–] Eventually these models will need to run on mobile devices, so commodity desktop GPUs are a good stepping stone. Alexnet / Caffe got traction because they could be run on commodity desktop machines. Then a few years later phones could run object detection etc. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Sequoia is a decoding framework designed for serving large language models (LLMs) on consumer GPUs without approximation, ensuring low latency.",
      "It accelerates LLM serving by offloading tasks, offering scalability and robustness while adapting to various hardware platforms and model sizes.",
      "Promising performance on current and upcoming hardware positions Sequoia as a versatile solution for hosting powerful LLMs and AI-generated content."
    ],
    "commentSummary": [
      "Discussions focus on comparing GPT-4 with newer AI models like Sonet, highlighting safety measures by major labs like OpenAI.",
      "Explore methods for efficiently serving large models on GPUs and considering local versus cloud services for running models.",
      "Examined factors include portability, privacy, scalability, and industry impacts, with potential applications for on-device inference."
    ],
    "points": 108,
    "commentCount": 43,
    "retryCount": 0,
    "time": 1714878229
  },
  {
    "id": 40262190,
    "title": "Judge Considers Sanctions in Google Monopoly Trial",
    "originLink": "https://arstechnica.com/tech-policy/2024/05/judge-mulls-sanctions-over-googles-shocking-destruction-of-internal-chats/",
    "originBody": "\"We just want to know what we don't know\" — Judge mulls sanctions over Google’s “shocking” destruction of internal chats Punishing Google for being the best would be “unprecedented,” lawyer argued. Ashley Belanger - 5/3/2024, 11:17 PM Enlarge / Kenneth Dintzer, litigator for the US Department of Justice, exits federal court in Washington, DC, on September 20, 2023, during the antitrust trial to determine if Alphabet Inc.'s Google maintains a monopoly in the online search business. Bloomberg / ContributorBloomberg reader comments 107 Near the end of the second day of closing arguments in the Google monopoly trial, US district judge Amit Mehta weighed whether sanctions were warranted over what the US Department of Justice described as Google's \"routine, regular, and normal destruction\" of evidence. Google was accused of enacting a policy instructing employees to turn chat history off by default when discussing sensitive topics, including Google's revenue-sharing and mobile application distribution agreements. These agreements, the DOJ and state attorneys general argued, work to maintain Google's monopoly over search. According to the DOJ, Google destroyed potentially hundreds of thousands of chat sessions not just during their investigation but also during litigation. Google only stopped the practice after the DOJ discovered the policy. DOJ's attorney Kenneth Dintzer told Mehta Friday that the DOJ believed the court should \"conclude that communicating with history off shows anti-competitive intent to hide information because they knew they were violating antitrust law.\" Mehta at least agreed that \"Google's document retention policy leaves a lot to be desired,\" expressing shock and surprise that a large company like Google would ever enact such a policy as best practice. Google's attorney Colette Connor told Mehta that the DOJ should have been aware of Google's policy long before the DOJ challenged the conduct. Google had explicitly disclosed the policy to Texas' attorney general, who was involved in DOJ's antitrust suit over both Google's search and adtech businesses, Connor said. Connor also argued that Google's conduct wasn't sanctionable because there is no evidence that any of the missing chats would've shed any new light on the case. Mehta challenged this somewhat, telling Connor, \"We just want to know what we don't know. We don't know if there was a treasure trove of material that was destroyed.\" Advertisement During rebuttal, Dintzer told Mehta that Google's decision to tell Texas about the policy but not the federal government did not satisfy their disclosure obligation under federal rules of civil procedure in the case. That rule says that \"only upon finding that the party acted with the intent to deprive another party of the information’s use in the litigation may\" the court \"presume that the lost information was unfavorable to the party.\" The DOJ has asked the court to make that ruling and issue four orders sanctioning Google. They want the court to order the \"presumption that deleted chats were unfavorable,\" the \"presumption that Google's proffered justification\" for deleting chats \"is pretextual\" (concealing Google's true rationale), and the \"presumption that Google intended\" to delete chats to \"maintain its monopoly.\" The government also wants a \"prohibition on argument by Google that the absence of evidence is evidence of adverse inference,\" which would stop Google from arguing that the DOJ is just assuming the deleted chats are unfavorable to Google. Mehta asked Connor if she would agree that, at \"minimum,\" it was \"negligent\" of Google to leave it to employees to preserve chats on sensitive discussions, but Connor disagreed. She argued that \"given the typical use of chat,\" Google's history-off policy was \"reasonable.\" Connor told Mehta that the DOJ must prove that Google intended to hide evidence for the court to order sanctions. That intent could be demonstrated another way, Mehta suggested, recalling that \"Google has been very deliberate in advising employees about what to say and what not to say\" in discussions that could indicate monopolistic behaviors. That included telling employees, \"Don't use the term markets,\" Mehta told Connor, asking if that kind of conduct could be interpreted as Google's intent to hide evidence. But Connor disagreed again. \"No, we don't think you can use it as evidence,\" Connor said. \"It's not relevant to the claims in this case.\" But during rebuttal, Dintzer argued that there was evidence of its relevance. He said that testimony from Google employees showed that Google's chat policy \"was uniformly used as a way of communicating without creating discoverable information\" intentionally to hide the alleged antitrust violations. Page: 1 2 Next → reader comments 107 Ashley Belanger Ashley is a senior policy reporter for Ars Technica, dedicated to tracking social impacts of emerging policies and new technologies. She is a Chicago-based journalist with 20 years of experience. Advertisement Channel Ars Technica ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=40262190",
    "commentBody": "Judge mulls sanctions over Google's \"shocking\" destruction of internal chats (arstechnica.com)107 points by botanical 5 hours agohidepastfavorite90 comments mjburgess 1 hour agoCritical information which commenters here are missing: > The Federal Rules of Civil Procedure *required* Google to suspend its auto-delete practices in mid-2019, when the company reasonably anticipated this litigation. > Google did not. > Instead, as described above, Google abdicated its burden to individual custodians to preserve potentially relevant chats. Few, if any, document custodians did so. That is, few custodians, if any, manually changed, on a chat-by-chat basis, the history default from off to on. This means that for nearly four years, Google systematically destroyed an entire category of written communications every 24 hours. reply edarchis 5 hours agoprev\"Google was accused of enacting a policy instructing employees to turn chat history off by default when discussing sensitive topics, including Google's revenue-sharing and mobile application distribution agreements.\" They didn't ask employees to destroy evidence but to avoid retaining evidence in the first place. Not leaving sensitive information in logs, backups etc is quite reasonable, even if it would have been useful to justice here. reply EMIRELADERO 4 hours agoparentThis comment[1] provides a good explanation on why this analogy is absurd: > Chats at google by default have 24 hours of chat history. (That is, after 24 hours, the chat history is delelted.) You can opt in to having 30 days of chat history instead. And when under a legal hold, Google continues to delete chats in the 24 hour history mode, but will not delete chats in the 30 day history mode. > That is, Google's theory here seems to be that if you have a policy to destroy certain letters and memos 24 hours after receiving or creating them, then you don't need to stop doing that and preserve them even if under a court ordered legal hold. But if your policy is to destroy certain documents 30 days after creating them, then you must stop deleting them and retain them if ordered by a court. > Which is....a.....theory! [1] https://news.ycombinator.com/item?id=35587100 reply _huayra_ 4 hours agorootparentI haven't worked at Google, so maybe there are better channels than \"chat that's holding everything together\", but I can't imagine that being unable to search a chat and look at who said what and when is useful. I use it all the time to look up some random useful command a coworker told me long ago that I neglected to write down, for instance, or to follow up with a long-stale thread to revive it. I suppose that's what email and design docs are for, but I feel a lot of that \"organizational glue\" in chat is quite valuable. How do Googlers live without it? reply mike_d 3 hours agorootparentEveryone bitches about it when they first start and eventually learns a better way. The culture is about creating explicit artifacts of knowledge (design docs, documentation, commit/review messages, etc) rather than trying to dig around in streams of consciousness hoping to find the bits of info you need later. reply rwiggins 3 hours agorootparentExactly this. It's not uncommon to ask a coworker how to do something, get a response, and then send them a follow-up CL adding that information to a playbook or doc somewhere so others can reference it. Heck, sometimes I responded to questions with a CL adding that info to docs. The internal search engine helps drive a lot of this, too - if you want to know how to do something, docs (via search) are like the #1 choice. So, everyone's pretty incentivized to make it a good resource. reply barrkel 42 minutes agorootparentprevChat isn't a good way to store information. reply ars 3 hours agorootparentprevPresumably it would be better if it was a phone call? Does a legal hold require recording all calls? > > Which is....a.....theory! I guess their theory is these ephemeral chats are like phone calls, as opposed to written communication. reply rokkitmensch 2 hours agorootparentWall Street records all calls. Has forever, since long before storage was cheap. reply whimsicalism 2 hours agorootparentBanks are required to by the SEC reply ruined 4 hours agoparentprevno. they were under legal obligation to retain that evidence, and were lying in court and claiming that they were retaining that evidence. https://storage.courtlistener.com/recap/gov.uscourts.dcd.223... >All this time, Google falsely told the United States that Google had \"put a legal hold in place\" that \"suspends auto-deletion.\" Indeed, during the United States' investigation and the discovery phase of this litigation, Google repeatedly misrepresented its document preservation policies, which conveyed the false impression that the company was preserving all custodial chats. Not only did Google unequivocally assert during the investigation that its legal hold suspended auto-deletion, but Google continually failed to disclose—both to the United States and to the Court—its 24-hour auto-deletion policy. Instead, at every turn, Google reaffirmed that it was preserving and searching all potentially relevant written communications. reply fyrn_ 4 hours agoparentprevUnder 18 U.S.C. § 151 \"Concealment\" is obstruction of justice, and almost certainly illegal in this case. There is a wide body of legal precedent on the topic, for example moving an email to the trash does not count, but \"delete forever\" (called \"double delete\" in the opinion) does. Have your chat setup to autodelete does not get you out of that. Especially when they have well documented how they only instructed to do that for legally sensitve topics. reply radium3d 4 hours agorootparentThat's interesting, but how is that any different at all from discussing a topic in person in which it is immediately discarded so long as it is not recorded? Seems identical and it should not be considered concealment in any way, even if it's for \"sensitive\" meetings, as they could have just as easily been done in person verbally with no recordings. This really feels like authorities overstepping because they want to know things they weren't meant to know and are just personally offended. reply fyrn_ 55 minutes agorootparent> after the filing of a case under title 11 or in contemplation thereof, knowingly and fraudulently conceals, destroys, mutilates, falsifies, or makes a false entry in any recorded information (including books, documents, records, and papers) relating to the property or financial affairs of a debtor; or It's different because the law covers recorded information, and the law cares about provable intent. Worth pointing out that this law is old with a lot of modern intepretations be wary of treating this literal text as the only substance of it reply airspresso 4 hours agorootparentprevDoing potentially illegal business verbally with no written trail is a well-known strategy. It is still concealment if those participating do not tell when interrogated. Bottom line is that companies should assume that the government has a right to audit their practices at any time for any reason, and avoid intentionally destroying potentially valuable information. reply oatmeal1 4 hours agoparentprev> They didn't ask employees to destroy evidence but to avoid retaining evidence in the first place. It certainly sounds like destruction. The words are written, then the words are encoded as text, stored, and sent, and then at the end, discarded. reply nl 4 hours agorootparentIf the chat is E2E encrypted this isn't really the case. There is no storage. reply EMIRELADERO 4 hours agorootparentThere is storage, it's just that what's stored is encrypted with a key that only the clients possess. A court could very well order the party involved to turn over the encryption keys. reply jsiepkes 3 hours agorootparentprevThis is about Google Chat (in Google Workspace) which doesn't have E2E. reply dragonwriter 1 hour agoparentprev> They didn't ask employees to destroy evidence but to avoid retaining evidence in the first place. Once evidence exists, not retaining it involves deleting it. So you’ve simply rephrased what they are accused of and framed it as if that is a denial. Which, you know, is kind of all there is to spin with when there are no favorable facts, but why spin for Google? reply 1vuio0pswjnm7 3 hours agoparentprev\"They didn't ask employees to destroy evidence but to avoid retaining evidence in the first place.\" Once a company knows its going to be sued it has a legal obligation under federal rules to preserve evidence. Google deliberately ignored that requirement. Nice try. reply busterarm 1 hour agoparentprevStringer Bell said it best: \"is you taking notes on a criminal fucking conspiracy?\" reply colechristensen 4 hours agoparentprevRegulated industries just require saving of communications. It seems like maybe that should be the standard. reply ceinewydd 4 hours agorootparentDo you think that should extend to recording some or all meetings at a company? A group of executives meets. Should that meeting be recorded, even if it’s not a “hybrid” meeting and entirely happened in person? A group of engineers meets. How about that one? This seems like a hard issue. If the court creates a precedent here, I expect any sensitive discussions that might have any sort of future liability will just go back to verbal conversations (if allowed), and then aren’t we back to where we are today, with no record? reply bdd8f1df777b 21 minutes agorootparentIf that becomes true, both remote work AND offshoring may soon be increasingly rare at Google. reply bawolff 4 hours agorootparentprevThere does seem a big difference between requiring things to be recorded and simply requiring that people don't actively take action to prevent recording. reply stale2002 4 hours agorootparentprev> Do you think that should extend to recording some or all meetings at a company? That depends. Did the court order that to happen? (Like in this situation!) If so, then yes, the company should follow the lawful order of the court. And if such an order is unlawful, then they should appeal it, and do what the appeal court orders them to do. reply dragonwriter 53 minutes agorootparentprev> Regulated industries just require saving of communications This isn't a regulated industries issue: all parties to actual or reasonably anticipated litigation are required to preserve evidence. reply acheong08 5 hours agoprevThe subtitle is really weird: > Punishing Google for being the best would be “unprecedented,” lawyer argued. How does deleting evidence make you “the best”? Also, obstruction of justice charges aren’t exactly unprecedented. We need obstruction of justice to be harsher than the crime itself to discourage blatant hiding of evidence reply nl 4 hours agoparentIt's weird writing in the article to include that. In the article this comes from a completely different argument in a completely different point against a MS argument that Google has violated the Sherman act in the same way MS did. The quote is from Google attorney who pointed out that all the companies that had signed exclusive deals with Google had testified that they chose Google because it was the best. It doesn't seem to have anything at all to do with the chat history argument. reply ceroxylon 4 hours agorootparentI felt the same way, it is almost as if they have a blog-generating AI prompt that says something along the lines of \"make the subheading a provocative counterpoint\". reply fbdab103 5 hours agoparentprevIf you have the facts on your side, pound the facts; if you have the law on your side, pound the law; if you have neither the facts nor the law, pound the table. reply tivert 5 hours agoparentprev> How does deleting evidence make you “the best”? Google is full of really clever people who know how clever they are. The government should just give up now and Google should just win by default. reply 77pt77 5 hours agorootparentIf the government wanted the entirety of google would be Boeinged tomorrow. The reverse is not true. There's power in information, but physical power trumps that. reply eastbound 2 hours agorootparent“Boeinged” means, heavily subsidized, under protection of all enforcement agencies (FAA) against the bad evils of the European Union, and every whistleblower would die like the two of Boeing. I don’t think it’s what you meant. reply JumpCrisscross 4 hours agoprevSorry, did I commit fraud by discussing business over dinner without a stenographer present? If you’re ordered to retain communications, sure. And in a civil court, it’s fair to conclude adversely if a party’s messages disappear suspiciously. But by the government? No. reply smt88 4 hours agoparentThe argument being made is that Google execs switched over to Signal when they wanted to discuss the crimes they were committing. They weren't always using Signal. They had many thousands of emails and other communications that were preserved and used as evidence. reply orangepanda 2 hours agorootparentMaybe the execs got tired of having to install yet another messenger, when the previous one gets deprecated? reply busterarm 1 hour agorootparentprevWouldn't be surprised. One startup I worked at launched their entire own separate private chat app/company and all of the execs and early employees used exclusively that software to communicate with each other, rather than email or the company slack. reply cornel_io 4 hours agorootparentprevI use Signal by default for every piece of communication that I can. Am I somehow held to a different standard just because I was paranoid about surveillance from the start? That seems wrong. reply zarzavat 2 hours agorootparent> Am I somehow held to a different standard just because I was paranoid about surveillance from the start? Yes, how else could it work? For example if you have a policy to destroy CCTV footage at the end of the day then it’s not illegal to do that. If you have a policy to keep CCTV footage but when the police come and ask you some questions you delete it because you know that it contains evidence of a crime, that’s destruction of evidence. It’s not illegal to delete things, otherwise the world would run out of storage, it’s illegal to delete things to knowingly impede an investigation. reply cornel_io 48 minutes agorootparentFair enough. I'm never talking to anyone over insecure channels again. reply ikiris 4 hours agorootparentprevIf you're telling a judge \"I specifically hide all written communication that I'm legally required to hold and produce so that you can't see it\" you're gonna have a bad time. reply dyauspitr 3 hours agorootparentprevSo what? Everyone everywhere does that about sensitive stuff. How many times have you heard- call me about it I don’t want to put it in writing. reply smt88 3 hours agorootparentSo you believe our legal system should not punish executives of a company that is: - publicly traded - more powerful than many entire countries - committing crimes ...when those executives explicitly switched to disappearing messaging in order to cover up those crimes? Context matters. I don't want to live in a world where people can avoid prosecution just by deleting evidence. reply Sakos 1 hour agorootparentI'm utterly appalled at how vigorously people are defending Google here. These are not the good guys. These aren't deserving of extra protection from the rabble. They can defend themselves in court with the billions of dollars they have. The kind of power and influence Google wields requires a higher standard. It's not being punished for success. It's ensuring that success doesn't allow them to subvert the law whenever they want. God forbid we have a functioning justice system anywhere. reply ETH_start 1 hour agorootparentprevPunishing them solely for their success is unfair. If power accumulated through capturing natural monopolies is the issue, then we need to address that directly with public funding of open source and decentralized alternatives that can become the market standard. Not punish people who fill the gaps the public left, by effectively providing essential services. reply seankurtz 1 hour agorootparentI think you really ought to justify your use of the terminology \"natural monopoly\". To the extent that such a thing exists, is this not exactly what we have anti-trust laws to prevent? It seems to me that we can, have, and should punish people for success if that success is likely to lead to well known kinds of market failure, and that such a policy is generally non-controversial outside of the most extreme forms of libertarian political philosophies (which unfortunately, are disproportionately prevalent in big tech and likely to get the whole industry in serious hot water). Certainly, you could make the same \"natural monopoly\" argument against just about every serious anti-trust action that has taken place in the past. It didn't work then and I don't think its going to work now. Pretending that closed platforms that exist on computer networks (and are not themselves networks) somehow makes this all different is not very convincing. Network effects, as we've come to understand the term, are not new, and have existed before computer networks. To the extent they exist, it's evidence of need for intervention because of a clear failure of a market to self regulate (what some may call a \"natural\" or just a regular monopoly) and maintain a competitive market, not evidence that everything is okay. I also seriously take issue with the idea that Google provides an \"essential service\". Power, food, water, and you could make an argument for computer and communications infrastructure, are \"essential\". Google search is not. If it disappeared, society would soldier on and we would all be fine. At worst it would be a minor inconvenience for users and a headache for IT departments and developers. reply ETH_start 1 hour agorootparentI think anti-trust laws are unjust for precisely that reason: they punish people solely for being successful. I also offered an alternative solution to private interests capturing natural monopolies: the state subsidizing a public option that can do that. My preference would be state funding for the development of open source software and decentralized platforms that can substitute for proprietary software and centrally managed platforms. In fact, the state is ideally situated to fund this kind of public goods development, as it is the only legal entity with a broad enough tax collective apparatus to capture the gains from such investments. As for essential/non-essential, perhaps you're right in terms of terminology. In any case, internet search is a widely used and extremely valuable service that enhances quality of life. >such a policy is generally non-controversial outside of the most extreme forms of libertarian political philosophies (which unfortunately, are disproportionately prevalent in big tech and likely to get the whole industry in serious hot water). Something being non-controversial doesn't make it right. It was uncontroversial anti-libertarian ideology that found a way to justify imprisoning hundreds of thousands of elderly people for a year during the COVID pandemic: https://www.cbc.ca/amp/1.5969825 reply dragonwriter 1 hour agoparentprev> If you’re ordered to retain communications, sure. And in a civil court, it’s fair to conclude adversely if a party’s messages disappear suspiciously. This is civil court. reply fsckboy 5 hours agoprevThe allegation in the article is not that google destroyed internal chats, but rather had instructed staff to turn off the chat saving feature before having such chats. Just as inculpatory, but not the same thing: \"Google was accused of enacting a policy instructing employees to turn chat history off by default when discussing sensitive topics, including Google's revenue-sharing and mobile application distribution agreements. These agreements, the DOJ and state attorneys general argued, work to maintain Google's monopoly over search.\" >Punishing Google for being the best would be “unprecedented,” lawyer argued. the article doesn't seem to cover this subhead, but I'm guessing this is a reference not to failing to keep records of their anti-competitive practices, but to something along the lines of \"google only seems like a monopoly because they're such a good competitor they wind up with all the customers\" reply dragonwriter 42 minutes agoprevNote that Google was sanctioned last year for similar conducted in the anti-trust litigation over the Play Store as they are now facing sanctions over in the anti-trust litigation over adtech. https://sfstandard.com/2023/03/28/judge-sanctions-google-for... reply ken47 4 hours agoprevTo those arguing about the semantics of deletion vs. never having saved it in the first place, do you believe the DOJ would make this claim without having considered such trivialities? reply px43 4 hours agoparentThe DOJ will make whatever claim they feel like to extend their surveillance apparatus as far and wide as possible. reply cornel_io 4 hours agoparentprevYes. The DOJ sometimes (if not often) overreaches because they are taking a shot. These sorts of cases are often attempts to establish precedent that they can use to go after others under similar theories. Flipping this around, do you think Google's legal team, which is probably equally qualified and vastly more well paid (and with a lot more to lose if they're wrong), would advise employees to do this if they hadn't considered such trivialities? reply ken47 4 hours agorootparent> do you think Google's legal team, which is probably equally qualified and vastly more well paid (and with a lot more to lose if they're wrong), would advise employees to do this if they hadn't considered such trivialities? Depends on what their risk-reward appetite is. Competent people will make surprising decisions under pressure. reply bdd8f1df777b 18 minutes agorootparent> Depends on what their risk-reward appetite is. That's the same for DoJ. In fact, for them it's all wins and no loses. In the worst case their argument just doesn't stick. reply cornel_io 50 minutes agorootparentprevGoogle has great lawyers. They don't flinch or buckle. reply dcchambers 5 hours agoprevELI5: why is there any expectation for corporations to keep chat logs or emails long term? Is there some legal requirement I don't understand? reply graton 4 hours agoparentGoogle records chat by default. But it appears they had a policy if they were going to discuss something that could look bad if subpoenaed that chat recording should be turned off before discussing it. reply tylerhou 2 hours agorootparentNo, this is not true. By default, starting a private chat with another employee defaulted to “history off” (as far as I can remember; I left Google >1 year ago now). reply cornel_io 4 hours agorootparentprevRight, but I'm not clear, is that actually illegal? I've seen similar policies at many companies. reply dwaite 4 hours agorootparentThey were under order from the court to preserve communications, and changed their corporate policy to use channels that auto-deleted communications when talking about things that they might not want the court to see. reply juped 4 hours agoparentprevWhen you're in or reasonably expect to be in litigation, you have an obligation to preserve evidence for discovery. reply nullc 3 hours agorootparentYou do not have an obligation to create records for that purpose, however, only to preserve what you do create. reply Dudhbbh3343 3 hours agorootparentprevIt's ridiculous that a private company can be forced to make private discussions available to be used against themselves. reply ARandomerDude 5 hours agoparentprevRead the article reply derwiki 4 hours agorootparentI just read the article and it doesn’t directly answer the question. Why would you be allowed to have unrecorded conversations in the office? Doesn’t that show the same intent? reply graton 4 hours agorootparentIs the default to record all audio conversations in the office? No. Is the default to record all chat conversations? Yes. Then Google enacted a policy to have employees turn off the recording of chat conversations if they were going to discuss certain topics. reply px43 4 hours agorootparentI've been primarily using end to end encrypted self destructing chats for maybe 20 years at this point, especially for work stuff. Why should other people's poor data hygiene put my data at risk? reply ikiris 4 hours agorootparentIts really simple, if you're in litigation, you are legally obligated not to destroy written records, and produce them based on discovery requests. If you delete chat, that is destroying written records. reply denton-scratch 1 hour agorootparentIsn't Google always in litigation? Are all written records supposed to be preserved when you're in litigation, or just relevant records? Disclosure issues, and destruction of records, are prominent issues in two UK scandals just now: the Contaminated Blookd Scandal, and the Post Office Horizon scandal. reply anothername12 5 hours agoprevIs there a technical name for the two tiered legal system we have here? The average jack off is never going to get away with destroying evidence, yet the rich, and corporations are given deference all the time for all sorts of shit. reply SOVIETIC-BOSS88 5 hours agoparentA double standard. Sadly present in most communities since the dawn of humanity. reply cess11 4 hours agorootparentAre you sure about that? reply Sakos 1 hour agoparentprevYou have people here defending Google. So, apparently the average jack off is okay with a two tiered system. reply nl 4 hours agoparentprevI think most people can turn off their chat history too? And indeed having a policy of always turning your chat history off for sensitive conversations is unlikely to be sanctioned unlike in this case. (Google didn't destroy chat logs, they just had history off for them) reply dwaite 3 hours agorootparentYou may find the judge isn't willing to entertain a game of semantics. My understanding is it is as simple as the following: Google was ordered to retain information, and afterward specifically changed their policies such that the information would not be retained, and also misled the court that the information was being properly retained. reply mcmoor 4 hours agoparentprevI've heard the idiom of \"knive policy\", sharp at the bottom, dull at the top. reply petesergeant 4 hours agoprevCertainly in the UK there are personal implications for both employees and company directors when breaking certain laws — I have in mind the Bribery Act 2010, which takes an absolutely scorched earth approach to bribery. Hard to see a reason not to apply that to discoverable communications of public companies too. reply Dudhbbh3343 3 hours agoparent> Hard to see a reason not to apply that to discoverable communications of public companies too. Maybe we don't want to live in a (self-imposed) surveillance state? reply ETH_start 1 hour agoprevThe state ought to have no legal right to compel people to keep records of their private communications. reply nullc 5 hours agoprevBeing able to privately discuss matters is apparently a freedom only afforded to personalities that are comfortable speaking voice in real-time. The same policy but in the form of \"discuss these things over the phone\" would not just be unremarkable, it's the norm. The failure to extend the same treatment to unlogged text chats is discriminatory to people with disabilities and different personalities. ... and it's at odd with the underlying principle that when you know you're subject to litigation you're required to retain records you created but you're not obligated to create new records. reply juped 5 hours agoprevLearn to use the phone when you want an undiscoverable chat, like everyone else on the planet. reply ceroxylon 4 hours agoparentAs long as you're not a target: https://www.wireshark.org/docs/wsug_html_chunked/ChTelRTP.ht... https://www.cs.unc.edu/~fabian/papers/foniks-oak11.pdf https://en.wikipedia.org/wiki/Wiretapping reply ARandomerDude 4 hours agoparentprevSadly we live in an era in which mass wiretapping, automated transcription and flagging, and parallel construction are not unreasonable concerns. reply varjag 3 hours agorootparentWiretapping google executives is two orders of legal magnitude more difficult than sending the company a discovery order. reply rl3 5 hours agoparentprevTechnically that's discoverable, too. Just needs to be a national security matter. reply hindsightbias 4 hours agoprev [–] my corp encourages using slack and said they’re going to purge stuff a couple years old. I guess we’re entering an era of come at us, fed bros. Iron Mountain is… empty. reply dwaite 4 hours agoparent [–] Is your corp under court order to preserve written communications? That may be an important difference between them and Google. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The US Department of Justice accused Google in the ongoing monopoly trial of intentionally destroying internal chats linked to antitrust violations, aiming to preserve its search monopoly.",
      "Google's attorney argued that the deleted chats were insignificant for the case, while the judge raised concerns about Google's document retention practices and discussed potential sanctions against the company.",
      "The DOJ requested four sanctions, including presuming the deleted chats were detrimental to Google and preventing Google from using lack of evidence as a defense in the trial, prompting debates on Google's motives for deleting chats and the implications of its chat policy on concealing antitrust violation evidence."
    ],
    "commentSummary": [
      "Google is facing criticism for potentially deleting internal chats on sensitive subjects by directing employees to disable chat history, sparking a debate about the significance of such chats and the legal consequences of not preserving evidence.",
      "Discussions encompass Google's transition to Signal for communication, worries about widespread surveillance, and the requirement for fair treatment in legal scenarios.",
      "Emphasizing the significance of documenting knowledge in organizational culture, there are calls for companies to maintain communication records when mandated by the court."
    ],
    "points": 107,
    "commentCount": 90,
    "retryCount": 0,
    "time": 1714882436
  }
]
